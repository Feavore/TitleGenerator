URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.04906v1,Faster feasibility for dynamic flows and transshipments on temporal networks,"In this paper we study flow problems on temporal networks, where edge capacities and travel times change over time. We consider a network with n𝑛nitalic_n nodes and m𝑚mitalic_m edges where the capacity and length of each edge is a piecewise constant function, and use μ=Ω⁢(m)𝜇Ω𝑚\mu=\Omega(m)italic_μ = roman_Ω ( italic_m ) to denote the total number of pieces in all of the 2⁢m2𝑚2m2 italic_m functions. Our goal is to design exact algorithms for various flow problems that run in time polynomial in the parameter μ𝜇\muitalic_μ. Importantly, the algorithms we design are strongly polynomial, i.e. have no dependence on the capacities, flow value, or the time horizon of the flow process, all of which can be exponentially large relative to the other parameters; and return an integral flow when all input parameters are integral.Our main result is an algorithm for checking feasibility of a dynamic transshipment problem on temporal networks – given multiple sources and sinks with supply and demand values, is it possible to satisfy the desired supplies and demands within a given time horizon? We develop a fast (O⁢(μ3)𝑂superscript𝜇3O(\mu^{3})italic_O ( italic_μ start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ) time) algorithm for this feasibility problem when the input network has a certain canonical form, by exploiting the cut structure of the associated time expanded network. We then adapt an approach of [HT00] to show how other flow problems on temporal networks can be reduced to the canonical format.For computing dynamic transshipments on temporal networks, this results in a O⁢(μ7)𝑂superscript𝜇7O(\mu^{7})italic_O ( italic_μ start_POSTSUPERSCRIPT 7 end_POSTSUPERSCRIPT ) time algorithm, whereas the previous best integral exact algorithm runs in time O~⁢(μ19)~𝑂superscript𝜇19\tilde{O}(\mu^{19})over~ start_ARG italic_O end_ARG ( italic_μ start_POSTSUPERSCRIPT 19 end_POSTSUPERSCRIPT ). We achieve similar improvements for other flow problems on temporal networks.","With the rapid ongoing deployment of constellations of small and nano satellites, intersatellite communication systems such as Starlink are quickly realizing the potential of a super fast “space internet”, bringing access to remote parts of the world [Han18]. But these systems present new challenges for algorithm design: standard routing and communication protocols designed for static networks do not work as-is on space networks. Objects in space are constantly in motion, and the ability for one object to communicate with another may exist at some times but not at others, such as when their connection is blocked by a planetary body or other object (see Figure 1). Further, the movement of objects may change the time it takes a message to travel from one point to another, depending on when the message departs. This adds temporal effects to routing in space networking that are not present in traditional networking. Temporal effects are also present in terrestrial networks, such as for example transportation networks. Consider a shipping company like FedEx transporting large loads between different cities. The scheduling of vehicles faces many temporal constraints – coordination with scheduled flights; transit times on highways varying over the course of a day; the availability of vehicles or drivers varying over time, etc. These constraints place transport networks outside the realm of settings most routing or flow algorithms are designed for. In this work we develop fast strongly polynomial time algorithms for flow problems in temporal networks. Figure 1: A visualization of the types of changes that can happen in space networks. Due to changes in orientation or movement of other celestial objects, capacities and travel times associated with individual connections may change over time. We focus on a network model where the existence, capacities, and lengths (i.e. travel times) of edges can vary with time but those variations are known in advance. This is a reasonable model for both of the applications mentioned above. In space networking, for example, objects move according to predictable patterns. The standard “contact graph” model [FDB21, HSC+22] assumes that connections between network nodes exist only for a specified period of time, but all such periods are known ahead of time, and each connection, or edge, has an associated length and capacity. Likewise, for transportation networks, transit times and other temporal constraints are often fixed in advance or predictable. We call networks with time-varying capacities or edge lengths temporal, and those with fixed parameters static.111Note that flow problems on “static” networks are different from “steady-state” flow problems as, in the former, flow takes time to traverse edges. Before we describe the problems we study, let us specify some key parameters and features of our setting. We consider a network with n𝑛nitalic_n nodes and m≤(n2)𝑚binomial𝑛2m\leq{n\choose 2}italic_m ≤ ( binomial start_ARG italic_n end_ARG start_ARG 2 end_ARG ) possible edges. Each edge in the network has a capacity and a length, which are functions of time. We assume that all capacities and lengths are integral. For edge i⁢j𝑖𝑗ijitalic_i italic_j with i,j∈[n]𝑖𝑗delimited-[]𝑛i,j\in[n]italic_i , italic_j ∈ [ italic_n ], we use μi⁢jsubscript𝜇𝑖𝑗\mu_{ij}italic_μ start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT to denote the number of times either of these features, length or capacity, of the edge change. Let μ:=∑i,j∈[n]μi⁢jassign𝜇subscript𝑖𝑗delimited-[]𝑛subscript𝜇𝑖𝑗\mu:=\sum_{i,j\in[n]}\mu_{ij}italic_μ := ∑ start_POSTSUBSCRIPT italic_i , italic_j ∈ [ italic_n ] end_POSTSUBSCRIPT italic_μ start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT. In problems involving multiple sources or sinks, we use k𝑘kitalic_k to denote the number of sources and sinks. We consider flow problems with a finite time horizon T𝑇Titalic_T, but we think of T𝑇Titalic_T as being much larger (e.g., exponential) than the other parameters of the problem, such as m𝑚mitalic_m, n𝑛nitalic_n, and μ𝜇\muitalic_μ. Accordingly we are primarily interested in strongly polynomial time algorithms that run in time poly⁡(n,m,k,μ)poly𝑛𝑚𝑘𝜇\operatorname{poly}(n,m,k,\mu)roman_poly ( italic_n , italic_m , italic_k , italic_μ ). Finally, as we are considering networks that are constantly changing, we are primarily interested in the regime where μ>m,n,k𝜇𝑚𝑛𝑘\mu>m,n,kitalic_μ > italic_m , italic_n , italic_k. There are a variety of interesting network problems under this model and we discuss them in detail in the following subsection. Our work focuses on the Dynamic Transshipment problem, where we are given a vector of demands and supply at different nodes in the network and ask whether there is a feasible flow over the period [0,T]0𝑇[0,T][ 0 , italic_T ] that satisfies these parameters. This problem is a key subroutine for many flow variants, such as maximum flow, quickest flow, and quickest transshipment. The seminal work of Hoppe and Tardos [HT00] was the first to develop strongly polynomial time algorithms for flow problems on temporal networks. For the Dynamic Transshipment problem on temporal networks, the Hoppe-Tardos approach achieves a running time of O~⁢(μ19)~𝑂superscript𝜇19\tilde{O}(\mu^{19})over~ start_ARG italic_O end_ARG ( italic_μ start_POSTSUPERSCRIPT 19 end_POSTSUPERSCRIPT ) in the large μ𝜇\muitalic_μ regime. We leverage the framework and reductions presented in Hoppe and Tardos to design our algorithms. Our primary technical contribution is a much more efficient feasibility subroutine that exploits the cut structure of the temporal network. This allows us to obtain an O⁢(μ7)𝑂superscript𝜇7O(\mu^{7})italic_O ( italic_μ start_POSTSUPERSCRIPT 7 end_POSTSUPERSCRIPT ) time algorithm for Dynamic Transshipment, a significant improvement. It is worth noting that [SST22] provide an algorithm for dynamic transshipment on static networks, that for temporal networks with large μ𝜇\muitalic_μ implies an O~⁢(μ7)~𝑂superscript𝜇7\tilde{O}(\mu^{7})over~ start_ARG italic_O end_ARG ( italic_μ start_POSTSUPERSCRIPT 7 end_POSTSUPERSCRIPT ) time algorithm.222Here we use O~~𝑂\tilde{O}over~ start_ARG italic_O end_ARG to suppress polylogarithmic terms. However, Schloter et al.’s algorithm returns a fractional flow whereas ours (and Hoppe-Tardos) returns an integral solution. To our knowledge, apart from Hoppe-Tardos, no other integral strongly polynomial time algorithms were known for this setting prior to our work. We now describe the literature on flow problems in temporal networks and our contributions in more detail. Flow in time-varying networks and our contributions One of the most basic routing problems on temporal networks is single source shortest paths (SSSP) – what is the earliest time that a message departing a location s𝑠sitalic_s at time 00 can arrive at a destination node? The complexity of this problem depends on whether or not we are allowed to temporarily “store” packets at intermediate nodes to wait out changes in capacity of travel times.333For SSSP, an equivalent assumption to the model where waiting is allowed is to assume that all travel times are FIFO, or in other words that for all t<t′𝑡superscript𝑡′t<t^{\prime}italic_t < italic_t start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT and all edges i⁢j𝑖𝑗ijitalic_i italic_j a message departing i𝑖iitalic_i at time t𝑡titalic_t cannot arrive at j𝑗jitalic_j later than a message departing i𝑖iitalic_i at time t′superscript𝑡′t^{\prime}italic_t start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT. In this paper, we will primarily use the waiting notation because it is more compatible with the flow problems we want to solve, and with the contact graph models often used in space networking. When storage is not allowed, the SSSP problem is NP-hard [Zei23]. In fact, [Zei23] shows that the problem is strongly NP-hard even when all travel times are forced to be piecewise linear. Jain et al. discuss how a variation of Dijkstra’s algorithm can solve this problem when the waiting is allowed [JFP04]. [MKSK+23] and [HMKSC23] further explore this problem and runtime optimization in the contact graph setting where waiting is allowed. These hardness results carry over to flow problems as well; accordingly, henceforth we assume that nodes in the network are endowed with unlimited storage. Our focus is on exact algorithms for flow and transshipment problems, and we consider the following variants. • Maximum flow on static/temporal networks (MFSN/MFTN): Given a static/temporal network with a single source and sink s𝑠sitalic_s and d𝑑ditalic_d and a time horizon T𝑇Titalic_T, find a flow that maximizes the net flow arriving at d𝑑ditalic_d (and has net 00 flow at all other nodes at the end of the period). The maximum flow problem for static networks dates back to the work of Ford and Fulkerson [FF56, FF58], and the problem can in fact be reduced to the (steady-state) minimum cost circulation problem, for which there are a variety of solutions (e.g. [Orl88, CKL+22]). Skutella provides a nice summary of these results [Sku09], and Fleischer and Tardos study a continuous-time extension of this problem (as well as several others) [FT98]. For temporal networks, the maximum flow problem (and a more complex variant called the universal maximum flow problem) have been studied and can be solved efficiently in the case that capacity functions are piecewise constant and all travel times are uniformly 00 [Ogi88, Fle99]. • Quickest flow on static/temporal networks (QFSN/QFTN): Given a static/temporal network with a single source and sink s𝑠sitalic_s and d𝑑ditalic_d and a positive real value v𝑣vitalic_v, find the smallest time horizon T𝑇Titalic_T such that there is a flow over time horizon T𝑇Titalic_T under which the net flow into d𝑑ditalic_d is v𝑣vitalic_v. This problem has been extensively studied on static networks [BDK93, LJ15, SS17a], and Hoppe and Tardos [Hop95, HT95, HT00] give a reduction of this problem (and in fact the more general transshipment variant) on temporal networks to quickest transshipment problem on static networks. • Quickest transhipment on static/temporal networks (QTSN/QTTN): Given a static/temporal network with node set V𝑉Vitalic_V and a vector of values v∈ℝ|V|𝑣superscriptℝ𝑉v\in\mathbb{R}^{|V|}italic_v ∈ blackboard_R start_POSTSUPERSCRIPT | italic_V | end_POSTSUPERSCRIPT (called the demand vector), find the smallest time horizon T𝑇Titalic_T such that there is a flow over time horizon T𝑇Titalic_T under which the net flow at time T𝑇Titalic_T into any node i∈V𝑖𝑉i\in Vitalic_i ∈ italic_V is visubscript𝑣𝑖v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. In the case that a temporal network has a single sink and piecewise constant capacity functions and all transit times are uniformly 00, [HO82] and [Fle01] give efficient algorithms for solving this problem. Hoppe and Tardos [Hop95, HT95, HT00] and Schloter et al. [SS17b, SST22] give efficient algorithms for this problem on static networks. Further, Hoppe and Tardos show that this problem on temporal networks reduces to the same problem on static networks, so in fact all of these results can also be used to solve the quickest transshipment problem on temporal networks. Additionally, the flow obtained by the Hoppe and Tardos algorithm is guaranteed to be integral if the input values are integral, while the Schloter et al. results may return a fractional solution. [FS02] and [HHS07] additionally show hardness and approximation results for a multicommodity variant of this problem on static networks. One may also consider the minimum cost variants of max flow and quickest transshipment; these problems are NP-hard even on a static network [KW95, KW04]. However [Sku23] shows that the quickest minimum cost transshipment problem is tractable - that is, it is tractable to find the flow of minimum time among all flows of minimum cost but not the one of minimum cost among all flows of minimal time. In this paper, we will primarily focus on the Dynamic Transshipment problem on Static/Temporal Networks (DTSN/DTTN), in which we receive a static/temporal network N𝑁Nitalic_N, a time horizon T𝑇Titalic_T, and a demand vector v𝑣vitalic_v, and the goal is to find a flow over the period [0,T]0𝑇[0,T][ 0 , italic_T ] such that the net flow into each node i𝑖iitalic_i is visubscript𝑣𝑖v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, if such a flow exists. Note that the quickest transshipment and maximum flow problems can be viewed as generalizations of this problem, which is why we begin by focusing on this simpler question. Most of the work discussed here will revolve around modifications to the transshipment algorithm proposed by Hoppe and Tardos. We begin by focusing on the static variant of this problem and then consider how our runtime compares to existing runtimes when the input to this problem is the output of Hoppe and Tardos’s reduction for temporal networks. (Notably, this reduction creates many new terminals, so we would like our algorithm to have small dependence on the number of terminals as well as the number of nodes and edges.) In this section, we let M⁢F⁢(n,m)𝑀𝐹𝑛𝑚MF(n,m)italic_M italic_F ( italic_n , italic_m ) and M⁢C⁢F⁢(n,m)𝑀𝐶𝐹𝑛𝑚MCF(n,m)italic_M italic_C italic_F ( italic_n , italic_m ) be the runtime for a maximum flow algorithm and minimum cost flow algorithm, respectively, on a steady-state network of n𝑛nitalic_n nodes and m𝑚mitalic_m edges. For a static network with n𝑛nitalic_n nodes, m𝑚mitalic_m edges, k𝑘kitalic_k terminals, maximum capacity U𝑈Uitalic_U, and time horizon T𝑇Titalic_T, Hoppe and Tardos show that the DTSN problem can be solved in weakly polynomial time O⁢(k3⁢M⁢C⁢F⁢(n,m)⁢log2⁡(n⁢U⁢T))𝑂superscript𝑘3𝑀𝐶𝐹𝑛𝑚superscript2𝑛𝑈𝑇O(k^{3}MCF(n,m)\log^{2}(nUT))italic_O ( italic_k start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT italic_M italic_C italic_F ( italic_n , italic_m ) roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_n italic_U italic_T ) ) or strongly polynomial time O~⁢(k15⁢m4)~𝑂superscript𝑘15superscript𝑚4\tilde{O}(k^{15}m^{4})over~ start_ARG italic_O end_ARG ( italic_k start_POSTSUPERSCRIPT 15 end_POSTSUPERSCRIPT italic_m start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT ). This algorithm calls a feasibility oracle, and the primary contribution of our work is the development of a new feasibility oracle that is more efficient in the regime where there are many terminals. In particular, the Hoppe-Tardos algorithm using our feasibility oracle runs in weakly polynomial time O⁢(k⋅M⁢F⁢(m,n⁢m)⁢log⁡(n⁢U⁢T))𝑂⋅𝑘𝑀𝐹𝑚𝑛𝑚𝑛𝑈𝑇O(k\cdot MF(m,nm)\log(nUT))italic_O ( italic_k ⋅ italic_M italic_F ( italic_m , italic_n italic_m ) roman_log ( italic_n italic_U italic_T ) ) or strongly polynomial time O⁢(k⁢(M⁢F⁢(m,n⁢m))2)𝑂𝑘superscript𝑀𝐹𝑚𝑛𝑚2O(k(MF(m,nm))^{2})italic_O ( italic_k ( italic_M italic_F ( italic_m , italic_n italic_m ) ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )444Note that in our strongly polynomial bound, the maximum flow algorithm must use only additions and comparisons as it relies on a parametric search algorithm of Megiddo [Meg78].. In particular, using the minimum cost flow algorithm of [CKL+22] and maximum flow algorithm of [Orl13], we get that the Hoppe-Tardos algorithm with our feasibility oracle is faster than that in the original paper if k=ω⁢(n)𝑘𝜔𝑛k=\omega(\sqrt{n})italic_k = italic_ω ( square-root start_ARG italic_n end_ARG ) (for the weakly polynomial case) or k=ω⁢(n1/7)𝑘𝜔superscript𝑛17k=\omega(n^{1/7})italic_k = italic_ω ( italic_n start_POSTSUPERSCRIPT 1 / 7 end_POSTSUPERSCRIPT ) (for the strongly polynomial case). source DTSN runtime DTTN runtime integral? [Hop95, HT00] O⁢(k3⁢M⁢C⁢F⁢(n,m)⁢log2⁡(n⁢U⁢T))𝑂superscript𝑘3𝑀𝐶𝐹𝑛𝑚superscript2𝑛𝑈𝑇O(k^{3}MCF(n,m)\log^{2}(nUT))italic_O ( italic_k start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT italic_M italic_C italic_F ( italic_n , italic_m ) roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_n italic_U italic_T ) ) O⁢(μ3⁢M⁢C⁢F⁢(μ,μ)⁢log2⁡(μ⁢U⁢T))𝑂superscript𝜇3𝑀𝐶𝐹𝜇𝜇superscript2𝜇𝑈𝑇O(\mu^{3}MCF(\mu,\mu)\log^{2}(\mu UT))italic_O ( italic_μ start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT italic_M italic_C italic_F ( italic_μ , italic_μ ) roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_μ italic_U italic_T ) ) integral O⁢(k3⁢m1+o⁢(1)⁢log⁡U⁢log2⁡(n⁢U⁢T))𝑂superscript𝑘3superscript𝑚1𝑜1𝑈superscript2𝑛𝑈𝑇O(k^{3}m^{1+o(1)}\log U\log^{2}(nUT))italic_O ( italic_k start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT italic_m start_POSTSUPERSCRIPT 1 + italic_o ( 1 ) end_POSTSUPERSCRIPT roman_log italic_U roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_n italic_U italic_T ) ) O⁢(μ4+o⁢(1)⁢log⁡U⁢log2⁡(μ⁢U⁢T))𝑂superscript𝜇4𝑜1𝑈superscript2𝜇𝑈𝑇O(\mu^{4+o(1)}\log U\log^{2}(\mu UT))italic_O ( italic_μ start_POSTSUPERSCRIPT 4 + italic_o ( 1 ) end_POSTSUPERSCRIPT roman_log italic_U roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_μ italic_U italic_T ) )† integral O~⁢(m4⁢k15)~𝑂superscript𝑚4superscript𝑘15\tilde{O}(m^{4}k^{15})over~ start_ARG italic_O end_ARG ( italic_m start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT italic_k start_POSTSUPERSCRIPT 15 end_POSTSUPERSCRIPT ) O~⁢(μ19)~𝑂superscript𝜇19\tilde{O}(\mu^{19})over~ start_ARG italic_O end_ARG ( italic_μ start_POSTSUPERSCRIPT 19 end_POSTSUPERSCRIPT ) integral [SS17b, SST22] O~⁢(m2⁢k5)~𝑂superscript𝑚2superscript𝑘5\tilde{O}(m^{2}k^{5})over~ start_ARG italic_O end_ARG ( italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_k start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT ) O~⁢(μ7)~𝑂superscript𝜇7\tilde{O}(\mu^{7})over~ start_ARG italic_O end_ARG ( italic_μ start_POSTSUPERSCRIPT 7 end_POSTSUPERSCRIPT ) fractional this paper O⁢(k⁢M⁢F⁢(m,n⁢m)⁢log⁡(n⁢U⁢T))𝑂𝑘𝑀𝐹𝑚𝑛𝑚𝑛𝑈𝑇O(kMF(m,nm)\log(nUT))italic_O ( italic_k italic_M italic_F ( italic_m , italic_n italic_m ) roman_log ( italic_n italic_U italic_T ) ) O⁢(μ⁢M⁢F⁢(μ,μ2)⁢log⁡(μ⁢U⁢T))𝑂𝜇𝑀𝐹𝜇superscript𝜇2𝜇𝑈𝑇O(\mu MF(\mu,\mu^{2})\log(\mu UT))italic_O ( italic_μ italic_M italic_F ( italic_μ , italic_μ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) roman_log ( italic_μ italic_U italic_T ) ) integral O⁢(k⁢(M⁢F⁢(m,n⁢m)+M⁢C⁢F⁢(n,m))2)∗𝑂superscript𝑘superscript𝑀𝐹𝑚𝑛𝑚𝑀𝐶𝐹𝑛𝑚2O(k(MF(m,nm)+MCF(n,m))^{2})^{*}italic_O ( italic_k ( italic_M italic_F ( italic_m , italic_n italic_m ) + italic_M italic_C italic_F ( italic_n , italic_m ) ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT O⁢(μ⁢(M⁢F⁢(μ,μ2)+M⁢C⁢F⁢(μ,μ))2)∗𝑂superscript𝜇superscript𝑀𝐹𝜇superscript𝜇2𝑀𝐶𝐹𝜇𝜇2O(\mu(MF(\mu,\mu^{2})+MCF(\mu,\mu))^{2})^{*}italic_O ( italic_μ ( italic_M italic_F ( italic_μ , italic_μ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) + italic_M italic_C italic_F ( italic_μ , italic_μ ) ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT integral O⁢(k⁢(n⁢m)1+o⁢(1)⁢log⁡(U⁢T)⁢log⁡(n⁢U⁢T))†𝑂superscript𝑘superscript𝑛𝑚1𝑜1𝑈𝑇𝑛𝑈𝑇†O(k(nm)^{1+o(1)}\log(UT)\log(nUT))^{\dagger}italic_O ( italic_k ( italic_n italic_m ) start_POSTSUPERSCRIPT 1 + italic_o ( 1 ) end_POSTSUPERSCRIPT roman_log ( italic_U italic_T ) roman_log ( italic_n italic_U italic_T ) ) start_POSTSUPERSCRIPT † end_POSTSUPERSCRIPT O⁢(μ3+o⁢(1)⁢log⁡(U⁢T)⁢log⁡(μ⁢U⁢T))𝑂superscript𝜇3𝑜1𝑈𝑇𝜇𝑈𝑇O(\mu^{3+o(1)}\log(UT)\log(\mu UT))italic_O ( italic_μ start_POSTSUPERSCRIPT 3 + italic_o ( 1 ) end_POSTSUPERSCRIPT roman_log ( italic_U italic_T ) roman_log ( italic_μ italic_U italic_T ) )† integral O⁢(k⁢n2⁢m4)‡𝑂superscript𝑘superscript𝑛2superscript𝑚4‡O(kn^{2}m^{4})^{\ddagger}italic_O ( italic_k italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_m start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ‡ end_POSTSUPERSCRIPT O⁢(μ7)𝑂superscript𝜇7O(\mu^{7})italic_O ( italic_μ start_POSTSUPERSCRIPT 7 end_POSTSUPERSCRIPT )‡ integral Table 1: A summary of runtimes for the DTSN and DTTN problems from [Hop95, HT00], [SS17b, SST22], and this paper. The first column lists the source of the given runtimes. The second column gives the runtime for DTSN on a static network of k𝑘kitalic_k terminals, n𝑛nitalic_n nodes, m𝑚mitalic_m edges, maximum capacity U𝑈Uitalic_U, and time horizon T𝑇Titalic_T. The third line is the DTTN runtime for a temporal network with parameter μ𝜇\muitalic_μ, maximum capacity U𝑈Uitalic_U, and time horizon T𝑇Titalic_T. The final column denotes whether the output is integral or fractional when the input values are integral. Highlighted in blue are the runtimes for DTTN with the state of the art maximum flow and minimum cost flow algorithms applied. These are the primary focus of this paper. Further, the strongly polynomial DTTN runtime can also be obtained using push-relabel algorithm of [GT88], as the number of edges is quadratic in the number of nodes. Note that in the third column, we have assumed that Ω⁢(m)Ω𝑚\Omega(m)roman_Ω ( italic_m ) edges are non-static. ∗ These runtimes require that the associated min cost flow and maximum flow algorithms use only additions and comparisons. † These runtimes are optimized using the min cost flow algorithm of [CKL+22] as a subroutine. ‡ These runtimes are optimized using the max flow algorithm of [Orl13] and the min cost flow algorithm from [Orl88] as a subroutine. Schloter, Skutella, and Tran [SS17b, SST22] also extensively study the problem of dynamic transshipments on static networks. They obtain a strongly polynomial algorithm that runs in time O~⁢(k5⁢m2)~𝑂superscript𝑘5superscript𝑚2\tilde{O}(k^{5}m^{2})over~ start_ARG italic_O end_ARG ( italic_k start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ), while our feasibility oracle paired with the Hoppe-Tardos algorithm obtains strongly polynomial runtime O⁢(k⁢n2⁢m4)𝑂𝑘superscript𝑛2superscript𝑚4O(kn^{2}m^{4})italic_O ( italic_k italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_m start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT ). Thus, our strongly polynomial algorithm only matches theirs when k=Ω⁢(n⁢m)𝑘Ω𝑛𝑚k=\Omega(\sqrt{nm})italic_k = roman_Ω ( square-root start_ARG italic_n italic_m end_ARG ). However, because we primarily focus on temporal networks, we care about runtimes on the kinds of networks that appear as the output of the Hoppe-Tardos reduction from the static problem to the temporal problem; in these networks we will have k=Θ⁢(μ)=Ω⁢(n,m)𝑘Θ𝜇Ω𝑛𝑚k=\Theta(\mu)=\Omega(n,m)italic_k = roman_Θ ( italic_μ ) = roman_Ω ( italic_n , italic_m ).555Note that if we do not have μ=Ω⁢(m)𝜇Ω𝑚\mu=\Omega(m)italic_μ = roman_Ω ( italic_m ), the algorithms of [SS17b, SST22] may be faster than that presented here. If a network has m𝑚mitalic_m edges, and μ𝜇\muitalic_μ is the number of constant pieces among the temporal edges only, then in the regime where k≤n≤μ≤m𝑘𝑛𝜇𝑚k\leq n\leq\mu\leq mitalic_k ≤ italic_n ≤ italic_μ ≤ italic_m, we get a strongly polynomial runtime of O⁢(μ3⁢m4)𝑂superscript𝜇3superscript𝑚4O(\mu^{3}m^{4})italic_O ( italic_μ start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT italic_m start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT ) for the DTTN problem, whereas [SS17b, SST22] get a runtime of O~⁢(μ5⁢m2)~𝑂superscript𝜇5superscript𝑚2\tilde{O}(\mu^{5}m^{2})over~ start_ARG italic_O end_ARG ( italic_μ start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) and [HT00] get a runtime of O~⁢(μ15⁢m4)~𝑂superscript𝜇15superscript𝑚4\tilde{O}(\mu^{15}m^{4})over~ start_ARG italic_O end_ARG ( italic_μ start_POSTSUPERSCRIPT 15 end_POSTSUPERSCRIPT italic_m start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT ). In the weakly polynomial setting, we get a runtime of O⁢(μ2+o⁢(1)⁢m1+o⁢(1)⁢log⁡(U⁢T)⁢log⁡(μ⁢U⁢T))𝑂superscript𝜇2𝑜1superscript𝑚1𝑜1𝑈𝑇𝜇𝑈𝑇O(\mu^{2+o(1)}m^{1+o(1)}\log(UT)\log(\mu UT))italic_O ( italic_μ start_POSTSUPERSCRIPT 2 + italic_o ( 1 ) end_POSTSUPERSCRIPT italic_m start_POSTSUPERSCRIPT 1 + italic_o ( 1 ) end_POSTSUPERSCRIPT roman_log ( italic_U italic_T ) roman_log ( italic_μ italic_U italic_T ) ) and [HT00] get a runtime of O⁢(μ3⁢m1+o⁢(1)⁢log⁡U⁢log2⁡(μ⁢U⁢T))𝑂superscript𝜇3superscript𝑚1𝑜1𝑈superscript2𝜇𝑈𝑇O(\mu^{3}m^{1+o(1)}\log U\log^{2}(\mu UT))italic_O ( italic_μ start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT italic_m start_POSTSUPERSCRIPT 1 + italic_o ( 1 ) end_POSTSUPERSCRIPT roman_log italic_U roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_μ italic_U italic_T ) ). Further, the Schloter et al. results can potentially return a non-integral solution even when all input values are integral, whereas Hoppe and Tardos (including under our feasibility oracle) always produces an integral solution on integral inputs. Table 1 describes the runtimes obtained for the DTSN problem via our new algorithms, via the algorithms of Hoppe-Tardos, and via the algorithms of Schloter et al. after applying the Hoppe-Tardos temporal to static reduction. Further, in Section 6, we will discuss how to efficiently find the optimal time horizon for quickest transshipment problems and the optimal flow value for maximum flow problems in this setting, so that (after applying the Hoppe-Tardos temporal to static reduction) these problems can also be solved in O⁢(μ7)𝑂superscript𝜇7O(\mu^{7})italic_O ( italic_μ start_POSTSUPERSCRIPT 7 end_POSTSUPERSCRIPT ) time on temporal networks. The rest of the paper will proceed as follows: In Section 2, we formally define the flow problems we will be considering, present some background information, and present our results in more detail. In Section 4, we describe a ""condensed"" time-expanded network that enables fast feasibility testing. In Section 3, we present special types of networks, including canonical temporal networks, and describe how the various problems relate to each other. In Section 5, we present an algorithm for feasibility on the DTSN problem that is efficient on canonical temporal networks. In Section 6, we discuss how to expand the scope of our DTTN results to deal with problems like quickest transshipment and maximum flow on temporal networks. Figure 2: A summary of the types of flow problems we will study in this paper and how they relate to each other. White arrows indicate that the source problem is a special case of the destination problem, and black errors indicate that there is a reduction from the source problem to the destination problem. Note that [HT95, HT00] showed that dynamic transshipment algorithms on static graphs can be used to solve dynamic transshipment problems on temporal graphs (when the capacity functions have few break points).(We will define the cDFSN problem in Section 4.)"
https://arxiv.org/html/2411.04846v1,On the Complexity of 2-club Cluster Editing with Vertex Splitting,"Editing a graph to obtain a disjoint union of s𝑠sitalic_s-clubs is one of the models for correlation clustering, which seeks a partition of the vertex set of a graph so that elements of each resulting set are close enough according to some given criterion. For example, in the case of editing into s𝑠sitalic_s-clubs, the criterion is proximity since any pair of vertices (in an s𝑠sitalic_s-club) are within a distance of s𝑠sitalic_s from each other. In this work we consider the vertex splitting operation, which allows a vertex to belong to more than one cluster. This operation was studied as one of the parameters associated with the Cluster Editing problem. We study the complexity and parameterized complexity of the s𝑠sitalic_s-Club Cluster Edge Deletion with Vertex Splitting and s𝑠sitalic_s-Club Cluster Vertex Splitting problems. Both problems are shown to be \NP\NP\NP-Complete and \APX\APX\APX-hard. On the positive side, we show that both problems are Fixed-Parameter Tractable with respect to the number of allowed editing operations and that s𝑠sitalic_s-Club Cluster Vertex Splitting is solvable in polynomial-time on the class of forests.","Correlation clustering is viewed as a graph modification problem where the objective is to perform a sequence of editing operations (or modifications) to obtain a disjoint union of clusters. Many variants of this problem have been studied in the literature, each with a different definition either of what a cluster means or of the various types of allowed modifications. In the Cluster Editing problem, for example, a cluster was defined to be a clique and the allowed editing operations were edge additions and deletions [12, 22, 18]. Later, some relaxation models such as s𝑠sitalic_s-Clubs and s𝑠sitalic_s-Clans emerged as they were deemed ideal models for clustering Biological Networks [7, 27]. Subsequent efforts studied overlapping clusters in a graph theoretical context [10, 15, 4]. In this work, we deal with overlapping communities by performing vertex splitting, which allows a vertex to be cloned and placed in more than one cluster. This operation was introduced in [4] in the study of the Cluster editing with Vertex Splitting problem. The notion of vertex splitting was first introduced in [19] but not in the context of correlation clustering. The Cluster Editing and Cluster Deletion problems were shown to be \NP\NP\NP-Complete in [22, 28]. Several other variants of the problem have also been proved to be \NP\NP\NP-Complete. This includes Cluster Vertex Deletion [23], 2-club Cluster Editing [25], 2-club Cluster Vertex Deletion [25], 2-club Cluster Edge Deletion [25] , Cluster Vertex Splitting [17], and Cluster Editing with Vertex Splitting [2, 5]. From a parameterized complexity standpoint, Cluster Editing, Cluster Deletion, and Cluster Vertex Deletion are known to be Fixed-Parameter Tractable (\FPT\FPT\FPT) [18, 21]. The same holds for the two club-variants: 2-club Cluster Edge Deletion and 2-club Cluster Vertex Deletion [25], while 2-club Cluster Editing was shown to be \W⁢[2]\Wdelimited-[]2\W[2][ 2 ]-Hard [16]. Furthermore, the Cluster Editing with Vertex Splitting problem has also been show to be \FPT\FPT\FPT [2, 5]. From a polynomial-time approximation standpoint, the Cluster Editing and Cluster Edge Deletion problems are \APX\APX\APX-Hard and have O⁢(log⁡n)𝑂𝑛O(\log n)italic_O ( roman_log italic_n ) approximation algorithms [13]. On the other hand, Cluster Vertex Deletion has a factor-two approximation algorithm [6]. To the best of our knowledge, problem variants with s𝑠sitalic_s-clubs or vertex splitting do not have any known approximation results. The problems mentioned above are all considered different models of correlation clustering. The s𝑠sitalic_s-Club models were shown to be effective in some networks where a clique could not capture all information needed to form better clusters [7, 27]. Vertex splitting proved to be useful, and in fact essential, when the input data has overlapping clusters, such as in protein networks [26]. So far, vertex splitting has been used along with cluster editing. In this paper we introduce the operation to the club-clustering variant by introducing two new problems: 2-club Cluster Vertex Splitting (2CCVS) and 2-club Cluster Edge Deletion with Vertex Splitting (2CCEDVS). These problems seek to modify a graph into a 2-clubs graph by performing a series of vertex splitting (2CCVS and 2CCEDVS) and edge deletion (2CCEDVS) operations. Our contribution. We prove that 2CCVS and 2CCEDVS are \NP\NP\NP-Complete. On the positive side, we prove that both problems are \FPT\FPT\FPT and that 2CCVS is solvable in polynomial-time on forests. We also show that, unless ¶=\NP¶\NP\P=\NP¶ =, the two problems cannot be approximated in polynomial time with a ratio better than a certain constant >1absent1>1> 1."
https://arxiv.org/html/2411.04803v1,Unbounded Error Correcting Codes,"We introduce a variant of Error Correcting Codes with no predetermined length. An Unbounded ECC with rate R𝑅Ritalic_R and distance ε𝜀\varepsilonitalic_ε is an encoding of a possibly infinite message into a possibly infinite codeword, such that for every large enough k𝑘kitalic_k we may recover the first R⁢k𝑅𝑘Rkitalic_R italic_k symbols of the message from the first k𝑘kitalic_k symbols of the codeword — even when up to 12⁢ε⁢k12𝜀𝑘\frac{1}{2}\varepsilon kdivide start_ARG 1 end_ARG start_ARG 2 end_ARG italic_ε italic_k of these codeword symbols are adversarially corrupted. We study unbounded codes over a binary alphabet in the regime of small distance ε𝜀\varepsilonitalic_ε, and obtain nearly-tight upper and lower bounds in several natural settings. We show that the optimal rate of such a code is between R<1−Ω⁢(ε)𝑅1Ω𝜀R<1-\Omega(\sqrt{\varepsilon})italic_R < 1 - roman_Ω ( square-root start_ARG italic_ε end_ARG ) and R>1−O⁢(ε⁢log⁡log⁡(1/ε))𝑅1𝑂𝜀1𝜀R>1-O\left(\sqrt{\varepsilon\log\log\left(1/\varepsilon\right)}\right)italic_R > 1 - italic_O ( square-root start_ARG italic_ε roman_log roman_log ( 1 / italic_ε ) end_ARG ). Surprisingly, our construction is non-linear, and we show that the optimal rate of a linear unbounded code is the asymptotically worse R=1−Θ⁢(ε⁢log⁡(1/ε))𝑅1Θ𝜀1𝜀R=1-\Theta\left(\sqrt{\varepsilon\log\left(1/\varepsilon\right)}\right)italic_R = 1 - roman_Θ ( square-root start_ARG italic_ε roman_log ( 1 / italic_ε ) end_ARG ). In the setting of random noise, the optimal rate of unbounded codes improves and matches the rate of standard codes at R=1−Θ⁢(ε⁢log⁡(1/ε))𝑅1Θ𝜀1𝜀R=1-\Theta({\varepsilon\log{\left(1/\varepsilon\right)}})italic_R = 1 - roman_Θ ( italic_ε roman_log ( 1 / italic_ε ) ).","Error Correcting Codes (ECCs) are the means to compensate for errors in the transmission of messages. An ECC encodes a message into a slightly larger codeword, such that even if a certain fraction of the codeword is corrupted the message can still be recovered. ECCs are extensively studied and it is long known that good ECCs can be constructed, even over a binary alphabet. [Ham50, Gil52, Var57, Jus72, SS96]. We define a natural generalization of ECCs, in which the length of the message (and hence also of the codeword) is not fixed. We call those Unbounded ECCs. We would require that for any k𝑘kitalic_k, the first k𝑘kitalic_k symbols of the message can be decoded from the first O⁢(k)𝑂𝑘O(k)italic_O ( italic_k ) symbols of the codeword, even if a small fraction of those codeword symbols are corrupted. This allows, for example, robust transmission of a long message in a connection that would abruptly halt at an unknown point in time. Recently, Zamir [Zam24] raised the same question in the specified model of noiseless feedback — as a part of a work on Large Language Models watermarking. We study this question in the more general setting. Definition (Unbounded codes). A code C:Σ⋆→Γ⋆:𝐶→superscriptΣ⋆superscriptΓ⋆C:\Sigma^{\star}\rightarrow\Gamma^{\star}italic_C : roman_Σ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT → roman_Γ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT is called an unbounded code with rate R𝑅Ritalic_R and distance ε𝜀\varepsilonitalic_ε, or a (R,ε)𝑅𝜀(R,\varepsilon)( italic_R , italic_ε )-unbounded code, if there exists some k0∈ℕsubscript𝑘0ℕk_{0}\in\mathbb{N}italic_k start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∈ blackboard_N such that the following holds. Let x,y∈Σ⋆𝑥𝑦superscriptΣ⋆x,y\in\Sigma^{\star}italic_x , italic_y ∈ roman_Σ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT, i≥k0𝑖subscript𝑘0i\geq k_{0}italic_i ≥ italic_k start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, and j≥iR𝑗𝑖𝑅j\geq\frac{i}{R}italic_j ≥ divide start_ARG italic_i end_ARG start_ARG italic_R end_ARG. If x[:i]≠y[:i]x[:i]\neq y[:i]italic_x [ : italic_i ] ≠ italic_y [ : italic_i ] then dH(C(x)[:j],C(y)[:j])≥εj.d_{H}(C(x)[:j],C(y)[:j])\geq\varepsilon j.italic_d start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ( italic_C ( italic_x ) [ : italic_j ] , italic_C ( italic_y ) [ : italic_j ] ) ≥ italic_ε italic_j . An unbounded ECC can be viewed as a family of standard ECCs that extend each other. For any large enough k𝑘kitalic_k, an (R,ε)𝑅𝜀(R,\varepsilon)( italic_R , italic_ε )-unbounded code indeed induces a standard ECC for messages of length k𝑘kitalic_k with rate R𝑅Ritalic_R and distance ε𝜀\varepsilonitalic_ε, which can be constructed by taking the prefix of length k/R𝑘𝑅k/Ritalic_k / italic_R of the code words of C𝐶Citalic_C. This object can be motivated by various natural scenarios, for example, streaming a large file such that at any point in time the receiver can sustain a certain fraction of errors. In this paper, we study unbounded codes with binary alphabet Σ=Γ=𝔽2ΣΓsubscript𝔽2\Sigma=\Gamma=\mathbb{F}_{2}roman_Σ = roman_Γ = blackboard_F start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, in the regime of small distance ε→0→𝜀0\varepsilon\rightarrow 0italic_ε → 0. We obtain upper and lower bounds for their optimal rates in several natural settings. Our main results are the following nearly-tight bounds on the optimal rate of unbounded ECCs. Theorem. For every small enough ε>0𝜀0\varepsilon>0italic_ε > 0 there exists a (R,ε)𝑅𝜀(R,\varepsilon)( italic_R , italic_ε )-unbounded code with R>1−O⁢(ε⁢log⁡log⁡(1/ε))𝑅1𝑂𝜀1𝜀R>1-O\left(\sqrt{\varepsilon\log\log\left(1/\varepsilon\right)}\right)italic_R > 1 - italic_O ( square-root start_ARG italic_ε roman_log roman_log ( 1 / italic_ε ) end_ARG ). Furthermore, for every (R,ε)𝑅𝜀(R,\varepsilon)( italic_R , italic_ε )-unbounded code it holds that R<1−Ω⁢(ε)𝑅1Ω𝜀R<1-\Omega\left(\sqrt{\varepsilon}\right)italic_R < 1 - roman_Ω ( square-root start_ARG italic_ε end_ARG ). The construction we present to achieve this bound is non-linear, which is surprising as in standard ECCs optimal (or nearly-optimal) bounds are achieved using linear constructions. We prove that this is inherent, and in fact, the optimal rate for linear unbounded ECCs is strictly worse. Theorem. For every small enough ε>0𝜀0\varepsilon>0italic_ε > 0 there exists a linear (R,ε)𝑅𝜀(R,\varepsilon)( italic_R , italic_ε )-unbounded code with R>1−O⁢(ε⁢log⁡(1/ε))𝑅1𝑂𝜀1𝜀R>1-O\left(\sqrt{\varepsilon\log\left(1/\varepsilon\right)}\right)italic_R > 1 - italic_O ( square-root start_ARG italic_ε roman_log ( 1 / italic_ε ) end_ARG ). Furthermore, for every linear (R,ε)𝑅𝜀(R,\varepsilon)( italic_R , italic_ε )-unbounded code it holds that R<1−Ω⁢(ε⁢log⁡(1/ε))𝑅1Ω𝜀1𝜀R<1-\Omega\left(\sqrt{\varepsilon\log\left(1/\varepsilon\right)}\right)italic_R < 1 - roman_Ω ( square-root start_ARG italic_ε roman_log ( 1 / italic_ε ) end_ARG ). Another divergence from standard ECCs is that for unbounded ECCs, random and adversarial errors lead to significantly different optimal rates. We show that if the errors are random bit-flips happening with probability ε𝜀\varepsilonitalic_ε instead of an adversarial ε𝜀\varepsilonitalic_ε-fraction of errors, then an unbounded ECC with a rate similar to that of standard codes is possible. Theorem. For every small enough ε>0𝜀0\varepsilon>0italic_ε > 0 there exists a (R,ε)𝑅𝜀(R,\varepsilon)( italic_R , italic_ε )-unbounded code resilient to BSC(ε)𝜀(\varepsilon)( italic_ε ) with R>1−O⁢(ε⁢log⁡(1/ε))𝑅1𝑂𝜀1𝜀R>1-O\left({\varepsilon\log\left(1/\varepsilon\right)}\right)italic_R > 1 - italic_O ( italic_ε roman_log ( 1 / italic_ε ) ). In a model where the encoder receives an immediate and noiseless feedback as to whether the previous symbol was received with or without errors, [Zam24] shows that a rate of 1−Θ⁢(ε)1Θ𝜀1-\Theta(\varepsilon)1 - roman_Θ ( italic_ε ) is possible for unbounded ECCs — the same optimal rate in this setting as standard ECCs. See Table 1 for a summary of the comparison between the optimal rates of standard and unbounded ECCs in the various regimes. As part of our proof, we study another variant of ECCs we call subset codes. A standard error correcting code can be viewed as a set of vectors such that the distance between every pair of them is large. A subset code is a collection of (possibly large) subsets of 𝔽2nsuperscriptsubscript𝔽2𝑛\mathbb{F}_{2}^{n}blackboard_F start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT such that the distance between every two such subsets is large (but on the other hand, vectors within the same subset might be close to each other). In Section 6 we study these codes and derive optimal asymptotic bounds, this can be of independent interest. Table 1: Optimal rate R𝑅Ritalic_R for error ε→0→𝜀0\varepsilon\rightarrow 0italic_ε → 0. Standard ECCs Unbounded ECCs Adversarial errors 1−Θ⁢(ε⁢log⁡(1/ε))1Θ𝜀1𝜀1-\Theta(\varepsilon\log(1/\varepsilon))1 - roman_Θ ( italic_ε roman_log ( 1 / italic_ε ) ) 1−Θ~⁢(ε)1~Θ𝜀1-\tilde{\Theta}(\sqrt{\varepsilon})1 - over~ start_ARG roman_Θ end_ARG ( square-root start_ARG italic_ε end_ARG ) Random errors 1−Θ⁢(ε⁢log⁡(1/ε))1Θ𝜀1𝜀1-\Theta(\varepsilon\log(1/\varepsilon))1 - roman_Θ ( italic_ε roman_log ( 1 / italic_ε ) ) 1−Θ⁢(ε⁢log⁡(1/ε))1Θ𝜀1𝜀1-\Theta(\varepsilon\log(1/\varepsilon))1 - roman_Θ ( italic_ε roman_log ( 1 / italic_ε ) ) Noiseless feedback 1−Θ⁢(ε)1Θ𝜀1-\Theta(\varepsilon)1 - roman_Θ ( italic_ε ) 1−Θ⁢(ε)1Θ𝜀1-\Theta(\varepsilon)1 - roman_Θ ( italic_ε ) 1.1 Connections to Prior Works This definition we consider is somewhat reminiscent of Tree Codes [Sch93, Sch96], but the definitions are qualitatively different: In a tree code, every symbol of the code-word directly corresponds to a single symbol of the message — in an unbounded code, there is no such direct correspondence and code symbols may depend on both previous and following message symbols. Furthermore, while in a tree code the distance property is defined with respect to only the part of the code-word succeeding the first disagreement between messages — in an unbounded code the distance property is always defined with respect to the entire codeword prefix. Tree codes and other types of codes were considered in the context of error correction of interactive protocols [BR11, GHS14, G+17, EKS20]. Contrary to the setting of interactive codes and also to that of noiseless feedback studied in [Zam24], in our settings there is no interaction whatsoever — that is, the receiver is not communicating anything and, in particular, no information about what symbols were corrupted is ever learned by the sender. While the optimal rates of unbounded codes are similar to those of interactive codes with small noise [KR13, Hae14] we are not aware of any formal connection. A somewhat related notion of anytime capacity [SM06] was studied in the context of control theory. Here, the sender does not have the whole message in advance but receives it online, and the model assumes stochastic (random) noise. The goal is that the probability of making a mistake on a bit’s decoding will decrease exponentially with the time passed since the sender received this bit. 1.2 Alphabet Size In the vast majority of this paper we focus on the binary alphabet Σ=Γ=𝔽2ΣΓsubscript𝔽2\Sigma=\Gamma=\mathbb{F}_{2}roman_Σ = roman_Γ = blackboard_F start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, which is equivalent to the case of an alphabet of any arbitrary constant size |Σ|,|Γ|=O⁢(1)ΣΓ𝑂1|\Sigma|,|\Gamma|=O(1)| roman_Σ | , | roman_Γ | = italic_O ( 1 ). Nonetheless, the same questions we present may also be asked for an alphabet size that is related to ε𝜀\varepsilonitalic_ε. In standard Error Correcting Codes, the optimal rate of 1−R=Θ⁢(ε⁢log⁡(1/ε))1𝑅Θ𝜀1𝜀1-R=\Theta(\varepsilon\log(1/\varepsilon))1 - italic_R = roman_Θ ( italic_ε roman_log ( 1 / italic_ε ) ) as ε→0→𝜀0\varepsilon\rightarrow 0italic_ε → 0 is refined to 1−R=Θ⁢(ε⁢(logq⁡(1/ε)+1))1𝑅Θ𝜀subscript𝑞1𝜀11-R=\Theta\left(\varepsilon\left(\log_{q}\left(1/\varepsilon\right)+1\right)\right)1 - italic_R = roman_Θ ( italic_ε ( roman_log start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ( 1 / italic_ε ) + 1 ) ) when the alphabet size q:=|Σ|assign𝑞Σq:=|\Sigma|italic_q := | roman_Σ | is taken into account. In our construction of the linear code of Section 4, the bound is similarly refined to 1−R=O⁢(ε⁢(logq⁡(1/ε)+1))1𝑅𝑂𝜀subscript𝑞1𝜀11-R=O\left(\sqrt{\varepsilon\left(\log_{q}\left(1/\varepsilon\right)+1\right)}\right)1 - italic_R = italic_O ( square-root start_ARG italic_ε ( roman_log start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ( 1 / italic_ε ) + 1 ) end_ARG ) when the dependence on q𝑞qitalic_q is considered. In particular, when q=Ω⁢(1ε)𝑞Ω1𝜀q=\Omega\left(\frac{1}{\varepsilon}\right)italic_q = roman_Ω ( divide start_ARG 1 end_ARG start_ARG italic_ε end_ARG ) we obtain a linear code with rate 1−R=O⁢(ε)1𝑅𝑂𝜀1-R=O\left(\sqrt{\varepsilon}\right)1 - italic_R = italic_O ( square-root start_ARG italic_ε end_ARG ). On the other hand, the rate upper bound for linear codes in Section 5 is independent of the alphabet size and thus shows 1−R≥ε1𝑅𝜀1-R\geq\sqrt{\varepsilon}1 - italic_R ≥ square-root start_ARG italic_ε end_ARG for any q𝑞qitalic_q, which is tight for q=Ω⁢(1ε)𝑞Ω1𝜀q=\Omega\left(\frac{1}{\varepsilon}\right)italic_q = roman_Ω ( divide start_ARG 1 end_ARG start_ARG italic_ε end_ARG ). 1.3 Organization of the Paper In Section 2 we give a high-level overview of the constructions and proofs in the paper. In Section 4 we construct linear unbounded codes and also show that their rate improves when the errors are random. In Section 5 we derive a simple rate upper bound for linear unbounded codes. In Section 6 we introduce and study subset codes, which we use in the consecutive sections. In Section 7 we present a rate upper bound for general unbounded codes as well as improve the bound for linear codes. In Section 8 we improve our construction using non-linear subset codes. Finally, we conclude and present open problems in Section 9."
https://arxiv.org/html/2411.04718v1,Approximate Counting of Permutation Patterns,"We consider the problem of counting the copies of a length-k𝑘kitalic_k pattern σ𝜎\sigmaitalic_σ in a sequence f:[n]→ℝ:𝑓→delimited-[]𝑛ℝf\colon[n]\to{\mathbb{R}}italic_f : [ italic_n ] → blackboard_R, where a copy is a subset of indices i1<…<ik∈[n]subscript𝑖1…subscript𝑖𝑘delimited-[]𝑛i_{1}<\ldots<i_{k}\in[n]italic_i start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT < … < italic_i start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∈ [ italic_n ] such that f⁢(ij)<f⁢(iℓ)𝑓subscript𝑖𝑗𝑓subscript𝑖ℓf(i_{j})<f(i_{\ell})italic_f ( italic_i start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) < italic_f ( italic_i start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT ) if and only if σ⁢(j)<σ⁢(ℓ)𝜎𝑗𝜎ℓ\sigma(j)<\sigma(\ell)italic_σ ( italic_j ) < italic_σ ( roman_ℓ ). This problem is motivated by a range of connections and applications in ranking, nonparametric statistics, combinatorics, and fine-grained complexity, especially when k𝑘kitalic_k is a small fixed constant.Recent advances have significantly improved our understanding of counting and detecting patterns. Guillemot and Marx [2014] demonstrated that the detection variant is solvable in O⁢(n)𝑂𝑛O(n)italic_O ( italic_n ) time for any fixed k𝑘kitalic_k. Their proof has laid the foundations for the discovery of the twin-width, a concept that has notably advanced parameterized complexity in recent years. Counting, in contrast, is harder: it has a conditional lower bound of nΩ⁢(k/log⁡k)superscript𝑛Ω𝑘𝑘n^{\Omega(k/\log k)}italic_n start_POSTSUPERSCRIPT roman_Ω ( italic_k / roman_log italic_k ) end_POSTSUPERSCRIPT [Berendsohn, Kozma, and Marx 2019] and is expected to be polynomially harder than detection as early as k=4𝑘4k=4italic_k = 4, given its equivalence to counting 4444-cycles in graphs [Dudek and Gawrychowski, 2020].In this work, we design a deterministic near-linear time (1+ε)1𝜀(1+\varepsilon)( 1 + italic_ε )-approximation algorithm for counting σ𝜎\sigmaitalic_σ-copies in f𝑓fitalic_f for all k≤5𝑘5k\leq 5italic_k ≤ 5. Combined with the conditional lower bound for k=4𝑘4k=4italic_k = 4, this establishes the first known separation between approximate and exact algorithms for pattern counting. Interestingly, our algorithm leverages the Birgé decomposition – a sublinear tool for monotone distributions widely used in distribution testing – which, to our knowledge, has not been applied in a pattern counting context before.","Detecting and counting structural patterns in a data sequence is a common algorithmic challenge in various theoretical and applied domains. Some of the numerous application domains include ranking and recommendation [DKNS01], time series analysis [BP02], and computational biology [FDRM09], among many others. On the mathematical/theoretical side, problems involving sequential pattern analysis naturally arise, e.g., in algebraic geometry [AB16], combinatorics [CDN23, Grü23], and nonparametric statistics [EZL21]. Formally, we are interested here in finding order patterns or permutation patterns, defined as follows. Given a real-valued sequence f:[n]→ℝ:𝑓→delimited-[]𝑛ℝf\colon[n]\to{\mathbb{R}}italic_f : [ italic_n ] → blackboard_R and a permutation pattern σ:[k]→[k]:𝜎→delimited-[]𝑘delimited-[]𝑘\sigma\colon[k]\to[k]italic_σ : [ italic_k ] → [ italic_k ], a copy of the pattern σ𝜎\sigmaitalic_σ in the sequence f𝑓fitalic_f is any subset of k𝑘kitalic_k indices i1<i2<…<iksubscript𝑖1subscript𝑖2…subscript𝑖𝑘i_{1}<i_{2}<\ldots<i_{k}italic_i start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT < italic_i start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT < … < italic_i start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT so that for j,ℓ∈[k]𝑗ℓdelimited-[]𝑘j,\ell\in[k]italic_j , roman_ℓ ∈ [ italic_k ], f⁢(ij)<f⁢(iℓ)𝑓subscript𝑖𝑗𝑓subscript𝑖ℓf(i_{j})<f(i_{\ell})italic_f ( italic_i start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) < italic_f ( italic_i start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT ) if and only if σ⁢(j)<σ⁢(ℓ)𝜎𝑗𝜎ℓ\sigma(j)<\sigma(\ell)italic_σ ( italic_j ) < italic_σ ( roman_ℓ ); see Figure 1. x𝑥xitalic_xf⁢(x)𝑓𝑥f(x)italic_f ( italic_x ) Figure 1: A configuration of n𝑛nitalic_n points in two dimensions (with no two points sharing the same x𝑥xitalic_x coordinate), represented as a function f:[n]→ℝ:𝑓→delimited-[]𝑛ℝf\colon[n]\to{\mathbb{R}}italic_f : [ italic_n ] → blackboard_R. The four full points form a copy of the permutation pattern 1432143214321432. In the permutation pattern matching (PPM) problem,111We shall interchangeably use the terms “pattern matching” and “pattern detection” to refer to this problem. the task is to determine whether f𝑓fitalic_f contains at least one copy of the pattern σ𝜎\sigmaitalic_σ. In the counting variant, the goal is to return the exact or approximate number of σ𝜎\sigmaitalic_σ-copies in f𝑓fitalic_f. Recent years have seen several breakthroughs in both detection and counting, revealing important implications in parameterized and fine-grained complexity. Of most importance is the case where k𝑘kitalic_k is a small constant, which has a large number of diverse applications and interesting connections: • Counting inversions, which are 21212121-patterns, that is, k=2𝑘2k=2italic_k = 2, is of fundamental importance for ranking applications [DKNS01]. It has thus attracted significant attention from the algorithmic community for the last several decades, for both exact counting [CP10, Die89, FS89] and approximate counting [CP10, AP98]. • Counting 4444-patterns222We henceforth use the abbreviation “k𝑘kitalic_k-pattern” to refer to a permutation pattern of length k𝑘kitalic_k. is equivalent, by a bidirectional reduction, to counting 4444-cycles in sparse graphs. The latter is a fundamental problem in algorithmic graph theory (e.g., [AYZ97, DKS17]) and fine-grained complexity (e.g., [WWWY15, ABKZ22, ABF23, JX23]). This equivalence was shown by Dudek and Gawrychowski [DG20]. • Pattern counting for fixed k𝑘kitalic_k (especially k≤5𝑘5k\leq 5italic_k ≤ 5) has deep and intricate connections to (bivariate) independece testing, a fundamental question in nonparametric statistics that asks the following. Given n𝑛nitalic_n pairs of samples (x1,y1),…,(xn,yn)subscript𝑥1subscript𝑦1…subscript𝑥𝑛subscript𝑦𝑛(x_{1},y_{1}),\ldots,(x_{n},y_{n})( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , … , ( italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) from two real continuous random variables X𝑋Xitalic_X and Y𝑌Yitalic_Y, should we deduce that X𝑋Xitalic_X and Y𝑌Yitalic_Y are independent? This question has seen a long line of work in nonparametric statistics (e.g., [EZ20, BD14, Yan70, Cha21, BKR61]). A line of work that started by Hoeffding in the 1940’s [Hoe48] and is still very active to this day establishes distribution-free methods to test independence by (i) ordering the sample pairs according to the values of the xisubscript𝑥𝑖x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT’s, effectively treating the yisubscript𝑦𝑖y_{i}italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT’s as a length-n𝑛nitalic_n sequence; and (ii) deciding whether X𝑋Xitalic_X and Y𝑌Yitalic_Y are independent based on the k𝑘kitalic_k-profile of yisubscript𝑦𝑖y_{i}italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT’s, for k≤5𝑘5k\leq 5italic_k ≤ 5. This is a special case of the much broader notion of U𝑈Uitalic_U-statistics [Lee90, KB94]. See [EZ20, Grü23] for more details on this fascinating connection. • A family of length-n𝑛nitalic_n permutations is considered quasirandom if, roughly speaking, the number of occurrences of every pattern in the family (of any length) is asymptotically similar to that of a random permutation. Quasirandomness turns out to be quite closely related to independence testing, discussed above, and it is known that the counts of patterns of length up to four suffice to determine quasirandomness, see, e.g., [CDN23, Grü23]. • Permutation pattern matching allows one to deduce whether an input f𝑓fitalic_f is free from some pattern σ𝜎\sigmaitalic_σ, and consequently run much faster algorithms tailored to σ𝜎\sigmaitalic_σ-free instances. Indeed, many classical optimization tasks, such as binary search trees, k𝑘kitalic_k-server, and Euclidean TSP [BKO24] become much faster on σ𝜎\sigmaitalic_σ-free inputs. For example, a recent fascintating result by Opler [Opl24] shows that sorting can be done in linear time in pattern-avoiding sequences. Pattern matching itself sometimes also becomes faster in classes of σ𝜎\sigmaitalic_σ-free permutations [JK17, JOP21, BBL98]. Consequently, there has been a long line of computational work on pattern matching and counting, e.g., [BL12, BD14, JK17, BKM21, EZ20, JOP21, Cha21, GR22]. Here, we focus on the most relevant results in the constant k𝑘kitalic_k case. Notably, the version of the problem where k𝑘kitalic_k is large (linear in n𝑛nitalic_n) is NP-hard [BBL98]. Both matching and counting admit a trivial algorithm with running time O⁢(k⁢nk)𝑂𝑘superscript𝑛𝑘O(kn^{k})italic_O ( italic_k italic_n start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ): the idea is to enumerate over all k𝑘kitalic_k-tuples of indices in f𝑓fitalic_f, and check if each such tuple in f𝑓fitalic_f induces a copy of the pattern. But can these algorithmic tasks be solved in time substantially smaller than nksuperscript𝑛𝑘n^{k}italic_n start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT? Pattern matching: a linear-time algorithm, and the twin-width connection. In the matching case, the answer is resoundingly positive. The seminal work of Guillemot and Marx [GM14] shows that PPM is a fixed parameter tractable (FPT) problem that takes O⁢(n)𝑂𝑛O(n)italic_O ( italic_n ) time for fixed k𝑘kitalic_k.333Unless mentioned otherwise, the computational model is Word RAM, that allows querying a single function value or comparing two values in constant time. Their running time is of the form 2O⁢(k2⁢log⁡k)⋅n⋅superscript2𝑂superscript𝑘2𝑘𝑛2^{O(k^{2}\log k)}\cdot n2 start_POSTSUPERSCRIPT italic_O ( italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT roman_log italic_k ) end_POSTSUPERSCRIPT ⋅ italic_n; the bound was slightly improved by Fox to 2O⁢(k2)⋅n⋅superscript2𝑂superscript𝑘2𝑛2^{O(k^{2})}\cdot n2 start_POSTSUPERSCRIPT italic_O ( italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_POSTSUPERSCRIPT ⋅ italic_n [Fox13]. The technical argument of [GM14] relies on two main ingredients: the first is the celebrated result of Marcus and Tardos [MT04] in their proof of the Stanley-Wilf conjecture [FH92, Kla00], while the second is a novel width notion for permutations suggested in their work. The latter subsequently led to the development of the very wide and useful notion of twin-width, which has revolutionized parametrized complexity in recent years. Indeed, the work of Bonnet, Kim, Thomassé, and Watrigant [BKTW21], which originally defined twin-width, begins with the following statement: “Inspired by a width invariant defined on permutations by Guillemot and Marx [GM14], we introduce the notion of twin-width on graphs and on matrices.” Pattern counting: algorithms and hardness. Exact counting, meanwhile, is unlikely to admit very efficient algorithms. A series of works from the last two decades has gradually improved the nksuperscript𝑛𝑘n^{k}italic_n start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT upper bound, obtaining bounds of the form n(c+o⁢(1))⁢ksuperscript𝑛𝑐𝑜1𝑘n^{(c+o(1))k}italic_n start_POSTSUPERSCRIPT ( italic_c + italic_o ( 1 ) ) italic_k end_POSTSUPERSCRIPT for constant c<1𝑐1c<1italic_c < 1 [AAAH01, AR08]. The current state of the art, proved by Bernedsohn, Kozma, and Marx [BKM21] is of the form nk/4+o⁢(k)superscript𝑛𝑘4𝑜𝑘n^{k/4+o(k)}italic_n start_POSTSUPERSCRIPT italic_k / 4 + italic_o ( italic_k ) end_POSTSUPERSCRIPT. The same work shows, however, that no⁢(k/log⁡k)superscript𝑛𝑜𝑘𝑘n^{o(k/\log k)}italic_n start_POSTSUPERSCRIPT italic_o ( italic_k / roman_log italic_k ) end_POSTSUPERSCRIPT-time algorithms for exact counting cannot exist unless the exponential-time hypothesis (ETH) is false. The above results treat k𝑘kitalic_k as a variable; we next focus on the case where k𝑘kitalic_k is very small, given the myriad of applications discussed before. In the case k=2𝑘2k=2italic_k = 2, it is easy to obtain an exact counting algorithm in time O⁢(n⁢log⁡n)𝑂𝑛𝑛O(n\log n)italic_O ( italic_n roman_log italic_n ) (in the Word RAM model), via a variant of merge sort. A line of work [Die89, FS89, AP98, CP10] sought to obtain improved algorithms for both exact and approximate counting (to within a 1+ϵ1italic-ϵ1+\epsilon1 + italic_ϵ multiplicative factor).444Formally, a (1+ϵ)1italic-ϵ(1+\epsilon)( 1 + italic_ϵ )-approximate counting algorithm is required, given access to a pattern σ𝜎\sigmaitalic_σ and a function f𝑓fitalic_f, to return a value between X/(1+ε)𝑋1𝜀X/(1+\varepsilon)italic_X / ( 1 + italic_ε ) and (1+ε)⁢X1𝜀𝑋(1+\varepsilon)X( 1 + italic_ε ) italic_X, where X𝑋Xitalic_X is the number of σ𝜎\sigmaitalic_σ-copies in f𝑓fitalic_f. The best known exact and approximate upper bounds for k=2𝑘2k=2italic_k = 2 are O⁢(n⁢log⁡n)𝑂𝑛𝑛O(n\sqrt{\log n})italic_O ( italic_n square-root start_ARG roman_log italic_n end_ARG ) and O⁢(n)𝑂𝑛O(n)italic_O ( italic_n ), respectively, both proved by Chan and Pătraşcu [CP10]. The cases of k=3𝑘3k=3italic_k = 3 and k=4𝑘4k=4italic_k = 4 have been the subject of multiple recent works. Even-Zohar and Leng [EZL21] developed an object called corner tree to count a family of patterns (that slightly differ from permutation patterns) in near-linear time. Using linear combinations of corner tree formulas, they obtained near-linear time algorithm for all patterns of length 3333 and some (8 out of 24) length-4444 patterns. For the remaining ones of length 4444, the same work obtains an O⁢(n3/2)𝑂superscript𝑛32O(n^{3/2})italic_O ( italic_n start_POSTSUPERSCRIPT 3 / 2 end_POSTSUPERSCRIPT ) time algorithm using different techniques. This interesting dichotomy between “easy” and “hard” 4444-patterns raises an interesting question: is the dichotomy an artifact of the specific technique, or is there an inherent computational barrier? Dudek and Gawrychowski [DG20] proved that the latter is true: exact counting of any “hard” 4444-pattern is equivalent (via bidirectional reductions) to exact counting of 4444-cycles in graphs, a central and very well studied problem in algorithmic graph theory. The concrete equivalence stated in their paper (see Theorem 1 there) is that an O~⁢(mγ)~𝑂superscript𝑚𝛾\tilde{O}(m^{\gamma})over~ start_ARG italic_O end_ARG ( italic_m start_POSTSUPERSCRIPT italic_γ end_POSTSUPERSCRIPT )-time algorithm for counting 4444-cycles in m𝑚mitalic_m-edge graphs implies an O~⁢(nγ)~𝑂superscript𝑛𝛾\tilde{O}(n^{\gamma})over~ start_ARG italic_O end_ARG ( italic_n start_POSTSUPERSCRIPT italic_γ end_POSTSUPERSCRIPT ) time algorithm for counting “hard” 4-patterns, and vice versa. While this has led to a slightly improved O⁢(n1.48)𝑂superscript𝑛1.48O(n^{1.48})italic_O ( italic_n start_POSTSUPERSCRIPT 1.48 end_POSTSUPERSCRIPT ) upper bound based on best known results for counting 4444-cycles in sparse graphs [WWWY15], the more interesting direction to us is the lower bound side. A line of recent works obtains conditional lower bounds on 4444-cycle counting, that apply already for the easier task of 4444-cycle detection [ABKZ22, ABF23, JX23]. These works imply that conditioning on the Strong 3-SUM conjecture, detecting whether a (sufficiently sparse) graph with m𝑚mitalic_m edges contains a 4444-cycle requires m1+Ω⁢(1)superscript𝑚1Ω1m^{1+\Omega(1)}italic_m start_POSTSUPERSCRIPT 1 + roman_Ω ( 1 ) end_POSTSUPERSCRIPT time (see, e.g., the discussion after Theorem 1.14 in [JX23]), which translates to an n1+Ω⁢(1)superscript𝑛1Ω1n^{1+\Omega(1)}italic_n start_POSTSUPERSCRIPT 1 + roman_Ω ( 1 ) end_POSTSUPERSCRIPT lower bound for exact counting 4444-patterns, via [DG20]. 1.1 Our results Given the separation between the O⁢(n)𝑂𝑛O(n)italic_O ( italic_n ) complexity of pattern detection and the n1+Ω⁢(1)superscript𝑛1Ω1n^{1+\Omega(1)}italic_n start_POSTSUPERSCRIPT 1 + roman_Ω ( 1 ) end_POSTSUPERSCRIPT conditional lower bound for pattern counting already for k=4𝑘4k=4italic_k = 4, and the importance of counting in the constant-k𝑘kitalic_k regime, we ask whether approximate counting can be performed in time substantially (polynomially) faster than exact counting. What is the computational landscape of (1+ε)1𝜀(1+\varepsilon)( 1 + italic_ε )-approximate counting of k𝑘kitalic_k-patterns, for small fixed k𝑘kitalic_k, as compared to exact counting and matching? Is approximate counting much faster than exact counting? The only case where the best known (1+ε)1𝜀(1+\varepsilon)( 1 + italic_ε )-approximate algorithm is faster than the best known exact algorithm is when k=2𝑘2k=2italic_k = 2 [CP10], but the gap is only of order log⁡n𝑛\sqrt{\log n}square-root start_ARG roman_log italic_n end_ARG (i.e., between O⁢(n)𝑂𝑛O(n)italic_O ( italic_n ) and O⁢(n⁢log⁡n)𝑂𝑛𝑛O(n\sqrt{\log n})italic_O ( italic_n square-root start_ARG roman_log italic_n end_ARG )), and no nontrivial exact counting lower bounds are known. Thus, it remains unknown whether exact counting is harder than approximate counting even for k=2𝑘2k=2italic_k = 2, and even if it is, the gap would be of lower order. Our main contribution, stated below, is a near-linear time approximate counting algorithm for k≤5𝑘5k\leq 5italic_k ≤ 5. Theorem 1.1. For every permutation pattern σ𝜎\sigmaitalic_σ of length k≤5𝑘5k\leq 5italic_k ≤ 5 and every ε>0𝜀0\varepsilon>0italic_ε > 0, the following holds. There exists a deterministic algorithm that, given access to a function f:[n]→ℝ:𝑓→delimited-[]𝑛ℝf\colon[n]\to{\mathbb{R}}italic_f : [ italic_n ] → blackboard_R, returns the number of σ𝜎\sigmaitalic_σ-copies in f𝑓fitalic_f, up to a multiplicative error of 1+ε1𝜀1+\varepsilon1 + italic_ε, in time n⋅(ε−1⁢log⁡n)O⁢(1)⋅𝑛superscriptsuperscript𝜀1𝑛𝑂1n\cdot\left(\varepsilon^{-1}\log n\right)^{O(1)}italic_n ⋅ ( italic_ε start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log italic_n ) start_POSTSUPERSCRIPT italic_O ( 1 ) end_POSTSUPERSCRIPT. Combined with the n1+Ω⁢(1)superscript𝑛1Ω1n^{1+{\Omega(1)}}italic_n start_POSTSUPERSCRIPT 1 + roman_Ω ( 1 ) end_POSTSUPERSCRIPT lower bound for counting “hard” 4444-patterns (e.g., 2413241324132413), our result implies a polynomial separation between exact and (1+ε)1𝜀(1+\varepsilon)( 1 + italic_ε )-approximate algorithms for 4-patterns and 5-patterns. The paper includes a full, self-contained proof for k=4𝑘4k=4italic_k = 4; recall that for k≤3𝑘3k\leq 3italic_k ≤ 3, even exact counting algorithms have near-linear time complexity [EZL21]. For k=5𝑘5k=5italic_k = 5, our proof is computer-assisted: the algorithm enumerates over multiple parameter choices and techniques, heavily depending on the pattern structure. Verifying that the algorithm works for all patterns requires a tedious case analysis for k=5𝑘5k=5italic_k = 5, involving 512 cases, each of which is straightforward to verify based on the output from our code. In this paper, we describe the set of techniques used, establish how they can be combined, provide examples of typical use cases, and delegate the full enumeration to the software. The source code for the enumeration and the full output (including for k=5𝑘5k=5italic_k = 5) are provided here: https://github.com/omribene/approx-counting. Our proof can be immediately adapted to provide an algorithm for enumerating (or listing) copies of the pattern. In the enumeration problem, we are given f,σ𝑓𝜎f,\sigmaitalic_f , italic_σ, and an integer t𝑡titalic_t, and are required to provide a list of t𝑡titalic_t copies of σ𝜎\sigmaitalic_σ in f𝑓fitalic_f (or the full list if there are less than t𝑡titalic_t copies). We obtain the following result. Theorem 1.2. For every permutation pattern σ𝜎\sigmaitalic_σ of length k≤5𝑘5k\leq 5italic_k ≤ 5 and every t∈ℕ𝑡ℕt\in{\mathbb{N}}italic_t ∈ blackboard_N, the following holds. There exists a deterministic algorithm that, given access to a function f:[n]→ℝ:𝑓→delimited-[]𝑛ℝf\colon[n]\to{\mathbb{R}}italic_f : [ italic_n ] → blackboard_R, returns a list of t𝑡titalic_t copies of σ𝜎\sigmaitalic_σ in f𝑓fitalic_f (or all such copies, if there are fewer than t𝑡titalic_t), in time (n+t)⋅logO⁢(1)⁡n⋅𝑛𝑡superscript𝑂1𝑛(n+t)\cdot\log^{O(1)}n( italic_n + italic_t ) ⋅ roman_log start_POSTSUPERSCRIPT italic_O ( 1 ) end_POSTSUPERSCRIPT italic_n. Our results further highlight the contrasting behavior between 4444-cycles in sparse graphs and 4444-patterns in sequences. The exact counting complexities for these objects are equal, due to the linear-size bidirectional reductions between these problems [DG20]. Meanwhile, for detection we have a separation between the O⁢(n)𝑂𝑛O(n)italic_O ( italic_n ) algorithm for patterns [GM14] and the n1+Ω⁢(1)superscript𝑛1Ω1n^{1+\Omega(1)}italic_n start_POSTSUPERSCRIPT 1 + roman_Ω ( 1 ) end_POSTSUPERSCRIPT conditional lower bound for cycles in sparse graphs [ABKZ22, ABF23, JX23]. Since the same lower bound also applies to approximate counting of 4444-cycles, this implies a separation for approximate counting. Finally, for enumeration the lower bounds of [ABF23, JX23] are stronger (and in fact tight), of order Ω⁢(min⁡{n2−o⁢(1),m4/3−o⁢(1)})Ωsuperscript𝑛2𝑜1superscript𝑚43𝑜1\Omega(\min\{n^{2-o(1)},m^{4/3-o(1)}\})roman_Ω ( roman_min { italic_n start_POSTSUPERSCRIPT 2 - italic_o ( 1 ) end_POSTSUPERSCRIPT , italic_m start_POSTSUPERSCRIPT 4 / 3 - italic_o ( 1 ) end_POSTSUPERSCRIPT } ), conditioning on the 3-SUM conjecture. Again, since enumeration of pattern detection is near-linear in n𝑛nitalic_n and t𝑡titalic_t, we get a separation here for sufficiently small values of t𝑡titalic_t. 1.2 Our techniques Our approach to approximate pattern counting is based on a novel application of a known tool in distribution testing, and on several new techniques. Each of these techniques contributes to efficient approximate counting for small fixed patterns. Here, we outline three main ideas central to our work: (i) the Birgé technique for exploiting structural monotonicity; (ii) using separators to impose additional structure on pattern instances; and (iii) a specialized data structure for approximating the counts of 12121212 copies within axis-parallel rectangles.555Throughout our work, we assume the input is a permutation. Nevertheless, our proofs also handle inputs/functions that contain points with the same y𝑦yitalic_y-coordinate, i.e., the proofs tolerate f⁢(i)=f⁢(j)𝑓𝑖𝑓𝑗f(i)=f(j)italic_f ( italic_i ) = italic_f ( italic_j ) for i≠j𝑖𝑗i\neq jitalic_i ≠ italic_j. Also, without loss of generality, for the problem of counting patterns, it can be assumed that f⁢(i)∈{0,1,…,n}𝑓𝑖01…𝑛f(i)\in\{0,1,\ldots,n\}italic_f ( italic_i ) ∈ { 0 , 1 , … , italic_n }. 1.2.1 Leveraging the Birgé decomposition for monotonicity-based counting (Section 3) Our proof makes crucial use of the Birgé approximation method. This is a simple method to approximate monotone distributions using a step function with few steps. It was developed by Lucien Birgé in the 1980’s [Bir87] and popularized in a number of distribution testing works, e.g., [DDS+13, DDS14] and the survey [Can20] (see Section 2.1.2 for more details). In our context, this method implies the following: to approximate the sum of a (weakly) monotone sequence x1≥x2≥…≥xnsubscript𝑥1subscript𝑥2…subscript𝑥𝑛x_{1}\geq x_{2}\geq\ldots\geq x_{n}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ≥ italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ≥ … ≥ italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT to within a 1+ε1𝜀1+\varepsilon1 + italic_ε multiplicative factor, one only needs to query a sublinear number, O⁢(ε−1⁢log⁡n)𝑂superscript𝜀1𝑛O(\varepsilon^{-1}\log n)italic_O ( italic_ε start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log italic_n ), of the elements in the sequence. We illustrate the idea of using the Birgé decomposition in counting 4444-patterns. Our approach to approximating the count of patterns like 1324132413241324 starts by fixing a value of the “3”. Specifically, we divide the set of all 1324132413241324 copies in the permutation based on the position of “3”, creating subsets C1,C2,…,Cnsubscript𝐶1subscript𝐶2…subscript𝐶𝑛C_{1},C_{2},\ldots,C_{n}italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_C start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT where each Cisubscript𝐶𝑖C_{i}italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT “3” appears at the i𝑖iitalic_i-th location. Once “3” is fixed to a certain position, we look at the possible positions for “4”. Fixing “4” further organizes Cisubscript𝐶𝑖C_{i}italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT into smaller groups based on the placement of “4” relative to “3”. One such scenario is illustrated in Figure 2. Figure 2: The illustration corresponds to permutation π=136548279𝜋136548279\pi=136548279italic_π = 136548279, depicted in a plane at points (i,πi)𝑖subscript𝜋𝑖(i,\pi_{i})( italic_i , italic_π start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ). The key insight is that each position of “4” constrains the remaining elements of the 1324132413241324 patterns in a monotone way. For example, after fixing “3” to a specific position in the permutation, we can identify all positions of “4” that can extend this configuration into valid 1324132413241324 copies. Within this subset, the positions of “4” exhibit a specific ordering: if “4” appears at a given position in the sequence, any more-to-the-right occurrence of “4” will continue to yield valid 1324132413241324 copies! Similarly, we fix “2” and then count the relevant candidates for “1”. In Section 3, we show that fixing “2” also exhibits a certain monotonicity. We use the Birgé decomposition to take advantage of this structure. The decomposition allows us to break down each subset Cisubscript𝐶𝑖C_{i}italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT into manageable, monotone classes and then efficiently approximate the count of each class in polylogarithmic time. By structuring the count around this monotonicity, we can approximately compute each |Ci|subscript𝐶𝑖|C_{i}|| italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | without directly enumerating all possibilities, which would be computationally expensive. So, by fixing values like “3”, then “4”, and then “2”, and using the Birgé decomposition to handle the emerging monotonic structures, we reduce the complexity of counting 1324132413241324 patterns to a series of fast approximations, leading to O⁢(n⋅poly⁢(n,ε−1))𝑂⋅𝑛poly𝑛superscript𝜀1O(n\cdot\mathrm{poly}(n,\varepsilon^{-1}))italic_O ( italic_n ⋅ roman_poly ( italic_n , italic_ε start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) ) running time. 1.2.2 Imposing structure through separators for 4444-patterns (Section 4) While the Birgé decomposition effectively handles some patterns, others (such as 2413241324132413) do not exhibit the same straightforward monotonic structure. For these patterns, we introduce separators to impose additional structural constraints. Consider the 4444-pattern 2413241324132413. Unlike 1324132413241324, this pattern does not naturally exhibit a straightforward monotonic structure. If we fix “4” to a particular position, we would ideally like the positions of other elements – “2”, “1”, and “3” – to show some consistent ordering so that we can apply an efficient counting method. However, without further structuring, the placements of “1” and “3” relative to “4” do not seem to reveal any particular order. Figure 3: An illustration of the idea of using separators to split the candidates for “1” and “3” into disjoint but neighboring regions based on their position. To handle this, we introduce a separator to divide the possible positions of elements in 2413241324132413 based on their relative positions to “4”. For instance, after fixing “4”, we introduce a position-based separator s𝑠sitalic_s that splits the plane into two regions. We then require that “1” appears to the left of s𝑠sitalic_s while “3” appears to the right of s𝑠sitalic_s. This allows us to approximate the count of 2413241324132413 copies within each configuration independently. We illustrate such a separator in Figure 3. With this separator in place, the counts of 2413241324132413 copies become monotone again, enabling us to apply the Birgé decomposition to each subset created by the separator. The complete analysis is presented in Section 4. 1.2.3 Global separators for 5555-patterns (Section 5) When extending our approach to 5555-patterns, we introduce an enhanced separator structure, which we refer to as global separators. This structure is specifically designed for handling the additional complexity that arises when counting 5-patterns, such as 24135241352413524135. These separators are easiest to describe using the language of two-dimensional segment trees. Consider a two-dimensional segment tree S𝑆Sitalic_S built over the plane. The outer segment tree divides the space along the x𝑥xitalic_x-axis, while each vertex in this tree contains an inner segment tree that further partitions the range along the y𝑦yitalic_y-axis. For each vertex v𝑣vitalic_v in the outer segment tree, we want to count all copies of a given 5555-pattern, e.g., 24135241352413524135, that exist within v𝑣vitalic_v but do not appear in any of its child vertices. This setup naturally leads to the concept of vertical separators. Given that v𝑣vitalic_v corresponds to an interval [a,b]𝑎𝑏[a,b][ italic_a , italic_b ] along the x𝑥xitalic_x-axis, we define a vertical separator at the midpoint (a+b)/2𝑎𝑏2(a+b)/2( italic_a + italic_b ) / 2. Any copy that spans both sides of this vertical separator is counted within v𝑣vitalic_v but not in any of v𝑣vitalic_v’s children. Figure 4: This sketch depicts the notion of vertical and horizontal global separators. In this example, the vertical dashed (blue) line is a vertical separator, splitting the range [a,b]𝑎𝑏[a,b][ italic_a , italic_b ] into two equal-sized halves. The horizontal dashed (red) line is a horizontal separator. The example also shows a (24135)24135(24135)( 24135 ) copy. This copy is counted only if (i) the “2” is to the left and the “5” is to the right of the vertical separator, and, (ii) if the “1” is below and the “5” is above the horizontal separator. In addition to vertical separators, we introduce horizontal separators that further partition each v𝑣vitalic_v based on the y𝑦yitalic_y-axis. This second layer of separation divides the region into four distinct quadrants. We refer to Figure 8 for an illustration. In addition, we consider all valid configurations of 24135241352413524135 copies relative to these quadrants. For instance, we can enforce that specific elements (e.g., “2” and “5”) fall on opposite sides of the vertical separator and that others (e.g., “1” and “5”) fall on opposite sides of the horizontal separator. This structure ensures that each copy of the pattern is counted exactly once within a unique configuration. Crucially, it turns out that this structure also induces monotonicity and allows for using the Birgé decomposition for efficient approximate counting. 1.2.4 A Primitive for Counting 12121212 Copies within Axis-Parallel Rectangles (Section 5.1) Our final technique introduces a data structure for counting simple 12121212 patterns (increasing pairs) within arbitrary axis-aligned rectangles. This primitive allows us to query the approximate number of 12121212 copies within any subregion of the input permutation. We employ this data structure to count 5555-patterns. To develop this 12121212-copy counting data structure, we employ a two-dimensional segment tree described in the previous subsection. With this tree, we pre-process the points in a bottom-up manner in O⁢(n⋅poly⁢(log⁡n,ε−1))𝑂⋅𝑛poly𝑛superscript𝜀1O(n\cdot\mathrm{poly}(\log n,\varepsilon^{-1}))italic_O ( italic_n ⋅ roman_poly ( roman_log italic_n , italic_ε start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) ) time. Section 5.1 details the implementation of this bottom-up pre-processing. This pre-processing computes an approximate number of 12121212 copies within each vertex of the segment tree. These pre-computed values are later used to answer queries for approximating the number of 12121212 copies within arbitrary rectangles, each answered in polylogarithmic time. 1.3 Open problems Our results and techniques open several interesting follow-up questions, both as the first approximate counting results for k>2𝑘2k>2italic_k > 2 and due to the novel use of the Birgé decomposition. The main open question is on the complexity of approximate counting for general (small) k𝑘kitalic_k. As discussed, there are complexity separations between detection and exact counting of permutation patterns: detection takes O⁢(n)𝑂𝑛O(n)italic_O ( italic_n ) time for any fixed length k𝑘kitalic_k, while exact counting requires n1+Ω⁢(1)superscript𝑛1Ω1n^{1+\Omega(1)}italic_n start_POSTSUPERSCRIPT 1 + roman_Ω ( 1 ) end_POSTSUPERSCRIPT time for k=4𝑘4k=4italic_k = 4, assuming Strong 3-SUM, and nΩ⁢(k/log⁡k)superscript𝑛Ω𝑘𝑘n^{\Omega(k/\log k)}italic_n start_POSTSUPERSCRIPT roman_Ω ( italic_k / roman_log italic_k ) end_POSTSUPERSCRIPT time when k𝑘kitalic_k is a parameter, assuming ETH. Approximate counting lies between exact counting and detection, and it is a priori unclear where its complexity sits between linear in n𝑛nitalic_n (for detection) and nearly worst-possible (for exact counting). Question 1.3 (Complexity of approximate counting). What is the time complexity of approximating the number of σ𝜎\sigmaitalic_σ-patterns in an input sequence f:[n]→ℝ:𝑓→delimited-[]𝑛ℝf\colon[n]\to{\mathbb{R}}italic_f : [ italic_n ] → blackboard_R to within a (1+ϵ)1italic-ϵ(1+\epsilon)( 1 + italic_ϵ )-multiplicative error, as a function of n𝑛nitalic_n and k=|σ|𝑘𝜎k=|\sigma|italic_k = | italic_σ |? Establishing tight upper and lower bounds for 1.3 appears to be challenging. Even for exact pattern counting, a more extensively studied problem, there remains a gap between the best known upper bound of nk/4+o⁢(k)superscript𝑛𝑘4𝑜𝑘n^{k/4+o(k)}italic_n start_POSTSUPERSCRIPT italic_k / 4 + italic_o ( italic_k ) end_POSTSUPERSCRIPT and the conditional lower bound of nΩ⁢(k/log⁡k)superscript𝑛Ω𝑘𝑘n^{\Omega(k/\log k)}italic_n start_POSTSUPERSCRIPT roman_Ω ( italic_k / roman_log italic_k ) end_POSTSUPERSCRIPT, both attained by Berendsohn, Kozma and Marx [BKM21]. Nevertheless, given the separation we establish for k=4𝑘4k=4italic_k = 4 and k=5𝑘5k=5italic_k = 5 (along with the new techniques which are specially suited for approximate computation) it is tempting to conjecture that the complexity of approximate counting in the general case, as a function of n𝑛nitalic_n and k𝑘kitalic_k, is fundamentally lower than that of exact counting. We make the following conjecture. Conjecture 1.4. The time complexity of approximate counting σ𝜎\sigmaitalic_σ-copies in a length-n𝑛nitalic_n sequence, as a function of n𝑛nitalic_n and k=|σ|𝑘𝜎k=|\sigma|italic_k = | italic_σ |, is asymptotically smaller than that of exact counting for the same parameters. Proving any bound of the form no⁢(k/log⁡k)superscript𝑛𝑜𝑘𝑘n^{o(k/\log k)}italic_n start_POSTSUPERSCRIPT italic_o ( italic_k / roman_log italic_k ) end_POSTSUPERSCRIPT would affirm this conjecture. But even improving upon the state of the art for exact counting would be interesting. The current best known approach of [BKM21] formulates the pattern matching instance as a constraint satisfaction problem (CSP) with binary constraints. The complexity of solving this CSP is O⁢(nt+1)𝑂superscript𝑛𝑡1O(n^{t+1})italic_O ( italic_n start_POSTSUPERSCRIPT italic_t + 1 end_POSTSUPERSCRIPT ), where t𝑡titalic_t is the treewidth of the incidence graph of the pattern π𝜋\piitalic_π (see also the work of Ahal and Rabinovich [AR08] for an earlier investigation of the role of treewidth in this context). The basic constraint graph has treewidth bounded by k/3+o⁢(k)𝑘3𝑜𝑘k/3+o(k)italic_k / 3 + italic_o ( italic_k ); Berendsohn et al. combine the tree-width based approach with a gridding technique based on ideas of Cygan, Kowalik, and Socała [CKS19] to reduce the exponent to k/4+o⁢(k)𝑘4𝑜𝑘k/4+o(k)italic_k / 4 + italic_o ( italic_k ). As we see here, algorithmic results for both detection and exact counting make use of central width notions from the parametrized complexity literature: the former gave rise to twin-width [GM14, BKTW21] and the latter makes heavy use of tree-width [AR08, BKM21]. It would be very intriguing to explore what role such width notions may play in the approximate version of pattern counting. The fact that approximate counting (in the small k𝑘kitalic_k case) admits techniques that go beyond the exact case may suggest that either a complexity notion other than tree-width is at play here, or we can use the new techniques to bound the tree-width of an easier subproblem (with more of the values constrained due to the use of, say, substructure monotonicity and Birgé approximation). From the lower bound side, essentially no nontrivial (superlinear) results are known for the Word RAM model, and proving any ω⁢(n)𝜔𝑛\omega(n)italic_ω ( italic_n ) lower bound that applies to the approximate counting of some fixed-length patterns would be interesting. We further conjecture that for large enough (constant) k𝑘kitalic_k, there should be a strongly superlinear bound. Conjecture 1.5. There exists a pattern σ𝜎\sigmaitalic_σ of constant length for which approximate counting of σ𝜎\sigmaitalic_σ in length-n𝑛nitalic_n sequences requires n1+Ω⁢(1)superscript𝑛1Ω1n^{1+\Omega(1)}italic_n start_POSTSUPERSCRIPT 1 + roman_Ω ( 1 ) end_POSTSUPERSCRIPT time. For k=3,4,5𝑘345k=3,4,5italic_k = 3 , 4 , 5, the existing algorithms for, say, 2-approximate counting (and exact counting, for k=3𝑘3k=3italic_k = 3) have time complexity n⁢logO⁢(1)⁡n𝑛superscript𝑂1𝑛n\log^{O(1)}nitalic_n roman_log start_POSTSUPERSCRIPT italic_O ( 1 ) end_POSTSUPERSCRIPT italic_n. This raises the question of whether the polylogarithmic dependence is necessary (for k=2𝑘2k=2italic_k = 2 it is not necessary [CP10]). We conjecture that the answer is positive already for k=4𝑘4k=4italic_k = 4. Finally, the use of Birgé decomposition in this paper seems to be novel in the context of pattern counting and, perhaps more generally, in combinatorial contexts beyond the scope of distribution testing. This decomposition is very useful in our setting as many sequences of quantities turn out to be monotone. It would be interesting to find other counting problems in low-dimensional geometric settings where this technique, of finding and exploiting monotone subsequences, may be useful."
https://arxiv.org/html/2411.04979v1,Quantum speedups in solving near-symmetric optimization problems by low-depth QAOA,"We present new advances in achieving exponential quantum speedups for solving optimization problems by low-depth quantum algorithms. Specifically, we focus on families of combinatorial optimization problems that exhibit symmetry and contain planted solutions. We rigorously prove that the 1-step Quantum Approximate Optimization Algorithm (QAOA) can achieve a success probability of Ω⁢(1/n)Ω1𝑛\Omega(1/\sqrt{n})roman_Ω ( 1 / square-root start_ARG italic_n end_ARG ), and sometimes Ω⁢(1)Ω1\Omega(1)roman_Ω ( 1 ), for finding the exact solution in many cases. Furthermore, we construct near-symmetric optimization problems by randomly sampling the individual clauses of symmetric problems, and prove that the QAOA maintains a strong success probability in this setting even when the symmetry is broken. Finally, we construct various families of near-symmetric Max-SAT problems and benchmark state-of-the-art classical solvers, discovering instances where all known classical algorithms require exponential time. Therefore, our results indicate that low-depth QAOA could achieve an exponential quantum speedup for optimization problems.","Optimization problems are critical to a wide range of real-world applications, and efficiently solving these problems is of great practical importance across many fields in science and industry. Quantum computers hold the promise of solving certain optimization problems faster than classical algorithms, offering potential breakthroughs in speed and efficiency. However, while there is hope for quantum speedups, we currently lack strong evidence that near-term quantum computers with limited circuit depth can achieve a substantial advantage over classical methods. In this work, we provide new evidence that a low-depth quantum algorithm, the Quantum Approximate Optimization Algorithm (QAOA) [1], can solve families of near-symmetric optimization problems exponentially faster than the best known classical algorithms. The QAOA has been proposed as a general-purpose quantum optimization algorithm that can be run on near-term quantum computers, and has seen implementation across a variety of experimental platforms [2, 3, 4, 5]. In the low-depth regime, however, limitations of the QAOA has been proven for various problems [6, 7, 8, 9, 10, 11, 12]. While there has been some evidence that shows low-depth QAOA can still provide a quantum speedup for both approximate [13] and exact optimization [14], they appear to be only small polynomial speedups. In contrast, our results demonstrate that for problems possessing some level of symmetry, an exponential speedup with the QAOA is possible even in the low-depth regime. The symmetric optimization problems we consider are maximum constraint satisfaction problems (Max-CSPs) that are defined with a planted n𝑛nitalic_n-bit string and has a cost function that exhibit certain symmetry. For example, when the symmetry is the symmetric group Snsubscript𝑆𝑛S_{n}italic_S start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT permuting all n𝑛nitalic_n bits, the cost function depends only on the Hamming distance to the planted bit string. Similar families of problems have been previously studied in the context of quantum annealing, which may take polynomial or exponential time to find the solution depending on the problem [15, 16, 17, 18]. Although a quantum speedup relative to certain general-purpose classical algorithms may be obtained in some cases, these symmetric problems are susceptible to attacks by tailored classical algorithms that know and take advantage of the symmetry. In this work, however, we take a step further to consider the situation where the symmetry is broken by randomly sampling the clauses in the cost function, and we call these problems “near-symmetric.” We also instantiate these problem explicitly in the form of Boolean satisfiability problems and benchmark them with state-of-the-art classical algorithms. As our main result, we prove that the 1-step QAOA can solve many symmetric and near-symmetric problem in polynomial time. Our findings are based on analytically deriving the success probability of 1-step QAOA in finding the planted solution using combinatorial calculations and a rigorous application of the saddle-point method. For example, given any Snsubscript𝑆𝑛S_{n}italic_S start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT-symmetric cost function that takes on n+1𝑛1n+1italic_n + 1 distinct values for the different Hamming distances, we show the 1-step QAOA has Ω⁢(1/n)Ω1𝑛\Omega(1/\sqrt{n})roman_Ω ( 1 / square-root start_ARG italic_n end_ARG ) success probability in finding the solution (Theorem 3.3). We also consider various problems with either Snsubscript𝑆𝑛S_{n}italic_S start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT or (Sn/2)2superscriptsubscript𝑆𝑛22(S_{n/2})^{2}( italic_S start_POSTSUBSCRIPT italic_n / 2 end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT symmetry and multiple local minima that confuse classical algorithms, and prove that 1-step QAOA succeeds in finding the global minima with Ω⁢(1)Ω1\Omega(1)roman_Ω ( 1 ) probability (Theorems 3.4 and 3.7). Furthermore, leveraging the high success probability from a low-depth QAOA, we show that the QAOA also succeed with similar probability even when the symmetry is broken by random sparsification of the cost function (Theorem 4.1). Hence, using repetitions of the 1-step QAOA, one can find the solution in polynomial time with O⁢(n)𝑂𝑛O(\sqrt{n})italic_O ( square-root start_ARG italic_n end_ARG ) or O⁢(1)𝑂1O(1)italic_O ( 1 ) queries to the cost function for these symmetric and near-symmetric problems. On the other hand, we show that any classical algorithm requires at least Ω⁢(n/log⁡n)Ω𝑛𝑛\Omega(n/\log n)roman_Ω ( italic_n / roman_log italic_n ) queries to the cost function to solve any Snsubscript𝑆𝑛S_{n}italic_S start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT-symmetric problem, even if the symmetry is known in advance (Claim 5.1). To explore quantum speedups, we explicitly construct instances of symmetric and near-symmetric Max-SAT problems, and study the performance of practical classical optimization algorithm such as simulated annealing and state-of-the-art classical SAT and Max-SAT solvers. In particular, we consider algorithms that are front-runners from recent SAT and Max-SAT competitions, and perform numerical experiments up to hundreds of bits to extract their run time scaling. For some instances, we show that all classical solvers known to us take exponential time to find the solution. Since one can solve these problems in polynomial time using 1-step QAOA, we have what appears to be an exponential quantum speedup over general-purpose classical algorithms by low-depth QAOA. Our results highlight the potential for significant quantum speedups even with constrained capabilities of near-term quantum devices, bringing us closer to realizing practical quantum advantages in optimization."
https://arxiv.org/html/2411.04564v1,A Generalisation of Voter Model: Influential Nodes and Convergence Properties,"Consider an undirected graph G𝐺Gitalic_G, representing a social network, where each node is blue or red, corresponding to positive or negative opinion on a topic. In the voter model, in discrete time rounds, each node picks a neighbour uniformly at random and adopts its colour. Despite its significant popularity, this model does not capture some fundamental real-world characteristics such as the difference in the strengths of individuals connections, individuals with neutral opinion on a topic, and individuals who are reluctant to update their opinion. To address these issues, we introduce and study a generalisation of the voter model.Motivating by campaigning strategies, we study the problem of selecting a set of seeds blue nodes to maximise the expected number of blue nodes after some rounds. We prove that the problem is NP-hard and provide a polynomial time approximation algorithm with the best possible approximation guarantee. Our experiments on real-world and synthetic graph data demonstrate that the proposed algorithm outperforms other algorithms.We also investigate the convergence properties of the model. We prove that the process could take an exponential number of rounds to converge. However, if we limit ourselves to strongly connected graphs, the convergence time is polynomial and the period (the number of states in convergence) divides the length of all cycles in the graph.","Humans constantly form and update their opinions on different topics, from minor subjects such as which movie to watch and which new café to try to major matters such as which political party to vote for and which company to invest in. In the process of making such decisions, we tend to rely not only on our own personal judgment and knowledge, but also that of others, especially those whose opinion we value and trust. As a result, opinion diffusion, influence propagation, and (mis)-information spreading can affect different aspects of our lives from economy and defense to fashion and personal affairs. Recent years have witnessed a booming development of online social networking platforms like Facebook, WeChat, and Instagram. The enormous popularity of these platforms has led to fundamental changes in how humans share and form opinions. Social phenomena such as disagreement and polarisation that have existed in human societies for millennia, are now taking place in an online virtual world and are tightly woven into everyday life, with a substantial impact on society. There has been a growing demand for a quantitative understanding of how opinions form and diffuse because of the existence of social ties among a community’s members and how the underlying structure of a social network can influence this process, cf. Faliszewski et al. (2022); Elkind et al. (2009); Brill et al. (2016); Bredereck and Elkind (2017); Gärtner and N. Zehmakan (2017). This would enable us to obtain better predictions of electoral results, control the effect of marketing and political campaigns, and in general advance our knowledge of the cognitive processes behind social influence. The motive to gain insights on how opinions are shaped and evolved in multi-agent systems has been the driving force behind an interdisciplinary research effort in diverse areas such as sociology Moussaïd et al. (2013), economics Jackson (2011), mathematics Balogh et al. (2012), physics Galam (2008), and computer science Chistikov et al. (2020). Within the field of computer science, especially computational social choice and algorithmic game theory, there has been a rising interest in developing and analyzing mathematical models which simulate the opinion diffusion in a network of individuals, cf. Bredereck and Elkind (2017); Out and Zehmakan (2021). Of course, in reality the opinion formation processes are too complex to be expressed in purely theoretical terms, but the goal is to shed some light on their general principles, which are otherwise hidden by the convolution of the full phenomenon. The following generic and abstract model is the foundation for most of the proposed opinion diffusion models. Consider a graph where each node has a colour and the nodes change their colour according to an updating rule, which is a function of the colour of the neighbouring nodes, in a defined order. The graph is meant to represent a social network, where the individuals are modeled as nodes and edges indicate relations between them, e.g., friendship, common interests, or advice. The colour of a node stands for its preference about a certain topic, e.g., an order over a set of candidates. In the very popular Voter Model, initially all nodes are either blue or red and then in each round, every node picks a random neighbour and adopts its colour. We generalise the voter model to capture several important real-world characteristics: • We relax the constraints of the graph being undirected and unweighted imposed in the original voter model. Directions are important for a realistic modelling since, for example in online social platforms, it is possible that one user follows the other user, but not the other way around. Furthermore, weights are utilised to model the strength of the relationships among individuals. • We allow uncoloured nodes in the initial colourings, which are nodes who are neither positive nor negative about the topic/product. • We allow some nodes to keep their colour unchanged. This accounts for stubborn or loyal individuals, who are not influenced by the opinion of their peers. Political parties and corporations frequently employ diverse strategies to persuade a specific segment of consumers on social media platforms to adopt a positive opinion about a specific product or topic. By harnessing the influence of these individuals on their social circles, a chain reaction of influence can be created, cf. Lin and Lui (2015); Myers and Leskovec (2012). This technique has emerged as a prominent method for promoting new ideas, products, and services, as it enables marketing or political campaigns to achieve extensive reach and exposure while keeping costs low. The question then becomes how to choose an initial subset of so-called early adopters to maximise the number of people that will eventually be reached, given some fixed budget. Motivated by this application, we study the problem of maximizing the expected number of blue nodes after some rounds by selecting a fixed number of initial blue nodes. We prove that the problem cannot be approximated better than (1−1/e)11𝑒(1-1/e)( 1 - 1 / italic_e ), unless P=NP, and provide a polynomial time algorithm with such approximation ratio. In addition to the theoretical guarantee, the proposed algorithm outperforms centrality based algorithms on real-world graph data. It is worth to emphasise that the red nodes can be seen as nodes who have a negative opinion about the political party or the product or prefer the competitor party or company. In either case, the voter (and consequently our model) captures the setup where switching colours is free or inexpensive, for example changing opinions about a controversial topic or switching from one grocery store chain to another. Another important problem in the area of opinion diffusion models is determining the convergence properties of the opinion dynamics: Is convergence to stable states guaranteed and if yes, what are the upper and lower bounds on the convergence time? Since in most cases, such as ours, the opinion dynamics can be modelled as a Markov process, this problem is usually equivalent to determining the stationary distribution and convergence time of the corresponding Markov chain, cf. Frischknecht et al. (2013). We prove that the convergence time can be exponential in the general case, but it becomes polynomial for strongly connected graphs. Outline. We provide some basic definitions and problem formulations in Section 2. Then, a short overview of prior work and our contributions are given in Sections 3 and 4. The complexity and algorithms results on maximum adoption problem, plus the experimental findings, are provided in Section 5. Finally, the convergence properties of the model are analyzed in Section 6."
https://arxiv.org/html/2411.04454v1,Mixing time of quantum Gibbs samplingfor random sparse Hamiltonians,"Providing evidence that quantum computers can efficiently prepare low-energy or thermal states of physically relevant interacting quantum systems is a major challenge in quantum information science. A newly developed quantum Gibbs sampling algorithm [CKG23] provides an efficient simulation of the detailed-balanced dissipative dynamics of non-commutative quantum systems. The running time of this algorithm depends on the mixing time of the corresponding quantum Markov chain, which has not been rigorously bounded except in the high-temperature regime. In this work, we establish a polylog⁢(n)polylog𝑛\mathrm{polylog}(n)roman_polylog ( italic_n ) upper bound on its mixing time for various families of random n×n𝑛𝑛n\times nitalic_n × italic_n sparse Hamiltonians at any constant temperature. We further analyze how the choice of the jump operators for the algorithm and the spectral properties of these sparse Hamiltonians influence the mixing time. Our result places this method for Gibbs sampling on par with other efficient algorithms for preparing low-energy states of quantumly easy Hamiltonians.","One of the main anticipated applications of quantum computers is the simulation and characterization of quantum systems in condensed matter physics [WHW+15], quantum chemistry [MEAG+20], and high-energy physics [Pre18, BDB+23]. The problem of simulating the dynamics (time evolution) of an interacting quantum system under a local or sparse Hamiltonian 𝑯𝑯\bm{H}bold_italic_H has largely been addressed, with efficient algorithms [HHKL18, LC17, BCC+15, LC19, GSLW19] that scale well with the number of particles, simulation time, and required precision. However, the ability of quantum computers to evaluate the static features of quantum systems, such as their ground state or thermal properties, is less understood. In this work, we focus on preparing the Gibbs (thermal) state 𝝆β=e−β⁢𝑯Tr⁢(e−β⁢𝑯)subscript𝝆𝛽superscript𝑒𝛽𝑯Trsuperscript𝑒𝛽𝑯\bm{\rho}_{\beta}=\frac{e^{-\beta\bm{H}}}{\mathrm{Tr}(e^{-\beta\bm{H}})}bold_italic_ρ start_POSTSUBSCRIPT italic_β end_POSTSUBSCRIPT = divide start_ARG italic_e start_POSTSUPERSCRIPT - italic_β bold_italic_H end_POSTSUPERSCRIPT end_ARG start_ARG roman_Tr ( italic_e start_POSTSUPERSCRIPT - italic_β bold_italic_H end_POSTSUPERSCRIPT ) end_ARG of a quantum system, which represents the equilibrium state when the system is in contact with a thermal bath at a fixed temperature β−1superscript𝛽1\beta^{-1}italic_β start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT. This computational problem, known as Gibbs sampling or “cooling,” is valuable not only for simulating thermodynamic properties but also as a subroutine in quantum algorithms for optimization and learning [BS17, vAG19, BKL+19]. However, to prepare the Gibbs state, quantum computers face challenges. In general, it is not believed that estimating the low-temperature properties of quantum systems can be solved efficiently by a quantum computer in the worst-case [KKR05]. Fortunately, it has been hypothesized that this worst-case hardness of finding low-temperature states implied by arguments from complexity theory is due to pathological Hamiltonians, which are not apparent in many physical systems that normally occur in nature. This hypothesis is substantiated by the empirical success of natural cooling, such as using refrigerators, in reaching thermal equilibrium. Quantum Gibbs sampling. Aiming to mimic nature’s cooling processes, a series of recent works have introduced quantum Markov Chain Monte Carlo (MCMC) algorithms, or quantum Gibbs samplers [CKG23, CKBG23, SM21, WT23, RWW23, JI24, ZBC23, DLL24, GCDK24], as promising alternatives for tackling a range of classically intractable low-temperature simulation tasks on quantum computers. These algorithms are designed to replicate the success of classical Markov chains in preparing Gibbs states for classical Hamiltonians. The analysis of classical MCMC algorithms relies on the principle of detailed balance; however, achieving this in the quantum setting has been challenging and was only recently addressed by an algorithm in [CKG23]. Part of the difficulty arises from a conflict between the finite energy resolution σEsubscript𝜎𝐸\sigma_{E}italic_σ start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT achievable by efficient quantum algorithms and the seemingly strict requirement to precisely distinguish energy levels to satisfy detailed balance. In this work, we focus primarily on this algorithm, referring to it as the CKG algorithm or the quantum Gibbs sampler when the context is clear. We give a detailed review of this algorithm in Section 4.1.3 and Appendix 7.1.1. The Gibbs sampling algorithm provides a fully general method for preparing Gibbs states by evolving an initial state 𝝆0subscript𝝆0\bm{\rho}_{0}bold_italic_ρ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT under a Lindbladian ℒβsubscriptℒ𝛽\mathcal{L}_{\beta}caligraphic_L start_POSTSUBSCRIPT italic_β end_POSTSUBSCRIPT, which is efficiently implementable on a quantum computer and produces the state 𝝆t=eℒβ⁢t⁢[𝝆0]subscript𝝆𝑡superscript𝑒subscriptℒ𝛽𝑡delimited-[]subscript𝝆0\bm{\rho}_{t}=e^{\mathcal{L}_{\beta}t}[\bm{\rho}_{0}]bold_italic_ρ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_e start_POSTSUPERSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_β end_POSTSUBSCRIPT italic_t end_POSTSUPERSCRIPT [ bold_italic_ρ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ] after time t𝑡titalic_t. The runtime of the quantum Gibbs sampler is governed by the mixing time of the corresponding quantum Markov chain, which is roughly the time required for 𝝆tsubscript𝝆𝑡\bm{\rho}_{t}bold_italic_ρ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT to approach the Gibbs state 𝝆βsubscript𝝆𝛽\bm{\rho}_{\beta}bold_italic_ρ start_POSTSUBSCRIPT italic_β end_POSTSUBSCRIPT. This in turn is bounded by the spectral gap λgap⁢(ℒβ)subscript𝜆gapsubscriptℒ𝛽\lambda_{\text{gap}}(\mathcal{L}_{\beta})italic_λ start_POSTSUBSCRIPT gap end_POSTSUBSCRIPT ( caligraphic_L start_POSTSUBSCRIPT italic_β end_POSTSUBSCRIPT ) of the Lindbladian by tmix⁢(ℒβ)≤𝒪⁢(β⁢‖𝑯‖+log⁡(n))λgap⁢(ℒβ).subscript𝑡mixsubscriptℒ𝛽𝒪𝛽norm𝑯𝑛subscript𝜆gapsubscriptℒ𝛽t_{\text{mix}}(\mathcal{L}_{\beta})\leq\frac{\mathcal{O}(\beta\|{\bm{H}}\|+% \log(n))}{\lambda_{\text{gap}}(\mathcal{L}_{\beta})}.italic_t start_POSTSUBSCRIPT mix end_POSTSUBSCRIPT ( caligraphic_L start_POSTSUBSCRIPT italic_β end_POSTSUBSCRIPT ) ≤ divide start_ARG caligraphic_O ( italic_β ∥ bold_italic_H ∥ + roman_log ( italic_n ) ) end_ARG start_ARG italic_λ start_POSTSUBSCRIPT gap end_POSTSUBSCRIPT ( caligraphic_L start_POSTSUBSCRIPT italic_β end_POSTSUBSCRIPT ) end_ARG . The mixing time varies based on the quantum system in question. Bounding this mixing time is challenging without access to fault-tolerant quantum computers, as we cannot run and benchmark the algorithm directly, making theoretical analysis essential. However, such analysis is hindered by a lack of technical tools for two key reasons. Firstly, the theory of convergence of quantum Markov chains is new, unlike the very mature twin field for classical Markov chains. Secondly, the Markov chain described by the algorithm is considerably complex, and depends on several parameters that we will discuss in more detail shortly: an energy resolution σEsubscript𝜎𝐸\sigma_{E}italic_σ start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT, a series of jump operators 𝑨asuperscript𝑨𝑎\bm{A}^{a}bold_italic_A start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT for a∈[M]𝑎delimited-[]𝑀a\in[M]italic_a ∈ [ italic_M ], and the inverse temperature β𝛽\betaitalic_β. The space of possibilities makes the algorithm’s performance more difficult to characterize. This motivates the identification of quantum systems whose mixing times are tractable for analysis yet exhibit rich features that provide insights into the performance of the quantum Gibbs sampler for more general non-commuting Hamiltonians. In line with this, the mixing time of the CKG algorithm has recently been bounded for local Hamiltonians, showing a polynomial scaling with system size at high enough temperatures [RFA24]. Mixing time of sparse Hamiltonians. In this work, we consider an alternative approach by characterizing the mixing time of a family of sparse Hamiltonians of the following form: which can be understood as the Hamiltonian on a graph G=(V,E)𝐺𝑉𝐸G=(V,E)italic_G = ( italic_V , italic_E ) with n=|V|𝑛𝑉n=|V|italic_n = | italic_V | vertices indexed by basis states \ket⁢ei\ketsubscript𝑒𝑖\ket{e_{i}}italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, i∈[n]𝑖delimited-[]𝑛i\in[n]italic_i ∈ [ italic_n ] and a set of edges E𝐸Eitalic_E connecting vertices with Hi⁢j≠0subscript𝐻𝑖𝑗0H_{ij}\neq 0italic_H start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ≠ 0. When non-zero entries Hi⁢jsubscript𝐻𝑖𝑗H_{ij}italic_H start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT are all equal to 1111, the Hamiltonian 𝑯𝑯\bm{H}bold_italic_H corresponds to the n×n𝑛𝑛n\times nitalic_n × italic_n adjacency matrix of the n𝑛nitalic_n-vertex graph. We define the degree d𝑑ditalic_d of the graph G𝐺Gitalic_G as the sparsity of the underlying Hamiltonian and refer to Hamiltonians with constant or slowly increasing degrees d=polylog⁢(n)𝑑polylog𝑛d=\mathrm{polylog}(n)italic_d = roman_polylog ( italic_n ) as sparse. Note that any log⁡(n)𝑛\log(n)roman_log ( italic_n )-qubit Hamiltonian that consists of m=polylog⁢(n)𝑚polylog𝑛m=\mathrm{polylog}(n)italic_m = roman_polylog ( italic_n ) terms each acting locally on κ=O⁢(1)𝜅𝑂1\kappa=O(1)italic_κ = italic_O ( 1 ) qubits is a sparse Hamiltonian with degree d≤m⁢2κ≤polylog⁢(n)𝑑𝑚superscript2𝜅polylog𝑛d\leq m2^{\kappa}\leq\mathrm{polylog}(n)italic_d ≤ italic_m 2 start_POSTSUPERSCRIPT italic_κ end_POSTSUPERSCRIPT ≤ roman_polylog ( italic_n ). However, not all sparse Hamiltonians admit local qubit encodings. Having defined sparse Hamiltonians, we now consider the dissipative dynamics of this system induced by a set of M𝑀Mitalic_M jump operators expressed as follows: 𝑨a=∑i,j∈[n]Ai⁢ja⁢\ket⁢ei⁢\bra⁢ej,a∈[M].formulae-sequencesuperscript𝑨𝑎subscript𝑖𝑗delimited-[]𝑛superscriptsubscript𝐴𝑖𝑗𝑎\ketsubscript𝑒𝑖\brasubscript𝑒𝑗𝑎delimited-[]𝑀\displaystyle\bm{A}^{a}=\sum_{i,j\in[n]}A_{ij}^{a}\ket{e_{i}}\bra{e_{j}},\quad% \quad a\in[M].bold_italic_A start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT = ∑ start_POSTSUBSCRIPT italic_i , italic_j ∈ [ italic_n ] end_POSTSUBSCRIPT italic_A start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_e start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_a ∈ [ italic_M ] . (1) We will soon explain how the jump operators 𝑨asuperscript𝑨𝑎\bm{A}^{a}bold_italic_A start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT relate to the Lindbladian ℒβsubscriptℒ𝛽\mathcal{L}_{\beta}caligraphic_L start_POSTSUBSCRIPT italic_β end_POSTSUBSCRIPT. Briefly, the resulting dynamics can be understood as a combination of two processes: a continuous-time quantum walk of a single particle on the graph of states due to the coherent evolution of the Hamiltonian 𝑯𝑯\bm{H}bold_italic_H, which is combined with stochastic jumps on the graph determined by the jump operators 𝑨asuperscript𝑨𝑎\bm{A}^{a}bold_italic_A start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT. Our interest in bounding the mixing time of the sparse Hamiltonians is multifaceted: (1) Single-particle dynamics. As stated earlier, bounding the mixing time of general interacting multipartite Hamiltonians is a challenging task. However, for simple choices of graphs G𝐺Gitalic_G, the mixing time of the quantum Gibbs sampler may be easier to analyze, potentially leading to relevant techniques for tackling the case of interacting particles. In fact, we can think of the dynamics induced by the Hamiltonian 𝑯𝑯\bm{H}bold_italic_H (LABEL:eq:sparseH) as the dynamics of a single-particle hopping on the graph G𝐺Gitalic_G. This single-particle evolution on path graphs or grids is commonly analyzed in the tight-binding model in condensed matter physics. That being said, even in the simplified case of a single particle, the Hamiltonian 𝑯𝑯\bm{H}bold_italic_H is non-commuting, characterizing a continuous-time quantum walk that can yield exponential quantum advantage for certain oracular problem on graphs such as the glued tress [CCD+03]. (2) Chaotic Hamiltonians. Our additional motivation for studying random sparse Hamiltonians stems from the fact that their spectra exhibit many of the same characteristics as chaotic Hamiltonians, such as the SYK model [SY93, Kit15a, Kit15b] and random p𝑝pitalic_p-spin models [SW24, WBB+22]. Understanding whether chaotic Hamiltonians have a fast mixing time as they approach their thermal and low-energy states is a fundamental question in the study of quantum chaos [CB21, ACKK24]. As a concrete step toward addressing this problem, we identify key spectral properties of random sparse Hamiltonians that can ensure a fast mixing time. (3) Algorithmic applications. Preparing quantum Gibbs states, and more broadly computing the matrix exponential of sparse matrices such as the adjacency or Laplacian of a graph, is a fundamental subroutine in solving various graph and optimization problems. For instance, the Estrada index—defined as the trace of the matrix exponential of a graph’s adjacency matrix—measures subgraph centrality and provides structural insights [ERV05]. Computing the matrix exponential is also related to matrix inversion and linear system solvers [SV13]. Moreover, quantum Gibbs sampling has been applied to solving semidefinite programs (SDPs) in optimization problems [GLBKSF22, BS17, BKL+19, vAGGdW20], offering quantum speedups for these problems."
https://arxiv.org/html/2411.04394v1,Statistical-Computational Trade-offs for Greedy Recursive Partitioning Estimators,"Models based on recursive partitioning such as decision trees and their ensembles are popular for high-dimensional regression as they can potentially avoid the curse of dimensionality. Because empirical risk minimization (ERM) is computationally infeasible, these models are typically trained using greedy algorithms. Although effective in many cases, these algorithms have been empirically observed to get stuck at local optima. We explore this phenomenon in the context of learning sparse regression functions over d𝑑ditalic_d binary features, showing that when the true regression function f∗superscript𝑓f^{*}italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT does not satisfy the so-called Merged Staircase Property (MSP), greedy training requires exp⁡(Ω⁢(d))Ω𝑑\exp(\Omega(d))roman_exp ( roman_Ω ( italic_d ) ) to achieve low estimation error. Conversely, when f∗superscript𝑓f^{*}italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT does satisfy MSP, greedy training can attain small estimation error with only O⁢(log⁡d)𝑂𝑑O(\log d)italic_O ( roman_log italic_d ) samples. This performance mirrors that of two-layer neural networks trained with stochastic gradient descent (SGD) in the mean-field regime, thereby establishing a head-to-head comparison between SGD-trained neural networks and greedy recursive partitioning estimators. Furthermore, ERM-trained recursive partitioning estimators achieve low estimation error with O⁢(log⁡d)𝑂𝑑O(\log d)italic_O ( roman_log italic_d ) samples irrespective of whether f∗superscript𝑓f^{*}italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT satisfies MSP, thereby demonstrating a statistical-computational trade-off for greedy training. Our proofs are based on a novel interpretation of greedy recursive partitioning using stochastic process theory and a coupling technique that may be of independent interest.","Decision tree models are piecewise constant supervised learning models obtained by recursive partitioning of the covariate space. Although classical, they remain among the most important supervised learning models because they are highly interpretable (Rudin et al., 2021; Murdoch et al., 2019) and yet are flexible enough to afford the potential for high prediction accuracy. This potential is maximized when decision trees are combined in ensembles via random forests (RFs) (Breiman, 2001) or gradient boosting (Friedman, 2001). These algorithms are widely recognized as having state-of-the-art performance on moderately-sized tabular datasets (Caruana et al., 2008; Fernández-Delgado et al., 2014; Olson et al., 2018), even outperforming state-of-the-art deep learning methods (Grinsztajn et al., 2022), despite the amount of attention lavished on the latter. Such datasets are common in many settings such as bioinformatics, healthcare, economics, and social sciences. Naturally, decision trees and their ensembles receive widespread use via their implementation in popular machine learning packages such as ranger (Wright et al., 2017), scikit-learn (Pedregosa et al., 2011), xgboost (Chen and Guestrin, 2016), and lgbm (Ke et al., 2017). Decision trees have also been adapted to a variety of tasks beyond regression and classification, including survival analysis (Ishwaran et al., 2008), heterogeneous treatment effect estimation (Athey and Imbens, 2016), time series analysis, and multi-task learning. While alternatives exist, most decision tree models used in practice make binary, axis-aligned splits at each partitioning stage. In this paper, we study the statistical-computational trade-offs of these objects, focusing on regression trees. We assume a nonparametric regression model under random design: Yi=f∗⁢(𝐗i)+εi,i=1,2,…,n.formulae-sequencesubscript𝑌𝑖superscript𝑓subscript𝐗𝑖subscript𝜀𝑖𝑖12…𝑛Y_{i}=f^{*}(\mathbf{X}_{i})+\varepsilon_{i},\quad i=1,2,\ldots,n.italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) + italic_ε start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_i = 1 , 2 , … , italic_n . (1) Here, the covariate space 𝒳𝒳\mathcal{X}caligraphic_X is a compact subset of ℝdsuperscriptℝ𝑑\mathbb{R}^{d}roman_ℝ start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT, 𝐗1,𝐗2,…⁢𝐗nsubscript𝐗1subscript𝐗2…subscript𝐗𝑛\mathbf{X}_{1},\mathbf{X}_{2},\ldots\mathbf{X}_{n}bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_X start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … bold_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT are drawn i.i.d. from a distribution ν𝜈\nuitalic_ν on 𝒳𝒳\mathcal{X}caligraphic_X, ε1,ε2,…,εnsubscript𝜀1subscript𝜀2…subscript𝜀𝑛\varepsilon_{1},\varepsilon_{2},\ldots,\varepsilon_{n}italic_ε start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_ε start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_ε start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT are noise variables drawn i.i.d. from a zero-mean noise distribution on ℝℝ\mathbb{R}roman_ℝ, with εi⟂⟂𝐗i≕(Xi⁢1,Xi⁢2,…,Xi⁢d)\varepsilon_{i}\perp\!\!\!\perp\mathbf{X}_{i}\eqqcolon(X_{i1},X_{i2},\ldots,X_% {id})italic_ε start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ⟂ ⟂ bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ≕ ( italic_X start_POSTSUBSCRIPT italic_i 1 end_POSTSUBSCRIPT , italic_X start_POSTSUBSCRIPT italic_i 2 end_POSTSUBSCRIPT , … , italic_X start_POSTSUBSCRIPT italic_i italic_d end_POSTSUBSCRIPT ), and f∗:𝒳→ℝ:superscript𝑓→𝒳ℝf^{*}\colon\mathcal{X}\to\mathbb{R}italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT : caligraphic_X → roman_ℝ is the conditional expectation function of Y𝑌Yitalic_Y given 𝐗=𝐱𝐗𝐱\mathbf{X}=\mathbf{x}bold_X = bold_x. The observed data is denoted as 𝒟n≔{(𝐗i,Yi):i=1,…,n}≔subscript𝒟𝑛conditional-setsubscript𝐗𝑖subscript𝑌𝑖𝑖1…𝑛\mathcal{D}_{n}\coloneqq\left\{(\mathbf{X}_{i},Y_{i})\colon i=1,\ldots,n\right\}caligraphic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ≔ { ( bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) : italic_i = 1 , … , italic_n }. We will compare and contrast various tree-based estimators for f∗superscript𝑓f^{*}italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT , evaluating the accuracy of an estimate f^⁢(−;𝒟n)^𝑓subscript𝒟𝑛\hat{f}(-;\mathcal{D}_{n})over^ start_ARG italic_f end_ARG ( - ; caligraphic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) using the L2superscript𝐿2L^{2}italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT risk (or estimation error) R⁢(f^⁢(−;𝒟n),f∗)≔𝔼𝐗∼ν⁢{(f^⁢(𝐗;𝒟n)−f∗⁢(𝐗))2},≔𝑅^𝑓subscript𝒟𝑛superscript𝑓subscript𝔼similar-to𝐗𝜈superscript^𝑓𝐗subscript𝒟𝑛superscript𝑓𝐗2R(\hat{f}(-;\mathcal{D}_{n}),f^{*})\coloneqq\mathbb{E}_{\mathbf{X}\sim\nu}% \left\{\left(\hat{f}(\mathbf{X};\mathcal{D}_{n})-f^{*}(\mathbf{X})\right)^{2}% \right\},italic_R ( over^ start_ARG italic_f end_ARG ( - ; caligraphic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) , italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ) ≔ roman_𝔼 start_POSTSUBSCRIPT bold_X ∼ italic_ν end_POSTSUBSCRIPT { ( over^ start_ARG italic_f end_ARG ( bold_X ; caligraphic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) - italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( bold_X ) ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT } , and the accuracy of an estimator f^⁢(−;−)^𝑓\hat{f}(-;-)over^ start_ARG italic_f end_ARG ( - ; - ) using the expected L2superscript𝐿2L^{2}italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT risk, 𝔼𝒟n⁢{R⁢(f^⁢(−;𝒟n),f∗)}subscript𝔼subscript𝒟𝑛𝑅^𝑓subscript𝒟𝑛superscript𝑓\mathbb{E}_{\mathcal{D}_{n}}\left\{R\left(\hat{f}(-;\mathcal{D}_{n}),f^{*}% \right)\right\}roman_𝔼 start_POSTSUBSCRIPT caligraphic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_POSTSUBSCRIPT { italic_R ( over^ start_ARG italic_f end_ARG ( - ; caligraphic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) , italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ) }. Note that throughout this paper, we will use the convention that random variables are denoted in upper case while any fixed value they may attain is denoted in lower case. Furthermore, vectors will be denoted using boldface, whereas scalars will be denoted using regular font. We denote [d]≔{1,2,…,d}≔delimited-[]𝑑12…𝑑[d]\coloneqq\left\{1,2,\ldots,d\right\}[ italic_d ] ≔ { 1 , 2 , … , italic_d }. To supplement standard Big-O𝑂Oitalic_O notation, we use O~~𝑂\tilde{O}over~ start_ARG italic_O end_ARG to suppress poly-logarithmic factors. 1.1 Consistency of ERM trees, Greedy Trees, and Non-Adaptive Trees Machine learning models are often fit by solving empirical risk minimization (ERM). Solutions to ERM over appropriate classes of decision tree functions can be shown to be consistent, but unfortunately are computationally infeasible to obtain. For instance, ERM for classification trees is known to be NP-hard in the worst case (Hyafil and Rivest, 1976). As a result, practitioners make use of greedy algorithms such as CART (Breiman et al., 1984), ID3 (Quinlan, 1986), or C4.5 (Quinlan, 1993). Such algorithms grow a tree in a top-down manner, with the split for a given node chosen by optimizing a local node-specific objective. Once made, a split is never reversed.111Decision trees are sometimes pruned as a one-off post-processing procedure after they are fully grown. These heuristics seem to work well in practice—both RFs and gradient boosting are built out of CART trees—but are notoriously difficult to analyze. Indeed, early efforts to prove consistency of greedy regression and classification trees under standard smoothness conditions on f∗superscript𝑓f^{*}italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT had to further assume that the bias of the tree converged to zero (Breiman et al., 1984; Chaudhuri et al., 1995). More recent works were able to replace this assumption with more reasonable alternatives on f∗superscript𝑓f^{*}italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT such as additivity (Scornet et al., 2015; Klusowski, 2020; Klusowski and Tian, 2024), a submodularity condition (Syrgkanis and Zampetakis, 2020), or the sufficient impurity decrease (SID) condition (Chi et al., 2022; Mazumder and Wang, 2024). In another line of work, researchers studied and were able to prove consistency for decision trees making splits that are non-adaptive (i.e., independent of the response). These algorithms include trees that make completely random splits (Geurts et al., 2006; Biau et al., 2008; Biau, 2012; Wager and Athey, 2018; Klusowski, 2021), Mondrian trees which make splits according to a Bayesian modeling of the covariate distribution (Roy and Teh, 2008; Lakshminarayanan et al., 2014; Mourtada et al., 2017; Cattaneo et al., 2023) and k𝑘kitalic_k-Potential Nearest Neighbors (Lin and Jeon, 2006; Biau and Devroye, 2010; Scornet, 2016; Biau and Scornet, 2016; Shi et al., 2024). Despite possessing theoretical guarantees requiring fewer assumptions, such algorithms exhibit performance gaps compared to greedy trees, in particular, in terms of adaptivity to heterogeneous truth. 1.2 High-Dimensional Consistency From the discussion thus far, we gather that considering nonparametric consistency alone does not explain why greedy trees have been so successful relative to non-adaptive trees, let alone other machine learning algorithms such as k𝑘kitalic_k-nearest neighbors or support vector machines. To explain this, we compare the performance of these algorithms in a high-dimensional setting (n,d→∞→𝑛𝑑n,d\to\inftyitalic_n , italic_d → ∞) with sparsity constraints on f∗superscript𝑓f^{*}italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT, i.e., if f∗superscript𝑓f^{*}italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT only depends on a constant number of s𝑠sitalic_s covariates. Informally, we say that an estimator avoids the curse of dimensionality, or that it is high-dimensional consistent, if its expected L2superscript𝐿2L^{2}italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT risk converges to zero under a scaling regime satisfying log⁡n=o⁢(d)𝑛𝑜𝑑\log n=o(d)roman_log italic_n = italic_o ( italic_d ). We first argue that non-adaptive trees are not high-dimensional consistent222See Appendix A for a formal definition of non-adaptive tree models and a rigorous proof of this statement.: Assuming relatively balanced splits, the average depth of a tree is O⁢(log⁡n)𝑂𝑛O(\log n)italic_O ( roman_log italic_n ). Hence, log⁡n=o⁢(d)𝑛𝑜𝑑\log n=o(d)roman_log italic_n = italic_o ( italic_d ) implies that not every covariate can receive a split, or in other words, that the tree needs to be able to selectively split on relevant covariates, which contradicts the definition of non-adaptivity. In contrast, several works (Syrgkanis and Zampetakis, 2020; Chi et al., 2022; Mazumder and Wang, 2024; Klusowski and Tian, 2024) have showed that CART and RFs are high-dimensional consistent given various condition on f∗superscript𝑓f^{*}italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT (e.g., submodularity, SID, or additivity). This helps to explain the successful application of RFs to high-dimensional problems such as genomics in which n≪dmuch-less-than𝑛𝑑n\ll ditalic_n ≪ italic_d. On the other hand, these works were not able to show whether their assumed conditions were necessary for high-dimensional consistency. Syrgkanis and Zampetakis (2020) used the exclusive-or (XOR) function to argue heuristically that some condition is indeed necessary (see Section 5 therein). To understand this argument, we focus on the setting of binary covariates (𝒳={±1}d𝒳superscriptplus-or-minus1𝑑\mathcal{X}=\{\pm 1\}^{d}caligraphic_X = { ± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT) under the uniform distribution. In this setting, the XOR function can be written as f∗⁢(𝐱)=x1⁢x2superscript𝑓𝐱subscript𝑥1subscript𝑥2f^{*}(\mathbf{x})=x_{1}x_{2}italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( bold_x ) = italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. Meanwhile, every tree node corresponds to a subcube 𝒞⊂{±1}d𝒞superscriptplus-or-minus1𝑑\mathcal{C}\subset\{\pm 1\}^{d}caligraphic_C ⊂ { ± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT and every potential split of 𝒞𝒞\mathcal{C}caligraphic_C divides it evenly into two subcubes of lower dimension. With each split fully determined by the choice of covariate index k=1,…,d𝑘1…𝑑k=1,\ldots,ditalic_k = 1 , … , italic_d, CART chooses the index of the covariate with the maximum plug-in squared correlation with the response within 𝒞𝒞\mathcal{C}caligraphic_C: Corr^2⁢{Y,Xk|𝐗∈𝒞}=(∑𝐗i∈𝒞Xi⁢k⁢(Yi−Y¯𝒞))2∑𝐗i∈𝒞(Xi⁢k−(X¯k)𝒞)2⁢∑𝐗i∈𝒞(Yi−Y¯𝒞)2;superscript^Corr2conditional-set𝑌subscript𝑋𝑘𝐗𝒞superscriptsubscriptsubscript𝐗𝑖𝒞subscript𝑋𝑖𝑘subscript𝑌𝑖subscript¯𝑌𝒞2subscriptsubscript𝐗𝑖𝒞superscriptsubscript𝑋𝑖𝑘subscriptsubscript¯𝑋𝑘𝒞2subscriptsubscript𝐗𝑖𝒞superscriptsubscript𝑌𝑖subscript¯𝑌𝒞2\widehat{\operatorname{Corr}}^{2}\left\{Y,X_{k}~{}|~{}\mathbf{X}\in\mathcal{C}% \right\}=\frac{\left(\sum_{\mathbf{X}_{i}\in\mathcal{C}}X_{ik}(Y_{i}-\bar{Y}_{% \mathcal{C}})\right)^{2}}{\sum_{\mathbf{X}_{i}\in\mathcal{C}}(X_{ik}-(\bar{X}_% {k})_{\mathcal{C}})^{2}\sum_{\mathbf{X}_{i}\in\mathcal{C}}(Y_{i}-\bar{Y}_{% \mathcal{C}})^{2}};over^ start_ARG roman_Corr end_ARG start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT { italic_Y , italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | bold_X ∈ caligraphic_C } = divide start_ARG ( ∑ start_POSTSUBSCRIPT bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ caligraphic_C end_POSTSUBSCRIPT italic_X start_POSTSUBSCRIPT italic_i italic_k end_POSTSUBSCRIPT ( italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT caligraphic_C end_POSTSUBSCRIPT ) ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG ∑ start_POSTSUBSCRIPT bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ caligraphic_C end_POSTSUBSCRIPT ( italic_X start_POSTSUBSCRIPT italic_i italic_k end_POSTSUBSCRIPT - ( over¯ start_ARG italic_X end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT caligraphic_C end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ∑ start_POSTSUBSCRIPT bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ caligraphic_C end_POSTSUBSCRIPT ( italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT caligraphic_C end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ; (2) (see Klusowski and Tian (2021) and Klusowski (2020), Equation (5)). Unless 𝒞𝒞\mathcal{C}caligraphic_C already depends on either x1subscript𝑥1x_{1}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT or x2subscript𝑥2x_{2}italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, however, we have zero correlation in the infinite-sample limit, i.e., Corr2⁡{f∗⁢(𝐗),Xk|𝐗∈𝒞}=𝔼⁢{X1⁢X2⁢Xk|𝐗∈𝒞}2=0,superscriptCorr2superscript𝑓𝐗conditionalsubscript𝑋𝑘𝐗𝒞𝔼superscriptconditional-setsubscript𝑋1subscript𝑋2subscript𝑋𝑘𝐗𝒞20\operatorname{Corr}^{2}\left\{f^{*}(\mathbf{X}),X_{k}~{}|~{}\mathbf{X}\in% \mathcal{C}\right\}=\mathbb{E}\left\{X_{1}X_{2}X_{k}~{}|~{}\mathbf{X}\in% \mathcal{C}\right\}^{2}=0,roman_Corr start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT { italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( bold_X ) , italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | bold_X ∈ caligraphic_C } = roman_𝔼 { italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_X start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | bold_X ∈ caligraphic_C } start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 0 , (3) for k=1,…,d𝑘1…𝑑k=1,\ldots,ditalic_k = 1 , … , italic_d. This implies that CART cannot differentiate between relevant and irrelevant covariates, instead tending to make non-adaptive random splits. Hence, it cannot avoid the curse of dimensionality. As an ensemble of CART trees, RF suffers from the same limitations. Beyond some heuristic calculations, Syrgkanis and Zampetakis (2020) were not able to make this argument rigorous, nor did they attempt to generalize beyond this example. In this paper, we seek to bridge this gap and establish a necessary and nearly sufficient condition on f∗superscript𝑓f^{*}italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT that characterizes when CART is high-dimensional consistent, and, in doing so, explicitly compare its risk performance against that of ERM and two-layer neural networks trained with SGD. We do this in the context of binary covariates under the uniform distribution, which although unlikely to describe any real dataset, provides a useful setting where both results and calculations can be stated cleanly and precisely, thereby illustrating the essence of the strengths and limitations of using a greedy splitting rule. We will later also provide extensions of these results to more general settings. 1.3 “Almost” Characterization Using the Merged-Staircase Property For any subset S⊂[d]𝑆delimited-[]𝑑S\subset[d]italic_S ⊂ [ italic_d ] of the coordinate indices, let χSsubscript𝜒𝑆\chi_{S}italic_χ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT be the monomial function defined by χS⁢(𝐱)=∏j∈Sxjsubscript𝜒𝑆𝐱subscriptproduct𝑗𝑆subscript𝑥𝑗\chi_{S}(\mathbf{x})=\prod_{j\in S}x_{j}italic_χ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( bold_x ) = ∏ start_POSTSUBSCRIPT italic_j ∈ italic_S end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT.333If |S|=s𝑆𝑠|S|=s| italic_S | = italic_s, such a function is sometimes called an s𝑠sitalic_s-sparse parity in the theoretical computer science literature. It is a classical result from Fourier analysis (Stein and Shakarchi, 2011) that every function f∗:{±1}d→ℝ:superscript𝑓→superscriptplus-or-minus1𝑑ℝf^{*}\colon\{\pm 1\}^{d}\to\mathbb{R}italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT : { ± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT → roman_ℝ can be uniquely written in the form f∗=∑i=1rαSi⁢χSi,superscript𝑓superscriptsubscript𝑖1𝑟subscript𝛼subscript𝑆𝑖subscript𝜒subscript𝑆𝑖f^{*}=\sum_{i=1}^{r}\alpha_{S_{i}}\chi_{S_{i}},italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT italic_α start_POSTSUBSCRIPT italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_χ start_POSTSUBSCRIPT italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT , (4) for some nonnegative integer r𝑟ritalic_r, subsets S1,S2,…,Srsubscript𝑆1subscript𝑆2…subscript𝑆𝑟S_{1},S_{2},\ldots,S_{r}italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_S start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT of {1,2,…,d}12…𝑑\left\{1,2,\ldots,d\right\}{ 1 , 2 , … , italic_d } and nonzero coefficients αSj,j=1,2,…,dformulae-sequencesubscript𝛼subscript𝑆𝑗𝑗12…𝑑\alpha_{S_{j}},j=1,2,\ldots,ditalic_α start_POSTSUBSCRIPT italic_S start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT , italic_j = 1 , 2 , … , italic_d. We say that f∗superscript𝑓f^{*}italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT satisfies the merged-staircase property (MSP) if the subsets S1,S2,…,Srsubscript𝑆1subscript𝑆2…subscript𝑆𝑟S_{1},S_{2},\ldots,S_{r}italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_S start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT can be reordered such that for any i∈{1,2,…,r}𝑖12…𝑟i\in\{1,2,\ldots,r\}italic_i ∈ { 1 , 2 , … , italic_r }, we have |Si\∪j=1i−1Sj|≤1.\left|S_{i}\backslash\cup_{j=1}^{i-1}S_{j}\right|\leq 1.| italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \ ∪ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i - 1 end_POSTSUPERSCRIPT italic_S start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | ≤ 1 . (5) Note that any monomial of degree k>1𝑘1k>1italic_k > 1 does not satisfy the MSP. In particular, the XOR function does not satisfy the MSP. On the other hand, a positive example is provided by the function f∗⁢(𝐱)=x1+x2+x1⁢x2⁢x3.superscript𝑓𝐱subscript𝑥1subscript𝑥2subscript𝑥1subscript𝑥2subscript𝑥3f^{*}(\mathbf{x})=x_{1}+x_{2}+x_{1}x_{2}x_{3}.italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( bold_x ) = italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT . (6) Our main theorem, stated informally, is the following: Theorem 1.1 (Informal). When 𝒳={±1}d𝒳superscriptplus-or-minus1𝑑\mathcal{X}=\{\pm 1\}^{d}caligraphic_X = { ± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT, ν=Unif⁡({±1}d)𝜈Unifsuperscriptplus-or-minus1𝑑\nu=\operatorname{Unif}\left(\{\pm 1\}^{d}\right)italic_ν = roman_Unif ( { ± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ), and f∗superscript𝑓f^{*}italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT depends only on s𝑠sitalic_s covariates, then the expected L2superscript𝐿2L^{2}italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT risk of the CART estimator f^CARTsubscript^𝑓CART\hat{f}_{\operatorname{CART}}over^ start_ARG italic_f end_ARG start_POSTSUBSCRIPT roman_CART end_POSTSUBSCRIPT satisfies: • (Necessity) If f∗superscript𝑓f^{*}italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT does not satisfy MSP, then 𝔼𝒟n⁢{R⁢(f^CART⁢(−;𝒟n),f∗)}=Ω⁢(1)subscript𝔼subscript𝒟𝑛𝑅subscript^𝑓CARTsubscript𝒟𝑛superscript𝑓Ω1\mathbb{E}_{\mathcal{D}_{n}}\left\{R\left(\hat{f}_{\operatorname{CART}}(-;% \mathcal{D}_{n}),f^{*}\right)\right\}=\Omega(1)roman_𝔼 start_POSTSUBSCRIPT caligraphic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_POSTSUBSCRIPT { italic_R ( over^ start_ARG italic_f end_ARG start_POSTSUBSCRIPT roman_CART end_POSTSUBSCRIPT ( - ; caligraphic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) , italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ) } = roman_Ω ( 1 ) whenever n=exp⁡(O⁢(d))𝑛𝑂𝑑n=\exp(O(d))italic_n = roman_exp ( italic_O ( italic_d ) ). • (Near sufficiency) If f∗superscript𝑓f^{*}italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT satisfies MSP and its Fourier coefficients {αSi}i=1rsuperscriptsubscriptsubscript𝛼subscript𝑆𝑖𝑖1𝑟\left\{\alpha_{S_{i}}\right\}_{i=1}^{r}{ italic_α start_POSTSUBSCRIPT italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT are generic, then 𝔼𝒟n⁢{R⁢(f^CART⁢(−;𝒟n),f∗)}=O⁢(2s/n)subscript𝔼subscript𝒟𝑛𝑅subscript^𝑓CARTsubscript𝒟𝑛superscript𝑓𝑂superscript2𝑠𝑛\mathbb{E}_{\mathcal{D}_{n}}\left\{R\left(\hat{f}_{\operatorname{CART}}(-;% \mathcal{D}_{n}),f^{*}\right)\right\}=O(2^{s}/n)roman_𝔼 start_POSTSUBSCRIPT caligraphic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_POSTSUBSCRIPT { italic_R ( over^ start_ARG italic_f end_ARG start_POSTSUBSCRIPT roman_CART end_POSTSUBSCRIPT ( - ; caligraphic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) , italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ) } = italic_O ( 2 start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT / italic_n ) whenever n=Ω⁢(2s⁢log⁡d)𝑛Ωsuperscript2𝑠𝑑n=\Omega(2^{s}\log d)italic_n = roman_Ω ( 2 start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT roman_log italic_d ). Furthermore, regardless of whether f∗superscript𝑓f^{*}italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT satisfies MSP, the expected L2superscript𝐿2L^{2}italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT risk of the ERM tree estimator satisfies 𝔼𝒟n⁢{R⁢(f^ERM⁢(−;𝒟n),f∗)}=O⁢(2s⁢log⁡d/n)subscript𝔼subscript𝒟𝑛𝑅subscript^𝑓ERMsubscript𝒟𝑛superscript𝑓𝑂superscript2𝑠𝑑𝑛\mathbb{E}_{\mathcal{D}_{n}}\left\{R\left(\hat{f}_{\operatorname{ERM}}(-;% \mathcal{D}_{n}),f^{*}\right)\right\}=O(2^{s}\log d/n)roman_𝔼 start_POSTSUBSCRIPT caligraphic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_POSTSUBSCRIPT { italic_R ( over^ start_ARG italic_f end_ARG start_POSTSUBSCRIPT roman_ERM end_POSTSUBSCRIPT ( - ; caligraphic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) , italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ) } = italic_O ( 2 start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT roman_log italic_d / italic_n ). Here, genericity of {αSi}i=1rsuperscriptsubscriptsubscript𝛼subscript𝑆𝑖𝑖1𝑟\left\{\alpha_{S_{i}}\right\}_{i=1}^{r}{ italic_α start_POSTSUBSCRIPT italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT is satisfied with probability 1 whenever they are drawn from a distribution with a density. Up to this asterisk, we see that high-dimensional consistency for CART can be completely characterized by MSP, a combinatorial condition on Fourier coefficients. In addition, we see in fact that CART behaves similarly to ERM trees when MSP holds, and similarly to non-adaptive trees when it does not, thereby also establishing performance gaps: In the former case, CART sharply improves upon non-adaptive trees, and in the latter case, it performs much worse compared to ERM. Finally, we remark that our lower bounds when f∗superscript𝑓f^{*}italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT does not satisfy MSP also hold more broadly for RFs, as well as for trees and ensembles grown with other greedy recursive partitioning strategies.444It is possible to extend the upper bound to various forms of RFs, but we leave this to future work. 1.4 Marginal Signal Bottleneck When a regression function f∗superscript𝑓f^{*}italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT does not satisfy MSP, its Fourier decomposition must contain a term αSi⁢χSisubscript𝛼subscript𝑆𝑖subscript𝜒subscript𝑆𝑖\alpha_{S_{i}}\chi_{S_{i}}italic_α start_POSTSUBSCRIPT italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_χ start_POSTSUBSCRIPT italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT that violates (5). This term creates problems for CART in a similar way as the XOR function. Let us call a covariate Xksubscript𝑋𝑘X_{k}italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT marginally undetectable in 𝒞𝒞\mathcal{C}caligraphic_C if (3) holds. As argued before, when Xksubscript𝑋𝑘X_{k}italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT is relevant but marginally undetectable, CART cannot distinguish it from irrelevant covariates. One can show that there exists covariate indices k1,k2∈Sisubscript𝑘1subscript𝑘2subscript𝑆𝑖k_{1},k_{2}\in S_{i}italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ∈ italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT such that Xk1subscript𝑋subscript𝑘1X_{k_{1}}italic_X start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT is marginally undetectable unless 𝒞𝒞\mathcal{C}caligraphic_C has already has split on xk2subscript𝑥subscript𝑘2x_{k_{2}}italic_x start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT and Xk2subscript𝑋subscript𝑘2X_{k_{2}}italic_X start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT is marginally undetectable unless 𝒞𝒞\mathcal{C}caligraphic_C has already has split on xk1subscript𝑥subscript𝑘1x_{k_{1}}italic_x start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT. We call this chicken and egg problem marginal signal bottleneck. It results in a highly probable event in which both these covariates remain marginally undetectable at every iteration of CART, leading to high-dimensional inconsistency. On the other hand, it is easy to show that MSP together with genericity implies the SID condition. In the setting of uniform binary features, this condition asserts a lower bound for what we call the marginal signal, max1≤k≤d⁡Corr2⁡{f∗⁢(𝐗),Xk|𝐗∈𝒞}≥λ,subscript1𝑘𝑑superscriptCorr2superscript𝑓𝐗conditionalsubscript𝑋𝑘𝐗𝒞𝜆\max_{1\leq k\leq d}\operatorname{Corr}^{2}\left\{f^{*}(\mathbf{X}),X_{k}~{}|~% {}\mathbf{X}\in\mathcal{C}\right\}\geq\lambda,roman_max start_POSTSUBSCRIPT 1 ≤ italic_k ≤ italic_d end_POSTSUBSCRIPT roman_Corr start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT { italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( bold_X ) , italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | bold_X ∈ caligraphic_C } ≥ italic_λ , (7) that holds uniformly over all subcubes 𝒞𝒞\mathcal{C}caligraphic_C on which Var⁡{f∗⁢(𝐗)|𝐗∈𝒞}>0Varconditionalsuperscript𝑓𝐗𝐗𝒞0\operatorname{Var}\left\{f^{*}(\mathbf{X})|\mathbf{X}\in\mathcal{C}\right\}>0roman_Var { italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( bold_X ) | bold_X ∈ caligraphic_C } > 0. When (7) holds for some 𝒞𝒞\mathcal{C}caligraphic_C, then with enough samples, CART is able to identify and select a relevant covariate for splitting 𝒞𝒞\mathcal{C}caligraphic_C. When (7) hold for all subcubes (i.e., SID), we are able to guarantee that CART is uniformly able to identify and select relevant covariates across every iteration loop of the algorithm, thereby yielding high-dimensional consistency. 1.5 Interpreting CART as a Stochastic Process Despite the simplicity of the lower bound argument outlined in the previous section, making it rigorous in finite samples, even in the special case of the XOR function, has proved elusive until now. To illustrate the difficulty, consider a fixed query point 𝐱∈{±1}d𝐱superscriptplus-or-minus1𝑑\mathbf{x}\in\{\pm 1\}^{d}bold_x ∈ { ± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT, and consider 𝒬𝒬\mathcal{Q}caligraphic_Q, the root-to-leaf path taken by 𝐱𝐱\mathbf{x}bold_x. We have to show that there is a highly probable event on which 𝒬𝒬\mathcal{Q}caligraphic_Q does not split on x1subscript𝑥1x_{1}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT or x2subscript𝑥2x_{2}italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. The usual approach to such a task is to attempt to prove concentration of the split criteria (2) over all nodes (subcubes) along the path. However, because of the data adaptivity of CART splits, we have little control over which subcubes appear, and we would be forced to derive concentration bounds that are uniform over all subcubes. Unfortunately, this desired result is too strong and does not hold. To resolve this, we interpret 𝒬𝒬\mathcal{Q}caligraphic_Q as a stochastic process. The measurement at “time point” t𝑡titalic_t comprises the values Corr2⁡{f∗⁢(𝐗),Xk|𝐗∈𝒞t}superscriptCorr2superscript𝑓𝐗conditionalsubscript𝑋𝑘𝐗subscript𝒞𝑡\operatorname{Corr}^{2}\left\{f^{*}(\mathbf{X}),X_{k}~{}|~{}\mathbf{X}\in% \mathcal{C}_{t}\right\}roman_Corr start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT { italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( bold_X ) , italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | bold_X ∈ caligraphic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT }, k=1⁢…,d𝑘1…𝑑k=1\ldots,ditalic_k = 1 … , italic_d, where 𝒞tsubscript𝒞𝑡\mathcal{C}_{t}caligraphic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is the node in 𝒬𝒬\mathcal{Q}caligraphic_Q at depth t𝑡titalic_t. We next couple this stochastic process to one that makes totally random splits, which allows us to use symmetry to calculate the probability that it does not split on x1subscript𝑥1x_{1}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT or x2subscript𝑥2x_{2}italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. On the event that the two processes are coupled, this also holds for the CART path 𝒬𝒬\mathcal{Q}caligraphic_Q. Although we have outlined a lower bound for the XOR function, it can be readily adapted to any function not satisfying MSP. Furthermore, there is nothing particularly special about the CART criterion. The proof relies on the fact that splits are determined by considering only the marginal distributions Xk|𝒞,Yconditionalsubscript𝑋𝑘𝒞𝑌X_{k}|~{}\mathcal{C},Yitalic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | caligraphic_C , italic_Y for k=1,…,d𝑘1…𝑑k=1,\ldots,ditalic_k = 1 , … , italic_d (as opposed to the joint distribution 𝐗|𝒞,Yconditional𝐗𝒞𝑌\mathbf{X}|~{}\mathcal{C},Ybold_X | caligraphic_C , italic_Y) and hence extends to a larger class of greedy recursive partitioning algorithms, which we formally define in Section 2. As far as we are aware, the proof technique is completely novel. 1.6 Robust Lower Bounds under “Soft” Bottlenecks Thus far, we have seen that when f∗superscript𝑓f^{*}italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT does not satisfy MSP, it experiences marginal signal bottleneck, which leads to high-dimensional inconsistency. In some sense, however, being non-MSP is not a robust property because a small perturbation of the zero coefficients of such a function results in one that does satisfy MSP. As such, our argument regarding the weaknesses of greedy recursive partitioning strategies would be stronger if we could show that their performance degrades smoothly as the regression function f∗superscript𝑓f^{*}italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT approaches the class of non-MSP functions. Fortunately, this is indeed the case, as reflected by the following informal theorem. Theorem 1.2 (Informal). When 𝒳={±1}d𝒳superscriptplus-or-minus1𝑑\mathcal{X}=\{\pm 1\}^{d}caligraphic_X = { ± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT, ν=Unif⁡({±1}d)𝜈Unifsuperscriptplus-or-minus1𝑑\nu=\operatorname{Unif}\left(\{\pm 1\}^{d}\right)italic_ν = roman_Unif ( { ± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ), f∗superscript𝑓f^{*}italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT depends only on s𝑠sitalic_s covariates, then the expected L2superscript𝐿2L^{2}italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT risk of the CART estimator f^CARTsubscript^𝑓CART\hat{f}_{\operatorname{CART}}over^ start_ARG italic_f end_ARG start_POSTSUBSCRIPT roman_CART end_POSTSUBSCRIPT satisfies E𝒟n⁢{R⁢(f^⁢(−;𝒟n),f∗)}=Ω⁢(1)subscript𝐸subscript𝒟𝑛𝑅^𝑓subscript𝒟𝑛superscript𝑓Ω1E_{\mathcal{D}_{n}}\left\{R\left(\hat{f}(-;\mathcal{D}_{n}),f^{*}\right)\right% \}=\Omega(1)italic_E start_POSTSUBSCRIPT caligraphic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_POSTSUBSCRIPT { italic_R ( over^ start_ARG italic_f end_ARG ( - ; caligraphic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) , italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ) } = roman_Ω ( 1 ) whenever n=min⁡{O⁢(1/minf∉MSP⁡R⁢(f,f∗)),exp⁡(O⁢(d))}𝑛𝑂1subscript𝑓MSP𝑅𝑓superscript𝑓𝑂𝑑n=\min\left\{O\left(1/\min_{f\notin\textsf{MSP}}R(f,f^{*})\right),\exp(O(d))\right\}italic_n = roman_min { italic_O ( 1 / roman_min start_POSTSUBSCRIPT italic_f ∉ MSP end_POSTSUBSCRIPT italic_R ( italic_f , italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ) ) , roman_exp ( italic_O ( italic_d ) ) }. The value, minf∉MSP⁡R⁢(f,f∗)subscript𝑓MSP𝑅𝑓superscript𝑓\min_{f\notin\textsf{MSP}}R(f,f^{*})roman_min start_POSTSUBSCRIPT italic_f ∉ MSP end_POSTSUBSCRIPT italic_R ( italic_f , italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ), can be thought of as measuring the width of a “soft” bottleneck. For example, if we set f∗⁢(𝐱)=α⁢x1+x2+x1⁢x2⁢x3superscript𝑓𝐱𝛼subscript𝑥1subscript𝑥2subscript𝑥1subscript𝑥2subscript𝑥3f^{*}(\mathbf{x})=\alpha x_{1}+x_{2}+x_{1}x_{2}x_{3}italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( bold_x ) = italic_α italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT, then we have minf∉MSP⁡R⁢(f,f∗)=α2subscript𝑓MSP𝑅𝑓superscript𝑓superscript𝛼2\min_{f\notin\textsf{MSP}}R(f,f^{*})=\alpha^{2}roman_min start_POSTSUBSCRIPT italic_f ∉ MSP end_POSTSUBSCRIPT italic_R ( italic_f , italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ) = italic_α start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. We thus see that the sample complexity lower bound scales inversely with the bottleneck width, thereby extending the performance gap between greedy and ERM trees to a larger range of settings. Finally, we remark that MSP can be re-interpreted in graph theoretic terms as form of connectedness. Under this interpretation, minf∉MSP⁡R⁢(f,f∗)subscript𝑓MSP𝑅𝑓superscript𝑓\min_{f\notin\textsf{MSP}}R(f,f^{*})roman_min start_POSTSUBSCRIPT italic_f ∉ MSP end_POSTSUBSCRIPT italic_R ( italic_f , italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ) is equal to the weight of the minimum vertex cut. We elaborate upon this perspective in Section 5. 1.7 Comparisons with Neural Networks Trained by SGD In computational learning theory, the problem of “learning” (i.e., achieving small estimation error for) sparse Boolean monomials, also called parities, from i.i.d. noiseless observations (i.e., εi=0subscript𝜀𝑖0\varepsilon_{i}=0italic_ε start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 0 in (1)) is known to be statistically easy but computationally hard (Barak et al., 2022), and hence has served as a useful benchmark for learning algorithms and computational frameworks. Recently, this has become a benchmark for studying neural networks (NNs) trained using SGD, which combined with our results, allows us to perform, to our knowledge, the first theoretical head-to-head comparison between this class of algorithms and greedy regression trees and ensembles. Indeed, the MSP was introduced by Abbe et al. (2022) to generalize Boolean monomials. They showed that in the mean-field regime (very wide two-layer NNs with very small step size, see also Mei et al. (2018)), C⁢d𝐶𝑑Cditalic_C italic_d iterations of online SGD (one sample per iteration) are sufficient (for some C𝐶Citalic_C) to achieve small estimation error whenever f∗superscript𝑓f^{*}italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT satisfies MSP and is generic but is insufficient (for any C𝐶Citalic_C) when f∗superscript𝑓f^{*}italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT does not satisfy MSP. Intuitively, functions that do not satisfy MSP produce optimization landscapes with saddle points, which mean-field online SGD struggles to escape from. Comparing with Theorem 1.1, this creates an interesting analogy between mean-field online SGD and greedy regression trees and ensembles. On the other hand, later works inadvertently emphasized the differences between greedy regression trees and ensembles and NNs trained outside the mean-field regime and beyond the O⁢(d)𝑂𝑑O(d)italic_O ( italic_d ) iteration horizon. In the classification setting, Glasgow (2024) showed that online minibatch SGD on a two-layer NN can learn the XOR function with Θ~⁢(d)~Θ𝑑\tilde{\Theta}(d)over~ start_ARG roman_Θ end_ARG ( italic_d ) samples and iterations. Kou et al. (2024) showed that for any k𝑘kitalic_k, online sign-SGD with a batch size of O~⁢(dk−1)~𝑂superscript𝑑𝑘1\tilde{O}(d^{k-1})over~ start_ARG italic_O end_ARG ( italic_d start_POSTSUPERSCRIPT italic_k - 1 end_POSTSUPERSCRIPT ) can learn a k𝑘kitalic_k-parity (i.e., degree k𝑘kitalic_k monomial) within O⁢(log⁡d)𝑂𝑑O(\log d)italic_O ( roman_log italic_d ) iterations, for a total sample complexity of O~⁢(dk−1)~𝑂superscript𝑑𝑘1\tilde{O}(d^{k-1})over~ start_ARG italic_O end_ARG ( italic_d start_POSTSUPERSCRIPT italic_k - 1 end_POSTSUPERSCRIPT ). Since our lower bound in Theorem 1.1 holds even with noiseless observations, this establishes a rigorous performance gap between the two algorithm classes for some non-MSP functions. Of further interest is Abbe et al. (2023)’s conjecture characterizing the sample complexity required to learn any given Boolean function in terms of its “leap complexity” (see Appendix J). If this conjecture is true, NNs trained with SGD outperform greedy regression trees and ensembles on all non-MSP functions but perform more poorly in comparison on MSP functions. We discuss these comparisons more elaborately in Appendix J."
https://arxiv.org/html/2411.03393v1,A refined graph container lemma and applications to the hard-core model on bipartite expanders,"We establish a refined version of a graph container lemma due to Galvin and discuss several applications related to the hard-core model on bipartite expander graphs. Given a graph G𝐺Gitalic_G and λ>0𝜆0\lambda>0italic_λ > 0, the hard-core model on G𝐺Gitalic_G at activity λ𝜆\lambdaitalic_λ is the probability distribution μG,λsubscript𝜇𝐺𝜆\mu_{G,\lambda}italic_μ start_POSTSUBSCRIPT italic_G , italic_λ end_POSTSUBSCRIPT on independent sets in G𝐺Gitalic_G given by μG,λ⁢(I)∝λ|I|proportional-tosubscript𝜇𝐺𝜆𝐼superscript𝜆𝐼\mu_{G,\lambda}(I)\propto\lambda^{|I|}italic_μ start_POSTSUBSCRIPT italic_G , italic_λ end_POSTSUBSCRIPT ( italic_I ) ∝ italic_λ start_POSTSUPERSCRIPT | italic_I | end_POSTSUPERSCRIPT. As one of our main applications, we show that the hard-core model at activity λ𝜆\lambdaitalic_λ on the hypercube Qdsubscript𝑄𝑑Q_{d}italic_Q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT exhibits a ‘structured phase’ for λ=Ω⁢(log2⁡d/d1/2)𝜆Ωsuperscript2𝑑superscript𝑑12\lambda=\Omega(\log^{2}d/d^{1/2})italic_λ = roman_Ω ( roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d / italic_d start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT ) in the following sense: in a typical sample from μQd,λsubscript𝜇subscript𝑄𝑑𝜆\mu_{Q_{d},\lambda}italic_μ start_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_λ end_POSTSUBSCRIPT, most vertices are contained in one side of the bipartition of Qdsubscript𝑄𝑑Q_{d}italic_Q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT. This improves upon a result of Galvin which establishes the same for λ=Ω⁢(log⁡d/d1/3)𝜆Ω𝑑superscript𝑑13\lambda=\Omega(\log d/d^{1/3})italic_λ = roman_Ω ( roman_log italic_d / italic_d start_POSTSUPERSCRIPT 1 / 3 end_POSTSUPERSCRIPT ). As another application, we establish a fully polynomial-time approximation scheme (FPTAS) for the hard-core model on a d𝑑ditalic_d-regular bipartite α𝛼\alphaitalic_α-expander, with α>0𝛼0\alpha>0italic_α > 0 fixed, when λ=Ω⁢(log2⁡d/d1/2)𝜆Ωsuperscript2𝑑superscript𝑑12\lambda=\Omega(\log^{2}d/d^{1/2})italic_λ = roman_Ω ( roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d / italic_d start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT ). This improves upon the bound λ=Ω⁢(log⁡d/d1/4)𝜆Ω𝑑superscript𝑑14\lambda=\Omega(\log d/d^{1/4})italic_λ = roman_Ω ( roman_log italic_d / italic_d start_POSTSUPERSCRIPT 1 / 4 end_POSTSUPERSCRIPT ) due to the first author, Perkins and Potukuchi. We discuss similar improvements to results of Galvin-Tetali, Balogh-Garcia-Li and Kronenberg-Spinka.","Given a graph G𝐺Gitalic_G, let ℐ⁢(G)ℐ𝐺\mathcal{I}(G)caligraphic_I ( italic_G ) denote the collection of independent sets in G𝐺Gitalic_G. The hard-core model on G𝐺Gitalic_G at activity λ>0𝜆0\lambda>0italic_λ > 0 is the probability distribution on ℐ⁢(G)ℐ𝐺\mathcal{I}(G)caligraphic_I ( italic_G ) given by (1) μG,λ⁢(I)=λ|I|ZG⁢(λ)subscript𝜇𝐺𝜆𝐼superscript𝜆𝐼subscript𝑍𝐺𝜆\displaystyle\mu_{G,\lambda}(I)=\frac{\lambda^{|I|}}{Z_{G}(\lambda)}italic_μ start_POSTSUBSCRIPT italic_G , italic_λ end_POSTSUBSCRIPT ( italic_I ) = divide start_ARG italic_λ start_POSTSUPERSCRIPT | italic_I | end_POSTSUPERSCRIPT end_ARG start_ARG italic_Z start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_λ ) end_ARG for I∈ℐ⁢(G)𝐼ℐ𝐺I\in\mathcal{I}(G)italic_I ∈ caligraphic_I ( italic_G ), where the normalising constant (2) ZG⁢(λ)=∑I∈ℐ⁢(G)λ|I|subscript𝑍𝐺𝜆subscript𝐼ℐ𝐺superscript𝜆𝐼\displaystyle Z_{G}(\lambda)=\sum_{I\in\mathcal{I}(G)}\lambda^{|I|}italic_Z start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_λ ) = ∑ start_POSTSUBSCRIPT italic_I ∈ caligraphic_I ( italic_G ) end_POSTSUBSCRIPT italic_λ start_POSTSUPERSCRIPT | italic_I | end_POSTSUPERSCRIPT is known as the hard-core model partition function. The hard-core model originated in statistical physics as a simple model of a gas. The vertices of the graph G𝐺Gitalic_G are to be thought of as ‘sites’ that can be occupied by particles, and neighbouring sites cannot both be occupied. This constraint models a system of particles with ‘hard cores’ that cannot overlap. In statistical physics, a major motivation for studying the hard-core model is that it provides a setting where the notion of phase transition can be rigorously investigated. In this context, the most common host graph of study is (the nearest neighbour graph on) the integer lattice ℤdsuperscriptℤ𝑑\mathbb{Z}^{d}blackboard_Z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT (see Section 7 for a more precise discussion of phase transitions and hard-core measures on ℤdsuperscriptℤ𝑑\mathbb{Z}^{d}blackboard_Z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT). For now a phase transition can be loosely thought of as follows: as λ𝜆\lambdaitalic_λ increases, a typical sample from the hard-core model on (a large box in) ℤdsuperscriptℤ𝑑\mathbb{Z}^{d}blackboard_Z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT transitions from being disordered to being structured, in the sense that it prefers vertices from either the odd or even sublattice. Motivated by this phenomenon, Kahn [25] initiated the study of the hard-core model on the hypercube Qdsubscript𝑄𝑑Q_{d}italic_Q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT (and regular bipartite graphs in general). Here Qdsubscript𝑄𝑑Q_{d}italic_Q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT denotes the graph on vertex set {0,1}dsuperscript01𝑑\{0,1\}^{d}{ 0 , 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT where two vertices are adjacent if and only if they have Hamming distance 1111. Qdsubscript𝑄𝑑Q_{d}italic_Q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT is a bipartite graph with bipartition ℰ∪𝒪ℰ𝒪\mathcal{E}\cup\mathcal{O}caligraphic_E ∪ caligraphic_O, where ℰℰ\mathcal{E}caligraphic_E and 𝒪𝒪\mathcal{O}caligraphic_O consist of the vertices with even and odd Hamming weight, respectively. Kahn showed that for fixed λ,ϵ>0𝜆italic-ϵ0\lambda,\epsilon>0italic_λ , italic_ϵ > 0 and 𝐈𝐈\mathbf{I}bold_I sampled according to μQd,λsubscript𝜇subscript𝑄𝑑𝜆\mu_{Q_{d},\lambda}italic_μ start_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_λ end_POSTSUBSCRIPT both ||𝐈|−λ1+λ⁢2d−1|≤2dd1−ϵ𝐈𝜆1𝜆superscript2𝑑1superscript2𝑑superscript𝑑1italic-ϵ\left||\mathbf{I}|-\frac{\lambda}{1+\lambda}2^{d-1}\right|\leq\frac{2^{d}}{d^{% 1-\epsilon}}| | bold_I | - divide start_ARG italic_λ end_ARG start_ARG 1 + italic_λ end_ARG 2 start_POSTSUPERSCRIPT italic_d - 1 end_POSTSUPERSCRIPT | ≤ divide start_ARG 2 start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT end_ARG start_ARG italic_d start_POSTSUPERSCRIPT 1 - italic_ϵ end_POSTSUPERSCRIPT end_ARG and min⁡{|𝐈∩ℰ|,|𝐈∩𝒪|}≤2dd1/2−ϵ𝐈ℰ𝐈𝒪superscript2𝑑superscript𝑑12italic-ϵ\min\{|\mathbf{I}\cap\mathcal{E}|,|\mathbf{I}\cap\mathcal{O}|\}\leq\frac{2^{d}% }{d^{1/2-\epsilon}}roman_min { | bold_I ∩ caligraphic_E | , | bold_I ∩ caligraphic_O | } ≤ divide start_ARG 2 start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT end_ARG start_ARG italic_d start_POSTSUPERSCRIPT 1 / 2 - italic_ϵ end_POSTSUPERSCRIPT end_ARG hold whp (that is, with probability tending to 1111 as d→∞→𝑑d\to\inftyitalic_d → ∞). Roughly speaking, these results show that for λ>0𝜆0\lambda>0italic_λ > 0 fixed, a sample from μQd,λsubscript𝜇subscript𝑄𝑑𝜆\mu_{Q_{d},\lambda}italic_μ start_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_λ end_POSTSUBSCRIPT resembles a random subset of either ℰℰ\mathcal{E}caligraphic_E or 𝒪𝒪\mathcal{O}caligraphic_O where each element is chosen independently with probability λ/(1+λ)𝜆1𝜆\lambda/(1+\lambda)italic_λ / ( 1 + italic_λ ). In other words, samples from μQd,λsubscript𝜇subscript𝑄𝑑𝜆\mu_{Q_{d},\lambda}italic_μ start_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_λ end_POSTSUBSCRIPT exhibit a significant degree of structure. This lies in stark contrast to the regime λ≤c/d𝜆𝑐𝑑\lambda\leq c/ditalic_λ ≤ italic_c / italic_d (c𝑐citalic_c small) where a sample 𝐈𝐈\mathbf{I}bold_I from μQd,λsubscript𝜇subscript𝑄𝑑𝜆\mu_{Q_{d},\lambda}italic_μ start_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_λ end_POSTSUBSCRIPT resembles a λ/(1+λ)𝜆1𝜆\lambda/(1+\lambda)italic_λ / ( 1 + italic_λ )-random subset of ℰ∪𝒪ℰ𝒪\mathcal{E}\cup\mathcal{O}caligraphic_E ∪ caligraphic_O; in particular, |𝐈∩ℰ|=(1+o⁢(1))⁢|𝐈∩𝒪|𝐈ℰ1𝑜1𝐈𝒪|\mathbf{I}\cap\mathcal{E}|=(1+o(1))|\mathbf{I}\cap\mathcal{O}|| bold_I ∩ caligraphic_E | = ( 1 + italic_o ( 1 ) ) | bold_I ∩ caligraphic_O | whp – see [38]. Galvin [14] later refined Kahn’s results, showing that the structured regime holds all the way down to λ=Ω~⁢(d−1/3)𝜆~Ωsuperscript𝑑13\lambda=\tilde{\Omega}(d^{-1/3})italic_λ = over~ start_ARG roman_Ω end_ARG ( italic_d start_POSTSUPERSCRIPT - 1 / 3 end_POSTSUPERSCRIPT ). More precisely, he showed that there exists C>0𝐶0C>0italic_C > 0 so that if C⁢log⁡d/d1/3≤λ≤2−1𝐶𝑑superscript𝑑13𝜆21C\log d/d^{1/3}\leq\lambda\leq\sqrt{2}-1italic_C roman_log italic_d / italic_d start_POSTSUPERSCRIPT 1 / 3 end_POSTSUPERSCRIPT ≤ italic_λ ≤ square-root start_ARG 2 end_ARG - 1 and 𝐈𝐈\mathbf{I}bold_I is a sample from μQd,λsubscript𝜇subscript𝑄𝑑𝜆\mu_{Q_{d},\lambda}italic_μ start_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_λ end_POSTSUBSCRIPT, then whp ||𝐈|−λ1+λ⁢2d−1|≤d⁢log⁡d⁢(21+λ)d𝐈𝜆1𝜆superscript2𝑑1𝑑𝑑superscript21𝜆𝑑\left||\mathbf{I}|-\frac{\lambda}{1+\lambda}2^{d-1}\right|\leq d\log d\left(% \frac{2}{1+\lambda}\right)^{d}| | bold_I | - divide start_ARG italic_λ end_ARG start_ARG 1 + italic_λ end_ARG 2 start_POSTSUPERSCRIPT italic_d - 1 end_POSTSUPERSCRIPT | ≤ italic_d roman_log italic_d ( divide start_ARG 2 end_ARG start_ARG 1 + italic_λ end_ARG ) start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT and 14⁢log⁡m⁢λ2⁢(21+λ)d≤min⁡{|𝐈∩ℰ|,|𝐈∩𝒪|}≤e⁢m2⁢λ2⁢(21+λ)d14𝑚𝜆2superscript21𝜆𝑑𝐈ℰ𝐈𝒪𝑒superscript𝑚2𝜆2superscript21𝜆𝑑\frac{1}{4\log m}\frac{\lambda}{2}\left(\frac{2}{1+\lambda}\right)^{d}\leq\min% \{|\mathbf{I}\cap\mathcal{E}|,|\mathbf{I}\cap\mathcal{O}|\}\leq em^{2}\frac{% \lambda}{2}\left(\frac{2}{1+\lambda}\right)^{d}\,divide start_ARG 1 end_ARG start_ARG 4 roman_log italic_m end_ARG divide start_ARG italic_λ end_ARG start_ARG 2 end_ARG ( divide start_ARG 2 end_ARG start_ARG 1 + italic_λ end_ARG ) start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ≤ roman_min { | bold_I ∩ caligraphic_E | , | bold_I ∩ caligraphic_O | } ≤ italic_e italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT divide start_ARG italic_λ end_ARG start_ARG 2 end_ARG ( divide start_ARG 2 end_ARG start_ARG 1 + italic_λ end_ARG ) start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT for some m=m⁢(λ,d)=o⁢(d/log⁡d)𝑚𝑚𝜆𝑑𝑜𝑑𝑑m=m(\lambda,d)=o(d/\sqrt{\log d})italic_m = italic_m ( italic_λ , italic_d ) = italic_o ( italic_d / square-root start_ARG roman_log italic_d end_ARG ). Similar bounds hold when λ>2−1𝜆21\lambda>\sqrt{2}-1italic_λ > square-root start_ARG 2 end_ARG - 1, but they take a slightly different form (see [14, Theorem 1.1]). Galvin’s proof is based on Sapozhenko’s graph container method [34] (see Section 1.1 for more on the container method). This influential method has now enjoyed numerous applications in the combinatorics literature. The restriction that λ=Ω~⁢(d−1/3)𝜆~Ωsuperscript𝑑13\lambda=\tilde{\Omega}(d^{-1/3})italic_λ = over~ start_ARG roman_Ω end_ARG ( italic_d start_POSTSUPERSCRIPT - 1 / 3 end_POSTSUPERSCRIPT ) in Galvin’s result is an artifact of a graph container lemma and as a consequence, the same restriction appears in many other applications of similar lemmas [1, 17, 23, 24, 30]. Galvin’s results [14] were extended by the first author and Perkins [22], who combined the graph container method with a method based on the theory of polymer models and cluster expansion from statistical physics. This allows for a very precise description of the hard-core measure on Qdsubscript𝑄𝑑Q_{d}italic_Q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT as a ‘perturbation’ of the measure which selects a side ℰ,𝒪ℰ𝒪\mathcal{E},\mathcal{O}caligraphic_E , caligraphic_O uniformly at random and then selects a λ/(1+λ)𝜆1𝜆\lambda/(1+\lambda)italic_λ / ( 1 + italic_λ )-subset of that side. As a consequence one can obtain detailed asymptotics for the partition function ZQd⁢(λ)subscript𝑍subscript𝑄𝑑𝜆Z_{Q_{d}}(\lambda)italic_Z start_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_λ ) and determine the asymptotic distribution of |𝐈|𝐈|\mathbf{I}|| bold_I | and min⁡{|𝐈∩ℰ|,|𝐈∩𝒪|}𝐈ℰ𝐈𝒪\min\{|\mathbf{I}\cap\mathcal{E}|,|\mathbf{I}\cap\mathcal{O}|\}roman_min { | bold_I ∩ caligraphic_E | , | bold_I ∩ caligraphic_O | }. These results rely on Galvin’s container lemma and are therefore also limited to the regime λ=Ω~⁢(d−1/3)𝜆~Ωsuperscript𝑑13\lambda=\tilde{\Omega}(d^{-1/3})italic_λ = over~ start_ARG roman_Ω end_ARG ( italic_d start_POSTSUPERSCRIPT - 1 / 3 end_POSTSUPERSCRIPT ). Here we prove a refined graph container lemma (Lemma 1.2 below), which allows us to extend these structure theorems to the range λ=Ω~⁢(d−1/2)𝜆~Ωsuperscript𝑑12\lambda=\tilde{\Omega}(d^{-1/2})italic_λ = over~ start_ARG roman_Ω end_ARG ( italic_d start_POSTSUPERSCRIPT - 1 / 2 end_POSTSUPERSCRIPT ). One can make similar improvements to several other applications of the graph container method and we discuss these further applications in Section 1.2. Theorem 1.1. There exists C>0𝐶0C>0italic_C > 0 so that the following holds. Let C⁢log2⁡d/d1/2≤λ≤2−1𝐶superscript2𝑑superscript𝑑12𝜆21C\log^{2}d/d^{1/2}\leq\lambda\leq\sqrt{2}-1italic_C roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d / italic_d start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT ≤ italic_λ ≤ square-root start_ARG 2 end_ARG - 1 and let 𝐈𝐈\mathbf{I}bold_I be sampled according to μQd,λsubscript𝜇subscript𝑄𝑑𝜆\mu_{Q_{d},\lambda}italic_μ start_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_λ end_POSTSUBSCRIPT. Then, with high probability, (3) ||𝐈|−λ1+λ⁢2d−1|≤ω⁢(1)⋅d⁢λ2⁢2d(1+λ)d𝐈𝜆1𝜆superscript2𝑑1⋅𝜔1𝑑superscript𝜆2superscript2𝑑superscript1𝜆𝑑\left||\mathbf{I}|-\frac{\lambda}{1+\lambda}2^{d-1}\right|\leq\omega(1)\cdot% \frac{d\lambda^{2}2^{d}}{(1+\lambda)^{d}}| | bold_I | - divide start_ARG italic_λ end_ARG start_ARG 1 + italic_λ end_ARG 2 start_POSTSUPERSCRIPT italic_d - 1 end_POSTSUPERSCRIPT | ≤ italic_ω ( 1 ) ⋅ divide start_ARG italic_d italic_λ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT 2 start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT end_ARG start_ARG ( 1 + italic_λ ) start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT end_ARG and (4) min⁡{|𝐈∩ℰ|,|𝐈∩𝒪|}=(1+o⁢(1))⁢λ2⋅(21+λ)d𝐈ℰ𝐈𝒪⋅1𝑜1𝜆2superscript21𝜆𝑑\min\{|\mathbf{I}\cap\mathcal{E}|,|\mathbf{I}\cap\mathcal{O}|\}=(1+o(1))\frac{% \lambda}{2}\cdot\left(\frac{2}{1+\lambda}\right)^{d}roman_min { | bold_I ∩ caligraphic_E | , | bold_I ∩ caligraphic_O | } = ( 1 + italic_o ( 1 ) ) divide start_ARG italic_λ end_ARG start_ARG 2 end_ARG ⋅ ( divide start_ARG 2 end_ARG start_ARG 1 + italic_λ end_ARG ) start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT where ω⁢(1)𝜔1\omega(1)italic_ω ( 1 ) is any function tending to infinity as d→∞→𝑑d\rightarrow\inftyitalic_d → ∞. We reiterate that the regime λ≥C⁢log⁡d/d1/3𝜆𝐶𝑑superscript𝑑13\lambda\geq C\log d/d^{1/3}italic_λ ≥ italic_C roman_log italic_d / italic_d start_POSTSUPERSCRIPT 1 / 3 end_POSTSUPERSCRIPT was already treated in detail in [14, 22] and the regime λ>2−1𝜆21\lambda>\sqrt{2}-1italic_λ > square-root start_ARG 2 end_ARG - 1 was treated in [14]. Theorem 1.1 shows that the ‘structured regime’, where a typical independent set is imbalanced, persists all the way down to λ=Ω~⁢(d−1/2)𝜆~Ωsuperscript𝑑12\lambda=\tilde{\Omega}(d^{-1/2})italic_λ = over~ start_ARG roman_Ω end_ARG ( italic_d start_POSTSUPERSCRIPT - 1 / 2 end_POSTSUPERSCRIPT ). Theorem 1.1 is an easy consequence of a detailed description of μQd,λsubscript𝜇subscript𝑄𝑑𝜆\mu_{Q_{d},\lambda}italic_μ start_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_λ end_POSTSUBSCRIPT (see Theorem 5.3 below) analogous to the one obtained in [22]. This structure theorem has several other consequences (many of which are elaborated upon in [22]), but here we focus on Theorem 1.1 as our main application for brevity. We describe our new graph container lemma (Lemma 1.2) in the following section, which is our main technical contribution. We then go on to discuss further applications of the lemma. 1.1. An improved graph container lemma The container method is a classical tool that has seen widespread use in the context of studying independent sets in graphs. Its roots can be traced back to the work of Kleitman and Winston [26] and Sapozhenko [34]. In recent years, the method has been generalized and developed into a powerful approach for studying independent sets in hypergraphs in the celebrated work of Balogh, Morris and Samotij [3] and Saxton and Thomason [35]. This method has enjoyed a wealth of applications in extremal, enumerative and probabilistic combinatorics and beyond. In this paper, we are interested in the graph container method specialized to the case of bipartite graphs Σ=X∪YΣ𝑋𝑌\Sigma=X\cup Yroman_Σ = italic_X ∪ italic_Y. A first result of this type was established by Sapozhenko [34]. Sapozhenko’s method was elaborated upon by Galvin [15] and the form of our container lemma is closely modelled after his results (see also [14, 17]). These results have seen numerous applications which we discuss further in Section 1.2. We formulate our new container lemma in enough generality to encompass all of the applications outlined in Section 1.2 (not just Theorem 1.1) and with a view to future applications. The statement is somewhat technical and so we first set up some notation. For δ≥1𝛿1\delta\geq 1italic_δ ≥ 1 and dY≤dXsubscript𝑑𝑌subscript𝑑𝑋d_{Y}\leq d_{X}italic_d start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT ≤ italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT, we say a bipartite graph Σ=X⊔YΣsquare-union𝑋𝑌\Sigma=X\sqcup Yroman_Σ = italic_X ⊔ italic_Y is δ𝛿\deltaitalic_δ-approximately (dX,dY)subscript𝑑𝑋subscript𝑑𝑌(d_{X},d_{Y})( italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT )-biregular if it satisfies d⁢(v)∈{[dX,δ⁢dX]∀v∈X,[δ−1⁢dY,dY]∀v∈Y𝑑𝑣casessubscript𝑑𝑋𝛿subscript𝑑𝑋for-all𝑣𝑋superscript𝛿1subscript𝑑𝑌subscript𝑑𝑌for-all𝑣𝑌d(v)\in\begin{cases}[d_{X},\delta d_{X}]&\forall v\in X,\\ [\delta^{-1}d_{Y},d_{Y}]&\forall v\in Y\end{cases}italic_d ( italic_v ) ∈ { start_ROW start_CELL [ italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT , italic_δ italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT ] end_CELL start_CELL ∀ italic_v ∈ italic_X , end_CELL end_ROW start_ROW start_CELL [ italic_δ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT ] end_CELL start_CELL ∀ italic_v ∈ italic_Y end_CELL end_ROW where d⁢(v)𝑑𝑣d(v)italic_d ( italic_v ) denotes the degree of v𝑣vitalic_v. For the rest of the paper, we assume ΣΣ\Sigmaroman_Σ is δ𝛿\deltaitalic_δ-approximately (dX,dY)subscript𝑑𝑋subscript𝑑𝑌(d_{X},d_{Y})( italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT )-biregular. For A⊆X𝐴𝑋A\subseteq Xitalic_A ⊆ italic_X, the closure of A𝐴Aitalic_A is defined to be [A]:={x∈X:N⁢(x)⊆N⁢(A)}.assigndelimited-[]𝐴conditional-set𝑥𝑋𝑁𝑥𝑁𝐴[A]:=\{x\in X:N(x)\subseteq N(A)\}.[ italic_A ] := { italic_x ∈ italic_X : italic_N ( italic_x ) ⊆ italic_N ( italic_A ) } . A set S⊆V⁢(Σ)𝑆𝑉ΣS\subseteq V(\Sigma)italic_S ⊆ italic_V ( roman_Σ ) is 2-linked if Σ2⁢[S]superscriptΣ2delimited-[]𝑆\Sigma^{2}[S]roman_Σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT [ italic_S ] is connected, where Σ2superscriptΣ2\Sigma^{2}roman_Σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT is the square of ΣΣ\Sigmaroman_Σ (i.e. V⁢(Σ2)=V⁢(Σ)𝑉superscriptΣ2𝑉ΣV(\Sigma^{2})=V(\Sigma)italic_V ( roman_Σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) = italic_V ( roman_Σ ) and two vertices x,y𝑥𝑦x,yitalic_x , italic_y are adjacent in Σ2superscriptΣ2\Sigma^{2}roman_Σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT iff their distance in ΣΣ\Sigmaroman_Σ is at most 2). Given a,g∈ℕ𝑎𝑔ℕa,g\in\mathbb{N}italic_a , italic_g ∈ blackboard_N, define (5) 𝒢⁢(a,g)=𝒢⁢(a,g,Σ)={A⊆X⁢ 2-linked :|[A]|=a⁢ and ⁢|N⁢(A)|=g},𝒢𝑎𝑔𝒢𝑎𝑔Σconditional-set𝐴𝑋 2-linked delimited-[]𝐴𝑎 and 𝑁𝐴𝑔\displaystyle\mathcal{G}(a,g)=\mathcal{G}(a,g,\Sigma)=\{A\subseteq X\text{ 2-% linked }:|[A]|=a\text{ and }|N(A)|=g\},caligraphic_G ( italic_a , italic_g ) = caligraphic_G ( italic_a , italic_g , roman_Σ ) = { italic_A ⊆ italic_X 2-linked : | [ italic_A ] | = italic_a and | italic_N ( italic_A ) | = italic_g } , and set t≔g−a≔𝑡𝑔𝑎t\coloneqq g-aitalic_t ≔ italic_g - italic_a and w≔g⁢dY−a⁢dX≔𝑤𝑔subscript𝑑𝑌𝑎subscript𝑑𝑋w\coloneqq gd_{Y}-ad_{X}italic_w ≔ italic_g italic_d start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT - italic_a italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT. The technical-looking definition below is a measurement of the expansion of ΣΣ\Sigmaroman_Σ. Given 1≤φ≤δ−1⁢dY−11𝜑superscript𝛿1subscript𝑑𝑌11\leq\varphi\leq\delta^{-1}d_{Y}-11 ≤ italic_φ ≤ italic_δ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT - 1, let mφ=mφ⁢(Σ)=min⁡{|N⁢(K)|:y∈Y,K⊆N⁢(y),|K|>φ}.subscript𝑚𝜑subscript𝑚𝜑Σ:𝑁𝐾formulae-sequence𝑦𝑌formulae-sequence𝐾𝑁𝑦𝐾𝜑m_{\varphi}=m_{\varphi}(\Sigma)=\min\{|N(K)|:y\in Y,K\subseteq N(y),|K|>% \varphi\}.italic_m start_POSTSUBSCRIPT italic_φ end_POSTSUBSCRIPT = italic_m start_POSTSUBSCRIPT italic_φ end_POSTSUBSCRIPT ( roman_Σ ) = roman_min { | italic_N ( italic_K ) | : italic_y ∈ italic_Y , italic_K ⊆ italic_N ( italic_y ) , | italic_K | > italic_φ } . The proof of Lemma 1.2 crucially relies on the expansion of ΣΣ\Sigmaroman_Σ, and the lower bounds on mφsubscript𝑚𝜑m_{\varphi}italic_m start_POSTSUBSCRIPT italic_φ end_POSTSUBSCRIPT and g−a𝑔𝑎g-aitalic_g - italic_a below provide quantification of the expansion that we need. All the graphs that we consider in our applications will satisfy these lower bounds (except in Theorem 1.5, where we require a stronger bound on g−a𝑔𝑎g-aitalic_g - italic_a and we relax the assumption on mφsubscript𝑚𝜑m_{\varphi}italic_m start_POSTSUBSCRIPT italic_φ end_POSTSUBSCRIPT). Lemma 1.2. Let dXsubscript𝑑𝑋d_{X}italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT and dYsubscript𝑑𝑌d_{Y}italic_d start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT be sufficiently large integers and let δ≥1𝛿1\delta\geq 1italic_δ ≥ 1, δ′,δ′′>0superscript𝛿′superscript𝛿′′0\delta^{\prime},\delta^{\prime\prime}>0italic_δ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , italic_δ start_POSTSUPERSCRIPT ′ ′ end_POSTSUPERSCRIPT > 0. Then there exist c=c⁢(δ,δ′,δ′′)>0𝑐𝑐𝛿superscript𝛿′superscript𝛿′′0c=c(\delta,\delta^{\prime},\delta^{\prime\prime})>0italic_c = italic_c ( italic_δ , italic_δ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , italic_δ start_POSTSUPERSCRIPT ′ ′ end_POSTSUPERSCRIPT ) > 0 and C=C⁢(δ,δ′,δ′′)>0𝐶𝐶𝛿superscript𝛿′superscript𝛿′′0C=C(\delta,\delta^{\prime},\delta^{\prime\prime})>0italic_C = italic_C ( italic_δ , italic_δ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , italic_δ start_POSTSUPERSCRIPT ′ ′ end_POSTSUPERSCRIPT ) > 0 such that the following holds. Let Σ=X⊔YΣsquare-union𝑋𝑌\Sigma=X\sqcup Yroman_Σ = italic_X ⊔ italic_Y be δ𝛿\deltaitalic_δ-approximately (dX,dY)subscript𝑑𝑋subscript𝑑𝑌(d_{X},d_{Y})( italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT )-biregular such that mφ≥δ′′⋅(φ⁢dX)subscript𝑚𝜑⋅superscript𝛿′′𝜑subscript𝑑𝑋{m_{\varphi}}\geq\delta^{\prime\prime}\cdot(\varphi d_{X})italic_m start_POSTSUBSCRIPT italic_φ end_POSTSUBSCRIPT ≥ italic_δ start_POSTSUPERSCRIPT ′ ′ end_POSTSUPERSCRIPT ⋅ ( italic_φ italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT ), where φ=dY/(2⁢δ)𝜑subscript𝑑𝑌2𝛿\varphi=d_{Y}/(2\delta)italic_φ = italic_d start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT / ( 2 italic_δ ). If a,g∈ℕ𝑎𝑔ℕa,g\in\mathbb{N}italic_a , italic_g ∈ blackboard_N satisfy g−a≥max⁡{δ′⁢gdY,c⁢dX(log⁡dX)2}𝑔𝑎superscript𝛿′𝑔subscript𝑑𝑌𝑐subscript𝑑𝑋superscriptsubscript𝑑𝑋2g-a\geq\max\left\{\frac{\delta^{\prime}g}{d_{Y}},\frac{cd_{X}}{(\log d_{X})^{2% }}\right\}italic_g - italic_a ≥ roman_max { divide start_ARG italic_δ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT italic_g end_ARG start_ARG italic_d start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT end_ARG , divide start_ARG italic_c italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT end_ARG start_ARG ( roman_log italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG } and λ>C⁢log2⁡dX(dX)1/2𝜆𝐶superscript2subscript𝑑𝑋superscriptsubscript𝑑𝑋12\lambda>\frac{C\log^{2}d_{X}}{(d_{X})^{1/2}}italic_λ > divide start_ARG italic_C roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT end_ARG start_ARG ( italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT end_ARG, then (6) ∑A∈𝒢⁢(a,g)λ|A|≤|Y|⁢(1+λ)g⁢e−(g−a)⁢log2⁡dX/(6⁢dX).subscript𝐴𝒢𝑎𝑔superscript𝜆𝐴𝑌superscript1𝜆𝑔superscript𝑒𝑔𝑎superscript2subscript𝑑𝑋6subscript𝑑𝑋\sum_{A\in\mathcal{G}(a,g)}\lambda^{|A|}\leq|Y|(1+\lambda)^{g}e^{-(g-a)\log^{2% }d_{X}/(6d_{X})}.∑ start_POSTSUBSCRIPT italic_A ∈ caligraphic_G ( italic_a , italic_g ) end_POSTSUBSCRIPT italic_λ start_POSTSUPERSCRIPT | italic_A | end_POSTSUPERSCRIPT ≤ | italic_Y | ( 1 + italic_λ ) start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT italic_e start_POSTSUPERSCRIPT - ( italic_g - italic_a ) roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT / ( 6 italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT ) end_POSTSUPERSCRIPT . We note that the main contribution of the above lemma is the improved lower bound on λ𝜆\lambdaitalic_λ, which was previously (essentially111Galvin considers regular graphs only, so that dX=dY=dsubscript𝑑𝑋subscript𝑑𝑌𝑑d_{X}=d_{Y}=ditalic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT = italic_d start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT = italic_d.) Ω~⁢(dX−1/3)~Ωsuperscriptsubscript𝑑𝑋13\tilde{\Omega}(d_{X}^{-1/3})over~ start_ARG roman_Ω end_ARG ( italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 / 3 end_POSTSUPERSCRIPT ) in [14]. As is typical with existing results of this type, the proof of Lemma 1.2 consists of two parts: an algorithmic procedure for constructing graph containers efficiently, and a ‘reconstruction’ argument that allows us to bound the sum on the left hand-side of (6) given the family of containers we have constructed. The main driving force behind our improvement in the range of λ𝜆\lambdaitalic_λ is a novel approach to the container construction algorithm, Lemma 2.4. We conjecture that a bound of the type given in Lemma 1.2 should hold for λ=Ω~⁢(1/dX)𝜆~Ω1subscript𝑑𝑋\lambda=\tilde{\Omega}(1/d_{X})italic_λ = over~ start_ARG roman_Ω end_ARG ( 1 / italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT ). Conjecture 1.3. There exists a constant κ𝜅\kappaitalic_κ and a function λ∗:ℕ→ℝ:superscript𝜆∗→ℕℝ\lambda^{\ast}:\mathbb{N}\to\mathbb{R}italic_λ start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT : blackboard_N → blackboard_R with λ∗⁢(d)=Ω~⁢(1/d)superscript𝜆∗𝑑~Ω1𝑑\lambda^{\ast}(d)=\tilde{\Omega}(1/d)italic_λ start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_d ) = over~ start_ARG roman_Ω end_ARG ( 1 / italic_d ) as d→∞→𝑑d\to\inftyitalic_d → ∞ such that, under the assumptions of Lemma 1.2, if λ≥λ∗⁢(dX)𝜆superscript𝜆∗subscript𝑑𝑋\lambda\geq\lambda^{\ast}(d_{X})italic_λ ≥ italic_λ start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT ), then ∑A∈𝒢⁢(a,g)λ|A|≤|Y|⁢(1+λ)g⁢exp⁡{−(g−a)/dXκ}.subscript𝐴𝒢𝑎𝑔superscript𝜆𝐴𝑌superscript1𝜆𝑔𝑔𝑎superscriptsubscript𝑑𝑋𝜅\sum_{A\in\mathcal{G}(a,g)}\lambda^{|A|}\leq|Y|(1+\lambda)^{g}\exp\left\{-(g-a% )/d_{X}^{\kappa}\right\}.∑ start_POSTSUBSCRIPT italic_A ∈ caligraphic_G ( italic_a , italic_g ) end_POSTSUBSCRIPT italic_λ start_POSTSUPERSCRIPT | italic_A | end_POSTSUPERSCRIPT ≤ | italic_Y | ( 1 + italic_λ ) start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT roman_exp { - ( italic_g - italic_a ) / italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_κ end_POSTSUPERSCRIPT } . Remark 1.4. We note that the assumption g−a=Ω⁢(g/dY)𝑔𝑎Ω𝑔subscript𝑑𝑌g-a=\Omega(g/d_{Y})italic_g - italic_a = roman_Ω ( italic_g / italic_d start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT ) in Lemma 1.2 can be relaxed to g−a=Ω⁢(g⁢log2⁡dXdX⁢dY⁢λ)𝑔𝑎Ω𝑔superscript2subscript𝑑𝑋subscript𝑑𝑋subscript𝑑𝑌𝜆g-a=\Omega(\frac{g\log^{2}d_{X}}{\sqrt{d_{X}}d_{Y}\lambda})italic_g - italic_a = roman_Ω ( divide start_ARG italic_g roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT end_ARG italic_d start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT italic_λ end_ARG ) (which is weaker for λ≫log2⁡dX(dX)1/2much-greater-than𝜆superscript2subscript𝑑𝑋superscriptsubscript𝑑𝑋12\lambda\gg\frac{\log^{2}d_{X}}{(d_{X})^{1/2}}italic_λ ≫ divide start_ARG roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT end_ARG start_ARG ( italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT end_ARG) as long as λ𝜆\lambdaitalic_λ grows at most polynomially fast in dXsubscript𝑑𝑋d_{X}italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT. We stick to the simpler lower bound g/dY𝑔subscript𝑑𝑌g/d_{Y}italic_g / italic_d start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT for simplicity, as this lower bound is enough for all of the current applications. 1.2. Further applications Approximation algorithms for the hard-core model on expanders Given Z,Z^,ϵ>0𝑍^𝑍italic-ϵ0Z,\hat{Z},\epsilon>0italic_Z , over^ start_ARG italic_Z end_ARG , italic_ϵ > 0, we say that Z^^𝑍\hat{Z}over^ start_ARG italic_Z end_ARG is an ϵitalic-ϵ\epsilonitalic_ϵ-relative approximation to Z𝑍Zitalic_Z if e−ϵ⁢Z≤Z^≤eϵ⁢Zsuperscript𝑒italic-ϵ𝑍^𝑍superscript𝑒italic-ϵ𝑍e^{-\epsilon}Z\leq\hat{Z}\leq e^{\epsilon}Zitalic_e start_POSTSUPERSCRIPT - italic_ϵ end_POSTSUPERSCRIPT italic_Z ≤ over^ start_ARG italic_Z end_ARG ≤ italic_e start_POSTSUPERSCRIPT italic_ϵ end_POSTSUPERSCRIPT italic_Z. Two natural computational tasks arise when considering the hard-core model on G𝐺Gitalic_G at activity λ𝜆\lambdaitalic_λ: (1) Compute an ϵitalic-ϵ\epsilonitalic_ϵ-relative approximation to ZG⁢(λ)subscript𝑍𝐺𝜆Z_{G}(\lambda)italic_Z start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_λ ); (2) Output an independent set with distribution μ^G,λsubscript^𝜇𝐺𝜆\hat{\mu}_{G,\lambda}over^ start_ARG italic_μ end_ARG start_POSTSUBSCRIPT italic_G , italic_λ end_POSTSUBSCRIPT such that ‖μ^G,λ−μG,λ‖T⁢V≤ϵsubscriptnormsubscript^𝜇𝐺𝜆subscript𝜇𝐺𝜆𝑇𝑉italic-ϵ\|\hat{\mu}_{G,\lambda}-\mu_{G,\lambda}\|_{TV}\leq\epsilon∥ over^ start_ARG italic_μ end_ARG start_POSTSUBSCRIPT italic_G , italic_λ end_POSTSUBSCRIPT - italic_μ start_POSTSUBSCRIPT italic_G , italic_λ end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT italic_T italic_V end_POSTSUBSCRIPT ≤ italic_ϵ. A deterministic algorithm which does Task 1 in time polynomial in n𝑛nitalic_n and 1/ϵ1italic-ϵ1/\epsilon1 / italic_ϵ is known as a fully polynomial time approximation scheme (FPTAS). An algorithm which does Task 2 is known as an efficient sampling scheme. Intuitively, one might expect the problem of approximating ZG⁢(λ)subscript𝑍𝐺𝜆Z_{G}(\lambda)italic_Z start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_λ ) to be easier on the class of bipartite graphs; for one, there is a polynomial-time algorithm to find a maximum-size independent set in a bipartite graph while the corresponding problem is NP-hard for general graphs. The problem of approximating ZG⁢(λ)subscript𝑍𝐺𝜆Z_{G}(\lambda)italic_Z start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_λ ) for bipartite G𝐺Gitalic_G belongs to the complexity class #BIS introduced by Dyer, Goldberg, Greenhill, and Jerrum [9]. They showed that several natural combinatorial counting problems are as hard to approximate as #BIS. Resolving the complexity of #BIS remains a major open problem and in recent years there has been an effort to design approximation algorithms for ZG⁢(λ)subscript𝑍𝐺𝜆Z_{G}(\lambda)italic_Z start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_λ ) that exploit bipartite structure. The line of work most relevant here is that which followed the breakthrough of Helmuth, Perkins and Regts [19] who designed efficient approximation algorithms for ZG⁢(λ)subscript𝑍𝐺𝜆Z_{G}(\lambda)italic_Z start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_λ ) on ℤdsuperscriptℤ𝑑\mathbb{Z}^{d}blackboard_Z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT in the previously intractable ‘low temperature’ (i.e. large λ𝜆\lambdaitalic_λ) regime. Their method was based on tools from statistical physics, namely Pirogov-Sinai Theory, polymer models and cluster expansions. Soon after the first author, Keevash and Perkins [20] gave an FPTAS and efficient sampling scheme for the low-temperature hard-core model in bounded-degree, bipartite expander graphs. This work was followed by several improvements, extensions, and generalizations including [5, 6, 7, 10, 11, 12, 31]. Many of these algorithms exploit the fact that on a bipartite graph G𝐺Gitalic_G with sufficient expansion, typical samples from μG,λsubscript𝜇𝐺𝜆\mu_{G,\lambda}italic_μ start_POSTSUBSCRIPT italic_G , italic_λ end_POSTSUBSCRIPT are imbalanced (preferring one side of the bipartition to the other). The container method is a particularly powerful tool for detecting such structure. This fact was exploited by the first author, Perkins and Potukuchi [24] who combined the cluster expansion method with graph containers to extend the range of λ𝜆\lambdaitalic_λ for which efficient approximation algorithms on bipartite expanders were known to exist. Our new container lemma (Lemma 1.2) can be used to extend the range further still. Given α>0𝛼0\alpha>0italic_α > 0, we say that a bipartite graph with parts X,Y𝑋𝑌X,Yitalic_X , italic_Y is a bipartite α𝛼\alphaitalic_α-expander if |N⁢(A)|≥(1+α)⁢|A|𝑁𝐴1𝛼𝐴|N(A)|\geq(1+\alpha)|A|| italic_N ( italic_A ) | ≥ ( 1 + italic_α ) | italic_A | for all A⊆X𝐴𝑋A\subseteq Xitalic_A ⊆ italic_X with |A|<|X|/2𝐴𝑋2|A|<|X|/2| italic_A | < | italic_X | / 2 and A⊆Y𝐴𝑌A\subseteq Yitalic_A ⊆ italic_Y with |A|<|Y|/2𝐴𝑌2|A|<|Y|/2| italic_A | < | italic_Y | / 2. Theorem 1.5. For every α>0𝛼0\alpha>0italic_α > 0 there exists a constant C>0𝐶0C>0italic_C > 0 such that for all sufficiently large d𝑑ditalic_d and λ≥C⁢log2⁡dd1/2𝜆𝐶superscript2𝑑superscript𝑑12\lambda\geq\frac{C\log^{2}d}{d^{1/2}}italic_λ ≥ divide start_ARG italic_C roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d end_ARG start_ARG italic_d start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT end_ARG there is an FPTAS for ZG⁢(λ)subscript𝑍𝐺𝜆Z_{G}(\lambda)italic_Z start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_λ ) and a polynomial-time sampling scheme for μG,λsubscript𝜇𝐺𝜆\mu_{G,\lambda}italic_μ start_POSTSUBSCRIPT italic_G , italic_λ end_POSTSUBSCRIPT for the class of d𝑑ditalic_d-regular, bipartite α𝛼\alphaitalic_α-expanders. The above result extends [24, Theorem 2], which assumes that λ=Ω~⁢(d−1/4)𝜆~Ωsuperscript𝑑14\lambda=\tilde{\Omega}(d^{-1/4})italic_λ = over~ start_ARG roman_Ω end_ARG ( italic_d start_POSTSUPERSCRIPT - 1 / 4 end_POSTSUPERSCRIPT ). In the remainder of this section, we discuss further applications more briefly and informally for the sake of brevity and in order to avoid excessive repetition of previous work. For these applications we do not provide formal proofs, and only indicate where improvements can be made. Slow mixing of Glauber dynamics. Given a graph G𝐺Gitalic_G, the Glauber dynamics for the hardcore model μG,λsubscript𝜇𝐺𝜆\mu_{G,\lambda}italic_μ start_POSTSUBSCRIPT italic_G , italic_λ end_POSTSUBSCRIPT is the following Markov chain on state space ℐ⁢(G)ℐ𝐺\mathcal{I}(G)caligraphic_I ( italic_G ), the family of independent sets of G𝐺Gitalic_G: (1) Begin with an arbitrary I∈ℐ⁢(G)𝐼ℐ𝐺I\in\mathcal{I}(G)italic_I ∈ caligraphic_I ( italic_G ), e.g. I=∅𝐼I=\emptysetitalic_I = ∅. (2) Choose a vertex v∈V⁢(G)𝑣𝑉𝐺v\in V(G)italic_v ∈ italic_V ( italic_G ) uniformly at random. (3) Sample X∼Ber⁢(λ/(1+λ))similar-to𝑋Ber𝜆1𝜆X\sim\text{Ber}(\lambda/(1+\lambda))italic_X ∼ Ber ( italic_λ / ( 1 + italic_λ ) ). If X=1𝑋1X=1italic_X = 1 and v𝑣vitalic_v has no neighbours in I𝐼Iitalic_I then update I←I∪{v}←𝐼𝐼𝑣I\leftarrow I\cup\{v\}italic_I ← italic_I ∪ { italic_v }. If X=0𝑋0X=0italic_X = 0 update I←I\{v}←𝐼\𝐼𝑣I\leftarrow I\backslash\{v\}italic_I ← italic_I \ { italic_v }. This Markov chain has stationary distribution μG,λsubscript𝜇𝐺𝜆\mu_{G,\lambda}italic_μ start_POSTSUBSCRIPT italic_G , italic_λ end_POSTSUBSCRIPT. If the chain mixes rapidly, it gives an efficient sampling scheme for the hard-core model on G𝐺Gitalic_G. Galvin and Tetali in [17] investigated the mixing time τℳλ⁢(G)subscript𝜏subscriptℳ𝜆𝐺\tau_{\mathcal{M}_{\lambda}(G)}italic_τ start_POSTSUBSCRIPT caligraphic_M start_POSTSUBSCRIPT italic_λ end_POSTSUBSCRIPT ( italic_G ) end_POSTSUBSCRIPT of the Glauber dynamics for μG,λsubscript𝜇𝐺𝜆\mu_{G,\lambda}italic_μ start_POSTSUBSCRIPT italic_G , italic_λ end_POSTSUBSCRIPT on a bipartite expander G𝐺Gitalic_G. They showed that for λ𝜆\lambdaitalic_λ sufficiently large the Glauber dynamics mixes slowly. The driving force behind this slow mixing is that, as alluded to in previous sections, a typical sample from μG,λsubscript𝜇𝐺𝜆\mu_{G,\lambda}italic_μ start_POSTSUBSCRIPT italic_G , italic_λ end_POSTSUBSCRIPT is highly imbalanced. Balanced independent sets therefore create a small ‘bottleneck’ in the state space. As a concrete example, Galvin and Tetali prove the following. Theorem 1.6 ([17, Corollary 1.4]). There exists C>0𝐶0C>0italic_C > 0 such that if d𝑑ditalic_d is sufficiently large and C⁢log3/2⁡dd1/4≤λ≤O⁢(1)𝐶superscript32𝑑superscript𝑑14𝜆𝑂1\frac{C\log^{3/2}d}{d^{1/4}}\leq\lambda\leq O(1)divide start_ARG italic_C roman_log start_POSTSUPERSCRIPT 3 / 2 end_POSTSUPERSCRIPT italic_d end_ARG start_ARG italic_d start_POSTSUPERSCRIPT 1 / 4 end_POSTSUPERSCRIPT end_ARG ≤ italic_λ ≤ italic_O ( 1 ), we have τℳλ⁢(Qd)≥exp⁡{Ω⁢(2d⁢log3⁡(1+λ)d⁢log2⁡d)}.subscript𝜏subscriptℳ𝜆subscript𝑄𝑑Ωsuperscript2𝑑superscript31𝜆𝑑superscript2𝑑\tau_{\mathcal{M}_{\lambda}(Q_{d})}\geq\exp\left\{\Omega\left(\frac{2^{d}\log^% {3}(1+\lambda)}{\sqrt{d}\log^{2}d}\right)\right\}.italic_τ start_POSTSUBSCRIPT caligraphic_M start_POSTSUBSCRIPT italic_λ end_POSTSUBSCRIPT ( italic_Q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT ≥ roman_exp { roman_Ω ( divide start_ARG 2 start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT roman_log start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ( 1 + italic_λ ) end_ARG start_ARG square-root start_ARG italic_d end_ARG roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d end_ARG ) } . Our improved container lemma (Lemma 1.2) used in place of [17, Theorem 2.1] establishes the same slow mixing phenomenon all the way down to λ=Ω~⁢(d−1/2)𝜆~Ωsuperscript𝑑12\lambda=\tilde{\Omega}(d^{-1/2})italic_λ = over~ start_ARG roman_Ω end_ARG ( italic_d start_POSTSUPERSCRIPT - 1 / 2 end_POSTSUPERSCRIPT ). More generally, Galvin and Tetali establish slow mixing results for d𝑑ditalic_d-regular bipartite α𝛼\alphaitalic_α-expanders provided λ𝜆\lambdaitalic_λ is sufficiently large as a function of α𝛼\alphaitalic_α and d𝑑ditalic_d. As above, Lemma 1.2 can be used to improve the range of λ𝜆\lambdaitalic_λ for which these results hold. Antichains in the Boolean lattice. Given n∈ℕ𝑛ℕn\in\mathbb{N}italic_n ∈ blackboard_N, let Bnsubscript𝐵𝑛B_{n}italic_B start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT denote the Boolean lattice of dimension n𝑛nitalic_n, i.e. the power set of [n]={1,…,n}delimited-[]𝑛1…𝑛[n]=\{1,\ldots,n\}[ italic_n ] = { 1 , … , italic_n }. Recall that ℱ⊆Bnℱsubscript𝐵𝑛\mathcal{F}\subseteq B_{n}caligraphic_F ⊆ italic_B start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT is called an antichain if it is inclusion-free (i.e. A⊈Bnot-subset-of-or-equals𝐴𝐵A\not\subseteq Bitalic_A ⊈ italic_B for all distinct A,B∈ℱ𝐴𝐵ℱA,B\in\mathcal{F}italic_A , italic_B ∈ caligraphic_F). Dedekind’s problem, dating back to 1897, asks for the total number of antichains in Bnsubscript𝐵𝑛B_{n}italic_B start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT. In recent work [21], the current authors applied the graph container method to study Dedekind’s problem in detail. As part of this study, the authors count and study the typical structure of antichains in Bnsubscript𝐵𝑛B_{n}italic_B start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT of a given size. For 0≤k≤n0𝑘𝑛0\leq k\leq n0 ≤ italic_k ≤ italic_n, we call the family Lk=([n]k)⊆Bnsubscript𝐿𝑘binomialdelimited-[]𝑛𝑘subscript𝐵𝑛L_{k}=\binom{[n]}{k}\subseteq B_{n}italic_L start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = ( FRACOP start_ARG [ italic_n ] end_ARG start_ARG italic_k end_ARG ) ⊆ italic_B start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT the kt⁢hsuperscript𝑘𝑡ℎk^{th}italic_k start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT layer of Bnsubscript𝐵𝑛B_{n}italic_B start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT. We say that Lk−1,Lk,Lk+1subscript𝐿𝑘1subscript𝐿𝑘subscript𝐿𝑘1L_{k-1},L_{k},L_{k+1}italic_L start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT , italic_L start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_L start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT are three central layers if k=⌊n/2⌋𝑘𝑛2k=\lfloor n/2\rflooritalic_k = ⌊ italic_n / 2 ⌋ or k=⌈n/2⌉𝑘𝑛2k=\lceil n/2\rceilitalic_k = ⌈ italic_n / 2 ⌉. One of the main results of [21] is the following. Theorem 1.7 ([21, Theorem 1.5]). There exists C>0𝐶0C>0italic_C > 0 such that if C⁢log2⁡nn<β≤1𝐶superscript2𝑛𝑛𝛽1\frac{C\log^{2}n}{\sqrt{n}}<\beta\leq 1divide start_ARG italic_C roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_n end_ARG start_ARG square-root start_ARG italic_n end_ARG end_ARG < italic_β ≤ 1, then almost all antichains of size β⁢(n⌊n/2⌋)𝛽binomial𝑛𝑛2\beta\displaystyle\binom{n}{\lfloor n/2\rfloor}italic_β ( FRACOP start_ARG italic_n end_ARG start_ARG ⌊ italic_n / 2 ⌋ end_ARG ) in Bnsubscript𝐵𝑛B_{n}italic_B start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT are a subset of three central layers. For the proof, we study the hard-core model on the graph Gnsubscript𝐺𝑛G_{n}italic_G start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT whose vertex set is Bnsubscript𝐵𝑛B_{n}italic_B start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT and u∼vsimilar-to𝑢𝑣u\sim vitalic_u ∼ italic_v if and only if u≠v𝑢𝑣u\neq vitalic_u ≠ italic_v and u𝑢uitalic_u is contained in v𝑣vitalic_v or vice versa. Note that independent sets in Gnsubscript𝐺𝑛G_{n}italic_G start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT are precisely antichains in Bnsubscript𝐵𝑛B_{n}italic_B start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT. We use our container lemma, Lemma 1.2 (and its proof), to study the hard-core measure on Gnsubscript𝐺𝑛G_{n}italic_G start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT and to prove Theorem 1.7. Recently Balogh, Garcia and Li [1] studied the hard-core model on the subgraph Mn⊆Gnsubscript𝑀𝑛subscript𝐺𝑛M_{n}\subseteq G_{n}italic_M start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ⊆ italic_G start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT induced by the middle two layers L(n−1)/2,L(n+1)/2subscript𝐿𝑛12subscript𝐿𝑛12L_{(n-1)/2},L_{(n+1)/2}italic_L start_POSTSUBSCRIPT ( italic_n - 1 ) / 2 end_POSTSUBSCRIPT , italic_L start_POSTSUBSCRIPT ( italic_n + 1 ) / 2 end_POSTSUBSCRIPT when n𝑛nitalic_n is odd. In particular, they obtain detailed asymptotics for the partition function ZMn⁢(λ)subscript𝑍subscript𝑀𝑛𝜆Z_{M_{n}}(\lambda)italic_Z start_POSTSUBSCRIPT italic_M start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_λ ) when λ≥C⁢log⁡n/n1/3𝜆𝐶𝑛superscript𝑛13\lambda\geq C\log n/n^{1/3}italic_λ ≥ italic_C roman_log italic_n / italic_n start_POSTSUPERSCRIPT 1 / 3 end_POSTSUPERSCRIPT (see [1, Theorem 5.1]). The now familiar Ω~⁢(n−1/3)~Ωsuperscript𝑛13\tilde{\Omega}(n^{-1/3})over~ start_ARG roman_Ω end_ARG ( italic_n start_POSTSUPERSCRIPT - 1 / 3 end_POSTSUPERSCRIPT ) bottleneck comes from the graph container method. Using Lemma 1.2 one can easily extend [1, Theorem 5.1] to the regime λ=Ω~⁢(n−1/2)𝜆~Ωsuperscript𝑛12\lambda=\tilde{\Omega}(n^{-1/2})italic_λ = over~ start_ARG roman_Ω end_ARG ( italic_n start_POSTSUPERSCRIPT - 1 / 2 end_POSTSUPERSCRIPT ). Independent sets in the percolated hypercube. Given d∈ℕ𝑑ℕd\in\mathbb{N}italic_d ∈ blackboard_N and p∈[0,1]𝑝01p\in[0,1]italic_p ∈ [ 0 , 1 ] we let Qd,psubscript𝑄𝑑𝑝Q_{d,p}italic_Q start_POSTSUBSCRIPT italic_d , italic_p end_POSTSUBSCRIPT denote the random subgraph of the hypercube Qdsubscript𝑄𝑑Q_{d}italic_Q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT where each edge is retained independently with probability p𝑝pitalic_p. Recently, Kronenberg and Spinka [30] studied the hard-core model on Qd,psubscript𝑄𝑑𝑝Q_{d,p}italic_Q start_POSTSUBSCRIPT italic_d , italic_p end_POSTSUBSCRIPT. In particular, they find asymptotic formulae for the expected value (and higher moments) of the partition function ZQd,p⁢(λ)subscript𝑍subscript𝑄𝑑𝑝𝜆Z_{Q_{d,p}}(\lambda)italic_Z start_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT italic_d , italic_p end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_λ ). For example, they prove the following. Theorem 1.8 ([30, Theorem 1.1]). For d𝑑ditalic_d sufficiently large and p≥C⁢log⁡dd1/3𝑝𝐶𝑑superscript𝑑13p\geq\frac{C\log d}{d^{1/3}}italic_p ≥ divide start_ARG italic_C roman_log italic_d end_ARG start_ARG italic_d start_POSTSUPERSCRIPT 1 / 3 end_POSTSUPERSCRIPT end_ARG, 𝔼⁢|ℐ⁢(Qd,p)|=2⋅22d−1⁢exp⁡[12⁢(2−p)d+(a⁢(p)⁢(d2)−14)⁢2d⁢(1−p2)2⁢d+O⁢(d4⁢2d⁢(1−p2)3⁢d)],𝔼ℐsubscript𝑄𝑑𝑝⋅2superscript2superscript2𝑑112superscript2𝑝𝑑𝑎𝑝binomial𝑑214superscript2𝑑superscript1𝑝22𝑑𝑂superscript𝑑4superscript2𝑑superscript1𝑝23𝑑\mathbb{E}|\mathcal{I}(Q_{d,p})|=2\cdot 2^{2^{d-1}}\exp\left[\tfrac{1}{2}(2-p)% ^{d}+\left(a(p)\tbinom{d}{2}-\tfrac{1}{4}\right)2^{d}(1-\tfrac{p}{2})^{2d}+O% \left(d^{4}2^{d}(1-\tfrac{p}{2})^{3d}\right)\right],blackboard_E | caligraphic_I ( italic_Q start_POSTSUBSCRIPT italic_d , italic_p end_POSTSUBSCRIPT ) | = 2 ⋅ 2 start_POSTSUPERSCRIPT 2 start_POSTSUPERSCRIPT italic_d - 1 end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT roman_exp [ divide start_ARG 1 end_ARG start_ARG 2 end_ARG ( 2 - italic_p ) start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT + ( italic_a ( italic_p ) ( FRACOP start_ARG italic_d end_ARG start_ARG 2 end_ARG ) - divide start_ARG 1 end_ARG start_ARG 4 end_ARG ) 2 start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ( 1 - divide start_ARG italic_p end_ARG start_ARG 2 end_ARG ) start_POSTSUPERSCRIPT 2 italic_d end_POSTSUPERSCRIPT + italic_O ( italic_d start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT 2 start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ( 1 - divide start_ARG italic_p end_ARG start_ARG 2 end_ARG ) start_POSTSUPERSCRIPT 3 italic_d end_POSTSUPERSCRIPT ) ] , where a⁢(p):=(1+(1−p)2)2(2−p)4−14.assign𝑎𝑝superscript1superscript1𝑝22superscript2𝑝414a(p):=\frac{(1+(1-p)^{2})^{2}}{(2-p)^{4}}-\frac{1}{4}.italic_a ( italic_p ) := divide start_ARG ( 1 + ( 1 - italic_p ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG ( 2 - italic_p ) start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT end_ARG - divide start_ARG 1 end_ARG start_ARG 4 end_ARG . The restriction on p𝑝pitalic_p here arises from an application of the graph container method. Our refined container method can be used to extend the above theorem to the range p=Ω~⁢(d−1/2)𝑝~Ωsuperscript𝑑12p=\tilde{\Omega}(d^{-1/2})italic_p = over~ start_ARG roman_Ω end_ARG ( italic_d start_POSTSUPERSCRIPT - 1 / 2 end_POSTSUPERSCRIPT ). Kronenberg and Spinka prove several results about the hard-core model on Qd,psubscript𝑄𝑑𝑝Q_{d,p}italic_Q start_POSTSUBSCRIPT italic_d , italic_p end_POSTSUBSCRIPT at activity λ𝜆\lambdaitalic_λ. The range of λ𝜆\lambdaitalic_λ and p𝑝pitalic_p for which these results hold may similarly be extended using the refined container methods of this paper. 1.3. Organization We collect preliminary results in Section 2 and prove our core result, Lemma 2.4, in Section 3. We then give the reconstruction argument that enables us to deduce Lemma 1.2 in Section 4. We prove Theorem 1.1 in Section 5 and Theorem 1.5 in Section 6. Finally, we conclude with some brief remarks in Section 7. 1.4. Notation and usage We use Σ=X⊔YΣsquare-union𝑋𝑌\Sigma=X\sqcup Yroman_Σ = italic_X ⊔ italic_Y for a bipartite graph with parts X𝑋Xitalic_X and Y𝑌Yitalic_Y, and V⁢(Σ)𝑉ΣV(\Sigma)italic_V ( roman_Σ ) and E⁢(Σ)𝐸ΣE(\Sigma)italic_E ( roman_Σ ) for the set of vertices and edges of ΣΣ\Sigmaroman_Σ, respectively. As usual, we write N⁢(x)𝑁𝑥N(x)italic_N ( italic_x ) for {y∈V⁢(Σ):{x,y}∈E⁢(Σ)}conditional-set𝑦𝑉Σ𝑥𝑦𝐸Σ\{y\in V(\Sigma):\{x,y\}\in E(\Sigma)\}{ italic_y ∈ italic_V ( roman_Σ ) : { italic_x , italic_y } ∈ italic_E ( roman_Σ ) } and dA⁢(x):=|N⁢(x)∩A|assignsubscript𝑑𝐴𝑥𝑁𝑥𝐴d_{A}(x):=|N(x)\cap A|italic_d start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT ( italic_x ) := | italic_N ( italic_x ) ∩ italic_A |. We use d⁢(x)𝑑𝑥d(x)italic_d ( italic_x ) for dΣ⁢(x)subscript𝑑Σ𝑥d_{\Sigma}(x)italic_d start_POSTSUBSCRIPT roman_Σ end_POSTSUBSCRIPT ( italic_x ) for simplicity. For S⊆V⁢(Σ)𝑆𝑉ΣS\subseteq V(\Sigma)italic_S ⊆ italic_V ( roman_Σ ), set N⁢(S):=∪x∈SN⁢(x)assign𝑁𝑆subscript𝑥𝑆𝑁𝑥N(S):=\cup_{x\in S}N(x)italic_N ( italic_S ) := ∪ start_POSTSUBSCRIPT italic_x ∈ italic_S end_POSTSUBSCRIPT italic_N ( italic_x ) and let Σ⁢[S]Σdelimited-[]𝑆\Sigma[S]roman_Σ [ italic_S ] denote the induced subgraph of ΣΣ\Sigmaroman_Σ on S𝑆Sitalic_S. For A,B⊆V⁢(Σ)𝐴𝐵𝑉ΣA,B\subseteq V(\Sigma)italic_A , italic_B ⊆ italic_V ( roman_Σ ), let ∇(A,B)={{x,y}∈E⁢(Σ):x∈A,y∈B},∇𝐴𝐵conditional-set𝑥𝑦𝐸Σformulae-sequence𝑥𝐴𝑦𝐵\nabla(A,B)=\{\{x,y\}\in E(\Sigma):x\in A,y\in B\},∇ ( italic_A , italic_B ) = { { italic_x , italic_y } ∈ italic_E ( roman_Σ ) : italic_x ∈ italic_A , italic_y ∈ italic_B } , and ∇(A)=∇(A,V⁢(Σ)∖A)∇𝐴∇𝐴𝑉Σ𝐴\nabla(A)=\nabla(A,V(\Sigma)\setminus A)∇ ( italic_A ) = ∇ ( italic_A , italic_V ( roman_Σ ) ∖ italic_A ). We write (n≤k)binomial𝑛absent𝑘\binom{n}{\leq k}( FRACOP start_ARG italic_n end_ARG start_ARG ≤ italic_k end_ARG ) for ∑i≤k(ni).subscript𝑖𝑘binomial𝑛𝑖\sum_{i\leq k}\binom{n}{i}.∑ start_POSTSUBSCRIPT italic_i ≤ italic_k end_POSTSUBSCRIPT ( FRACOP start_ARG italic_n end_ARG start_ARG italic_i end_ARG ) . We will make frequent use of the basic binomial estimate (7) (n≤k)≤exp⁡{k⁢log⁡(e⁢nk)} for ⁢k≤n,formulae-sequencebinomial𝑛absent𝑘𝑘𝑒𝑛𝑘 for 𝑘𝑛{n\choose\leq k}\leq\exp\left\{k\log\left(\frac{en}{k}\right)\right\}\quad% \text{ for }k\leq n,( binomial start_ARG italic_n end_ARG start_ARG ≤ italic_k end_ARG ) ≤ roman_exp { italic_k roman_log ( divide start_ARG italic_e italic_n end_ARG start_ARG italic_k end_ARG ) } for italic_k ≤ italic_n , where here and throughout the paper log\logroman_log is used for the natural logarithm. For two functions f,g:ℕ→ℝ:𝑓𝑔→ℕℝf,g:\mathbb{N}\to\mathbb{R}italic_f , italic_g : blackboard_N → blackboard_R we write f⁢(d)=O~⁢(g⁢(d))𝑓𝑑~𝑂𝑔𝑑f(d)=\tilde{O}(g(d))italic_f ( italic_d ) = over~ start_ARG italic_O end_ARG ( italic_g ( italic_d ) ) if there exists C∈ℝ𝐶ℝC\in\mathbb{R}italic_C ∈ blackboard_R such that |f⁢(d)|≤(log⁡d)C⁢|g⁢(d)|𝑓𝑑superscript𝑑𝐶𝑔𝑑|f(d)|\leq(\log d)^{C}|g(d)|| italic_f ( italic_d ) | ≤ ( roman_log italic_d ) start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPT | italic_g ( italic_d ) | for d𝑑ditalic_d sufficiently large. We use Ω~~Ω\tilde{\Omega}over~ start_ARG roman_Ω end_ARG analogously. Throughout the paper ΣΣ\Sigmaroman_Σ will denote a δ𝛿\deltaitalic_δ-approximately (dX,dY)subscript𝑑𝑋subscript𝑑𝑌(d_{X},d_{Y})( italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT )-biregular graph (for some choice of absolute constant δ𝛿\deltaitalic_δ) and all asymptotic notation is to be understood with respect to the limits dX,dY→∞→subscript𝑑𝑋subscript𝑑𝑌d_{X},d_{Y}\to\inftyitalic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT → ∞. The cost of a choice means the logarithm (in base two) of the number of its possibilities. We employ a common abuse of notation by often omitting floor and ceiling symbols for notational convenience."
https://arxiv.org/html/2411.03871v1,Safe Paths and Sequences forScalable ILPs in RNA Transcript Assembly Problems,"A common step at the core of many RNA transcript assembly tools is to find a set of weighted paths that best explain the weights of a DAG. While such problems easily become NP-hard, scalable solvers exist only for a basic error-free version of this problem, namely minimally decomposing a network flow into weighted paths.The main result of this paper is to show that we can achieve speedups of two orders of magnitude also for path-finding problems in the realistic setting (i.e., the weights do not induce a flow). We obtain these by employing the safety information that is encoded in the graph structure inside Integer Linear Programming (ILP) solvers for these problems. We first characterize the paths that appear in all path covers of the DAG, generalizing a graph reduction commonly used in the error-free setting (e.g. by Kloster et al. [ALENEX 2018]). Secondly, following the work of Ma, Zheng and Kingsford [RECOMB 2021], we characterize the sequences of arcs that appear in all path covers of the DAG.We experiment with a path-finding ILP model (least squares) and with a more recent and accurate one. We use a variety of datasets originally created by Shao and Kingsford [TCBB, 2017], as well as graphs built from sequencing reads by the state-of-the-art tool for long-read transcript discovery, IsoQuant [Prjibelski et al., Nat. Biotechnology 2023]. The ILPs armed with safe paths or sequences exhibit significant speed-ups over the original ones. On graphs with a large width, average speed-ups are in the range 50−160×50-160\times50 - 160 × in the latter ILP model and in the range 100−1000×100-1000\times100 - 1000 × in the least squares model.Our scaling techniques apply to any ILP whose solution paths are a path cover of the arcs of the DAG. As such, they can become a scalable building block of practical RNA transcript assembly tools, avoiding heuristic trade-offs currently needed on complex graphs.","Background and motivation. The genome-guided RNA transcript assembly problem, one the most famous assembly problems in bioinformatics, can be succinctly described as follows. Given a set of RNA-seq reads, a directed acyclic graph (DAG) is constructed from their alignments to a reference genome. The graph nodes correspond to e.g. exons, the arcs correspond to reads overlapping two consecutive exons, and the node or arc weights corresponding their read coverage. The RNA transcripts then correspond to a set of source-to-sink weighted paths in the DAG that “best explain” the nodes, arcs and their weights [tomescu2015explaining], under various definitions of optimality. On perfect, error-free data, the arc weights satisfy flow conservation. In this setting, the most well-known definition of optimality is to require a minimum number of weighted paths whose superposition fully equals to the given flow weights (minimum flow decomposition, or MFD). This is a classical NP-hard problem [ahuja1988network, vatinlen2008simple], with many applications also in other fields, such as transportation [olsen2022study] and networking [hartman2012split, vatinlen2008simple]. On real data, the arc weights do not satisfy flow conservation because of errors in the reads and in their alignment, biases in sequencing coverage, and trimming in the reads, see e.g. [lrgasp]. As such, many practical tools model the RNA transcript assembly problem as a path-finding Integer Linear Program (ILP), for which they use a fast solver, such as Gurobi [gurobi] or CPLEX [cplex2009v12]. ILP is a powerful paradigm to model and efficiently solve NP-hard problems, including in bioinformatics, see e.g. [gusfield2019integer]. RNA transcript assembly based on ILP include CIDANE [cidane], CLASS2 [class2], TransLiG [translig], CLIIQ [cliiq], IsoInfer [isoinfer], IsoLasso [isolasso], MultiTrans [multitrans], NSMAP [nsmap], SSP [ssp], JUMPER [jumper]. Most of the existing ILP-based RNA transcript assembly tools (e.g. [cidane, class2, cliiq, isoinfer, isolasso, multitrans, nsmap, ssp, translig]) in principle do not scale with large graphs. The reason is that one first needs to enumerate all possible paths in the graphs, and then add an ILP variable for each path. This possibly leads to an exponential pre-processing time, and to an exponentially-sized ILP. Thus, many of these tools e.g. [cidane, cliiq, ireckon, isoinfer, multitrans, nsmap] use the heuristic of enumerating only some of all possible paths, potentially leaving some transcripts undiscovered, or leading to incorrect answers. Recently, [dias2022fast, jumper] observed that the enumeration step can be avoided by modeling the search for paths in the ILP itself, via only polynomially-many additional variables and constraints. However, some datasets still require tens of hours to solve [acceleratingILP, dias2024robust]. Despite this pressing need for fast solutions to path-finding ILPs modeling real-world data, most research effort has been put in the error-free setting, namely in the MFD problem. This includes fast heuristics [shao2017theory, vatinlen2008simple, hartman2012split, bernard2014efficient], fixed-parameter tractable algorithms [kloster2018practical], and approximation algorithms [hartman2012split, caceres2024width]. Recently, [acceleratingILP] showed that also the above-mentioned polynomially-sized ILP models for the MFD problem can be sped-up using some insight into the input flow structure, via the notion of safety [omnitigs_tomescu] (which we also review below). These optimizations also apply to an MFD variant where we are also given subpath constraints corresponding to long-reads aligned to the graph. On the hardest instances this leads to speedups of two orders of magnitude [acceleratingILP]. Contributions. In this paper we show that similar speedups can be obtained also for real-world inputs where the arc weights do not satisfy flow conservation, and for any “path-finding” problem formulation, as long as their solution paths are a path cover of the arcs of the DAG.111In this paper we assume that the input DAG has a unique source node s𝑠sitalic_s and a unique sink node t𝑡titalic_t; if this is not the case, one can just add a new global source s𝑠sitalic_s connected to all existing graph sources (and symmetrically for a global sink), and specially handle these extra arcs in the problem formulations. Moreover, by a path cover we mean a set of paths from s𝑠sitalic_s to t𝑡titalic_t (s𝑠sitalic_s-t𝑡titalic_t paths), such that every arc belongs to at least one path. We obtain these speedups by exploiting the graph structure of the DAG, in particular by exploiting the safe paths (and safe sequences of arcs) that appear in all path covers of the DAG. As such, we prove new results about the structure of directed acyclic graphs, which may also be of independent interest. More specifically, we give the following contributions. 1.0.1 1. Generalizing the Y-to-V reduction as finding safe paths for path covers. For the minimum flow decomposition problem, Kloster et al. [kloster2018practical] used a graph reduction operation that decreases the size of the DAG, while preserving all flow decompositions. Namely, as long as the graph has a node v𝑣vitalic_v with only one in-neighbor u𝑢uitalic_u, one removes v𝑣vitalic_v, and adds arcs from u𝑢uitalic_u to each out-neighbor w𝑤witalic_w of v𝑣vitalic_v, with flow value f⁢(u,w)=f⁢(v,w)𝑓𝑢𝑤𝑓𝑣𝑤f(u,w)=f(v,w)italic_f ( italic_u , italic_w ) = italic_f ( italic_v , italic_w ). This is a correct operation for MFD because of flow conservation: intuitively, the flow on any arc (v,w)𝑣𝑤(v,w)( italic_v , italic_w ) must come to v𝑣vitalic_v via its unique in-neighbor u𝑢uitalic_u. A symmetric operation applies to nodes with only one out-neighbor (see Figure 4 (a) in Appendix 0.B). These operations have later been used in other works on the MFD problem, see [acceleratingILP, dias2023safety]. Moreover, they are also common in the context of the genome assembly problem [medvedev2007computability, jackson2009parallel, kingsford2010assembly], where this operation was called the “Y-to-V reduction” [omnitigs_tomescu] because of the shape of the subgraphs before and after reduction. We will also use this name to refer to this operation in this paper. (a) Four safe paths, shown as colored lines, such that there is no s𝑠sitalic_s-t𝑡titalic_t path containing any two of them. (b) Four safe sequences such that there is no s𝑠sitalic_s-t𝑡titalic_t path containing any two of them. Dotted lines indicate gaps in the sequences. Figure 1: Example of safe paths and safe sequences for the s𝑠sitalic_s-t𝑡titalic_t path covers in a DAG G𝐺Gitalic_G with unique source s𝑠sitalic_s and unique sink t𝑡titalic_t. That is, for every path cover of G𝐺Gitalic_G (i.e. set of paths from s𝑠sitalic_s to t𝑡titalic_t in G𝐺Gitalic_G such that every arc of G𝐺Gitalic_G appears in a path of the cover), there is a path in the cover containing the safe path or the safe sequence. We can see that safe sequences extend safe paths over complex subgraphs. If we have a set of safe path, or safe sequences, respectively, such that no two of them can appear on the same path of the DAG, then each of them must clearly appear in different paths of any path cover. Suppose that an ILP model has k𝑘kitalic_k binary variables xu⁢v⁢isubscript𝑥𝑢𝑣𝑖x_{uvi}italic_x start_POSTSUBSCRIPT italic_u italic_v italic_i end_POSTSUBSCRIPT, i∈{1,…,k}𝑖1…𝑘i\in\{1,\dots,k\}italic_i ∈ { 1 , … , italic_k }, for every arc (u,v)𝑢𝑣(u,v)( italic_u , italic_v ) of G𝐺Gitalic_G, with the interpretation that (u,v)𝑢𝑣(u,v)( italic_u , italic_v ) appears in solution path i𝑖iitalic_i iff xu⁢v⁢i=1subscript𝑥𝑢𝑣𝑖1x_{uvi}=1italic_x start_POSTSUBSCRIPT italic_u italic_v italic_i end_POSTSUBSCRIPT = 1. Then, using such set of safe paths or sequences we can fix to 1 some of these binary variables, as in [acceleratingILP]. For example, we can assign the blue, orange, green and violet safe paths to solution paths 1,2,3 and 4, respectively (we show this assignment only for the blue and violet safe paths). We can proceed in a similar manner for safe sequences, now fixing more variables because sequences are overall longer (we show this assignment only for the blue and violet safe sequences). However, when flow conservation does not hold, and when considering other path-finding problems than MFD, these are invalid operations. As such, we generalize the Y-to-V reduction so that we can use it for any problem whose solution is some path cover of the arcs. Specifically, we say that a path P𝑃Pitalic_P is safe (with respect to the path covers of a DAG G𝐺Gitalic_G) if for any path cover of G𝐺Gitalic_G, there is a path in the path cover that contains P𝑃Pitalic_P as subpath (see Figure 1(a) for an example). Using this framework, one can see the correctness of the Y-to-V reduction because the paths (u,v,w)𝑢𝑣𝑤(u,v,w)( italic_u , italic_v , italic_w ), for any out-neighbor w𝑤witalic_w of v𝑣vitalic_v are safe. However, by repeatedly applying the Y-to-V reduction, one misses safe paths (see Figure 4), and thus this reduction is not a correct algorithm for finding all safe paths. In this paper, we characterize all the safe paths with respect to the path covers of a DAG G𝐺Gitalic_G (Section 2). Using this characterization, we show that all maximal safe paths (i.e., those that contain all other safe paths as subpaths) can be computed in optimal linear time: {restatable*} [Maximal safe paths enumeration]theoremmaxsafepathsenum Given a DAG G𝐺Gitalic_G with m𝑚mitalic_m arcs, there is an O⁢(m+o)𝑂𝑚𝑜O(m+o)italic_O ( italic_m + italic_o )-time algorithm computing all the maximal safe paths of G𝐺Gitalic_G, where o𝑜oitalic_o is the total length of the output, namely of all maximal safe paths. 1.0.2 2. Generalizing safe paths to safe sequences for path covers. While safe paths fully capture contiguous safety information, it may be that the only way to reach a safe path P2subscript𝑃2P_{2}italic_P start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT is by passing through a safe path P1subscript𝑃1P_{1}italic_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, with some “complex” subgraph between P1subscript𝑃1P_{1}italic_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and P2subscript𝑃2P_{2}italic_P start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. As such, the sequence P1,P2subscript𝑃1subscript𝑃2P_{1},P_{2}italic_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_P start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT appears in some path of any path cover. Therefore, inspired by the results of Ma, Zheng and Kingsford [and_or_quant], we generalize the previous notion of safe paths to safe sequences of arcs that appears in all paths covers of a DAG G𝐺Gitalic_G (see Figure 1(b) for an example). More specifically, an application of the AND-Quant problem from [and_or_quant] is to characterize when a sequence (e1,…,et)subscript𝑒1…subscript𝑒𝑡(e_{1},\dots,e_{t})( italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) of arcs of G𝐺Gitalic_G appears in all flow decompositions of G𝐺Gitalic_G. Namely, when it holds that for any flow decomposition, there is a path Pisubscript𝑃𝑖P_{i}italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT of the decomposition such that e1,…,etsubscript𝑒1…subscript𝑒𝑡e_{1},\dots,e_{t}italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT appear in Pisubscript𝑃𝑖P_{i}italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, in this order. This is based on solving a max flow problem from (e1,…,et)subscript𝑒1…subscript𝑒𝑡(e_{1},\dots,e_{t})( italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) and G𝐺Gitalic_G. In this paper we characterize safe sequences with respect to path covers of G𝐺Gitalic_G. For this, we generalize our results for safe paths by using the notion of u𝑢uitalic_u-v𝑣vitalic_v bridges (i.e. arcs belonging to all u𝑢uitalic_u-v𝑣vitalic_v paths), for a suitable choice of nodes u𝑢uitalic_u and v𝑣vitalic_v. {restatable*} [Maximal safe sequences enumeration]theoremmaxsafeseqsenum Given a DAG G𝐺Gitalic_G with m𝑚mitalic_m arcs, there is an O⁢(m+o2)𝑂𝑚superscript𝑜2O(m+o^{2})italic_O ( italic_m + italic_o start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )-time algorithm computing all the maximal safe sequences of G𝐺Gitalic_G, where o𝑜oitalic_o is the total length of the output, namely of all maximal safe sequences. 1.0.3 3. Scaling path-finding ILPs via safe paths and sequences. Safety characterizes the information that must appear in all solutions to a problem. If the solutions to a real-world path-finding problem are a subset of all the path covers of a DAG, then safe paths and safe sequences for path covers also appear in the solution paths of our path-finding problem. As such, we can use them to simplify a solver for the problem. For this, we use the approach of Grigorjew et al. [acceleratingILP]: if we are given a set of paths (or sequences, in our case) that must each be used by different solution paths, then we can fix some binary variables in the ILP model that encode the solution paths. See Figure 1 for an illustration. We apply safe paths and safe sequences in this manner for two path-finding ILPs. In the first one (LeastSquares), we need to find a set of paths minimizing the sum of the squared errors between the weight of each arc and the weight of the solutions paths going through the arc. This is at the core of several RNA assembly tools e.g. [isolasso, cidane, ryuto, slide, ireckon, traph]. The second one (MinPathError) was recently introduced in [dias2024robust] and shown to be more accurate than LeastSquares (and than other ones, such as minimum inexact flow decomposition [inexact], see [dias2024robust]). The goal here is to account for errors not at the level of individual arcs, but at the level of solution paths, and minimize the sum of the errors of the paths. We describe all the above in detail in Appendix 0.B. The ILPs optimized with safe paths or sequences exhibit significant speed-ups over the original ones, with average speed-up of at least 10×10\times10 × on many types of graphs. On graphs with a large width, average speed-ups are in the range 50−150×50-150\times50 - 150 × for MinPathError, and in the range 100−1000×100-1000\times100 - 1000 × for LeastSquares. As such, our optimizations can become a scalable building block of practical RNA transcript assembly tools, avoiding heuristic trade-offs on complex graphs currently needed."
https://arxiv.org/html/2411.03784v1,Optimal prefix-suffix queries with applications,"We revisit the classic border tree data structure [Gu, Farach, Beigel, SODA 1994] that answers the following prefix-suffix queries on a string T𝑇Titalic_T of length n𝑛nitalic_n over an integer alphabet Σ=[0,σ)Σ0𝜎\Sigma=[0,\sigma)roman_Σ = [ 0 , italic_σ ): for any i,j∈[0,n)𝑖𝑗0𝑛i,j\in[0,n)italic_i , italic_j ∈ [ 0 , italic_n ) return all occurrences of T𝑇Titalic_T in T⁢[0⁢..⁢i]⁢T⁢[j⁢..⁢n−1]T[0\mathinner{.\,.}i]T[j\mathinner{.\,.}n-1]italic_T [ 0 start_ATOM . . end_ATOM italic_i ] italic_T [ italic_j start_ATOM . . end_ATOM italic_n - 1 ]. The border tree of T𝑇Titalic_T can be constructed in 𝒪⁢(n)𝒪𝑛\mathcal{O}(n)caligraphic_O ( italic_n ) time and answers prefix-suffix queries in 𝒪⁢(log⁡n+Occ)𝒪𝑛Occ\mathcal{O}(\log n+\textsf{Occ})caligraphic_O ( roman_log italic_n + Occ ) time, where Occ is the number of occurrences of T𝑇Titalic_T in T⁢[0⁢..⁢i]⁢T⁢[j⁢..⁢n−1]T[0\mathinner{.\,.}i]T[j\mathinner{.\,.}n-1]italic_T [ 0 start_ATOM . . end_ATOM italic_i ] italic_T [ italic_j start_ATOM . . end_ATOM italic_n - 1 ]. Our contribution here is the following. We present a completely different and remarkably simple data structure that can be constructed in the optimal 𝒪⁢(n/logσ⁡n)𝒪𝑛subscript𝜎𝑛\mathcal{O}(n/\log_{\sigma}n)caligraphic_O ( italic_n / roman_log start_POSTSUBSCRIPT italic_σ end_POSTSUBSCRIPT italic_n ) time and supports queries in the optimal 𝒪⁢(1)𝒪1\mathcal{O}(1)caligraphic_O ( 1 ) time. Our result is based on a new structural lemma that lets us encode the output of any query in constant time and space. We also show a new direct application of our result in pattern matching on node-labeled graphs.","Let T=T[0..n−1]=T[0..n)T=T[0\mathinner{.\,.}n-1]=T[0\mathinner{.\,.}n)italic_T = italic_T [ 0 start_ATOM . . end_ATOM italic_n - 1 ] = italic_T [ 0 start_ATOM . . end_ATOM italic_n ) be a string of length n𝑛nitalic_n over an integer alphabet Σ=[0,σ)Σ0𝜎\Sigma=[0,\sigma)roman_Σ = [ 0 , italic_σ ) with σ=n𝒪⁢(1)𝜎superscript𝑛𝒪1\sigma=n^{\mathcal{O}(1)}italic_σ = italic_n start_POSTSUPERSCRIPT caligraphic_O ( 1 ) end_POSTSUPERSCRIPT. We would like to preprocess T𝑇Titalic_T in order to answer the following type of queries: for any i,j∈[0,n)𝑖𝑗0𝑛i,j\in[0,n)italic_i , italic_j ∈ [ 0 , italic_n ), return all occurrences of T𝑇Titalic_T in T⁢[0⁢..⁢i]⁢T⁢[j⁢..⁢n−1]T[0\mathinner{.\,.}i]T[j\mathinner{.\,.}n-1]italic_T [ 0 start_ATOM . . end_ATOM italic_i ] italic_T [ italic_j start_ATOM . . end_ATOM italic_n - 1 ]. This type of query, which we denote here by PrefSuf⁢(i,j)PrefSuf𝑖𝑗\textsf{PrefSuf}(i,j)PrefSuf ( italic_i , italic_j ), is supported by the classic border tree data structure in 𝒪⁢(log⁡n+Occ)𝒪𝑛Occ\mathcal{O}(\log n+\textsf{Occ})caligraphic_O ( roman_log italic_n + Occ ) time after an 𝒪⁢(n)𝒪𝑛\mathcal{O}(n)caligraphic_O ( italic_n )-time preprocessing [17]. We show the following optimal data structure for PrefSuf⁢(i,j)PrefSuf𝑖𝑗\textsf{PrefSuf}(i,j)PrefSuf ( italic_i , italic_j ) queries. Theorem 1. For any string T𝑇Titalic_T of length n𝑛nitalic_n over an alphabet Σ=[0,σ)Σ0𝜎\Sigma=[0,\sigma)roman_Σ = [ 0 , italic_σ ) with σ=n𝒪⁢(1)𝜎superscript𝑛𝒪1\sigma=n^{\mathcal{O}(1)}italic_σ = italic_n start_POSTSUPERSCRIPT caligraphic_O ( 1 ) end_POSTSUPERSCRIPT, we can answer PrefSuf⁢(i,j)PrefSuf𝑖𝑗\textsf{PrefSuf}(i,j)PrefSuf ( italic_i , italic_j ) queries, for any i,j∈[0,n)𝑖𝑗0𝑛i,j\in[0,n)italic_i , italic_j ∈ [ 0 , italic_n ), in 𝒪⁢(1)𝒪1\mathcal{O}(1)caligraphic_O ( 1 ) time after an 𝒪⁢(n/logσ⁡n)𝒪𝑛subscript𝜎𝑛\mathcal{O}(n/\log_{\sigma}n)caligraphic_O ( italic_n / roman_log start_POSTSUBSCRIPT italic_σ end_POSTSUBSCRIPT italic_n )-time preprocessing. The data structure size is 𝒪⁢(n/logσ⁡n)𝒪𝑛subscript𝜎𝑛\mathcal{O}(n/\log_{\sigma}n)caligraphic_O ( italic_n / roman_log start_POSTSUBSCRIPT italic_σ end_POSTSUBSCRIPT italic_n ) and the output is given as a compact representation of 𝒪⁢(1)𝒪1\mathcal{O}(1)caligraphic_O ( 1 ) size. In the word RAM model with w𝑤witalic_w-bit machine words and w=Ω⁢(log⁡n)𝑤Ω𝑛w=\Omega(\log n)italic_w = roman_Ω ( roman_log italic_n ), T𝑇Titalic_T is represented as an array: each letter occupies one machine word. However, a single letter can be represented using ⌈log⁡σ⌉𝜎\lceil\log\sigma\rceil⌈ roman_log italic_σ ⌉ bits (i.e., packed representation), which could be (significantly) less than w𝑤witalic_w (e.g., for a constant-sized alphabet). Thus, in the word RAM model, it takes 𝒪⁢(n/logσ⁡n)𝒪𝑛subscript𝜎𝑛\mathcal{O}(n/\log_{\sigma}n)caligraphic_O ( italic_n / roman_log start_POSTSUBSCRIPT italic_σ end_POSTSUBSCRIPT italic_n ) words to store T𝑇Titalic_T and 𝒪⁢(n/logσ⁡n)𝒪𝑛subscript𝜎𝑛\mathcal{O}(n/\log_{\sigma}n)caligraphic_O ( italic_n / roman_log start_POSTSUBSCRIPT italic_σ end_POSTSUBSCRIPT italic_n ) time to read it. Hence Theorem 1 is optimal with respect to the construction time and the query time. Border tree. We start with an informal description of the border tree data structure. The classic KMP algorithm [20] constructs an automaton over string T=T⁢[0⁢..⁢n−1]T=T[0\mathinner{.\,.}n-1]italic_T = italic_T [ 0 start_ATOM . . end_ATOM italic_n - 1 ]. The automaton consists of an initial state and: (1) one state per prefix of T𝑇Titalic_T numbered from 00 to n−1𝑛1n-1italic_n - 1; (2) a success transition from state i𝑖iitalic_i to state i+1𝑖1i+1italic_i + 1; and (3) a failure transition from state i𝑖iitalic_i to the state representing the longest string that is both a prefix and a suffix (known as border) of T⁢[0⁢..⁢i]T[0\mathinner{.\,.}i]italic_T [ 0 start_ATOM . . end_ATOM italic_i ]. The failure transitions form the failure tree. Thus any path in the failure tree from the root to a state i𝑖iitalic_i specifies all the borders of T⁢[0⁢..⁢i]T[0\mathinner{.\,.}i]italic_T [ 0 start_ATOM . . end_ATOM italic_i ]. Since the number of distinct borders of T⁢[0⁢..⁢i]T[0\mathinner{.\,.}i]italic_T [ 0 start_ATOM . . end_ATOM italic_i ] can be Θ⁢(n)Θ𝑛\Theta(n)roman_Θ ( italic_n ), we apply a grouping of the borders based on periodicity that results in the border tree: a compacted version of the failure tree with 𝒪⁢(n)𝒪𝑛\mathcal{O}(n)caligraphic_O ( italic_n ) states and 𝒪⁢(log⁡n)𝒪𝑛\mathcal{O}(\log n)caligraphic_O ( roman_log italic_n ) depth. The construction time is 𝒪⁢(n)𝒪𝑛\mathcal{O}(n)caligraphic_O ( italic_n ) and the size of the data structure is 𝒪⁢(n)𝒪𝑛\mathcal{O}(n)caligraphic_O ( italic_n ). To answer a prefix-suffix query PrefSuf⁢(i,j)PrefSuf𝑖𝑗\textsf{PrefSuf}(i,j)PrefSuf ( italic_i , italic_j ), we use the border tree 𝒯𝒯\mathcal{T}caligraphic_T of T𝑇Titalic_T and the border tree 𝒯Rsuperscript𝒯𝑅\mathcal{T}^{R}caligraphic_T start_POSTSUPERSCRIPT italic_R end_POSTSUPERSCRIPT of TR=T⁢[n−1]⁢…⁢T⁢[0]superscript𝑇𝑅𝑇delimited-[]𝑛1…𝑇delimited-[]0T^{R}=T[n-1]\ldots T[0]italic_T start_POSTSUPERSCRIPT italic_R end_POSTSUPERSCRIPT = italic_T [ italic_n - 1 ] … italic_T [ 0 ]. Given that T𝑇Titalic_T is of a fixed length n𝑛nitalic_n and the border tree has 𝒪⁢(log⁡n)𝒪𝑛\mathcal{O}(\log n)caligraphic_O ( roman_log italic_n ) depth, we need to check 𝒪⁢(log⁡n)𝒪𝑛\mathcal{O}(\log n)caligraphic_O ( roman_log italic_n ) pairs of states: one from 𝒯𝒯\mathcal{T}caligraphic_T and one from 𝒯Rsuperscript𝒯𝑅\mathcal{T}^{R}caligraphic_T start_POSTSUPERSCRIPT italic_R end_POSTSUPERSCRIPT. For each pair, we solve one linear equation in 𝒪⁢(1)𝒪1\mathcal{O}(1)caligraphic_O ( 1 ) time to check for the length constraints. The query time is 𝒪⁢(log⁡n+Occ)𝒪𝑛Occ\mathcal{O}(\log n+\textsf{Occ})caligraphic_O ( roman_log italic_n + Occ ). Gu, Farach, and Beigel [17] used prefix-suffix queries in their dynamic text indexing algorithm to efficiently locate the occurrences of a pattern spanning an edit operation in the text by maintaining the longest prefix and the longest suffix of the pattern occurring right before and right after the edit, respectively. Since its introduction [17], the border tree has been used for several pattern matching tasks (e.g., [14, 3, 4, 6]). Our contribution. We present a completely different and remarkably simple data structure that can be constructed in the optimal 𝒪⁢(n/logσ⁡n)𝒪𝑛subscript𝜎𝑛\mathcal{O}(n/\log_{\sigma}n)caligraphic_O ( italic_n / roman_log start_POSTSUBSCRIPT italic_σ end_POSTSUBSCRIPT italic_n ) time in the word RAM model and answers PrefSuf⁢(i,j)PrefSuf𝑖𝑗\textsf{PrefSuf}(i,j)PrefSuf ( italic_i , italic_j ) queries in the optimal 𝒪⁢(1)𝒪1\mathcal{O}(1)caligraphic_O ( 1 ) time. We remark that it is quite standard to represent the set of occurrences of a string X𝑋Xitalic_X in another string Y𝑌Yitalic_Y with |Y|<2⁢|X|𝑌2𝑋|Y|<2|X|| italic_Y | < 2 | italic_X | in 𝒪⁢(1)𝒪1\mathcal{O}(1)caligraphic_O ( 1 ) space due to the following folklore fact: Fact 1 ([25]). Let X𝑋Xitalic_X and Y𝑌Yitalic_Y be strings with |Y|<2⁢|X|𝑌2𝑋|Y|<2|X|| italic_Y | < 2 | italic_X |. The set of occurrences (starting positions) of X𝑋Xitalic_X in Y𝑌Yitalic_Y forms a single arithmetic progression. However, to the best of our knowledge, the set of occurrences of T𝑇Titalic_T in T⁢[0⁢..⁢i]⁢T⁢[j⁢..⁢n−1]T[0\mathinner{.\,.}i]T[j\mathinner{.\,.}n-1]italic_T [ 0 start_ATOM . . end_ATOM italic_i ] italic_T [ italic_j start_ATOM . . end_ATOM italic_n - 1 ], for all i,j∈[0,n)𝑖𝑗0𝑛i,j\in[0,n)italic_i , italic_j ∈ [ 0 , italic_n ), has not been characterized before. To arrive at Theorem 1, we prove a structural lemma that lets us encode the output of PrefSuf⁢(i,j)PrefSuf𝑖𝑗\textsf{PrefSuf}(i,j)PrefSuf ( italic_i , italic_j ) queries for any i,j∈[0,n)𝑖𝑗0𝑛i,j\in[0,n)italic_i , italic_j ∈ [ 0 , italic_n ) in constant time and space. The border tree, and, in particular prefix-suffix queries, have been mainly used in dynamic text indexing algorithms, which however involve many other crucial primitives to arrive at their final query time. We show here instead a new direct application of our data structure in pattern matching on node-labeled graphs, a very active topic of research [12, 10, 6, 2]. In particular, we formalize bipartite pattern matching as a core problem that underlies any algorithm for pattern matching on node-labeled graphs. For intuition, consider two nodes u𝑢uitalic_u and v𝑣vitalic_v in a directed graph where nodes are labeled by strings. Some suffixes of node u𝑢uitalic_u match some prefixes of a given pattern P𝑃Pitalic_P, and some prefixes of node v𝑣vitalic_v match some suffixes of P𝑃Pitalic_P. We would like to have a data structure, constructed over P𝑃Pitalic_P, that takes 𝒪⁢(1)𝒪1\mathcal{O}(1)caligraphic_O ( 1 ) time to process the directed edge (u,v)𝑢𝑣(u,v)( italic_u , italic_v ). Namely, in this setting, we find all occurrences of P𝑃Pitalic_P spanning at most two nodes of the graph. (It is trivial to find the occurrences of P𝑃Pitalic_P in a single node using any linear-time pattern matching algorithm [20].) Indeed, the bipartite pattern matching problem has been (implicitly) introduced by Ascone et al. [6] for pattern matching on block graphs (a restricted version of node-labeled graphs [22, 13, 26]), and the border tree [17] was used to solve it. If we apply Theorem 1 on P𝑃Pitalic_P, we can answer any such query in 𝒪⁢(1)𝒪1\mathcal{O}(1)caligraphic_O ( 1 ) time instead of 𝒪⁢(log⁡|P|+Occ)𝒪𝑃Occ\mathcal{O}(\log|P|+\textsf{Occ})caligraphic_O ( roman_log | italic_P | + Occ ) time. Paper organization. In Section 2, we present the proof of Theorem 1. In Section 3, we present the application of Theorem 1 on bipartite pattern matching. We conclude this paper in Section 4."
https://arxiv.org/html/2411.03570v1,Learning Constant-Depth Circuits inMalicious Noise Models,"The seminal work of Linial, Mansour, and Nisan gave a quasipolynomial-time algorithm for learning constant-depth circuits (𝖠𝖢0superscript𝖠𝖢0\mathsf{AC}^{0}sansserif_AC start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT) with respect to the uniform distribution on the hypercube. Extending their algorithm to the setting of malicious noise, where both covariates and labels can be adversarially corrupted, has remained open. Here we achieve such a result, inspired by recent work on learning with distribution shift. Our running time essentially matches their algorithm, which is known to be optimal assuming various cryptographic primitives.Our proof uses a simple outlier-removal method combined with Braverman’s theorem for fooling constant-depth circuits. We attain the best possible dependence on the noise rate and succeed in the harshest possible noise model (i.e., contamination or so-called “nasty noise”).","In their famous paper, Linial, Mansour, and Nisan [LMN93] introduced the “low-degree” algorithm for learning Boolean functions with respect to the uniform distribution on {±1}dsuperscriptplus-or-minus1𝑑\{\pm 1\}^{d}{ ± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT. The running time and sample complexity of their algorithm scales in terms of the Fourier concentration of the underlying concept class, and, using this framework, they obtained a quasipolynomial-time algorithm for learning constant-depth, polynomial-size circuits (𝖠𝖢0superscript𝖠𝖢0\mathsf{AC}^{0}sansserif_AC start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT). Prior work [KKMS08] had extended their result to the agnostic setting, where the labels can be adversarially corrupted, but the marginal distribution on inputs must still be uniform over {±1}dsuperscriptplus-or-minus1𝑑\{\pm 1\}^{d}{ ± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT. Remarkably, there had been no progress on this problem in the last three decades for malicious noise models where both covariates and labels can be adversarially corrupted [Val85, KL93]. In this paper, we completely resolve this problem and obtain a quasipolynomial-time algorithm for learning 𝖠𝖢0superscript𝖠𝖢0\mathsf{AC}^{0}sansserif_AC start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT in the harshest possible noise model, the so-called “nasty noise” model of [BEK02]. We define this model below and refer to it simply as learning with contamination, in line with recent work in computationally efficient robust statistics (see e.g., [DK23]). Definition 1.1 (Learning from Contaminated Samples). A set of N𝑁Nitalic_N labeled examples S¯inpsubscript¯𝑆inp\bar{S}_{\mathrm{inp}}over¯ start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_inp end_POSTSUBSCRIPT is an η𝜂\etaitalic_η-contaminated (uniform) sample with respect to some class 𝒞⊆{{±1}d→{±1}}𝒞→superscriptplus-or-minus1𝑑plus-or-minus1\mathcal{C}\subseteq\{\{\pm 1\}^{d}\to\{\pm 1\}\}caligraphic_C ⊆ { { ± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT → { ± 1 } }, where N∈ℕ𝑁ℕN\in\mathbb{N}italic_N ∈ blackboard_N and η∈(0,1)𝜂01\eta\in(0,1)italic_η ∈ ( 0 , 1 ), if it is formed by an adversary as follows. 1. The adversary receives a set of N𝑁Nitalic_N clean i.i.d. labeled examples S¯clnsubscript¯𝑆cln\bar{S}_{\mathrm{cln}}over¯ start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_cln end_POSTSUBSCRIPT, drawn from the uniform distribution over {±1}dsuperscriptplus-or-minus1𝑑\{\pm 1\}^{d}{ ± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT and labeled by some unknown concept f∗superscript𝑓f^{*}italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT in 𝒞𝒞\mathcal{C}caligraphic_C. 2. The adversary removes an arbitrary set S¯remsubscript¯𝑆rem\bar{S}_{\mathrm{rem}}over¯ start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_rem end_POSTSUBSCRIPT of ⌊η⁢N⌋𝜂𝑁\lfloor\eta N\rfloor⌊ italic_η italic_N ⌋ labeled examples from S¯clnsubscript¯𝑆cln\bar{S}_{\mathrm{cln}}over¯ start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_cln end_POSTSUBSCRIPT and substitutes it with an adversarial set of ⌊η⁢N⌋𝜂𝑁\lfloor\eta N\rfloor⌊ italic_η italic_N ⌋ labeled examples S¯advsubscript¯𝑆adv\bar{S}_{\mathrm{adv}}over¯ start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_adv end_POSTSUBSCRIPT. Namely, S¯inp=(S¯cln∖S¯rem)∪S¯advsubscript¯𝑆inpsubscript¯𝑆clnsubscript¯𝑆remsubscript¯𝑆adv\bar{S}_{\mathrm{inp}}=(\bar{S}_{\mathrm{cln}}\setminus\bar{S}_{\mathrm{rem}})% \cup\bar{S}_{\mathrm{adv}}over¯ start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_inp end_POSTSUBSCRIPT = ( over¯ start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_cln end_POSTSUBSCRIPT ∖ over¯ start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_rem end_POSTSUBSCRIPT ) ∪ over¯ start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_adv end_POSTSUBSCRIPT. For the corresponding unlabeled set Sinpsubscript𝑆inpS_{\mathrm{inp}}italic_S start_POSTSUBSCRIPT roman_inp end_POSTSUBSCRIPT, we say that it is an η𝜂\etaitalic_η-contaminated (uniform) sample. In this model, the goal of the learner is to output (with probability 1−δ1𝛿1-\delta1 - italic_δ) a hypothesis h:{±1}d→{±1}:ℎ→superscriptplus-or-minus1𝑑plus-or-minus1h:\{\pm 1\}^{d}\to\{\pm 1\}italic_h : { ± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT → { ± 1 } such that ℙ𝐱∼Unif⁢({±1}d)[h⁢(𝐱)≠f∗⁢(𝐱)]≤2⁢η+ϵsubscriptℙsimilar-to𝐱Unifsuperscriptplus-or-minus1𝑑ℎ𝐱superscript𝑓𝐱2𝜂italic-ϵ\operatorname*{\mathbb{P}}_{\mathbf{x}\sim\mathrm{Unif}(\{\pm 1\}^{d})}[h(% \mathbf{x})\neq f^{*}(\mathbf{x})]\leq 2\eta+\epsilonblackboard_P start_POSTSUBSCRIPT bold_x ∼ roman_Unif ( { ± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) end_POSTSUBSCRIPT [ italic_h ( bold_x ) ≠ italic_f start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( bold_x ) ] ≤ 2 italic_η + italic_ϵ. The factor 2222 is known to be the best possible constant achievable by any algorithm [BEK02]. Although there is now a long line of research giving computationally efficient algorithms for learning Boolean function classes in malicious noise models, these algorithms primarily apply to geometric concept classes and continuous marginal distributions, such as halfspaces or intersections of halfspaces with respect to Gaussian or log-concave densities [KKMS08, KLS09, ABL17, DKS18, SZ21]. In particular, nothing was known for the case of 𝖠𝖢𝟢superscript𝖠𝖢0\mathsf{AC^{0}}sansserif_AC start_POSTSUPERSCRIPT sansserif_0 end_POSTSUPERSCRIPT. Our main theorem is as follows: Theorem 1.2. For any s,ℓ,d∈ℕ𝑠ℓ𝑑ℕs,\ell,d\in{\mathbb{N}}italic_s , roman_ℓ , italic_d ∈ blackboard_N, and ϵ,δ∈(0,1)italic-ϵ𝛿01\epsilon,\delta\in(0,1)italic_ϵ , italic_δ ∈ ( 0 , 1 ), there is an algorithm that learns the class of 𝖠𝖢0superscript𝖠𝖢0\mathsf{AC}^{0}sansserif_AC start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT circuits of size s𝑠sitalic_s and depth ℓℓ\ellroman_ℓ and achieves error 2⁢η+ϵ2𝜂italic-ϵ2\eta+\epsilon2 italic_η + italic_ϵ, with running time and sample complexity dO⁢(k)⁢log⁡(1/δ)superscript𝑑𝑂𝑘1𝛿d^{O(k)}\log(1/\delta)italic_d start_POSTSUPERSCRIPT italic_O ( italic_k ) end_POSTSUPERSCRIPT roman_log ( 1 / italic_δ ), where k=(log⁡(s))O⁢(ℓ)⁢log⁡(1/ϵ)𝑘superscript𝑠𝑂ℓ1italic-ϵk={(\log(s))^{O(\ell)}\log(1/\epsilon)}italic_k = ( roman_log ( italic_s ) ) start_POSTSUPERSCRIPT italic_O ( roman_ℓ ) end_POSTSUPERSCRIPT roman_log ( 1 / italic_ϵ ), from contaminated samples of any noise rate η𝜂\etaitalic_η. Our running time essentially matches the Linial, Mansour, and Nisan result, which is known to be optimal assuming various cryptographic primitives [Kha95]. More generally, we prove that any concept class 𝒞𝒞\mathcal{C}caligraphic_C that admits ℓ1subscriptℓ1\ell_{1}roman_ℓ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-sandwiching polynomials of degree k𝑘kitalic_k can be learned in time dO⁢(k)superscript𝑑𝑂𝑘d^{O(k)}italic_d start_POSTSUPERSCRIPT italic_O ( italic_k ) end_POSTSUPERSCRIPT from contaminated samples. Recent work due to [GSSV24] had obtained a similar result achieving the weaker bound of O⁢(η)+ϵ𝑂𝜂italic-ϵO(\eta)+\epsilonitalic_O ( italic_η ) + italic_ϵ for learning functions with ℓ2subscriptℓ2\ell_{2}roman_ℓ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT-sandwiching polynomials. Crucially, it remains unclear how to obtain such ℓ2subscriptℓ2\ell_{2}roman_ℓ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT sandwiching approximators for constant depth circuits 111Braverman’s celebrated result on 𝖠𝖢𝟢superscript𝖠𝖢0\mathsf{AC^{0}}sansserif_AC start_POSTSUPERSCRIPT sansserif_0 end_POSTSUPERSCRIPT [Bra08] obtains only ℓ1subscriptℓ1\ell_{1}roman_ℓ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-sandwiching., and so their result does not apply here. In 2005, Kalai et al. [KKMS08] showed that ℓ1subscriptℓ1\ell_{1}roman_ℓ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-approximation suffices for agnostic learning. Here we complete the analogy for malicious learning, showing that ℓ1subscriptℓ1\ell_{1}roman_ℓ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-sandwiching implies learnability with respect to contamination. Proof Overview. The input set S¯inpsubscript¯𝑆inp\bar{S}_{\mathrm{inp}}over¯ start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_inp end_POSTSUBSCRIPT is η𝜂\etaitalic_η-contaminated. This might make it hard to find a hypothesis with near-optimal error on S¯inpsubscript¯𝑆inp\bar{S}_{\mathrm{inp}}over¯ start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_inp end_POSTSUBSCRIPT. However, we are only interested in finding a hypothesis with error 2⁢η+ϵ2𝜂italic-ϵ2\eta+\epsilon2 italic_η + italic_ϵ on the clean distribution, which is structured (in particular, the marginal distribution on the features is uniform over {±1}dsuperscriptplus-or-minus1𝑑\{\pm 1\}^{d}{ ± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT). In order to take advantage of the structure of the clean distribution despite only having access to the contaminated sample, we make use of the notion of sandwiching polynomials: Definition 1.3 (Sandwiching polynomials). Let f:{±1}d→{±1}:𝑓→superscriptplus-or-minus1𝑑plus-or-minus1f:\{\pm 1\}^{d}\to\{\pm 1\}italic_f : { ± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT → { ± 1 }. We say that the (ℓ1subscriptℓ1\ell_{1}roman_ℓ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT) ϵitalic-ϵ\epsilonitalic_ϵ-sandwiching degree of f𝑓fitalic_f with respect to the uniform distribution over the hypercube {±1}dsuperscriptplus-or-minus1𝑑\{\pm 1\}^{d}{ ± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT is k𝑘kitalic_k if there are polynomials pup,pdown:{±1}d→ℝ:subscript𝑝upsubscript𝑝down→superscriptplus-or-minus1𝑑ℝp_{\mathrm{up}},p_{\mathrm{down}}:\{\pm 1\}^{d}\to{\mathbb{R}}italic_p start_POSTSUBSCRIPT roman_up end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT roman_down end_POSTSUBSCRIPT : { ± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT → blackboard_R of degree at most k𝑘kitalic_k such that (1) pdown⁢(𝐱)≤f⁢(𝐱)≤pup⁢(𝐱)subscript𝑝down𝐱𝑓𝐱subscript𝑝up𝐱p_{\mathrm{down}}(\mathbf{x})\leq f(\mathbf{x})\leq p_{\mathrm{up}}(\mathbf{x})italic_p start_POSTSUBSCRIPT roman_down end_POSTSUBSCRIPT ( bold_x ) ≤ italic_f ( bold_x ) ≤ italic_p start_POSTSUBSCRIPT roman_up end_POSTSUBSCRIPT ( bold_x ) for all 𝐱∈{±1}d𝐱superscriptplus-or-minus1𝑑\mathbf{x}\in\{\pm 1\}^{d}bold_x ∈ { ± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT and (2) 𝔼𝐱∼Unif⁡({±1}d)[pup⁢(𝐱)−pdown⁢(𝐱)]≤ϵsubscript𝔼similar-to𝐱Unifsuperscriptplus-or-minus1𝑑subscript𝑝up𝐱subscript𝑝down𝐱italic-ϵ\operatorname*{\mathbb{E}}_{\mathbf{x}\sim\operatorname{Unif}(\{\pm 1\}^{d})}[% p_{\mathrm{up}}(\mathbf{x})-p_{\mathrm{down}}(\mathbf{x})]\leq\epsilonblackboard_E start_POSTSUBSCRIPT bold_x ∼ roman_Unif ( { ± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) end_POSTSUBSCRIPT [ italic_p start_POSTSUBSCRIPT roman_up end_POSTSUBSCRIPT ( bold_x ) - italic_p start_POSTSUBSCRIPT roman_down end_POSTSUBSCRIPT ( bold_x ) ] ≤ italic_ϵ. The sandwiching degree of size-s𝑠sitalic_s depth-ℓℓ\ellroman_ℓ 𝖠𝖢0superscript𝖠𝖢0\mathsf{AC}^{0}sansserif_AC start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT circuits is bounded by k=(log⁡(s))O⁢(ℓ)⁢log⁡(1/ϵ)𝑘superscript𝑠𝑂ℓ1italic-ϵk=(\log(s))^{O(\ell)}\log(1/\epsilon)italic_k = ( roman_log ( italic_s ) ) start_POSTSUPERSCRIPT italic_O ( roman_ℓ ) end_POSTSUPERSCRIPT roman_log ( 1 / italic_ϵ ), due to the result of Braverman on fooling constant-depth circuits (see Theorem 4.2 from [Bra08, Tal17, HS19]). Suppose that S¯¯𝑆\bar{S}over¯ start_ARG italic_S end_ARG is a subset of S¯inpsubscript¯𝑆inp\bar{S}_{\mathrm{inp}}over¯ start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_inp end_POSTSUBSCRIPT that preserves the expectations of low-degree and non-negative polynomials (e.g., pup−pdownsubscript𝑝upsubscript𝑝downp_{\mathrm{up}}-p_{\mathrm{down}}italic_p start_POSTSUBSCRIPT roman_up end_POSTSUBSCRIPT - italic_p start_POSTSUBSCRIPT roman_down end_POSTSUBSCRIPT) compared to the uniform distribution. Under this condition, low-degree polynomial regression gives a hypothesis with near-optimal error on S¯¯𝑆\bar{S}over¯ start_ARG italic_S end_ARG (see Section 4). We show in Lemma 3.1 that a simple procedure that iteratively removes samples from S¯inpsubscript¯𝑆inp\bar{S}_{\mathrm{inp}}over¯ start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_inp end_POSTSUBSCRIPT can be used to form such a set S¯¯𝑆\bar{S}over¯ start_ARG italic_S end_ARG (that preserves the expectations of non-negative, degree-k𝑘kitalic_k and low-expectation polynomials) and, moreover, this procedure removes more contaminated points than clean points. The last property is important, because it implies that S¯¯𝑆\bar{S}over¯ start_ARG italic_S end_ARG is representative for the ground truth distribution, i.e., any near-optimal hypothesis for S¯¯𝑆\bar{S}over¯ start_ARG italic_S end_ARG will also have error 2⁢η+ϵ2𝜂italic-ϵ2\eta+\epsilon2 italic_η + italic_ϵ on the ground truth. This is possible because the only way the adversary can significantly increase the expectation of a non-negative polynomial p𝑝pitalic_p is by inserting examples 𝐱𝐱\mathbf{x}bold_x where p⁢(𝐱)𝑝𝐱p(\mathbf{x})italic_p ( bold_x ) is unreasonably large compared to the typical values of p𝑝pitalic_p over the uniform distribution. Our algorithm iteratively finds the non-negative polynomial q𝑞qitalic_q with the largest expectation over a given set through a simple linear program and then removes the points 𝐱𝐱\mathbf{x}bold_x for which q⁢(x)𝑞𝑥q(x)italic_q ( italic_x ) is large. Our iterative outlier removal procedure is inspired by prior work on TDS learning (Testable Learning with Distribution Shift) and PQ learning [KSV24, GSSV24] as well as the work of [DKS18] on learning geometric concepts from contaminated examples. Both of these works use outlier removal procedures that give bounds on the variance of polynomials rather than the expectation of non-negative polynomials and, instead of linear programming, they use spectral algorithms."
https://arxiv.org/html/2411.03451v1,Redundancy Is All You Need,"The seminal work of Benczúr and Karger demonstrated cut sparsifiers of near-linear size, with several applications throughout theoretical computer science. Subsequent extensions have yielded sparsifiers for hypergraph cuts and more recently linear codes over Abelian groups. A decade ago, Kogan and Krauthgamer asked about the sparsifiability of arbitrary constraint satisfaction problems (CSPs). For this question, a trivial lower bound is the size of a non-redundant CSP instance, which admits, for each constraint, an assignment satisfying only that constraint (so that no constraint can be dropped by the sparsifier). For instance, for graph cuts, spanning trees are non-redundant instances.Our main result is that redundant clauses are sufficient for sparsification: for any CSP predicate R𝑅Ritalic_R, every unweighted instance of CSP⁡(R)CSP𝑅\operatorname{CSP}(R)roman_CSP ( italic_R ) has a sparsifier of size at most its non-redundancy (up to polylog factors). For weighted instances, we similarly pin down the sparsifiability to the so-called chain length of the predicate. These results precisely determine the extent to which any CSP can be sparsified. A key technical ingredient in our work is a novel application of the entropy method from Gilmer’s recent breakthrough on the union-closed sets conjecture.As an immediate consequence of our main theorem, a number of results in the non-redundancy literature immediately extend to CSP sparsification. We also contribute new techniques for understanding the non-redundancy of CSP predicates. In particular, we give an explicit family of predicates whose non-redundancy roughly corresponds to the structure of matching vector families in coding theory. By adapting methods from the matching vector codes literature, we are able to construct an explicit predicate whose non-redundancy lies between Ω⁢(n1.5)Ωsuperscript𝑛1.5\Omega(n^{1.5})roman_Ω ( italic_n start_POSTSUPERSCRIPT 1.5 end_POSTSUPERSCRIPT ) and O~⁢(n1.6)~𝑂superscript𝑛1.6\widetilde{O}(n^{1.6})over~ start_ARG italic_O end_ARG ( italic_n start_POSTSUPERSCRIPT 1.6 end_POSTSUPERSCRIPT ), the first example with a provably non-integral exponent.","The broad goal in sparsification is to replace an object by a more compact surrogate, typically a carefully chosen subsample, that preserves the behavior of the object under some metric of interest. For instance, for preserving cuts in undirected graphs, the influential works of Karger [Kar93] and Benczúr and Karger [BK96] showed that every graph has an edge-weighted subgraph with near-linear number of edges that preserves the value of all (edge) cuts up to a (1±ϵ)plus-or-minus1italic-ϵ(1\pm\epsilon)( 1 ± italic_ϵ ) multiplicative factor. These papers have had a substantial impact in shaping the last thirty years of work in areas such as spectral sparsifiers [ST11, BSS12, LS18], clustering [KVV04, SPR11], hypergraph sparsifiers [KK15, CKN20, KKTY21, KK23, KPS24c], linear solvers [ST04, Vis13, KMP14], convex optimization [LS14, AK16, Tod16], sketching/streaming algorithms [AG09, AGM12b, AGM12a, ACK+16, McG14, KLM+17, BHM+21], max-flow/min-cut algorithms [LR99, CKM+11, KLOS14, CKL+22], machine learning [LCY+21, CSZ22, ZSW+23, GBY+24], submodular functions [KK23, Sch24, Raf24, Qua24], differential privacy [BBDS12, AU19], PageRank [Chu14], and even theoretical physics [HKTH16, Van18, TN22], among many other works. Among the multiple exciting dimensions in which cut sparsification has been generalized, we now highlight two which form the backdrop for our work. Note that the graph cut problem can be modeled by the arity-two Boolean constraint x+y=1(mod2)𝑥𝑦annotated1pmod2x+y=1\pmod{2}italic_x + italic_y = 1 start_MODIFIER ( roman_mod start_ARG 2 end_ARG ) end_MODIFIER. One can thus generalize cut sparsification by allowing for arbitrary constraints (of any arity over some finite domain) as considered in the field of constraint satisfaction problems (CSPs), leading to CSP sparsification. This direction was proposed by Kogan and Krauthgamer [KK15] in their work on hypergraph cut sparsifiers, where the not-all-equal constraint captures hypergraph cut. As as special case, arbitrary binary CSPs (where each constraint has two variables) were studied in [FK17] for the Boolean domain and in [BŽ20] for general domains, leading to a dichotomy: either near-linear sized sparsifiers exist, or no improvement over quadratic is possible. In another direction, one can instead look toward more general structures to sparsify. For instance, a recent line of work by Khanna, Putterman, and Sudan turned toward sparsifying linear codes [KPS24a], or more generally subgroups of powers of Abelian groups [KPS24b]. Beyond being algorithmically efficient [KPS24b], these structural results have led to exciting new results in CSP sparsification by constructing optimal sparsifiers when the constraints can be embedded into linear/Abelian equations. In this work, we obtain sparsifiers encompassing both these generalizations via a unified approach to sparsification of non-linear codes. The resulting sparsifiers for CSPs have optimal asymptotic size up to polylogarithmic factors, for every choice of predicate defining the CSP. In other words, we pinpoint the optimal extent to which an arbitrary CSP can be sparsified.111In this work we focus on the existence of sparsifiers, which is already highly non-trivial (e.g., [KPS24a, BŽ20] are also non-algorithmic). Future directions (and barriers) for algorithmic aspects are briefly discussed in Sections 1.7 and 9. 1.1 Non-linear code sparsification We first state our result for codes as it is very general and crisply stated, and then turn to the consequences and further new results for CSPs. For a non-linear code C⊆{0,1}m𝐶superscript01𝑚C\subseteq\{0,1\}^{m}italic_C ⊆ { 0 , 1 } start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT, an ε𝜀\varepsilonitalic_ε-sparsifier (for a parameter ε∈(0,1)𝜀01\varepsilon\in(0,1)italic_ε ∈ ( 0 , 1 )) is a weight function w:[m]→ℝ≥0:𝑤→delimited-[]𝑚subscriptℝabsent0w:[m]\to\mathbb{R}_{\geq 0}italic_w : [ italic_m ] → blackboard_R start_POSTSUBSCRIPT ≥ 0 end_POSTSUBSCRIPT such that for every codeword c𝑐citalic_c, adding up the weights of its nonzero positions, i.e., ∑iw⁢(i)⁢cisubscript𝑖𝑤𝑖subscript𝑐𝑖\sum_{i}w(i)c_{i}∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_w ( italic_i ) italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, is an accurate estimate of the Hamming weight of c𝑐citalic_c (i.e., ∑icisubscript𝑖subscript𝑐𝑖\sum_{i}c_{i}∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT) to within a (1±ε)plus-or-minus1𝜀(1\pm\varepsilon)( 1 ± italic_ε ) multiplicative factor (Definition 2.8). The goal is to minimize the support of w𝑤witalic_w (i.e., the number of nonzero entries w⁢(i)𝑤𝑖w(i)italic_w ( italic_i )), and the minimum value is called ε𝜀\varepsilonitalic_ε-sparsifiability of C𝐶Citalic_C and is denoted SPR⁡(C,ε)SPR𝐶𝜀\operatorname{SPR}(C,\varepsilon)roman_SPR ( italic_C , italic_ε ). One of our main results is an upper bound on the sparsifiability in terms of a natural combinatorial parameter of the code called its non-redundancy NRD⁡(C)NRD𝐶\operatorname{NRD}(C)roman_NRD ( italic_C ), defined as follows: NRD⁡(C)NRD𝐶\operatorname{NRD}(C)roman_NRD ( italic_C ) is the size of the largest subset of indices I⊆[m]𝐼delimited-[]𝑚I\subseteq[m]italic_I ⊆ [ italic_m ] such that for each i∈I𝑖𝐼i\in Iitalic_i ∈ italic_I, there is a codeword c∈C𝑐𝐶c\in Citalic_c ∈ italic_C with ci=1subscript𝑐𝑖1c_{i}=1italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1 and ci′=0subscript𝑐superscript𝑖′0c_{i^{\prime}}=0italic_c start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT = 0 for i′∈I∖{i}superscript𝑖′𝐼𝑖i^{\prime}\in I\setminus\{i\}italic_i start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ∈ italic_I ∖ { italic_i }. In other words, if we imagine the code as a matrix whose rows are codewords, its non-redundancy is largest square submatrix which is a permutation matrix. Our result can then be stated compactly as follows. Theorem 1.1 (Main). For all C⊆{0,1}m𝐶superscript01𝑚C\subseteq\{0,1\}^{m}italic_C ⊆ { 0 , 1 } start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT and ε∈(0,1)𝜀01\varepsilon\in(0,1)italic_ε ∈ ( 0 , 1 ), SPR⁡(C,ε)=O⁢(NRD⁡(C)⁢(log⁡m)6/ε2).SPR𝐶𝜀𝑂NRD𝐶superscript𝑚6superscript𝜀2\operatorname{SPR}(C,\varepsilon)=O(\operatorname{NRD}(C)(\log m)^{6}/% \varepsilon^{2}).roman_SPR ( italic_C , italic_ε ) = italic_O ( roman_NRD ( italic_C ) ( roman_log italic_m ) start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT / italic_ε start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) . To see how our theorem generalizes linear code sparsification [KPS24a, KPS24b], let 𝔽𝔽\mathbb{F}blackboard_F be a (finite) field and let V⊆𝔽m𝑉superscript𝔽𝑚V\subseteq\mathbb{F}^{m}italic_V ⊆ blackboard_F start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT be a subspace. Let C:={(𝟏⁢[v1=0],…,𝟏⁢[vm=0]):v∈V}assign𝐶conditional-set1delimited-[]subscript𝑣10…1delimited-[]subscript𝑣𝑚0𝑣𝑉C:=\{({\bf 1}[v_{1}=0],\ldots,{\bf 1}[v_{m}=0]):v\in V\}italic_C := { ( bold_1 [ italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0 ] , … , bold_1 [ italic_v start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT = 0 ] ) : italic_v ∈ italic_V } be the zero/non-zero pattern of V𝑉Vitalic_V. Then, any ε𝜀\varepsilonitalic_ε-sparsifier of C𝐶Citalic_C is an ε𝜀\varepsilonitalic_ε-sparsifier of V𝑉Vitalic_V and NRD⁡(C)=dimVNRD𝐶dimension𝑉\operatorname{NRD}(C)=\dim Vroman_NRD ( italic_C ) = roman_dim italic_V. In fact, for any finite group G𝐺Gitalic_G and subgroup H≤Gm𝐻superscript𝐺𝑚H\leq G^{m}italic_H ≤ italic_G start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT, we can show for the corresponding C𝐶Citalic_C that NRD⁡(C)≤log2⁡|H|NRD𝐶subscript2𝐻\operatorname{NRD}(C)\leq\log_{2}|H|roman_NRD ( italic_C ) ≤ roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | italic_H | (see Theorem 5.1), matching [KPS24b]’s result for Abelian groups (modulo their efficiency). If we view C⊆{0,1}m𝐶superscript01𝑚C\subseteq\{0,1\}^{m}italic_C ⊆ { 0 , 1 } start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT as a set family over the universe [m]delimited-[]𝑚[m][ italic_m ], the above result in effect says that the non-redundancy NRD⁡(C)NRD𝐶\operatorname{NRD}(C)roman_NRD ( italic_C ) plays the role of the VC dimension of C𝐶Citalic_C when the goal is to estimate the size of the set c∈C𝑐𝐶c\in Citalic_c ∈ italic_C rather than learn c𝑐citalic_c itself. In fact, it turns out that NRD⁡(C)NRD𝐶\operatorname{NRD}(C)roman_NRD ( italic_C ) is precisely the VC dimension of the union-closure of C𝐶Citalic_C. This connection to union-closed families plays a crucial role in the proof of Theorem 1.1. See the technical overview (Section 1.6) for more details, including discussion of a significantly simpler O~ε⁢(NRD⁡(C)⁢log⁡|C|)subscript~𝑂𝜀NRD𝐶𝐶\widetilde{O}_{\varepsilon}(\operatorname{NRD}(C)\log|C|)over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_ε end_POSTSUBSCRIPT ( roman_NRD ( italic_C ) roman_log | italic_C | )-sized sparsifier. 1.2 CSP sparsification We now turn to (unweighted222The weighted case is discussed in Section 1.4.) CSP sparsification. For a relation R⊆Dr𝑅superscript𝐷𝑟R\subseteq D^{r}italic_R ⊆ italic_D start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT of arity r𝑟ritalic_r over a finite domain D𝐷Ditalic_D, an instance ΨΨ\Psiroman_Ψ of the CSP⁡(R)CSP𝑅\operatorname{CSP}(R)roman_CSP ( italic_R ) problem consists a variable set X𝑋Xitalic_X and a constraint set Y⊆Xr𝑌superscript𝑋𝑟Y\subseteq X^{r}italic_Y ⊆ italic_X start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT. An assignment σ:X→D:𝜎→𝑋𝐷\sigma:X\to Ditalic_σ : italic_X → italic_D satisfies a constraint y=(x1,x2,…,xr)∈Y𝑦subscript𝑥1subscript𝑥2…subscript𝑥𝑟𝑌y=(x_{1},x_{2},\dots,x_{r})\in Yitalic_y = ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ) ∈ italic_Y if (σ⁢(x1),σ⁢(x2),…,σ⁢(xr))∈R𝜎subscript𝑥1𝜎subscript𝑥2…𝜎subscript𝑥𝑟𝑅(\sigma(x_{1}),\sigma(x_{2}),\dots,\sigma(x_{r}))\in R( italic_σ ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , italic_σ ( italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) , … , italic_σ ( italic_x start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ) ) ∈ italic_R. The value val⁡(Ψ,σ)valΨ𝜎\operatorname{val}(\Psi,\sigma)roman_val ( roman_Ψ , italic_σ ) of an assignment σ𝜎\sigmaitalic_σ is the number of constraints y∈Y𝑦𝑌y\in Yitalic_y ∈ italic_Y that it satisfies. Similarly, for a weight function w:Y→ℝ≥0:𝑤→𝑌subscriptℝabsent0w:Y\to\mathbb{R}_{\geq 0}italic_w : italic_Y → blackboard_R start_POSTSUBSCRIPT ≥ 0 end_POSTSUBSCRIPT, the weighted value val⁡(Ψ,w,σ)valΨ𝑤𝜎\operatorname{val}(\Psi,w,\sigma)roman_val ( roman_Ψ , italic_w , italic_σ ) is the sum of weights w⁢(y)𝑤𝑦w(y)italic_w ( italic_y ) of all constraints y∈Y𝑦𝑌y\in Yitalic_y ∈ italic_Y that σ𝜎\sigmaitalic_σ satisfies. The goal in CSP sparsification is to output a weight function w:Y→ℝ≥0:𝑤→𝑌subscriptℝabsent0w:Y\to\mathbb{R}_{\geq 0}italic_w : italic_Y → blackboard_R start_POSTSUBSCRIPT ≥ 0 end_POSTSUBSCRIPT of small support, such that for every assignment σ:X→D:𝜎→𝑋𝐷\sigma:X\to Ditalic_σ : italic_X → italic_D, (1−ε)⁢val⁡(Ψ,σ)≤val⁡(Ψ,w,σ)≤(1+ε)⁢val⁡(Ψ,σ),1𝜀valΨ𝜎valΨ𝑤𝜎1𝜀valΨ𝜎(1-\varepsilon)\operatorname{val}(\Psi,\sigma)\leq\operatorname{val}(\Psi,w,% \sigma)\leq(1+\varepsilon)\operatorname{val}(\Psi,\sigma)\ ,( 1 - italic_ε ) roman_val ( roman_Ψ , italic_σ ) ≤ roman_val ( roman_Ψ , italic_w , italic_σ ) ≤ ( 1 + italic_ε ) roman_val ( roman_Ψ , italic_σ ) , and minimum such support size is denoted SPR⁡(Ψ,ε)SPRΨ𝜀\operatorname{SPR}(\Psi,\varepsilon)roman_SPR ( roman_Ψ , italic_ε ). The ε𝜀\varepsilonitalic_ε-sparsifiability of the relation R⊆Dr𝑅superscript𝐷𝑟R\subseteq D^{r}italic_R ⊆ italic_D start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT, as a function of number of variables, is defined to the maximum (i.e., worst-case) value of SPR⁡(Ψ,ε)SPRΨ𝜀\operatorname{SPR}(\Psi,\varepsilon)roman_SPR ( roman_Ψ , italic_ε ) over all n𝑛nitalic_n-variables instances ΨΨ\Psiroman_Ψ of CSP⁡(R)CSP𝑅\operatorname{CSP}(R)roman_CSP ( italic_R ). We denote it by SPR⁡(R,n,ε)SPR𝑅𝑛𝜀\operatorname{SPR}(R,n,\varepsilon)roman_SPR ( italic_R , italic_n , italic_ε ) and it is the chief object of our study. Note that this is for the unweighted case, see Section 1.4 how this result can be (tightly) applied to the weighted case. Let us note an obvious obstruction to sparsification. Suppose we have an instance Ψ=(X,Y)Ψ𝑋𝑌\Psi=(X,Y)roman_Ψ = ( italic_X , italic_Y ) of CSP⁡(R)CSP𝑅\operatorname{CSP}(R)roman_CSP ( italic_R ) such that for each of its constraints y∈Y𝑦𝑌y\in Yitalic_y ∈ italic_Y, there is an assignment σy:X→D:subscript𝜎𝑦→𝑋𝐷\sigma_{y}:X\to Ditalic_σ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT : italic_X → italic_D that satisfies only y𝑦yitalic_y and no other constraint. Then clearly ΨΨ\Psiroman_Ψ cannot be sparsified at all—dropping any constraint y𝑦yitalic_y would make the value of σysubscript𝜎𝑦\sigma_{y}italic_σ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT drop from 1111 to 00. We call such an instance a non-redundant instance of CSP⁡(R¯)CSP¯𝑅\operatorname{CSP}(\overline{R})roman_CSP ( over¯ start_ARG italic_R end_ARG ), where R¯=Dr∖R¯𝑅superscript𝐷𝑟𝑅\overline{R}=D^{r}\setminus Rover¯ start_ARG italic_R end_ARG = italic_D start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ∖ italic_R (cf., [BCH+13, BCK20]).333We use R¯¯𝑅\overline{R}over¯ start_ARG italic_R end_ARG rather than R𝑅Ritalic_R due to the conventions of each community. See Remark 2.6 for deeper technical reasons. As introduced by Bessiere, Carbonnel, and Katsirelos [BCK20], we denote the size of the largest such non-redundant instance of CSP⁡(R¯)CSP¯𝑅\operatorname{CSP}(\overline{R})roman_CSP ( over¯ start_ARG italic_R end_ARG ) on n𝑛nitalic_n-variables by NRD⁡(R¯,n)NRD¯𝑅𝑛\operatorname{NRD}(\overline{R},n)roman_NRD ( over¯ start_ARG italic_R end_ARG , italic_n ) and call it the non-redundancy of R¯¯𝑅\overline{R}over¯ start_ARG italic_R end_ARG. Thus a trivial lower bound on sparsifiability of CSP⁡(R)CSP𝑅\operatorname{CSP}(R)roman_CSP ( italic_R ), regardless of the choice of ε∈(0,1)𝜀01\varepsilon\in(0,1)italic_ε ∈ ( 0 , 1 ), is given by SPR⁡(R,n,ε)≥NRD⁡(R¯,n),SPR𝑅𝑛𝜀NRD¯𝑅𝑛\displaystyle\operatorname{SPR}(R,n,\varepsilon)\geq\operatorname{NRD}(% \overline{R},n)\ ,roman_SPR ( italic_R , italic_n , italic_ε ) ≥ roman_NRD ( over¯ start_ARG italic_R end_ARG , italic_n ) , (1) and this holds even if the goal is merely to preserve which assignments have nonzero value. Rather remarkably, this simplistic lower bound can be met and one can sparsify all the way down to NRD⁡(R¯,n)NRD¯𝑅𝑛\operatorname{NRD}(\overline{R},n)roman_NRD ( over¯ start_ARG italic_R end_ARG , italic_n ) times polylogarithmic factors! In fact, this turns out to be an easy corollary of Theorem 1.1. One can associate a canonical code CΨ⊆{0,1}Ysubscript𝐶Ψsuperscript01𝑌C_{\Psi}\subseteq\{0,1\}^{Y}italic_C start_POSTSUBSCRIPT roman_Ψ end_POSTSUBSCRIPT ⊆ { 0 , 1 } start_POSTSUPERSCRIPT italic_Y end_POSTSUPERSCRIPT with any CSP⁡(R)CSP𝑅\operatorname{CSP}(R)roman_CSP ( italic_R ) instance Ψ=(X,Y)Ψ𝑋𝑌\Psi=(X,Y)roman_Ψ = ( italic_X , italic_Y ) whose codewords cσsubscript𝑐𝜎c_{\sigma}italic_c start_POSTSUBSCRIPT italic_σ end_POSTSUBSCRIPT correspond to the assignments σ:X→D:𝜎→𝑋𝐷\sigma:X\to Ditalic_σ : italic_X → italic_D, and cσ,ysubscript𝑐𝜎𝑦c_{\sigma,y}italic_c start_POSTSUBSCRIPT italic_σ , italic_y end_POSTSUBSCRIPT is 1111 precisely when σ𝜎\sigmaitalic_σ satisfies y𝑦yitalic_y. It is easy to check that CSP sparsification of ΨΨ\Psiroman_Ψ reduces to code sparsification of CΨsubscript𝐶ΨC_{\Psi}italic_C start_POSTSUBSCRIPT roman_Ψ end_POSTSUBSCRIPT, and the non-redundancy of C𝐶Citalic_C equals the size of the largest non-redundant sub-instance of ΨΨ\Psiroman_Ψ (viewed as an instance of CSP⁡(R¯)CSP¯𝑅\operatorname{CSP}(\overline{R})roman_CSP ( over¯ start_ARG italic_R end_ARG )). Combining Theorem 1.1 and (1), we therefore have our main result pinning down the sparsifiability of every CSP up to polylogarithmic factors. Theorem 1.2. For every nonempty R⊊Dr𝑅superscript𝐷𝑟R\subsetneq D^{r}italic_R ⊊ italic_D start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT and ε∈(0,1)𝜀01\varepsilon\in(0,1)italic_ε ∈ ( 0 , 1 ), we have that NRD⁡(R¯,n)≤SPR⁡(R,n,ε)≤O⁢(NRD⁡(R¯,n)⁢(r⁢log⁡n)6/ε2).NRD¯𝑅𝑛SPR𝑅𝑛𝜀𝑂NRD¯𝑅𝑛superscript𝑟𝑛6superscript𝜀2\operatorname{NRD}(\overline{R},n)\leq\operatorname{SPR}(R,n,\varepsilon)\leq O% (\operatorname{NRD}(\overline{R},n)(r\log n)^{6}/\varepsilon^{2}).roman_NRD ( over¯ start_ARG italic_R end_ARG , italic_n ) ≤ roman_SPR ( italic_R , italic_n , italic_ε ) ≤ italic_O ( roman_NRD ( over¯ start_ARG italic_R end_ARG , italic_n ) ( italic_r roman_log italic_n ) start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT / italic_ε start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) . 1.3 Non-redundancy of specific relations The non-redundancy of relations is readily computed in some simple cases. For example, for the relation ORr:=Dr∖{0r}assignsubscriptOR𝑟superscript𝐷𝑟superscript0𝑟\operatorname{OR}_{r}:=D^{r}\setminus\{0^{r}\}roman_OR start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT := italic_D start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ∖ { 0 start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT }, we have that NRD⁡(ORr,n)=Θ⁢(nr)NRDsubscriptOR𝑟𝑛Θsuperscript𝑛𝑟\operatorname{NRD}(\operatorname{OR}_{r},n)=\Theta(n^{r})roman_NRD ( roman_OR start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT , italic_n ) = roman_Θ ( italic_n start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ). Indeed Y=(Xr)𝑌binomial𝑋𝑟Y=\binom{X}{r}italic_Y = ( FRACOP start_ARG italic_X end_ARG start_ARG italic_r end_ARG ) is a non-redundant instance because setting all but r𝑟ritalic_r variables to 1111 fails to satisfy exactly that r𝑟ritalic_r-tuple (see [FK17, Car22, KPS24b]). When R𝑅Ritalic_R is affine, NRD⁡(R,n)=Θ⁢(n)NRD𝑅𝑛Θ𝑛\operatorname{NRD}(R,n)=\Theta(n)roman_NRD ( italic_R , italic_n ) = roman_Θ ( italic_n ), and when R𝑅Ritalic_R is defined as the zero set of a degree k𝑘kitalic_k polynomial, NRD⁡(R,n)=O⁢(nk)NRD𝑅𝑛𝑂superscript𝑛𝑘\operatorname{NRD}(R,n)=O(n^{k})roman_NRD ( italic_R , italic_n ) = italic_O ( italic_n start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ); these follow from simple rank arguments (e.g., [LW20]). Via Theorem 1.2, these special cases (plus simple gadget reductions) already capture all the previously known upper and lower bounds for CSP sparsification (see Section 1.5 for more details on the CSP sparsification literature). Furthermore, there are also some non-trivial upper bounds known on NRD in the literature, which we can now import to sparsifiability for free courtesy Theorem 1.2. For instance, the so-called Mal’tsev relations, which generalize affine predicates (i.e., cosets) over Abelian groups, have been shown to have OD⁢(n)subscript𝑂𝐷𝑛O_{D}(n)italic_O start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT ( italic_n ) non-redundancy [LW20, BCK20], and therefore by Theorem 1.2 their complements have near-linear sparsifiability. Carbonnel [Car22] showed that if R𝑅Ritalic_R is an arity r𝑟ritalic_r relation that doesn’t contain444See Theorem 5.2 for a precise definition. any copy of ORrsubscriptOR𝑟\operatorname{OR}_{r}roman_OR start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT, then NRD⁡(R,n)≤O⁢(nr−δr)NRD𝑅𝑛𝑂superscript𝑛𝑟subscript𝛿𝑟\operatorname{NRD}(R,n)\leq O(n^{r-\delta_{r}})roman_NRD ( italic_R , italic_n ) ≤ italic_O ( italic_n start_POSTSUPERSCRIPT italic_r - italic_δ start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ) for δr=21−rsubscript𝛿𝑟superscript21𝑟\delta_{r}=2^{1-r}italic_δ start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT = 2 start_POSTSUPERSCRIPT 1 - italic_r end_POSTSUPERSCRIPT (the specific bound arises from a classic hypergraph Turán result [Erd64]). By Theorem 1.2 this immediately implies SPR⁡(R¯,n,ε)≤O~ε⁢(nr−δr)SPR¯𝑅𝑛𝜀subscript~𝑂𝜀superscript𝑛𝑟subscript𝛿𝑟\operatorname{SPR}(\overline{R},n,\varepsilon)\leq\widetilde{O}_{\varepsilon}(% n^{r-\delta_{r}})roman_SPR ( over¯ start_ARG italic_R end_ARG , italic_n , italic_ε ) ≤ over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_ε end_POSTSUBSCRIPT ( italic_n start_POSTSUPERSCRIPT italic_r - italic_δ start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ), where O~⁢(⋅)~𝑂⋅\widetilde{O}(\cdot)over~ start_ARG italic_O end_ARG ( ⋅ ) hides polylogarithmic factors in n𝑛nitalic_n, yielding an Ω⁢(nr)Ωsuperscript𝑛𝑟\Omega(n^{r})roman_Ω ( italic_n start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ) vs O~⁢(nr−δr)~𝑂superscript𝑛𝑟subscript𝛿𝑟\widetilde{O}(n^{r-\delta_{r}})over~ start_ARG italic_O end_ARG ( italic_n start_POSTSUPERSCRIPT italic_r - italic_δ start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ) dichotomy for sparsification of arity r𝑟ritalic_r CSPs. (This was known for the Boolean case [KPS24b]; see the related work subsection.) The non-redundancy of a relation can in general be difficult to estimate. Thus while in principle Theorem 1.2 pins down the sparisifiability of every CSP, for specific relations, it can still be non-trivial to actually determine the asymptotic behavior of its sparsifiability. Our next set of results makes progress in this direction via novel methods to bound non-redundancy. Given that the non-redundancy of linear predicates is easy to pin down, we consider a natural family of relations which are very close to being linear. Specifically, let 3⁢L⁢I⁢NG={(x,y,z)∣x+y+z=0}subscript3LIN𝐺conditional-set𝑥𝑦𝑧𝑥𝑦𝑧0\operatorname{3LIN}_{G}=\{(x,y,z)\mid x+y+z=0\}start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT = { ( italic_x , italic_y , italic_z ) ∣ italic_x + italic_y + italic_z = 0 } over an Abelian group G𝐺Gitalic_G, and consider 3⁢L⁢I⁢NG∗=3⁢L⁢I⁢NG∖{(0,0,0)}subscriptsuperscript3LIN𝐺subscript3LIN𝐺000\operatorname{3LIN}^{*}_{G}=\operatorname{3LIN}_{G}\setminus\{(0,0,0)\}start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT = start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ∖ { ( 0 , 0 , 0 ) }. (We pick arity 3333 since the arity 2222 case is already fully resolved [FK17, BŽ20].) Being defined by a linear equation over an Abelian group, we already know that NRD⁡(3⁢L⁢I⁢NG,n)=ΘG⁢(n)NRDsubscript3LIN𝐺𝑛subscriptΘ𝐺𝑛\operatorname{NRD}(\operatorname{3LIN}_{G},n)=\Theta_{G}(n)roman_NRD ( start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT , italic_n ) = roman_Θ start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_n ). However the non-redundancy of 3⁢L⁢I⁢NG∗subscriptsuperscript3LIN𝐺\operatorname{3LIN}^{*}_{G}start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT seems challenging to understand. Existing methods in the literature only yield NRD⁡(3⁢L⁢I⁢NG∗,n)∈[ΩG⁢(n),OG⁢(n2)]NRDsubscriptsuperscript3LIN𝐺𝑛subscriptΩ𝐺𝑛subscript𝑂𝐺superscript𝑛2\operatorname{NRD}(\operatorname{3LIN}^{*}_{G},n)\in[\Omega_{G}(n),O_{G}(n^{2})]roman_NRD ( start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT , italic_n ) ∈ [ roman_Ω start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_n ) , italic_O start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) ]. We introduce a new method for bounding the non-redundancy of predicates like 3⁢L⁢I⁢NG∗subscriptsuperscript3LIN𝐺\operatorname{3LIN}^{*}_{G}start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT by connecting them to the theory of matching vector (MV) families [Yek08, DGY11] that have been used in the construction of locally decodable codes. Exploiting this connection, we construct a non-redundant instance to establish that NRD⁡(3⁢L⁢I⁢NG,n)≥Ω⁢(n1.5)NRDsubscript3LIN𝐺𝑛Ωsuperscript𝑛1.5\operatorname{NRD}(\operatorname{3LIN}_{G},n)\geq\Omega(n^{1.5})roman_NRD ( start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT , italic_n ) ≥ roman_Ω ( italic_n start_POSTSUPERSCRIPT 1.5 end_POSTSUPERSCRIPT ) for all Abelian groups of order ≥3absent3\geq 3≥ 3. Adapting ideas from the analysis of MV families together with some combinatorial ideas, we also prove an upper bound NRD⁡(3⁢L⁢I⁢Nℤ/p⁢ℤ,n)=O~p⁢(n2−εp)NRDsubscript3LINℤ𝑝ℤ𝑛subscript~𝑂𝑝superscript𝑛2subscript𝜀𝑝\operatorname{NRD}(\operatorname{3LIN}_{\mathbb{Z}/p\mathbb{Z}},n)=\widetilde{% O}_{p}(n^{2-\varepsilon_{p}})roman_NRD ( start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUBSCRIPT blackboard_Z / italic_p blackboard_Z end_POSTSUBSCRIPT , italic_n ) = over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_n start_POSTSUPERSCRIPT 2 - italic_ε start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ) for εp=22⁢p−1subscript𝜀𝑝22𝑝1\varepsilon_{p}=\tfrac{2}{2p-1}italic_ε start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT = divide start_ARG 2 end_ARG start_ARG 2 italic_p - 1 end_ARG and p𝑝pitalic_p prime. Specializing for p=3𝑝3p=3italic_p = 3, we have the following result, which also gives the first examples of relations whose non-redundancy and sparsifiability have a non-integral exponent. Theorem 1.3. We have NRD⁡(3⁢L⁢I⁢Nℤ/3⁢ℤ∗,n)NRDsubscriptsuperscript3LINℤ3ℤ𝑛\displaystyle\operatorname{NRD}(\operatorname{3LIN}^{*}_{\mathbb{Z}/3\mathbb{Z% }},n)roman_NRD ( start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT blackboard_Z / 3 blackboard_Z end_POSTSUBSCRIPT , italic_n ) ∈[Ω⁢(n1.5),O~⁢(n1.6)], andabsentΩsuperscript𝑛1.5~𝑂superscript𝑛1.6 and\displaystyle\in[\Omega(n^{1.5}),\widetilde{O}(n^{1.6})],\ \ \ \text{ and }∈ [ roman_Ω ( italic_n start_POSTSUPERSCRIPT 1.5 end_POSTSUPERSCRIPT ) , over~ start_ARG italic_O end_ARG ( italic_n start_POSTSUPERSCRIPT 1.6 end_POSTSUPERSCRIPT ) ] , and SPR⁡(3⁢L⁢I⁢Nℤ/3⁢ℤ∗¯,n,ε)SPR¯subscriptsuperscript3LINℤ3ℤ𝑛𝜀\displaystyle\operatorname{SPR}(\overline{\operatorname{3LIN}^{*}_{\mathbb{Z}/% 3\mathbb{Z}}},n,\varepsilon)roman_SPR ( over¯ start_ARG start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT blackboard_Z / 3 blackboard_Z end_POSTSUBSCRIPT end_ARG , italic_n , italic_ε ) ∈[Ω⁢(n1.5),O~⁢(n1.6/ε2)].absentΩsuperscript𝑛1.5~𝑂superscript𝑛1.6superscript𝜀2\displaystyle\in[\Omega(n^{1.5}),\widetilde{O}(n^{1.6}/\varepsilon^{2})].∈ [ roman_Ω ( italic_n start_POSTSUPERSCRIPT 1.5 end_POSTSUPERSCRIPT ) , over~ start_ARG italic_O end_ARG ( italic_n start_POSTSUPERSCRIPT 1.6 end_POSTSUPERSCRIPT / italic_ε start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) ] . 1.4 Weighted CSP sparsification The discussion so far has focused on unweighted CSP instances, and we now shift our focus to the weighted case, where each constraint of Y𝑌Yitalic_Y comes with a weight. We also get a tight characterization of weighted CSP sparsifiablity, in terms of a parameter called the chain length, which was defined by Lagerkvist and Wahlström [LW17, LW20] in the context of CSP kernelization and later utilized by Bessiere, Carbonnel, and Katsirelos [BCK20] in the context of learning CSPs in a certain query model (see Section 1.5 for more details on these connections). As before, the result is obtained in the setting of weighted non-linear codes, with the consequence for weighted CSPs being an easy corollary. We just state the result for codes here (see Section 8 for the full treatment of weighted CSPs). For weighted sparsification of a code C⊆{0,1}m𝐶superscript01𝑚C\subseteq\{0,1\}^{m}italic_C ⊆ { 0 , 1 } start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT, we might have an arbitrary input weighting ζ:[m]→ℝ≥0:𝜁→delimited-[]𝑚subscriptℝabsent0\zeta:[m]\to\mathbb{R}_{\geq 0}italic_ζ : [ italic_m ] → blackboard_R start_POSTSUBSCRIPT ≥ 0 end_POSTSUBSCRIPT of its coordinates, and we must find a sparsifier w~:[n]→ℝ≥0:~𝑤→delimited-[]𝑛subscriptℝabsent0\widetilde{w}:[n]\to\mathbb{R}_{\geq 0}over~ start_ARG italic_w end_ARG : [ italic_n ] → blackboard_R start_POSTSUBSCRIPT ≥ 0 end_POSTSUBSCRIPT of low support that sparsifies C𝐶Citalic_C with respect to the weighting ζ𝜁\zetaitalic_ζ, i.e., ⟨w~,c⟩∈(1±ε)⁢⟨ζ,c⟩~𝑤𝑐plus-or-minus1𝜀𝜁𝑐\langle\widetilde{w},c\rangle\in(1\pm\varepsilon)\langle\zeta,c\rangle⟨ over~ start_ARG italic_w end_ARG , italic_c ⟩ ∈ ( 1 ± italic_ε ) ⟨ italic_ζ , italic_c ⟩. The minimum possible support of sparsifiers over all weightings ζ𝜁\zetaitalic_ζ is called the weighted ε𝜀\varepsilonitalic_ε-sparsity wSPR⁡(C,ε)wSPR𝐶𝜀\operatorname{wSPR}(C,\varepsilon)roman_wSPR ( italic_C , italic_ε ). Now we define chain length. If we line up the codewords of C𝐶Citalic_C as rows of an |C|×m𝐶𝑚|C|\times m| italic_C | × italic_m matrix and allow arbitrary column permutations, the chain length of C𝐶Citalic_C, denoted CL⁡(C)CL𝐶\operatorname{CL}(C)roman_CL ( italic_C ), is the dimension of the largest upper triangular square submatrix with 1111’s on the diagonal.555In this view NRD⁡(C)NRD𝐶\operatorname{NRD}(C)roman_NRD ( italic_C ) is the dimension of the largest identity submatrix, so clearly NRD⁡(C)≤CL⁡(C)NRD𝐶CL𝐶\operatorname{NRD}(C)\leq\operatorname{CL}(C)roman_NRD ( italic_C ) ≤ roman_CL ( italic_C ). The quantity CL⁡(C)CL𝐶\operatorname{CL}(C)roman_CL ( italic_C ) was called visible rank in [AG21] and served as a field independent lower bound on the rank of C𝐶Citalic_C. In our main result for the weighted setting, we pin the sparsifiability of a weighted code to its chain length. Note that in the weighted case CL⁡(C)CL𝐶\operatorname{CL}(C)roman_CL ( italic_C ) is also a lower bound. Theorem 1.4. For all C⊆{0,1}m𝐶superscript01𝑚C\subseteq\{0,1\}^{m}italic_C ⊆ { 0 , 1 } start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT and ε∈(0,1)𝜀01\varepsilon\in(0,1)italic_ε ∈ ( 0 , 1 ), we have CL⁡(C)≤wSPR⁡(C,ε)=O⁢(CL⁡(C)⁢(log⁡m)6/ε2).CL𝐶wSPR𝐶𝜀𝑂CL𝐶superscript𝑚6superscript𝜀2\operatorname{CL}(C)\leq\operatorname{wSPR}(C,\varepsilon)=O(\operatorname{CL}% (C)(\log m)^{6}/\varepsilon^{2}).roman_CL ( italic_C ) ≤ roman_wSPR ( italic_C , italic_ε ) = italic_O ( roman_CL ( italic_C ) ( roman_log italic_m ) start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT / italic_ε start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) . The upper bound proceeds by using Theorem 1.1 as a black-box together with a geometric weight bucketing technique from [KPS24b]. The lower bound proceeds by applying an exponential sequence of weights to the indices i1,…,iCL⁡(C)∈[m]subscript𝑖1…subscript𝑖CL𝐶delimited-[]𝑚i_{1},\ldots,i_{\operatorname{CL}{(C)}}\in[m]italic_i start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_i start_POSTSUBSCRIPT roman_CL ( italic_C ) end_POSTSUBSCRIPT ∈ [ italic_m ] forming a maximal chain. Of note, if for a particular set of weights, the ratio between maximum and minimal weights is λ≪exp⁡(CL⁡(C)/NRD⁡(C))much-less-than𝜆CL𝐶NRD𝐶\lambda\ll\exp(\operatorname{CL}(C)/\operatorname{NRD}(C))italic_λ ≪ roman_exp ( roman_CL ( italic_C ) / roman_NRD ( italic_C ) ), we get a sharper upper bound of O~ε⁢(NRD⁡(C)⁢log⁡λ)subscript~𝑂𝜀NRD𝐶𝜆\widetilde{O}_{\varepsilon}(\operatorname{NRD}(C)\log\lambda)over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_ε end_POSTSUBSCRIPT ( roman_NRD ( italic_C ) roman_log italic_λ ) (see Corollary 8.18). We now transition to discussing the broader context of our work in the literature. 1.5 Related Work Our results and techniques have connections to many areas including computational complexity theory, extremal combinatorics, coding theory, and learning theory. We now give a general overview of these connections. CSP Sparsification. Since we already discussed the history of CSP sparsification, we give a comprehensive list of known results about CSP sparsification (up to polylog factors). • The case of binary CSPs (r=2𝑟2r=2italic_r = 2) is fully classified. In particular, for every finite domain D𝐷Ditalic_D and R⊆D2𝑅superscript𝐷2R\subseteq D^{2}italic_R ⊆ italic_D start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, we either have that SPR⁡(R,n,ε)=O⁢(n/ε2)SPR𝑅𝑛𝜀𝑂𝑛superscript𝜀2\operatorname{SPR}(R,n,\varepsilon)=O(n/\varepsilon^{2})roman_SPR ( italic_R , italic_n , italic_ε ) = italic_O ( italic_n / italic_ε start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) or SPR⁡(R,n,ε)=Ω⁢(n2)SPR𝑅𝑛𝜀Ωsuperscript𝑛2\operatorname{SPR}(R,n,\varepsilon)=\Omega(n^{2})roman_SPR ( italic_R , italic_n , italic_ε ) = roman_Ω ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) [BŽ20]. However, the sparsification routine is only efficient in the Boolean case [FK17]. Of note, SPR⁡(R,n,ε)=Ω⁢(n2)SPR𝑅𝑛𝜀Ωsuperscript𝑛2\operatorname{SPR}(R,n,\varepsilon)=\Omega(n^{2})roman_SPR ( italic_R , italic_n , italic_ε ) = roman_Ω ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) if and only if there exist D1,D2⊆Dsubscript𝐷1subscript𝐷2𝐷D_{1},D_{2}\subseteq Ditalic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ⊆ italic_D of size exactly 2222 such that |R∩(D1×D2)|=1𝑅subscript𝐷1subscript𝐷21|R\cap(D_{1}\times D_{2})|=1| italic_R ∩ ( italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT × italic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) | = 1 (informally R𝑅Ritalic_R has an “induced copy” of AND2subscriptAND2\operatorname{AND}_{2}roman_AND start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT). • For r≥3𝑟3r\geq 3italic_r ≥ 3, much less is known. Kogan and Krauthgamer [KK15] contributed near-linear hypergraph cut sparsifiers (i.e., the predicate is NAEr:={0,1}r∖{0r,1r}assignsubscriptNAE𝑟superscript01𝑟superscript0𝑟superscript1𝑟\operatorname{NAE}_{r}:=\{0,1\}^{r}\setminus\{0^{r},1^{r}\}roman_NAE start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT := { 0 , 1 } start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ∖ { 0 start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT , 1 start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT }). Since then, there have been multiple improvements in efficiently constructing hypergraph sparsifiers/sketches (e.g., [CKN20, KKTY21, KPS24c]). • The breakthroughs of Khanna, Putterman, and Sudan [KPS24a, KPS24b] construct non-linear sparsifiers for any predicate which can defined by a system of linear (in)equations (possibly over a higher domain). For example NAEr={x∈{0,1}r:x1+⋯+xr≢0modr}subscriptNAE𝑟conditional-set𝑥superscript01𝑟not-equivalent-tosubscript𝑥1⋯subscript𝑥𝑟modulo0𝑟\operatorname{NAE}_{r}=\{x\in\{0,1\}^{r}:x_{1}+\cdots+x_{r}\not\equiv 0\mod r\}roman_NAE start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT = { italic_x ∈ { 0 , 1 } start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT : italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + ⋯ + italic_x start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ≢ 0 roman_mod italic_r }. Of note, their first paper [KPS24a] only proved the result over finite fields (and was nonalgorithmic), whereas their second paper [KPS24b] extended the result to all Abelian groups and was computationally efficient. • The framework of Khanna, Putterman, and Sudan [KPS24b] produced numerous corollaries. In particular, if a predicate can be expressed as the nonzero set of a degree k𝑘kitalic_k polynomial, then it has a sparsifier of size O~ε⁢(nk)subscript~𝑂𝜀superscript𝑛𝑘\widetilde{O}_{\varepsilon}(n^{k})over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_ε end_POSTSUBSCRIPT ( italic_n start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ). Furthermore, they show if a predicate R𝑅Ritalic_R can express666More specifically, we say that R⊆{0,1}r𝑅superscript01𝑟R\subseteq\{0,1\}^{r}italic_R ⊆ { 0 , 1 } start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT can express ANDksubscriptAND𝑘\operatorname{AND}_{k}roman_AND start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT if there exits a map z:[r]→{0,1,x1,…,xk,x1¯,…,xk¯}:𝑧→delimited-[]𝑟01subscript𝑥1…subscript𝑥𝑘¯subscript𝑥1…¯subscript𝑥𝑘z:[r]\to\{0,1,x_{1},\ldots,x_{k},\overline{x_{1}},\ldots,\overline{x_{k}}\}italic_z : [ italic_r ] → { 0 , 1 , italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , over¯ start_ARG italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG , … , over¯ start_ARG italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG } such that R⁢(z⁢(1),…,z⁢(r))=ANDk⁡(x1,…,xk)𝑅𝑧1…𝑧𝑟subscriptAND𝑘subscript𝑥1…subscript𝑥𝑘R(z(1),\ldots,z(r))=\operatorname{AND}_{k}(x_{1},\ldots,x_{k})italic_R ( italic_z ( 1 ) , … , italic_z ( italic_r ) ) = roman_AND start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ). We discuss a more general framework of gadget reductions in Section 5.4. ANDk:={1k}assignsubscriptAND𝑘superscript1𝑘\operatorname{AND}_{k}:=\{1^{k}\}roman_AND start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT := { 1 start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT }, then SPR⁡(R,n,ε)=Ω⁢(nk)SPR𝑅𝑛𝜀Ωsuperscript𝑛𝑘\operatorname{SPR}(R,n,\varepsilon)=\Omega(n^{k})roman_SPR ( italic_R , italic_n , italic_ε ) = roman_Ω ( italic_n start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ). As a consequence, they also classify all ternary Boolean predicates (r=3𝑟3r=3italic_r = 3) as well as which Boolean predicates of arity r𝑟ritalic_r cannot be sparsified below Ω⁢(nr)Ωsuperscript𝑛𝑟\Omega(n^{r})roman_Ω ( italic_n start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ) (just ANDrsubscriptAND𝑟\operatorname{AND}_{r}roman_AND start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT and its bit flips), while also constructing a sparsifier of size O~ε⁢(nr−1)subscript~𝑂𝜀superscript𝑛𝑟1\widetilde{O}_{\varepsilon}(n^{r-1})over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_ε end_POSTSUBSCRIPT ( italic_n start_POSTSUPERSCRIPT italic_r - 1 end_POSTSUPERSCRIPT ) in the other cases. • It appears that lower bounds with a nontrivial dependence on ε𝜀\varepsilonitalic_ε are only known for cut sparsifiers (and thus hypergraph cut sparsifiers via a simple gadget reduction). See [ACK+16, CKST19] as well as Section 9 for further discussion. CSP Kernelization. Another question similar in spirit to CSP sparsification is that of CSP kernelization.777More commonly, CSP kernelization is referred to as CSP sparsification (e.g., [DvM14, LW20]). However, we refer to this line of work by the former name to reduce ambiguity. This similarity in name has been noted before in the literature (e.g., [BŽ20]), but we appear to be the first work to notice both variants of “CSP sparsification” can be analyzed with similar techniques. The basic question is to, given an instance ΨΨ\Psiroman_Ψ of CSP⁡(R)CSP𝑅\operatorname{CSP}(R)roman_CSP ( italic_R ), efficiently find as small of an instance Ψ′superscriptΨ′\Psi^{\prime}roman_Ψ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT of CSP⁡(R)CSP𝑅\operatorname{CSP}(R)roman_CSP ( italic_R ) as possible (not necessarily a subinstance) such that ΨΨ\Psiroman_Ψ and Ψ′superscriptΨ′\Psi^{\prime}roman_Ψ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT are either both satisfiable or both unsatisfiable. This particular question can be attributed to Dell and van Melkebeek [DvM14], who were particularly inspired Impagliazzo, Paturi, and Zane’s sparsification lemma [IPZ01] and Harnik and Naor’s compression framework [HN10]. See the literature review in [DvM14] for further motivations. At first, the problem seems rather unrelated to CSP sparsification. For example, if CSP⁡(R)CSP𝑅\operatorname{CSP}(R)roman_CSP ( italic_R ) is polynomial-time tractable, then there trivially exists a kernel of size O⁢(1)𝑂1O(1)italic_O ( 1 ). When CSP⁡(R)CSP𝑅\operatorname{CSP}(R)roman_CSP ( italic_R ) is NP-hard, however, the size of the smallest possible kernelization seems to much more closely track with the non-redundancy of R𝑅Ritalic_R. In particular, Dell and van Melkebeek [DvM14], proved that assuming 𝖼𝗈𝖭𝖯⊈𝖭𝖯/𝗉𝗈𝗅𝗒not-subset-of-nor-equals𝖼𝗈𝖭𝖯𝖭𝖯𝗉𝗈𝗅𝗒\mathsf{coNP}\nsubseteq\mathsf{NP/poly}sansserif_coNP ⊈ sansserif_NP / sansserif_poly, the problem k𝑘kitalic_k-SAT cannot be kernelized below Ω⁢(nk−ε)Ωsuperscript𝑛𝑘𝜀\Omega(n^{k-\varepsilon})roman_Ω ( italic_n start_POSTSUPERSCRIPT italic_k - italic_ε end_POSTSUPERSCRIPT ) for any constant ε>0𝜀0\varepsilon>0italic_ε > 0, which is close to k𝑘kitalic_k-SAT’s non-redundancy of Θ⁢(nk)Θsuperscript𝑛𝑘\Theta(n^{k})roman_Θ ( italic_n start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ). Furthermore, most upper bounds on the kernelization of NP-hard predicates follow from upper bounds on non-redundancy (see [Car22]). For example the works of Chen, Jansen, and Pieterse [CJP20] as well as Lagerkvist and Wahlström [LW17, LW20] develop various kernelization methods that happen to just be “efficient” non-redundancy upper bounds. For example, these works show that if the predicate R𝑅Ritalic_R can be expressed as the zero set of a polynomial of degree k𝑘kitalic_k, then there exist a kernel of size O⁢(nk)𝑂superscript𝑛𝑘O(n^{k})italic_O ( italic_n start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ). This kernel happens to preserve every solution to R𝑅Ritalic_R, so it is also a non-redundancy upper bound. Using techniques like these, they are able to prove a number of results similar to the state-of-the-art in CSP sparsification, such as a complete classification of ternary Boolean predicates and a O⁢(nr−1)𝑂superscript𝑛𝑟1O(n^{r-1})italic_O ( italic_n start_POSTSUPERSCRIPT italic_r - 1 end_POSTSUPERSCRIPT ) vs Ω⁢(nr−ε)Ωsuperscript𝑛𝑟𝜀\Omega(n^{r-\varepsilon})roman_Ω ( italic_n start_POSTSUPERSCRIPT italic_r - italic_ε end_POSTSUPERSCRIPT ) Boolean dichotomy [CJP20]. See [JP19, JW20, Jan20, Tak23, Beu21] and citations therein for related work. We seek to emphasize that any efficient CSP sparsification algorithm for CSP⁡(R)CSP𝑅\operatorname{CSP}(R)roman_CSP ( italic_R ) is by design a kernelization algorithm for CSP⁡(R¯)CSP¯𝑅\operatorname{CSP}(\overline{R})roman_CSP ( over¯ start_ARG italic_R end_ARG ) (since all codewords with weight 00 are preserved). As such, making Theorem 1.2 efficient would require explicitly proving that every CSP can be kernelized to (approximately) its non-redundancy, which is a significant open question in the CSP kernelization community (see [Car22]). See Section 1.7 and Section 9 for further discussion. The Union-closed Sets Conjecture. A family ℱℱ\mathcal{F}caligraphic_F of subsets of [n]delimited-[]𝑛[n][ italic_n ] is union-closed if A,B∈ℱ𝐴𝐵ℱA,B\in\mathcal{F}italic_A , italic_B ∈ caligraphic_F imply that A∪B∈ℱ𝐴𝐵ℱA\cup B\in\mathcal{F}italic_A ∪ italic_B ∈ caligraphic_F. In 1979, Frankl [Fra95] conjectured that there always exists i∈[n]𝑖delimited-[]𝑛i\in[n]italic_i ∈ [ italic_n ] which appears in at least half of the sets of ℱℱ\mathcal{F}caligraphic_F. For decades, progress on the conjecture was minimal, with the best general result being that some i∈[n]𝑖delimited-[]𝑛i\in[n]italic_i ∈ [ italic_n ] appears in Ω⁢(1/log2⁡|ℱ|)Ω1subscript2ℱ\Omega(1/\log_{2}|\mathcal{F}|)roman_Ω ( 1 / roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | caligraphic_F | ) of the sets [Kni94, Wój99, Gil22]. However, in 2022, Gilmer [Gil22] shocked the combinatorics community by using an entropy-based approach to prove that some i∈[n]𝑖delimited-[]𝑛i\in[n]italic_i ∈ [ italic_n ] appears in 1/10011001/1001 / 100 of the sets. This immediately led to a large number of follow-up works refining Gilmer’s entropy method [AHS22, CL22, Peb22, Saw23, Yu23, Cam22]. In particular, we can now replace ‘1/10011001/1001 / 100’ with ‘0.382⁢…0.382…0.382\ldots0.382 …’, leaving Frankl’s conjecture (technically) still open. For our application to CSP sparsification, the entropy method used by Gilmer (and its subsequent refinements by many other reseachers) is the key idea needed to show that non-redundancy is essentially the optimal size for a CSP sparsifier. In particular, the improvement from 1/log2⁡|ℱ|1subscript2ℱ1/\log_{2}|\mathcal{F}|1 / roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | caligraphic_F | to Ω⁢(1)Ω1\Omega(1)roman_Ω ( 1 ) is precisely the same “gain” we utilize to go from a very simple O~ε⁢(NRD⁡(C)⋅log2⁡|C|)subscript~𝑂𝜀⋅NRD𝐶subscript2𝐶\widetilde{O}_{\varepsilon}(\operatorname{NRD}(C)\cdot\log_{2}|C|)over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_ε end_POSTSUBSCRIPT ( roman_NRD ( italic_C ) ⋅ roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | italic_C | ) sparsifier (see Section 3) to our O~ε⁢(NRD⁡(C))subscript~𝑂𝜀NRD𝐶\widetilde{O}_{\varepsilon}(\operatorname{NRD}(C))over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_ε end_POSTSUBSCRIPT ( roman_NRD ( italic_C ) ) sparsifier. See the technical overview (Section 1.6) for more details. To the best of our knowledge, our work is the first application of Gilmer’s entropy method to sparsification.888Gilmer’s breakthough is cited in the literature review of [CDL+24], but the property-testing question they study on union-closed families has no technical connection to Gilmer’s entropy method. See also [Wak24] for applications of the entropy method to learning theory and statistical physics. Matching Vector Families and Locally Decodable Codes. In coding theory, locally decodable codes (LDCs) are a class of codes which allow for jthe reliable recovery of any message symbol based on a small sample of codeword symbols, even in the presence of a constant fraction of errors. A particularly interesting familiy of constructions of LDCs has arisen out of a theory of matching vector codes [Yek08] and follow-ups [Rag07, Gop09, Efr09, DGY11]. See [DGY11] for a literature survey. Simply stated, a matching vector (MV) family over a (finite) ring ℛℛ\mathcal{R}caligraphic_R is a pair of lists of vectors u1,…,uk,v1,…,vk∈ℛdsubscript𝑢1…subscript𝑢𝑘subscript𝑣1…subscript𝑣𝑘superscriptℛ𝑑u_{1},\ldots,u_{k},v_{1},\ldots,v_{k}\in\mathcal{R}^{d}italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_u start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_v start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∈ caligraphic_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT such that the inner products ⟨ui,vj⟩subscript𝑢𝑖subscript𝑣𝑗\langle u_{i},v_{j}\rangle⟨ italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ⟩ are nonzero999Or, more generally the inner products lie in some restricted subset of ℛℛ\mathcal{R}caligraphic_R. if and only i≠j𝑖𝑗i\neq jitalic_i ≠ italic_j. Informally, the uisubscript𝑢𝑖u_{i}italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT’s play a role in the encoding of the i𝑖iitalic_i’th message symbol, with the matching vector visubscript𝑣𝑖v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT helping with its local decoding. Given a choice of ℛℛ\mathcal{R}caligraphic_R and d𝑑ditalic_d, the primary question of interest is to find the maximal possible value of k𝑘kitalic_k. This “spin off” question about LDCs has become a topic of interest in its own right [DGY11, Yek12, GHSY12, BDL13]. In this work, we demonstrate a novel application of matching vector families to the study of non-redundancy and thus (by Theorem 1.2) sparsification. In particular, we construct an explicit family of predicates such that their non-redundant instances can be viewed as a generalized MV family. We then use techniques developed for MV families to given nontrivial bounds on the non-redundancy of the predicates. See Section 6 and the technical overview (Section 1.6) for more details. Extremal Combinatorics. Computing the non-redundancy of a predicate can be viewed as a problem in extremal combinatorics known as a hypergraph Turán problem. In particular, for an instance of a CSP to be non-redundant, every instance induced by a subset of the variables must also be non-redundant. In particular, if ℱℱ\mathcal{F}caligraphic_F is a family of hypergraphs which can never appear in non-redundant instances of CSP⁡(R)CSP𝑅\operatorname{CSP}(R)roman_CSP ( italic_R ), then NRD⁡(R,n)≤exr⁡(n,ℱ)NRD𝑅𝑛subscriptex𝑟𝑛ℱ\operatorname{NRD}(R,n)\leq\operatorname{ex}_{r}(n,\mathcal{F})roman_NRD ( italic_R , italic_n ) ≤ roman_ex start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ( italic_n , caligraphic_F ), where the hypergraph Turán number exr⁡(n,ℱ)subscriptex𝑟𝑛ℱ\operatorname{ex}_{r}(n,\mathcal{F})roman_ex start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ( italic_n , caligraphic_F ) is the size of the largest r𝑟ritalic_r-uniform hypergraph on n𝑛nitalic_n vertices without any F∈ℱ𝐹ℱF\in\mathcal{F}italic_F ∈ caligraphic_F as a subgraph. This observation was first made explicit by Carbonnel [Car22] although the technique was also used in earlier work [BCK20]. As far as we are aware, ours is the first work to observe that these insights can also benefit the study of CSP sparsification. The literature on hypergraph Turán numbers is quite rich. For instance, Keevash [Kee11] surveys the vast body of work on the “non-degenerate” case in which exr⁡(n,ℱ)=Ωr⁢(nr)subscriptex𝑟𝑛ℱsubscriptΩ𝑟superscript𝑛𝑟\operatorname{ex}_{r}(n,\mathcal{F})=\Omega_{r}(n^{r})roman_ex start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ( italic_n , caligraphic_F ) = roman_Ω start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ( italic_n start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ). However, for our applications, we are mostly interested in the “denegerate” case in which exr⁡(n,ℱ)=O⁢(nc)subscriptex𝑟𝑛ℱ𝑂superscript𝑛𝑐\operatorname{ex}_{r}(n,\mathcal{F})=O(n^{c})roman_ex start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ( italic_n , caligraphic_F ) = italic_O ( italic_n start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ) for some c∈[1,r)𝑐1𝑟c\in[1,r)italic_c ∈ [ 1 , italic_r ). The works [BCK20, Car22] apply some of the most well-known works in this setting [Erd64, SEB73, RS78] to get some nontrivial results such as classifying precisely which predicates R𝑅Ritalic_R have NRD⁡(R,n)=Θ⁢(nr)NRD𝑅𝑛Θsuperscript𝑛𝑟\operatorname{NRD}(R,n)=\Theta(n^{r})roman_NRD ( italic_R , italic_n ) = roman_Θ ( italic_n start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ), extending Chen, Jansen, and Pieterse’s result for the Boolean case [CJP20]. See Sections 5.2, 7.1, and 8.4.2 for more details on specific applications. Query Complexity and Learning Theory. Rather surprisingly, the definition of non-redundancy appears to have come out of the artificial intelligence community [BCK20]. In particular, a rather broad and well-studied question (e.g., [FW02, PBS08, LLMV10, BK12, BCH+13, BCK20]) is that of constraint acquisition: how can an agent learn the constraints defining an instance of a constraint satisfaction problem? A model specifically relevant to our work is the partial membership queries model studied by Bessiere, Carbonnel, and Katsirelos [BCK20]. In this model, the domain D𝐷Ditalic_D, the constraint type R𝑅Ritalic_R (or types), and the set of variables X𝑋Xitalic_X are known but the constraints are hidden. For each query, the agent picks some subset of variables X′⊆Xsuperscript𝑋′𝑋X^{\prime}\subseteq Xitalic_X start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ⊆ italic_X as well as a partial assignment σ:X′→D:𝜎→superscript𝑋′𝐷\sigma:X^{\prime}\to Ditalic_σ : italic_X start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT → italic_D. The response to the query is ‘YES’ if σ𝜎\sigmaitalic_σ satisfies every constraint induced by X′superscript𝑋′X^{\prime}italic_X start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT, and ‘NO’ otherwise. The goal is to construct an instance of CSP⁡(R)CSP𝑅\operatorname{CSP}(R)roman_CSP ( italic_R ) with the same solution set as the hidden CSP. For every CSP predicate R𝑅Ritalic_R, they prove that the query complexity of an instance of CSP⁡(R)CSP𝑅\operatorname{CSP}(R)roman_CSP ( italic_R ) on n𝑛nitalic_n variables is bounded between Ω⁢(NRD⁡(R,n))ΩNRD𝑅𝑛\Omega(\operatorname{NRD}(R,n))roman_Ω ( roman_NRD ( italic_R , italic_n ) ) and O⁢(CL⁡(R,n)⋅log⁡n)𝑂⋅CL𝑅𝑛𝑛O(\operatorname{CL}(R,n)\cdot\log n)italic_O ( roman_CL ( italic_R , italic_n ) ⋅ roman_log italic_n ). Notably, the lower bound is proved by showing that the VC dimension of the query complexity problem equals NRD⁡(R,n)NRD𝑅𝑛\operatorname{NRD}(R,n)roman_NRD ( italic_R , italic_n ).101010This observation is directly used in proving our main result, see Section 4.1. 1.6 Technical Overview We next describe the primary techniques we use to prove Theorem 1.1 and Theorem 1.3. A Simple Sparsifier. To begin, we discuss a warm-up version of Theorem 1.1 which proves a weaker upper bound of SPR⁡(C,ε)≤O~ε⁢(NRD⁡(C)⋅log⁡|C|)SPR𝐶𝜀subscript~𝑂𝜀⋅NRD𝐶𝐶\operatorname{SPR}(C,\varepsilon)\leq\widetilde{O}_{\varepsilon}(\operatorname% {NRD}(C)\cdot\log|C|)roman_SPR ( italic_C , italic_ε ) ≤ over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_ε end_POSTSUBSCRIPT ( roman_NRD ( italic_C ) ⋅ roman_log | italic_C | ) (see Theorem 3.1), which for CSPs corresponds to an extra factor of the number of variables n𝑛nitalic_n. The key technical insight (Lemma 3.3) is that for all d∈[m]𝑑delimited-[]𝑚d\in[m]italic_d ∈ [ italic_m ], the set of codewords of C𝐶Citalic_C with Hamming weight at most d𝑑ditalic_d (denoted by C≤dsubscript𝐶absent𝑑C_{\leq d}italic_C start_POSTSUBSCRIPT ≤ italic_d end_POSTSUBSCRIPT) has total support size at most d⋅NRD⁡(C)⋅𝑑NRD𝐶d\cdot\operatorname{NRD}(C)italic_d ⋅ roman_NRD ( italic_C ). This can proved inductively by noticing that dropping a suitable non-redundant set of coordinates decreases the Hamming weight of every codeword of C𝐶Citalic_C by at least one. With this lemma, we can recursively construct a sparsifier as follows, similar to the divide-and-conquer framework in [KPS24a, KPS24b] for linear codes. Pick d≈Θ~ε⁢(log⁡|C|)𝑑subscript~Θ𝜀𝐶d\approx\widetilde{\Theta}_{\varepsilon}(\log|C|)italic_d ≈ over~ start_ARG roman_Θ end_ARG start_POSTSUBSCRIPT italic_ε end_POSTSUBSCRIPT ( roman_log | italic_C | ) and let I⊆[m]𝐼delimited-[]𝑚I\subseteq[m]italic_I ⊆ [ italic_m ] be the support of C≤dsubscript𝐶absent𝑑C_{\leq d}italic_C start_POSTSUBSCRIPT ≤ italic_d end_POSTSUBSCRIPT. Every i∈I𝑖𝐼i\in Iitalic_i ∈ italic_I is given weight 1111 in our sparsifier. For the rest of [m]delimited-[]𝑚[m][ italic_m ], let J⊆[m]∖I𝐽delimited-[]𝑚𝐼J\subseteq[m]\setminus Iitalic_J ⊆ [ italic_m ] ∖ italic_I be a subsample where each i∈[m]∖I𝑖delimited-[]𝑚𝐼i\in[m]\setminus Iitalic_i ∈ [ italic_m ] ∖ italic_I is kept independently with probability 1/3131/31 / 3. Using a standard Chernoff bound, we can show that with positive111111We only need positive probability since we are focused on existence. This can easily be amplified to 1−1/mΩ⁢(1)11superscript𝑚Ω11-1/m^{\Omega(1)}1 - 1 / italic_m start_POSTSUPERSCRIPT roman_Ω ( 1 ) end_POSTSUPERSCRIPT probability by making d𝑑ditalic_d a factor of log⁡m𝑚\log mroman_log italic_m bigger. In applications to CSPs, the main algorithmic bottleneck is (approximately) finding I𝐼Iitalic_I, which appears to be similar in difficulty to an open problem in CSP kernelization (see Section 1.7). probability the following holds for all c∈C𝑐𝐶c\in Citalic_c ∈ italic_C: 3⁢Ham⁡(c|J)+Ham⁡(c|I)∈[1−ε2⁢log2⁡m,1+ε2⁢log2⁡m]⋅Ham⁡(c).3Hamevaluated-at𝑐𝐽Hamevaluated-at𝑐𝐼⋅1𝜀2subscript2𝑚1𝜀2subscript2𝑚Ham𝑐3\operatorname{Ham}(c|_{J})+\operatorname{Ham}(c|_{I})\in\left[1-\frac{% \varepsilon}{2\log_{2}m},1+\frac{\varepsilon}{2\log_{2}m}\right]\cdot% \operatorname{Ham}(c).3 roman_Ham ( italic_c | start_POSTSUBSCRIPT italic_J end_POSTSUBSCRIPT ) + roman_Ham ( italic_c | start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT ) ∈ [ 1 - divide start_ARG italic_ε end_ARG start_ARG 2 roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_m end_ARG , 1 + divide start_ARG italic_ε end_ARG start_ARG 2 roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_m end_ARG ] ⋅ roman_Ham ( italic_c ) . By induction, we can find a O~ε′⁢(NRD⁡(C′)⋅log⁡|C′|)subscript~𝑂superscript𝜀′⋅NRDsuperscript𝐶′superscript𝐶′\widetilde{O}_{\varepsilon^{\prime}}(\operatorname{NRD}(C^{\prime})\cdot\log|C% ^{\prime}|)over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_ε start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( roman_NRD ( italic_C start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) ⋅ roman_log | italic_C start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT | ) ε′superscript𝜀′\varepsilon^{\prime}italic_ε start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT-sparsifier for C′:=C|Jassignsuperscript𝐶′evaluated-at𝐶𝐽C^{\prime}:=C|_{J}italic_C start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT := italic_C | start_POSTSUBSCRIPT italic_J end_POSTSUBSCRIPT with ε′:=(1−1/log2⁡m)⁢εassignsuperscript𝜀′11subscript2𝑚𝜀\varepsilon^{\prime}:=(1-1/\log_{2}m)\varepsilonitalic_ε start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT := ( 1 - 1 / roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_m ) italic_ε. Scaling this sparsifier by 3333 and adding weights for I𝐼Iitalic_I gives us an ε𝜀\varepsilonitalic_ε-sparsifier of C𝐶Citalic_C. Entropy-based Sparsification. The key inefficiency of the O~ε⁢(NRD⁡(C)⋅log⁡|C|)subscript~𝑂𝜀⋅NRD𝐶𝐶\widetilde{O}_{\varepsilon}(\operatorname{NRD}(C)\cdot\log|C|)over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_ε end_POSTSUBSCRIPT ( roman_NRD ( italic_C ) ⋅ roman_log | italic_C | ) bound is that the use of Lemma 3.3 is too conservative. For the purposes of this overview, assume that all codewords of C𝐶Citalic_C have the same Hamming weight d≈NRD⁡(C)𝑑NRD𝐶d\approx\operatorname{NRD}(C)italic_d ≈ roman_NRD ( italic_C ) as that is is the most representative case. Naively, Lemma 3.3 says we should set aside d2superscript𝑑2d^{2}italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT coordinates of [m]delimited-[]𝑚[m][ italic_m ] to “sparsify” all codewords of weight d𝑑ditalic_d. However, we can give a heuristic argument that far fewer than d2superscript𝑑2d^{2}italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT of these potential coordinates contain useful information for our sparsifier. Assume without loss of generality that the support of C𝐶Citalic_C lies in [d2]delimited-[]superscript𝑑2[d^{2}][ italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ]. For each i∈[d2]𝑖delimited-[]superscript𝑑2i\in[d^{2}]italic_i ∈ [ italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ], let pisubscript𝑝𝑖p_{i}italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT be the probability that a codeword c∈C𝑐𝐶c\in Citalic_c ∈ italic_C selected uniformly at random has ci=1subscript𝑐𝑖1c_{i}=1italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1. Since each codeword of C𝐶Citalic_C has Hamming weight d𝑑ditalic_d, we have that p1+⋯+pd2=dsubscript𝑝1⋯subscript𝑝superscript𝑑2𝑑p_{1}+\cdots+p_{d^{2}}=ditalic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + ⋯ + italic_p start_POSTSUBSCRIPT italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT = italic_d. Thus, the average value of pisubscript𝑝𝑖p_{i}italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is 1/d1𝑑1/d1 / italic_d. Consider the case in which each pi=O⁢(1/d)subscript𝑝𝑖𝑂1𝑑p_{i}=O(1/d)italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_O ( 1 / italic_d ). In particular, no coordinate is distinguishing itself as a “must” to add to the sparsifier. A priori, the size of C𝐶Citalic_C may be exp⁡(Ω~⁢(d))~Ω𝑑\exp(\widetilde{\Omega}(d))roman_exp ( over~ start_ARG roman_Ω end_ARG ( italic_d ) ), so we cannot immediately use Chernoff bounds to analyze a random subsampling of the coordinates. To get around this issue, we need to prove a much stronger upper bound on the size of C𝐶Citalic_C, similar to Benczúr and Karger’s cut-counting bound [BK96] and its adaptation to linear codes [KPS24a, KPS24b]. However, we use an entirely new method for proving such bounds based on the entropy method Gilmer [Gil22] developed to prove the union-closed sets conjecture up to a constant factor. In our context, pick t=Θ~⁢(d)𝑡~Θ𝑑t=\widetilde{\Theta}(d)italic_t = over~ start_ARG roman_Θ end_ARG ( italic_d ) and sample uniformly and independently t𝑡titalic_t codewords c1,…,ct∈Csubscript𝑐1…subscript𝑐𝑡𝐶c_{1},\ldots,c_{t}\in Citalic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∈ italic_C. Let c𝑐citalic_c be the bitwise OR of these t𝑡titalic_t codewords, and let 𝒟𝒟\mathcal{D}caligraphic_D be the distribution of c𝑐citalic_c over {0,1}d2superscript01superscript𝑑2\{0,1\}^{d^{2}}{ 0 , 1 } start_POSTSUPERSCRIPT italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT (recall that the weight d𝑑ditalic_d codewords are supported on d2superscript𝑑2d^{2}italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT coordinates). Since each pi=O⁢(1/d)subscript𝑝𝑖𝑂1𝑑p_{i}=O(1/d)italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_O ( 1 / italic_d ), by adapting Gilmer’s method (or more precisely, a refinement due to Sawin [Saw23]), we can show the entropy of 𝒟𝒟\mathcal{D}caligraphic_D is at least Θ~⁢(t)=Θ~⁢(d)~Θ𝑡~Θ𝑑\widetilde{\Theta}(t)=\widetilde{\Theta}(d)over~ start_ARG roman_Θ end_ARG ( italic_t ) = over~ start_ARG roman_Θ end_ARG ( italic_d ) times the entropy of the uniform distribution over C𝐶Citalic_C (i.e., log2⁡|C|subscript2𝐶\log_{2}|C|roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | italic_C |)–a similar inequality appears in [Wak24]. To apply this fact, observe that each sample of 𝒟𝒟\mathcal{D}caligraphic_D lies in the “OROR\operatorname{OR}roman_OR-closure” of C𝐶Citalic_C (denoted by spanOR⁡(C)subscriptspanOR𝐶\operatorname{span}_{\operatorname{OR}}(C)roman_span start_POSTSUBSCRIPT roman_OR end_POSTSUBSCRIPT ( italic_C )). As such, the entropy of 𝒟𝒟\mathcal{D}caligraphic_D is at most log⁡|spanOR⁡(C)|subscriptspanOR𝐶\log\lvert\operatorname{span}_{\operatorname{OR}}(C)\rvertroman_log | roman_span start_POSTSUBSCRIPT roman_OR end_POSTSUBSCRIPT ( italic_C ) |, which by the Sauer-Shelah-Peres lemma is at most (up to log factors) the VC dimension of spanOR⁡(C)subscriptspanOR𝐶\operatorname{span}_{\operatorname{OR}}(C)roman_span start_POSTSUBSCRIPT roman_OR end_POSTSUBSCRIPT ( italic_C ). It is easily seen that the VC dimension of spanOR⁡(C)subscriptspanOR𝐶\operatorname{span}_{\operatorname{OR}}(C)roman_span start_POSTSUBSCRIPT roman_OR end_POSTSUBSCRIPT ( italic_C ) equals the non-redundancy of C𝐶Citalic_C [BCK20]. Therefore, we have proved that Θ~⁢(t)⋅log2⁡(C)≤O~⁢(NRD⁡(C))⋅~Θ𝑡subscript2𝐶~𝑂NRD𝐶\widetilde{\Theta}(t)\cdot\log_{2}(C)\leq\widetilde{O}(\operatorname{NRD}(C))over~ start_ARG roman_Θ end_ARG ( italic_t ) ⋅ roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_C ) ≤ over~ start_ARG italic_O end_ARG ( roman_NRD ( italic_C ) ). Since t≈d≈NRD⁡(C)𝑡𝑑NRD𝐶t\approx d\approx\operatorname{NRD}(C)italic_t ≈ italic_d ≈ roman_NRD ( italic_C ), C𝐶Citalic_C is actually at most quasipolynomial in size! Thus we can now use a Chernoff bound to prove that C𝐶Citalic_C can be subsampled to O~ε⁢(d)subscript~𝑂𝜀𝑑\widetilde{O}_{\varepsilon}(d)over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_ε end_POSTSUBSCRIPT ( italic_d ) coordinates while approximately preserving all Hamming weights. Recall this discussion was purely about the “uniform” case pi=O⁢(1/d)subscript𝑝𝑖𝑂1𝑑p_{i}=O(1/d)italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_O ( 1 / italic_d ). In general, we apply minimax theorem to prove the following “skewed” versus “sparse” dichotomy (see Proposition 4.14): for every code C𝐶Citalic_C and parameter choice θ≥1𝜃1\theta\geq 1italic_θ ≥ 1 there is either a probability distribution 𝒫𝒫\mathcal{P}caligraphic_P over C𝐶Citalic_C for which each coordinate equals 1111 with probability at most 1/θ1𝜃1/\theta1 / italic_θ (i.e., 𝒫𝒫\mathcal{P}caligraphic_P is “θ𝜃\thetaitalic_θ-sparse”); or, there is a probability distribution 𝒬𝒬\mathcal{Q}caligraphic_Q over the coordinates of C𝐶Citalic_C such that for every (nonzero) c∈C𝑐𝐶c\in Citalic_c ∈ italic_C, we have that 𝒬𝒬\mathcal{Q}caligraphic_Q’s measure of supp⁡(c)supp𝑐\operatorname{supp}(c)roman_supp ( italic_c ) is at least 1/θ1𝜃1/\theta1 / italic_θ (i.e., 𝒬𝒬\mathcal{Q}caligraphic_Q is a “θ𝜃\thetaitalic_θ-cover.”) For a suitable choice of θ𝜃\thetaitalic_θ, we repeatedly apply Proposition 4.14 to recursively build the sparsifier: in the θ𝜃\thetaitalic_θ-sparse case, we use the entropy method to prove that a “small” number of codewords of C𝐶Citalic_C can be removed to put us in the θ𝜃\thetaitalic_θ-cover case (see Lemma 4.15); and in the θ𝜃\thetaitalic_θ-cover case, we sample from the θ𝜃\thetaitalic_θ-cover to get a coordinate to add to our sparsifier. This procedure culminates in showing that we can set aside O~ε⁢(NRD⁡(C))subscript~𝑂𝜀NRD𝐶\widetilde{O}_{\varepsilon}(\operatorname{NRD}(C))over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_ε end_POSTSUBSCRIPT ( roman_NRD ( italic_C ) ) coordinates to have weight 1111 in our sparsifier with the remainder of the code being sufficiently sparse that subsampling can be used (Theorem 4.16). Note that the statement of Theorem 4.16 resembles the analogous decompositions for linear codes [KPS24a, KPS24b]. However, their method found all the coordinates to set aside in “one pass,” whereas we iteratively understand the dense and sparse structure of our non-linear code. With Theorem 4.16 in hand, we construct the sparsifier with a recursive argument similar to that of Theorem 3.1. As mentioned earlier, extended these ideas to weighted sparsification (Theorem 1.4) is relatively straightforward. We adapt a weight-binning argument of [KPS24b] by essentially computing an (unweighted) sparsifier for each group of coordinates that is similar in weight (within poly⁡(m)poly𝑚\operatorname{poly}(m)roman_poly ( italic_m )). We then analyze the aggregated size of these sparsifiers by comparing the sum of the non-redundancies of the groups of coordinates to the chain length of the code. Connections to Matching Vector Families. We now switch gears to briefly discussing the key ideas behind Theorem 1.3. Let G:=ℤ/3⁢ℤassign𝐺ℤ3ℤG:=\mathbb{Z}/3\mathbb{Z}italic_G := blackboard_Z / 3 blackboard_Z and recall that 3⁢L⁢I⁢NG={(x,y,z)∣x+y+z=0}subscript3LIN𝐺conditional-set𝑥𝑦𝑧𝑥𝑦𝑧0\operatorname{3LIN}_{G}=\{(x,y,z)\mid x+y+z=0\}start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT = { ( italic_x , italic_y , italic_z ) ∣ italic_x + italic_y + italic_z = 0 } and 3⁢L⁢I⁢NG∗=3⁢L⁢I⁢NG∖{(0,0,0)}subscriptsuperscript3LIN𝐺subscript3LIN𝐺000\operatorname{3LIN}^{*}_{G}=\operatorname{3LIN}_{G}\setminus\{(0,0,0)\}start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT = start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ∖ { ( 0 , 0 , 0 ) }. It is well-known that since 3⁢L⁢I⁢NGsubscript3LIN𝐺\operatorname{3LIN}_{G}start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT is an affine predicate, we have that NRD⁡(3⁢L⁢I⁢NG,n)=Θ⁢(n)NRDsubscript3LIN𝐺𝑛Θ𝑛\operatorname{NRD}(\operatorname{3LIN}_{G},n)=\Theta(n)roman_NRD ( start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT , italic_n ) = roman_Θ ( italic_n ), which is much smaller than our bound on NRD⁡(3⁢L⁢I⁢NG∗,n)NRDsubscriptsuperscript3LIN𝐺𝑛\operatorname{NRD}(\operatorname{3LIN}^{*}_{G},n)roman_NRD ( start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT , italic_n ). As such, we prove that to understand the asymptotics of NRD⁡(3⁢L⁢I⁢NG∗,n)NRDsubscriptsuperscript3LIN𝐺𝑛\operatorname{NRD}(\operatorname{3LIN}^{*}_{G},n)roman_NRD ( start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT , italic_n ) it suffices to look at specially-structured non-redundant instances. Recall that an instance Ψ:=(X,Y)assignΨ𝑋𝑌\Psi:=(X,Y)roman_Ψ := ( italic_X , italic_Y ) of CSP⁡(3⁢L⁢I⁢NG∗)CSPsubscriptsuperscript3LIN𝐺\operatorname{CSP}(\operatorname{3LIN}^{*}_{G})roman_CSP ( start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ) is non redundant if for every clause y∈Y𝑦𝑌y\in Yitalic_y ∈ italic_Y there is an assignment σysubscript𝜎𝑦\sigma_{y}italic_σ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT which satisfies every clause of ΨΨ\Psiroman_Ψ except y𝑦yitalic_y. We show that with at most an additive Θ⁢(n)Θ𝑛\Theta(n)roman_Θ ( italic_n ) change in size, we can assume that σysubscript𝜎𝑦\sigma_{y}italic_σ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT maps y𝑦yitalic_y to (0,0,0)000(0,0,0)( 0 , 0 , 0 ). In other words, each σysubscript𝜎𝑦\sigma_{y}italic_σ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT is a satisfying assignment to ΨΨ\Psiroman_Ψ when viewed as an instance of CSP⁡(3⁢L⁢I⁢NG)CSPsubscript3LIN𝐺\operatorname{CSP}(\operatorname{3LIN}_{G})roman_CSP ( start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ) (see Proposition 6.3). This idea of “conditional” non-redundancy abstracts and generalizes an approach from [BCK20]. Since the set of solutions to an instance of CSP⁡(3⁢L⁢I⁢NG)CSPsubscript3LIN𝐺\operatorname{CSP}(\operatorname{3LIN}_{G})roman_CSP ( start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ) form a vector space (of some dimension, say d𝑑ditalic_d) over 𝔽3subscript𝔽3\mathbb{F}_{3}blackboard_F start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT, we can think of each variable x∈X𝑥𝑋x\in Xitalic_x ∈ italic_X of ΨΨ\Psiroman_Ψ as a vector vx∈𝔽3dsubscript𝑣𝑥superscriptsubscript𝔽3𝑑v_{x}\in\mathbb{F}_{3}^{d}italic_v start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT ∈ blackboard_F start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT and the assignments as linear maps on the vectors. Because we are studying satisfying assignment to CSP⁡(3⁢L⁢I⁢NG)CSPsubscript3LIN𝐺\operatorname{CSP}(\operatorname{3LIN}_{G})roman_CSP ( start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ), these vectors are highly structured: for each y:=(x1,x2,x3)∈Yassign𝑦subscript𝑥1subscript𝑥2subscript𝑥3𝑌y:=(x_{1},x_{2},x_{3})\in Yitalic_y := ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ) ∈ italic_Y, we have that vx1+vx2+vx3=0subscript𝑣subscript𝑥1subscript𝑣subscript𝑥2subscript𝑣subscript𝑥30v_{x_{1}}+v_{x_{2}}+v_{x_{3}}=0italic_v start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT + italic_v start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT + italic_v start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = 0. Further, σysubscript𝜎𝑦\sigma_{y}italic_σ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT can be viewed as a linear map taking each of vx1,vx2,vx3subscript𝑣subscript𝑥1subscript𝑣subscript𝑥2subscript𝑣subscript𝑥3v_{x_{1}},v_{x_{2}},v_{x_{3}}italic_v start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT end_POSTSUBSCRIPT to 00, while mapping at least one vector in every other triple in Y𝑌Yitalic_Y to a nonzero value. We call this family of vectors together with these assignments a G𝐺Gitalic_G-ensemble (Definition 6.5), and note that it bears a strong resemblance to matching vector families. In particular, we adapt techniques used by Dvir, Gopalan, and Yekhanin [DGY11] for constraining the size of matching vector families to give nontrivial upper and lower bounds on the size of G𝐺Gitalic_G-ensembles. For the lower bound (Theorem 6.8), we directly construct a non-redundant instance with Ω⁢(n1.5)Ωsuperscript𝑛1.5\Omega(n^{1.5})roman_Ω ( italic_n start_POSTSUPERSCRIPT 1.5 end_POSTSUPERSCRIPT ) clauses. The proof is self-contained and elementary. The upper bound (Theorem 6.13) is slightly more technical. We break the proof into cases based on whether the embedding dimension d𝑑ditalic_d of the vectors is small (d=O~⁢(n0.4)𝑑~𝑂superscript𝑛0.4d=\widetilde{O}(n^{0.4})italic_d = over~ start_ARG italic_O end_ARG ( italic_n start_POSTSUPERSCRIPT 0.4 end_POSTSUPERSCRIPT )) or large (d=Ω~⁢(n0.4)𝑑~Ωsuperscript𝑛0.4d=\widetilde{\Omega}(n^{0.4})italic_d = over~ start_ARG roman_Ω end_ARG ( italic_n start_POSTSUPERSCRIPT 0.4 end_POSTSUPERSCRIPT )). For small d𝑑ditalic_d, we adapt the polynomial method used in [DGY11] to prove there can be at most O⁢(d4)=O~⁢(n1.6)𝑂superscript𝑑4~𝑂superscript𝑛1.6O(d^{4})=\widetilde{O}(n^{1.6})italic_O ( italic_d start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT ) = over~ start_ARG italic_O end_ARG ( italic_n start_POSTSUPERSCRIPT 1.6 end_POSTSUPERSCRIPT ) non-redundant clauses. On the other hand, when d𝑑ditalic_d is large, we ignore the assignments σysubscript𝜎𝑦\sigma_{y}italic_σ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT and use a careful induction (Lemma 6.11) to show that the geometry of the vectors imply that some x∈X𝑥𝑋x\in Xitalic_x ∈ italic_X is a member of at most O~⁢(n/d)=O~⁢(n0.6)~𝑂𝑛𝑑~𝑂superscript𝑛0.6\widetilde{O}(n/d)=\widetilde{O}(n^{0.6})over~ start_ARG italic_O end_ARG ( italic_n / italic_d ) = over~ start_ARG italic_O end_ARG ( italic_n start_POSTSUPERSCRIPT 0.6 end_POSTSUPERSCRIPT ) clauses, thereby leading to a bound of at most O~⁢(n1.6)~𝑂superscript𝑛1.6\widetilde{O}(n^{1.6})over~ start_ARG italic_O end_ARG ( italic_n start_POSTSUPERSCRIPT 1.6 end_POSTSUPERSCRIPT ) clauses total. Closing the gap between Ω⁢(n1.5)Ωsuperscript𝑛1.5\Omega(n^{1.5})roman_Ω ( italic_n start_POSTSUPERSCRIPT 1.5 end_POSTSUPERSCRIPT ) and O~⁢(n1.6)~𝑂superscript𝑛1.6\widetilde{O}(n^{1.6})over~ start_ARG italic_O end_ARG ( italic_n start_POSTSUPERSCRIPT 1.6 end_POSTSUPERSCRIPT ) for NRD⁡(3⁢L⁢I⁢Nℤ/3⁢ℤ∗,n)NRDsuperscriptsubscript3LINℤ3ℤ𝑛\operatorname{NRD}(\operatorname{3LIN}_{\mathbb{Z}/3\mathbb{Z}}^{*},n)roman_NRD ( start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUBSCRIPT blackboard_Z / 3 blackboard_Z end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT , italic_n ) is a tantalizing open question. 1.7 Open Questions We conclude the introduction with a few directions of further study. See Section 7 and Section 9 for a more thorough discussion of directions for future exploration. • Making Theorem 1.2 efficient. Note that the underlying construction for Theorem 1.1, if made algorithmic, runs in polynomial time with respect to the size of the code, yielding an exp⁡(O⁢(n))𝑂𝑛\exp(O(n))roman_exp ( italic_O ( italic_n ) )-time algorithm121212This is already nontrivial, as a naive guess-and-check algorithm would require exp⁡(O~⁢(NRD⁡(R¯,n)))~𝑂NRD¯𝑅𝑛\exp(\widetilde{O}(\operatorname{NRD}(\overline{R},n)))roman_exp ( over~ start_ARG italic_O end_ARG ( roman_NRD ( over¯ start_ARG italic_R end_ARG , italic_n ) ) ) time. for Theorem 1.2. The primary barrier in constructing our sparsifier in poly⁡(n)poly𝑛\operatorname{poly}(n)roman_poly ( italic_n ) time is the fact that an efficient sparsifier is also a kernelization algorithm, but kernelizing every CSP instance to its non-redundancy is a significant open question in the kernelization community [Car22]. • Computing NRD⁡(R,n)NRD𝑅𝑛\operatorname{NRD}(R,n)roman_NRD ( italic_R , italic_n ). For a general predicate R⊆Dr𝑅superscript𝐷𝑟R\subseteq D^{r}italic_R ⊆ italic_D start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT, there is no simple (even conjectured) expression for NRD⁡(R,n)NRD𝑅𝑛\operatorname{NRD}(R,n)roman_NRD ( italic_R , italic_n ). In fact, even determining when NRD⁡(R,n)=Θ⁢(n)NRD𝑅𝑛Θ𝑛\operatorname{NRD}(R,n)=\Theta(n)roman_NRD ( italic_R , italic_n ) = roman_Θ ( italic_n ) is an open question (e.g., [BCK20, Car22]). In Section 7, we explore a number of predicates from the various parts of the literature whose status is unresolved, including a predicate we categorize as the “simplest unresolved predicate.” • Non-redundancy versus Chain Length. Recall we show that unweighted sparsification is closely tied to non-redundancy while weighted sparsification is closely tied to chain length. For non-linear codes, NRDNRD\operatorname{NRD}roman_NRD and CLCL\operatorname{CL}roman_CL can be very different (e.g., Example 8.8), but the relationship for CSPs is unknown [BCK20, Car22]. In particular, it seems quite possible that there exists a CSP predicate R𝑅Ritalic_R for which wSPR⁡(R,n,ε)/SPR⁡(R,n,ε)=nΩ⁢(1).wSPR𝑅𝑛𝜀SPR𝑅𝑛𝜀superscript𝑛Ω1\operatorname{wSPR}(R,n,\varepsilon)/\operatorname{SPR}(R,n,\varepsilon)=n^{% \Omega(1)}.roman_wSPR ( italic_R , italic_n , italic_ε ) / roman_SPR ( italic_R , italic_n , italic_ε ) = italic_n start_POSTSUPERSCRIPT roman_Ω ( 1 ) end_POSTSUPERSCRIPT . • Average-case behavior. From Theorem 1.2, we know that every instance CSP⁡(R)CSP𝑅\operatorname{CSP}(R)roman_CSP ( italic_R ) has a sparsifier of size approximately its own non-redundancy, even if that value is much smaller than NRD⁡(R¯,n)NRD¯𝑅𝑛\operatorname{NRD}(\overline{R},n)roman_NRD ( over¯ start_ARG italic_R end_ARG , italic_n ). As such, it may be possible that ‘average’ instances of CSP⁡(R)CSP𝑅\operatorname{CSP}(R)roman_CSP ( italic_R ) admit sparsifiers much smaller than the worst case. 1.8 Organization In Section 2, we prove some basic facts about non-redundancy, sparsification and their relationship. In Section 3, we give a straightforward proof that SPR⁡(C,ε)=O~ε⁢(NRD⁡(C)⁢log2⁡|C|)SPR𝐶𝜀subscript~𝑂𝜀NRD𝐶subscript2𝐶\operatorname{SPR}(C,\varepsilon)=\widetilde{O}_{\varepsilon}(\operatorname{% NRD}(C)\log_{2}|C|)roman_SPR ( italic_C , italic_ε ) = over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_ε end_POSTSUBSCRIPT ( roman_NRD ( italic_C ) roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | italic_C | ). In Section 4, we prove Theorem 1.2 by connecting CSP sparsification to non-redundancy via Gilmer’s entropy method. In Section 5, we discuss the immediate applications of Theorem 1.2 based on what is known about non-redundancy in the literature. In Section 6, we bound the non-redundancy of a family of predicates via methods related to matching vector families. In Section 7, we give examples of CSP predicates in the literature whose non-redundancy is unresolved. In Section 8, we extend Theorem 1.2 to weighted instances. In Section 9, we wrap up with other directions of exploration. 1.9 Acknowledgments We thank Libor Barto, Dmitry Zhuk, Madhu Sudan, and Aaron Putterman for valuable conversations. This research was supported in part by a Simons Investigator award and NSF grant CCF-2211972."
https://arxiv.org/html/2411.04057v1,A unified approach to quantum de Finetti theorems and SoS rounding via geometric quantization,"The sum-of-squares hierarchy of semidefinite programs has become a common tool for algorithm design in theoretical computer science, including problems in quantum information. In this work we study a connection between a Hermitian version of the SoS hierarchy, related to the quantum de Finetti theorem, and geometric quantization of compact Kähler manifolds (such as complex projective space ℂ⁢Pdℂsuperscript𝑃𝑑\mathbb{C}P^{d}blackboard_C italic_P start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT, the set of all pure states in a (d+1)𝑑1(d+1)( italic_d + 1 )-dimensional Hilbert space). We show that previously known HSoS rounding algorithms can be recast as quantizing an objective function to obtain a finite-dimensional matrix, finding its top eigenvector, and then (possibly nonconstructively) rounding it by using a version of the Husimi quasiprobability distribution. Dually, we recover most known quantum de Finetti theorems by doing the same steps in the reverse order: a quantum state is first approximated by its Husimi distribution, and then quantized to obtain a separable state approximating the original one. In cases when there is a transitive group action on the manifold we give some new proofs of existing de Finetti theorems, as well as some applications including a new version of Renner’s exponential de Finetti theorem proven using the Borel–Weil–Bott theorem, and hardness of approximation results and optimal degree-2 integrality gaps for the basic SDP relaxation of Quantum Max-d𝑑ditalic_d-Cut (for arbitrary d𝑑ditalic_d). We also describe how versions of these results can be proven when there is no transitive group action. In these cases we can deduce some error bounds for the HSoS hierarchy on complex projective varieties which are smooth.","Quantum mechanics and quantum information are often thought of as being similar to classical probability theory, but the analogy is of course not exact and must break down at some point. The point where the analogy fails is often illustrated through the use of quasiprobability distributions, which associate a classical “distribution” (over phase space) to a quantum state in a way which preserves as many properties as possible. In the semiclassical limit ℏ→0→Planck-constant-over-2-pi0\hbar\to 0roman_ℏ → 0, the mapping must preserve essentially all properties of the state. A popular choice is the Wigner quasiprobability distribution, which preserves many properties of the original quantum state, but may take on negative values and is thus not a classical probability distribution. Elsewhere in computer science, negative probabilities have arisen in optimization. One technique for approximating solutions to non-convex problems involves relaxing a problem of the form maxx∈D⁡p⁢(x)subscript𝑥𝐷𝑝𝑥\max_{x\in D}p(x)roman_max start_POSTSUBSCRIPT italic_x ∈ italic_D end_POSTSUBSCRIPT italic_p ( italic_x ) for some domain D𝐷Ditalic_D and objective function p𝑝pitalic_p into a convex problem of the form maxμ∈𝒫⁢(D)⁡Eμ⁢[p⁢(x)]subscript𝜇𝒫𝐷subscript𝐸𝜇delimited-[]𝑝𝑥\max_{\mu\in\mathcal{P}(D)}E_{\mu}[p(x)]roman_max start_POSTSUBSCRIPT italic_μ ∈ caligraphic_P ( italic_D ) end_POSTSUBSCRIPT italic_E start_POSTSUBSCRIPT italic_μ end_POSTSUBSCRIPT [ italic_p ( italic_x ) ], where 𝒫⁢(D)𝒫𝐷\mathcal{P}(D)caligraphic_P ( italic_D ) is set of probability distributions over D𝐷Ditalic_D. Since this new convex problem often has dimension which is exponentially or infinitely large, the strategy of sum-of-squares (SoS) optimization is to further enlarge the set of distributions to degree-k𝑘kitalic_k pseudo-distributions, which have an efficient finite-dimensional representation. Such pseudo-distributions also have an interpretation in terms of negative probabilities: a degree-k𝑘kitalic_k pseudo-distribution can be defined as a density function P:D→ℝ:𝑃→𝐷ℝP:D\to\mathbb{R}italic_P : italic_D → blackboard_R (relative to some base measure μ𝜇\muitalic_μ) such that ∫Dq⁢(x)⁢P⁢(x)⁢𝑑μ⁢(x)≥0subscript𝐷𝑞𝑥𝑃𝑥differential-d𝜇𝑥0\int_{D}q(x)P(x)\,d\mu(x)\geq 0∫ start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT italic_q ( italic_x ) italic_P ( italic_x ) italic_d italic_μ ( italic_x ) ≥ 0 for all degree-k𝑘kitalic_k polynomials q𝑞qitalic_q which are sums-of-squares. As the degree k→∞→𝑘k\to\inftyitalic_k → ∞, the value of the relaxed problem approaches that of the original one. It is natural to ask whether there is a connection between these two notions of negative probability, where where the inverse 1/ℏ1Planck-constant-over-2-pi1/\hbar1 / roman_ℏ of the semiclassical parameter is analogous to the SoS degree k𝑘kitalic_k. In this work, we propose an affirmative answer to this question. 1.1. Approaches to nonlinear classical optimization For solving purely a classical problem which is nonlinear, such as optimization of a nonlinear polynomial function over a compact semialgebraic set, a common approach involves first constructing a finite-dimensional linear approximation to the problem, solving it, and then using the solution to approximate the original nonlinear problem. Indeed, the sum-of-squares optimization hierarchy does exactly this, where the finite-dimensional approximation is a semidefinite program with a linear objective function. Another common approach is to construct a finite set of reasonably separated points on which the objective function can be evaluated, and then approximating the optimum on the entire space by the optimum on the finite set. In either case, the approximation to the nonlinear problem must be finite in some way – being finite-dimensional in the case of linearization, or having finite cardinality in the case of brute-forcing over a finite subset. Motivated by physics, one general approach to approximating a nonlinear function by a linear operator is quantization. In quantization one thinks of a nonlinear function as being a classical observable on some phase space, and the corresponding linear operator is the corresponding quantum observable acting on a Hilbert space, which should again correspond to the classical phase space in some way. The correspondence should depend on a parameter ℏPlanck-constant-over-2-pi\hbarroman_ℏ, and in the semiclassical limit ℏ→0→Planck-constant-over-2-pi0\hbar\to 0roman_ℏ → 0 the correspondence should become closer and closer. This approach suggests that there should be corresponding algorithms for various computational tasks, but for optimizing a nonlinear polynomial function in particular there is a clear analogy: one would first quantize the objective function, and then approximate its minimum or maximum by the minimum or maximum of the corresponding observable. One could even try to approximate the optimal solution itself by finding a classical distribution over the phase space which approximates the top eigenstate of the observable, and then sampling from it. One of the most common such quasiprobability distributions is the Husimi Q-function, which, unlike other common quasiprobability distributions, such as the Wigner function or Glauber-Sudarshan P-function, is nonnegative and integrates to 1, giving a well-defined classical probability distribution. The parameter ℏPlanck-constant-over-2-pi\hbarroman_ℏ then allows one to tune the approximation, for which a smaller value of ℏPlanck-constant-over-2-pi\hbarroman_ℏ would give a more accurate approximation but require more resources computationally. Conversely, one could also try to approximate a quantum state by a classical objective. Without additional assumptions one typically expects a quantum state on a large number of sites to have nontrivial entanglement. However, under symmetry conditions the principle of monogamy of entanglement suggests that no two sites should be strongly entangled with each other, implying that the overall state behaves classically. Indeed, the quantum de Finetti theorem [Christandl_Koenig_Mitchison_Renner_2007] formalizes this intuition with a quantitative error bound. The construction in the de Finetti theorem is quite similar to the rounding algorithm described above – one can think of the Husimi quasiprobability distribution as a mapping from a quantum mixed state to a classical mixed state, and the Glauber-Sudharshan P-quantization as a mapping in the opposite direction, such that the two maps are approximate inverses. 1.2. Results in this paper As we will see, it is possible to make sense of some version of this quantization-based optimization algorithm. It will turn out to be exactly the same as a Hermitian version of the sum-of-squares hierarchy, but several assumptions will need to be made on the original problem. First, the domain M𝑀Mitalic_M of the optimization problem should be a compact semialgebraic set in order for the original optimization problem to make sense and for a Hermitian SoS relaxation to even be defined. To be able to think of M𝑀Mitalic_M as a classical phase space we will also need M𝑀Mitalic_M to be a smooth manifold, and have a Poisson bracket defined on pairs of classical observables, which is the minimum structure needed to define time-evolution of a system in classical mechanics. Because of the quantization procedure we use, we will actually need to further assume that the Poisson bracket comes from a symplectic structure on M𝑀Mitalic_M and that M𝑀Mitalic_M is also a polarized Kähler manifold (comes with a holomorphic line bundle ℒ→M→ℒ𝑀\mathcal{L}\to Mcaligraphic_L → italic_M which defines an embedding into a projective space). 1.3. Content of this paper In Section 2, we give a more detailed technical overview and give a more precise definition of quantization. Then in Section 3 we describe one of the simplest examples of quantization, which is a single bosonic mode with phase space ℂ≅ℝ2ℂsuperscriptℝ2\mathbb{C}\cong\mathbb{R}^{2}blackboard_C ≅ blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. (Despite the Hilbert space being infinite-dimensional in this case, it is overall simpler because there is no topology involved.) In Section 4 we describe geometric quantization, which is one standard approach to quantizing a compact phase space, which in general must be topologically nontrivial. In Section 5 we state and prove Theorem 19, which is a version of the de Finetti theorem generalized to the geometric quantization framework. Finally in Section 6 we show how to deduce various known quantum de Finetti theorems, including a version of Renner’s exponential de Finetti theorem and the de Finetti theorem for irreducible representations of U⁢(n)𝑈𝑛U(n)italic_U ( italic_n ), from our general theorem, and how to deduce some new de Finetti theorems for other phase spaces, including smooth projective varities; in these cases we need to apply some recent results in geoemtric analysis and complex geometry to show that the assumptions of Theorem 19 can be satisfied, and also to explain how they can be used to deduce quantitative error bounds."
https://arxiv.org/html/2411.04013v1,k𝑘kitalic_kNN Attention Demystified: A Theoretical Exploration for Scalable Transformers,"Despite their power, Transformers [39] face challenges with long sequences due to the quadratic complexity of self-attention. To address this limitation, methods like k𝑘kitalic_k-Nearest-Neighbor (k𝑘kitalic_kNN) attention have been introduced [32], enabling each token to attend to only its k𝑘kitalic_k closest tokens. While k𝑘kitalic_kNN attention has shown empirical success in making Transformers more efficient, its exact approximation guarantees have not been theoretically analyzed. In this work, we establish a theoretical framework for k𝑘kitalic_kNN attention, reformulating self-attention as expectations over softmax distributions and leveraging lazy Gumbel sampling [28] with k𝑘kitalic_kNN indices for efficient approximation. Building on this framework, we also propose novel sub-quadratic algorithms that approximate self-attention gradients by leveraging efficient sampling techniques, such as Markov Chain-based estimation. Finally, we demonstrate the practical effectiveness of these algorithms through empirical experiments, showcasing their benefits in both training and inference.","Transformer models have become the dominant neural architecture across language, vision, and other domains [39, 13]. However, scaling them to handle larger input sequences remains a significant challenge [38], primarily due to the quadratic complexity of computing self-attention. Overcoming this limitation is crucial for advancing neural networks. Extending context length would enable Transformers to tackle complex tasks like book summarization [23] and time-series forecasting [43, 45, 47]. Furthermore, improving attention efficiency would reduce the computational burden of training, making these models more accessible. Bridging this “compute divide” is vital for democratizing AI [5]. Efficient computation of self-attention has been a focal point of research in recent years [15]. Flash Attention [14] and related work [36] optimize the exact calculation of attention by minimizing wasted computation during GPU I/O operations. However, most approaches focus on approximating the attention function. Sparse Transformers improve efficiency by allowing each token to attend to only a small subset of tokens [27]. These subsets are identified through deterministic methods [8, 18, 35, 26, 31, 7], randomized algorithms [22, 19, 46, 30], or adaptive techniques [11]. Additionally, self-attention is often approximated using low-rank matrices and kernel methods [40, 37, 44, 24, 10]. On the negative side, recent fine-grained complexity reductions indicate that achieving a good approximation with sub-quadratic time is not feasible across all scenarios [25, 3]. In this work, we focus on sparse attention methods where each token vector qi∈ℝdsubscript𝑞𝑖superscriptℝ𝑑q_{i}\in\mathbb{R}^{d}italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT attends to the k𝑘kitalic_k tokens kj∈ℝdsubscript𝑘𝑗superscriptℝ𝑑k_{j}\in\mathbb{R}^{d}italic_k start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT with the largest inner products qiT⁢kjsuperscriptsubscript𝑞𝑖𝑇subscript𝑘𝑗q_{i}^{T}k_{j}italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_k start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT [16, 42], a paradigm we refer to as k𝑘kitalic_kNN Attention. The Routing Transformer [32] was an early example, using k𝑘kitalic_k-means clustering to ensure each query only attends to keys within the same cluster. Memorizing Transformers [41] later extended this approach by leveraging k𝑘kitalic_kNN search within a stored memory, enabling models to memorize new data during inference. More recently, Unlimiformer models [6] have improved efficiency by using a single k𝑘kitalic_kNN data structure (or index) across all attention heads and layers. Previous works have empirically shown that k𝑘kitalic_kNN Attention not only improves computational efficiency, but also enhances model architectures and capabilities. However, a rigorous theoretical analysis of k𝑘kitalic_kNN Attention is still lacking. Key questions remain unresolved, including the precise approximation guarantees it offers, the optimal value of k𝑘kitalic_k, and how to extend the method to approximate the backward pass. Notation Let Q,K,V∈ℝn×d𝑄𝐾𝑉superscriptℝ𝑛𝑑Q,K,V\in\mathbb{R}^{n\times d}italic_Q , italic_K , italic_V ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_d end_POSTSUPERSCRIPT be our query, key and value matrices. Let qi=Qi,:∈ℝdsubscript𝑞𝑖subscript𝑄𝑖:superscriptℝ𝑑q_{i}=Q_{i,:}\in\mathbb{R}^{d}italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_Q start_POSTSUBSCRIPT italic_i , : end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT be i𝑖iitalic_i-th row of Q𝑄Qitalic_Q written as a column vector. We will also denote the j𝑗jitalic_j-th column of Q𝑄Qitalic_Q by Q:,jsubscript𝑄:𝑗Q_{:,j}italic_Q start_POSTSUBSCRIPT : , italic_j end_POSTSUBSCRIPT. We define A:=Q⁢KT∈ℝn×nassign𝐴𝑄superscript𝐾𝑇superscriptℝ𝑛𝑛A:=QK^{T}\in\mathbb{R}^{n\times n}italic_A := italic_Q italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_n end_POSTSUPERSCRIPT to be the attention matrix, and O=softmax⁢(A)⋅V∈ℝn×d𝑂⋅softmax𝐴𝑉superscriptℝ𝑛𝑑O=\text{softmax}(A)\cdot V\in\mathbb{R}^{n\times d}italic_O = softmax ( italic_A ) ⋅ italic_V ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_d end_POSTSUPERSCRIPT to be the output of the attention function. The softmax function is applied row-wise to A𝐴Aitalic_A and is defined as a vector valued function σ:ℝn→ℝn:𝜎→superscriptℝ𝑛superscriptℝ𝑛\sigma:\mathbb{R}^{n}\to\mathbb{R}^{n}italic_σ : blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT → blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT: σ⁢(y1,…,yn)i=exp⁡(yi)∑s=1nexp⁡(ys)𝜎subscriptsubscript𝑦1…subscript𝑦𝑛𝑖subscript𝑦𝑖superscriptsubscript𝑠1𝑛subscript𝑦𝑠\sigma(y_{1},...,y_{n})_{i}=\frac{\exp(y_{i})}{\sum_{s=1}^{n}\exp(y_{s})}italic_σ ( italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_y start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = divide start_ARG roman_exp ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_s = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT roman_exp ( italic_y start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) end_ARG We also let [n]:={1,2,…,n}assigndelimited-[]𝑛12…𝑛[n]:=\{1,2,...,n\}[ italic_n ] := { 1 , 2 , … , italic_n } and use the notation polylog⁢(n)polylog𝑛\text{polylog}(n)polylog ( italic_n ) as a substitute of logk⁡(n)superscript𝑘𝑛\log^{k}(n)roman_log start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ( italic_n ) for some arbitrary constant k∈ℤ+𝑘superscriptℤk\in\mathbb{Z}^{+}italic_k ∈ blackboard_Z start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT that is independent of n𝑛nitalic_n. Finally, we use the O~~𝑂\widetilde{O}over~ start_ARG italic_O end_ARG notation to hide polylogarithmic factors. We will often make use of the following boosting lemma: Lemma 1 (Median-Of-Means Boosting, [9]). If Q^^𝑄\widehat{Q}over^ start_ARG italic_Q end_ARG is an unbiased estimator of some statistic, then one can obtain an (ε,δ)𝜀𝛿(\varepsilon,\delta)( italic_ε , italic_δ )-multiplicative estimate of that statistic by suitably combining K:=Cε2⁢Var⁢[Q^]𝔼⁢[Q^]2⁢ln⁡2δassign𝐾𝐶superscript𝜀2Vardelimited-[]^𝑄𝔼superscriptdelimited-[]^𝑄22𝛿K:=\frac{C}{\varepsilon^{2}}\frac{\text{Var}[\widehat{Q}]}{\mathbb{E}[\widehat% {Q}]^{2}}\ln\frac{2}{\delta}italic_K := divide start_ARG italic_C end_ARG start_ARG italic_ε start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG divide start_ARG Var [ over^ start_ARG italic_Q end_ARG ] end_ARG start_ARG blackboard_E [ over^ start_ARG italic_Q end_ARG ] start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG roman_ln divide start_ARG 2 end_ARG start_ARG italic_δ end_ARG independent samples of Q^^𝑄\widehat{Q}over^ start_ARG italic_Q end_ARG, where C𝐶Citalic_C is a universal constant. For a comprehensive outline of preliminary results and theory, please refer to Appendix A. 1.1 Our Contributions 1.1.1 A Theoretical Framework for k𝑘kitalic_kNN Attention Our work provides a theoretical framework to explain both the efficiency and effectiveness of k𝑘kitalic_kNN Attention. Our framework reformulates self-attention as expectations over softmax distributions. These expectations are approximated by sampling from each distribution in sublinear time using Lazy Gumbel Noise Sampling. By connecting k𝑘kitalic_kNN, k𝑘kitalic_k-Maximum Inner Product Search (MIPS), and Gumbel noise sampling, we develop a new sub-quadratic self-attention approximation algorithm aligning with the k𝑘kitalic_kNN Attention paradigm, as summarized in the following informal theorem: Theorem 2. Let Q,K,V∈ℝn×d𝑄𝐾𝑉superscriptℝ𝑛𝑑Q,K,V\in\mathbb{R}^{n\times d}italic_Q , italic_K , italic_V ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_d end_POSTSUPERSCRIPT and ε,δ𝜀𝛿\varepsilon,\deltaitalic_ε , italic_δ be positive constants. Assume ‖V‖∞=O⁢(log⁡n)subscriptnorm𝑉𝑂𝑛||V||_{\infty}=O(\log n)| | italic_V | | start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT = italic_O ( roman_log italic_n ). Then k𝑘kitalic_kNN-Attention as presented in Algorithm 2 with k=n𝑘𝑛k=\sqrt{n}italic_k = square-root start_ARG italic_n end_ARG outputs a matrix O^∈ℝn×d^𝑂superscriptℝ𝑛𝑑\widehat{O}\in\mathbb{R}^{n\times d}over^ start_ARG italic_O end_ARG ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_d end_POSTSUPERSCRIPT such that: |O^i⁢j−Oi⁢j|≤ε⁢Oi⁢jsubscript^𝑂𝑖𝑗subscript𝑂𝑖𝑗𝜀subscript𝑂𝑖𝑗\displaystyle|\widehat{O}_{ij}-O_{ij}|\leq\varepsilon O_{ij}| over^ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT - italic_O start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT | ≤ italic_ε italic_O start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT (1) for all (i,j)∈[n]×[d]𝑖𝑗delimited-[]𝑛delimited-[]𝑑(i,j)\in[n]\times[d]( italic_i , italic_j ) ∈ [ italic_n ] × [ italic_d ] with probability at least 1−δ1𝛿1-\delta1 - italic_δ and in sub-quadratic time and space. 1.1.2 Approximating the Backward Pass Our framework can be extended to solve the problem of approximating attention gradients. Even though backpropagation is the main memory bottleneck for large models, few methods approximate attention gradients directly. Alman and Song’s work () is most relevant, deriving inapproximability results for certain parameter regimes. We present new approximation algorithms for self-attention gradients using k𝑘kitalic_kNN search. A key challenge is the need to multiply by the transpose of a stochastic matrix, which disrupts our expectation-based reformulation. To address this, we use a Markov-Chain sampling technique, treating the attention matrix as a transition matrix and applying a single-step iteration. Our main theorem can be informally stated as follows: Theorem 3. Let ϕitalic-ϕ\phiitalic_ϕ be a scalar loss function and ∂ϕ/∂O∈ℝn×ditalic-ϕ𝑂superscriptℝ𝑛𝑑\partial\phi/\partial O\in\mathbb{R}^{n\times d}∂ italic_ϕ / ∂ italic_O ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_d end_POSTSUPERSCRIPT. Then, under certain assumptions on the ||⋅||∞||\cdot||_{\infty}| | ⋅ | | start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT norms of Q,K,V𝑄𝐾𝑉Q,K,Vitalic_Q , italic_K , italic_V, there exist sub-quadratic time algorithms that output estimates D^Q,D^K,D^V∈ℝn×dsuperscript^𝐷𝑄superscript^𝐷𝐾superscript^𝐷𝑉superscriptℝ𝑛𝑑\widehat{D}^{Q},\widehat{D}^{K},\widehat{D}^{V}\in\mathbb{R}^{n\times d}over^ start_ARG italic_D end_ARG start_POSTSUPERSCRIPT italic_Q end_POSTSUPERSCRIPT , over^ start_ARG italic_D end_ARG start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT , over^ start_ARG italic_D end_ARG start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_d end_POSTSUPERSCRIPT for which with probability at least 1−δ1𝛿1-\delta1 - italic_δ it holds that: ‖D^Q−∂ϕ/∂Q‖∞≤eQ,‖D^K−∂ϕ/∂K‖∞≤eK⁢ and ⁢‖D^V−∂ϕ/∂V‖∞≤eVformulae-sequencesubscriptnormsuperscript^𝐷𝑄italic-ϕ𝑄subscript𝑒𝑄subscriptnormsuperscript^𝐷𝐾italic-ϕ𝐾subscript𝑒𝐾 and subscriptnormsuperscript^𝐷𝑉italic-ϕ𝑉subscript𝑒𝑉\displaystyle||\widehat{D}^{Q}-\partial\phi/\partial Q||_{\infty}\leq e_{Q},\,% ||\widehat{D}^{K}-\partial\phi/\partial K||_{\infty}\leq e_{K}\,\text{ and }\,% ||\widehat{D}^{V}-\partial\phi/\partial V||_{\infty}\leq e_{V}| | over^ start_ARG italic_D end_ARG start_POSTSUPERSCRIPT italic_Q end_POSTSUPERSCRIPT - ∂ italic_ϕ / ∂ italic_Q | | start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT ≤ italic_e start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT , | | over^ start_ARG italic_D end_ARG start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT - ∂ italic_ϕ / ∂ italic_K | | start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT ≤ italic_e start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT and | | over^ start_ARG italic_D end_ARG start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT - ∂ italic_ϕ / ∂ italic_V | | start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT ≤ italic_e start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT (2) where eQ,eK,eVsubscript𝑒𝑄subscript𝑒𝐾subscript𝑒𝑉e_{Q},e_{K},e_{V}italic_e start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT are explicit error parameters that can roughly be bounded by O⁢(ε⁢n⋅polylog⁢(n))𝑂⋅𝜀𝑛polylog𝑛O(\varepsilon n\cdot\text{polylog}(n))italic_O ( italic_ε italic_n ⋅ polylog ( italic_n ) ) Algorithm 4 computes D^Vsuperscript^𝐷𝑉\widehat{D}^{V}over^ start_ARG italic_D end_ARG start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT, while Algorithms for D^Ksuperscript^𝐷𝐾\widehat{D}^{K}over^ start_ARG italic_D end_ARG start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT and D^Qsuperscript^𝐷𝑄\widehat{D}^{Q}over^ start_ARG italic_D end_ARG start_POSTSUPERSCRIPT italic_Q end_POSTSUPERSCRIPT can be found in Appendices E and F."
https://arxiv.org/html/2411.03390v1,Six Candidates Suffice to Win a Voter Majority,"A cornerstone of social choice theory is Condorcet’s paradox which says that in an election where n𝑛nitalic_n voters rank m𝑚mitalic_m candidates it is possible that, no matter which candidate is declared the winner, a majority of voters would have preferred an alternative candidate. Instead, can we always choose a small committee of winning candidates that is preferred to any alternative candidate by a majority of voters?Elkind, Lang, and Saffidine raised this question and called such a committee a Condorcet winning set. They showed that winning sets of size 2222 may not exist, but sets of size logarithmic in the number of candidates always do. In this work, we show that Condorcet winning sets of size 6666 always exist, regardless of the number of candidates or the number of voters. More generally, we show that if α1−ln⁡α≥2k+1𝛼1𝛼2𝑘1\frac{\alpha}{1-\ln\alpha}\geq\frac{2}{k+1}divide start_ARG italic_α end_ARG start_ARG 1 - roman_ln italic_α end_ARG ≥ divide start_ARG 2 end_ARG start_ARG italic_k + 1 end_ARG, then there always exists a committee of size k𝑘kitalic_k such that less than an α𝛼\alphaitalic_α fraction of the voters prefer an alternate candidate. These are the first nontrivial positive results that apply for all k≥2𝑘2k\geq 2italic_k ≥ 2.Our proof uses the probabilistic method and the minimax theorem, inspired by recent work on approximately stable committee selection. We construct a distribution over committees that performs sufficiently well (when compared against any candidate on any small subset of the voters) so that this distribution must contain a committee with the desired property in its support.","Voting is a versatile model for the aggregation of individual preferences to reach a collective decision. Disparate situations, such as constituents choosing representatives, organizations hiring employees, judges choosing prize winners, and even friends choosing games to play, can all be understood as a group of voters choosing from a pool of candidates. Voting theory seeks to understand how winning candidates can be selected in a fair and representative manner. One of the longest known challenges with voting is Condorcet’s paradox, discovered by Nicolas de Condorcet around the French Revolution [dC85].111It is plausible that in early academic explorations of voting, 13th-century philosopher Ramon Llull had already discovered the possibility of this paradoxical situation [Llu83, HP00]. The paradox is that in an election where voters have ranked preferences over candidates, the preferences of the “majority” can be contradictory — no matter which candidate is declared the winner, a majority of the voters would have preferred another candidate. In fact, the contradiction can be even more dramatic, with “majority” replaced by a fraction arbitrarily close to 1. An illustrative example is when the voters have cyclic preferences as, for example, displayed in Table 1. v1subscript𝑣1v_{1}italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT v2subscript𝑣2v_{2}italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT v3subscript𝑣3v_{3}italic_v start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT v4subscript𝑣4v_{4}italic_v start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT v5subscript𝑣5v_{5}italic_v start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT v6subscript𝑣6v_{6}italic_v start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPT 1 2 3 4 5 6 2 3 4 5 6 1 3 4 5 6 1 2 4 5 6 1 2 3 5 6 1 2 3 4 6 1 2 3 4 5 Table 1: An election where voters have cyclic preferences. The column headed with visubscript𝑣𝑖v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT represents the i𝑖iitalic_ith voter’s ranking of the candidates (labeled 1,2,…,612…61,2,\dots,61 , 2 , … , 6 from top to bottom). For each candidate, another candidate is preferred by every voter except one. Though it is impossible to always find a single candidate that is always preferred over the others by a majority (called a Condorcet winner), one hope is that relaxations of this condition are still possible to achieve. A natural relaxation arises in the setting of committee selection, where rather than choosing a single winner, the goal is to choose a committee of k𝑘kitalic_k winners. For example, a political system may have districts with multiple representatives, organizations may make many hires at once, and friends might play more than one game in an evening. Another view is that committee selection can be used as an filtering step in a process with more than one round, like primaries or runoffs, choosing interviewees for a position, or nominations for a prize. In this context, Elkind, Lang, and Saffidine [ELS11, ELS15] asked: is it always possible to find a small committee of candidates such that no other candidate is preferred by a majority of voters over each member of the committee? They called this committee-analogue of a Condorcet winner a Condorcet winning set, and defined the Condorcet dimension of an election as the size of its smallest Condorcet winning set. For example, the election depicted in Table 1 has Condorcet dimension 2, since any pair of diametrically opposite candidates such as {3,6}36\{3,6\}{ 3 , 6 } would be a Condorcet winning set. More generally, [ELS15] raised the following question for an arbitrary threshold of α𝛼\alphaitalic_α in place of 1212\frac{1}{2}divide start_ARG 1 end_ARG start_ARG 2 end_ARG, and a target committee size k𝑘kitalic_k. Question 1 ([ELS15]). A committee S𝑆Sitalic_S is α𝛼\alphaitalic_α-undominated if for all candidates a∉S𝑎𝑆a\notin Sitalic_a ∉ italic_S, less than an α𝛼\alphaitalic_α fraction of voters prefer a𝑎aitalic_a over each member of S𝑆Sitalic_S. For what values of k∈ℤ+𝑘superscriptℤk\in\mathbb{Z}^{+}italic_k ∈ blackboard_Z start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT and α∈(0,1]𝛼01\alpha\in(0,1]italic_α ∈ ( 0 , 1 ] does every election have an α𝛼\alphaitalic_α-undominated committee of size k𝑘kitalic_k? In particular, we would like to know, for each k𝑘kitalic_k, what is the smallest α𝛼\alphaitalic_α for which α𝛼\alphaitalic_α-undominated committees of size k𝑘kitalic_k always exist (and, equivalently, for each α𝛼\alphaitalic_α, the smallest k𝑘kitalic_k such that these committees always exist). Condorcet’s paradox (or rather, its aformentioned generalization) shows that for k=1𝑘1k=1italic_k = 1 and any α𝛼\alphaitalic_α bounded away from 1, there are elections with no α𝛼\alphaitalic_α-undominated singleton candidates. For the threshold of α=12𝛼12\alpha=\frac{1}{2}italic_α = divide start_ARG 1 end_ARG start_ARG 2 end_ARG, [ELS15] constructed instances with Condorcet dimension 3 by taking the Kronecker product of two elections with cyclic preferences (see Table 3). This construction can be easily extended to give a lower bound of 2k+12𝑘1\frac{2}{k+1}divide start_ARG 2 end_ARG start_ARG italic_k + 1 end_ARG on the smallest α𝛼\alphaitalic_α such that there always exists an α𝛼\alphaitalic_α-undominated committee of size k𝑘kitalic_k (see Appendix B). They also showed that an election with m𝑚mitalic_m candidates has Condorcet dimension at most ⌈log2⁡m⌉subscript2𝑚\lceil\log_{2}m\rceil⌈ roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_m ⌉; to see this, note that some candidate beats a majority of the other candidates, so we can iteratively add such a candidate to our committee and remove all the candidates that it beats. 1.1 Our Contributions We prove that every election has Condorcet dimension at most 6. This result is a corollary of our main theorem, which gives a nontrivial existence result for α𝛼\alphaitalic_α-undominated committees of size k≥2𝑘2k\geq 2italic_k ≥ 2. We note that the final result we prove (Theorem 5) is stronger, but we start with the approximation below as it is easier to get a handle on. (For a comparison, see Table 2.) Theorem 1. If α1−ln⁡α≥2k+1𝛼1𝛼2𝑘1\frac{\alpha}{1-\ln\alpha}\geq\frac{2}{k+1}divide start_ARG italic_α end_ARG start_ARG 1 - roman_ln italic_α end_ARG ≥ divide start_ARG 2 end_ARG start_ARG italic_k + 1 end_ARG, then in any election, there exists an α𝛼\alphaitalic_α-undominated committee of size k𝑘kitalic_k. For the specific threshold of α=12𝛼12\alpha=\frac{1}{2}italic_α = divide start_ARG 1 end_ARG start_ARG 2 end_ARG, Theorem 1 applies as long as k≥3+4⁢ln⁡2≈5.77𝑘3425.77k\geq 3+4\ln 2\approx 5.77italic_k ≥ 3 + 4 roman_ln 2 ≈ 5.77, and so any election has Condorcet dimension at most 6666 (which is not far from the lower bound of 3333). Taking k=2𝑘2k=2italic_k = 2, Theorem 1 implies that there always exists a pair of candidates such that no third candidate is preferred by more than roughly 80%percent8080\%80 % of the voters. Even replacing 80%percent8080\%80 % with 99%percent9999\%99 %, this was previously unknown. These results show that just by having a few winners instead of one, the most dramatic failures of Condorcet’s paradox are avoidable. We emphasize that these results hold for any election, regardless of the number of voters, the number of candidates, or the preferences that the voters have over candidates. Our starting point for proving Theorem 1 is the observation that 1 is closely linked to the problem of approximate stability in committee selection [JMW20]. The principle behind stability is that a subset of voters should have control over a subset of the committee of proportional size. That is, a committee of size k𝑘kitalic_k is stable (also referred to as in the core [Sca67, Fol70, FMS18]) if the fraction of voters that prefers any committee of size k′superscript𝑘′k^{\prime}italic_k start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT is less than k′ksuperscript𝑘′𝑘\frac{k^{\prime}}{k}divide start_ARG italic_k start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_ARG start_ARG italic_k end_ARG. We note that in this setting, voters have preferences over committees rather than candidates. This more expressive space of preferences gives it the power to model a wide variety of preference structures, such as approval voting and participatory budgeting. Unfortunately, in many settings, stable committees do not always exist. To remedy this, [JMW20] put forth the following approximate notion of stability, and showed the surprising result that for any monotone preference structure and any k𝑘kitalic_k, a 32323232-stable committee of size k𝑘kitalic_k exists. Definition 1 (Approximately stable committees [JMW20]). A committee S𝑆Sitalic_S of k𝑘kitalic_k candidates is c𝑐citalic_c-stable if for any committee S′superscript𝑆′S^{\prime}italic_S start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT of size k′superscript𝑘′k^{\prime}italic_k start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT, the fraction of voters that prefers S′superscript𝑆′S^{\prime}italic_S start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT over S𝑆Sitalic_S is less than c⋅k′k⋅𝑐superscript𝑘′𝑘c\cdot\frac{k^{\prime}}{k}italic_c ⋅ divide start_ARG italic_k start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_ARG start_ARG italic_k end_ARG. Consider the natural preference order over committees induced by rankings over candidates, where v𝑣vitalic_v prefers S′superscript𝑆′S^{\prime}italic_S start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT over S𝑆Sitalic_S if and only if she prefers her favorite candidate in S′superscript𝑆′S^{\prime}italic_S start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT over her favorite in S𝑆Sitalic_S. A simple observation (explained more fully in Appendix A) shows that a committee of size k𝑘kitalic_k is c𝑐citalic_c-stable if and only if it is ck𝑐𝑘\frac{c}{k}divide start_ARG italic_c end_ARG start_ARG italic_k end_ARG-undominated. For this ranked preference structure, the constant of 32323232 in the result of [JMW20] can be improved to 16161616 using the existence of stable lotteries for these preferences [CJMW20]. Then, as a black box, [JMW20] implies that 16k16𝑘\frac{16}{k}divide start_ARG 16 end_ARG start_ARG italic_k end_ARG-undominated committees of size k𝑘kitalic_k always exist, which in turn implies that we can always find Condorcet winning sets of size at most 32323232. Since this conclusion follows easily from [JMW20], we attribute the first constant upper bound on the size of Condorcet winning sets to their work. One can interpret the approximately stable committee problem as a version of 1 focused on the asymptotics of α𝛼\alphaitalic_α as the committee size k𝑘kitalic_k grows large. For this purpose, [JMW20] implies a result that is optimal up to a constant factor, but it says nothing nontrivial for committees of size at most 16161616. In contrast, Theorem 1 gives results even for k=2𝑘2k=2italic_k = 2, and outperforms the bound implied by [JMW20] for k≤1.75×104𝑘1.75superscript104k\leq 1.75\times 10^{4}italic_k ≤ 1.75 × 10 start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT, despite only implying the existence of O⁢(log⁡k)𝑂𝑘O(\log k)italic_O ( roman_log italic_k )-stable committees. Nonetheless, we show that our techniques can be applied to the asymptotic setting as well, giving an improvement over [JMW20]. Theorem 2. In any election, there exists a 9.8217k9.8217𝑘\frac{9.8217}{k}divide start_ARG 9.8217 end_ARG start_ARG italic_k end_ARG-undominated committee of size k𝑘kitalic_k. As a corollary, Theorem 2 implies the existence of 9.82179.82179.82179.8217-stable committees for preferences induced by rankings over candidates. We note that Theorem 2 improves Theorem 1 for k≥496𝑘496k\geq 496italic_k ≥ 496. 1.2 Technical Overview Our approach, building on [JMW20], is to first construct a particular distribution over committees of size k𝑘kitalic_k, and then to show that by sampling from this distribution, the resulting committee is α𝛼\alphaitalic_α-undominated in expectation. In fact, [ELS15]’s proof that the existence of O⁢(log⁡m)𝑂𝑚O(\log m)italic_O ( roman_log italic_m ) size Condorcet winning committees in elections with m𝑚mitalic_m candidates can also be viewed through this framework. There, we can consider the uniform distribution over candidates. To construct the committee, we sample from this distribution, remove the candidates that are beaten, and recurse on the remaining candidates. In expectation, half of the candidates are removed in each round, so the algorithm is likely to end with a committee of O⁢(log⁡m)𝑂𝑚O(\log m)italic_O ( roman_log italic_m ) candidates. The greedy algorithm of choosing the candidate that beats the most others in each round can be viewed as derandomization via conditional expectation. In this light, a natural approach to improving the O⁢(log⁡m)𝑂𝑚O(\log m)italic_O ( roman_log italic_m ) guarantee is to find a better distribution over committees. One of the insights in [JMW20] was to construct this distribution via the equilibrium of a zero-sum game. In the game, the defender chooses a committee S𝑆Sitalic_S of size k𝑘kitalic_k, and the attacker chooses a candidate a𝑎aitalic_a. After the choices are made, the defender pays the attacker a dollar for each voter that prefers a𝑎aitalic_a over all members of S𝑆Sitalic_S. The optimal strategy for the defender is to choose a committee randomly according to some distribution, which [JMW20] call the stable lottery. Then to create a committee of size k𝑘kitalic_k, [JMW20] take a recursive approach. First, they sample a committee S𝑆Sitalic_S of size k/2𝑘2k/2italic_k / 2, and show that ignoring the 25% of voters that least like S𝑆Sitalic_S, any candidate a𝑎aitalic_a is preferred over S𝑆Sitalic_S by less than a 8k8𝑘\frac{8}{k}divide start_ARG 8 end_ARG start_ARG italic_k end_ARG fraction of the voters (which are treated as an irrevocable loss). In the next step, they recurse on the ignored voters, sample a committee of size k/4𝑘4k/4italic_k / 4, and lose less than another 4k4𝑘\frac{4}{k}divide start_ARG 4 end_ARG start_ARG italic_k end_ARG fraction of the voters against any candidate a𝑎aitalic_a. The committee size and fraction of voters we lose continue to decrease exponentially, and in the end we have a committee of size k𝑘kitalic_k such that less than 16k16𝑘\frac{16}{k}divide start_ARG 16 end_ARG start_ARG italic_k end_ARG voters prefer any candidate a𝑎aitalic_a. To prove Theorem 1, we introduce three twists into this framework. Two are part of how we set up the zero-sum game in order to construct a distribution over committees that individual candidates perform poorly against (Lemma 1), and one is in how we show that in expectation, a random committee sampled from the distribution performs well (Lemma 2 and 4). Improving the game by confining the adversary. First, we modify the setup of the game so that the adversary must choose both a candidate a𝑎aitalic_a and a subset U𝑈Uitalic_U of an α𝛼\alphaitalic_α fraction of the voters. The adversary then only gets paid for the voters in U𝑈Uitalic_U that prefer a𝑎aitalic_a over the committee S𝑆Sitalic_S. By tying the hands of the adversary in this way, we can drive down the value of the game, which gives a more favorable guarantee for the distribution over committees. Once we fix the distribution over committees (referred to by ΔΔ\Deltaroman_Δ), we measure the quality of a candidate a𝑎aitalic_a or committee S𝑆Sitalic_S with respect to a voter v𝑣vitalic_v with a crucial notion that we call the rank, denoted rankv⁡(a)subscriptrank𝑣𝑎\operatorname{rank}_{v}(a)roman_rank start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_a ) or rankv⁡(S)subscriptrank𝑣𝑆\operatorname{rank}_{v}(S)roman_rank start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_S ) (see Definition 3). Roughly speaking, this is simply the probability when we sample from ΔΔ\Deltaroman_Δ that we get a committee that is worse than a𝑎aitalic_a or S𝑆Sitalic_S in v𝑣vitalic_v’s preference. The activation function. The second twist is what we call the activation function g𝑔gitalic_g, which allows us freedom in how we measure each voter’s preferences for candidates and committees. This function may seem somewhat enigmatic in the proof, but here we try to give some rough intuition for the idea behind it. The initial observation is that by using versions of the zero-sum game with different committee sizes, we can construct distributions over committees of size k𝑘kitalic_k in a variety of ways. The simplest would be to take the optimal mixed strategy for the defender in the original game with committee size k𝑘kitalic_k, but we could also take the optimal strategy for size k/2𝑘2k/2italic_k / 2, sample twice from it and take the union. These different ways of constructing the distributions can actually be interpreted as attaching different activation functions to the defender’s distribution in the payoffs of the original game. For example, sampling twice from the k/2𝑘2k/2italic_k / 2 distribution is equivalent to attaching the function g⁢(x)=x𝑔𝑥𝑥g(x)=\sqrt{x}italic_g ( italic_x ) = square-root start_ARG italic_x end_ARG, and the reason corresponds to the fact that sampling two uniform reals from [0,1]01[0,1][ 0 , 1 ] and taking the max is equivalent to sampling one uniform real from [0,1]01[0,1][ 0 , 1 ] and taking the square root. In the end, thanks to the generality of the minimax theorem, the proof works for any non-constant, non-decreasing function g:[0,1]→ℝ≥0:𝑔→01subscriptℝabsent0g\colon[0,1]\to\mathbb{R}_{\geq 0}italic_g : [ 0 , 1 ] → blackboard_R start_POSTSUBSCRIPT ≥ 0 end_POSTSUBSCRIPT such that g⁢(xk)𝑔superscript𝑥𝑘g(x^{k})italic_g ( italic_x start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ) is convex. These functions give a richer continuous space of options for modifying the game, some of which are not easily interpretable in terms of the intuition described above. Each choice of g𝑔gitalic_g gives a different bound for α𝛼\alphaitalic_α as a function of k𝑘kitalic_k, and so we can simply choose the function that gives the best guarantee. A one-shot approach with finer accounting of all voters. Third, we use a more precise approach for showing that some committee in the support of our distribution performs well, by diligently accounting for the contributions of each voter. In each step of [JMW20]’s recursion, when they sample committee S𝑆Sitalic_S, they consider for each voter v𝑣vitalic_v whether or not rankv⁡(S)subscriptrank𝑣𝑆\operatorname{rank}_{v}(S)roman_rank start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_S ) is above some threshold (called β𝛽\betaitalic_β, which is set to 1414\frac{1}{4}divide start_ARG 1 end_ARG start_ARG 4 end_ARG). The voters below the threshold are ignored, and then recursed on in the next iteration. There are two potential roadblocks in using this approach for small committee sizes. Intuitively, if we are aiming for a final committee size of around 6, the recursion cannot be very deep. Each iteration can only choose 2 or 3 candidates, for which the guarantee is insufficient. That is, the benefits of the recursion only kick in for sufficiently large committees, and for small committees, it is better to sample the whole committee in one shot (without recursion). Second, there is too much loss in evaluating each voter with a binary threshold, and without recursion, we need better accounting for voters with a low opinion of the committee. In Lemma 2 and 4, we give a smoother analysis, which allows us to more precisely account for the contributions of each voter. To give some rough intuition, what we would like to show is that there is some S𝑆Sitalic_S such that the total sum of rankv⁡(S)subscriptrank𝑣𝑆\operatorname{rank}_{v}(S)roman_rank start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_S ) is large for any subset of an α𝛼\alphaitalic_α fraction of the voters. If we fix S𝑆Sitalic_S and plot each rankv⁡(S)subscriptrank𝑣𝑆\operatorname{rank}_{v}(S)roman_rank start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_S ), ordered from smallest to largest, it suffices to bound the area under the bottom α𝛼\alphaitalic_α fraction. It turns out that the worst case for these ranks (that minimizes the area for all S𝑆Sitalic_S) is not a step function with a sharp threshold, but rather a linear function with slope 1111 (akin to the cyclic preferences depicted in Table 1). Finally, to prove Theorem 2, we use our modifications in tandem with the recursive approach. In the proof of this theorem, the idea above that does the heavy lifting is the use of the activation function g𝑔gitalic_g. 1.3 Related Work Proportionality in committee selection. In the context of committee selection, the principle of proportionality says that large voter coalitions should have their preferences fairly represented — an idea that dates back to at least the 19th century [Dro81]. Since its advent, a substantial body of research has been dedicated to studying the possibility and implications of proportionality. One of the most widely studied models is approval voting, where voters express their preferences by selecting a subset of candidates they approve of. We refer the reader to a survey by Lackner and Skowron [LS23] for a detailed discussion on the topic. A key appeal of this model is that there are a wide variety of proportionality axioms such as justified representation (JR) [ABC+17] and its variants (for example, [FEL+17, BP23]) that are satisfied by natural rules (such as Proportional Approval Voting (Thiele’s rule) [Thi95, Kil10, ABC+17, PS20], Phragmén’s rule [Phr94, PS20], and the Method of Equal Shares [PS20, PPS21]). These ideas have also been impactful in practice, with for example, the historical use of Thiele’s rule and Phragmén’s rule [Jan16], and the recent successful implementation of the Method of Equal Shares for participatory budgeting in several European cities [PS]. Additionally, this line of work is driven forward by intriguing conjectures that even stronger axioms, such as core stability [ABC+17, FMS18], might be universally satisfiable as well. In comparison, proportionality in committee selection with ranked preferences is relatively under-explored. As [LS23] suggest, part of the challenge is that notions of proportionality in the approval setting do not always generalize to the ranking setting. (Or, like with core stability, the analogous axioms are not always satisfiable [CJMW20].) One particularly well studied class of rank-based committee selection rules is that of committee scoring rules [EFSS17]. These voting rules, which generalize scoring rules in the single-winner setting, capture several natural committee selection rules, and have been axiomatically characterized [FSST19, SFS19]. We refer the reader to [FSST17] for a more in-depth discussion. Committee analogues of Condorcet winners. Grappling with Condorcet’s paradox has been a major driving force in social choice theory, and naturally, other attempts have been made to extend the notion of Condorcet winners to the multi-winner setting. Fishburn [Fis81b, Fis81a] introduced the idea of a majority committee, defined as a committee preferred by a majority of voters over any other committee of the same size. The Smith set [Goo71, Smi73] S𝑆Sitalic_S is defined as the minimal committee such that for any a∉S𝑎𝑆a\notin Sitalic_a ∉ italic_S and b∈S𝑏𝑆b\in Sitalic_b ∈ italic_S, a majority of voters prefers b𝑏bitalic_b over a𝑎aitalic_a. Uncovered sets [Fis77, Mil80], bipartisan sets [LLLB93] (the support of maximal lotteries [Fis84]), and other tournament solutions [BBH16] can also be viewed as multi-winner generalizations of Condorcet winners. However, these notions face the same challenge as Condorcet winners: they either do not always exist or sometimes coincide with the entire (potentially large) set of candidates. As in the single-winner case, the goal shifts to identifying Condorcet-consistent rules, which select a Condorcet winner (or the analogous multi-winner notion) when one exists [Coe05, BC08]. In this context, Theorem 1 highlights a distinct advantage of the approach by Elkind, Lang, and Saffidine [ELS15]: small Condorcet-undominated sets are guaranteed to exist. Other explorations of 1. Lastly, we mention a few other interesting explorations of Condorcet winning sets, and more generally α𝛼\alphaitalic_α-undominated sets. [Gei14] used SAT solving to determine the largest Condorcet dimension in elections with a small number of voters and candidates. Their search did not turn up any instances with dimension larger than 3, but they found an election with just 6 voters and candidates with dimension 3, and they showed that this is the smallest possible. (We include one such instance in Table 4.) [Blo18] also explored whether elections with Condorcet dimension 4 could be constructed by exploring dominating sets in tourament graphs, but that approach did not yield any such elections. On the positive side, [LVvS24] very recently showed that in elections where the voters and candidates are embedded in a 2-dimensional space, and their preferences are defined by their distance according to the ℓ1subscriptℓ1\ell_{1}roman_ℓ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT or ℓ∞subscriptℓ\ell_{\infty}roman_ℓ start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT norm, the Condorcet dimension is at most 3. In a more informal setting, a question isomorphic to 1 has also been explored from a combinatorial perspective in a series of Math Overflow posts [Pál13, Spe13, Bra13]. These posts offer an intriguing window into different approaches to resolving the problem, including why some natural approaches fall short. In their formulation [Pál13], each candidate a𝑎aitalic_a is represented by a function fa:[n]→ℕ:subscript𝑓𝑎→delimited-[]𝑛ℕf_{a}\colon[n]\to\mathbb{N}italic_f start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT : [ italic_n ] → blackboard_N, which can be thought of as a map from each voter v𝑣vitalic_v to the rank of a𝑎aitalic_a in v𝑣vitalic_v’s preference order. They ask 1, with a particular focus on the case where k=2𝑘2k=2italic_k = 2. The responses contain examples of elections with Condorcet dimension 3, including the general lower bound that α𝛼\alphaitalic_α-undominated committees of size k𝑘kitalic_k do not always exist when α<2k+1𝛼2𝑘1\alpha<\frac{2}{k+1}italic_α < divide start_ARG 2 end_ARG start_ARG italic_k + 1 end_ARG [Zba14]. One natural approach towards positive results, suggested by Speyer [Spe13], is to solve the following graph theory question. Question 2. For what choices of ℓ,k∈ℤ+ℓ𝑘superscriptℤ\ell,k\in\mathbb{Z}^{+}roman_ℓ , italic_k ∈ blackboard_Z start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT does there exist a directed graph with girth larger than ℓℓ\ellroman_ℓ such that every subset of k𝑘kitalic_k vertices has a common in-neighbor? If there does not exist such a graph for some choice of ℓℓ\ellroman_ℓ and k𝑘kitalic_k, then this implies that every election has a (1−1ℓ)11ℓ(1-\frac{1}{\ell})( 1 - divide start_ARG 1 end_ARG start_ARG roman_ℓ end_ARG )-undominated set of size k𝑘kitalic_k, by considering the graph on candidates where there is an edge (a,b)𝑎𝑏(a,b)( italic_a , italic_b ) whenever more than 1−1ℓ11ℓ1-\frac{1}{\ell}1 - divide start_ARG 1 end_ARG start_ARG roman_ℓ end_ARG fraction of the voters prefer a𝑎aitalic_a over b𝑏bitalic_b. In particular, if every triangle-free graph has a pair of vertices without a common in-neighbor (ℓ=3ℓ3\ell=3roman_ℓ = 3 and k=2𝑘2k=2italic_k = 2), then this would imply that 2323\frac{2}{3}divide start_ARG 2 end_ARG start_ARG 3 end_ARG-undominated sets of size 2222 always exist, which would resolve 1 for k=2𝑘2k=2italic_k = 2. Unfortunately, such graphs do exist. [AHL+15] gave a positive answer to 2 for every ℓ,k∈ℤ+ℓ𝑘superscriptℤ\ell,k\in\mathbb{Z}^{+}roman_ℓ , italic_k ∈ blackboard_Z start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT, using a construction based on additive combinatorics."
https://arxiv.org/html/2411.03331v1,"Hypergraphs as Weighted Directed Self-Looped Graphs:Spectral Properties, Clustering, Cheeger Inequality","Hypergraphs naturally arise when studying group relations and have been widely used in the field of machine learning. There has not been a unified formulation of hypergraphs, yet the recently proposed edge-dependent vertex weights (EDVW) modeling [7] is one of the most generalized modeling methods of hypergraphs, i.e., most existing hypergraphs can be formulated as EDVW hypergraphs without any information loss to the best of our knowledge. However, the relevant algorithmic developments on EDVW hypergraphs remain nascent: compared to spectral graph theories, the formulations are incomplete, the spectral clustering algorithms are not well-developed, and one result regarding hypergraph Cheeger Inequality is even incorrect. To this end, deriving a unified random walk-based formulation, we propose our definitions of hypergraph Rayleigh Quotient, NCut, boundary/cut, volume, and conductance, which are consistent with the corresponding definitions on graphs. Then, we prove that the normalized hypergraph Laplacian is associated with the NCut value, which inspires our HyperClus-G algorithm for spectral clustering on EDVW hypergraphs. Finally, we prove that HyperClus-G can always find an approximately linearly optimal partitioning in terms of Both NCut111The NCut of the returned partition 𝒩𝒩\mathcal{N}caligraphic_N and the optimal NCut of any partition 𝒩∗superscript𝒩\mathcal{N}^{*}caligraphic_N start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT satisfy 𝒩≤O⁢(𝒩∗)𝒩𝑂superscript𝒩\mathcal{N}\leq O(\mathcal{N}^{*})caligraphic_N ≤ italic_O ( caligraphic_N start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ). and conductance 222The conductance of the returned partition ΦΦ\Phiroman_Φ and the optimal conductance Φ∗superscriptΦ\Phi^{*}roman_Φ start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT satisfy Φ≤O⁢(Φ∗)Φ𝑂superscriptΦ\Phi\leq O(\Phi^{*})roman_Φ ≤ italic_O ( roman_Φ start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ). Additionally, we provide extensive experiments to validate our theoretical findings from an empirical perspective.","Higher-order relations are ubiquitous in nature, such as co-authorship [14, 48, 40], interactions between multiple proteins or chemicals [13, 47], items that are liked by the same person [49, 46], and interactions between multiple species in an ecosystem [20, 38]. Hypergraphs, extended from graphs, with the powerful capacity to model group interactions (i.e., higher-order relations), show extraordinary potential to be applied to many real-world tasks where the connections are beyond pair-wise. Therefore, hypergraphs have been used widely in recommendation systems [56, 36, 18], information retrieval [27, 53, 35] and link prediction [26, 12]. Hypergraphs modeled by edge-dependent vertex weights (EDVW) were necessitated in a recent work [7], with a motivating example that in citation networks, each scholar (i.e., vertex) may contribute differently to each co-authored publication (i.e., hyperedge). The authors show that hypergraphs with edge-independent vertex weights (EIVW) do not actually utilize the higher-order relations for the following two reasons. First, the hypergraph Laplacian matrix proposed by the seminal work [52], which serves as a basis of many follow-up algorithms, is equal to the Laplacian matrix of a closely related graph with only pair-wise relations. In this way, all the linear Laplacian operators utilize only pair-wise relationships between vertices [1]. Second, many hypergraph algorithms [37, 31, 4] are based on random walks [42, 9, 34], but it has been proved that for any EIVW hypergraph, there exists a weighted pair-wise graph on which a random walk is equivalent to that on the original hypergraph [7]. Table 1: Properties of graph models/formulations. EDVW hypergraphs generalized EIVW hypergraphs by allowing each hyperedge to distribute its vertex weights, bringing better formulation flexibility. Modeling/Formulation undirected graphs EIVW hypergraphs EDVW hypergraphs edge/hyperedge weights √square-root\surd√ √square-root\surd√ √square-root\surd√ vertex weights √square-root\surd√ √square-root\surd√ √square-root\surd√ hyperedges ×\times× √square-root\surd√ √square-root\surd√ edge-dependent vertex weights ×\times× ×\times× √square-root\surd√ Figure 1: Undirected graphs ⊂\subset⊂ EIVW hypergraphs ⊂\subset⊂ EDVW hypergraphs. Each undirected graph can be reformulated to EIVW hypergraph by regarding each pair-wise edge as a hyperedge; each EIVW hypergraph can be reformulated to EDVW hypergraph by setting each vertex’s weight to be the same across hyperedges, yet allowing different vertices to have different weights. In nature, ”EDVW hypergraph” is not a special case of hypergraphs, but a more generalized way to model hypergraphs (Figure 1). Any algorithm designed for EDVW hypergraphs, taking EDVW inputs, also works on typical (EIVW) hypergraphs by setting all the EDVW to 1 (i.e., independent w.r.t. edge). In other words, the properties and algorithms on EDVW-formulated hypergraphs can be applied to most hypergraphs. In this paper, we focus on further developing the incomplete yet fundamental spectral theories for EDVW hypergraphs, with a straightforward application on spectral clustering, a.k.a., k-way global partitioning, where typically k=2𝑘2k=2italic_k = 2. To be specific, k-way global partitioning aims to partition an entire graph into k𝑘kitalic_k clusters, where the vertices in one cluster are densely connected within this cluster while having sparser connections to vertices outside this cluster. On the one hand, although the spectral theories and spectral clustering on graphs have been well studied [10], converting the hypergraphs to graphs and applying those methods may ignore the higher-order relations and result in sub-optimal results [44]. On the other hand, despite the advantage of EDVW modeling in terms of utilizing high-order relations, directly developing a spectral clustering algorithm on EDVW hypergraphs is still an open question. To this end, for the first time, we propose a provably linearly optimal spectral clustering algorithm on EDVW hypergraphs, together with theoretical analysis concerning the Rayleigh Quotient, Normalized Cut (i.e., NCut), and conductance. In the context of EDVW hypergraphs, we bridge the eigensystem of Laplacian with the NCut value through our proposed Rayleigh Quotient. The proposed algorithm can also be applied to EIVW hypergraphs by setting all the vertex weights to 1, thus works generally for all hypergraphs. 1.1 Main Results In this paper, we further develop the spectral hypergraph theory for EDVW hypergraphs, and then study global partitioning on EDVW hypergraphs. Theorem 1. (algebraic connections between hypergraph NCut, Rayleigh Quotient and Laplacian) Given any hypergraph in the EDVW formatting ℋ=(𝒱,ℰ,ω,γ)ℋ𝒱ℰ𝜔𝛾\mathcal{H}=(\mathcal{V},\mathcal{E},\omega,\gamma)caligraphic_H = ( caligraphic_V , caligraphic_E , italic_ω , italic_γ ) with positive edge weights ω⁢(⋅)>0𝜔⋅0\omega(\cdot)>0italic_ω ( ⋅ ) > 0 and non-negative edge-dependent vertex weights γe⁢(⋅)subscript𝛾𝑒⋅\gamma_{e}(\cdot)italic_γ start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ( ⋅ ) for any e∈ℰ𝑒ℰe\in\mathcal{E}italic_e ∈ caligraphic_E, define Normalized Cut N⁢C⁢u⁢t⁢(⋅)𝑁𝐶𝑢𝑡⋅NCut(\cdot)italic_N italic_C italic_u italic_t ( ⋅ ), Volume of a vertex set v⁢o⁢l⁢(⋅)𝑣𝑜𝑙⋅vol(\cdot)italic_v italic_o italic_l ( ⋅ ), Rayleigh Quotient R⁢(⋅)𝑅⋅R(\cdot)italic_R ( ⋅ ), Laplacian L𝐿Litalic_L, and stationary distribution matrix ΠΠ\Piroman_Π as Definition 10, 8, 5, 13, and 4. For any vertex set 𝒮⊆𝒱𝒮𝒱\mathcal{S}\subseteq\mathcal{V}caligraphic_S ⊆ caligraphic_V, we define a |𝒱|𝒱|\mathcal{V}|| caligraphic_V |-dimensional vector x𝑥xitalic_x such that, x⁢(u)=v⁢o⁢l⁢(𝒮¯)v⁢o⁢l⁢(𝒮),∀u∈𝒮,x⁢(u¯)=−v⁢o⁢l⁢(𝒮)v⁢o⁢l⁢(𝒮¯),∀u¯∈𝒮¯.formulae-sequence𝑥𝑢𝑣𝑜𝑙¯𝒮𝑣𝑜𝑙𝒮formulae-sequencefor-all𝑢𝒮formulae-sequence𝑥¯𝑢𝑣𝑜𝑙𝒮𝑣𝑜𝑙¯𝒮for-all¯𝑢¯𝒮\begin{split}x(u)&=\sqrt{\frac{vol(\bar{\mathcal{S}})}{vol(\mathcal{S})}},% \leavevmode\nobreak\ \forall\,\,u\in\mathcal{S},\\ x(\bar{u})&=-\sqrt{\frac{vol(\mathcal{S})}{vol(\bar{\mathcal{S}})}},% \leavevmode\nobreak\ \forall\,\,\bar{u}\in\bar{\mathcal{S}}.\\ \end{split}start_ROW start_CELL italic_x ( italic_u ) end_CELL start_CELL = square-root start_ARG divide start_ARG italic_v italic_o italic_l ( over¯ start_ARG caligraphic_S end_ARG ) end_ARG start_ARG italic_v italic_o italic_l ( caligraphic_S ) end_ARG end_ARG , ∀ italic_u ∈ caligraphic_S , end_CELL end_ROW start_ROW start_CELL italic_x ( over¯ start_ARG italic_u end_ARG ) end_CELL start_CELL = - square-root start_ARG divide start_ARG italic_v italic_o italic_l ( caligraphic_S ) end_ARG start_ARG italic_v italic_o italic_l ( over¯ start_ARG caligraphic_S end_ARG ) end_ARG end_ARG , ∀ over¯ start_ARG italic_u end_ARG ∈ over¯ start_ARG caligraphic_S end_ARG . end_CELL end_ROW (1) then,⁢N⁢C⁢u⁢t⁢(𝒮,𝒮¯)=12⁢R⁢(x)=xT⁢L⁢xxT⁢Π⁢xthen,𝑁𝐶𝑢𝑡𝒮¯𝒮12𝑅𝑥superscript𝑥𝑇𝐿𝑥superscript𝑥𝑇Π𝑥\textit{then,}\,\,NCut(\mathcal{S},\bar{\mathcal{S}})=\frac{1}{2}R(x)=\frac{x^% {T}Lx}{x^{T}\Pi x}then, italic_N italic_C italic_u italic_t ( caligraphic_S , over¯ start_ARG caligraphic_S end_ARG ) = divide start_ARG 1 end_ARG start_ARG 2 end_ARG italic_R ( italic_x ) = divide start_ARG italic_x start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_L italic_x end_ARG start_ARG italic_x start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT roman_Π italic_x end_ARG (2) This is the first work regarding the Rayleigh Quotient on hypergraphs. Inspired by this Theorem, we develop a spectral clustering algorithm HyperClus-G to optimize the NCut value by loosing the combinatorial optimization constraint. Theorem 2. (Hypergraph Spectral Clustering Algorithm) There exists a algorithm for hypergraph spectral clustering that can be applied to EDVW-formatted hypergraphs, and always returns approximately linearly optimal clustering in terms of Normalized Cut and conductance. In other words, approximately, the NCut of the returned partition 𝒩𝒩\mathcal{N}caligraphic_N and the optimal NCut of any partition 𝒩∗superscript𝒩\mathcal{N}^{*}caligraphic_N start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT satisfy 𝒩≤O⁢(𝒩∗)𝒩𝑂superscript𝒩\mathcal{N}\leq O(\mathcal{N}^{*})caligraphic_N ≤ italic_O ( caligraphic_N start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ). We name this algorithm as HyperClus-G. The pseudo code of HyperClus-G is given in Algorithm 1. Moreover, to extend the hypergraph spectral theory, for the first time we give a complete proof regarding the hypergraph Cheeger Inequality. In the mean time, by proving Theorem 3, the previous result on hypergraph Cheeger Inequality (Theorem 5.1 in [7]) is incorrect as it refers to the eigenvector of unnormalized hypergraph Laplacian. Theorem 3. (Hypergraph Cheeger Inequality) Let ℋ=(𝒱,ℰ,ω,γ)ℋ𝒱ℰ𝜔𝛾\mathcal{H}=(\mathcal{V},\mathcal{E},\omega,\gamma)caligraphic_H = ( caligraphic_V , caligraphic_E , italic_ω , italic_γ ) be any hypergraph in the EDVW formatting with positive edge weights ω⁢(⋅)>0𝜔⋅0\omega(\cdot)>0italic_ω ( ⋅ ) > 0 and non-negative edge-dependent vertex weights γe⁢(⋅)subscript𝛾𝑒⋅\gamma_{e}(\cdot)italic_γ start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ( ⋅ ) for any e∈ℰ𝑒ℰe\in\mathcal{E}italic_e ∈ caligraphic_E. Define Φ⁢(ℋ)=min𝒮⊆𝒱Φ⁢(𝒮)Φℋsubscript𝒮𝒱Φ𝒮\Phi(\mathcal{H})=\mathop{\min}_{\mathcal{S}\subseteq\mathcal{V}}{\Phi(% \mathcal{S})}roman_Φ ( caligraphic_H ) = roman_min start_POSTSUBSCRIPT caligraphic_S ⊆ caligraphic_V end_POSTSUBSCRIPT roman_Φ ( caligraphic_S ). Then the second smallest eigenvector λ𝜆\lambdaitalic_λ of the normalized hypergraph Laplacian Π−12⁢L⁢Π−12superscriptΠ12𝐿superscriptΠ12\Pi^{-\frac{1}{2}}L\Pi^{-\frac{1}{2}}roman_Π start_POSTSUPERSCRIPT - divide start_ARG 1 end_ARG start_ARG 2 end_ARG end_POSTSUPERSCRIPT italic_L roman_Π start_POSTSUPERSCRIPT - divide start_ARG 1 end_ARG start_ARG 2 end_ARG end_POSTSUPERSCRIPT satisfies Φ⁢(ℋ)22≤λ≤2⁢Φ⁢(ℋ)Φsuperscriptℋ22𝜆2Φℋ\frac{\Phi(\mathcal{H})^{2}}{2}\leq\lambda\leq 2\Phi(\mathcal{H})divide start_ARG roman_Φ ( caligraphic_H ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG 2 end_ARG ≤ italic_λ ≤ 2 roman_Φ ( caligraphic_H ) (3) In fact, this theorem shows that our HyperClus-G is also approximately linearly optimal in terms of conductance. In other words, the conductance of the returned cluster ΦΦ\Phiroman_Φ and the optimal conductance Φ∗superscriptΦ\Phi^{*}roman_Φ start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT satisfy Φ≤O⁢(Φ∗)Φ𝑂superscriptΦ\Phi\leq O(\Phi^{*})roman_Φ ≤ italic_O ( roman_Φ start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ). It is worth mentioning that the previous non-proved result in [7] regarding hypergraph Cheeger Inequality now can be proved by using normalized Laplacian instead of the conjecture of combinatorial Laplacian. Technical Overview. Given the EDVW modeling, the relevant algorithmic development still remains in a nascent stage, which hinders the application of hypergraphs in many real-world scenarios. To this end, we first re-analyze the random walks on EDVW hypergraphs, then propose the HyperClus-G for hypergraph partitioning. Finally, we prove the approximation of normalized cut, as well as the upper bound of NCut and conductance. The key insight from the previous work [7] is to model the hypergraphs similar to directed graphs through the equivalence of random walks. Unlike classical graph theory, such directed graphs are edge-weighted, node-weighted, and contain self-loops. In this work, inspired by the definitions of Rayleigh Quotient, NCut, boundary/cut, volume, and conductance in graphs, we develop these definitions in the context of EDVW hypergraphs. We show that Theorem 1 and Theorem 3, properties that hold for graphs, still hold for hypergraphs using our unified definitions. From Theorem 3, we can further prove that our proposed HyperClus-G is approximately linearly optimal in terms of both NCut and conductance. Our Appendix contains supplementary contents, such as trivial proofs and experimental details. Paper Organization. This paper is organized as follows. In Section 2, we introduce necessary notations and our definitions regarding hypergraphs. In Section 3, we introduce our definition of hypergraph Rayleigh Quotient and show its connection with the Laplacian and NCut. Then, we propose our HyperClus-G inspired from such connection. In section 4, we give complete proof regarding hypergraph Cheeger Inequality, then show the linear optimality of our HyperClus-G in terms of both NCut and conductance. In Section 5, we analyze the complexity of our algorithms. Finally, in Section 6, we prepare comprehensive experiments to validate our theoretical findings. 1.2 Other Related Works Early Hypergraphs [6] typically model graph structures and do not allow node or hyperedge weights. Later formulations [52] start to allow hyperedge weights. Among various choices of modeling hypergraphs [6, 52, 15, 33], EDVW modeling [7] shows great generalization because it allows both hyperedge weights and node weights. Many hypergraph-related techniques have been proposed [17, 3, 30], while very few of them involve EDVW hypergraphs. Inhomogeneous hypergraph partitioning was proposed in [32]. Later on, authors of [33] proposed submodular hypergraphs, a special group of weighted hypergraphs, and analyzed their spectral clustering. A recent work [23] demonstrates how random walks with EDVW are used to construct the EDVW hypergraph Laplacian. However, it stops at the construction of the Laplacian and only uses partial information encoded in the Laplacian for clustering. Some recent works [2, 43, 11] study partitioning edge-colored hypergraphs. There are several research works [57, 54, 55] targeting EDVW hypergraph global partitioning, but they do not actually directly work on EDVW hypergraphs, but are based on submodular hypergraphs; They propose to construct a submodular hypergraph from the given EDVW hypergraph [33], then apply learning-based approaches to optimize the global partitioning objective. Several works also study specific applications of hypergraph clustering [28, 5]."
https://arxiv.org/html/2411.03299v1,Concurrent Composition for Continual Mechanisms,"A series of recent works by Lyu, Wang, Vadhan, and Zhang (TCC ‘21, NeurIPS ‘22, STOC ‘23) showed that composition theorems for non-interactive differentially private mechanisms extend to the concurrent composition of interactive differentially private mechanism, when differential privacy is measured using f𝑓fitalic_f-DP and the adversary is adaptive. We extend their work to the continual observation setting, where the data is arriving online in a potentially adaptive manner. More specifically, we show that all composition theorems for non-interactive differentially private mechanisms extend to the concurrent composition of continual differentially private mechanism, where the adversary is adaptive. We show this result for f𝑓fitalic_f-DP, which also implies the result for pure DP and (ϵ,δ)italic-ϵ𝛿(\epsilon,\delta)( italic_ϵ , italic_δ )-DP.","Differential privacy [7] is a popular measure of the privacy protection offered by an algorithm that performs statistical analysis on a sensitive dataset about individuals. While differential private mechanisms for the setting where the dataset is static (i.e., the batch setting) are well studied for a wide variety of problems, the setting where the dataset changes dynamically, i.e., where questions about the dataset and updates of the dataset are arbitrarily interleaved, has only recently received more attention. This setting was introduced in 2010 by Dwork, Naor, Pitassi, and Rothblum [8] and the corresponding privacy definition was called differential privacy under continual observation. These mechanisms are data structures (as they can (usually) process an arbitrary number of queries and updates) that are differentially private under continual observation. We call them continual mechanisms for short below. In recent years, mechanisms that are differential private under continual observation have been developed and analyzed for summing a sequence of (binary) numbers [8, 4, 9, 12, 1], weighted sums [2, 16], histograms and histogram-based queries [3, 14, 10], set cardinality [17, 15], various graph properties [6, 11, 12, 18], and clustering points in Euclidean space [20]. Some of these works are performed by reduction to another continual observation problem. That is, the continual mechanism ℳℳ\mathcal{M}caligraphic_M for the new problem uses a continual mechanism ℳ′superscriptℳ′\mathcal{M}^{\prime}caligraphic_M start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT for a previously studied problem mediate all of its access to the dataset. Thus, all steps taken by the mechanism can be seen as post-processing the output of ℳ′superscriptℳ′\mathcal{M}^{\prime}caligraphic_M start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT, leading immediately to the guarantee that ℳℳ\mathcal{M}caligraphic_M is differentially private under continual observation. However, for some of the above works (see e.g., [9, 14]), multiple continual mechanisms are used, i.e., ℳℳ\mathcal{M}caligraphic_M is interacting with multiple continual mechanisms M′,M′′,…superscript𝑀′superscript𝑀′′…M^{\prime},M^{\prime\prime},\dotsitalic_M start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , italic_M start_POSTSUPERSCRIPT ′ ′ end_POSTSUPERSCRIPT , … in an arbitrary way. Thus, to guarantee privacy for ℳℳ\mathcal{M}caligraphic_M, the concurrent composition of the involved continual mechanisms needs to be analyzed. A series of recent works by Lyu, Wang, Vadhan, and Zhang ([21], [19], [21], [13]) analyzed the concurrent composition of interactive mechanisms in the setting where the dataset is static and unchanging (but there can be adaptive queries answered in a potentially stateful manner.) More specifically, they showed that all composition theorems for non-interactive differentially private mechanisms extend to the concurrent composition of interactive differentially private mechanism. The privacy definition used in their work encompasses f𝑓fitalic_f-differential privacy [5], (ϵ,δ)italic-ϵ𝛿(\epsilon,\delta)( italic_ϵ , italic_δ )-differential privacy, and pure differential privacy. This paper analyzes the concurrent composition of continual mechanisms, where there are updates as well as queries, and shows the corresponding result: Composition theorems for non-interactive differentially private mechanisms extend to the concurrent composition of continual differentially private mechanism. As our theorem is based on their result, it applies to the same privacy definitions."
https://arxiv.org/html/2411.03227v1,Tight Sampling Bounds for Eigenvalue Approximation,"We consider the problem of estimating the spectrum of a symmetric bounded entry (not necessarily PSD) matrix via entrywise sampling. This problem was introduced by [Bhattacharjee, Dexter, Drineas, Musco, Ray ’22], where it was shown that one can obtain an ϵ⁢nitalic-ϵ𝑛\epsilon nitalic_ϵ italic_n additive approximation to all eigenvalues of A𝐴Aitalic_A by sampling a principal submatrix of dimension poly⁢(log⁡n)ϵ3poly𝑛superscriptitalic-ϵ3\frac{\text{poly}(\log n)}{\epsilon^{3}}divide start_ARG poly ( roman_log italic_n ) end_ARG start_ARG italic_ϵ start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT end_ARG. We improve their analysis by showing that it suffices to sample a principal submatrix of dimension O~⁢(1ϵ2)~𝑂1superscriptitalic-ϵ2\tilde{O}(\frac{1}{\epsilon^{2}})over~ start_ARG italic_O end_ARG ( divide start_ARG 1 end_ARG start_ARG italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ) (with no dependence on n𝑛nitalic_n). This matches known lower bounds and therefore resolves the sample complexity of this problem up to log⁡1ϵ1italic-ϵ\log\frac{1}{\epsilon}roman_log divide start_ARG 1 end_ARG start_ARG italic_ϵ end_ARG factors. Using similar techniques, we give a tight O~⁢(1ϵ2)~𝑂1superscriptitalic-ϵ2\tilde{O}(\frac{1}{\epsilon^{2}})over~ start_ARG italic_O end_ARG ( divide start_ARG 1 end_ARG start_ARG italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ) bound for obtaining an additive ϵ⁢‖A‖Fitalic-ϵsubscriptnorm𝐴𝐹\epsilon\|A\|_{F}italic_ϵ ∥ italic_A ∥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT approximation to the spectrum of A𝐴Aitalic_A via squared row-norm sampling, improving on the previous best O~⁢(1ϵ8)~𝑂1superscriptitalic-ϵ8\tilde{O}(\frac{1}{\epsilon^{8}})over~ start_ARG italic_O end_ARG ( divide start_ARG 1 end_ARG start_ARG italic_ϵ start_POSTSUPERSCRIPT 8 end_POSTSUPERSCRIPT end_ARG ) bound. We also address the problem of approximating the top eigenvector for a bounded entry, PSD matrix A.𝐴A.italic_A . In particular, we show that sampling O⁢(1ϵ)𝑂1italic-ϵO(\frac{1}{\epsilon})italic_O ( divide start_ARG 1 end_ARG start_ARG italic_ϵ end_ARG ) columns of A𝐴Aitalic_A suffices to produce a unit vector u𝑢uitalic_u with uT⁢A⁢u≥λ1⁢(A)−ϵ⁢nsuperscript𝑢𝑇𝐴𝑢subscript𝜆1𝐴italic-ϵ𝑛u^{T}Au\geq\lambda_{1}(A)-\epsilon nitalic_u start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_A italic_u ≥ italic_λ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_A ) - italic_ϵ italic_n. This matches what one could achieve via the sampling bound of [Musco, Musco’17] for the special case of approximating the top eigenvector, but does not require adaptivity.As additional applications, we observe that our sampling results can be used to design a faster eigenvalue estimation sketch for dense matrices resolving a question of [Swartworth, Woodruff’23], and can also be combined with [Musco, Musco’17] to achieve O⁢(1/ϵ3)𝑂1superscriptitalic-ϵ3O(1/\epsilon^{3})italic_O ( 1 / italic_ϵ start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ) (adaptive) sample complexity for approximating the spectrum of a bounded entry PSD matrix to ϵ⁢nitalic-ϵ𝑛\epsilon nitalic_ϵ italic_n additive error.","Computing the spectrum of a matrix is a fundamental problem with many applications. There are well-known high-precision algorithms that run in polynomial time [francis1962qr, golub2000eigenvalue], although any such algorithm is necessarily at least linear time in the input size. As data grows larger, even linear algorithms can be prohibitive. This has motivated a flurry of activity studying sublinear time estimation of problems in numerical linear algebra, for instance for low-rank approximation [musco2017recursive, musco2017sublinear, bakshi2018sublinear, bakshi2020robust], kernel density estimation [charikar2017hashing, siminelakis2019rehashing, charikar2020kernel], testing positive-semidefiniteness [bakshi2020testing], and matrix sparsification [bhattacharjee2023universal, drineas2011note]. For eigenvalue estimation, variants of the power method have long been known to give good approximations to the top eigenvalues and eigenvectors of A∈ℝn×n𝐴superscriptℝ𝑛𝑛A\in\mathbb{R}^{n\times n}italic_A ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_n end_POSTSUPERSCRIPT while revealing sublinear information about A𝐴Aitalic_A, i.e., using o⁢(n)𝑜𝑛o(n)italic_o ( italic_n ) matrix-vector queries [rokhlin2010randomized, musco2015randomized]. However it was only recently asked in [bhattacharjee2024sublinear] whether there are spectral approximation algorithms for symmetric, but non-PSD matrices that run in sublinear time in the entry query model. This is perhaps the most natural model if one imagines having an extremely large matrix saved on disk for example. This may be in the form of a graph for instance, where one could be interested in obtaining spectral information about its Laplacian or adjacency matrix. One could also imagine having a large collection of data points with some kernel function that can be computed for pairs of points. Obtaining a rough spectral summary of the associated kernel matrix is a natural step for data analysis, for instance, to spot low-rank structure in the data. If data points are large or expensive to collect, or if kernel evaluation is expensive, it is natural to aim for minimizing entry queries to the kernel matrix. Of course, it is not reasonable to ask for sublinear time spectral approximation algorithms, without some additional assumptions. For instance, our matrix A𝐴Aitalic_A could contain all zeros but with a single large entry at indices (i,j)𝑖𝑗(i,j)( italic_i , italic_j ) and (j,i).𝑗𝑖(j,i).( italic_j , italic_i ) . Given only the ability to query entries, and no additional information, even distinguishing A𝐴Aitalic_A from the all zeros matrix would take Ω⁢(n2)Ωsuperscript𝑛2\Omega(n^{2})roman_Ω ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) queries. We consider two assumptions that allow for improved guarantees. The first is an assumption on the structure of A𝐴Aitalic_A called the bounded entry model, which assumes that A𝐴Aitalic_A has entries bounded by 1111 in magnitude. This condition was introduced in [balcan2019testing] and studied further by [bakshi2020testing] who showed that it was sufficient in order to test for positive semi-definiteness with sublinear entry queries. Motivated by this result, [bhattacharjee2024sublinear] showed that all eigenvalues of a symmetric bounded-entry matrix can be approximated using a sublinear number of queries, simply by sampling a poly⁡(log⁡nϵ)poly𝑛italic-ϵ\operatorname{poly}(\frac{\log n}{\epsilon})roman_poly ( divide start_ARG roman_log italic_n end_ARG start_ARG italic_ϵ end_ARG ) sized submatrix. Another way of getting sublinear sample complexity is to give the sampler additional power. In our case, as in [bhattacharjee2024sublinear], we consider having access to a sampler that can produce a row index with probability proportional to its squared row norm. Such samplers have been increasingly studied under the guise of “quantum-inspired” machine-learning algorithms [tang2019quantum, chepurko2022quantum, gilyen2018quantum, gilyen2022improved]. Such samplers are practical to maintain when A𝐴Aitalic_A is stored entrywise. For example by using an appropriate data structure, they can be built in nnz⁡(A)nnz𝐴\operatorname{nnz}(A)roman_nnz ( italic_A ) time, admit O⁢(log⁡n)𝑂𝑛O(\log n)italic_O ( roman_log italic_n ) time sampling, and can handle entry updates in O⁢(log⁡n)𝑂𝑛O(\log n)italic_O ( roman_log italic_n ) time. [bhattacharjee2024sublinear] showed that given a real symmetric matrix A𝐴Aitalic_A with entries bounded by 1111, one can sample a principal submatrix of A𝐴Aitalic_A of dimensions O⁢(poly⁡log⁡nϵ3)×O⁢(poly⁡log⁡nϵ3)𝑂poly𝑛superscriptitalic-ϵ3𝑂poly𝑛superscriptitalic-ϵ3O(\frac{\operatorname{poly}\log n}{\epsilon^{3}})\times O(\frac{\operatorname{% poly}\log n}{\epsilon^{3}})italic_O ( divide start_ARG roman_poly roman_log italic_n end_ARG start_ARG italic_ϵ start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT end_ARG ) × italic_O ( divide start_ARG roman_poly roman_log italic_n end_ARG start_ARG italic_ϵ start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT end_ARG ) and then output an additive ϵ⁢nitalic-ϵ𝑛\epsilon nitalic_ϵ italic_n approximation to the entire spectrum, i.e., to all eigenvalues of A.𝐴A.italic_A . On the other hand, the best lower bound states that a principal-submatrix algorithm must sample at least O⁢(1ϵ4)𝑂1superscriptitalic-ϵ4O(\frac{1}{\epsilon^{4}})italic_O ( divide start_ARG 1 end_ARG start_ARG italic_ϵ start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT end_ARG ) entries of A.𝐴A.italic_A . There are two ways that one could hope to improve the sampling bound of [bhattacharjee2024sublinear]. First one could hope to improve the ϵitalic-ϵ\epsilonitalic_ϵ dependence in the dimension from O⁢(1/ϵ3)𝑂1superscriptitalic-ϵ3O(1/\epsilon^{3})italic_O ( 1 / italic_ϵ start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ) to O⁢(1/ϵ2).𝑂1superscriptitalic-ϵ2O(1/\epsilon^{2}).italic_O ( 1 / italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) . Several prior results suggested that this might be possible. For example [bhattacharjee2024sublinear] showed that their ϵitalic-ϵ\epsilonitalic_ϵ dependence could be improved both when A𝐴Aitalic_A is PSD, and when the spectrum of A𝐴Aitalic_A is flat. Concurrently [swartworth2023optimal] showed that one can obtain an ϵ⁢‖A‖Fitalic-ϵsubscriptnorm𝐴𝐹\epsilon\left\|A\right\|_{F}italic_ϵ ∥ italic_A ∥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT additive approximation to the spectrum of A𝐴Aitalic_A by using a so-called bilinear sketch of A𝐴Aitalic_A of dimensions O⁢(1/ϵ2)×O⁢(1/ϵ2).𝑂1superscriptitalic-ϵ2𝑂1superscriptitalic-ϵ2O(1/\epsilon^{2})\times O(1/\epsilon^{2}).italic_O ( 1 / italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) × italic_O ( 1 / italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) . Such a sketch would give ϵ⁢nitalic-ϵ𝑛\epsilon nitalic_ϵ italic_n additive error for approximating all eigenvalues when A𝐴Aitalic_A has bounded entries. Unfortunately this sketch is Gaussian, and it seems difficult to directly adapt its analysis to obtain a sampling bound instead. In this paper we close the gap between sketching and sampling for bounded entry matrices by showing that uniformly sampling an O~⁢(1/ϵ2)×O~⁢(1/ϵ2)~𝑂1superscriptitalic-ϵ2~𝑂1superscriptitalic-ϵ2\tilde{O}(1/\epsilon^{2})\times\tilde{O}(1/\epsilon^{2})over~ start_ARG italic_O end_ARG ( 1 / italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) × over~ start_ARG italic_O end_ARG ( 1 / italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) principal submatrix of A𝐴Aitalic_A suffices to approximate all eigenvalues of A𝐴Aitalic_A up to ϵ⁢nitalic-ϵ𝑛\epsilon nitalic_ϵ italic_n additive error, even when A𝐴Aitalic_A is not necessarily PSD. In addition to obtaining an optimal ϵitalic-ϵ\epsilonitalic_ϵ-dependence, we note that our uniform sampling bound contains no dependence on n.𝑛n.italic_n . We also address the squared-row norm sampling model. Here we improve the analysis of [bhattacharjee2024sublinear] to show that it suffices to query a principal submatrix of size O~⁢(poly⁡log⁡nϵ2)~𝑂poly𝑛superscriptitalic-ϵ2\tilde{O}(\frac{\operatorname{poly}\log n}{\epsilon^{2}})over~ start_ARG italic_O end_ARG ( divide start_ARG roman_poly roman_log italic_n end_ARG start_ARG italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ), compared to the O~⁢(poly⁡log⁡nϵ8)~𝑂poly𝑛superscriptitalic-ϵ8\tilde{O}(\frac{\operatorname{poly}\log n}{\epsilon^{8}})over~ start_ARG italic_O end_ARG ( divide start_ARG roman_poly roman_log italic_n end_ARG start_ARG italic_ϵ start_POSTSUPERSCRIPT 8 end_POSTSUPERSCRIPT end_ARG ) dimensional principal submatrix required by [bhattacharjee2024sublinear]. Our approximation model. We note that all of the guarantees considered in our work and prior work focus on additive approximations to the spectrum. Ideally, one might like to aim for a relative error guarantee. However as pointed out, by [bhattacharjee2024sublinear] for example, this is not possible for entry queries. Such an algorithm would be able to distinguish the 00 matrix from a matrix with a single off-diagonal pair of nonzero entries, which clearly requires Ω⁢(n2)Ωsuperscript𝑛2\Omega(n^{2})roman_Ω ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) samples. Indeed even with squared row-norm sampling, relative error is still too much to hope for. In fact, even for sketches, approximating the top eigenvalue to within a constant factor requires Ω⁢(n2)Ωsuperscript𝑛2\Omega(n^{2})roman_Ω ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) sketching dimension in general [li2016tight, woodruff2014sketching]. One can always turn such a sketching lower bound into a sampling bound, even allowing for row-norm sampling - simply conjugate by a random orthogonal matrix to flatten all rows. Then squared row sampling is effectively uniform, and so a sampling algorithm could be used to construct a sketch of the same dimensions. These existing lower bounds are why we (as well as prior work) choose to focus on an additive approximation guarantee."
https://arxiv.org/html/2411.03121v1,"Fully Dynamick𝑘kitalic_k-Median with
Near-OptimalUpdate Time and Recourse","In metric k𝑘kitalic_k-clustering, we are given as input a set of n𝑛nitalic_n points in a general metric space, and we have to pick k𝑘kitalic_k centers and cluster the input points around these chosen centers, so as to minimize an appropriate objective function. In recent years, significant effort has been devoted to the study of metric k𝑘kitalic_k-clustering problems in a dynamic setting, where the input keeps changing via updates (point insertions/deletions), and we have to maintain a good clustering throughout these updates [Fichtenberger, Lattanzi, Norouzi-Fard and Svensson, SODA’21; Bateni, Esfandiari, Fichtenberger, Henzinger, Jayaram, Mirrokni and Weise, SODA’23; Lacki, Haeupler, Grunau, Rozhon and Jayaram, SODA’24; Bhattacharya, Costa, Garg, Lattanzi and Parotsidis, FOCS’24; Forster and Skarlatos, SODA’25]. The performance of such a dynamic algorithm is measured in terms of three parameters: (i) Approximation ratio, which signifies the quality of the maintained solution, (ii) Recourse, which signifies how stable the maintained solution is, and (iii) Update time, which signifies the efficiency of the algorithm.We consider a textbook metric k𝑘kitalic_k-clustering problem, metric k𝑘kitalic_k-median, where the objective is the sum of the distances of the points to their nearest centers. We design the first dynamic algorithm for this problem with near-optimal guarantees across all three performance measures (up to a constant factor in approximation ratio, and polylogarithmic factors in recourse and update time). Specifically, we obtain a O⁢(1)𝑂1O(1)italic_O ( 1 )-approximation algorithm for dynamic metric k𝑘kitalic_k-median with O~⁢(1)~𝑂1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) recourse and O~⁢(k)~𝑂𝑘\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) update time. Prior to our work, the state-of-the-art here was the recent result of [Bhattacharya, Costa, Garg, Lattanzi and Parotsidis, FOCS’24], who obtained O⁢(ϵ−1)𝑂superscriptitalic-ϵ1O(\epsilon^{-1})italic_O ( italic_ϵ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT )-approximation ratio with O~⁢(kϵ)~𝑂superscript𝑘italic-ϵ\tilde{O}(k^{\epsilon})over~ start_ARG italic_O end_ARG ( italic_k start_POSTSUPERSCRIPT italic_ϵ end_POSTSUPERSCRIPT ) recourse and O~⁢(k1+ϵ)~𝑂superscript𝑘1italic-ϵ\tilde{O}(k^{1+\epsilon})over~ start_ARG italic_O end_ARG ( italic_k start_POSTSUPERSCRIPT 1 + italic_ϵ end_POSTSUPERSCRIPT ) update time.We achieve our results by carefully synthesizing the concept of robust centers introduced in [Fichtenberger, Lattanzi, Norouzi-Fard and Svensson, SODA’21] along with the randomized local search subroutine from [Bhattacharya, Costa, Garg, Lattanzi and Parotsidis, FOCS’24], in addition to several key technical insights of our own.","Part I Extended Abstract 1 Introduction Consider a metric space (𝒫,d)𝒫𝑑(\mathcal{P},d)( caligraphic_P , italic_d ) over a set 𝒫𝒫\mathcal{P}caligraphic_P of n𝑛nitalic_n points, with a distance function d:𝒫×𝒫→ℝ≥0:𝑑→𝒫𝒫superscriptℝabsent0d:\mathcal{P}\times\mathcal{P}\to\mathbb{R}^{\geq 0}italic_d : caligraphic_P × caligraphic_P → blackboard_R start_POSTSUPERSCRIPT ≥ 0 end_POSTSUPERSCRIPT, and a positive integer k≤n𝑘𝑛k\leq nitalic_k ≤ italic_n. In the metric k𝑘kitalic_k-median problem, we have to pick a set 𝒰⊆𝒫𝒰𝒫\mathcal{U}\subseteq\mathcal{P}caligraphic_U ⊆ caligraphic_P of k𝑘kitalic_k centers, so as to minimize the objective function Cost⁢(𝒰,𝒫):=∑p∈𝒫d⁢(p,𝒰)assignCost𝒰𝒫subscript𝑝𝒫𝑑𝑝𝒰\textnormal{{Cost}}\left(\mathcal{U},\mathcal{P}\right):=\sum_{p\in\mathcal{P}% }d(p,\mathcal{U})Cost ( caligraphic_U , caligraphic_P ) := ∑ start_POSTSUBSCRIPT italic_p ∈ caligraphic_P end_POSTSUBSCRIPT italic_d ( italic_p , caligraphic_U ), where d⁢(p,𝒰):=minq∈𝒰⁡d⁢(p,q)assign𝑑𝑝𝒰subscript𝑞𝒰𝑑𝑝𝑞d(p,\mathcal{U}):=\min_{q\in\mathcal{U}}d(p,q)italic_d ( italic_p , caligraphic_U ) := roman_min start_POSTSUBSCRIPT italic_q ∈ caligraphic_U end_POSTSUBSCRIPT italic_d ( italic_p , italic_q ) denotes the distance between a point p𝑝pitalic_p and its nearest center in 𝒰𝒰\mathcal{U}caligraphic_U. We assume that we have access to the function d𝑑ditalic_d via a distance oracle, which returns the value of d⁢(p,q)𝑑𝑝𝑞d(p,q)italic_d ( italic_p , italic_q ) for any two points p,q∈𝒫𝑝𝑞𝒫p,q\in\mathcal{P}italic_p , italic_q ∈ caligraphic_P in O⁢(1)𝑂1O(1)italic_O ( 1 ) time. We further assume that 1≤d⁢(p,q)≤Δ1𝑑𝑝𝑞Δ1\leq d(p,q)\leq\Delta1 ≤ italic_d ( italic_p , italic_q ) ≤ roman_Δ for all p,q∈𝒫,p≠qformulae-sequence𝑝𝑞𝒫𝑝𝑞p,q\in\mathcal{P},p\neq qitalic_p , italic_q ∈ caligraphic_P , italic_p ≠ italic_q, where ΔΔ\Deltaroman_Δ is an upper bound on the aspect ratio of the metric space. Metric k𝑘kitalic_k-median is a foundational problem in clustering, is known to be NP-hard, and approximation algorithms for this problem are taught in standard textbooks [WS11]. In particular, it has a O⁢(1)𝑂1O(1)italic_O ( 1 )-approximation algorithm that runs in O~⁢(k⁢n)~𝑂𝑘𝑛\tilde{O}(kn)over~ start_ARG italic_O end_ARG ( italic_k italic_n ) time [MP02],111Throughout this paper, we use the O~⁢(⋅)~𝑂⋅\tilde{O}(\cdot)over~ start_ARG italic_O end_ARG ( ⋅ ) notation to hide polylogarithmic factors in k,n𝑘𝑛k,nitalic_k , italic_n and ΔΔ\Deltaroman_Δ. and it is known that we cannot have any O⁢(1)𝑂1O(1)italic_O ( 1 )-approximation algorithm for metric k𝑘kitalic_k-median with o⁢(k⁢n)𝑜𝑘𝑛o(kn)italic_o ( italic_k italic_n ) runtime [BCIS05]. In recent years, substantive effort have been devoted to the study of this problem in a dynamic setting, when the underlying input changes over time [LV17, CHP+19, FLNS21, HK20, BCLP23, DHS24, BCG+24]. To be more specific, here the input changes by a sequence of updates; each update inserts/deletes a point in 𝒫𝒫\mathcal{P}caligraphic_P. Throughout these updates, we have to maintain a set of k𝑘kitalic_k centers 𝒰⊆𝒫𝒰𝒫\mathcal{U}\subseteq\mathcal{P}caligraphic_U ⊆ caligraphic_P which form an approximate k𝑘kitalic_k-median solution to the current input 𝒫𝒫\mathcal{P}caligraphic_P. Such a dynamic algorithm’s performance is measured in terms of its: (i) Approximation ratio, (ii) Recourse, which is the number of changes (i.e., point insertions/deletions) in the maintained solution 𝒰𝒰\mathcal{U}caligraphic_U per update, and (iii) Update time, which is the time taken by the algorithm to process an update. In a sense, approximation ratio and recourse respectively measures the “quality” and the “stability” of the maintained solution, whereas update time measures the “efficiency” of the algorithm. We design a dynamic algorithm for this problem with almost optimal performance guarantees with respect to all these measures. Our main result is summarized in the theorem below. Theorem 1.1. There is a randomized dynamic algorithm for the metric k𝑘kitalic_k-median problem that has O⁢(1)𝑂1O(1)italic_O ( 1 )-approximation ratio, O⁢(log2⁡Δ)𝑂superscript2ΔO(\log^{2}\Delta)italic_O ( roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT roman_Δ ) recourse and O~⁢(k)~𝑂𝑘\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) update time, w.h.p.222Both our recourse and update time bounds are amortized. Throughout the paper, we do not make any distinction between amortized vs worst-case bounds. Remarks. A few important remarks are in order. First, note that there cannot exist a dynamic O⁢(1)𝑂1O(1)italic_O ( 1 )-approximation algorithm for our problem with o⁢(k)𝑜𝑘o(k)italic_o ( italic_k ) update time, for otherwise we would get a static algorithm for metric k𝑘kitalic_k-median with O⁢(1)𝑂1O(1)italic_O ( 1 )-approximation ratio and o⁢(k⁢n)𝑜𝑘𝑛o(kn)italic_o ( italic_k italic_n ) runtime: Simply let the dynamic algorithm handle a sequence of n𝑛nitalic_n insertions corresponding to the points in the static input, and return the solution maintained by the dynamic algorithm at the end of this update sequence. This would contradict the Ω⁢(k⁢n)Ω𝑘𝑛\Omega(kn)roman_Ω ( italic_k italic_n ) lower bound on the runtime of any such static algorithm, derived in [BCIS05]. Furthermore, it is easy to verify that we cannot achieve o⁢(1)𝑜1o(1)italic_o ( 1 ) recourse in the fully dynamic setting, and hence, our dynamic algorithm is almost optimal (up to a O⁢(1)𝑂1O(1)italic_O ( 1 ) factor in approximation ratio and polylogarithmic factors in recourse and update time). Second, in this extended abstract we focus the unweighted metric k𝑘kitalic_k-median problem, only to ease notations. In the full version (see Part III), we show that Theorem 1.1 seamlessly extends to the weighted setting, where each point p∈𝒫𝑝𝒫p\in\mathcal{P}italic_p ∈ caligraphic_P has a weight w⁢(p)>0𝑤𝑝0w(p)>0italic_w ( italic_p ) > 0 associated with it, and we have to maintain a set 𝒰𝒰\mathcal{U}caligraphic_U of k𝑘kitalic_k centers that (approximately) minimizes ∑p∈𝒫w⁢(p)⋅d⁢(p,𝒰)subscript𝑝𝒫⋅𝑤𝑝𝑑𝑝𝒰\sum_{p\in\mathcal{P}}w(p)\cdot d(p,\mathcal{U})∑ start_POSTSUBSCRIPT italic_p ∈ caligraphic_P end_POSTSUBSCRIPT italic_w ( italic_p ) ⋅ italic_d ( italic_p , caligraphic_U ). Moreover, our result extends to the related metric k𝑘kitalic_k-means problem as well, where we have to pick a set 𝒰⊆𝒫𝒰𝒫\mathcal{U}\subseteq\mathcal{P}caligraphic_U ⊆ caligraphic_P of k𝑘kitalic_k centers so as to minimize ∑p∈𝒫(d⁢(p,𝒰))2subscript𝑝𝒫superscript𝑑𝑝𝒰2\sum_{p\in\mathcal{P}}\left(d(p,\mathcal{U})\right)^{2}∑ start_POSTSUBSCRIPT italic_p ∈ caligraphic_P end_POSTSUBSCRIPT ( italic_d ( italic_p , caligraphic_U ) ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. We can get a dynamic O⁢(1)𝑂1O(1)italic_O ( 1 )-approximation algorithm for (weighted) metric k𝑘kitalic_k-means that has O~⁢(1)~𝑂1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) recourse and O~⁢(k)~𝑂𝑘\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) update time, w.h.p. Finally, Table 1 compares our result against prior state-of-the-art. Until very recently, all known algorithms [CHP+19, HK20, BCLP23] for fully dynamic metric k𝑘kitalic_k-median had a trivial recourse bound of Ω⁢(k)Ω𝑘\Omega(k)roman_Ω ( italic_k ), which can be obtained by computing a new set of k𝑘kitalic_k centers from scratch after every update (at the expense of Ω⁢(k⁢n)Ω𝑘𝑛\Omega(kn)roman_Ω ( italic_k italic_n ) update time). Then, in FOCS 2024, [BCG+24] took a major step towards designing an almost optimal algorithm for this problem, by achieving O⁢(ϵ−1)𝑂superscriptitalic-ϵ1O(\epsilon^{-1})italic_O ( italic_ϵ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT )-approximation ratio, O~⁢(k1+ϵ)~𝑂superscript𝑘1italic-ϵ\tilde{O}(k^{1+\epsilon})over~ start_ARG italic_O end_ARG ( italic_k start_POSTSUPERSCRIPT 1 + italic_ϵ end_POSTSUPERSCRIPT ) update time and O~⁢(kϵ)~𝑂superscript𝑘italic-ϵ\tilde{O}(k^{\epsilon})over~ start_ARG italic_O end_ARG ( italic_k start_POSTSUPERSCRIPT italic_ϵ end_POSTSUPERSCRIPT ) recourse. To achieve truly polylogarithmic recourse and O~⁢(k)~𝑂𝑘\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) update time using the algorithm of [BCG+24], we have to set ϵ=O⁢(log⁡log⁡klog⁡k)italic-ϵ𝑂𝑘𝑘\epsilon=O\left(\frac{\log\log k}{\log k}\right)italic_ϵ = italic_O ( divide start_ARG roman_log roman_log italic_k end_ARG start_ARG roman_log italic_k end_ARG ). This, however, increases the approximation guarantee to Ω⁢(log⁡klog⁡log⁡k)Ω𝑘𝑘\Omega\left(\frac{\log k}{\log\log k}\right)roman_Ω ( divide start_ARG roman_log italic_k end_ARG start_ARG roman_log roman_log italic_k end_ARG ). In contrast, we achieve O⁢(1)𝑂1O(1)italic_O ( 1 )-approximation ratio, O~⁢(k)~𝑂𝑘\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) update time and O~⁢(1)~𝑂1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) recourse. Approximation Ratio Update Time Recourse Paper O⁢(1)𝑂1O(1)italic_O ( 1 ) O~⁢(n+k2)~𝑂𝑛superscript𝑘2\tilde{O}(n+k^{2})over~ start_ARG italic_O end_ARG ( italic_n + italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) O⁢(k)𝑂𝑘O(k)italic_O ( italic_k ) [CHP+19] O⁢(1)𝑂1O(1)italic_O ( 1 ) O~⁢(k2)~𝑂superscript𝑘2\tilde{O}(k^{2})over~ start_ARG italic_O end_ARG ( italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) O⁢(k)𝑂𝑘O(k)italic_O ( italic_k ) [HK20] O⁢(1)𝑂1O(1)italic_O ( 1 ) O~⁢(k2)~𝑂superscript𝑘2\tilde{O}(k^{2})over~ start_ARG italic_O end_ARG ( italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) O⁢(k)𝑂𝑘O(k)italic_O ( italic_k ) [BCLP23]333We remark that [BCLP23] actually maintain a O⁢(1)𝑂1O(1)italic_O ( 1 ) approximate “sparsifier” of size O~⁢(k)~𝑂𝑘\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) in O~⁢(k)~𝑂𝑘\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) update time, and they need to run the static algorithm of [MP00] on top of this sparsifier after every update. This leads to an update time of O~⁢(k2)~𝑂superscript𝑘2\tilde{O}(k^{2})over~ start_ARG italic_O end_ARG ( italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) for the dynamic k𝑘kitalic_k-median and k𝑘kitalic_k-means problems. O⁢(ϵ−1)𝑂superscriptitalic-ϵ1O\left(\epsilon^{-1}\right)italic_O ( italic_ϵ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) O~⁢(k1+ϵ)~𝑂superscript𝑘1italic-ϵ\tilde{O}(k^{1+\epsilon})over~ start_ARG italic_O end_ARG ( italic_k start_POSTSUPERSCRIPT 1 + italic_ϵ end_POSTSUPERSCRIPT ) O~⁢(kϵ)~𝑂superscript𝑘italic-ϵ\tilde{O}(k^{\epsilon})over~ start_ARG italic_O end_ARG ( italic_k start_POSTSUPERSCRIPT italic_ϵ end_POSTSUPERSCRIPT ) [BCG+24] O⁢(1)𝑂1O(1)italic_O ( 1 ) O~⁢(k)~𝑂𝑘\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) O⁢(log2⁡Δ)𝑂superscript2ΔO(\log^{2}\Delta)italic_O ( roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT roman_Δ ) Our Result Table 1: State-of-the-art for fully dynamic metric k𝑘kitalic_k-median. The table for fully dynamic metric k𝑘kitalic_k-means is identical, except that the approximation ratio of [BCG+24] is O⁢(ϵ−2)𝑂superscriptitalic-ϵ2O\left(\epsilon^{-2}\right)italic_O ( italic_ϵ start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT ). Related Work. In addition to metric k𝑘kitalic_k-median, two related clustering problems have been extensively studied in the dynamic setting: (i) metric k𝑘kitalic_k-center [CGS18, BEF+23, LHG+24, BCG+24, FS25, BCLP24] and (ii) metric facility location [CHP+19, BLP22]. Both these problems are relatively well-understood by now. For example, it is known how to simultaneously achieve O⁢(1)𝑂1O(1)italic_O ( 1 )-approximation ratio, O~⁢(1)~𝑂1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) recourse and O~⁢(k)~𝑂𝑘\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) update time for dynamic metric k𝑘kitalic_k-center [BCLP24]. They have also been studied under special classes of metrics, such as Euclidean spaces [GHL+21, BEF+23, BGJ+24], or shortest-path metrics in graphs undergoing edge-updates [CFG+24]. There is another line of work on dynamic metric k𝑘kitalic_k-center and metric k𝑘kitalic_k-median, which considers the incremental (insertion only) setting, and achieves total recourse guarantees that are sublinear in the total number of updates [LV17, FLNS21]. However, the update times of these algorithms are large polynomials in n𝑛nitalic_n. Section 2.2 contains a detailed discussion on the algorithm of [FLNS21]. 2 Technical Overview In Sections 2.1 and 2.2, we summarize the technical contributions of two relevant papers [BCG+24, FLNS21]. We obtain our algorithm via carefully synthesizing the techniques from both these papers, along with some key, new insights of our own. In Section 2.3, we outline the major technical challenges we face while trying to prove Theorem 1.1, and how we overcome them. 2.1 The Fully Dynamic Algorithm of [BCG+24] There are two main technical contributions in [BCG+24]; we briefly review each of them below. 2.1.1 A Hierarchical Approach to Dynamic k𝑘kitalic_k-Median This approach allows the authors to obtain O⁢(ϵ−1)𝑂superscriptitalic-ϵ1O(\epsilon^{-1})italic_O ( italic_ϵ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT )-approximation ratio with O~⁢(kϵ)~𝑂superscript𝑘italic-ϵ\tilde{O}(k^{\epsilon})over~ start_ARG italic_O end_ARG ( italic_k start_POSTSUPERSCRIPT italic_ϵ end_POSTSUPERSCRIPT ) recourse, and works as follows. We maintain a hierarchy of nested subsets of centers S0⊇⋯⊇Sℓ+1superset-of-or-equalssubscript𝑆0⋯superset-of-or-equalssubscript𝑆ℓ1S_{0}\supseteq\dots\supseteq S_{\ell+1}italic_S start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ⊇ ⋯ ⊇ italic_S start_POSTSUBSCRIPT roman_ℓ + 1 end_POSTSUBSCRIPT, where ℓ=1/ϵℓ1italic-ϵ\ell=1/\epsilonroman_ℓ = 1 / italic_ϵ and |Si|=k+⌊k1−i⁢ϵ⌋subscript𝑆𝑖𝑘superscript𝑘1𝑖italic-ϵ|S_{i}|=k+\lfloor k^{1-i\epsilon}\rfloor| italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | = italic_k + ⌊ italic_k start_POSTSUPERSCRIPT 1 - italic_i italic_ϵ end_POSTSUPERSCRIPT ⌋ for each i∈[0,ℓ+1]𝑖0ℓ1i\in[0,\ell+1]italic_i ∈ [ 0 , roman_ℓ + 1 ]. We refer to si:=⌊k1−i⁢ϵ⌋assignsubscript𝑠𝑖superscript𝑘1𝑖italic-ϵs_{i}:=\lfloor k^{1-i\epsilon}\rflooritalic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT := ⌊ italic_k start_POSTSUPERSCRIPT 1 - italic_i italic_ϵ end_POSTSUPERSCRIPT ⌋ as the slack at layer i𝑖iitalic_i of the hierarchy. The set Sℓ+1subscript𝑆ℓ1S_{\ell+1}italic_S start_POSTSUBSCRIPT roman_ℓ + 1 end_POSTSUBSCRIPT has size exactly k𝑘kitalic_k and is the k𝑘kitalic_k-median solution maintained by the algorithm. We always maintain the following invariant. Invariant 2.1. Cost⁢(S0,𝒫)=O⁢(1)⋅OPTk⁢(𝒫)Costsubscript𝑆0𝒫⋅𝑂1subscriptOPT𝑘𝒫\textnormal{{Cost}}\left(S_{0},\mathcal{P}\right)=O(1)\cdot\textnormal{{OPT}}_% {k}(\mathcal{P})Cost ( italic_S start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , caligraphic_P ) = italic_O ( 1 ) ⋅ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ), and Cost⁢(Si,𝒫)≤Cost⁢(Si−1,𝒫)+O⁢(1)⋅OPTk⁢(𝒫)Costsubscript𝑆𝑖𝒫Costsubscript𝑆𝑖1𝒫⋅𝑂1subscriptOPT𝑘𝒫\textnormal{{Cost}}\left(S_{i},\mathcal{P}\right)\leq\textnormal{{Cost}}\left(% S_{i-1},\mathcal{P}\right)+O(1)\cdot\textnormal{{OPT}}_{k}(\mathcal{P})Cost ( italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_P ) ≤ Cost ( italic_S start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT , caligraphic_P ) + italic_O ( 1 ) ⋅ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ) for each i∈[1,ℓ]𝑖1ℓi\in[1,\ell]italic_i ∈ [ 1 , roman_ℓ ]. Here, OPTk⁢(𝒫)subscriptOPT𝑘𝒫\textnormal{{OPT}}_{k}(\mathcal{P})OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ) is the optimal k𝑘kitalic_k-median objective w.r.t. the input point-set 𝒫𝒫\mathcal{P}caligraphic_P. Given this invariant, we infer that Cost⁢(Sℓ+1)=O⁢(ℓ)⋅OPTk⁢(𝒫)=O⁢(1/ϵ)⋅OPTk⁢(𝒫)Costsubscript𝑆ℓ1⋅𝑂ℓsubscriptOPT𝑘𝒫⋅𝑂1italic-ϵsubscriptOPT𝑘𝒫\textnormal{{Cost}}\left(S_{\ell+1}\right)=O(\ell)\cdot\textnormal{{OPT}}_{k}(% \mathcal{P})=O(1/\epsilon)\cdot\textnormal{{OPT}}_{k}(\mathcal{P})Cost ( italic_S start_POSTSUBSCRIPT roman_ℓ + 1 end_POSTSUBSCRIPT ) = italic_O ( roman_ℓ ) ⋅ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ) = italic_O ( 1 / italic_ϵ ) ⋅ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ). Thus, the approximation guarantee is proportional to the number of layers in this hierarchy. The hierarchy is maintained by periodically reconstructing each of the sets Sisubscript𝑆𝑖S_{i}italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. To be more specific, each set Sisubscript𝑆𝑖S_{i}italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT (along with the sets Si+1,…,Sℓ+1subscript𝑆𝑖1…subscript𝑆ℓ1S_{i+1},\dots,S_{\ell+1}italic_S start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT , … , italic_S start_POSTSUBSCRIPT roman_ℓ + 1 end_POSTSUBSCRIPT) is reconstructed from scratch every sisubscript𝑠𝑖s_{i}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT many updates, without modifying the sets S0,…,Si−1subscript𝑆0…subscript𝑆𝑖1S_{0},\dots,S_{i-1}italic_S start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , … , italic_S start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT. In between these updates, the subset Sisubscript𝑆𝑖S_{i}italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is maintained lazily: Whenever a point p𝑝pitalic_p is inserted into 𝒫𝒫\mathcal{P}caligraphic_P, we set Si←Si∪{p}←subscript𝑆𝑖subscript𝑆𝑖𝑝S_{i}\leftarrow S_{i}\cup\{p\}italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ← italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∪ { italic_p }, and whenever a point is deleted from 𝒫𝒫\mathcal{P}caligraphic_P, no changes are made to Sisubscript𝑆𝑖S_{i}italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT.444We can afford to handle the deletions in this lazy manner if we consider the improper k𝑘kitalic_k-median problem, where we are allowed to open a center at a point that got deleted. See the discussion in the beginning of Section 3. To analyze the recourse, consider the solution Sℓ+1subscript𝑆ℓ1S_{\ell+1}italic_S start_POSTSUBSCRIPT roman_ℓ + 1 end_POSTSUBSCRIPT before and after an update during which Sisubscript𝑆𝑖S_{i}italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is reconstructed. Let Sℓ+1′superscriptsubscript𝑆ℓ1′S_{\ell+1}^{\prime}italic_S start_POSTSUBSCRIPT roman_ℓ + 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT and Sℓ+1′′superscriptsubscript𝑆ℓ1′′S_{\ell+1}^{\prime\prime}italic_S start_POSTSUBSCRIPT roman_ℓ + 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ ′ end_POSTSUPERSCRIPT denote the status of the solution before and after the update, respectively. Observe that the total recourse incurred in the solution Sℓ+1subscript𝑆ℓ1S_{\ell+1}italic_S start_POSTSUBSCRIPT roman_ℓ + 1 end_POSTSUBSCRIPT during this update, i.e. the value |Sℓ+1′⊕Sℓ+1′′|direct-sumsuperscriptsubscript𝑆ℓ1′superscriptsubscript𝑆ℓ1′′|S_{\ell+1}^{\prime}\oplus S_{\ell+1}^{\prime\prime}|| italic_S start_POSTSUBSCRIPT roman_ℓ + 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ⊕ italic_S start_POSTSUBSCRIPT roman_ℓ + 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ ′ end_POSTSUPERSCRIPT |, is O⁢(si−1)𝑂subscript𝑠𝑖1O(s_{i-1})italic_O ( italic_s start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ). This follows immediately from the fact that Sℓ+1′superscriptsubscript𝑆ℓ1′S_{\ell+1}^{\prime}italic_S start_POSTSUBSCRIPT roman_ℓ + 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT and Sℓ+1′′superscriptsubscript𝑆ℓ1′′S_{\ell+1}^{\prime\prime}italic_S start_POSTSUBSCRIPT roman_ℓ + 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ ′ end_POSTSUPERSCRIPT are both subsets of size k𝑘kitalic_k of the set Si−1subscript𝑆𝑖1S_{i-1}italic_S start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT, which has size ≤k+2⁢si−1absent𝑘2subscript𝑠𝑖1\leq k+2s_{i-1}≤ italic_k + 2 italic_s start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT. We can amortize this recourse over the sisubscript𝑠𝑖s_{i}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT many lazy updates performed since the last time that Sisubscript𝑆𝑖S_{i}italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT was reconstructed. This implies that the amortized recourse of Sℓ+1subscript𝑆ℓ1S_{\ell+1}italic_S start_POSTSUBSCRIPT roman_ℓ + 1 end_POSTSUBSCRIPT which is caused by reconstructing Sisubscript𝑆𝑖S_{i}italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is O⁢(si−1/si)=O⁢(kϵ)𝑂subscript𝑠𝑖1subscript𝑠𝑖𝑂superscript𝑘italic-ϵO(s_{i-1}/s_{i})=O(k^{\epsilon})italic_O ( italic_s start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT / italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = italic_O ( italic_k start_POSTSUPERSCRIPT italic_ϵ end_POSTSUPERSCRIPT ). Summing over all i∈[0,ℓ+1]𝑖0ℓ1i\in[0,\ell+1]italic_i ∈ [ 0 , roman_ℓ + 1 ], we get an overall amortized recourse bound of O⁢((ℓ+2)⋅kϵ)=O⁢(kϵ/ϵ)𝑂⋅ℓ2superscript𝑘italic-ϵ𝑂superscript𝑘italic-ϵitalic-ϵO((\ell+2)\cdot k^{\epsilon})=O(k^{\epsilon}/\epsilon)italic_O ( ( roman_ℓ + 2 ) ⋅ italic_k start_POSTSUPERSCRIPT italic_ϵ end_POSTSUPERSCRIPT ) = italic_O ( italic_k start_POSTSUPERSCRIPT italic_ϵ end_POSTSUPERSCRIPT / italic_ϵ ). Barrier towards achieving O⁢(1)𝑂1O(1)italic_O ( 1 )-approximation with O~⁢(1)~𝑂1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) recourse. If we want to use this hierarchy to obtain a recourse of O~⁢(1)~𝑂1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ), then we need to ensure that si=Ω~⁢(si−1)subscript𝑠𝑖~Ωsubscript𝑠𝑖1s_{i}=\tilde{\Omega}(s_{i-1})italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = over~ start_ARG roman_Ω end_ARG ( italic_s start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ) (i.e., we need the slacks at the layers to decrease by at most a O~⁢(1)~𝑂1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) factor between the layers). If this is not the case, then the amortized recourse in Sℓ+1subscript𝑆ℓ1S_{\ell+1}italic_S start_POSTSUBSCRIPT roman_ℓ + 1 end_POSTSUBSCRIPT caused by reconstructing Sisubscript𝑆𝑖S_{i}italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT will be Ω⁢(si−1/si)=ω~⁢(1)Ωsubscript𝑠𝑖1subscript𝑠𝑖~𝜔1\Omega(s_{i-1}/s_{i})=\tilde{\omega}(1)roman_Ω ( italic_s start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT / italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = over~ start_ARG italic_ω end_ARG ( 1 ). Unfortunately, to have such a guarantee, the number of layers needs to be Ω⁢(log⁡k/log⁡log⁡k)Ω𝑘𝑘\Omega(\log k/\log\log k)roman_Ω ( roman_log italic_k / roman_log roman_log italic_k ); and since the approximation ratio of the algorithm is proportional to the number of layers (see 2.1), this leads to an approximation ratio of ω⁢(1)𝜔1\omega(1)italic_ω ( 1 ). Thus, it is not at all clear if this approach can be used to obtain O⁢(1)𝑂1O(1)italic_O ( 1 )-approximation and O~⁢(1)~𝑂1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) recourse simultaneously. 2.1.2 Achieving O~⁢(k1+ϵ)~𝑂superscript𝑘1italic-ϵ\tilde{O}(k^{1+\epsilon})over~ start_ARG italic_O end_ARG ( italic_k start_POSTSUPERSCRIPT 1 + italic_ϵ end_POSTSUPERSCRIPT ) Update Time via Randomized Local Search The second technical contribution in [BCG+24] is to show that the hierarchy from Section 2.1.1 can be maintained in O~⁢(k1+ϵ)~𝑂superscript𝑘1italic-ϵ\tilde{O}(k^{1+\epsilon})over~ start_ARG italic_O end_ARG ( italic_k start_POSTSUPERSCRIPT 1 + italic_ϵ end_POSTSUPERSCRIPT ) update time, using a specific type of randomized local search. To be more specific, consider a set of n𝑛nitalic_n points 𝒫𝒫\mathcal{P}caligraphic_P, a set of k𝑘kitalic_k centers 𝒰𝒰\mathcal{U}caligraphic_U, and an integer s∈[1,k−1]𝑠1𝑘1s\in[1,k-1]italic_s ∈ [ 1 , italic_k - 1 ]. Suppose that we want to compute a subset 𝒰′⊆𝒰superscript𝒰′𝒰\mathcal{U}^{\prime}\subseteq\mathcal{U}caligraphic_U start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ⊆ caligraphic_U of (k−s)𝑘𝑠(k-s)( italic_k - italic_s ) centers, so as to minimize Cost⁢(𝒰′,𝒫)Costsuperscript𝒰′𝒫\textnormal{{Cost}}\left(\mathcal{U}^{\prime},\mathcal{P}\right)Cost ( caligraphic_U start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , caligraphic_P ). In [BCG+24], the authors present a O⁢(1)𝑂1O(1)italic_O ( 1 )-approximation algorithm for this problem using randomized local search, that runs in only O~⁢(n⁢s)~𝑂𝑛𝑠\tilde{O}(ns)over~ start_ARG italic_O end_ARG ( italic_n italic_s ) time, assuming the algorithm has access to some “auxiliary data structures” to begin with (see Lemma 6.1). Morally, the important message here is that the runtime of randomized local search (when given access to some auxiliary data structures) is proportional to the slack s𝑠sitalic_s, and independent of k𝑘kitalic_k. In [BCG+24], the authors call this procedure as a subroutine while reconstructing a set Sisubscript𝑆𝑖S_{i}italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT (along with the sets Si+1,…,Sℓ+1subscript𝑆𝑖1…subscript𝑆ℓ1S_{i+1},\dots,S_{\ell+1}italic_S start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT , … , italic_S start_POSTSUBSCRIPT roman_ℓ + 1 end_POSTSUBSCRIPT) in the hierarchy from scratch, after every sisubscript𝑠𝑖s_{i}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT many updates. Although our algorithm does not use a hierarchical approach while bounding the approximation ratio and recourse, we use randomized local search to achieve fast update time (see Section 2.3). 2.2 The Incremental Algorithm of [FLNS21] In [FLNS21], the authors obtain O~⁢(k)~𝑂𝑘\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) total recourse, while maintaining a O⁢(1)𝑂1O(1)italic_O ( 1 )-approximate k𝑘kitalic_k-median solution over a sequence of n𝑛nitalic_n point insertions (starting from an empty input). Note that in this incremental setting, the total recourse is sublinear in the number of updates; this is achieved by using a technique known as Myerson sketch [Mey01]. Since it is not possible to achieve such a sublinear total recourse bound in the fully dynamic setting (the focus of our paper), in Theorem 2.2 we summarize the main result of [FLNS21] without invoking Myerson sketch. We emphasize that the update time in [FLNS21] is already prohibitively high (some large polynomial in n𝑛nitalic_n) for our purpose. Accordingly, to highlight the main ideas in the rest of this section, we will outline a variant of the algorithm in [FLNS21] with exponential update time. Theorem 2.2 ([FLNS21]). Suppose that the input 𝒫𝒫\mathcal{P}caligraphic_P undergoes a sequence of point-insertions. Then, we can maintain a O⁢(1)𝑂1O(1)italic_O ( 1 )-approximate k𝑘kitalic_k-median solution to 𝒫𝒫\mathcal{P}caligraphic_P with O~⁢(1)~𝑂1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) amortized recourse. A major technical insight in [FLNS21] was to introduce the notion of robust centers. Informally, a set of centers 𝒰𝒰\mathcal{U}caligraphic_U is robust w.r.t. a point-set 𝒫𝒫\mathcal{P}caligraphic_P, iff each u∈𝒰𝑢𝒰u\in\mathcal{U}italic_u ∈ caligraphic_U is a good approximate 1111-median solution at every “distance-scale” w.r.t. the points in 𝒫𝒫\mathcal{P}caligraphic_P that are sufficiently close to u𝑢uitalic_u. See Section 3.2 for a formal definition. Below, we present our interpretation of the incremental algorithm in [FLNS21]. We start with a key lemma summarizing an important property of robust centers. Consider any integer 0≤ℓ≤k0ℓ𝑘0\leq\ell\leq k0 ≤ roman_ℓ ≤ italic_k. We say that a set of k𝑘kitalic_k centers 𝒰𝒰\mathcal{U}caligraphic_U is maximally ℓℓ\ellroman_ℓ-stable w.r.t. a point-set 𝒫𝒫\mathcal{P}caligraphic_P iff OPTk−ℓ𝒰⁢(𝒫)≤c⋅Cost⁢(𝒰,𝒫)superscriptsubscriptOPT𝑘ℓ𝒰𝒫⋅𝑐Cost𝒰𝒫\textnormal{{OPT}}_{k-\ell}^{\mathcal{U}}(\mathcal{P})\leq c\cdot\textnormal{{% Cost}}\left(\mathcal{U},\mathcal{P}\right)OPT start_POSTSUBSCRIPT italic_k - roman_ℓ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT caligraphic_U end_POSTSUPERSCRIPT ( caligraphic_P ) ≤ italic_c ⋅ Cost ( caligraphic_U , caligraphic_P ) and OPTk−(ℓ+1)𝒰⁢(𝒫)>c⋅Cost⁢(𝒰,𝒫)superscriptsubscriptOPT𝑘ℓ1𝒰𝒫⋅𝑐Cost𝒰𝒫\textnormal{{OPT}}_{k-(\ell+1)}^{\mathcal{U}}(\mathcal{P})>c\cdot\textnormal{{% Cost}}\left(\mathcal{U},\mathcal{P}\right)OPT start_POSTSUBSCRIPT italic_k - ( roman_ℓ + 1 ) end_POSTSUBSCRIPT start_POSTSUPERSCRIPT caligraphic_U end_POSTSUPERSCRIPT ( caligraphic_P ) > italic_c ⋅ Cost ( caligraphic_U , caligraphic_P ), where c=456000𝑐456000c=456000italic_c = 456000 is an absolute constant, and OPTt𝒰⁢(𝒫):=minZ⊆𝒰:|Z|≤t⁡Cost⁢(Z,𝒫)assignsuperscriptsubscriptOPT𝑡𝒰𝒫subscript:𝑍𝒰𝑍𝑡Cost𝑍𝒫\textnormal{{OPT}}_{t}^{\mathcal{U}}(\mathcal{P}):=\min_{Z\subseteq\mathcal{U}% :|Z|\leq t}\textnormal{{Cost}}\left(Z,\mathcal{P}\right)OPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT caligraphic_U end_POSTSUPERSCRIPT ( caligraphic_P ) := roman_min start_POSTSUBSCRIPT italic_Z ⊆ caligraphic_U : | italic_Z | ≤ italic_t end_POSTSUBSCRIPT Cost ( italic_Z , caligraphic_P ) is the objective of the optimal t𝑡titalic_t-median solution subject to the restriction that all the centers must be from the set 𝒰𝒰\mathcal{U}caligraphic_U. This means that we can afford to remove ℓℓ\ellroman_ℓ centers from 𝒰𝒰\mathcal{U}caligraphic_U without increasing the objective value by more than a O⁢(1)𝑂1O(1)italic_O ( 1 ) factor, but not more than ℓℓ\ellroman_ℓ centers. We defer the proof of Lemma 2.3 to Section 9. Lemma 2.3 ([FLNS21]). Consider any two point-sets 𝒫initsubscript𝒫init\mathcal{P}_{\textnormal{{init}}}caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT and 𝒫finalsubscript𝒫final\mathcal{P}_{\textnormal{{final}}}caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT with |𝒫init⊕𝒫final|≤ℓ+1direct-sumsubscript𝒫initsubscript𝒫finalℓ1\left|\mathcal{P}_{\textnormal{{init}}}\oplus\mathcal{P}_{\textnormal{{final}}% }\right|\leq\ell+1| caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ⊕ caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT | ≤ roman_ℓ + 1, for ℓ∈[0,k]ℓ0𝑘\ell\in[0,k]roman_ℓ ∈ [ 0 , italic_k ].555Here, the notation ⊕direct-sum\oplus⊕ denotes the symmetric difference between two sets. W.r.t. 𝒫initsubscript𝒫init\mathcal{P}_{\textnormal{{init}}}caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT, let 𝒰initsubscript𝒰init\mathcal{U}_{\textnormal{{init}}}caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT be any set of k𝑘kitalic_k centers that is robust and maximally ℓℓ\ellroman_ℓ-stable. Let 𝒱𝒱\mathcal{V}caligraphic_V be any set of k𝑘kitalic_k centers such that Cost⁢(𝒱,𝒫init)≤18⋅Cost⁢(𝒰init,𝒫init)Cost𝒱subscript𝒫init⋅18Costsubscript𝒰initsubscript𝒫init\textnormal{{Cost}}\left(\mathcal{V},\mathcal{P}_{\textnormal{{init}}}\right)% \leq 18\cdot\textnormal{{Cost}}\left(\mathcal{U}_{\textnormal{{init}}},% \mathcal{P}_{\textnormal{{init}}}\right)Cost ( caligraphic_V , caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ) ≤ 18 ⋅ Cost ( caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT , caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ). Then, there is a set of k𝑘kitalic_k centers 𝒲⋆superscript𝒲⋆\mathcal{W}^{\star}caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT, such that |𝒲⋆⊕𝒰init|≤5⁢ℓ+5direct-sumsuperscript𝒲⋆subscript𝒰init5ℓ5\left|\mathcal{W}^{\star}\oplus\mathcal{U}_{\textnormal{{init}}}\right|\leq 5% \ell+5| caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ⊕ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT | ≤ 5 roman_ℓ + 5 and Cost⁢(𝒲⋆,𝒫final)≤3⋅Cost⁢(𝒱,𝒫final)Costsuperscript𝒲⋆subscript𝒫final⋅3Cost𝒱subscript𝒫final\textnormal{{Cost}}\left(\mathcal{W}^{\star},\mathcal{P}_{\textnormal{{final}}% }\right)\leq 3\cdot\textnormal{{Cost}}\left(\mathcal{V},\mathcal{P}_{% \textnormal{{final}}}\right)Cost ( caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT , caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT ) ≤ 3 ⋅ Cost ( caligraphic_V , caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT ). The algorithm works in O~⁢(1)~𝑂1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) many phases, where each phase consists of a sequence of consecutive updates (only insertions) such that the optimal objective value, given by OPTk⁢(𝒫)subscriptOPT𝑘𝒫\textnormal{{OPT}}_{k}(\mathcal{P})OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ), does not increase by more than a factor of 18181818 within any given phase. The algorithm restarts whenever one phase terminates and the next phase begins, and computes a new k𝑘kitalic_k-median solution to the current input from scratch. Within each phase, the algorithm incurs O~⁢(n)~𝑂𝑛\tilde{O}(n)over~ start_ARG italic_O end_ARG ( italic_n ) total recourse, and this implies the amortized recourse guarantee of O~⁢(1)~𝑂1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ). Each phase is further partitioned into epochs, as follows. By induction hypothesis, we start an epoch with an 100100100100-approximate k𝑘kitalic_k-median solution 𝒰initsubscript𝒰init\mathcal{U}_{\textnormal{{init}}}caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT that is robust and also maximally ℓℓ\ellroman_ℓ-stable, for some ℓ∈[0,k]ℓ0𝑘\ell\in[0,k]roman_ℓ ∈ [ 0 , italic_k ], w.r.t. the current input 𝒫initsubscript𝒫init\mathcal{P}_{\textnormal{{init}}}caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT. Let λinit:=OPTk⁢(𝒫init)assignsubscript𝜆initsubscriptOPT𝑘subscript𝒫init\lambda_{\textnormal{{init}}}:=\textnormal{{OPT}}_{k}(\mathcal{P}_{\textnormal% {{init}}})italic_λ start_POSTSUBSCRIPT init end_POSTSUBSCRIPT := OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ) denote the optimal objective value at this point in time. We then compute a subset 𝒰⊆𝒰init𝒰subscript𝒰init\mathcal{U}\subseteq\mathcal{U}_{\textnormal{{init}}}caligraphic_U ⊆ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT of (k−ℓ)𝑘ℓ(k-\ell)( italic_k - roman_ℓ ) centers, such that Cost⁢(𝒰,𝒫init)≤c⋅Cost⁢(𝒰init,𝒫init)≤100⁢c⋅OPTk⁢(𝒫init)=100⁢c⋅λinitCost𝒰subscript𝒫init⋅𝑐Costsubscript𝒰initsubscript𝒫init⋅100𝑐subscriptOPT𝑘subscript𝒫init⋅100𝑐subscript𝜆init\textnormal{{Cost}}\left(\mathcal{U},\mathcal{P}_{\textnormal{{init}}}\right)% \leq c\cdot\textnormal{{Cost}}\left(\mathcal{U}_{\textnormal{{init}}},\mathcal% {P}_{\textnormal{{init}}}\right)\leq 100c\cdot\textnormal{{OPT}}_{k}(\mathcal{% P}_{\textnormal{{init}}})=100c\cdot\lambda_{\textnormal{{init}}}Cost ( caligraphic_U , caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ) ≤ italic_c ⋅ Cost ( caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT , caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ) ≤ 100 italic_c ⋅ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ) = 100 italic_c ⋅ italic_λ start_POSTSUBSCRIPT init end_POSTSUBSCRIPT. The epoch lasts for the next (ℓ+1)ℓ1(\ell+1)( roman_ℓ + 1 ) updates. The algorithm lazily handles the first ℓℓ\ellroman_ℓ updates in the epoch, incurring a worst-case recourse of one per update: Whenever a point p𝑝pitalic_p gets inserted into 𝒫𝒫\mathcal{P}caligraphic_P, it sets 𝒰←𝒰∪{p}←𝒰𝒰𝑝\mathcal{U}\leftarrow\mathcal{U}\cup\{p\}caligraphic_U ← caligraphic_U ∪ { italic_p }. Since initially |𝒰|=k−ℓ𝒰𝑘ℓ|\mathcal{U}|=k-\ell| caligraphic_U | = italic_k - roman_ℓ, the set 𝒰𝒰\mathcal{U}caligraphic_U never grows large enough to contain more than k𝑘kitalic_k centers. Further, the objective of the maintained solution 𝒰𝒰\mathcal{U}caligraphic_U does not increase due to these ℓℓ\ellroman_ℓ updates, and remains Cost⁢(𝒰,𝒫)≤100⁢c⋅λinit≤200⁢c⋅OPTk⁢(𝒫),Cost𝒰𝒫⋅100𝑐subscript𝜆init⋅200𝑐subscriptOPT𝑘𝒫\textnormal{{Cost}}\left(\mathcal{U},\mathcal{P}\right)\leq 100c\cdot\lambda_{% \textnormal{{init}}}\leq 200c\cdot\textnormal{{OPT}}_{k}(\mathcal{P}),Cost ( caligraphic_U , caligraphic_P ) ≤ 100 italic_c ⋅ italic_λ start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ≤ 200 italic_c ⋅ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ) , (1) where the last inequality holds because OPTk⁢(𝒫)subscriptOPT𝑘𝒫\textnormal{{OPT}}_{k}(\mathcal{P})OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ) is almost monotone as 𝒫𝒫\mathcal{P}caligraphic_P undergoes point-insertions (more precisely, it can decrease by at most a factor of 2222). While handling the last (i.e., (ℓ+1)t⁢hsuperscriptℓ1𝑡ℎ(\ell+1)^{th}( roman_ℓ + 1 ) start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT) update in the epoch, our goal is to come up with a k𝑘kitalic_k-median solution 𝒰finalsubscript𝒰final\mathcal{U}_{\textnormal{{final}}}caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT such that: (i) the induction hypothesis holds w.r.t. 𝒰finalsubscript𝒰final\mathcal{U}_{\textnormal{{final}}}caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT and 𝒫finalsubscript𝒫final\mathcal{P}_{\textnormal{{final}}}caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT (where 𝒫finalsubscript𝒫final\mathcal{P}_{\textnormal{{final}}}caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT is the state of the input at the end of the epoch), and (ii) the recourse remains small, i.e., |𝒰final⊕𝒰init|=O⁢(ℓ+1)direct-sumsubscript𝒰finalsubscript𝒰init𝑂ℓ1\left|\mathcal{U}_{\textnormal{{final}}}\oplus\mathcal{U}_{\textnormal{{init}}% }\right|=O(\ell+1)| caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT ⊕ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT | = italic_O ( roman_ℓ + 1 ). We can assume that OPTk⁢(𝒫final)≤18⋅λinitsubscriptOPT𝑘subscript𝒫final⋅18subscript𝜆init\textnormal{{OPT}}_{k}(\mathcal{P}_{\textnormal{{final}}})\leq 18\cdot\lambda_% {\textnormal{{init}}}OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT ) ≤ 18 ⋅ italic_λ start_POSTSUBSCRIPT init end_POSTSUBSCRIPT, for otherwise we would initiate a new phase at this point in time. We find the set 𝒰finalsubscript𝒰final\mathcal{U}_{\textnormal{{final}}}caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT as follows. Let 𝒱𝒱\mathcal{V}caligraphic_V be an optimal k𝑘kitalic_k-median solution w.r.t. 𝒫finalsubscript𝒫final\mathcal{P}_{\textnormal{{final}}}caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT, so that: Cost⁢(𝒱,𝒫init)≤Cost⁢(𝒱,𝒫final)=OPTk⁢(𝒫final)≤18⋅OPTk⁢(𝒫init)≤18⋅Cost⁢(𝒰init,𝒫init).Cost𝒱subscript𝒫initCost𝒱subscript𝒫finalsubscriptOPT𝑘subscript𝒫final⋅18subscriptOPT𝑘subscript𝒫init⋅18Costsubscript𝒰initsubscript𝒫init\textnormal{{Cost}}\left(\mathcal{V},\mathcal{P}_{\textnormal{{init}}}\right)% \leq\textnormal{{Cost}}\left(\mathcal{V},\mathcal{P}_{\textnormal{{final}}}% \right)=\textnormal{{OPT}}_{k}(\mathcal{P}_{\textnormal{{final}}})\leq 18\cdot% \textnormal{{OPT}}_{k}(\mathcal{P}_{\textnormal{{init}}})\leq 18\cdot% \textnormal{{Cost}}\left(\mathcal{U}_{\textnormal{{init}}},\mathcal{P}_{% \textnormal{{init}}}\right).Cost ( caligraphic_V , caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ) ≤ Cost ( caligraphic_V , caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT ) = OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT ) ≤ 18 ⋅ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ) ≤ 18 ⋅ Cost ( caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT , caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ) . (2) Applying Lemma 2.3, we find a set of k𝑘kitalic_k centers 𝒲⋆superscript𝒲⋆\mathcal{W}^{\star}caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT such that |𝒲⋆⊕𝒰init|≤5⁢ℓ+5⁢ and Cost⁢(𝒲⋆,𝒫final)≤3⋅Cost⁢(𝒱,𝒫final)=3⋅OPTk⁢(𝒫final).direct-sumsuperscript𝒲⋆subscript𝒰init5ℓ5 and Costsuperscript𝒲⋆subscript𝒫final⋅3Cost𝒱subscript𝒫final⋅3subscriptOPT𝑘subscript𝒫final\left|\mathcal{W}^{\star}\oplus\mathcal{U}_{\textnormal{{init}}}\right|\leq 5% \ell+5\text{ and }\textnormal{{Cost}}\left(\mathcal{W}^{\star},\mathcal{P}_{% \textnormal{{final}}}\right)\leq 3\cdot\textnormal{{Cost}}\left(\mathcal{V},% \mathcal{P}_{\textnormal{{final}}}\right)=3\cdot\textnormal{{OPT}}_{k}(% \mathcal{P}_{\textnormal{{final}}}).| caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ⊕ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT | ≤ 5 roman_ℓ + 5 and sansserif_Cost ( caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT , caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT ) ≤ 3 ⋅ Cost ( caligraphic_V , caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT ) = 3 ⋅ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT ) . (3) Next, we call a subroutine Robustify(𝒲⋆)superscript𝒲⋆(\mathcal{W}^{\star})( caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) which returns a set of k𝑘kitalic_k robust centers 𝒰finalsubscript𝒰final\mathcal{U}_{\textnormal{{final}}}caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT such that Cost⁢(𝒰final,𝒫final)≤(3/2)⋅Cost⁢(𝒲⋆,𝒫final)=(9/2)⋅OPTk⁢(𝒫final)≤100⋅OPTk⁢(𝒫final)Costsubscript𝒰finalsubscript𝒫final⋅32Costsuperscript𝒲⋆subscript𝒫final⋅92subscriptOPT𝑘subscript𝒫final⋅100subscriptOPT𝑘subscript𝒫final\textnormal{{Cost}}\left(\mathcal{U}_{\textnormal{{final}}},\mathcal{P}_{% \textnormal{{final}}}\right)\leq(3/2)\cdot\textnormal{{Cost}}\left(\mathcal{W}% ^{\star},\mathcal{P}_{\textnormal{{final}}}\right)=(9/2)\cdot\textnormal{{OPT}% }_{k}(\mathcal{P}_{\textnormal{{final}}})\leq 100\cdot\textnormal{{OPT}}_{k}(% \mathcal{P}_{\textnormal{{final}}})Cost ( caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT , caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT ) ≤ ( 3 / 2 ) ⋅ Cost ( caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT , caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT ) = ( 9 / 2 ) ⋅ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT ) ≤ 100 ⋅ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT ) (see Lemma 3.7). This restores the induction hypothesis for the next epoch, w.r.t. 𝒰finalsubscript𝒰final\mathcal{U}_{\textnormal{{final}}}caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT and 𝒫finalsubscript𝒫final\mathcal{P}_{\textnormal{{final}}}caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT. We can show that the subroutine Robustify works in such a manner that the step where we transition from 𝒲⋆superscript𝒲⋆\mathcal{W}^{\star}caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT to 𝒰finalsubscript𝒰final\mathcal{U}_{\textnormal{{final}}}caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT incurs at most O~⁢(1)~𝑂1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) recourse, amortized over the entire sequence of updates within a phase (spanning across multiple epochs). This implies Theorem 2.2. 2.3 Our Approach At a high level, we achieve our result in two parts. First, we generalize the framework of [FLNS21] to achieve O⁢(1)𝑂1O(1)italic_O ( 1 )-approximation and O~⁢(1)~𝑂1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) recourse in the fully dynamic setting. Second, we use the randomized local search procedure to implement our algorithm in O~⁢(k)~𝑂𝑘\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) update time. In addition, both these parts require us to come up with important and new technical insights of our own. Below, we explain three significant challenges and outline how we overcome them. Challenge I and Challenge II refers to the first part (approximation and recourse guarantees), whereas Challenge III refers to the third part (update time guarantee). 2.3.1 Challenge I: Double-sided Stability In Section 2.2, we crucially relied on the observation that the optimal k𝑘kitalic_k-median objective is (almost) monotonically increasing as more and more points get inserted into 𝒫𝒫\mathcal{P}caligraphic_P. This allowed us to derive Equation 1, which guarantees that the maintained solution 𝒰𝒰\mathcal{U}caligraphic_U remains O⁢(1)𝑂1O(1)italic_O ( 1 )-approximate while we lazily handle the first ℓℓ\ellroman_ℓ updates within the epoch. This guarantee, however, breaks down in the fully dynamic setting: If points can get deleted from 𝒫𝒫\mathcal{P}caligraphic_P, then within an epoch we might end up in a situation where OPTk⁢(𝒫)≪λinitmuch-less-thansubscriptOPT𝑘𝒫subscript𝜆init\textnormal{{OPT}}_{k}(\mathcal{P})\ll\lambda_{\textnormal{{init}}}OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ) ≪ italic_λ start_POSTSUBSCRIPT init end_POSTSUBSCRIPT. To address this issue, we derive a new double-sided stability property in the fully dynamic setting (see Lemma 4.1). Informally, this implies that if OPTk−ℓ⁢(𝒫init)=Θ⁢(1)⋅OPTk⁢(𝒫init)subscriptOPT𝑘ℓsubscript𝒫init⋅Θ1subscriptOPT𝑘subscript𝒫init\textnormal{{OPT}}_{k-\ell}(\mathcal{P}_{\textnormal{{init}}})=\Theta(1)\cdot% \textnormal{{OPT}}_{k}(\mathcal{P}_{\textnormal{{init}}})OPT start_POSTSUBSCRIPT italic_k - roman_ℓ end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ) = roman_Θ ( 1 ) ⋅ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ) (which follows from the hypothesis at the start of an epoch), then for some Θ⁢(ℓ)=r≤ℓΘℓ𝑟ℓ\Theta(\ell)=r\leq\ellroman_Θ ( roman_ℓ ) = italic_r ≤ roman_ℓ we have OPTk+r⁢(𝒫init)=Θ⁢(1)⋅OPTk⁢(𝒫init)subscriptOPT𝑘𝑟subscript𝒫init⋅Θ1subscriptOPT𝑘subscript𝒫init\textnormal{{OPT}}_{k+r}(\mathcal{P}_{\textnormal{{init}}})=\Theta(1)\cdot% \textnormal{{OPT}}_{k}(\mathcal{P}_{\textnormal{{init}}})OPT start_POSTSUBSCRIPT italic_k + italic_r end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ) = roman_Θ ( 1 ) ⋅ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ). Furthermore, we have |𝒫⊕𝒫init|≤rdirect-sum𝒫subscript𝒫init𝑟\left|\mathcal{P}\oplus\mathcal{P}_{\textnormal{{init}}}\right|\leq r| caligraphic_P ⊕ caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT | ≤ italic_r throughout the first r𝑟ritalic_r updates in the epoch, which gives us: OPTk⁢(𝒫)≥OPTk+r⁢(𝒫init)subscriptOPT𝑘𝒫subscriptOPT𝑘𝑟subscript𝒫init\textnormal{{OPT}}_{k}(\mathcal{P})\geq\textnormal{{OPT}}_{k+r}(\mathcal{P}_{% \textnormal{{init}}})OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ) ≥ OPT start_POSTSUBSCRIPT italic_k + italic_r end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ) (see Lemma 3.11). It follows that for the first r𝑟ritalic_r updates in the epoch, we have OPTk⁢(𝒫)≥OPTk+r⁢(𝒫init)=Θ⁢(1)⋅OPTk⁢(𝒫init)=Θ⁢(1)⋅λinitsubscriptOPT𝑘𝒫subscriptOPT𝑘𝑟subscript𝒫init⋅Θ1subscriptOPT𝑘subscript𝒫init⋅Θ1subscript𝜆init\textnormal{{OPT}}_{k}(\mathcal{P})\geq\textnormal{{OPT}}_{k+r}(\mathcal{P}_{% \textnormal{{init}}})=\Theta(1)\cdot\textnormal{{OPT}}_{k}(\mathcal{P}_{% \textnormal{{init}}})=\Theta(1)\cdot\lambda_{\textnormal{{init}}}OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ) ≥ OPT start_POSTSUBSCRIPT italic_k + italic_r end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ) = roman_Θ ( 1 ) ⋅ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ) = roman_Θ ( 1 ) ⋅ italic_λ start_POSTSUBSCRIPT init end_POSTSUBSCRIPT. Accordingly, we truncate the epoch to last for only r+1𝑟1r+1italic_r + 1 updates, and now we can rule out the scenario where OPTk⁢(𝒫)subscriptOPT𝑘𝒫\textnormal{{OPT}}_{k}(\mathcal{P})OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ) drops significantly below λinitsubscript𝜆init\lambda_{\textnormal{{init}}}italic_λ start_POSTSUBSCRIPT init end_POSTSUBSCRIPT during the epoch. But since r=Ω⁢(ℓ)𝑟Ωℓr=\Omega(\ell)italic_r = roman_Ω ( roman_ℓ ), the epoch remains sufficiently long, so that we can still manage to generalize the recourse analysis from Section 2.2. 2.3.2 Challenge II: Getting Rid of the Phases To derive Equation 2, we need to have OPTk⁢(𝒫final)≤18⋅OPTk⁢(𝒫init)subscriptOPT𝑘subscript𝒫final⋅18subscriptOPT𝑘subscript𝒫init\textnormal{{OPT}}_{k}(\mathcal{P}_{\textnormal{{final}}})\leq 18\cdot% \textnormal{{OPT}}_{k}(\mathcal{P}_{\textnormal{{init}}})OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT ) ≤ 18 ⋅ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ). This is precisely the reason why the algorithm in [FLNS21] works in phases, so that OPTk⁢(𝒫)subscriptOPT𝑘𝒫\textnormal{{OPT}}_{k}(\mathcal{P})OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ) increases by at most a O⁢(1)𝑂1O(1)italic_O ( 1 ) factor within each phase. Further, the analysis in [FLNS21] crucially relies on showing that we incur O~⁢(n)~𝑂𝑛\tilde{O}(n)over~ start_ARG italic_O end_ARG ( italic_n ) total recourse within a phase. This, combined with the fact that there are O~⁢(1)~𝑂1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) phases overall in the incremental setting, implies the amortized recourse guarantee. From the preceding discussion, it becomes apparent that we cannot hope to extend such an argument in the fully dynamic setting, because it is not possible to argue that we have at most O~⁢(1)~𝑂1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) phases when the value of OPTk⁢(𝒫)subscriptOPT𝑘𝒫\textnormal{{OPT}}_{k}(\mathcal{P})OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ) can fluctuate in either direction (go up or down) over a sequence of fully dynamic updates. To circumvent this obstacle, we make the following subtle but important change to the framework of [FLNS21]. Recall Equation 3. Note that we can find the set 𝒲⋆superscript𝒲⋆\mathcal{W}^{\star}caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT by solving the following computational task:666For now, we ignore any consideration about keeping the update time of our algorithm low, or even polynomial. Compute the set 𝒲⋆superscript𝒲⋆\mathcal{W}^{\star}caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT of k𝑘kitalic_k centers, which minimizes the k𝑘kitalic_k-median objective w.r.t. 𝒫finalsubscript𝒫final\mathcal{P}_{\textnormal{{final}}}caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT, subject to the constraint that it can be obtained by adding/removing Θ⁢(ℓ+1)Θℓ1\Theta(\ell+1)roman_Θ ( roman_ℓ + 1 ) points in 𝒰initsubscript𝒰init\mathcal{U}_{\textnormal{{init}}}caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT. Informally, in our algorithm, we replace this task by three separate (and new) tasks, which we perform one after another (see Step 4 in Section 5.1). • Task (i). Find a set of k+Θ⁢(ℓ+1)𝑘Θℓ1k+\Theta(\ell+1)italic_k + roman_Θ ( roman_ℓ + 1 ) centers 𝒰⋆superscript𝒰⋆\mathcal{U}^{\star}caligraphic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT, which minimizes the (k+ℓ+1)𝑘ℓ1(k+\ell+1)( italic_k + roman_ℓ + 1 )-median objective w.r.t. 𝒫initsubscript𝒫init\mathcal{P}_{\textnormal{{init}}}caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT, such that 𝒰⋆superscript𝒰⋆\mathcal{U}^{\star}caligraphic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT is obtained by adding Θ⁢(ℓ+1)Θℓ1\Theta(\ell+1)roman_Θ ( roman_ℓ + 1 ) many centers to 𝒰initsubscript𝒰init\mathcal{U}_{\textnormal{{init}}}caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT. • Task (ii). Set 𝒱⋆←𝒰⋆∪(𝒫final∖𝒫init)←superscript𝒱⋆superscript𝒰⋆subscript𝒫finalsubscript𝒫init\mathcal{V}^{\star}\leftarrow\mathcal{U}^{\star}\cup(\mathcal{P}_{\textnormal{% {final}}}\setminus\mathcal{P}_{\textnormal{{init}}})caligraphic_V start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ← caligraphic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ∪ ( caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT ∖ caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ). • Task (iii). Find a set of k𝑘kitalic_k centers 𝒲⋆superscript𝒲⋆\mathcal{W}^{\star}caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT, which minimizes the k𝑘kitalic_k-median objective w.r.t. 𝒫finalsubscript𝒫final\mathcal{P}_{\textnormal{{final}}}caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT, such that 𝒲⋆superscript𝒲⋆\mathcal{W}^{\star}caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT is obtained by removing Θ⁢(ℓ+1)Θℓ1\Theta(\ell+1)roman_Θ ( roman_ℓ + 1 ) many centers from 𝒱⋆superscript𝒱⋆\mathcal{V}^{\star}caligraphic_V start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT. Thus, while adding centers in Task (i), we optimize the (k+ℓ+1)𝑘ℓ1(k+\ell+1)( italic_k + roman_ℓ + 1 )-median objective w.r.t. 𝒫initsubscript𝒫init\mathcal{P}_{\textnormal{{init}}}caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT. On the other hand, while removing centers in Task (iii), we optimize the k𝑘kitalic_k-median objective w.r.t. 𝒫finalsubscript𝒫final\mathcal{P}_{\textnormal{{final}}}caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT. This is in sharp contrast to the approach in [FLNS21], where we need to optimize the k𝑘kitalic_k-median objective w.r.t. 𝒫finalsubscript𝒫final\mathcal{P}_{\textnormal{{final}}}caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT, both while adding centers and while removing centers. Strikingly, we show that this modification allows us to get rid of the concept of phases altogether. In particular, our algorithm can be cast in the classical periodic recomputation framework: We work in epochs. Within an epoch we handle the updates lazily, and at the end of the epoch we reinitialize our maintained solution so that we get ready to handle the next epoch. See Section 5.3 for details. 2.3.3 Challenge III: Achieving Fast Update Time As mentioned previously, the update time of the algorithm in [FLNS21] is prohibitively large. This occurs because of the following computationally expensive steps at the start and at the end of an epoch.777Note that it is straightforward to lazily handle the updates within the epoch. (1) At the start of an epoch, [FLNS21] computes the value of ℓℓ\ellroman_ℓ, by solving an LP for the (k−s)𝑘𝑠(k-s)( italic_k - italic_s )-median problem with potential centers 𝒰initsubscript𝒰init\mathcal{U}_{\textnormal{{init}}}caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT, for each s∈[0,k−1]𝑠0𝑘1s\in[0,k-1]italic_s ∈ [ 0 , italic_k - 1 ]. (2) Next, to initialize the subset 𝒰⊆𝒰init𝒰subscript𝒰init\mathcal{U}\subseteq\mathcal{U}_{\textnormal{{init}}}caligraphic_U ⊆ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT of (k−ℓ)𝑘ℓ(k-\ell)( italic_k - roman_ℓ ) centers at the start of the epoch, [FLNS21] again invokes an algorithm for the (k−ℓ)𝑘ℓ(k-\ell)( italic_k - roman_ℓ )-median problem from scratch. (3) At the end of the epoch, [FLNS21] solves another LP and applies a rounding procedure, to get an approximation of the desired set 𝒲⋆superscript𝒲⋆\mathcal{W}^{\star}caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT (see Equation 3). (4) Finally, at the end of the epoch, the call to the Robustify subroutine also takes a prohibitively long time for our purpose. In contrast, we take alternative approaches while performing the above steps. At the start of an epoch, we implement Steps (1) and (2) via randomized local search (see Section 2.1.2). For Step (3), we compute (an approximation) of the set 𝒲⋆superscript𝒲⋆\mathcal{W}^{\star}caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT by solving the three tasks outlined in Section 2.3.2. One of our contributions is to design a new algorithm for Task (i) that runs in O~⁢(n⋅(ℓ+1))~𝑂⋅𝑛ℓ1\tilde{O}(n\cdot(\ell+1))over~ start_ARG italic_O end_ARG ( italic_n ⋅ ( roman_ℓ + 1 ) ) time, assuming it has access to some auxiliary data structures (see Lemma 6.2). It is trivial to perform Task (ii). For Task (iii), we again invoke the randomized local search procedure (see Section 2.1.2). Finally, for step (4), we need to efficiently implement the calls to Robustify⁢(⋅)Robustify⋅\textsc{Robustify}(\cdot)Robustify ( ⋅ ). See Section 6.2.1 for a more detailed discussion on this challenge, and how we overcome it (we defer the discussion to Section 6.2.1 because it requires an understanding of the inner workings of the Robustify⁢(⋅)Robustify⋅\textsc{Robustify}(\cdot)Robustify ( ⋅ ) subroutine, which we have not described until now). 3 Preliminaries We now define some basic notations, and recall some relevant results from the existing literature. For the sake of completeness, we provide self-contained proofs for most of the lemmas stated in this section, but defer those proofs (since we do not take any credit for them) to Section 8. Consider a set of points 𝐏𝐏\mathbf{P}bold_P and a distance function d:𝐏×𝐏→ℝ+:𝑑→𝐏𝐏superscriptℝd:\mathbf{P}\times\mathbf{P}\rightarrow\mathbb{R}^{+}italic_d : bold_P × bold_P → blackboard_R start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT that together form a metric space. The input to our dynamic algorithm is a subset 𝒫⊆𝐏𝒫𝐏\mathcal{P}\subseteq\mathbf{P}caligraphic_P ⊆ bold_P, which changes by means of updates. Let n𝑛nitalic_n be an upper bound on the maximum size of 𝒫𝒫\mathcal{P}caligraphic_P throughout these updates. Each update either inserts a point p∈𝐏∖𝒫𝑝𝐏𝒫p\in\mathbf{P}\setminus\mathcal{P}italic_p ∈ bold_P ∖ caligraphic_P into 𝒫𝒫\mathcal{P}caligraphic_P, or deletes a point p∈𝒫𝑝𝒫p\in\mathcal{P}italic_p ∈ caligraphic_P from 𝒫𝒫\mathcal{P}caligraphic_P. At all times, we have to maintain a set 𝒰⊆𝐏𝒰𝐏\mathcal{U}\subseteq\mathbf{P}caligraphic_U ⊆ bold_P of at most k𝑘kitalic_k “centers”, so as to minimize the objective function Cost⁢(𝒰,𝒫):=∑p∈𝒫d⁢(p,𝒰), where ⁢d⁢(p,𝒰):=minq∈𝒰⁡d⁢(p,q)⁢ is the distance from ⁢p⁢ to the set ⁢𝒰.formulae-sequenceassignCost𝒰𝒫subscript𝑝𝒫𝑑𝑝𝒰assign where 𝑑𝑝𝒰subscript𝑞𝒰𝑑𝑝𝑞 is the distance from 𝑝 to the set 𝒰\textnormal{{Cost}}\left(\mathcal{U},\mathcal{P}\right):=\sum_{p\in\mathcal{P}% }d(p,\mathcal{U}),\text{ where }d(p,\mathcal{U}):=\min_{q\in\mathcal{U}}d(p,q)% \text{ is the distance from }p\text{ to the set }\mathcal{U}.Cost ( caligraphic_U , caligraphic_P ) := ∑ start_POSTSUBSCRIPT italic_p ∈ caligraphic_P end_POSTSUBSCRIPT italic_d ( italic_p , caligraphic_U ) , where italic_d ( italic_p , caligraphic_U ) := roman_min start_POSTSUBSCRIPT italic_q ∈ caligraphic_U end_POSTSUBSCRIPT italic_d ( italic_p , italic_q ) is the distance from italic_p to the set caligraphic_U . We will refer to this as the dynamic improper k𝑘kitalic_k-median problem. Our goal is to design an algorithm for this problem that has: (1) good approximation ratio, (2) small update time, which is the time it takes to process an update in 𝒫𝒫\mathcal{P}caligraphic_P, and (3) small recourse, which is the number of changes (point insertions/deletions) in the maintained solution 𝒰𝒰\mathcal{U}caligraphic_U per update. What makes this setting distinct from the standard k𝑘kitalic_k-median problem is this: Here, we are allowed to open centers at locations that are not part of the current input, i.e., we can have 𝒰∩(𝐏∖𝒫)≠∅𝒰𝐏𝒫\mathcal{U}\cap(\mathbf{P}\setminus\mathcal{P})\neq\emptysetcaligraphic_U ∩ ( bold_P ∖ caligraphic_P ) ≠ ∅. Nevertheless, in a black-box manner we can convert any dynamic algorithm for improper k𝑘kitalic_k-median into a dynamic algorithm for k𝑘kitalic_k-median, with essentially the same guarantees (see Lemma 3.1). Accordingly, for the rest of this paper, we focus on designing a dynamic algorithm for improper k𝑘kitalic_k-median. Lemma 3.1 ([BCG+24]). Given an α𝛼\alphaitalic_α-approximation algorithm for dynamic improper k𝑘kitalic_k-median, we can get a 2⁢α2𝛼2\alpha2 italic_α-approximation algorithm for dynamic k𝑘kitalic_k-median, with an extra O⁢(1)𝑂1O(1)italic_O ( 1 ) multiplicative factor overhead in the recourse, and an extra O~⁢(n)~𝑂𝑛\tilde{O}(n)over~ start_ARG italic_O end_ARG ( italic_n ) additive factor overhead in the update time. Remark. At this point, the reader might get alarmed by the fact that Lemma 3.1 incurs an additive overhead of O~⁢(n)~𝑂𝑛\tilde{O}(n)over~ start_ARG italic_O end_ARG ( italic_n ) update time. To assuage this concern, in Section 6.3, we explain how to bring down the update time from O~⁢(n)~𝑂𝑛\tilde{O}(n)over~ start_ARG italic_O end_ARG ( italic_n ) to O~⁢(k)~𝑂𝑘\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ), using standard sparsification techniques. 3.1 Basic Notations By a simple scaling, we can assume that all of the distances in the metric space lie in the range [1,Δ]1Δ[1,\Delta][ 1 , roman_Δ ], where ΔΔ\Deltaroman_Δ is the aspect ratio. Throughout the paper, we use the symbol 𝐏𝐏\mathbf{P}bold_P to denote the underlying metric space with distance function d:𝐏×𝐏→ℝ≥0:𝑑→𝐏𝐏superscriptℝabsent0d:\mathbf{P}\times\mathbf{P}\rightarrow\mathbb{R}^{\geq 0}italic_d : bold_P × bold_P → blackboard_R start_POSTSUPERSCRIPT ≥ 0 end_POSTSUPERSCRIPT, and 𝒫⊆𝐏𝒫𝐏\mathcal{P}\subseteq\mathbf{P}caligraphic_P ⊆ bold_P to denote the current input. For simplicity, for each set S𝑆Sitalic_S and element p𝑝pitalic_p, we denote S∪{p}𝑆𝑝S\cup\{p\}italic_S ∪ { italic_p } and S∖{p}𝑆𝑝S\setminus\{p\}italic_S ∖ { italic_p } by S+p𝑆𝑝S+pitalic_S + italic_p and S−p𝑆𝑝S-pitalic_S - italic_p respectively. For two sets of points S𝑆Sitalic_S and S′superscript𝑆′S^{\prime}italic_S start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT, we use S⊕S′direct-sum𝑆superscript𝑆′S\oplus S^{\prime}italic_S ⊕ italic_S start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT to denote their symmetric difference. For each S⊆𝐏𝑆𝐏S\subseteq\mathbf{P}italic_S ⊆ bold_P, we define πS:𝐏→S:subscript𝜋𝑆→𝐏𝑆\pi_{S}:\mathbf{P}\rightarrow Sitalic_π start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT : bold_P → italic_S to be the projection function onto S𝑆Sitalic_S, i.e., πS⁢(x):=arg⁡mins∈S⁡d⁢(x,s)assignsubscript𝜋𝑆𝑥subscript𝑠𝑆𝑑𝑥𝑠\pi_{S}(x):=\arg\min_{s\in S}d(x,s)italic_π start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_x ) := roman_arg roman_min start_POSTSUBSCRIPT italic_s ∈ italic_S end_POSTSUBSCRIPT italic_d ( italic_x , italic_s ), breaking the ties arbitrarily. For each 𝒰⊆𝐏𝒰𝐏\mathcal{U}\subseteq\mathbf{P}caligraphic_U ⊆ bold_P and S⊆𝒫𝑆𝒫S\subseteq\mathcal{P}italic_S ⊆ caligraphic_P, we also define AverageCost⁢(𝒰,S):=Cost⁢(𝒰,S)|S|=∑p∈Sd⁢(p,𝒰)|S|=∑p∈Sminq∈𝒰⁡d⁢(p,q)|S|.assignAverageCost𝒰𝑆Cost𝒰𝑆𝑆subscript𝑝𝑆𝑑𝑝𝒰𝑆subscript𝑝𝑆subscript𝑞𝒰𝑑𝑝𝑞𝑆\textnormal{{AverageCost}}\left(\mathcal{U},S\right):=\frac{\textnormal{{Cost}% }\left(\mathcal{U},S\right)}{|S|}=\frac{\sum_{p\in S}d(p,\mathcal{U})}{|S|}=% \frac{\sum_{p\in S}\min_{q\in\mathcal{U}}d(p,q)}{|S|}.AverageCost ( caligraphic_U , italic_S ) := divide start_ARG Cost ( caligraphic_U , italic_S ) end_ARG start_ARG | italic_S | end_ARG = divide start_ARG ∑ start_POSTSUBSCRIPT italic_p ∈ italic_S end_POSTSUBSCRIPT italic_d ( italic_p , caligraphic_U ) end_ARG start_ARG | italic_S | end_ARG = divide start_ARG ∑ start_POSTSUBSCRIPT italic_p ∈ italic_S end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT italic_q ∈ caligraphic_U end_POSTSUBSCRIPT italic_d ( italic_p , italic_q ) end_ARG start_ARG | italic_S | end_ARG . Consider any subset of points 𝒞⊆𝐏𝒞𝐏\mathcal{C}\subseteq\mathbf{P}caligraphic_C ⊆ bold_P. For every k≥1𝑘1k\geq 1italic_k ≥ 1, we let OPTk𝒞⁢(𝒫)superscriptsubscriptOPT𝑘𝒞𝒫\textnormal{{OPT}}_{k}^{\mathcal{C}}(\mathcal{P})OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT caligraphic_C end_POSTSUPERSCRIPT ( caligraphic_P ) denote the cost of the optimum k𝑘kitalic_k-median solution for 𝒫⊆𝐏𝒫𝐏\mathcal{P}\subseteq\mathbf{P}caligraphic_P ⊆ bold_P, where we can only open centers from 𝒞𝒞\mathcal{C}caligraphic_C. Thus, we have OPTk𝒞⁢(𝒫)=min𝒰⊆𝒞,|𝒰|≤k⁡Cost⁢(𝒰,𝒫).subscriptsuperscriptOPT𝒞𝑘𝒫subscriptformulae-sequence𝒰𝒞𝒰𝑘Cost𝒰𝒫\textnormal{{OPT}}^{\mathcal{C}}_{k}(\mathcal{P})=\min\limits_{\begin{subarray% }{c}\mathcal{U}\subseteq\mathcal{C},|\mathcal{U}|\leq k\end{subarray}}% \textnormal{{Cost}}\left(\mathcal{U},\mathcal{P}\right).OPT start_POSTSUPERSCRIPT caligraphic_C end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ) = roman_min start_POSTSUBSCRIPT start_ARG start_ROW start_CELL caligraphic_U ⊆ caligraphic_C , | caligraphic_U | ≤ italic_k end_CELL end_ROW end_ARG end_POSTSUBSCRIPT Cost ( caligraphic_U , caligraphic_P ) . When 𝒞=𝐏𝒞𝐏\mathcal{C}=\mathbf{P}caligraphic_C = bold_P, we slightly abuse the notation and write OPTk⁢(𝒫)subscriptOPT𝑘𝒫\textnormal{{OPT}}_{k}(\mathcal{P})OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ) instead of OPTk𝒞⁢(𝒫)superscriptsubscriptOPT𝑘𝒞𝒫\textnormal{{OPT}}_{k}^{\mathcal{C}}(\mathcal{P})OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT caligraphic_C end_POSTSUPERSCRIPT ( caligraphic_P ). Next, for each 𝒰⊆𝐏𝒰𝐏\mathcal{U}\subseteq\mathbf{P}caligraphic_U ⊆ bold_P and u∈𝒰𝑢𝒰u\in\mathcal{U}italic_u ∈ caligraphic_U, we define Cu⁢(𝒰,𝒫):={p∈𝒫∣π𝒰⁢(p)=u}assignsubscript𝐶𝑢𝒰𝒫conditional-set𝑝𝒫subscript𝜋𝒰𝑝𝑢C_{u}(\mathcal{U},\mathcal{P}):=\{p\in\mathcal{P}\mid\pi_{\mathcal{U}}(p)=u\}italic_C start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT ( caligraphic_U , caligraphic_P ) := { italic_p ∈ caligraphic_P ∣ italic_π start_POSTSUBSCRIPT caligraphic_U end_POSTSUBSCRIPT ( italic_p ) = italic_u } to be the set of points in 𝒫𝒫\mathcal{P}caligraphic_P that are “assigned to” the center u𝑢uitalic_u in the solution 𝒰𝒰\mathcal{U}caligraphic_U (breaking ties arbitrarily). For each point p∈𝐏𝑝𝐏p\in\mathbf{P}italic_p ∈ bold_P and value r≥0𝑟0r\geq 0italic_r ≥ 0, let Ballr𝒫⁢(p):={q∈𝒫∣d⁢(p,q)≤r}assignsubscriptsuperscriptBall𝒫𝑟𝑝conditional-set𝑞𝒫𝑑𝑝𝑞𝑟\text{Ball}^{\mathcal{P}}_{r}(p):=\{q\in\mathcal{P}\mid d(p,q)\leq r\}Ball start_POSTSUPERSCRIPT caligraphic_P end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ( italic_p ) := { italic_q ∈ caligraphic_P ∣ italic_d ( italic_p , italic_q ) ≤ italic_r } denote the ball of radius r𝑟ritalic_r around p𝑝pitalic_p. Note that if p∈𝐏∖𝒫𝑝𝐏𝒫p\in\mathbf{P}\setminus\mathcal{P}italic_p ∈ bold_P ∖ caligraphic_P, then p𝑝pitalic_p itself is not part of the ball Ballr𝒫⁢(p)subscriptsuperscriptBall𝒫𝑟𝑝\text{Ball}^{\mathcal{P}}_{r}(p)Ball start_POSTSUPERSCRIPT caligraphic_P end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ( italic_p ). Finally, throughout the paper we use a sufficiently large constant parameter γ=4000𝛾4000\gamma=4000italic_γ = 4000. 3.2 Robust Centers We will use the notion of robust centers [FLNS21]. Morally, a point p∈𝐏𝑝𝐏p\in\mathbf{P}italic_p ∈ bold_P is t𝑡titalic_t-robust for an integer t≥1𝑡1t\geq 1italic_t ≥ 1 iff it satisfies the following condition for all i∈[1,t]𝑖1𝑡i\in[1,t]italic_i ∈ [ 1 , italic_t ]: Let Bi=Ball10i𝒫⁢(p)subscript𝐵𝑖superscriptsubscriptBallsuperscript10𝑖𝒫𝑝B_{i}=\text{Ball}_{10^{i}}^{\mathcal{P}}(p)italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = Ball start_POSTSUBSCRIPT 10 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT caligraphic_P end_POSTSUPERSCRIPT ( italic_p ), and consider any point q∈𝒫𝑞𝒫q\in\mathcal{P}italic_q ∈ caligraphic_P with d⁢(p,q)≪10imuch-less-than𝑑𝑝𝑞superscript10𝑖d(p,q)\ll 10^{i}italic_d ( italic_p , italic_q ) ≪ 10 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT, i.e., q𝑞qitalic_q is sufficiently close to p𝑝pitalic_p compared to the radius of Bisubscript𝐵𝑖B_{i}italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. Then Cost⁢(p,S)≤O⁢(1)⋅Cost⁢(q,S)Cost𝑝𝑆⋅𝑂1Cost𝑞𝑆\textnormal{{Cost}}\left(p,S\right)\leq O(1)\cdot\textnormal{{Cost}}\left(q,S\right)Cost ( italic_p , italic_S ) ≤ italic_O ( 1 ) ⋅ Cost ( italic_q , italic_S ) for all Bi⊆S⊆𝒫subscript𝐵𝑖𝑆𝒫B_{i}\subseteq S\subseteq\mathcal{P}italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ⊆ italic_S ⊆ caligraphic_P. In words, the point p𝑝pitalic_p is a good approximate 1111-median solution, compared to any other nearby point, at every “distance scale” up to 10tsuperscript10𝑡10^{t}10 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT. The above definition, however, is too strong, in the sense that there might not exist any t𝑡titalic_t-robust point under this definition.888For instance, it might be the case that there are (say) λ𝜆\lambdaitalic_λ many points in Bisubscript𝐵𝑖B_{i}italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, and all of them (except p𝑝pitalic_p) are in the exact same location as q𝑞qitalic_q, i.e., d⁢(p′,q)=0𝑑superscript𝑝′𝑞0d(p^{\prime},q)=0italic_d ( italic_p start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , italic_q ) = 0 for all p′∈Bi−psuperscript𝑝′subscript𝐵𝑖𝑝p^{\prime}\in B_{i}-pitalic_p start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ∈ italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_p. Then, the condition Cost⁢(p,Bi)≤O⁢(1)⋅Cost⁢(q,Bi)Cost𝑝subscript𝐵𝑖⋅𝑂1Cost𝑞subscript𝐵𝑖\textnormal{{Cost}}\left(p,B_{i}\right)\leq O(1)\cdot\textnormal{{Cost}}\left(% q,B_{i}\right)Cost ( italic_p , italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ≤ italic_O ( 1 ) ⋅ Cost ( italic_q , italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) clearly does not hold, since the LHS is (λ−1)⋅d⁢(p,q)⋅𝜆1𝑑𝑝𝑞(\lambda-1)\cdot d(p,q)( italic_λ - 1 ) ⋅ italic_d ( italic_p , italic_q ), whereas the RHS is only d⁢(p,q)𝑑𝑝𝑞d(p,q)italic_d ( italic_p , italic_q ). Instead, the actual definition that we will use is stated below (see Definition 3.2), along with the relevant properties that follow from it (see Lemma 3.3 and Lemma 3.4). Conceptually, here the key difference from the idealized definition is that the balls {Bi}isubscriptsubscript𝐵𝑖𝑖\{B_{i}\}_{i}{ italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT are centered around different points {pi}isubscriptsubscript𝑝𝑖𝑖\{p_{i}\}_{i}{ italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, with p0=psubscript𝑝0𝑝p_{0}=pitalic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = italic_p and d⁢(p,pi)≪10imuch-less-than𝑑𝑝subscript𝑝𝑖superscript10𝑖d(p,p_{i})\ll 10^{i}italic_d ( italic_p , italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ≪ 10 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT for all i∈[1,t]𝑖1𝑡i\in[1,t]italic_i ∈ [ 1 , italic_t ]. Definition 3.2. Let (p0,p1,…,pt)subscript𝑝0subscript𝑝1…subscript𝑝𝑡(p_{0},p_{1},\ldots,p_{t})( italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) be a sequence of t+1𝑡1t+1italic_t + 1 points in 𝐏𝐏\mathbf{P}bold_P, and let Bi=Ball10i𝒫⁢(pi)subscript𝐵𝑖superscriptsubscriptBallsuperscript10𝑖𝒫subscript𝑝𝑖B_{i}=\text{Ball}_{10^{i}}^{\mathcal{P}}(p_{i})italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = Ball start_POSTSUBSCRIPT 10 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT caligraphic_P end_POSTSUPERSCRIPT ( italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) for each i∈[0,t]𝑖0𝑡i\in[0,t]italic_i ∈ [ 0 , italic_t ]. We refer to (p0,p1,…,pt)subscript𝑝0subscript𝑝1…subscript𝑝𝑡(p_{0},p_{1},\ldots,p_{t})( italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) as a t𝑡titalic_t-robust sequence iff for every i∈[1,t]𝑖1𝑡i\in[1,t]italic_i ∈ [ 1 , italic_t ]: pi−1={piif⁢AverageCost⁢(pi,Bi)≥10i/5;qiotherwise, where ⁢qi=arg⁡minq∈Bi+pi⁡Cost⁢(q,Bi).subscript𝑝𝑖1casessubscript𝑝𝑖ifAverageCostsubscript𝑝𝑖subscript𝐵𝑖superscript10𝑖5subscript𝑞𝑖otherwise, where subscript𝑞𝑖subscript𝑞subscript𝐵𝑖subscript𝑝𝑖Cost𝑞subscript𝐵𝑖\displaystyle p_{i-1}=\begin{cases}p_{i}\quad&\text{if}\ \textnormal{{% AverageCost}}\left(p_{i},B_{i}\right)\geq 10^{i}/5;\\ q_{i}\quad&\text{otherwise, where }q_{i}=\arg\min\limits_{q\in B_{i}+p_{i}}% \textnormal{{Cost}}\left(q,B_{i}\right).\end{cases}italic_p start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT = { start_ROW start_CELL italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_CELL start_CELL if AverageCost ( italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ≥ 10 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT / 5 ; end_CELL end_ROW start_ROW start_CELL italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_CELL start_CELL otherwise, where italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = roman_arg roman_min start_POSTSUBSCRIPT italic_q ∈ italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT Cost ( italic_q , italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) . end_CELL end_ROW We say that a point p∈𝐏𝑝𝐏p\in\mathbf{P}italic_p ∈ bold_P is t𝑡titalic_t-robust iff there exists a t𝑡titalic_t-robust sequence (p0,p1,…,pt)subscript𝑝0subscript𝑝1…subscript𝑝𝑡(p_{0},p_{1},\ldots,p_{t})( italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) with p0=psubscript𝑝0𝑝p_{0}=pitalic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = italic_p. Lemma 3.3 ([FLNS21]). Let (p0,p1,…,pt)subscript𝑝0subscript𝑝1…subscript𝑝𝑡(p_{0},p_{1},\ldots,p_{t})( italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) be a t𝑡titalic_t-robust sequence and let Bi=Ball10i𝒫⁢(pi)subscript𝐵𝑖subscriptsuperscriptBall𝒫superscript10𝑖subscript𝑝𝑖B_{i}=\text{Ball}^{\mathcal{P}}_{10^{i}}(p_{i})italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = Ball start_POSTSUPERSCRIPT caligraphic_P end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 10 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) for all i∈[0,t]𝑖0𝑡i\in[0,t]italic_i ∈ [ 0 , italic_t ]. Then, for all i∈[1,t]𝑖1𝑡i\in[1,t]italic_i ∈ [ 1 , italic_t ], we have d⁢(pi−1,pi)≤10i/2𝑑subscript𝑝𝑖1subscript𝑝𝑖superscript10𝑖2d(p_{i-1},p_{i})\leq 10^{i}/2italic_d ( italic_p start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ≤ 10 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT / 2, Bi−1⊆Bisubscript𝐵𝑖1subscript𝐵𝑖B_{i-1}\subseteq B_{i}italic_B start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ⊆ italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and d⁢(p0,pi)≤10i/2𝑑subscript𝑝0subscript𝑝𝑖superscript10𝑖2d(p_{0},p_{i})\leq 10^{i}/2italic_d ( italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ≤ 10 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT / 2. Lemma 3.4 ([FLNS21]). Let (p0,p1,…,pt)subscript𝑝0subscript𝑝1…subscript𝑝𝑡(p_{0},p_{1},\ldots,p_{t})( italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) be a t𝑡titalic_t-robust sequence and let Bi=Ball10i𝒫⁢(pi)subscript𝐵𝑖subscriptsuperscriptBall𝒫superscript10𝑖subscript𝑝𝑖B_{i}=\text{Ball}^{\mathcal{P}}_{10^{i}}(p_{i})italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = Ball start_POSTSUPERSCRIPT caligraphic_P end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 10 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) for all i∈[0,t]𝑖0𝑡i\in[0,t]italic_i ∈ [ 0 , italic_t ]. Then, for every i∈[0,t]𝑖0𝑡i\in[0,t]italic_i ∈ [ 0 , italic_t ] and every Bi⊆S⊆𝒫subscript𝐵𝑖𝑆𝒫B_{i}\subseteq S\subseteq\mathcal{P}italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ⊆ italic_S ⊆ caligraphic_P, we have Cost⁢(p0,S)≤(3/2)⋅Cost⁢(pi,S)Costsubscript𝑝0𝑆⋅32Costsubscript𝑝𝑖𝑆\textnormal{{Cost}}\left(p_{0},S\right)\leq(3/2)\cdot\textnormal{{Cost}}\left(% p_{i},S\right)Cost ( italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_S ) ≤ ( 3 / 2 ) ⋅ Cost ( italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_S ). We next define the concept of a robust collection of centers. Definition 3.5. A set of centers 𝒲⊆𝐏𝒲𝐏\mathcal{W}\subseteq\mathbf{P}caligraphic_W ⊆ bold_P is robust iff the following holds for every w∈𝒲𝑤𝒲w\in\mathcal{W}italic_w ∈ caligraphic_W: w⁢is⁢t⁢-robust, where⁢t⁢is the smallest integer satisfying⁢ 10t≥d⁢(w,𝒲−w)/200.𝑤is𝑡-robust, where𝑡is the smallest integer satisfyingsuperscript10𝑡𝑑𝑤𝒲𝑤200w\ \text{is}\ t\text{-robust, where}\ t\ \text{is the smallest integer % satisfying}\ 10^{t}\geq d(w,\mathcal{W}-w)/200.italic_w is italic_t -robust, where italic_t is the smallest integer satisfying 10 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ≥ italic_d ( italic_w , caligraphic_W - italic_w ) / 200 . (4) Suppose that we have a set of centers 𝒲⊆𝐏𝒲𝐏\mathcal{W}\subseteq\mathbf{P}caligraphic_W ⊆ bold_P that is not robust. A natural way to convert them into a robust set of centers is to call the subroutine Robustify(𝒲)𝒲(\mathcal{W})( caligraphic_W ), as described below. 1 while there exist a w∈𝒲𝑤𝒲w\in\mathcal{W}italic_w ∈ caligraphic_W violating (4) do 2 t←←𝑡absentt\leftarrowitalic_t ← Smallest integer satisfying 10t≥d⁢(w,𝒲−w)/100superscript10𝑡𝑑𝑤𝒲𝑤10010^{t}\geq d(w,\mathcal{W}-w)/10010 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ≥ italic_d ( italic_w , caligraphic_W - italic_w ) / 100. 3 w0←←subscript𝑤0absentw_{0}\leftarrowitalic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ← Make-Robust(w,t)𝑤𝑡(w,t)( italic_w , italic_t ). 4 𝒲←𝒲−w+w0←𝒲𝒲𝑤subscript𝑤0\mathcal{W}\leftarrow\mathcal{W}-w+w_{0}caligraphic_W ← caligraphic_W - italic_w + italic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. Algorithm 1 Robustify(𝒲)𝒲(\mathcal{W})( caligraphic_W ) 1 pt←p←subscript𝑝𝑡𝑝p_{t}\leftarrow pitalic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ← italic_p. 2for i=t𝑖𝑡i=titalic_i = italic_t down to 1111 do 3 Bi←Ball10i𝒫⁢(pi)←subscript𝐵𝑖superscriptsubscriptBallsuperscript10𝑖𝒫subscript𝑝𝑖B_{i}\leftarrow\text{Ball}_{10^{i}}^{\mathcal{P}}(p_{i})italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ← Ball start_POSTSUBSCRIPT 10 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT caligraphic_P end_POSTSUPERSCRIPT ( italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ). 4 if AverageCost⁢(pi,Bi)≥10i/5AverageCostsubscript𝑝𝑖subscript𝐵𝑖superscript10𝑖5\textnormal{{AverageCost}}\left(p_{i},B_{i}\right)\geq 10^{i}/5AverageCost ( italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ≥ 10 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT / 5 then 5 pi−1←pi←subscript𝑝𝑖1subscript𝑝𝑖p_{i-1}\leftarrow p_{i}italic_p start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ← italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. 6 else 7 pi−1←arg⁡minq∈Bi+pi⁡Cost⁢(q,Bi)←subscript𝑝𝑖1subscript𝑞subscript𝐵𝑖subscript𝑝𝑖Cost𝑞subscript𝐵𝑖p_{i-1}\leftarrow\arg\min\limits_{q\in B_{i}+p_{i}}\textnormal{{Cost}}\left(q,% B_{i}\right)italic_p start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ← roman_arg roman_min start_POSTSUBSCRIPT italic_q ∈ italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT Cost ( italic_q , italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ). return p0subscript𝑝0p_{0}italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. Algorithm 2 Make-Robust(p,t)𝑝𝑡(p,t)( italic_p , italic_t ) During a call to Make-Robust(p,t)𝑝𝑡(p,t)( italic_p , italic_t ), we simply apply the rule from Definition 3.2 to obtain a t𝑡titalic_t-robust sequence (p0,p1,…,pt)subscript𝑝0subscript𝑝1…subscript𝑝𝑡(p_{0},p_{1},\dots,p_{t})( italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) with pt=psubscript𝑝𝑡𝑝p_{t}=pitalic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_p, and then return the point p0subscript𝑝0p_{0}italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. Further, Line 1 of the subroutine Robustify(𝒲)𝒲(\mathcal{W})( caligraphic_W ) considers the inequality 10t≥d⁢(w,𝒲−w)/100superscript10𝑡𝑑𝑤𝒲𝑤10010^{t}\geq d(w,\mathcal{W}-w)/10010 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ≥ italic_d ( italic_w , caligraphic_W - italic_w ) / 100, whereas (4) refers to the inequality 10t≥d⁢(w,𝒲−w)/200superscript10𝑡𝑑𝑤𝒲𝑤20010^{t}\geq d(w,\mathcal{W}-w)/20010 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ≥ italic_d ( italic_w , caligraphic_W - italic_w ) / 200. This discrepancy in the constants on the right hand side (100100100100 vs 200200200200) of these two inequalities is intentional, and plays a crucial role in deriving Lemma 3.6. Lemma 3.6 ([FLNS21]). Consider any call to Robustify(𝒲)𝒲(\mathcal{W})( caligraphic_W ), and suppose that it sets w0←Make-Robust⁢(w,t)←subscript𝑤0Make-Robust𝑤𝑡w_{0}\leftarrow\textsc{Make-Robust}(w,t)italic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ← Make-Robust ( italic_w , italic_t ) during some iteration of the while loop. Then in subsequent iterations of the while loop in the same call to Robustify(𝒲)𝒲(\mathcal{W})( caligraphic_W ), we will not make any call to Make-Robust(w0,⋅)subscript𝑤0⋅(w_{0},\cdot)( italic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , ⋅ ). Lemma 3.7 ([FLNS21]). If 𝒰𝒰\mathcal{U}caligraphic_U is the output of Robustify(𝒲)𝒲(\mathcal{W})( caligraphic_W ), then Cost⁢(𝒰,𝒫)≤32⋅Cost⁢(𝒲,𝒫)Cost𝒰𝒫⋅32Cost𝒲𝒫\textnormal{{Cost}}\left(\mathcal{U},\mathcal{P}\right)\leq\frac{3}{2}\cdot% \textnormal{{Cost}}\left(\mathcal{W},\mathcal{P}\right)Cost ( caligraphic_U , caligraphic_P ) ≤ divide start_ARG 3 end_ARG start_ARG 2 end_ARG ⋅ Cost ( caligraphic_W , caligraphic_P ). 3.3 Well-Separated Pairs We will also use the notion of a well-separated pair of points [FLNS21], defined as follows. Definition 3.8. Consider any 𝒰,𝒱⊆𝐏𝒰𝒱𝐏\mathcal{U},\mathcal{V}\subseteq\mathbf{P}caligraphic_U , caligraphic_V ⊆ bold_P. A pair (u,v)∈𝒰×𝒱𝑢𝑣𝒰𝒱(u,v)\in\mathcal{U}\times\mathcal{V}( italic_u , italic_v ) ∈ caligraphic_U × caligraphic_V is well-separated w.r.t. (𝒰,𝒱)𝒰𝒱(\mathcal{U},\mathcal{V})( caligraphic_U , caligraphic_V ) iff d⁢(u,𝒰−u)≥γ⋅d⁢(u,v)⁢ and ⁢d⁢(v,𝒱−v)≥γ⋅d⁢(u,v).𝑑𝑢𝒰𝑢⋅𝛾𝑑𝑢𝑣 and 𝑑𝑣𝒱𝑣⋅𝛾𝑑𝑢𝑣d(u,\mathcal{U}-u)\geq\gamma\cdot d(u,v)\text{ and }d(v,\mathcal{V}-v)\geq% \gamma\cdot d(u,v).italic_d ( italic_u , caligraphic_U - italic_u ) ≥ italic_γ ⋅ italic_d ( italic_u , italic_v ) and italic_d ( italic_v , caligraphic_V - italic_v ) ≥ italic_γ ⋅ italic_d ( italic_u , italic_v ) . Using triangle inequality, it is easy to verify that each point u∈𝒰𝑢𝒰u\in\mathcal{U}italic_u ∈ caligraphic_U either forms a well-separated pair with a unique v∈𝒱𝑣𝒱v\in\mathcal{V}italic_v ∈ caligraphic_V, or it does not form a well-separated pair with any v∈𝒱𝑣𝒱v\in\mathcal{V}italic_v ∈ caligraphic_V. The next lemma implies that if 𝒰𝒰\mathcal{U}caligraphic_U is robust, then we can replace every center v∈𝒱𝑣𝒱v\in\mathcal{V}italic_v ∈ caligraphic_V that is well-separated by its counterpart in 𝒰𝒰\mathcal{U}caligraphic_U, and this will increase the cost of the solution 𝒱𝒱\mathcal{V}caligraphic_V by at most a constant factor. Lemma 3.9 ([FLNS21]). Consider any two sets of centers 𝒰,𝒱⊆𝐏𝒰𝒱𝐏\mathcal{U},\mathcal{V}\subseteq\mathbf{P}caligraphic_U , caligraphic_V ⊆ bold_P such that 𝒰𝒰\mathcal{U}caligraphic_U is robust. Then, for every well-separated pair (u,v)∈𝒰×𝒱𝑢𝑣𝒰𝒱(u,v)\in\mathcal{U}\times\mathcal{V}( italic_u , italic_v ) ∈ caligraphic_U × caligraphic_V, we have Cost⁢(u,Cv⁢(𝒱,𝒫))≤3⋅Cost⁢(v,Cv⁢(𝒱,𝒫))Cost𝑢subscript𝐶𝑣𝒱𝒫⋅3Cost𝑣subscript𝐶𝑣𝒱𝒫\textnormal{{Cost}}\left(u,C_{v}(\mathcal{V},\mathcal{P})\right)\leq 3\cdot% \textnormal{{Cost}}\left(v,C_{v}(\mathcal{V},\mathcal{P})\right)Cost ( italic_u , italic_C start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( caligraphic_V , caligraphic_P ) ) ≤ 3 ⋅ Cost ( italic_v , italic_C start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( caligraphic_V , caligraphic_P ) ). 3.4 Projection Lemma and Lazy-Updates Lemma We conclude by recalling two lemmas that are folklore in the literature on clustering [BCG+24]. Intuitively, the projection lemma says that if we have a set 𝒰𝒰\mathcal{U}caligraphic_U of more than k𝑘kitalic_k centers, then the cost of the best possible k𝑘kitalic_k-median solution, subject to the constraint that all of the k𝑘kitalic_k centers must be picked from 𝒰𝒰\mathcal{U}caligraphic_U, is not too large compared to Cost⁢(𝒰,𝒫)Cost𝒰𝒫\textnormal{{Cost}}\left(\mathcal{U},\mathcal{P}\right)Cost ( caligraphic_U , caligraphic_P ). Lemma 3.10 (Projection Lemma [BCG+24]). Consider any set of centers 𝒰⊆𝐏𝒰𝐏\mathcal{U}\subseteq\mathbf{P}caligraphic_U ⊆ bold_P of size |𝒰|≥k𝒰𝑘|\mathcal{U}|\geq k| caligraphic_U | ≥ italic_k, where k𝑘kitalic_k is a positive integer. Then we have OPTk𝒰⁢(𝒫)≤Cost⁢(𝒰,𝒫)+2⋅OPTk⁢(𝒫)superscriptsubscriptOPT𝑘𝒰𝒫Cost𝒰𝒫⋅2subscriptOPT𝑘𝒫\textnormal{{OPT}}_{k}^{\mathcal{U}}(\mathcal{P})\leq\textnormal{{Cost}}\left(% \mathcal{U},\mathcal{P}\right)+2\cdot\textnormal{{OPT}}_{k}(\mathcal{P})OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT caligraphic_U end_POSTSUPERSCRIPT ( caligraphic_P ) ≤ Cost ( caligraphic_U , caligraphic_P ) + 2 ⋅ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ). The lazy updates lemma, stated below, is derived from the following observation. Suppose that whenever a new point gets inserted into 𝒫𝒫\mathcal{P}caligraphic_P, we create a center at the position of the newly inserted point; and whenever a point gets deleted from 𝒫𝒫\mathcal{P}caligraphic_P, we do not make any changes to the set of centers. Then this lazy rule for handling updates ensures that the cost of the solution we maintain does not increase over time (although the solution might consist of more than k𝑘kitalic_k centers). Lemma 3.11 (Lazy-Updates Lemma [BCG+24]). Consider any two sets of input points 𝒫,𝒫′⊆𝐏𝒫superscript𝒫′𝐏\mathcal{P},\mathcal{P}^{\prime}\subseteq\mathbf{P}caligraphic_P , caligraphic_P start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ⊆ bold_P such that |𝒫⊕𝒫′|≤sdirect-sum𝒫superscript𝒫′𝑠|\mathcal{P}\oplus\mathcal{P}^{\prime}|\leq s| caligraphic_P ⊕ caligraphic_P start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT | ≤ italic_s. Then for every k≥1𝑘1k\geq 1italic_k ≥ 1, we have OPTk+s⁢(𝒫′)≤OPTk⁢(𝒫)subscriptOPT𝑘𝑠superscript𝒫′subscriptOPT𝑘𝒫\textnormal{{OPT}}_{k+s}(\mathcal{P}^{\prime})\leq\textnormal{{OPT}}_{k}(% \mathcal{P})OPT start_POSTSUBSCRIPT italic_k + italic_s end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) ≤ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ). 4 Two Key Lemmas We now state two key lemmas that will be used in the design and analysis of our dynamic algorithm. We defer the formal proofs of these two lemmas to Section 7. Lemma 4.1 (Double-Sided Stability Lemma). Consider any r∈[0,k−1]𝑟0𝑘1r\in[0,k-1]italic_r ∈ [ 0 , italic_k - 1 ] and any η≥1𝜂1\eta\geq 1italic_η ≥ 1. If OPTk−r⁢(𝒫)≤η⋅OPTk⁢(𝒫)subscriptOPT𝑘𝑟𝒫⋅𝜂subscriptOPT𝑘𝒫\textnormal{{OPT}}_{k-r}(\mathcal{P})\leq\eta\cdot\textnormal{{OPT}}_{k}(% \mathcal{P})OPT start_POSTSUBSCRIPT italic_k - italic_r end_POSTSUBSCRIPT ( caligraphic_P ) ≤ italic_η ⋅ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ), then we must have OPTk⁢(𝒫)≤4⋅OPTk+⌊r/(12⁢η)⌋⁢(𝒫)subscriptOPT𝑘𝒫⋅4subscriptOPT𝑘𝑟12𝜂𝒫\textnormal{{OPT}}_{k}(\mathcal{P})\leq 4\cdot\textnormal{{OPT}}_{k+\lfloor r/% (12\eta)\rfloor}(\mathcal{P})OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ) ≤ 4 ⋅ OPT start_POSTSUBSCRIPT italic_k + ⌊ italic_r / ( 12 italic_η ) ⌋ end_POSTSUBSCRIPT ( caligraphic_P ). To interpret Lemma 4.1, first note that OPTk⁢(𝒫)subscriptOPT𝑘𝒫\textnormal{{OPT}}_{k}(\mathcal{P})OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ) is a monotonically non-increasing function of k𝑘kitalic_k, since the objective value can only drop if we open extra centers. Now, suppose there is a sufficiently large integer r∈[0,k−1]𝑟0𝑘1r\in[0,k-1]italic_r ∈ [ 0 , italic_k - 1 ] such that OPTk−r⁢(𝒫)≤Θ⁢(1)⋅OPTk⁢(𝒫)subscriptOPT𝑘𝑟𝒫⋅Θ1subscriptOPT𝑘𝒫\textnormal{{OPT}}_{k-r}(\mathcal{P})\leq\Theta(1)\cdot\textnormal{{OPT}}_{k}(% \mathcal{P})OPT start_POSTSUBSCRIPT italic_k - italic_r end_POSTSUBSCRIPT ( caligraphic_P ) ≤ roman_Θ ( 1 ) ⋅ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ). Then, Lemma 4.1 guarantees that OPTk⁢(𝒫)≤Θ⁢(1)⋅OPTk+r′⁢(𝒫)subscriptOPT𝑘𝒫⋅Θ1subscriptOPT𝑘superscript𝑟′𝒫\textnormal{{OPT}}_{k}(\mathcal{P})\leq\Theta(1)\cdot\textnormal{{OPT}}_{k+r^{% \prime}}(\mathcal{P})OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ) ≤ roman_Θ ( 1 ) ⋅ OPT start_POSTSUBSCRIPT italic_k + italic_r start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( caligraphic_P ) for some integer r′=Θ⁢(r)superscript𝑟′Θ𝑟r^{\prime}=\Theta(r)italic_r start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = roman_Θ ( italic_r ). In other words, if the optimal objective remains stable as we decrease the number of centers by some additive r𝑟ritalic_r, then it also remains stable as we increase the number of centers by roughly the same amount. Conceptually, this holds because of two reasons: (i) the optimal objective of the fractional version of the k𝑘kitalic_k-median problem (encoded by its standard LP-relaxation) is convex as a function of k𝑘kitalic_k, and (ii) the concerned LP-relaxation has Θ⁢(1)Θ1\Theta(1)roman_Θ ( 1 )-integrality gap. Lemma 4.2 (Generalization of Lemma 7.3 in the arXiv version of [FLNS21]). Let r≥0𝑟0r\geq 0italic_r ≥ 0 and m∈[0,k]𝑚0𝑘m\in[0,k]italic_m ∈ [ 0 , italic_k ]. Consider any two sets of centers 𝒰,𝒱⊆𝐏𝒰𝒱𝐏\mathcal{U},\mathcal{V}\subseteq\mathbf{P}caligraphic_U , caligraphic_V ⊆ bold_P such that |𝒰|=k𝒰𝑘|\mathcal{U}|=k| caligraphic_U | = italic_k and |𝒱|=k+r𝒱𝑘𝑟|\mathcal{V}|=k+r| caligraphic_V | = italic_k + italic_r. If the number of well-separated pairs w.r.t. (𝒰,𝒱)𝒰𝒱(\mathcal{U},\mathcal{V})( caligraphic_U , caligraphic_V ) is k−m𝑘𝑚k-mitalic_k - italic_m, then there exists a subset 𝒰~⊆𝒰~𝒰𝒰\tilde{\mathcal{U}}\subseteq\mathcal{U}over~ start_ARG caligraphic_U end_ARG ⊆ caligraphic_U of size at most k−⌊(m−r)/4⌋𝑘𝑚𝑟4k-\lfloor(m-r)/4\rflooritalic_k - ⌊ ( italic_m - italic_r ) / 4 ⌋ such that Cost⁢(𝒰~,𝒫)≤6⁢γ⋅(Cost⁢(𝒰,𝒫)+Cost⁢(𝒱,𝒫))Cost~𝒰𝒫⋅6𝛾Cost𝒰𝒫Cost𝒱𝒫\textnormal{{Cost}}\left(\tilde{\mathcal{U}},\mathcal{P}\right)\leq 6\gamma% \cdot\left(\textnormal{{Cost}}\left(\mathcal{U},\mathcal{P}\right)+\textnormal% {{Cost}}\left(\mathcal{V},\mathcal{P}\right)\right)Cost ( over~ start_ARG caligraphic_U end_ARG , caligraphic_P ) ≤ 6 italic_γ ⋅ ( Cost ( caligraphic_U , caligraphic_P ) + Cost ( caligraphic_V , caligraphic_P ) ). Intuitively, think of 𝒰𝒰\mathcal{U}caligraphic_U as the k𝑘kitalic_k-median solution maintained by our algorithm, and let 𝒱𝒱\mathcal{V}caligraphic_V be another set of k+r𝑘𝑟k+ritalic_k + italic_r centers such that Cost⁢(𝒱,𝒫)≤Θ⁢(1)⋅OPTk⁢(𝒫)Cost𝒱𝒫⋅Θ1subscriptOPT𝑘𝒫\textnormal{{Cost}}\left(\mathcal{V},\mathcal{P}\right)\leq\Theta(1)\cdot% \textnormal{{OPT}}_{k}(\mathcal{P})Cost ( caligraphic_V , caligraphic_P ) ≤ roman_Θ ( 1 ) ⋅ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ). The above lemma implies that if m≫rmuch-greater-than𝑚𝑟m\gg ritalic_m ≫ italic_r (i.e., the number of well-separated pairs w.r.t. (𝒰,𝒱)𝒰𝒱(\mathcal{U},\mathcal{V})( caligraphic_U , caligraphic_V ) is sufficiently small), then we can delete ⌊(m−r)/4⌋=Ω⁢(r)𝑚𝑟4Ω𝑟\lfloor(m-r)/4\rfloor=\Omega(r)⌊ ( italic_m - italic_r ) / 4 ⌋ = roman_Ω ( italic_r ) centers from 𝒰𝒰\mathcal{U}caligraphic_U without significantly increasing the objective Cost⁢(𝒰,𝒫)Cost𝒰𝒫\textnormal{{Cost}}\left(\mathcal{U},\mathcal{P}\right)Cost ( caligraphic_U , caligraphic_P ). 5 Achieving O⁢(1)𝑂1O(1)italic_O ( 1 ) Approximation Ratio and O⁢(log2⁡Δ)𝑂superscript2ΔO(\log^{2}\Delta)italic_O ( roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT roman_Δ ) Recourse In this section, we focus only on achieving good approximation ratio and recourse bounds. We prove the following theorem, without any concern for the update time of the algorithm. In particular, to keep the exposition as simple as possible, we present an algorithm with exponential update time. Theorem 5.1. There is a deterministic O⁢(1)𝑂1O(1)italic_O ( 1 )-approximation algorithm for dynamic metric k𝑘kitalic_k-median with O⁢(log2⁡Δ)𝑂superscript2ΔO(\log^{2}\Delta)italic_O ( roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT roman_Δ ) recourse. 5.1 Description of the Algorithm Our algorithm works in epochs; each epoch lasts for some consecutive updates in 𝒫𝒫\mathcal{P}caligraphic_P. Let 𝒰⊆𝐏𝒰𝐏\mathcal{U}\subseteq\mathbf{P}caligraphic_U ⊆ bold_P denote the maintained solution (set of k𝑘kitalic_k centers). We satisfy the following invariant. Invariant 5.2. At the start of an epoch, the set 𝒰𝒰\mathcal{U}caligraphic_U is robust and Cost⁢(𝒰,𝒫)≤8⋅OPTk⁢(𝒫)Cost𝒰𝒫⋅8subscriptOPT𝑘𝒫\textnormal{{Cost}}\left(\mathcal{U},\mathcal{P}\right)\leq 8\cdot\textnormal{% {OPT}}_{k}(\mathcal{P})Cost ( caligraphic_U , caligraphic_P ) ≤ 8 ⋅ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ). We now describe how our dynamic algorithm works in a given epoch, in four steps. Step 1: Determining the length of the epoch. At the start of an epoch, we compute the maximum ℓ⋆≥0superscriptℓ⋆0\ell^{\star}\geq 0roman_ℓ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ≥ 0 such that OPTk−ℓ⋆⁢(𝒫)≤54⁢γ⋅OPTk⁢(𝒫)subscriptOPT𝑘superscriptℓ⋆𝒫⋅54𝛾subscriptOPT𝑘𝒫\textnormal{{OPT}}_{k-\ell^{\star}}(\mathcal{P})\leq 54\gamma\cdot\textnormal{% {OPT}}_{k}(\mathcal{P})OPT start_POSTSUBSCRIPT italic_k - roman_ℓ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( caligraphic_P ) ≤ 54 italic_γ ⋅ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ),999Recall γ𝛾\gammaitalic_γ from Section 3.1 and set ℓ←⌊ℓ⋆/(12⋅54⁢γ)⌋←ℓsuperscriptℓ⋆⋅1254𝛾\ell\leftarrow\lfloor\ell^{\star}/(12\cdot 54\gamma)\rfloorroman_ℓ ← ⌊ roman_ℓ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT / ( 12 ⋅ 54 italic_γ ) ⌋. Since ℓ≤ℓ⋆ℓsuperscriptℓ⋆\ell\leq\ell^{\star}roman_ℓ ≤ roman_ℓ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT, it follows that OPTk−ℓ⁢(𝒫)≤OPTk−ℓ⋆⁢(𝒫)subscriptOPT𝑘ℓ𝒫subscriptOPT𝑘superscriptℓ⋆𝒫\textnormal{{OPT}}_{k-\ell}(\mathcal{P})\leq\textnormal{{OPT}}_{k-\ell^{\star}% }(\mathcal{P})OPT start_POSTSUBSCRIPT italic_k - roman_ℓ end_POSTSUBSCRIPT ( caligraphic_P ) ≤ OPT start_POSTSUBSCRIPT italic_k - roman_ℓ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( caligraphic_P ). Thus, by setting η=54⁢γ𝜂54𝛾\eta=54\gammaitalic_η = 54 italic_γ and r=ℓ⋆𝑟superscriptℓ⋆r=\ell^{\star}italic_r = roman_ℓ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT in Lemma 4.1, at the start of the epoch we have: OPTk−ℓ⁢(𝒫)54⁢γ≤OPTk⁢(𝒫)≤4⋅OPTk+ℓ⁢(𝒫).subscriptOPT𝑘ℓ𝒫54𝛾subscriptOPT𝑘𝒫⋅4subscriptOPT𝑘ℓ𝒫\frac{\textnormal{{OPT}}_{k-\ell}(\mathcal{P})}{54\gamma}\leq\textnormal{{OPT}% }_{k}(\mathcal{P})\leq 4\cdot\textnormal{{OPT}}_{k+\ell}(\mathcal{P}).divide start_ARG OPT start_POSTSUBSCRIPT italic_k - roman_ℓ end_POSTSUBSCRIPT ( caligraphic_P ) end_ARG start_ARG 54 italic_γ end_ARG ≤ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ) ≤ 4 ⋅ OPT start_POSTSUBSCRIPT italic_k + roman_ℓ end_POSTSUBSCRIPT ( caligraphic_P ) . (5) The epoch will last for the next ℓ+1ℓ1\ell+1roman_ℓ + 1 updates.101010Note that we might very well have ℓ=0ℓ0\ell=0roman_ℓ = 0. From now on, we will use the superscript t∈[0,ℓ+1]𝑡0ℓ1t\in[0,\ell+1]italic_t ∈ [ 0 , roman_ℓ + 1 ] to denote the status of some object after our algorithm has finished processing the tt⁢hsuperscript𝑡𝑡ℎt^{th}italic_t start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT update in the epoch. For example, at the start of the epoch we have 𝒫=𝒫(0)𝒫superscript𝒫0\mathcal{P}=\mathcal{P}^{(0)}caligraphic_P = caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT. Step 2: Preprocessing at the start of the epoch. Let 𝒰init←𝒰←subscript𝒰init𝒰\mathcal{U}_{\textnormal{{init}}}\leftarrow\mathcal{U}caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ← caligraphic_U be the solution maintained by the algorithm after it finished processing the last update in the previous epoch. Before handling the very first update in the current epoch, we initialize the maintained solution by setting 𝒰(0)←arg⁡min𝒰′⊆𝒰init:|𝒰′|=k−ℓ⁡Cost⁢(𝒰′,𝒫(0)).←superscript𝒰0subscript:superscript𝒰′subscript𝒰initsuperscript𝒰′𝑘ℓCostsuperscript𝒰′superscript𝒫0\mathcal{U}^{(0)}\leftarrow\arg\min_{\mathcal{U}^{\prime}\subseteq\mathcal{U}_% {\textnormal{{init}}}\,:\,|\mathcal{U}^{\prime}|=k-\ell}\textnormal{{Cost}}% \left(\mathcal{U}^{\prime},\mathcal{P}^{(0)}\right).caligraphic_U start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ← roman_arg roman_min start_POSTSUBSCRIPT caligraphic_U start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ⊆ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT : | caligraphic_U start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT | = italic_k - roman_ℓ end_POSTSUBSCRIPT Cost ( caligraphic_U start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) . (6) Thus, at this point in time, we have Cost⁢(𝒰(0),𝒫(0))=OPTk−ℓ𝒰init⁢(𝒫(0))≤Cost⁢(𝒰init,𝒫(0))+2⋅OPTk−ℓ⁢(𝒫(0))≤8⋅OPTk⁢(𝒫(0))+2⋅OPTk−ℓ⁢(𝒫(0))≤(32+432⁢γ)⋅OPTk+ℓ⁢(𝒫(0))Costsuperscript𝒰0superscript𝒫0superscriptsubscriptOPT𝑘ℓsubscript𝒰initsuperscript𝒫0Costsubscript𝒰initsuperscript𝒫0⋅2subscriptOPT𝑘ℓsuperscript𝒫0⋅8subscriptOPT𝑘superscript𝒫0⋅2subscriptOPT𝑘ℓsuperscript𝒫0⋅32432𝛾subscriptOPT𝑘ℓsuperscript𝒫0\textnormal{{Cost}}\left(\mathcal{U}^{(0)},\mathcal{P}^{(0)}\right)=% \textnormal{{OPT}}_{k-\ell}^{\mathcal{U}_{\textnormal{{init}}}}\left(\mathcal{% P}^{(0)}\right)\leq\textnormal{{Cost}}\left(\mathcal{U}_{\textnormal{{init}}},% \mathcal{P}^{(0)}\right)+2\cdot\textnormal{{OPT}}_{k-\ell}\left(\mathcal{P}^{(% 0)}\right)\leq 8\cdot\textnormal{{OPT}}_{k}\left(\mathcal{P}^{(0)}\right)+2% \cdot\textnormal{{OPT}}_{k-\ell}\left(\mathcal{P}^{(0)}\right)\leq(32+432% \gamma)\cdot\textnormal{{OPT}}_{k+\ell}\left(\mathcal{P}^{(0)}\right)Cost ( caligraphic_U start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) = OPT start_POSTSUBSCRIPT italic_k - roman_ℓ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) ≤ Cost ( caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) + 2 ⋅ OPT start_POSTSUBSCRIPT italic_k - roman_ℓ end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) ≤ 8 ⋅ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) + 2 ⋅ OPT start_POSTSUBSCRIPT italic_k - roman_ℓ end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) ≤ ( 32 + 432 italic_γ ) ⋅ OPT start_POSTSUBSCRIPT italic_k + roman_ℓ end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ), where the first inequality follows from Lemma 3.10, the second inequality follows from Invariant 5.2, and the last inequality follows from (5). To summarize, we get: Cost⁢(𝒰(0),𝒫(0))≤(32+432⁢γ)⋅OPTk+ℓ⁢(𝒫(0))⁢ and ⁢|𝒰(0)|≤k−ℓ.Costsuperscript𝒰0superscript𝒫0⋅32432𝛾subscriptOPT𝑘ℓsuperscript𝒫0 and superscript𝒰0𝑘ℓ\textnormal{{Cost}}\left(\mathcal{U}^{(0)},\mathcal{P}^{(0)}\right)\leq(32+432% \gamma)\cdot\textnormal{{OPT}}_{k+\ell}\left(\mathcal{P}^{(0)}\right)\text{ % and }\left|\mathcal{U}^{(0)}\right|\leq k-\ell.Cost ( caligraphic_U start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) ≤ ( 32 + 432 italic_γ ) ⋅ OPT start_POSTSUBSCRIPT italic_k + roman_ℓ end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) and | caligraphic_U start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT | ≤ italic_k - roman_ℓ . (7) In words, before we deal with the very first update in the epoch, the maintained solution 𝒰(0)superscript𝒰0\mathcal{U}^{(0)}caligraphic_U start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT is a (32+432⁢γ)=Θ⁢(1)32432𝛾Θ1(32+432\gamma)=\Theta(1)( 32 + 432 italic_γ ) = roman_Θ ( 1 )-approximation of OPTk+ℓ⁢(𝒫(0))subscriptOPT𝑘ℓsuperscript𝒫0\textnormal{{OPT}}_{k+\ell}(\mathcal{P}^{(0)})OPT start_POSTSUBSCRIPT italic_k + roman_ℓ end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ), and consists of at most (k−ℓ)𝑘ℓ(k-\ell)( italic_k - roman_ℓ ) centers. Both these properties will be crucially exploited while handling the updates within the epoch. Step 3: Handling the updates within the epoch. Consider the tt⁢hsuperscript𝑡𝑡ℎt^{th}italic_t start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT update in the epoch, for t∈[1,ℓ+1]𝑡1ℓ1t\in[1,\ell+1]italic_t ∈ [ 1 , roman_ℓ + 1 ]. We handle this update in a lazy manner, as follows. If the update involves the deletion of a point from 𝒫𝒫\mathcal{P}caligraphic_P, then we do not change our maintained solution, and set 𝒰(t)←𝒰(t−1)←superscript𝒰𝑡superscript𝒰𝑡1\mathcal{U}^{(t)}\leftarrow\mathcal{U}^{(t-1)}caligraphic_U start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT ← caligraphic_U start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT. (The maintained solution remains valid, since we are considering the improper k𝑘kitalic_k-median problem). In contrast, if the update involves the insertion of a point p𝑝pitalic_p into 𝒫𝒫\mathcal{P}caligraphic_P, then we set 𝒰(t)←𝒰(t−1)+p←superscript𝒰𝑡superscript𝒰𝑡1𝑝\mathcal{U}^{(t)}\leftarrow\mathcal{U}^{(t-1)}+pcaligraphic_U start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT ← caligraphic_U start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT + italic_p. It is easy to verify that this lazy way of dealing with an update does not increase the objective, and increases the number of centers in the maintained solution by at most one. Thus, we have Cost⁢(𝒰(t),𝒫(t))≤Cost⁢(𝒰(t−1),𝒫(t−1))Costsuperscript𝒰𝑡superscript𝒫𝑡Costsuperscript𝒰𝑡1superscript𝒫𝑡1\textnormal{{Cost}}\left(\mathcal{U}^{(t)},\mathcal{P}^{(t)}\right)\leq% \textnormal{{Cost}}\left(\mathcal{U}^{(t-1)},\mathcal{P}^{(t-1)}\right)Cost ( caligraphic_U start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT ) ≤ Cost ( caligraphic_U start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) and |𝒰(t)|≤|𝒰(t−1)|+1superscript𝒰𝑡superscript𝒰𝑡11\left|\mathcal{U}^{(t)}\right|\leq\left|\mathcal{U}^{(t-1)}\right|+1| caligraphic_U start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT | ≤ | caligraphic_U start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT | + 1. From (7), we now derive that Cost⁢(𝒰(t),𝒫(t))≤Cost⁢(𝒰(0),𝒫(0))⁢ and ⁢|𝒰(t)|≤k−ℓ+t, for all ⁢t∈[1,ℓ+1].formulae-sequenceCostsuperscript𝒰𝑡superscript𝒫𝑡Costsuperscript𝒰0superscript𝒫0 and superscript𝒰𝑡𝑘ℓ𝑡 for all 𝑡1ℓ1\textnormal{{Cost}}\left(\mathcal{U}^{(t)},\mathcal{P}^{(t)}\right)\leq% \textnormal{{Cost}}\left(\mathcal{U}^{(0)},\mathcal{P}^{(0)}\right)\text{ and % }\left|\mathcal{U}^{(t)}\right|\leq k-\ell+t,\text{ for all }t\in[1,\ell+1].Cost ( caligraphic_U start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT ) ≤ Cost ( caligraphic_U start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) and | caligraphic_U start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT | ≤ italic_k - roman_ℓ + italic_t , for all italic_t ∈ [ 1 , roman_ℓ + 1 ] . (8) Step 4: Post-processing at the end of the epoch. By (8), the set 𝒰(t)superscript𝒰𝑡\mathcal{U}^{(t)}caligraphic_U start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT remains a valid solution for the improper k𝑘kitalic_k-median problem, for all t∈[1,ℓ]𝑡1ℓt\in[1,\ell]italic_t ∈ [ 1 , roman_ℓ ]. After the very last update in the epoch, however, the set 𝒰(ℓ+1)superscript𝒰ℓ1\mathcal{U}^{(\ell+1)}caligraphic_U start_POSTSUPERSCRIPT ( roman_ℓ + 1 ) end_POSTSUPERSCRIPT might have more than k𝑘kitalic_k centers. At this point in time, we do some post-processing, and compute another set 𝒰final⊆𝐏subscript𝒰final𝐏\mathcal{U}_{\textnormal{{final}}}\subseteq\mathbf{P}caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT ⊆ bold_P of at most k𝑘kitalic_k centers (i.e., |𝒰final|≤ksubscript𝒰final𝑘|\mathcal{U}_{\textnormal{{final}}}|\leq k| caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT | ≤ italic_k) that satisfies Invariant 5.2. We then initiate the next epoch, with 𝒰←𝒰final←𝒰subscript𝒰final\mathcal{U}\leftarrow\mathcal{U}_{\textnormal{{final}}}caligraphic_U ← caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT being the current solution. The post-processing is done as follows. We first add O⁢(ℓ+1)𝑂ℓ1O(\ell+1)italic_O ( roman_ℓ + 1 ) extra centers to the set 𝒰initsubscript𝒰init\mathcal{U}_{\textnormal{{init}}}caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT, while minimizing the cost of the resulting solution w.r.t. 𝒫(0)superscript𝒫0\mathcal{P}^{(0)}caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT. This gives us the set of centers 𝒰⋆superscript𝒰⋆\mathcal{U}^{\star}caligraphic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT. Note that |𝒰⋆|=k+O⁢(ℓ+1)superscript𝒰⋆𝑘𝑂ℓ1\left|\mathcal{U}^{\star}\right|=k+O(\ell+1)| caligraphic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT | = italic_k + italic_O ( roman_ℓ + 1 ). ℱ⋆←arg⁡minℱ⊆𝐏:|ℱ|≤2700⁢γ⋅(ℓ+1)⁡Cost⁢(𝒰init+ℱ,𝒫(0)), and ⁢𝒰⋆←𝒰init+ℱ⋆.formulae-sequence←superscriptℱ⋆subscript:ℱ𝐏ℱ⋅2700𝛾ℓ1Costsubscript𝒰initℱsuperscript𝒫0← and superscript𝒰⋆subscript𝒰initsuperscriptℱ⋆\mathcal{F}^{\star}\leftarrow\arg\min\limits_{\begin{subarray}{c}\mathcal{F}% \subseteq\mathbf{P}:|\mathcal{F}|\leq 2700\gamma\cdot(\ell+1)\end{subarray}}% \textnormal{{Cost}}\left(\mathcal{U}_{\textnormal{{init}}}+\mathcal{F},% \mathcal{P}^{(0)}\right),\text{ and }\mathcal{U}^{\star}\leftarrow\mathcal{U}_% {\textnormal{{init}}}+\mathcal{F}^{\star}.caligraphic_F start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ← roman_arg roman_min start_POSTSUBSCRIPT start_ARG start_ROW start_CELL caligraphic_F ⊆ bold_P : | caligraphic_F | ≤ 2700 italic_γ ⋅ ( roman_ℓ + 1 ) end_CELL end_ROW end_ARG end_POSTSUBSCRIPT Cost ( caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT + caligraphic_F , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) , and caligraphic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ← caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT + caligraphic_F start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT . (9) We next add the newly inserted points within the epoch to the set of centers, so as to obtain the set 𝒱⋆superscript𝒱⋆\mathcal{V}^{\star}caligraphic_V start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT. Since the epoch lasts for ℓ+1ℓ1\ell+1roman_ℓ + 1 updates, we have |𝒱⋆|=k+O⁢(ℓ+1)superscript𝒱⋆𝑘𝑂ℓ1|\mathcal{V}^{\star}|=k+O(\ell+1)| caligraphic_V start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT | = italic_k + italic_O ( roman_ℓ + 1 ). Next, we identify the subset 𝒲⋆⊆𝒱⋆superscript𝒲⋆superscript𝒱⋆\mathcal{W}^{\star}\subseteq\mathcal{V}^{\star}caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ⊆ caligraphic_V start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT of k𝑘kitalic_k centers that minimizes the k𝑘kitalic_k-median objective w.r.t. 𝒫(ℓ+1)superscript𝒫ℓ1\mathcal{P}^{(\ell+1)}caligraphic_P start_POSTSUPERSCRIPT ( roman_ℓ + 1 ) end_POSTSUPERSCRIPT. 𝒱⋆←𝒰⋆+(𝒫(ℓ+1)−𝒫(0)), and ⁢𝒲⋆←arg⁡min𝒲⊆𝒱⋆:|𝒲|=k⁡Cost⁢(𝒲,𝒫(ℓ+1)).formulae-sequence←superscript𝒱⋆superscript𝒰⋆superscript𝒫ℓ1superscript𝒫0← and superscript𝒲⋆subscript:𝒲superscript𝒱⋆𝒲𝑘Cost𝒲superscript𝒫ℓ1\mathcal{V}^{\star}\leftarrow\mathcal{U}^{\star}+\left(\mathcal{P}^{(\ell+1)}-% \mathcal{P}^{(0)}\right),\text{ and }\mathcal{W}^{\star}\leftarrow\arg\min% \limits_{\begin{subarray}{c}\mathcal{W}\subseteq\mathcal{V}^{\star}\,:\,|% \mathcal{W}|=k\end{subarray}}\textnormal{{Cost}}\left(\mathcal{W},\mathcal{P}^% {(\ell+1)}\right).caligraphic_V start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ← caligraphic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT + ( caligraphic_P start_POSTSUPERSCRIPT ( roman_ℓ + 1 ) end_POSTSUPERSCRIPT - caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) , and caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ← roman_arg roman_min start_POSTSUBSCRIPT start_ARG start_ROW start_CELL caligraphic_W ⊆ caligraphic_V start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT : | caligraphic_W | = italic_k end_CELL end_ROW end_ARG end_POSTSUBSCRIPT Cost ( caligraphic_W , caligraphic_P start_POSTSUPERSCRIPT ( roman_ℓ + 1 ) end_POSTSUPERSCRIPT ) . (10) Finally, we call Robustify(𝒲⋆)superscript𝒲⋆(\mathcal{W}^{\star})( caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) and let 𝒰finalsubscript𝒰final\mathcal{U}_{\textnormal{{final}}}caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT be the set of k𝑘kitalic_k centers returned by this subroutine. Before starting the next epoch, we set 𝒰←𝒰final←𝒰subscript𝒰final\mathcal{U}\leftarrow\mathcal{U}_{\textnormal{{final}}}caligraphic_U ← caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT. 𝒰final←Robustify⁢(𝒲⋆).←subscript𝒰finalRobustifysuperscript𝒲⋆\mathcal{U}_{\textnormal{{final}}}\leftarrow\textsc{Robustify}(\mathcal{W}^{% \star}).caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT ← Robustify ( caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) . (11) It is easy to verify that we always maintain a set 𝒰⊆𝐏𝒰𝐏\mathcal{U}\subseteq\mathbf{P}caligraphic_U ⊆ bold_P of k𝑘kitalic_k centers. In Section 5.2, we show that 𝒰=𝒰final𝒰subscript𝒰final\mathcal{U}=\mathcal{U}_{\textnormal{{final}}}caligraphic_U = caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT satisfies Invariant 5.2 at the end of Step 4, and analyze the approximation ratio of the overall algorithm. Finally, Section 5.3 bounds the recourse of the algorithm. We conclude this section with a corollary that will play an important role in our recourse analysis. Corollary 5.3. We have |𝒲⋆⊕𝒰init|=O⁢(ℓ+1)direct-sumsuperscript𝒲⋆subscript𝒰init𝑂ℓ1\left|\mathcal{W}^{\star}\oplus\mathcal{U}_{\textnormal{{init}}}\right|=O(\ell% +1)| caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ⊕ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT | = italic_O ( roman_ℓ + 1 ). Proof. From (9) and (10), we infer that |𝒱⋆⊕𝒰init|≤|𝒱⋆⊕𝒰⋆|+|𝒰⋆⊕𝒰init|≤|𝒫(ℓ+1)−𝒫(0)|+|ℱ⋆|≤(2700⁢γ⋅(ℓ+1))+(ℓ+1)=O⁢(ℓ+1)direct-sumsuperscript𝒱⋆subscript𝒰initdirect-sumsuperscript𝒱⋆superscript𝒰⋆direct-sumsuperscript𝒰⋆subscript𝒰initsuperscript𝒫ℓ1superscript𝒫0superscriptℱ⋆⋅2700𝛾ℓ1ℓ1𝑂ℓ1\left|\mathcal{V}^{\star}\oplus\mathcal{U}_{\textnormal{{init}}}\right|\leq% \left|\mathcal{V}^{\star}\oplus\mathcal{U}^{\star}\right|+\left|\mathcal{U}^{% \star}\oplus\mathcal{U}_{\textnormal{{init}}}\right|\leq\left|\mathcal{P}^{(% \ell+1)}-\mathcal{P}^{(0)}\right|+\left|\mathcal{F}^{\star}\right|\leq(2700% \gamma\cdot(\ell+1))+(\ell+1)=O(\ell+1)| caligraphic_V start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ⊕ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT | ≤ | caligraphic_V start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ⊕ caligraphic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT | + | caligraphic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ⊕ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT | ≤ | caligraphic_P start_POSTSUPERSCRIPT ( roman_ℓ + 1 ) end_POSTSUPERSCRIPT - caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT | + | caligraphic_F start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT | ≤ ( 2700 italic_γ ⋅ ( roman_ℓ + 1 ) ) + ( roman_ℓ + 1 ) = italic_O ( roman_ℓ + 1 ). Next, recall that |𝒱⋆|=k+O⁢(ℓ+1)superscript𝒱⋆𝑘𝑂ℓ1\left|\mathcal{V}^{\star}\right|=k+O(\ell+1)| caligraphic_V start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT | = italic_k + italic_O ( roman_ℓ + 1 ), and 𝒲⋆superscript𝒲⋆\mathcal{W}^{\star}caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT is a subset of 𝒱⋆superscript𝒱⋆\mathcal{V}^{\star}caligraphic_V start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT of size k𝑘kitalic_k. Thus, we get: |𝒲⋆⊕𝒰init|≤|𝒲⋆⊕𝒱⋆|+|𝒱⋆⊕𝒰init|=O⁢(ℓ+1)+O⁢(ℓ+1)=O⁢(ℓ+1)direct-sumsuperscript𝒲⋆subscript𝒰initdirect-sumsuperscript𝒲⋆superscript𝒱⋆direct-sumsuperscript𝒱⋆subscript𝒰init𝑂ℓ1𝑂ℓ1𝑂ℓ1\left|\mathcal{W}^{\star}\oplus\mathcal{U}_{\textnormal{{init}}}\right|\leq% \left|\mathcal{W}^{\star}\oplus\mathcal{V}^{\star}\right|+\left|\mathcal{V}^{% \star}\oplus\mathcal{U}_{\textnormal{{init}}}\right|=O(\ell+1)+O(\ell+1)=O(% \ell+1)| caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ⊕ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT | ≤ | caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ⊕ caligraphic_V start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT | + | caligraphic_V start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ⊕ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT | = italic_O ( roman_ℓ + 1 ) + italic_O ( roman_ℓ + 1 ) = italic_O ( roman_ℓ + 1 ). This concludes the proof. ∎ 5.2 Analyzing the Approximation Ratio Consider any t∈[0,ℓ]𝑡0ℓt\in[0,\ell]italic_t ∈ [ 0 , roman_ℓ ], and note that |𝒫(t)⊕𝒫(0)|≤t≤ℓdirect-sumsuperscript𝒫𝑡superscript𝒫0𝑡ℓ\left|\mathcal{P}^{(t)}\oplus\mathcal{P}^{(0)}\right|\leq t\leq\ell| caligraphic_P start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT ⊕ caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT | ≤ italic_t ≤ roman_ℓ. Thus, by Lemma 3.11, we have: OPTk+ℓ⁢(𝒫(0))≤OPTk⁢(𝒫(t)).subscriptOPT𝑘ℓsuperscript𝒫0subscriptOPT𝑘superscript𝒫𝑡\textnormal{{OPT}}_{k+\ell}\left(\mathcal{P}^{(0)}\right)\leq\textnormal{{OPT}% }_{k}\left(\mathcal{P}^{(t)}\right).OPT start_POSTSUBSCRIPT italic_k + roman_ℓ end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) ≤ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT ) . (12) From (7), (8) and (12), we now infer that: Cost⁢(𝒰(t),𝒫(t))≤(32+432⁢γ)⋅OPTk⁢(𝒫(t))⁢ and ⁢|𝒰(t)|≤k, for all ⁢t∈[0,ℓ].formulae-sequenceCostsuperscript𝒰𝑡superscript𝒫𝑡⋅32432𝛾subscriptOPT𝑘superscript𝒫𝑡 and superscript𝒰𝑡𝑘 for all 𝑡0ℓ\textnormal{{Cost}}\left(\mathcal{U}^{(t)},\mathcal{P}^{(t)}\right)\leq(32+432% \gamma)\cdot\textnormal{{OPT}}_{k}\left(\mathcal{P}^{(t)}\right)\text{ and }% \left|\mathcal{U}^{(t)}\right|\leq k,\text{ for all }t\in[0,\ell].Cost ( caligraphic_U start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT ) ≤ ( 32 + 432 italic_γ ) ⋅ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT ) and | caligraphic_U start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT | ≤ italic_k , for all italic_t ∈ [ 0 , roman_ℓ ] . (13) In other words, at all times within an epoch, the set 𝒰(t)superscript𝒰𝑡\mathcal{U}^{(t)}caligraphic_U start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT maintained by our algorithm remains a valid Θ⁢(1)Θ1\Theta(1)roman_Θ ( 1 )-approximate solution to the improper k𝑘kitalic_k-median problem on the current input 𝒫(t)superscript𝒫𝑡\mathcal{P}^{(t)}caligraphic_P start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT. It remains to show that the algorithm successfully restores Invariant 5.2 when the epoch terminates after the (ℓ+1)t⁢hsuperscriptℓ1𝑡ℎ(\ell+1)^{th}( roman_ℓ + 1 ) start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT update. Accordingly, we devote the rest of this section to the proof of Lemma 5.4. Lemma 5.4. At the end of Step 4 in Section 5.1, the set 𝒰=𝒰final𝒰subscript𝒰final\mathcal{U}=\mathcal{U}_{\textnormal{{final}}}caligraphic_U = caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT satisfies Invariant 5.2. The claim below bounds the cost of the solution 𝒰⋆superscript𝒰⋆\mathcal{U}^{\star}caligraphic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT w.r.t. the point-set 𝒫(0)superscript𝒫0\mathcal{P}^{(0)}caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT. Claim 5.5. We have Cost⁢(𝒰⋆,𝒫(0))≤3⋅OPTk+ℓ+1⁢(𝒫(0))Costsuperscript𝒰⋆superscript𝒫0⋅3subscriptOPT𝑘ℓ1superscript𝒫0\textnormal{{Cost}}\left(\mathcal{U}^{\star},\mathcal{P}^{(0)}\right)\leq 3% \cdot\textnormal{{OPT}}_{k+\ell+1}\left(\mathcal{P}^{(0)}\right)Cost ( caligraphic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) ≤ 3 ⋅ OPT start_POSTSUBSCRIPT italic_k + roman_ℓ + 1 end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ). Before proving Claim 5.5, we explain how it implies Lemma 5.4. Towards this end, note that: Cost⁢(𝒰final,𝒫(ℓ+1))Costsubscript𝒰finalsuperscript𝒫ℓ1\displaystyle\textnormal{{Cost}}\left(\mathcal{U}_{\textnormal{{final}}},% \mathcal{P}^{(\ell+1)}\right)Cost ( caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( roman_ℓ + 1 ) end_POSTSUPERSCRIPT ) ≤\displaystyle\leq≤ 32⋅Cost⁢(𝒲⋆,𝒫(ℓ+1))=32⋅OPTk𝒱⋆⁢(𝒫(ℓ+1))⋅32Costsuperscript𝒲⋆superscript𝒫ℓ1⋅32superscriptsubscriptOPT𝑘superscript𝒱⋆superscript𝒫ℓ1\displaystyle\frac{3}{2}\cdot\textnormal{{Cost}}\left(\mathcal{W}^{\star},% \mathcal{P}^{(\ell+1)}\right)=\frac{3}{2}\cdot\textnormal{{OPT}}_{k}^{\mathcal% {V}^{\star}}\left(\mathcal{P}^{(\ell+1)}\right)divide start_ARG 3 end_ARG start_ARG 2 end_ARG ⋅ Cost ( caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( roman_ℓ + 1 ) end_POSTSUPERSCRIPT ) = divide start_ARG 3 end_ARG start_ARG 2 end_ARG ⋅ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT caligraphic_V start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( roman_ℓ + 1 ) end_POSTSUPERSCRIPT ) (14) ≤\displaystyle\leq≤ 32⋅Cost⁢(𝒱⋆,𝒫(ℓ+1))+3⋅OPTk⁢(𝒫(ℓ+1))⋅32Costsuperscript𝒱⋆superscript𝒫ℓ1⋅3subscriptOPT𝑘superscript𝒫ℓ1\displaystyle\frac{3}{2}\cdot\textnormal{{Cost}}\left(\mathcal{V}^{\star},% \mathcal{P}^{(\ell+1)}\right)+3\cdot\textnormal{{OPT}}_{k}\left(\mathcal{P}^{(% \ell+1)}\right)divide start_ARG 3 end_ARG start_ARG 2 end_ARG ⋅ Cost ( caligraphic_V start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( roman_ℓ + 1 ) end_POSTSUPERSCRIPT ) + 3 ⋅ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( roman_ℓ + 1 ) end_POSTSUPERSCRIPT ) (15) ≤\displaystyle\leq≤ 32⋅Cost⁢(𝒰⋆,𝒫(0))+3⋅OPTk⁢(𝒫(ℓ+1))⋅32Costsuperscript𝒰⋆superscript𝒫0⋅3subscriptOPT𝑘superscript𝒫ℓ1\displaystyle\frac{3}{2}\cdot\textnormal{{Cost}}\left(\mathcal{U}^{\star},% \mathcal{P}^{(0)}\right)+3\cdot\textnormal{{OPT}}_{k}\left(\mathcal{P}^{(\ell+% 1)}\right)divide start_ARG 3 end_ARG start_ARG 2 end_ARG ⋅ Cost ( caligraphic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) + 3 ⋅ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( roman_ℓ + 1 ) end_POSTSUPERSCRIPT ) (16) ≤\displaystyle\leq≤ 92⋅OPTk+ℓ+1⁢(𝒫(0))+3⋅OPTk⁢(𝒫(ℓ+1))⋅92subscriptOPT𝑘ℓ1superscript𝒫0⋅3subscriptOPT𝑘superscript𝒫ℓ1\displaystyle\frac{9}{2}\cdot\textnormal{{OPT}}_{k+\ell+1}\left(\mathcal{P}^{(% 0)}\right)+3\cdot\textnormal{{OPT}}_{k}\left(\mathcal{P}^{(\ell+1)}\right)divide start_ARG 9 end_ARG start_ARG 2 end_ARG ⋅ OPT start_POSTSUBSCRIPT italic_k + roman_ℓ + 1 end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) + 3 ⋅ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( roman_ℓ + 1 ) end_POSTSUPERSCRIPT ) (17) ≤\displaystyle\leq≤ 92⋅OPTk⁢(𝒫(ℓ+1))+3⋅OPTk⁢(𝒫(ℓ+1))⋅92subscriptOPT𝑘superscript𝒫ℓ1⋅3subscriptOPT𝑘superscript𝒫ℓ1\displaystyle\frac{9}{2}\cdot\textnormal{{OPT}}_{k}\left(\mathcal{P}^{(\ell+1)% }\right)+3\cdot\textnormal{{OPT}}_{k}\left(\mathcal{P}^{(\ell+1)}\right)divide start_ARG 9 end_ARG start_ARG 2 end_ARG ⋅ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( roman_ℓ + 1 ) end_POSTSUPERSCRIPT ) + 3 ⋅ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( roman_ℓ + 1 ) end_POSTSUPERSCRIPT ) (18) ≤\displaystyle\leq≤ 8⋅OPTk⁢(𝒫(ℓ+1)).⋅8subscriptOPT𝑘superscript𝒫ℓ1\displaystyle 8\cdot\textnormal{{OPT}}_{k}\left(\mathcal{P}^{(\ell+1)}\right).8 ⋅ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( roman_ℓ + 1 ) end_POSTSUPERSCRIPT ) . (19) In the above derivation, the first step (14) follows from (10), (11) and Lemma 3.7. The second step (15) follows from Lemma 3.10. The third step (16) follows from (10). The fourth step (17) follows from Claim 5.5. The fifth step (18) follows from Lemma 3.11 and the observation that |𝒫(ℓ+1)⊕𝒫(0)|≤ℓ+1direct-sumsuperscript𝒫ℓ1superscript𝒫0ℓ1\left|\mathcal{P}^{(\ell+1)}\oplus\mathcal{P}^{(0)}\right|\leq\ell+1| caligraphic_P start_POSTSUPERSCRIPT ( roman_ℓ + 1 ) end_POSTSUPERSCRIPT ⊕ caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT | ≤ roman_ℓ + 1. From (19), we infer that at the start of the next epoch Cost⁢(𝒰,𝒫)≤8⋅OPTk⁢(𝒫)Cost𝒰𝒫⋅8subscriptOPT𝑘𝒫\textnormal{{Cost}}\left(\mathcal{U},\mathcal{P}\right)\leq 8\cdot\textnormal{% {OPT}}_{k}(\mathcal{P})Cost ( caligraphic_U , caligraphic_P ) ≤ 8 ⋅ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ), and the set 𝒰𝒰\mathcal{U}caligraphic_U is robust because of (11). This implies Lemma 5.4. 5.2.1 Proof of Claim 5.5 Let 𝒱⊆𝐏𝒱𝐏\mathcal{V}\subseteq\mathbf{P}caligraphic_V ⊆ bold_P be an optimal improper (k+ℓ+1)𝑘ℓ1(k+\ell+1)( italic_k + roman_ℓ + 1 )-median solution for the point-set 𝒫(0)superscript𝒫0\mathcal{P}^{(0)}caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT, i.e., |𝒱|=k+ℓ+1𝒱𝑘ℓ1\left|\mathcal{V}\right|=k+\ell+1| caligraphic_V | = italic_k + roman_ℓ + 1 and Cost⁢(𝒱,𝒫(0))=OPTk+ℓ+1⁢(𝒫(0))Cost𝒱superscript𝒫0subscriptOPT𝑘ℓ1superscript𝒫0\textnormal{{Cost}}\left(\mathcal{V},\mathcal{P}^{(0)}\right)=\textnormal{{OPT% }}_{k+\ell+1}\left(\mathcal{P}^{(0)}\right)Cost ( caligraphic_V , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) = OPT start_POSTSUBSCRIPT italic_k + roman_ℓ + 1 end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ). Let m∈[0,k]𝑚0𝑘m\in[0,k]italic_m ∈ [ 0 , italic_k ] be the unique integer such that there are (k−m)𝑘𝑚(k-m)( italic_k - italic_m ) well-separated pairs w.r.t. (𝒰init,𝒱)subscript𝒰init𝒱\left(\mathcal{U}_{\textnormal{{init}}},\mathcal{V}\right)( caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT , caligraphic_V ). Let {(u1,v1),(u2,v2),…,(uk−m,vk−m)}⊆𝒰init×𝒱subscript𝑢1subscript𝑣1subscript𝑢2subscript𝑣2…subscript𝑢𝑘𝑚subscript𝑣𝑘𝑚subscript𝒰init𝒱\{(u_{1},v_{1}),(u_{2},v_{2}),\ldots,(u_{k-m},v_{k-m})\}\subseteq\mathcal{U}_{% \textnormal{{init}}}\times\mathcal{V}{ ( italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , ( italic_u start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) , … , ( italic_u start_POSTSUBSCRIPT italic_k - italic_m end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_k - italic_m end_POSTSUBSCRIPT ) } ⊆ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT × caligraphic_V denote the collection of (k−m)𝑘𝑚(k-m)( italic_k - italic_m ) well-separated pairs w.r.t. (𝒰init,𝒱)subscript𝒰init𝒱\left(\mathcal{U}_{\textnormal{{init}}},\mathcal{V}\right)( caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT , caligraphic_V ). Define the set ℱ~:=𝒱∖{v1,…,vk−m}assign~ℱ𝒱subscript𝑣1…subscript𝑣𝑘𝑚\tilde{\mathcal{F}}:=\mathcal{V}\setminus\{v_{1},\ldots,v_{k-m}\}over~ start_ARG caligraphic_F end_ARG := caligraphic_V ∖ { italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_v start_POSTSUBSCRIPT italic_k - italic_m end_POSTSUBSCRIPT }. It is easy to verify that: |ℱ~|=|𝒱|−(k−m)=m+ℓ+1.~ℱ𝒱𝑘𝑚𝑚ℓ1\left|\tilde{\mathcal{F}}\right|=\left|\mathcal{V}\right|-(k-m)=m+\ell+1.| over~ start_ARG caligraphic_F end_ARG | = | caligraphic_V | - ( italic_k - italic_m ) = italic_m + roman_ℓ + 1 . (20) Claim 5.6. We have m≤2600⁢γ⋅(ℓ+1)𝑚⋅2600𝛾ℓ1m\leq 2600\gamma\cdot(\ell+1)italic_m ≤ 2600 italic_γ ⋅ ( roman_ℓ + 1 ). Claim 5.7. We have Cost⁢(𝒰init+ℱ~,𝒫(0))≤3⋅OPTk+ℓ+1⁢(𝒫(0))Costsubscript𝒰init~ℱsuperscript𝒫0⋅3subscriptOPT𝑘ℓ1superscript𝒫0\textnormal{{Cost}}\left(\mathcal{U}_{\textnormal{{init}}}+\tilde{\mathcal{F}}% ,\mathcal{P}^{(0)}\right)\leq 3\cdot\textnormal{{OPT}}_{k+\ell+1}\left(% \mathcal{P}^{(0)}\right)Cost ( caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT + over~ start_ARG caligraphic_F end_ARG , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) ≤ 3 ⋅ OPT start_POSTSUBSCRIPT italic_k + roman_ℓ + 1 end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ). By (20), Claim 5.6 and Claim 5.7, there exists a set ℱ~⊆𝐏~ℱ𝐏\tilde{\mathcal{F}}\subseteq\mathbf{P}over~ start_ARG caligraphic_F end_ARG ⊆ bold_P of m+ℓ+1≤2700⁢γ⋅(ℓ+1)𝑚ℓ1⋅2700𝛾ℓ1m+\ell+1\leq 2700\gamma\cdot(\ell+1)italic_m + roman_ℓ + 1 ≤ 2700 italic_γ ⋅ ( roman_ℓ + 1 ) centers such that Cost⁢(𝒰init+ℱ~,𝒫(0))≤3⋅OPTk+ℓ+1⁢(𝒫(0))Costsubscript𝒰init~ℱsuperscript𝒫0⋅3subscriptOPT𝑘ℓ1superscript𝒫0\textnormal{{Cost}}\left(\mathcal{U}_{\textnormal{{init}}}+\tilde{\mathcal{F}}% ,\mathcal{P}^{(0)}\right)\leq 3\cdot\textnormal{{OPT}}_{k+\ell+1}\left(% \mathcal{P}^{(0)}\right)Cost ( caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT + over~ start_ARG caligraphic_F end_ARG , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) ≤ 3 ⋅ OPT start_POSTSUBSCRIPT italic_k + roman_ℓ + 1 end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ). Accordingly, from (9), we get Cost⁢(𝒰init+ℱ⋆,𝒫(0))≤Cost⁢(𝒰init+ℱ~,𝒫(0))≤3⋅OPTk+ℓ+1⁢(𝒫(0))Costsubscript𝒰initsuperscriptℱ⋆superscript𝒫0Costsubscript𝒰init~ℱsuperscript𝒫0⋅3subscriptOPT𝑘ℓ1superscript𝒫0\textnormal{{Cost}}\left(\mathcal{U}_{\textnormal{{init}}}+\mathcal{F}^{\star}% ,\mathcal{P}^{(0)}\right)\leq\textnormal{{Cost}}\left(\mathcal{U}_{\textnormal% {{init}}}+\tilde{\mathcal{F}},\mathcal{P}^{(0)}\right)\leq 3\cdot\textnormal{{% OPT}}_{k+\ell+1}\left(\mathcal{P}^{(0)}\right)Cost ( caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT + caligraphic_F start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) ≤ Cost ( caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT + over~ start_ARG caligraphic_F end_ARG , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) ≤ 3 ⋅ OPT start_POSTSUBSCRIPT italic_k + roman_ℓ + 1 end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ), which implies Claim 5.5. It now remains to prove Claim 5.6 and Claim 5.7. Proof of Claim 5.6 We apply Lemma 4.2, by setting r=ℓ+1𝑟ℓ1r=\ell+1italic_r = roman_ℓ + 1, 𝒰=𝒰init𝒰subscript𝒰init\mathcal{U}=\mathcal{U}_{\textnormal{{init}}}caligraphic_U = caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT and 𝒫=𝒫(0)𝒫superscript𝒫0\mathcal{P}=\mathcal{P}^{(0)}caligraphic_P = caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT. This implies the existence of a set 𝒰~⊆𝒰init~𝒰subscript𝒰init\tilde{\mathcal{U}}\subseteq\mathcal{U}_{\textnormal{{init}}}over~ start_ARG caligraphic_U end_ARG ⊆ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT of at most (k−b)𝑘𝑏(k-b)( italic_k - italic_b ) centers, with b=⌊(m−ℓ−1)/4⌋𝑏𝑚ℓ14b=\lfloor(m-\ell-1)/4\rflooritalic_b = ⌊ ( italic_m - roman_ℓ - 1 ) / 4 ⌋, such that Cost⁢(𝒰~,𝒫(0))Cost~𝒰superscript𝒫0\displaystyle\textnormal{{Cost}}\left(\tilde{\mathcal{U}},\mathcal{P}^{(0)}\right)Cost ( over~ start_ARG caligraphic_U end_ARG , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) ≤\displaystyle\leq≤ 6⁢γ⋅(Cost⁢(𝒰init,𝒫(0))+Cost⁢(𝒱,𝒫(0)))⋅6𝛾Costsubscript𝒰initsuperscript𝒫0Cost𝒱superscript𝒫0\displaystyle 6\gamma\cdot\left(\textnormal{{Cost}}\left(\mathcal{U}_{% \textnormal{{init}}},\mathcal{P}^{(0)}\right)+\textnormal{{Cost}}\left(% \mathcal{V},\mathcal{P}^{(0)}\right)\right)6 italic_γ ⋅ ( Cost ( caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) + Cost ( caligraphic_V , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) ) ≤\displaystyle\leq≤ 6⁢γ⋅(8⋅OPTk⁢(𝒫(0))+OPTk+ℓ+1⁢(𝒫(0)))≤54⁢γ⋅OPTk⁢(𝒫(0)).⋅6𝛾⋅8subscriptOPT𝑘superscript𝒫0subscriptOPT𝑘ℓ1superscript𝒫0⋅54𝛾subscriptOPT𝑘superscript𝒫0\displaystyle 6\gamma\cdot\left(8\cdot\textnormal{{OPT}}_{k}\left(\mathcal{P}^% {(0)}\right)+\textnormal{{OPT}}_{k+\ell+1}\left(\mathcal{P}^{(0)}\right)\right% )\leq 54\gamma\cdot\textnormal{{OPT}}_{k}\left(\mathcal{P}^{(0)}\right).6 italic_γ ⋅ ( 8 ⋅ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) + OPT start_POSTSUBSCRIPT italic_k + roman_ℓ + 1 end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) ) ≤ 54 italic_γ ⋅ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) . In the above derivation, the second inequality follows from Invariant 5.2, and the last inequality holds because OPTk+ℓ+1⁢(𝒫(0))≤OPTk⁢(𝒫(0))subscriptOPT𝑘ℓ1superscript𝒫0subscriptOPT𝑘superscript𝒫0\textnormal{{OPT}}_{k+\ell+1}\left(\mathcal{P}^{(0)}\right)\leq\textnormal{{% OPT}}_{k}(\mathcal{P}^{(0)})OPT start_POSTSUBSCRIPT italic_k + roman_ℓ + 1 end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) ≤ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ). Since OPTk−b⁢(𝒫(0))≤Cost⁢(𝒰~,𝒫(0))subscriptOPT𝑘𝑏superscript𝒫0Cost~𝒰superscript𝒫0\textnormal{{OPT}}_{k-b}\left(\mathcal{P}^{(0)}\right)\leq\textnormal{{Cost}}% \left(\tilde{\mathcal{U}},\mathcal{P}^{(0)}\right)OPT start_POSTSUBSCRIPT italic_k - italic_b end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) ≤ Cost ( over~ start_ARG caligraphic_U end_ARG , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ), we get OPTk−b⁢(𝒫(0))≤54⁢γ⋅OPTk⁢(𝒫(0)).subscriptOPT𝑘𝑏superscript𝒫0⋅54𝛾subscriptOPT𝑘superscript𝒫0\textnormal{{OPT}}_{k-b}\left(\mathcal{P}^{(0)}\right)\leq 54\gamma\cdot% \textnormal{{OPT}}_{k}\left(\mathcal{P}^{(0)}\right).OPT start_POSTSUBSCRIPT italic_k - italic_b end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) ≤ 54 italic_γ ⋅ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) . (21) Recall the way we defined ℓ,ℓ⋆ℓsuperscriptℓ⋆\ell,\ell^{\star}roman_ℓ , roman_ℓ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT at Step 1 in Section 5.1. From (21), it follows that b≤ℓ⋆𝑏superscriptℓ⋆b\leq\ell^{\star}italic_b ≤ roman_ℓ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT. Since b≥m−ℓ−14−1=m−ℓ−54𝑏𝑚ℓ141𝑚ℓ54b\geq\frac{m-\ell-1}{4}-1=\frac{m-\ell-5}{4}italic_b ≥ divide start_ARG italic_m - roman_ℓ - 1 end_ARG start_ARG 4 end_ARG - 1 = divide start_ARG italic_m - roman_ℓ - 5 end_ARG start_ARG 4 end_ARG and ℓ≥ℓ⋆12⋅54⁢γ−1ℓsuperscriptℓ⋆⋅1254𝛾1\ell\geq\frac{\ell^{\star}}{12\cdot 54\gamma}-1roman_ℓ ≥ divide start_ARG roman_ℓ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT end_ARG start_ARG 12 ⋅ 54 italic_γ end_ARG - 1, we get m−ℓ−54≤12⋅54⁢γ⋅(ℓ+1), and hence ⁢m≤(2592⁢γ+1)⁢(ℓ+1)+4≤2600⁢γ⋅(ℓ+1)formulae-sequence𝑚ℓ54⋅⋅1254𝛾ℓ1 and hence 𝑚2592𝛾1ℓ14⋅2600𝛾ℓ1\frac{m-\ell-5}{4}\leq 12\cdot 54\gamma\cdot(\ell+1),\text{ and hence }m\leq(2% 592\gamma+1)(\ell+1)+4\leq 2600\gamma\cdot(\ell+1)divide start_ARG italic_m - roman_ℓ - 5 end_ARG start_ARG 4 end_ARG ≤ 12 ⋅ 54 italic_γ ⋅ ( roman_ℓ + 1 ) , and hence italic_m ≤ ( 2592 italic_γ + 1 ) ( roman_ℓ + 1 ) + 4 ≤ 2600 italic_γ ⋅ ( roman_ℓ + 1 ). This concludes the proof of the claim. Proof of Claim 5.7 We define assignment σ:𝒫(0)→𝒰init+ℱ~:𝜎→superscript𝒫0subscript𝒰init~ℱ\sigma:\mathcal{P}^{(0)}\rightarrow\mathcal{U}_{\textnormal{{init}}}+\tilde{% \mathcal{F}}italic_σ : caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT → caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT + over~ start_ARG caligraphic_F end_ARG, as follows.111111Recall the notations Cu⁢(𝒰,𝒫)subscript𝐶𝑢𝒰𝒫C_{u}(\mathcal{U},\mathcal{P})italic_C start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT ( caligraphic_U , caligraphic_P ) and π𝒰⁢(p)subscript𝜋𝒰𝑝\pi_{\mathcal{U}}(p)italic_π start_POSTSUBSCRIPT caligraphic_U end_POSTSUBSCRIPT ( italic_p ) from Section 3.1. Consider any point p∈𝒫(0)𝑝superscript𝒫0p\in\mathcal{P}^{(0)}italic_p ∈ caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT. • If p∈Cvi⁢(𝒱,𝒫(0))𝑝subscript𝐶subscript𝑣𝑖𝒱superscript𝒫0p\in C_{v_{i}}\left(\mathcal{V},\mathcal{P}^{(0)}\right)italic_p ∈ italic_C start_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( caligraphic_V , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) for some i∈[1,k−m]𝑖1𝑘𝑚i\in[1,k-m]italic_i ∈ [ 1 , italic_k - italic_m ], then σ⁢(p):=uiassign𝜎𝑝subscript𝑢𝑖\sigma(p):=u_{i}italic_σ ( italic_p ) := italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. • Otherwise, σ⁢(p):=π𝒱⁢(p)assign𝜎𝑝subscript𝜋𝒱𝑝\sigma(p):=\pi_{\mathcal{V}}(p)italic_σ ( italic_p ) := italic_π start_POSTSUBSCRIPT caligraphic_V end_POSTSUBSCRIPT ( italic_p ). In words, for every well-separated pair (ui,vi)∈𝒰init×𝒱subscript𝑢𝑖subscript𝑣𝑖subscript𝒰init𝒱(u_{i},v_{i})\in\mathcal{U}_{\textnormal{{init}}}\times\mathcal{V}( italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ∈ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT × caligraphic_V all the points in the cluster of visubscript𝑣𝑖v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT get reassigned to the center uisubscript𝑢𝑖u_{i}italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, and the assignment of all other points remain unchanged (note that their assigned centers are present in 𝒰init+ℱ~subscript𝒰init~ℱ\mathcal{U}_{\textnormal{{init}}}+\tilde{\mathcal{F}}caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT + over~ start_ARG caligraphic_F end_ARG as well as 𝒱𝒱\mathcal{V}caligraphic_V). Now, recall that the set of centers 𝒰initsubscript𝒰init\mathcal{U}_{\textnormal{{init}}}caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT is robust (see Invariant 5.2). Hence, by applying Lemma 3.9, we infer that Cost⁢(𝒰init+ℱ~,𝒫(0))≤∑p∈𝒫(0)d⁢(p,σ⁢(p))≤3⋅Cost⁢(𝒱,𝒫(0)).Costsubscript𝒰init~ℱsuperscript𝒫0subscript𝑝superscript𝒫0𝑑𝑝𝜎𝑝⋅3Cost𝒱superscript𝒫0\textnormal{{Cost}}\left(\mathcal{U}_{\textnormal{{init}}}+\tilde{\mathcal{F}}% ,\mathcal{P}^{(0)}\right)\leq\sum_{p\in\mathcal{P}^{(0)}}d(p,\sigma(p))\leq 3% \cdot\textnormal{{Cost}}\left(\mathcal{V},\mathcal{P}^{(0)}\right).Cost ( caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT + over~ start_ARG caligraphic_F end_ARG , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) ≤ ∑ start_POSTSUBSCRIPT italic_p ∈ caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_d ( italic_p , italic_σ ( italic_p ) ) ≤ 3 ⋅ Cost ( caligraphic_V , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) . The claim follows since Cost⁢(𝒱,𝒫(0))=OPTk+ℓ+1⁢(𝒫(0))Cost𝒱superscript𝒫0subscriptOPT𝑘ℓ1superscript𝒫0\textnormal{{Cost}}\left(\mathcal{V},\mathcal{P}^{(0)}\right)=\textnormal{{OPT% }}_{k+\ell+1}\left(\mathcal{P}^{(0)}\right)Cost ( caligraphic_V , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) = OPT start_POSTSUBSCRIPT italic_k + roman_ℓ + 1 end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) by definition. 5.3 Analyzing the Recourse Recall the description of our algorithm from Section 5.1, and consider a given epoch (say) ℰℰ\mathcal{E}caligraphic_E of length (ℓ+1)ℓ1(\ell+1)( roman_ℓ + 1 ). The total recourse incurred by the algorithm during this epoch is Rℰ≤|𝒰init⊕𝒰(0)|+(∑t=1ℓ+1|𝒰(t)⊕𝒰(t−1)|)+|𝒰(ℓ+1)⊕𝒰final|.subscript𝑅ℰdirect-sumsubscript𝒰initsuperscript𝒰0superscriptsubscript𝑡1ℓ1direct-sumsuperscript𝒰𝑡superscript𝒰𝑡1direct-sumsuperscript𝒰ℓ1subscript𝒰finalR_{\mathcal{E}}\leq\left|\mathcal{U}_{\textnormal{{init}}}\oplus\mathcal{U}^{(% 0)}\right|+\left(\sum_{t=1}^{\ell+1}\left|\mathcal{U}^{(t)}\oplus\mathcal{U}^{% (t-1)}\right|\right)+\left|\mathcal{U}^{(\ell+1)}\oplus\mathcal{U}_{% \textnormal{{final}}}\right|.italic_R start_POSTSUBSCRIPT caligraphic_E end_POSTSUBSCRIPT ≤ | caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ⊕ caligraphic_U start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT | + ( ∑ start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ℓ + 1 end_POSTSUPERSCRIPT | caligraphic_U start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT ⊕ caligraphic_U start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT | ) + | caligraphic_U start_POSTSUPERSCRIPT ( roman_ℓ + 1 ) end_POSTSUPERSCRIPT ⊕ caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT | . (22) We will now bound each term on the right hand side of (22). Towards this end, recall that 𝒰(0)superscript𝒰0\mathcal{U}^{(0)}caligraphic_U start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT is obtained by deleting ℓℓ\ellroman_ℓ centers from 𝒰initsubscript𝒰init\mathcal{U}_{\textnormal{{init}}}caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT, and hence we have |𝒰init⊕𝒰(0)|=ℓdirect-sumsubscript𝒰initsuperscript𝒰0ℓ\left|\mathcal{U}_{\textnormal{{init}}}\oplus\mathcal{U}^{(0)}\right|=\ell| caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ⊕ caligraphic_U start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT | = roman_ℓ. Next, it is easy to verify that in Step 3 (see Section 5.1) we incur a worst-case recourse of at most one per update. Specifically, we have |𝒰(t)⊕𝒰(t−1)|≤1direct-sumsuperscript𝒰𝑡superscript𝒰𝑡11\left|\mathcal{U}^{(t)}\oplus\mathcal{U}^{(t-1)}\right|\leq 1| caligraphic_U start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT ⊕ caligraphic_U start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT | ≤ 1 for all t∈[1,ℓ+1]𝑡1ℓ1t\in[1,\ell+1]italic_t ∈ [ 1 , roman_ℓ + 1 ], and hence |𝒰(ℓ+1)⊕𝒰init|≤|𝒰(ℓ+1)⊕𝒰(0)|+|𝒰(0)⊕𝒰init|≤(ℓ+1)+ℓ=2⁢ℓ+1direct-sumsuperscript𝒰ℓ1subscript𝒰initdirect-sumsuperscript𝒰ℓ1superscript𝒰0direct-sumsuperscript𝒰0subscript𝒰initℓ1ℓ2ℓ1\left|\mathcal{U}^{(\ell+1)}\oplus\mathcal{U}_{\textnormal{{init}}}\right|\leq% \left|\mathcal{U}^{(\ell+1)}\oplus\mathcal{U}^{(0)}\right|+\left|\mathcal{U}^{% (0)}\oplus\mathcal{U}_{\textnormal{{init}}}\right|\leq(\ell+1)+\ell=2\ell+1| caligraphic_U start_POSTSUPERSCRIPT ( roman_ℓ + 1 ) end_POSTSUPERSCRIPT ⊕ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT | ≤ | caligraphic_U start_POSTSUPERSCRIPT ( roman_ℓ + 1 ) end_POSTSUPERSCRIPT ⊕ caligraphic_U start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT | + | caligraphic_U start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ⊕ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT | ≤ ( roman_ℓ + 1 ) + roman_ℓ = 2 roman_ℓ + 1. Thus, from (22) we get: Rℰsubscript𝑅ℰ\displaystyle R_{\mathcal{E}}italic_R start_POSTSUBSCRIPT caligraphic_E end_POSTSUBSCRIPT ≤\displaystyle\leq≤ ℓ+(ℓ+1)+|𝒰(ℓ+1)⊕𝒰final|ℓℓ1direct-sumsuperscript𝒰ℓ1subscript𝒰final\displaystyle\ell+(\ell+1)+\left|\mathcal{U}^{(\ell+1)}\oplus\mathcal{U}_{% \textnormal{{final}}}\right|roman_ℓ + ( roman_ℓ + 1 ) + | caligraphic_U start_POSTSUPERSCRIPT ( roman_ℓ + 1 ) end_POSTSUPERSCRIPT ⊕ caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT | (23) ≤\displaystyle\leq≤ (2⁢ℓ+1)+|𝒰(ℓ+1)⊕𝒰init|+|𝒰init⊕𝒰final|2ℓ1direct-sumsuperscript𝒰ℓ1subscript𝒰initdirect-sumsubscript𝒰initsubscript𝒰final\displaystyle(2\ell+1)+\left|\mathcal{U}^{(\ell+1)}\oplus\mathcal{U}_{% \textnormal{{init}}}\right|+\left|\mathcal{U}_{\textnormal{{init}}}\oplus% \mathcal{U}_{\textnormal{{final}}}\right|( 2 roman_ℓ + 1 ) + | caligraphic_U start_POSTSUPERSCRIPT ( roman_ℓ + 1 ) end_POSTSUPERSCRIPT ⊕ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT | + | caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ⊕ caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT | ≤\displaystyle\leq≤ (4⁢ℓ+2)+|𝒰init⊕𝒲⋆|+|𝒲⋆⊕𝒰final|4ℓ2direct-sumsubscript𝒰initsuperscript𝒲⋆direct-sumsuperscript𝒲⋆subscript𝒰final\displaystyle(4\ell+2)+\left|\mathcal{U}_{\textnormal{{init}}}\oplus\mathcal{W% }^{\star}\right|+\left|\mathcal{W}^{\star}\oplus\mathcal{U}_{\textnormal{{% final}}}\right|( 4 roman_ℓ + 2 ) + | caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ⊕ caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT | + | caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ⊕ caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT | =\displaystyle== O⁢(ℓ+1)+|𝒲⋆⊕𝒰final|.𝑂ℓ1direct-sumsuperscript𝒲⋆subscript𝒰final\displaystyle O(\ell+1)+\left|\mathcal{W}^{\star}\oplus\mathcal{U}_{% \textnormal{{final}}}\right|.italic_O ( roman_ℓ + 1 ) + | caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ⊕ caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT | . In the above derivation, the last step follows from Corollary 5.3. Since the epoch lasts for ℓ+1ℓ1\ell+1roman_ℓ + 1 updates, the O⁢(ℓ+1)𝑂ℓ1O(\ell+1)italic_O ( roman_ℓ + 1 ) term in the right hand side of (23) contributes an amortized recourse of O⁢(1)𝑂1O(1)italic_O ( 1 ). Moreover, the term |𝒲⋆⊕𝒰final|direct-sumsuperscript𝒲⋆subscript𝒰final\left|\mathcal{W}^{\star}\oplus\mathcal{U}_{\textnormal{{final}}}\right|| caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ⊕ caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT | is proportional to the number of Make-Robust⁢(⋅,⋅)Make-Robust⋅⋅\textsc{Make-Robust}(\cdot\,,\cdot)Make-Robust ( ⋅ , ⋅ ) calls made while computing 𝒰final←Robustify⁢(𝒲⋆)←subscript𝒰finalRobustifysuperscript𝒲⋆\mathcal{U}_{\textnormal{{final}}}\leftarrow\textsc{Robustify}\left(\mathcal{W% }^{\star}\right)caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT ← Robustify ( caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ). So, the recourse of our algorithm is dominated by the number of calls made to the Make-Robust⁢(⋅,⋅)Make-Robust⋅⋅\textsc{Make-Robust}(\cdot\,,\cdot)Make-Robust ( ⋅ , ⋅ ) subroutine, and Lemma 5.8 implies that our algorithm has an amortized recourse of O⁢(log2⁡Δ)𝑂superscript2ΔO(\log^{2}\Delta)italic_O ( roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT roman_Δ ). Lemma 5.8. The dynamic algorithm from Section 5.1 makes at most O⁢(log2⁡Δ)𝑂superscript2ΔO(\log^{2}\Delta)italic_O ( roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT roman_Δ ) many calls to Make-Robust⁢(⋅,⋅)Make-Robust⋅⋅\textsc{Make-Robust}(\cdot\,,\cdot)Make-Robust ( ⋅ , ⋅ ), amortized over the entire sequence of updates (spanning multiple epochs). We devote the rest of this section towards proving Lemma 5.8. Contaminated vs Clean Centers. Focus on the scenario at the start of a given epoch (see Section 5.1). By Invariant 5.2, the set 𝒰initsubscript𝒰init\mathcal{U}_{\textnormal{{init}}}caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT is robust w.r.t. 𝒫(0)superscript𝒫0\mathcal{P}^{(0)}caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT. For each center u∈𝒰init𝑢subscript𝒰initu\in\mathcal{U}_{\textnormal{{init}}}italic_u ∈ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT, we maintain an integer t⁢[u]𝑡delimited-[]𝑢t[u]italic_t [ italic_u ] such that: (i) u𝑢uitalic_u is t⁢[u]𝑡delimited-[]𝑢t[u]italic_t [ italic_u ] robust w.r.t. 𝒫(0)superscript𝒫0\mathcal{P}^{(0)}caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT, and (ii) 10t⁢[u]≥d⁢(u,𝒰init−u)/200superscript10𝑡delimited-[]𝑢𝑑𝑢subscript𝒰init𝑢20010^{t[u]}\geq d(u,\mathcal{U}_{\textnormal{{init}}}-u)/20010 start_POSTSUPERSCRIPT italic_t [ italic_u ] end_POSTSUPERSCRIPT ≥ italic_d ( italic_u , caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT - italic_u ) / 200. The existence of t⁢[u]𝑡delimited-[]𝑢t[u]italic_t [ italic_u ] is guaranteed by Definition 3.5.121212We use t⁢[u]𝑡delimited-[]𝑢t[u]italic_t [ italic_u ] only for the sake of analyzing recourse. Here, the actual algorithm remains the same as in Section 5.1. But, we make use of these integers in Section 6 to get fast update time. Let (p0⁢(u),p1⁢(u),…,pt⁢[u]⁢(u))subscript𝑝0𝑢subscript𝑝1𝑢…subscript𝑝𝑡delimited-[]𝑢𝑢(p_{0}(u),p_{1}(u),\ldots,p_{t[u]}(u))( italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( italic_u ) , italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_u ) , … , italic_p start_POSTSUBSCRIPT italic_t [ italic_u ] end_POSTSUBSCRIPT ( italic_u ) ) be the t⁢[u]𝑡delimited-[]𝑢t[u]italic_t [ italic_u ]-robust sequence w.r.t. 𝒫(0)superscript𝒫0\mathcal{P}^{(0)}caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT corresponding to u𝑢uitalic_u (i.e., u=p0⁢(u)𝑢subscript𝑝0𝑢u=p_{0}(u)italic_u = italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( italic_u )), and for each i∈[1,t⁢[u]]𝑖1𝑡delimited-[]𝑢i\in[1,t[u]]italic_i ∈ [ 1 , italic_t [ italic_u ] ], let Bi⁢(u)=Ball10i𝒫(0)⁢(pi⁢(u))subscript𝐵𝑖𝑢superscriptsubscriptBallsuperscript10𝑖superscript𝒫0subscript𝑝𝑖𝑢B_{i}(u)=\text{Ball}_{10^{i}}^{\mathcal{P}^{(0)}}(p_{i}(u))italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_u ) = Ball start_POSTSUBSCRIPT 10 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT ( italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_u ) ). Recall that by Lemma 3.3, we have B1⁢(u)⊆B2⁢(u)⊆⋯⊆Bt⁢[u]⁢(u)subscript𝐵1𝑢subscript𝐵2𝑢⋯subscript𝐵𝑡delimited-[]𝑢𝑢B_{1}(u)\subseteq B_{2}(u)\subseteq\cdots\subseteq B_{t[u]}(u)italic_B start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_u ) ⊆ italic_B start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_u ) ⊆ ⋯ ⊆ italic_B start_POSTSUBSCRIPT italic_t [ italic_u ] end_POSTSUBSCRIPT ( italic_u ). Now, consider any j∈[1,ℓ+1]𝑗1ℓ1j\in[1,\ell+1]italic_j ∈ [ 1 , roman_ℓ + 1 ], and let qj∈𝐏subscript𝑞𝑗𝐏q_{j}\in\mathbf{P}italic_q start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ∈ bold_P denote the point being inserted/deleted in 𝒫𝒫\mathcal{P}caligraphic_P during the jt⁢hsuperscript𝑗𝑡ℎj^{th}italic_j start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT update in the epoch, i.e., 𝒫(j)⊕𝒫(j−1)={qj}direct-sumsuperscript𝒫𝑗superscript𝒫𝑗1subscript𝑞𝑗\mathcal{P}^{(j)}\oplus\mathcal{P}^{(j-1)}=\{q_{j}\}caligraphic_P start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT ⊕ caligraphic_P start_POSTSUPERSCRIPT ( italic_j - 1 ) end_POSTSUPERSCRIPT = { italic_q start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT }. We say that this jt⁢hsuperscript𝑗𝑡ℎj^{th}italic_j start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT update contaminates a center u∈𝒰init𝑢subscript𝒰initu\in\mathcal{U}_{\textnormal{{init}}}italic_u ∈ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT iff d⁢(qj,pt⁢[u]⁢(u))≤10t⁢[u]𝑑subscript𝑞𝑗subscript𝑝𝑡delimited-[]𝑢𝑢superscript10𝑡delimited-[]𝑢d(q_{j},p_{t[u]}(u))\leq 10^{t[u]}italic_d ( italic_q start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT italic_t [ italic_u ] end_POSTSUBSCRIPT ( italic_u ) ) ≤ 10 start_POSTSUPERSCRIPT italic_t [ italic_u ] end_POSTSUPERSCRIPT. Intuitively, this means that if this jt⁢hsuperscript𝑗𝑡ℎj^{th}italic_j start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT update were taken into account while defining the balls {Bi⁢(u)}isubscriptsubscript𝐵𝑖𝑢𝑖\{B_{i}(u)\}_{i}{ italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_u ) } start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT at the start of the epoch, then it might have impacted our decision to classify u𝑢uitalic_u as being t⁢[u]𝑡delimited-[]𝑢t[u]italic_t [ italic_u ]-robust at that time. Furthermore, we say that the center u∈𝒰init𝑢subscript𝒰initu\in\mathcal{U}_{\textnormal{{init}}}italic_u ∈ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT is clean at the end of the epoch if no update j∈[1,ℓ+1]𝑗1ℓ1j\in[1,\ell+1]italic_j ∈ [ 1 , roman_ℓ + 1 ] contaminated it (i.e. Bt⁢[u]⁢(u)subscript𝐵𝑡delimited-[]𝑢𝑢B_{t[u]}(u)italic_B start_POSTSUBSCRIPT italic_t [ italic_u ] end_POSTSUBSCRIPT ( italic_u ) and accordingly all of the balls Bi⁢(u)subscript𝐵𝑖𝑢B_{i}(u)italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_u ) remain intact during the updates in this epoch); otherwise we say that the center u𝑢uitalic_u is contaminated at the end of the epoch. In the two claims below, we summarize a few key properties of clean and contaminated centers. We defer the proof of Claim 5.10 to Section 5.3.1. Claim 5.9. If u∈𝒰init𝑢subscript𝒰initu\in\mathcal{U}_{\textnormal{{init}}}italic_u ∈ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT is clean at the end of the epoch, then u𝑢uitalic_u is t⁢[u]𝑡delimited-[]𝑢t[u]italic_t [ italic_u ]-robust w.r.t. 𝒫(ℓ+1)superscript𝒫ℓ1\mathcal{P}^{(\ell+1)}caligraphic_P start_POSTSUPERSCRIPT ( roman_ℓ + 1 ) end_POSTSUPERSCRIPT. Proof. Let u∈𝒰init𝑢subscript𝒰initu\in\mathcal{U}_{\textnormal{{init}}}italic_u ∈ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT be a clean center at the end of the epoch. So, during the epoch no point was inserted into/deleted from Bt⁢[u]subscript𝐵𝑡delimited-[]𝑢B_{t[u]}italic_B start_POSTSUBSCRIPT italic_t [ italic_u ] end_POSTSUBSCRIPT. As B1⁢(u)⊆B2⁢(u)⊆⋯⊆Bt⁢[u]⁢(u)subscript𝐵1𝑢subscript𝐵2𝑢⋯subscript𝐵𝑡delimited-[]𝑢𝑢B_{1}(u)\subseteq B_{2}(u)\subseteq\cdots\subseteq B_{t[u]}(u)italic_B start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_u ) ⊆ italic_B start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_u ) ⊆ ⋯ ⊆ italic_B start_POSTSUBSCRIPT italic_t [ italic_u ] end_POSTSUBSCRIPT ( italic_u ) by Lemma 3.3, this implies that during the epoch no point was inserted into/deleted from any of the balls B1⁢(u),B2⁢(u),…,Bt⁢[u]⁢(u)subscript𝐵1𝑢subscript𝐵2𝑢…subscript𝐵𝑡delimited-[]𝑢𝑢B_{1}(u),B_{2}(u),\ldots,B_{t[u]}(u)italic_B start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_u ) , italic_B start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_u ) , … , italic_B start_POSTSUBSCRIPT italic_t [ italic_u ] end_POSTSUBSCRIPT ( italic_u ). Thus, we have Ball10i𝒫(0)⁢(pi⁢(u))=Ball10i𝒫(ℓ+1)⁢(pi⁢(u))superscriptsubscriptBallsuperscript10𝑖superscript𝒫0subscript𝑝𝑖𝑢superscriptsubscriptBallsuperscript10𝑖superscript𝒫ℓ1subscript𝑝𝑖𝑢\text{Ball}_{10^{i}}^{\mathcal{P}^{(0)}}(p_{i}(u))=\text{Ball}_{10^{i}}^{% \mathcal{P}^{(\ell+1)}}(p_{i}(u))Ball start_POSTSUBSCRIPT 10 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT ( italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_u ) ) = Ball start_POSTSUBSCRIPT 10 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT caligraphic_P start_POSTSUPERSCRIPT ( roman_ℓ + 1 ) end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT ( italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_u ) ) for all i∈[0,ℓ+1]𝑖0ℓ1i\in[0,\ell+1]italic_i ∈ [ 0 , roman_ℓ + 1 ]. The claim now follows from Definition 3.2. ∎ Claim 5.10. Each update during the epoch contaminates at most O⁢(log⁡Δ)𝑂ΔO(\log\Delta)italic_O ( roman_log roman_Δ ) centers in 𝒰initsubscript𝒰init\mathcal{U}_{\textnormal{{init}}}caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT. At the end of the epoch, we set 𝒰final←Robustify⁢(𝒲⋆)←subscript𝒰finalRobustifysuperscript𝒲⋆\mathcal{U}_{\textnormal{{final}}}\leftarrow\textsc{Robustify}\left(\mathcal{W% }^{\star}\right)caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT ← Robustify ( caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ). By Lemma 3.6, the subroutine Robustify⁢(𝒲⋆)Robustifysuperscript𝒲⋆\textsc{Robustify}\left(\mathcal{W}^{\star}\right)Robustify ( caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) makes at most one call to Make-Robust⁢(w,⋅)Make-Robust𝑤⋅\textsc{Make-Robust}(w,\cdot)Make-Robust ( italic_w , ⋅ ) for each point w∈𝒲⋆𝑤superscript𝒲⋆w\in\mathcal{W}^{\star}italic_w ∈ caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT, and zero call to Make-Robust⁢(w,⋅)Make-Robust𝑤⋅\textsc{Make-Robust}(w,\cdot)Make-Robust ( italic_w , ⋅ ) for each point w∈𝐏∖𝒲⋆𝑤𝐏superscript𝒲⋆w\in\mathbf{P}\setminus\mathcal{W}^{\star}italic_w ∈ bold_P ∖ caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT. Accordingly, we can partition the calls to Make-Robust⁢(⋅,⋅)Make-Robust⋅⋅\textsc{Make-Robust}(\cdot\,,\cdot)Make-Robust ( ⋅ , ⋅ ) that are made by Robustify⁢(𝒲⋆)Robustifysuperscript𝒲⋆\textsc{Robustify}\left(\mathcal{W}^{\star}\right)Robustify ( caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) into the following three types. Type I. A call to Make-Robust⁢(w,⋅)Make-Robust𝑤⋅\textsc{Make-Robust}(w,\cdot)Make-Robust ( italic_w , ⋅ ) for some w∈𝒲⋆∖𝒰init𝑤superscript𝒲⋆subscript𝒰initw\in\mathcal{W}^{\star}\setminus\mathcal{U}_{\textnormal{{init}}}italic_w ∈ caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ∖ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT. The total number of such calls is at most |𝒲⋆∖𝒰init|≤|𝒲⋆⊕𝒰init|=O⁢(ℓ+1)superscript𝒲⋆subscript𝒰initdirect-sumsuperscript𝒲⋆subscript𝒰init𝑂ℓ1\left|\mathcal{W}^{\star}\setminus\mathcal{U}_{\textnormal{{init}}}\right|\leq% \left|\mathcal{W}^{\star}\oplus\mathcal{U}_{\textnormal{{init}}}\right|=O(\ell% +1)| caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ∖ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT | ≤ | caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ⊕ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT | = italic_O ( roman_ℓ + 1 ) (see Corollary 5.3). Since the epoch lasts for (ℓ+1)ℓ1(\ell+1)( roman_ℓ + 1 ) updates, the amortized number of Type I calls to Make-Robust⁢(⋅,⋅)Make-Robust⋅⋅\textsc{Make-Robust}(\cdot\,,\cdot)Make-Robust ( ⋅ , ⋅ ), per update, is O⁢(1)𝑂1O(1)italic_O ( 1 ). Type II. A call to Make-Robust⁢(w,⋅)Make-Robust𝑤⋅\textsc{Make-Robust}(w,\cdot)Make-Robust ( italic_w , ⋅ ) for some w∈𝒲⋆∩𝒰init𝑤superscript𝒲⋆subscript𝒰initw\in\mathcal{W}^{\star}\cap\mathcal{U}_{\textnormal{{init}}}italic_w ∈ caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ∩ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT that is contaminated at the end of the epoch. By Claim 5.10, the amortized number of such Type II Make-Robust⁢(⋅,⋅)Make-Robust⋅⋅\textsc{Make-Robust}(\cdot\,,\cdot)Make-Robust ( ⋅ , ⋅ ) calls, per update, is O⁢(log⁡Δ)𝑂ΔO(\log\Delta)italic_O ( roman_log roman_Δ ). Type III. A call to Make-Robust⁢(w,⋅)Make-Robust𝑤⋅\textsc{Make-Robust}(w,\cdot)Make-Robust ( italic_w , ⋅ ) for some w∈𝒲⋆∩𝒰init𝑤superscript𝒲⋆subscript𝒰initw\in\mathcal{W}^{\star}\cap\mathcal{U}_{\textnormal{{init}}}italic_w ∈ caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ∩ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT that is clean at the end of the epoch. Recall that the center w𝑤witalic_w was t⁢[w]𝑡delimited-[]𝑤t[w]italic_t [ italic_w ]-robust w.r.t. 𝒫(0)superscript𝒫0\mathcal{P}^{(0)}caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT at the start of the epoch, and by Claim 5.9 it remains t⁢[w]𝑡delimited-[]𝑤t[w]italic_t [ italic_w ]-robust w.r.t. 𝒫(ℓ+1)superscript𝒫ℓ1\mathcal{P}^{(\ell+1)}caligraphic_P start_POSTSUPERSCRIPT ( roman_ℓ + 1 ) end_POSTSUPERSCRIPT at the end of the epoch. Furthermore, note that if a center is t′superscript𝑡′t^{\prime}italic_t start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT-robust, then it is also t′′superscript𝑡′′t^{\prime\prime}italic_t start_POSTSUPERSCRIPT ′ ′ end_POSTSUPERSCRIPT-robust for all t′′≤t′superscript𝑡′′superscript𝑡′t^{\prime\prime}\leq t^{\prime}italic_t start_POSTSUPERSCRIPT ′ ′ end_POSTSUPERSCRIPT ≤ italic_t start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT. Thus, at the end of the epoch, the call to Make-Robust⁢(w,t)Make-Robust𝑤𝑡\textsc{Make-Robust}(w,t)Make-Robust ( italic_w , italic_t ) could have been made for only one reason: The subroutine Robustify⁢(𝒲⋆)Robustifysuperscript𝒲⋆\textsc{Robustify}\left(\mathcal{W}^{\star}\right)Robustify ( caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) wanted to ensure that w𝑤witalic_w was t𝑡titalic_t-robust w.r.t. 𝒫(ℓ+1)superscript𝒫ℓ1\mathcal{P}^{(\ell+1)}caligraphic_P start_POSTSUPERSCRIPT ( roman_ℓ + 1 ) end_POSTSUPERSCRIPT for some t>t⁢[w]𝑡𝑡delimited-[]𝑤t>t[w]italic_t > italic_t [ italic_w ], but it was not the case. Suppose that w′←Make-Robust⁢(w,t)←superscript𝑤′Make-Robust𝑤𝑡w^{\prime}\leftarrow\textsc{Make-Robust}(w,t)italic_w start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ← Make-Robust ( italic_w , italic_t ) was the center returned by this call to Make-Robust⁢(⋅,⋅)Make-Robust⋅⋅\textsc{Make-Robust}(\cdot\,,\cdot)Make-Robust ( ⋅ , ⋅ ). Then at the end of this call, we set t⁢[w′]←t←𝑡delimited-[]superscript𝑤′𝑡t[w^{\prime}]\leftarrow titalic_t [ italic_w start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ] ← italic_t. Clearly, we have t⁢[w′]>t⁢[w]𝑡delimited-[]superscript𝑤′𝑡delimited-[]𝑤t[w^{\prime}]>t[w]italic_t [ italic_w start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ] > italic_t [ italic_w ]. To bound the amortized number of Type III calls, we need to invoke a more “global” argument, that spans across multiple epochs. Consider a maximal “chain” of j𝑗jitalic_j many Type III calls (possibly spanning across multiple different epochs), in increasing order of time: w1←Make-Robust⁢(w0,t⁢[w1]),w2←Make-Robust⁢(w1,t⁢[w2]),formulae-sequence←subscript𝑤1Make-Robustsubscript𝑤0𝑡delimited-[]subscript𝑤1←subscript𝑤2Make-Robustsubscript𝑤1𝑡delimited-[]subscript𝑤2\displaystyle w_{1}\leftarrow\textsc{Make-Robust}(w_{0},t[w_{1}]),w_{2}% \leftarrow\textsc{Make-Robust}(w_{1},t[w_{2}]),italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ← Make-Robust ( italic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_t [ italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ] ) , italic_w start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ← Make-Robust ( italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_t [ italic_w start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ] ) , …,wj←Make-Robust⁢(wj−1,t⁢[wj]).←…subscript𝑤𝑗Make-Robustsubscript𝑤𝑗1𝑡delimited-[]subscript𝑤𝑗\displaystyle\ldots,w_{j}\leftarrow\textsc{Make-Robust}(w_{j-1},t[w_{j}]).… , italic_w start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ← Make-Robust ( italic_w start_POSTSUBSCRIPT italic_j - 1 end_POSTSUBSCRIPT , italic_t [ italic_w start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ] ) . Note that the calls in the above chain can be interspersed with other Type I, Type II or Type III calls that are not part of the chain. Still, from the above discussion, we get 0≤t⁢[w0]<t⁢[w1]<⋯<t⁢[wj]≤⌈log⁡Δ⌉0𝑡delimited-[]subscript𝑤0𝑡delimited-[]subscript𝑤1⋯𝑡delimited-[]subscript𝑤𝑗Δ0\leq t[w_{0}]<t[w_{1}]<\cdots<t[w_{j}]\leq\lceil\log\Delta\rceil0 ≤ italic_t [ italic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ] < italic_t [ italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ] < ⋯ < italic_t [ italic_w start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ] ≤ ⌈ roman_log roman_Δ ⌉. So, the chain has length at most O⁢(log⁡Δ)𝑂ΔO(\log\Delta)italic_O ( roman_log roman_Δ ). Also, for the chain to start in the first place, we must have had a Type I or Type II call to Make-Robust⁢(⋅,⋅)Make-Robust⋅⋅\textsc{Make-Robust}(\cdot\,,\cdot)Make-Robust ( ⋅ , ⋅ ) which returned the center w0subscript𝑤0w_{0}italic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. We can thus “charge” the length (total number of Type III calls) in this chain to the Type I or Type II call that triggered it (by returning the center w0subscript𝑤0w_{0}italic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT). In other words, the total number of Type III calls ever made is at most O⁢(log⁡Δ)𝑂ΔO(\log\Delta)italic_O ( roman_log roman_Δ ) times the total number of Type I plus Type II calls. Since the amortized number of Type I and Type II calls per update is O⁢(log⁡Δ)𝑂ΔO(\log\Delta)italic_O ( roman_log roman_Δ ), the amortized number of Type III calls per update is O⁢(log2⁡Δ)𝑂superscript2ΔO(\log^{2}\Delta)italic_O ( roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT roman_Δ ). This concludes the proof of Lemma 5.8. 5.3.1 Proof of Claim 5.10 Assume p∈𝒫(0)⊕𝒫(ℓ+1)𝑝direct-sumsuperscript𝒫0superscript𝒫ℓ1p\in\mathcal{P}^{(0)}\oplus\mathcal{P}^{(\ell+1)}italic_p ∈ caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ⊕ caligraphic_P start_POSTSUPERSCRIPT ( roman_ℓ + 1 ) end_POSTSUPERSCRIPT is an updated point during the epoch. Let u∈𝒰init𝑢subscript𝒰initu\in\mathcal{U}_{\textnormal{{init}}}italic_u ∈ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT be contaminated by p𝑝pitalic_p, i.e., d⁢(p,pt⁢[u]⁢(u))≤10t⁢[u]𝑑𝑝subscript𝑝𝑡delimited-[]𝑢𝑢superscript10𝑡delimited-[]𝑢d(p,p_{t[u]}(u))\leq 10^{t[u]}italic_d ( italic_p , italic_p start_POSTSUBSCRIPT italic_t [ italic_u ] end_POSTSUBSCRIPT ( italic_u ) ) ≤ 10 start_POSTSUPERSCRIPT italic_t [ italic_u ] end_POSTSUPERSCRIPT. According to Lemma 3.3, d⁢(pt⁢[u]⁢(u),u)=d⁢(pt⁢[u]⁢(u),p0⁢(u))≤10t⁢[u]/2𝑑subscript𝑝𝑡delimited-[]𝑢𝑢𝑢𝑑subscript𝑝𝑡delimited-[]𝑢𝑢subscript𝑝0𝑢superscript10𝑡delimited-[]𝑢2d(p_{t[u]}(u),u)=d(p_{t[u]}(u),p_{0}(u))\leq 10^{t[u]}/2italic_d ( italic_p start_POSTSUBSCRIPT italic_t [ italic_u ] end_POSTSUBSCRIPT ( italic_u ) , italic_u ) = italic_d ( italic_p start_POSTSUBSCRIPT italic_t [ italic_u ] end_POSTSUBSCRIPT ( italic_u ) , italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( italic_u ) ) ≤ 10 start_POSTSUPERSCRIPT italic_t [ italic_u ] end_POSTSUPERSCRIPT / 2, which concludes d⁢(p,u)≤d⁢(p,pt⁢[u]⁢(u))+d⁢(pt⁢[u]⁢(u),u)≤10t⁢[u]+10t⁢[u]/2≤2⋅10t⁢[u].𝑑𝑝𝑢𝑑𝑝subscript𝑝𝑡delimited-[]𝑢𝑢𝑑subscript𝑝𝑡delimited-[]𝑢𝑢𝑢superscript10𝑡delimited-[]𝑢superscript10𝑡delimited-[]𝑢2⋅2superscript10𝑡delimited-[]𝑢d(p,u)\leq d(p,p_{t[u]}(u))+d(p_{t[u]}(u),u)\leq 10^{t[u]}+10^{t[u]}/2\leq 2% \cdot 10^{t[u]}.italic_d ( italic_p , italic_u ) ≤ italic_d ( italic_p , italic_p start_POSTSUBSCRIPT italic_t [ italic_u ] end_POSTSUBSCRIPT ( italic_u ) ) + italic_d ( italic_p start_POSTSUBSCRIPT italic_t [ italic_u ] end_POSTSUBSCRIPT ( italic_u ) , italic_u ) ≤ 10 start_POSTSUPERSCRIPT italic_t [ italic_u ] end_POSTSUPERSCRIPT + 10 start_POSTSUPERSCRIPT italic_t [ italic_u ] end_POSTSUPERSCRIPT / 2 ≤ 2 ⋅ 10 start_POSTSUPERSCRIPT italic_t [ italic_u ] end_POSTSUPERSCRIPT . (24) Let {u1,u2,…,uμ}⊆𝒰initsubscript𝑢1subscript𝑢2…subscript𝑢𝜇subscript𝒰init\{u_{1},u_{2},\ldots,u_{\mu}\}\subseteq\mathcal{U}_{\textnormal{{init}}}{ italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_u start_POSTSUBSCRIPT italic_μ end_POSTSUBSCRIPT } ⊆ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT be all centers contaminated by p𝑝pitalic_p ordered in decreasing order by the time they were added to the main solution via a call to Make-Robust(⋅,⋅)⋅⋅(\cdot,\cdot)( ⋅ , ⋅ ). So, when uisubscript𝑢𝑖u_{i}italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is added to the main solution 𝒰𝒰\mathcal{U}caligraphic_U, ui+1subscript𝑢𝑖1u_{i+1}italic_u start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT was already present in 𝒰𝒰\mathcal{U}caligraphic_U, which concludes 10t⁢[ui]≤d⁢(ui,ui+1)/10,superscript10𝑡delimited-[]subscript𝑢𝑖𝑑subscript𝑢𝑖subscript𝑢𝑖11010^{t[u_{i}]}\leq d(u_{i},u_{i+1})/10,10 start_POSTSUPERSCRIPT italic_t [ italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ] end_POSTSUPERSCRIPT ≤ italic_d ( italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT ) / 10 , for every i∈[1,μ−1]𝑖1𝜇1i\in[1,\mu-1]italic_i ∈ [ 1 , italic_μ - 1 ] (by the choice of t⁢[ui]𝑡delimited-[]subscript𝑢𝑖t[u_{i}]italic_t [ italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ] in Make-Robust at that time). Hence, 10t⁢[ui+1]≥d⁢(p,ui+1)/2≥(d⁢(ui,ui+1)−d⁢(p,ui))/2≥5⋅10t⁢[ui]−10t⁢[ui]=4⋅10t⁢[ui].superscript10𝑡delimited-[]subscript𝑢𝑖1𝑑𝑝subscript𝑢𝑖12𝑑subscript𝑢𝑖subscript𝑢𝑖1𝑑𝑝subscript𝑢𝑖2⋅5superscript10𝑡delimited-[]subscript𝑢𝑖superscript10𝑡delimited-[]subscript𝑢𝑖⋅4superscript10𝑡delimited-[]subscript𝑢𝑖10^{t[u_{i+1}]}\geq d(p,u_{i+1})/2\geq(d(u_{i},u_{i+1})-d(p,u_{i}))/2\geq 5% \cdot 10^{t[u_{i}]}-10^{t[u_{i}]}=4\cdot 10^{t[u_{i}]}.10 start_POSTSUPERSCRIPT italic_t [ italic_u start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT ] end_POSTSUPERSCRIPT ≥ italic_d ( italic_p , italic_u start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT ) / 2 ≥ ( italic_d ( italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT ) - italic_d ( italic_p , italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) / 2 ≥ 5 ⋅ 10 start_POSTSUPERSCRIPT italic_t [ italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ] end_POSTSUPERSCRIPT - 10 start_POSTSUPERSCRIPT italic_t [ italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ] end_POSTSUPERSCRIPT = 4 ⋅ 10 start_POSTSUPERSCRIPT italic_t [ italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ] end_POSTSUPERSCRIPT . The first and last inequalities hold by (24) for u=ui+1,ui𝑢subscript𝑢𝑖1subscript𝑢𝑖u=u_{i+1},u_{i}italic_u = italic_u start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. Finally, this concludes t⁢[uμ]>⋯>t⁢[u2]>t⁢[u1]𝑡delimited-[]subscript𝑢𝜇⋯𝑡delimited-[]subscript𝑢2𝑡delimited-[]subscript𝑢1t[u_{\mu}]>\cdots>t[u_{2}]>t[u_{1}]italic_t [ italic_u start_POSTSUBSCRIPT italic_μ end_POSTSUBSCRIPT ] > ⋯ > italic_t [ italic_u start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ] > italic_t [ italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ]. Since distances between any two point in the space is between 1111 and ΔΔ\Deltaroman_Δ, we know 0≤t⁢[ui]≤⌈log⁡Δ⌉0𝑡delimited-[]subscript𝑢𝑖Δ0\leq t[u_{i}]\leq\lceil\log\Delta\rceil0 ≤ italic_t [ italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ] ≤ ⌈ roman_log roman_Δ ⌉ for each i∈[1,μ]𝑖1𝜇i\in[1,\mu]italic_i ∈ [ 1 , italic_μ ], which concludes μ=O⁢(log⁡Δ)𝜇𝑂Δ\mu=O(\log\Delta)italic_μ = italic_O ( roman_log roman_Δ ). 6 Achieving O~⁢(k)~𝑂𝑘\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) Update Time We first outline how to implement the algorithm from Section 5.1 in O~⁢(n)~𝑂𝑛\tilde{O}(n)over~ start_ARG italic_O end_ARG ( italic_n ) update time, by incurring only a O⁢(1)𝑂1O(1)italic_O ( 1 ) multiplicative overhead in approximation ratio and recourse (n𝑛nitalic_n is an upper bound on the size of the input 𝒫⊆𝐏𝒫𝐏\mathcal{P}\subseteq\mathbf{P}caligraphic_P ⊆ bold_P, throughout the sequence of updates). In Section 6.3, we show how to further improve the update time from O~⁢(n)~𝑂𝑛\tilde{O}(n)over~ start_ARG italic_O end_ARG ( italic_n ) to O~⁢(k)~𝑂𝑘\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) using standard sparsification techniques. Disclaimer. In this section, we often use asymptotic notations and informal arguments without proofs. The reader can find the correct values of the parameters together with complete formal proofs for the statements of this section in Part III (full version). 6.1 Auxiliary Data Structure and Randomized Local Search Recall that 𝒰⊆𝐏𝒰𝐏\mathcal{U}\subseteq\mathbf{P}caligraphic_U ⊆ bold_P is the solution (set of k𝑘kitalic_k centers) maintained by our algorithm, and 𝒫⊆𝐏𝒫𝐏\mathcal{P}\subseteq\mathbf{P}caligraphic_P ⊆ bold_P denotes the current input. For each point p∈𝒫∪𝒰𝑝𝒫𝒰p\in\mathcal{P}\cup\mathcal{U}italic_p ∈ caligraphic_P ∪ caligraphic_U, we maintain a BST (balanced search tree) 𝒯psubscript𝒯𝑝\mathcal{T}_{p}caligraphic_T start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT that stores the centers 𝒰𝒰\mathcal{U}caligraphic_U in increasing order of their distances to p𝑝pitalic_p. Note that after every change (insertion/deletion of a center) in the set 𝒰𝒰\mathcal{U}caligraphic_U, we can update all these BSTs in O~⁢(|𝒫|+|𝒰|)=O~⁢(n+k)=O~⁢(n)~𝑂𝒫𝒰~𝑂𝑛𝑘~𝑂𝑛\tilde{O}(\left|\mathcal{P}\right|+\left|\mathcal{U}\right|)=\tilde{O}(n+k)=% \tilde{O}(n)over~ start_ARG italic_O end_ARG ( | caligraphic_P | + | caligraphic_U | ) = over~ start_ARG italic_O end_ARG ( italic_n + italic_k ) = over~ start_ARG italic_O end_ARG ( italic_n ) time. Similarly, after the insertion/deletion of a point p∈𝒫𝑝𝒫p\in\mathcal{P}italic_p ∈ caligraphic_P, we can construct/destroy the relevant BST 𝒯psubscript𝒯𝑝\mathcal{T}_{p}caligraphic_T start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT in O~⁢(|𝒰|)=O~⁢(k)=O~⁢(n)~𝑂𝒰~𝑂𝑘~𝑂𝑛\tilde{O}(\left|\mathcal{U}\right|)=\tilde{O}(k)=\tilde{O}(n)over~ start_ARG italic_O end_ARG ( | caligraphic_U | ) = over~ start_ARG italic_O end_ARG ( italic_k ) = over~ start_ARG italic_O end_ARG ( italic_n ) time. In other words, if the algorithm from Section 5.1 incurs a total recourse of τ𝜏\tauitalic_τ while handling a sequence of μ𝜇\muitalic_μ updates, then we spend O~⁢(n⋅(τ+μ))~𝑂⋅𝑛𝜏𝜇\tilde{O}(n\cdot(\tau+\mu))over~ start_ARG italic_O end_ARG ( italic_n ⋅ ( italic_τ + italic_μ ) ) total time on maintaining this auxiliary data structure (collection of BSTs) over the same update-sequence. Since τ=O⁢(μ⋅log2⁡Δ)𝜏𝑂⋅𝜇superscript2Δ\tau=O(\mu\cdot\log^{2}\Delta)italic_τ = italic_O ( italic_μ ⋅ roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT roman_Δ ) (see Section 5.3), this incurs an amortized update time of O~⁢(n)~𝑂𝑛\tilde{O}(n)over~ start_ARG italic_O end_ARG ( italic_n ), which is within our budget. We will use the randomized local search algorithm, developed in [BCG+24] and summarized in the lemma below, as a crucial subroutine. Lemma 6.1 (Randomized Local Search [BCG+24]). Suppose that we have access to the auxiliary data structure described above. Then, given any integer s∈[0,k−1]𝑠0𝑘1s\in[0,k-1]italic_s ∈ [ 0 , italic_k - 1 ], in O~⁢(n⁢s)~𝑂𝑛𝑠\tilde{O}(ns)over~ start_ARG italic_O end_ARG ( italic_n italic_s ) time we can find a subset 𝒰⋆⊆𝒰superscript𝒰⋆𝒰\mathcal{U}^{\star}\subseteq\mathcal{U}caligraphic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ⊆ caligraphic_U of (k−s)𝑘𝑠(k-s)( italic_k - italic_s ) centers such that Cost⁢(𝒰⋆,𝒫)≤O⁢(1)⋅OPTk−s𝒰⁢(𝒫)Costsuperscript𝒰⋆𝒫⋅𝑂1superscriptsubscriptOPT𝑘𝑠𝒰𝒫\textnormal{{Cost}}\left(\mathcal{U}^{\star},\mathcal{P}\right)\leq O(1)\cdot% \textnormal{{OPT}}_{k-s}^{\mathcal{U}}\left(\mathcal{P}\right)Cost ( caligraphic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT , caligraphic_P ) ≤ italic_O ( 1 ) ⋅ OPT start_POSTSUBSCRIPT italic_k - italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT caligraphic_U end_POSTSUPERSCRIPT ( caligraphic_P ). 6.2 Implementing Our Dynamic Algorithm Henceforth, we focus on a given epoch of our dynamic algorithm that lasts for (ℓ+1)ℓ1(\ell+1)( roman_ℓ + 1 ) updates (see Section 5.1), and outline how to implement the algorithm in such a manner that it spends O~⁢(n⋅(ℓ+1))~𝑂⋅𝑛ℓ1\tilde{O}(n\cdot(\ell+1))over~ start_ARG italic_O end_ARG ( italic_n ⋅ ( roman_ℓ + 1 ) ) total time during the whole epoch, except the call to Robustify (see Equation 11). For Robustify, we provide an implementation that takes an amortized time of O~⁢(n)~𝑂𝑛\tilde{O}(n)over~ start_ARG italic_O end_ARG ( italic_n ), over the entire sequence of updates (spanning multiple epochs). This implies an overall amortized update time of O~⁢(n)~𝑂𝑛\tilde{O}(n)over~ start_ARG italic_O end_ARG ( italic_n ). Below, we first show how to implement each of Steps 1 - 4, as described in Section 5.1, one after another. Then, we provide the implementation of Robustify in Section 6.2.1. Implementing Step 1. Our task here is to compute an estimate of the value of ℓ⋆superscriptℓ⋆\ell^{\star}roman_ℓ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT. For each i∈[0,log2⁡k]𝑖0subscript2𝑘i\in[0,\log_{2}k]italic_i ∈ [ 0 , roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_k ] define si:=2iassignsubscript𝑠𝑖superscript2𝑖s_{i}:=2^{i}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT := 2 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT, and let s−1:=0assignsubscript𝑠10s_{-1}:=0italic_s start_POSTSUBSCRIPT - 1 end_POSTSUBSCRIPT := 0. We now run a for loop, as described below. 1 for i=0𝑖0i=0italic_i = 0 to log2⁡ksubscript2𝑘\log_{2}kroman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_k do 2 Using Lemma 6.1, in O~⁢(n⁢si)~𝑂𝑛subscript𝑠𝑖\tilde{O}(ns_{i})over~ start_ARG italic_O end_ARG ( italic_n italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) time compute a subset 𝒰i⋆⊆𝒰initsubscriptsuperscript𝒰⋆𝑖subscript𝒰init\mathcal{U}^{\star}_{i}\subseteq\mathcal{U}_{\textnormal{{init}}}caligraphic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ⊆ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT of (k−si)𝑘subscript𝑠𝑖(k-s_{i})( italic_k - italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) centers, 3 that is a O⁢(1)𝑂1O(1)italic_O ( 1 )-approximation to OPTk−si𝒰init⁢(𝒫(0))superscriptsubscriptOPT𝑘subscript𝑠𝑖subscript𝒰initsuperscript𝒫0\textnormal{{OPT}}_{k-s_{i}}^{\mathcal{U}_{\textnormal{{init}}}}\left(\mathcal% {P}^{(0)}\right)OPT start_POSTSUBSCRIPT italic_k - italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ). 4 if Cost⁢(𝒰i⋆,𝒫(0))≥Θ⁢(γ)⋅Cost⁢(𝒰init,𝒫(0))Costsubscriptsuperscript𝒰⋆𝑖superscript𝒫0⋅Θ𝛾Costsubscript𝒰initsuperscript𝒫0\textnormal{{Cost}}\left(\mathcal{U}^{\star}_{i},\mathcal{P}^{(0)}\right)\geq% \Theta(\gamma)\cdot\textnormal{{Cost}}\left(\mathcal{U}_{\textnormal{{init}}},% \mathcal{P}^{(0)}\right)Cost ( caligraphic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) ≥ roman_Θ ( italic_γ ) ⋅ Cost ( caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) then 5 return ℓ^:=si−1assign^ℓsubscript𝑠𝑖1\hat{\ell}:=s_{i-1}over^ start_ARG roman_ℓ end_ARG := italic_s start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT. Algorithm 3 Computing an estimate ℓ^^ℓ\hat{\ell}over^ start_ARG roman_ℓ end_ARG of the value of ℓ⋆superscriptℓ⋆\ell^{\star}roman_ℓ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT. After finding ℓ^^ℓ\hat{\ell}over^ start_ARG roman_ℓ end_ARG, we set the length of the epoch to be ℓ+1ℓ1\ell+1roman_ℓ + 1 where ℓ←⌊ℓ^12⋅Θ⁢(γ)⌋←ℓ^ℓ⋅12Θ𝛾\ell\leftarrow\left\lfloor\frac{\hat{\ell}}{12\cdot\Theta(\gamma)}\right\rfloorroman_ℓ ← ⌊ divide start_ARG over^ start_ARG roman_ℓ end_ARG end_ARG start_ARG 12 ⋅ roman_Θ ( italic_γ ) end_ARG ⌋. With some extra calculations, we can show that ℓ+1=Ω⁢(ℓ^+1)=Ω⁢(ℓ⋆+1)ℓ1Ω^ℓ1Ωsuperscriptℓ⋆1\ell+1=\Omega(\hat{\ell}+1)=\Omega(\ell^{\star}+1)roman_ℓ + 1 = roman_Ω ( over^ start_ARG roman_ℓ end_ARG + 1 ) = roman_Ω ( roman_ℓ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT + 1 ) and OPTk−ℓ⁢(𝒫(0))Θ⁢(γ)≤OPTk⁢(𝒫(0))≤4⋅OPTk+ℓ⁢(𝒫(0)).subscriptOPT𝑘ℓsuperscript𝒫0Θ𝛾subscriptOPT𝑘superscript𝒫0⋅4subscriptOPT𝑘ℓsuperscript𝒫0\frac{\textnormal{{OPT}}_{k-{\ell}}(\mathcal{P}^{(0)})}{\Theta(\gamma)}\leq% \textnormal{{OPT}}_{k}(\mathcal{P}^{(0)})\leq 4\cdot\textnormal{{OPT}}_{k+{% \ell}}(\mathcal{P}^{(0)}).divide start_ARG OPT start_POSTSUBSCRIPT italic_k - roman_ℓ end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) end_ARG start_ARG roman_Θ ( italic_γ ) end_ARG ≤ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) ≤ 4 ⋅ OPT start_POSTSUBSCRIPT italic_k + roman_ℓ end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) . The running time of this for loop is O~⁢(n⁢∑j=0i⋆sj)=O~⁢(n⁢si⋆)~𝑂𝑛superscriptsubscript𝑗0superscript𝑖⋆subscript𝑠𝑗~𝑂𝑛subscript𝑠superscript𝑖⋆\tilde{O}\left(n\sum_{j=0}^{i^{\star}}s_{j}\right)=\tilde{O}(ns_{i^{\star}})over~ start_ARG italic_O end_ARG ( italic_n ∑ start_POSTSUBSCRIPT italic_j = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT italic_s start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) = over~ start_ARG italic_O end_ARG ( italic_n italic_s start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ), where i⋆superscript𝑖⋆i^{\star}italic_i start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT is the index s.t. si⋆−1=ℓ^subscript𝑠superscript𝑖⋆1^ℓs_{i^{\star}-1}=\hat{\ell}italic_s start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT - 1 end_POSTSUBSCRIPT = over^ start_ARG roman_ℓ end_ARG. Thus, we have si⋆=O⁢(ℓ^)=O⁢(ℓ+1)subscript𝑠superscript𝑖⋆𝑂^ℓ𝑂ℓ1s_{i^{\star}}=O(\hat{\ell})=O(\ell+1)italic_s start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT = italic_O ( over^ start_ARG roman_ℓ end_ARG ) = italic_O ( roman_ℓ + 1 ), and hence we can implement Step 1 in O~⁢(n⋅(ℓ+1))~𝑂⋅𝑛ℓ1\tilde{O}(n\cdot(\ell+1))over~ start_ARG italic_O end_ARG ( italic_n ⋅ ( roman_ℓ + 1 ) ) time. Implementing Step 2. Instead of finding the optimum set of (k−ℓ)𝑘ℓ(k-\ell)( italic_k - roman_ℓ ) centers within 𝒰initsubscript𝒰init\mathcal{U}_{\textnormal{{init}}}caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT, we approximate it using Lemma 6.1: We compute a set of (k−ℓ)𝑘ℓ(k-\ell)( italic_k - roman_ℓ ) centers 𝒰(0)⊆𝒰initsuperscript𝒰0subscript𝒰init\mathcal{U}^{(0)}\subseteq\mathcal{U}_{\textnormal{{init}}}caligraphic_U start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ⊆ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT such that Cost⁢(𝒰(0),𝒫(0))≤O⁢(1)⋅OPTk−ℓ𝒰init⁢(𝒫(0))Costsuperscript𝒰0superscript𝒫0⋅𝑂1superscriptsubscriptOPT𝑘ℓsubscript𝒰initsuperscript𝒫0\textnormal{{Cost}}\left(\mathcal{U}^{(0)},\mathcal{P}^{(0)}\right)\leq O(1)% \cdot\textnormal{{OPT}}_{k-\ell}^{\mathcal{U}_{\textnormal{{init}}}}(\mathcal{% P}^{(0)})Cost ( caligraphic_U start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) ≤ italic_O ( 1 ) ⋅ OPT start_POSTSUBSCRIPT italic_k - roman_ℓ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ). With the same arguments as before, we get Cost⁢(𝒰(0),𝒫(0))≤O⁢(1)⋅OPTk+ℓ⁢(𝒫(0))Costsuperscript𝒰0superscript𝒫0⋅𝑂1subscriptOPT𝑘ℓsuperscript𝒫0\textnormal{{Cost}}\left(\mathcal{U}^{(0)},\mathcal{P}^{(0)}\right)\leq O(1)% \cdot\textnormal{{OPT}}_{k+\ell}(\mathcal{P}^{(0)})Cost ( caligraphic_U start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) ≤ italic_O ( 1 ) ⋅ OPT start_POSTSUBSCRIPT italic_k + roman_ℓ end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ). Note that the running time for this step is also O~⁢(n⋅(ℓ+1))~𝑂⋅𝑛ℓ1\tilde{O}(n\cdot(\ell+1))over~ start_ARG italic_O end_ARG ( italic_n ⋅ ( roman_ℓ + 1 ) ). Implementing Step 3. Trivially, we can implement each of these updates in constant time. Implementing Step 4. We first need to add O⁢(ℓ+1)𝑂ℓ1O(\ell+1)italic_O ( roman_ℓ + 1 ) centers to 𝒰initsubscript𝒰init\mathcal{U}_{\textnormal{{init}}}caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT, while minimizing the cost of the solution w.r.t. 𝒫(0)superscript𝒫0\mathcal{P}^{(0)}caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT. We compute an approximation of 𝒰⋆superscript𝒰⋆\mathcal{U}^{\star}caligraphic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT, by setting 𝒰=𝒰init𝒰subscript𝒰init\mathcal{U}=\mathcal{U}_{\textnormal{{init}}}caligraphic_U = caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT, 𝒫=𝒫(0)𝒫superscript𝒫0\mathcal{P}=\mathcal{P}^{(0)}caligraphic_P = caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT and s=Θ⁢(γ⋅(ℓ+1))𝑠Θ⋅𝛾ℓ1s=\Theta(\gamma\cdot(\ell+1))italic_s = roman_Θ ( italic_γ ⋅ ( roman_ℓ + 1 ) ) in Lemma 6.2 below (see Equation 9). This also takes O~⁢(n⋅(ℓ+1))~𝑂⋅𝑛ℓ1\tilde{O}(n\cdot(\ell+1))over~ start_ARG italic_O end_ARG ( italic_n ⋅ ( roman_ℓ + 1 ) ) time. We defer the proof sketch of Lemma 6.2 to Section 6.2.2. Lemma 6.2. Suppose that we have access to the auxiliary data structure described above (see Section 6.1). Then, given any integer s≥1𝑠1s\geq 1italic_s ≥ 1, in O~⁢(n⁢s)~𝑂𝑛𝑠\tilde{O}(ns)over~ start_ARG italic_O end_ARG ( italic_n italic_s ) time we can find a superset 𝒰⋆⊇𝒰𝒰superscript𝒰⋆\mathcal{U}^{\star}\supseteq\mathcal{U}caligraphic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ⊇ caligraphic_U of (k+s)𝑘𝑠(k+s)( italic_k + italic_s ) centers such that Cost⁢(𝒰⋆,𝒫)≤O⁢(1)⋅minℱ⊆𝐏:|ℱ|≤s⁡Cost⁢(𝒰+ℱ,𝒫)Costsuperscript𝒰⋆𝒫⋅𝑂1subscript:ℱ𝐏ℱ𝑠Cost𝒰ℱ𝒫\textnormal{{Cost}}\left(\mathcal{U}^{\star},\mathcal{P}\right)\leq O(1)\cdot% \min\limits_{\mathcal{F}\subseteq\mathbf{P}:|\mathcal{F}|\leq s}\textnormal{{% Cost}}\left(\mathcal{U}+\mathcal{F},\mathcal{P}\right)Cost ( caligraphic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT , caligraphic_P ) ≤ italic_O ( 1 ) ⋅ roman_min start_POSTSUBSCRIPT caligraphic_F ⊆ bold_P : | caligraphic_F | ≤ italic_s end_POSTSUBSCRIPT Cost ( caligraphic_U + caligraphic_F , caligraphic_P ). At this stage, we compute 𝒱⋆:=𝒰⋆+(𝒫(ℓ+1)−𝒫(0))assignsuperscript𝒱⋆superscript𝒰⋆superscript𝒫ℓ1superscript𝒫0\mathcal{V}^{\star}:=\mathcal{U}^{\star}+\left(\mathcal{P}^{(\ell+1)}-\mathcal% {P}^{(0)}\right)caligraphic_V start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT := caligraphic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT + ( caligraphic_P start_POSTSUPERSCRIPT ( roman_ℓ + 1 ) end_POSTSUPERSCRIPT - caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) as in Equation 10, in only O~⁢(ℓ+1)~𝑂ℓ1\tilde{O}(\ell+1)over~ start_ARG italic_O end_ARG ( roman_ℓ + 1 ) time. Next, we compute an approximation of 𝒲⋆superscript𝒲⋆\mathcal{W}^{\star}caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT (see Equation 10) using Lemma 6.1, which again takes O~⁢(n⋅(ℓ+1))~𝑂⋅𝑛ℓ1\tilde{O}(n\cdot(\ell+1))over~ start_ARG italic_O end_ARG ( italic_n ⋅ ( roman_ℓ + 1 ) ) time. It follows that Cost⁢(𝒲⋆,𝒫(ℓ+1))≤O⁢(1)⋅OPTk⁢(𝒫(ℓ+1))Costsuperscript𝒲⋆superscript𝒫ℓ1⋅𝑂1subscriptOPT𝑘superscript𝒫ℓ1\textnormal{{Cost}}\left(\mathcal{W}^{\star},\mathcal{P}^{(\ell+1)}\right)\leq O% (1)\cdot\textnormal{{OPT}}_{k}(\mathcal{P}^{(\ell+1)})Cost ( caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( roman_ℓ + 1 ) end_POSTSUPERSCRIPT ) ≤ italic_O ( 1 ) ⋅ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( roman_ℓ + 1 ) end_POSTSUPERSCRIPT ). Finally, we explain below how we implement the call to Robustify⁢(𝒲⋆)Robustifysuperscript𝒲⋆\textsc{Robustify}(\mathcal{W}^{\star})Robustify ( caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) (see Equation 11). 6.2.1 Implementing the calls to Robustify⁢(⋅)Robustify⋅\textsc{Robustify}(\cdot)Robustify ( ⋅ ) subroutine Recall Algorithm 1 and Algorithm 2 from Section 3.2. In the static setting, there are known O⁢(1)𝑂1O(1)italic_O ( 1 )-approximation algorithms for 1111-median with O~⁢(n)~𝑂𝑛\tilde{O}(n)over~ start_ARG italic_O end_ARG ( italic_n ) runtime [MP02]. Using any such 1111-median algorithm, it is relatively straightforward to (approximately) implement a call to Make-Robust⁢(⋅,⋅)Make-Robust⋅⋅\textsc{Make-Robust}(\cdot\,,\cdot)Make-Robust ( ⋅ , ⋅ ) in O~⁢(n)~𝑂𝑛\tilde{O}(n)over~ start_ARG italic_O end_ARG ( italic_n ) time (see Algorithm 1 in Algorithm 1). Using the auxiliary data structure (see Section 6.1), it is easy to implement each invocation of Algorithm 1 in Algorithm 1 in O~⁢(1)~𝑂1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) time, whereas Algorithm 1 in Algorithm 1 trivially takes O~⁢(1)~𝑂1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) time. By Lemma 5.8, our dynamic algorithm makes O~⁢(1)~𝑂1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) many amortized calls to Make-Robust⁢(⋅,⋅)Make-Robust⋅⋅\textsc{Make-Robust}(\cdot\,,\cdot)Make-Robust ( ⋅ , ⋅ ), per update. Thus, it follows that we spend O~⁢(n)~𝑂𝑛\tilde{O}(n)over~ start_ARG italic_O end_ARG ( italic_n ) amortized time per update on implementing Algorithms 1, 1 and 1 in Algorithm 1. There remains a significant challenge: We might have to iterate over |𝒲|=k𝒲𝑘\left|\mathcal{W}\right|=k| caligraphic_W | = italic_k centers in Algorithm 1 in Algorithm 1, before we find a center w∈𝒲𝑤𝒲w\in\mathcal{W}italic_w ∈ caligraphic_W that violates Equation 4. Let us refer to this operation as “testing a center w𝑤witalic_w”; this occurs when we check whether w𝑤witalic_w violates Equation 4. In other words, we need to perform O⁢(k)𝑂𝑘O(k)italic_O ( italic_k ) many such tests in Algorithm 1, before we execute a Make-Robust⁢(⋅,⋅)Make-Robust⋅⋅\textsc{Make-Robust}(\cdot\,,\cdot)Make-Robust ( ⋅ , ⋅ ) call in Algorithm 1 in Algorithm 1. Note that because of Lemma 5.8, we perform O~⁢(k)~𝑂𝑘\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) many tests, on average, per update. Thus, if we could hypothetically perform each test in O~⁢(1)~𝑂1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) time, then we would incur an additive overhead of O~⁢(k)~𝑂𝑘\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) in our amortized update time, and everything would be fine. The issue, however, is that testing a center can be prohibitively expensive. This is because Equation 4 consists of two conditions. The second condition (which finds the value of t𝑡titalic_t) requires us to know the value of d⁢(w,𝒲−w)𝑑𝑤𝒲𝑤d(w,\mathcal{W}-w)italic_d ( italic_w , caligraphic_W - italic_w ), and this can indeed be implemented in O~⁢(1)~𝑂1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) time using our auxiliary data structure (see Section 6.1). The first condition asks us to check whether w𝑤witalic_w is t𝑡titalic_t-robust, and there does not seem to be any efficient way in which we can implement this check (see Definition 3.2). To address this significant challenge, we modify the execution of a call to Robustify⁢(𝒲⋆)Robustifysuperscript𝒲⋆\textsc{Robustify}(\mathcal{W}^{\star})Robustify ( caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) at the end of an epoch, as described below. Modified version of the call to Robustify⁢(𝒲⋆)Robustifysuperscript𝒲⋆\textsc{Robustify}(\mathcal{W}^{\star})Robustify ( caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ). At the end of an epoch, the call to Robustify⁢(𝒲⋆)Robustifysuperscript𝒲⋆\textsc{Robustify}(\mathcal{W}^{\star})Robustify ( caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) is supposed to return the set 𝒰finalsubscript𝒰final\mathcal{U}_{\textnormal{{final}}}caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT (see Equation 11). We replace this call to Robustify⁢(𝒲⋆)Robustifysuperscript𝒲⋆\textsc{Robustify}(\mathcal{W}^{\star})Robustify ( caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) by the procedure in Algorithm 4 below. To appreciate what Algorithm 4 does, recall the recourse analysis in Section 5.3; in particular, the distinction between contaminated vs clean centers in 𝒰initsubscript𝒰init\mathcal{U}_{\textnormal{{init}}}caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT, and the three types of calls to the Make-Robust⁢(⋅,⋅)Make-Robust⋅⋅\textsc{Make-Robust}(\cdot\,,\cdot)Make-Robust ( ⋅ , ⋅ ) subroutine. Note that in Algorithms 4, 4 and 4 in Algorithm 4, the sets 𝒲1,𝒲2subscript𝒲1subscript𝒲2\mathcal{W}_{1},\mathcal{W}_{2}caligraphic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , caligraphic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT and 𝒲3subscript𝒲3\mathcal{W}_{3}caligraphic_W start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT respectively correspond to those centers w𝑤witalic_w that might potentially be the sources of Type I, Type II and Type III calls to Make-Robust⁢(w,⋅)Make-Robust𝑤⋅\textsc{Make-Robust}(w,\cdot)Make-Robust ( italic_w , ⋅ ). We refer to the centers in 𝒲1,𝒲2subscript𝒲1subscript𝒲2\mathcal{W}_{1},\mathcal{W}_{2}caligraphic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , caligraphic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT and 𝒲3subscript𝒲3\mathcal{W}_{3}caligraphic_W start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT respectively as Type I, Type II and Type III centers. The key difference between Algorithm 4 and the previous version of Robustify⁢(𝒲⋆)Robustifysuperscript𝒲⋆\textsc{Robustify}(\mathcal{W}^{\star})Robustify ( caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) is this: In Algorithm 4, we proactively make calls to Make-Robust⁢(w,⋅)Make-Robust𝑤⋅\textsc{Make-Robust}(w\,,\cdot)Make-Robust ( italic_w , ⋅ ) without even checking the first condition in Equation 4, which was the main bottleneck in achieving efficient update time. To be more specific, we proactively call Make-Robust⁢(w,⋅)Make-Robust𝑤⋅\textsc{Make-Robust}(w,\cdot)Make-Robust ( italic_w , ⋅ ) for every Type I and Type II center w𝑤witalic_w (see Algorithm 4). In contrast, for a Type III center w𝑤witalic_w, we call Make-Robust⁢(w,⋅)Make-Robust𝑤⋅\textsc{Make-Robust}(w,\cdot)Make-Robust ( italic_w , ⋅ ) whenever we observe that t>t⁢[w]𝑡𝑡delimited-[]𝑤t>t[w]italic_t > italic_t [ italic_w ], where t𝑡titalic_t is the smallest integer satisfying 10t≥d⁢(w,𝒲−w)/200superscript10𝑡𝑑𝑤𝒲𝑤20010^{t}\geq d(w,\mathcal{W}-w)/20010 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ≥ italic_d ( italic_w , caligraphic_W - italic_w ) / 200 (see Algorithms 4 and 4). Note that if t≤t⁢[w]𝑡𝑡delimited-[]𝑤t\leq t[w]italic_t ≤ italic_t [ italic_w ] in Algorithm 4, then by 5.9 the center w𝑤witalic_w does not violate Equation 4. 1 𝒲1←𝒲⋆∖𝒰init←subscript𝒲1superscript𝒲⋆subscript𝒰init\mathcal{W}_{1}\leftarrow\mathcal{W}^{\star}\setminus\mathcal{U}_{\textnormal{% {init}}}caligraphic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ← caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ∖ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT 2 𝒲2←{w∈𝒲⋆∩𝒰init:w⁢ is contaminated}←subscript𝒲2conditional-set𝑤superscript𝒲⋆subscript𝒰init𝑤 is contaminated\mathcal{W}_{2}\leftarrow\{w\in\mathcal{W}^{\star}\cap\mathcal{U}_{\textnormal% {{init}}}:w\text{ is contaminated}\}caligraphic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ← { italic_w ∈ caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ∩ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT : italic_w is contaminated } 3 𝒲3←{w∈𝒲⋆∩𝒰init:w⁢ is clean}←subscript𝒲3conditional-set𝑤superscript𝒲⋆subscript𝒰init𝑤 is clean\mathcal{W}_{3}\leftarrow\{w\in\mathcal{W}^{\star}\cap\mathcal{U}_{\textnormal% {{init}}}:w\text{ is clean}\}caligraphic_W start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ← { italic_w ∈ caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ∩ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT : italic_w is clean } // The set 𝒲⋆superscript𝒲⋆\mathcal{W}^{\star}caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT is partitioned into the subsets 𝒲1,𝒲2,𝒲3subscript𝒲1subscript𝒲2subscript𝒲3\mathcal{W}_{1},\mathcal{W}_{2},\mathcal{W}_{3}caligraphic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , caligraphic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , caligraphic_W start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT 4 𝒲←𝒲⋆←𝒲superscript𝒲⋆\mathcal{W}\leftarrow\mathcal{W}^{\star}caligraphic_W ← caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT 5 for each center w∈𝒲1∪𝒲2𝑤subscript𝒲1subscript𝒲2w\in\mathcal{W}_{1}\cup\mathcal{W}_{2}italic_w ∈ caligraphic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ∪ caligraphic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT do 6 t←←𝑡absentt\leftarrowitalic_t ← Smallest integer satisfying 10t≥d⁢(w,𝒲−w)/100superscript10𝑡𝑑𝑤𝒲𝑤10010^{t}\geq d(w,\mathcal{W}-w)/10010 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ≥ italic_d ( italic_w , caligraphic_W - italic_w ) / 100 7 w0←←subscript𝑤0absentw_{0}\leftarrowitalic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ← Make-Robust(w,t)𝑤𝑡(w,t)( italic_w , italic_t ) 8 𝒲←𝒲−w+w0←𝒲𝒲𝑤subscript𝑤0\mathcal{W}\leftarrow\mathcal{W}-w+w_{0}caligraphic_W ← caligraphic_W - italic_w + italic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT 9 Save t⁢[w0]←t←𝑡delimited-[]subscript𝑤0𝑡t[w_{0}]\leftarrow titalic_t [ italic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ] ← italic_t and pt⁢[w0]⁢(w0)←w←subscript𝑝𝑡delimited-[]subscript𝑤0subscript𝑤0𝑤p_{t[w_{0}]}(w_{0})\leftarrow witalic_p start_POSTSUBSCRIPT italic_t [ italic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ] end_POSTSUBSCRIPT ( italic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) ← italic_w (see Section 5.3) together with w0subscript𝑤0w_{0}italic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT 10 11while true do 12 for each center w∈𝒲3𝑤subscript𝒲3w\in\mathcal{W}_{3}italic_w ∈ caligraphic_W start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT do 13 t←←𝑡absentt\leftarrowitalic_t ← Smallest integer satisfying 10t≥d⁢(w,𝒲−w)/200superscript10𝑡𝑑𝑤𝒲𝑤20010^{t}\geq d(w,\mathcal{W}-w)/20010 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ≥ italic_d ( italic_w , caligraphic_W - italic_w ) / 200 14 if t>t⁢[w]𝑡𝑡delimited-[]𝑤t>t[w]italic_t > italic_t [ italic_w ] then 15 t′←←superscript𝑡′absentt^{\prime}\leftarrowitalic_t start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ← Smallest integer satisfying 10t≥d⁢(w,𝒲−w)/100superscript10𝑡𝑑𝑤𝒲𝑤10010^{t}\geq d(w,\mathcal{W}-w)/10010 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ≥ italic_d ( italic_w , caligraphic_W - italic_w ) / 100 16 w0←←subscript𝑤0absentw_{0}\leftarrowitalic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ← Make-Robust(w,t′)𝑤superscript𝑡′(w,t^{\prime})( italic_w , italic_t start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) 17 𝒲←𝒲−w+w0←𝒲𝒲𝑤subscript𝑤0\mathcal{W}\leftarrow\mathcal{W}-w+w_{0}caligraphic_W ← caligraphic_W - italic_w + italic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT 18 Save t⁢[w0]←t′←𝑡delimited-[]subscript𝑤0superscript𝑡′t[w_{0}]\leftarrow t^{\prime}italic_t [ italic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ] ← italic_t start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT and pt⁢[w0]⁢(w0)←w←subscript𝑝𝑡delimited-[]subscript𝑤0subscript𝑤0𝑤p_{t[w_{0}]}(w_{0})\leftarrow witalic_p start_POSTSUBSCRIPT italic_t [ italic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ] end_POSTSUBSCRIPT ( italic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) ← italic_w (see Section 5.3) together with w0subscript𝑤0w_{0}italic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT 19 go back to Line 4. 20 return 𝒰final←𝒲←subscript𝒰final𝒲\mathcal{U}_{\textnormal{{final}}}\leftarrow\mathcal{W}caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT ← caligraphic_W . Algorithm 4 Modified version of the call to Robustify⁢(𝒲⋆)Robustifysuperscript𝒲⋆\textsc{Robustify}(\mathcal{W}^{\star})Robustify ( caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) at the end of an epoch. Lemma 6.3 (Informal). Lemma 5.8 continues to hold even after the call to Robustify⁢(𝒲⋆)Robustifysuperscript𝒲⋆\textsc{Robustify}(\mathcal{W}^{\star})Robustify ( caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) at the end of every epoch is replaced by the procedure in Algorithm 4. Proof. (Sketch) The lemma holds because the procedure in Algorithm 4 is perfectly aligned with the recourse analysis in Section 5.3. In other words, the recourse analysis accounts for the scenario where we make proactive calls to Make-Robust⁢(⋅,⋅)Make-Robust⋅⋅\textsc{Make-Robust}(\cdot\,,\cdot)Make-Robust ( ⋅ , ⋅ ), precisely in the manner specified by Algorithm 4. For example, the recourse analysis bounds the number of Type I and Type II calls to Make-Robust⁢(⋅,⋅)Make-Robust⋅⋅\textsc{Make-Robust}(\cdot\,,\cdot)Make-Robust ( ⋅ , ⋅ ), by pretending that every Type I or Type II center makes such a call, regardless of whether or not it violates Equation 4. ∎ Bounding the Update Time. First, note that we can implement each invocation of Algorithms 4, 4, 4 and 4 in Algorithm 4 in O~⁢(k⋅(ℓ+1))~𝑂⋅𝑘ℓ1\tilde{O}(k\cdot(\ell+1))over~ start_ARG italic_O end_ARG ( italic_k ⋅ ( roman_ℓ + 1 ) ) time, which gets amortized over the length of the epoch, as an epoch lasts for (ℓ+1)ℓ1(\ell+1)( roman_ℓ + 1 ) updates. Specifically, to compute the sets 𝒲2subscript𝒲2\mathcal{W}_{2}caligraphic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT and 𝒲3subscript𝒲3\mathcal{W}_{3}caligraphic_W start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT, we iterate over all w∈𝒲⋆∩𝒰init𝑤superscript𝒲⋆subscript𝒰initw\in\mathcal{W}^{\star}\cap\mathcal{U}_{\textnormal{{init}}}italic_w ∈ caligraphic_W start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ∩ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT and q∈𝒫(0)⊕𝒫(ℓ+1)𝑞direct-sumsuperscript𝒫0superscript𝒫ℓ1q\in\mathcal{P}^{(0)}\oplus\mathcal{P}^{(\ell+1)}italic_q ∈ caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ⊕ caligraphic_P start_POSTSUPERSCRIPT ( roman_ℓ + 1 ) end_POSTSUPERSCRIPT, and check whether the update involving q𝑞qitalic_q contaminates w𝑤witalic_w, using the value t⁢[w]𝑡delimited-[]𝑤t[w]italic_t [ italic_w ] and the point pt⁢[w]⁢(w)subscript𝑝𝑡delimited-[]𝑤𝑤p_{t[w]}(w)italic_p start_POSTSUBSCRIPT italic_t [ italic_w ] end_POSTSUBSCRIPT ( italic_w ). Moreover, Algorithms 4 and 4 trivially take O⁢(k)𝑂𝑘O(k)italic_O ( italic_k ) time. Using the auxiliary data structure (see Section 6.1), we can implement each invocation of Algorithms 4, 4 and 4 in O~⁢(1)~𝑂1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) time. Next, say that an iteration of the for loop in Algorithm 4 is uninterrupted if we find that t≤t⁢[w]𝑡𝑡delimited-[]𝑤t\leq t[w]italic_t ≤ italic_t [ italic_w ] in Algorithm 4 (and accordingly do not execute any line within the if block) and interrupted otherwise. Each uninterrupted iteration of the for loop takes O~⁢(1)~𝑂1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) time. Furthermore, there can be at most |𝒲3|=O⁢(k)subscript𝒲3𝑂𝑘|\mathcal{W}_{3}|=O(k)| caligraphic_W start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT | = italic_O ( italic_k ) many consecutive uninterrupted iterations of the for loop in Algorithm 4: Any such chain of uninterrupted iterations is broken (i) either by an interrupted iteration, which involves a call to Make-Robust⁢(w,t′)Make-Robust𝑤superscript𝑡′\textsc{Make-Robust}(w,t^{\prime})Make-Robust ( italic_w , italic_t start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) in Algorithm 4, (ii) or by the termination of the procedure in Algorithm 4 (this can happen only once in an epoch). From the preceding discussion, it follows that total time spent on the remaining lines in Algorithm 4 is dominated by the time spent on the calls to Make-Robust⁢(⋅,⋅)Make-Robust⋅⋅\textsc{Make-Robust}(\cdot\,,\cdot)Make-Robust ( ⋅ , ⋅ ). Recall that at the start of Section 6.2.1, we have already explained that we spend O~⁢(n)~𝑂𝑛\tilde{O}(n)over~ start_ARG italic_O end_ARG ( italic_n ) time to implement each call to Make-Robust⁢(⋅,⋅)Make-Robust⋅⋅\textsc{Make-Robust}(\cdot\,,\cdot)Make-Robust ( ⋅ , ⋅ ). Finally, Lemmas 6.3 and 5.8 imply that we make O~⁢(1)~𝑂1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) amortized calls to Make-Robust⁢(⋅,⋅)Make-Robust⋅⋅\textsc{Make-Robust}(\cdot\,,\cdot)Make-Robust ( ⋅ , ⋅ ), per update. This gives us an overall amortized update time of O~⁢(n)~𝑂𝑛\tilde{O}(n)over~ start_ARG italic_O end_ARG ( italic_n ). 6.2.2 Algorithm for Lemma 6.2 Since we have a set of fixed k𝑘kitalic_k centers 𝒰𝒰\mathcal{U}caligraphic_U that must be contained in 𝒰⋆superscript𝒰⋆\mathcal{U}^{\star}caligraphic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT, if we can treat these fixed centers as a single center instead of a set of k𝑘kitalic_k centers, we might be able to reduce the problem to a (s+1)𝑠1(s+1)( italic_s + 1 )-median problem. So, we contract all of the points 𝒰𝒰\mathcal{U}caligraphic_U to a single point u⋆superscript𝑢⋆u^{\star}italic_u start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT and define a new space 𝒫′=(𝒫−𝒰)+u⋆superscript𝒫′𝒫𝒰superscript𝑢⋆\mathcal{P}^{\prime}=(\mathcal{P}-\mathcal{U})+u^{\star}caligraphic_P start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = ( caligraphic_P - caligraphic_U ) + italic_u start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT with a new metric d′superscript𝑑′d^{\prime}italic_d start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT as follows. {d′⁢(x,u⋆):=d⁢(x,𝒰)∀x∈𝒫d′⁢(x,y):=min⁡{d′⁢(x,u⋆)+d′⁢(y,u⋆),d⁢(x,y)}∀x,y∈𝒫casesassignsuperscript𝑑′𝑥superscript𝑢⋆𝑑𝑥𝒰for-all𝑥𝒫assignsuperscript𝑑′𝑥𝑦superscript𝑑′𝑥superscript𝑢⋆superscript𝑑′𝑦superscript𝑢⋆𝑑𝑥𝑦for-all𝑥𝑦𝒫\begin{cases}d^{\prime}(x,u^{\star}):=d(x,\mathcal{U})&\forall x\in\mathcal{P}% \\ d^{\prime}(x,y):=\min\{d^{\prime}(x,u^{\star})+d^{\prime}(y,u^{\star}),d(x,y)% \}&\forall x,y\in\mathcal{P}\end{cases}{ start_ROW start_CELL italic_d start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_x , italic_u start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) := italic_d ( italic_x , caligraphic_U ) end_CELL start_CELL ∀ italic_x ∈ caligraphic_P end_CELL end_ROW start_ROW start_CELL italic_d start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_x , italic_y ) := roman_min { italic_d start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_x , italic_u start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) + italic_d start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_y , italic_u start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) , italic_d ( italic_x , italic_y ) } end_CELL start_CELL ∀ italic_x , italic_y ∈ caligraphic_P end_CELL end_ROW (25) This function d′superscript𝑑′d^{\prime}italic_d start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT defines a metric on 𝒫′superscript𝒫′\mathcal{P}^{\prime}caligraphic_P start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT. A simple way to verify this is that d′superscript𝑑′d^{\prime}italic_d start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT is the metric derived from the shortest path in a complete graph where weight of edges between any two nodes in 𝒰𝒰\mathcal{U}caligraphic_U is zero and the weight of the other edges is the d𝑑ditalic_d distance of their endpoints (you can find a complete proof in 11.2). We also define weights for each x∈𝒫′𝑥superscript𝒫′x\in\mathcal{P}^{\prime}italic_x ∈ caligraphic_P start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT. We define the weight of all x∈𝒫′−u⋆𝑥superscript𝒫′superscript𝑢⋆x\in\mathcal{P}^{\prime}-u^{\star}italic_x ∈ caligraphic_P start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT - italic_u start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT to be 1111 and the weight of u⋆superscript𝑢⋆u^{\star}italic_u start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT to be very large denoted by ∞\infty∞ to enforce any constant approximate solution for (s+1)𝑠1(s+1)( italic_s + 1 )-median in 𝒫′superscript𝒫′\mathcal{P}^{\prime}caligraphic_P start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT to open center u⋆superscript𝑢⋆u^{\star}italic_u start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT. In order to have access to metric d′superscript𝑑′d^{\prime}italic_d start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT, we can simply construct an oracle D′superscript𝐷′D^{\prime}italic_D start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT to compute it. This is because we have access to sorted distances of any p∈𝒫𝑝𝒫p\in\mathcal{P}italic_p ∈ caligraphic_P to 𝒰𝒰\mathcal{U}caligraphic_U through 𝒯psubscript𝒯𝑝\mathcal{T}_{p}caligraphic_T start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT in our auxiliary data structure. Combining with the definition of d′superscript𝑑′d^{\prime}italic_d start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT, it is easy to see that we can compute d′⁢(x,y)superscript𝑑′𝑥𝑦d^{\prime}(x,y)italic_d start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_x , italic_y ) in O⁢(1)𝑂1O(1)italic_O ( 1 ) time for each x,y∈𝒫′𝑥𝑦superscript𝒫′x,y\in\mathcal{P}^{\prime}italic_x , italic_y ∈ caligraphic_P start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT. Next, we run the algorithm of [MP02] for (s+1)𝑠1(s+1)( italic_s + 1 )-median problem on 𝒫′superscript𝒫′\mathcal{P}^{\prime}caligraphic_P start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT w.r.t. metric d′superscript𝑑′d^{\prime}italic_d start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT to find a ℱℱ\mathcal{F}caligraphic_F of size at most (s+1)𝑠1(s+1)( italic_s + 1 ) which is a constant approximation for OPTs+1⁢(𝒫′)subscriptOPT𝑠1superscript𝒫′\textnormal{{OPT}}_{s+1}(\mathcal{P}^{\prime})OPT start_POSTSUBSCRIPT italic_s + 1 end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) in a total of O~⁢(|𝒫′|⋅(s+1))=O~⁢(n⋅(ℓ+1))~𝑂⋅superscript𝒫′𝑠1~𝑂⋅𝑛ℓ1\tilde{O}(|\mathcal{P}^{\prime}|\cdot(s+1))=\tilde{O}(n\cdot(\ell+1))over~ start_ARG italic_O end_ARG ( | caligraphic_P start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT | ⋅ ( italic_s + 1 ) ) = over~ start_ARG italic_O end_ARG ( italic_n ⋅ ( roman_ℓ + 1 ) ) time. Finally, we let 𝒰⋆←𝒰+(ℱ−u⋆)←superscript𝒰⋆𝒰ℱsuperscript𝑢⋆\mathcal{U}^{\star}\leftarrow\mathcal{U}+(\mathcal{F}-u^{\star})caligraphic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ← caligraphic_U + ( caligraphic_F - italic_u start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ). Note that ℱ−u⋆⊆𝒫ℱsuperscript𝑢⋆𝒫\mathcal{F}-u^{\star}\subseteq\mathcal{P}caligraphic_F - italic_u start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ⊆ caligraphic_P and its size is at most s𝑠sitalic_s since u⋆∈ℱsuperscript𝑢⋆ℱu^{\star}\in\mathcal{F}italic_u start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ∈ caligraphic_F. This set 𝒰+(ℱ−u⋆)𝒰ℱsuperscript𝑢⋆\mathcal{U}+(\mathcal{F}-u^{\star})caligraphic_U + ( caligraphic_F - italic_u start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) is going to be a good solution w.r.t. metric d𝑑ditalic_d as well as d′superscript𝑑′d^{\prime}italic_d start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT (see Lemma 11.3). 1 𝒫′←(𝒫−𝒰)+u⋆←superscript𝒫′𝒫𝒰superscript𝑢⋆\mathcal{P}^{\prime}\leftarrow(\mathcal{P}-\mathcal{U})+u^{\star}caligraphic_P start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ← ( caligraphic_P - caligraphic_U ) + italic_u start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT. 2w⁢(u⋆)←∞←𝑤superscript𝑢⋆w(u^{\star})\leftarrow\inftyitalic_w ( italic_u start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) ← ∞, w⁢(x)←1⁢∀x∈𝒫′−u⋆←𝑤𝑥1for-all𝑥superscript𝒫′superscript𝑢⋆w(x)\leftarrow 1\ \forall x\in\mathcal{P}^{\prime}-u^{\star}italic_w ( italic_x ) ← 1 ∀ italic_x ∈ caligraphic_P start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT - italic_u start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT. 3Consider D′superscript𝐷′D^{\prime}italic_D start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT as an oracle to the metric d′superscript𝑑′d^{\prime}italic_d start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT defined in Equation 25. 4Compute any O⁢(1)𝑂1O(1)italic_O ( 1 ) approximate solution ℱ⊆𝒫′ℱsuperscript𝒫′\mathcal{F}\subseteq\mathcal{P}^{\prime}caligraphic_F ⊆ caligraphic_P start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT for (s+1)𝑠1(s+1)( italic_s + 1 )-median problem on weighted metric space 𝒫′superscript𝒫′\mathcal{P}^{\prime}caligraphic_P start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT in O~⁢(|𝒫′|⋅(s+1))~𝑂⋅superscript𝒫′𝑠1\tilde{O}(|\mathcal{P}^{\prime}|\cdot(s+1))over~ start_ARG italic_O end_ARG ( | caligraphic_P start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT | ⋅ ( italic_s + 1 ) ) time using [MP02] with access to distance oracle D′superscript𝐷′D^{\prime}italic_D start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT. 5return ←𝒰+(ℱ−u⋆)←absent𝒰ℱsuperscript𝑢⋆\leftarrow\mathcal{U}+(\mathcal{F}-u^{\star})← caligraphic_U + ( caligraphic_F - italic_u start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ). Algorithm 5 Computing an approximation of the optimum 𝒰⋆⊇𝒰𝒰superscript𝒰⋆\mathcal{U}^{\star}\supseteq\mathcal{U}caligraphic_U start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ⊇ caligraphic_U of size (k+s)𝑘𝑠(k+s)( italic_k + italic_s ). 6.3 Improving the Update Time to O~⁢(k)~𝑂𝑘\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) Extension to Weighted Case. First, we argue that our algorithm can be extended to the weighted case defined as follows. We have a metric space 𝒫𝒫\mathcal{P}caligraphic_P with positive weights w⁢(p)𝑤𝑝w(p)italic_w ( italic_p ) for each p∈𝒫𝑝𝒫p\in\mathcal{P}italic_p ∈ caligraphic_P and distance function d𝑑ditalic_d. Denote this weighted space by (𝒫,w,d)𝒫𝑤𝑑(\mathcal{P},w,d)( caligraphic_P , italic_w , italic_d ). The cost of a collection 𝒰𝒰\mathcal{U}caligraphic_U of k𝑘kitalic_k centers is defined as Cost⁢(𝒰,𝒫)=∑p∈𝒫w⁢(p)⋅d⁢(p,𝒰)Cost𝒰𝒫subscript𝑝𝒫⋅𝑤𝑝𝑑𝑝𝒰\textnormal{{Cost}}\left(\mathcal{U},\mathcal{P}\right)=\sum_{p\in\mathcal{P}}% w(p)\cdot d(p,\mathcal{U})Cost ( caligraphic_U , caligraphic_P ) = ∑ start_POSTSUBSCRIPT italic_p ∈ caligraphic_P end_POSTSUBSCRIPT italic_w ( italic_p ) ⋅ italic_d ( italic_p , caligraphic_U ) and subsequently AverageCost⁢(𝒰,S)=Cost⁢(𝒰,S)/(∑p∈Sw⁢(p))AverageCost𝒰𝑆Cost𝒰𝑆subscript𝑝𝑆𝑤𝑝\textnormal{{AverageCost}}\left(\mathcal{U},S\right)=\textnormal{{Cost}}\left(% \mathcal{U},S\right)/(\sum_{p\in S}w(p))AverageCost ( caligraphic_U , italic_S ) = Cost ( caligraphic_U , italic_S ) / ( ∑ start_POSTSUBSCRIPT italic_p ∈ italic_S end_POSTSUBSCRIPT italic_w ( italic_p ) ) for all S⊆𝒫𝑆𝒫S\subseteq\mathcal{P}italic_S ⊆ caligraphic_P. We can extend our algorithm and all of the arguments for weighted case. Sparsification. Note that parameter n𝑛nitalic_n in our algorithm is the maximum size of the space at any time during the total sequence of updates (it is not the size of the underlying ground metric space 𝐏𝐏\mathbf{P}bold_P). As a result, if we make sure that the size of the space 𝒫𝒫\mathcal{P}caligraphic_P is at most O~⁢(k)~𝑂𝑘\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) at any point in time, then the amortized running time of the algorithm would be O~⁢(k)~𝑂𝑘\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) as desired. We use the result of [BCLP23] to sparsify the input. A simple generalization of this result is presented in Section 10 of [BCG+24] which extends this sparsifier to weighted metric spaces. The authors provided an algorithm to sparsify the space to O~⁢(k)~𝑂𝑘\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) weighted points. More precisely, given a dynamic metric space (𝒫,w,d)𝒫𝑤𝑑(\mathcal{P},w,d)( caligraphic_P , italic_w , italic_d ) and parameter k∈ℕ𝑘ℕk\in\mathbb{N}italic_k ∈ blackboard_N, there is an algorithm that maintains a dynamic metric space (𝒫′,w′,d)superscript𝒫′superscript𝑤′𝑑(\mathcal{P}^{\prime},w^{\prime},d)( caligraphic_P start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , italic_w start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , italic_d ) in O~⁢(k)~𝑂𝑘\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) amortized update time such that the following holds. • 𝒫′⊆𝒫superscript𝒫′𝒫\mathcal{P}^{\prime}\subseteq\mathcal{P}caligraphic_P start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ⊆ caligraphic_P and the size of 𝒫′superscript𝒫′\mathcal{P}^{\prime}caligraphic_P start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT at any time is O~⁢(k)~𝑂𝑘\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ). • A sequence of T𝑇Titalic_T updates in (𝒫,w,d)𝒫𝑤𝑑(\mathcal{P},w,d)( caligraphic_P , italic_w , italic_d ), leads to a sequence of O⁢(T)𝑂𝑇O(T)italic_O ( italic_T ) updates in (𝒫′,w′,d)superscript𝒫′superscript𝑤′𝑑(\mathcal{P}^{\prime},w^{\prime},d)( caligraphic_P start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , italic_w start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , italic_d ).131313This guarantee follows from a slightly more refined analysis of the recourse of this sparsifier which is presented in [BCLP24] (see Lemma 3.4 of [BCLP24]). • Every α𝛼\alphaitalic_α approximate solution to the k𝑘kitalic_k-median problem in the metric space (𝒫′,w′,d)superscript𝒫′superscript𝑤′𝑑(\mathcal{P}^{\prime},w^{\prime},d)( caligraphic_P start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , italic_w start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , italic_d ) is also a O⁢(α)𝑂𝛼O(\alpha)italic_O ( italic_α ) approximate solution to the k𝑘kitalic_k-median problem in the metric space (𝒫,w,d)𝒫𝑤𝑑(\mathcal{P},w,d)( caligraphic_P , italic_w , italic_d ) with probability at least 1−O~⁢(1/nc)1~𝑂1superscript𝑛𝑐1-\tilde{O}(1/n^{c})1 - over~ start_ARG italic_O end_ARG ( 1 / italic_n start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ). Now, suppose that we are given a sequence of updates σ1,σ2,…,σTsubscript𝜎1subscript𝜎2…subscript𝜎𝑇\sigma_{1},\sigma_{2},\ldots,\sigma_{T}italic_σ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_σ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_σ start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT in a dynamic metric space (𝒫,w,d)𝒫𝑤𝑑(\mathcal{P},w,d)( caligraphic_P , italic_w , italic_d ). Instead of feeding the metric space (𝒫,w,d)𝒫𝑤𝑑(\mathcal{P},w,d)( caligraphic_P , italic_w , italic_d ) directly to our algorithm, we can perform this dynamic sparsification to obtain a sequence of updates σ1′,σ2′,…,σT′′subscriptsuperscript𝜎′1subscriptsuperscript𝜎′2…subscriptsuperscript𝜎′superscript𝑇′\sigma^{\prime}_{1},\sigma^{\prime}_{2},\ldots,\sigma^{\prime}_{T^{\prime}}italic_σ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_σ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_σ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_T start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT for a metric space (𝒫′,w′,d)superscript𝒫′superscript𝑤′𝑑(\mathcal{P}^{\prime},w^{\prime},d)( caligraphic_P start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , italic_w start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , italic_d ), where T′=O⁢(T)superscript𝑇′𝑂𝑇T^{\prime}=O(T)italic_T start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = italic_O ( italic_T ), and feed the metric space (𝒫′,w′,d)superscript𝒫′superscript𝑤′𝑑(\mathcal{P}^{\prime},w^{\prime},d)( caligraphic_P start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , italic_w start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , italic_d ) to our dynamic algorithm instead. Since our algorithm maintains a O⁢(1)𝑂1O(1)italic_O ( 1 ) approximate solution 𝒰𝒰\mathcal{U}caligraphic_U to the k𝑘kitalic_k-median problem in (𝒫′,w′,d)superscript𝒫′superscript𝑤′𝑑(\mathcal{P}^{\prime},w^{\prime},d)( caligraphic_P start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , italic_w start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , italic_d ), then 𝒰𝒰\mathcal{U}caligraphic_U is also a O⁢(1)𝑂1O(1)italic_O ( 1 ) approximate solution for the k𝑘kitalic_k-median problem in (𝒫,w,d)𝒫𝑤𝑑(\mathcal{P},w,d)( caligraphic_P , italic_w , italic_d ) with high probability. Since the length of the stream is multiplied by O⁢(1)𝑂1O(1)italic_O ( 1 ), we would have a multiplicative overhead of O⁢(1)𝑂1O(1)italic_O ( 1 ) in the amortized update time and recourse (amortized w.r.t. the original input stream). 7 Missing Proofs From Section 4 7.1 Proof of Lemma 4.1: Double-Sided Stability Lemma Consider the LP relaxation for improper k𝑘kitalic_k-median problem on 𝒫𝒫\mathcal{P}caligraphic_P for each k𝑘kitalic_k as follows. min\displaystyle\minroman_min ∑p∈𝒫∑c∈𝐏d⁢(c,p)⋅xc⁢psubscript𝑝𝒫subscript𝑐𝐏⋅𝑑𝑐𝑝subscript𝑥𝑐𝑝\displaystyle\sum_{p\in\mathcal{P}}\sum_{c\in\mathbf{P}}d(c,p)\cdot x_{cp}∑ start_POSTSUBSCRIPT italic_p ∈ caligraphic_P end_POSTSUBSCRIPT ∑ start_POSTSUBSCRIPT italic_c ∈ bold_P end_POSTSUBSCRIPT italic_d ( italic_c , italic_p ) ⋅ italic_x start_POSTSUBSCRIPT italic_c italic_p end_POSTSUBSCRIPT s.t. xc⁢p≤ycsubscript𝑥𝑐𝑝subscript𝑦𝑐\displaystyle\quad x_{cp}\leq y_{c}italic_x start_POSTSUBSCRIPT italic_c italic_p end_POSTSUBSCRIPT ≤ italic_y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ∀c∈𝐏,p∈𝒫formulae-sequencefor-all𝑐𝐏𝑝𝒫\displaystyle\forall c\in\mathbf{P},p\in\mathcal{P}∀ italic_c ∈ bold_P , italic_p ∈ caligraphic_P ∑c∈𝐏xc⁢p≥1subscript𝑐𝐏subscript𝑥𝑐𝑝1\displaystyle\sum_{c\in\mathbf{P}}x_{cp}\geq 1∑ start_POSTSUBSCRIPT italic_c ∈ bold_P end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_c italic_p end_POSTSUBSCRIPT ≥ 1 ∀p∈𝒫for-all𝑝𝒫\displaystyle\forall p\in\mathcal{P}∀ italic_p ∈ caligraphic_P ∑c∈𝐏yc≤ksubscript𝑐𝐏subscript𝑦𝑐𝑘\displaystyle\sum_{c\in\mathbf{P}}y_{c}\leq k∑ start_POSTSUBSCRIPT italic_c ∈ bold_P end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ≤ italic_k xc⁢p,yc≥0subscript𝑥𝑐𝑝subscript𝑦𝑐0\displaystyle x_{cp},y_{c}\geq 0italic_x start_POSTSUBSCRIPT italic_c italic_p end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ≥ 0 ∀c∈𝐏,p∈𝒫formulae-sequencefor-all𝑐𝐏𝑝𝒫\displaystyle\forall c\in\mathbf{P},p\in\mathcal{P}∀ italic_c ∈ bold_P , italic_p ∈ caligraphic_P Denote the cost of the optimal fractional solution for this LP by FOPTksubscriptFOPT𝑘\textnormal{{FOPT}}_{k}FOPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT. Since space 𝒫𝒫\mathcal{P}caligraphic_P is fixed here, we denote OPTk⁢(𝒫)subscriptOPT𝑘𝒫\textnormal{{OPT}}_{k}(\mathcal{P})OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ) by OPTksubscriptOPT𝑘\textnormal{{OPT}}_{k}OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT. It is known that the integrality gap of this relaxation is at most 3333 [CS11]. So, for every k𝑘kitalic_k we have FOPTk≤OPTk≤3⋅FOPTk.subscriptFOPT𝑘subscriptOPT𝑘⋅3subscriptFOPT𝑘\textnormal{{FOPT}}_{k}\leq\textnormal{{OPT}}_{k}\leq 3\cdot\textnormal{{FOPT}% }_{k}.FOPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ≤ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ≤ 3 ⋅ FOPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT . (26) Claim 7.1. For every k1subscript𝑘1k_{1}italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, k2subscript𝑘2k_{2}italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT and 0≤α,β≤1formulae-sequence0𝛼𝛽10\leq\alpha,\beta\leq 10 ≤ italic_α , italic_β ≤ 1 such that α+β=1𝛼𝛽1\alpha+\beta=1italic_α + italic_β = 1, we have FOPTα⁢k1+β⁢k2≤α⋅FOPTk1+β⋅FOPTk2.subscriptFOPT𝛼subscript𝑘1𝛽subscript𝑘2⋅𝛼subscriptFOPTsubscript𝑘1⋅𝛽subscriptFOPTsubscript𝑘2\textnormal{{FOPT}}_{\alpha k_{1}+\beta k_{2}}\leq\alpha\cdot\textnormal{{FOPT% }}_{k_{1}}+\beta\cdot\textnormal{{FOPT}}_{k_{2}}.FOPT start_POSTSUBSCRIPT italic_α italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_β italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ≤ italic_α ⋅ FOPT start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT + italic_β ⋅ FOPT start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT . Proof. Assume optimal fractional solutions (x1∗,y1∗)subscriptsuperscript𝑥1subscriptsuperscript𝑦1(x^{*}_{1},y^{*}_{1})( italic_x start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_y start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) and (x2∗,y2∗)subscriptsuperscript𝑥2subscriptsuperscript𝑦2(x^{*}_{2},y^{*}_{2})( italic_x start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_y start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) to be an optimal solution for above LP for fractional k1subscript𝑘1k_{1}italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and k2subscript𝑘2k_{2}italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT-median problems respectively. It is easy to verify that their convex combination (α⁢x1∗+β⁢x2∗,α⁢y1∗+β⁢y2∗)𝛼subscriptsuperscript𝑥1𝛽subscriptsuperscript𝑥2𝛼subscriptsuperscript𝑦1𝛽subscriptsuperscript𝑦2(\alpha x^{*}_{1}+\beta x^{*}_{2},\alpha y^{*}_{1}+\beta y^{*}_{2})( italic_α italic_x start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_β italic_x start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_α italic_y start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_β italic_y start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) is a feasible solution for LP relaxation of (α⁢k1+β⁢k2)𝛼subscript𝑘1𝛽subscript𝑘2(\alpha k_{1}+\beta k_{2})( italic_α italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_β italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT )-median problem whose cost is α⁢FOPTk1+β⁢FOPTk2𝛼subscriptFOPTsubscript𝑘1𝛽subscriptFOPTsubscript𝑘2\alpha\ \textnormal{{FOPT}}_{k_{1}}+\beta\ \textnormal{{FOPT}}_{k_{2}}italic_α FOPT start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT + italic_β FOPT start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT which concludes the claim. ∎ Now, plug k1=k−rsubscript𝑘1𝑘𝑟k_{1}=k-ritalic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = italic_k - italic_r, k2=k+r/(12⁢η)subscript𝑘2𝑘𝑟12𝜂k_{2}=k+r/(12\eta)italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = italic_k + italic_r / ( 12 italic_η ), α=1/(12⁢η)𝛼112𝜂\alpha=1/(12\eta)italic_α = 1 / ( 12 italic_η ) and β=1−α𝛽1𝛼\beta=1-\alphaitalic_β = 1 - italic_α in the claim. We have α⁢k1+β⁢k2=112⁢η⁢(k−r)+(1−112⁢η)⁢(k+r12⁢η)=k−r(12⁢η)2≤k.𝛼subscript𝑘1𝛽subscript𝑘2112𝜂𝑘𝑟1112𝜂𝑘𝑟12𝜂𝑘𝑟superscript12𝜂2𝑘\alpha k_{1}+\beta k_{2}=\frac{1}{12\eta}(k-r)+\left(1-\frac{1}{12\eta}\right)% \left(k+\frac{r}{12\eta}\right)=k-\frac{r}{(12\eta)^{2}}\leq k.italic_α italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_β italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG 12 italic_η end_ARG ( italic_k - italic_r ) + ( 1 - divide start_ARG 1 end_ARG start_ARG 12 italic_η end_ARG ) ( italic_k + divide start_ARG italic_r end_ARG start_ARG 12 italic_η end_ARG ) = italic_k - divide start_ARG italic_r end_ARG start_ARG ( 12 italic_η ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ≤ italic_k . As a result, FOPTk≤FOPTα⁢k1+β⁢k2≤α⁢FOPTk1+β⁢FOPTk2subscriptFOPT𝑘subscriptFOPT𝛼subscript𝑘1𝛽subscript𝑘2𝛼subscriptFOPTsubscript𝑘1𝛽subscriptFOPTsubscript𝑘2\textnormal{{FOPT}}_{k}\leq\textnormal{{FOPT}}_{\alpha k_{1}+\beta k_{2}}\leq% \alpha\ \textnormal{{FOPT}}_{k_{1}}+\beta\ \textnormal{{FOPT}}_{k_{2}}FOPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ≤ FOPT start_POSTSUBSCRIPT italic_α italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_β italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ≤ italic_α FOPT start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT + italic_β FOPT start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT. Together with Equation 26, we have OPTk≤3⁢α⋅OPTk1+3⁢β⋅OPTk2subscriptOPT𝑘⋅3𝛼subscriptOPTsubscript𝑘1⋅3𝛽subscriptOPTsubscript𝑘2\textnormal{{OPT}}_{k}\leq 3\alpha\cdot\textnormal{{OPT}}_{k_{1}}+3\beta\cdot% \textnormal{{OPT}}_{k_{2}}OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ≤ 3 italic_α ⋅ OPT start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT + 3 italic_β ⋅ OPT start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT. We also have the assumption that OPTk1=OPTk−r≤η⋅OPTksubscriptOPTsubscript𝑘1subscriptOPT𝑘𝑟⋅𝜂subscriptOPT𝑘\textnormal{{OPT}}_{k_{1}}=\textnormal{{OPT}}_{k-r}\leq\eta\cdot\textnormal{{% OPT}}_{k}OPT start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = OPT start_POSTSUBSCRIPT italic_k - italic_r end_POSTSUBSCRIPT ≤ italic_η ⋅ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, which implies OPTk≤3⁢α⁢η⋅OPTk+3⁢β⋅OPTk2subscriptOPT𝑘⋅3𝛼𝜂subscriptOPT𝑘⋅3𝛽subscriptOPTsubscript𝑘2\textnormal{{OPT}}_{k}\leq 3\alpha\eta\cdot\textnormal{{OPT}}_{k}+3\beta\cdot% \textnormal{{OPT}}_{k_{2}}OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ≤ 3 italic_α italic_η ⋅ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + 3 italic_β ⋅ OPT start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT. Finally, OPTk≤(3⁢β1−3⁢α⁢η)⋅OPTk2≤4⋅OPTk2≤4⋅OPTk+⌊r/(12⁢η)⌋.subscriptOPT𝑘⋅3𝛽13𝛼𝜂subscriptOPTsubscript𝑘2⋅4subscriptOPTsubscript𝑘2⋅4subscriptOPT𝑘𝑟12𝜂\textnormal{{OPT}}_{k}\leq\left(\frac{3\beta}{1-3\alpha\eta}\right)\cdot% \textnormal{{OPT}}_{k_{2}}\leq 4\cdot\textnormal{{OPT}}_{k_{2}}\leq 4\cdot% \textnormal{{OPT}}_{k+\lfloor r/(12\eta)\rfloor}.OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ≤ ( divide start_ARG 3 italic_β end_ARG start_ARG 1 - 3 italic_α italic_η end_ARG ) ⋅ OPT start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ≤ 4 ⋅ OPT start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ≤ 4 ⋅ OPT start_POSTSUBSCRIPT italic_k + ⌊ italic_r / ( 12 italic_η ) ⌋ end_POSTSUBSCRIPT . The second inequality holds since 3⁢β1−3⁢α⁢η≤31−3⁢α⁢η=31−1/4=4.3𝛽13𝛼𝜂313𝛼𝜂31144\frac{3\beta}{1-3\alpha\eta}\leq\frac{3}{1-3\alpha\eta}=\frac{3}{1-1/4}=4.divide start_ARG 3 italic_β end_ARG start_ARG 1 - 3 italic_α italic_η end_ARG ≤ divide start_ARG 3 end_ARG start_ARG 1 - 3 italic_α italic_η end_ARG = divide start_ARG 3 end_ARG start_ARG 1 - 1 / 4 end_ARG = 4 . 7.2 Proof of Lemma 4.2 Consider the standard LP relaxation for the weighted k𝑘kitalic_k-median problem as follows. min\displaystyle\minroman_min ∑p∈𝒫∑u∈𝒰d⁢(u,p)⋅xu⁢psubscript𝑝𝒫subscript𝑢𝒰⋅𝑑𝑢𝑝subscript𝑥𝑢𝑝\displaystyle\sum_{p\in\mathcal{P}}\sum_{u\in\mathcal{U}}d(u,p)\cdot x_{up}∑ start_POSTSUBSCRIPT italic_p ∈ caligraphic_P end_POSTSUBSCRIPT ∑ start_POSTSUBSCRIPT italic_u ∈ caligraphic_U end_POSTSUBSCRIPT italic_d ( italic_u , italic_p ) ⋅ italic_x start_POSTSUBSCRIPT italic_u italic_p end_POSTSUBSCRIPT s.t. xu⁢p≤yusubscript𝑥𝑢𝑝subscript𝑦𝑢\displaystyle\quad x_{up}\leq y_{u}italic_x start_POSTSUBSCRIPT italic_u italic_p end_POSTSUBSCRIPT ≤ italic_y start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT ∀u∈𝒰,p∈𝒫formulae-sequencefor-all𝑢𝒰𝑝𝒫\displaystyle\forall u\in\mathcal{U},p\in\mathcal{P}∀ italic_u ∈ caligraphic_U , italic_p ∈ caligraphic_P ∑u∈𝒰xu⁢p≥1subscript𝑢𝒰subscript𝑥𝑢𝑝1\displaystyle\sum_{u\in\mathcal{U}}x_{up}\geq 1∑ start_POSTSUBSCRIPT italic_u ∈ caligraphic_U end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_u italic_p end_POSTSUBSCRIPT ≥ 1 ∀p∈𝒫for-all𝑝𝒫\displaystyle\forall p\in\mathcal{P}∀ italic_p ∈ caligraphic_P ∑u∈𝒰yu≤k−(m−r)/4subscript𝑢𝒰subscript𝑦𝑢𝑘𝑚𝑟4\displaystyle\sum_{u\in\mathcal{U}}y_{u}\leq k-(m-r)/4∑ start_POSTSUBSCRIPT italic_u ∈ caligraphic_U end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT ≤ italic_k - ( italic_m - italic_r ) / 4 xu⁢p,yu≥0subscript𝑥𝑢𝑝subscript𝑦𝑢0\displaystyle x_{up},y_{u}\geq 0italic_x start_POSTSUBSCRIPT italic_u italic_p end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT ≥ 0 ∀u∈𝒰,p∈𝒫formulae-sequencefor-all𝑢𝒰𝑝𝒫\displaystyle\forall u\in\mathcal{U},p\in\mathcal{P}∀ italic_u ∈ caligraphic_U , italic_p ∈ caligraphic_P We consider the set of potential centers to open is 𝒰𝒰\mathcal{U}caligraphic_U, and we want to open at most k−(m−r)/4𝑘𝑚𝑟4k-(m-r)/4italic_k - ( italic_m - italic_r ) / 4 many centers. Since the integrality gap of this LP is known to be at most 3333 [CS11], it suffices to show the existence of a fractional solution whose cost is at most 2⁢γ⋅(Cost⁢(𝒰,𝒫)+Cost⁢(𝒱,𝒫))⋅2𝛾Cost𝒰𝒫Cost𝒱𝒫2\gamma\cdot\left(\textnormal{{Cost}}\left(\mathcal{U},\mathcal{P}\right)+% \textnormal{{Cost}}\left(\mathcal{V},\mathcal{P}\right)\right)2 italic_γ ⋅ ( Cost ( caligraphic_U , caligraphic_P ) + Cost ( caligraphic_V , caligraphic_P ) ) and this solution opens at most k−(m−r)/4𝑘𝑚𝑟4k-(m-r)/4italic_k - ( italic_m - italic_r ) / 4 centers. Now, we explain how to construct a fractional solution for this LP. Fractional Opening of Centers. Consider the projection π𝒰:𝒱→𝒰:subscript𝜋𝒰→𝒱𝒰\pi_{\mathcal{U}}:\mathcal{V}\rightarrow\mathcal{U}italic_π start_POSTSUBSCRIPT caligraphic_U end_POSTSUBSCRIPT : caligraphic_V → caligraphic_U function. Assume 𝒰=𝒰I+𝒰F𝒰subscript𝒰𝐼subscript𝒰𝐹\mathcal{U}=\mathcal{U}_{I}+\mathcal{U}_{F}caligraphic_U = caligraphic_U start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT + caligraphic_U start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT is a partition of 𝒰𝒰\mathcal{U}caligraphic_U where 𝒰Isubscript𝒰𝐼\mathcal{U}_{I}caligraphic_U start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT contains those centers u∈𝒰𝑢𝒰u\in\mathcal{U}italic_u ∈ caligraphic_U satisfying at least one of the following conditions: • u𝑢uitalic_u forms a well-separated pair with one center in 𝒱𝒱\mathcal{V}caligraphic_V. • |π𝒰−1⁢(u)|≥2subscriptsuperscript𝜋1𝒰𝑢2|\pi^{-1}_{\mathcal{U}}(u)|\geq 2| italic_π start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT caligraphic_U end_POSTSUBSCRIPT ( italic_u ) | ≥ 2. For every u∈𝒰I𝑢subscript𝒰𝐼u\in\mathcal{U}_{I}italic_u ∈ caligraphic_U start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT, set yu=1subscript𝑦𝑢1y_{u}=1italic_y start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT = 1 and for every u∈𝒰F𝑢subscript𝒰𝐹u\in\mathcal{U}_{F}italic_u ∈ caligraphic_U start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT set yu=1/2subscript𝑦𝑢12y_{u}=1/2italic_y start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT = 1 / 2. Each center u∈𝒰𝑢𝒰u\in\mathcal{U}italic_u ∈ caligraphic_U that forms a well-separated pair with a center v∈𝒱𝑣𝒱v\in\mathcal{V}italic_v ∈ caligraphic_V has |π𝒰−1⁢(u)|≥1subscriptsuperscript𝜋1𝒰𝑢1|\pi^{-1}_{\mathcal{U}}(u)|\geq 1| italic_π start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT caligraphic_U end_POSTSUBSCRIPT ( italic_u ) | ≥ 1 since u𝑢uitalic_u must be the closest center to v𝑣vitalic_v in 𝒰𝒰\mathcal{U}caligraphic_U. Since the number of well-separated pairs is k−m𝑘𝑚k-mitalic_k - italic_m, we have k+r=|𝒱|=∑u∈𝒰|π𝒰−1⁢(u)|≥∑u∈𝒰I|π𝒰−1⁢(u)|≥1⋅(k−m)+2⋅(|𝒰I|−(k−m)).𝑘𝑟𝒱subscript𝑢𝒰subscriptsuperscript𝜋1𝒰𝑢subscript𝑢subscript𝒰𝐼subscriptsuperscript𝜋1𝒰𝑢⋅1𝑘𝑚⋅2subscript𝒰𝐼𝑘𝑚k+r=|\mathcal{V}|=\sum_{u\in\mathcal{U}}|\pi^{-1}_{\mathcal{U}}(u)|\geq\sum_{u% \in\mathcal{U}_{I}}|\pi^{-1}_{\mathcal{U}}(u)|\geq 1\cdot(k-m)+2\cdot\left(|% \mathcal{U}_{I}|-(k-m)\right).italic_k + italic_r = | caligraphic_V | = ∑ start_POSTSUBSCRIPT italic_u ∈ caligraphic_U end_POSTSUBSCRIPT | italic_π start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT caligraphic_U end_POSTSUBSCRIPT ( italic_u ) | ≥ ∑ start_POSTSUBSCRIPT italic_u ∈ caligraphic_U start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT end_POSTSUBSCRIPT | italic_π start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT caligraphic_U end_POSTSUBSCRIPT ( italic_u ) | ≥ 1 ⋅ ( italic_k - italic_m ) + 2 ⋅ ( | caligraphic_U start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT | - ( italic_k - italic_m ) ) . Hence, |𝒰I|≤k+r+(k−m)2=k−m−r2subscript𝒰𝐼𝑘𝑟𝑘𝑚2𝑘𝑚𝑟2|\mathcal{U}_{I}|\leq\frac{k+r+(k-m)}{2}=k-\frac{m-r}{2}| caligraphic_U start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT | ≤ divide start_ARG italic_k + italic_r + ( italic_k - italic_m ) end_ARG start_ARG 2 end_ARG = italic_k - divide start_ARG italic_m - italic_r end_ARG start_ARG 2 end_ARG. Finally, we conclude ∑u∈𝒰yu=∑u∈𝒰Iyu+∑u∈𝒰Fyu≤1⋅(k−m−r2)+12⋅m−r2=k−m−r4.subscript𝑢𝒰subscript𝑦𝑢subscript𝑢subscript𝒰𝐼subscript𝑦𝑢subscript𝑢subscript𝒰𝐹subscript𝑦𝑢⋅1𝑘𝑚𝑟2⋅12𝑚𝑟2𝑘𝑚𝑟4\displaystyle\sum_{u\in\mathcal{U}}y_{u}=\sum_{u\in\mathcal{U}_{I}}y_{u}+\sum_% {u\in\mathcal{U}_{F}}y_{u}\leq 1\cdot\left(k-\frac{m-r}{2}\right)+\frac{1}{2}% \cdot\frac{m-r}{2}=k-\frac{m-r}{4}.∑ start_POSTSUBSCRIPT italic_u ∈ caligraphic_U end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT = ∑ start_POSTSUBSCRIPT italic_u ∈ caligraphic_U start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT + ∑ start_POSTSUBSCRIPT italic_u ∈ caligraphic_U start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT ≤ 1 ⋅ ( italic_k - divide start_ARG italic_m - italic_r end_ARG start_ARG 2 end_ARG ) + divide start_ARG 1 end_ARG start_ARG 2 end_ARG ⋅ divide start_ARG italic_m - italic_r end_ARG start_ARG 2 end_ARG = italic_k - divide start_ARG italic_m - italic_r end_ARG start_ARG 4 end_ARG . Fractional Assignment of Points. For every p∈𝒫𝑝𝒫p\in\mathcal{P}italic_p ∈ caligraphic_P, assume vp=π𝒱⁢(p)subscript𝑣𝑝subscript𝜋𝒱𝑝v_{p}=\pi_{\mathcal{V}}(p)italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT = italic_π start_POSTSUBSCRIPT caligraphic_V end_POSTSUBSCRIPT ( italic_p ) is the closest center to p𝑝pitalic_p in 𝒱𝒱\mathcal{V}caligraphic_V and up=π𝒰⁢(vp)subscript𝑢𝑝subscript𝜋𝒰subscript𝑣𝑝u_{p}=\pi_{\mathcal{U}}(v_{p})italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT = italic_π start_POSTSUBSCRIPT caligraphic_U end_POSTSUBSCRIPT ( italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) is the closest center in 𝒰𝒰\mathcal{U}caligraphic_U to vpsubscript𝑣𝑝v_{p}italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT. We have three cases: • If yup=1subscript𝑦subscript𝑢𝑝1y_{u_{p}}=1italic_y start_POSTSUBSCRIPT italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT end_POSTSUBSCRIPT = 1, then set xup⁢p=1subscript𝑥subscript𝑢𝑝𝑝1x_{u_{p}p}=1italic_x start_POSTSUBSCRIPT italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT = 1. The cost of this assignment would be d⁢(up,p)𝑑subscript𝑢𝑝𝑝d(u_{p},p)italic_d ( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_p ). • If yup=1/2subscript𝑦subscript𝑢𝑝12y_{u_{p}}=1/2italic_y start_POSTSUBSCRIPT italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT end_POSTSUBSCRIPT = 1 / 2 and there is a center up′∈𝒰−upsubscriptsuperscript𝑢′𝑝𝒰subscript𝑢𝑝u^{\prime}_{p}\in\mathcal{U}-u_{p}italic_u start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ∈ caligraphic_U - italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT such that d⁢(up,up′)≤γ⋅d⁢(up,vp)𝑑subscript𝑢𝑝subscriptsuperscript𝑢′𝑝⋅𝛾𝑑subscript𝑢𝑝subscript𝑣𝑝d(u_{p},u^{\prime}_{p})\leq\gamma\cdot d(u_{p},v_{p})italic_d ( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_u start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) ≤ italic_γ ⋅ italic_d ( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ), then set xup⁢p=xup′⁢p=1/2subscript𝑥subscript𝑢𝑝𝑝subscript𝑥superscriptsubscript𝑢𝑝′𝑝12x_{u_{p}p}=x_{u_{p}^{\prime}p}=1/2italic_x start_POSTSUBSCRIPT italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT italic_p end_POSTSUBSCRIPT = 1 / 2. Note that up′≠upsuperscriptsubscript𝑢𝑝′subscript𝑢𝑝u_{p}^{\prime}\neq u_{p}italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ≠ italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT which means this solution is feasible to the LP. The cost of this assignment would be 12⋅(d⁢(p,up)+d⁢(p,up′))⋅12𝑑𝑝subscript𝑢𝑝𝑑𝑝superscriptsubscript𝑢𝑝′\displaystyle\frac{1}{2}\cdot\left(d(p,u_{p})+d(p,u_{p}^{\prime})\right)divide start_ARG 1 end_ARG start_ARG 2 end_ARG ⋅ ( italic_d ( italic_p , italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) + italic_d ( italic_p , italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) ) ≤\displaystyle\leq≤ 12⋅(d⁢(p,up)+d⁢(p,up)+d⁢(up′,up))⋅12𝑑𝑝subscript𝑢𝑝𝑑𝑝subscript𝑢𝑝𝑑subscriptsuperscript𝑢′𝑝subscript𝑢𝑝\displaystyle\frac{1}{2}\cdot\left(d(p,u_{p})+d(p,u_{p})+d(u^{\prime}_{p},u_{p% })\right)divide start_ARG 1 end_ARG start_ARG 2 end_ARG ⋅ ( italic_d ( italic_p , italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) + italic_d ( italic_p , italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) + italic_d ( italic_u start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) ) ≤\displaystyle\leq≤ d⁢(p,up)+γ2⋅d⁢(up,vp).𝑑𝑝subscript𝑢𝑝⋅𝛾2𝑑subscript𝑢𝑝subscript𝑣𝑝\displaystyle d(p,u_{p})+\frac{\gamma}{2}\cdot d(u_{p},v_{p}).italic_d ( italic_p , italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) + divide start_ARG italic_γ end_ARG start_ARG 2 end_ARG ⋅ italic_d ( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) . • If yup=1/2subscript𝑦subscript𝑢𝑝12y_{u_{p}}=1/2italic_y start_POSTSUBSCRIPT italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT end_POSTSUBSCRIPT = 1 / 2 and the previous case does not hold, then since (up,vp)subscript𝑢𝑝subscript𝑣𝑝(u_{p},v_{p})( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) is not a well-separated pair, there is a center vp′∈𝒱−vpsubscriptsuperscript𝑣′𝑝𝒱subscript𝑣𝑝v^{\prime}_{p}\in\mathcal{V}-v_{p}italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ∈ caligraphic_V - italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT such that d⁢(vp,vp′)≤γ⋅d⁢(up,vp)𝑑subscript𝑣𝑝subscriptsuperscript𝑣′𝑝⋅𝛾𝑑subscript𝑢𝑝subscript𝑣𝑝d(v_{p},v^{\prime}_{p})\leq\gamma\cdot d(u_{p},v_{p})italic_d ( italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) ≤ italic_γ ⋅ italic_d ( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ). Let up′=π𝒰⁢(vp′)superscriptsubscript𝑢𝑝′subscript𝜋𝒰subscriptsuperscript𝑣′𝑝u_{p}^{\prime}=\pi_{\mathcal{U}}(v^{\prime}_{p})italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = italic_π start_POSTSUBSCRIPT caligraphic_U end_POSTSUBSCRIPT ( italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) and set xup⁢p=xup′⁢p=1/2subscript𝑥subscript𝑢𝑝𝑝subscript𝑥subscriptsuperscript𝑢′𝑝𝑝12x_{u_{p}p}=x_{u^{\prime}_{p}p}=1/2italic_x start_POSTSUBSCRIPT italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT italic_u start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT = 1 / 2. First, we show that up′≠upsubscriptsuperscript𝑢′𝑝subscript𝑢𝑝u^{\prime}_{p}\neq u_{p}italic_u start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ≠ italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT. Since yup=1/2subscript𝑦subscript𝑢𝑝12y_{u_{p}}=1/2italic_y start_POSTSUBSCRIPT italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT end_POSTSUBSCRIPT = 1 / 2, we have up∈𝒰Fsubscript𝑢𝑝subscript𝒰𝐹u_{p}\in\mathcal{U}_{F}italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ∈ caligraphic_U start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT which concludes |π𝒰−1⁢(up)|≤1subscriptsuperscript𝜋1𝒰subscript𝑢𝑝1|\pi^{-1}_{\mathcal{U}}(u_{p})|\leq 1| italic_π start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT caligraphic_U end_POSTSUBSCRIPT ( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) | ≤ 1. We also know that π𝒰⁢(vp)=upsubscript𝜋𝒰subscript𝑣𝑝subscript𝑢𝑝\pi_{\mathcal{U}}(v_{p})=u_{p}italic_π start_POSTSUBSCRIPT caligraphic_U end_POSTSUBSCRIPT ( italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) = italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT. So, vpsubscript𝑣𝑝v_{p}italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT is the only center in 𝒱𝒱\mathcal{V}caligraphic_V mapped to upsubscript𝑢𝑝u_{p}italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT which implies π𝒰⁢(vp′)≠upsubscript𝜋𝒰subscriptsuperscript𝑣′𝑝subscript𝑢𝑝\pi_{\mathcal{U}}(v^{\prime}_{p})\neq u_{p}italic_π start_POSTSUBSCRIPT caligraphic_U end_POSTSUBSCRIPT ( italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) ≠ italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT or up′≠upsuperscriptsubscript𝑢𝑝′subscript𝑢𝑝u_{p}^{\prime}\neq u_{p}italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ≠ italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT (note that vp′≠vpsuperscriptsubscript𝑣𝑝′subscript𝑣𝑝v_{p}^{\prime}\neq v_{p}italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ≠ italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT). So, point p𝑝pitalic_p is assigned one unit to centers. The cost of this assignment would be 12⋅(d⁢(p,up)+d⁢(p,up′))⋅12𝑑𝑝subscript𝑢𝑝𝑑𝑝subscriptsuperscript𝑢′𝑝\displaystyle\frac{1}{2}\cdot\left(d(p,u_{p})+d(p,u^{\prime}_{p})\right)divide start_ARG 1 end_ARG start_ARG 2 end_ARG ⋅ ( italic_d ( italic_p , italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) + italic_d ( italic_p , italic_u start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) ) ≤\displaystyle\leq≤ 12⋅(d⁢(p,up)+d⁢(p,up)+d⁢(up,vp′)+d⁢(vp′,up′))⋅12𝑑𝑝subscript𝑢𝑝𝑑𝑝subscript𝑢𝑝𝑑subscript𝑢𝑝subscriptsuperscript𝑣′𝑝𝑑subscriptsuperscript𝑣′𝑝subscriptsuperscript𝑢′𝑝\displaystyle\frac{1}{2}\cdot\left(d(p,u_{p})+d(p,u_{p})+d(u_{p},v^{\prime}_{p% })+d(v^{\prime}_{p},u^{\prime}_{p})\right)divide start_ARG 1 end_ARG start_ARG 2 end_ARG ⋅ ( italic_d ( italic_p , italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) + italic_d ( italic_p , italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) + italic_d ( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) + italic_d ( italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_u start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) ) ≤\displaystyle\leq≤ 12⋅(d⁢(p,up)+d⁢(p,up)+d⁢(up,vp′)+d⁢(vp′,up))⋅12𝑑𝑝subscript𝑢𝑝𝑑𝑝subscript𝑢𝑝𝑑subscript𝑢𝑝subscriptsuperscript𝑣′𝑝𝑑subscriptsuperscript𝑣′𝑝subscript𝑢𝑝\displaystyle\frac{1}{2}\cdot\left(d(p,u_{p})+d(p,u_{p})+d(u_{p},v^{\prime}_{p% })+d(v^{\prime}_{p},u_{p})\right)divide start_ARG 1 end_ARG start_ARG 2 end_ARG ⋅ ( italic_d ( italic_p , italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) + italic_d ( italic_p , italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) + italic_d ( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) + italic_d ( italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) ) =\displaystyle== d⁢(p,up)+d⁢(up,vp′)𝑑𝑝subscript𝑢𝑝𝑑subscript𝑢𝑝superscriptsubscript𝑣𝑝′\displaystyle d(p,u_{p})+d(u_{p},v_{p}^{\prime})italic_d ( italic_p , italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) + italic_d ( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) ≤\displaystyle\leq≤ d⁢(p,up)+d⁢(up,vp)+d⁢(vp,vp′)𝑑𝑝subscript𝑢𝑝𝑑subscript𝑢𝑝subscript𝑣𝑝𝑑subscript𝑣𝑝superscriptsubscript𝑣𝑝′\displaystyle d(p,u_{p})+d(u_{p},v_{p})+d(v_{p},v_{p}^{\prime})italic_d ( italic_p , italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) + italic_d ( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) + italic_d ( italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) ≤\displaystyle\leq≤ d⁢(p,up)+(γ+1)⋅d⁢(up,vp).𝑑𝑝subscript𝑢𝑝⋅𝛾1𝑑subscript𝑢𝑝subscript𝑣𝑝\displaystyle d(p,u_{p})+(\gamma+1)\cdot d(u_{p},v_{p}).italic_d ( italic_p , italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) + ( italic_γ + 1 ) ⋅ italic_d ( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) . The second inequality is because of the choice of up′=π𝒰⁢(vp′)subscriptsuperscript𝑢′𝑝subscript𝜋𝒰superscriptsubscript𝑣𝑝′u^{\prime}_{p}=\pi_{\mathcal{U}}(v_{p}^{\prime})italic_u start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT = italic_π start_POSTSUBSCRIPT caligraphic_U end_POSTSUBSCRIPT ( italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) and the last inequality is because d⁢(vp,vp′)≤γ⋅d⁢(up,vp)𝑑subscript𝑣𝑝superscriptsubscript𝑣𝑝′⋅𝛾𝑑subscript𝑢𝑝subscript𝑣𝑝d(v_{p},v_{p}^{\prime})\leq\gamma\cdot d(u_{p},v_{p})italic_d ( italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) ≤ italic_γ ⋅ italic_d ( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ). Bounding the Cost. Assume up∗=π𝒰⁢(p)superscriptsubscript𝑢𝑝subscript𝜋𝒰𝑝u_{p}^{*}=\pi_{\mathcal{U}}(p)italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT = italic_π start_POSTSUBSCRIPT caligraphic_U end_POSTSUBSCRIPT ( italic_p ) for each p∈𝒫𝑝𝒫p\in\mathcal{P}italic_p ∈ caligraphic_P. We have d⁢(up,vp)≤d⁢(up∗,vp)≤d⁢(up∗,p)+d⁢(p,vp).𝑑subscript𝑢𝑝subscript𝑣𝑝𝑑superscriptsubscript𝑢𝑝subscript𝑣𝑝𝑑superscriptsubscript𝑢𝑝𝑝𝑑𝑝subscript𝑣𝑝d(u_{p},v_{p})\leq d(u_{p}^{*},v_{p})\leq d(u_{p}^{*},p)+d(p,v_{p}).italic_d ( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) ≤ italic_d ( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT , italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) ≤ italic_d ( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT , italic_p ) + italic_d ( italic_p , italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) . The first inequality is by the choice of up=π𝒰⁢(vp)subscript𝑢𝑝subscript𝜋𝒰subscript𝑣𝑝u_{p}=\pi_{\mathcal{U}}(v_{p})italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT = italic_π start_POSTSUBSCRIPT caligraphic_U end_POSTSUBSCRIPT ( italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ). As a result, ∑p∈𝒫d⁢(up,vp)≤∑p∈𝒫(d⁢(up∗,p)+d⁢(p,vp))=Cost⁢(𝒰,𝒫)+Cost⁢(𝒱,𝒫).subscript𝑝𝒫𝑑subscript𝑢𝑝subscript𝑣𝑝subscript𝑝𝒫𝑑subscriptsuperscript𝑢𝑝𝑝𝑑𝑝subscript𝑣𝑝Cost𝒰𝒫Cost𝒱𝒫\sum_{p\in\mathcal{P}}d(u_{p},v_{p})\leq\sum_{p\in\mathcal{P}}\left(d(u^{*}_{p% },p)+d(p,v_{p})\right)=\textnormal{{Cost}}\left(\mathcal{U},\mathcal{P}\right)% +\textnormal{{Cost}}\left(\mathcal{V},\mathcal{P}\right).∑ start_POSTSUBSCRIPT italic_p ∈ caligraphic_P end_POSTSUBSCRIPT italic_d ( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) ≤ ∑ start_POSTSUBSCRIPT italic_p ∈ caligraphic_P end_POSTSUBSCRIPT ( italic_d ( italic_u start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_p ) + italic_d ( italic_p , italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) ) = Cost ( caligraphic_U , caligraphic_P ) + Cost ( caligraphic_V , caligraphic_P ) . (27) In each of the cases of fractional assignments of points to centers, the cost of assigning a point p∈Cv⁢(𝒱,𝒫)𝑝subscript𝐶𝑣𝒱𝒫p\in C_{v}(\mathcal{V},\mathcal{P})italic_p ∈ italic_C start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( caligraphic_V , caligraphic_P ) is at most d⁢(p,up)+(γ+1)⋅d⁢(up,vp)𝑑𝑝subscript𝑢𝑝⋅𝛾1𝑑subscript𝑢𝑝subscript𝑣𝑝d(p,u_{p})+(\gamma+1)\cdot d(u_{p},v_{p})italic_d ( italic_p , italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) + ( italic_γ + 1 ) ⋅ italic_d ( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ). As a result, the total cost of this assignment is upper bounded by ∑p∈P(d⁢(p,up)+(γ+1)⋅d⁢(up,vp))subscript𝑝𝑃𝑑𝑝subscript𝑢𝑝⋅𝛾1𝑑subscript𝑢𝑝subscript𝑣𝑝\displaystyle\sum_{p\in P}\left(d(p,u_{p})+(\gamma+1)\cdot d(u_{p},v_{p})\right)∑ start_POSTSUBSCRIPT italic_p ∈ italic_P end_POSTSUBSCRIPT ( italic_d ( italic_p , italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) + ( italic_γ + 1 ) ⋅ italic_d ( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) ) ≤\displaystyle\leq≤ ∑p∈𝒫(d⁢(p,vp)+(γ+2)⋅d⁢(up,vp))subscript𝑝𝒫𝑑𝑝subscript𝑣𝑝⋅𝛾2𝑑subscript𝑢𝑝subscript𝑣𝑝\displaystyle\sum_{p\in\mathcal{P}}\left(d(p,v_{p})+(\gamma+2)\cdot d(u_{p},v_% {p})\right)∑ start_POSTSUBSCRIPT italic_p ∈ caligraphic_P end_POSTSUBSCRIPT ( italic_d ( italic_p , italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) + ( italic_γ + 2 ) ⋅ italic_d ( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) ) ≤\displaystyle\leq≤ Cost⁢(𝒱,𝒫)+(γ+2)⋅(Cost⁢(𝒰,𝒫)+Cost⁢(𝒱,𝒫))Cost𝒱𝒫⋅𝛾2Cost𝒰𝒫Cost𝒱𝒫\displaystyle\textnormal{{Cost}}\left(\mathcal{V},\mathcal{P}\right)+(\gamma+2% )\cdot\left(\textnormal{{Cost}}\left(\mathcal{U},\mathcal{P}\right)+% \textnormal{{Cost}}\left(\mathcal{V},\mathcal{P}\right)\right)Cost ( caligraphic_V , caligraphic_P ) + ( italic_γ + 2 ) ⋅ ( Cost ( caligraphic_U , caligraphic_P ) + Cost ( caligraphic_V , caligraphic_P ) ) ≤\displaystyle\leq≤ 2⁢γ⋅(Cost⁢(𝒰,𝒫)+Cost⁢(𝒱,𝒫)).⋅2𝛾Cost𝒰𝒫Cost𝒱𝒫\displaystyle 2\gamma\cdot\left(\textnormal{{Cost}}\left(\mathcal{U},\mathcal{% P}\right)+\textnormal{{Cost}}\left(\mathcal{V},\mathcal{P}\right)\right).2 italic_γ ⋅ ( Cost ( caligraphic_U , caligraphic_P ) + Cost ( caligraphic_V , caligraphic_P ) ) . The second inequality follows by Equation 27."
https://arxiv.org/html/2411.03071v1,Multi-dimensional Approximate Counting††thanks:This work was supported by NSF Grant CCF-2221980.,"The celebrated Morris counter uses log2⁡log2⁡n+O⁢(log2⁡σ−1)subscript2subscript2𝑛𝑂subscript2superscript𝜎1\log_{2}\log_{2}n+O(\log_{2}\sigma^{-1})roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_n + italic_O ( roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_σ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) bits to count up to n𝑛nitalic_n with a relative error σ𝜎\sigmaitalic_σ, where if λ^^𝜆\hat{\lambda}over^ start_ARG italic_λ end_ARG is the estimate of the current count λ𝜆\lambdaitalic_λ, then 𝔼⁢|λ^−λ|2<σ2⁢λ2𝔼superscript^𝜆𝜆2superscript𝜎2superscript𝜆2\mathbb{E}|\hat{\lambda}-\lambda|^{2}<\sigma^{2}\lambda^{2}blackboard_E | over^ start_ARG italic_λ end_ARG - italic_λ | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT < italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_λ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. The Morris counter was proved to be optimal in space complexity by Nelson and Yu [14], even when considering the error tails. A natural generalization is multi-dimensional approximate counting. Let d≥1𝑑1d\geq 1italic_d ≥ 1 be the dimension. The count vector x∈ℕd𝑥superscriptℕ𝑑x\in\mathbb{N}^{d}italic_x ∈ blackboard_N start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT is incremented entry-wisely over a stream of coordinates (w1,…,wn)∈[d]nsubscript𝑤1…subscript𝑤𝑛superscriptdelimited-[]𝑑𝑛(w_{1},\ldots,w_{n})\in[d]^{n}( italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_w start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) ∈ [ italic_d ] start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, where upon receiving wk∈[d]subscript𝑤𝑘delimited-[]𝑑w_{k}\in[d]italic_w start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∈ [ italic_d ], xwk←xwk+1←subscript𝑥subscript𝑤𝑘subscript𝑥subscript𝑤𝑘1x_{w_{k}}\leftarrow x_{w_{k}}+1italic_x start_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT ← italic_x start_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT + 1. A d𝑑ditalic_d-dimensional approximate counter is required to count d𝑑ditalic_d coordinates simultaneously and return an estimate x^^𝑥\hat{x}over^ start_ARG italic_x end_ARG of the count vector x𝑥xitalic_x. Aden-Ali, Han, Nelson, and Yu [1] showed that the trivial solution of using d𝑑ditalic_d Morris counters that track d𝑑ditalic_d coordinates separately is already optimal in space, if each entry only allows error relative to itself, i.e., 𝔼⁢|x^j−xj|2<σ2⁢|xj|2𝔼superscriptsubscript^𝑥𝑗subscript𝑥𝑗2superscript𝜎2superscriptsubscript𝑥𝑗2\mathbb{E}|\hat{x}_{j}-x_{j}|^{2}<\sigma^{2}|x_{j}|^{2}blackboard_E | over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT - italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT < italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT | italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT for each j∈[d]𝑗delimited-[]𝑑j\in[d]italic_j ∈ [ italic_d ]. However, for another natural error metric—the Euclidean mean squared error 𝔼⁢|x^−x|2𝔼superscript^𝑥𝑥2\mathbb{E}|\hat{x}-x|^{2}blackboard_E | over^ start_ARG italic_x end_ARG - italic_x | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT—we show that using d𝑑ditalic_d separate Morris counters is sub-optimal.In this work, we present a simple and optimal d𝑑ditalic_d-dimensional counter with Euclidean relative error σ𝜎\sigmaitalic_σ, i.e., 𝔼⁢|x^−x|2<σ2⁢|x|2𝔼superscript^𝑥𝑥2superscript𝜎2superscript𝑥2\mathbb{E}|\hat{x}-x|^{2}<\sigma^{2}|x|^{2}blackboard_E | over^ start_ARG italic_x end_ARG - italic_x | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT < italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT | italic_x | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT where |x|=∑j=1dxj2𝑥superscriptsubscript𝑗1𝑑superscriptsubscript𝑥𝑗2|x|=\sqrt{\sum_{j=1}^{d}x_{j}^{2}}| italic_x | = square-root start_ARG ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG, with a matching lower bound. We prove the following.There exists a (log2⁡log2⁡n+O⁢(d⁢log2⁡σ−1))subscript2subscript2𝑛𝑂𝑑subscript2superscript𝜎1(\log_{2}\log_{2}n+O(d\log_{2}\sigma^{-1}))( roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_n + italic_O ( italic_d roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_σ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) )-bit d𝑑ditalic_d-dimensional counter with relative error σ𝜎\sigmaitalic_σ.Any d𝑑ditalic_d-dimensional counter with relative error σ𝜎\sigmaitalic_σ takes at least (log2⁡log2⁡n+Ω⁢(d⁢log2⁡σ−1))subscript2subscript2𝑛Ω𝑑subscript2superscript𝜎1(\log_{2}\log_{2}n+\Omega(d\log_{2}\sigma^{-1}))( roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_n + roman_Ω ( italic_d roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_σ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) ) bits of space.The upper and lower bounds are proved with ideas that are strikingly simple. The upper bound is constructed with a certain variable-length integer encoding and the lower bound is derived from a straightforward volumetric estimation of sphere covering.","In 1978, Morris [13] invented the classic approximate counter which can count up to n𝑛nitalic_n with log2⁡log2⁡n+O⁢(log2⁡σ−1)subscript2subscript2𝑛𝑂subscript2superscript𝜎1\log_{2}\log_{2}n+O(\log_{2}\sigma^{-1})roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_n + italic_O ( roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_σ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) bits, returning an estimate of the current count with a relative error σ𝜎\sigmaitalic_σ. Such approximate counters are invented mainly to save space. However, it was noticed quite recently that approximate counters can also be much faster on modern hardware in comparison to the deterministic counter due to their low write complexity [18, 17, 9]. In the original paper [13], Morris analyzed the mean and variance of the estimates. Flajolet [6] analyzed the mean and variance of the index111Roughly speaking, Morris counter stores an index v𝑣vitalic_v to represent a count around 2vsuperscript2𝑣2^{v}2 start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT. Therefore, to count up to n𝑛nitalic_n, the index increases to log⁡n𝑛\log nroman_log italic_n and thus it takes log⁡log⁡n𝑛\log\log nroman_log roman_log italic_n to store the index. of the Morris counter, from which a quantified space bound can be derived with Chebyshev’s bound. Nelson and Yu [14] analyzed the tail of the index, obtaining a sharper dependence on the failure probability. Specifically, for increments up to λ𝜆\lambdaitalic_λ and ϵ,δ>0italic-ϵ𝛿0\epsilon,\delta>0italic_ϵ , italic_δ > 0, if an estimate λ^^𝜆\hat{\lambda}over^ start_ARG italic_λ end_ARG is produced with ℙ⁢(|λ^−λ|<ϵ⁢λ)≥1−δℙ^𝜆𝜆italic-ϵ𝜆1𝛿\mathbb{P}(|\hat{\lambda}-\lambda|<\epsilon\lambda)\geq 1-\deltablackboard_P ( | over^ start_ARG italic_λ end_ARG - italic_λ | < italic_ϵ italic_λ ) ≥ 1 - italic_δ then the Morris counter needs O⁢(log⁡log⁡n+log⁡ϵ−1+log⁡log⁡δ−1)𝑂𝑛superscriptitalic-ϵ1superscript𝛿1O(\log\log n+\log\epsilon^{-1}+\log\log\delta^{-1})italic_O ( roman_log roman_log italic_n + roman_log italic_ϵ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT + roman_log roman_log italic_δ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) space, and this is proved to be optimal [14].222Technically, one needs to keep a separate deterministic counter for small λ=O⁢(a)𝜆𝑂𝑎\lambda=O(a)italic_λ = italic_O ( italic_a ) to obtain the optimal failure rate [14]. It is the common scenario in real-world applications that many different counters are maintained simultaneously. For example, the original motivation of Morris is to count the number of each trigram in texts [12, 14] where d=263𝑑superscript263d=26^{3}italic_d = 26 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT different trigrams are counted simultaneously. Aden-Ali, Han, Nelson, and Yu [1] showed that, essentially, d𝑑ditalic_d independent counters cannot be compressed as long as each counter is required to report an estimate with an error relative to itself. However, as we will later show, the space can be compressed if the error metric is the d𝑑ditalic_d-dimensional Euclidean norm. While the per-entry relative error requirement considered in [1] is reasonable when an approximation for each individual entry is needed, we remark that in applications where an approximation of the whole count vector is needed, the Euclidean error metric is more natural. We now formally define the problem of d𝑑ditalic_d-dimensional counting in terms of Euclidean mean squared error. Let (e1,e2,…,ed)subscript𝑒1subscript𝑒2…subscript𝑒𝑑(e_{1},e_{2},\ldots,e_{d})( italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_e start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) be the standard basis of ℝdsuperscriptℝ𝑑\mathbb{R}^{d}blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT where eksubscript𝑒𝑘e_{k}italic_e start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT has its k𝑘kitalic_kth component being one and other components being zero. Definition 1 ((Euclidean) d𝑑ditalic_d-dimensional counting). Let n≥1𝑛1n\geq 1italic_n ≥ 1 be the maximum stream length, d≥1𝑑1d\geq 1italic_d ≥ 1 be the dimension, and σ>0𝜎0\sigma>0italic_σ > 0 be the relative error. An (n,d,σ)𝑛𝑑𝜎(n,d,\sigma)( italic_n , italic_d , italic_σ )-counter is a randomized data structure that, for any input sequence (w1,…,wk)∈[d]ksubscript𝑤1…subscript𝑤𝑘superscriptdelimited-[]𝑑𝑘(w_{1},\ldots,w_{k})\in[d]^{k}( italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_w start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ∈ [ italic_d ] start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT with k≤n𝑘𝑛k\leq nitalic_k ≤ italic_n, returns an estimate x^^𝑥\hat{x}over^ start_ARG italic_x end_ARG of x=∑j=1kewj𝑥superscriptsubscript𝑗1𝑘subscript𝑒subscript𝑤𝑗x=\sum_{j=1}^{k}e_{w_{j}}italic_x = ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_e start_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT with 𝔼⁢|x^−x|2<σ2⁢|x|2𝔼superscript^𝑥𝑥2superscript𝜎2superscript𝑥2\mathbb{E}|\hat{x}-x|^{2}<\sigma^{2}|x|^{2}blackboard_E | over^ start_ARG italic_x end_ARG - italic_x | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT < italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT | italic_x | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, where |x|=∑j=1dxj2𝑥superscriptsubscript𝑗1𝑑superscriptsubscript𝑥𝑗2|x|=\sqrt{\sum_{j=1}^{d}x_{j}^{2}}| italic_x | = square-root start_ARG ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG is the Euclidean length. We denote the state space of the counter as ΩΩ\Omegaroman_Ω with |Ω|≥1Ω1|\Omega|\geq 1| roman_Ω | ≥ 1. One counter thus uses log2⁡|Ω|subscript2Ω\log_{2}|\Omega|roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | roman_Ω | bits of space. Recall that the original Morris counter [13] with parameter a≥1𝑎1a\geq 1italic_a ≥ 1, denoted as Morris⁢(a)Morris𝑎\mathrm{Morris}(a)roman_Morris ( italic_a ), returns an unbiased estimate of the count with relative variance O⁢(1/a)𝑂1𝑎O(1/a)italic_O ( 1 / italic_a ). Thus by definition, Morris⁢(a)Morris𝑎\mathrm{Morris}(a)roman_Morris ( italic_a ) is an (n,1,O⁢(1/a))𝑛1𝑂1𝑎(n,1,O(\sqrt{1/a}))( italic_n , 1 , italic_O ( square-root start_ARG 1 / italic_a end_ARG ) )-counter. The error here is measured by the mean squared error 𝔼⁢|x^−x|2𝔼superscript^𝑥𝑥2\mathbb{E}|\hat{x}-x|^{2}blackboard_E | over^ start_ARG italic_x end_ARG - italic_x | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, which is equivalent to measure the variance if the estimator is unbiased. We note that there are, of course, many other reasonable error metrics to consider, depending on the specific applications. The mean squared error is a good starting point for a new algorithmic problem due to its simple probabilistic/geometric structures. One natural way to construct a d𝑑ditalic_d-dimensional counter is to simply use d𝑑ditalic_d separate Morris counters, which is the original method that Morris used to count trigrams [12]. Suppose now we have d𝑑ditalic_d (n,1,σ)𝑛1𝜎(n,1,\sigma)( italic_n , 1 , italic_σ )-counters to count x1,…,xdsubscript𝑥1…subscript𝑥𝑑x_{1},\ldots,x_{d}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT. Let the estimates be x1^,…,xd^^subscript𝑥1…^subscript𝑥𝑑\widehat{x_{1}},\ldots,\widehat{x_{d}}over^ start_ARG italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG , … , over^ start_ARG italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_ARG respectively. Then we have 𝔼⁢|x^−x|2𝔼superscript^𝑥𝑥2\displaystyle\mathbb{E}|\hat{x}-x|^{2}blackboard_E | over^ start_ARG italic_x end_ARG - italic_x | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT =𝔼⁢∑j=1d|xj^−xj|2=∑j=1d𝔼⁢|xj^−xj|2absent𝔼superscriptsubscript𝑗1𝑑superscript^subscript𝑥𝑗subscript𝑥𝑗2superscriptsubscript𝑗1𝑑𝔼superscript^subscript𝑥𝑗subscript𝑥𝑗2\displaystyle=\mathbb{E}\sum_{j=1}^{d}|\widehat{x_{j}}-x_{j}|^{2}=\sum_{j=1}^{% d}\mathbb{E}|\widehat{x_{j}}-x_{j}|^{2}= blackboard_E ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT | over^ start_ARG italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_ARG - italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT blackboard_E | over^ start_ARG italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_ARG - italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT Since for any j𝑗jitalic_j, xj^^subscript𝑥𝑗\widehat{x_{j}}over^ start_ARG italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_ARG is an (n,1,σ)𝑛1𝜎(n,1,\sigma)( italic_n , 1 , italic_σ )-counter for xjsubscript𝑥𝑗x_{j}italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT, we have 𝔼⁢|xj^−xj|2<σ2⁢xj2𝔼superscript^subscript𝑥𝑗subscript𝑥𝑗2superscript𝜎2superscriptsubscript𝑥𝑗2\mathbb{E}|\widehat{x_{j}}-x_{j}|^{2}<\sigma^{2}x_{j}^{2}blackboard_E | over^ start_ARG italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_ARG - italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT < italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. Thus, 𝔼⁢|x^−x|2𝔼superscript^𝑥𝑥2\displaystyle\mathbb{E}|\hat{x}-x|^{2}blackboard_E | over^ start_ARG italic_x end_ARG - italic_x | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT <∑j=1dσ2⁢xj2=σ2⁢|x|2.absentsuperscriptsubscript𝑗1𝑑superscript𝜎2superscriptsubscript𝑥𝑗2superscript𝜎2superscript𝑥2\displaystyle<\sum_{j=1}^{d}\sigma^{2}x_{j}^{2}=\sigma^{2}|x|^{2}.< ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT | italic_x | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . We thus see d𝑑ditalic_d (n,1,σ)𝑛1𝜎(n,1,\sigma)( italic_n , 1 , italic_σ )-counters do form an (n,d,σ)𝑛𝑑𝜎(n,d,\sigma)( italic_n , italic_d , italic_σ )-counter (Definition 1). Each (n,1,σ)𝑛1𝜎(n,1,\sigma)( italic_n , 1 , italic_σ )-counter can be implemented with a Morris⁢(O⁢(σ−2))Morris𝑂superscript𝜎2\mathrm{Morris}(O(\sigma^{-2}))roman_Morris ( italic_O ( italic_σ start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT ) ) counter, taking log2⁡log2⁡n+O⁢(log2⁡σ−1)subscript2subscript2𝑛𝑂subscript2superscript𝜎1\log_{2}\log_{2}n+O(\log_{2}\sigma^{-1})roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_n + italic_O ( roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_σ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) bits. Thus the total space needed is d⁢log2⁡log2⁡n+O⁢(d⁢log2⁡σ−1)𝑑subscript2subscript2𝑛𝑂𝑑subscript2superscript𝜎1d\log_{2}\log_{2}n+O(d\log_{2}\sigma^{-1})italic_d roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_n + italic_O ( italic_d roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_σ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) bits, which is sub-optimal for this task. The main contribution of this work is the construction of a simple and optimal (n,d,σ)𝑛𝑑𝜎(n,d,\sigma)( italic_n , italic_d , italic_σ )-counter, with a matching lower bound. Theorem 1. The following statements are true. Upper bound For any n≥2𝑛2n\geq 2italic_n ≥ 2, d≥1𝑑1d\geq 1italic_d ≥ 1, and σ∈(0,1/3)𝜎013\sigma\in(0,1/3)italic_σ ∈ ( 0 , 1 / 3 ), there exists an (n,d,σ)𝑛𝑑𝜎(n,d,\sigma)( italic_n , italic_d , italic_σ )-counter with space size log2⁡|Ω|=log2⁡log2⁡n+O⁢(d⁢log2⁡σ−1)subscript2Ωsubscript2subscript2𝑛𝑂𝑑subscript2superscript𝜎1\log_{2}|\Omega|=\log_{2}\log_{2}n+O(d\log_{2}\sigma^{-1})roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | roman_Ω | = roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_n + italic_O ( italic_d roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_σ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ). Lower bound For any n≥e2𝑛superscript𝑒2n\geq e^{2}italic_n ≥ italic_e start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, d≥1𝑑1d\geq 1italic_d ≥ 1, and σ∈(0,1/16)𝜎0116\sigma\in(0,1/16)italic_σ ∈ ( 0 , 1 / 16 ), if there is an (n,d,σ)𝑛𝑑𝜎(n,d,\sigma)( italic_n , italic_d , italic_σ )-counter with state space ΩΩ\Omegaroman_Ω, then log2⁡|Ω|≥log2⁡log2⁡n+Ω⁢(d⁢log2⁡σ−1)subscript2Ωsubscript2subscript2𝑛Ω𝑑subscript2superscript𝜎1\log_{2}|\Omega|\geq\log_{2}\log_{2}n+\Omega(d\log_{2}\sigma^{-1})roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | roman_Ω | ≥ roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_n + roman_Ω ( italic_d roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_σ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ). A related data structure is the Count-Min sketch [3] by Cormode and Muthukrishnan. Given a count vector x∈ℕd𝑥superscriptℕ𝑑x\in\mathbb{N}^{d}italic_x ∈ blackboard_N start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT with ∑j=1dxj=nsuperscriptsubscript𝑗1𝑑subscript𝑥𝑗𝑛\sum_{j=1}^{d}x_{j}=n∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = italic_n, Count-Min is able to return an estimate xj^^subscript𝑥𝑗\widehat{x_{j}}over^ start_ARG italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_ARG of xjsubscript𝑥𝑗x_{j}italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT, for any j∈[d]𝑗delimited-[]𝑑j\in[d]italic_j ∈ [ italic_d ], such that xj^∈[xj,xj+ϵ⁢n]^subscript𝑥𝑗subscript𝑥𝑗subscript𝑥𝑗italic-ϵ𝑛\widehat{x_{j}}\in[x_{j},x_{j}+\epsilon n]over^ start_ARG italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_ARG ∈ [ italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT + italic_ϵ italic_n ] with probability 1−δ1𝛿1-\delta1 - italic_δ, using only O⁢(ϵ−1⁢log⁡δ−1⁢log⁡n)𝑂superscriptitalic-ϵ1superscript𝛿1𝑛O(\epsilon^{-1}\log\delta^{-1}\log n)italic_O ( italic_ϵ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log italic_δ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log italic_n ) bits of space. It may seem that Count-Min with the power of hash functions has a chance to beat the lower bound in Theorem 1 by combining the estimates for each coordinate. However, it is hopeless to use Count-Min as a d𝑑ditalic_d-dimensional counter unless ϵ=O⁢(1/d)italic-ϵ𝑂1𝑑\epsilon=O(1/d)italic_ϵ = italic_O ( 1 / italic_d ), in which case it still lies above the lower bound in Theorem 1. Indeed, suppose xj^∈[xj,xj+ϵ⁢n]^subscript𝑥𝑗subscript𝑥𝑗subscript𝑥𝑗italic-ϵ𝑛\widehat{x_{j}}\in[x_{j},x_{j}+\epsilon n]over^ start_ARG italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_ARG ∈ [ italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT + italic_ϵ italic_n ] holds for all j𝑗jitalic_j. The vector estimate x^=(x1^,…,xd^)^𝑥^subscript𝑥1…^subscript𝑥𝑑\hat{x}=(\widehat{x_{1}},\ldots,\widehat{x_{d}})over^ start_ARG italic_x end_ARG = ( over^ start_ARG italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG , … , over^ start_ARG italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_ARG ) has error |x^−x|2≤d⁢ϵ2⁢n2≤d2⁢ϵ2⁢|x|2,superscript^𝑥𝑥2𝑑superscriptitalic-ϵ2superscript𝑛2superscript𝑑2superscriptitalic-ϵ2superscript𝑥2\displaystyle|\hat{x}-x|^{2}\leq d\epsilon^{2}n^{2}\leq d^{2}\epsilon^{2}|x|^{% 2},| over^ start_ARG italic_x end_ARG - italic_x | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ≤ italic_d italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ≤ italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT | italic_x | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , where we use the bound |x|2≥(∑j=1dxj)2/d=n2/dsuperscript𝑥2superscriptsuperscriptsubscript𝑗1𝑑subscript𝑥𝑗2𝑑superscript𝑛2𝑑|x|^{2}\geq(\sum_{j=1}^{d}x_{j})^{2}/d=n^{2}/d| italic_x | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ≥ ( ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT / italic_d = italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT / italic_d. Therefore ϵitalic-ϵ\epsilonitalic_ϵ has to be O⁢(1/d)𝑂1𝑑O(1/d)italic_O ( 1 / italic_d ) for x^^𝑥\hat{x}over^ start_ARG italic_x end_ARG to be a multiplicative estimate of x𝑥xitalic_x. 1.1 Technical Overview 1.1.1 Upper Bound The upper bound in Theorem 1 uses the natural idea of maintaining a common scale counter U𝑈Uitalic_U for all d𝑑ditalic_d dimensions and tracking the relative magnitude of each coordinate with a vector V∈ℕd𝑉superscriptℕ𝑑V\in\mathbb{N}^{d}italic_V ∈ blackboard_N start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT. The estimate x^^𝑥\hat{x}over^ start_ARG italic_x end_ARG is equal to 2U⁢Vsuperscript2𝑈𝑉2^{U}V2 start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT italic_V. For demonstration, we will first describe a very simple but sub-optimal solution and then discuss how to modify it to reach optimal space-accuracy tradeoff. The transition of the states is designed similarly with the design of the Morris counter, in which the estimate is maintained unbiased at any moment (𝔼⁢2U⁢V=x𝔼superscript2𝑈𝑉𝑥\mathbb{E}2^{U}V=xblackboard_E 2 start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT italic_V = italic_x). Initially we set U=0𝑈0U=0italic_U = 0 and V=(0,…,0)𝑉0…0V=(0,\ldots,0)italic_V = ( 0 , … , 0 ). • When the j𝑗jitalic_jth coordinate is incremented (i.e., xj←xj+1←subscript𝑥𝑗subscript𝑥𝑗1x_{j}\leftarrow x_{j}+1italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ← italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT + 1), we update Vj←Vj+1←subscript𝑉𝑗subscript𝑉𝑗1V_{j}\leftarrow V_{j}+1italic_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ← italic_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT + 1 with probability 2−Usuperscript2𝑈2^{-U}2 start_POSTSUPERSCRIPT - italic_U end_POSTSUPERSCRIPT, ensuring the unbiasedness of the estimate. • Each coordinate Vjsubscript𝑉𝑗V_{j}italic_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT is restricted to the range [0,a]0𝑎[0,a][ 0 , italic_a ], so that the space to store V𝑉Vitalic_V is d⁢log2⁡(1+a)𝑑subscript21𝑎d\log_{2}(1+a)italic_d roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( 1 + italic_a ). Whenever some Vjsubscript𝑉𝑗V_{j}italic_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT exceeds a𝑎aitalic_a, we have to scale up to keep Vjsubscript𝑉𝑗V_{j}italic_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT in range. Again, we want to maintain the property that 𝔼⁢2U⁢V=x𝔼superscript2𝑈𝑉𝑥\mathbb{E}2^{U}V=xblackboard_E 2 start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT italic_V = italic_x. Thus, the scale-up should be done as follows. – U←U+1←𝑈𝑈1U\leftarrow U+1italic_U ← italic_U + 1. The scale counter is increased by 1. – For the estimate to be unbiased, Vjsubscript𝑉𝑗V_{j}italic_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT should become Vj/2subscript𝑉𝑗2V_{j}/2italic_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT / 2 for every j𝑗jitalic_j. Here we also want Vjsubscript𝑉𝑗V_{j}italic_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT to always be an integer. Therefore, if Vjsubscript𝑉𝑗V_{j}italic_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT is even, then Vj←Vj/2←subscript𝑉𝑗subscript𝑉𝑗2V_{j}\leftarrow V_{j}/2italic_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ← italic_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT / 2. If Vjsubscript𝑉𝑗V_{j}italic_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT is odd, then Vj←(Vj+ξ)/2←subscript𝑉𝑗subscript𝑉𝑗𝜉2V_{j}\leftarrow(V_{j}+\xi)/2italic_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ← ( italic_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT + italic_ξ ) / 2 where ξ∈{−1,1}𝜉11\xi\in\{-1,1\}italic_ξ ∈ { - 1 , 1 } is a freshly sampled Rademacher random variable. The algorithm above is unbiased by design. However, the memory-accuracy tradeoff is not optimal. There is a fundamental problem of the design above that the parameter a𝑎aitalic_a has to be Ω⁢(d)Ω𝑑\Omega(\sqrt{d})roman_Ω ( square-root start_ARG italic_d end_ARG ) (with σ=O⁢(1)𝜎𝑂1\sigma=O(1)italic_σ = italic_O ( 1 )) for it to be a d𝑑ditalic_d-dimensional counter. Instead of analyzing the algorithm in details, this problem can be identified by simply checking the set of all possible estimates that are produced by the algorithm: 𝒟={2U⁢(V1,…,Vd):U∈ℕ,∀j∈[d],Vj∈[0,a]}.𝒟conditional-setsuperscript2𝑈subscript𝑉1…subscript𝑉𝑑formulae-sequence𝑈ℕformulae-sequencefor-all𝑗delimited-[]𝑑subscript𝑉𝑗0𝑎\displaystyle\mathcal{D}=\{2^{U}(V_{1},\ldots,V_{d}):U\in\mathbb{N},\forall j% \in[d],V_{j}\in[0,a]\}.caligraphic_D = { 2 start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT ( italic_V start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_V start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) : italic_U ∈ blackboard_N , ∀ italic_j ∈ [ italic_d ] , italic_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ∈ [ 0 , italic_a ] } . It turns out that 𝒟𝒟\mathcal{D}caligraphic_D is not dense enough for estimating some input x𝑥xitalic_x. Indeed, we choose x=2s⁢(a,…,a⏟r,1/2,…,1/2⏟d−r),𝑥superscript2𝑠subscript⏟𝑎…𝑎𝑟subscript⏟12…12𝑑𝑟\displaystyle x=2^{s}(\underbrace{a,\ldots,a}_{r},\underbrace{1/2,\ldots,1/2}_% {d-r}),italic_x = 2 start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ( under⏟ start_ARG italic_a , … , italic_a end_ARG start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT , under⏟ start_ARG 1 / 2 , … , 1 / 2 end_ARG start_POSTSUBSCRIPT italic_d - italic_r end_POSTSUBSCRIPT ) , for some r∈[0,d]𝑟0𝑑r\in[0,d]italic_r ∈ [ 0 , italic_d ] so that both y1=2s⁢(a,…,a,1,…,1)subscript𝑦1superscript2𝑠𝑎…𝑎1…1y_{1}=2^{s}(a,\ldots,a,1,\ldots,1)italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 2 start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ( italic_a , … , italic_a , 1 , … , 1 ) and y2=2s−1⁢(a,…,a,1,…,1)subscript𝑦2superscript2𝑠1𝑎…𝑎1…1y_{2}=2^{s-1}(a,\ldots,a,1,\ldots,1)italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 2 start_POSTSUPERSCRIPT italic_s - 1 end_POSTSUPERSCRIPT ( italic_a , … , italic_a , 1 , … , 1 ) are far from x𝑥xitalic_x. By construction, the nearest estimate of x𝑥xitalic_x in 𝒟𝒟\mathcal{D}caligraphic_D has to be either y1subscript𝑦1y_{1}italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT or y2subscript𝑦2y_{2}italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT.333Note that y1subscript𝑦1y_{1}italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT is one possible probabilistic rounding of x𝑥xitalic_x. Other roundings {2s⁢(a,…,a,z1,…,zd−r)∣zj∈{0,1}}conditional-setsuperscript2𝑠𝑎…𝑎subscript𝑧1…subscript𝑧𝑑𝑟subscript𝑧𝑗01\{2^{s}(a,\ldots,a,z_{1},\ldots,z_{d-r})\mid z_{j}\in\{0,1\}\}{ 2 start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ( italic_a , … , italic_a , italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_z start_POSTSUBSCRIPT italic_d - italic_r end_POSTSUBSCRIPT ) ∣ italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ∈ { 0 , 1 } } all have the same distance to x𝑥xitalic_x. By symmetry, we only need to consider y1subscript𝑦1y_{1}italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. The vector y2subscript𝑦2y_{2}italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT is produced by decreasing the scale to 2s−1superscript2𝑠12^{s-1}2 start_POSTSUPERSCRIPT italic_s - 1 end_POSTSUPERSCRIPT, so the smaller entries can be represented precisely with the cost of underestimating the large entries. We choose r𝑟ritalic_r so that the two choices have equal distances to x𝑥xitalic_x. A simple calculation shows that we need to set r=d/(1+4⁢a2)𝑟𝑑14superscript𝑎2r=d/(1+4a^{2})italic_r = italic_d / ( 1 + 4 italic_a start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ).444For simplicity, we only require r>1𝑟1r>1italic_r > 1 and omit the details of rounding r𝑟ritalic_r to an integer. Assume now a<d/3𝑎𝑑3a<\sqrt{d}/3italic_a < square-root start_ARG italic_d end_ARG / 3 and then r>1𝑟1r>1italic_r > 1. Thus miny∈𝒟⁡|x−y|2=22⁢s⁢d⁢a21+4⁢a2subscript𝑦𝒟superscript𝑥𝑦2superscript22𝑠𝑑superscript𝑎214superscript𝑎2\min_{y\in\mathcal{D}}|x-y|^{2}=2^{2s}\frac{da^{2}}{1+4a^{2}}roman_min start_POSTSUBSCRIPT italic_y ∈ caligraphic_D end_POSTSUBSCRIPT | italic_x - italic_y | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 2 start_POSTSUPERSCRIPT 2 italic_s end_POSTSUPERSCRIPT divide start_ARG italic_d italic_a start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG 1 + 4 italic_a start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG, while |x|2=22⁢s⋅2⁢a2⁢d/(1+4⁢a2)superscript𝑥2⋅superscript22𝑠2superscript𝑎2𝑑14superscript𝑎2|x|^{2}=2^{2s}\cdot 2a^{2}d/(1+4a^{2})| italic_x | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 2 start_POSTSUPERSCRIPT 2 italic_s end_POSTSUPERSCRIPT ⋅ 2 italic_a start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d / ( 1 + 4 italic_a start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ). Therefore, if the algorithm is an (n,d,σ)𝑛𝑑𝜎(n,d,\sigma)( italic_n , italic_d , italic_σ )-counter, then 𝔼⁢|x^−x|2<σ2⁢|x|2𝔼superscript^𝑥𝑥2superscript𝜎2superscript𝑥2\mathbb{E}|\hat{x}-x|^{2}<\sigma^{2}|x|^{2}blackboard_E | over^ start_ARG italic_x end_ARG - italic_x | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT < italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT | italic_x | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, which means there is at least one x∗∈𝒟subscript𝑥𝒟x_{*}\in\mathcal{D}italic_x start_POSTSUBSCRIPT ∗ end_POSTSUBSCRIPT ∈ caligraphic_D such that |x∗−x|2<σ2⁢|x|2superscriptsubscript𝑥𝑥2superscript𝜎2superscript𝑥2|x_{*}-x|^{2}<\sigma^{2}|x|^{2}| italic_x start_POSTSUBSCRIPT ∗ end_POSTSUBSCRIPT - italic_x | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT < italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT | italic_x | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. This suggests that 22⁢s⁢a2⁢d/(1+4⁢a2)<σ2⁢22⁢s⋅2⁢a2⁢d/(1+4⁢a2)superscript22𝑠superscript𝑎2𝑑14superscript𝑎2⋅superscript𝜎2superscript22𝑠2superscript𝑎2𝑑14superscript𝑎22^{2s}a^{2}d/(1+4a^{2})<\sigma^{2}2^{2s}\cdot 2a^{2}d/(1+4a^{2})2 start_POSTSUPERSCRIPT 2 italic_s end_POSTSUPERSCRIPT italic_a start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d / ( 1 + 4 italic_a start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) < italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT 2 start_POSTSUPERSCRIPT 2 italic_s end_POSTSUPERSCRIPT ⋅ 2 italic_a start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d / ( 1 + 4 italic_a start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) and therefore σ>1/2𝜎12\sigma>1/\sqrt{2}italic_σ > 1 / square-root start_ARG 2 end_ARG. In other words, if a<d/3𝑎𝑑3a<\sqrt{d}/3italic_a < square-root start_ARG italic_d end_ARG / 3, then any (n,d,σ)𝑛𝑑𝜎(n,d,\sigma)( italic_n , italic_d , italic_σ )-counter must have σ>1/2𝜎12\sigma>1/\sqrt{2}italic_σ > 1 / square-root start_ARG 2 end_ARG. We conclude that a𝑎aitalic_a needs to be Ω⁢(d)Ω𝑑\Omega(\sqrt{d})roman_Ω ( square-root start_ARG italic_d end_ARG ) when σ≤1/2𝜎12\sigma\leq 1/\sqrt{2}italic_σ ≤ 1 / square-root start_ARG 2 end_ARG, indicating an additional O⁢(d⁢log⁡d)𝑂𝑑𝑑O(d\log d)italic_O ( italic_d roman_log italic_d ) space overhead in comparison to the optimal space. From the discussion above, we see that one has to have a dense enough set of estimates 𝒟𝒟\mathcal{D}caligraphic_D for a correct (n,d,σ)𝑛𝑑𝜎(n,d,\sigma)( italic_n , italic_d , italic_σ )-counter. Roughly speaking, one wants to spend more bits on large coordinates and fewer bits on small coordinates. This can be done with a simple and natural algorithmic idea: variable-length integer encoding. The usual binary encoding of non-negative integers automatically uses more bits on large numbers and fewer bits on small numbers. We will prove the following set of estimates is dense enough. 𝒟compressed={2U⁢(V1,…,Vd):U∈ℕ,the encoding length of V is at most O⁢(d⁢log2⁡σ−1)},subscript𝒟compressedconditional-setsuperscript2𝑈subscript𝑉1…subscript𝑉𝑑𝑈ℕthe encoding length of V is at most O⁢(d⁢log2⁡σ−1)\displaystyle\mathcal{D}_{\text{compressed}}=\{2^{U}(V_{1},\ldots,V_{d}):U\in% \mathbb{N},\text{the encoding length of $V$ is at most $O(d\log_{2}\sigma^{-1}% )$}\},caligraphic_D start_POSTSUBSCRIPT compressed end_POSTSUBSCRIPT = { 2 start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT ( italic_V start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_V start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) : italic_U ∈ blackboard_N , the encoding length of italic_V is at most italic_O ( italic_d roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_σ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) } , This leads to our upper bound in Theorem 1. One may observe how this encoding idea better handles the previous counterexample x=2s⁢(a,…,a,1/2,…,1/2)𝑥superscript2𝑠𝑎…𝑎12…12x=2^{s}(a,\ldots,a,1/2,\ldots,1/2)italic_x = 2 start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ( italic_a , … , italic_a , 1 / 2 , … , 1 / 2 ). An estimate of 2s−1⁢(2⁢a,…,2⁢a,1,…,1)superscript2𝑠12𝑎…2𝑎1…12^{s-1}(2a,\ldots,2a,1,\ldots,1)2 start_POSTSUPERSCRIPT italic_s - 1 end_POSTSUPERSCRIPT ( 2 italic_a , … , 2 italic_a , 1 , … , 1 ) can now be returned because though each entry with value 2⁢a2𝑎2a2 italic_a needs above average space to encode, each entry with value 1111 uses less than average space to encode, saving the memory space for the large entries. 1.1.2 Lower Bound As we have discussed above, the set of all estimates produced by the counter has to be dense enough for the task of d𝑑ditalic_d-dimensional counting to be possible. The lower bound in Theorem 1 thus arises from estimating the size of multiplicative space coverings. Definition 2 (multiplicative space covering). For any subsets A,R⊂ℝd𝐴𝑅superscriptℝ𝑑A,R\subset\mathbb{R}^{d}italic_A , italic_R ⊂ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT and σ>0𝜎0\sigma>0italic_σ > 0, we say R𝑅Ritalic_R is a σ𝜎\sigmaitalic_σ-multiplicative covering of A𝐴Aitalic_A if for any x∈A𝑥𝐴x\in Aitalic_x ∈ italic_A there exists y∈R𝑦𝑅y\in Ritalic_y ∈ italic_R such that |x−y|<σ⁢|x|𝑥𝑦𝜎𝑥|x-y|<\sigma|x|| italic_x - italic_y | < italic_σ | italic_x |. Clearly, the set of all estimates of an (n,d,σ)𝑛𝑑𝜎(n,d,\sigma)( italic_n , italic_d , italic_σ )-counter will form a σ𝜎\sigmaitalic_σ-multiplicative space covering of 𝒳={x∈ℕd∣x1+⋯+xd≤n}𝒳conditional-set𝑥superscriptℕ𝑑subscript𝑥1⋯subscript𝑥𝑑𝑛\mathcal{X}=\{x\in\mathbb{N}^{d}\mid x_{1}+\cdots+x_{d}\leq n\}caligraphic_X = { italic_x ∈ blackboard_N start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ∣ italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + ⋯ + italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ≤ italic_n }. The lower bound is thus proved by estimating the covering size of 𝒳𝒳\mathcal{X}caligraphic_X, which can be further reduced to the classic problem of covering spheres with smaller spheres [16]. 1.2 Related Work There have been many variants and analysis of Morris counters before [10, 11, 8, 7, 14]. Nevertheless, they have only considered the one-dimensional case. Tracking a multi-dimensional count vector (with a vector norm) is a new problem to the best of our knowledge. In the one-dimensional case (d=1𝑑1d=1italic_d = 1), our algorithm is equivalent to the variant—floating-point counter—by Csűrös [4], except for that the floating-point counter restricts a𝑎aitalic_a to be a power of two so no probabilistic rounding is needed when halving. Nevertheless, when d≥2𝑑2d\geq 2italic_d ≥ 2, probabilistic rounding is inevitable. In comparison to the classic Morris counter, the floating-point variant is much more convenient to implement on binary machines, where only simple integer operations and bit operations are needed [4]. Another advantage is that the number of random bits needed per increment is at most 2 in expectation, since the probability 2−Usuperscript2𝑈2^{-U}2 start_POSTSUPERSCRIPT - italic_U end_POSTSUPERSCRIPT can be simulated by generating random bits sequentially, looking for U𝑈Uitalic_U consecutive ones [4] (stop generating random bits when a zero shows up). Such advantages are inherited by our d𝑑ditalic_d-dimensional counter as well. 1.3 Organization We will construct the upper bound in §2 and prove the lower bound in §3. The main theorem (Theorem 1) follows directly from Corollary 1 and 3. We have included a sample run in Appendix A to help illustrate the patterns of the new algorithm."
https://arxiv.org/html/2411.03037v1,Top-k Stabbing Interval Queries,"We investigate a weighted variant of the interval stabbing problem, where the goal is to design an efficient data structure for a given set ℐℐ\mathcal{I}caligraphic_I of weighted intervals such that, for a query point q𝑞qitalic_q and an integer k>0𝑘0k>0italic_k > 0, we can report the k𝑘kitalic_k intervals with largest weights among those stabbed by q𝑞qitalic_q. In this paper, we present a linear space solution with O⁢(log⁡n+k)𝑂𝑛𝑘O(\log n+k)italic_O ( roman_log italic_n + italic_k ) query time. Moreover, we also present another trade-off for the problem.","The interval stabbing problem is an important problem in computational geometry [2, 1, 4], which asks to compute all intervals stabbed by a query point. In this paper, we study a natural variant of this problem where, instead of computing all stabbed intervals, the goal is to find a subset of the stabbed intervals that “best” represent them [1]. Formally, the problem is defined as follows. Let ℐℐ\mathcal{I}caligraphic_I be a set of n𝑛nitalic_n intervals on the real line, each with a real valued weight. The set ℐℐ\mathcal{I}caligraphic_I is static. The goal is to preprocess ℐℐ\mathcal{I}caligraphic_I so that, for a query value q𝑞qitalic_q and a positive integer k𝑘kitalic_k, the k𝑘kitalic_k intervals stabbed by q𝑞qitalic_q with largest weights can be reported efficiently. An interval [si,ei]∈ℐsubscript𝑠𝑖subscript𝑒𝑖ℐ[s_{i},e_{i}]\in\mathcal{I}[ italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ] ∈ caligraphic_I is stabbed by q𝑞qitalic_q if and only if si≤q≤eisubscript𝑠𝑖𝑞subscript𝑒𝑖s_{i}\leq q\leq e_{i}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ≤ italic_q ≤ italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. The problem is useful in cryptocurrency and stock applications [6, 7]. We will refer this variant as the top-k𝑘kitalic_k interval stabbing problem. In this paper, we present a linear space solution with O⁢(log⁡n+k)𝑂𝑛𝑘O(\log n+k)italic_O ( roman_log italic_n + italic_k ) query time. In the rank space, the query time can be reduced to O⁢(k)𝑂𝑘O(k)italic_O ( italic_k ). Moreover, we also present a solution that uses only segment tree structure. It takes O⁢(n⁢log⁡n)𝑂𝑛𝑛O(n\log n)italic_O ( italic_n roman_log italic_n ) space and time to build, and O⁢(log⁡n+k⁢log⁡log⁡n)𝑂𝑛𝑘𝑛O(\log n+k\log\log n)italic_O ( roman_log italic_n + italic_k roman_log roman_log italic_n ) time to answer a query. In rank space, the query time can be improved to O⁢(log⁡n+k)𝑂𝑛𝑘O(\log n+k)italic_O ( roman_log italic_n + italic_k ). Amagata et al. [1] has proposed two algorithms for the problem; one algorithm uses interval tree structure while the other one employs segment tree structure. The algorithm based on interval tree structure takes linear space and O⁢(n⁢log⁡n+k)𝑂𝑛𝑛𝑘O(\sqrt{n}\log n+k)italic_O ( square-root start_ARG italic_n end_ARG roman_log italic_n + italic_k ) time to answer a query. The preprocessing takes O⁢(n⁢log⁡n)𝑂𝑛𝑛O(n\log n)italic_O ( italic_n roman_log italic_n ) time. The other algorithm based on segment tree achieves O⁢(log⁡n+k)𝑂𝑛𝑘O(\log n+k)italic_O ( roman_log italic_n + italic_k ) query time, the preprocessing takes O⁢(n⁢log⁡n⁢log⁡log⁡n)𝑂𝑛𝑛𝑛O(n\log n\log\log n)italic_O ( italic_n roman_log italic_n roman_log roman_log italic_n ) time and O⁢(n⁢log2⁡n)𝑂𝑛superscript2𝑛O(n\log^{2}n)italic_O ( italic_n roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_n ) space."
https://arxiv.org/html/2411.02845v1,Max-Distance Sparsification for Diversification and Clustering,"Let 𝒟𝒟\mathcal{D}caligraphic_D be a set family that is the solution domain of some combinatorial problem. The max-min diversification problem on 𝒟𝒟\mathcal{D}caligraphic_D is the problem to select k𝑘kitalic_k sets from 𝒟𝒟\mathcal{D}caligraphic_D such that the Hamming distance between any two selected sets is at least d𝑑ditalic_d. FPT algorithms parameterized by k,l:=maxD∈𝒟⁡|D|assign𝑘𝑙subscript𝐷𝒟𝐷k,l:=\max_{D\in\mathcal{D}}|D|italic_k , italic_l := roman_max start_POSTSUBSCRIPT italic_D ∈ caligraphic_D end_POSTSUBSCRIPT | italic_D | and k,d𝑘𝑑k,ditalic_k , italic_d have been actively studied recently for several specific domains.This paper provides unified algorithmic frameworks to solve this problem. Specifically, for each parameterization k,l𝑘𝑙k,litalic_k , italic_l and k,d𝑘𝑑k,ditalic_k , italic_d, we provide an FPT oracle algorithm for the max-min diversification problem using oracles related to 𝒟𝒟\mathcal{D}caligraphic_D. We then demonstrate that our frameworks generalize most of the existing domain-specific tractability results and provide the first FPT algorithms for several domains.Our main technical breakthrough is introducing the notion of max-distance sparsifier of 𝒟𝒟\mathcal{D}caligraphic_D, a domain on which the max-min diversification problem is equivalent to the same problem on the original domain 𝒟𝒟\mathcal{D}caligraphic_D. The core of our framework is to design FPT oracle algorithms that construct a constant-size max-distance sparsifier of 𝒟𝒟\mathcal{D}caligraphic_D. Using max-distance sparsifiers, we provide FPT algorithms for the max-min and max-sum diversification problems on 𝒟𝒟\mathcal{D}caligraphic_D, as well as k𝑘kitalic_k-center and k𝑘kitalic_k-sum-of-radii clustering problems on 𝒟𝒟\mathcal{D}caligraphic_D, which are also natural problems in the context of diversification and have their own interests.","1.1 Background and Motivation The procedure for approaching real-world problems with optimization algorithms involves formulating the real-world motivations as mathematical problems and then solving them. However, real-world problems are complex, and the idea of a “good” solution cannot always be correctly formulated. Attempting to impose a formulation may ignore important perspectives such as human sensibilities and ethical considerations. The paradigm of diversification, introduced by [baste2019fpt] and [baste2022diversity], is a “formulation of the unformulatable problems”, which formulates diversity measures for a set of multiple solutions, rather than attempting to formulate the “goodness” of a single solution. By computing a set of solutions that maximize this measure, the algorithm provides effective options to evaluators who have the correct criteria for judging the “goodness” of a solution. Let U𝑈Uitalic_U be a finite set, k∈ℤ≥1𝑘subscriptℤabsent1k\in\mathbb{Z}_{\geq 1}italic_k ∈ blackboard_Z start_POSTSUBSCRIPT ≥ 1 end_POSTSUBSCRIPT and d∈ℤ≥0𝑑subscriptℤabsent0d\in\mathbb{Z}_{\geq 0}italic_d ∈ blackboard_Z start_POSTSUBSCRIPT ≥ 0 end_POSTSUBSCRIPT. Let 𝒟⊆2U𝒟superscript2𝑈\mathcal{D}\subseteq 2^{U}caligraphic_D ⊆ 2 start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT be the feasible domain of some combinatorial problem. The following problem frameworks, defined by two types of diversity measures, have been studied extensively. Max-Min Diversification Problem on 𝒟𝒟\mathcal{D}caligraphic_D: Does there exist a k𝑘kitalic_k-tuple (D1,…,Dk)∈𝒟ksubscript𝐷1…subscript𝐷𝑘superscript𝒟𝑘(D_{1},\dots,D_{k})\in\mathcal{D}^{k}( italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ∈ caligraphic_D start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT of sets in 𝒟𝒟\mathcal{D}caligraphic_D such that min1≤i<j≤k⁡|Di⁢△⁢Dj|≥dsubscript1𝑖𝑗𝑘subscript𝐷𝑖△subscript𝐷𝑗𝑑\min_{1\leq i<j\leq k}|D_{i}\triangle D_{j}|\geq droman_min start_POSTSUBSCRIPT 1 ≤ italic_i < italic_j ≤ italic_k end_POSTSUBSCRIPT | italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT △ italic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | ≥ italic_d?111We define Z1⁢△⁢Z2:=(Z1∖Z2)∪(Z2∖Z1)assignsubscript𝑍1△subscript𝑍2subscript𝑍1subscript𝑍2subscript𝑍2subscript𝑍1Z_{1}\triangle Z_{2}:=(Z_{1}\setminus Z_{2})\cup(Z_{2}\setminus Z_{1})italic_Z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT △ italic_Z start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT := ( italic_Z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ∖ italic_Z start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) ∪ ( italic_Z start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ∖ italic_Z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ). Max-Sum Diversification Problem on 𝒟𝒟\mathcal{D}caligraphic_D: Does there exist a k𝑘kitalic_k-tuple (D1,…,Dk)∈𝒟ksubscript𝐷1…subscript𝐷𝑘superscript𝒟𝑘(D_{1},\dots,D_{k})\in\mathcal{D}^{k}( italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ∈ caligraphic_D start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT of sets in 𝒟𝒟\mathcal{D}caligraphic_D such that ∑1≤i<j≤k|Di⁢△⁢Dj|≥dsubscript1𝑖𝑗𝑘subscript𝐷𝑖△subscript𝐷𝑗𝑑\sum_{1\leq i<j\leq k}|D_{i}\triangle D_{j}|\geq d∑ start_POSTSUBSCRIPT 1 ≤ italic_i < italic_j ≤ italic_k end_POSTSUBSCRIPT | italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT △ italic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | ≥ italic_d? These problems ensure diversity by aiming to output solutions that are as dissimilar as possible in terms of Hamming distance. Parameterized algorithms for diversification problems have been actively studied. Particularly, FPT algorithms for the max-min diversification problems parameterized by k𝑘kitalic_k and l:=maxD∈𝒟⁡|D|assign𝑙subscript𝐷𝒟𝐷l:=\max_{D\in\mathcal{D}}|D|italic_l := roman_max start_POSTSUBSCRIPT italic_D ∈ caligraphic_D end_POSTSUBSCRIPT | italic_D | [baste2022diversity, baste2019fpt, fomin2024diversecollection, hanaka2021finding, shida2024finding], as well as by k𝑘kitalic_k and d𝑑ditalic_d [eiben2024determinantal, fomin2024diversepair, fomin2024diversecollection, funayama2024parameterized], have been the focus of research. Since assuming d≤2⁢l𝑑2𝑙d\leq 2litalic_d ≤ 2 italic_l does not lose generality in the max-min diversification problem, the latter addresses a more general situation than the former. The max-sum diversification problem is empirically more tractable than the max-min diversification problem, and most of the existing algorithms for the max-min diversification problems can be straightforwardly converted to solve the max-sum diversification problems on the corresponding domain. Therefore, for some time hereafter, we will restrict our discussion to the max-min diversification problem. The most successful technical framework in this field is color-coding [alon1995color]. [hanaka2021finding] constructed a color-coding based framework designing FPT algorithms parameterized by k,l𝑘𝑙k,litalic_k , italic_l for the max-min diversification problem on several domains. [shida2024finding] provided the same result for the case where 𝒟𝒟\mathcal{D}caligraphic_D is the family of longest common subsequences. Color-coding has also been successful for the case where the parameter is k,d𝑘𝑑k,ditalic_k , italic_d. [fomin2024diversepair] provided a color-coding based FPT algorithm for the max-min diversification problem on 𝒟𝒟\mathcal{D}caligraphic_D parameterized by d𝑑ditalic_d, where 𝒟𝒟\mathcal{D}caligraphic_D is the family of maximum matchings and k=2𝑘2k=2italic_k = 2. Furthermore, [fomin2024diversecollection] provided a color-coding based FPT algorithm parameterized by k,d𝑘𝑑k,ditalic_k , italic_d for the family of perfect matchings, and [funayama2024parameterized] provided the same result for the family of shortest paths. All of these algorithms use a subroutine to find a colorful set that belongs to a domain related to 𝒟𝒟\mathcal{D}caligraphic_D for a given coloring of U𝑈Uitalic_U. This research provides general algorithmic frameworks for FPT algorithms addressing max-min (and max-sum) diversification problems for both parameterizations k,l𝑘𝑙k,litalic_k , italic_l and k,d𝑘𝑑k,ditalic_k , italic_d. Our framework is very general and can be applied to all domains [bang2021k, bang2016parameterized, baste2022diversity, baste2019fpt, eiben2024determinantal, fomin2024diversepair, fomin2024diversecollection, funayama2024parameterized, gutin2018k, hanaka2021finding, shida2024finding] for which FPT algorithms parameterized by k,l𝑘𝑙k,litalic_k , italic_l and k,d𝑘𝑑k,ditalic_k , italic_d are currently known for the case that diversity measure is defined using an unweighted Hamming distance and several domains where such algorithms were previously unknown. Unlike existing studies, our framework is not based on color-coding. Very roughly speaking, our framework solves the max-min diversification problem on 𝒟𝒟\mathcal{D}caligraphic_D by utilizing an oracle that determines whether there exists a set of a specified size in the domain related to 𝒟𝒟\mathcal{D}caligraphic_D that satisfies additional simple constraints. The design of this oracle is generally much simpler than the design of an oracle searching for colorful sets in the same domain, making our framework easy-to-use and applicable for a broader domain 𝒟𝒟\mathcal{D}caligraphic_D. Our main technical breakthrough is introducing a notion of max-distance sparsifier as an intermediate step for solving the max-min diversification problem, whose precise definition is given in Section 1.3. The most critical fact is that when 𝒦𝒦\mathcal{K}caligraphic_K is a max-distance sparsifier of 𝒟𝒟\mathcal{D}caligraphic_D, the max-min diversification problem on 𝒟𝒟\mathcal{D}caligraphic_D is equivalent to the same problem on 𝒦𝒦\mathcal{K}caligraphic_K. Our framework constructs a max-distance sparsifier 𝒦𝒦\mathcal{K}caligraphic_K of 𝒟𝒟\mathcal{D}caligraphic_D with a constant size, enabling us to solve the max-min diversification problems on 𝒟𝒟\mathcal{D}caligraphic_D by brute-force search on 𝒦𝒦\mathcal{K}caligraphic_K. The power of max-distance sparsification is not limited to solving max-min (and max-sum) diversification problems. Specifically, the following k𝑘kitalic_k-center [hsu1979easy] and k𝑘kitalic_k-sum-of-radii clustering problems on 𝒟𝒟\mathcal{D}caligraphic_D [charikar2001clustering] can also be solved via max-distance sparsification. k𝑘kitalic_k-Center Clustering Problem on 𝒟𝒟\mathcal{D}caligraphic_D: Does there exist a k𝑘kitalic_k-tuple of subsets (D1,…,Dk)∈𝒟ksubscript𝐷1…subscript𝐷𝑘superscript𝒟𝑘(D_{1},\dots,D_{k})\in\mathcal{D}^{k}( italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ∈ caligraphic_D start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT such that for all D∈𝒟𝐷𝒟D\in\mathcal{D}italic_D ∈ caligraphic_D, there exists an i∈{1,…,k}𝑖1…𝑘i\in\{1,\dots,k\}italic_i ∈ { 1 , … , italic_k } satisfying |Di⁢△⁢D|≤dsubscript𝐷𝑖△𝐷𝑑|D_{i}\triangle D|\leq d| italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT △ italic_D | ≤ italic_d? k𝑘kitalic_k-Sum-of-Radii Clustering Problem on 𝒟𝒟\mathcal{D}caligraphic_D: Does there exist a k𝑘kitalic_k-tuple of subsets (D1,…,Dk)∈𝒟ksubscript𝐷1…subscript𝐷𝑘superscript𝒟𝑘(D_{1},\dots,D_{k})\in\mathcal{D}^{k}( italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ∈ caligraphic_D start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT and a k𝑘kitalic_k-tuple of non-negative integers (d1,…,dk)∈ℤ≥0ksubscript𝑑1…subscript𝑑𝑘superscriptsubscriptℤabsent0𝑘(d_{1},\dots,d_{k})\in\mathbb{Z}_{\geq 0}^{k}( italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ∈ blackboard_Z start_POSTSUBSCRIPT ≥ 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT with ∑i∈{1,…,k}di≤dsubscript𝑖1…𝑘subscript𝑑𝑖𝑑\sum_{i\in\{1,\dots,k\}}d_{i}\leq d∑ start_POSTSUBSCRIPT italic_i ∈ { 1 , … , italic_k } end_POSTSUBSCRIPT italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ≤ italic_d such that for all D∈𝒟𝐷𝒟D\in\mathcal{D}italic_D ∈ caligraphic_D, there exists an i∈{1,…,k}𝑖1…𝑘i\in\{1,\dots,k\}italic_i ∈ { 1 , … , italic_k } satisfying |Di⁢△⁢D|≤disubscript𝐷𝑖△𝐷subscript𝑑𝑖|D_{i}\triangle D|\leq d_{i}| italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT △ italic_D | ≤ italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT? When 𝒟𝒟\mathcal{D}caligraphic_D is an explicitly given set of points, parameterized algorithms for these problems have been extensively studied in the area of clustering [amir2014efficiency, bandyapadhyayL023a, chen2024parameterized, demaine2005fixed, eiben2023parameterized, feldmann2020parameterized, inamdar2020capacitated]. This research initiates the study of clustering problems when 𝒟𝒟\mathcal{D}caligraphic_D is implicitly given as the solution domain of combinatorial problems. These problems are also natural in the context of diversification, since in real situations, the concept of diversity often means that the extracted elements cover the entire space comprehensively rather than being mutually dissimilar. This motivation is formulated by clustering problems, which extract a list of sets in 𝒟𝒟\mathcal{D}caligraphic_D such that for each set in 𝒟𝒟\mathcal{D}caligraphic_D, there is an extracted set near to it. 1.2 Our Results This paper consists of two parts. In the first part, we design general frameworks for solving diversification and clustering problems. In the second part, we apply our frameworks to several specific domains 𝒟𝒟\mathcal{D}caligraphic_D. 1.2.1 The Frameworks We define the following (−1,1)11(-1,1)( - 1 , 1 )-optimization oracle on 𝒟𝒟\mathcal{D}caligraphic_D and the exact extension oracle on 𝒟𝒟\mathcal{D}caligraphic_D. (−1,1)11(-1,1)( - 1 , 1 )-Optimization Oracle on 𝒟𝒟\mathcal{D}caligraphic_D: Let U𝑈Uitalic_U be a finite set, 𝒟⊆2U𝒟superscript2𝑈\mathcal{D}\subseteq 2^{U}caligraphic_D ⊆ 2 start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT, and w∈{−1,1}U𝑤superscript11𝑈w\in\{-1,1\}^{U}italic_w ∈ { - 1 , 1 } start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT be a weight vector. Return a set D∈𝒟𝐷𝒟D\in\mathcal{D}italic_D ∈ caligraphic_D that maximizes ∑e∈Dwesubscript𝑒𝐷subscript𝑤𝑒\sum_{e\in D}w_{e}∑ start_POSTSUBSCRIPT italic_e ∈ italic_D end_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT. Exact Extension Oracle on 𝒟𝒟\mathcal{D}caligraphic_D: Let U𝑈Uitalic_U be a finite set, r∈ℤ≥0𝑟subscriptℤabsent0r\in\mathbb{Z}_{\geq 0}italic_r ∈ blackboard_Z start_POSTSUBSCRIPT ≥ 0 end_POSTSUBSCRIPT, 𝒟⊆2U𝒟superscript2𝑈\mathcal{D}\subseteq 2^{U}caligraphic_D ⊆ 2 start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT, and C∈𝒟𝐶𝒟C\in\mathcal{D}italic_C ∈ caligraphic_D. Let X,Y⊆U𝑋𝑌𝑈X,Y\subseteq Uitalic_X , italic_Y ⊆ italic_U be two disjoint subsets of U𝑈Uitalic_U. If there exists a set D∈𝒟𝐷𝒟D\in\mathcal{D}italic_D ∈ caligraphic_D such that |D⁢△⁢C|=r𝐷△𝐶𝑟|D\triangle C|=r| italic_D △ italic_C | = italic_r, X⊆D𝑋𝐷X\subseteq Ditalic_X ⊆ italic_D, and Y∩D=∅𝑌𝐷Y\cap D=\emptysetitalic_Y ∩ italic_D = ∅, return one such set. If no such set exists, return ⊥bottom\bot⊥. When C=∅𝐶C=\emptysetitalic_C = ∅ and X=∅𝑋X=\emptysetitalic_X = ∅, we specifically call the exact extension oracle on 𝒟𝒟\mathcal{D}caligraphic_D the exact empty extension oracle on 𝒟𝒟\mathcal{D}caligraphic_D. Exact Empty Extension Oracle on 𝒟𝒟\mathcal{D}caligraphic_D: Let U𝑈Uitalic_U be a finite set, r∈ℤ≥0𝑟subscriptℤabsent0r\in\mathbb{Z}_{\geq 0}italic_r ∈ blackboard_Z start_POSTSUBSCRIPT ≥ 0 end_POSTSUBSCRIPT, 𝒟⊆2U𝒟superscript2𝑈\mathcal{D}\subseteq 2^{U}caligraphic_D ⊆ 2 start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT, and Y⊆U𝑌𝑈Y\subseteq Uitalic_Y ⊆ italic_U. If there exists a set D∈𝒟𝐷𝒟D\in\mathcal{D}italic_D ∈ caligraphic_D such that |D|=r𝐷𝑟|D|=r| italic_D | = italic_r and Y∩D=∅𝑌𝐷Y\cap D=\emptysetitalic_Y ∩ italic_D = ∅, return one such set. If no such set exists, return ⊥bottom\bot⊥. Let 𝒫𝒟subscript𝒫𝒟\mathcal{P}_{\mathcal{D}}caligraphic_P start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT be the max-min/max-sum diversification problem or the k𝑘kitalic_k-center/k𝑘kitalic_k-sum-of-radii clustering problem on 𝒟𝒟\mathcal{D}caligraphic_D. Our main result is FPT algorithms for solving 𝒫𝒟subscript𝒫𝒟\mathcal{P}_{\mathcal{D}}caligraphic_P start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT using these oracles. We construct frameworks for both types of parameterizations, k,l𝑘𝑙k,litalic_k , italic_l and k,d𝑘𝑑k,ditalic_k , italic_d. The result for the parameterization by k,l𝑘𝑙k,litalic_k , italic_l is as follows. Theorem 1.1. There exists an oracle algorithm solving 𝒫𝒟subscript𝒫𝒟\mathcal{P}_{\mathcal{D}}caligraphic_P start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT using the exact empty extension oracle on 𝒟𝒟\mathcal{D}caligraphic_D, where the number of oracle calls and time complexity are both FPT parameterized by k,l𝑘𝑙k,litalic_k , italic_l and for each call of an oracle, r𝑟ritalic_r and |Y|𝑌|Y|| italic_Y | are bounded by constants that depend only on k,l𝑘𝑙k,litalic_k , italic_l. The result for the parameterization by k,d𝑘𝑑k,ditalic_k , italic_d is as follows. Theorem 1.2. There exists a randomized oracle algorithm solving 𝒫𝒟subscript𝒫𝒟\mathcal{P}_{\mathcal{D}}caligraphic_P start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT using the (−1,1)11(-1,1)( - 1 , 1 )-optimization oracle on 𝒟𝒟\mathcal{D}caligraphic_D and the exact extension oracle on 𝒟𝒟\mathcal{D}caligraphic_D, where the number of oracle calls and time complexity are both FPT parameterized by k,d𝑘𝑑k,ditalic_k , italic_d and for each call of the exact extension oracle, r,|X|,|Y|𝑟𝑋𝑌r,|X|,|Y|italic_r , | italic_X | , | italic_Y | are bounded by constants that depend only on k,d𝑘𝑑k,ditalic_k , italic_d. 1.2.2 Applications of Theorem 1.1 For most domains 𝒟𝒟\mathcal{D}caligraphic_D, the exact empty extension oracle on 𝒟𝒟\mathcal{D}caligraphic_D can be straightforwardly designed by using an algorithm for extracting a single element from 𝒟𝒟\mathcal{D}caligraphic_D. Combining Theorem 1.1 with this empirical fact, we can claim the following surprising fact: For most domain 𝒟𝒟\mathcal{D}caligraphic_D, the diversification and clustering problems parameterized by k,l𝑘𝑙k,litalic_k , italic_l on 𝒟𝒟\mathcal{D}caligraphic_D are as easy as determining the non-emptiness of 𝒟𝒟\mathcal{D}caligraphic_D. We demonstrate this by providing an FPT algorithm for the exact empty extension oracle on several domains 𝒟𝒟\mathcal{D}caligraphic_D on which an FPT algorithm for the max-min diversification problem parameterized by k,l𝑘𝑙k,litalic_k , italic_l is known. Specifically, we design FPT algorithms for 𝒟𝒟\mathcal{D}caligraphic_D being the vertex cover [baste2022diversity, baste2019fpt], t𝑡titalic_t-hitting set [baste2022diversity], feedback vertex set [baste2022diversity], and common independent set of two matroids [fomin2024diversecollection]. Furthermore, we prove that our framework generalizes existing frameworks provided by [baste2019fpt] and [hanaka2021finding] in both the broader applicability to more general domains 𝒟𝒟\mathcal{D}caligraphic_D and the applicability to clustering problems. 1.2.3 Applications of Theorem 1.2 FPT algorithms for the max-min diversification problems on 𝒟𝒟\mathcal{D}caligraphic_D parameterized by k,d𝑘𝑑k,ditalic_k , italic_d are known for the cases where 𝒟𝒟\mathcal{D}caligraphic_D is the family of matroid bases [fomin2024diversecollection], perfect matchings [fomin2024diversecollection], and shortest paths [funayama2024parameterized]. Additionally, FPT algorithms parameterized by k,l𝑘𝑙k,litalic_k , italic_l are known for the cases where 𝒟𝒟\mathcal{D}caligraphic_D is the family of interval schedulings [hanaka2021finding] and the longest common subsequences of absolute constant number of strings [shida2024finding]. We consider the following domains 𝒟𝒟\mathcal{D}caligraphic_D, which include, generalize, or are entirely novel compared to, the cases mentioned above. Matroid Base Domain: Let M=(U,ℐ)𝑀𝑈ℐM=(U,\mathcal{I})italic_M = ( italic_U , caligraphic_I ) be a matroid. 𝒟⊆2U𝒟superscript2𝑈\mathcal{D}\subseteq 2^{U}caligraphic_D ⊆ 2 start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT is the family of bases of M𝑀Mitalic_M. Branching Domain: Let G=(V,E)𝐺𝑉𝐸G=(V,E)italic_G = ( italic_V , italic_E ) be a directed graph and r∈V𝑟𝑉r\in Vitalic_r ∈ italic_V. 𝒟⊆2E𝒟superscript2𝐸\mathcal{D}\subseteq 2^{E}caligraphic_D ⊆ 2 start_POSTSUPERSCRIPT italic_E end_POSTSUPERSCRIPT is the family of edge sets of branchings with root r𝑟ritalic_r in G𝐺Gitalic_G. Matching Domain: Let G=(V,E)𝐺𝑉𝐸G=(V,E)italic_G = ( italic_V , italic_E ) be an undirected graph and l∈ℤ≥0𝑙subscriptℤabsent0l\in\mathbb{Z}_{\geq 0}italic_l ∈ blackboard_Z start_POSTSUBSCRIPT ≥ 0 end_POSTSUBSCRIPT. 𝒟⊆2E𝒟superscript2𝐸\mathcal{D}\subseteq 2^{E}caligraphic_D ⊆ 2 start_POSTSUPERSCRIPT italic_E end_POSTSUPERSCRIPT is the family of matchings of size l𝑙litalic_l in G𝐺Gitalic_G. Minimum Edge s,t𝑠𝑡s,titalic_s , italic_t-Flow Domain: Let G=(V,E)𝐺𝑉𝐸G=(V,E)italic_G = ( italic_V , italic_E ) be a directed graph, b∈ℤ≥0𝑏subscriptℤabsent0b\in\mathbb{Z}_{\geq 0}italic_b ∈ blackboard_Z start_POSTSUBSCRIPT ≥ 0 end_POSTSUBSCRIPT, and s,t∈V𝑠𝑡𝑉s,t\in Vitalic_s , italic_t ∈ italic_V. 𝒟⊆2E𝒟superscript2𝐸\mathcal{D}\subseteq 2^{E}caligraphic_D ⊆ 2 start_POSTSUPERSCRIPT italic_E end_POSTSUPERSCRIPT is the family of the edge sets of s,t𝑠𝑡s,titalic_s , italic_t-flows of amount b𝑏bitalic_b using minimum number of edges. Minimum Steiner Tree Domain: Let G=(V,E)𝐺𝑉𝐸G=(V,E)italic_G = ( italic_V , italic_E ) be an undirected graph and T⊆V𝑇𝑉T\subseteq Vitalic_T ⊆ italic_V. 𝒟⊆2E𝒟superscript2𝐸\mathcal{D}\subseteq 2^{E}caligraphic_D ⊆ 2 start_POSTSUPERSCRIPT italic_E end_POSTSUPERSCRIPT is the family of edge sets of minimum Steiner trees in G𝐺Gitalic_G that connects the terminal set T𝑇Titalic_T. Minimum s,t𝑠𝑡s,titalic_s , italic_t-Cut Domain: Let G=(V,E)𝐺𝑉𝐸G=(V,E)italic_G = ( italic_V , italic_E ) be a directed graph and s,t∈V𝑠𝑡𝑉s,t\in Vitalic_s , italic_t ∈ italic_V. 𝒟⊆2V𝒟superscript2𝑉\mathcal{D}\subseteq 2^{V}caligraphic_D ⊆ 2 start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT is the family of vertex sets Z⊆V𝑍𝑉Z\subseteq Vitalic_Z ⊆ italic_V that include s𝑠sitalic_s, exclude t𝑡titalic_t, and minimize the number of edges between Z𝑍Zitalic_Z and V∖Z𝑉𝑍V\setminus Zitalic_V ∖ italic_Z. Edge Bipartization Domain: Let G=(V,E)𝐺𝑉𝐸G=(V,E)italic_G = ( italic_V , italic_E ) be an undirected graph. 𝒟⊆2V𝒟superscript2𝑉\mathcal{D}\subseteq 2^{V}caligraphic_D ⊆ 2 start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT is the family of vertex sets Z⊆V𝑍𝑉Z\subseteq Vitalic_Z ⊆ italic_V such that the total number of edges either within Z𝑍Zitalic_Z or within V∖Z𝑉𝑍V\setminus Zitalic_V ∖ italic_Z takes the minimum value s𝑠sitalic_s. Dynamic Programming Problems Domain: Let U𝑈Uitalic_U be a finite set, G=(V,E)𝐺𝑉𝐸G=(V,E)italic_G = ( italic_V , italic_E ) be a directed acyclic graph, and q∈VU𝑞superscript𝑉𝑈q\in V^{U}italic_q ∈ italic_V start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT be a labeling of vertices such that no path in G𝐺Gitalic_G passes through multiple vertices with the same label. 𝒟⊆2U𝒟superscript2𝑈\mathcal{D}\subseteq 2^{U}caligraphic_D ⊆ 2 start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT is the family of subsets D⊆U𝐷𝑈D\subseteq Uitalic_D ⊆ italic_U such that there exists a longest path in G𝐺Gitalic_G where the set of labels of the vertices on the path is D𝐷Ditalic_D. See Section 5.2 for the more precise definitions of each domain. By applying Theorem 1.2, we provide FPT algorithms for diversification and clustering problems on all those domains 𝒟𝒟\mathcal{D}caligraphic_D, where the parameterization is k𝑘kitalic_k and d𝑑ditalic_d except for the minimum Steiner tree domain, which is parameterized by k,d,|T|𝑘𝑑𝑇k,d,|T|italic_k , italic_d , | italic_T |, and the edge bipartization domain, which is parameterized by k,d,s𝑘𝑑𝑠k,d,sitalic_k , italic_d , italic_s. This result generalizes several existing results in both the broader applicability to more general domain 𝒟𝒟\mathcal{D}caligraphic_D and the applicability to clustering problems. Specifically, for the matching domain, our result for the case of |V|=2⁢l𝑉2𝑙|V|=2l| italic_V | = 2 italic_l corresponds to the result in [fomin2024diversecollection] for perfect matchings, and for the minimum edge s,t𝑠𝑡s,titalic_s , italic_t-flow domain and minimum Steiner tree domain, our results for the cases of b=1𝑏1b=1italic_b = 1 and |T|=2𝑇2|T|=2| italic_T | = 2, respectively, correspond to the results in [funayama2024parameterized] for the shortest paths. Our result for the dynamic programming problems domain implies that for a wide range of problems solvable by dynamic programming, the corresponding diversification and clustering problems have FPT algorithms parameterized by k𝑘kitalic_k and d𝑑ditalic_d. Particularly, this improves the parameterization of the result in [hanaka2021finding] for the interval schedulings and the result in [shida2024finding] for the longest common subsequences from k,l𝑘𝑙k,litalic_k , italic_l to k,d𝑘𝑑k,ditalic_k , italic_d. Furthermore, the technique for the branching domain leads to another FPT algorithm for the d𝑑ditalic_d-distinct branchings problem [bang2021k, bang2016parameterized, gutin2018k], which asks to find a pair consisting of a branching and an in-branching such that the Hamming distance between them is at least d𝑑ditalic_d. We also generalize this algorithm to the version of choosing multiple branchings and in-branchings, which is a new result in the literature of the d𝑑ditalic_d-distinct branchings problem. The rest of our results are entirely new, even for the max-min diversification problem. 1.3 Framework Overview In this section, we provide an overview of the entire flow of our frameworks. 1.3.1 From d𝑑ditalic_d-Limited k𝑘kitalic_k-Max-Distance Sparsifier to Diversification and Clustering The key idea for Theorem 1.1 is to design the following k𝑘kitalic_k-max-distance sparsifier of 𝒟𝒟\mathcal{D}caligraphic_D. Definition 1.3 (k𝑘kitalic_k-max-distance sparsifier). Let k∈ℤ≥1𝑘subscriptℤabsent1k\in\mathbb{Z}_{\geq 1}italic_k ∈ blackboard_Z start_POSTSUBSCRIPT ≥ 1 end_POSTSUBSCRIPT. Let U𝑈Uitalic_U be a finite set and 𝒟,ℱ⊆2U𝒟ℱsuperscript2𝑈\mathcal{D},\mathcal{F}\subseteq 2^{U}caligraphic_D , caligraphic_F ⊆ 2 start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT. We say that 𝒦⊆𝒟𝒦𝒟\mathcal{K}\subseteq\mathcal{D}caligraphic_K ⊆ caligraphic_D is a k𝑘kitalic_k-max-distance sparsifier of 𝒟𝒟\mathcal{D}caligraphic_D with respect to ℱℱ\mathcal{F}caligraphic_F if for any (F1,…,Fk)∈ℱksubscript𝐹1…subscript𝐹𝑘superscriptℱ𝑘(F_{1},\dots,F_{k})\in\mathcal{F}^{k}( italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_F start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ∈ caligraphic_F start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT and (z1,…,zk)∈ℤ≥0ksubscript𝑧1…subscript𝑧𝑘superscriptsubscriptℤabsent0𝑘(z_{1},\dots,z_{k})\in\mathbb{Z}_{\geq 0}^{k}( italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ∈ blackboard_Z start_POSTSUBSCRIPT ≥ 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT, the two conditions • There exists D∈𝒟𝐷𝒟D\in\mathcal{D}italic_D ∈ caligraphic_D such that for each i∈{1,…,k}𝑖1…𝑘i\in\{1,\dots,k\}italic_i ∈ { 1 , … , italic_k }, |Fi⁢△⁢D|≥zisubscript𝐹𝑖△𝐷subscript𝑧𝑖|F_{i}\triangle D|\geq z_{i}| italic_F start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT △ italic_D | ≥ italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. • There exists K∈𝒦𝐾𝒦K\in\mathcal{K}italic_K ∈ caligraphic_K such that for each i∈{1,…,k}𝑖1…𝑘i\in\{1,\dots,k\}italic_i ∈ { 1 , … , italic_k }, |Fi⁢△⁢K|≥zisubscript𝐹𝑖△𝐾subscript𝑧𝑖|F_{i}\triangle K|\geq z_{i}| italic_F start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT △ italic_K | ≥ italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. are equivalent. Unless specifically noted, when we write k𝑘kitalic_k-max-distance sparsifier of 𝒟𝒟\mathcal{D}caligraphic_D, we mean the case where 𝒟=ℱ𝒟ℱ\mathcal{D}=\mathcal{F}caligraphic_D = caligraphic_F. Similarly, the key idea for Theorem 1.2 is to design the following d𝑑ditalic_d-limited k𝑘kitalic_k-max-distance sparsifier of 𝒟𝒟\mathcal{D}caligraphic_D. Definition 1.4 (d𝑑ditalic_d-limited k𝑘kitalic_k-max-distance sparsifier). Let k∈ℤ≥1𝑘subscriptℤabsent1k\in\mathbb{Z}_{\geq 1}italic_k ∈ blackboard_Z start_POSTSUBSCRIPT ≥ 1 end_POSTSUBSCRIPT and d∈ℤ≥0𝑑subscriptℤabsent0d\in\mathbb{Z}_{\geq 0}italic_d ∈ blackboard_Z start_POSTSUBSCRIPT ≥ 0 end_POSTSUBSCRIPT. Let U𝑈Uitalic_U be a finite set and 𝒟,ℱ⊆2U𝒟ℱsuperscript2𝑈\mathcal{D},\mathcal{F}\subseteq 2^{U}caligraphic_D , caligraphic_F ⊆ 2 start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT. We say that 𝒦⊆𝒟𝒦𝒟\mathcal{K}\subseteq\mathcal{D}caligraphic_K ⊆ caligraphic_D is a d𝑑ditalic_d-limited k𝑘kitalic_k-max-distance sparsifier of 𝒟𝒟\mathcal{D}caligraphic_D with respect to ℱℱ\mathcal{F}caligraphic_F if for any (F1,…,Fk)∈ℱksubscript𝐹1…subscript𝐹𝑘superscriptℱ𝑘(F_{1},\dots,F_{k})\in\mathcal{F}^{k}( italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_F start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ∈ caligraphic_F start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT and (z1,…,zk)∈{0,…,d}ksubscript𝑧1…subscript𝑧𝑘superscript0…𝑑𝑘(z_{1},\dots,z_{k})\in\{0,\dots,d\}^{k}( italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ∈ { 0 , … , italic_d } start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT, the two conditions • There exists D∈𝒟𝐷𝒟D\in\mathcal{D}italic_D ∈ caligraphic_D such that for each i∈{1,…,k}𝑖1…𝑘i\in\{1,\dots,k\}italic_i ∈ { 1 , … , italic_k }, |Fi⁢△⁢D|≥zisubscript𝐹𝑖△𝐷subscript𝑧𝑖|F_{i}\triangle D|\geq z_{i}| italic_F start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT △ italic_D | ≥ italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. • There exists K∈𝒦𝐾𝒦K\in\mathcal{K}italic_K ∈ caligraphic_K such that for each i∈{1,…,k}𝑖1…𝑘i\in\{1,\dots,k\}italic_i ∈ { 1 , … , italic_k }, |Fi⁢△⁢K|≥zisubscript𝐹𝑖△𝐾subscript𝑧𝑖|F_{i}\triangle K|\geq z_{i}| italic_F start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT △ italic_K | ≥ italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. are equivalent. Unless specifically noted, when we write d𝑑ditalic_d-limited k𝑘kitalic_k-max-distance sparsifier of 𝒟𝒟\mathcal{D}caligraphic_D, we mean the case where 𝒟=ℱ𝒟ℱ\mathcal{D}=\mathcal{F}caligraphic_D = caligraphic_F. The difference between the two sparsifiers is that the domain of (z1,…,zk)subscript𝑧1…subscript𝑧𝑘(z_{1},\dots,z_{k})( italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) is ℤ≥0ksuperscriptsubscriptℤabsent0𝑘\mathbb{Z}_{\geq 0}^{k}blackboard_Z start_POSTSUBSCRIPT ≥ 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT in the former case, while it is {0,…,d}ksuperscript0…𝑑𝑘\{0,\dots,d\}^{k}{ 0 , … , italic_d } start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT in the latter. By definition, any k𝑘kitalic_k-max-distance sparsifier is also a d𝑑ditalic_d-limited k𝑘kitalic_k-max-distance sparsifier for any d∈ℤ≥0𝑑subscriptℤabsent0d\in\mathbb{Z}_{\geq 0}italic_d ∈ blackboard_Z start_POSTSUBSCRIPT ≥ 0 end_POSTSUBSCRIPT. We can prove that given a d𝑑ditalic_d-limited (k−1)𝑘1(k-1)( italic_k - 1 )-max-distance sparsifier of 𝒟𝒟\mathcal{D}caligraphic_D with size bounded by a constant that depends only on k𝑘kitalic_k and d𝑑ditalic_d, we can construct FPT algorithms parameterized by k𝑘kitalic_k and d𝑑ditalic_d for the max-min/max-sum diversification problems on 𝒟𝒟\mathcal{D}caligraphic_D. Similarly, we can prove that given a (d+1)𝑑1(d+1)( italic_d + 1 )-limited k𝑘kitalic_k-max-distance sparsifier of 𝒟𝒟\mathcal{D}caligraphic_D with size bounded by a constant that depends only on k𝑘kitalic_k and d𝑑ditalic_d, we can construct FPT algorithms parameterized by k𝑘kitalic_k and d𝑑ditalic_d for the k𝑘kitalic_k-center/k𝑘kitalic_k-sum-of-radii clustering problems on 𝒟𝒟\mathcal{D}caligraphic_D. Therefore, to prove Theorems 1.1 and 1.2, it suffices to construct FPT oracle algorithms for designing k𝑘kitalic_k-max-distance sparsifiers and d𝑑ditalic_d-limited k𝑘kitalic_k-max-distance sparsifiers, respectively. 1.3.2 Computing k𝑘kitalic_k-Max-Distance Sparsifier The remaining task towards Theorem 1.1 is to provide an FPT algorithm parameterized by k,l𝑘𝑙k,litalic_k , italic_l that constructs a k𝑘kitalic_k-max-distance sparsifier of 𝒟𝒟\mathcal{D}caligraphic_D with size bounded by a constant that depends only on k𝑘kitalic_k and l𝑙litalic_l. Our algorithm starts with 𝒦=∅𝒦\mathcal{K}=\emptysetcaligraphic_K = ∅ and exhaustively adds sets of 𝒟𝒟\mathcal{D}caligraphic_D to 𝒦𝒦\mathcal{K}caligraphic_K until 𝒦𝒦\mathcal{K}caligraphic_K becomes a k𝑘kitalic_k-max-distance sparsifier. We can prove that if adding a set to 𝒦𝒦\mathcal{K}caligraphic_K would cause a sufficiently large sunflower (see Section 3 for the definition) consisting of sets of the same size to appear in 𝒦𝒦\mathcal{K}caligraphic_K, then we do not need to add that set to 𝒦𝒦\mathcal{K}caligraphic_K (in fact, for the sake of simplifying the framework, we prove a slightly stronger statement). By using the well-known sunflower lemma (Lemma 3.1), we can bound the number of repetitions by a constant. To choose a set to be added, we design an FPT algorithm using calls of the exact empty extension oracle. 1.3.3 Computing d𝑑ditalic_d-Limited k𝑘kitalic_k-Max-Distance Sparsifier The algorithm in the previous section alone is insufficient to prove Theorem 1.2 since l𝑙litalic_l is unbounded and the bound for the number of repetitions cannot be used. Our algorithm divides 𝒟𝒟\mathcal{D}caligraphic_D into at most k𝑘kitalic_k clusters, computes a d𝑑ditalic_d-limited k𝑘kitalic_k-max-distance sparsifier for each cluster, and outputs their union. Let p>2⁢d𝑝2𝑑p>2ditalic_p > 2 italic_d be a constant that depends only on k𝑘kitalic_k and d𝑑ditalic_d. We first find 𝒞⊆𝒟𝒞𝒟\mathcal{C}\subseteq\mathcal{D}caligraphic_C ⊆ caligraphic_D satisfying the following properties. • |𝒞|≤k𝒞𝑘|\mathcal{C}|\leq k| caligraphic_C | ≤ italic_k. • For all distinct C,C′∈𝒞𝐶superscript𝐶′𝒞C,C^{\prime}\in\mathcal{C}italic_C , italic_C start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ∈ caligraphic_C, |C⁢△⁢C′|>2⁢d𝐶△superscript𝐶′2𝑑|C\triangle C^{\prime}|>2d| italic_C △ italic_C start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT | > 2 italic_d. • For all D∈𝒟𝐷𝒟D\in\mathcal{D}italic_D ∈ caligraphic_D, there exists C∈𝒞𝐶𝒞C\in\mathcal{C}italic_C ∈ caligraphic_C such that |D⁢△⁢C|≤p𝐷△𝐶𝑝|D\triangle C|\leq p| italic_D △ italic_C | ≤ italic_p. If such a family does not exist, a trivial d𝑑ditalic_d-limited k𝑘kitalic_k-max-distance sparsifier of 𝒟𝒟\mathcal{D}caligraphic_D will be found, and we output it and terminate. We provide an algorithm for computing 𝒞𝒞\mathcal{C}caligraphic_C. Our algorithm starts with 𝒞=∅𝒞\mathcal{C}=\emptysetcaligraphic_C = ∅ and exhaustively adds sets in 𝒟𝒟\mathcal{D}caligraphic_D to 𝒞𝒞\mathcal{C}caligraphic_C until 𝒞𝒞\mathcal{C}caligraphic_C satisfies the above conditions or its size exceeds k𝑘kitalic_k. To choose the elements to be added, we randomly sample w∈{−1,1}U𝑤superscript11𝑈w\in\{-1,1\}^{U}italic_w ∈ { - 1 , 1 } start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT and call a (−1,1)11(-1,1)( - 1 , 1 )-optimization oracle. We can prove for sufficiently large constant p𝑝pitalic_p that if there exists D∈𝒟𝐷𝒟D\in\mathcal{D}italic_D ∈ caligraphic_D such that |D⁢△⁢C|>p𝐷△𝐶𝑝|D\triangle C|>p| italic_D △ italic_C | > italic_p for any C∈𝒞𝐶𝒞C\in\mathcal{C}italic_C ∈ caligraphic_C, with a constant probability, the (−1,1)11(-1,1)( - 1 , 1 )-optimization oracle will find a D∈𝒟𝐷𝒟D\in\mathcal{D}italic_D ∈ caligraphic_D such that |D⁢△⁢C|>2⁢d𝐷△𝐶2𝑑|D\triangle C|>2d| italic_D △ italic_C | > 2 italic_d for any C∈𝒞𝐶𝒞C\in\mathcal{C}italic_C ∈ caligraphic_C. Thus, if 𝒞𝒞\mathcal{C}caligraphic_C does not meet the conditions, by calling the (−1,1)11(-1,1)( - 1 , 1 )-optimization oracle a sufficient number of times, we can find a set to add to 𝒞𝒞\mathcal{C}caligraphic_C with high probability. Here, we provide an algorithm for computing a d𝑑ditalic_d-limited k𝑘kitalic_k-max-distance sparsifier of 𝒟𝒟\mathcal{D}caligraphic_D using 𝒞𝒞\mathcal{C}caligraphic_C. For each cluster 𝒟C:={D∈𝒟:|D⁢△⁢C|≤p}assignsubscript𝒟𝐶conditional-set𝐷𝒟𝐷△𝐶𝑝\mathcal{D}_{C}:=\{D\in\mathcal{D}\colon|D\triangle C|\leq p\}caligraphic_D start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT := { italic_D ∈ caligraphic_D : | italic_D △ italic_C | ≤ italic_p }, let 𝒟C∗:={D⁢△⁢C:D∈𝒟C}assignsubscriptsuperscript𝒟𝐶conditional-set𝐷△𝐶𝐷subscript𝒟𝐶\mathcal{D}^{*}_{C}:=\{D\triangle C\colon D\in\mathcal{D}_{C}\}caligraphic_D start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT := { italic_D △ italic_C : italic_D ∈ caligraphic_D start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT }. The algorithm computes a k𝑘kitalic_k-max-distance sparsifier of each 𝒟C∗subscriptsuperscript𝒟𝐶\mathcal{D}^{*}_{C}caligraphic_D start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT and outputs their union. For technical reasons, we actually compute a slightly more general object, but we will not delve into the details here. Since each 𝒟C∗subscriptsuperscript𝒟𝐶\mathcal{D}^{*}_{C}caligraphic_D start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT consists only of sets whose size is at most p𝑝pitalic_p, the k𝑘kitalic_k-max-distance sparsifier of 𝒟C∗subscriptsuperscript𝒟𝐶\mathcal{D}^{*}_{C}caligraphic_D start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT can be constructed using the algorithm in Section 1.3.2. The exact empty extension oracle on 𝒟C∗subscriptsuperscript𝒟𝐶\mathcal{D}^{*}_{C}caligraphic_D start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT corresponds to the exact extension oracle on 𝒟𝒟\mathcal{D}caligraphic_D. Here, we note the difference between our framework and that used by [fomin2024diversecollection] and [funayama2024parameterized] to provide FPT algorithms for the max-min diversification problem on 𝒟𝒟\mathcal{D}caligraphic_D when 𝒟𝒟\mathcal{D}caligraphic_D is the family of perfect matchings and shortest paths, respectively. Their algorithms also start by dividing 𝒟𝒟\mathcal{D}caligraphic_D into clusters. However, their algorithms perform stricter clustering than ours. Specifically, in their clustering, clusters 𝒟Csubscript𝒟𝐶\mathcal{D}_{C}caligraphic_D start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT corresponding to different C∈𝒞𝐶𝒞C\in\mathcal{C}italic_C ∈ caligraphic_C must be well-separated. In contrast, we allow clusters to overlap. This simplifies the clustering step compared to their approach at the cost of a more challenging task afterward. We resolve this more challenging task by introducing and designing the d𝑑ditalic_d-limited k𝑘kitalic_k-max-distance sparsifier. 1.4 Further Related Work 1.4.1 Max-Sum Diversification Problems The max-sum diversification problem is often more tractable compared to the max-min diversification problem, and polynomial-time algorithms are known for multiple domains 𝒟𝒟\mathcal{D}caligraphic_D. [hanaka2021finding] provided a polynomial-time algorithm for the case where 𝒟𝒟\mathcal{D}caligraphic_D is the base family of a matroid. [hanaka2022computing] provided polynomial-time algorithms for the cases where 𝒟𝒟\mathcal{D}caligraphic_D is the family of shortest paths, branchings, and bipartite matchings. [BergMS23] provided a polynomial-time algorithm for the case where 𝒟𝒟\mathcal{D}caligraphic_D is the family of edge sets of minimum s,t𝑠𝑡s,titalic_s , italic_t-cuts. There has also been active research on approximation algorithms for the max-sum diversification problem. [hanaka2023framework] proposed a generic framework that provides local search-based approximation algorithms for max-sum diversification problems. [gao2022obtaining] provided a framework for bicriteria approximation algorithms for the case where 𝒟𝒟\mathcal{D}caligraphic_D is a family of (not necessarily optimal) solutions of an optimization problem. [do2023diverse] discussed the tradeoff between solution quality and diversity for submodular maximization on matroids. 1.4.2 Further Parameterized Algorithms for Max-Min Diversification Problems There are several other research directions on parameterized algorithms for max-min diversification problems. [eiben2024determinantal] introduced a technique called determinantal sieving and improved the time complexity of the algorithms by [fomin2024diversecollection] for the cases where 𝒟𝒟\mathcal{D}caligraphic_D is the base of a matroid, a matching, and the common independent set of two matroids. [drabik2024finding] provided an FPT algorithm for diversification problems on domains expressible by MSO1subscriptMSO1\mathrm{MSO}_{1}roman_MSO start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT formulas on graphs with bounded cliquewidth, parameterized by k𝑘kitalic_k, cliquewidth, and the length of the MSO1subscriptMSO1\mathrm{MSO}_{1}roman_MSO start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT formula. [arrighi2023synchronization] proposed an FPT algorithm for the max-min diversification problem on the set of synchronizing words for a deterministic finite automaton, where the distance between two words is measured by the edit distance. 1.4.3 Parameterized Algorithms for Clustering Problems For the case where the point set is explicitly given, several researches have been conducted on parameterized algorithms for clustering problems. It is easy to see that when parameterized solely by k𝑘kitalic_k and the metric is general, the k𝑘kitalic_k-center clustering problem includes the dominating set problem, which is known to be W[2]-hard. Therefore, the study of FPT approximation algorithms is very active recently [bandyapadhyayL023a, chen2024parameterized, eiben2023parameterized, feldmann2020parameterized, inamdar2020capacitated]. Research on providing exact parameterized algorithms on restricted metrics is also being conducted [demaine2005fixed]. Probably, the research most closely related to our situation is the extension of the closest string problem by [amir2014efficiency]. They defined the k𝑘kitalic_k-center and k𝑘kitalic_k-sum-of-radii clustering problems for sets of strings and investigated their parameterized complexity. When the alphabet is binary, they consider the same problem as ours. However, their results do not directly imply our results since, in our setting, the points are implicitly given as the solution domain of combinatorial problems, which are generally of exponential size. 1.5 Organization The rest of this paper is organized as follows. In Section 2, we provide FPT algorithms for solving diversification and clustering problems on 𝒟𝒟\mathcal{D}caligraphic_D using a d𝑑ditalic_d-limited k𝑘kitalic_k-max-distance sparsifier of 𝒟𝒟\mathcal{D}caligraphic_D of constant size. In Section 3, we complete the proof of Theorem 1.1 by providing an FPT oracle algorithm parameterized by k,l𝑘𝑙k,litalic_k , italic_l that computes a k𝑘kitalic_k-max-distance sparsifier of 𝒟𝒟\mathcal{D}caligraphic_D with size bounded by a constant that depends only on k,l𝑘𝑙k,litalic_k , italic_l. In Section 4, we complete the proof of Theorem 1.2 by providing an FPT oracle algorithm parameterized by k,d𝑘𝑑k,ditalic_k , italic_d that computes a d𝑑ditalic_d-limited k𝑘kitalic_k-max-distance sparsifier of 𝒟𝒟\mathcal{D}caligraphic_D with size bounded by a constant that depends only on k,d𝑘𝑑k,ditalic_k , italic_d. The discussion in Section 4 internally uses the results from Section 3. Finally, in Section 5, we apply the results of Theorems 1.1 and 1.2 to several domains 𝒟𝒟\mathcal{D}caligraphic_D to obtain FPT algorithms for diversification and clustering problems."
https://arxiv.org/html/2411.02821v1,Faster Exact and Parameterized Algorithm for Feedback Vertex Set in Bipartite Tournaments,"A bipartite tournament is a directed graph T:=(A∪B,E)assign𝑇𝐴𝐵𝐸T:=(A\cup B,E)italic_T := ( italic_A ∪ italic_B , italic_E ) such that every pair of vertices (a,b),a∈A,b∈Bformulae-sequence𝑎𝑏𝑎𝐴𝑏𝐵(a,b),a\in A,b\in B( italic_a , italic_b ) , italic_a ∈ italic_A , italic_b ∈ italic_B are connected by an arc, and no arc connects two vertices of A𝐴Aitalic_A or two vertices of B𝐵Bitalic_B. A feedback vertex set is a set S𝑆Sitalic_S of vertices in T𝑇Titalic_T such that T−S𝑇𝑆T-Sitalic_T - italic_S is acyclic. In this article we consider the Feedback Vertex Set problem in bipartite tournaments. Here the input is a bipartite tournament T𝑇Titalic_T on n𝑛nitalic_n vertices together with an integer k𝑘kitalic_k, and the task is to determine whether T𝑇Titalic_T has a feedback vertex set of size at most k𝑘kitalic_k. We give a new algorithm for Feedback Vertex Set in Bipartite Tournaments. The running time of our algorithm is upper-bounded by O⁢(1.6181k+nO⁢(1))𝑂superscript1.6181𝑘superscript𝑛𝑂1O(1.6181^{k}+n^{O(1)})italic_O ( 1.6181 start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT + italic_n start_POSTSUPERSCRIPT italic_O ( 1 ) end_POSTSUPERSCRIPT ), improving over the previously best known algorithm with running time 2k⁢kO⁢(1)+nO⁢(1)superscript2𝑘superscript𝑘𝑂1superscript𝑛𝑂12^{k}k^{O(1)}+n^{O(1)}2 start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_k start_POSTSUPERSCRIPT italic_O ( 1 ) end_POSTSUPERSCRIPT + italic_n start_POSTSUPERSCRIPT italic_O ( 1 ) end_POSTSUPERSCRIPT [Hsiao, ISAAC 2011]. As a by-product, we also obtain the fastest currently known exact exponential-time algorithm for the problem, with running time O⁢(1.3820n)𝑂superscript1.3820𝑛O(1.3820^{n})italic_O ( 1.3820 start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ).","A feedback vertex set in a graph G𝐺Gitalic_G is a vertex set whose removal makes the graph acyclic. The Feedback Vertex Set problem is a well-studied graph problem where input is a graph G𝐺Gitalic_G (directed or undirected) and the task is to find a smallest possible feedback vertex set. Finding such an optimal feedback vertex set turns out to be NP-complete [22], indeed the problem is one of the very first to be shown NP-complete in the influential paper of Karp [26]. Since, polynomial time algorithms are highly unlikely, Feedback Vertex Set on general directed and undirected graphs has been extensively studied from the perspective of approximation algorithms [2, 15], parameterized algorithms [6, 10, 27], exact exponential-time algorithms [29, 35] as well as graph theory [14, 30]. This paper belongs to a long line of work studying the complexity of Feedback Vertex Set on restricted classes of graphs. On one hand Feedback Vertex Set remains NP-complete on tournaments and bipartite tournaments [5], planar undirected graphs [22], planar directed graphs with in-degree and out-degree at most 3333 [22] as well as directed graphs with in-degree and out-degree at most 2222 [22]. On the other hand the problem is polynomial time solvable on undirected graphs of maximum degree 3333 [33], chordal graphs [16] and weakly chordal graphs [20], indeed on any class of graphs with polynomially many potential maximal cliques [20]. Being a problem of fundamental importance, Feedback Vertex Set has been approached algorithmically even on the classes of graphs where it remains NP-complete. For example the problem admits (efficient) polynomial time approximation schemes [8, 12, 18], sub-exponential time parameterized algorithms [11] and linear kernels [19] on classes of graphs excluding a fixed graph H𝐻Hitalic_H as a minor. In this paper we study the problem on bipartite tournaments. A tournament is a subclass of directed graphs where every pair of vertices are connected by an arc. A bipartite tournament is a directed graph where the vertices are partitioned into two sets A𝐴Aitalic_A and B𝐵Bitalic_B, there is an arc connecting every vertex in A𝐴Aitalic_A with every vertex in B𝐵Bitalic_B, and there are no edges between vertices of A𝐴Aitalic_A and vertices of B𝐵Bitalic_B. Tournaments arise naturally from round-robin competitions whereas bipartite tournaments model a two-team competition in which every player in one team plays against every player of the other team. Here arcs are drawn from the winning to the losing player, and often one seeks to rank the players from “best” to “worst” such that players that appear higher in the ranking beat all lower ranked players they played against. Such an absolute ranking possible only if there are no cycles in the tournament. The size of the smallest feedback vertex set then becomes a measure of how far the tournament is from admitting a consistent ranking. For this reason the structure of cycles and feedback vertex sets in (bipartite) tournaments has been studied both from the perspective of graph theory [3, 7, 21] and algorithms. For bipartite tournaments, finding a feedback vertex set reduces to hitting all cycles of length 4444. For this reason the Feedback Vertex Set problem is more computationally tractable on bipartite tournaments than on general directed graphs. Specifically the best known approximation algorithm for Feedback Vertex Set on directed graphs has an approximation factor of O⁢(log⁡n⋅log⁡log⁡n)𝑂⋅𝑛𝑛O(\log n\cdot\log\log n)italic_O ( roman_log italic_n ⋅ roman_log roman_log italic_n ) [15], and the problem does not admit a constant factor approximation assuming the Unique Games Conjecture [23]. On bipartite tournaments it is easy to obtain a 4444-approximation (see Lemma 2.2). Further, an improved approximation algorithm with ratio 2222 was obtained by Zuylen. [34]. Similarly, it was open for a long time whether Feedback Vertex Set on general directed graphs admits an FPT algorithm, that is an algorithm that determines whether there exists a solution of size at most k𝑘kitalic_k in time f⁢(k)⁢nO⁢(1)𝑓𝑘superscript𝑛𝑂1f(k)n^{O(1)}italic_f ( italic_k ) italic_n start_POSTSUPERSCRIPT italic_O ( 1 ) end_POSTSUPERSCRIPT. In 2008, Chen et al. [6] gave an algorithm with running time O⁢(4k⁢kO⁢(1)⁢k!⁢n⁢m)𝑂superscript4𝑘superscript𝑘𝑂1𝑘𝑛𝑚O(4^{k}k^{O(1)}k!nm)italic_O ( 4 start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_k start_POSTSUPERSCRIPT italic_O ( 1 ) end_POSTSUPERSCRIPT italic_k ! italic_n italic_m ), and it is an outstanding open problem whether there exists an algorithm with running time 2O⁢(k)⁢nO⁢(1)superscript2𝑂𝑘superscript𝑛𝑂12^{O(k)}n^{O(1)}2 start_POSTSUPERSCRIPT italic_O ( italic_k ) end_POSTSUPERSCRIPT italic_n start_POSTSUPERSCRIPT italic_O ( 1 ) end_POSTSUPERSCRIPT. For bipartite tournaments, the realization that it is necessary and sufficient to hit all cycles of length 4444 yields a simple 4k⁢nO⁢(1)superscript4𝑘superscript𝑛𝑂14^{k}n^{O(1)}4 start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_n start_POSTSUPERSCRIPT italic_O ( 1 ) end_POSTSUPERSCRIPT time parameterized algorithm: recursively branch on vertices of a cycle of length 4444. Truß [32] gave an improved algorithm with running time 3.12k⁢nO⁢(1)superscript3.12𝑘superscript𝑛𝑂13.12^{k}n^{O(1)}3.12 start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_n start_POSTSUPERSCRIPT italic_O ( 1 ) end_POSTSUPERSCRIPT, Sasatte [31] further improved the running time to 3k⁢nO⁢(1)superscript3𝑘superscript𝑛𝑂13^{k}n^{O(1)}3 start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_n start_POSTSUPERSCRIPT italic_O ( 1 ) end_POSTSUPERSCRIPT, while Hsiao [25] gave an algorithm with running time 2k⁢nO⁢(1)superscript2𝑘superscript𝑛𝑂12^{k}n^{O(1)}2 start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_n start_POSTSUPERSCRIPT italic_O ( 1 ) end_POSTSUPERSCRIPT. Prior to this work, this was the fastest known parameterized algorithm for Feedback Vertex Set on bipartite tournaments. Our main result is an algorithm with running time O⁢(1.6181k+nO⁢(1))𝑂superscript1.6181𝑘superscript𝑛𝑂1O(1.6181^{k}+n^{O(1)})italic_O ( 1.6181 start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT + italic_n start_POSTSUPERSCRIPT italic_O ( 1 ) end_POSTSUPERSCRIPT ). Using the recent black-box reduction from parameterized to exact exponential time algorithms of Fomin et al. [17] we also obtain an exponential-time algorithm running in O⁢(1.3820n)𝑂superscript1.3820𝑛O(1.3820^{n})italic_O ( 1.3820 start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ) time. Methods. Our algorithm is based on the recent parameterized algorithm with running time O⁢(1.6181k+nO⁢(1))𝑂superscript1.6181𝑘superscript𝑛𝑂1O(1.6181^{k}+n^{O(1)})italic_O ( 1.6181 start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT + italic_n start_POSTSUPERSCRIPT italic_O ( 1 ) end_POSTSUPERSCRIPT ) by the authors [28] for Feedback Vertex Set in tournaments. The main idea of this algorithm is that tournaments are very rigid. Given as input a tournament T𝑇Titalic_T, by obtaining a large set M𝑀Mitalic_M of vertices that is disjoint from the feedback vertex set H𝐻Hitalic_H sought for, we can get a rough sketch of the rigid structure of T−H𝑇𝐻T-Hitalic_T - italic_H. This structure is then very useful for recovering the solution H𝐻Hitalic_H. Indeed, the only way that vertices that are “far apart” in the approximate sketch of the structure of T−H𝑇𝐻T-Hitalic_T - italic_H can interact with each other is by being “in conflict”. Out of two vertices that are in conflict, one of them has to be deleted. Thus, dealing with conflicts can be done in a similar fashion as with edges in the Vertex Cover problem. For any vertex v𝑣vitalic_v appearing in at least two conflicts, branch into two sub-problems. In the first sub-problem v𝑣vitalic_v is deleted, in the second all vertices in conflict with v𝑣vitalic_v are deleted. If there are no conflicts it is sufficient to solve the Feedback Vertex Set problem “locally”. If every vertex appears in at most one conflict a divide and conquer approach can be taken. Because bipartite tournaments are also quite “rigid”, we expected that the same approach would easily give an algorithm for Feedback Vertex Set on bipartite tournaments with the same running time. Our expectations were both wrong and correct; indeed we do obtain an algorithm for Feedback Vertex Set on bipartite tournaments with the same template and the same running time as the algorithm for tournaments [28], yet the adaptation turned out to be anything but easy. Specifically, in virtually every step of the algorithm, the lack of a unique topological sort of acyclic bipartite tournaments presented significant challenges. The fact that these challenges still could be overcome by sub-exponential time cleaning procedures gives hope that the same template could be applicable in several situations where one seeks a “small” set of vertices or edges to delete in order to modify the input graph to a “rigid” structure; such as Cluster Vertex Deletion, Cograph Vertex Deletion and Feedback Vertex Set in the more general setting when the input graph is a multi-partite tournament [24]. Organization of the paper. In Section 2 we set up definitions and notation, and state a few useful preliminary results. The standard graph notation and parameterized complexity terminology is set up in the appendix. In Section 3 we define and prove some properties of M𝑀Mitalic_M-sequence. In Section 4 we define and give an algorithm for Constrained Feedback Vertex Set problem."
https://arxiv.org/html/2411.02764v1,"Fast, robust approximate message passing","We give a fast, spectral procedure for implementing approximate-message passing (AMP) algorithms robustly. For any quadratic optimization problem over symmetric matrices X𝑋Xitalic_X with independent subgaussian entries, and any separable AMP algorithm 𝒜𝒜\mathcal{A}caligraphic_A, our algorithm performs a spectral pre-processing step and then mildly modifies the iterates of 𝒜𝒜\mathcal{A}caligraphic_A. If given the perturbed input X+E∈ℝn×n𝑋𝐸superscriptℝ𝑛𝑛X+E\in\mathbb{R}^{n\times n}italic_X + italic_E ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_n end_POSTSUPERSCRIPT for any E𝐸Eitalic_E supported on a ε⁢n×ε⁢n𝜀𝑛𝜀𝑛\varepsilon n\times\varepsilon nitalic_ε italic_n × italic_ε italic_n principal minor, our algorithm outputs a solution v^^𝑣\hat{v}over^ start_ARG italic_v end_ARG which is guaranteed to be close to the output of 𝒜𝒜\mathcal{A}caligraphic_A on the uncorrupted X𝑋Xitalic_X, with ‖𝒜⁢(X)−v^‖2⩽f⁢(ε)⁢‖𝒜⁢(X)‖2subscriptnorm𝒜𝑋^𝑣2𝑓𝜀subscriptnorm𝒜𝑋2\|\mathcal{A}(X)-\hat{v}\|_{2}\leqslant f(\varepsilon)\|\mathcal{A}(X)\|_{2}∥ caligraphic_A ( italic_X ) - over^ start_ARG italic_v end_ARG ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ⩽ italic_f ( italic_ε ) ∥ caligraphic_A ( italic_X ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT where f⁢(ε)→0→𝑓𝜀0f(\varepsilon)\to 0italic_f ( italic_ε ) → 0 as ε→0→𝜀0\varepsilon\to 0italic_ε → 0 depending only on ε𝜀\varepsilonitalic_ε.","Approximate Message Passing (AMP) is a family of algorithmic methods which generalize matrix power iteration. Suppose we are given a symmetric matrix X∈ℝn×n𝑋superscriptℝ𝑛𝑛X\in\mathbb{R}^{n\times n}italic_X ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_n end_POSTSUPERSCRIPT, and our goal is to maximize the quadratic form v⊤⁢X⁢vsuperscript𝑣top𝑋𝑣v^{\top}Xvitalic_v start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT italic_X italic_v over vectors v𝑣vitalic_v in some constraint set K𝐾Kitalic_K. The basic AMP algorithm starts from some initialization x(0)∈ℝnsuperscript𝑥0superscriptℝ𝑛x^{(0)}\in\mathbb{R}^{n}italic_x start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT and computes iterates x(1),x(2),…superscript𝑥1superscript𝑥2…x^{(1)},x^{(2)},\ldotsitalic_x start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT , italic_x start_POSTSUPERSCRIPT ( 2 ) end_POSTSUPERSCRIPT , … by setting x(t+1)≈X⁢f⁢(x(t))superscript𝑥𝑡1𝑋𝑓superscript𝑥𝑡x^{(t+1)}\approx Xf(x^{(t)})italic_x start_POSTSUPERSCRIPT ( italic_t + 1 ) end_POSTSUPERSCRIPT ≈ italic_X italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT ),111The ≈\approx≈ relation hides a lower-order additive term, the “Onsager correction,” which depends on x(t)superscript𝑥𝑡x^{(t)}italic_x start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT. For the sake of simplicity we ignore this in the present discussion. where the “denoiser” f𝑓fitalic_f is a function (of the algorithm designers’ choosing) from ℝ→ℝ→ℝℝ\mathbb{R}\to\mathbb{R}blackboard_R → blackboard_R applied coordinate-wise. The goal of the “powering” action, X⁢x(t)𝑋superscript𝑥𝑡Xx^{(t)}italic_X italic_x start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT, is to increase the quadratic form, while the denoiser f𝑓fitalic_f is chosen to bring f⁢(x(t))𝑓superscript𝑥𝑡f(x^{(t)})italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT ) close to the constraint set K𝐾Kitalic_K. AMP algorithms are extremely popular in high-dimensional statistics. In this context, given a prior distribution over the matrix X𝑋Xitalic_X, it is often possible to optimize the design of the denoisers f𝑓fitalic_f in such a way that AMP gives an FPTAS, in that x(t)superscript𝑥𝑡x^{(t)}italic_x start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT obtains an (1−ε)1𝜀(1-\varepsilon)( 1 - italic_ε )-optimal solution for t𝑡titalic_t large enough as a function of ε𝜀\varepsilonitalic_ε. Introduced initially as a generalization of Belief Propagation methods from statistical physics [Bol14, DMM09, BM11], AMP algorithms are now state-of-the-art for a variety of average-case optimization problems, including compressed sensing [DMM09], sparse Principal Components Analysis (PCA) [DM14], linear regression [DMM09, BM11, KMS+12], non-negative PCA [MR15], and more (many additional examples may be found in the surveys [Mon12, FVRS22]). One especially notable recent application is the breakthrough work of Montanari for optimizing the Sherrington-Kirkpatrick Hamiltonian, an average-case version of max-cut [Mon21]. One major drawback of AMP algorithms is that they are not robust. The NP-hardness of quadratic optimization means that, obviously, one cannot hope for the optimality of AMP on average-case inputs to generalize to arbitrary inputs X𝑋Xitalic_X. But even structured perturbations can throw AMP off [CZK14, RSFS19]; for example, an additive perturbation to X𝑋Xitalic_X by a rank-1111 matrix of large norm, or planting a principal minor of uniform sign (as described in [IS24]). Our prior work addressing this issue [IS24] shows that for a certain class of adversarial corruptions, AMP can be simulated robustly by polynomial-sized semidefinite programming relaxations in the “local statistics hierarchy.” While this result is a proof of concept that a robust version of AMP is possible, it is perhaps more interesting from a complexity-theoretic perspective than an algorithmic one: the semidefinite programs are of size nexp⁡(t)superscript𝑛𝑡n^{\exp(t)}italic_n start_POSTSUPERSCRIPT roman_exp ( italic_t ) end_POSTSUPERSCRIPT, where t𝑡titalic_t is the number of AMP iterations. When AMP is an FPTAS, the algorithm of [IS24] gives a robust PTAS, but the running time is too slow to feasibly implement on any computer. In the present work, we obtain simple and fast spectral algorithms which run in time O⁢(n3)𝑂superscript𝑛3O(n^{3})italic_O ( italic_n start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ), while not just matching but even improving on the robustness guarantees of [IS24]. In the “spectral algorithms from sum-of-squares analyses” line of work (initiated in [HSSS16]), our result stands out as giving a particularly dramatic reduction in running time, as well as in yielding a significantly simpler analysis. 1.1 Setup and definitions We give some necessary definitions of AMP and the noise model that we consider. Definition 1.1 (AMP algorithm). An Approximate Message Passing algorithm is specified by a sequence of denoiser functions ℱ=f0,f1,f2,…ℱsubscript𝑓0subscript𝑓1subscript𝑓2…\mathcal{F}=f_{0},f_{1},f_{2},\ldotscaligraphic_F = italic_f start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , …, with ft:ℝt+1→ℝ:subscript𝑓𝑡→superscriptℝ𝑡1ℝf_{t}:\mathbb{R}^{t+1}\to\mathbb{R}italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT : blackboard_R start_POSTSUPERSCRIPT italic_t + 1 end_POSTSUPERSCRIPT → blackboard_R for each t∈ℕ𝑡ℕt\in\mathbb{N}italic_t ∈ blackboard_N. It takes as input a symmetric n×n𝑛𝑛n\times nitalic_n × italic_n matrix X𝑋Xitalic_X, a number of iterations T∈ℕ𝑇ℕT\in\mathbb{N}italic_T ∈ blackboard_N, and produces a sequence of iterates x(0),x(1),…,x(T)superscript𝑥0superscript𝑥1…superscript𝑥𝑇x^{(0)},x^{(1)},\ldots,x^{(T)}italic_x start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT , italic_x start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT , … , italic_x start_POSTSUPERSCRIPT ( italic_T ) end_POSTSUPERSCRIPT, with x(0)=1→superscript𝑥0→1x^{(0)}=\vec{1}italic_x start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT = over→ start_ARG 1 end_ARG and x(t+1)=X⁢ft⁢(x(t),x(t−1),…,x(0))−Δt⁢(x(t),x(t−1),…,x(0)),superscript𝑥𝑡1𝑋subscript𝑓𝑡superscript𝑥𝑡superscript𝑥𝑡1…superscript𝑥0subscriptΔ𝑡superscript𝑥𝑡superscript𝑥𝑡1…superscript𝑥0x^{(t+1)}=Xf_{t}(x^{(t)},x^{(t-1)},\ldots,x^{(0)})-\Delta_{t}(x^{(t)},x^{(t-1)% },\ldots,x^{(0)}),italic_x start_POSTSUPERSCRIPT ( italic_t + 1 ) end_POSTSUPERSCRIPT = italic_X italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_x start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT , italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT , … , italic_x start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) - roman_Δ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_x start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT , italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT , … , italic_x start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) , where ftsubscript𝑓𝑡f_{t}italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is applied coordinate-wise, and ΔtsubscriptΔ𝑡\Delta_{t}roman_Δ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is the Onsager correction term for decreasing correlations between iterates and is fully determined by ℱℱ\mathcal{F}caligraphic_F (see Definition 2.1). AMP algorithms often also come with a rounding procedure which is applied to the final iterate, in order to ensure it satisfies the optimization constraints. We note that we are considering separable AMP algorithms (where the denoisers are applied coordinate-wise) with fixed starting point x(0)=1→superscript𝑥0→1x^{(0)}=\vec{1}italic_x start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT = over→ start_ARG 1 end_ARG. In full generality AMP may relax both of these criteria, but the majority of AMP analyses are compatible with these assumptions. Example 1.2 (non-negative PCA). In the non-negative principal components analysis (PCA) problem, one is given a matrix X∈ℝn×n𝑋superscriptℝ𝑛𝑛X\in\mathbb{R}^{n\times n}italic_X ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_n end_POSTSUPERSCRIPT and asked to maximize v⊤⁢X⁢vsuperscript𝑣top𝑋𝑣v^{\top}Xvitalic_v start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT italic_X italic_v over non-negative unit vectors v⩾0𝑣0v\geqslant 0italic_v ⩾ 0. The AMP algorithm which starts from x(0)=1→superscript𝑥0→1x^{(0)}=\vec{1}italic_x start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT = over→ start_ARG 1 end_ARG and uniformly chooses the separable denoiser fs⁢(x(s),…,x(0))=f⁢(x(s))subscript𝑓𝑠superscript𝑥𝑠…superscript𝑥0𝑓superscript𝑥𝑠f_{s}(x^{(s)},\ldots,x^{(0)})=f(x^{(s)})italic_f start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ( italic_x start_POSTSUPERSCRIPT ( italic_s ) end_POSTSUPERSCRIPT , … , italic_x start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) = italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_s ) end_POSTSUPERSCRIPT ), with f⁢(x)=max⁡(x,0)𝑓𝑥𝑥0f(x)=\max(x,0)italic_f ( italic_x ) = roman_max ( italic_x , 0 ), is an FPTAS for non-negative PCA on X𝑋Xitalic_X with i.i.d. subgaussian entries [MR15].222Technically x(t)superscript𝑥𝑡x^{(t)}italic_x start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT may not be a unit vector nor non-negative, but AMP algorithms such as this one usually include a final “rounding” step—in this case, the rounding is just applying f⁢(x)=max⁡(x,0)𝑓𝑥𝑥0f(x)=\max(x,0)italic_f ( italic_x ) = roman_max ( italic_x , 0 ) followed by projection to the unit ball. In this case, up to the Onsager correction, AMP coincides with projected gradient ascent with “infinite” step size. We will allow adversarially-chosen perturbations in the following model. Definition 1.3 (ε𝜀\varepsilonitalic_ε-principal minor corruption). Given matrices X,Y∈ℝn×n𝑋𝑌superscriptℝ𝑛𝑛X,Y\in\mathbb{R}^{n\times n}italic_X , italic_Y ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_n end_POSTSUPERSCRIPT, we say Y𝑌Yitalic_Y is an ε𝜀\varepsilonitalic_ε-principal minor corruption of X𝑋Xitalic_X if Y−X𝑌𝑋Y-Xitalic_Y - italic_X is supported on an ε⁢n×ε⁢n𝜀𝑛𝜀𝑛\varepsilon n\times\varepsilon nitalic_ε italic_n × italic_ε italic_n-principal minor. A mean-00 random variable 𝑿𝑿\bm{X}bold_italic_X is said to be σ𝜎\sigmaitalic_σ-subgaussian if for each integer k∈ℕ𝑘ℕk\in\mathbb{N}italic_k ∈ blackboard_N, 𝐄[|𝑿|k]⩽σk⁢kk/2𝐄superscript𝑿𝑘superscript𝜎𝑘superscript𝑘𝑘2\operatorname*{\mathbf{E}}[|\bm{X}|^{k}]\leqslant\sigma^{k}k^{k/2}bold_E [ | bold_italic_X | start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ] ⩽ italic_σ start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_k start_POSTSUPERSCRIPT italic_k / 2 end_POSTSUPERSCRIPT. For example, a mean-00 Gaussian with variance σ2superscript𝜎2\sigma^{2}italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT is σ𝜎\sigmaitalic_σ-subgaussian, and a uniformly random sign ∈{±1}absentplus-or-minus1\in\{\pm 1\}∈ { ± 1 } is 1111-subgaussian. Note that rescaling a σ𝜎\sigmaitalic_σ-subgaussian variable 𝑿𝑿\bm{X}bold_italic_X to C⁢𝑿𝐶𝑿C\bm{X}italic_C bold_italic_X for constant C𝐶Citalic_C rescales the subgaussian parameter to C⁢σ𝐶𝜎C\sigmaitalic_C italic_σ. 1.2 Results Our main theorem is the following. Theorem 1.4 (Informal version of Theorem 3.1). Suppose 𝒜𝒜\mathcal{A}caligraphic_A is a T𝑇Titalic_T-step AMP algorithm with O⁢(1)𝑂1O(1)italic_O ( 1 )-Lipschitz or polynomial denoiser functions. Let X𝑋Xitalic_X be a symmetric n×n𝑛𝑛n\times nitalic_n × italic_n matrix with i.i.d. O⁢(1)n𝑂1𝑛\frac{O(1)}{\sqrt{n}}divide start_ARG italic_O ( 1 ) end_ARG start_ARG square-root start_ARG italic_n end_ARG end_ARG-subgaussian entries having mean 00 and variance 1n1𝑛\frac{1}{n}divide start_ARG 1 end_ARG start_ARG italic_n end_ARG, and let vAMP⁢(X)subscript𝑣AMP𝑋v_{\mathrm{AMP}}(X)italic_v start_POSTSUBSCRIPT roman_AMP end_POSTSUBSCRIPT ( italic_X ) be the output of 𝒜𝒜\mathcal{A}caligraphic_A on X𝑋Xitalic_X. Then there exists an algorithm which when given access to an ε𝜀\varepsilonitalic_ε-principal minor corruption Y𝑌Yitalic_Y produces in time O⁢(ε⁢n3⁢log⁡n)𝑂𝜀superscript𝑛3𝑛O(\varepsilon n^{3}\log n)italic_O ( italic_ε italic_n start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT roman_log italic_n ) a vector v^⁢(Y)^𝑣𝑌\hat{v}(Y)over^ start_ARG italic_v end_ARG ( italic_Y ) satisfying ‖v^⁢(Y)−vAMP⁢(X)‖2⩽O⁢(ε⁢logd⁡1ε)⋅‖vAMP⁢(X)‖2,superscriptnorm^𝑣𝑌subscript𝑣AMP𝑋2⋅𝑂𝜀superscript𝑑1𝜀superscriptnormsubscript𝑣AMP𝑋2\|\hat{v}(Y)-v_{\mathrm{AMP}}(X)\|^{2}\leqslant O(\varepsilon\log^{d}\tfrac{1}% {\varepsilon})\cdot\|v_{\mathrm{AMP}}(X)\|^{2},∥ over^ start_ARG italic_v end_ARG ( italic_Y ) - italic_v start_POSTSUBSCRIPT roman_AMP end_POSTSUBSCRIPT ( italic_X ) ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ⩽ italic_O ( italic_ε roman_log start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT divide start_ARG 1 end_ARG start_ARG italic_ε end_ARG ) ⋅ ∥ italic_v start_POSTSUBSCRIPT roman_AMP end_POSTSUBSCRIPT ( italic_X ) ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , with probability 1−o⁢(1)1𝑜11-o(1)1 - italic_o ( 1 ) over the randomness of X𝑋Xitalic_X, where d=1𝑑1d=1italic_d = 1 if the denoisers are Lipschitz, and d=kT𝑑superscript𝑘𝑇d=k^{T}italic_d = italic_k start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT if the denoisers are degree ⩽kabsent𝑘\leqslant k⩽ italic_k polynomials. In words, given access to an adversarially corrupted matrix Y𝑌Yitalic_Y, our algorithm can find a vector v^⁢(Y)^𝑣𝑌\hat{v}(Y)over^ start_ARG italic_v end_ARG ( italic_Y ) which is close to the output of AMP on the uncorrupted matrix X𝑋Xitalic_X.333Since X𝑋Xitalic_X has bounded operator norm, this implies that v^⁢(Y)^𝑣𝑌\hat{v}(Y)over^ start_ARG italic_v end_ARG ( italic_Y ) has objective value v^⊤⁢X⁢v^superscript^𝑣top𝑋^𝑣\hat{v}^{\top}X\hat{v}over^ start_ARG italic_v end_ARG start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT italic_X over^ start_ARG italic_v end_ARG within an additive O~⁢(ε)~𝑂𝜀\tilde{O}(\sqrt{\varepsilon})over~ start_ARG italic_O end_ARG ( square-root start_ARG italic_ε end_ARG ) of the objective of vAMP⁢(X)subscript𝑣AMP𝑋v_{\mathrm{AMP}}(X)italic_v start_POSTSUBSCRIPT roman_AMP end_POSTSUBSCRIPT ( italic_X ). The result improves on that of [IS24] in that it (1) runs in time O⁢(ε⁢n3⁢log⁡n)𝑂𝜀superscript𝑛3𝑛O(\varepsilon n^{3}\log n)italic_O ( italic_ε italic_n start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT roman_log italic_n ) rather than nexp⁡(T)superscript𝑛𝑇n^{\exp(T)}italic_n start_POSTSUPERSCRIPT roman_exp ( italic_T ) end_POSTSUPERSCRIPT, and (2) guarantees that ‖𝒜⁢(X)−v^⁢(Y)‖⩽f⁢(ε)⁢‖𝒜⁢(X)‖norm𝒜𝑋^𝑣𝑌𝑓𝜀norm𝒜𝑋\|\mathcal{A}(X)-\hat{v}(Y)\|\leqslant f(\varepsilon)\|\mathcal{A}(X)\|∥ caligraphic_A ( italic_X ) - over^ start_ARG italic_v end_ARG ( italic_Y ) ∥ ⩽ italic_f ( italic_ε ) ∥ caligraphic_A ( italic_X ) ∥ for a function f⁢(ε)→ε0subscript→𝜀𝑓𝜀0f(\varepsilon)\to_{\varepsilon}0italic_f ( italic_ε ) → start_POSTSUBSCRIPT italic_ε end_POSTSUBSCRIPT 0 which is independent of n𝑛nitalic_n (but does depend on T𝑇Titalic_T), whereas in [IS24] the function f⁢(ε)𝑓𝜀f(\varepsilon)italic_f ( italic_ε ) included a multiplicative factor of poly⁢log⁡(n)poly𝑛\mathrm{poly}\log(n)roman_poly roman_log ( italic_n ), and thus was trivial unless ε=o⁢(1)𝜀𝑜1\varepsilon=o(1)italic_ε = italic_o ( 1 ). As noted in [IS24], an equivalent result is information-theoretically impossible under the stronger corruption model in which X−Y𝑋𝑌X-Yitalic_X - italic_Y is supported on ε⁢n2𝜀superscript𝑛2\varepsilon n^{2}italic_ε italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT arbitrary entries (unless ε=o⁢(n−1/2)𝜀𝑜superscript𝑛12\varepsilon=o(n^{-1/2})italic_ε = italic_o ( italic_n start_POSTSUPERSCRIPT - 1 / 2 end_POSTSUPERSCRIPT )). As a direct corollary, we can robustly simulate Montanari’s algorithm [Mon21] for finding the ground state of the Sherrington-Kirkpatrick Hamiltonian—that is, an approximately optimal solution for Max-Cut with i.i.d. Gaussian edge weights. Corollary 1.5 (Fast, robust Sherrington Kirkpatrick). Suppose X𝑋Xitalic_X is a symmetric matrix with entries sampled i.i.d. from 𝒩⁢(0,1n)𝒩01𝑛\mathcal{N}(0,\frac{1}{n})caligraphic_N ( 0 , divide start_ARG 1 end_ARG start_ARG italic_n end_ARG ). Then there is an algorithm which when run on an ε𝜀\varepsilonitalic_ε-principal minor corruption Y𝑌Yitalic_Y of X𝑋Xitalic_X, with probability 1−o⁢(1)1𝑜11-o(1)1 - italic_o ( 1 ) produces in time O⁢(ε⁢n3⁢log⁡n)𝑂𝜀superscript𝑛3𝑛O(\varepsilon n^{3}\log n)italic_O ( italic_ε italic_n start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT roman_log italic_n ) a unit vector v^⁢(Y)∈{±1/n}n^𝑣𝑌superscriptplus-or-minus1𝑛𝑛\hat{v}(Y)\in\{\pm 1/\sqrt{n}\}^{n}over^ start_ARG italic_v end_ARG ( italic_Y ) ∈ { ± 1 / square-root start_ARG italic_n end_ARG } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT achieving objective value v^⁢(Y)⊤⁢X⁢v^⁢(Y)⩾OBJAMP−O⁢(ε⁢log⁡1ε)^𝑣superscript𝑌top𝑋^𝑣𝑌subscriptOBJAMP𝑂𝜀1𝜀\hat{v}(Y)^{\top}X\hat{v}(Y)\geqslant\mathrm{OBJ_{AMP}}-O(\sqrt{\varepsilon% \log\frac{1}{\varepsilon}})over^ start_ARG italic_v end_ARG ( italic_Y ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT italic_X over^ start_ARG italic_v end_ARG ( italic_Y ) ⩾ roman_OBJ start_POSTSUBSCRIPT roman_AMP end_POSTSUBSCRIPT - italic_O ( square-root start_ARG italic_ε roman_log divide start_ARG 1 end_ARG start_ARG italic_ε end_ARG end_ARG ). The value OBJAMPsubscriptOBJAMP\mathrm{OBJ_{AMP}}roman_OBJ start_POSTSUBSCRIPT roman_AMP end_POSTSUBSCRIPT is the objective value achieved by Montanari’s AMP algorithm; modulo a widely-believed conjecture in statistical physics, OBJAMPsubscriptOBJAMP\mathrm{OBJ_{AMP}}roman_OBJ start_POSTSUBSCRIPT roman_AMP end_POSTSUBSCRIPT approaches OPT=maxv∈{±1/n⁡v⊤⁢X⁢v≈1.52\mathrm{OPT}=\max_{v\in\{\pm 1/\sqrt{n}}v^{\top}Xv\approx 1.52roman_OPT = roman_max start_POSTSUBSCRIPT italic_v ∈ { ± 1 / square-root start_ARG italic_n end_ARG end_POSTSUBSCRIPT italic_v start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT italic_X italic_v ≈ 1.52 as T→∞→𝑇T\to\inftyitalic_T → ∞. The corollary follows from Theorem 1.4 because Montanari’s denoisers are Lipschitz, and the rounding scheme applied to place the final iterate in the hypercube is also Lipschitz. In Section 4, we give a simple proof (along similar lines as the proof of Theorem 1.4) that AMP is robust to adversarial perturbations of small spectral norm. This fact is folklore, but we feel our proof is quite simple and may be of interest. 1.3 Experiments Our algorithm is fast enough that it can be easily implemented and run on a laptop. We have run some experiments to demonstrate the utility of our method. We consider the non-negative PCA objective described in Example 1.2. In [MR15], it was shown that AMP with denoiser function f⁢(x)=max⁡(0,x)𝑓𝑥0𝑥f(x)=\max(0,x)italic_f ( italic_x ) = roman_max ( 0 , italic_x ) is an FPTAS for OPT=maxv⩾0,‖v‖=1⁡v⊤⁢X⁢v=2OPTsubscriptformulae-sequence𝑣0norm𝑣1superscript𝑣top𝑋𝑣2\mathrm{OPT}=\max_{v\geqslant 0,\|v\|=1}v^{\top}Xv=\sqrt{2}roman_OPT = roman_max start_POSTSUBSCRIPT italic_v ⩾ 0 , ∥ italic_v ∥ = 1 end_POSTSUBSCRIPT italic_v start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT italic_X italic_v = square-root start_ARG 2 end_ARG. In Figure 1, we show the result for n=3000,ε=0.02formulae-sequence𝑛3000𝜀0.02n=3000,\varepsilon=0.02italic_n = 3000 , italic_ε = 0.02, with the adversarial corruption given by perturbing an ε⁢n×ε⁢n𝜀𝑛𝜀𝑛\varepsilon n\times\varepsilon nitalic_ε italic_n × italic_ε italic_n principal minor by sampling two independent rank 50=56⁢ε⁢n5056𝜀𝑛50=\frac{5}{6}\varepsilon n50 = divide start_ARG 5 end_ARG start_ARG 6 end_ARG italic_ε italic_n Wishart matrices, each normalized to have expected Frobenius norm 100100100100, and adding one and subtracting the other. Without having taken pains to optimize the running time, the implementation in Python on a laptop takes less than 5 minutes. We have plotted (1) the correlation of our algorithm’s output, v^⁢(Y)^𝑣𝑌\hat{v}(Y)over^ start_ARG italic_v end_ARG ( italic_Y ), with vAMP⁢(X)subscript𝑣AMP𝑋v_{\mathrm{AMP}}(X)italic_v start_POSTSUBSCRIPT roman_AMP end_POSTSUBSCRIPT ( italic_X ), and (2) the objective value of the output for the uncorrupted matrix X𝑋Xitalic_X, v^⁢(Y)⊤⁢X⁢v^⁢(Y)^𝑣superscript𝑌top𝑋^𝑣𝑌\hat{v}(Y)^{\top}X\hat{v}(Y)over^ start_ARG italic_v end_ARG ( italic_Y ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT italic_X over^ start_ARG italic_v end_ARG ( italic_Y ), as a function of the number of iterations. For comparison, we plot in Figure 1 the performance of (a) AMP on the corrupt matrix, vAMP⁢(Y)subscript𝑣AMP𝑌v_{\mathrm{AMP}}(Y)italic_v start_POSTSUBSCRIPT roman_AMP end_POSTSUBSCRIPT ( italic_Y ), and (b) AMP on a “naive” spectral cleaning Y~~𝑌\tilde{Y}over~ start_ARG italic_Y end_ARG of Y𝑌Yitalic_Y, given by deleting all larger-than-expected eigenvalues. Our procedure performs much better than AMP on the corrupt input. Empirically, the naive cleaning performance is comparable to ours, but unlike our algorithm, the naive procedure does not come with provable guarantees for arbitrary perturbations (and we suspect the naive procedure may be succeeding due to a small-n𝑛nitalic_n effect). Figure 1: Plot of the correlation of the vector v^⁢(Y)^𝑣𝑌\hat{v}(Y)over^ start_ARG italic_v end_ARG ( italic_Y ) with the output of AMP on the “clean” matrix X𝑋Xitalic_X, and of the objective value attained by v^⁢(Y)^𝑣𝑌\hat{v}(Y)over^ start_ARG italic_v end_ARG ( italic_Y ) on the clean matrix X𝑋Xitalic_X. 1.4 Discussion We give a fast spectral algorithm for simulating AMP under adversarial principal minor corruptions. Our algorithm is an implementation of the “spectral algorithms from sum-of-squares (SoS) analyses” strategy introduced in [HSSS16]. We find it to be a particularly striking example of this strategy—not only was the running time reduced from nexp⁡(T)superscript𝑛𝑇n^{\exp(T)}italic_n start_POSTSUPERSCRIPT roman_exp ( italic_T ) end_POSTSUPERSCRIPT to O⁢(n3)𝑂superscript𝑛3O(n^{3})italic_O ( italic_n start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ), but also, the analysis very transparently mimics/distills that of [IS24] to yield a much cleaner argument. We draw a comparison to previous spectral-to-SoS analyses in robust statistics, most of which have been based on a “filtering” approach (e.g. [JLST21, DKK+19]); in the filtering algorithms, the non-SoS analysis required significant additional tools. Another fitting comparison is to recent works obtaining robust spectral algorithms for community recovery in the stochastic block model [MRW24, DdHS23, DdNS22], where it was important to have a very fine-grained understanding of the spectrum of specific matrices. In our case, we are able to get away with a much simpler analysis. Though we have improved on the result in [IS24] in terms of running time and the robustness-accuracy tradeoff, we differ from our prior work in one aspect: we require a description of the denoisers ℱℱ\mathcal{F}caligraphic_F used in the AMP algorithm 𝒜𝒜\mathcal{A}caligraphic_A, whereas the algorithm in [IS24] has access only to the low-degree moments of the joint distribution over X,𝒜⁢(X)𝑋𝒜𝑋X,\mathcal{A}(X)italic_X , caligraphic_A ( italic_X ). We find it unlikely that a fast algorithm could succeed without a description of ℱℱ\mathcal{F}caligraphic_F, but we pose this as a question nonetheless. Another question is whether our error guarantees are optimal, as a function of the number of AMP iterations T𝑇Titalic_T. In our theorem, the O~⁢(ε)~𝑂𝜀\tilde{O}(\sqrt{\varepsilon})over~ start_ARG italic_O end_ARG ( square-root start_ARG italic_ε end_ARG ) hides factors that grow with the number of AMP iterations; however our experiments (Figure 1) seem to suggest that the error stabilizes—is this a small n𝑛nitalic_n effect? Or perhaps an artifact of the specific perturbation from our experiments? One clear direction for future work is making AMP robust when the input matrix X𝑋Xitalic_X has planted structure, rather than just having i.i.d. subgaussian entries. For example, AMP has been a successful algorithm for “spiked matrix models” in which X=G+λ⁢u⁢u⊤𝑋𝐺𝜆𝑢superscript𝑢topX=G+\lambda uu^{\top}italic_X = italic_G + italic_λ italic_u italic_u start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT with G𝐺Gitalic_G a Gaussian matrix and u⁢u⊤𝑢superscript𝑢topuu^{\top}italic_u italic_u start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT a rank-1 spike, the goal often being to find u𝑢uitalic_u given X𝑋Xitalic_X. In this case, it is not completely clear which noise model to study. In some cases (e.g. when u𝑢uitalic_u is sparse) a principal minor corruption could simply erase the spike u⁢u⊤𝑢superscript𝑢topuu^{\top}italic_u italic_u start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT. However, it is an interesting question whether our techniques can be extended to this case—currently, our algorithm incorporates information about i.i.d. subgaussian variables, which makes it inappropriate for planted models (the same is true of [IS24]). Finally, it is interesting to consider alternative corruption models. The principal minor corruption is tractable to study, and the fact that it is adversarial makes it a powerful model. We know from [IS24] that a similar result is information-theoretically impossible under the strongest sparse adversarial corruption model, in which an arbitrary subset of ε⁢n2𝜀superscript𝑛2\varepsilon n^{2}italic_ε italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT entries is perturbed. However, it would be interesting to consider alternative corruption models that more faithfully model the distribution shift one expects to see in practice, for example in the application of compressed sensing. 1.5 Technical overview Though the proof of Theorem 1.4 is not long, we briefly summarize the main ideas here. For the sake of simplicity, in this technical overview we pretend that the AMP iteration has the form x(t)=X⁢f⁢(x(t−1))superscript𝑥𝑡𝑋𝑓superscript𝑥𝑡1x^{(t)}=Xf(x^{(t-1)})italic_x start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT = italic_X italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ), ignoring the Onsager correction and the dependence on more than one prior iterate. Recall that we are given an ε𝜀\varepsilonitalic_ε-principal minor corruption Y𝑌Yitalic_Y of X𝑋Xitalic_X. The fact that X𝑋Xitalic_X has i.i.d. subgaussian entries of variance 1n1𝑛\frac{1}{n}divide start_ARG 1 end_ARG start_ARG italic_n end_ARG implies that with high probability, ‖X‖𝗈𝗉=O⁢(1)subscriptnorm𝑋𝗈𝗉𝑂1\|X\|_{\operatorname{\mathsf{op}}}=O(1)∥ italic_X ∥ start_POSTSUBSCRIPT sansserif_op end_POSTSUBSCRIPT = italic_O ( 1 ). The first step of our algorithm is a spectral procedure which removes O⁢(ε⁢n)𝑂𝜀𝑛O(\varepsilon n)italic_O ( italic_ε italic_n ) rows and columns of Y𝑌Yitalic_Y, producing a matrix Y^^𝑌\hat{Y}over^ start_ARG italic_Y end_ARG with ‖Y^‖𝗈𝗉=O⁢(1)subscriptnorm^𝑌𝗈𝗉𝑂1\|\hat{Y}\|_{\operatorname{\mathsf{op}}}=O(1)∥ over^ start_ARG italic_Y end_ARG ∥ start_POSTSUBSCRIPT sansserif_op end_POSTSUBSCRIPT = italic_O ( 1 ). Then, we run a modified version of the AMP algorithm on the cleaned input matrix Y^^𝑌\hat{Y}over^ start_ARG italic_Y end_ARG, producing iterates y(1),y(2),…superscript𝑦1superscript𝑦2…y^{(1)},y^{(2)},\ldotsitalic_y start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT , italic_y start_POSTSUPERSCRIPT ( 2 ) end_POSTSUPERSCRIPT , … just as the original AMP algorithm would have, except that at each iteration we clip the entries y(t)=Y^⁢f⁢(𝖼𝗅𝗂𝗉⁡(y(t−1)))superscript𝑦𝑡^𝑌𝑓𝖼𝗅𝗂𝗉superscript𝑦𝑡1y^{(t)}=\hat{Y}f(\operatorname{\mathsf{clip}}(y^{(t-1)}))italic_y start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT = over^ start_ARG italic_Y end_ARG italic_f ( sansserif_clip ( italic_y start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) ), so that the magnitude of all entries of 𝖼𝗅𝗂𝗉⁡(y(t−1))𝖼𝗅𝗂𝗉superscript𝑦𝑡1\operatorname{\mathsf{clip}}(y^{(t-1)})sansserif_clip ( italic_y start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) does not exceed the ε𝜀\varepsilonitalic_ε-quantile444In the proof we choose the threshold to not exactly correspond to the ε𝜀\varepsilonitalic_ε-quantile, but this choice would have also worked and is simpler for the sake of this overview. value O⁢(poly⁢log⁡1ε)𝑂poly1𝜀O(\mathrm{poly}\log\frac{1}{\varepsilon})italic_O ( roman_poly roman_log divide start_ARG 1 end_ARG start_ARG italic_ε end_ARG ) of the entries in a typical iterate x(t−1)superscript𝑥𝑡1x^{(t-1)}italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT from a clean input matrix. We argue that ‖y(t)−x(t)‖⩽O~⁢(ε)⁢‖x(t)‖normsuperscript𝑦𝑡superscript𝑥𝑡~𝑂𝜀normsuperscript𝑥𝑡\|y^{(t)}-x^{(t)}\|\leqslant\tilde{O}(\sqrt{\varepsilon})\|x^{(t)}\|∥ italic_y start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT - italic_x start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT ∥ ⩽ over~ start_ARG italic_O end_ARG ( square-root start_ARG italic_ε end_ARG ) ∥ italic_x start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT ∥ by induction on t𝑡titalic_t; In the base case, t=0𝑡0t=0italic_t = 0, the iterates are identical as x(t)=1→=y(t)superscript𝑥𝑡→1superscript𝑦𝑡x^{(t)}=\vec{1}=y^{(t)}italic_x start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT = over→ start_ARG 1 end_ARG = italic_y start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT. Now for t⩾1𝑡1t\geqslant 1italic_t ⩾ 1, suppose that x(t)superscript𝑥𝑡x^{(t)}italic_x start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT is the (unobserved) iterate AMP would have produced on X𝑋Xitalic_X. Then ‖y(t)−x(t)‖normsuperscript𝑦𝑡superscript𝑥𝑡\displaystyle\left\|y^{(t)}-x^{(t)}\right\|∥ italic_y start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT - italic_x start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT ∥ =‖Y^⁢f⁢(𝖼𝗅𝗂𝗉⁡(y(t−1)))−X⁢f⁢(x(t−1))‖absentnorm^𝑌𝑓𝖼𝗅𝗂𝗉superscript𝑦𝑡1𝑋𝑓superscript𝑥𝑡1\displaystyle=\left\|\hat{Y}f(\operatorname{\mathsf{clip}}(y^{(t-1)}))-Xf(x^{(% t-1)})\right\|= ∥ over^ start_ARG italic_Y end_ARG italic_f ( sansserif_clip ( italic_y start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) ) - italic_X italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) ∥ ⩽‖Y^⁢(f⁢(𝖼𝗅𝗂𝗉⁡(y(t−1)))−f⁢(x(t−1)))‖+‖(Y^−X)⁢f⁢(x(t−1))‖absentnorm^𝑌𝑓𝖼𝗅𝗂𝗉superscript𝑦𝑡1𝑓superscript𝑥𝑡1norm^𝑌𝑋𝑓superscript𝑥𝑡1\displaystyle\leqslant\left\|\hat{Y}(f(\operatorname{\mathsf{clip}}(y^{(t-1)})% )-f(x^{(t-1)}))\right\|+\left\|(\hat{Y}-X)f(x^{(t-1)})\right\|⩽ ∥ over^ start_ARG italic_Y end_ARG ( italic_f ( sansserif_clip ( italic_y start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) ) - italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) ) ∥ + ∥ ( over^ start_ARG italic_Y end_ARG - italic_X ) italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) ∥ ⩽‖Y^‖𝗈𝗉⁢‖f⁢(𝖼𝗅𝗂𝗉⁡(y(t−1)))−f⁢(x(t−1))‖+‖(Y^−X)⁢f⁢(x(t−1))‖absentsubscriptnorm^𝑌𝗈𝗉norm𝑓𝖼𝗅𝗂𝗉superscript𝑦𝑡1𝑓superscript𝑥𝑡1norm^𝑌𝑋𝑓superscript𝑥𝑡1\displaystyle\leqslant\left\|\hat{Y}\right\|_{\operatorname{\mathsf{op}}}\left% \|f(\operatorname{\mathsf{clip}}(y^{(t-1)}))-f(x^{(t-1)})\right\|+\left\|(\hat% {Y}-X)f(x^{(t-1)})\right\|⩽ ∥ over^ start_ARG italic_Y end_ARG ∥ start_POSTSUBSCRIPT sansserif_op end_POSTSUBSCRIPT ∥ italic_f ( sansserif_clip ( italic_y start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) ) - italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) ∥ + ∥ ( over^ start_ARG italic_Y end_ARG - italic_X ) italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) ∥ (1) The spectral cleaning ensures that ‖Y‖𝗈𝗉=O⁢(1)subscriptnorm𝑌𝗈𝗉𝑂1\|Y\|_{\operatorname{\mathsf{op}}}=O(1)∥ italic_Y ∥ start_POSTSUBSCRIPT sansserif_op end_POSTSUBSCRIPT = italic_O ( 1 ). To further bound the first term in (1), consider the illustrative case of the denoiser f⁢(x)=x2𝑓𝑥superscript𝑥2f(x)=x^{2}italic_f ( italic_x ) = italic_x start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. Then for any vectors a,b𝑎𝑏a,bitalic_a , italic_b, f⁢(a)−f⁢(b)=(a+b)∘(a−b)𝑓𝑎𝑓𝑏𝑎𝑏𝑎𝑏f(a)-f(b)=(a+b)\circ(a-b)italic_f ( italic_a ) - italic_f ( italic_b ) = ( italic_a + italic_b ) ∘ ( italic_a - italic_b ), for ∘\circ∘ the entrywise product. Thus we have ‖f⁢(𝖼𝗅𝗂𝗉⁡(y(t−1)))−f⁢(x(t−1))‖norm𝑓𝖼𝗅𝗂𝗉superscript𝑦𝑡1𝑓superscript𝑥𝑡1\displaystyle\left\|f(\operatorname{\mathsf{clip}}(y^{(t-1)}))-f(x^{(t-1)})\right\|∥ italic_f ( sansserif_clip ( italic_y start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) ) - italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) ∥ =‖(𝖼𝗅𝗂𝗉⁡(y(t−1))+x(t−1))∘(𝖼𝗅𝗂𝗉⁡(y(t−1))−x(t−1))‖absentnorm𝖼𝗅𝗂𝗉superscript𝑦𝑡1superscript𝑥𝑡1𝖼𝗅𝗂𝗉superscript𝑦𝑡1superscript𝑥𝑡1\displaystyle=\left\|(\operatorname{\mathsf{clip}}(y^{(t-1)})+x^{(t-1)})\circ(% \operatorname{\mathsf{clip}}(y^{(t-1)})-x^{(t-1)})\right\|= ∥ ( sansserif_clip ( italic_y start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) + italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) ∘ ( sansserif_clip ( italic_y start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) - italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) ∥ ⩽‖𝖼𝗅𝗂𝗉⁡(y(t−1))‖∞⋅‖𝖼𝗅𝗂𝗉⁡(y(t−1))−x(t−1)‖+‖x(t−1)∘(𝖼𝗅𝗂𝗉⁡(y(t−1))−x(t−1))‖absent⋅subscriptnorm𝖼𝗅𝗂𝗉superscript𝑦𝑡1norm𝖼𝗅𝗂𝗉superscript𝑦𝑡1superscript𝑥𝑡1normsuperscript𝑥𝑡1𝖼𝗅𝗂𝗉superscript𝑦𝑡1superscript𝑥𝑡1\displaystyle\leqslant\left\|\operatorname{\mathsf{clip}}(y^{(t-1)})\right\|_{% \infty}\cdot\left\|\operatorname{\mathsf{clip}}(y^{(t-1)})-x^{(t-1)}\right\|+% \left\|x^{(t-1)}\circ(\operatorname{\mathsf{clip}}(y^{(t-1)})-x^{(t-1)})\right\|⩽ ∥ sansserif_clip ( italic_y start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) ∥ start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT ⋅ ∥ sansserif_clip ( italic_y start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) - italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ∥ + ∥ italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ∘ ( sansserif_clip ( italic_y start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) - italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) ∥ (2) The first and second terms of (2) are bounded in a similar manner, we begin by explaining the first. Because of the clipping procedure, ‖𝖼𝗅𝗂𝗉⁡(y(t−1))‖∞=O⁢(poly⁢log⁡1ε)subscriptnorm𝖼𝗅𝗂𝗉superscript𝑦𝑡1𝑂poly1𝜀\|\operatorname{\mathsf{clip}}(y^{(t-1)})\|_{\infty}=O(\mathrm{poly}\log\frac{% 1}{\varepsilon})∥ sansserif_clip ( italic_y start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) ∥ start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT = italic_O ( roman_poly roman_log divide start_ARG 1 end_ARG start_ARG italic_ε end_ARG ). Further, by the triangle inequality, ‖𝖼𝗅𝗂𝗉⁡(y(t−1))−x(t−1)‖⩽‖𝖼𝗅𝗂𝗉⁡(y(t−1))−𝖼𝗅𝗂𝗉⁡(x(t−1))‖+‖𝖼𝗅𝗂𝗉⁡(x(t−1))−x(t−1)‖.norm𝖼𝗅𝗂𝗉superscript𝑦𝑡1superscript𝑥𝑡1norm𝖼𝗅𝗂𝗉superscript𝑦𝑡1𝖼𝗅𝗂𝗉superscript𝑥𝑡1norm𝖼𝗅𝗂𝗉superscript𝑥𝑡1superscript𝑥𝑡1\|\operatorname{\mathsf{clip}}(y^{(t-1)})-x^{(t-1)}\|\leqslant\|\operatorname{% \mathsf{clip}}(y^{(t-1)})-\operatorname{\mathsf{clip}}(x^{(t-1)})\|+\|% \operatorname{\mathsf{clip}}(x^{(t-1)})-x^{(t-1)}\|.∥ sansserif_clip ( italic_y start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) - italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ∥ ⩽ ∥ sansserif_clip ( italic_y start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) - sansserif_clip ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) ∥ + ∥ sansserif_clip ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) - italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ∥ . (3) The first term on the right of (3) can be bounded by O~⁢(ε)⋅‖x(t−1)‖⋅~𝑂𝜀normsuperscript𝑥𝑡1\tilde{O}(\sqrt{\varepsilon})\cdot\|x^{(t-1)}\|over~ start_ARG italic_O end_ARG ( square-root start_ARG italic_ε end_ARG ) ⋅ ∥ italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ∥ from the inductive hypothesis, because the 𝖼𝗅𝗂𝗉𝖼𝗅𝗂𝗉\operatorname{\mathsf{clip}}sansserif_clip function is 1111-Lipschitz. The second term in (3) can be bounded by O~⁢(ε)⋅‖x(t−1)‖⋅~𝑂𝜀normsuperscript𝑥𝑡1\tilde{O}(\sqrt{\varepsilon})\cdot\|x^{(t-1)}\|over~ start_ARG italic_O end_ARG ( square-root start_ARG italic_ε end_ARG ) ⋅ ∥ italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ∥, because the distribution of x(t−1)superscript𝑥𝑡1x^{(t-1)}italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT’s entries is known, and is roughly that of independent polynomials in Gaussian random variables. To bound the second term from (2), we separate the contribution of the entries of x(t−1)superscript𝑥𝑡1x^{(t-1)}italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT which are bounded by O⁢(poly⁢log⁡1ε)𝑂poly1𝜀O(\mathrm{poly}\log\frac{1}{\varepsilon})italic_O ( roman_poly roman_log divide start_ARG 1 end_ARG start_ARG italic_ε end_ARG ), to which we can apply an identical argument, and the entries which exceed this threshold, and then appeal to the fact that these integrate to a small total. A similar argument can be used for arbitrary polynomial f𝑓fitalic_f (for Lipschitz f𝑓fitalic_f, (1) can be bounded directly and the clipping is not necessary). To bound the second term in (1), we use the fact that Y^−X^𝑌𝑋\hat{Y}-Xover^ start_ARG italic_Y end_ARG - italic_X can be written as the sum of a matrix E𝐸Eitalic_E, supported on an ε⁢n×ε⁢n𝜀𝑛𝜀𝑛\varepsilon n\times\varepsilon nitalic_ε italic_n × italic_ε italic_n principal minor, and a matrix F𝐹Fitalic_F which is equal to the support of −X𝑋-X- italic_X on at most O⁢(ε⁢n)𝑂𝜀𝑛O(\varepsilon n)italic_O ( italic_ε italic_n ) rows/columns—these are precisely the rows/columns of Y𝑌Yitalic_Y which were removed to form Y^^𝑌\hat{Y}over^ start_ARG italic_Y end_ARG, but were not involved in the initial principal minor corruption. So, ‖(Y^−X)⁢f⁢(x(t−1))‖⩽‖E⁢f⁢(x(t−1))‖+‖F⁢f⁢(x(t−1))‖norm^𝑌𝑋𝑓superscript𝑥𝑡1norm𝐸𝑓superscript𝑥𝑡1norm𝐹𝑓superscript𝑥𝑡1\|(\hat{Y}-X)f(x^{(t-1)})\|\leqslant\|Ef(x^{(t-1)})\|+\|Ff(x^{(t-1)})\|∥ ( over^ start_ARG italic_Y end_ARG - italic_X ) italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) ∥ ⩽ ∥ italic_E italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) ∥ + ∥ italic_F italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) ∥. Since E𝐸Eitalic_E is supported on ε⁢n𝜀𝑛\varepsilon nitalic_ε italic_n columns, ‖E⁢f⁢(x(t−1))‖⩽‖E‖𝗈𝗉⋅maxI⊂[n],|I|=ε⁢n⁢∑i∈If⁢(x(t−1))i2.norm𝐸𝑓superscript𝑥𝑡1⋅subscriptnorm𝐸𝗈𝗉subscriptformulae-sequence𝐼delimited-[]𝑛𝐼𝜀𝑛subscript𝑖𝐼𝑓superscriptsubscriptsuperscript𝑥𝑡1𝑖2\|Ef(x^{(t-1)})\|\leqslant\|E\|_{\operatorname{\mathsf{op}}}\cdot\max_{I% \subset[n],|I|=\varepsilon n}\sum_{i\in I}f(x^{(t-1)})_{i}^{2}.∥ italic_E italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) ∥ ⩽ ∥ italic_E ∥ start_POSTSUBSCRIPT sansserif_op end_POSTSUBSCRIPT ⋅ roman_max start_POSTSUBSCRIPT italic_I ⊂ [ italic_n ] , | italic_I | = italic_ε italic_n end_POSTSUBSCRIPT ∑ start_POSTSUBSCRIPT italic_i ∈ italic_I end_POSTSUBSCRIPT italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . Here again, because we know the order statistics of x(t−1)superscript𝑥𝑡1x^{(t-1)}italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT, and because f𝑓fitalic_f is required to be a well-behaved function, the maximum norm of f⁢(x(t−1))𝑓superscript𝑥𝑡1f(x^{(t-1)})italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) when restricted to a subset of ε⁢n𝜀𝑛\varepsilon nitalic_ε italic_n coordinates is on the order of O~⁢(ε)⁢‖x(t−1)‖~𝑂𝜀normsuperscript𝑥𝑡1\tilde{O}(\sqrt{\varepsilon})\|x^{(t-1)}\|over~ start_ARG italic_O end_ARG ( square-root start_ARG italic_ε end_ARG ) ∥ italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ∥. Also, since E𝐸Eitalic_E is a submatrix of Y^−X^𝑌𝑋\hat{Y}-Xover^ start_ARG italic_Y end_ARG - italic_X, ‖E‖𝗈𝗉⩽‖Y^‖𝗈𝗉+‖X‖𝗈𝗉⩽12subscriptnorm𝐸𝗈𝗉subscriptnorm^𝑌𝗈𝗉subscriptnorm𝑋𝗈𝗉12\|E\|_{\operatorname{\mathsf{op}}}\leqslant\|\hat{Y}\|_{\operatorname{\mathsf{% op}}}+\|X\|_{\operatorname{\mathsf{op}}}\leqslant 12∥ italic_E ∥ start_POSTSUBSCRIPT sansserif_op end_POSTSUBSCRIPT ⩽ ∥ over^ start_ARG italic_Y end_ARG ∥ start_POSTSUBSCRIPT sansserif_op end_POSTSUBSCRIPT + ∥ italic_X ∥ start_POSTSUBSCRIPT sansserif_op end_POSTSUBSCRIPT ⩽ 12. The matrix F𝐹Fitalic_F can be split into the part F1subscript𝐹1F_{1}italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT supported on O⁢(ε⁢n)𝑂𝜀𝑛O(\varepsilon n)italic_O ( italic_ε italic_n ) columns, for which the argument is identical to the case of Ef(x(t−1)Ef(x^{(t-1)}italic_E italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT above. But there is also a part F2subscript𝐹2F_{2}italic_F start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT supported on O⁢(ε⁢n)𝑂𝜀𝑛O(\varepsilon n)italic_O ( italic_ε italic_n ) rows. Here, we have to take a different perspective: since F2subscript𝐹2F_{2}italic_F start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT is a restriction of −X𝑋-X- italic_X to the rows indexed by some set T⊂[n]𝑇delimited-[]𝑛T\subset[n]italic_T ⊂ [ italic_n ] with |T|=ε⁢n𝑇𝜀𝑛|T|=\varepsilon n| italic_T | = italic_ε italic_n, we have that F2⁢f⁢(x(t−1))=(−X⁢f⁢(x(t−1)))Tsubscript𝐹2𝑓superscript𝑥𝑡1subscript𝑋𝑓superscript𝑥𝑡1𝑇F_{2}f(x^{(t-1)})=(-Xf(x^{(t-1)}))_{T}italic_F start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) = ( - italic_X italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) ) start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT, which is an ε⁢n𝜀𝑛\varepsilon nitalic_ε italic_n-sparse subset of the vector −Xf(x(t−1)-Xf(x^{(t-1)}- italic_X italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT. But we understand the order statistics of this vector too! Hence we have that ‖F2⁢f⁢(x(t−1))‖=O~⁢(ε)⁢‖x(t−1)‖normsubscript𝐹2𝑓superscript𝑥𝑡1~𝑂𝜀normsuperscript𝑥𝑡1\|F_{2}f(x^{(t-1)})\|=\tilde{O}(\sqrt{\varepsilon})\|x^{(t-1)}\|∥ italic_F start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) ∥ = over~ start_ARG italic_O end_ARG ( square-root start_ARG italic_ε end_ARG ) ∥ italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ∥ as desired. Putting everything together, we have that ‖y(t)−x(t)‖⩽O~⁢(ε)⋅‖x(t−1)‖normsuperscript𝑦𝑡superscript𝑥𝑡⋅~𝑂𝜀normsuperscript𝑥𝑡1\|y^{(t)}-x^{(t)}\|\leqslant\tilde{O}(\sqrt{\varepsilon})\cdot\|x^{(t-1)}\|∥ italic_y start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT - italic_x start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT ∥ ⩽ over~ start_ARG italic_O end_ARG ( square-root start_ARG italic_ε end_ARG ) ⋅ ∥ italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ∥. The argument is now finished by again using our knowledge of the distribution of x(t−1)superscript𝑥𝑡1x^{(t-1)}italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT to conclude that ‖x(t−1)‖normsuperscript𝑥𝑡1\|x^{(t-1)}\|∥ italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ∥ and ‖x(t)‖normsuperscript𝑥𝑡\|x^{(t)}\|∥ italic_x start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT ∥ are within constant scalings of each other. Much of this analysis mirrors and simplifies the analysis in [IS24]. There, a semidefinite program is used to obtain a pseudoexpectation of a “cleaned” version X^^𝑋\hat{X}over^ start_ARG italic_X end_ARG of Y𝑌Yitalic_Y. The semidefinite program has formal variables for low-degree symmetric polynomials of X^^𝑋\hat{X}over^ start_ARG italic_X end_ARG. It adds constraints to try to enforce that ‖X^‖𝗈𝗉=O⁢(1)subscriptnorm^𝑋𝗈𝗉𝑂1\|\hat{X}\|_{\operatorname{\mathsf{op}}}=O(1)∥ over^ start_ARG italic_X end_ARG ∥ start_POSTSUBSCRIPT sansserif_op end_POSTSUBSCRIPT = italic_O ( 1 ), that X^−Y^𝑋𝑌\hat{X}-Yover^ start_ARG italic_X end_ARG - italic_Y be supported on a principal minor (by introducing indicator variables for “clean” rows and columns), as well as the constraint that some symmetric vector-valued polynomials in the entries of X^^𝑋\hat{X}over^ start_ARG italic_X end_ARG have entries which are no larger than corresponding polynomials in X𝑋Xitalic_X. The high-level sequence of arguments mirrors those outlined in (1) and the subsequent lines. We introduce some additional structure/arguments because our spectral cleaning step (for which we design a natural-in-hindsight spectral cleaning algorithm) deletes rows and columns. One advantage of the present argument over that in [IS24] is that it is unclear how to make a semidefinite program leverage the order statistics of vector-valued polynomials, so in our prior work we crudely enforce a bound on the infinity norm of the vectors, which gives rise to poly⁢log⁡npoly𝑛\mathrm{poly}\log nroman_poly roman_log italic_n factors. Here we are able to circumvent this because we clip our iterates by hand."
https://arxiv.org/html/2411.02585v1,A Linear Time Gap-ETH-Tight Approximation Scheme forTSP in the Euclidean Plane,"The Traveling Salesman Problem (TSP) in the two-dimensional Euclidean plane is among the oldest and most famous NP-hard optimization problems. In breakthrough works, Arora [J. ACM 1998] and Mitchell [SICOMP 1999] gave the first polynomial time approximation schemes. The running time of their approximation schemes was improved by Rao and Smith [STOC 1998] to (1/ε)O⁢(1/ε)⁢n⁢log⁡nsuperscript1𝜀𝑂1𝜀𝑛𝑛(1/\varepsilon)^{O(1/\varepsilon)}n\log n( 1 / italic_ε ) start_POSTSUPERSCRIPT italic_O ( 1 / italic_ε ) end_POSTSUPERSCRIPT italic_n roman_log italic_n. Bartal and Gottlieb [FOCS 2013] gave an approximation scheme of running time 2(1/ε)O⁢(1)⁢nsuperscript2superscript1𝜀𝑂1𝑛2^{(1/\varepsilon)^{O(1)}}n2 start_POSTSUPERSCRIPT ( 1 / italic_ε ) start_POSTSUPERSCRIPT italic_O ( 1 ) end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT italic_n, which is optimal in n𝑛nitalic_n. Recently, Kisfaludi-Bak, Nederlof, and Węgrzycki [FOCS 2021] gave a 2O⁢(1/ε)⁢n⁢log⁡nsuperscript2𝑂1𝜀𝑛𝑛2^{O(1/\varepsilon)}n\log n2 start_POSTSUPERSCRIPT italic_O ( 1 / italic_ε ) end_POSTSUPERSCRIPT italic_n roman_log italic_n time approximation scheme, achieving the optimal running time in ε𝜀\varepsilonitalic_ε under the Gap-ETH conjecture. In our work, we give a 2O⁢(1/ε)⁢nsuperscript2𝑂1𝜀𝑛2^{O(1/\varepsilon)}n2 start_POSTSUPERSCRIPT italic_O ( 1 / italic_ε ) end_POSTSUPERSCRIPT italic_n time approximation scheme, achieving the optimal running time both in n𝑛nitalic_n and in ε𝜀\varepsilonitalic_ε under the Gap-ETH conjecture.","The Traveling Salesman Problem (TSP) in the two-dimensional Euclidean plane is among the oldest and most famous NP-hard optimization problems. The problem is to find a shortest tour on a given set of n𝑛nitalic_n points in ℝ2superscriptℝ2\mathbb{R}^{2}blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. The Gödel-prize-winning approximation schemes for this problem due to Arora [Aro98] and Mitchell [Mit99] are among the most prominent results in approximation algorithms. They are classics in advanced algorithmic courses as well as in textbooks on approximation algorithms, optimization, and geometric algorithms [Vaz10, WS11, KV12, HP11, NS07]. The methods of Arora [Aro98] and Mitchell [Mit99] have numerous applications, see, e.g., [Aro03] for a survey. The running times of the approximation schemes in [Aro98] and [Mit99] are n⁢(log⁡n)O⁢(1/ε)𝑛superscript𝑛𝑂1𝜀n(\log n)^{O(1/\varepsilon)}italic_n ( roman_log italic_n ) start_POSTSUPERSCRIPT italic_O ( 1 / italic_ε ) end_POSTSUPERSCRIPT and nO⁢(1/ε)superscript𝑛𝑂1𝜀n^{O(1/\varepsilon)}italic_n start_POSTSUPERSCRIPT italic_O ( 1 / italic_ε ) end_POSTSUPERSCRIPT, respectively. To improve their running times, Rao and Smith [RS98] designed a (1/ε)O⁢(1/ε)⁢n⁢log⁡nsuperscript1𝜀𝑂1𝜀𝑛𝑛(1/\varepsilon)^{O(1/\varepsilon)}n\log n( 1 / italic_ε ) start_POSTSUPERSCRIPT italic_O ( 1 / italic_ε ) end_POSTSUPERSCRIPT italic_n roman_log italic_n approximation scheme using geometric spanners. Bartal and Gottlieb [BG13] gave a 2(1/ε)O⁢(1)⁢nsuperscript2superscript1𝜀𝑂1𝑛2^{(1/\varepsilon)^{O(1)}}n2 start_POSTSUPERSCRIPT ( 1 / italic_ε ) start_POSTSUPERSCRIPT italic_O ( 1 ) end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT italic_n time approximation scheme in the real-RAM model with atomic floor or mod operations. That running time is optimal in n𝑛nitalic_n, though the dependency on ε𝜀\varepsilonitalic_ε is not as good as in [RS98]. Recently, Kisfaludi-Bak, Nederlof, and Węgrzycki [KNW21] gave a 2O⁢(1/ε)⁢n⁢log⁡nsuperscript2𝑂1𝜀𝑛𝑛2^{O(1/\varepsilon)}n\log n2 start_POSTSUPERSCRIPT italic_O ( 1 / italic_ε ) end_POSTSUPERSCRIPT italic_n roman_log italic_n time approximation scheme, achieving the optimal running time in ε𝜀\varepsilonitalic_ε under the Gap-ETH conjecture, though the dependency in n𝑛nitalic_n is within a factor log⁡n𝑛\log nroman_log italic_n to the optimal. Among the above mentioned works, [Aro98, RS98, BG13, KNW21] generalize to d𝑑ditalic_d dimensions, for any fixed constant d≥2𝑑2d\geq 2italic_d ≥ 2. The main open question in the above line of works is to achieve an approximation scheme with an optimal running time both in n𝑛nitalic_n and in ε𝜀\varepsilonitalic_ε. That question is central in many contexts. For example, for planar graphs, Klein [Kle08] gave a 2O⁢(1/ε)⁢nsuperscript2𝑂1𝜀𝑛2^{O(1/\varepsilon)}n2 start_POSTSUPERSCRIPT italic_O ( 1 / italic_ε ) end_POSTSUPERSCRIPT italic_n time approximation scheme for planar TSP, and Marx [Mar07] showed that the dependence on ε𝜀\varepsilonitalic_ε in Klein’s algorithm is conditionally near-optimal. In our work, we settle the above open question for TSP in the Euclidean plane. Our main result is an approximation scheme whose running time is linear in n𝑛nitalic_n and has an optimal dependence on ε𝜀\varepsilonitalic_ε under the Gap-ETH conjecture. Theorem 1 (main theorem). There is a randomized (1+ε)1𝜀(1+\varepsilon)( 1 + italic_ε )-approximation scheme for the Euclidean TSP in ℝ2superscriptℝ2\mathbb{R}^{2}blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT that runs in time 2O⁢(1/ε)⁢nsuperscript2𝑂1𝜀𝑛2^{O(1/\varepsilon)}n2 start_POSTSUPERSCRIPT italic_O ( 1 / italic_ε ) end_POSTSUPERSCRIPT italic_n in the real-RAM model with atomic floor operations. Remark. The choice of the model of computation needs care in order to achieve an algorithm with running time linear in n𝑛nitalic_n. Indeed, Euclidean TSP requires Ω⁢(n⁢log⁡n)Ω𝑛𝑛\Omega(n\log n)roman_Ω ( italic_n roman_log italic_n ) time in the decision tree model, see, e.g., [DKS97, RS98]. Thus we use the real RAM model with an additional floor function as atomic operation. The same model was assumed in the open problem stated by Rao and Smith [RS98] regarding linear time computation and in the linear time algorithm of Bartal and Gottlieb [BG13].111Theorem I.1 of [BG13] uses the integer RAM model but requires additional assumptions. Similar to [BG13], our algorithm uses only a limited amount of the power offered by the model. The Gap-ETH hardness due to Kisfaludi-Bak, Nederlof, and Węgrzycki [KNW21] extends to that model. See Appendix A for details. 1.1 Preliminaries and Previous Techniques Let P⊆ℝ2𝑃superscriptℝ2P\subseteq\mathbb{R}^{2}italic_P ⊆ blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT denote a set of n𝑛nitalic_n points. By preprocessing the input instance, we may assume that P⊆{0,…,L}2𝑃superscript0…𝐿2P\subseteq\{0,\dots,L\}^{2}italic_P ⊆ { 0 , … , italic_L } start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT for some integer L=O⁢(n/ε)𝐿𝑂𝑛𝜀L=O(n/\varepsilon)italic_L = italic_O ( italic_n / italic_ε ) that is a power of 2. This preprocessing step can be done in O⁢(n)𝑂𝑛O(n)italic_O ( italic_n ) time [BG13]. For a segment I𝐼Iitalic_I in ℝ2superscriptℝ2\mathbb{R}^{2}blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, let length⁢(I)length𝐼\mathrm{length}(I)roman_length ( italic_I ) denote the length of I𝐼Iitalic_I. For any path π𝜋\piitalic_π of points x1,x2,…,xmsubscript𝑥1subscript𝑥2…subscript𝑥𝑚x_{1},x_{2},\dots,x_{m}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT in ℝ2superscriptℝ2\mathbb{R}^{2}blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT where m∈ℕ𝑚ℕm\in\mathbb{N}italic_m ∈ blackboard_N, define cost⁢(π)=∑i=1m−1dist⁢(xi,xi+1)cost𝜋superscriptsubscript𝑖1𝑚1distsubscript𝑥𝑖subscript𝑥𝑖1\mathrm{cost}(\pi)=\sum_{i=1}^{m-1}\mathrm{dist}(x_{i},x_{i+1})roman_cost ( italic_π ) = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m - 1 end_POSTSUPERSCRIPT roman_dist ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT ). We say that π𝜋\piitalic_π is a tour if x1=xmsubscript𝑥1subscript𝑥𝑚x_{1}=x_{m}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT. Let OPTOPT\mathrm{OPT}roman_OPT denote an optimal tour on P𝑃Pitalic_P. Let optopt\mathrm{opt}roman_opt denote cost⁢(OPT)costOPT\mathrm{cost}(\mathrm{OPT})roman_cost ( roman_OPT ). 1.1.1 Dissection and Quadtree We follow the notations in [KNW21]. Let d≥2𝑑2d\geq 2italic_d ≥ 2 be a constant. We pick a1,…,ad∈{1,…,L}subscript𝑎1…subscript𝑎𝑑1…𝐿a_{1},\dots,a_{d}\in\{1,\dots,L\}italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_a start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ∈ { 1 , … , italic_L } independently and uniformly at random and define 𝒂:=(a1,…,ad)∈{0,…,L}dassign𝒂subscript𝑎1…subscript𝑎𝑑superscript0…𝐿𝑑\bm{a}:=(a_{1},\ldots,a_{d})\in\{0,\dots,L\}^{d}bold_italic_a := ( italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_a start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) ∈ { 0 , … , italic_L } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT. Consider the hypercube C(𝒂):=×i=1d[−ai+1/2,2L−ai+1/2].C(\bm{a}):=\bigtimes\limits_{i=1}^{d}[-a_{i}+1/2,2L-a_{i}+1/2].italic_C ( bold_italic_a ) := × start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT [ - italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + 1 / 2 , 2 italic_L - italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + 1 / 2 ] . Note that C⁢(𝒂)𝐶𝒂C(\bm{a})italic_C ( bold_italic_a ) has side length 2⁢L2𝐿2L2 italic_L and each point from P𝑃Pitalic_P is contained in C⁢(𝒂)𝐶𝒂C(\bm{a})italic_C ( bold_italic_a ) since P⊆{0,…,L}d𝑃superscript0…𝐿𝑑P\subseteq\{0,\dots,L\}^{d}italic_P ⊆ { 0 , … , italic_L } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT. We define the dissection D⁢(𝒂)𝐷𝒂D(\bm{a})italic_D ( bold_italic_a ) of C⁢(𝒂)𝐶𝒂C(\bm{a})italic_C ( bold_italic_a ) to be a tree constructed recursively, where each vertex is associated with a hypercube in ℝdsuperscriptℝ𝑑\mathbb{R}^{d}blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT. The root of the tree is associated with C⁢(𝒂)𝐶𝒂C(\bm{a})italic_C ( bold_italic_a ). Each non-leaf vertex of the tree that is associated with a hypercube ×i=1d[li,ui]superscriptsubscript𝑖1𝑑absentsubscript𝑙𝑖subscript𝑢𝑖\bigtimes_{i=1}^{d}[l_{i},u_{i}]× start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT [ italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ] has 2dsuperscript2𝑑2^{d}2 start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT children with which we associate hypercubes ×i=1dIisuperscriptsubscript𝑖1𝑑absentsubscript𝐼𝑖\bigtimes_{i=1}^{d}I_{i}× start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, where Iisubscript𝐼𝑖I_{i}italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is either [li,(li+ui)/2]subscript𝑙𝑖subscript𝑙𝑖subscript𝑢𝑖2[l_{i},(l_{i}+u_{i})/2][ italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , ( italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) / 2 ] or [(li+ui)/2,ui]subscript𝑙𝑖subscript𝑢𝑖2subscript𝑢𝑖[(l_{i}+u_{i})/2,u_{i}][ ( italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) / 2 , italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ]. Each leaf vertex of the tree is associated with a hypercube of unit length. A quadtree is obtained from D⁢(𝒂)𝐷𝒂D(\bm{a})italic_D ( bold_italic_a ), except we stop the recursive partitioning as soon as the associated hypercube of a vertex contains at most one point from P𝑃Pitalic_P. Each hypercube associated with a vertex in the quadtree is called a cell. We say that a cell C𝐶Citalic_C is redundant if it has a child that contains the same set of input points as the parent of C𝐶Citalic_C. A redundant path is a maximal ancestor-descendant path in the tree whose internal vertices are redundant. The compressed quadtree is obtained from the quadtree by removing all the empty children of redundant cells and replacing the redundant paths with edges. In the resulting tree some internal cells may have a single child; we call these compressed cells. It is well-known that compressed quadtrees have O⁢(n)𝑂𝑛O(n)italic_O ( italic_n ) vertices, see, e.g., [BG13]. 1.1.2 Approach of Arora [Aro98] In the quadtree decomposition, each side of a cell has O⁢((log⁡n)/ε)𝑂𝑛𝜀O((\log n)/\varepsilon)italic_O ( ( roman_log italic_n ) / italic_ε ) equidistant portals. Lemma 2 (Arora’s Patching Lemma, [Aro98]). Let I𝐼Iitalic_I be any line segment and π𝜋\piitalic_π be a closed path that crosses I𝐼Iitalic_I at least thrice. Then there exist line segments on I𝐼Iitalic_I whose total length is at most 6⋅length⁢(I)⋅6length𝐼6\cdot\mathrm{length}(I)6 ⋅ roman_length ( italic_I ) and whose addition to π𝜋\piitalic_π changes it into a closed path π′superscript𝜋′\pi^{\prime}italic_π start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT that crosses I𝐼Iitalic_I at most twice. Using Lemma 2, Arora prove a structure theorem, which states that there is a near-optimal tour that crosses each side of a cell O⁢(1/ε)𝑂1𝜀O(1/\varepsilon)italic_O ( 1 / italic_ε ) times and only through portals. While such a near-optimal tour does not necessarily exist for a fixed quadtree, a randomly shifted quadtree works with high probability. Thus the algorithm first computes a randomly shifted quadtree, and then apply a dynamic program to compute the best solution satisfying the properties in the structure theorem. 1.1.3 Approach of Kisfaludi-Bak, Nederlof, and Węgrzycki [KNW21] To achieve Gap-ETH tight running time, Kisfaludi-Bak, Nederlof, and Węgrzycki introduce a powerful technique of the sparsity-sensitive patching: for a side of a cell that is crossed by a tour at 1<k≤O⁢(1/ε)1𝑘𝑂1𝜀1<k\leq O(1/\varepsilon)1 < italic_k ≤ italic_O ( 1 / italic_ε ) crossings, modify the tour by mapping each crossing to the nearest portal from the set of g⁢(k)𝑔𝑘g(k)italic_g ( italic_k ) equidistant portals. Here g⁢(k)=Θ⁢(1/(ε2⁢k))𝑔𝑘Θ1superscript𝜀2𝑘g(k)=\Theta(1/(\varepsilon^{2}k))italic_g ( italic_k ) = roman_Θ ( 1 / ( italic_ε start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_k ) ) is a granularity parameter that depends on k𝑘kitalic_k. Kisfaludi-Bak, Nederlof, and Węgrzycki analyze the patching cost αi⁢(x)subscript𝛼𝑖𝑥\alpha_{i}(x)italic_α start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_x ) for a crossing x𝑥xitalic_x on a line hℎhitalic_h of level i𝑖iitalic_i as follows: αi⁢(x)={pro⁢(x),if pro⁢(x)≤L2i⁢r(L2i⁢r)2⁢1pro⁢(x),if pro⁢(x)>L2i⁢r,subscript𝛼𝑖𝑥casespro𝑥if pro⁢(x)≤L2i⁢rsuperscript𝐿superscript2𝑖𝑟21pro𝑥if pro⁢(x)>L2i⁢r\alpha_{i}(x)=\begin{cases}\mathrm{pro}(x),&\text{if $\mathrm{pro}(x)\leq\frac% {L}{2^{i}r}$}\\ \left(\frac{L}{2^{i}r}\right)^{2}\frac{1}{\mathrm{pro}(x)},&\text{if $\mathrm{% pro}(x)>\frac{L}{2^{i}r}$},\end{cases}italic_α start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_x ) = { start_ROW start_CELL roman_pro ( italic_x ) , end_CELL start_CELL if roman_pro ( italic_x ) ≤ divide start_ARG italic_L end_ARG start_ARG 2 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT italic_r end_ARG end_CELL end_ROW start_ROW start_CELL ( divide start_ARG italic_L end_ARG start_ARG 2 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT italic_r end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT divide start_ARG 1 end_ARG start_ARG roman_pro ( italic_x ) end_ARG , end_CELL start_CELL if roman_pro ( italic_x ) > divide start_ARG italic_L end_ARG start_ARG 2 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT italic_r end_ARG , end_CELL end_ROW where r=O⁢(1/ε)𝑟𝑂1𝜀r=O(1/\varepsilon)italic_r = italic_O ( 1 / italic_ε ) and pro⁢(x)pro𝑥\mathrm{pro}(x)roman_pro ( italic_x ) is the distance between x𝑥xitalic_x and the previous crossing on line hℎhitalic_h. The above property on αi⁢(x)subscript𝛼𝑖𝑥\alpha_{i}(x)italic_α start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_x ) is key to show that the sparsity-sensitive patching leads to a near-optimal solution. Applying the sparsity-sensitive patching has a condition: the number of crossings k𝑘kitalic_k is such that k≠1𝑘1k\neq 1italic_k ≠ 1. This condition is necessary: when k=1𝑘1k=1italic_k = 1, Kisfaludi-Bak, Nederlof, and Węgrzycki provide an example showing that moving the single crossing to the closest portal does not lead to a near-optimal solution. To deal with single crossings, their approach is to make use of a spanner. The O⁢(n⁢log⁡n)𝑂𝑛𝑛O(n\log n)italic_O ( italic_n roman_log italic_n ) running time in [KNW21] is due to the computation of the spanner. Kisfaludi-Bak, Nederlof, and Węgrzycki raised an open question whether it is possible to improve the running time by a factor of log⁡n𝑛\log nroman_log italic_n by using some new ideas to handle single crossings. In our work, we answer their open question positively for TSP in ℝ2superscriptℝ2\mathbb{R}^{2}blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. See Section 1.2 for an overview of our approach. 1.1.4 Linear-time 2-approximation by Bartal and Gottlieb [BG13] One step in our approach requires computing a 2-approximate solution. This can be achieved in O⁢(n)𝑂𝑛O(n)italic_O ( italic_n ) time, by setting ε=1𝜀1\varepsilon=1italic_ε = 1 in the main result of Bartal and Gottlieb [BG13]: Lemma 3 (corollary of Bartal and Gottlieb [BG13]). There is randomized 2-approximation algorithm for the TSP in ℝ2superscriptℝ2\mathbb{R}^{2}blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT that runs in time O⁢(n)𝑂𝑛O(n)italic_O ( italic_n ) in the real-RAM model with atomic floor or mod operations. 1.1.5 Portals Definition 4 (grid). For a segment F𝐹Fitalic_F and an integer m𝑚mitalic_m, let grid⁢(F,m)grid𝐹𝑚\mathrm{grid}(F,m)roman_grid ( italic_F , italic_m ) denote an m𝑚mitalic_m-regular set of portals on F𝐹Fitalic_F. Note that grid⁢(F,m)grid𝐹𝑚\mathrm{grid}(F,m)roman_grid ( italic_F , italic_m ) contains the two endpoints of F𝐹Fitalic_F. The following function g⁢(k)𝑔𝑘g(k)italic_g ( italic_k ) computes the number of portals to place on a side, given the number k𝑘kitalic_k of crossing points on that side. Definition 5 (adaptation from [KNW21]). Let r𝑟ritalic_r be a positive integer. For a positive integer k𝑘kitalic_k, let q𝑞qitalic_q denote the smallest integer such that q≥r2/(4⁢k)𝑞superscript𝑟24𝑘q\geq r^{2}/(4k)italic_q ≥ italic_r start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT / ( 4 italic_k ). Let p𝑝pitalic_p denote the smallest integer power of 2 such that p≥q𝑝𝑞p\geq qitalic_p ≥ italic_q. Then we define g⁢(k):=passign𝑔𝑘𝑝g(k):=pitalic_g ( italic_k ) := italic_p. 1.2 Overview of Our Methods In this section, we give an overview of our methods in the proof of Theorem 1. Our approach is based on the approach of Kisfaludi-Bak, Nederlof, and Węgrzycki [KNW21] and uses several new ideas. To improve upon [KNW21], a main difficulty is to deal with the single crossing case, i.e., when a side has a single crossing with a tour, see Section 1.1.3. That is the reason why the running time in [KNW21] requires a factor log⁡(n)𝑛\log(n)roman_log ( italic_n ). In order to get rid of the log⁡(n)𝑛\log(n)roman_log ( italic_n ) factor, we deal with the single crossing case in a new way. We start by computing a 2-approximate solution T𝑇Titalic_T in linear time, using an algorithm of Bartal and Gottlieb [BG13], see Section 1.1.4. The solution T𝑇Titalic_T helps us in dealing with single crossings. 1.2.1 New set of portals We define a new set of portals for each side F𝐹Fitalic_F, consisting of the portals in [KNW21], plus possibly an additional point, which is a crossing between T𝑇Titalic_T and F𝐹Fitalic_F if exists.222When T𝑇Titalic_T and F𝐹Fitalic_F has multiple crossings, an arbitrary crossing would work. See Definition 6. Definition 6. Let r=O⁢(1/ε)𝑟𝑂1𝜀r=O(1/\varepsilon)italic_r = italic_O ( 1 / italic_ε ). We say that a side F𝐹Fitalic_F of a cell is T𝑇Titalic_T-crossing if T𝑇Titalic_T crosses F𝐹Fitalic_F through at least one point. If F𝐹Fitalic_F is T𝑇Titalic_T-crossing, we let xFsubscript𝑥𝐹x_{F}italic_x start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT denote an arbitrary crossing point between F𝐹Fitalic_F and T𝑇Titalic_T. For a positive integer k𝑘kitalic_k, define Z⁢(F,k):={grid⁢(F,g⁢(k))∪{xF},if F is T-crossing;grid⁢(F,g⁢(k)),otherwise,assign𝑍𝐹𝑘casesgrid𝐹𝑔𝑘subscript𝑥𝐹if F is T-crossing;grid𝐹𝑔𝑘otherwise,Z(F,k):=\begin{cases}\mathrm{grid}(F,g(k))\cup\{x_{F}\},&\text{if $F$ is $T$-% crossing;}\\ \mathrm{grid}(F,g(k)),&\text{otherwise,}\end{cases}italic_Z ( italic_F , italic_k ) := { start_ROW start_CELL roman_grid ( italic_F , italic_g ( italic_k ) ) ∪ { italic_x start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT } , end_CELL start_CELL if italic_F is italic_T -crossing; end_CELL end_ROW start_ROW start_CELL roman_grid ( italic_F , italic_g ( italic_k ) ) , end_CELL start_CELL otherwise, end_CELL end_ROW where the function g𝑔gitalic_g and gridgrid\mathrm{grid}roman_grid are defined in Section 1.1.5. 1.2.2 r𝑟ritalic_r-basic Tour We introduce a new notion of r𝑟ritalic_r-basic tour (Definition 7), which is analogous to r𝑟ritalic_r-simple tour in [KNW21]. At a high level, a tour that respects the portals in Definition 6 is called r𝑟ritalic_r-basic. For purely technical reasons, we will define \AC⁢(π,C)\AC𝜋𝐶\AC(\pi,C)( italic_π , italic_C ) to be a subset of the crossings between a tour π𝜋\piitalic_π and the boundary of a cell C𝐶Citalic_C as follows. For a tour π𝜋\piitalic_π and a cell C𝐶Citalic_C, let I0subscript𝐼0I_{0}italic_I start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT denote the set of curves obtained by restricting π𝜋\piitalic_π to the interior of C𝐶Citalic_C. Let I⊆I0𝐼subscript𝐼0I\subseteq I_{0}italic_I ⊆ italic_I start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT be the set of curves Q∈I0𝑄subscript𝐼0Q\in I_{0}italic_Q ∈ italic_I start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT such that Q𝑄Qitalic_Q visits at least one input point from P𝑃Pitalic_P. Let \AC⁢(π,C)\AC𝜋𝐶\AC(\pi,C)( italic_π , italic_C ) denote the (multi-)set of points consisting of the two endpoints of all curves in I𝐼Iitalic_I. Note that when every curve in I0subscript𝐼0I_{0}italic_I start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT visits some point from P𝑃Pitalic_P, then \AC⁢(π,C)\AC𝜋𝐶\AC(\pi,C)( italic_π , italic_C ) is simply the set of crossings between π𝜋\piitalic_π and the boundary of C𝐶Citalic_C. Definition 7 (r𝑟ritalic_r-basic). We say that a tour π𝜋\piitalic_π is r𝑟ritalic_r-basic if for every side F𝐹Fitalic_F of a cell C𝐶Citalic_C in D⁢(𝒂)𝐷𝒂D(\bm{a})italic_D ( bold_italic_a ), we have \AC⁢(π,C)∩F⊆Z⁢(F,|\AC⁢(π,C)∩F|)\AC𝜋𝐶𝐹𝑍𝐹\AC𝜋𝐶𝐹\AC(\pi,C)\cap F\subseteq Z(F,|\AC(\pi,C)\cap F|)( italic_π , italic_C ) ∩ italic_F ⊆ italic_Z ( italic_F , | ( italic_π , italic_C ) ∩ italic_F | ). Moreover, every element in \AC⁢(π,C)\AC𝜋𝐶\AC(\pi,C)( italic_π , italic_C ) has multiplicity at most 2. 1.2.3 Structure Theorem We prove a structure theorem (Theorem 8), which shows the existence of an r𝑟ritalic_r-basic solution that is near optimal. Theorem 8 (Structure Theorem). Let P𝑃Pitalic_P be a set of n𝑛nitalic_n points in ℝ2superscriptℝ2\mathbb{R}^{2}blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. Let 𝐚𝐚\bm{a}bold_italic_a be a random shift. For any large enough integer r𝑟ritalic_r, there is an r𝑟ritalic_r-basic tour OPT′superscriptOPT′\mathrm{OPT}^{\prime}roman_OPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT on P⊆ℝ2𝑃superscriptℝ2P\subseteq\mathbb{R}^{2}italic_P ⊆ blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT such that 𝔼𝒂⁢[cost⁢(OPT′)]≤(1+O⁢(1/r))⋅opt.subscript𝔼𝒂delimited-[]costsuperscriptOPT′⋅1𝑂1𝑟opt\mathbb{E}_{\bm{a}}[\mathrm{cost}(\mathrm{OPT}^{\prime})]\leq(1+O(1/r))\cdot% \mathrm{opt}.blackboard_E start_POSTSUBSCRIPT bold_italic_a end_POSTSUBSCRIPT [ roman_cost ( roman_OPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) ] ≤ ( 1 + italic_O ( 1 / italic_r ) ) ⋅ roman_opt . The proof of the Structure Theorem is a main novelty in this paper. In the rest of this section, we sketch high level ideas of the proof. To begin with, when F𝐹Fitalic_F has a single crossing with OPTOPT\mathrm{OPT}roman_OPT and a crossing with T𝑇Titalic_T, then we move the crossing of OPTOPT\mathrm{OPT}roman_OPT to the crossing of T𝑇Titalic_T. We can show that this cost is negligible (Lemma 12). Therefore, the non-trivial remaining case is when F𝐹Fitalic_F has a single crossing with OPTOPT\mathrm{OPT}roman_OPT but no crossing with T𝑇Titalic_T. We call such a side F𝐹Fitalic_F interesting. A crucial observation is that, when F𝐹Fitalic_F is interesting, we are able to afford moving the single crossing between F𝐹Fitalic_F and OPTOPT\mathrm{OPT}roman_OPT to the closest portal. This is proved in Lemma 14, which is our main technical contribution. To show Lemma 14, for each edge e∈OPT𝑒OPTe\in\mathrm{OPT}italic_e ∈ roman_OPT, we analyze the cost due to its crossings with interesting sides. For interesting sides that are shorter than the longest side crossing e𝑒eitalic_e, we analyze the cost using the notion of active crossings and by considering horizontal connections and vertical connections simultaneously, see Section 2.4. For the longest interesting side crossing e𝑒eitalic_e, we bound the connection cost in a creative way. To begin with, we define badly cut edges (Definition 16). This definition is inspired by Cohen-Addad [CA20]. Our analysis on badly cut edges is completely different from [CA20]. We now sketch high level ideas of our analysis. The non-trivial case is when a badly cut edge e𝑒eitalic_e crosses an interesting side. In this case, we distinguish whether e𝑒eitalic_e is critical (Fig. 3) or non-critical (Fig. 4), depending on which sides of the cell has connections in T𝑇Titalic_T with the endpoints of e𝑒eitalic_e. For critical edges, our analysis (Lemma 19) is based on [KNW21] and uses several new ingredients. To prove Lemma 19, first, we observe that, in [KNW21], a single crossing is problematic because the proximity pro⁢(x)pro𝑥\mathrm{pro}(x)roman_pro ( italic_x ) of that crossing may be arbitrarily large. To overcome that difficulty, we show that for a critical edge, the proximity of a crossing on a perpendicular side is bounded. Cares are needed, because in [KNW21], for a single crossing x𝑥xitalic_x along a side, the value αi⁢(x)subscript𝛼𝑖𝑥\alpha_{i}(x)italic_α start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_x ) is undefined. We define that value to be the interportal distance with small probability, by analyzing horizontal and vertical lines simultaneously. This is in contrast to the analysis in [KNW21] that is only on horizontal lines or only on vertical lines. Finally, for non-critical edges, we bound their connection cost using an elementary but delicate argument, see Lemma 20. The complete proof of the Structure Theorem is given in Section 2 . 1.2.4 Algorithm From the Structure Theorem, in order to compute a near-optimal solution, it suffices to compute an optimal r𝑟ritalic_r-basic solution using a standard dynamic program discussed in Section 3. The description of our algorithm for Theorem 1 is given in Algorithm 1. Algorithm 1 Algorithm for TSP in the Euclidean plane 1:Compute a 2-approximate solution T𝑇Titalic_T according to Lemma 3 2:Pick a random shift 𝒂𝒂\bm{a}bold_italic_a and compute a compressed quadtree from the dissection D⁢(𝒂)D𝒂\mathrm{D}(\bm{a})roman_D ( bold_italic_a ) 3:for each side F𝐹Fitalic_F of each cell C𝐶Citalic_C do 4: if F𝐹Fitalic_F is T𝑇Titalic_T-crossing then 5: xF←←subscript𝑥𝐹absentx_{F}\leftarrowitalic_x start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT ← an arbitrary crossing point between T𝑇Titalic_T and F𝐹Fitalic_F 6:Use a dynamic program to compute an optimal r𝑟ritalic_r-basic solution. The running time analysis of Algorithm 1 is given in Section 4. Theorem 1 follows. Open problem. It is an interesting open question whether our approach can be extended to achieve a linear-time Gap-ETH tight approximation scheme in d𝑑ditalic_d dimension, for fixed constant d𝑑ditalic_d."
https://arxiv.org/html/2411.02422v1,About the Kannan–Bachem algorithm,"The Smith reduction is a basic tool when analyzing integer matrices up to equivalence, and the Kannan-Bachem (KB) algorithm is the first polynomial algorithm computing such a reduction. Using this algorithm in complicated situations where the rank of the studied matrix is not maximal revealed an unexpected obstacle in the algorithm. This difficulty is described, analyzed, a simple solution is given to overcome it, finally leading to a general organization of the KB algorithm, simpler than the original one, efficient and having a general scope.An equivalent algorithm is used by the Magma program, without any detailed explanation, without any reference. The present text could so be useful.","The Smith reduction of an n×m𝑛𝑚n\times mitalic_n × italic_m integer matrix d:ℤm→ℤn:𝑑→superscriptℤ𝑚superscriptℤ𝑛d:\mathbb{Z}^{m}\rightarrow\mathbb{Z}^{n}italic_d : blackboard_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT → blackboard_Z start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT is a diagonal matrix s:ℤm→ℤn:𝑠→superscriptℤ𝑚superscriptℤ𝑛s:\mathbb{Z}^{m}\rightarrow\mathbb{Z}^{n}italic_s : blackboard_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT → blackboard_Z start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT satisfying the following conditions: • The matrices s𝑠sitalic_s and d𝑑ditalic_d are equivalent, that is, there exist two invertible matrices u:ℤm→ℤm:𝑢→superscriptℤ𝑚superscriptℤ𝑚u:\mathbb{Z}^{m}\rightarrow\mathbb{Z}^{m}italic_u : blackboard_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT → blackboard_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT and v:ℤn→ℤn:𝑣→superscriptℤ𝑛superscriptℤ𝑛v:\mathbb{Z}^{n}\rightarrow\mathbb{Z}^{n}italic_v : blackboard_Z start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT → blackboard_Z start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT with s=v⁢d⁢u𝑠𝑣𝑑𝑢s=vduitalic_s = italic_v italic_d italic_u. • The non-null entries d1,…,dksubscript𝑑1…subscript𝑑𝑘d_{1},\ldots,d_{k}italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT of the s𝑠sitalic_s-diagonal are positive and satisfy the divisor condition: every entry disubscript𝑑𝑖d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT divides the next one di+1subscript𝑑𝑖1d_{i+1}italic_d start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT; in particular the last one dksubscript𝑑𝑘d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT divides the possible 0 in position k+1𝑘1k+1italic_k + 1 on the diagonal if the rank k𝑘kitalic_k is less than m𝑚mitalic_m and n𝑛nitalic_n. The Smith reduction of integer matrices is used in many domains, in particular intensively used in computational Algebraic Topology. Calculating a homology group often consists in determining two boundary integer matrices d𝑑ditalic_d and d′superscript𝑑′d^{\prime}italic_d start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT satisfying d′⁢d=0superscript𝑑′𝑑0d^{\prime}d=0italic_d start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT italic_d = 0 and the looked-for homology group is the quotient ker⁡d′/im⁡dkernelsuperscript𝑑′im𝑑\ker d^{\prime}/\operatorname{im}droman_ker italic_d start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT / roman_im italic_d. Then the Smith reductions of the matrices d𝑑ditalic_d and d′superscript𝑑′d^{\prime}italic_d start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT directly give the corresponding homology group. In Constructive Algebraic Topology [12], sophisticated computations often needing several weeks or months of runtime on powerful computers finally produce such matrices d𝑑ditalic_d and d′superscript𝑑′d^{\prime}italic_d start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT, the corresponding homology group ker⁡d′/im⁡dkernelsuperscript𝑑′im𝑑\ker d^{\prime}/\operatorname{im}droman_ker italic_d start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT / roman_im italic_d being some homology or homotopy group unreachable by other means. Recently, such a calculation produced an integer matrix T8subscript𝑇8T_{8}italic_T start_POSTSUBSCRIPT 8 end_POSTSUBSCRIPT of size 684×19956841995684\times 1995684 × 1995 and a matrix T9subscript𝑇9T_{9}italic_T start_POSTSUBSCRIPT 9 end_POSTSUBSCRIPT of size 1995×5796199557961995\times 57961995 × 5796 with T8⁢T9=0subscript𝑇8subscript𝑇90T_{8}T_{9}=0italic_T start_POSTSUBSCRIPT 8 end_POSTSUBSCRIPT italic_T start_POSTSUBSCRIPT 9 end_POSTSUBSCRIPT = 0, the final hoped-for result being the homology group ker⁡T8/im⁡T9kernelsubscript𝑇8imsubscript𝑇9\ker T_{8}/\operatorname{im}T_{9}roman_ker italic_T start_POSTSUBSCRIPT 8 end_POSTSUBSCRIPT / roman_im italic_T start_POSTSUBSCRIPT 9 end_POSTSUBSCRIPT. A careful implementation of the KB algorithm was available in our environment, giving for example the Smith reduction of T8subscript𝑇8T_{8}italic_T start_POSTSUBSCRIPT 8 end_POSTSUBSCRIPT in 4 seconds. But the same algorithm used for T9subscript𝑇9T_{9}italic_T start_POSTSUBSCRIPT 9 end_POSTSUBSCRIPT failed. After a few days of runtime without any output, it was obvious a devil was hidden somewhere. It is well known that the first naive algorithms computing the Smith reductions very quickly generate huge intermediate integers with thousands of digits, often making it impossible for the calculation to terminate in reasonable time. The Smith reduction consists in using elementary operations on the studied matrix, without changing its equivalence class, to cancel the non-diagonal entries of the matrix. Kannan and Bachem in their article [8] showed how a careful and simple organization of the order of the entries to be cancelled gives a polynomial algorithm, avoiding the combinatorial explosion of the naive methods. The Hermite and Smith reductions in [8] are obtained only for square invertible matrices, but obvious adaptations extend the scope of the KB algorithm to the most general situation, for rectangular matrices of arbitrary rank. A careful tracing of our implementation of the KB algorithm, detailed in this text, revealed in fact the devil was hidden in these “obvious” adaptations to extend the KB organisation of [8] from the square non-singular case to arbitrary matrices, rectangular and arbitrary rank. Once this point is identified, it is easy (obvious!) to add a small complement in the organization of the “generalized” KB algorithm to dramatically improve our implementation in the general case. For example for our matrix T9subscript𝑇9T_{9}italic_T start_POSTSUBSCRIPT 9 end_POSTSUBSCRIPT, then the result is obtained in a few minutes. Two symmetric Hermite reductions are defined. For a square matrix, let us call HNF-1 the column-style reduction giving a lower triangular matrix, and HNF-2 the row-style reduction giving an upper triangular matrix. In the original KB algorithm for the Smith reduction, the HNF-1 reduction was privileged, with a sort of minimal potion of HNF-2. It happens the gap in our first erroneous generalization of the KB algorithm was fixed thanks to an extra dose of HNF-2. This relatively complicated mixture of HNF-1 and HNF-2 gives another idea. Starting with a rectangular matrix M0subscript𝑀0M_{0}italic_M start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT of arbitrary rank, the HNF-1 reduction produces a first matrix M1subscript𝑀1M_{1}italic_M start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, lower triangular above a rectangle. Now let us simply apply the HNF-2 reduction to M1subscript𝑀1M_{1}italic_M start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, this produces a matrix M2subscript𝑀2M_{2}italic_M start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, upper triangular. Experience shows it is in general much more “reduced” than M1subscript𝑀1M_{1}italic_M start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. Why not continue this game? We apply now HNF-1 to the last matrix M2subscript𝑀2M_{2}italic_M start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, obtaining M3subscript𝑀3M_{3}italic_M start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT, and so on. We prove that this process converges toward a diagonal matrix immediately giving the Smith reduction. The same proof like in [8] establishes this version of the KB algorithm is also polynomial, and experience shows after applying this method to numerous examples that it is the fastest one. Furthermore programming this version is very simple. Allowing easy extensions to other situations, for example for matrices of polynomials. This organization of the KB algorithm does not use any modular technology, it uses only left and right multiplications by unimodular matrices, mainly some appropriate Bézout matrices, so that if the matrices u𝑢uitalic_u and v𝑣vitalic_v satisfying s=v⁢d⁢u𝑠𝑣𝑑𝑢s=vduitalic_s = italic_v italic_d italic_u are desired, they can be easily determined along this version of the KB algorithm, like in the original KB algorithm. Cf in [5, Section IV-2] the complementary calculations which are necessary if the Smith form has been obtained via modular reduction. Along the same lines, see the article [7] how the research of the right modulus can be sophisticated, even in favorable situations, in case of small valence, which cannot be applied here. See also the comments of the author after [4, Algorithm 2.4.8]. This point is important in constructive homology, when an explicit ℤℤ\mathbb{Z}blackboard_Z-cycle is required for some homology class, the very basis of constructive Algebraic Topology, see Section 4.4. About our “numerous” examples, an error would consist in testing the various versions of the algorithm with banal random matrices. Such matrices are generally too simple with respect to the possible difficulties. The same for the matrices coming from elementary contexts in algebraic topology, typically the homology groups of simplicial complexes; even when these matrices are geant, the Smith reduction of these matrices is easy. Our difficult matrix T9subscript𝑇9T_{9}italic_T start_POSTSUBSCRIPT 9 end_POSTSUBSCRIPT was the result of a sophisticated work in Algebraic Topology, explained in the text, and was not at all arbitrary, allowing us to identify a severe drawback if the KB algorithm is too lazily extended for the rectangular matrices of arbitrary rank. But how it is possible to generate “interesting” difficult matrices with respect to the Smith reduction? A section of this text is devoted to this question, allowing us to easily generate “difficult” matrices and to present a statistical study of the results of the various versions of the KB algorithm with respect to these matrices. This generation process could be used to obtain good benchmarks for other Smith reduction algorithms. As explained in the abstract, an equivalent algorithm is in fact used by the program Magma, see[9]. Just a few lines are given in this program handbook, so that the present text, including detailed explanations, could be useful. Plan: Section 2 recalls the key tool of the reduction, known as the Bézout matrices. Section 3 explains the original KB algorithm, defined only for the non-singular square matrices. Section 4 gives the obvious complements extending the KB algorithm to the general case of a rectangular matrix of arbitrary rank. Section 5 describes the problem of Algebraic Topology producing, via our constructive methods, boundary matrices giving the homology group H8subscript𝐻8H_{8}italic_H start_POSTSUBSCRIPT 8 end_POSTSUBSCRIPT of a relatively complicated topological space. Section 6 explains the observed “accident” when we tried to apply our extended KB algorithm to the matrix T9subscript𝑇9T_{9}italic_T start_POSTSUBSCRIPT 9 end_POSTSUBSCRIPT, a matrix 1995×5796199557961995\times 57961995 × 5796. The reason of this accident is described and a simple solution to overcome it is given, a successful one. Section 7 observes the solution so obtained is nothing but a mixture a little complicated of the two classical Hermite reductions HNF-1 and HNF-2. This gives the idea of a direct iterative combination of HNF-1 and HNF-2. The algorithm then obtained is quite simple. The last Section 8 relates various tests illustrating that this last version of the KB algorithm is the fastest one. Designing interesting benchmark matrices is not simple, we explain how to obtain such matrices."
https://arxiv.org/html/2411.03255v1,Error Interference in Quantum Simulation,"Understanding algorithmic error accumulation in quantum simulation is crucial due to its fundamental significance and practical applications in simulating quantum many-body system dynamics. Conventional theories typically apply the triangle inequality to provide an upper bound for the error. However, these often yield overly conservative and inaccurate estimates as they neglect error interference — a phenomenon where errors in different segments can destructively interfere. Here, we introduce a novel method that directly estimates the long-time algorithmic errors with multiple segments, thereby establishing a comprehensive framework for characterizing algorithmic error interference. We identify the sufficient and necessary condition for strict error interference and introduce the concept of approximate error interference, which is more broadly applicable to scenarios such as power-law interaction models, the Fermi-Hubbard model, and higher-order Trotter formulas. Our work demonstrates significant improvements over prior ones and opens new avenues for error analysis in quantum simulation, offering potential advancements in both theoretical algorithm design and experimental implementation of Hamiltonian simulation.","Simulating quantum dynamics is a central application of quantum computation [1, 2]. Efficient quantum algorithms have been proposed for realizing the Schrödinger equation of a general quantum system [3, 4, 5], a notoriously difficult task for classical computers. These algorithms have been applied for various tasks, such as in quantum chemistry [6, 7, 8, 9] and quantum field theories [10, 11, 12, 13], and also serve as an indispensable subroutine for other fundamental quantum algorithms, such as quantum phase estimation [14] and the HHL algorithm for solving linear systems [15]. Since Lloyd’s proposal of the first digital quantum simulation algorithm based on the product formula (PF), also known as the Trotter-Suzuki formula, novel techniques such as Linear Combination of Unitaries (LCU) [16, 17, 18] and Quantum Signal Processing (QSP) [19, 20, 21] have been developed. These advancements have led to algorithms with optimal or nearly optimal dependence on several key parameters [22, 23, 24, 19, 20, 21, 21]. Nevertheless, variants of the product formula are still promising and popular candidates for near-term quantum devices [25] or intermediate problems [26] due to their simplicity, mild hardware requirements, and decent performance in practice. For a given Hamiltonian H=∑l=1LHl𝐻superscriptsubscript𝑙1𝐿subscript𝐻𝑙H=\sum_{l=1}^{L}H_{l}italic_H = ∑ start_POSTSUBSCRIPT italic_l = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT italic_H start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT with L𝐿Litalic_L terms of Hlsubscript𝐻𝑙H_{l}italic_H start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT, the key idea of the product formula method is to divide the long-time evolution e−i⁢H⁢tsuperscript𝑒i𝐻𝑡e^{-\textup{i}Ht}italic_e start_POSTSUPERSCRIPT - i italic_H italic_t end_POSTSUPERSCRIPT into several segments r𝑟ritalic_r and approximate the short-time evolution e−i⁢H⁢t/rsuperscript𝑒i𝐻𝑡𝑟e^{-\textup{i}Ht/r}italic_e start_POSTSUPERSCRIPT - i italic_H italic_t / italic_r end_POSTSUPERSCRIPT using the p𝑝pitalic_pth-order product formula (PFp𝑝pitalic_p) denoted 𝒰p⁢(t)subscript𝒰𝑝𝑡\mathscr{U}_{p}(t)script_U start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_t ) [27]. For example, the PF1 is defined as 𝒰1⁢(t/r):=e−i⁢H1⁢t/r⁢e−i⁢H2⁢t/r⁢⋯⁢e−i⁢HL⁢t/rassignsubscript𝒰1𝑡𝑟superscript𝑒isubscript𝐻1𝑡𝑟superscript𝑒isubscript𝐻2𝑡𝑟⋯superscript𝑒isubscript𝐻𝐿𝑡𝑟\mathscr{U}_{1}(t/r):=e^{-\textup{i}H_{1}t/r}e^{-\textup{i}H_{2}t/r}\cdots e^{% -\textup{i}H_{L}t/r}script_U start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_t / italic_r ) := italic_e start_POSTSUPERSCRIPT - i italic_H start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_t / italic_r end_POSTSUPERSCRIPT italic_e start_POSTSUPERSCRIPT - i italic_H start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_t / italic_r end_POSTSUPERSCRIPT ⋯ italic_e start_POSTSUPERSCRIPT - i italic_H start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT italic_t / italic_r end_POSTSUPERSCRIPT and the algorithmic (Trotter) error of each segment is given by ϵ=‖𝒰1⁢(t/r)−e−i⁢H⁢t/r‖italic-ϵnormsubscript𝒰1𝑡𝑟superscript𝑒i𝐻𝑡𝑟\epsilon=\|\mathscr{U}_{1}(t/r)-e^{-\textup{i}Ht/r}\|italic_ϵ = ∥ script_U start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_t / italic_r ) - italic_e start_POSTSUPERSCRIPT - i italic_H italic_t / italic_r end_POSTSUPERSCRIPT ∥. Understanding the Trotter error is an important problem, providing the basis for analyzing and optimizing the performance of the algorithm. A tighter error analysis can help save gate costs in both Hamiltonian simulation [26, 24, 28] and other product-formula-related tasks, such as imaginary time evolution [29], quantum Monte Carlo [30, 31], quantum adiabatic algorithms [32], and quantum phase estimation [33]. Many efforts [34, 35, 36, 37, 38, 39, 40] have been devoted to have tighter upper bounds of ϵitalic-ϵ\epsilonitalic_ϵ. However, the total Trotter error estimation is occasionally loose, as it is crudely upper-bounded by r⁢ϵ𝑟italic-ϵr\epsilonitalic_r italic_ϵ through the triangle inequality, which overlooks destructive error interference between different segments. Interestingly, it has been recently found that Trotter errors from different segments do destructively interfere in the PF1. Thus, the total error may not increase linearly with the number of segments r𝑟ritalic_r [39]. This error interference phenomenon can be explained from a PF2 perspective [41] and find various applications, e.g., quantum adiabatic algorithms [32] and quantum phase estimation [33]. However, this interesting phenomenon has only been investigated in a highly specific case, i.e., the two-term Hamiltonian H=A+B𝐻𝐴𝐵H=A+Bitalic_H = italic_A + italic_B with PF1, and its generalization and systematic understanding remain limited. In reality, error interference is more prevalent, even in simple cases such as simulating power-law decaying interaction Hamiltonians with PF1 [42]. In this work, we establish a general framework for analyzing algorithmic Trotter error interference in quantum simulation. The framework is based on a novel method that directly estimates the algorithmic Trotter error for a long time evolution, allowing us to derive the necessary and sufficient condition for strict error interference. Next, we give a general theoretical lower bound for algorithmic error accumulation, which in various cases excludes the possibility of error interference, e.g., 1D nearest-neighbor Heisenberg model with PF2, and generic Hamiltonians with polynomial series expansion algorithms. As applications, we provide more examples with (approximate) error interference phenomena beyond the H=A+B𝐻𝐴𝐵H=A+Bitalic_H = italic_A + italic_B case, including H=A+B+C𝐻𝐴𝐵𝐶H=A+B+Citalic_H = italic_A + italic_B + italic_C cases in the Heisenberg model, Fermi-Hubbard model, and power-law interaction models. Our results yield significantly improved error bounds compared to the triangle bounds in these cases. Interestingly, approximate error interference also exists in higher-order product formula approximation of perturbative evolution and can lead to speed-ups in some regions when implementing digital adiabatic algorithms. Our results are fundamentally interesting — furthering our understanding of the product formula theory, and practically useful — directly applicable to the implementation of Hamiltonian simulation using realist quantum hardware. Leveraging our error interference bounds, our results may also inspire better algorithm designs to reduce the total error [40]."
https://arxiv.org/html/2411.03253v1,Discovering Data Structures:Nearest Neighbor Search and Beyond,"We propose a general framework for end-to-end learning of data structures. Our framework adapts to the underlying data distribution and provides fine-grained control over query and space complexity. Crucially, the data structure is learned from scratch, and does not require careful initialization or seeding with candidate data structures. We first apply this framework to the problem of nearest neighbor search. In several settings, we are able to reverse-engineer the learned data structures and query algorithms. For 1D nearest neighbor search, the model discovers optimal distribution (in)dependent algorithms such as binary search and variants of interpolation search. In higher dimensions, the model learns solutions that resemble k-d trees in some regimes, while in others, elements of locality-sensitive hashing emerge. Additionally, the model learns useful representations of high-dimensional data and exploits them to design effective data structures. We also adapt our framework to the problem of estimating frequencies over a data stream, and believe it could be a powerful discovery tool for new problems.","Can deep learning models be trained to discover data structures from scratch? There are several motivations for this question. The first is scientific. Deep learning models are increasingly performing tasks once considered exclusive to humans, from image recognition and mastering the game of Go to engaging in natural language conversations. Designing data structures and algorithms, along with solving complex math problems, are particularly challenging tasks. They require searching through a vast combinatorial space with a difficult to define structure. It is therefore natural to ask what it would take for deep learning models to solve such problems. There are already promising signs: these models have discovered fast matrix-multiplication algorithms (Fawzi et al., 2022), solved SAT problems (Selsam et al., 2018), and learned optimization algorithms for various learning tasks (Garg et al., 2022; Akyürek et al., 2022; Fu et al., 2023; Von Oswald et al., 2023). In this work, we investigate the problem of data structure discovery, with a focus on nearest neighbor search. The second motivation is practical. Data structures are ubiquitous objects that enable efficient querying. Traditionally, they have been designed to be worst-case optimal and therefore agnostic to the underlying data and query distributions. However, in many applications there are patterns in these distributions that can be exploited to design more efficient data structures. This has motivated recent work on learning-augmented data structures which leverages knowledge of the data distribution to modify existing data structures with predictions (Lykouris & Vassilvitskii, 2018; Ding et al., 2020; Lin et al., 2022a; Mitzenmacher & Vassilvitskii, 2022). In much of this work, the goal of the learning algorithm is to learn distributional properties of the data, while the underlying query algorithm/data structure is hand-designed. Though this line of work clearly demonstrates the potential of leveraging distributional information, it still relies on expert knowledge to incorporate learning into such structures. In our work, we ask if it is possible to go one step further and let deep learning models discover entire data structures and query algorithms in an end-to-end manner. 1.1 Framework for data structure discovery Figure 1: Our model has two components: 1) A data-processing network that transforms raw data into structured data, arranging it for efficient querying and generating additional statistics when given extra space (not shown in the figure). 2) A query-execution network that performs M𝑀Mitalic_M lookups into the output of the data-processing network in order to retrieve the answer to some query q𝑞qitalic_q. Each lookup i𝑖iitalic_i is managed by a separate query model Qisuperscript𝑄𝑖Q^{i}italic_Q start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT, which takes q𝑞qitalic_q and the lookup history Hisubscript𝐻𝑖H_{i}italic_H start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, and outputs a one-hot lookup vector misubscript𝑚𝑖m_{i}italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT indicating the position to query. Data structure problems can often be decomposed into two steps: 1) data structure construction and 2) query execution. The first step transforms a raw dataset D𝐷Ditalic_D into a structured database D^^𝐷\hat{D}over^ start_ARG italic_D end_ARG, while query-execution performs lookups into D^^𝐷\hat{D}over^ start_ARG italic_D end_ARG to retrieve the answer for some query q𝑞qitalic_q. The performance of a data structure is typically quantified in terms of two measures: space complexity—how much memory is required to store the data structure, and query complexity—how many lookups into the data structure are required to answer some query. One can typically tradeoff larger space complexity for smaller query complexity, and vice versa. We focus on these criteria as they are widely studied and have clear practical connections to efficiency. To learn such data structures, we have a data-processing network which learns how to map a raw dataset to a data structure, and a query network which learns an algorithm for using the data structure to answer queries (Figure 1). In order to learn efficient data structures and query algorithms we impose constraints on the size of the data structures and on the number of lookups that the query network can make into the data structure. Crucially, we propose end-to-end training of both networks such that the learned data structure and query algorithm are optimized for one another. Moreover, in settings where it is beneficial to learn lower-dimensional representations from high-dimensional data, end-to-end training encourages the representations to capture features of the problem that the data structure can exploit. On the one hand, learning the data-processing network and query network jointly in an end-to-end fashion seems obvious, especially given the many successes of end-to-end learning over the past decade. On the other hand, it might be hard to imagine such learning getting off the ground. For instance, if the data-processing network produces a random garbled function of the dataset, we cannot hope the query model to do anything meaningful. This is further complicated by the fact that these data structure tasks are more discrete and combinatorial in terms of how the query model accesses the data structure. 1.2 Summary of Results We apply this framework to the problem of nearest neighbor (NN) search in both low and high dimensions. Given the extensive theoretical work on this topic, along with its widespread practical applications, NN search is an ideal starting point for understanding the landscape of end-to-end data structure discovery. Beyond NN search, we explore the problem of frequency estimation in streaming data and discuss other potential applications of this framework. Our findings are: Sorting and searching in 1D (Section 2.2) For 1D nearest neighbor search, the data-processing network learns to sort, while the query network simultaneously learns to search over the sorted data. When the data follows a uniform or Zipfian distribution, the query network exploits this structure to outperform binary search. On harder distributions lacking structure, the network adapts by discovering binary search, which is worst-case optimal. Importantly, the model discovers that sorting followed by the appropriate search algorithm is effective for NN search in 1D without explicit supervision for these primitives. K-d trees in 2D (Section 2.3) In 2D, when the data is drawn from a uniform distribution, the model discovers a data structure that outperforms k-d trees. On harder distributions, the learned data structure shows surprising resemblance to a k-d tree. This is striking as a k-d tree is a non-trivial data structure, constructed by recursively partitioning the data and finding the median along alternating dimensions. Useful representations in high dimensions (Section 2.4) For high-dimensional data, the model learns representations that make NN search efficient. For example, with data from a uniform distribution on a 30-dimensional hypersphere, the model partitions the space by projecting onto a pair of vectors, similar to locality-sensitive hashing. When trained on an extended 3-digit MNIST dataset, the model finds features that capture the relative ordering of the digits, sorts the images using these features, and performs a search on the sorted images—all of which is learned jointly from scratch. Trading off space and query efficiency (Section 2.5) An ideal data structure can use extra space to improve query efficiency by storing additional statistics. The learned model demonstrates this behavior, with performance improving monotonically as more space is provided, in both low and high dimensions. Thus, the model learns to effectively trade off space for query efficiency. Beyond nearest neighbor search (Section 3) We also explore the classical problem of frequency estimation, where a memory-constrained model observes a stream of items and must approximate the frequency of a query item. The learned structure exploits the underlying data distribution to outperform baselines like CountMin sketch, demonstrating the broader applicability of the framework beyond nearest neighbor search."
https://arxiv.org/html/2411.03133v1,Reconstructing edge-deleted unicyclic graphs,"The Harary reconstruction conjecture states that any graph with more than four edges can be uniquely reconstructed from its set of maximal edge-deleted subgraphs [harary1965reconstruction]. In 1977, Müller verified the conjecture for graphs with n𝑛nitalic_n vertices and n⁢log2⁡(n)𝑛subscript2𝑛n\log_{2}(n)italic_n roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_n ) edges, improving on Lováz’s bound of (n2−n)/4superscript𝑛2𝑛4\nicefrac{{(n^{2}-n)}}{{4}}/ start_ARG ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - italic_n ) end_ARG start_ARG 4 end_ARG [Muller_1977]. Here, we show that the reconstruction conjecture holds for graphs which have exactly one cycle and three non-isomorphic subtrees.","A graph G𝐺Gitalic_G is a pair G=(V,E)𝐺𝑉𝐸G=(V,E)italic_G = ( italic_V , italic_E ) of vertices V𝑉Vitalic_V and edges E⊆V×V𝐸𝑉𝑉E\subseteq V\times Vitalic_E ⊆ italic_V × italic_V. A graph is simple when (u,v)=(v,u)𝑢𝑣𝑣𝑢(u,v)=(v,u)( italic_u , italic_v ) = ( italic_v , italic_u ) for any edge (u,v)𝑢𝑣(u,v)( italic_u , italic_v ), and no edge is of the form (u,u)𝑢𝑢(u,u)( italic_u , italic_u ). A (length-n𝑛nitalic_n) path in G𝐺Gitalic_G is a sequence of edges p=(e1,…,en)𝑝subscript𝑒1…subscript𝑒𝑛p=(e_{1},\dots,e_{n})italic_p = ( italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_e start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) such that: eisubscript𝑒𝑖e_{i}italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and ejsubscript𝑒𝑗e_{j}italic_e start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT share a vertex if and only if j=(i+1)⁢(mod⁢n)𝑗𝑖1mod𝑛j=(i+1)~{}(\mathrm{mod}\ n)italic_j = ( italic_i + 1 ) ( roman_mod italic_n ); any vertex is shared by exactly two edges in p𝑝pitalic_p. A graph G𝐺Gitalic_G is connected if and only if there exists a path beginning at u𝑢uitalic_u and terminating at v𝑣vitalic_v for any pair of distinct vertices u≠v𝑢𝑣u\neq vitalic_u ≠ italic_v. A cycle is a path which begins and ends at the same vertex, and a graph is unicyclic when it has exactly one cycle, as in Figure 1(a); a graph without cycles is called forest, and a connected forest graph is called a tree. For two graphs G𝐺Gitalic_G and H𝐻Hitalic_H, a bijective map φ:VG→VH:𝜑→subscript𝑉𝐺subscript𝑉𝐻\varphi:V_{G}\to V_{H}italic_φ : italic_V start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT → italic_V start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT where (u,v)∈EG⇔(φ⁢(u),φ⁢(v))∈EHiff𝑢𝑣subscript𝐸𝐺𝜑𝑢𝜑𝑣subscript𝐸𝐻(u,v)\in E_{G}\iff(\varphi(u),\varphi(v))\in E_{H}( italic_u , italic_v ) ∈ italic_E start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ⇔ ( italic_φ ( italic_u ) , italic_φ ( italic_v ) ) ∈ italic_E start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT for all pairs of distinct vertices u𝑢uitalic_u and v𝑣vitalic_v is called a graph isomorphism; if such a φ𝜑\varphiitalic_φ exists, G𝐺Gitalic_G and H𝐻Hitalic_H are isomorphic. From here, all graphs are finite and simple. In 1965, Harary posited in [harary1965reconstruction] that Conjecture (Harary). For G=(V,E)𝐺𝑉𝐸G=(V,E)italic_G = ( italic_V , italic_E ) a graph with |E|>4𝐸4|E|>4| italic_E | > 4, let Gisubscript𝐺𝑖G_{i}italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT denote the maximal subgraph of G𝐺Gitalic_G with the i𝑖iitalic_ith edge deleted, and Deck⁡(G)≔{Gi}i=1m≔Deck𝐺superscriptsubscriptsubscript𝐺𝑖𝑖1𝑚\operatorname{\mathrm{Deck}}(G)\coloneqq\{G_{i}\}_{i=1}^{m}roman_Deck ( italic_G ) ≔ { italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT the set of maximal edge-deleted subgraphs of G𝐺Gitalic_G. If Deck⁡(G)=Deck⁡(H)Deck𝐺Deck𝐻\operatorname{\mathrm{Deck}}(G)=\operatorname{\mathrm{Deck}}(H)roman_Deck ( italic_G ) = roman_Deck ( italic_H ) for some graph H𝐻Hitalic_H, then G≅H𝐺𝐻G\cong Hitalic_G ≅ italic_H. Here, we show the conjecture holds for the class 𝒰𝒰\mathscr{U}script_U of unicyclic graphs with at least three non-isomorphic subtrees."
https://arxiv.org/html/2411.02942v1,Constant Approximation for Weighted Nash Social Welfare with Submodular Valuations,"We study the problem of assigning items to agents so as to maximize the weighted Nash Social Welfare (NSW) under submodular valuations. The best-known result for the problem is an O⁢(n⁢wmax)𝑂𝑛subscript𝑤O(nw_{\max})italic_O ( italic_n italic_w start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT )-approximation due to Garg, Husic, Li, Vega, and Vondrak [13], where wmaxsubscript𝑤w_{\max}italic_w start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT is the maximum weight over all agents. Obtaining a constant approximation algorithm is an open problem in the field that has recently attracted considerable attention.We give the first such algorithm for the problem, thus solving the open problem in the affirmative. Our algorithm is based on the natural Configuration LP for the problem, which was introduced recently by Feng and Li [11] for the additive valuation case. Our rounding algorithm is similar to that of Li [25] developed for the unrelated machine scheduling problem to minimize weighted completion time. Roughly speaking, we designate the largest item in each configuration as a large item and the remaining items as small items. So, every agent gets precisely 1 fractional large item in the configuration LP solution. With the rounding algorithm in [25], we can ensure that in the obtained solution, every agent gets precisely 1 large item, and the assignments of small items are negatively correlated.","We study the problem of allocating a set M𝑀Mitalic_M of indivisible items among a set N𝑁Nitalic_N of agents, where each agent i∈N𝑖𝑁i\in Nitalic_i ∈ italic_N has a monotone non-negative submodular valuation vi:2M→ℝ≥0:subscript𝑣𝑖→superscript2𝑀subscriptℝabsent0v_{i}:2^{M}\to\mathbb{R}_{\geq 0}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT : 2 start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT → blackboard_R start_POSTSUBSCRIPT ≥ 0 end_POSTSUBSCRIPT and a weight wi∈(0,1)subscript𝑤𝑖01w_{i}\in(0,1)italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ ( 0 , 1 ) with ∑i∈Nwi=1subscript𝑖𝑁subscript𝑤𝑖1\sum_{i\in N}w_{i}=1∑ start_POSTSUBSCRIPT italic_i ∈ italic_N end_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1. The weighted Nash Social Welfare (NSW) problem under submodular valuations asks for partition 𝒮:=(Si)i∈Nassign𝒮subscriptsubscript𝑆𝑖𝑖𝑁{\cal S}:=(S_{i})_{i\in N}caligraphic_S := ( italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i ∈ italic_N end_POSTSUBSCRIPT of M𝑀Mitalic_M that maximizes the weighted geometric mean of the agents’ valuations: 𝖭𝖲𝖶⁢(𝒮)=∏i∈N(vi⁢(Si))wi.𝖭𝖲𝖶𝒮subscriptproduct𝑖𝑁superscriptsubscript𝑣𝑖subscript𝑆𝑖subscript𝑤𝑖\mathsf{NSW}({\cal S})=\prod_{i\in N}\left(v_{i}(S_{i})\right)^{w_{i}}.sansserif_NSW ( caligraphic_S ) = ∏ start_POSTSUBSCRIPT italic_i ∈ italic_N end_POSTSUBSCRIPT ( italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) start_POSTSUPERSCRIPT italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT . The case when all wisubscript𝑤𝑖w_{i}italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT’s are equal to 1/n1𝑛1/n1 / italic_n is called the unweighted Nash Social Welfare problem. As usual, we assume we are given a value oracle for each visubscript𝑣𝑖v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. W.l.o.g, we assume vi⁢(∅)=0subscript𝑣𝑖0v_{i}(\emptyset)=0italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( ∅ ) = 0 for every agent i∈N𝑖𝑁i\in Nitalic_i ∈ italic_N 111If vi⁢(∅)>0subscript𝑣𝑖0v_{i}(\emptyset)>0italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( ∅ ) > 0 for some i∈N𝑖𝑁i\in Nitalic_i ∈ italic_N, we can create a “private” item jisubscript𝑗𝑖j_{i}italic_j start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT for i𝑖iitalic_i which has 00-value to all agents other than i𝑖iitalic_i. We replace the valuation of i𝑖iitalic_i with vi′subscriptsuperscript𝑣′𝑖v^{\prime}_{i}italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, which is defined as follows: vi′⁢(S):=vi⁢(S)−vi⁢(∅)assignsubscriptsuperscript𝑣′𝑖𝑆subscript𝑣𝑖𝑆subscript𝑣𝑖v^{\prime}_{i}(S):=v_{i}(S)-v_{i}(\emptyset)italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_S ) := italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_S ) - italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( ∅ ) if ji∉Ssubscript𝑗𝑖𝑆j_{i}\notin Sitalic_j start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∉ italic_S and vi′⁢(S)=vi⁢(S∖ji)subscriptsuperscript𝑣′𝑖𝑆subscript𝑣𝑖𝑆subscript𝑗𝑖v^{\prime}_{i}(S)=v_{i}(S\setminus j_{i})italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_S ) = italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_S ∖ italic_j start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) if ji∈Ssubscript𝑗𝑖𝑆j_{i}\in Sitalic_j start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ italic_S.. Fair and efficient allocation of resources is a central problem in computer science, game theory, and social choices, with applications across diverse domains [1, 3, 4, 22, 27, 31, 32, 37]. Three distinct communities independently discovered the notation of Nash social welfare: as a solution to the bargaining problem in classical game theory [28], as a well-established concept of proportional fairness in networking [23], and as the celebrated notion of competitive equilibrium with equal incomes in economics [35]. The unweighted case for the problem was introduced by Nash [28], and it was later extended to the weighted case [17, 21]. This extension has since been widely studied and applied across various fields, such as bargaining theory [7, 24, 34], water allocation [9, 19], climate agreements [38], and more. One of the most important features of the NSW objective is that it offers a tradeoff between the frequently conflicting demands of fairness and efficiency. A special case for the valuations visubscript𝑣𝑖v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is when they are additive. The unweighted NSW problem with additive valuations is an important topic in optimization and has received considerable interest. Barman, Krishnamurthy, and Vaish [2] developed a (𝖾1/𝖾≈1.445)superscript𝖾1𝖾1.445(\mathsf{e}^{1/\mathsf{e}}\approx 1.445)( sansserif_e start_POSTSUPERSCRIPT 1 / sansserif_e end_POSTSUPERSCRIPT ≈ 1.445 )-approximation algorithm that finds an allocation that is both Pareto-efficient and envy-free up to one item (EF1). They showed that this problem can be reduced to the case of identical valuations, where any EF1 allocation can achieve an approximation ratio of 𝖾1/𝖾≈1.445superscript𝖾1𝖾1.445\mathsf{e}^{1/\mathsf{e}}\approx 1.445sansserif_e start_POSTSUPERSCRIPT 1 / sansserif_e end_POSTSUPERSCRIPT ≈ 1.445. On the negative side, Garg, Hoefer, and Mehlhorn [12] established a hardness of 8/787\sqrt{8/7}square-root start_ARG 8 / 7 end_ARG. For the weighted case with additive valuations, Brown, Laddha, Pittu, and Singh [5] introduced an approximation algorithm with a ratio of 5⋅exp⁢(2⁢log⁡n+2⁢∑i∈Awi⁢log⁡wi)⋅5exp2𝑛2subscript𝑖𝐴subscript𝑤𝑖subscript𝑤𝑖5\cdot\text{exp}(2\log n+2\sum_{i\in A}w_{i}\log w_{i})5 ⋅ exp ( 2 roman_log italic_n + 2 ∑ start_POSTSUBSCRIPT italic_i ∈ italic_A end_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT roman_log italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ). Later, Feng and Li [11] presented an elegant (𝖾1/𝖾+ϵ)superscript𝖾1𝖾italic-ϵ(\mathsf{e}^{1/\mathsf{e}}+\epsilon)( sansserif_e start_POSTSUPERSCRIPT 1 / sansserif_e end_POSTSUPERSCRIPT + italic_ϵ )-approximation algorithm for the weighted case, using their novel configuration LP and the Shmoys-Tardos rounding procedure developed in the context of unrelated machine scheduling. The approximation ratio matches the best-known ratio for the unweighted case. When the n𝑛nitalic_n valuation functions are additive and identical, Nguyen and Rothe [29] developed a PTAS for the unweighted NSW problem. Later, Inoue and Kobayashi [20] gave an additive PTAS for the problem, i.e., a polynomial-time algorithm that maximizes the Nash social welfare within an additive error of ϵ⁢vmaxitalic-ϵsubscript𝑣\epsilon v_{\max}italic_ϵ italic_v start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT, where vmaxsubscript𝑣v_{\max}italic_v start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT is the maximum utility of an item. Li and Vondrak [26] developed the first constant approximation algorithm for unweighted NSW with submodular valuations using convex programming. The ratio has been improved by Garg, Husic, Li, Vega, and Vondrak [13] to (4+ϵ)4italic-ϵ(4+\epsilon)( 4 + italic_ϵ ) using an elegant local-search-based algorithm. When additionally n=O⁢(1)𝑛𝑂1n=O(1)italic_n = italic_O ( 1 ), by guessing the value and the O⁢(1)𝑂1O(1)italic_O ( 1 ) largest items for each agent, and using the multilinear extension of submodular functions, a 𝖾/(𝖾−1)𝖾𝖾1\mathsf{e}/(\mathsf{e}-1)sansserif_e / ( sansserif_e - 1 )-approximation can be achieved [16]. In the same paper, [16] proved that unweighted NSW with submodular valuations is hard to approximate within 𝖾/(𝖾−1)−ϵ𝖾𝖾1italic-ϵ\mathsf{e}/(\mathsf{e}-1)-\epsilonsansserif_e / ( sansserif_e - 1 ) - italic_ϵ. The hardness holds even for the case n=O⁢(1)𝑛𝑂1n=O(1)italic_n = italic_O ( 1 ). For the weighted NSW problem with submodular valuations, [13] showed that the approximation ratio of the local search algorithm becomes O⁢(n⁢wmax)𝑂𝑛subscript𝑤O(nw_{\max})italic_O ( italic_n italic_w start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT ), where wmax:=maxi∈[n]⁡wiassignsubscript𝑤subscript𝑖delimited-[]𝑛subscript𝑤𝑖w_{\max}:=\max_{i\in[n]}w_{i}italic_w start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT := roman_max start_POSTSUBSCRIPT italic_i ∈ [ italic_n ] end_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is the maximum weight over the agents. In the new version [15] of the paper, the authors presented a (6⁢𝖾+ϵ)6𝖾italic-ϵ(6\mathsf{e}+\epsilon)( 6 sansserif_e + italic_ϵ )-approximation algorithm with running time 2O⁢(n⁢log⁡n)⁢poly⁢(m,1/ϵ)superscript2𝑂𝑛𝑛poly𝑚1italic-ϵ2^{O(n\log n)}\mathrm{poly}(m,1/\epsilon)2 start_POSTSUPERSCRIPT italic_O ( italic_n roman_log italic_n ) end_POSTSUPERSCRIPT roman_poly ( italic_m , 1 / italic_ϵ ), which is polynomial when n=O⁢(1)𝑛𝑂1n=O(1)italic_n = italic_O ( 1 ). For the more general setting where the valuations are subadditive, Dobzinski, Li, Rubinstein, and Vondrák [10] recently proposed a constant approximation algorithm when agents are unweighted, provided that we have access to demand oracles for the valuation functions. Our Result. In this paper, we give the first polynomial-time O⁢(1)𝑂1O(1)italic_O ( 1 )-approximation algorithm for weighted Nash social welfare under the submodular valuations. The best result prior to this work was the O⁢(n⁢wmax)𝑂𝑛subscript𝑤O(nw_{\max})italic_O ( italic_n italic_w start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT )-approximation due to Garg, Husic, Li, Vega, and Vondrak [13]. Theorem 1.1. For any ϵ>0italic-ϵ0\epsilon>0italic_ϵ > 0, there is a randomized (233+ϵ)233italic-ϵ(233+\epsilon)( 233 + italic_ϵ )-approximation algorithm for the weighted Nash social welfare problem with submodular valuations, with running time polynomial in the size of the input and 1ϵ1italic-ϵ\frac{1}{\epsilon}divide start_ARG 1 end_ARG start_ARG italic_ϵ end_ARG. For convenience, we list the known approximation results for the NSW problem in table 1. Additive Submodular Subadditive LB UB LB UB LB UB Unweighted 8787\sqrt{\frac{8}{7}}square-root start_ARG divide start_ARG 8 end_ARG start_ARG 7 end_ARG end_ARG [12] 𝖾1/𝖾+ϵsuperscript𝖾1𝖾italic-ϵ\mathsf{e}^{1/\mathsf{e}}+\epsilonsansserif_e start_POSTSUPERSCRIPT 1 / sansserif_e end_POSTSUPERSCRIPT + italic_ϵ [2] 𝖾𝖾−1𝖾𝖾1\frac{\mathsf{e}}{\mathsf{e}-1}divide start_ARG sansserif_e end_ARG start_ARG sansserif_e - 1 end_ARG [16] 4+ϵ4italic-ϵ4+\epsilon4 + italic_ϵ [13] O⁢(1)∗𝑂superscript1O(1)^{*}italic_O ( 1 ) start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT [10] Weighted 𝖾1/𝖾+ϵsuperscript𝖾1𝖾italic-ϵ\mathsf{e}^{1/\mathsf{e}}+\epsilonsansserif_e start_POSTSUPERSCRIPT 1 / sansserif_e end_POSTSUPERSCRIPT + italic_ϵ [11] 233+ϵ233italic-ϵ233+\epsilon233 + italic_ϵ (theorem 1.1) Table 1: Known Results for Nash social welfare. LB and UB stand for lower and upper bounds, respectively. The result with ∗ requires demand oracles for valuation functions. When the function is identical additive, the upper bound for the unweighted case is PTAS [20, 29]. When n=O⁢(1)𝑛𝑂1n=O(1)italic_n = italic_O ( 1 ), the upper bounds for unweighted and weighted NSW with submodular valuations are respectively 𝖾/(𝖾−1)𝖾𝖾1\mathsf{e}/(\mathsf{e}-1)sansserif_e / ( sansserif_e - 1 ) [16] and 6⁢𝖾+ϵ6𝖾italic-ϵ6\mathsf{e}+\epsilon6 sansserif_e + italic_ϵ [15]. 1.1 Overview of Our Techniques Our algorithm leverages the configuration LP introduced in [11] for the additive valuation case. For each agent i∈N𝑖𝑁i\in Nitalic_i ∈ italic_N and subset of items S⊆M𝑆𝑀S\subseteq Mitalic_S ⊆ italic_M, we define a variable yi,Ssubscript𝑦𝑖𝑆y_{i,S}italic_y start_POSTSUBSCRIPT italic_i , italic_S end_POSTSUBSCRIPT to indicate whether the set of items assigned to i𝑖iitalic_i is precisely S𝑆Sitalic_S. The objective of this LP is to minimize ∑i,Syi,S⋅wi⋅ln⁡vi⁢(S)subscript𝑖𝑆⋅subscript𝑦𝑖𝑆subscript𝑤𝑖subscript𝑣𝑖𝑆\sum_{i,S}y_{i,S}\cdot w_{i}\cdot\ln v_{i}(S)∑ start_POSTSUBSCRIPT italic_i , italic_S end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_i , italic_S end_POSTSUBSCRIPT ⋅ italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ⋅ roman_ln italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_S ), the logarithm of the NSW objective. After solving the LP, we apply the rounding procedure from [25], developed for the weighted completion time minimization problem in the unrelated machine scheduling setting. Then we prove concentration bounds for the values obtained by each agent, using arguments developed for pipage rounding. To build intuition, let us focus on the unweighted case. For the special case where |M|=|N|𝑀𝑁|M|=|N|| italic_M | = | italic_N |, the problem reduces to a maximum-weight bipartite matching problem with weights given by the logarithm of values. So, any general algorithm for the problem must capture the maximum weight of the bipartite matching algorithm as a special case. Interestingly, previous results showed that if one is given the largest (i.e., the most valuable) item assigned to every agent, then an O⁢(1)𝑂1O(1)italic_O ( 1 )-approximation algorithm is easy to obtain using local search [13] or LP rounding [14, 26]. For example, with this idea, Garg, Husic, Li, Vega, and Vondrak [13] designed an elegant 4-approximation local search algorithm. They first compute an initial matching of one item to every agent so as to maximize the NSW objective, then assign the remaining items using local search with an endowed valuation function, and finally rematch the initially assigned items to agents to maximize the final Nash social welfare. Unfortunately, their algorithm fails to give an O⁢(1)𝑂1O(1)italic_O ( 1 )-approximation when the agents are weighted. Our algorithm implements the idea of “matching largest items to agents” using the configuration LP solution as a guide. We achieve a per-client guarantee, allowing us to give an O⁢(1)𝑂1O(1)italic_O ( 1 )-approximation for the weighted NSW problem with submodular valuations. After obtaining an LP solution (yi,S∗)i,Ssubscriptsubscriptsuperscript𝑦𝑖𝑆𝑖𝑆(y^{*}_{i,S})_{i,S}( italic_y start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i , italic_S end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i , italic_S end_POSTSUBSCRIPT, for each agent i𝑖iitalic_i and configuration S𝑆Sitalic_S, we designate the largest item in S𝑆Sitalic_S as a “large” item for i𝑖iitalic_i, while treating the remaining items as “small”. This creates a fractional assignment in which each agent receives exactly one fractional large item. While maintaining marginal probabilities in our rounding algorithm, we ensure that each agent gets exactly one large item, and the assignment of small items are negatively correlated. That is, we select a random matching for large items. If the large and small items were disjoint, the rounding algorithm would be straightforward. However, complications arise when an item may be large for one agent and small for another — or even for the same agent in different configurations. This necessitates a correlated assignment strategy for large and small items. This is where we employ the iterative rounding procedure of [25]. We construct a bipartite multi-graph between agents and items, with two edge types: marked edges for large items and unmarked edges for small items. During iterative rounding, we identify either a simple cycle of marked edges or a pseudo-marked path – a simple path of marked edges with two unmarked edges at the ends – and apply rotation or shifting operations on the cycle or path in each iteration. This process ultimately yields an integral assignment. To analyze the approximation ratio, we focus on each agent i𝑖iitalic_i and analyze 𝔼⁢[ln⁡(vi⁢(T))]𝔼delimited-[]subscript𝑣𝑖𝑇\mathbb{E}[\ln(v_{i}(T))]blackboard_E [ roman_ln ( italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_T ) ) ], where T𝑇Titalic_T is the set of items assigned to i𝑖iitalic_i. Note that T𝑇Titalic_T includes exactly one large item, respecting the marginal probabilities. Let TSsuperscript𝑇ST^{\mathrm{S}}italic_T start_POSTSUPERSCRIPT roman_S end_POSTSUPERSCRIPT denote the remaining items, i.e., the small items assigned to i𝑖iitalic_i. The assignments of the large item and the small items may be positively correlated, so we analyze the worst-case scenario for this correlation. However, the assignments of the small items are negatively correlated; more precisely, they are determined through a pipage-rounding procedure. Using the concave pessimistic estimator technique from [18] and the submodularity of the function visubscript𝑣𝑖v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, we can establish concentration bounds for vi⁢(TS)subscript𝑣𝑖superscript𝑇Sv_{i}(T^{\mathrm{S}})italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_T start_POSTSUPERSCRIPT roman_S end_POSTSUPERSCRIPT ). With the bounds, we can lower bound 𝔼⁢[ln⁡(vi⁢(T))]𝔼delimited-[]subscript𝑣𝑖𝑇\mathbb{E}[\ln(v_{i}(T))]blackboard_E [ roman_ln ( italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_T ) ) ] by ∑Syi,S∗⁢ln⁡vi⁢(S)−O⁢(1)subscript𝑆subscriptsuperscript𝑦𝑖𝑆subscript𝑣𝑖𝑆𝑂1\sum_{S}y^{*}_{i,S}\ln v_{i}(S)-O(1)∑ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT italic_y start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i , italic_S end_POSTSUBSCRIPT roman_ln italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_S ) - italic_O ( 1 ). Organization. The rest of the paper is organized as follows. We introduce some preliminaries in section 2, describe our algorithm in section 3, and give its analysis in section 4. For a smoother flow in the main text, we defer some proofs to the appendix."
https://arxiv.org/html/2411.02148v1,Optimality of Frequency Moment Estimation,"Estimating the second frequency moment of a stream up to (1±ε)plus-or-minus1𝜀(1\pm\varepsilon)( 1 ± italic_ε ) multiplicative error requires at most O⁢(log⁡n/ε2)𝑂𝑛superscript𝜀2O(\log n/\varepsilon^{2})italic_O ( roman_log italic_n / italic_ε start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) bits of space, due to a seminal result of Alon, Matias, and Szegedy. It is also known that at least Ω⁢(log⁡n+1/ε2)Ω𝑛1superscript𝜀2\Omega(\log n+1/\varepsilon^{2})roman_Ω ( roman_log italic_n + 1 / italic_ε start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) space is needed. We prove an optimal lower bound of Ω⁢(log⁡(n⁢ε2)/ε2)Ω𝑛superscript𝜀2superscript𝜀2\Omega\left(\log\left(n\varepsilon^{2}\right)/\varepsilon^{2}\right)roman_Ω ( roman_log ( italic_n italic_ε start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) / italic_ε start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) for all ε=Ω⁢(1/n)𝜀Ω1𝑛\varepsilon=\Omega(1/\sqrt{n})italic_ε = roman_Ω ( 1 / square-root start_ARG italic_n end_ARG ). Note that when ε>n−1/2+c𝜀superscript𝑛12𝑐\varepsilon>n^{-1/2+c}italic_ε > italic_n start_POSTSUPERSCRIPT - 1 / 2 + italic_c end_POSTSUPERSCRIPT, where c>0𝑐0c>0italic_c > 0, our lower bound matches the classic upper bound of AMS. For smaller values of ε𝜀\varepsilonitalic_ε we also introduce a revised algorithm that improves the classic AMS bound and matches our lower bound. Our lower bound holds also for the more general problem of p𝑝pitalic_p-th frequency moment estimation for the range of p∈(1,2]𝑝12p\in(1,2]italic_p ∈ ( 1 , 2 ], giving a tight bound in the only remaining range to settle the optimal space complexity of estimating frequency moments.","An extensive body of literature is devoted to the streaming model of computation, which is important for the analysis of massive datasets and in network traffic monitoring. A central problem in this model is the frequency moment estimation problem: Elements from a universe U𝑈Uitalic_U are given to the algorithm one-by-one, defining a vector of frequencies — that is, fx∈ℕsubscript𝑓𝑥ℕf_{x}\in\mathbb{N}italic_f start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT ∈ blackboard_N is the number of times the element x∈U𝑥𝑈x\in Uitalic_x ∈ italic_U appeared in the stream; Finally, the algorithm has to return, with good probability, a (1±ε)plus-or-minus1𝜀(1\pm\varepsilon)( 1 ± italic_ε )-estimation of Fp:=∑x∈Ufxpassignsubscript𝐹𝑝subscript𝑥𝑈superscriptsubscript𝑓𝑥𝑝F_{p}:=\sum_{x\in U}f_{x}^{p}italic_F start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT := ∑ start_POSTSUBSCRIPT italic_x ∈ italic_U end_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT — the p𝑝pitalic_p-th frequency moment of the stream. We generally denote the length of the stream by n𝑛nitalic_n and assume that |U|=poly⁢(n)𝑈poly𝑛|U|=\text{poly}(n)| italic_U | = poly ( italic_n ). The main complexity parameter studied in this model is how much space is needed for the algorithm to succeed. The study of both the streaming model and of frequency moment estimation in it was initiated in the seminal 1996 work of Alon, Matias, and Szegedy [AMS96]. The case of p=2𝑝2p=2italic_p = 2, or second moment estimation, is of particular importance. It is often called the repeat rate or surprise index, and is used in various tasks such as database query optimization [AGMS99], network traffic anomaly detection [KSZC03], approximate histogram maintenance [GGI+02] and more. Other moments of particular interest are p=1𝑝1p=1italic_p = 1, corresponding to the approximate counting problem [Mor78, NY22], and p=0𝑝0p=0italic_p = 0, corresponding to the distinct elements problem [FM85, IW03, KNW10b]. Among these special cases, only the space complexity of the first remains not fully understood. The original algorithm for F2subscript𝐹2F_{2}italic_F start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT-estimation given by Alon, Matias, and Szegedy uses O⁢(log⁡n/ε2)𝑂𝑛superscript𝜀2O(\log n/\varepsilon^{2})italic_O ( roman_log italic_n / italic_ε start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) bits of space; while the highest known lower bound due to Woodruff in 2004 [Woo04] is Ω⁢(log⁡n+1/ε2)Ω𝑛1superscript𝜀2\Omega(\log n+1/\varepsilon^{2})roman_Ω ( roman_log italic_n + 1 / italic_ε start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) — leaving up to a quadratic gap between the upper and lower bounds for certain choices of ε𝜀\varepsilonitalic_ε. While Fpsubscript𝐹𝑝F_{p}italic_F start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT-estimation for p≤2𝑝2p\leq 2italic_p ≤ 2 uses amount of space that is only logarithmic in the length of the stream, it was shown that for p>2𝑝2p>2italic_p > 2 at least Ω⁢(n1−2/p/poly⁢(ε))Ωsuperscript𝑛12𝑝poly𝜀\Omega(n^{1-2/p}/\text{poly}(\varepsilon))roman_Ω ( italic_n start_POSTSUPERSCRIPT 1 - 2 / italic_p end_POSTSUPERSCRIPT / poly ( italic_ε ) ) space is needed [BYJKS04, CKS03b] — which is polynomial in the stream’s length. A long list of works [IW05, BGKS06, MW10, AKO11, BO10, And17, Gan11b, WZ12, Gan11a, LW13] resulted in a nearly-tight bound of Θ~⁢(n1−2/p/ε2)~Θsuperscript𝑛12𝑝superscript𝜀2\tilde{\Theta}\left(n^{1-2/p}/\varepsilon^{2}\right)over~ start_ARG roman_Θ end_ARG ( italic_n start_POSTSUPERSCRIPT 1 - 2 / italic_p end_POSTSUPERSCRIPT / italic_ε start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) for Fpsubscript𝐹𝑝F_{p}italic_F start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT-estimation for every p>2𝑝2p>2italic_p > 2 (not necessarily an integer) and ε𝜀\varepsilonitalic_ε, for some ranges of parameters the bounds are tight — in others there is a gap between the bounds that is poly-logarithmic in the bound itself. For p≤2𝑝2p\leq 2italic_p ≤ 2 the space complexity is not as well understood. Woodruff [Woo04] showed a lower bound of Ω⁢(log⁡n+1/ε2)Ω𝑛1superscript𝜀2\Omega(\log n+1/\varepsilon^{2})roman_Ω ( roman_log italic_n + 1 / italic_ε start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) for every p≠1𝑝1p\neq 1italic_p ≠ 1, this is optimal in terms of ε𝜀\varepsilonitalic_ε alone and is also known to be optimal for the distinct elements problem (that is, p=0𝑝0p=0italic_p = 0). For the special case of approximate counting (that is, p=1𝑝1p=1italic_p = 1), a tight bound of Θ⁢(log⁡log⁡n+log⁡ε−1)Θ𝑛superscript𝜀1\Theta(\log\log n+\log\varepsilon^{-1})roman_Θ ( roman_log roman_log italic_n + roman_log italic_ε start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) is known [NY22]. For the range of p∈[0,1)𝑝01p\in[0,1)italic_p ∈ [ 0 , 1 ), the upper bound of AMS was improved by Jayaram and Woodruff who presented a nearly-tight O~⁢(log⁡n+1/ε2)~𝑂𝑛1superscript𝜀2\tilde{O}\left(\log n+1/\varepsilon^{2}\right)over~ start_ARG italic_O end_ARG ( roman_log italic_n + 1 / italic_ε start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) bound in that range [JW19]. This leaves p∈(1,2]𝑝12p\in(1,2]italic_p ∈ ( 1 , 2 ] as the last remaining range within no nearly-tight bounds are known. For certain generalizations more is known: When the stream is randomly shuffled and given in random order, then in the range p∈(1,2)𝑝12p\in(1,2)italic_p ∈ ( 1 , 2 ) (excluding p=2𝑝2p=2italic_p = 2) [BVWY18] showed an improved upper bound of O~⁢(log⁡n+1/ε2)~𝑂𝑛1superscript𝜀2\tilde{O}\left(\log n+1/\varepsilon^{2}\right)over~ start_ARG italic_O end_ARG ( roman_log italic_n + 1 / italic_ε start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ). When updates are allowed in the stream, that is, elements can also be deleted and not only added to it, then [KNW10a] showed that Θ⁢(log⁡n/ε2)Θ𝑛superscript𝜀2\Theta(\log n/\varepsilon^{2})roman_Θ ( roman_log italic_n / italic_ε start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) is optimal for p≤2𝑝2p\leq 2italic_p ≤ 2. In this work, we settle the space complexity of frequency moment estimation in the entire remaining range of p∈(1,2]𝑝12p\in(1,2]italic_p ∈ ( 1 , 2 ], including the special case of second frequency moment estimation. For p=2𝑝2p=2italic_p = 2, we show that the AMS bound is essentially tight. Theorem. Let 𝒜𝒜\mathcal{A}caligraphic_A be a streaming algorithm that gives an (1±ε)plus-or-minus1𝜀\left(1\pm\varepsilon\right)( 1 ± italic_ε ) multiplicative approximation to the F2subscript𝐹2F_{2}italic_F start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT of its input stream and succeeds with probability ≥2/3absent23\geq 2/3≥ 2 / 3, for some ε=Ω⁢(1/n)𝜀Ω1𝑛\varepsilon=\Omega(1/\sqrt{n})italic_ε = roman_Ω ( 1 / square-root start_ARG italic_n end_ARG ). Then, the space used by 𝒜𝒜\mathcal{A}caligraphic_A is Ω⁢(log⁡(ε2⁢n)/ε2)Ωsuperscript𝜀2𝑛superscript𝜀2\Omega\left(\log\left(\varepsilon^{2}n\right)/\varepsilon^{2}\right)roman_Ω ( roman_log ( italic_ε start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_n ) / italic_ε start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ). Note that the range ε<1/n𝜀1𝑛\varepsilon<1/\sqrt{n}italic_ε < 1 / square-root start_ARG italic_n end_ARG is less interesting as O⁢(min⁡{n⁢log⁡n,|U|})𝑂𝑛𝑛𝑈O\left(\min\{n\log n,|U|\}\right)italic_O ( roman_min { italic_n roman_log italic_n , | italic_U | } ) space suffices for exactly maintaining the vector of frequencies. We observe that in the range where ε𝜀\varepsilonitalic_ε is very close to 1/n1𝑛1/\sqrt{n}1 / square-root start_ARG italic_n end_ARG our lower bound is (slightly) lower than the AMS upper bound, we show that this is inherent by introducing a modification of the AMS algorithm that matches our lower bound in this range. Theorem. For ε=Ω⁢(1/n)𝜀Ω1𝑛\varepsilon=\Omega(1/\sqrt{n})italic_ε = roman_Ω ( 1 / square-root start_ARG italic_n end_ARG ), we can get a (1±ε)plus-or-minus1𝜀(1\pm\varepsilon)( 1 ± italic_ε )-approximation of the F2subscript𝐹2F_{2}italic_F start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT of a stream of length n𝑛nitalic_n using O⁢(log⁡(ε2⁢n)/ε2)𝑂superscript𝜀2𝑛superscript𝜀2O\left(\log\left(\varepsilon^{2}n\right)/\varepsilon^{2}\right)italic_O ( roman_log ( italic_ε start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_n ) / italic_ε start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) space with success probability >2/3absent23>2/3> 2 / 3. 0≤p<10𝑝10\leq p<10 ≤ italic_p < 1 p=1𝑝1p=1italic_p = 1 1<p≤21𝑝21<p\leq 21 < italic_p ≤ 2 p>2𝑝2p>2italic_p > 2 Θ~⁢(log⁡n+1/ε2)~Θ𝑛1superscript𝜀2\tilde{\Theta}(\log n+1/\varepsilon^{2})over~ start_ARG roman_Θ end_ARG ( roman_log italic_n + 1 / italic_ε start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) Θ⁢(log⁡log⁡n+log⁡ε−1)Θ𝑛superscript𝜀1\Theta(\log\log n+\log\varepsilon^{-1})roman_Θ ( roman_log roman_log italic_n + roman_log italic_ε start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) Θ⁢(log⁡n/ε2)Θ𝑛superscript𝜀2\Theta(\log n/\varepsilon^{2})roman_Θ ( roman_log italic_n / italic_ε start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) Θ~⁢(n1−2/p/ε2)~Θsuperscript𝑛12𝑝superscript𝜀2\tilde{\Theta}(n^{1-2/p}/\varepsilon^{2})over~ start_ARG roman_Θ end_ARG ( italic_n start_POSTSUPERSCRIPT 1 - 2 / italic_p end_POSTSUPERSCRIPT / italic_ε start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) Figure 1: Space complexity of Fpsubscript𝐹𝑝F_{p}italic_F start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT-estimation. We also extend our lower bound to the range p∈(1,2]𝑝12p\in(1,2]italic_p ∈ ( 1 , 2 ], which settles the space complexity of Fpsubscript𝐹𝑝F_{p}italic_F start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT-estimation for all values of p𝑝pitalic_p. See Figure 1 for a summary of the space complexity of Fpsubscript𝐹𝑝F_{p}italic_F start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT-estimation for all p≥0𝑝0p\geq 0italic_p ≥ 0. Theorem. Fix p∈(1,2]𝑝12p\in(1,2]italic_p ∈ ( 1 , 2 ]. Let 𝒜𝒜\mathcal{A}caligraphic_A be a streaming algorithm that gives an (1±ε)plus-or-minus1𝜀\left(1\pm\varepsilon\right)( 1 ± italic_ε ) approximation to the Fpsubscript𝐹𝑝F_{p}italic_F start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT of its input stream, for some ε∈(Ω⁢(n−1/p),4−1/(p−1))𝜀Ωsuperscript𝑛1𝑝superscript41𝑝1\varepsilon\in\left(\Omega\left(n^{-1/p}\right),{4^{-1/(p-1)}}\right)italic_ε ∈ ( roman_Ω ( italic_n start_POSTSUPERSCRIPT - 1 / italic_p end_POSTSUPERSCRIPT ) , 4 start_POSTSUPERSCRIPT - 1 / ( italic_p - 1 ) end_POSTSUPERSCRIPT ), and succeeds with probability ≥2/3absent23\geq 2/3≥ 2 / 3. Then, the space used by 𝒜𝒜\mathcal{A}caligraphic_A is Ω⁢(log⁡(ε1/p⁢n)/ε2)Ωsuperscript𝜀1𝑝𝑛superscript𝜀2\Omega\left(\log\left(\varepsilon^{1/p}n\right)/\varepsilon^{2}\right)roman_Ω ( roman_log ( italic_ε start_POSTSUPERSCRIPT 1 / italic_p end_POSTSUPERSCRIPT italic_n ) / italic_ε start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ). Most of the lower bounds for streaming problems are based on reductions from communication complexity. In [JW19], a natural barrier to prove a better than Ω~⁢(1/ε2)~Ω1superscript𝜀2\tilde{\Omega}(1/\varepsilon^{2})over~ start_ARG roman_Ω end_ARG ( 1 / italic_ε start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) lower bound was shown: Even in a very strong model of communication, O⁢(1/ε2⋅(log⁡log⁡n+log⁡d+log⁡ε−1))𝑂⋅1superscript𝜀2𝑛𝑑superscript𝜀1O\left(1/\varepsilon^{2}\cdot\left(\log\log n+\log d+\log\varepsilon^{-1}% \right)\right)italic_O ( 1 / italic_ε start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ⋅ ( roman_log roman_log italic_n + roman_log italic_d + roman_log italic_ε start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) ) bits of communication suffice for the players to correctly produce a Fpsubscript𝐹𝑝F_{p}italic_F start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT estimation, where d𝑑ditalic_d is the diameter of the communication graph. This means that problems who reduce to Fpsubscript𝐹𝑝F_{p}italic_F start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT-estimation have a too low communication complexity to improve the existing lower bounds. To overcome this natural barrier, we present a new type of a direct sum theorem that takes place at the level of the streaming algorithm rather than the level of the communication model — informally, we pack many instances of problems with communication complexity Θ⁢(1/ε2)Θ1superscript𝜀2\Theta(1/\varepsilon^{2})roman_Θ ( 1 / italic_ε start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) into a single stream, and then directly show that a successful streaming algorithm must solve them all. In Section 2 we give a detailed high-level overview of our proofs. The lower bound is presented in Section 4 and Section 5, and then extended from p=2𝑝2p=2italic_p = 2 to p∈(1,2]𝑝12p\in(1,2]italic_p ∈ ( 1 , 2 ] in Section 5.3. The improved upper bound is presented in Section 6. We conclude and present remaining open problems in Section 7."
https://arxiv.org/html/2411.01486v1,The Gap Between Greedy Algorithm and Minimum Multiplicative Spanner,"Given any undirected graph G=(V,E)𝐺𝑉𝐸G=(V,E)italic_G = ( italic_V , italic_E ) with n𝑛nitalic_n vertices, if its subgraph H⊆G𝐻𝐺H\subseteq Gitalic_H ⊆ italic_G satisfies 𝖽𝗂𝗌𝗍H⁢(u,v)≤k⋅𝖽𝗂𝗌𝗍G⁢(u,v)subscript𝖽𝗂𝗌𝗍𝐻𝑢𝑣⋅𝑘subscript𝖽𝗂𝗌𝗍𝐺𝑢𝑣\mathsf{dist}_{H}(u,v)\leq k\cdot\mathsf{dist}_{G}(u,v)sansserif_dist start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ( italic_u , italic_v ) ≤ italic_k ⋅ sansserif_dist start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_u , italic_v ) for any two vertices (u,v)∈V×V𝑢𝑣𝑉𝑉(u,v)\in V\times V( italic_u , italic_v ) ∈ italic_V × italic_V, we call H𝐻Hitalic_H a k𝑘kitalic_k-spanner of G𝐺Gitalic_G. The greedy algorithm adapted from Kruskal’s algorithm is an efficient and folklore way to produce a k𝑘kitalic_k-spanner with girth at least k+2𝑘2k+2italic_k + 2. The greedy algorithm has shown to be ‘existentially optimal’, while it’s not ‘universally optimal’ for any constant k𝑘kitalic_k. Here, ‘universal optimality’ means an algorithm can produce the smallest k𝑘kitalic_k-spanner H𝐻Hitalic_H given any n𝑛nitalic_n-vertex input graph G𝐺Gitalic_G.However, how well the greedy algorithm works compared to ‘universal optimality’ is still unclear for superconstant k:=k⁢(n)assign𝑘𝑘𝑛k:=k(n)italic_k := italic_k ( italic_n ). In this paper, we aim to give a new and fine-grained analysis of this problem in undirected unweighted graph setting. Specifically, we show some bounds on this problem including the following twoOn the negative side, when k<13⁢n−O⁢(1)𝑘13𝑛𝑂1k<\frac{1}{3}n-O(1)italic_k < divide start_ARG 1 end_ARG start_ARG 3 end_ARG italic_n - italic_O ( 1 ), the greedy algorithm is not ‘universally optimal’.On the positive side, when k>23⁢n+O⁢(1)𝑘23𝑛𝑂1k>\frac{2}{3}n+O(1)italic_k > divide start_ARG 2 end_ARG start_ARG 3 end_ARG italic_n + italic_O ( 1 ), the greedy algorithm is ‘universally optimal’.We also introduce an appropriate notion for ‘approximately universal optimality’. An algorithm is (α,β)𝛼𝛽(\alpha,\beta)( italic_α , italic_β )-universally optimal iff given any n𝑛nitalic_n-vertex input graph G𝐺Gitalic_G, it can produce a k𝑘kitalic_k-spanner H𝐻Hitalic_H of G𝐺Gitalic_G with size |H|≤n+α⁢(|H∗|−n)+β𝐻𝑛𝛼superscript𝐻𝑛𝛽|H|\leq n+\alpha(|H^{*}|-n)+\beta| italic_H | ≤ italic_n + italic_α ( | italic_H start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT | - italic_n ) + italic_β, where H∗superscript𝐻H^{*}italic_H start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT is the smallest k𝑘kitalic_k-spanner of G𝐺Gitalic_G. We show the following positive bounds.When k>47⁢n+O⁢(1)𝑘47𝑛𝑂1k>\frac{4}{7}n+O(1)italic_k > divide start_ARG 4 end_ARG start_ARG 7 end_ARG italic_n + italic_O ( 1 ), the greedy algorithm is (2,O⁢(1))2𝑂1(2,O(1))( 2 , italic_O ( 1 ) )-universally optimal.When k>1223⁢n+O⁢(1)𝑘1223𝑛𝑂1k>\frac{12}{23}n+O(1)italic_k > divide start_ARG 12 end_ARG start_ARG 23 end_ARG italic_n + italic_O ( 1 ), the greedy algorithm is (18,O⁢(1))18𝑂1(18,O(1))( 18 , italic_O ( 1 ) )-universally optimal.When k>12⁢n+O⁢(1)𝑘12𝑛𝑂1k>\frac{1}{2}n+O(1)italic_k > divide start_ARG 1 end_ARG start_ARG 2 end_ARG italic_n + italic_O ( 1 ), the greedy algorithm is (32,O⁢(1))32𝑂1(32,O(1))( 32 , italic_O ( 1 ) )-universally optimal.All our proofs are constructive building on new structural analysis on spanners. We give some ideas about how to break small cycles in a spanner to increase the girth. These ideas may help us to understand the relation between girth and spanners.","A spanner of a graph is a subgraph that approximately preserves distances between all vertex pairs in the original graph. This notion was introduced by [PS89] and widely used in many areas such as graph algorithms [CKL+22, DJWW22], distributed networks [Awe85, BCLR86] and chip design [CKR+91, CKR+92]. In most applications, people aim to construct spanners as sparse as possible. The sparsity is measured by the number of edges in the spanner. In this paper, we will focus on multiplicative spanners for undirected unweighted graphs defined below. Unless otherwise stated, all graphs in this paper are undirected unweighted graphs. Definition 1.1. Given k:=k⁢(n)assign𝑘𝑘𝑛k:=k(n)italic_k := italic_k ( italic_n ) and an undirected unweighted graph G:=(V,E)assign𝐺𝑉𝐸G:=(V,E)italic_G := ( italic_V , italic_E ) with n𝑛nitalic_n vertices, a k𝑘kitalic_k-spanner of G𝐺Gitalic_G is a subgraph H=(V,E′),E′⊆Eformulae-sequence𝐻𝑉superscript𝐸′superscript𝐸′𝐸H=(V,E^{\prime}),E^{\prime}\subseteq Eitalic_H = ( italic_V , italic_E start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) , italic_E start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ⊆ italic_E of G𝐺Gitalic_G such that for any vertex pair (u,v)∈V×V𝑢𝑣𝑉𝑉(u,v)\in V\times V( italic_u , italic_v ) ∈ italic_V × italic_V, there is 𝖽𝗂𝗌𝗍H⁢(u,v)≤k⋅𝖽𝗂𝗌𝗍G⁢(u,v)subscript𝖽𝗂𝗌𝗍𝐻𝑢𝑣⋅𝑘subscript𝖽𝗂𝗌𝗍𝐺𝑢𝑣\mathsf{dist}_{H}(u,v)\leq k\cdot\mathsf{dist}_{G}(u,v)sansserif_dist start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ( italic_u , italic_v ) ≤ italic_k ⋅ sansserif_dist start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_u , italic_v ). We call k𝑘kitalic_k the stretch of H𝐻Hitalic_H. One of the simplest and most widely-used spanner construction algorithms is the greedy algorithm introduced by [ADD+93]. It is adapted from Kruskal’s algorithm for computing the minimum spanning tree. In undirected unweighted settings, the greedy algorithm runs like this: Given any input graph G=(V,E)𝐺𝑉𝐸G=(V,E)italic_G = ( italic_V , italic_E ) and stretch k𝑘kitalic_k, we first choose an arbitrary total order σ𝜎\sigmaitalic_σ on its edges, denoted by σ⁢(E)=(e1<e2<⋯<em)𝜎𝐸subscript𝑒1subscript𝑒2⋯subscript𝑒𝑚\sigma(E)=(e_{1}<e_{2}<\dots<e_{m})italic_σ ( italic_E ) = ( italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT < italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT < ⋯ < italic_e start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ). Then, starting from empty subgraph H=(V,∅)𝐻𝑉H=(V,\varnothing)italic_H = ( italic_V , ∅ ), we proceed edges {ei}i∈[m]subscriptsubscript𝑒𝑖𝑖delimited-[]𝑚\{e_{i}\}_{i\in[m]}{ italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i ∈ [ italic_m ] end_POSTSUBSCRIPT one by one. For each edge ei=(u,v)subscript𝑒𝑖𝑢𝑣e_{i}=(u,v)italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = ( italic_u , italic_v ), if 𝖽𝗂𝗌𝗍H⁢(u,v)>ksubscript𝖽𝗂𝗌𝗍𝐻𝑢𝑣𝑘\mathsf{dist}_{H}(u,v)>ksansserif_dist start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ( italic_u , italic_v ) > italic_k, we add eisubscript𝑒𝑖e_{i}italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT to edgeset of H𝐻Hitalic_H. Otherwise, we do nothing. After checking all m𝑚mitalic_m edges, we output the latest H𝐻Hitalic_H as the spanner. [ADD+93] proved the following fact about H𝐻Hitalic_H Proposition 1.2. The subgraph H𝐻Hitalic_H outputted by the greedy algorithm with input ⟨G,k⟩𝐺𝑘\langle G,k\rangle⟨ italic_G , italic_k ⟩ is a k𝑘kitalic_k-spanner of G𝐺Gitalic_G. Moreover, the girth of H𝐻Hitalic_H is at least k+2𝑘2k+2italic_k + 2. Proposition 1.2 shows that for any graph G=(V,E)𝐺𝑉𝐸G=(V,E)italic_G = ( italic_V , italic_E ) with n𝑛nitalic_n vertices and k𝑘kitalic_k, we can use the greedy algorithm to compute one of its k𝑘kitalic_k-spanners with girth at least k+2𝑘2k+2italic_k + 2. A simple observation is that the backward direction also holds. Concretely, we have the following Proposition 1.3. Given any graph G=(V,E)𝐺𝑉𝐸G=(V,E)italic_G = ( italic_V , italic_E ) with n𝑛nitalic_n vertices, stretch k𝑘kitalic_k and any of its k𝑘kitalic_k-spanner H𝐻Hitalic_H with girth at least k+2𝑘2k+2italic_k + 2, there exists an edge ordering σ𝜎\sigmaitalic_σ of E𝐸Eitalic_E such that the greedy algorithm outputs H𝐻Hitalic_H after running on σ𝜎\sigmaitalic_σ, with input ⟨G,k⟩𝐺𝑘\langle G,k\rangle⟨ italic_G , italic_k ⟩. Proof. Let H=(V,E′⊆E)𝐻𝑉superscript𝐸′𝐸H=(V,E^{\prime}\subseteq E)italic_H = ( italic_V , italic_E start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ⊆ italic_E ). Consider the edge ordering σ𝜎\sigmaitalic_σ on E𝐸Eitalic_E such that all edges in E′superscript𝐸′E^{\prime}italic_E start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT appear earlier in σ𝜎\sigmaitalic_σ than any edge in E\E′\𝐸superscript𝐸′E\backslash E^{\prime}italic_E \ italic_E start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT. Let H′superscript𝐻′H^{\prime}italic_H start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT be the spanner outputted by the greedy algorithm running on σ𝜎\sigmaitalic_σ. Since we first proceed with edges in E′superscript𝐸′E^{\prime}italic_E start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT, all edges in E′superscript𝐸′E^{\prime}italic_E start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT will be in H′superscript𝐻′H^{\prime}italic_H start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT by the girth assumption. For any edge e=(u,v)∈E\E′𝑒𝑢𝑣\𝐸superscript𝐸′e=(u,v)\in E\backslash E^{\prime}italic_e = ( italic_u , italic_v ) ∈ italic_E \ italic_E start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT, since H𝐻Hitalic_H is a k𝑘kitalic_k-spanner of G𝐺Gitalic_G, there must be 𝖽𝗂𝗌𝗍H⁢(u,v)≤ksubscript𝖽𝗂𝗌𝗍𝐻𝑢𝑣𝑘\mathsf{dist}_{H}(u,v)\leq ksansserif_dist start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ( italic_u , italic_v ) ≤ italic_k, and therefore e𝑒eitalic_e won’t be added in H′superscript𝐻′H^{\prime}italic_H start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT after all edges in E′superscript𝐸′E^{\prime}italic_E start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT have been added. We conclude H′=Hsuperscript𝐻′𝐻H^{\prime}=Hitalic_H start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = italic_H. ∎ Proposition 1.3 shows that: Given ⟨G=(V,E),k⟩delimited-⟨⟩𝐺𝑉𝐸𝑘\langle G=(V,E),k\rangle⟨ italic_G = ( italic_V , italic_E ) , italic_k ⟩, the set of subgraphs the greedy algorithm can output (by choosing arbitrary edge ordering) is exactly the set of k𝑘kitalic_k-spanners with girth at least k+2𝑘2k+2italic_k + 2. By the Moore bound [AHL02], all graphs H𝐻Hitalic_H with girth at least k+2𝑘2k+2italic_k + 2 and n𝑛nitalic_n vertices have at most m=O⁢(n1+2/k)𝑚𝑂superscript𝑛12𝑘m=O(n^{1+2/k})italic_m = italic_O ( italic_n start_POSTSUPERSCRIPT 1 + 2 / italic_k end_POSTSUPERSCRIPT ) edges. Therefore, we can get a size upper bound of k𝑘kitalic_k-spanners possibly outputted by the greedy algorithm. There are also papers studying this upper bound on weighted graph setting [ADD+93, CDNS95, ENS14, Bod24]. Then, a natural question arises: How large is the gap between the minimum (optimal) spanners and spanners outputted by the greedy algorithm? Here, the minimum k𝑘kitalic_k-spanner of a graph G𝐺Gitalic_G is its k𝑘kitalic_k-spanner with the fewest edges. In a seminal work [FS20], it is shown that the greedy algorithm is existentially optimal. Namely, for any n,k𝑛𝑘n,kitalic_n , italic_k, they show that there exists an n𝑛nitalic_n-vertex graph G𝐺Gitalic_G and its minimum k𝑘kitalic_k-spanner H∗superscript𝐻H^{*}italic_H start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT such that any k𝑘kitalic_k-spanner H𝐻Hitalic_H outputted by the greedy algorithm running on any n𝑛nitalic_n-vertex input graph is no larger than H∗superscript𝐻H^{*}italic_H start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT. A more classical and difficult notion is universally optimal. It requires the greedy algorithm to output a minimum k𝑘kitalic_k-spanner for any n𝑛nitalic_n-vertex graph. However, [FS20] also shows that for many parameter settings (n,k)𝑛𝑘(n,k)( italic_n , italic_k ), the greedy algorithm for weighted graphs is ‘far from’ universally optimal on stretch k𝑘kitalic_k and n𝑛nitalic_n-vertex graphs. It means that the greedy algorithm cannot be universally optimal on general parameters. On the other hand, there is still hope that the greedy algorithm is universally optimal on some specific parameters (n,k)𝑛𝑘(n,k)( italic_n , italic_k ). In fact, a simple observation is that when we set k=n−1𝑘𝑛1k=n-1italic_k = italic_n - 1, the k𝑘kitalic_k-spanners of n𝑛nitalic_n-vertex graphs are actually equivalent to their reachability preservers, and we can assert that the greedy algorithm is universally optimal on parameters (n,k=n−1)𝑛𝑘𝑛1(n,k=n-1)( italic_n , italic_k = italic_n - 1 ) since reachability preservers are just spanning trees, which can be outputted by the greedy algorithm. It is natural to ask: for what parameters (n,k)𝑛𝑘(n,k)( italic_n , italic_k ), the greedy algorithm is universally optimal? That is, it outputs a minimum k𝑘kitalic_k-spanner for any n𝑛nitalic_n-vertex graphs. In this paper, we will try to partially answer this question. 1.1 Our Results We first define some terms, which is closely related to the performance of the greedy algorithm. Definition 1.4. For any pair (n,k)∈ℕ×ℕ𝑛𝑘ℕℕ(n,k)\in\mathbb{N}\times\mathbb{N}( italic_n , italic_k ) ∈ blackboard_N × blackboard_N, we define: (a) If for any n𝑛nitalic_n-vertex graph, all of its minimum k𝑘kitalic_k-spanners have girth at least k+2𝑘2k+2italic_k + 2, we call (n,k)𝑛𝑘(n,k)( italic_n , italic_k ) an extremely good pair. (b) If for any n𝑛nitalic_n-vertex graph, at least one of its minimum k𝑘kitalic_k-spanners has girth at least k+2𝑘2k+2italic_k + 2, we call (n,k)𝑛𝑘(n,k)( italic_n , italic_k ) a good pair. (c) If for any n𝑛nitalic_n-vertex graph G𝐺Gitalic_G, all of its minimum k𝑘kitalic_k-spanners can be outputted by the greedy algorithm on some edge ordering σ𝜎\sigmaitalic_σ, we say the greedy algorithm is completely universally optimal on (n,k)𝑛𝑘(n,k)( italic_n , italic_k ). (d) If for any n𝑛nitalic_n-vertex graph G𝐺Gitalic_G, there exists one of its minimum k𝑘kitalic_k-spanners that can be outputted by the greedy algorithm on some edge ordering σ𝜎\sigmaitalic_σ, we say the greedy algorithm is universally optimal on (n,k)𝑛𝑘(n,k)( italic_n , italic_k ). From Proposition 1.3, we can connect the above notions Corollary 1.5. For any pair (n,k)∈ℕ×ℕ𝑛𝑘ℕℕ(n,k)\in\mathbb{N}\times\mathbb{N}( italic_n , italic_k ) ∈ blackboard_N × blackboard_N, we have: If (n,k)𝑛𝑘(n,k)( italic_n , italic_k ) is extremely good, then the greedy algorithm is completely universal optimal on (n,k)𝑛𝑘(n,k)( italic_n , italic_k ). If (n,k)𝑛𝑘(n,k)( italic_n , italic_k ) is good, then the greedy algorithm is universally optimal on (n,k)𝑛𝑘(n,k)( italic_n , italic_k ). By Corollary 1.5, if we want to argue (completely) universal optimality on specific (n,k)𝑛𝑘(n,k)( italic_n , italic_k ), it suffices to identify the category of (n,k)𝑛𝑘(n,k)( italic_n , italic_k ): extremely good, good or ‘not even good’. Lower Bound First, we give a lower (negative) bound. It asserts that if k𝑘kitalic_k is too small compared to n𝑛nitalic_n, then (n,k)𝑛𝑘(n,k)( italic_n , italic_k ) is not even a good pair. Theorem 1.6. For every sufficiently large n𝑛nitalic_n and O⁢(1)<k<13⁢n−O⁢(1)𝑂1𝑘13𝑛𝑂1O(1)<k<\frac{1}{3}n-O(1)italic_O ( 1 ) < italic_k < divide start_ARG 1 end_ARG start_ARG 3 end_ARG italic_n - italic_O ( 1 ), (n,k)𝑛𝑘(n,k)( italic_n , italic_k ) is not a good pair. In fact, the proof of Theorem 1.6 is constructive: We can construct a graph G𝐺Gitalic_G with n𝑛nitalic_n vertices such that all of its minimum k𝑘kitalic_k-spanners have girth at most k+1𝑘1k+1italic_k + 1. Upper Bound Extremely good (n,k)𝑛𝑘(n,k)( italic_n , italic_k ) is the strongest definition, so we first find an upper (positive) bound about it. Our proof is algorithmic. Concretely, if k𝑘kitalic_k is large enough compared to n𝑛nitalic_n, for any k𝑘kitalic_k-spanner H𝐻Hitalic_H of n𝑛nitalic_n-vertex graph G𝐺Gitalic_G with girth at most k+1𝑘1k+1italic_k + 1, we can algorithmically remove some edges of H𝐻Hitalic_H such that H𝐻Hitalic_H is still a k𝑘kitalic_k-spanner after removal. Theorem 1.7. There is a deterministic polynomial time algorithm A𝐴Aitalic_A such that for all sufficiently large n𝑛nitalic_n and any k>34⁢n+O⁢(1)𝑘34𝑛𝑂1k>\frac{3}{4}n+O(1)italic_k > divide start_ARG 3 end_ARG start_ARG 4 end_ARG italic_n + italic_O ( 1 ), given any n𝑛nitalic_n-vertex graph G𝐺Gitalic_G and its k𝑘kitalic_k-spanner H⊆G𝐻𝐺H\subseteq Gitalic_H ⊆ italic_G, A⁢(G,H,k)𝐴𝐺𝐻𝑘A(G,H,k)italic_A ( italic_G , italic_H , italic_k ) outputs a subgraph R⊆H𝑅𝐻R\subseteq Hitalic_R ⊆ italic_H of H𝐻Hitalic_H such that R𝑅Ritalic_R is a k𝑘kitalic_k-spanner of G𝐺Gitalic_G and R𝑅Ritalic_R has girth at least k+2𝑘2k+2italic_k + 2. This immediately gives us an upper bound for extremely good (n,k)𝑛𝑘(n,k)( italic_n , italic_k ). Corollary 1.8. For all sufficiently large n𝑛nitalic_n and any k>34⁢n+O⁢(1)𝑘34𝑛𝑂1k>\frac{3}{4}n+O(1)italic_k > divide start_ARG 3 end_ARG start_ARG 4 end_ARG italic_n + italic_O ( 1 ), (n,k)𝑛𝑘(n,k)( italic_n , italic_k ) is extremely good. Proof. Suppose by contradiction that for some n𝑛nitalic_n-vertex G𝐺Gitalic_G, it has some minimum k𝑘kitalic_k-spanner H𝐻Hitalic_H with girth at most k+1𝑘1k+1italic_k + 1. Then by Theorem 1.7 we can construct its subgraph R⊆H𝑅𝐻R\subseteq Hitalic_R ⊆ italic_H such that R𝑅Ritalic_R is also a k𝑘kitalic_k-spanner of G𝐺Gitalic_G. Since R𝑅Ritalic_R has strictly larger girth than H𝐻Hitalic_H, R𝑅Ritalic_R must be a strictly smaller k𝑘kitalic_k-spanner of G𝐺Gitalic_G than H𝐻Hitalic_H, which contradicts the assumption that H𝐻Hitalic_H is the minimum k𝑘kitalic_k-spanner. ∎ We can also give an upper bound using a similar but more complicated algorithm for the weaker notion, good pair (n,k)𝑛𝑘(n,k)( italic_n , italic_k ). Theorem 1.9. There is a deterministic polynomial time algorithm A𝐴Aitalic_A such that for all sufficiently large n𝑛nitalic_n and any k>23⁢n+O⁢(1)𝑘23𝑛𝑂1k>\frac{2}{3}n+O(1)italic_k > divide start_ARG 2 end_ARG start_ARG 3 end_ARG italic_n + italic_O ( 1 ), given any n𝑛nitalic_n-vertex graph G𝐺Gitalic_G and its k𝑘kitalic_k-spanner H=(V,EH)⊆G𝐻𝑉subscript𝐸𝐻𝐺H=(V,E_{H})\subseteq Gitalic_H = ( italic_V , italic_E start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ) ⊆ italic_G, A⁢(G,H,k)𝐴𝐺𝐻𝑘A(G,H,k)italic_A ( italic_G , italic_H , italic_k ) outputs R=(V,ER)⊆G𝑅𝑉subscript𝐸𝑅𝐺R=(V,E_{R})\subseteq Gitalic_R = ( italic_V , italic_E start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT ) ⊆ italic_G such that R𝑅Ritalic_R is a k𝑘kitalic_k-spanner of G𝐺Gitalic_G with girth at least k+2𝑘2k+2italic_k + 2. Moreover, the size of R𝑅Ritalic_R satisfies |ER|≤|EH|subscript𝐸𝑅subscript𝐸𝐻|E_{R}|\leq|E_{H}|| italic_E start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT | ≤ | italic_E start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT |. Similarly, this immediately gives us an upper bound for good pairs Corollary 1.10. For all sufficiently large n𝑛nitalic_n and any k>23⁢n+O⁢(1)𝑘23𝑛𝑂1k>\frac{2}{3}n+O(1)italic_k > divide start_ARG 2 end_ARG start_ARG 3 end_ARG italic_n + italic_O ( 1 ), (n,k)𝑛𝑘(n,k)( italic_n , italic_k ) is good. Approximately Universally Optimal By the previous results, we have known whether the greedy algorithm is universally optimal when k<13⁢n−O⁢(1)𝑘13𝑛𝑂1k<\frac{1}{3}n-O(1)italic_k < divide start_ARG 1 end_ARG start_ARG 3 end_ARG italic_n - italic_O ( 1 ) or k>23⁢n+O⁢(1)𝑘23𝑛𝑂1k>\frac{2}{3}n+O(1)italic_k > divide start_ARG 2 end_ARG start_ARG 3 end_ARG italic_n + italic_O ( 1 ). It remains open when 13⁢n<k<23⁢n13𝑛𝑘23𝑛\frac{1}{3}n<k<\frac{2}{3}ndivide start_ARG 1 end_ARG start_ARG 3 end_ARG italic_n < italic_k < divide start_ARG 2 end_ARG start_ARG 3 end_ARG italic_n. To understand the gap between the greedy algorithm and minimum spanners in this range, we need to introduce a notion called ‘approximately universally optimal’. Since we actually have a polynomial time algorithm in Theorem 1.9, we can consider slacking the size requirement of outputted spanner R𝑅Ritalic_R. Concretely, rather than requiring |ER|≤|EH|subscript𝐸𝑅subscript𝐸𝐻|E_{R}|\leq|E_{H}|| italic_E start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT | ≤ | italic_E start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT |, we can output a k𝑘kitalic_k-spanner R𝑅Ritalic_R with girth at least k+2𝑘2k+2italic_k + 2 that is just ‘slightly’ larger than H𝐻Hitalic_H. In the classical approximation algorithm setting, we use a multiplicative slack factor. Formally, Suppose H𝐻Hitalic_H is the minimum k𝑘kitalic_k-spanner, if our outputted large-girth k𝑘kitalic_k-spanner R𝑅Ritalic_R has size |ER|≤α⁢|EH|subscript𝐸𝑅𝛼subscript𝐸𝐻|E_{R}|\leq\alpha|E_{H}|| italic_E start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT | ≤ italic_α | italic_E start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT |, we can call it α𝛼\alphaitalic_α-optimal. However, there is a critical issue about the multiplicative approximation factor in our setting. Since we are considering the case when k>Ω⁢(n)𝑘Ω𝑛k>\Omega(n)italic_k > roman_Ω ( italic_n ), by Moore bound [AHL02] the classical greedy algorithm will always give a k𝑘kitalic_k-spanner with girth at least k+2𝑘2k+2italic_k + 2 and size at most n1+O⁢(1/n)=n+O⁢(log⁡n)superscript𝑛1𝑂1𝑛𝑛𝑂𝑛n^{1+O(1/n)}=n+O(\log{n})italic_n start_POSTSUPERSCRIPT 1 + italic_O ( 1 / italic_n ) end_POSTSUPERSCRIPT = italic_n + italic_O ( roman_log italic_n ). Since a trivial lower bound on the size of minimum k𝑘kitalic_k-spanner H=(V,EH)𝐻𝑉subscript𝐸𝐻H=(V,E_{H})italic_H = ( italic_V , italic_E start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ) of a connected graph is |EH|≥n−1subscript𝐸𝐻𝑛1|E_{H}|\geq n-1| italic_E start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT | ≥ italic_n - 1, any constant multiplicative approximation factor α=1+Ω⁢(1)𝛼1Ω1\alpha=1+\Omega(1)italic_α = 1 + roman_Ω ( 1 ) will become trivial since the well-known greedy algorithm always outputs a k𝑘kitalic_k-spanner with size n+o⁢(n)≤(1+o⁢(1))⁢|EH|≤α⁢|EH|𝑛𝑜𝑛1𝑜1subscript𝐸𝐻𝛼subscript𝐸𝐻n+o(n)\leq(1+o(1))|E_{H}|\leq\alpha|E_{H}|italic_n + italic_o ( italic_n ) ≤ ( 1 + italic_o ( 1 ) ) | italic_E start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT | ≤ italic_α | italic_E start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT |. These spanners are usually called ultrasparse spanners [Pet08, BEG+22]. Therefore, we must introduce a more refined notion of ‘approximation’. The first observation is that since different connected components are isolated when considering spanners, we can only focus on connected graphs without loss of generality. For any connected n𝑛nitalic_n-vertex graph G=(V,E)𝐺𝑉𝐸G=(V,E)italic_G = ( italic_V , italic_E ) and its minimum k𝑘kitalic_k-spanner H=(V,EH)𝐻𝑉subscript𝐸𝐻H=(V,E_{H})italic_H = ( italic_V , italic_E start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ), since |EH|≥n−1subscript𝐸𝐻𝑛1|E_{H}|\geq n-1| italic_E start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT | ≥ italic_n - 1 is a trivial lower bound, we can use α=|ER|−n|EH|−n𝛼subscript𝐸𝑅𝑛subscript𝐸𝐻𝑛\alpha=\frac{|E_{R}|-n}{|E_{H}|-n}italic_α = divide start_ARG | italic_E start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT | - italic_n end_ARG start_ARG | italic_E start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT | - italic_n end_ARG as a new definition for the approximation factor, where R=(V,ER)𝑅𝑉subscript𝐸𝑅R=(V,E_{R})italic_R = ( italic_V , italic_E start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT ) denotes the k𝑘kitalic_k-spanner of G𝐺Gitalic_G outputted by some approximation algorithm. Definition 1.11. For any pair (n,k)𝑛𝑘(n,k)( italic_n , italic_k ) and constants α>1,β≥0formulae-sequence𝛼1𝛽0\alpha>1,\beta\geq 0italic_α > 1 , italic_β ≥ 0, if any n𝑛nitalic_n-vertex connected graph G=(V,E)𝐺𝑉𝐸G=(V,E)italic_G = ( italic_V , italic_E ) has a k𝑘kitalic_k-spanner H=(V,EH)𝐻𝑉subscript𝐸𝐻H=(V,E_{H})italic_H = ( italic_V , italic_E start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ) with girth at least k+2𝑘2k+2italic_k + 2 such that |EH|−n≤α⁢(𝖮𝖯𝖳−n)+βsubscript𝐸𝐻𝑛𝛼𝖮𝖯𝖳𝑛𝛽|E_{H}|-n\leq\alpha(\mathsf{OPT}-n)+\beta| italic_E start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT | - italic_n ≤ italic_α ( sansserif_OPT - italic_n ) + italic_β, we call (n,k)𝑛𝑘(n,k)( italic_n , italic_k ) an (α,β)𝛼𝛽(\alpha,\beta)( italic_α , italic_β )-approx good pair. Here 𝖮𝖯𝖳𝖮𝖯𝖳\mathsf{OPT}sansserif_OPT denotes the number of edges in any minimum k𝑘kitalic_k-spanner of G𝐺Gitalic_G. We can observe that the fact (n,k)𝑛𝑘(n,k)( italic_n , italic_k ) is (α,β)𝛼𝛽(\alpha,\beta)( italic_α , italic_β )-approx good is equivalent to ‘(α,β)𝛼𝛽(\alpha,\beta)( italic_α , italic_β )-approximately universal optimality’ of the greedy algorithm. Namely, the greedy algorithm is ‘(α,β)𝛼𝛽(\alpha,\beta)( italic_α , italic_β )-approximately universally optimal’ on (n,k)𝑛𝑘(n,k)( italic_n , italic_k ) iff for any n𝑛nitalic_n-vertex graph G𝐺Gitalic_G, the greedy algorithm can output a (α,β)𝛼𝛽(\alpha,\beta)( italic_α , italic_β )-approximately minimum k𝑘kitalic_k-spanner of G𝐺Gitalic_G on some edge ordering σ𝜎\sigmaitalic_σ. Using the above definitions, we can try to understand the power of the greedy algorithm when 13⁢n≤k≤23⁢n13𝑛𝑘23𝑛\frac{1}{3}n\leq k\leq\frac{2}{3}ndivide start_ARG 1 end_ARG start_ARG 3 end_ARG italic_n ≤ italic_k ≤ divide start_ARG 2 end_ARG start_ARG 3 end_ARG italic_n. The first bound is the following Theorem 1.12. There is a deterministic polynomial time algorithm A𝐴Aitalic_A such that for all sufficiently large n𝑛nitalic_n and any k>47⁢n+O⁢(1)𝑘47𝑛𝑂1k>\frac{4}{7}n+O(1)italic_k > divide start_ARG 4 end_ARG start_ARG 7 end_ARG italic_n + italic_O ( 1 ), given any n𝑛nitalic_n-vertex graph G𝐺Gitalic_G and its k𝑘kitalic_k-spanner H=(V,EH)⊆G𝐻𝑉subscript𝐸𝐻𝐺H=(V,E_{H})\subseteq Gitalic_H = ( italic_V , italic_E start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ) ⊆ italic_G, A⁢(G,H,k)𝐴𝐺𝐻𝑘A(G,H,k)italic_A ( italic_G , italic_H , italic_k ) outputs a k𝑘kitalic_k-spanner R=(V,ER)𝑅𝑉subscript𝐸𝑅R=(V,E_{R})italic_R = ( italic_V , italic_E start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT ) of G𝐺Gitalic_G with girth at least k+2𝑘2k+2italic_k + 2. Moreover, the size of R𝑅Ritalic_R satisfies |ER|−n≤2⁢(E⁢(H)−n)+1subscript𝐸𝑅𝑛2𝐸𝐻𝑛1|E_{R}|-n\leq 2(E(H)-n)+1| italic_E start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT | - italic_n ≤ 2 ( italic_E ( italic_H ) - italic_n ) + 1. As Corollaries 1.5 and 1.8, we can use Theorem 1.12 to derive the following Corollary 1.13. For all sufficiently large n𝑛nitalic_n and every k>47⁢n+O⁢(1)𝑘47𝑛𝑂1k>\frac{4}{7}n+O(1)italic_k > divide start_ARG 4 end_ARG start_ARG 7 end_ARG italic_n + italic_O ( 1 ), (n,k)𝑛𝑘(n,k)( italic_n , italic_k ) is (2,O⁢(1))2𝑂1(2,O(1))( 2 , italic_O ( 1 ) )-approx good. We also generalize Theorem 1.12 (but use a less fine-grained way to analyze it) to smaller k𝑘kitalic_k but with a larger approximation factor as follows: Theorem 1.14. For t∈{1,2,3,4}𝑡1234t\in\{1,2,3,4\}italic_t ∈ { 1 , 2 , 3 , 4 }, there is a deterministic polynomial time algorithm A𝐴Aitalic_A such that for all sufficiently large n𝑛nitalic_n and any k>4⁢t9⁢t−4⁢n+O⁢(1)𝑘4𝑡9𝑡4𝑛𝑂1k>\frac{4t}{9t-4}n+O(1)italic_k > divide start_ARG 4 italic_t end_ARG start_ARG 9 italic_t - 4 end_ARG italic_n + italic_O ( 1 ), given any n𝑛nitalic_n-vertex graph G𝐺Gitalic_G and its k𝑘kitalic_k-spanner H=(V,EH)⊆G𝐻𝑉subscript𝐸𝐻𝐺H=(V,E_{H})\subseteq Gitalic_H = ( italic_V , italic_E start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ) ⊆ italic_G, A⁢(G,H,k)𝐴𝐺𝐻𝑘A(G,H,k)italic_A ( italic_G , italic_H , italic_k ) outputs a k𝑘kitalic_k-spanner R=(V,ER)𝑅𝑉subscript𝐸𝑅R=(V,E_{R})italic_R = ( italic_V , italic_E start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT ) of G𝐺Gitalic_G with girth at least k+2𝑘2k+2italic_k + 2. Moreover, the size of R𝑅Ritalic_R satisfies |ER|−n≤2⁢t2⁢(E⁢(H)−n)+2⁢t2subscript𝐸𝑅𝑛2superscript𝑡2𝐸𝐻𝑛2superscript𝑡2|E_{R}|-n\leq 2t^{2}(E(H)-n)+2t^{2}| italic_E start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT | - italic_n ≤ 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_E ( italic_H ) - italic_n ) + 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. When t=2𝑡2t=2italic_t = 2, the requirement for k𝑘kitalic_k is just k>47⁢n+O⁢(1)𝑘47𝑛𝑂1k>\frac{4}{7}n+O(1)italic_k > divide start_ARG 4 end_ARG start_ARG 7 end_ARG italic_n + italic_O ( 1 ) which is the same as Theorem 1.12, but the approximation factor 2⁢t2=82superscript𝑡282t^{2}=82 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 8 is worse. Theorem 1.14 is useful when t={3,4}𝑡34t=\{3,4\}italic_t = { 3 , 4 }, which gives us k>1223⁢n+O⁢(1)𝑘1223𝑛𝑂1k>\frac{12}{23}n+O(1)italic_k > divide start_ARG 12 end_ARG start_ARG 23 end_ARG italic_n + italic_O ( 1 ) and k>12⁢n+O⁢(1)𝑘12𝑛𝑂1k>\frac{1}{2}n+O(1)italic_k > divide start_ARG 1 end_ARG start_ARG 2 end_ARG italic_n + italic_O ( 1 ) bounds respectively. In fact, Theorem 1.14 is a ‘bucket-decomposition’ generalized version of Theorem 1.12, and t𝑡titalic_t is a decomposition parameter that can be set to any positive integer. However, setting t≥5𝑡5t\geq 5italic_t ≥ 5 cannot derive new approximation bounds so larger t𝑡titalic_t is useless in the current technique. We give a detailed explanation in Remark 4.8. Corollary 1.15. For t∈{1,2,3,4}𝑡1234t\in\{1,2,3,4\}italic_t ∈ { 1 , 2 , 3 , 4 }, for all sufficiently large n𝑛nitalic_n and any k>4⁢t9⁢t−4⁢n+O⁢(1)𝑘4𝑡9𝑡4𝑛𝑂1k>\frac{4t}{9t-4}n+O(1)italic_k > divide start_ARG 4 italic_t end_ARG start_ARG 9 italic_t - 4 end_ARG italic_n + italic_O ( 1 ), (n,k)𝑛𝑘(n,k)( italic_n , italic_k ) is (2⁢t2,O⁢(1))2superscript𝑡2𝑂1(2t^{2},O(1))( 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , italic_O ( 1 ) )-approx good. Remark 1.16. All proofs of our upper (positive) bounds are ‘partially constructive’. Informally, Theorems 1.7, 1.9, 1.12 and 1.14 all provide algorithms to enlarge the girth of an input k𝑘kitalic_k-spanner H𝐻Hitalic_H to at least k+2𝑘2k+2italic_k + 2, and guarantee the outputted graph is still a k𝑘kitalic_k-spanner whose size won’t become too large compared to the size of H𝐻Hitalic_H. These algorithms can be appended behind some other good algorithms that don’t care about girth as ‘girth enlarger’. For example, suppose there is an algorithm B𝐵Bitalic_B that constructs very sparse spanners but doesn’t guarantee the girth of the outputted spanner, we can feed its output spanner into our algorithms A𝐴Aitalic_A such that the final spanner outputted by A𝐴Aitalic_A has large girth, and at the same time it is only ‘slightly’ larger than B𝐵Bitalic_B’s original output. Figure 1: Different Bounds To the best of our knowledge, our proofs are built upon new ideas of analyzing the structure of spanners using the girth information. We hope these ideas could help us to understand the relation between girth and sparse spanners. Figure 1 is a summary of our main results. 1.2 Notations For any n∈ℕ𝑛ℕn\in\mathbb{N}italic_n ∈ blackboard_N, let [n]={1,…,n}delimited-[]𝑛1…𝑛[n]=\{1,\dots,n\}[ italic_n ] = { 1 , … , italic_n }. For any undirected graph G𝐺Gitalic_G, we use VGsubscript𝑉𝐺V_{G}italic_V start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT and EGsubscript𝐸𝐺E_{G}italic_E start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT to denote its vertex-set and edge-set unless otherwise stated. For any (u,v)∈VG×VG𝑢𝑣subscript𝑉𝐺subscript𝑉𝐺(u,v)\in V_{G}\times V_{G}( italic_u , italic_v ) ∈ italic_V start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT × italic_V start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT, we use 𝖽𝗂𝗌𝗍G⁢(u,v)subscript𝖽𝗂𝗌𝗍𝐺𝑢𝑣\mathsf{dist}_{G}(u,v)sansserif_dist start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_u , italic_v ) to denote the length of the shortest path between u,v𝑢𝑣u,vitalic_u , italic_v in G𝐺Gitalic_G. For any vertex-subset U⊆VG𝑈subscript𝑉𝐺U\subseteq V_{G}italic_U ⊆ italic_V start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT, we use G⁢[U]𝐺delimited-[]𝑈G[U]italic_G [ italic_U ] to denote the induced subgraph of G𝐺Gitalic_G on U𝑈Uitalic_U. For any two graphs G,H𝐺𝐻G,Hitalic_G , italic_H, we use G\H\𝐺𝐻G\backslash Hitalic_G \ italic_H to denote the graph (VG\VH,EG\EH)\subscript𝑉𝐺subscript𝑉𝐻\subscript𝐸𝐺subscript𝐸𝐻(V_{G}\backslash V_{H},E_{G}\backslash E_{H})( italic_V start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT \ italic_V start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT , italic_E start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT \ italic_E start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ). We can similarly define G\VH:=(VG\VH,EG)assign\𝐺subscript𝑉𝐻\subscript𝑉𝐺subscript𝑉𝐻subscript𝐸𝐺G\backslash V_{H}:=(V_{G}\backslash V_{H},E_{G})italic_G \ italic_V start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT := ( italic_V start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT \ italic_V start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT , italic_E start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ) and G\EH:=(VG,EG\EH)assign\𝐺subscript𝐸𝐻subscript𝑉𝐺\subscript𝐸𝐺subscript𝐸𝐻G\backslash E_{H}:=(V_{G},E_{G}\backslash E_{H})italic_G \ italic_E start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT := ( italic_V start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT , italic_E start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT \ italic_E start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ). For a simple path p𝑝pitalic_p and any two vertices s,t∈Vp𝑠𝑡subscript𝑉𝑝s,t\in V_{p}italic_s , italic_t ∈ italic_V start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT, we use p⁢[s,t]𝑝𝑠𝑡p[s,t]italic_p [ italic_s , italic_t ] to denote the sub-path between s𝑠sitalic_s and t𝑡titalic_t. p⁢[s,t),p⁢(s,t],p⁢(s,t)𝑝𝑠𝑡𝑝𝑠𝑡𝑝𝑠𝑡p[s,t),p(s,t],p(s,t)italic_p [ italic_s , italic_t ) , italic_p ( italic_s , italic_t ] , italic_p ( italic_s , italic_t ) are similarly defined. We use S⁢CG𝑆subscript𝐶𝐺SC_{G}italic_S italic_C start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT to denote an arbitrary smallest cycle of G𝐺Gitalic_G, and LG:=|VS⁢CG|assignsubscript𝐿𝐺subscript𝑉𝑆subscript𝐶𝐺L_{G}:=|V_{SC_{G}}|italic_L start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT := | italic_V start_POSTSUBSCRIPT italic_S italic_C start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT end_POSTSUBSCRIPT |. 1.3 Paper Organization and Overview In Section 2, we give a brief illustration of the negative lower bound regarding the greedy algorithm as stated in Theorem 1.6. Then, in Section 3 we give detailed proofs of positive upper bounds Theorem 1.7 and Theorem 1.9. Finally, we will prove Theorem 1.12 and Theorem 1.14 in Section 4. Let’s give an overview of our proof strategy used in establishing positive upper bounds. Let H𝐻Hitalic_H be a minimum (in fact arbitrary) k𝑘kitalic_k-spanner of G𝐺Gitalic_G and S⁢CH𝑆subscript𝐶𝐻SC_{H}italic_S italic_C start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT denote its smallest cycle. Let LH=|VS⁢CH|subscript𝐿𝐻subscript𝑉𝑆subscript𝐶𝐻L_{H}=|V_{SC_{H}}|italic_L start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT = | italic_V start_POSTSUBSCRIPT italic_S italic_C start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT end_POSTSUBSCRIPT |. If LH≥k+2subscript𝐿𝐻𝑘2L_{H}\geq k+2italic_L start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ≥ italic_k + 2, our girth lower bound has been satisfied and we are done. Otherwise, we want to find a cycle-edge e∈ES⁢CH𝑒subscript𝐸𝑆subscript𝐶𝐻e\in E_{SC_{H}}italic_e ∈ italic_E start_POSTSUBSCRIPT italic_S italic_C start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT end_POSTSUBSCRIPT and at most m𝑚mitalic_m edges e1,…,em∈EGsubscript𝑒1…subscript𝑒𝑚subscript𝐸𝐺e_{1},\dots,e_{m}\in E_{G}italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_e start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ∈ italic_E start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT, such that we can break the smallest cycle S⁢CH𝑆subscript𝐶𝐻SC_{H}italic_S italic_C start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT by removing e𝑒eitalic_e, and then preserve the property of k𝑘kitalic_k-spanner by adding these m𝑚mitalic_m edges. Formally, let H′superscript𝐻′H^{\prime}italic_H start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT denote (H\{e})∪{e1,…,em}\𝐻𝑒subscript𝑒1…subscript𝑒𝑚\left(H\backslash\{e\}\right)\cup\{e_{1},\dots,e_{m}\}( italic_H \ { italic_e } ) ∪ { italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_e start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT }, we want • H′superscript𝐻′H^{\prime}italic_H start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT is a k𝑘kitalic_k-spanner. • e1,…,emsubscript𝑒1…subscript𝑒𝑚e_{1},\dots,e_{m}italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_e start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT don’t create any new cycle with length at most k+1𝑘1k+1italic_k + 1 in H′superscript𝐻′H^{\prime}italic_H start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT compared to H𝐻Hitalic_H. If for some pair (n,k)𝑛𝑘(n,k)( italic_n , italic_k ), the above condition can be met, then the greedy algorithm is (m,O⁢(m))𝑚𝑂𝑚(m,O(m))( italic_m , italic_O ( italic_m ) )-Approximately universally optimal on (n,k)𝑛𝑘(n,k)( italic_n , italic_k ). (See the proof of Theorem 4.4 for details.) Therefore, our goal is to prove the above statement in different m𝑚mitalic_m and (n,k)𝑛𝑘(n,k)( italic_n , italic_k ) settings. First, in Section 3.1 we will show that as long as LH>2⁢(n−k)subscript𝐿𝐻2𝑛𝑘L_{H}>2(n-k)italic_L start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT > 2 ( italic_n - italic_k ), we can achieve m=0𝑚0m=0italic_m = 0 regardless. It suffices to consider the case when LH≤2⁢(n−k)subscript𝐿𝐻2𝑛𝑘L_{H}\leq 2(n-k)italic_L start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ≤ 2 ( italic_n - italic_k ): In Section 3.2, we give a common structural framework required in all following proofs. Then, we work on different parameter settings as follows • In Section 3.3, we will prove when k>34⁢n+O⁢(1)𝑘34𝑛𝑂1k>\frac{3}{4}n+O(1)italic_k > divide start_ARG 3 end_ARG start_ARG 4 end_ARG italic_n + italic_O ( 1 ), we can achieve m=0𝑚0m=0italic_m = 0, and derive Theorem 1.7. • In Section 3.4, we will prove when k>23⁢n+O⁢(1)𝑘23𝑛𝑂1k>\frac{2}{3}n+O(1)italic_k > divide start_ARG 2 end_ARG start_ARG 3 end_ARG italic_n + italic_O ( 1 ), we can achieve m=1𝑚1m=1italic_m = 1, which confirms Theorem 1.9. • In Section 4.1, we will prove when k>47⁢n+O⁢(1)𝑘47𝑛𝑂1k>\frac{4}{7}n+O(1)italic_k > divide start_ARG 4 end_ARG start_ARG 7 end_ARG italic_n + italic_O ( 1 ), we can achieve m=2𝑚2m=2italic_m = 2 and derive Theorem 1.12. The proof strategy is a generalization of Section 3.4 • In Section 4.2, we will prove when k>1223⁢n+O⁢(1)𝑘1223𝑛𝑂1k>\frac{12}{23}n+O(1)italic_k > divide start_ARG 12 end_ARG start_ARG 23 end_ARG italic_n + italic_O ( 1 ), we can achieve m=18𝑚18m=18italic_m = 18, and when k>12⁢n+O⁢(1)𝑘12𝑛𝑂1k>\frac{1}{2}n+O(1)italic_k > divide start_ARG 1 end_ARG start_ARG 2 end_ARG italic_n + italic_O ( 1 ), we can achieve m=32𝑚32m=32italic_m = 32. These two results follow from a ‘bucket-decomposition’ generalized version of Theorem 1.12."
https://arxiv.org/html/2411.01384v1,Near-Optimal Relative Error Streaming Quantile Estimationvia Elastic Compactors,"Computing the approximate quantiles or ranks of a stream is a fundamental task in data monitoring. Given a stream of elements x1,x2,…,xnsubscript𝑥1subscript𝑥2…subscript𝑥𝑛x_{1},x_{2},\dots,x_{n}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT and a query x𝑥xitalic_x, a relative-error quantile estimation algorithm can estimate the rank of x𝑥xitalic_x with respect to the stream, up to a multiplicative ±ϵ⋅rank⁢(x)plus-or-minus⋅italic-ϵrank𝑥\pm\epsilon\cdot\mathrm{rank}(x)± italic_ϵ ⋅ roman_rank ( italic_x ) error. Notably, this requires the sketch to obtain more precise estimates for the ranks of elements on the tails of the distribution, as compared to the additive ±ϵ⁢nplus-or-minusitalic-ϵ𝑛\pm\epsilon n± italic_ϵ italic_n error regime. This is particularly favorable for some practical applications, such as anomaly detection.Previously, the best known algorithms for relative error achieved space O~⁢(ϵ−1⁢log1.5⁡(ϵ⁢n))~𝑂superscriptitalic-ϵ1superscript1.5italic-ϵ𝑛\tilde{O}(\epsilon^{-1}\log^{1.5}(\epsilon n))over~ start_ARG italic_O end_ARG ( italic_ϵ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log start_POSTSUPERSCRIPT 1.5 end_POSTSUPERSCRIPT ( italic_ϵ italic_n ) ) (Cormode, Karnin, Liberty, Thaler, Veselỳ, 2021) and O~⁢(ϵ−2⁢log⁡(ϵ⁢n))~𝑂superscriptitalic-ϵ2italic-ϵ𝑛\tilde{O}(\epsilon^{-2}\log(\epsilon n))over~ start_ARG italic_O end_ARG ( italic_ϵ start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT roman_log ( italic_ϵ italic_n ) ) (Zhang, Lin, Xu, Korn, Wang, 2006). In this work, we present a nearly-optimal streaming algorithm for the relative-error quantile estimation problem using O~⁢(ϵ−1⁢log⁡(ϵ⁢n))~𝑂superscriptitalic-ϵ1italic-ϵ𝑛\tilde{O}(\epsilon^{-1}\log(\epsilon n))over~ start_ARG italic_O end_ARG ( italic_ϵ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log ( italic_ϵ italic_n ) ) space, which almost matches the trivial Ω⁢(ϵ−1⁢log⁡(ϵ⁢n))Ωsuperscriptitalic-ϵ1italic-ϵ𝑛\Omega(\epsilon^{-1}\log(\epsilon n))roman_Ω ( italic_ϵ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log ( italic_ϵ italic_n ) ) space lower bound.To surpass the Ω⁢(ϵ−1⁢log1.5⁡(ϵ⁢n))Ωsuperscriptitalic-ϵ1superscript1.5italic-ϵ𝑛\Omega(\epsilon^{-1}\log^{1.5}(\epsilon n))roman_Ω ( italic_ϵ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log start_POSTSUPERSCRIPT 1.5 end_POSTSUPERSCRIPT ( italic_ϵ italic_n ) ) barrier of the previous approach, our algorithm crucially relies on a new data structure, called an elastic compactor, which can be dynamically resized over the course of the stream. Interestingly, we design a space allocation scheme which adaptively allocates space to each compactor based on the “hardness” of the input stream. This approach allows us to avoid using the maximal space simultaneously for every compactor and facilitates the improvement in the total space complexity.Along the way, we also propose and study a new problem called the Top Quantiles Problem, which only requires the sketch to provide estimates for the ranks of elements in a fixed-length tail of the distribution. This problem serves as an important subproblem in our algorithm, though it is also an interesting problem of its own right.","Learning the distribution of data that are represented as a stream is an important task in streaming data analysis. A concrete problem that captures this task is the streaming quantile estimation problem. Given a stream of elements π=(x1,x2,…,xn)𝜋subscript𝑥1subscript𝑥2…subscript𝑥𝑛\pi=(x_{1},x_{2},\ldots,x_{n})italic_π = ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ), the quantile estimation problem asks us to process the stream, while maintaining a small memory that stores a few input elements, such that at the end of the stream, for any given query y𝑦yitalic_y, the algorithm must output an approximation of the rank of y𝑦yitalic_y in π𝜋\piitalic_π with high probability, i.e., an approximation of rankπ⁢(y):=|{i∈[n]:xi<y}|assignsubscriptrank𝜋𝑦conditional-set𝑖delimited-[]𝑛subscript𝑥𝑖𝑦\mathrm{rank}_{\pi}(y):=\left|\left\{i\in[n]:x_{i}<y\right\}\right|roman_rank start_POSTSUBSCRIPT italic_π end_POSTSUBSCRIPT ( italic_y ) := | { italic_i ∈ [ italic_n ] : italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT < italic_y } |.555In this work, we consider algorithms in comparison-based model, wherein stream elements are drawn from a universe equipped with a total-ordering. At any time, the algorithm may only performed comparisons between any two elements stored in memory, and does not depend on the true value of each element. The problem has been extensively studied [9, 13, 1, 8, 14, 2, 16] when we allow additive error, i.e., the algorithm outputs an estimate rank^π⁢(y)=rankπ⁢(y)±ϵ⁢nsubscript^rank𝜋𝑦plus-or-minussubscriptrank𝜋𝑦italic-ϵ𝑛\widehat{\mathrm{rank}}_{\pi}(y)=\mathrm{rank}_{\pi}(y)\pm\epsilon nover^ start_ARG roman_rank end_ARG start_POSTSUBSCRIPT italic_π end_POSTSUBSCRIPT ( italic_y ) = roman_rank start_POSTSUBSCRIPT italic_π end_POSTSUBSCRIPT ( italic_y ) ± italic_ϵ italic_n with high probability. Optimal bounds are known in this setting: Karnin, Lang, and Liberty proposed an algorithm with O⁢((1/ϵ)⁢log⁡log⁡(1/δ))𝑂1italic-ϵ1𝛿O((1/\epsilon)\log\log(1/\delta))italic_O ( ( 1 / italic_ϵ ) roman_log roman_log ( 1 / italic_δ ) ) space, which matches the best space one can hope for even for offline algorithms when the failure probability δ𝛿\deltaitalic_δ is a constant [13].666An offline algorithm sees the elements all at once and computes a small sketch that can answer rank queries approximately. On the other hand, oftentimes, the application needs to accurately learn the tail distribution of the data stream. For instance, this need arises when monitoring network latencies: the distribution of response times is often very long-tailed, and understanding the occasional, yet problematic, high response times is a key purpose of the task [4]. For such applications, algorithms that guarantee relative errors give high accuracy on the tail distribution, and are thus more aligned with this stricter requirement. That is, the algorithm must return rank^π⁢(y)=(1±ϵ)⁢rankπ⁢(y)subscript^rank𝜋𝑦plus-or-minus1italic-ϵsubscriptrank𝜋𝑦\widehat{\mathrm{rank}}_{\pi}(y)=(1\pm\epsilon)\mathrm{rank}_{\pi}(y)over^ start_ARG roman_rank end_ARG start_POSTSUBSCRIPT italic_π end_POSTSUBSCRIPT ( italic_y ) = ( 1 ± italic_ϵ ) roman_rank start_POSTSUBSCRIPT italic_π end_POSTSUBSCRIPT ( italic_y ).777The definition as-is gives higher accuracy for queries with small ranks. By running the algorithm with a reversed total-ordering of stream elements, we can obtain high accuracy at the tail of the distribution. The relative-error quantile estimation task also arises when approximately counting the inversions in a stream [11]. The optimal bound for offline algorithms with relative error is Θ⁢(ϵ−1⁢log⁡(ϵ⁢n))Θsuperscriptitalic-ϵ1italic-ϵ𝑛\Theta(\epsilon^{-1}\log(\epsilon n))roman_Θ ( italic_ϵ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log ( italic_ϵ italic_n ) ), by simply storing elements with ranks {1,2,…,ϵ−1}12…superscriptitalic-ϵ1\{1,2,\ldots,\epsilon^{-1}\}{ 1 , 2 , … , italic_ϵ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT } and {ϵ−1⁢(1+ϵ),ϵ−1⁢(1+ϵ)2,…}superscriptitalic-ϵ11italic-ϵsuperscriptitalic-ϵ1superscript1italic-ϵ2…\{\epsilon^{-1}(1+\epsilon),\epsilon^{-1}(1+\epsilon)^{2},\ldots\}{ italic_ϵ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( 1 + italic_ϵ ) , italic_ϵ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( 1 + italic_ϵ ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , … }. Best-known streaming algorithms are Multi-Layer Randomization (“MR” algorithm) by Zhang, Lin, Xu, Korn and Wang [20] with O⁢(ϵ−2⁢log⁡(ϵ2⁢n))𝑂superscriptitalic-ϵ2superscriptitalic-ϵ2𝑛O(\epsilon^{-2}\log(\epsilon^{2}n))italic_O ( italic_ϵ start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT roman_log ( italic_ϵ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_n ) ) space, and a recent breakthrough [3] by Cormode, Karnin, Liberty, Thaler and Veselỳ with O⁢(ϵ−1⁢log1.5⁡(ϵ⁢n))𝑂superscriptitalic-ϵ1superscript1.5italic-ϵ𝑛O(\epsilon^{-1}\log^{1.5}(\epsilon n))italic_O ( italic_ϵ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log start_POSTSUPERSCRIPT 1.5 end_POSTSUPERSCRIPT ( italic_ϵ italic_n ) ) space (we will refer to it as the CKLTV algorithm below). The MR algorithm [20] maintains logarithmically many sketches of size O⁢(ϵ−2)𝑂superscriptitalic-ϵ2O(\epsilon^{-2})italic_O ( italic_ϵ start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT ). Each sketch is responsible for queries with rank in [ϵ−2⁢2i,ϵ−2⁢2i+1)superscriptitalic-ϵ2superscript2𝑖superscriptitalic-ϵ2superscript2𝑖1[\epsilon^{-2}2^{i},\epsilon^{-2}2^{i+1})[ italic_ϵ start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT 2 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , italic_ϵ start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT 2 start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT ) for some i𝑖iitalic_i. More recently, Cormode et al. [3] introduced “relative compactors”. Roughly speaking, a relative compactor takes a stream of elements as input and outputs a shorter stream such that the rank of any query in the input stream can be approximated with small relative error based on its rank in the output stream (see Section 5 for a more detailed overview). Then, the algorithm of [3] “connects” logarithmically many relative compactors, i.e., the output stream of the previous relative compactor is fed (online, in the streaming sense) to the next relative compactor as its input stream. However, both of the aforementioned algorithms ([3], [20]) have the optimal dependence on one of the parameters ϵitalic-ϵ\epsilonitalic_ϵ and n𝑛nitalic_n, while are suboptimal by a polynomial factor on the other. A natural question is whether we can improve the sketch size in the MR algorithm, or improve the relative compactor space in CKLTV, so that the offline optimal space for this problem can also be achieved in streaming. It turns out that the answer to this question is yes and no. Neither of the two subroutines can be improved in general, due to a lower bound that we prove in Section 7. On the other hand, the “bad” streams that lead to such lower bounds inherently cannot be combined – the algorithm maintains logarithmically many sketches of relative compactors, there are bad streams that would force one of them to consume large space, but not all of them simultaneously. Based on this observation, we propose a new streaming algorithm for quantile estimation with relative errors using nearly optimal space O~⁢(ϵ−1⁢log⁡ϵ⁢n)~𝑂superscriptitalic-ϵ1italic-ϵ𝑛\tilde{O}(\epsilon^{-1}\log\epsilon n)over~ start_ARG italic_O end_ARG ( italic_ϵ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log italic_ϵ italic_n )888The O~~𝑂\tilde{O}over~ start_ARG italic_O end_ARG hides the dominated log⁡(1/ϵ)1italic-ϵ\log(1/\epsilon)roman_log ( 1 / italic_ϵ ), log⁡log⁡n𝑛\log\log nroman_log roman_log italic_n and log⁡log⁡(1/δ)1𝛿\log\log(1/\delta)roman_log roman_log ( 1 / italic_δ ) terms. For a precise space bound, see 1., nearly matching the trivial Ω⁢(ϵ−1⁢log⁡(ϵ⁢n))Ωsuperscriptitalic-ϵ1italic-ϵ𝑛\Omega(\epsilon^{-1}\log(\epsilon n))roman_Ω ( italic_ϵ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log ( italic_ϵ italic_n ) ) offline lower bound999The Ω⁢(ϵ−1⁢log⁡(ϵ⁢n))Ωsuperscriptitalic-ϵ1italic-ϵ𝑛\Omega(\epsilon^{-1}\log(\epsilon n))roman_Ω ( italic_ϵ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log ( italic_ϵ italic_n ) ) lower bound can be shown by inserting ϵ−1⁢log⁡(ϵ⁢n)superscriptitalic-ϵ1italic-ϵ𝑛\epsilon^{-1}\log(\epsilon n)italic_ϵ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log ( italic_ϵ italic_n ) many distinct elements x1<x2<⋯<xϵ−1⋅log⁡(ϵ⁢n)subscript𝑥1subscript𝑥2⋯subscript𝑥⋅superscriptitalic-ϵ1italic-ϵ𝑛x_{1}<x_{2}<\dots<x_{\epsilon^{-1}\cdot\log(\epsilon n)}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT < italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT < ⋯ < italic_x start_POSTSUBSCRIPT italic_ϵ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ⋅ roman_log ( italic_ϵ italic_n ) end_POSTSUBSCRIPT where for any 1≤i≤log⁡(ϵ⁢n)1𝑖italic-ϵ𝑛1\leq i\leq\log(\epsilon n)1 ≤ italic_i ≤ roman_log ( italic_ϵ italic_n ), the elements xϵ−1⁢(i−1)+1,…,xϵ−1⁢isubscript𝑥superscriptitalic-ϵ1𝑖11…subscript𝑥superscriptitalic-ϵ1𝑖x_{\epsilon^{-1}(i-1)+1},\dots,x_{\epsilon^{-1}i}italic_x start_POSTSUBSCRIPT italic_ϵ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( italic_i - 1 ) + 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_ϵ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT italic_i end_POSTSUBSCRIPT are inserted 2isuperscript2𝑖2^{i}2 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT times each. Any algorithm, even offline one that can see the entire stream, must keep all elements x1,x2,…,xϵ−1⋅log⁡(ϵ⁢n)subscript𝑥1subscript𝑥2…subscript𝑥⋅superscriptitalic-ϵ1italic-ϵ𝑛x_{1},x_{2},\dots,x_{\epsilon^{-1}\cdot\log(\epsilon n)}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_ϵ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ⋅ roman_log ( italic_ϵ italic_n ) end_POSTSUBSCRIPT in memory to satisfy the error guarantee.. We use the framework of MR, and maintain logarithmically many sketches such that each sketch is responsible for answering queries with rank in [ϵ−1⁢2i,ϵ−1⁢2i+1)superscriptitalic-ϵ1superscript2𝑖superscriptitalic-ϵ1superscript2𝑖1[\epsilon^{-1}2^{i},\epsilon^{-1}2^{i+1})[ italic_ϵ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT 2 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , italic_ϵ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT 2 start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT ). Now, each sketch is implemented using a collection of new data structures, which we call elastic compactors. Elastic compactors are inspired by relative compactors, but they have one crucial additional feature: they are resizable. Since our sketch for each of the (logarithmically-many) scales will be built using these elastic compactors, we will actually be able to resize the entire sketch for each scale as needed. Depending on how “difficult” the input stream for each sketch is, our algorithm dynamically allocates space to the sketches, and resizes them to the current space on-the-fly as the stream is observed. Whenever a piece of “bad input stream” targeting a specific sketch occurs (i.e. the one that we construct in Section 7), the algorithm automatically allocates more space to that sketch temporarily, while still guaranteeing that the total space of all sketches is always bounded by O~⁢(ϵ−1⁢log⁡ϵ⁢n)~𝑂superscriptitalic-ϵ1italic-ϵ𝑛\tilde{O}(\epsilon^{-1}\log\epsilon n)over~ start_ARG italic_O end_ARG ( italic_ϵ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log italic_ϵ italic_n ) with high probability. Theorem 1. Let 0<δ≤0.50𝛿0.50<\delta\leq 0.50 < italic_δ ≤ 0.5 and 0<ϵ≤10italic-ϵ10<\epsilon\leq 10 < italic_ϵ ≤ 1. There is a randomized, comparison-based, one-pass streaming algorithm that, when processing a stream π𝜋\piitalic_π consisting of n𝑛nitalic_n elements, produces a sketch satisfying the following: for any query x∈𝒰𝑥𝒰x\in\mathcal{U}italic_x ∈ caligraphic_U, the sketch returns an estimate rank^π⁢(x)subscript^rank𝜋𝑥\widehat{\mathrm{rank}}_{\pi}(x)over^ start_ARG roman_rank end_ARG start_POSTSUBSCRIPT italic_π end_POSTSUBSCRIPT ( italic_x ) for rankπ⁢(x)subscriptrank𝜋𝑥\mathrm{rank}_{\pi}(x)roman_rank start_POSTSUBSCRIPT italic_π end_POSTSUBSCRIPT ( italic_x ) such that with probability 1−δ1𝛿1-\delta1 - italic_δ, |rank^π⁢(x)−rankπ⁢(x)|≤ϵ⋅rankπ⁢(x),subscript^rank𝜋𝑥subscriptrank𝜋𝑥⋅italic-ϵsubscriptrank𝜋𝑥|\widehat{\mathrm{rank}}_{\pi}(x)-\mathrm{rank}_{\pi}(x)|\leq\epsilon\cdot% \mathrm{rank}_{\pi}(x),| over^ start_ARG roman_rank end_ARG start_POSTSUBSCRIPT italic_π end_POSTSUBSCRIPT ( italic_x ) - roman_rank start_POSTSUBSCRIPT italic_π end_POSTSUBSCRIPT ( italic_x ) | ≤ italic_ϵ ⋅ roman_rank start_POSTSUBSCRIPT italic_π end_POSTSUBSCRIPT ( italic_x ) , where the probability is over the internal randomness of the streaming algorithm. Moreover, the total space used by the sketch is O⁢(ϵ−1⁢log⁡(ϵ⁢n)⋅log⁡(1/ϵ)⋅(log⁡log⁡n+log⁡(1/ϵ))⋅(log⁡log⁡1/δ)3).𝑂⋅superscriptitalic-ϵ1italic-ϵ𝑛1italic-ϵ𝑛1italic-ϵsuperscript1𝛿3O(\epsilon^{-1}\log(\epsilon n)\cdot\log(1/\epsilon)\cdot(\log\log n+\log(1/% \epsilon))\cdot(\log\log 1/\delta)^{3}).italic_O ( italic_ϵ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log ( italic_ϵ italic_n ) ⋅ roman_log ( 1 / italic_ϵ ) ⋅ ( roman_log roman_log italic_n + roman_log ( 1 / italic_ϵ ) ) ⋅ ( roman_log roman_log 1 / italic_δ ) start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ) . There are several consequences of our construction, which to recall, consists of logarithmically-many resizable sketches for each scale [ϵ−1⁢2i,ϵ−1⁢2i+1)superscriptitalic-ϵ1superscript2𝑖superscriptitalic-ϵ1superscript2𝑖1[\epsilon^{-1}2^{i},\epsilon^{-1}2^{i+1})[ italic_ϵ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT 2 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , italic_ϵ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT 2 start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT ). First, since the sketch can be easily resized, our algorithm actually does not need to know the stream length n𝑛nitalic_n in advance, and the same algorithm works as the stream length increases. Another important feature in practice is mergeability, i.e. it is useful to be able to summarize two substreams π1subscript𝜋1\pi_{1}italic_π start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and π2subscript𝜋2\pi_{2}italic_π start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT separately into sketches ℳ1,ℳ2subscriptℳ1subscriptℳ2\mathcal{M}_{1},\mathcal{M}_{2}caligraphic_M start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , caligraphic_M start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, and then create a merged sketch ℳℳ\mathcal{M}caligraphic_M which applies to the combined stream π=π1⁢⨆π2𝜋subscript𝜋1square-unionsubscript𝜋2\pi=\pi_{1}\bigsqcup\pi_{2}italic_π = italic_π start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ⨆ italic_π start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT with similar error and space guarantees. Currently, it is not clear whether our relative-error quantiles sketch is fully-mergeable (See Section 8 for more discussion). All-quantiles estimation. As a straight-forward corollary of 1, we obtain a sketch that satisfies the all-quantiles guarantee, meaning that for all queries x∈𝒰𝑥𝒰x\in\mathcal{U}italic_x ∈ caligraphic_U simultaneously, the sketch provides an accurate estimate with high probability. The proof proceeds by a standard union bound over an ϵitalic-ϵ\epsilonitalic_ϵ-net, and is nearly identical to argument given in Appendix B of [3]. Corollary 2. Let 0<δ≤0.50𝛿0.50<\delta\leq 0.50 < italic_δ ≤ 0.5 and 0<ϵ≤10italic-ϵ10<\epsilon\leq 10 < italic_ϵ ≤ 1. There is a randomized, comparison-based, one pass streaming algorithm that, when processing a stream π𝜋\piitalic_π consisting of n𝑛nitalic_n elements, produces a sketch satisfying the all-quantiles guarantee: for all queries x∈𝒰𝑥𝒰x\in\mathcal{U}italic_x ∈ caligraphic_U simultaneously, the sketch returns an estimate rank^π⁢(x)subscript^rank𝜋𝑥\widehat{\mathrm{rank}}_{\pi}(x)over^ start_ARG roman_rank end_ARG start_POSTSUBSCRIPT italic_π end_POSTSUBSCRIPT ( italic_x ) such that with probability 1−δ1𝛿1-\delta1 - italic_δ, |rank^π⁢(x)−rankπ⁢(x)|≥ϵ⋅rankπ⁢(x),subscript^rank𝜋𝑥subscriptrank𝜋𝑥⋅italic-ϵsubscriptrank𝜋𝑥|\widehat{\mathrm{rank}}_{\pi}(x)-\mathrm{rank}_{\pi}(x)|\geq\epsilon\cdot% \mathrm{rank}_{\pi}(x),| over^ start_ARG roman_rank end_ARG start_POSTSUBSCRIPT italic_π end_POSTSUBSCRIPT ( italic_x ) - roman_rank start_POSTSUBSCRIPT italic_π end_POSTSUBSCRIPT ( italic_x ) | ≥ italic_ϵ ⋅ roman_rank start_POSTSUBSCRIPT italic_π end_POSTSUBSCRIPT ( italic_x ) , where the probability is over the internal randomness of the streaming algorithm. The total space used by the sketch is O⁢(ϵ−1⁢log⁡(ϵ⁢n)⋅log⁡(1/ϵ)⋅(log⁡log⁡n+log⁡(1/ϵ))⋅(log⁡log⁡(log⁡(ϵ⁢n)δ⁢ϵ))3)𝑂⋅superscriptitalic-ϵ1italic-ϵ𝑛1italic-ϵ𝑛1italic-ϵsuperscriptitalic-ϵ𝑛𝛿italic-ϵ3O\left(\epsilon^{-1}\log(\epsilon n)\cdot\log(1/\epsilon)\cdot(\log\log n+\log% (1/\epsilon))\cdot\left(\log\log\left(\frac{\log(\epsilon n)}{\delta\epsilon}% \right)\right)^{3}\right)italic_O ( italic_ϵ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log ( italic_ϵ italic_n ) ⋅ roman_log ( 1 / italic_ϵ ) ⋅ ( roman_log roman_log italic_n + roman_log ( 1 / italic_ϵ ) ) ⋅ ( roman_log roman_log ( divide start_ARG roman_log ( italic_ϵ italic_n ) end_ARG start_ARG italic_δ italic_ϵ end_ARG ) ) start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ) 1.1 Further Related Works Deterministic Sketches. In the deterministic additive-error setting, Greenwald and Khanna constructed the GK sketch that stores O⁢(ϵ−1⁢log⁡(ϵ⁢n))𝑂superscriptitalic-ϵ1italic-ϵ𝑛O(\epsilon^{-1}\log(\epsilon n))italic_O ( italic_ϵ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log ( italic_ϵ italic_n ) ) elements [9]; more recently, [6] showed that the GK sketch is optimal and [10] gave a simplification of the GK sketch which still achieves optimal space. In the relative-error case, Zhang et al. [19] proposed a deterministic algorithm that uses O⁢(ϵ−1⁢log3⁡(ϵ⁢n))𝑂superscriptitalic-ϵ1superscript3italic-ϵ𝑛O(\epsilon^{-1}\log^{3}(\epsilon n))italic_O ( italic_ϵ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ( italic_ϵ italic_n ) ) space. This algorithm maintains logarithmically many sketches based on the chronological order of the elements, and keeps merging sketches with similar sizes. Currently, the best known lower bound is Ω⁢(log2⁡(ϵ⁢n)ϵ)Ωsuperscript2italic-ϵ𝑛italic-ϵ\Omega\left(\frac{\log^{2}(\epsilon n)}{\epsilon}\right)roman_Ω ( divide start_ARG roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_ϵ italic_n ) end_ARG start_ARG italic_ϵ end_ARG ) [6]. Sketches with Known Universe. Additionally, some works focus on the case when the universe 𝒰𝒰\mathcal{U}caligraphic_U is known in advance to the streaming algorithm [5, 17]. In the additive error regime, the classical q𝑞qitalic_q-digest algorithm gave an optimal deterministic quantile summary using O⁢(ϵ−1⁢log⁡|𝒰|)𝑂superscriptitalic-ϵ1𝒰O(\epsilon^{-1}\log|\mathcal{U}|)italic_O ( italic_ϵ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log | caligraphic_U | ) words of memory. A recent work of [12] improved this bound to O⁢(ϵ−1)𝑂superscriptitalic-ϵ1O(\epsilon^{-1})italic_O ( italic_ϵ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) words, achieving an optimal space if the stream length n≤poly⁢(|𝒰|)𝑛poly𝒰n\leq\textrm{poly}(|\mathcal{U}|)italic_n ≤ poly ( | caligraphic_U | ). For the relative error setting, [5] designed a deterministic bq-summary algorithm using O⁢(log⁡(ϵ⁢n)⁢log⁡|𝒰|ϵ)𝑂italic-ϵ𝑛𝒰italic-ϵO\left(\frac{\log(\epsilon n)\log|\mathcal{U}|}{\epsilon}\right)italic_O ( divide start_ARG roman_log ( italic_ϵ italic_n ) roman_log | caligraphic_U | end_ARG start_ARG italic_ϵ end_ARG ) words of memory, while the offline lower bound is only Ω⁢(log⁡ϵ⁢nϵ)Ωitalic-ϵ𝑛italic-ϵ\Omega\left(\frac{\log\epsilon n}{\epsilon}\right)roman_Ω ( divide start_ARG roman_log italic_ϵ italic_n end_ARG start_ARG italic_ϵ end_ARG ) words. Resizable Sketches. During the past ten years, there has been a flurry of works on “resizable sketches,” wherein the goal is to design sketches that provide a fixed guarantee on the accuracy while allowing the space allocation to be dynamically adjusted throughout the runtime of the algorithm. This is especially important in practice, where the sketch size may start out being very small, but may need to grow sublinearly until it reaches some fixed maximum size. In particular, resizable sketches have been designed for filters [15, 7] 101010In a talk on “Resizable Sketches” at the Simons Institute Workshop on “Sketching and Algorithm Design” in October 2023, it was mentioned that there are also expandable sketches for the k𝑘kitalic_k-minimum values problem and the well-known Misra-Gries sketch for deterministic heavy-hitter detection [18].. Recently in [18], it was posed as an open question to design resizable sketches for the quantile estimation problem. In fact, we actually answer this question since our final sketch for the relative-error quantile estimation problem is actually resizable. Thus, we show that our sketch can achieve near-optimal space O~⁢(ϵ−1⁢log⁡(ϵ⁢n))~𝑂superscriptitalic-ϵ1italic-ϵ𝑛\tilde{O}(\epsilon^{-1}\log(\epsilon n))over~ start_ARG italic_O end_ARG ( italic_ϵ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log ( italic_ϵ italic_n ) ) while also having this practically-favorable “resizability” feature."
https://arxiv.org/html/2411.01191v1,Prophet Secretary and Matching: the Significance of the Largest Item,"The prophet secretary problem is a combination of the prophet inequality and the secretary problem, where elements are drawn from known independent distributions and arrive in uniformly random order. In this work, we design 1) a 0.6880.6880.6880.688-competitive algorithm, that breaks the 0.6750.6750.6750.675 barrier of blind strategies (Correa, Saona, Ziliotto, 2021), and 2) a 0.6410.6410.6410.641-competitive algorithm for the prophet secretary matching problem, that breaks the 1−1/e≈0.63211𝑒0.6321-1/e\approx 0.6321 - 1 / italic_e ≈ 0.632 barrier for the first time. Our second result also applies to the query-commit model of weighted stochastic matching and improves the state-of-the-art ratio (Derakhshan and Farhadi, 2023).","The study of prophet inequality dates back to the 1970s [31, 32] from optimal stopping theory. Consider n𝑛nitalic_n items with independent random values arriving one by one in an adversarial order. The value distribution Fisubscript𝐹𝑖F_{i}italic_F start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT of each item i𝑖iitalic_i is known upfront to the algorithm, but the realization of value vi∼Fisimilar-tosubscript𝑣𝑖subscript𝐹𝑖v_{i}\sim F_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∼ italic_F start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is only revealed on the item’s arrival. After seeing the item’s identity i𝑖iitalic_i and value visubscript𝑣𝑖v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, the algorithm decides immediately whether to accept the item and collect its value; the algorithm can accept at most one item in this problem. The goal is to maximize the expected value of the accepted item and compete against the prophet, i.e., the expected maximum value 𝐄⁡[maxi⁡vi]𝐄subscript𝑖subscript𝑣𝑖\operatorname{\mathbf{E}}\mathchoice{\left[\max_{i}v_{i}\right]}{[\max_{i}v_{i% }]}{[\max_{i}v_{i}]}{[\max_{i}v_{i}]}bold_E [ roman_max start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ]. It is known that the optimal competitive ratio is 1212\frac{1}{2}divide start_ARG 1 end_ARG start_ARG 2 end_ARG for this problem. A fundamental extension of the prophet inequality is prophet matching. Consider an underlying bipartite graph with edge weights drawn from known distributions. The vertices on one side are known upfront and those on the other side arrive online. On the arrival of an online vertex visubscript𝑣𝑖v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, the weights of its incident edges are revealed and the algorithm decides whether to match visubscript𝑣𝑖v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and to which offline vertex. The classic prophet inequality is captured by this model with one offline vertex. Feldman, Gravin, and Lucier [19] gave a tight 1212\frac{1}{2}divide start_ARG 1 end_ARG start_ARG 2 end_ARG competitive algorithm for the matching setting, and their result was further generalized to settings when all vertices arrive online [17]. In this work, we consider the secretary variants (a.k.a. the random order variants) of prophet inequality and prophet matching, i.e., the setting where the arrival order of items (resp. vertices) is uniformly at random. The study of prophet secretary was initiated by Esfandiari et al. [16], who designed a 1−1/e≈0.63211𝑒0.6321-1/e\approx 0.6321 - 1 / italic_e ≈ 0.632 competitive algorithm and provided an upper bound of 0.750.750.750.75.111The competitive ratio of an algorithm is a number between [0,1]01[0,1][ 0 , 1 ]. A lower bound corresponds to an algorithm and an upper bound corresponds to an impossibility result. Since then, a sequence of follow-up works [3, 10, 25, 7, 23] have focused on closing the gap. The state-of-the-art lower and upper bounds are 0.6720.6720.6720.672 by Harb [25] and 0.7230.7230.7230.723 by Giambartolomei et al. [23] respectively. Less progress has been made on the prophet secretary matching problem. Ehsani et al. [15] gave a 1−1/e11𝑒1-1/e1 - 1 / italic_e competitive algorithm. Very recently, the 1−1/e11𝑒1-1/e1 - 1 / italic_e barrier was surpassed in two special cases: 1) the i.i.d. setting studied by Yan [40] and Qiu et al. [37]; and 2) the query-commit setting studied by Derakhshan and Farhadi [12]. Beating 1−1/e11𝑒1-1/e1 - 1 / italic_e for the general case of prophet secretary matching remains one of the most intriguing open questions to the online algorithms community. 1.1 Our Contributions Result for Prophet Secretary. We design a 0.6880.6880.6880.688 competitive algorithm for the prophet secretary problem. Besides the improvement over the state-of-the-art 0.6720.6720.6720.672 ratio, our result further surpasses the 0.6750.6750.6750.675 barrier of blind strategies [10], the family of algorithms that Correa et al. [10] and Harb [25] focused on. Blind strategies rely on only the distribution of the maximum value, but not the fine-grained distributional information of individual items’ values. Intuitively, such fine-grained information must be crucial because the items are heterogeneous in the prophet secretary problem. However, it is technically challenging to incorporate such information to design and analyze item-dependent strategies: changing the strategy for one item would unavoidably affect the probability of accepting other items since we can accept only one of them. Technique: Activation-Based Algorithms. We introduce two ideas to address this difficulty. First, we change our point of view from designing acceptance probabilities to choosing activation rates. In general, an online algorithm is defined by the probability of accepting an item based on its identity i𝑖iitalic_i, value v𝑣vitalic_v, and the set of future items that will arrive later. Following the conventional wisdom, the current item’s arrival time t𝑡titalic_t is a good surrogate for aggregating information about exponentially many possible sets of future items over their random arrivals. Here, we interpret the random order as having each item arrive within a time horizon from 00 to 1111 uniformly at random. In short, algorithms are represented by the acceptance probabilities for item i𝑖iitalic_i when it has value v𝑣vitalic_v and arrives at time t𝑡titalic_t. However, it is difficult to analyze the algorithms based on this representation. By contrast, we will consider the activation rates aiv⁢(t)superscriptsubscript𝑎𝑖𝑣𝑡a_{i}^{v}(t)italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT ( italic_t ) for item i𝑖iitalic_i when it has value v𝑣vitalic_v and arrives at time t𝑡titalic_t, and the overall activation rates Ai⁢(t)=𝐄v∼Fi⁡[aiv⁢(t)]subscript𝐴𝑖𝑡subscript𝐄similar-to𝑣subscript𝐹𝑖superscriptsubscript𝑎𝑖𝑣𝑡A_{i}(t)=\operatorname{\mathbf{E}}_{v\sim F_{i}}\mathchoice{\left[a_{i}^{v}(t)% \right]}{[a_{i}^{v}(t)]}{[a_{i}^{v}(t)]}{[a_{i}^{v}(t)]}italic_A start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_t ) = bold_E start_POSTSUBSCRIPT italic_v ∼ italic_F start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT ( italic_t ) ] of the item at time t𝑡titalic_t. We activate this item (and accept it if no item has been accepted yet) with probability: giv⁢(t)=aiv⁢(t)⋅e−∫0tAi⁢(x)⁢dx.superscriptsubscript𝑔𝑖𝑣𝑡⋅superscriptsubscript𝑎𝑖𝑣𝑡superscript𝑒superscriptsubscript0𝑡subscript𝐴𝑖𝑥differential-d𝑥g_{i}^{v}(t)=a_{i}^{v}(t)\cdot e^{-\int_{0}^{t}A_{i}(x)\mathrm{d}x}~{}.italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT ( italic_t ) = italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT ( italic_t ) ⋅ italic_e start_POSTSUPERSCRIPT - ∫ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT italic_A start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_x ) roman_d italic_x end_POSTSUPERSCRIPT . This new viewpoint offers two useful invariants. By definition, the probability that we activate item i𝑖iitalic_i before time t𝑡titalic_t equals: ∫0t𝐄v∼Fi⁡[giv⁢(x)]⁢dx=1−e−∫0tAi⁢(x)⁢dx.superscriptsubscript0𝑡subscript𝐄similar-to𝑣subscript𝐹𝑖superscriptsubscript𝑔𝑖𝑣𝑥differential-d𝑥1superscript𝑒superscriptsubscript0𝑡subscript𝐴𝑖𝑥differential-d𝑥\int_{0}^{t}\operatorname{\mathbf{E}}_{v\sim F_{i}}\mathchoice{\left[g_{i}^{v}% (x)\right]}{[g_{i}^{v}(x)]}{[g_{i}^{v}(x)]}{[g_{i}^{v}(x)]}\mathrm{d}x~{}=~{}1% -e^{-\int_{0}^{t}A_{i}(x)\mathrm{d}x}~{}.∫ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT bold_E start_POSTSUBSCRIPT italic_v ∼ italic_F start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT ( italic_x ) ] roman_d italic_x = 1 - italic_e start_POSTSUPERSCRIPT - ∫ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT italic_A start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_x ) roman_d italic_x end_POSTSUPERSCRIPT . Hence, the activation events effectively follow a Poisson process with rates Ai⁢(t)subscript𝐴𝑖𝑡A_{i}(t)italic_A start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_t ). Accordingly, the probability that we activate item i𝑖iitalic_i with value v𝑣vitalic_v and arrival time t𝑡titalic_t is: 𝐏𝐫⁡[vi=v]⋅aiv⁢(t)⋅e−∫0t∑j=1nAj⁢(x)⁢d⁢x⏟(⋆)⁢d⁢t.⋅⋅𝐏𝐫subscript𝑣𝑖𝑣superscriptsubscript𝑎𝑖𝑣𝑡subscript⏟superscript𝑒superscriptsubscript0𝑡superscriptsubscript𝑗1𝑛subscript𝐴𝑗𝑥d𝑥⋆d𝑡\operatorname{\mathbf{Pr}}\mathchoice{\left[v_{i}=v\right]}{[v_{i}=v]}{[v_{i}=% v]}{[v_{i}=v]}\cdot a_{i}^{v}(t)\cdot\underbrace{\vphantom{\big{|}}e^{-\int_{0% }^{t}\sum_{j=1}^{n}A_{j}(x)\mathrm{d}x}}_{(\star)}\mathrm{d}t~{}.bold_Pr [ italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_v ] ⋅ italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT ( italic_t ) ⋅ under⏟ start_ARG italic_e start_POSTSUPERSCRIPT - ∫ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_A start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_x ) roman_d italic_x end_POSTSUPERSCRIPT end_ARG start_POSTSUBSCRIPT ( ⋆ ) end_POSTSUBSCRIPT roman_d italic_t . Note that the second part (⋆)⋆(\star)( ⋆ ) is independent of the item’s identity i𝑖iitalic_i and value v𝑣vitalic_v. Therefore, we can simplify the dependence of different items’ strategies by introducing an upper bound on ∑j=1nAj⁢(t)superscriptsubscript𝑗1𝑛subscript𝐴𝑗𝑡\sum_{j=1}^{n}A_{j}(t)∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_A start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_t ) for any time t𝑡titalic_t. Subject to this invariant, we can freely design the activation rates aiv⁢(t)superscriptsubscript𝑎𝑖𝑣𝑡a_{i}^{v}(t)italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT ( italic_t ) for each item i𝑖iitalic_i and value v𝑣vitalic_v to approximately match its contribution to the prophet benchmark. To further simplify the analysis, we focus on activation rates aiv⁢(t)superscriptsubscript𝑎𝑖𝑣𝑡a_{i}^{v}(t)italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT ( italic_t ) that are step functions that change their values at a common threshold time. We demonstrate the effectiveness of this viewpoint and such simple step activation rates in Section 2.4.2 by proving a 0.6940.6940.6940.694 competitive ratio when all items are small in the sense that each contributes only o⁢(1)𝑜1o(1)italic_o ( 1 ) to the prophet benchmark. Technique: Significance of the Largest Item. To further handle the general case of prophet secretary, we will focus on the item i0subscript𝑖0i_{0}italic_i start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT with the largest probability of being selected by the prophet. This is partly inspired by the existing hard instances (e.g., [10, 7, 23]), all of which involve one large item and many small items. The significance of the largest item is twofold: 1) we need to design a special strategy for it beyond the step-function activation rates, and 2) its characteristics provide sufficient information for selecting the invariants for the other items’ activation rates. Why do we need a special strategy for this largest item? Consider the extreme case when it is the only item that matters. Intuitively, we would like to select it with certainty on its arrival. However, we cannot do that using the step-function activation rates. Within a time interval where the activation rates aiv⁢(t)superscriptsubscript𝑎𝑖𝑣𝑡a_{i}^{v}(t)italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT ( italic_t ) remain a constant, the e−∫0tAi⁢(x)⁢dxsuperscript𝑒superscriptsubscript0𝑡subscript𝐴𝑖𝑥differential-d𝑥e^{-\int_{0}^{t}A_{i}(x)\mathrm{d}x}italic_e start_POSTSUPERSCRIPT - ∫ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT italic_A start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_x ) roman_d italic_x end_POSTSUPERSCRIPT term decreases the activation probability over time. This decrease is mild for smaller items, but could be substantial for the largest item. Remarkably, this seemingly trivial instance plays an important role of establishing the 0.6750.6750.6750.675 barrier for blind strategies [10]. Motivated by this extreme case, we let the largest item’s activation probability rather than its activation rate be piece-wise constant. While it is difficult to analyze algorithms based on the representation by activation/acceptance probabilities in general, we show that it is manageable to do that for just one largest item. This special treatment of just one largest item is sufficient for breaking the 0.6750.6750.6750.675 barrier. We will consider two characteristics of this largest item: its probability of being selected by the prophet, i.e., x0=𝐏𝐫⁡[vi0=maxj⁡vj]subscript𝑥0𝐏𝐫subscript𝑣subscript𝑖0subscript𝑗subscript𝑣𝑗x_{0}=\operatorname{\mathbf{Pr}}\mathchoice{\left[v_{i_{0}}=\max_{j}v_{j}% \right]}{[v_{i_{0}}=\max_{j}v_{j}]}{[v_{i_{0}}=\max_{j}v_{j}]}{[v_{i_{0}}=\max% _{j}v_{j}]}italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = bold_Pr [ italic_v start_POSTSUBSCRIPT italic_i start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = roman_max start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ], and a quantity h0subscriptℎ0h_{0}italic_h start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT that measures the extent to which item i0subscript𝑖0i_{0}italic_i start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT would be selected by the prophet with probability more than half, over the randomness of the other items’ values. Based on just x0subscript𝑥0x_{0}italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT and h0subscriptℎ0h_{0}italic_h start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, we will choose the (1) invariants ∑j≠i0Aj⁢(t)subscript𝑗subscript𝑖0subscript𝐴𝑗𝑡\sum_{j\neq i_{0}}A_{j}(t)∑ start_POSTSUBSCRIPT italic_j ≠ italic_i start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_A start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_t ) for the activation rates of other items, (2) the shared threshold time for the step-function activation rates of other items, and (3) the three-stage step-function activation probabilities of the largest item i0subscript𝑖0i_{0}italic_i start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. It is surprising that these two characteristics of the largest item alone are sufficient for choosing all important invariants for our algorithm and analysis. Result for Prophet Secretary Matching. We design a 0.6410.6410.6410.641-competitive algorithm for the prophet secretary matching problem, breaking the 1−1/e11𝑒1-1/e1 - 1 / italic_e barrier for the first time. As a corollary of this result, we also improve the state-of-the-art ratio of the query-commit setting from 0.6330.6330.6330.633 [12] to 0.6410.6410.6410.641 through a reduction [22, 11] from the query-commit model to the secretary model. Summary of Techniques for Prophet Secretary Matching. We start by extending the activation-based framework to matching. First, let us consider a simple strategy: upon the arrival of an online vertex, assign it to an offline vertex with probability proportional to how likely the prophet would match them. We remark that the assignment is independent of the arrival of earlier vertices and their matching results. Then, from each offline vertex’s viewpoint, it may treat the online vertices (more precisely, the corresponding edges) as online items in the prophet secretary problem, treating those not assigned to it as having zero values. However, the online stochastic matching literature suggests that we should not naïvely follow this approach and apply the two-stage step-function activation rates from prophet secretary, or we would miss the opportunity of exploiting second-chance (re-)assignments (a.k.a. the power of two choices). If an offline vertex is already matched, we should no longer assign online vertices to it, but instead redirect the opportunities to other unmatched offline vertices. Hence, we introduce a third stage into the activation-based algorithm, which has the same activation rates as the second stage, but takes into account the assignments redirected from the other offline vertices. This may be viewed as reinterpreting the three-stage algorithm by Yan [40] for the i.i.d. special case within the activation-based framework. By doing so, we achieve the same 0.6450.6450.6450.645 competitive ratio but more generally for all non-i.i.d. instances in which all edges are small, i.e., when each edge contributes only o⁢(1)𝑜1o(1)italic_o ( 1 ) to the prophet benchmark. On the other hand, the worst-case competitive ratio of this approach degenerates to 1−1/e11𝑒1-1/e1 - 1 / italic_e if there is a large edge adjacent to every offline vertex. To complement this scenario, we introduce a variant of the random order contention resolution scheme (RCRS) algorithm for matching [33, 21]. This may be viewed under the activation-based framework as follows. For each offline vertex u𝑢uitalic_u and its largest edge (u,v)𝑢𝑣(u,v)( italic_u , italic_v ), let its adjacent edges other than (u,v)𝑢𝑣(u,v)( italic_u , italic_v ) have constant activation rates; let edge (u,v)𝑢𝑣(u,v)( italic_u , italic_v )’s activation probability be a 00-1111 step-function. This is consistent with our approach for the prophet secretary problem, but the design of activation rates and probabilities is simpler due to the complications in the analysis of the more general matching problem, and the fact that we only need to beat the 1−1e11𝑒1-\frac{1}{e}1 - divide start_ARG 1 end_ARG start_ARG italic_e end_ARG barrier in this case. We show that a hybrid algorithm that randomizes over the above two approaches achieves the stated 0.6410.6410.6410.641 competitive ratio. 1.2 Related Works Dütting et al. [14] studied the computational complexity of the optimal online algorithm for prophet secretary and gave a PTAS, though it does not imply any competitive ratio of the optimal online algorithm. Abolhassani et al. [1] and Liu et al. [34] studied the prophet secretary problem under small-item assumptions. They proved that if either 1) every distribution appears sufficiently many times [1, 34] or 2) every distribution has only a negligible probability of being non-zero [34], there exists a 0.7450.7450.7450.745-competitive algorithm, matching the optimal competitive ratio as in the i.i.d. setting. The order-selection prophet inequality lies between the i.i.d. setting and the secretary setting. In this variant, the algorithm is given the extra power of selecting the arrival order of the items. This is motivated by the application of prophet inequalities to sequential posted pricing mechanisms, and has been studied by [8, 5, 36, 7]. The current state-of-the-art competitive ratio is 0.7250.7250.7250.725 by Bubna and Chiplunkar [7]. Besides matching, the prophet inequality has also been generalized to other combinatorial settings, including matroids [30], combinatorial auctions [19, 13], and general downward-closed constraints [38]. Ehsani et al. [15] also studied the prophet secretary problem under matroid constraints and achieved a competitive ratio of 1−1/e11𝑒1-1/e1 - 1 / italic_e. Beating this ratio for general matroid constraints remains an important open question. Finally, the unweighted and vertex-weighted online stochastic matching problems have attracted a lot of attention in the online algorithms community [18, 4, 24, 35, 28, 6, 27, 39]. Most of these works assumed i.i.d. arrivals of online vertices, in order to surpass the optimal 1−1/e11𝑒1-1/e1 - 1 / italic_e competitive ratio of online (vertex-weighted) bipartite matching [29, 2]."
https://arxiv.org/html/2411.00819v1,A Bellman-Ford algorithm for the path-length-weighted distance in graphs,"Consider a finite directed graph without cycles in which the arrows are weighted. We present an algorithm for the computation of a new distance, called path-length-weighted distance, which has proven useful for graph analysis in the context of fraud detection. The idea is that the new distance explicitly takes into account the size of the paths in the calculations. Thus, although our algorithm is based on arguments similar to those at work for the Bellman-Ford and Dijkstra methods, it is in fact essentially different. We lay out the appropriate framework for its computation, showing the constraints and requirements for its use, along with some illustrative examples.","Algorithms for calculating the (weighted) path-distance between vertices in a graph appeared in the middle of the 20th century, motivated by the growing interest of the time in the applications of mathematical analysis of graphs. The Bellman-Ford algorithm is the main reference of these early studies [2]. Dijkstra’s algorithm for solving the same problem appeared at about the same time [11], and differs from the other, being more efficient depending on the particular problem. After these original works, the growing interest in the subject (due to the numerous applications that graph theory has found in many fields) has given rise to a great deal of research on graph analysis, which often includes the study of these structures when considered as metric spaces. The idea of considering a graph also as a metric space goes back to the beginnings of the theory of graphs. Metric notions begin to appear explicitly in mathematical works in the second half of the last century. The main metric that was considered (and in a sense the only one until the latter part of the century) was the so-called path distance ([12, 15, 18]): for undirected and connected graphs, this metric evaluated between two vertices (nodes) is defined as the length of the shortest path between them (see for example [4, §.2.2.2]). One of the first advances in the metric analysis of graphs was the introduction of weights in the definition of the path distance, assigning weights to the individual paths connecting two consecutive nodes and calculating the infimum of the sum of these weights. Some recent papers on the subject using weighted distances that have inspired this paper are [8, 14]. As in the case of other notions of fundamental graph theory, the relevant theoretical ideas appeared together with research topics from other scientific fields, such as sociology [1, 16, 22]. The definition of different metrics and algorithms to compute them increased greatly in the last decade of the last century, often proposed by problems from other disciplines such as chemistry or crystallography (see [18, 19] and references therein). In this sense, there is a particular case that deserves attention, that is the resistance distance [9, 23]. Coming from some ideas in theoretical chemistry [19] and social network analysis [22], this definition turned out to be a useful tool in the study of molecular configurations in chemistry, although it has also been used in network analysis and other fields [24, 5, 3]. In general, metrics could play a relevant role when studying properties such as robustness (see for example the survey [21]; see also [20]). The interested reader can find more information on applications of metric graphs in the books [6, 4, 13]. In the same direction, in this paper we provide an algorithm to compute a new distance that also appeared in an applied context, in connection with the automatic analysis of fraud in economic networks (see [7]). In this paper, we show that this metric allows to consider vertices that are far away when the distance is measured using the path-distance, becoming close with respect to this metric. It is especially useful in the economic analysis of fraud in networks of companies, since very often the strategy used to hide such fraud is to use the path-distance. Technically, it is defined as a weighted metric, but by dividing the sums of weights appearing in the infimum that provides the value of the metric by a new term that depends on the number of steps involved in the summation. We will show that this change in the weighting process forces to radically change the algorithm to calculate it, since in this new case, longer paths could give shorter distances. However, in the present work we consider the case of acyclic directed graphs, in order to avoid restrictions that make it impossible to define a weighted path metric, since continuous passage through a cycle could always give a null value to the metric (which would mean that it is not a metric). There are other ways to avoid this (see for example Proposition 4.1 in [7]), but in our case we decided to compute the distance between vertices by restricting the set of possible paths in the infimum that gives it. The way to do this is to avoid cycles, and to consider directed graphs. As a result, what we compute is not a metric on the whole graph, but only a distance between two vertices chosen in it. Let us give now some basic definitions about graphs and metric spaces. Let us explain some concepts related to the general definition of what a metric is, which will adapted later to the graph theoretic framework. Let ℝ+superscriptℝ\mathbb{R}^{+}blackboard_R start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT be the non-negative real numbers. An (extended) quasi-metric on a set ΩΩ\Omegaroman_Ω is a function d:Ω×Ω→ℝ+∪{∞}:𝑑→ΩΩsuperscriptℝd:\Omega\times\Omega\to\mathbb{R}^{+}\cup\{\infty\}italic_d : roman_Ω × roman_Ω → blackboard_R start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT ∪ { ∞ } such that for all a,b,c∈Ω𝑎𝑏𝑐Ωa,b,c\in\Omegaitalic_a , italic_b , italic_c ∈ roman_Ω, the axioms (1) d⁢(a,b)=0=d⁢(b,a)𝑑𝑎𝑏0𝑑𝑏𝑎d(a,b)=0=d(b,a)italic_d ( italic_a , italic_b ) = 0 = italic_d ( italic_b , italic_a ) if and only if a=b,𝑎𝑏a=b,italic_a = italic_b , and (2) d⁢(a,b)≤d⁢(a,c)+d⁢(c,b)𝑑𝑎𝑏𝑑𝑎𝑐𝑑𝑐𝑏d(a,b)\leq d(a,c)+d(c,b)italic_d ( italic_a , italic_b ) ≤ italic_d ( italic_a , italic_c ) + italic_d ( italic_c , italic_b ) hold. The resulting quasi-metric structure (Ω,d)Ω𝑑(\Omega,d)( roman_Ω , italic_d ) is called a quasi-metric space. For the specific framework of this paper, a useful summary of the notions of distance in graphs, with sufficient explanation and many examples, is given in Chapter 15 in [10]. In this paper we will deal with the so called path-length-weighted distance, that was introduced in [7, §.4]. It should be noted that the version defined there is given for non-directed graphs, and so the definition we will use is slightly different. However, we will define an extended quasi-metric also in this case by giving the value d⁢(a,b)=∞𝑑𝑎𝑏d(a,b)=\inftyitalic_d ( italic_a , italic_b ) = ∞ when there is no path for going from a𝑎aitalic_a to b,𝑏b,italic_b , and considering only allowed paths between vertices. Since we are interested in how to compute the distance, and not in theoretical questions about its metric space structure, we will focus attention on the computational algorithm. All the notions on graph theory that are needed can be found in books on this subject, as for example [4]. We will introduce some of them in the next section."
https://arxiv.org/html/2411.02298v1,Sample-Efficient Private Learning of Mixtures of Gaussians,"We study the problem of learning mixtures of Gaussians with approximate differential privacy. We prove that roughly k⁢d2+k1.5⁢d1.75+k2⁢d𝑘superscript𝑑2superscript𝑘1.5superscript𝑑1.75superscript𝑘2𝑑kd^{2}+k^{1.5}d^{1.75}+k^{2}ditalic_k italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_k start_POSTSUPERSCRIPT 1.5 end_POSTSUPERSCRIPT italic_d start_POSTSUPERSCRIPT 1.75 end_POSTSUPERSCRIPT + italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d samples suffice to learn a mixture of k𝑘kitalic_k arbitrary d𝑑ditalic_d-dimensional Gaussians up to low total variation distance, with differential privacy. Our work improves over the previous best result [AAL24b] (which required roughly k2⁢d4superscript𝑘2superscript𝑑4k^{2}d^{4}italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT samples) and is provably optimal when d𝑑ditalic_d is much larger than k2superscript𝑘2k^{2}italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. Moreover, we give the first optimal bound for privately learning mixtures of k𝑘kitalic_k univariate (i.e., 1111-dimensional) Gaussians. Importantly, we show that the sample complexity for privately learning mixtures of univariate Gaussians is linear in the number of components k𝑘kitalic_k, whereas the previous best sample complexity [AAL21] was quadratic in k𝑘kitalic_k. Our algorithms utilize various techniques, including the inverse sensitivity mechanism [AD20b, AD20a, HKMN23], sample compression for distributions [ABDH+20], and methods for bounding volumes of sumsets.","Learning Gaussian Mixture Models (GMMs) is one of the most fundamental problems in algorithmic statistics. Gaussianity is a common data assumption, and the setting of Gaussian mixture models is motivated by heterogeneous data that can be split into numerous clusters, where each cluster follows a Gaussian distribution. Learning mixture models is among the most important problems in machine learning [Bis06], and is at the heart of several unsupervised and semi-supervised machine learning models. The study of Gaussian mixture models has had numerous scientific applications dating back to the 1890s [Pea94], and is a crucial tool in modern data analysis techniques in a variety of fields, including bioinformatics [LKWB22], anomaly detection [ZSM+18], and handwriting analysis [Bis06]. In this work, we study the problem of learning a GMM from samples. We focus on the density estimation setting, where the goal is to learn the overall mixture distribution up to low total variation distance. Unlike the parameter estimation setting for GMMs, density estimation can be done even without any boundedness or separation assumptions on the parameters of the components. In fact, it is known that mixtures of k𝑘kitalic_k Gaussians in d𝑑ditalic_d-dimensions can be learned up to total variation distance α𝛼\alphaitalic_α using O~⁢(k⁢d2/α2)~𝑂𝑘superscript𝑑2superscript𝛼2\widetilde{O}(kd^{2}/\alpha^{2})over~ start_ARG italic_O end_ARG ( italic_k italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT / italic_α start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) samples [ABH+18]. Ensuring data privacy has emerged as an increasingly important challenge in modern data analysis and statistics. Differential privacy (DP) [DMNS06] is a rigorous way of defining privacy, and is considered to be the gold standard both in theory and practice, with deployments by Apple [Tea17], Google [EPK14], Microsoft [DKY17], and the US Census Bureau [DLS+17]. As is the case for many data analysis tasks, standard algorithms for learning GMMs leak potentially sensitive information about the individuals who contributed data. This raises the question of whether we can do density estimation for GMMs under the constraint of differential privacy. Private density estimation for GMMs with unrestricted Gaussian components is a challenging task. In fact, privately learning a single unrestricted Gaussian has been the subject of multiple recent studies [AAK21, KMS+22b, AL22, KMV22, AKT+23, HKMN23]. Private learning of GMMs is significantly more challenging, because even without privacy constraints, parameter estimation for GMMs requires exponentially many samples in terms of the number of components [MV10]. Therefore, it is not clear how to use the typical recipe of “adding noise” to the estimated parameters or “privately choosing” from the finite-dimensional space of parameters. Consequently, the only known sample complexity bounds for privately learning unrestricted GMMs are loose [AAL24b, AAL21]. Let us first formally define the problem of learning GMMs. We represent a GMM 𝒟=∑i=1kwi⁢𝒩⁢(μi,Σi)𝒟superscriptsubscript𝑖1𝑘subscript𝑤𝑖𝒩subscript𝜇𝑖subscriptΣ𝑖\mathcal{D}=\sum_{i=1}^{k}w_{i}\mathcal{N}(\mu_{i},\Sigma_{i})caligraphic_D = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT caligraphic_N ( italic_μ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , roman_Σ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) by its parameters, namely {(wi,μi,Σi)}i=1ksuperscriptsubscriptsubscript𝑤𝑖subscript𝜇𝑖subscriptΣ𝑖𝑖1𝑘\{(w_{i},\mu_{i},\Sigma_{i})\}_{i=1}^{k}{ ( italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_μ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , roman_Σ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT, where wi≥0subscript𝑤𝑖0w_{i}\geq 0italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ≥ 0, ∑iwi=1subscript𝑖subscript𝑤𝑖1\sum_{i}w_{i}=1∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1, μi∈ℝdsubscript𝜇𝑖superscriptℝ𝑑\mu_{i}\in\mathbb{R}^{d}italic_μ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT, and ΣisubscriptΣ𝑖\Sigma_{i}roman_Σ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is a positive definite matrix. In the following, a GMM learning algorithm 𝒜𝒜\mathcal{A}caligraphic_A receives a set of data points in ℝdsuperscriptℝ𝑑\mathbb{R}^{d}blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT and outputs a (representation of) a GMM. The total variation distance between two distributions is dTV⁡(𝒟~,𝒟)=12⁢∫ℝd|𝒟⁢(x)−𝒟~⁢(x)|⁢𝑑xsubscriptdTV~𝒟𝒟12subscriptsuperscriptℝ𝑑𝒟𝑥~𝒟𝑥differential-d𝑥\operatorname{d_{TV}}(\tilde{\mathcal{D}},\mathcal{D})=\frac{1}{2}\int_{% \mathbb{R}^{d}}|\mathcal{D}(x)-\tilde{\mathcal{D}}(x)|dxstart_OPFUNCTION roman_d start_POSTSUBSCRIPT roman_TV end_POSTSUBSCRIPT end_OPFUNCTION ( over~ start_ARG caligraphic_D end_ARG , caligraphic_D ) = divide start_ARG 1 end_ARG start_ARG 2 end_ARG ∫ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT end_POSTSUBSCRIPT | caligraphic_D ( italic_x ) - over~ start_ARG caligraphic_D end_ARG ( italic_x ) | italic_d italic_x111We are slightly abusing the notation and using 𝒟⁢(x)𝒟𝑥\mathcal{D}(x)caligraphic_D ( italic_x ) as the pdf of 𝒟𝒟\mathcal{D}caligraphic_D at points x𝑥xitalic_x.. Definition 1.1 (Learning GMMs). For α,β∈(0,1)𝛼𝛽01\alpha,\beta\in(0,1)italic_α , italic_β ∈ ( 0 , 1 ), we say 𝒜𝒜\mathcal{A}caligraphic_A learns GMMs with n𝑛nitalic_n samples up to accuracy α𝛼\alphaitalic_α and failure probability β𝛽\betaitalic_β if for every GMM 𝒟𝒟\mathcal{D}caligraphic_D, given samples X1,…,Xn⁢∼i.i.d.⁢𝒟X_{1},\dots,X_{n}\overset{i.i.d.}{\sim}\mathcal{D}italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_OVERACCENT italic_i . italic_i . italic_d . end_OVERACCENT start_ARG ∼ end_ARG caligraphic_D, it outputs (a representation of) a GMM 𝒟~~𝒟\tilde{\mathcal{D}}over~ start_ARG caligraphic_D end_ARG such that dTV⁡(𝒟~,𝒟)≤αsubscriptdTV~𝒟𝒟𝛼\operatorname{d_{TV}}(\tilde{\mathcal{D}},\mathcal{D})\leq\alphastart_OPFUNCTION roman_d start_POSTSUBSCRIPT roman_TV end_POSTSUBSCRIPT end_OPFUNCTION ( over~ start_ARG caligraphic_D end_ARG , caligraphic_D ) ≤ italic_α with probability at least 1−β1𝛽1-\beta1 - italic_β. α𝛼\alphaitalic_α and β𝛽\betaitalic_β are called the accuracy and failure probability, respectively. For clarity of presentation, we will typically fix the value of β𝛽\betaitalic_β (e.g., β=1/3𝛽13\beta=1/3italic_β = 1 / 3). The above definition does not enforce the constraint of differential privacy. The following definitions formalizes (approximate) differential privacy. Definition 1.2 (Differential Privacy (DP) [DMNS06, DKM+06]). Let ε,δ≥0𝜀𝛿0\varepsilon,\delta\geq 0italic_ε , italic_δ ≥ 0. A randomized algorithm 𝒜:𝒳n→𝒪:𝒜→superscript𝒳𝑛𝒪\mathcal{A}:\mathcal{X}^{n}\to\mathcal{O}caligraphic_A : caligraphic_X start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT → caligraphic_O is said to be (ε,δ)𝜀𝛿(\varepsilon,\delta)( italic_ε , italic_δ )-differentially private ((ε,δ)𝜀𝛿(\varepsilon,\delta)( italic_ε , italic_δ )-DP) if for any two neighboring datasets 𝐗,𝐗′∈𝒳n𝐗superscript𝐗′superscript𝒳𝑛\mathbf{X},\mathbf{X}^{\prime}\in\mathcal{X}^{n}bold_X , bold_X start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ∈ caligraphic_X start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT and any measurable subset O⊂𝒪𝑂𝒪O\subset\mathcal{O}italic_O ⊂ caligraphic_O, ℙ⁢[𝒜⁢(𝐗′)∈O]≤eε⋅ℙ⁢[𝒜⁢(𝐗)∈O]+δ.ℙdelimited-[]𝒜superscript𝐗′𝑂⋅superscript𝑒𝜀ℙdelimited-[]𝒜𝐗𝑂𝛿\mathbb{P}[\mathcal{A}(\mathbf{X}^{\prime})\in O]\leq e^{\varepsilon}\cdot% \mathbb{P}[\mathcal{A}(\mathbf{X})\in O]+\delta.blackboard_P [ caligraphic_A ( bold_X start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) ∈ italic_O ] ≤ italic_e start_POSTSUPERSCRIPT italic_ε end_POSTSUPERSCRIPT ⋅ blackboard_P [ caligraphic_A ( bold_X ) ∈ italic_O ] + italic_δ . If the GMM learner 𝒜𝒜\mathcal{A}caligraphic_A of Definition 1.1 is (ε,δ)𝜀𝛿(\varepsilon,\delta)( italic_ε , italic_δ )-DP, we say that 𝒜𝒜\mathcal{A}caligraphic_A privately learns GMMs. Formally, we have the following definition. Definition 1.3 (Privately learning GMMs). Fix the number of samples n𝑛nitalic_n, dimension d𝑑ditalic_d, and number of mixture components k𝑘kitalic_k. For α,β∈(0,1)𝛼𝛽01\alpha,\beta\in(0,1)italic_α , italic_β ∈ ( 0 , 1 ) and ε,δ≥0𝜀𝛿0\varepsilon,\delta\geq 0italic_ε , italic_δ ≥ 0, a randomized algorithm 𝒜𝒜\mathcal{A}caligraphic_A, that takes as input X1,…,Xn∈ℝdsubscript𝑋1…subscript𝑋𝑛superscriptℝ𝑑X_{1},\dots,X_{n}\in\mathbb{R}^{d}italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT, (ε,δ)𝜀𝛿(\varepsilon,\delta)( italic_ε , italic_δ )-privately learns GMMs up to accuracy α𝛼\alphaitalic_α and failure probability β𝛽\betaitalic_β, if: 1. For any GMM 𝒟𝒟\mathcal{D}caligraphic_D that is a mixture of up to k𝑘kitalic_k Gaussians in d𝑑ditalic_d dimensions, if 𝐗={X1,…,Xn}⁢∼i.i.d.⁢𝒟\mathbf{X}=\{X_{1},\dots,X_{n}\}\overset{i.i.d.}{\sim}\mathcal{D}bold_X = { italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } start_OVERACCENT italic_i . italic_i . italic_d . end_OVERACCENT start_ARG ∼ end_ARG caligraphic_D, 𝒜⁢(X1,…,Xn)𝒜subscript𝑋1…subscript𝑋𝑛\mathcal{A}(X_{1},\dots,X_{n})caligraphic_A ( italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) outputs a GMM 𝒟~~𝒟\tilde{\mathcal{D}}over~ start_ARG caligraphic_D end_ARG such that dTV⁡(𝒟~,𝒟)≤αsubscriptdTV~𝒟𝒟𝛼\operatorname{d_{TV}}(\tilde{\mathcal{D}},\mathcal{D})\leq\alphastart_OPFUNCTION roman_d start_POSTSUBSCRIPT roman_TV end_POSTSUBSCRIPT end_OPFUNCTION ( over~ start_ARG caligraphic_D end_ARG , caligraphic_D ) ≤ italic_α with probability at least 1−β1𝛽1-\beta1 - italic_β (over the randomness of the data 𝐗𝐗\mathbf{X}bold_X and the algorithm 𝒜𝒜\mathcal{A}caligraphic_A). 2. For any neighboring datasets 𝐗,𝐗′∈(ℝd)n𝐗superscript𝐗′superscriptsuperscriptℝ𝑑𝑛\mathbf{X},\mathbf{X}^{\prime}\in(\mathbb{R}^{d})^{n}bold_X , bold_X start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ∈ ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT (not necessarily drawn from any GMM) and any measurable subset O⊂𝒪𝑂𝒪O\subset\mathcal{O}italic_O ⊂ caligraphic_O, ℙ⁢[𝒜⁢(𝐗′)∈O]≤eε⋅ℙ⁢[𝒜⁢(𝐗)∈O]+δ.ℙdelimited-[]𝒜superscript𝐗′𝑂⋅superscript𝑒𝜀ℙdelimited-[]𝒜𝐗𝑂𝛿\mathbb{P}[\mathcal{A}(\mathbf{X}^{\prime})\in O]\leq e^{\varepsilon}\cdot% \mathbb{P}[\mathcal{A}(\mathbf{X})\in O]+\delta.blackboard_P [ caligraphic_A ( bold_X start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) ∈ italic_O ] ≤ italic_e start_POSTSUPERSCRIPT italic_ε end_POSTSUPERSCRIPT ⋅ blackboard_P [ caligraphic_A ( bold_X ) ∈ italic_O ] + italic_δ . Finally, we assume a default value for β𝛽\betaitalic_β of 1/3131/31 / 3, meaning that if not stated, the failure probability β𝛽\betaitalic_β is assumed to equal 1/3131/31 / 3. Our main goal in this paper is to understand the number of samples (as a function of the dimension d𝑑ditalic_d, the number of mixture components k𝑘kitalic_k, the accuracy α𝛼\alphaitalic_α, and the privacy parameters ε,δ𝜀𝛿\varepsilon,\deltaitalic_ε , italic_δ) that are needed to privately and accurately learn the GMM up to low total variation distance. 1.1 Results In this work, we provide improved sample complexity bounds for privately learning mixtures of arbitrary Gaussians, improving over previous work of [AAL21, AAL24b]. Moreover, our sample complexity bounds are optimal in certain regimes, when the dimension is either 1111 or a sufficiently large polynomial in k𝑘kitalic_k and log⁡1δ1𝛿\log\frac{1}{\delta}roman_log divide start_ARG 1 end_ARG start_ARG italic_δ end_ARG. For general dimension d𝑑ditalic_d, we prove the following theorem. Theorem 1.4. For any α,ε,δ∈(0,1),k,d∈ℕformulae-sequence𝛼𝜀𝛿01𝑘𝑑ℕ\alpha,\varepsilon,\delta\in(0,1),k,d\in\mathbb{N}italic_α , italic_ε , italic_δ ∈ ( 0 , 1 ) , italic_k , italic_d ∈ blackboard_N, there exists an inefficient (ε,δ)𝜀𝛿(\varepsilon,\delta)( italic_ε , italic_δ )-DP algorithm that can learn a mixture of k𝑘kitalic_k arbitrary full-dimensional Gaussians in d𝑑ditalic_d dimensions up to accuracy α𝛼\alphaitalic_α, using the following number of samples: n=O~⁢(k⁢d2α2+k⁢d2+d1.75⁢k1.5⁢log0.5⁡(1/δ)+k1.5⁢log1.5⁡(1/δ)α⁢ε+k2⁢dα).𝑛~𝑂𝑘superscript𝑑2superscript𝛼2𝑘superscript𝑑2superscript𝑑1.75superscript𝑘1.5superscript0.51𝛿superscript𝑘1.5superscript1.51𝛿𝛼𝜀superscript𝑘2𝑑𝛼n=\widetilde{O}\left(\frac{kd^{2}}{\alpha^{2}}+\frac{kd^{2}+d^{1.75}k^{1.5}% \log^{0.5}(1/\delta)+k^{1.5}\log^{1.5}(1/\delta)}{\alpha\varepsilon}+\frac{k^{% 2}d}{\alpha}\right).italic_n = over~ start_ARG italic_O end_ARG ( divide start_ARG italic_k italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_α start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG + divide start_ARG italic_k italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_d start_POSTSUPERSCRIPT 1.75 end_POSTSUPERSCRIPT italic_k start_POSTSUPERSCRIPT 1.5 end_POSTSUPERSCRIPT roman_log start_POSTSUPERSCRIPT 0.5 end_POSTSUPERSCRIPT ( 1 / italic_δ ) + italic_k start_POSTSUPERSCRIPT 1.5 end_POSTSUPERSCRIPT roman_log start_POSTSUPERSCRIPT 1.5 end_POSTSUPERSCRIPT ( 1 / italic_δ ) end_ARG start_ARG italic_α italic_ε end_ARG + divide start_ARG italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d end_ARG start_ARG italic_α end_ARG ) . Notably, the mixing weights and the means can be arbitrary and the covariances of the Gaussians can be arbitrarily poorly conditioned, as long as the covariances are non-singular222For clarity of presentation, we assume the covariance matrices are not singular. However, extending our results to degenerate matrices is straightforward.. We remark that we omit the dependence on β𝛽\betaitalic_β (and assume by default a failure probability of 1/3131/31 / 3). However, it is well-known that one can obtain failure probability β𝛽\betaitalic_β with only a multiplicative O⁢(log⁡1/β)𝑂1𝛽O(\log 1/\beta)italic_O ( roman_log 1 / italic_β ) blowup in sample complexity, in a black-box fashion333To obtain success probability β𝛽\betaitalic_β with O⁢(n⋅log⁡1/β)𝑂⋅𝑛1𝛽O(n\cdot\log 1/\beta)italic_O ( italic_n ⋅ roman_log 1 / italic_β ) samples, we repeat the procedure T=O⁢(log⁡1/β)𝑇𝑂1𝛽T=O(\log 1/\beta)italic_T = italic_O ( roman_log 1 / italic_β ) times on independent groups of n𝑛nitalic_n samples each, to get T𝑇Titalic_T estimates 𝒟~1,…,𝒟~Tsubscript~𝒟1…subscript~𝒟𝑇\tilde{\mathcal{D}}_{1},\dots,\tilde{\mathcal{D}}_{T}over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT, and by a Chernoff bound, at least 51%percent5151\%51 % of the estimates are within total variation distance α𝛼\alphaitalic_α of the true mixture 𝒟𝒟\mathcal{D}caligraphic_D. So, by choosing an estimate that is within 2⁢α2𝛼2\alpha2 italic_α of at least 51%percent5151\%51 % of the estimates, it is still within 3⁢α3𝛼3\alpha3 italic_α total variation distance of 𝒟𝒟\mathcal{D}caligraphic_D.. In fact, our analysis can yield even better dependencies on β𝛽\betaitalic_β in some regimes, though to avoid too much complication, we do not analyze this. For reasonably large dimension, i.e., d≥k2⁢log2⁡(1/δ)𝑑superscript𝑘2superscript21𝛿d\geq k^{2}\log^{2}(1/\delta)italic_d ≥ italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 / italic_δ ), this can be simplified to O~⁢(k⁢d2α2+k⁢d2α⁢ε)~𝑂𝑘superscript𝑑2superscript𝛼2𝑘superscript𝑑2𝛼𝜀\tilde{O}\left(\frac{kd^{2}}{\alpha^{2}}+\frac{kd^{2}}{\alpha\varepsilon}\right)over~ start_ARG italic_O end_ARG ( divide start_ARG italic_k italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_α start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG + divide start_ARG italic_k italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_α italic_ε end_ARG ), which is in fact optimal (see Theorem 1.6). Hence, we obtain the optimal sample complexity for sufficiently large dimension. Theorem 1.4 also improves over the previous best sample complexity upper bound of [AAL24b], which uses O~⁢(k2⁢d4+k⁢d2⁢log⁡(1/δ)α2⁢ε+k⁢d⁢log⁡(1/δ)α3⁢ε+k2⁢d2α4⁢ε)~𝑂superscript𝑘2superscript𝑑4𝑘superscript𝑑21𝛿superscript𝛼2𝜀𝑘𝑑1𝛿superscript𝛼3𝜀superscript𝑘2superscript𝑑2superscript𝛼4𝜀\widetilde{O}\left(\frac{k^{2}d^{4}+kd^{2}\log(1/\delta)}{\alpha^{2}% \varepsilon}+\frac{kd\log(1/\delta)}{\alpha^{3}\varepsilon}+\frac{k^{2}d^{2}}{% \alpha^{4}\varepsilon}\right)over~ start_ARG italic_O end_ARG ( divide start_ARG italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT + italic_k italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT roman_log ( 1 / italic_δ ) end_ARG start_ARG italic_α start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_ε end_ARG + divide start_ARG italic_k italic_d roman_log ( 1 / italic_δ ) end_ARG start_ARG italic_α start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT italic_ε end_ARG + divide start_ARG italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_α start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT italic_ε end_ARG ) samples. Our results provide a polynomial improvement in all parameters, but to simplify the comparison, if we ignore dependencies in the error parameter α𝛼\alphaitalic_α and privacy parameters ε,δ𝜀𝛿\varepsilon,\deltaitalic_ε , italic_δ, we improve the sample complexity from k2⁢d4superscript𝑘2superscript𝑑4k^{2}d^{4}italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT to k⁢d2+k2⁢d+k1.5⁢d1.75𝑘superscript𝑑2superscript𝑘2𝑑superscript𝑘1.5superscript𝑑1.75kd^{2}+k^{2}d+k^{1.5}d^{1.75}italic_k italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d + italic_k start_POSTSUPERSCRIPT 1.5 end_POSTSUPERSCRIPT italic_d start_POSTSUPERSCRIPT 1.75 end_POSTSUPERSCRIPT: note that our result is quadratic in the dimension whereas [AAL24b] is quartic. When the dimension is d=1𝑑1d=1italic_d = 1, we can provide an improved result, which is optimal for learning mixtures of univariate Gaussians (see Theorem 1.6 for a matching lower bound). Theorem 1.5. For any α,ε,δ∈(0,1),k∈ℕformulae-sequence𝛼𝜀𝛿01𝑘ℕ\alpha,\varepsilon,\delta\in(0,1),k\in\mathbb{N}italic_α , italic_ε , italic_δ ∈ ( 0 , 1 ) , italic_k ∈ blackboard_N, there exists an inefficient (ε,δ)𝜀𝛿(\varepsilon,\delta)( italic_ε , italic_δ )-DP algorithm that can learn a mixture of k𝑘kitalic_k arbitrary univariate Gaussians (of nonzero variance) up to accuracy α𝛼\alphaitalic_α, using the following number of samples: n=O~⁢(kα2+k⁢log⁡(1/δ)α⁢ε).𝑛~𝑂𝑘superscript𝛼2𝑘1𝛿𝛼𝜀n=\widetilde{O}\left(\frac{k}{\alpha^{2}}+\frac{k\log(1/\delta)}{\alpha% \varepsilon}\right).italic_n = over~ start_ARG italic_O end_ARG ( divide start_ARG italic_k end_ARG start_ARG italic_α start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG + divide start_ARG italic_k roman_log ( 1 / italic_δ ) end_ARG start_ARG italic_α italic_ε end_ARG ) . For privately learning mixtures of univariate Gaussians, the previous best-known result for arbitrary Gaussians required O~⁢(k2⁢log3/2⁡(1/δ)α2⁢ε)~𝑂superscript𝑘2superscript321𝛿superscript𝛼2𝜀\widetilde{O}\left(\frac{k^{2}\log^{3/2}(1/\delta)}{\alpha^{2}\varepsilon}\right)over~ start_ARG italic_O end_ARG ( divide start_ARG italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT roman_log start_POSTSUPERSCRIPT 3 / 2 end_POSTSUPERSCRIPT ( 1 / italic_δ ) end_ARG start_ARG italic_α start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_ε end_ARG ) samples [AAL21]. Importantly, we are the first paper to show that the sample complexity can be linear in the number of components. Our work purely focuses on sample complexity, and as noted in Theorems 1.4 and 1.5, they do not have polynomial time algorithms. We note that the previous works of [AAL21, AAL24b] also do not run in polynomial time. Indeed, there is reason to believe that even non-privately, it is impossible to learn GMMs in polynomial time (in terms of the optimal sample complexity) [DKS17, BRST21, GVV22]. Finally, we prove the following lower bound for learning GMMs in any fixed dimension d𝑑ditalic_d. Theorem 1.6. Fix any dimension d≥1𝑑1d\geq 1italic_d ≥ 1 number of components k≥2𝑘2k\geq 2italic_k ≥ 2, any α,ε𝛼𝜀\alpha,\varepsilonitalic_α , italic_ε at most a sufficiently small constant c∗superscript𝑐c^{*}italic_c start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT, and δ≤(α⁢ε/d)O⁢(1)𝛿superscript𝛼𝜀𝑑𝑂1\delta\leq(\alpha\varepsilon/d)^{O(1)}italic_δ ≤ ( italic_α italic_ε / italic_d ) start_POSTSUPERSCRIPT italic_O ( 1 ) end_POSTSUPERSCRIPT. Then, any (ε,δ)𝜀𝛿(\varepsilon,\delta)( italic_ε , italic_δ )-DP algorithm that can learn a mixture of k𝑘kitalic_k arbitrary full-dimensional Gaussians in d𝑑ditalic_d dimensions up to total variation distance α𝛼\alphaitalic_α, with probability at least 2/3232/32 / 3, requires at least the following number of samples: Ω~⁢(k⁢d2α2+k⁢d2α⁢ε+k⁢log⁡(1/δ)α⁢ε).~Ω𝑘superscript𝑑2superscript𝛼2𝑘superscript𝑑2𝛼𝜀𝑘1𝛿𝛼𝜀\tilde{\Omega}\left(\frac{kd^{2}}{\alpha^{2}}+\frac{kd^{2}}{\alpha\varepsilon}% +\frac{k\log(1/\delta)}{\alpha\varepsilon}\right).over~ start_ARG roman_Ω end_ARG ( divide start_ARG italic_k italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_α start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG + divide start_ARG italic_k italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_α italic_ε end_ARG + divide start_ARG italic_k roman_log ( 1 / italic_δ ) end_ARG start_ARG italic_α italic_ε end_ARG ) . Note that for d=1𝑑1d=1italic_d = 1, this matches the upper bound of Theorem 1.5, thus showing that our univariate result is near-optimal in all parameters k,α,ε,δ𝑘𝛼𝜀𝛿k,\alpha,\varepsilon,\deltaitalic_k , italic_α , italic_ε , italic_δ. Moreover, our lower bound refutes the conjecture of [AAL21], which conjectures that only Θ⁢(kα2+kα⁢ε+log⁡(1/δ)ε)Θ𝑘superscript𝛼2𝑘𝛼𝜀1𝛿𝜀\Theta\left(\frac{k}{\alpha^{2}}+\frac{k}{\alpha\varepsilon}+\frac{\log(1/% \delta)}{\varepsilon}\right)roman_Θ ( divide start_ARG italic_k end_ARG start_ARG italic_α start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG + divide start_ARG italic_k end_ARG start_ARG italic_α italic_ε end_ARG + divide start_ARG roman_log ( 1 / italic_δ ) end_ARG start_ARG italic_ε end_ARG ) samples are needed in the univariate case and Θ⁢(k⁢d2α2+k⁢d2α⁢ε+log⁡(1/δ)ε)Θ𝑘superscript𝑑2superscript𝛼2𝑘superscript𝑑2𝛼𝜀1𝛿𝜀\Theta\left(\frac{kd^{2}}{\alpha^{2}}+\frac{kd^{2}}{\alpha\varepsilon}+\frac{% \log(1/\delta)}{\varepsilon}\right)roman_Θ ( divide start_ARG italic_k italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_α start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG + divide start_ARG italic_k italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_α italic_ε end_ARG + divide start_ARG roman_log ( 1 / italic_δ ) end_ARG start_ARG italic_ε end_ARG ) samples are needed in the d𝑑ditalic_d-dimensional case. However, we note that our lower bound asymptotically differs from the conjectured bound in [AAL21] only when δ𝛿\deltaitalic_δ is extremely small. 1.2 Related work In the non-private setting, the sample complexity of learning unrestricted GMMs with respect to total variation distance (a.k.a. density estimation) is known to be Θ~⁢(k⁢d2/α2)~Θ𝑘superscript𝑑2superscript𝛼2\widetilde{\Theta}(kd^{2}/\alpha^{2})over~ start_ARG roman_Θ end_ARG ( italic_k italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT / italic_α start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) [ABM18, ABH+18], where the upper bound is obtained by the so-called distributional compression schemes. In the private setting, the only known sample complexity upper bound for unrestricted GMMs [AAL24b] is roughly k2⁢d4⁢log⁡(1/δ)/(α4⁢ε)superscript𝑘2superscript𝑑41𝛿superscript𝛼4𝜀k^{2}d^{4}\log(1/\delta)/(\alpha^{4}\varepsilon)italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT roman_log ( 1 / italic_δ ) / ( italic_α start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT italic_ε ), which exhibits sub-optimal dependence on various parameters444More precisely, the upper bound is O~⁢(k2⁢d4α2⁢ε+k⁢d2⁢log⁡(1/δ)α2⁢ε+k⁢d⁢log⁡(1/δ)α3⁢ε+k2⁢d2α4⁢ε)~𝑂superscript𝑘2superscript𝑑4superscript𝛼2𝜀𝑘superscript𝑑21𝛿superscript𝛼2𝜀𝑘𝑑1𝛿superscript𝛼3𝜀superscript𝑘2superscript𝑑2superscript𝛼4𝜀\widetilde{O}\left(\frac{k^{2}d^{4}}{\alpha^{2}\varepsilon}+\frac{kd^{2}\log(1% /\delta)}{\alpha^{2}\varepsilon}+\frac{kd\log(1/\delta)}{\alpha^{3}\varepsilon% }+\frac{k^{2}d^{2}}{\alpha^{4}\varepsilon}\right)over~ start_ARG italic_O end_ARG ( divide start_ARG italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT end_ARG start_ARG italic_α start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_ε end_ARG + divide start_ARG italic_k italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT roman_log ( 1 / italic_δ ) end_ARG start_ARG italic_α start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_ε end_ARG + divide start_ARG italic_k italic_d roman_log ( 1 / italic_δ ) end_ARG start_ARG italic_α start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT italic_ε end_ARG + divide start_ARG italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_α start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT italic_ε end_ARG ). This bound is achieved by running multiple non-private list-decoders and then privately aggregating the results. For the special case of axis-aligned GMMs, an upper bound of k2dlog(1/δ)3/2/(α2ε)k^{2}d\log(1/\delta)^{3/2}/(\alpha^{2}\varepsilon)italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d roman_log ( 1 / italic_δ ) start_POSTSUPERSCRIPT 3 / 2 end_POSTSUPERSCRIPT / ( italic_α start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_ε ) is known [AAL21]. These are the only known results even for privately learning (unbounded) univariate GMMs. In other words, the best known upper bound for sample complexity of privately learning univariate GMMs has quadratic dependence on k𝑘kitalic_k. In the related public-private setting [BKS22, BBC+23], it is assumed that the learner has access to some public data. In this setting, [BBC+23] show that unrestricted GMMs can be learned with a moderate amount of public and private data. Assuming the parameters of the Gaussian components (and the condition numbers of the covariance matrices) are bounded, one can create a cover for GMMs and use private hypothesis selection [BSKW19] or the private minimum distance estimator [AAK21] to learn the GMM. On the flip side, [ASZ21] prove a lower bound on the sample complexity of learning GMMs, though their lower bound is weaker than ours and is only against pure-DP algorithms. The focus of our work is on density estimation. A related problem is learning the parameters a GMM, which has received extensive attention in the (non-private) literature (e.g., [Das99, MV10, BS10, LM21, BDJ+22, LL22] among many other papers). To avoid identifiability issues, one has to assume that the Gaussian components are sufficiently separated and have large-enough weights. In the private setting, the early work of [NRS07] demonstrated a privatized version of [VW04] for learning GMMs with fixed (known) covariance matrices. The strong separation assumption (of Ω⁢(k1/4)Ωsuperscript𝑘14\Omega(k^{1/4})roman_Ω ( italic_k start_POSTSUPERSCRIPT 1 / 4 end_POSTSUPERSCRIPT )) between the Gaussian components in [NRS07] was later relaxed to a weaker separation assumption [CCAd+23]. A substantially more general result for privately learning GMMs with unknown covariance matrices was established in [KSSU19], based on a privatized version of [AM05]. Yet, this approach also requires a polynomial separation (in terms of k𝑘kitalic_k) between the components, as well as a bound on the spectrum of the covariance matrices. [CKM+21] weakened the separation assumption of [KSSU19] and improved over their sample complexity. This result is based on a generic method that learns a GMM using a private learner for Gaussians and a non-private clustering method for GMMs. Finally, [AAL23] designed an efficient reduction from private learning of GMMs to its non-private counterpart, removing the boundedness assumptions on the parameters and achieving minimal separation (e.g., by reducing to [MV10]). Nevertheless, unlike density estimation, parameter estimation for unrestricted GMMs requires exponentially many samples in terms of k𝑘kitalic_k [MV10]. A final important question is that of efficient algorithms for learning GMMs. Much of the work on learning GMM parameters focuses on computational efficiency (e.g,. [MV10, BS10, LM21, BDJ+22, LL22]), as does some work on density estimation (e.g., [CDSS14, ADLS17]). However, under some standard hardness assumptions, it is known that even non-privately learning mixtures of k𝑘kitalic_k d𝑑ditalic_d-dimensional Gaussians with respect to total variation distance cannot be done in polynomial time as a function of k𝑘kitalic_k and d𝑑ditalic_d [DKS17, BRST21, GVV22]. Addendum. In a concurrent submission, [AAL24a] extended the result of [AAL24b] for learning unrestricted GMMs to the agnostic (i.e., robust) setting. In contrast, our algorithm works only in the realizable (non-robust) setting. Moreover, [AAL24a] slightly improved the sample complexity result of [AAL24b] from O~⁢(log⁡(1/δ)⁢k2⁢d4/(ε⁢α4))~𝑂1𝛿superscript𝑘2superscript𝑑4𝜀superscript𝛼4\widetilde{O}({\log(1/\delta)k^{2}d^{4}}/({\varepsilon\alpha^{4}}))over~ start_ARG italic_O end_ARG ( roman_log ( 1 / italic_δ ) italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT / ( italic_ε italic_α start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT ) ) to O~⁢(log⁡(1/δ)⁢k2⁢d4/(ε⁢α2))~𝑂1𝛿superscript𝑘2superscript𝑑4𝜀superscript𝛼2\widetilde{O}({\log(1/\delta)k^{2}d^{4}}/({\varepsilon\alpha^{2}}))over~ start_ARG italic_O end_ARG ( roman_log ( 1 / italic_δ ) italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT / ( italic_ε italic_α start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) ). The sample complexity of our approach is still significantly better than [AAL24a] in terms of all parameters—similar to the way it improved over [AAL24b]."
https://arxiv.org/html/2411.02087v2,"An Exponential Separation Between Quantum and Quantum-Inspired
Classical Algorithms for Machine Learning","Achieving a provable exponential quantum speedup for an important machine learning task has been a central research goal since the seminal HHL quantum algorithm for solving linear systems and the subsequent quantum recommender systems algorithm by Kerenidis and Prakash. These algorithms were initially believed to be strong candidates for exponential speedups, but a lower bound ruling out similar classical improvements remained absent. In breakthrough work by Tang, it was demonstrated that this lack of progress in classical lower bounds was for good reasons. Concretely, she gave a classical counterpart of the quantum recommender systems algorithm, reducing the quantum advantage to a mere polynomial. Her approach is quite general and was named quantum-inspired classical algorithms. Since then, almost all the initially exponential quantum machine learning speedups have been reduced to polynomial via new quantum-inspired classical algorithms. From the current state-of-affairs, it is unclear whether we can hope for exponential quantum speedups for any natural machine learning task.In this work, we present the first such provable exponential separation between quantum and quantum-inspired classical algorithms. We prove the separation for the basic problem of solving a linear system when the input matrix is well-conditioned and has sparse rows and columns.","Demonstrating an exponential quantum advantage for a relevant machine learning task has been an important research goal since the promising quantum algorithm by Harrow, Hassidim and Lloyd [12] for solving linear systems. Ignoring a few details, the HHL algorithm (and later improvements [4, 9]) generates a quantum state ∑i=1nxi⁢|i⟩superscriptsubscript𝑖1𝑛subscript𝑥𝑖ket𝑖\sum_{i=1}^{n}x_{i}|i\rangle∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_i ⟩ corresponding to the solution x=M−1⁢y𝑥superscript𝑀1𝑦x=M^{-1}yitalic_x = italic_M start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT italic_y to an n×n𝑛𝑛n\times nitalic_n × italic_n linear system of equations M⁢x=y𝑀𝑥𝑦Mx=yitalic_M italic_x = italic_y in just poly⁡(ln⁡n)poly𝑛\operatorname{poly}(\ln n)roman_poly ( roman_ln italic_n ) time. At first sight, this seems exponentially faster than any classic algorithm, which probably has to read the entire input matrix M𝑀Mitalic_M to solve the same problem. However, as pointed out e.g. by Aaronson [1], the analysis of the HHL algorithm assumes the input matrix is given in a carefully chosen input format. Taking this state preparation into consideration, it was initially unclear how the performance could be compared to a classical algorithm and whether any quantum advantage remained. The shortcoming of the HHL algorithm regarding state preparation was later addressed in seminal work by Kerenedis and Prakash [14], who gave an end-to-end analysis (i.e. including state preparation) that can be directly compared to a classical algorithm. Concretely, their framework assumes that the input matrices and vectors to a linear algebraic machine learning problem are given as simple classical data structures, but with quantum access to the memory representations. At the time, their new quantum algorithm (for recommender systems) was exponentially faster than the best classical counterpart (which is given the same classical data structures as input). Their work sparked a fruitful line of research, yielding exponential speedups for a host of important machine learning tasks, including solving linear systems [6], linear regression [6], PCA [6], recommender systems [14], supervised clustering [15] and Hamiltonian simulation [10]. Despite the exponential speedups over classical algorithms, a lower bound for classical algorithms ruling out a similar improvement via new algorithmic ideas remained illusive. It turned out that this was for good reasons: In breakthrough work by Tang [20], it was demonstrated that on all inputs where the recommender systems algorithm by Kerenedis and Prakash yielded an exponential speedup, a similar speedup could be obtained via a classical algorithmic approach that she dubbed quantum-inspired classical (QIC) algorithms. Since then, almost all the initially exponential speedups from quantum algorithms have been reduced to mere polynomial speedups through the development of new efficient QIC algorithms, see e.g. [5, 7, 19]. The disheartening state-of-affairs is thus that only a few machine learning problems remain where there is still an exponential gap between quantum and QIC algorithms. Based on Tang’s work, it remains entirely plausible that new QIC algorithms may close these gaps as well. Our Contribution. In this work, we present the first provable exponential separation between quantum and quantum-inspired classical algorithms for a central machine learning problem. Concretely, we prove a lower bound for any QIC algorithm for solving linear systems with sparse rows and columns. The lower bound is exponentially higher than known quantum upper bounds [6] when the matrix is well-conditioned, thus establishing the separation. 1.1 Quantum-Inspired Classical Algorithms In the following, we formally introduce QIC algorithms, the linear system problem, our lower bound statement and previous work on proving separations between quantum and QIC algorithms. As mentioned earlier, the work by Kerenidis and Prakash [14] gave a rigorous framework for directly comparing a quantum algorithm for a machine learning task with a classical counterpart. Taking state preparation into account, they define a natural input format for matrices and vectors in linear algebraic problems. At a high level, they assume the input is presented as a classical binary tree based data structure over the entries of the rows and columns of a matrix. They then built their quantum recommender system algorithm assuming quantum access to the memory representation of this classical data structure. Follow-up works have used essentially the same input representation or equivalent formulations. In many cases, for sufficiently well-conditioned matrices, the obtained quantum algorithms run in just poly⁡(ln⁡n)poly𝑛\operatorname{poly}(\ln n)roman_poly ( roman_ln italic_n ) time. Now to prove a separation between quantum and classical algorithms, any fair comparison should use the same input representation. Given the simplicity of the data structure by Kerenidis and Prakash for representing the input, it seemed reasonable to conjecture that any classical algorithm for e.g. recommender systems would need polynomial time even when given this data structure. This intuition was however proven false by Tang [20]. Her key insight was that the classical data structure allows efficient classical (i.e. poly⁡(ln⁡n)poly𝑛\operatorname{poly}(\ln n)roman_poly ( roman_ln italic_n ) time) ℓ22superscriptsubscriptℓ22\ell_{2}^{2}roman_ℓ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT sampling (formally defined below) from the rows and columns of the input, as well as efficient reading of individual entries. Exploiting this sampling access, she gave a classical algorithm for recommender systems that runs in just poly⁡(ln⁡n)poly𝑛\operatorname{poly}(\ln n)roman_poly ( roman_ln italic_n ) time on all matrices where the quantum algorithm by Kerenidis and Prakash does. She referred to such classical algorithms with ℓ22superscriptsubscriptℓ22\ell_{2}^{2}roman_ℓ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT sampling access to input matrices and vectors as quantum-inspired classical algorithms. This sampling access has since then proved extremely useful in other machine learning tasks, see e.g. [5, 7, 19]. Tang [20] summarized the above discussion as follows: “when quantum machine learning algorithms are compared to classical machine learning algorithms in the context of finding speedups, any state preparation assumptions in the quantum machine learning model should be matched with ℓ22superscriptsubscriptℓ22\ell_{2}^{2}roman_ℓ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT-norm sampling assumptions in the classical machine learning model’’. Using the notation of Mande and Shao [17], QIC algorithms formally have the following access to the input: Definition 1 (Query Access). For a vector v∈ℝn𝑣superscriptℝ𝑛v\in\mathbb{R}^{n}italic_v ∈ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, we have Q⁢(v)𝑄𝑣Q(v)italic_Q ( italic_v ), query access to v𝑣vitalic_v, if for all i𝑖iitalic_i, we can query visubscript𝑣𝑖v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. Likewise for a matrix M∈ℝm×n𝑀superscriptℝ𝑚𝑛M\in\mathbb{R}^{m\times n}italic_M ∈ blackboard_R start_POSTSUPERSCRIPT italic_m × italic_n end_POSTSUPERSCRIPT, we have query access to M𝑀Mitalic_M if for all (i,j)∈[m]×[n]𝑖𝑗delimited-[]𝑚delimited-[]𝑛(i,j)\in[m]\times[n]( italic_i , italic_j ) ∈ [ italic_m ] × [ italic_n ], we can query Mi,jsubscript𝑀𝑖𝑗M_{i,j}italic_M start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT. Definition 2 (Sampling and Query Access to a Vector). For a vector v∈ℝn𝑣superscriptℝ𝑛v\in\mathbb{R}^{n}italic_v ∈ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, we have S⁢Q⁢(v)𝑆𝑄𝑣SQ(v)italic_S italic_Q ( italic_v ), sampling and query access to v𝑣vitalic_v, if we can • Query for entries of v𝑣vitalic_v as in Q⁢(v)𝑄𝑣Q(v)italic_Q ( italic_v ). • Obtain independent samples of indices i∈[n]𝑖delimited-[]𝑛i\in[n]italic_i ∈ [ italic_n ], each distributed as ℙ⁢[i]=vi2/‖v‖2ℙdelimited-[]𝑖superscriptsubscript𝑣𝑖2superscriptnorm𝑣2\mathbb{P}[i]=v_{i}^{2}/\|v\|^{2}blackboard_P [ italic_i ] = italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT / ∥ italic_v ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. • Query for ‖v‖norm𝑣\|v\|∥ italic_v ∥. Definition 3 (Sampling and Query Access to a Matrix). For a matrix M∈ℝm×n𝑀superscriptℝ𝑚𝑛M\in\mathbb{R}^{m\times n}italic_M ∈ blackboard_R start_POSTSUPERSCRIPT italic_m × italic_n end_POSTSUPERSCRIPT, we have S⁢Q⁢(M)𝑆𝑄𝑀SQ(M)italic_S italic_Q ( italic_M ) if we have S⁢Q⁢(Mi,⋆)𝑆𝑄subscript𝑀𝑖⋆SQ(M_{i,\star})italic_S italic_Q ( italic_M start_POSTSUBSCRIPT italic_i , ⋆ end_POSTSUBSCRIPT ), S⁢Q⁢(M⋆,j)𝑆𝑄subscript𝑀⋆𝑗SQ(M_{\star,j})italic_S italic_Q ( italic_M start_POSTSUBSCRIPT ⋆ , italic_j end_POSTSUBSCRIPT ), S⁢Q⁢(r)𝑆𝑄𝑟SQ(r)italic_S italic_Q ( italic_r ) and S⁢Q⁢(c)𝑆𝑄𝑐SQ(c)italic_S italic_Q ( italic_c ) for all i∈m𝑖𝑚i\in mitalic_i ∈ italic_m and j∈n𝑗𝑛j\in nitalic_j ∈ italic_n where r⁢(M)=(‖M1,⋆‖,…,‖Mm,⋆‖)𝑟𝑀normsubscript𝑀1⋆…normsubscript𝑀𝑚⋆r(M)=(\|M_{1,\star}\|,\dots,\|M_{m,\star}\|)italic_r ( italic_M ) = ( ∥ italic_M start_POSTSUBSCRIPT 1 , ⋆ end_POSTSUBSCRIPT ∥ , … , ∥ italic_M start_POSTSUBSCRIPT italic_m , ⋆ end_POSTSUBSCRIPT ∥ ) and c⁢(M)=(‖M⋆,1‖,…,‖M⋆,n‖)𝑐𝑀normsubscript𝑀⋆1…normsubscript𝑀⋆𝑛c(M)=(\|M_{\star,1}\|,\dots,\|M_{\star,n}\|)italic_c ( italic_M ) = ( ∥ italic_M start_POSTSUBSCRIPT ⋆ , 1 end_POSTSUBSCRIPT ∥ , … , ∥ italic_M start_POSTSUBSCRIPT ⋆ , italic_n end_POSTSUBSCRIPT ∥ ). Here Mi,⋆subscript𝑀𝑖⋆M_{i,\star}italic_M start_POSTSUBSCRIPT italic_i , ⋆ end_POSTSUBSCRIPT is the i𝑖iitalic_i’th row of M𝑀Mitalic_M, M⋆,jsubscript𝑀⋆𝑗M_{\star,j}italic_M start_POSTSUBSCRIPT ⋆ , italic_j end_POSTSUBSCRIPT is the j𝑗jitalic_j’th column, r⁢(M)𝑟𝑀r(M)italic_r ( italic_M ) is the vector of row-norms and c⁢(M)𝑐𝑀c(M)italic_c ( italic_M ) is the vector of column-norms of M𝑀Mitalic_M. With the input representation defined, we proceed to present the problem of solving a linear system via a QIC algorithm. Here one again needs to be careful for a fair comparison between quantum and QIC algorithms. Concretely, the known quantum algorithms for solving a linear system M⁢x=y𝑀𝑥𝑦Mx=yitalic_M italic_x = italic_y do not output the full solution x𝑥xitalic_x (which would take linear time), but instead a quantum state ∑ix~i⁢|i⟩subscript𝑖subscript~𝑥𝑖ket𝑖\sum_{i}\tilde{x}_{i}|i\rangle∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_i ⟩ for a x~~𝑥\tilde{x}over~ start_ARG italic_x end_ARG approximating the solution x𝑥xitalic_x. Taking measurements on such a state allows one to sample an index i𝑖iitalic_i with probability x~i2/‖x~‖2superscriptsubscript~𝑥𝑖2superscriptnorm~𝑥2\tilde{x}_{i}^{2}/\|\tilde{x}\|^{2}over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT / ∥ over~ start_ARG italic_x end_ARG ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. With this in mind, the classical analog of solving a linear system is as follows. Problem 1 (Linear Systems). Given S⁢Q⁢(M)𝑆𝑄𝑀SQ(M)italic_S italic_Q ( italic_M ) and S⁢Q⁢(y)𝑆𝑄𝑦SQ(y)italic_S italic_Q ( italic_y ) for a symmetric and real matrix M∈ℝn×n𝑀superscriptℝ𝑛𝑛M\in\mathbb{R}^{n\times n}italic_M ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_n end_POSTSUPERSCRIPT of full rank, a vector y∈ℝn𝑦superscriptℝ𝑛y\in\mathbb{R}^{n}italic_y ∈ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT and precision ε>0𝜀0\varepsilon>0italic_ε > 0, the Linear Systems problem is to support sampling an index i𝑖iitalic_i with probability x~i2/‖x~‖2superscriptsubscript~𝑥𝑖2superscriptnorm~𝑥2\tilde{x}_{i}^{2}/\|\tilde{x}\|^{2}over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT / ∥ over~ start_ARG italic_x end_ARG ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT from a vector x~~𝑥\tilde{x}over~ start_ARG italic_x end_ARG satisfying that ‖x~−x‖≤ε⁢‖x‖norm~𝑥𝑥𝜀norm𝑥\|\tilde{x}-x\|\leq\varepsilon\|x\|∥ over~ start_ARG italic_x end_ARG - italic_x ∥ ≤ italic_ε ∥ italic_x ∥ where x=M−1⁢y𝑥superscript𝑀1𝑦x=M^{-1}yitalic_x = italic_M start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT italic_y is the solution to the linear system of equations M⁢x=y𝑀𝑥𝑦Mx=yitalic_M italic_x = italic_y. The query complexity of a QIC algorithm for solving a linear system, is the number of queries to S⁢Q⁢(M)𝑆𝑄𝑀SQ(M)italic_S italic_Q ( italic_M ) and S⁢Q⁢(y)𝑆𝑄𝑦SQ(y)italic_S italic_Q ( italic_y ) necessary to sample one index i𝑖iitalic_i from x~~𝑥\tilde{x}over~ start_ARG italic_x end_ARG. We remark that the known QIC algorithms furthermore output the value x~isubscript~𝑥𝑖\tilde{x}_{i}over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT upon sampling i𝑖iitalic_i. Since we aim to prove a lower bound, our results are only stronger if we prove it for merely sampling i𝑖iitalic_i. Quantum Benchmark. To prove our exponential separation, we first present the state-of-the-art performance of quantum algorithms for linear systems. Here we focus on the case where the input matrix M𝑀Mitalic_M has sparse rows and columns, i.e. every row and column has at most s𝑠sitalic_s non-zero entries. The running time of the best known quantum algorithm depends on the condition number of M𝑀Mitalic_M, defined as κ=σmax/σmin.𝜅subscript𝜎subscript𝜎\kappa=\sigma_{\max}/\sigma_{\min}.italic_κ = italic_σ start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT / italic_σ start_POSTSUBSCRIPT roman_min end_POSTSUBSCRIPT . Here σmaxsubscript𝜎\sigma_{\max}italic_σ start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT is the largest singular value of M𝑀Mitalic_M and σminsubscript𝜎\sigma_{\min}italic_σ start_POSTSUBSCRIPT roman_min end_POSTSUBSCRIPT is the smallest singular value. Note that for real symmetric M𝑀Mitalic_M of full rank, all eigenvalues λ1≥⋯≥λnsubscript𝜆1⋯subscript𝜆𝑛\lambda_{1}\geq\cdots\geq\lambda_{n}italic_λ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ≥ ⋯ ≥ italic_λ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT of M𝑀Mitalic_M are real and non-zero, and the singular values σmax=σ1≥⋯≥σn=σmin>0subscript𝜎subscript𝜎1⋯subscript𝜎𝑛subscript𝜎0\sigma_{\max}=\sigma_{1}\geq\cdots\geq\sigma_{n}=\sigma_{\min}>0italic_σ start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT = italic_σ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ≥ ⋯ ≥ italic_σ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT = italic_σ start_POSTSUBSCRIPT roman_min end_POSTSUBSCRIPT > 0 are the absolute values of the eigenvalues {|λi|}i=1nsuperscriptsubscriptsubscript𝜆𝑖𝑖1𝑛\{|\lambda_{i}|\}_{i=1}^{n}{ | italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT in sorted order. Given a precision ε>0𝜀0\varepsilon>0italic_ε > 0, matrix M𝑀Mitalic_M and vector y𝑦yitalic_y as input (in the classical data structure format), the quantum algorithm by Chakraborty, Gilyén and Jeffery [6] runs in time poly⁡(s,κ,ln⁡(1/ε),ln⁡n)poly𝑠𝜅1𝜀𝑛\displaystyle\operatorname{poly}(s,\kappa,\ln(1/\varepsilon),\ln n)roman_poly ( italic_s , italic_κ , roman_ln ( 1 / italic_ε ) , roman_ln italic_n ) (1) to produce a quantum state ∑ix~i⁢|i⟩subscript𝑖subscript~𝑥𝑖ket𝑖\sum_{i}\tilde{x}_{i}|i\rangle∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_i ⟩ for a x~~𝑥\tilde{x}over~ start_ARG italic_x end_ARG with ‖x~−x‖≤ε⁢‖x‖norm~𝑥𝑥𝜀norm𝑥\|\tilde{x}-x\|\leq\varepsilon\|x\|∥ over~ start_ARG italic_x end_ARG - italic_x ∥ ≤ italic_ε ∥ italic_x ∥ with x=M−1⁢y𝑥superscript𝑀1𝑦x=M^{-1}yitalic_x = italic_M start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT italic_y. We remark that to derive (1) from [6], one invokes their Lemma 11 (originating in [11]) to obtain a block-encoding of a sparse matrix and then invoke their Theorem 30. See also the recent work [16]. QIC Benchmark. The best QIC algorithm [19] for sparse linear systems instead has a query complexity (and running time) of poly⁡(s,κF,ln⁡(1/ε),ln⁡n),poly𝑠subscript𝜅𝐹1𝜀𝑛\displaystyle\operatorname{poly}(s,\kappa_{F},\ln(1/\varepsilon),\ln n),roman_poly ( italic_s , italic_κ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT , roman_ln ( 1 / italic_ε ) , roman_ln italic_n ) , (2) where κF=‖M‖F/σmin=∑iσi2σmin.subscript𝜅𝐹subscriptnorm𝑀𝐹subscript𝜎subscript𝑖superscriptsubscript𝜎𝑖2subscript𝜎\kappa_{F}=\|M\|_{F}/\sigma_{\min}=\frac{\sqrt{\sum_{i}\sigma_{i}^{2}}}{\sigma% _{\min}}.italic_κ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT = ∥ italic_M ∥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT / italic_σ start_POSTSUBSCRIPT roman_min end_POSTSUBSCRIPT = divide start_ARG square-root start_ARG ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_σ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG end_ARG start_ARG italic_σ start_POSTSUBSCRIPT roman_min end_POSTSUBSCRIPT end_ARG . Since κFsubscript𝜅𝐹\kappa_{F}italic_κ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT may be larger than κ𝜅\kappaitalic_κ by as much as a n𝑛\sqrt{n}square-root start_ARG italic_n end_ARG factor, there are thus matrices with κ,s=poly⁡(ln⁡n)𝜅𝑠poly𝑛\kappa,s=\operatorname{poly}(\ln n)italic_κ , italic_s = roman_poly ( roman_ln italic_n ) where there is an exponential gap between (1) and (2). However, proving that a QIC algorithm with a performance matching (1) cannot be developed has so far remained out of reach. Our Result. We show the following strong lower bound for QIC algorithms Theorem 1. There is a constant c>0𝑐0c>0italic_c > 0, such that for n≥c𝑛𝑐n\geq citalic_n ≥ italic_c and any precision ε≤(c⁢ln2.5⁡n)−1𝜀superscript𝑐superscript2.5𝑛1\varepsilon\leq(c\ln^{2.5}n)^{-1}italic_ε ≤ ( italic_c roman_ln start_POSTSUPERSCRIPT 2.5 end_POSTSUPERSCRIPT italic_n ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT, it holds that for any QIC algorithm 𝒜𝒜\mathcal{A}caligraphic_A for linear systems, there exists a full rank n×n𝑛𝑛n\times nitalic_n × italic_n symmetric real matrix M𝑀Mitalic_M with condition number κ≤c⁢ln2⁡n𝜅𝑐superscript2𝑛\kappa\leq c\ln^{2}nitalic_κ ≤ italic_c roman_ln start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_n and 4444-sparse rows and columns, such that 𝒜𝒜\mathcal{A}caligraphic_A must make Ω⁢(n1/12)Ωsuperscript𝑛112\Omega(n^{1/12})roman_Ω ( italic_n start_POSTSUPERSCRIPT 1 / 12 end_POSTSUPERSCRIPT ) queries to S⁢Q⁢(M)𝑆𝑄𝑀SQ(M)italic_S italic_Q ( italic_M ) on the linear system M⁢x=e1𝑀𝑥subscript𝑒1Mx=e_{1}italic_M italic_x = italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. Observe that the complexity of the best quantum algorithm (1) for this setting of s,κ𝑠𝜅s,\kappaitalic_s , italic_κ and ε𝜀\varepsilonitalic_ε is just poly⁡(ln⁡n)poly𝑛\operatorname{poly}(\ln n)roman_poly ( roman_ln italic_n ), hence the claimed exponential separation. Furthermore, the matrix M𝑀Mitalic_M is extremely sparse, with only s=4𝑠4s=4italic_s = 4 non-zeroes per row and column, and the vector y𝑦yitalic_y in the linear system M⁢x=y𝑀𝑥𝑦Mx=yitalic_M italic_x = italic_y is simply the first standard unit vector e1subscript𝑒1e_{1}italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. Previous Separations. Finally, let us mention recent work by Mande and Shao [17] that also focuses on separating quantum and QIC algorithms. Using reductions from number-in-hand multiparty communication complexity [18], they prove a number of lower bounds for QIC algorithms for linear regression, supervised clustering, PCA, recommender systems and Hamiltonian simulation. Their lower bounds are of the form Ω~⁢(κF2)~Ωsuperscriptsubscript𝜅𝐹2\tilde{\Omega}(\kappa_{F}^{2})over~ start_ARG roman_Ω end_ARG ( italic_κ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ), but only for problems where the best known quantum algorithms are no better than O~⁢(κF)~𝑂subscript𝜅𝐹\tilde{O}(\kappa_{F})over~ start_ARG italic_O end_ARG ( italic_κ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT ), thus establishing quadratic separations compared to our exponential separation. Let us also remark that our lower bound proof takes a completely different approach, instead reducing from a problem of random walks by Childs et al. [8]."
https://arxiv.org/html/2411.01246v1,"CAMP: A Cost Adaptive Multi-Queue Eviction Policy for Key-Value Stores††thanks:Sandy Irani and Jenny Lam are with the University of California, Irvine. Their research is supported in part by the NSF grant CCF-0916181. Shahram Ghandeharizadeh and Jason Yap are with the University of Southern California.111A shorter version of CAMP appeared in the Proceedings of the ACM/IFIP/USENIX
Middleware Conference, Bordeaux, France, December 2014. Seehttps://github.com/scdblab/CAMPfor an implementation.","Cost Adaptive Multi-queue eviction Policy (CAMP) is an algorithm for a general purpose key-value store (KVS) that manages key-value pairs computed by applications with different access patterns, key-value sizes, and varying costs for each key-value pair. CAMP is an approximation of the Greedy Dual Size (GDS) algorithm that can be implemented as efficiently as LRU. In particular, CAMP’s eviction policies are as effective as those of GDS but require only a small fraction of the updates to an internal data structure in order to make those decisions. Similar to an implementation of LRU using queues, it adapts to changing workload patterns based on the history of requests for different key-value pairs. It is superior to LRU because it considers both the size and cost of key-value pairs to maximize the utility of the available memory across competing applications. We compare CAMP with both LRU and an alternative that requires human intervention to partition memory into pools and assign grouping of key-value pairs to different pools. The results demonstrate CAMP is as fast as LRU while outperforming both LRU and the pooled alternative. We also present results from an implementation of CAMP using Twitter’s version of memcached.","Applications with a high read-to-write ratio augment their persistent infrastructure with an in-memory key-value store (KVS) to enhance performance. An example is memcached in use by popular Internet destinations such as Facebook, Twitter, and Wikipedia. Using a general purpose caching layer requires workloads to share infrastructure despite different access patterns, key-value sizes, and time required to compute a key-value pair [21]. An algorithm that considers only one factor may cause different application workloads to impact one another negatively, decreasing the overall effectiveness of the caching layer. As an example, consider two different applications of a social networking site: one shows the profile of members while a second determines the displayed advertisements. There may exist millions of key-value pairs corresponding to different member profiles, each computed using a simple database look-up that executed in a few milliseconds. The second application may consist of thousands of key-value pairs computed using a machine-learning algorithm that processed Terabytes of data and required hours of execution. This processing time is one definition of the cost of a key-value pair. With a limited memory size and a high frequency of access for member profile key-value pairs, a simple algorithm that manages memory using recency of references (LRU) may evict most of the key-value pairs of the second application, increasing the incurred cost. In general, reducing the incurred cost translates into a faster system that processes a larger number of requests per unit of time and may provide a better quality of service. The latter is due to availability of data (e.g., cache hit for a key-value computed using the machine learning algorithm) that enables the application to provide a user with more relevant content than content selected randomly. A possible approach is for a human expert to partition the available memory into disjoint pools with each pool managed using LRU. Next, the expert groups key-value pairs with similar costs together and assigns each group to a different pool [18]. With our example, the expert would construct two pools. One for the key-value pairs corresponding to members profiles and a second corresponding to advertisements. The primary limitation222Partitioning is known to reduce the utilization of resources by resulting in formation of hot spots and bottlenecks. One may address this limitation by over-provisioning resources. of this approach is that it requires a human familiar with the different classes of applications to identify the pools, construct grouping of key-value pairs, and assign each group to a pool. Over time, the service provider may either introduce a new application or discontinue an existing one. This means the human expert must again become involved to identify the pool for the key-value pairs of the new application and possibly rebalance memory across the pools once an application is discontinued. This paper introduces a novel caching method called Cost Adaptive Multi-queue eviction Policy (CAMP), that manages the available memory without partitioning it. CAMP is an approximation of the Greedy Dual Size (GDS) algorithm [4] that processes cache hits and misses more efficiently using queues. Hence, it is significantly faster than GDS and as fast as LRU. It is novel and different from LRU in that it constructs multiple LRU queues dynamically based on the size and cost of key-value pairs. The number of constructed LRU queues depends on the distribution of costs and sizes of the key-value pairs. CAMP manages these LRU queues without partitioning memory. Thus, there is no need for human involvement to construct groups of key-value pairs, dictate assignment of groups to the pools, or configure and adjust the memory pool characteristics. CAMP is robust enough to prevent an aged expensive key-value pair from occupying memory indefinitely. Such a key-value pair is evicted by CAMP as competing applications issue more requests. CAMP is parameterized by a variable that controls its precision. At the highest precision, CAMP’s eviction decisions are essentially equivalent to those made by GDS. Our empirical results show that CAMP does not suffer any degradation in the quality of its eviction decisions at lower precisions. Moreover, it is able to make those decisions much more efficiently than GDS. GDS requires an internal priority queue to determine a key-value pair to evict from the cache. The time to maintain its data structures consistent in a thread-safe manner is expensive because it requires synchronization primitives [12] with multiple threads performing caching decisions. Moreover, CAMP performs a significantly fewer updates of its internal data structures than GDS, reducing the number of times it executes the thread-safe software dramatically. The rest of this paper is organized as follows. Section 2 starts with a description of GDS to motivate CAMP and details its design decisions. Section 3 presents a simulation study of CAMP and compares it with LRU and the pooled approach that partitions resources, demonstrating its superiority. Section 4 describes an implementation of CAMP using a variant of Twemcache and compares this implementation with the original that uses LRU. Obtained results demonstrate that CAMP is as fast as LRU and provides superior performance as it considers, in addition to recency of requests, the size and the cost of the key-value pairs. Section 5 describes related work. Section 6 provides brief words of conclusions and future research directions."
https://arxiv.org/html/2411.01115v1,Relax and Merge: A Simple Yet Effective Framework for Solving Fairk𝑘kitalic_k-Means andk𝑘kitalic_k-sparse Wasserstein Barycenter Problems,"The fairness of clustering algorithms has gained widespread attention across various areas in machine learning. In this paper, we study fair k𝑘kitalic_k-means clustering in Euclidean space. Given a dataset comprising several groups, the fairness constraint requires that each cluster should contain a proportion of points from each group within specified lower and upper bounds. Due to these fairness constraints, determining the locations of k𝑘kitalic_k centers and finding the induced partition are quite challenging tasks. We propose a novel “Relax and Merge” framework that returns a (1+4⁢ρ+O⁢(ϵ))14𝜌𝑂italic-ϵ(1+4\rho+O(\epsilon))( 1 + 4 italic_ρ + italic_O ( italic_ϵ ) )-approximate solution, where ρ𝜌\rhoitalic_ρ is the approximate ratio of an off-the-shelf vanilla k𝑘kitalic_k-means algorithm and O⁢(ϵ)𝑂italic-ϵO(\epsilon)italic_O ( italic_ϵ ) can be an arbitrarily small positive number. If equipped with a PTAS of k𝑘kitalic_k-means, our solution can achieve an approximation ratio of (5+O⁢(ϵ))5𝑂italic-ϵ(5+O(\epsilon))( 5 + italic_O ( italic_ϵ ) ) with only a slight violation of the fairness constraints, which improves the current state-of-the-art approximation guarantee. Furthermore, using our framework, we can also obtain a (1+4⁢ρ+O⁢(ϵ))14𝜌𝑂italic-ϵ(1+4\rho+O(\epsilon))( 1 + 4 italic_ρ + italic_O ( italic_ϵ ) )-approximate solution for the k𝑘kitalic_k-sparse Wasserstein Barycenter problem, which is a fundamental optimization problem in the field of optimal transport, and a (2+6⁢ρ)26𝜌(2+6\rho)( 2 + 6 italic_ρ )-approximate solution for the strictly fair k𝑘kitalic_k-means clustering with no violation, both of which are better than the current state-of-the-art methods. In addition, the empirical results demonstrate that our proposed algorithm can significantly outperform baseline approaches in terms of clustering cost.","Clustering is one of the most fundamental problems in the area of machine learning. A wide range of practical applications rely on effective clustering algorithms, such as feature engineering (Glassman et al., 2014; Alelyani et al., 2018), image processing (Coleman and Andrews, 1979; Chang et al., 2017), and bioinformatics (Ronan et al., 2016; Nugent and Meila, 2010). In particular, the k𝑘kitalic_k-means clustering problem has been extensively studied in the past decades (Jain, 2010). Given an input dataset P⊂ℝd𝑃superscriptℝ𝑑P\subset\mathbb{R}^{d}italic_P ⊂ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT, the goal of the k𝑘kitalic_k-means problem is to find a set S𝑆Sitalic_S of at most k𝑘kitalic_k points for minimizing the clustering cost, which is the sum of the squared distances from every point of P𝑃Pitalic_P to its nearest neighbor in S𝑆Sitalic_S. In recent years, motivated by various fields like education, social security, and cultural communication, the study on fairness of clustering has in particular attracted a great amount of attention (Chierichetti et al., 2017; Bera et al., 2019; Huang et al., 2019; Chen et al., 2019; Ghadiri et al., 2021). In this paper, we consider the problem of (α,β)αβ(\alpha,\beta)( italic_α , italic_β )-fair kkkitalic_k-means clustering that was initially proposed by Chierichetti et al. (2017) and then generalized by Bera et al. (2019). Informally speaking, we assume that the given dataset P𝑃Pitalic_P consists of m𝑚mitalic_m groups of points, and the “fairness” constraint requires that in each obtained cluster, the points from each group should take a fraction between pre-specified lower and upper bounds. Bera et al. (2019) showed that a ρ𝜌\rhoitalic_ρ-approximate algorithm for vanilla k𝑘kitalic_k-means can provide a (2+ρ)2superscript2𝜌2(2+\sqrt{\rho})^{2}( 2 + square-root start_ARG italic_ρ end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT- approximate solution 111In their paper, the approximate ratio is written as (2+ρ)2𝜌(2+\rho)( 2 + italic_ρ ) because they added a squared root to the k𝑘kitalic_k-means cost function. for (α,β)𝛼𝛽(\alpha,\beta)( italic_α , italic_β )-fair k𝑘kitalic_k-clustering with a slight violation on the fairness constraints, where the “violation” is formally defined in Section 2. Furthermore, Böhm et al. (2021) studied the “strictly” fair k𝑘kitalic_k-means clustering problem, where it requires that the number of points from each group should be uniform in every cluster; they obtained a (2+ρ)2superscript2𝜌2(2+\sqrt{\rho})^{2}( 2 + square-root start_ARG italic_ρ end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT approximate solution without violation. These fair k𝑘kitalic_k-means algorithms can also be accelerated by using the coreset techniques, such as (Huang et al., 2019; Braverman et al., 2022; Bandyapadhyay et al., 2024). There also exist polynomial-time approximation scheme (PTAS) for fair k𝑘kitalic_k-means, such as the algorithms proposed in (Böhm et al., 2021; Schmidt et al., 2020; Bandyapadhyay et al., 2024), but their methods have an exponential time complexity in k𝑘kitalic_k. We are also aware of several other different definitions of fairness for clustering problems, such as the proportionally fair clustering (Chen et al., 2019; Micha and Shah, 2020) and socially fair k𝑘kitalic_k-means clustering (Ghadiri et al., 2021). Another problem closely related to fair k𝑘kitalic_k-means is the so-called “k𝑘kitalic_k-sparse Wassertein Barycenter (WB)” (Agueh and Carlier, 2011) (the formal definition is shown in Section 2). The Wasserstein Barycenter is a fundamental concept in optimal transport theory, and it represents the “average” or central distribution of a set of probability distributions. It plays a crucial role in various applications such as image processing (Bonneel et al., 2015; Cuturi and Doucet, 2014), data analysis (Rabin et al., 2012), and machine learning (Backhoff-Veraguas et al., 2022; Metelli et al., 2019). Given m>1𝑚1m>1italic_m > 1 discrete distributions, the goal of the k𝑘kitalic_k-sparse WB problem is to find a discrete distribution (i.e., the barycenter) that minimizes the sum of the Wasserstein distances (Villani, 2021) between itself to all the given distributions, and meanwhile the support size of the barycenter is restricted to be no larger than a given integer k≥1𝑘1k\geq 1italic_k ≥ 1. If relaxing the “k𝑘kitalic_k-sparse” constraint (i.e., the barycenter is allowed to take a support size larger than k𝑘kitalic_k), Altschuler and Boix-Adsera (2021) presented an algorithm based on linear programming, which can compute the WB within fixed dimensions in polynomial time. If the locations of the WB supports are given, the problem is called “fixed support WB”, which can be solved by using several existing algorithms (Claici et al., 2018; Cuturi and Doucet, 2014; Cuturi and Peyré, 2016; Lin et al., 2020). If we keep the “k𝑘kitalic_k-sparse” constraint, it has been proved that the problem is NP-hard (Borgwardt and Patterson, 2021). To the best of our knowledge, the current lowest approximation ratio of k-sparse WB problem is also (2+ρ)2superscript2𝜌2(2+\sqrt{\rho})^{2}( 2 + square-root start_ARG italic_ρ end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT (same with the aforementioned approximation factor for fair k𝑘kitalic_k-means), as recently studied by Yang and Ding (2024). In fact, we can regard this problem as a special case of fair k𝑘kitalic_k-means clustering, where each input distribution is an individual group and the unique cost measured by “Wasserstein distance” is implicitly endowed with a kind of fairness. This observation from Yang and Ding (2024) inspires us to consider solving the k𝑘kitalic_k-sparse WB problem under our framework. Why fair k𝑘kitalic_k-means is so challenging? Though the fair k𝑘kitalic_k-means clustering has been extensively studied in recent years, their current state-of-the-art approximation qualities are still not that satisfying. The major difficulty arises from the lack of “locality property” (Ding and Xu, 2020; Bhattacharya et al., 2018) caused by fair constraints. More precisely, in a clustering result of vanilla k𝑘kitalic_k-means, each client point obviously belongs to its closest center. That is, a k𝑘kitalic_k-means clustering implicitly forms a Voronoi diagram, where the cell centers are exactly the k𝑘kitalic_k cluster centers, and the client points in each Voronoi cell form a cluster. However, when we add some fair constraints, such as requiring that the proportion of points of each group should be equal in each cluster, the situation becomes more complicated. Given a set of cluster center locations, because the groups of client points within a Voronoi cell may not be equally distributed, some points are forced to be assigned to other Voronoi cells. This loss of locality introduces significant uncertainty for the selection of cluster center positions. The previous works (Bera et al., 2019; Böhm et al., 2021) do not pay much attention on how to handle this locality issue when searching for the cluster centers, instead, they directly apply vanilla k𝑘kitalic_k-means algorithms to the entire input dataset or a group, and use the obtained center locations as the center locations for fair k𝑘kitalic_k-means. It is easy to notice that their methods could result in a certain gap with the optimal fair k𝑘kitalic_k-means solution. To narrow this gap, we attempt to design some more effective way to determine the center locations, where the key part that we believe, should be how to encode the fair constraints into the searching algorithm. Our key ideas and main results. Our key idea relies on an important observation, where we find that the fair k𝑘kitalic_k-means problem is inherently related to a classic geometric structure, “ϵitalic-ϵ\epsilonitalic_ϵ-approximate centroid set”, which was firstly proposed by Matoušek (2000). Roughly speaking, given a dataset, an ϵitalic-ϵ\epsilonitalic_ϵ-approximate centroid set should contain at least one point that approximately represents the centroid location of any subset of this given dataset. It means that the ϵitalic-ϵ\epsilonitalic_ϵ-approximate centroid set contains not only the approximate centroids based on the Voronoi diagram, but also the approximate centroids of those potential fairness-preserving clusters. Inspired by the above observation, we illustrate the relationship between fair k𝑘kitalic_k-means and ϵitalic-ϵ\epsilonitalic_ϵ-approximate centroid set first, and then propose a novel Relax-and-Merge framework. In this framework, we relax the constraints on the number of clusters k𝑘kitalic_k; we focus on utilizing fair constraints to cluster the data into small and fair clusters, which are then merged together to determine the positions of k𝑘kitalic_k cluster centers. As shown in Table 1, our result is better than the state of the art works (Bera et al., 2019; Böhm et al., 2021). Equipped with a PTAS for k𝑘kitalic_k-means problem (e.g., the algorithm from Cohen-Addad et al. (2019)), our algorithm yields a 5+O⁢(ϵ)5𝑂italic-ϵ{5+O(\epsilon)}5 + italic_O ( italic_ϵ ) approximation factor. We also present two important extensions from our work. The first extension is an (1+4⁢ρ+O⁢(ϵ))14𝜌𝑂italic-ϵ(1+4\rho+O(\epsilon))( 1 + 4 italic_ρ + italic_O ( italic_ϵ ) ) solution for k-sparse Wasserstein Barycenter. The second one is about strictly fair k𝑘kitalic_k-means. We give a refined algorithm of Relax and Merge that yields a no-violation solution with a (2+6⁢ρ)26𝜌(2+6\rho)( 2 + 6 italic_ρ ) approximation factor, which is better than the state of the art work (Böhm et al., 2021). Table 1: Comparison of the approximation ratios for fair k𝑘kitalic_k-means and k𝑘kitalic_k-sparse WB. The “general case” includes (α,β)𝛼𝛽(\alpha,\beta)( italic_α , italic_β )-fair k𝑘kitalic_k-means, strictly (α,β)𝛼𝛽(\alpha,\beta)( italic_α , italic_β )-fair k𝑘kitalic_k-means and k𝑘kitalic_k-sparse WB. Algorithms Approximation ratio When ρ=1+O⁢(ϵ)𝜌1𝑂italic-ϵ\rho=1+O(\epsilon)italic_ρ = 1 + italic_O ( italic_ϵ ) Note on the quality Bera et al. (2019) (2+ρ)2superscript2𝜌2(2+\sqrt{\rho})^{2}( 2 + square-root start_ARG italic_ρ end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT 9+O⁢(ϵ)9𝑂italic-ϵ9+O(\epsilon)9 + italic_O ( italic_ϵ ) general case Schmidt et al. (2020) 5.5⁢ρ+15.5𝜌15.5\rho+15.5 italic_ρ + 1 6.5+O⁢(ϵ)6.5𝑂italic-ϵ6.5+O(\epsilon)6.5 + italic_O ( italic_ϵ ) two groups only Böhm et al. (2021) (2+ρ)2superscript2𝜌2(2+\sqrt{\rho})^{2}( 2 + square-root start_ARG italic_ρ end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT 9+O⁢(ϵ)9𝑂italic-ϵ9+O(\epsilon)9 + italic_O ( italic_ϵ ) strictly only, no violation Yang and Ding (2024) (2+ρ)2superscript2𝜌2(2+\sqrt{\rho})^{2}( 2 + square-root start_ARG italic_ρ end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT 9+O⁢(ϵ)9𝑂italic-ϵ9+O(\epsilon)9 + italic_O ( italic_ϵ ) k𝑘kitalic_k-sparse WB Algorithm 1, now 1+4⁢ρ+O⁢(ϵ)14𝜌𝑂italic-ϵ1+4\rho+O(\epsilon)1 + 4 italic_ρ + italic_O ( italic_ϵ ) 5+O⁢(ϵ)5𝑂italic-ϵ5+O(\epsilon)5 + italic_O ( italic_ϵ ) general case Algorithm 2, now 2+6⁢ρ26𝜌2+6\rho2 + 6 italic_ρ 8+O⁢(ϵ)8𝑂italic-ϵ8+O(\epsilon)8 + italic_O ( italic_ϵ ) strictly only, no violation Other Related Works on k𝑘kitalic_k-Means The vanilla k𝑘kitalic_k-means problem is a topic that has been widely studied in both theory and practice. It has been proved that k𝑘kitalic_k-means clustering is NP-hard even in 2⁢D2𝐷2D2 italic_D if k𝑘kitalic_k is large (Mahajan et al., 2012). In high dimensions, even if k𝑘kitalic_k is fixed, say k=2𝑘2k=2italic_k = 2, the k𝑘kitalic_k-means problem is still NP-hard (Drineas et al., 2004). Furthermore, Lee et al. (2017) proved the APX-hardness result for Euclidean k𝑘kitalic_k-means problem, which implies that it is impossible to approximate the optimal solution of k𝑘kitalic_k-means below a factor 1.0013 in polynomial time under the assumption of P ≠\neq≠ NP. Therefore, a number of approximation algorithms have been proposed in theory. If the dimension d𝑑ditalic_d is fixed, Kanungo et al. (2002) obtained a (9+O⁢(ϵ)9𝑂italic-ϵ9+O(\epsilon)9 + italic_O ( italic_ϵ ))-approximate solution by using the local search technique. Roughly speaking, the idea of local search is swapping a small number of points in every iteration, so as to incrementally improve the solution until converging at some local optimum. Following this idea, Cohen-Addad et al. (2019) and Friggstad et al. (2019) proposed the PTAS for k𝑘kitalic_k-means in low dimensional space. For high-dimensional case with constant k𝑘kitalic_k, Kumar et al. (2010) proposed an elegant peeling algorithm that iteratively finds the k𝑘kitalic_k cluster centers and eventually obtain the PTAS."
https://arxiv.org/html/2411.00384v1,Perfect Matchings and Popularity in the Many-to-Many Setting††thanks:This result appeared in FSTTCS 2023[29]. This version gives a simple proof that corrects a technical gap in the proof for the many-to-many setting in the conference version.,"We consider a matching problem in a bipartite graph G𝐺Gitalic_G where every vertex has a capacity and a strict preference order on its neighbors. Furthermore, there is a cost function on the edge set. We assume G𝐺Gitalic_G admits a perfect matching, i.e., one that fully matches all vertices. It is only perfect matchings that are feasible for us and we are interested in those perfect matchings that are popular within the set of perfect matchings. It is known that such matchings (called popular perfect matchings) always exist and can be efficiently computed. What we seek here is not any popular perfect matching, but a min-cost one. We show a polynomial-time algorithm for finding such a matching; this is via a characterization of popular perfect matchings in G𝐺Gitalic_G in terms of stable matchings in a colorful auxiliary instance. This is a generalization of such a characterization that was known in the one-to-one setting.","Consider a matching problem in a bipartite graph G=(A∪B,E)𝐺𝐴𝐵𝐸G=(A\cup B,E)italic_G = ( italic_A ∪ italic_B , italic_E ) where every vertex v∈A∪B𝑣𝐴𝐵v\in A\cup Bitalic_v ∈ italic_A ∪ italic_B has a strict ranking of its neighbors. For convenience, vertices in A𝐴Aitalic_A will be called agents and those in B𝐵Bitalic_B will be called jobs. Every agent/job v𝑣vitalic_v has an integral capacity 𝖼𝖺𝗉⁢(v)≥1𝖼𝖺𝗉𝑣1\mathsf{cap}(v)\geq 1sansserif_cap ( italic_v ) ≥ 1 and seeks to be matched to 𝖼𝖺𝗉⁢(v)𝖼𝖺𝗉𝑣\mathsf{cap}(v)sansserif_cap ( italic_v ) many neighbors. This is the bipartite b𝑏bitalic_b-matching framework. This model is also called the many-to-many setting in matchings under preferences. This is a well-studied model that is a natural generalization of several real-world settings such as matching students to schools and colleges [1, 3] and residents to hospitals [8, 35]. Note that agents (i.e., students/residents) in these applications have capacity 1 while in the many-to-many setting, agents are allowed to have capacity more than 1. For example, when A𝐴Aitalic_A is a set of students and B𝐵Bitalic_B is a set of projects, the many-to-many setting allows for the flexibility that not only can several students work on one project, but a student can be part of several projects. One such application is a management system for a conference where program committee members have preferences over submissions and each submission has a ranking of PC members as per their suitability – in terms of their areas of expertise – to review this paper.111PC members typically have rankings with ties over papers; we can instead assume a project evaluation committee where each member has a strict ranking of the project proposals that he/she would like to evaluate and each proposal needs a certain number of independent evaluations. Every PC member (also, assume every paper) has a capacity. Definition 1 A matching M𝑀Mitalic_M in a many-to-many instance G=(A∪B,E)𝐺𝐴𝐵𝐸G=(A\cup B,E)italic_G = ( italic_A ∪ italic_B , italic_E ) is a subset of the edge set E𝐸Eitalic_E such that |M⁢(v)|≤𝖼𝖺𝗉⁢(v)𝑀𝑣𝖼𝖺𝗉𝑣|M(v)|\leq\mathsf{cap}(v)| italic_M ( italic_v ) | ≤ sansserif_cap ( italic_v ) for each vertex v𝑣vitalic_v, where M⁢(v)={u:(u,v)∈M}𝑀𝑣conditional-set𝑢𝑢𝑣𝑀M(v)=\{u:(u,v)\in M\}italic_M ( italic_v ) = { italic_u : ( italic_u , italic_v ) ∈ italic_M }. Though vertices in both the sets A𝐴Aitalic_A and B𝐵Bitalic_B have capacities, note that M𝑀Mitalic_M can contain at most one copy of any edge since M𝑀Mitalic_M is a set (and not a multiset). Let us assume that the input instance admits a perfect matching, i.e., one that matches each vertex in A∪B𝐴𝐵A\cup Bitalic_A ∪ italic_B fully up to its capacity. Such a model occurs in the conference management system application mentioned above where PC members have to place bids and then they are explicitly asked to bid again for submissions that have received too few bids. Thus it will be ensured that the input instance admits a matching where each PC member a𝑎aitalic_a gets assigned to 𝖼𝖺𝗉⁢(a)𝖼𝖺𝗉𝑎\mathsf{cap}(a)sansserif_cap ( italic_a ) many papers and each paper b𝑏bitalic_b is assigned to 𝖼𝖺𝗉⁢(b)𝖼𝖺𝗉𝑏\mathsf{cap}(b)sansserif_cap ( italic_b ) many PC members. So we assume our input instance admits a perfect matching and we seek a best perfect matching as per vertex preferences. In the domain of matchings under preferences, stable matchings are usually regarded as the best matchings. Stable matchings. A matching M𝑀Mitalic_M is stable if there is no edge that blocks M𝑀Mitalic_M, where an edge (a,b)∉M𝑎𝑏𝑀(a,b)\notin M( italic_a , italic_b ) ∉ italic_M blocks M𝑀Mitalic_M if (i) either a𝑎aitalic_a has less than 𝖼𝖺𝗉⁢(a)𝖼𝖺𝗉𝑎\mathsf{cap}(a)sansserif_cap ( italic_a ) partners in M𝑀Mitalic_M or a𝑎aitalic_a prefers b𝑏bitalic_b to its worst partner in M𝑀Mitalic_M and (ii) either b𝑏bitalic_b has less than 𝖼𝖺𝗉⁢(b)𝖼𝖺𝗉𝑏\mathsf{cap}(b)sansserif_cap ( italic_b ) partners in M𝑀Mitalic_M or b𝑏bitalic_b prefers a𝑎aitalic_a to its worst partner in M𝑀Mitalic_M. Stable matchings always exist in G𝐺Gitalic_G and a natural modification of the Gale-Shapley algorithm [17] finds one. However stability is a strong and rather restrictive notion, e.g., stable matchings need not have maximum cardinality. Consider the following instance where A={a,a′},B={b,b′}formulae-sequence𝐴𝑎superscript𝑎′𝐵𝑏superscript𝑏′A=\{a,a^{\prime}\},B=\{b,b^{\prime}\}italic_A = { italic_a , italic_a start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT } , italic_B = { italic_b , italic_b start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT }, and all vertex capacities are 1. The preferences of agents are as follows: a:b≻b′:𝑎succeeds𝑏superscript𝑏′a:b\succ b^{\prime}italic_a : italic_b ≻ italic_b start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT and a′:b:superscript𝑎′𝑏a^{\prime}:bitalic_a start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT : italic_b. That is, the top choice of a𝑎aitalic_a is b𝑏bitalic_b and its second choice is b′superscript𝑏′b^{\prime}italic_b start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT while a′superscript𝑎′a^{\prime}italic_a start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT has only one neighbor b𝑏bitalic_b. Similarly, the preferences of jobs are as follows: b:a≻a′:𝑏succeeds𝑎superscript𝑎′b:a\succ a^{\prime}italic_b : italic_a ≻ italic_a start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT and b′:a:superscript𝑏′𝑎b^{\prime}:aitalic_b start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT : italic_a. This instance has only one stable matching S={(a,b)}𝑆𝑎𝑏S=\{(a,b)\}italic_S = { ( italic_a , italic_b ) } that leaves a′superscript𝑎′a^{\prime}italic_a start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT and b′superscript𝑏′b^{\prime}italic_b start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT unmatched. Observe that this instance has a perfect matching M={(a,b′),(a′,b)}𝑀𝑎superscript𝑏′superscript𝑎′𝑏M=\{(a,b^{\prime}),(a^{\prime},b)\}italic_M = { ( italic_a , italic_b start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) , ( italic_a start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , italic_b ) } that matches all agents and jobs. However M𝑀Mitalic_M is not stable as the edge (a,b)𝑎𝑏(a,b)( italic_a , italic_b ) blocks M𝑀Mitalic_M. For the problem of selecting a best perfect matching, one with the least number of blocking edges would be a natural relaxation of stability. But finding such a matching is NP-hard [5] even in the one-to-one setting. A well-studied relaxation of stability that offers a meaningful and tractable solution to the problem of finding a “best perfect matching” is the notion of popularity. Popularity. Popularity is based on voting by vertices on matchings. In the one-to-one setting, the preferences of a vertex over its neighbors extend naturally to preferences over matchings—so every vertex orders matchings in the order of its partners in these matchings. Popular matchings are weak Condorcet winners [13, 32] in this voting instance where vertices are voters and matchings are candidates. In other words, a popular matching M𝑀Mitalic_M does not lose a head-to-head election against any matching N𝑁Nitalic_N where each vertex v𝑣vitalic_v either casts a vote for the matching in {M,N}𝑀𝑁\{M,N\}{ italic_M , italic_N } that it prefers or v𝑣vitalic_v abstains from voting if its assignment is the same in M𝑀Mitalic_M and N𝑁Nitalic_N. For any vertex v𝑣vitalic_v and its neighbors u𝑢uitalic_u and u′superscript𝑢′u^{\prime}italic_u start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT, let 𝗏𝗈𝗍𝖾v⁢(u,u′)subscript𝗏𝗈𝗍𝖾𝑣𝑢superscript𝑢′\mathsf{vote}_{v}(u,u^{\prime})sansserif_vote start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_u , italic_u start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) be v𝑣vitalic_v’s vote for u𝑢uitalic_u versus u′superscript𝑢′u^{\prime}italic_u start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT. More precisely, 𝗏𝗈𝗍𝖾v⁢(u,u′)=1subscript𝗏𝗈𝗍𝖾𝑣𝑢superscript𝑢′1\mathsf{vote}_{v}(u,u^{\prime})=1sansserif_vote start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_u , italic_u start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) = 1 if v𝑣vitalic_v prefers u𝑢uitalic_u to u′superscript𝑢′u^{\prime}italic_u start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT, it is −11-1- 1 if v𝑣vitalic_v prefers u′superscript𝑢′u^{\prime}italic_u start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT to u𝑢uitalic_u, and it is 0 otherwise (so u=u′𝑢superscript𝑢′u=u^{\prime}italic_u = italic_u start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT). Thus in the one-to-one setting, every vertex v𝑣vitalic_v casts its vote which is 𝗏𝗈𝗍𝖾v⁢(u,u′)∈{0,±1}subscript𝗏𝗈𝗍𝖾𝑣𝑢superscript𝑢′0plus-or-minus1\mathsf{vote}_{v}(u,u^{\prime})\in\{0,\pm 1\}sansserif_vote start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_u , italic_u start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) ∈ { 0 , ± 1 }, where M⁢(v)={u}𝑀𝑣𝑢M(v)=\{u\}italic_M ( italic_v ) = { italic_u } and N⁢(v)={u′}𝑁𝑣superscript𝑢′N(v)=\{u^{\prime}\}italic_N ( italic_v ) = { italic_u start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT }, for M𝑀Mitalic_M versus N𝑁Nitalic_N in their head-to-head election. Recall that we are in the many-to-many setting, i.e., vertices have capacities. So we need to specify how a vertex votes over different subsets of its neighbors. Thus we need to compare two subsets M⁢(v)𝑀𝑣M(v)italic_M ( italic_v ) and N⁢(v)𝑁𝑣N(v)italic_N ( italic_v ) of 𝖭𝖻𝗋⁢(v)𝖭𝖻𝗋𝑣\mathsf{Nbr}(v)sansserif_Nbr ( italic_v ) for any vertex v𝑣vitalic_v, where 𝖭𝖻𝗋⁢(v)𝖭𝖻𝗋𝑣\mathsf{Nbr}(v)sansserif_Nbr ( italic_v ) is the set of neighbors of v𝑣vitalic_v. We will follow the method from [7] for this comparison where v𝑣vitalic_v is allowed to cast up to 𝖼𝖺𝗉⁢(v)𝖼𝖺𝗉𝑣\mathsf{cap}(v)sansserif_cap ( italic_v ) many votes. Any two subsets S𝑆Sitalic_S and T𝑇Titalic_T of 𝖭𝖻𝗋⁢(v)𝖭𝖻𝗋𝑣\mathsf{Nbr}(v)sansserif_Nbr ( italic_v ) are compared by vertex v𝑣vitalic_v as follows: • a bijection ψ𝜓\psiitalic_ψ is chosen from S′=S∖Tsuperscript𝑆′𝑆𝑇S^{\prime}=S\setminus Titalic_S start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = italic_S ∖ italic_T to T′=T∖Ssuperscript𝑇′𝑇𝑆T^{\prime}=T\setminus Sitalic_T start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = italic_T ∖ italic_S;222If the two sets are not of equal size, then dummy vertices that are less preferred to all non-dummy vertices are added to the smaller set. • every neighbor u∈S′𝑢superscript𝑆′u\in S^{\prime}italic_u ∈ italic_S start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT is compared with ψ⁢(u)∈T′𝜓𝑢superscript𝑇′\psi(u)\in T^{\prime}italic_ψ ( italic_u ) ∈ italic_T start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT; • the number of wins minus the number of losses is v𝑣vitalic_v’s vote for S𝑆Sitalic_S versus T𝑇Titalic_T. The bijection ψ𝜓\psiitalic_ψ from S′superscript𝑆′S^{\prime}italic_S start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT to T′superscript𝑇′T^{\prime}italic_T start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT that is chosen will be the one that minimizes v𝑣vitalic_v’s vote for S𝑆Sitalic_S versus T𝑇Titalic_T. More formally, the vote of v𝑣vitalic_v for S𝑆Sitalic_S versus T𝑇Titalic_T, denoted by 𝗏𝗈𝗍𝖾v⁢(S,T)subscript𝗏𝗈𝗍𝖾𝑣𝑆𝑇\mathsf{vote}_{v}(S,T)sansserif_vote start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_S , italic_T ), is defined as follows where |S′|=|T′|=ksuperscript𝑆′superscript𝑇′𝑘|S^{\prime}|=|T^{\prime}|=k| italic_S start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT | = | italic_T start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT | = italic_k and Π⁢[k]Πdelimited-[]𝑘\Pi[k]roman_Π [ italic_k ] is the set of permutations on {1,…,k}1…𝑘\{1,\ldots,k\}{ 1 , … , italic_k }: 𝗏𝗈𝗍𝖾v⁢(S,T)=minσ∈Π⁢[k]⁢∑i=1k𝗏𝗈𝗍𝖾v⁢(S′⁢[i],T′⁢[σ⁢(i)]).subscript𝗏𝗈𝗍𝖾𝑣𝑆𝑇subscript𝜎Πdelimited-[]𝑘superscriptsubscript𝑖1𝑘subscript𝗏𝗈𝗍𝖾𝑣superscript𝑆′delimited-[]𝑖superscript𝑇′delimited-[]𝜎𝑖\mathsf{vote}_{v}(S,T)=\min_{\sigma\in\Pi[k]}\sum_{i=1}^{k}\mathsf{vote}_{v}(S% ^{\prime}[i],T^{\prime}[\sigma(i)]).sansserif_vote start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_S , italic_T ) = roman_min start_POSTSUBSCRIPT italic_σ ∈ roman_Π [ italic_k ] end_POSTSUBSCRIPT ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT sansserif_vote start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_S start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT [ italic_i ] , italic_T start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT [ italic_σ ( italic_i ) ] ) . (1) Here S′⁢[i]superscript𝑆′delimited-[]𝑖S^{\prime}[i]italic_S start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT [ italic_i ] is the i𝑖iitalic_i-th ranked resident in S′superscript𝑆′S^{\prime}italic_S start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT and T′⁢[σ⁢(i)]superscript𝑇′delimited-[]𝜎𝑖T^{\prime}[\sigma(i)]italic_T start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT [ italic_σ ( italic_i ) ] is the σ⁢(i)𝜎𝑖\sigma(i)italic_σ ( italic_i )-th ranked resident in T′superscript𝑇′T^{\prime}italic_T start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT. Consider the following example from [7] where 𝖭𝖻𝗋⁢(v)={u1,u2,u3,u4,u5,u6}𝖭𝖻𝗋𝑣subscript𝑢1subscript𝑢2subscript𝑢3subscript𝑢4subscript𝑢5subscript𝑢6\mathsf{Nbr}(v)=\{u_{1},u_{2},u_{3},u_{4},u_{5},u_{6}\}sansserif_Nbr ( italic_v ) = { italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPT } and v𝑣vitalic_v’s preference order is u1≻u2≻u3≻u4≻u5≻u6succeedssubscript𝑢1subscript𝑢2succeedssubscript𝑢3succeedssubscript𝑢4succeedssubscript𝑢5succeedssubscript𝑢6u_{1}\succ u_{2}\succ u_{3}\succ u_{4}\succ u_{5}\succ u_{6}italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ≻ italic_u start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ≻ italic_u start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ≻ italic_u start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT ≻ italic_u start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT ≻ italic_u start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPT. Let S={u1,u3,u5}𝑆subscript𝑢1subscript𝑢3subscript𝑢5S=\{u_{1},u_{3},u_{5}\}italic_S = { italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT } and T={u2,u4,u6}𝑇subscript𝑢2subscript𝑢4subscript𝑢6T=\{u_{2},u_{4},u_{6}\}italic_T = { italic_u start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPT }. So S′=Ssuperscript𝑆′𝑆S^{\prime}=Sitalic_S start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = italic_S and T′=Tsuperscript𝑇′𝑇T^{\prime}=Titalic_T start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = italic_T. The bijection ψ:S′→T′:𝜓→superscript𝑆′superscript𝑇′\psi:S^{\prime}\rightarrow T^{\prime}italic_ψ : italic_S start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT → italic_T start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT is ψ⁢(u1)=u6𝜓subscript𝑢1subscript𝑢6\psi(u_{1})=u_{6}italic_ψ ( italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) = italic_u start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPT, ψ⁢(u3)=u2𝜓subscript𝑢3subscript𝑢2\psi(u_{3})=u_{2}italic_ψ ( italic_u start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ) = italic_u start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, and ψ⁢(u5)=u4𝜓subscript𝑢5subscript𝑢4\psi(u_{5})=u_{4}italic_ψ ( italic_u start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT ) = italic_u start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT as this is the most adversarial way of comparing S′superscript𝑆′S^{\prime}italic_S start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT with T′superscript𝑇′T^{\prime}italic_T start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT. This results in 𝗏𝗈𝗍𝖾v⁢(S,T)=𝗏𝗈𝗍𝖾v⁢(u1,u6)+𝗏𝗈𝗍𝖾v⁢(u3,u2)+𝗏𝗈𝗍𝖾v⁢(u5,u4)=1−1−1=−1subscript𝗏𝗈𝗍𝖾𝑣𝑆𝑇subscript𝗏𝗈𝗍𝖾𝑣subscript𝑢1subscript𝑢6subscript𝗏𝗈𝗍𝖾𝑣subscript𝑢3subscript𝑢2subscript𝗏𝗈𝗍𝖾𝑣subscript𝑢5subscript𝑢41111\mathsf{vote}_{v}(S,T)=\mathsf{vote}_{v}(u_{1},u_{6})+\mathsf{vote}_{v}(u_{3},% u_{2})+\mathsf{vote}_{v}(u_{5},u_{4})=1-1-1=-1sansserif_vote start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_S , italic_T ) = sansserif_vote start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPT ) + sansserif_vote start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_u start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) + sansserif_vote start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_u start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT ) = 1 - 1 - 1 = - 1. Similarly, 𝗏𝗈𝗍𝖾v⁢(T,S)=𝗏𝗈𝗍𝖾v⁢(u2,u1)+𝗏𝗈𝗍𝖾v⁢(u4,u3)+𝗏𝗈𝗍𝖾v⁢(u6,u5)=−3subscript𝗏𝗈𝗍𝖾𝑣𝑇𝑆subscript𝗏𝗈𝗍𝖾𝑣subscript𝑢2subscript𝑢1subscript𝗏𝗈𝗍𝖾𝑣subscript𝑢4subscript𝑢3subscript𝗏𝗈𝗍𝖾𝑣subscript𝑢6subscript𝑢53\mathsf{vote}_{v}(T,S)=\mathsf{vote}_{v}(u_{2},u_{1})+\mathsf{vote}_{v}(u_{4},% u_{3})+\mathsf{vote}_{v}(u_{6},u_{5})=-3sansserif_vote start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_T , italic_S ) = sansserif_vote start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_u start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) + sansserif_vote start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_u start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ) + sansserif_vote start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_u start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT ) = - 3. For any pair of matchings M,N𝑀𝑁M,Nitalic_M , italic_N and vertex v𝑣vitalic_v, let 𝗏𝗈𝗍𝖾v⁢(M,N)=𝗏𝗈𝗍𝖾v⁢(S,T)subscript𝗏𝗈𝗍𝖾𝑣𝑀𝑁subscript𝗏𝗈𝗍𝖾𝑣𝑆𝑇\mathsf{vote}_{v}(M,N)=\mathsf{vote}_{v}(S,T)sansserif_vote start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_M , italic_N ) = sansserif_vote start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_S , italic_T ) where S=M⁢(v)𝑆𝑀𝑣S=M(v)italic_S = italic_M ( italic_v ) and T=N⁢(v)𝑇𝑁𝑣T=N(v)italic_T = italic_N ( italic_v ). So 𝗏𝗈𝗍𝖾v⁢(M,N)subscript𝗏𝗈𝗍𝖾𝑣𝑀𝑁\mathsf{vote}_{v}(M,N)sansserif_vote start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_M , italic_N ) counts the number of votes by v𝑣vitalic_v for M⁢(v)𝑀𝑣M(v)italic_M ( italic_v ) versus N⁢(v)𝑁𝑣N(v)italic_N ( italic_v ) when the two sets M⁢(v)∖N⁢(v)𝑀𝑣𝑁𝑣M(v)\setminus N(v)italic_M ( italic_v ) ∖ italic_N ( italic_v ) and N⁢(v)∖M⁢(v)𝑁𝑣𝑀𝑣N(v)\setminus M(v)italic_N ( italic_v ) ∖ italic_M ( italic_v ) are compared in the order that is most adversarial or negative for M𝑀Mitalic_M. The two matchings M𝑀Mitalic_M and N𝑁Nitalic_N are compared using Δ⁢(M,N)=∑v∈A∪B𝗏𝗈𝗍𝖾v⁢(M⁢(v),N⁢(v))Δ𝑀𝑁subscript𝑣𝐴𝐵subscript𝗏𝗈𝗍𝖾𝑣𝑀𝑣𝑁𝑣\Delta(M,N)=\sum_{v\in A\cup B}\mathsf{vote}_{v}(M(v),N(v))roman_Δ ( italic_M , italic_N ) = ∑ start_POSTSUBSCRIPT italic_v ∈ italic_A ∪ italic_B end_POSTSUBSCRIPT sansserif_vote start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_M ( italic_v ) , italic_N ( italic_v ) ). We will say matching M𝑀Mitalic_M is more popular than matching N𝑁Nitalic_N if Δ⁢(M,N)>0Δ𝑀𝑁0\Delta(M,N)>0roman_Δ ( italic_M , italic_N ) > 0. Definition 2 A matching M𝑀Mitalic_M is popular if Δ⁢(M,N)≥0Δ𝑀𝑁0\Delta(M,N)\geq 0roman_Δ ( italic_M , italic_N ) ≥ 0 for all matchings N𝑁Nitalic_N in G𝐺Gitalic_G. Thus M𝑀Mitalic_M is popular if there is no matching more popular than M𝑀Mitalic_M. Recall our assumption that the set of feasible solutions is the set of perfect matchings. Hence the matchings of interest to us are popular perfect matchings, defined below. Definition 3 A perfect matching M𝑀Mitalic_M is a popular perfect matching if Δ⁢(M,N)≥0Δ𝑀𝑁0\Delta(M,N)\geq 0roman_Δ ( italic_M , italic_N ) ≥ 0 for all perfect matchings N𝑁Nitalic_N in G𝐺Gitalic_G. So a popular perfect matching need not be popular—it may lose to a smaller size matching; nevertheless, it never loses to a perfect matching. Since weak Condorcet winners do not always exist, it is not a priori clear if a popular perfect matching always exists. It was shown in [27] that there always exists a popular perfect matching in the one-to-one setting;333It was shown in [27] that a popular maximum matching (i.e., a maximum matching that is popular among maximum matchings) always exists in the one-to-one setting. moreover, such a matching can be computed in polynomial time. The following problem in the many-to-many setting was considered in [33]: vertices have capacity lower bounds and feasible matchings are those that satisfy these lower bounds. It was shown that popular feasible matchings always exist and can be computed in polynomial time. Popular perfect matchings are a special case of this problem where every vertex has to be fully matched, i.e., every vertex v𝑣vitalic_v has a lower bound of 𝖼𝖺𝗉⁢(v)𝖼𝖺𝗉𝑣\mathsf{cap}(v)sansserif_cap ( italic_v ). Thus popular perfect matchings always exist in the many-to-many setting and can be computed in polynomial time. Our problem. We assume there is a function 𝖼𝗈𝗌𝗍:E→ℝ:𝖼𝗈𝗌𝗍→𝐸ℝ\mathsf{cost}:E\rightarrow\mathbb{R}sansserif_cost : italic_E → blackboard_R, which is part of the input. Hence the cost of any matching M𝑀Mitalic_M is ∑e∈M𝖼𝗈𝗌𝗍⁢(e)subscript𝑒𝑀𝖼𝗈𝗌𝗍𝑒\sum_{e\in M}\mathsf{cost}(e)∑ start_POSTSUBSCRIPT italic_e ∈ italic_M end_POSTSUBSCRIPT sansserif_cost ( italic_e ). There might be exponentially many popular perfect matchings in G𝐺Gitalic_G, hence we would like to find an min-cost one, i.e., a popular perfect matching whose cost is least among all popular perfect matchings. Solving the min-cost popular perfect matching problem efficiently implies efficient algorithms for several desirable popular perfect matching problems such as finding one with the highest utility when every edge has an associated utility or one with forced/forbidden edges or an egalitarian one. In the conference management system application discussed earlier, we would like to find a popular perfect matching that matches as many PC members as possible along top ranked edges, subject to that, as many PC members as possible along second ranked edges, and so on [22]. Such a popular perfect matching is a min-cost popular perfect matching for an appropriate cost function. A polynomial-time algorithm for the min-cost popular perfect matching problem in the one-to-one setting was shown in [28]. The conference version of our paper [29] gave a polynomial-time algorithm for this problem in the many-to-one setting (i.e., 𝖼𝖺𝗉⁢(a)=1𝖼𝖺𝗉𝑎1\mathsf{cap}(a)=1sansserif_cap ( italic_a ) = 1 for all a∈A𝑎𝐴a\in Aitalic_a ∈ italic_A) by reducing it to the min-cost popular perfect matching problem in the one-to-one setting and using the polynomial-time algorithm for computing a min-cost popular perfect matching in the one-to-one setting [28]. The hospitals/residents setting. The many-to-one setting is usually referred to as the hospitals/residents setting. The reduction in [29] from the hospitals/residents setting to the one-to-one setting is via the cloned instance G′=(A∪B′,E′)superscript𝐺′𝐴superscript𝐵′superscript𝐸′G^{\prime}=(A\cup B^{\prime},E^{\prime})italic_G start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = ( italic_A ∪ italic_B start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , italic_E start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) corresponding to the given hospitals/residents instance G=(A∪B,E)𝐺𝐴𝐵𝐸G=(A\cup B,E)italic_G = ( italic_A ∪ italic_B , italic_E ). Every b∈B𝑏𝐵b\in Bitalic_b ∈ italic_B is replaced by 𝖼𝖺𝗉⁢(b)𝖼𝖺𝗉𝑏\mathsf{cap}(b)sansserif_cap ( italic_b ) many clones b1,b2,…subscript𝑏1subscript𝑏2…b_{1},b_{2},\ldotsitalic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … in B′superscript𝐵′B^{\prime}italic_B start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT and the preference order of every bisubscript𝑏𝑖b_{i}italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is the same as the preference order of b𝑏bitalic_b. Furthermore, every a∈A𝑎𝐴a\in Aitalic_a ∈ italic_A that is a neighbor of b𝑏bitalic_b in G𝐺Gitalic_G replaces the occurrence of b𝑏bitalic_b in its preference order with b1≻⋯≻b𝖼𝖺𝗉⁢(b)succeedssubscript𝑏1⋯succeedssubscript𝑏𝖼𝖺𝗉𝑏b_{1}\succ\cdots\succ b_{\mathsf{cap}(b)}italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ≻ ⋯ ≻ italic_b start_POSTSUBSCRIPT sansserif_cap ( italic_b ) end_POSTSUBSCRIPT. There is a natural map f𝑓fitalic_f from the set of popular perfect matchings in the one-to-one instance G′superscript𝐺′G^{\prime}italic_G start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT to the set of perfect matchings in the original instance G𝐺Gitalic_G, where for any popular perfect matching M′superscript𝑀′M^{\prime}italic_M start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT in G′superscript𝐺′G^{\prime}italic_G start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT, the many-to-one matching f⁢(M′)𝑓superscript𝑀′f(M^{\prime})italic_f ( italic_M start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) is obtained by replacing each edge (a,bi)𝑎subscript𝑏𝑖(a,b_{i})( italic_a , italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) in M′superscript𝑀′M^{\prime}italic_M start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT with the original edge (a,b)𝑎𝑏(a,b)( italic_a , italic_b ). It can be shown that f⁢(M′)𝑓superscript𝑀′f(M^{\prime})italic_f ( italic_M start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) is a popular perfect matching in G𝐺Gitalic_G. So we have f:{f:\{italic_f : {popular perfect matchings in G′}→G^{\prime}\}\rightarrowitalic_G start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT } → {{\{{popular perfect matchings in G}G\}italic_G }. It was shown in [29] that the mapping f𝑓fitalic_f is surjective. Hence solving the min-cost popular perfect matching problem in G′superscript𝐺′G^{\prime}italic_G start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT solves the min-cost popular perfect matching problem in G𝐺Gitalic_G. Though it was claimed in [29] that the same approach can be generalized to the many-to-many setting, unfortunately it does not extend to the many-to-many setting. The many-to-many setting. Consider G=(A∪B,E)𝐺𝐴𝐵𝐸G=(A\cup B,E)italic_G = ( italic_A ∪ italic_B , italic_E ) where A={a,a′}𝐴𝑎superscript𝑎′A=\{a,a^{\prime}\}italic_A = { italic_a , italic_a start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT } and B={b,b′}𝐵𝑏superscript𝑏′B=\{b,b^{\prime}\}italic_B = { italic_b , italic_b start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT } and every vertex has capacity 2. Vertex preferences are as follows: a:b≻b′:𝑎succeeds𝑏superscript𝑏′a\colon b\succ b^{\prime}italic_a : italic_b ≻ italic_b start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT and a′:b′≻b:superscript𝑎′succeedssuperscript𝑏′𝑏a^{\prime}\colon b^{\prime}\succ bitalic_a start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT : italic_b start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ≻ italic_b along with b:a≻a′:𝑏succeeds𝑎superscript𝑎′b\colon a\succ a^{\prime}italic_b : italic_a ≻ italic_a start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT and b′:a′≻a:superscript𝑏′succeedssuperscript𝑎′𝑎b^{\prime}\colon a^{\prime}\succ aitalic_b start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT : italic_a start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ≻ italic_a. There is only one perfect matching M={(a,b),(a,b′),(a′,b),(a′,b′)}𝑀𝑎𝑏𝑎superscript𝑏′superscript𝑎′𝑏superscript𝑎′superscript𝑏′M=\{(a,b),(a,b^{\prime}),(a^{\prime},b),(a^{\prime},b^{\prime})\}italic_M = { ( italic_a , italic_b ) , ( italic_a , italic_b start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) , ( italic_a start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , italic_b ) , ( italic_a start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , italic_b start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) } in G𝐺Gitalic_G, hence M𝑀Mitalic_M is a popular perfect matching. But M𝑀Mitalic_M cannot be realized as a popular perfect matching in the corresponding one-to-one instance G′superscript𝐺′G^{\prime}italic_G start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT. The vertex set of G′=(A′∪B′,E′)superscript𝐺′superscript𝐴′superscript𝐵′superscript𝐸′G^{\prime}=(A^{\prime}\cup B^{\prime},E^{\prime})italic_G start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = ( italic_A start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ∪ italic_B start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , italic_E start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) is given by A′={a1,a2,a1′,a2′}superscript𝐴′subscript𝑎1subscript𝑎2subscriptsuperscript𝑎′1subscriptsuperscript𝑎′2A^{\prime}=\{a_{1},a_{2},a^{\prime}_{1},a^{\prime}_{2}\}italic_A start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = { italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_a start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_a start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT } and B′={b1,b2,b1′,b2′}superscript𝐵′subscript𝑏1subscript𝑏2subscriptsuperscript𝑏′1subscriptsuperscript𝑏′2B^{\prime}=\{b_{1},b_{2},b^{\prime}_{1},b^{\prime}_{2}\}italic_B start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = { italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_b start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_b start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT }. Vertex preferences in G′superscript𝐺′G^{\prime}italic_G start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT are as follows. a1:b1≻b2≻b1′≻b2′:subscript𝑎1succeedssubscript𝑏1subscript𝑏2succeedssubscriptsuperscript𝑏′1succeedssubscriptsuperscript𝑏′2\displaystyle a_{1}\colon b_{1}\succ b_{2}\succ b^{\prime}_{1}\succ b^{\prime}% _{2}\qquad\qquaditalic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT : italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ≻ italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ≻ italic_b start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ≻ italic_b start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT b1:a1≻a2≻a1′≻a2′:subscript𝑏1succeedssubscript𝑎1subscript𝑎2succeedssubscriptsuperscript𝑎′1succeedssubscriptsuperscript𝑎′2\displaystyle b_{1}\colon a_{1}\succ a_{2}\succ a^{\prime}_{1}\succ a^{\prime}% _{2}italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT : italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ≻ italic_a start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ≻ italic_a start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ≻ italic_a start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT a2:b1≻b2≻b1′≻b2′:subscript𝑎2succeedssubscript𝑏1subscript𝑏2succeedssubscriptsuperscript𝑏′1succeedssubscriptsuperscript𝑏′2\displaystyle a_{2}\colon b_{1}\succ b_{2}\succ b^{\prime}_{1}\succ b^{\prime}% _{2}\qquad\qquaditalic_a start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT : italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ≻ italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ≻ italic_b start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ≻ italic_b start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT b2:a1≻a2≻a1′≻a2′:subscript𝑏2succeedssubscript𝑎1subscript𝑎2succeedssubscriptsuperscript𝑎′1succeedssubscriptsuperscript𝑎′2\displaystyle b_{2}\colon a_{1}\succ a_{2}\succ a^{\prime}_{1}\succ a^{\prime}% _{2}italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT : italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ≻ italic_a start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ≻ italic_a start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ≻ italic_a start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT a1′:b1′≻b2′≻b1≻b2:subscriptsuperscript𝑎′1succeedssubscriptsuperscript𝑏′1subscriptsuperscript𝑏′2succeedssubscript𝑏1succeedssubscript𝑏2\displaystyle a^{\prime}_{1}\colon b^{\prime}_{1}\succ b^{\prime}_{2}\succ b_{% 1}\succ b_{2}\qquad\qquaditalic_a start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT : italic_b start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ≻ italic_b start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ≻ italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ≻ italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT b1′:a1′≻a2′≻a1≻a2:subscriptsuperscript𝑏′1succeedssubscriptsuperscript𝑎′1subscriptsuperscript𝑎′2succeedssubscript𝑎1succeedssubscript𝑎2\displaystyle b^{\prime}_{1}\colon a^{\prime}_{1}\succ a^{\prime}_{2}\succ a_{% 1}\succ a_{2}italic_b start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT : italic_a start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ≻ italic_a start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ≻ italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ≻ italic_a start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT a2′:b1′≻b2′≻b1≻b2:subscriptsuperscript𝑎′2succeedssubscriptsuperscript𝑏′1subscriptsuperscript𝑏′2succeedssubscript𝑏1succeedssubscript𝑏2\displaystyle a^{\prime}_{2}\colon b^{\prime}_{1}\succ b^{\prime}_{2}\succ b_{% 1}\succ b_{2}\qquad\qquaditalic_a start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT : italic_b start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ≻ italic_b start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ≻ italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ≻ italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT b2′:a1′≻a2′≻a1≻a2:subscriptsuperscript𝑏′2succeedssubscriptsuperscript𝑎′1subscriptsuperscript𝑎′2succeedssubscript𝑎1succeedssubscript𝑎2\displaystyle b^{\prime}_{2}\colon a^{\prime}_{1}\succ a^{\prime}_{2}\succ a_{% 1}\succ a_{2}italic_b start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT : italic_a start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ≻ italic_a start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ≻ italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ≻ italic_a start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT Observe that S′={(a1,b1),(a2,b2),(a1′,b1′),(a2′,b2′)}superscript𝑆′subscript𝑎1subscript𝑏1subscript𝑎2subscript𝑏2subscriptsuperscript𝑎′1subscriptsuperscript𝑏′1subscriptsuperscript𝑎′2subscriptsuperscript𝑏′2S^{\prime}=\{(a_{1},b_{1}),(a_{2},b_{2}),(a^{\prime}_{1},b^{\prime}_{1}),(a^{% \prime}_{2},b^{\prime}_{2})\}italic_S start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = { ( italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , ( italic_a start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) , ( italic_a start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_b start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , ( italic_a start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_b start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) } is a stable matching in G′superscript𝐺′G^{\prime}italic_G start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT. Hence it is popular [18], so S′superscript𝑆′S^{\prime}italic_S start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT is a popular perfect matching in G′superscript𝐺′G^{\prime}italic_G start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT. However the matching {(a,b),(a′,b′)}𝑎𝑏superscript𝑎′superscript𝑏′\{(a,b),(a^{\prime},b^{\prime})\}{ ( italic_a , italic_b ) , ( italic_a start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , italic_b start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) } obtained by replacing edges in G′superscript𝐺′G^{\prime}italic_G start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT with original edges is not perfect in G𝐺Gitalic_G. Note that we are not allowed to have two copies of any edge here, hence both (a,b)𝑎𝑏(a,b)( italic_a , italic_b ) and (a′,b′)superscript𝑎′superscript𝑏′(a^{\prime},b^{\prime})( italic_a start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , italic_b start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) are present with multiplicity 1. Thus popular perfect matchings in G𝐺Gitalic_G and G′superscript𝐺′G^{\prime}italic_G start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT are quite different from each other. Our approach. We need a different approach to solve the min-cost popular perfect matching problem in the many-to-many setting. Our approach is to reduce the min-cost popular perfect matching problem in G=(A∪B,E)𝐺𝐴𝐵𝐸G=(A\cup B,E)italic_G = ( italic_A ∪ italic_B , italic_E ) to the min-cost stable matching problem in an auxiliary many-to-many instance G∗=(A∪B,E∗)superscript𝐺𝐴𝐵superscript𝐸G^{*}=(A\cup B,E^{*})italic_G start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT = ( italic_A ∪ italic_B , italic_E start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ). This instance G∗superscript𝐺G^{*}italic_G start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT is a multigraph, i.e., parallel edges are present in G∗superscript𝐺G^{*}italic_G start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT. A min-cost stable matching in a many-to-many instance can be computed in polynomial time [15, 20]. Furthermore, Fleiner’s algorithm [15] works even in the case when the input instance is a multigraph. Hence a min-cost popular perfect matching in the original many-to-many instance G𝐺Gitalic_G can be computed in polynomial time. Thus we show the following result. {reptheorem} restate_thm:many-many Let G=(A∪B,E)𝐺𝐴𝐵𝐸G=(A\cup B,E)italic_G = ( italic_A ∪ italic_B , italic_E ) be a many-to-many matching instance where vertices have strict preferences (possibly incomplete) and there is a function 𝖼𝗈𝗌𝗍:E→ℝ:𝖼𝗈𝗌𝗍→𝐸ℝ\mathsf{cost}:E\rightarrow\mathbb{R}sansserif_cost : italic_E → blackboard_R. If G𝐺Gitalic_G admits a perfect matching then a min-cost popular perfect matching in G𝐺Gitalic_G can be computed in polynomial time. 1.1 Background and Related Results The notion of popularity was proposed by Gärdenfors [18] in 1975 in the stable marriage problem (i.e., the one-to-one setting) where he observed that stable matchings are popular. It was shown in [4, 11] that when preferences include ties (even one-sided ties), it is NP-hard to decide if a popular matching exists or not. It was shown in [21] that every stable matching in a marriage instance is a min-size popular matching. Polynomial-time algorithms to find a max-size popular matching were shown in [21, 27]. We refer to [10] for a survey on results in popular matchings in the one-to-one setting. The notion of popularity was extended from one-to-one setting to many-to-many setting in [7] and [34], independently. A polynomial-time algorithm to compute a max-size popular matching in the many-to-many setting was given in [7]. It was also shown in [7] that every stable matching in the many-to-many setting is popular; so though a rather strong definition of popularity was adopted here, popular matchings always exist. The definition of popularity considered in [34] is weaker than the one in [7]; in order to compare a pair of matchings M𝑀Mitalic_M and N𝑁Nitalic_N, every vertex v𝑣vitalic_v uses the bijection that compares the top neighbor in M⁢(v)∖N⁢(v)𝑀𝑣𝑁𝑣M(v)\setminus N(v)italic_M ( italic_v ) ∖ italic_N ( italic_v ) with the top neighbor in N⁢(v)∖M⁢(v)𝑁𝑣𝑀𝑣N(v)\setminus M(v)italic_N ( italic_v ) ∖ italic_M ( italic_v ), and so on, i.e., the permutation σ𝜎\sigmaitalic_σ in Eq. (1) is the identity permutation. The max-size popular matching problem with matroid constraints (this model generalizes popular many-to-many matchings) was considered in [9, 26] and shown to be tractable. Strongly popular matchings (such a matching defeats every other matching) in many-to-many instances were studied in [31]. The stable matching problem has been extensively studied in the hospitals/residents and the many-to-many settings [2, 6, 15, 20, 19, 23, 24, 36, 38] and a min-cost stable matching in the one-to-one setting (and thus in the hospitals/residents setting) can be computed in polynomial time [37, 39]. Note that min-cost stable matching algorithms in the one-to-one setting do not generalize to the many-to-many setting. This is because (unlike the hospitals/residents setting) the many-to-many setting cannot be reduced to the one-to-one setting via cloning [20, Footnote 6]. The algorithms in [15, 20] solve the min-cost stable matching problem in the many-to-many setting. Fleiner’s algorithm [15] solves the min-cost matroid kernel problem in the intersection of two strictly ordered matroids. This generalizes the min-cost stable matching problem in the many-to-many setting. Huang’s algorithm [20] solves the min-cost classified stable matching problem when each vertex on one side of the graph has classifications with upper bounds on the number of partners it can have in each class. Note that this problem generalizes the min-cost stable matching problem in the many-to-many setting. The min-cost classified stable matching problem when vertices on both sides of the graph have classifications was solved in [16]. There is a polynomial-time algorithm to find a min-cost popular maximum matching in the one-to-one setting [28]. In contrast to this, finding a min-cost popular matching in the one-to-one setting is NP-hard [14]. Nevertheless, when preferences are complete, there is a polynomial-time algorithm to find a min-cost popular matching in the one-to-one setting [12]. This result was recently extended to the hospitals/residents setting in [30]; note that this is a non-trivial extension as the set of popular matchings in a hospitals/residents instance can be richer than those in the corresponding one-to-one instance. The main result in the conference version of our paper [29] (now included in [30]) showed that in contrast to popular matchings, the set of popular perfect matchings in a hospitals/residents instance is not richer than the set of popular perfect matchings in the corresponding one-to-one instance. That is, each popular perfect matching in a hospitals/residents instance can be realized as a popular perfect matching in the corresponding one-to-one instance. As seen in our example earlier, this is not the case for popular perfect matchings in the many-to-many setting. Interestingly, the tractability of a matching problem in the one-to-one setting does not always imply its tractability in the hospitals/residents setting. One such problem is that of finding a matching that maximizes Nash social welfare (i.e., the geometric mean of edge utilities). Such a matching can be easily found by the maximum weight matching algorithm in the one-to-one setting,444For any edge (a,b)𝑎𝑏(a,b)( italic_a , italic_b ), the weight of this edge is log\logroman_log of the product of utilities that a𝑎aitalic_a and b𝑏bitalic_b have for each other. however it is NP-hard to find such a matching in the hospitals/residents setting [25]. 1.2 Techniques We will use the characterization of popular perfect matchings in the one-to-one setting. It was shown in [27] that in order to find a popular maximum matching in a given one-to-one instance, it suffices to run the Gale-Shapley algorithm in an appropriate auxiliary instance (a multigraph). It was shown in [28] that this mapping (essentially, a projection) from the set of stable matchings in this auxiliary instance to the set of popular maximum matchings in the original instance is surjective. Thus for every popular maximum matching in the original instance, there exists a corresponding stable matching in the auxiliary instance. Hence in the one-to-one setting, the min-cost stable matching algorithm in the auxiliary instance solves the min-cost popular maximum matching problem in the original instance. Our problem is to find a min-cost popular perfect matching in a many-to-many instance G𝐺Gitalic_G. We will show a many-to-many instance G∗superscript𝐺G^{*}italic_G start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT (as before, a multigraph) such that there is a surjective mapping from the set of stable matchings in G∗superscript𝐺G^{*}italic_G start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT to the set of popular perfect matchings in G𝐺Gitalic_G. Let M𝑀Mitalic_M be any perfect matching in G𝐺Gitalic_G. We will show in Section 2 that M𝑀Mitalic_M is a popular perfect matching in G𝐺Gitalic_G if and only if there is a realization M′superscript𝑀′M^{\prime}italic_M start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT of M𝑀Mitalic_M such that M′superscript𝑀′M^{\prime}italic_M start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT is a popular perfect matching in an appropriate one-to-one instance—this instance is a subgraph of the one-to-one cloned instance G′superscript𝐺′G^{\prime}italic_G start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT described earlier. This subgraph depends on the matching M′superscript𝑀′M^{\prime}italic_M start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT, so let us call it GM′′subscriptsuperscript𝐺′superscript𝑀′G^{\prime}_{M^{\prime}}italic_G start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_M start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT. Thus this reduction from the many-to-many setting to the one-to-one setting does not seem particularly helpful since it depends on the matching M𝑀Mitalic_M that we seek. We will overcome the above bottleneck in Section 3 by using the result for one-to-one instances in [28] to construct a one-to-one multigraph GM0subscriptsuperscript𝐺0𝑀G^{0}_{M}italic_G start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT such that M′superscript𝑀′M^{\prime}italic_M start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT can be realized as a stable matching in GM0subscriptsuperscript𝐺0𝑀G^{0}_{M}italic_G start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT. We will then use this to show a many-to-many multigraph G∗superscript𝐺G^{*}italic_G start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT such that M𝑀Mitalic_M can be realized as a stable matching M∗superscript𝑀M^{*}italic_M start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT in the many-to-many instance G∗superscript𝐺G^{*}italic_G start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT. Conversely, every stable matching in the instance G∗superscript𝐺G^{*}italic_G start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT projects to a popular perfect matching in G𝐺Gitalic_G. Thus finding a min-cost stable matching in G∗superscript𝐺G^{*}italic_G start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT (by Fleiner’s algorithm [15]) solves the min-cost popular perfect matching problem in G𝐺Gitalic_G."
https://arxiv.org/html/2411.00255v1,Dynamic Accountable Storage: An Efficient Protocol for Real-time Cloud Storage Auditing,"Ateniese, Goodrich, Lekakis, Papamanthou, Paraskevas, and Tamassia introduced the Accountable Storage protocol, which is a way for a client to outsource their data to a cloud storage provider while allowing the client to periodically perform accountability challenges. An accountability challenge efficiently recovers any pieces of data the server has lost or corrupted, allowing the client to extract the original copies of the damaged or lost data objects. A severe limitation of the prior accountable storage scheme of Ateniese et al., however, is that it is not fully dynamic. That is, it does not allow a client to freely insert and delete data from the outsourced data set after initializing the protocol, giving the protocol limited practical use in the real world. In this paper, we present Dynamic Accountable Storage, which is an efficient way for a client to periodically audit their cloud storage while also supporting insert and delete operations on the data set. To do so, we introduce a data structure, the IBLT tree, which allows either the server or the client to reconstruct data the server has lost or corrupted in a space-efficient way.","Cloud storage providers often advertise the number of “nines” of durability they achieve, such as the “11 nines” in the durability probability of 99.999999999% advertised by Amazon S3 for not losing a given data object in a given year,111See, e.g., https://aws.amazon.com/s3/storage-classes/. which implies an expected loss of at most one object out of 100 billion per year. Such statements might seem at first to imply that cloud storage data loss is impossible, until one considers that there are hundreds of trillions of objects currently being stored in Amazon S3.222E.g., see https://aws.amazon.com/blogs/aws/welcome-to-aws-pi-day-2022/. Moreover, such durability statements do not address data corruption or data loss caused by misuse or misconfiguration, e.g., see [23, 25, 27]. Such durability statements also beg the question of how to determine whether one or more of a client’s data objects has been lost or corrupted. For example, when data is lost, a client or server may not even realize it and may also have no ability to recover from the damage. We propose an efficient way for a client and/or server to audit and recover client data. Our work is an extension of the Accountable Storage protocol first described by Ateniese et al. [4]. In this scheme, a client, Alice, outsources her data to a cloud storage provider, Bob, and she then stores a small sketch representation of her data set along with some metadata in a data structure called an Invertible Bloom Lookup Table (IBLT) [17, 20]. At any time, the client can issue an accountability challenge, requiring that the storage provider, Bob, send the client a similar encoding of the data set representing everything that has not been lost. The client can then compare the two IBLTs to peel out any data blocks that were lost, assess how much the blocks deviate from their original versions, and demand compensation accordingly. Furthermore, Ateniese et al. claim that in their scheme the server is forced to acknowledge and pay for lost data. However, we show that the server can easily thwart their scheme. Another caveat in the protocol by Ateniese et al. is that their scheme is static, i.e., it does not support insert or delete operations. This severely limits the usefulness of the scheme in the real world. Our work fixes this limitation; hence, we call our scheme “Dynamic Accountable Storage.” Furthermore, our scheme does not rely on forcing the server to pay for lost data or store large amounts of metadata, as in the original (static) Accountable Storage scheme [4]. Instead, our protocol provides the server with an efficient low-overhead way to recover lost data for the client. 1.1 Other Related Work There are many existing schemes that allow a client to verify that a cloud storage provider is keeping their data intact, but not with the same levels of efficiency or dynamism as our scheme. For example, Provable Data Possession (PDP) schemes [2, 3, 16, 19, 31, 21] and Proofs of Retrievability (POR) schemes [10, 22, 28, 29, 1] use versions of cryptographic tags to verify that data is maintained correctly. A common technique used in these schemes is homomorphic tags [2, 5], which enable batch verification and reduces communication complexity from linear to constant in the size of the client’s data. Verifiable Database schemes [6, 11, 13, 14, 26, 12] do something similar, except use cryptographic commitment primitives rather than tags. Unfortunately, dynamic PDP or POR schemes, such as the one by Erway, Kupcu, Papamanthou, and Tamassia [19], stop at answering whether the client’s data has been corrupted by a cloud service provider. In contrast, our protocol goes further, by recovering the lost data. Other work, such as that by Ateniese, Di Pietro, Mancini, and Tsudik [2] or Shacham and Waters [28], implements a type of accountability challenge with dynamic data, but their schemes require the client to run O⁢(n)𝑂𝑛O(n)italic_O ( italic_n ) expensive cryptographic operations per update, where n𝑛nitalic_n is the size of the database. Jin et al. [21] solved this recomputation problem at the expense of having the client store O⁢(n)𝑂𝑛O(n)italic_O ( italic_n ) extra metadata, somewhat defeating the purpose of the client outsourcing their data. 1.2 Our Contributions In this paper, we describe a scheme for Dynamic Accountable Storage, which supports insertions, deletions, and data recovery such that both the client and server have low overheads in terms of additional storage requirements and running times required to update metadata. For example, given a parameter, δ𝛿\deltaitalic_δ, for the number of blocks our scheme can efficiently tolerate being lost or corrupted, our scheme requires only o⁢(n)𝑜𝑛o(n)italic_o ( italic_n ) additional space at the server rather than the Θ⁢(n⁢δ)Θ𝑛𝛿\Theta(n\delta)roman_Θ ( italic_n italic_δ ) space required by the original Accountable Storage scheme [4]. Another difference of our approach from prior approaches is that our scheme does not assume the server is malicious, since any reputable cloud service company has an incentive to help the client maintain her data. As a result, our work investigates efficient ways for the server to maintain the client’s data with added reliability and to be able to detect and repair corruptions on the client’s behalf, assuming that the server is honest-but-curious; hence, we provide a way for the client, Alice, to encrypt both her keys and their values while still allowing her to outsource her data with low overheads for both the client and the server. We wish to emphasize that an honest-but-curious server does not preclude the need for accountability mechanisms. As described above, any cloud storage system of a sufficient size will inevitably produce errors. Even if the server is not maliciously corrupting data, identify errant data objects among billions of others is a non-trivial problem. In addition to cloud storage verification, schemes like ours have applications in version control systems [19], verifiably-secure logging [15, 30], and public data auditing [21, 24, 4]. We have two main contributions in this work. The first is to show how the the server in the original Accountable Storage protocol of Ateniese et al. [4] can thwart their original scheme so as to recover lost data without paying the client or revealing that there has been a data loss. Indeed, we see this as a feature, not a bug, and we build our scheme on the assumption that the honest-but-curious server, Bob, is motivated to recover the client’s data whenever this is possible. The second contribution of our work is that we describe an extension to the Accountable Storage protocol [4] that can support insertions and deletions of key-value pairs. To allow the server to efficiently detect and repair corruptions in the client’s data, we introduce a new data structure that is maintained at the server, which we call an IBLT tree. This data structure takes o⁢(n)𝑜𝑛o(n)italic_o ( italic_n ) extra space at the server, compared to the O⁢(n)𝑂𝑛O(n)italic_O ( italic_n ) extra metadata required by the original Accountable Storage protocol. Although the server must already store n𝑛nitalic_n data blocks for the client, the space savings across a large number of clients are significant. The remainder of the paper is organized as follows. In Section 2, we briefly review the original Accountable Storage protocol of Ateniese et al. [4], showing how the server can thwart the requirement to pay for lost data. In Section 3, we describe our new data structure, the IBLT tree. Lastly, in Section 4, we provide an overview of our protocol and a formal construction with an efficiency analysis."
https://arxiv.org/html/2411.00082v1,Testing and learning structured quantum Hamiltonians,"We consider the problems of testing and learning an n𝑛nitalic_n-qubit Hamiltonian H=∑xλx⁢σx𝐻subscript𝑥subscript𝜆𝑥subscript𝜎𝑥H=\sum_{x}\lambda_{x}\sigma_{x}italic_H = ∑ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT italic_λ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT italic_σ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT expressed in its Pauli basis, from queries to its evolution operator U=e−i⁢H⁢t𝑈superscript𝑒𝑖𝐻𝑡U=e^{-iHt}italic_U = italic_e start_POSTSUPERSCRIPT - italic_i italic_H italic_t end_POSTSUPERSCRIPT with respect the normalized Frobenius norm. To this end, we prove the following results (with and without quantum memory) for Hamiltonians whose Pauli spectrum involves only k𝑘kitalic_k-local terms or has sparsity at most s𝑠sitalic_s:Local Hamiltonians: We give a tolerant testing protocol to decide if a Hamiltonian is ε1subscript𝜀1\varepsilon_{1}italic_ε start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-close to k𝑘kitalic_k-local or ε2subscript𝜀2\varepsilon_{2}italic_ε start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT-far from k𝑘kitalic_k-local, with O⁢(1/(ε2−ε1)4)𝑂1superscriptsubscript𝜀2subscript𝜀14O(1/(\varepsilon_{2}-\varepsilon_{1})^{4})italic_O ( 1 / ( italic_ε start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_ε start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT ) queries, thereby solving two open questions posed in a recent work by Bluhm, Caro and Oufkir [5]. For learning a k𝑘kitalic_k-local Hamiltonian up to error ε𝜀\varepsilonitalic_ε, we give a protocol with query complexity and total time evolution exp⁡(O⁢(k2+k⁢log⁡(1/ε)))𝑂superscript𝑘2𝑘1𝜀\exp(O(k^{2}+k\log(1/\varepsilon)))roman_exp ( italic_O ( italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_k roman_log ( 1 / italic_ε ) ) ). Our algorithm leverages the non-commutative Bohnenblust-Hille inequality in order to get a complexity independent of n𝑛nitalic_n.Sparse Hamiltonians: We give a protocol for testing whether a Hamiltonian is ε1subscript𝜀1\varepsilon_{1}italic_ε start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-close to being s𝑠sitalic_s-sparse or ε2subscript𝜀2\varepsilon_{2}italic_ε start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT-far from being s𝑠sitalic_s-sparse, with O⁢(s6/(ε22−ε12)6)𝑂superscript𝑠6superscriptsuperscriptsubscript𝜀22superscriptsubscript𝜀126O(s^{6}/(\varepsilon_{2}^{2}-\varepsilon_{1}^{2})^{6})italic_O ( italic_s start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT / ( italic_ε start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - italic_ε start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT ) queries. For learning up to error ε𝜀\varepsilonitalic_ε, we show that O⁢(s4/ε8)𝑂superscript𝑠4superscript𝜀8O(s^{4}/\varepsilon^{8})italic_O ( italic_s start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT / italic_ε start_POSTSUPERSCRIPT 8 end_POSTSUPERSCRIPT ) queries suffices.Learning without quantum memory: The learning results stated above have no dependence on the system size n𝑛nitalic_n, but require n𝑛nitalic_n-qubit quantum memory. We give subroutines that allow us to reproduce all the above learning results without quantum memory; increasing the query complexity by a (log⁡n)𝑛(\log n)( roman_log italic_n )-factor in the local case and an n𝑛nitalic_n-factor in the sparse case.Testing without quantum memory: We give a new subroutine called Pauli hashing, which allows one to tolerantly test s𝑠sitalic_s-sparse Hamiltonians using O~⁢(s14/(ε22−ε12)18)~𝑂superscript𝑠14superscriptsuperscriptsubscript𝜀22superscriptsubscript𝜀1218\tilde{O}(s^{14}/(\varepsilon_{2}^{2}-\varepsilon_{1}^{2})^{18})over~ start_ARG italic_O end_ARG ( italic_s start_POSTSUPERSCRIPT 14 end_POSTSUPERSCRIPT / ( italic_ε start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - italic_ε start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT 18 end_POSTSUPERSCRIPT ) query complexity. A key ingredient is showing that s𝑠sitalic_s-sparse Pauli channels can be tested in a tolerant fashion as being ε1subscript𝜀1\varepsilon_{1}italic_ε start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-close to being s𝑠sitalic_s-sparse or ε2subscript𝜀2\varepsilon_{2}italic_ε start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT-far under the diamond norm, using O~⁢(s2/(ε2−ε1)6)~𝑂superscript𝑠2superscriptsubscript𝜀2subscript𝜀16\tilde{O}(s^{2}/(\varepsilon_{2}-\varepsilon_{1})^{6})over~ start_ARG italic_O end_ARG ( italic_s start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT / ( italic_ε start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_ε start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT ) queries via Pauli hashing.In order to prove these results, we prove new structural theorems for local Hamiltonians, sparse Pauli channels and sparse Hamiltonians. We complement our learning algorithms with lower bounds that are polynomially weaker. Furthermore, our algorithms use short time evolutions and do not assume prior knowledge of the terms on which the Pauli spectrum is supported on, i.e., we do not require prior knowledge about the support of the Hamiltonian terms.","A fundamental and important challenge with building quantum devices is being able to characterize and calibrate its behavior. One approach to do so is Hamiltonian learning which seeks to learn the Hamiltonian governing the dynamics of a quantum system given finite classical and quantum resources. Beyond system characterization, it is also carried out during validation of physical systems and designing control strategies for implementing quantum gates [39]. However, learning an n𝑛nitalic_n-qubit Hamiltonian is known to be difficult, requiring complexity that scales exponential in the number of qubits [12]. In practice, however, prior knowledge on the structure of Hamiltonians is available e.g., those of engineered quantum devices [53] where the underlying Hamiltonians primarily involve local interactions with few non-local interactions, and even naturally occurring physical quantum systems such as those with translationally invariant Hamiltonians. To highlight these structural properties, consider an n𝑛nitalic_n-qubit Hamiltonian H𝐻Hitalic_H (which is a self-adjoint operator acting on (ℂ2)⊗nsuperscriptsuperscriptℂ2tensor-productabsent𝑛(\mathbb{C}^{2})^{\otimes n}( blackboard_C start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ⊗ italic_n end_POSTSUPERSCRIPT) expanded in terms of the n𝑛nitalic_n-qubit Pauli operators: H=∑x∈{0,1}2⁢nλx⁢σx,𝐻subscript𝑥superscript012𝑛subscript𝜆𝑥subscript𝜎𝑥H=\sum_{x\in\{0,1\}^{2n}}\lambda_{x}\sigma_{x},italic_H = ∑ start_POSTSUBSCRIPT italic_x ∈ { 0 , 1 } start_POSTSUPERSCRIPT 2 italic_n end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_λ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT italic_σ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT , where λxsubscript𝜆𝑥\lambda_{x}italic_λ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT are real-valued coefficients (also called interaction strengths) of the Pauli operators σxsubscript𝜎𝑥\sigma_{x}italic_σ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT denoted by the string x=(a,b)∈𝔽2n𝑥𝑎𝑏superscriptsubscript𝔽2𝑛x=(a,b)\in\mathbb{F}_{2}^{n}italic_x = ( italic_a , italic_b ) ∈ blackboard_F start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT with σ(a,b)=ia⋅b⊗i=1nXai⁢Zbisubscript𝜎𝑎𝑏superscriptsubscripttensor-product𝑖1𝑛superscript𝑖⋅𝑎𝑏superscript𝑋subscript𝑎𝑖superscript𝑍subscript𝑏𝑖\sigma_{(a,b)}=i^{a\cdot b}\otimes_{i=1}^{n}X^{a_{i}}Z^{b_{i}}italic_σ start_POSTSUBSCRIPT ( italic_a , italic_b ) end_POSTSUBSCRIPT = italic_i start_POSTSUPERSCRIPT italic_a ⋅ italic_b end_POSTSUPERSCRIPT ⊗ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_X start_POSTSUPERSCRIPT italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT italic_Z start_POSTSUPERSCRIPT italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT. We call the set of Paulis with non-zero coefficients λxsubscript𝜆𝑥\lambda_{x}italic_λ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT as the Pauli spectrum of the Hamiltonian denoted by 𝒮={x∈{0,1}2⁢n:λx≠0}𝒮conditional-set𝑥superscript012𝑛subscript𝜆𝑥0\mathcal{S}=\{x\in\{0,1\}^{2n}:\ \lambda_{x}\neq 0\}caligraphic_S = { italic_x ∈ { 0 , 1 } start_POSTSUPERSCRIPT 2 italic_n end_POSTSUPERSCRIPT : italic_λ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT ≠ 0 }. Of particular relevance are k𝑘kitalic_k-local Hamiltonians which involve Pauli operators that act non-trivially on all but at most k𝑘kitalic_k qubits and s𝑠sitalic_s-sparse Hamiltonians whose Pauli expansion contains at most s𝑠sitalic_s non-zero Pauli operators i.e., |𝒮|≤s𝒮𝑠|\mathcal{S}|\leq s| caligraphic_S | ≤ italic_s. There has thus been a growing suite of Hamiltonian learning results that have shown that when the underlying n𝑛nitalic_n-qubit Hamiltonian H𝐻Hitalic_H satisfies these structural properties, learning can be performed with only poly⁡(n)poly𝑛\operatorname{poly}(n)roman_poly ( italic_n ) query complexity, either by making “queries” to the unitary evolution operator U⁢(t)=exp⁡(−i⁢H⁢t)𝑈𝑡𝑖𝐻𝑡U(t)=\exp(-iHt)italic_U ( italic_t ) = roman_exp ( - italic_i italic_H italic_t ) [23, 34, 62, 36, 59, 21, 38, 40, 52, 28], or by assuming one has access to Gibbs state [1, 36, 51, 48, 8, 28]. Notably, [9] considered the problem of learning Hamiltonians that are both local and sparse, without prior knowledge of the support. Several of the learning algorithms mentioned above however require assumptions on the support of the Hamiltonian beyond locality or sparsity, such as [38] which considers geometrically-local Hamiltonians (a subset of local Hamiltonians) and [59] which requires assumptions on the support. Moreover, before learning, it might be desirable to uncover what is the structure of an unknown Hamiltonian in order to choose specialized learning algorithms. Even deciding if a Hamiltonian has a particular structure is a fundamental challenge and constitutes the problem of testing if an unknown Hamiltonian satisfies a certain structural property. As far as we know, this line of investigation is nascent with only a few works on Hamiltonian property testing [54, 2, 41] with Blum et al. [6] having considered the problem of testing local Hamiltonians and the problem of testing sparse Hamiltonians yet to be tackled. This leads us to the motivating question of our work: What is the query complexity of learning and testing structured Hamiltonians? 1.1 Problem statement Before we state our results answering the question above, we clearly mention our learning and testing problems first. If H𝐻Hitalic_H is the Hamiltonian describing the dynamics of a certain physical system, then the state of that system evolves according to the time evolution operator U⁢(t)=e−i⁢H⁢t𝑈𝑡superscript𝑒𝑖𝐻𝑡U(t)=e^{-iHt}italic_U ( italic_t ) = italic_e start_POSTSUPERSCRIPT - italic_i italic_H italic_t end_POSTSUPERSCRIPT. This means that if ρ⁢(0)𝜌0\rho(0)italic_ρ ( 0 ) is the state at time 00, at time t𝑡titalic_t the state would have evolved to ρ⁢(t)=U⁢(t)⁢ρ⁢(0)⁢U†⁢(t)𝜌𝑡𝑈𝑡𝜌0superscript𝑈†𝑡\rho(t)=U(t)\rho(0)U^{\dagger}(t)italic_ρ ( italic_t ) = italic_U ( italic_t ) italic_ρ ( 0 ) italic_U start_POSTSUPERSCRIPT † end_POSTSUPERSCRIPT ( italic_t ). Hence, to test and learn a Hamiltonian one can do the following: prepare a desired state, apply U⁢(t)𝑈𝑡U(t)italic_U ( italic_t ) or tensor products of U⁢(t)𝑈𝑡U(t)italic_U ( italic_t ) with identity to the state, and finally measure in a chosen basis. From here onwards, this is what we mean by querying the unitary U⁢(t)𝑈𝑡U(t)italic_U ( italic_t ). It is usual to impose the normalization condition ∥H∥∞≤1subscriptdelimited-∥∥𝐻1\lVert H\rVert_{\infty}\leq 1∥ italic_H ∥ start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT ≤ 1 (i.e., that the eigenvalues of H𝐻Hitalic_H are bounded in absolute value by 1111), as otherwise the complexities scale with the norm of the Hamiltonian. Throughout this paper, we will consider the normalized Frobenius norm as the distance between Hamiltonians, namely d⁢(H,H′)=∥H−H′∥2=Tr⁡[(H−H′)2]2n,𝑑𝐻superscript𝐻′subscriptdelimited-∥∥𝐻superscript𝐻′2Trsuperscript𝐻superscript𝐻′2superscript2𝑛d(H,H^{\prime})=\lVert H-H^{\prime}\rVert_{2}=\sqrt{\frac{\operatorname{Tr}[(H% -H^{\prime})^{2}]}{2^{n}}},italic_d ( italic_H , italic_H start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) = ∥ italic_H - italic_H start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = square-root start_ARG divide start_ARG roman_Tr [ ( italic_H - italic_H start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] end_ARG start_ARG 2 start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_ARG end_ARG , which equals the 2-norm of the Pauli spectrum, d⁢(H,H′)=∑|λx−λx′|2𝑑𝐻superscript𝐻′superscriptsubscript𝜆𝑥subscriptsuperscript𝜆′𝑥2d(H,H^{\prime})=\sqrt{\sum|\lambda_{x}-\lambda^{\prime}_{x}|^{2}}italic_d ( italic_H , italic_H start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) = square-root start_ARG ∑ | italic_λ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT - italic_λ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG. A property of a Hamiltonian, denoted ℋℋ\mathcal{H}caligraphic_H is a class of Hamiltonians that satisfy the property (here we will be interested in sparse and local properties). We say that H𝐻Hitalic_H is ε𝜀\varepsilonitalic_ε-far from having a property ℋℋ\mathcal{H}caligraphic_H if d⁢(H,H′)>ε𝑑𝐻superscript𝐻′𝜀d(H,H^{\prime})>\varepsilonitalic_d ( italic_H , italic_H start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) > italic_ε for every H′∈ℋsuperscript𝐻′ℋH^{\prime}\in\mathcal{H}italic_H start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ∈ caligraphic_H, and otherwise is ε𝜀\varepsilonitalic_ε-close. Now, we are ready to state the testing and learning problems. Let ℋℋ\mathcal{H}caligraphic_H be a property and let H𝐻Hitalic_H be an unknown Hamiltonian with ∥H∥∞≤1subscriptdelimited-∥∥𝐻1\lVert H\rVert_{\infty}\leq 1∥ italic_H ∥ start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT ≤ 1 and Tr⁡[H]=0Tr𝐻0\operatorname{Tr}[H]=0roman_Tr [ italic_H ] = 0. Problem 1.1 (Tolerant testing). Promised H𝐻Hitalic_H is either ε1subscript𝜀1\varepsilon_{1}italic_ε start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-close or ε2subscript𝜀2\varepsilon_{2}italic_ε start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT-far from satisfying property ℋℋ\mathcal{H}caligraphic_H, decide which is the case by making queries to U⁢(t)𝑈𝑡U(t)italic_U ( italic_t ). Problem 1.2 (Hamiltonian learning). Promised H∈ℋ𝐻ℋH\in\mathcal{H}italic_H ∈ caligraphic_H, output a classical description of H~∈ℋ~𝐻ℋ\widetilde{H}\in\mathcal{H}over~ start_ARG italic_H end_ARG ∈ caligraphic_H such that ‖H−H~‖2≤εsubscriptnorm𝐻~𝐻2𝜀\|H-\widetilde{H}\|_{2}\leq\varepsilon∥ italic_H - over~ start_ARG italic_H end_ARG ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ≤ italic_ε by making queries to U⁢(t)𝑈𝑡U(t)italic_U ( italic_t ). 1.2 Summary of results The main results of this submission are query-efficient algorithms for testing and learning Hamiltonians that are local and/or sparse. We can reproduce these results without using quantum memory by increasing the number of queries. We summarize our results in the table below (for simplicity we state our results for constant accuracy). Testing Learning with memory w/o memory with memory w/o memory s𝑠sitalic_s-sparse poly⁡(s)poly𝑠\operatorname{poly}(s)roman_poly ( italic_s ) poly⁡(s)poly𝑠\operatorname{poly}(s)roman_poly ( italic_s ) poly⁡(s)poly𝑠\operatorname{poly}(s)roman_poly ( italic_s ) n⋅poly⁡(s)⋅𝑛poly𝑠n\cdot\operatorname{poly}(s)italic_n ⋅ roman_poly ( italic_s ) k𝑘kitalic_k-local O⁢(1)𝑂1O(1)italic_O ( 1 ) O⁢(1)𝑂1O(1)italic_O ( 1 ) [6] exp⁡(k2)superscript𝑘2\exp(k^{2})roman_exp ( italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) (log⁡n)⋅exp⁡(k2)⋅𝑛superscript𝑘2(\log n)\cdot\exp(k^{2})( roman_log italic_n ) ⋅ roman_exp ( italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) k𝑘kitalic_k-local & s𝑠sitalic_s-sparse poly⁡(s)poly𝑠\operatorname{poly}(s)roman_poly ( italic_s ) poly⁡(s)poly𝑠\operatorname{poly}(s)roman_poly ( italic_s ) min⁡{exp⁡(k2),poly⁡(s⁢k)}superscript𝑘2poly𝑠𝑘\min\{\exp(k^{2}),\operatorname{poly}(sk)\}roman_min { roman_exp ( italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) , roman_poly ( italic_s italic_k ) } (log⁡n)⋅min⁡{exp⁡(k2),poly⁡(s⁢k)}⋅𝑛superscript𝑘2poly𝑠𝑘(\log n)\cdot\min\{\exp(k^{2}),\operatorname{poly}(sk)\}( roman_log italic_n ) ⋅ roman_min { roman_exp ( italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) , roman_poly ( italic_s italic_k ) } Table 1: Query complexity for learning and testing n𝑛nitalic_n-qubit structured Hamiltonians. Dependence on n𝑛nitalic_n and the structural property is shown for constant accuracy. Results are indicated with quantum memory (i.e., an n𝑛nitalic_n-qubit ancillary system is available) and without quantum memory. Before we discuss our results in more detail, we make a few remarks about our main results. (i)𝑖(i)( italic_i ) As far as we know, this is the first work: (a)𝑎(a)( italic_a ) with complexities that are independent of n𝑛nitalic_n (with memory)111We remark that there are a few works that achieve n𝑛nitalic_n-independent complexities for learning local Hamiltonians in the ∞\infty∞-norm of the Pauli coefficients, but when transformed into 2222-norm learners they yield complexities depending on nksuperscript𝑛𝑘n^{k}italic_n start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT., and (b)𝑏(b)( italic_b ) that does not assume knowledge of the support.222Soon after the third-author’s work [32], Bakshi et al. [9] presented a learning algorithm that does not require prior knowledge of the support, although their result achieve Heisenberg scaling in complexity using heavy machinery. (i⁢v)𝑖𝑣(iv)( italic_i italic_v ) We give the first learning algorithm for Hamiltonians that are only promised to be sparse, and not necessarily local. Similarly, our local Hamiltonian learning problem doesn’t assume geometric locality which was assumed in several prior works. (i⁢i⁢i)𝑖𝑖𝑖(iii)( italic_i italic_i italic_i ) Our testing algorithms are tolerant, i.e., they can handle the setting where ε1≠0subscript𝜀10\varepsilon_{1}\neq 0italic_ε start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ≠ 0. As far as we know, there are only a handful of polynomial-time tolerant testers for quantum objects. (i⁢v)𝑖𝑣(iv)( italic_i italic_v ) We show that all our all the learning protocols with quantum memory can be translated to ones which require no quantum memory. In the case of learning structured Hamiltonians, we obtain a protocol with only a factor log⁡n𝑛\log nroman_log italic_n overhead for local Hamiltonians and a protocol with a factor n𝑛nitalic_n overhead for sparse Hamiltonians. (v)𝑣(v)( italic_v ) We also give a tolerant testing algorithm for s𝑠sitalic_s-sparse Hamiltonians that requires no quantum memory based on a new subroutine called Pauli hashing. The query complexity is O⁢(poly⁡(s))𝑂poly𝑠O(\operatorname{poly}(s))italic_O ( roman_poly ( italic_s ) ) and is notably independent of dimension n𝑛nitalic_n. We remark that most previous work on Hamiltonian learning (that we highlighted earlier) are done under the distance induced by the supremum norm of the Pauli spectrum and with extra constraints apart from locality [23, 34, 62, 36, 58, 59, 12, 21, 38, 40, 42, 52, 28]. When transformed into learning algorithms under the finer distance induced by the 2-norm of the Pauli spectrum, these proposals yield complexities that depend polynomially on nksuperscript𝑛𝑘n^{k}italic_n start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT and only work for a restricted family of k𝑘kitalic_k-local Hamiltonians. The works that explicitly consider the problem of learning under the 2-norm have complexities depending on n𝑛nitalic_n and assume a stronger access model [19, 9]. 1.3 Results Local Hamiltonians. Recently, Bluhm, Caro and Oufkir proposed a non-tolerant testing algorithm, meaning that it only works for the case ε1=0,subscript𝜀10\varepsilon_{1}=0,italic_ε start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0 , whose query complexity is O⁢(n2⁢k+2/(ε2−ε1)4)𝑂superscript𝑛2𝑘2superscriptsubscript𝜀2subscript𝜀14O(n^{2k+2}/(\varepsilon_{2}-\varepsilon_{1})^{4})italic_O ( italic_n start_POSTSUPERSCRIPT 2 italic_k + 2 end_POSTSUPERSCRIPT / ( italic_ε start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_ε start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT ) and with total evolution time O⁢(nk+1/(ε2−ε1)3)𝑂superscript𝑛𝑘1superscriptsubscript𝜀2subscript𝜀13O(n^{k+1}/(\varepsilon_{2}-\varepsilon_{1})^{3})italic_O ( italic_n start_POSTSUPERSCRIPT italic_k + 1 end_POSTSUPERSCRIPT / ( italic_ε start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_ε start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ). They posed as open questions whether the dependence on n𝑛nitalic_n could be removed and whether an efficient tolerant-tester was possible [5, Section 1.5]. Our first result gives positive answer to both questions. Result 1.3. There is an algorithm that solves 1.1 for k𝑘kitalic_k-local Hamiltonians by making poly⁡(1/(ε2−ε1))poly1subscript𝜀2subscript𝜀1\operatorname{poly}(1/(\varepsilon_{2}-\varepsilon_{1}))roman_poly ( 1 / ( italic_ε start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_ε start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ) queries to the evolution operator and with poly⁡(1/(ε2−ε1))poly1subscript𝜀2subscript𝜀1\operatorname{poly}(1/(\varepsilon_{2}-\varepsilon_{1}))roman_poly ( 1 / ( italic_ε start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_ε start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ) total evolution time. See Theorem 4.1 for a formal statement of this result. Our algorithm to test for locality is simple. It consists of repeating the following process 1/(ε2−ε1)41superscriptsubscript𝜀2subscript𝜀141/(\varepsilon_{2}-\varepsilon_{1})^{4}1 / ( italic_ε start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_ε start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT times: prepare n𝑛nitalic_n EPR pairs, apply U⁢(ε2−ε1)⊗Id2ntensor-product𝑈subscript𝜀2subscript𝜀1subscriptIdsuperscript2𝑛U(\varepsilon_{2}-\varepsilon_{1})\otimes\mathop{\rm Id}\nolimits_{2^{n}}italic_U ( italic_ε start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_ε start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ⊗ roman_Id start_POSTSUBSCRIPT 2 start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_POSTSUBSCRIPT to them and measure in the Bell basis. Each time that we repeat this process, we sample from the Pauli sprectrum of U⁢(ε2−ε1)𝑈subscript𝜀2subscript𝜀1U(\varepsilon_{2}-\varepsilon_{1})italic_U ( italic_ε start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_ε start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ).333The Pauli spectrum of a unitary U=∑xU^x⁢σx𝑈subscript𝑥subscript^𝑈𝑥subscript𝜎𝑥U=\sum_{x}\widehat{U}_{x}\sigma_{x}italic_U = ∑ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT over^ start_ARG italic_U end_ARG start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT italic_σ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT determines a probability distribution because ∑x|U^x|2=1subscript𝑥superscriptsubscript^𝑈𝑥21\sum_{x}|\widehat{U}_{x}|^{2}=1∑ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT | over^ start_ARG italic_U end_ARG start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 1. As ε2−ε1subscript𝜀2subscript𝜀1\varepsilon_{2}-\varepsilon_{1}italic_ε start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_ε start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT is small, Taylor expansion ensures that U⁢(ε2−ε1)≈Id2n−i⁢(ε2−ε1)⁢H𝑈subscript𝜀2subscript𝜀1subscriptIdsuperscript2𝑛𝑖subscript𝜀2subscript𝜀1𝐻U(\varepsilon_{2}-\varepsilon_{1})\approx\mathop{\rm Id}\nolimits_{2^{n}}-i(% \varepsilon_{2}-\varepsilon_{1})Hitalic_U ( italic_ε start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_ε start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ≈ roman_Id start_POSTSUBSCRIPT 2 start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_POSTSUBSCRIPT - italic_i ( italic_ε start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_ε start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) italic_H, so sampling from the Pauli spectrum of U⁢(ε2−ε1)𝑈subscript𝜀2subscript𝜀1U(\varepsilon_{2}-\varepsilon_{1})italic_U ( italic_ε start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_ε start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) allows us to estimate the weight of the non-local terms of H𝐻Hitalic_H. If that weight is big, we output that H𝐻Hitalic_H is far from k𝑘kitalic_k-local, and otherwise we conclude that H𝐻Hitalic_H is close to k𝑘kitalic_k-local. Our second result is a learning algorithm for k𝑘kitalic_k-local Hamiltonians. Result 1.4. There is an algorithm that solves 1.2 for k𝑘kitalic_k-local Hamiltonians by making exp⁡(k2+k⁢log⁡(1/ε))superscript𝑘2𝑘1𝜀\exp(k^{2}+k\log(1/\varepsilon))roman_exp ( italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_k roman_log ( 1 / italic_ε ) ) queries to the evolution operator with exp⁡(k2+k⁢log⁡(1/ε))superscript𝑘2𝑘1𝜀\exp(k^{2}+k\log(1/\varepsilon))roman_exp ( italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_k roman_log ( 1 / italic_ε ) ) total evolution time. See Theorem 4.3 for a formal statement of this result. The learning algorithm has two stages. In the first stage one samples from the Pauli distribution of U⁢(ε)𝑈𝜀U(\varepsilon)italic_U ( italic_ε ), as in the testing algorithm, and from that one can detect which are the big Pauli coefficients of H𝐻Hitalic_H. In the second stage we learn the big Pauli coefficients the swap test. One can ensure that the coefficients not detected as big in the first stage of the algorithm can be neglected. To argue this formally, we use the non-commutative Bohnenblust-Hille inequality, which has been used recently for various quantum learning algorithms [35, 56]. Sparse Hamiltonians. Despite the numerous papers in the classical literature studying the problems of testing and learning sparse Boolean functions [31, 46, 61, 24], there are not many results on learning Hamiltonians that are sparse (and not necessarily local) and the only testing result that we are aware of requires O⁢(s⁢n)𝑂𝑠𝑛O(sn)italic_O ( italic_s italic_n ) queries [6, Remark B.2]. Here, we present the first sparsity testing algorithm whose complexity does not depend on n𝑛nitalic_n and the first learning algorithm for sparse Hamiltonians which does not make any assumptions regarding the support of the Hamiltonian beyond sparsity. Result 1.5. There is an algorithm that solves 1.1 for s𝑠sitalic_s-sparse Hamiltonians by making poly⁡(s/(ε2−ε1))poly𝑠subscript𝜀2subscript𝜀1\operatorname{poly}(s/(\varepsilon_{2}-\varepsilon_{1}))roman_poly ( italic_s / ( italic_ε start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_ε start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ) queries to the evolution operator and with poly⁡(s/(ε2−ε1))poly𝑠subscript𝜀2subscript𝜀1\operatorname{poly}(s/(\varepsilon_{2}-\varepsilon_{1}))roman_poly ( italic_s / ( italic_ε start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_ε start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ) total evolution time. See Theorem 4.5 for a formal statement. This testing algorithm consists on performing Pauli sampling of U⁢((ε22−ε12)/s)𝑈superscriptsubscript𝜀22superscriptsubscript𝜀12𝑠U(\sqrt{(\varepsilon_{2}^{2}-\varepsilon_{1}^{2})/s})italic_U ( square-root start_ARG ( italic_ε start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - italic_ε start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) / italic_s end_ARG ) a total of O⁢(s4/(ε22−ε12)4)𝑂superscript𝑠4superscriptsuperscriptsubscript𝜀22superscriptsubscript𝜀124O(s^{4}/(\varepsilon_{2}^{2}-\varepsilon_{1}^{2})^{4})italic_O ( italic_s start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT / ( italic_ε start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - italic_ε start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT ) times. From these samples one can estimate the sum of the squares of the top s𝑠sitalic_s Pauli coefficients of U𝑈Uitalic_U. If this quantity is big enough, we output that the Hamiltonian is close to s𝑠sitalic_s-sparse, and otherwise that is far. Although from this high-level description the algorithm seems similar to the locality testing one, the analysis is more involved and requires taking the second order Taylor expansion, which is the reason why the dependence on (ε2−ε1)subscript𝜀2subscript𝜀1(\varepsilon_{2}-\varepsilon_{1})( italic_ε start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_ε start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) is worse in this case. Result 1.6. There is an algorithm that solves 1.2 for s𝑠sitalic_s-sparse Hamiltonians by making poly⁡(s/ε)poly𝑠𝜀\operatorname{poly}(s/\varepsilon)roman_poly ( italic_s / italic_ε ) queries to the evolution operator with poly⁡(s/ε)poly𝑠𝜀\operatorname{poly}(s/\varepsilon)roman_poly ( italic_s / italic_ε ) total evolution time. See Theorem 4.6 for a formal statement. This learning algorithm begins by detecting which are the top s𝑠sitalic_s Pauli coefficients, which can be done via Pauli sampling, and concludes by learning the top s𝑠sitalic_s Pauli coefficients using a swap test, similarly to the locality learning algorithm. Learning and testing without quantum memory. Motivated by the limitations of current devices, there has been a series of recent works aiming to understand the power of quantum memory in testing and learning tasks, exhibiting exponential separations in some cases [14, 16, 17]. A natural question is, if the problems that we mentioned above become harder without quantum memory? Learning without memory. We surprisingly show that, the learning protocols that we mention above, can be implemented efficiently when one has no quantum memory. To this end, we provide two crucial subroutines for (i)𝑖(i)( italic_i ) estimating the Pauli spectrum of a unitary, (i⁢i)𝑖𝑖(ii)( italic_i italic_i ) estimating a single Pauli coefficient to make our protocols work in the memory-less setting. Subroutine (i⁢i)𝑖𝑖(ii)( italic_i italic_i ) incurs in no extra query-cost, and subroutine (i)𝑖(i)( italic_i ) only incurs in a factor-n𝑛nitalic_n overhead in the case of learning s𝑠sitalic_s-sparse Hamiltonians and a factor log⁡(n)𝑛\log(n)roman_log ( italic_n ) in the case of learning k𝑘kitalic_k-local Hamiltonians. These subroutines can also be useful in other contexts. In particular, we propose tolerant tester to decide if an unknown unitary is a k𝑘kitalic_k-junta which uses O⁢(4k)𝑂superscript4𝑘O(4^{k})italic_O ( 4 start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ) queries (see Proposition 3.5), making progress on a question of Chen et al. [18, Section 1.3], and then we use subroutine (i)𝑖(i)( italic_i ) to turn it into a memory-less tester that only makes O⁢(4k⁢n)𝑂superscript4𝑘𝑛O(4^{k}n)italic_O ( 4 start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_n ) queries. Testing sparse Pauli channels via Pauli hashing. In order to test for sparsity of Hamiltonian without memory we reduce to the problem of testing sparsity of a Pauli channel Φ:ρ↦∑xp⁢(x)⁢σx⁢ρ⁢σx:Φmaps-to𝜌subscript𝑥𝑝𝑥subscript𝜎𝑥𝜌subscript𝜎𝑥\Phi:\rho\mapsto\sum_{x}p(x)\sigma_{x}\rho\sigma_{x}roman_Φ : italic_ρ ↦ ∑ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT italic_p ( italic_x ) italic_σ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT italic_ρ italic_σ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT, which is of independent interest. To do that, we introduce a new technique called Pauli Hashing which allows to construct random partitions of Pauli operators. The high-level idea is to bucket the error rates p⁢(x)𝑝𝑥p(x)italic_p ( italic_x ) and thereby the corresponding Pauli operators: for this, we choose a random subgroup G𝐺Gitalic_G of the n𝑛nitalic_n-qubit Pauli group with dimension t=O⁢(log⁡s)𝑡𝑂𝑠t=O(\log s)italic_t = italic_O ( roman_log italic_s ). Pauli hashing allows us to partition all the Pauli operators into cosets the centralizer of G𝐺Gitalic_G which contains all the Paulis that commute with the Paulis in G𝐺Gitalic_G. The buckets are then the O⁢(s)𝑂𝑠O(s)italic_O ( italic_s ) cosets of the centralizer of G𝐺Gitalic_G. The main work then goes into arguing that the sum of the weights of the top s𝑠sitalic_s buckets is a good estimate of the top s𝑠sitalic_s error rates, and then a structural lemma we prove shows this is a good proxy for indicating whether the Pauli channel is close to being s𝑠sitalic_s-sparse or not. Putting everything together, with some careful analysis, we get an efficient tolerant tester for s𝑠sitalic_s-sparse Pauli channels. Result 1.7. There is an algorithm requiring no quantum memory that tests if a Pauli channel ε1subscript𝜀1\varepsilon_{1}italic_ε start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-close to or ε1subscript𝜀1\varepsilon_{1}italic_ε start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-far-from being s𝑠sitalic_s-sparse in diamond norm by making O~⁢(s2/(ε2−ε1)6)~𝑂superscript𝑠2superscriptsubscript𝜀2subscript𝜀16\widetilde{O}(s^{2}/(\varepsilon_{2}-\varepsilon_{1})^{6})over~ start_ARG italic_O end_ARG ( italic_s start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT / ( italic_ε start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_ε start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT ) queries to the channel. See Theorem 5.3 for a formal statement. We remark that Pauli Hashing only requires the preparation of Pauli eigenstates and Pauli measurements, making it suitable for the near-term. Testing sparse Hamiltonians without memory. We provide a memory-less testing algorithm for s𝑠sitalic_s-sparse Hamiltonians that uses Pauli hashing, that is completely independent of our tester with memory and only requires poly⁡(s/ε)poly𝑠𝜀\operatorname{poly}(s/\varepsilon)roman_poly ( italic_s / italic_ε ) queries and total evolution time, notably avoiding any dependence on n𝑛nitalic_n. To do this, we reduce the problem of testing Hamiltonian sparsity to testing the sparsity of an associated Pauli channel. To be precise, given the time evolution channel ℋt:ρ→U⁢(t)⁢ρ⁢U†⁢(t):subscriptℋ𝑡→𝜌𝑈𝑡𝜌superscript𝑈†𝑡\mathcal{H}_{t}:\rho\to U(t)\rho U^{\dagger}(t)caligraphic_H start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT : italic_ρ → italic_U ( italic_t ) italic_ρ italic_U start_POSTSUPERSCRIPT † end_POSTSUPERSCRIPT ( italic_t ), we define its Pauli-twirled channel via ℋt𝒯⁢(ρ)=𝔼x⁢[σx⁢ℋt⁢(σx⁢ρ⁢σx)⁢σx],superscriptsubscriptℋ𝑡𝒯𝜌subscript𝔼𝑥delimited-[]subscript𝜎𝑥subscriptℋ𝑡subscript𝜎𝑥𝜌subscript𝜎𝑥subscript𝜎𝑥\mathcal{H}_{t}^{\mathcal{T}}(\rho)=\mathbb{E}_{x}[\sigma_{x}\mathcal{H}_{t}(% \sigma_{x}\rho\sigma_{x})\sigma_{x}],caligraphic_H start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT caligraphic_T end_POSTSUPERSCRIPT ( italic_ρ ) = blackboard_E start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT [ italic_σ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT caligraphic_H start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_σ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT italic_ρ italic_σ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT ) italic_σ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT ] , and we prove that ℋtsubscriptℋ𝑡\mathcal{H}_{t}caligraphic_H start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is sparse if and only if ℋt𝒯superscriptsubscriptℋ𝑡𝒯\mathcal{H}_{t}^{\mathcal{T}}caligraphic_H start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT caligraphic_T end_POSTSUPERSCRIPT is sparse. Our result is then as follows. See Theorem 5.7 for a formal statement. Result 1.8. There is an algorithm requiring no quantum memory that solves 1.2 for s𝑠sitalic_s-sparse Hamiltonians by making poly⁡(s/ε)poly𝑠𝜀\operatorname{poly}(s/\varepsilon)roman_poly ( italic_s / italic_ε ) queries to the evolution operator with poly⁡(s/ε)poly𝑠𝜀\operatorname{poly}(s/\varepsilon)roman_poly ( italic_s / italic_ε ) total evolution time. Lower bounds. One drawback of our learning and testing algorithms is the exponent of the sparsity parameter s𝑠sitalic_s, locality parameter k𝑘kitalic_k and the tolerance (ε2−ε1)subscript𝜀2subscript𝜀1(\varepsilon_{2}-\varepsilon_{1})( italic_ε start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_ε start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ). Reducing to classical Boolean functions, we show lower bounds without memory that cerfity that the dependence on these parameters cannot be completely avoided, but an interesting and important future direction is to obtain the optimal results for these near-term relevant problems.444We remark that, Bakshi et al. [9] used highly non-trivial ideas to get Heisenberg scaling for their learning algorithm, and potentially similar ideas could be useful here. 1.4 Discussion and open questions Our work opens up several interesting directions which we state here and leave for future work. 1. Dependence on parameters ε1,ε2subscript𝜀1subscript𝜀2\varepsilon_{1},\varepsilon_{2}italic_ε start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_ε start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. Our main objective in this work was to obtain query complexities for testing and learning with good dependence on the structural parameters. It is natural to ask if we could improve the dependence on the error parameters and perhaps achieve Heisenberg limited scaling as has been shown to be possible in some particular cases for Hamiltonian learning [38, 9]. 2. Robustness to SPAM noise. It would be desirable to make the protocols introduced in this work to be robust to SPAM noise. A potential approach is to adapt strategies in [26]. 3. Adaptivity. For learning structured Hamiltonians, adaptive strategies [29, 22] can improve query complexity by shedding constant factors over baseline learning algorithms, thereby improving performance in practice. Another direction is to then explore adaptive protocols for testing structured Hamiltonians and the performance gains they may bring. 4. Testing and learning with limited quantum memory. For estimating properties of quantum states, Chen et al. [14] showcased the utility of the resource of quantum memory or a k𝑘kitalic_k-qubit ancillary system (k<n𝑘𝑛k<nitalic_k < italic_n). Large separations in query complexity when learning with memory (even for k≪nmuch-less-than𝑘𝑛k\ll nitalic_k ≪ italic_n) and without memory have been reported for learning Pauli channels [20, 15] and shadow tomography [16]. We could thus imagine having access to only limited quantum memory during learning or testing structured Hamiltonians as well. However, it should be noted that given the separation between the query complexities (see Table 1) with access to n𝑛nitalic_n-qubit quantum memory and without any, only marginal gains in complexity are expected from having access to limited quantum memory. 5. Testing and learning Hamiltonians from Gibbs states. Another natural learning model is that of having access to copies of the Gibbs state of a quantum Hamiltonian at a certain inverse temperature. There has been a suite of work investigating learning local Hamiltonians from Gibbs states [1, 8] but answering the question of testing structured Hamiltonians given access to copies of the Gibbs state remains wide open. Note added. After sharing Theorem 4.1 with Bluhm et al., they independently improved the analysis of their testing algorithm and showed that it only requires O⁢(1/(ε2−ε1)3⁢ε2)𝑂1superscriptsubscript𝜀2subscript𝜀13subscript𝜀2O(1/(\varepsilon_{2}-\varepsilon_{1})^{3}\varepsilon_{2})italic_O ( 1 / ( italic_ε start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_ε start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT italic_ε start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) queries and O⁢(1/(ε2−ε1)2.5⁢ε20.5)𝑂1superscriptsubscript𝜀2subscript𝜀12.5superscriptsubscript𝜀20.5O(1/(\varepsilon_{2}-\varepsilon_{1})^{2.5}\varepsilon_{2}^{0.5})italic_O ( 1 / ( italic_ε start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_ε start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2.5 end_POSTSUPERSCRIPT italic_ε start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 0.5 end_POSTSUPERSCRIPT ) total evolution time, which is very similar to our Theorem 4.1 [6]. In addition, for a wide range of k=O⁢(n)𝑘𝑂𝑛k=O(n)italic_k = italic_O ( italic_n ), their algorithm does not require the use of auxiliary qubits. Acknowledgements. S.A. and A.D. thank the Institute for Pure and Applied Mathematics (IPAM) for its hospitality throughout the long program “Mathematical and Computational Challenges in Quantum Computing” in Fall 2023 during which part of this work was initiated. This work was done in part while S.A. was visiting the Simons Institute for the Theory of Computing, supported by DOE QSA grant #FP00010905. This research was supported by the Europea union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement no. 945045, and by the NWO Gravitation project NETWORKS under grant no. 024.002.003. We thank Amira Abbas, Francesco Anna Mele, Andreas Bluhm, Jop Briët, Matthias Caro, Nunzia Cerrato, Aadil Oufkir, and Daniel Liang for useful comments and discussions. A.D. thanks Patrick Rall for multiple conversations on stabilizer subgroups and Pauli twirling. A.D. thanks Isaac Chuang for discussions on the problem of testing Hamiltonians. F.E.G. is funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy – EXC-2047/1 – 390685813."
