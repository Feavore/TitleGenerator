URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.04906v1,Faster feasibility for dynamic flows and transshipments on temporal networks,"In this paper we study flow problems on temporal networks, where edge capacities and travel times change over time. We consider a network with nğ‘›nitalic_n nodes and mğ‘šmitalic_m edges where the capacity and length of each edge is a piecewise constant function, and use Î¼=Î©â¢(m)ğœ‡Î©ğ‘š\mu=\Omega(m)italic_Î¼ = roman_Î© ( italic_m ) to denote the total number of pieces in all of the 2â¢m2ğ‘š2m2 italic_m functions. Our goal is to design exact algorithms for various flow problems that run in time polynomial in the parameter Î¼ğœ‡\muitalic_Î¼. Importantly, the algorithms we design are strongly polynomial, i.e. have no dependence on the capacities, flow value, or the time horizon of the flow process, all of which can be exponentially large relative to the other parameters; and return an integral flow when all input parameters are integral.Our main result is an algorithm for checking feasibility of a dynamic transshipment problem on temporal networks â€“ given multiple sources and sinks with supply and demand values, is it possible to satisfy the desired supplies and demands within a given time horizon? We develop a fast (Oâ¢(Î¼3)ğ‘‚superscriptğœ‡3O(\mu^{3})italic_O ( italic_Î¼ start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ) time) algorithm for this feasibility problem when the input network has a certain canonical form, by exploiting the cut structure of the associated time expanded network. We then adapt an approach of [HT00] to show how other flow problems on temporal networks can be reduced to the canonical format.For computing dynamic transshipments on temporal networks, this results in a Oâ¢(Î¼7)ğ‘‚superscriptğœ‡7O(\mu^{7})italic_O ( italic_Î¼ start_POSTSUPERSCRIPT 7 end_POSTSUPERSCRIPT ) time algorithm, whereas the previous best integral exact algorithm runs in time O~â¢(Î¼19)~ğ‘‚superscriptğœ‡19\tilde{O}(\mu^{19})over~ start_ARG italic_O end_ARG ( italic_Î¼ start_POSTSUPERSCRIPT 19 end_POSTSUPERSCRIPT ). We achieve similar improvements for other flow problems on temporal networks.","With the rapid ongoing deployment of constellations of small and nano satellites, intersatellite communication systems such as Starlink are quickly realizing the potential of a super fast â€œspace internetâ€, bringing access to remote parts of the world [Han18]. But these systems present new challenges for algorithm design: standard routing and communication protocols designed for static networks do not work as-is on space networks. Objects in space are constantly in motion, and the ability for one object to communicate with another may exist at some times but not at others, such as when their connection is blocked by a planetary body or other object (see Figure 1). Further, the movement of objects may change the time it takes a message to travel from one point to another, depending on when the message departs. This adds temporal effects to routing in space networking that are not present in traditional networking. Temporal effects are also present in terrestrial networks, such as for example transportation networks. Consider a shipping company like FedEx transporting large loads between different cities. The scheduling of vehicles faces many temporal constraints â€“ coordination with scheduled flights; transit times on highways varying over the course of a day; the availability of vehicles or drivers varying over time, etc. These constraints place transport networks outside the realm of settings most routing or flow algorithms are designed for. In this work we develop fast strongly polynomial time algorithms for flow problems in temporal networks. Figure 1: A visualization of the types of changes that can happen in space networks. Due to changes in orientation or movement of other celestial objects, capacities and travel times associated with individual connections may change over time. We focus on a network model where the existence, capacities, and lengths (i.e. travel times) of edges can vary with time but those variations are known in advance. This is a reasonable model for both of the applications mentioned above. In space networking, for example, objects move according to predictable patterns. The standard â€œcontact graphâ€ model [FDB21, HSC+22] assumes that connections between network nodes exist only for a specified period of time, but all such periods are known ahead of time, and each connection, or edge, has an associated length and capacity. Likewise, for transportation networks, transit times and other temporal constraints are often fixed in advance or predictable. We call networks with time-varying capacities or edge lengths temporal, and those with fixed parameters static.111Note that flow problems on â€œstaticâ€ networks are different from â€œsteady-stateâ€ flow problems as, in the former, flow takes time to traverse edges. Before we describe the problems we study, let us specify some key parameters and features of our setting. We consider a network with nğ‘›nitalic_n nodes and mâ‰¤(n2)ğ‘šbinomialğ‘›2m\leq{n\choose 2}italic_m â‰¤ ( binomial start_ARG italic_n end_ARG start_ARG 2 end_ARG ) possible edges. Each edge in the network has a capacity and a length, which are functions of time. We assume that all capacities and lengths are integral. For edge iâ¢jğ‘–ğ‘—ijitalic_i italic_j with i,jâˆˆ[n]ğ‘–ğ‘—delimited-[]ğ‘›i,j\in[n]italic_i , italic_j âˆˆ [ italic_n ], we use Î¼iâ¢jsubscriptğœ‡ğ‘–ğ‘—\mu_{ij}italic_Î¼ start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT to denote the number of times either of these features, length or capacity, of the edge change. Let Î¼:=âˆ‘i,jâˆˆ[n]Î¼iâ¢jassignğœ‡subscriptğ‘–ğ‘—delimited-[]ğ‘›subscriptğœ‡ğ‘–ğ‘—\mu:=\sum_{i,j\in[n]}\mu_{ij}italic_Î¼ := âˆ‘ start_POSTSUBSCRIPT italic_i , italic_j âˆˆ [ italic_n ] end_POSTSUBSCRIPT italic_Î¼ start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT. In problems involving multiple sources or sinks, we use kğ‘˜kitalic_k to denote the number of sources and sinks. We consider flow problems with a finite time horizon Tğ‘‡Titalic_T, but we think of Tğ‘‡Titalic_T as being much larger (e.g., exponential) than the other parameters of the problem, such as mğ‘šmitalic_m, nğ‘›nitalic_n, and Î¼ğœ‡\muitalic_Î¼. Accordingly we are primarily interested in strongly polynomial time algorithms that run in time polyâ¡(n,m,k,Î¼)polyğ‘›ğ‘šğ‘˜ğœ‡\operatorname{poly}(n,m,k,\mu)roman_poly ( italic_n , italic_m , italic_k , italic_Î¼ ). Finally, as we are considering networks that are constantly changing, we are primarily interested in the regime where Î¼>m,n,kğœ‡ğ‘šğ‘›ğ‘˜\mu>m,n,kitalic_Î¼ > italic_m , italic_n , italic_k. There are a variety of interesting network problems under this model and we discuss them in detail in the following subsection. Our work focuses on the Dynamic Transshipment problem, where we are given a vector of demands and supply at different nodes in the network and ask whether there is a feasible flow over the period [0,T]0ğ‘‡[0,T][ 0 , italic_T ] that satisfies these parameters. This problem is a key subroutine for many flow variants, such as maximum flow, quickest flow, and quickest transshipment. The seminal work of Hoppe and Tardos [HT00] was the first to develop strongly polynomial time algorithms for flow problems on temporal networks. For the Dynamic Transshipment problem on temporal networks, the Hoppe-Tardos approach achieves a running time of O~â¢(Î¼19)~ğ‘‚superscriptğœ‡19\tilde{O}(\mu^{19})over~ start_ARG italic_O end_ARG ( italic_Î¼ start_POSTSUPERSCRIPT 19 end_POSTSUPERSCRIPT ) in the large Î¼ğœ‡\muitalic_Î¼ regime. We leverage the framework and reductions presented in Hoppe and Tardos to design our algorithms. Our primary technical contribution is a much more efficient feasibility subroutine that exploits the cut structure of the temporal network. This allows us to obtain an Oâ¢(Î¼7)ğ‘‚superscriptğœ‡7O(\mu^{7})italic_O ( italic_Î¼ start_POSTSUPERSCRIPT 7 end_POSTSUPERSCRIPT ) time algorithm for Dynamic Transshipment, a significant improvement. It is worth noting that [SST22] provide an algorithm for dynamic transshipment on static networks, that for temporal networks with large Î¼ğœ‡\muitalic_Î¼ implies an O~â¢(Î¼7)~ğ‘‚superscriptğœ‡7\tilde{O}(\mu^{7})over~ start_ARG italic_O end_ARG ( italic_Î¼ start_POSTSUPERSCRIPT 7 end_POSTSUPERSCRIPT ) time algorithm.222Here we use O~~ğ‘‚\tilde{O}over~ start_ARG italic_O end_ARG to suppress polylogarithmic terms. However, Schloter et al.â€™s algorithm returns a fractional flow whereas ours (and Hoppe-Tardos) returns an integral solution. To our knowledge, apart from Hoppe-Tardos, no other integral strongly polynomial time algorithms were known for this setting prior to our work. We now describe the literature on flow problems in temporal networks and our contributions in more detail. Flow in time-varying networks and our contributions One of the most basic routing problems on temporal networks is single source shortest paths (SSSP) â€“ what is the earliest time that a message departing a location sğ‘ sitalic_s at time 00 can arrive at a destination node? The complexity of this problem depends on whether or not we are allowed to temporarily â€œstoreâ€ packets at intermediate nodes to wait out changes in capacity of travel times.333For SSSP, an equivalent assumption to the model where waiting is allowed is to assume that all travel times are FIFO, or in other words that for all t<tâ€²ğ‘¡superscriptğ‘¡â€²t<t^{\prime}italic_t < italic_t start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT and all edges iâ¢jğ‘–ğ‘—ijitalic_i italic_j a message departing iğ‘–iitalic_i at time tğ‘¡titalic_t cannot arrive at jğ‘—jitalic_j later than a message departing iğ‘–iitalic_i at time tâ€²superscriptğ‘¡â€²t^{\prime}italic_t start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT. In this paper, we will primarily use the waiting notation because it is more compatible with the flow problems we want to solve, and with the contact graph models often used in space networking. When storage is not allowed, the SSSP problem is NP-hard [Zei23]. In fact, [Zei23] shows that the problem is strongly NP-hard even when all travel times are forced to be piecewise linear. Jain et al. discuss how a variation of Dijkstraâ€™s algorithm can solve this problem when the waiting is allowed [JFP04]. [MKSK+23] and [HMKSC23] further explore this problem and runtime optimization in the contact graph setting where waiting is allowed. These hardness results carry over to flow problems as well; accordingly, henceforth we assume that nodes in the network are endowed with unlimited storage. Our focus is on exact algorithms for flow and transshipment problems, and we consider the following variants. â€¢ Maximum flow on static/temporal networks (MFSN/MFTN): Given a static/temporal network with a single source and sink sğ‘ sitalic_s and dğ‘‘ditalic_d and a time horizon Tğ‘‡Titalic_T, find a flow that maximizes the net flow arriving at dğ‘‘ditalic_d (and has net 00 flow at all other nodes at the end of the period). The maximum flow problem for static networks dates back to the work of Ford and Fulkerson [FF56, FF58], and the problem can in fact be reduced to the (steady-state) minimum cost circulation problem, for which there are a variety of solutions (e.g. [Orl88, CKL+22]). Skutella provides a nice summary of these results [Sku09], and Fleischer and Tardos study a continuous-time extension of this problem (as well as several others) [FT98]. For temporal networks, the maximum flow problem (and a more complex variant called the universal maximum flow problem) have been studied and can be solved efficiently in the case that capacity functions are piecewise constant and all travel times are uniformly 00 [Ogi88, Fle99]. â€¢ Quickest flow on static/temporal networks (QFSN/QFTN): Given a static/temporal network with a single source and sink sğ‘ sitalic_s and dğ‘‘ditalic_d and a positive real value vğ‘£vitalic_v, find the smallest time horizon Tğ‘‡Titalic_T such that there is a flow over time horizon Tğ‘‡Titalic_T under which the net flow into dğ‘‘ditalic_d is vğ‘£vitalic_v. This problem has been extensively studied on static networks [BDK93, LJ15, SS17a], and Hoppe and Tardos [Hop95, HT95, HT00] give a reduction of this problem (and in fact the more general transshipment variant) on temporal networks to quickest transshipment problem on static networks. â€¢ Quickest transhipment on static/temporal networks (QTSN/QTTN): Given a static/temporal network with node set Vğ‘‰Vitalic_V and a vector of values vâˆˆâ„|V|ğ‘£superscriptâ„ğ‘‰v\in\mathbb{R}^{|V|}italic_v âˆˆ blackboard_R start_POSTSUPERSCRIPT | italic_V | end_POSTSUPERSCRIPT (called the demand vector), find the smallest time horizon Tğ‘‡Titalic_T such that there is a flow over time horizon Tğ‘‡Titalic_T under which the net flow at time Tğ‘‡Titalic_T into any node iâˆˆVğ‘–ğ‘‰i\in Vitalic_i âˆˆ italic_V is visubscriptğ‘£ğ‘–v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. In the case that a temporal network has a single sink and piecewise constant capacity functions and all transit times are uniformly 00, [HO82] and [Fle01] give efficient algorithms for solving this problem. Hoppe and Tardos [Hop95, HT95, HT00] and Schloter et al. [SS17b, SST22] give efficient algorithms for this problem on static networks. Further, Hoppe and Tardos show that this problem on temporal networks reduces to the same problem on static networks, so in fact all of these results can also be used to solve the quickest transshipment problem on temporal networks. Additionally, the flow obtained by the Hoppe and Tardos algorithm is guaranteed to be integral if the input values are integral, while the Schloter et al. results may return a fractional solution. [FS02] and [HHS07] additionally show hardness and approximation results for a multicommodity variant of this problem on static networks. One may also consider the minimum cost variants of max flow and quickest transshipment; these problems are NP-hard even on a static network [KW95, KW04]. However [Sku23] shows that the quickest minimum cost transshipment problem is tractable - that is, it is tractable to find the flow of minimum time among all flows of minimum cost but not the one of minimum cost among all flows of minimal time. In this paper, we will primarily focus on the Dynamic Transshipment problem on Static/Temporal Networks (DTSN/DTTN), in which we receive a static/temporal network Nğ‘Nitalic_N, a time horizon Tğ‘‡Titalic_T, and a demand vector vğ‘£vitalic_v, and the goal is to find a flow over the period [0,T]0ğ‘‡[0,T][ 0 , italic_T ] such that the net flow into each node iğ‘–iitalic_i is visubscriptğ‘£ğ‘–v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, if such a flow exists. Note that the quickest transshipment and maximum flow problems can be viewed as generalizations of this problem, which is why we begin by focusing on this simpler question. Most of the work discussed here will revolve around modifications to the transshipment algorithm proposed by Hoppe and Tardos. We begin by focusing on the static variant of this problem and then consider how our runtime compares to existing runtimes when the input to this problem is the output of Hoppe and Tardosâ€™s reduction for temporal networks. (Notably, this reduction creates many new terminals, so we would like our algorithm to have small dependence on the number of terminals as well as the number of nodes and edges.) In this section, we let Mâ¢Fâ¢(n,m)ğ‘€ğ¹ğ‘›ğ‘šMF(n,m)italic_M italic_F ( italic_n , italic_m ) and Mâ¢Câ¢Fâ¢(n,m)ğ‘€ğ¶ğ¹ğ‘›ğ‘šMCF(n,m)italic_M italic_C italic_F ( italic_n , italic_m ) be the runtime for a maximum flow algorithm and minimum cost flow algorithm, respectively, on a steady-state network of nğ‘›nitalic_n nodes and mğ‘šmitalic_m edges. For a static network with nğ‘›nitalic_n nodes, mğ‘šmitalic_m edges, kğ‘˜kitalic_k terminals, maximum capacity Uğ‘ˆUitalic_U, and time horizon Tğ‘‡Titalic_T, Hoppe and Tardos show that the DTSN problem can be solved in weakly polynomial time Oâ¢(k3â¢Mâ¢Câ¢Fâ¢(n,m)â¢log2â¡(nâ¢Uâ¢T))ğ‘‚superscriptğ‘˜3ğ‘€ğ¶ğ¹ğ‘›ğ‘šsuperscript2ğ‘›ğ‘ˆğ‘‡O(k^{3}MCF(n,m)\log^{2}(nUT))italic_O ( italic_k start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT italic_M italic_C italic_F ( italic_n , italic_m ) roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_n italic_U italic_T ) ) or strongly polynomial time O~â¢(k15â¢m4)~ğ‘‚superscriptğ‘˜15superscriptğ‘š4\tilde{O}(k^{15}m^{4})over~ start_ARG italic_O end_ARG ( italic_k start_POSTSUPERSCRIPT 15 end_POSTSUPERSCRIPT italic_m start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT ). This algorithm calls a feasibility oracle, and the primary contribution of our work is the development of a new feasibility oracle that is more efficient in the regime where there are many terminals. In particular, the Hoppe-Tardos algorithm using our feasibility oracle runs in weakly polynomial time Oâ¢(kâ‹…Mâ¢Fâ¢(m,nâ¢m)â¢logâ¡(nâ¢Uâ¢T))ğ‘‚â‹…ğ‘˜ğ‘€ğ¹ğ‘šğ‘›ğ‘šğ‘›ğ‘ˆğ‘‡O(k\cdot MF(m,nm)\log(nUT))italic_O ( italic_k â‹… italic_M italic_F ( italic_m , italic_n italic_m ) roman_log ( italic_n italic_U italic_T ) ) or strongly polynomial time Oâ¢(kâ¢(Mâ¢Fâ¢(m,nâ¢m))2)ğ‘‚ğ‘˜superscriptğ‘€ğ¹ğ‘šğ‘›ğ‘š2O(k(MF(m,nm))^{2})italic_O ( italic_k ( italic_M italic_F ( italic_m , italic_n italic_m ) ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )444Note that in our strongly polynomial bound, the maximum flow algorithm must use only additions and comparisons as it relies on a parametric search algorithm of Megiddo [Meg78].. In particular, using the minimum cost flow algorithm of [CKL+22] and maximum flow algorithm of [Orl13], we get that the Hoppe-Tardos algorithm with our feasibility oracle is faster than that in the original paper if k=Ï‰â¢(n)ğ‘˜ğœ”ğ‘›k=\omega(\sqrt{n})italic_k = italic_Ï‰ ( square-root start_ARG italic_n end_ARG ) (for the weakly polynomial case) or k=Ï‰â¢(n1/7)ğ‘˜ğœ”superscriptğ‘›17k=\omega(n^{1/7})italic_k = italic_Ï‰ ( italic_n start_POSTSUPERSCRIPT 1 / 7 end_POSTSUPERSCRIPT ) (for the strongly polynomial case). source DTSN runtime DTTN runtime integral? [Hop95, HT00] Oâ¢(k3â¢Mâ¢Câ¢Fâ¢(n,m)â¢log2â¡(nâ¢Uâ¢T))ğ‘‚superscriptğ‘˜3ğ‘€ğ¶ğ¹ğ‘›ğ‘šsuperscript2ğ‘›ğ‘ˆğ‘‡O(k^{3}MCF(n,m)\log^{2}(nUT))italic_O ( italic_k start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT italic_M italic_C italic_F ( italic_n , italic_m ) roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_n italic_U italic_T ) ) Oâ¢(Î¼3â¢Mâ¢Câ¢Fâ¢(Î¼,Î¼)â¢log2â¡(Î¼â¢Uâ¢T))ğ‘‚superscriptğœ‡3ğ‘€ğ¶ğ¹ğœ‡ğœ‡superscript2ğœ‡ğ‘ˆğ‘‡O(\mu^{3}MCF(\mu,\mu)\log^{2}(\mu UT))italic_O ( italic_Î¼ start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT italic_M italic_C italic_F ( italic_Î¼ , italic_Î¼ ) roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_Î¼ italic_U italic_T ) ) integral Oâ¢(k3â¢m1+oâ¢(1)â¢logâ¡Uâ¢log2â¡(nâ¢Uâ¢T))ğ‘‚superscriptğ‘˜3superscriptğ‘š1ğ‘œ1ğ‘ˆsuperscript2ğ‘›ğ‘ˆğ‘‡O(k^{3}m^{1+o(1)}\log U\log^{2}(nUT))italic_O ( italic_k start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT italic_m start_POSTSUPERSCRIPT 1 + italic_o ( 1 ) end_POSTSUPERSCRIPT roman_log italic_U roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_n italic_U italic_T ) ) Oâ¢(Î¼4+oâ¢(1)â¢logâ¡Uâ¢log2â¡(Î¼â¢Uâ¢T))ğ‘‚superscriptğœ‡4ğ‘œ1ğ‘ˆsuperscript2ğœ‡ğ‘ˆğ‘‡O(\mu^{4+o(1)}\log U\log^{2}(\mu UT))italic_O ( italic_Î¼ start_POSTSUPERSCRIPT 4 + italic_o ( 1 ) end_POSTSUPERSCRIPT roman_log italic_U roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_Î¼ italic_U italic_T ) )â€  integral O~â¢(m4â¢k15)~ğ‘‚superscriptğ‘š4superscriptğ‘˜15\tilde{O}(m^{4}k^{15})over~ start_ARG italic_O end_ARG ( italic_m start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT italic_k start_POSTSUPERSCRIPT 15 end_POSTSUPERSCRIPT ) O~â¢(Î¼19)~ğ‘‚superscriptğœ‡19\tilde{O}(\mu^{19})over~ start_ARG italic_O end_ARG ( italic_Î¼ start_POSTSUPERSCRIPT 19 end_POSTSUPERSCRIPT ) integral [SS17b, SST22] O~â¢(m2â¢k5)~ğ‘‚superscriptğ‘š2superscriptğ‘˜5\tilde{O}(m^{2}k^{5})over~ start_ARG italic_O end_ARG ( italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_k start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT ) O~â¢(Î¼7)~ğ‘‚superscriptğœ‡7\tilde{O}(\mu^{7})over~ start_ARG italic_O end_ARG ( italic_Î¼ start_POSTSUPERSCRIPT 7 end_POSTSUPERSCRIPT ) fractional this paper Oâ¢(kâ¢Mâ¢Fâ¢(m,nâ¢m)â¢logâ¡(nâ¢Uâ¢T))ğ‘‚ğ‘˜ğ‘€ğ¹ğ‘šğ‘›ğ‘šğ‘›ğ‘ˆğ‘‡O(kMF(m,nm)\log(nUT))italic_O ( italic_k italic_M italic_F ( italic_m , italic_n italic_m ) roman_log ( italic_n italic_U italic_T ) ) Oâ¢(Î¼â¢Mâ¢Fâ¢(Î¼,Î¼2)â¢logâ¡(Î¼â¢Uâ¢T))ğ‘‚ğœ‡ğ‘€ğ¹ğœ‡superscriptğœ‡2ğœ‡ğ‘ˆğ‘‡O(\mu MF(\mu,\mu^{2})\log(\mu UT))italic_O ( italic_Î¼ italic_M italic_F ( italic_Î¼ , italic_Î¼ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) roman_log ( italic_Î¼ italic_U italic_T ) ) integral Oâ¢(kâ¢(Mâ¢Fâ¢(m,nâ¢m)+Mâ¢Câ¢Fâ¢(n,m))2)âˆ—ğ‘‚superscriptğ‘˜superscriptğ‘€ğ¹ğ‘šğ‘›ğ‘šğ‘€ğ¶ğ¹ğ‘›ğ‘š2O(k(MF(m,nm)+MCF(n,m))^{2})^{*}italic_O ( italic_k ( italic_M italic_F ( italic_m , italic_n italic_m ) + italic_M italic_C italic_F ( italic_n , italic_m ) ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT Oâ¢(Î¼â¢(Mâ¢Fâ¢(Î¼,Î¼2)+Mâ¢Câ¢Fâ¢(Î¼,Î¼))2)âˆ—ğ‘‚superscriptğœ‡superscriptğ‘€ğ¹ğœ‡superscriptğœ‡2ğ‘€ğ¶ğ¹ğœ‡ğœ‡2O(\mu(MF(\mu,\mu^{2})+MCF(\mu,\mu))^{2})^{*}italic_O ( italic_Î¼ ( italic_M italic_F ( italic_Î¼ , italic_Î¼ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) + italic_M italic_C italic_F ( italic_Î¼ , italic_Î¼ ) ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT integral Oâ¢(kâ¢(nâ¢m)1+oâ¢(1)â¢logâ¡(Uâ¢T)â¢logâ¡(nâ¢Uâ¢T))â€ ğ‘‚superscriptğ‘˜superscriptğ‘›ğ‘š1ğ‘œ1ğ‘ˆğ‘‡ğ‘›ğ‘ˆğ‘‡â€ O(k(nm)^{1+o(1)}\log(UT)\log(nUT))^{\dagger}italic_O ( italic_k ( italic_n italic_m ) start_POSTSUPERSCRIPT 1 + italic_o ( 1 ) end_POSTSUPERSCRIPT roman_log ( italic_U italic_T ) roman_log ( italic_n italic_U italic_T ) ) start_POSTSUPERSCRIPT â€  end_POSTSUPERSCRIPT Oâ¢(Î¼3+oâ¢(1)â¢logâ¡(Uâ¢T)â¢logâ¡(Î¼â¢Uâ¢T))ğ‘‚superscriptğœ‡3ğ‘œ1ğ‘ˆğ‘‡ğœ‡ğ‘ˆğ‘‡O(\mu^{3+o(1)}\log(UT)\log(\mu UT))italic_O ( italic_Î¼ start_POSTSUPERSCRIPT 3 + italic_o ( 1 ) end_POSTSUPERSCRIPT roman_log ( italic_U italic_T ) roman_log ( italic_Î¼ italic_U italic_T ) )â€  integral Oâ¢(kâ¢n2â¢m4)â€¡ğ‘‚superscriptğ‘˜superscriptğ‘›2superscriptğ‘š4â€¡O(kn^{2}m^{4})^{\ddagger}italic_O ( italic_k italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_m start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT â€¡ end_POSTSUPERSCRIPT Oâ¢(Î¼7)ğ‘‚superscriptğœ‡7O(\mu^{7})italic_O ( italic_Î¼ start_POSTSUPERSCRIPT 7 end_POSTSUPERSCRIPT )â€¡ integral Table 1: A summary of runtimes for the DTSN and DTTN problems from [Hop95, HT00], [SS17b, SST22], and this paper. The first column lists the source of the given runtimes. The second column gives the runtime for DTSN on a static network of kğ‘˜kitalic_k terminals, nğ‘›nitalic_n nodes, mğ‘šmitalic_m edges, maximum capacity Uğ‘ˆUitalic_U, and time horizon Tğ‘‡Titalic_T. The third line is the DTTN runtime for a temporal network with parameter Î¼ğœ‡\muitalic_Î¼, maximum capacity Uğ‘ˆUitalic_U, and time horizon Tğ‘‡Titalic_T. The final column denotes whether the output is integral or fractional when the input values are integral. Highlighted in blue are the runtimes for DTTN with the state of the art maximum flow and minimum cost flow algorithms applied. These are the primary focus of this paper. Further, the strongly polynomial DTTN runtime can also be obtained using push-relabel algorithm of [GT88], as the number of edges is quadratic in the number of nodes. Note that in the third column, we have assumed that Î©â¢(m)Î©ğ‘š\Omega(m)roman_Î© ( italic_m ) edges are non-static. âˆ— These runtimes require that the associated min cost flow and maximum flow algorithms use only additions and comparisons. â€  These runtimes are optimized using the min cost flow algorithm of [CKL+22] as a subroutine. â€¡ These runtimes are optimized using the max flow algorithm of [Orl13] and the min cost flow algorithm from [Orl88] as a subroutine. Schloter, Skutella, and Tran [SS17b, SST22] also extensively study the problem of dynamic transshipments on static networks. They obtain a strongly polynomial algorithm that runs in time O~â¢(k5â¢m2)~ğ‘‚superscriptğ‘˜5superscriptğ‘š2\tilde{O}(k^{5}m^{2})over~ start_ARG italic_O end_ARG ( italic_k start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ), while our feasibility oracle paired with the Hoppe-Tardos algorithm obtains strongly polynomial runtime Oâ¢(kâ¢n2â¢m4)ğ‘‚ğ‘˜superscriptğ‘›2superscriptğ‘š4O(kn^{2}m^{4})italic_O ( italic_k italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_m start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT ). Thus, our strongly polynomial algorithm only matches theirs when k=Î©â¢(nâ¢m)ğ‘˜Î©ğ‘›ğ‘šk=\Omega(\sqrt{nm})italic_k = roman_Î© ( square-root start_ARG italic_n italic_m end_ARG ). However, because we primarily focus on temporal networks, we care about runtimes on the kinds of networks that appear as the output of the Hoppe-Tardos reduction from the static problem to the temporal problem; in these networks we will have k=Î˜â¢(Î¼)=Î©â¢(n,m)ğ‘˜Î˜ğœ‡Î©ğ‘›ğ‘šk=\Theta(\mu)=\Omega(n,m)italic_k = roman_Î˜ ( italic_Î¼ ) = roman_Î© ( italic_n , italic_m ).555Note that if we do not have Î¼=Î©â¢(m)ğœ‡Î©ğ‘š\mu=\Omega(m)italic_Î¼ = roman_Î© ( italic_m ), the algorithms of [SS17b, SST22] may be faster than that presented here. If a network has mğ‘šmitalic_m edges, and Î¼ğœ‡\muitalic_Î¼ is the number of constant pieces among the temporal edges only, then in the regime where kâ‰¤nâ‰¤Î¼â‰¤mğ‘˜ğ‘›ğœ‡ğ‘šk\leq n\leq\mu\leq mitalic_k â‰¤ italic_n â‰¤ italic_Î¼ â‰¤ italic_m, we get a strongly polynomial runtime of Oâ¢(Î¼3â¢m4)ğ‘‚superscriptğœ‡3superscriptğ‘š4O(\mu^{3}m^{4})italic_O ( italic_Î¼ start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT italic_m start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT ) for the DTTN problem, whereas [SS17b, SST22] get a runtime of O~â¢(Î¼5â¢m2)~ğ‘‚superscriptğœ‡5superscriptğ‘š2\tilde{O}(\mu^{5}m^{2})over~ start_ARG italic_O end_ARG ( italic_Î¼ start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) and [HT00] get a runtime of O~â¢(Î¼15â¢m4)~ğ‘‚superscriptğœ‡15superscriptğ‘š4\tilde{O}(\mu^{15}m^{4})over~ start_ARG italic_O end_ARG ( italic_Î¼ start_POSTSUPERSCRIPT 15 end_POSTSUPERSCRIPT italic_m start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT ). In the weakly polynomial setting, we get a runtime of Oâ¢(Î¼2+oâ¢(1)â¢m1+oâ¢(1)â¢logâ¡(Uâ¢T)â¢logâ¡(Î¼â¢Uâ¢T))ğ‘‚superscriptğœ‡2ğ‘œ1superscriptğ‘š1ğ‘œ1ğ‘ˆğ‘‡ğœ‡ğ‘ˆğ‘‡O(\mu^{2+o(1)}m^{1+o(1)}\log(UT)\log(\mu UT))italic_O ( italic_Î¼ start_POSTSUPERSCRIPT 2 + italic_o ( 1 ) end_POSTSUPERSCRIPT italic_m start_POSTSUPERSCRIPT 1 + italic_o ( 1 ) end_POSTSUPERSCRIPT roman_log ( italic_U italic_T ) roman_log ( italic_Î¼ italic_U italic_T ) ) and [HT00] get a runtime of Oâ¢(Î¼3â¢m1+oâ¢(1)â¢logâ¡Uâ¢log2â¡(Î¼â¢Uâ¢T))ğ‘‚superscriptğœ‡3superscriptğ‘š1ğ‘œ1ğ‘ˆsuperscript2ğœ‡ğ‘ˆğ‘‡O(\mu^{3}m^{1+o(1)}\log U\log^{2}(\mu UT))italic_O ( italic_Î¼ start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT italic_m start_POSTSUPERSCRIPT 1 + italic_o ( 1 ) end_POSTSUPERSCRIPT roman_log italic_U roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_Î¼ italic_U italic_T ) ). Further, the Schloter et al. results can potentially return a non-integral solution even when all input values are integral, whereas Hoppe and Tardos (including under our feasibility oracle) always produces an integral solution on integral inputs. Table 1 describes the runtimes obtained for the DTSN problem via our new algorithms, via the algorithms of Hoppe-Tardos, and via the algorithms of Schloter et al. after applying the Hoppe-Tardos temporal to static reduction. Further, in Section 6, we will discuss how to efficiently find the optimal time horizon for quickest transshipment problems and the optimal flow value for maximum flow problems in this setting, so that (after applying the Hoppe-Tardos temporal to static reduction) these problems can also be solved in Oâ¢(Î¼7)ğ‘‚superscriptğœ‡7O(\mu^{7})italic_O ( italic_Î¼ start_POSTSUPERSCRIPT 7 end_POSTSUPERSCRIPT ) time on temporal networks. The rest of the paper will proceed as follows: In Section 2, we formally define the flow problems we will be considering, present some background information, and present our results in more detail. In Section 4, we describe a ""condensed"" time-expanded network that enables fast feasibility testing. In Section 3, we present special types of networks, including canonical temporal networks, and describe how the various problems relate to each other. In Section 5, we present an algorithm for feasibility on the DTSN problem that is efficient on canonical temporal networks. In Section 6, we discuss how to expand the scope of our DTTN results to deal with problems like quickest transshipment and maximum flow on temporal networks. Figure 2: A summary of the types of flow problems we will study in this paper and how they relate to each other. White arrows indicate that the source problem is a special case of the destination problem, and black errors indicate that there is a reduction from the source problem to the destination problem. Note that [HT95, HT00] showed that dynamic transshipment algorithms on static graphs can be used to solve dynamic transshipment problems on temporal graphs (when the capacity functions have few break points).(We will define the cDFSN problem in Section 4.)"
https://arxiv.org/html/2411.04846v1,On the Complexity of 2-club Cluster Editing with Vertex Splitting,"Editing a graph to obtain a disjoint union of sğ‘ sitalic_s-clubs is one of the models for correlation clustering, which seeks a partition of the vertex set of a graph so that elements of each resulting set are close enough according to some given criterion. For example, in the case of editing into sğ‘ sitalic_s-clubs, the criterion is proximity since any pair of vertices (in an sğ‘ sitalic_s-club) are within a distance of sğ‘ sitalic_s from each other. In this work we consider the vertex splitting operation, which allows a vertex to belong to more than one cluster. This operation was studied as one of the parameters associated with the Cluster Editing problem. We study the complexity and parameterized complexity of the sğ‘ sitalic_s-Club Cluster Edge Deletion with Vertex Splitting and sğ‘ sitalic_s-Club Cluster Vertex Splitting problems. Both problems are shown to be \NP\NP\NP-Complete and \APX\APX\APX-hard. On the positive side, we show that both problems are Fixed-Parameter Tractable with respect to the number of allowed editing operations and that sğ‘ sitalic_s-Club Cluster Vertex Splitting is solvable in polynomial-time on the class of forests.","Correlation clustering is viewed as a graph modification problem where the objective is to perform a sequence of editing operations (or modifications) to obtain a disjoint union of clusters. Many variants of this problem have been studied in the literature, each with a different definition either of what a cluster means or of the various types of allowed modifications. In the Cluster Editing problem, for example, a cluster was defined to be a clique and the allowed editing operations were edge additions and deletions [12, 22, 18]. Later, some relaxation models such as sğ‘ sitalic_s-Clubs and sğ‘ sitalic_s-Clans emerged as they were deemed ideal models for clustering Biological Networks [7, 27]. Subsequent efforts studied overlapping clusters in a graph theoretical context [10, 15, 4]. In this work, we deal with overlapping communities by performing vertex splitting, which allows a vertex to be cloned and placed in more than one cluster. This operation was introduced in [4] in the study of the Cluster editing with Vertex Splitting problem. The notion of vertex splitting was first introduced in [19] but not in the context of correlation clustering. The Cluster Editing and Cluster Deletion problems were shown to be \NP\NP\NP-Complete in [22, 28]. Several other variants of the problem have also been proved to be \NP\NP\NP-Complete. This includes Cluster Vertex Deletion [23], 2-club Cluster Editing [25], 2-club Cluster Vertex Deletion [25], 2-club Cluster Edge Deletion [25] , Cluster Vertex Splitting [17], and Cluster Editing with Vertex Splitting [2, 5]. From a parameterized complexity standpoint, Cluster Editing, Cluster Deletion, and Cluster Vertex Deletion are known to be Fixed-Parameter Tractable (\FPT\FPT\FPT) [18, 21]. The same holds for the two club-variants: 2-club Cluster Edge Deletion and 2-club Cluster Vertex Deletion [25], while 2-club Cluster Editing was shown to be \Wâ¢[2]\Wdelimited-[]2\W[2][ 2 ]-Hard [16]. Furthermore, the Cluster Editing with Vertex Splitting problem has also been show to be \FPT\FPT\FPT [2, 5]. From a polynomial-time approximation standpoint, the Cluster Editing and Cluster Edge Deletion problems are \APX\APX\APX-Hard and have Oâ¢(logâ¡n)ğ‘‚ğ‘›O(\log n)italic_O ( roman_log italic_n ) approximation algorithms [13]. On the other hand, Cluster Vertex Deletion has a factor-two approximation algorithm [6]. To the best of our knowledge, problem variants with sğ‘ sitalic_s-clubs or vertex splitting do not have any known approximation results. The problems mentioned above are all considered different models of correlation clustering. The sğ‘ sitalic_s-Club models were shown to be effective in some networks where a clique could not capture all information needed to form better clusters [7, 27]. Vertex splitting proved to be useful, and in fact essential, when the input data has overlapping clusters, such as in protein networks [26]. So far, vertex splitting has been used along with cluster editing. In this paper we introduce the operation to the club-clustering variant by introducing two new problems: 2-club Cluster Vertex Splitting (2CCVS) and 2-club Cluster Edge Deletion with Vertex Splitting (2CCEDVS). These problems seek to modify a graph into a 2-clubs graph by performing a series of vertex splitting (2CCVS and 2CCEDVS) and edge deletion (2CCEDVS) operations. Our contribution. We prove that 2CCVS and 2CCEDVS are \NP\NP\NP-Complete. On the positive side, we prove that both problems are \FPT\FPT\FPT and that 2CCVS is solvable in polynomial-time on forests. We also show that, unless Â¶=\NPÂ¶\NP\P=\NPÂ¶ =, the two problems cannot be approximated in polynomial time with a ratio better than a certain constant >1absent1>1> 1."
https://arxiv.org/html/2411.04803v1,Unbounded Error Correcting Codes,"We introduce a variant of Error Correcting Codes with no predetermined length. An Unbounded ECC with rate Rğ‘…Ritalic_R and distance Îµğœ€\varepsilonitalic_Îµ is an encoding of a possibly infinite message into a possibly infinite codeword, such that for every large enough kğ‘˜kitalic_k we may recover the first Râ¢kğ‘…ğ‘˜Rkitalic_R italic_k symbols of the message from the first kğ‘˜kitalic_k symbols of the codeword â€” even when up to 12â¢Îµâ¢k12ğœ€ğ‘˜\frac{1}{2}\varepsilon kdivide start_ARG 1 end_ARG start_ARG 2 end_ARG italic_Îµ italic_k of these codeword symbols are adversarially corrupted. We study unbounded codes over a binary alphabet in the regime of small distance Îµğœ€\varepsilonitalic_Îµ, and obtain nearly-tight upper and lower bounds in several natural settings. We show that the optimal rate of such a code is between R<1âˆ’Î©â¢(Îµ)ğ‘…1Î©ğœ€R<1-\Omega(\sqrt{\varepsilon})italic_R < 1 - roman_Î© ( square-root start_ARG italic_Îµ end_ARG ) and R>1âˆ’Oâ¢(Îµâ¢logâ¡logâ¡(1/Îµ))ğ‘…1ğ‘‚ğœ€1ğœ€R>1-O\left(\sqrt{\varepsilon\log\log\left(1/\varepsilon\right)}\right)italic_R > 1 - italic_O ( square-root start_ARG italic_Îµ roman_log roman_log ( 1 / italic_Îµ ) end_ARG ). Surprisingly, our construction is non-linear, and we show that the optimal rate of a linear unbounded code is the asymptotically worse R=1âˆ’Î˜â¢(Îµâ¢logâ¡(1/Îµ))ğ‘…1Î˜ğœ€1ğœ€R=1-\Theta\left(\sqrt{\varepsilon\log\left(1/\varepsilon\right)}\right)italic_R = 1 - roman_Î˜ ( square-root start_ARG italic_Îµ roman_log ( 1 / italic_Îµ ) end_ARG ). In the setting of random noise, the optimal rate of unbounded codes improves and matches the rate of standard codes at R=1âˆ’Î˜â¢(Îµâ¢logâ¡(1/Îµ))ğ‘…1Î˜ğœ€1ğœ€R=1-\Theta({\varepsilon\log{\left(1/\varepsilon\right)}})italic_R = 1 - roman_Î˜ ( italic_Îµ roman_log ( 1 / italic_Îµ ) ).","Error Correcting Codes (ECCs) are the means to compensate for errors in the transmission of messages. An ECC encodes a message into a slightly larger codeword, such that even if a certain fraction of the codeword is corrupted the message can still be recovered. ECCs are extensively studied and it is long known that good ECCs can be constructed, even over a binary alphabet. [Ham50, Gil52, Var57, Jus72, SS96]. We define a natural generalization of ECCs, in which the length of the message (and hence also of the codeword) is not fixed. We call those Unbounded ECCs. We would require that for any kğ‘˜kitalic_k, the first kğ‘˜kitalic_k symbols of the message can be decoded from the first Oâ¢(k)ğ‘‚ğ‘˜O(k)italic_O ( italic_k ) symbols of the codeword, even if a small fraction of those codeword symbols are corrupted. This allows, for example, robust transmission of a long message in a connection that would abruptly halt at an unknown point in time. Recently, Zamir [Zam24] raised the same question in the specified model of noiseless feedback â€” as a part of a work on Large Language Models watermarking. We study this question in the more general setting. Definition (Unbounded codes). A code C:Î£â‹†â†’Î“â‹†:ğ¶â†’superscriptÎ£â‹†superscriptÎ“â‹†C:\Sigma^{\star}\rightarrow\Gamma^{\star}italic_C : roman_Î£ start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT â†’ roman_Î“ start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT is called an unbounded code with rate Rğ‘…Ritalic_R and distance Îµğœ€\varepsilonitalic_Îµ, or a (R,Îµ)ğ‘…ğœ€(R,\varepsilon)( italic_R , italic_Îµ )-unbounded code, if there exists some k0âˆˆâ„•subscriptğ‘˜0â„•k_{0}\in\mathbb{N}italic_k start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT âˆˆ blackboard_N such that the following holds. Let x,yâˆˆÎ£â‹†ğ‘¥ğ‘¦superscriptÎ£â‹†x,y\in\Sigma^{\star}italic_x , italic_y âˆˆ roman_Î£ start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT, iâ‰¥k0ğ‘–subscriptğ‘˜0i\geq k_{0}italic_i â‰¥ italic_k start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, and jâ‰¥iRğ‘—ğ‘–ğ‘…j\geq\frac{i}{R}italic_j â‰¥ divide start_ARG italic_i end_ARG start_ARG italic_R end_ARG. If x[:i]â‰ y[:i]x[:i]\neq y[:i]italic_x [ : italic_i ] â‰  italic_y [ : italic_i ] then dH(C(x)[:j],C(y)[:j])â‰¥Îµj.d_{H}(C(x)[:j],C(y)[:j])\geq\varepsilon j.italic_d start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ( italic_C ( italic_x ) [ : italic_j ] , italic_C ( italic_y ) [ : italic_j ] ) â‰¥ italic_Îµ italic_j . An unbounded ECC can be viewed as a family of standard ECCs that extend each other. For any large enough kğ‘˜kitalic_k, an (R,Îµ)ğ‘…ğœ€(R,\varepsilon)( italic_R , italic_Îµ )-unbounded code indeed induces a standard ECC for messages of length kğ‘˜kitalic_k with rate Rğ‘…Ritalic_R and distance Îµğœ€\varepsilonitalic_Îµ, which can be constructed by taking the prefix of length k/Rğ‘˜ğ‘…k/Ritalic_k / italic_R of the code words of Cğ¶Citalic_C. This object can be motivated by various natural scenarios, for example, streaming a large file such that at any point in time the receiver can sustain a certain fraction of errors. In this paper, we study unbounded codes with binary alphabet Î£=Î“=ğ”½2Î£Î“subscriptğ”½2\Sigma=\Gamma=\mathbb{F}_{2}roman_Î£ = roman_Î“ = blackboard_F start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, in the regime of small distance Îµâ†’0â†’ğœ€0\varepsilon\rightarrow 0italic_Îµ â†’ 0. We obtain upper and lower bounds for their optimal rates in several natural settings. Our main results are the following nearly-tight bounds on the optimal rate of unbounded ECCs. Theorem. For every small enough Îµ>0ğœ€0\varepsilon>0italic_Îµ > 0 there exists a (R,Îµ)ğ‘…ğœ€(R,\varepsilon)( italic_R , italic_Îµ )-unbounded code with R>1âˆ’Oâ¢(Îµâ¢logâ¡logâ¡(1/Îµ))ğ‘…1ğ‘‚ğœ€1ğœ€R>1-O\left(\sqrt{\varepsilon\log\log\left(1/\varepsilon\right)}\right)italic_R > 1 - italic_O ( square-root start_ARG italic_Îµ roman_log roman_log ( 1 / italic_Îµ ) end_ARG ). Furthermore, for every (R,Îµ)ğ‘…ğœ€(R,\varepsilon)( italic_R , italic_Îµ )-unbounded code it holds that R<1âˆ’Î©â¢(Îµ)ğ‘…1Î©ğœ€R<1-\Omega\left(\sqrt{\varepsilon}\right)italic_R < 1 - roman_Î© ( square-root start_ARG italic_Îµ end_ARG ). The construction we present to achieve this bound is non-linear, which is surprising as in standard ECCs optimal (or nearly-optimal) bounds are achieved using linear constructions. We prove that this is inherent, and in fact, the optimal rate for linear unbounded ECCs is strictly worse. Theorem. For every small enough Îµ>0ğœ€0\varepsilon>0italic_Îµ > 0 there exists a linear (R,Îµ)ğ‘…ğœ€(R,\varepsilon)( italic_R , italic_Îµ )-unbounded code with R>1âˆ’Oâ¢(Îµâ¢logâ¡(1/Îµ))ğ‘…1ğ‘‚ğœ€1ğœ€R>1-O\left(\sqrt{\varepsilon\log\left(1/\varepsilon\right)}\right)italic_R > 1 - italic_O ( square-root start_ARG italic_Îµ roman_log ( 1 / italic_Îµ ) end_ARG ). Furthermore, for every linear (R,Îµ)ğ‘…ğœ€(R,\varepsilon)( italic_R , italic_Îµ )-unbounded code it holds that R<1âˆ’Î©â¢(Îµâ¢logâ¡(1/Îµ))ğ‘…1Î©ğœ€1ğœ€R<1-\Omega\left(\sqrt{\varepsilon\log\left(1/\varepsilon\right)}\right)italic_R < 1 - roman_Î© ( square-root start_ARG italic_Îµ roman_log ( 1 / italic_Îµ ) end_ARG ). Another divergence from standard ECCs is that for unbounded ECCs, random and adversarial errors lead to significantly different optimal rates. We show that if the errors are random bit-flips happening with probability Îµğœ€\varepsilonitalic_Îµ instead of an adversarial Îµğœ€\varepsilonitalic_Îµ-fraction of errors, then an unbounded ECC with a rate similar to that of standard codes is possible. Theorem. For every small enough Îµ>0ğœ€0\varepsilon>0italic_Îµ > 0 there exists a (R,Îµ)ğ‘…ğœ€(R,\varepsilon)( italic_R , italic_Îµ )-unbounded code resilient to BSC(Îµ)ğœ€(\varepsilon)( italic_Îµ ) with R>1âˆ’Oâ¢(Îµâ¢logâ¡(1/Îµ))ğ‘…1ğ‘‚ğœ€1ğœ€R>1-O\left({\varepsilon\log\left(1/\varepsilon\right)}\right)italic_R > 1 - italic_O ( italic_Îµ roman_log ( 1 / italic_Îµ ) ). In a model where the encoder receives an immediate and noiseless feedback as to whether the previous symbol was received with or without errors, [Zam24] shows that a rate of 1âˆ’Î˜â¢(Îµ)1Î˜ğœ€1-\Theta(\varepsilon)1 - roman_Î˜ ( italic_Îµ ) is possible for unbounded ECCs â€” the same optimal rate in this setting as standard ECCs. See Table 1 for a summary of the comparison between the optimal rates of standard and unbounded ECCs in the various regimes. As part of our proof, we study another variant of ECCs we call subset codes. A standard error correcting code can be viewed as a set of vectors such that the distance between every pair of them is large. A subset code is a collection of (possibly large) subsets of ğ”½2nsuperscriptsubscriptğ”½2ğ‘›\mathbb{F}_{2}^{n}blackboard_F start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT such that the distance between every two such subsets is large (but on the other hand, vectors within the same subset might be close to each other). In Section 6 we study these codes and derive optimal asymptotic bounds, this can be of independent interest. Table 1: Optimal rate Rğ‘…Ritalic_R for error Îµâ†’0â†’ğœ€0\varepsilon\rightarrow 0italic_Îµ â†’ 0. Standard ECCs Unbounded ECCs Adversarial errors 1âˆ’Î˜â¢(Îµâ¢logâ¡(1/Îµ))1Î˜ğœ€1ğœ€1-\Theta(\varepsilon\log(1/\varepsilon))1 - roman_Î˜ ( italic_Îµ roman_log ( 1 / italic_Îµ ) ) 1âˆ’Î˜~â¢(Îµ)1~Î˜ğœ€1-\tilde{\Theta}(\sqrt{\varepsilon})1 - over~ start_ARG roman_Î˜ end_ARG ( square-root start_ARG italic_Îµ end_ARG ) Random errors 1âˆ’Î˜â¢(Îµâ¢logâ¡(1/Îµ))1Î˜ğœ€1ğœ€1-\Theta(\varepsilon\log(1/\varepsilon))1 - roman_Î˜ ( italic_Îµ roman_log ( 1 / italic_Îµ ) ) 1âˆ’Î˜â¢(Îµâ¢logâ¡(1/Îµ))1Î˜ğœ€1ğœ€1-\Theta(\varepsilon\log(1/\varepsilon))1 - roman_Î˜ ( italic_Îµ roman_log ( 1 / italic_Îµ ) ) Noiseless feedback 1âˆ’Î˜â¢(Îµ)1Î˜ğœ€1-\Theta(\varepsilon)1 - roman_Î˜ ( italic_Îµ ) 1âˆ’Î˜â¢(Îµ)1Î˜ğœ€1-\Theta(\varepsilon)1 - roman_Î˜ ( italic_Îµ ) 1.1 Connections to Prior Works This definition we consider is somewhat reminiscent of Tree Codes [Sch93, Sch96], but the definitions are qualitatively different: In a tree code, every symbol of the code-word directly corresponds to a single symbol of the message â€” in an unbounded code, there is no such direct correspondence and code symbols may depend on both previous and following message symbols. Furthermore, while in a tree code the distance property is defined with respect to only the part of the code-word succeeding the first disagreement between messages â€” in an unbounded code the distance property is always defined with respect to the entire codeword prefix. Tree codes and other types of codes were considered in the context of error correction of interactive protocols [BR11, GHS14, G+17, EKS20]. Contrary to the setting of interactive codes and also to that of noiseless feedback studied in [Zam24], in our settings there is no interaction whatsoever â€” that is, the receiver is not communicating anything and, in particular, no information about what symbols were corrupted is ever learned by the sender. While the optimal rates of unbounded codes are similar to those of interactive codes with small noise [KR13, Hae14] we are not aware of any formal connection. A somewhat related notion of anytime capacity [SM06] was studied in the context of control theory. Here, the sender does not have the whole message in advance but receives it online, and the model assumes stochastic (random) noise. The goal is that the probability of making a mistake on a bitâ€™s decoding will decrease exponentially with the time passed since the sender received this bit. 1.2 Alphabet Size In the vast majority of this paper we focus on the binary alphabet Î£=Î“=ğ”½2Î£Î“subscriptğ”½2\Sigma=\Gamma=\mathbb{F}_{2}roman_Î£ = roman_Î“ = blackboard_F start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, which is equivalent to the case of an alphabet of any arbitrary constant size |Î£|,|Î“|=Oâ¢(1)Î£Î“ğ‘‚1|\Sigma|,|\Gamma|=O(1)| roman_Î£ | , | roman_Î“ | = italic_O ( 1 ). Nonetheless, the same questions we present may also be asked for an alphabet size that is related to Îµğœ€\varepsilonitalic_Îµ. In standard Error Correcting Codes, the optimal rate of 1âˆ’R=Î˜â¢(Îµâ¢logâ¡(1/Îµ))1ğ‘…Î˜ğœ€1ğœ€1-R=\Theta(\varepsilon\log(1/\varepsilon))1 - italic_R = roman_Î˜ ( italic_Îµ roman_log ( 1 / italic_Îµ ) ) as Îµâ†’0â†’ğœ€0\varepsilon\rightarrow 0italic_Îµ â†’ 0 is refined to 1âˆ’R=Î˜â¢(Îµâ¢(logqâ¡(1/Îµ)+1))1ğ‘…Î˜ğœ€subscriptğ‘1ğœ€11-R=\Theta\left(\varepsilon\left(\log_{q}\left(1/\varepsilon\right)+1\right)\right)1 - italic_R = roman_Î˜ ( italic_Îµ ( roman_log start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ( 1 / italic_Îµ ) + 1 ) ) when the alphabet size q:=|Î£|assignğ‘Î£q:=|\Sigma|italic_q := | roman_Î£ | is taken into account. In our construction of the linear code of Section 4, the bound is similarly refined to 1âˆ’R=Oâ¢(Îµâ¢(logqâ¡(1/Îµ)+1))1ğ‘…ğ‘‚ğœ€subscriptğ‘1ğœ€11-R=O\left(\sqrt{\varepsilon\left(\log_{q}\left(1/\varepsilon\right)+1\right)}\right)1 - italic_R = italic_O ( square-root start_ARG italic_Îµ ( roman_log start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ( 1 / italic_Îµ ) + 1 ) end_ARG ) when the dependence on qğ‘qitalic_q is considered. In particular, when q=Î©â¢(1Îµ)ğ‘Î©1ğœ€q=\Omega\left(\frac{1}{\varepsilon}\right)italic_q = roman_Î© ( divide start_ARG 1 end_ARG start_ARG italic_Îµ end_ARG ) we obtain a linear code with rate 1âˆ’R=Oâ¢(Îµ)1ğ‘…ğ‘‚ğœ€1-R=O\left(\sqrt{\varepsilon}\right)1 - italic_R = italic_O ( square-root start_ARG italic_Îµ end_ARG ). On the other hand, the rate upper bound for linear codes in Section 5 is independent of the alphabet size and thus shows 1âˆ’Râ‰¥Îµ1ğ‘…ğœ€1-R\geq\sqrt{\varepsilon}1 - italic_R â‰¥ square-root start_ARG italic_Îµ end_ARG for any qğ‘qitalic_q, which is tight for q=Î©â¢(1Îµ)ğ‘Î©1ğœ€q=\Omega\left(\frac{1}{\varepsilon}\right)italic_q = roman_Î© ( divide start_ARG 1 end_ARG start_ARG italic_Îµ end_ARG ). 1.3 Organization of the Paper In Section 2 we give a high-level overview of the constructions and proofs in the paper. In Section 4 we construct linear unbounded codes and also show that their rate improves when the errors are random. In Section 5 we derive a simple rate upper bound for linear unbounded codes. In Section 6 we introduce and study subset codes, which we use in the consecutive sections. In Section 7 we present a rate upper bound for general unbounded codes as well as improve the bound for linear codes. In Section 8 we improve our construction using non-linear subset codes. Finally, we conclude and present open problems in Section 9."
https://arxiv.org/html/2411.04718v1,Approximate Counting of Permutation Patterns,"We consider the problem of counting the copies of a length-kğ‘˜kitalic_k pattern Ïƒğœ\sigmaitalic_Ïƒ in a sequence f:[n]â†’â„:ğ‘“â†’delimited-[]ğ‘›â„f\colon[n]\to{\mathbb{R}}italic_f : [ italic_n ] â†’ blackboard_R, where a copy is a subset of indices i1<â€¦<ikâˆˆ[n]subscriptğ‘–1â€¦subscriptğ‘–ğ‘˜delimited-[]ğ‘›i_{1}<\ldots<i_{k}\in[n]italic_i start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT < â€¦ < italic_i start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT âˆˆ [ italic_n ] such that fâ¢(ij)<fâ¢(iâ„“)ğ‘“subscriptğ‘–ğ‘—ğ‘“subscriptğ‘–â„“f(i_{j})<f(i_{\ell})italic_f ( italic_i start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) < italic_f ( italic_i start_POSTSUBSCRIPT roman_â„“ end_POSTSUBSCRIPT ) if and only if Ïƒâ¢(j)<Ïƒâ¢(â„“)ğœğ‘—ğœâ„“\sigma(j)<\sigma(\ell)italic_Ïƒ ( italic_j ) < italic_Ïƒ ( roman_â„“ ). This problem is motivated by a range of connections and applications in ranking, nonparametric statistics, combinatorics, and fine-grained complexity, especially when kğ‘˜kitalic_k is a small fixed constant.Recent advances have significantly improved our understanding of counting and detecting patterns. Guillemot and Marx [2014] demonstrated that the detection variant is solvable in Oâ¢(n)ğ‘‚ğ‘›O(n)italic_O ( italic_n ) time for any fixed kğ‘˜kitalic_k. Their proof has laid the foundations for the discovery of the twin-width, a concept that has notably advanced parameterized complexity in recent years. Counting, in contrast, is harder: it has a conditional lower bound of nÎ©â¢(k/logâ¡k)superscriptğ‘›Î©ğ‘˜ğ‘˜n^{\Omega(k/\log k)}italic_n start_POSTSUPERSCRIPT roman_Î© ( italic_k / roman_log italic_k ) end_POSTSUPERSCRIPT [Berendsohn, Kozma, and Marx 2019] and is expected to be polynomially harder than detection as early as k=4ğ‘˜4k=4italic_k = 4, given its equivalence to counting 4444-cycles in graphs [Dudek and Gawrychowski, 2020].In this work, we design a deterministic near-linear time (1+Îµ)1ğœ€(1+\varepsilon)( 1 + italic_Îµ )-approximation algorithm for counting Ïƒğœ\sigmaitalic_Ïƒ-copies in fğ‘“fitalic_f for all kâ‰¤5ğ‘˜5k\leq 5italic_k â‰¤ 5. Combined with the conditional lower bound for k=4ğ‘˜4k=4italic_k = 4, this establishes the first known separation between approximate and exact algorithms for pattern counting. Interestingly, our algorithm leverages the BirgÃ© decomposition â€“ a sublinear tool for monotone distributions widely used in distribution testing â€“ which, to our knowledge, has not been applied in a pattern counting context before.","Detecting and counting structural patterns in a data sequence is a common algorithmic challenge in various theoretical and applied domains. Some of the numerous application domains include ranking and recommendation [DKNS01], time series analysis [BP02], and computational biology [FDRM09], among many others. On the mathematical/theoretical side, problems involving sequential pattern analysis naturally arise, e.g., in algebraic geometry [AB16], combinatorics [CDN23, GrÃ¼23], and nonparametric statistics [EZL21]. Formally, we are interested here in finding order patterns or permutation patterns, defined as follows. Given a real-valued sequence f:[n]â†’â„:ğ‘“â†’delimited-[]ğ‘›â„f\colon[n]\to{\mathbb{R}}italic_f : [ italic_n ] â†’ blackboard_R and a permutation pattern Ïƒ:[k]â†’[k]:ğœâ†’delimited-[]ğ‘˜delimited-[]ğ‘˜\sigma\colon[k]\to[k]italic_Ïƒ : [ italic_k ] â†’ [ italic_k ], a copy of the pattern Ïƒğœ\sigmaitalic_Ïƒ in the sequence fğ‘“fitalic_f is any subset of kğ‘˜kitalic_k indices i1<i2<â€¦<iksubscriptğ‘–1subscriptğ‘–2â€¦subscriptğ‘–ğ‘˜i_{1}<i_{2}<\ldots<i_{k}italic_i start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT < italic_i start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT < â€¦ < italic_i start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT so that for j,â„“âˆˆ[k]ğ‘—â„“delimited-[]ğ‘˜j,\ell\in[k]italic_j , roman_â„“ âˆˆ [ italic_k ], fâ¢(ij)<fâ¢(iâ„“)ğ‘“subscriptğ‘–ğ‘—ğ‘“subscriptğ‘–â„“f(i_{j})<f(i_{\ell})italic_f ( italic_i start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) < italic_f ( italic_i start_POSTSUBSCRIPT roman_â„“ end_POSTSUBSCRIPT ) if and only if Ïƒâ¢(j)<Ïƒâ¢(â„“)ğœğ‘—ğœâ„“\sigma(j)<\sigma(\ell)italic_Ïƒ ( italic_j ) < italic_Ïƒ ( roman_â„“ ); see Figure 1. xğ‘¥xitalic_xfâ¢(x)ğ‘“ğ‘¥f(x)italic_f ( italic_x ) Figure 1: A configuration of nğ‘›nitalic_n points in two dimensions (with no two points sharing the same xğ‘¥xitalic_x coordinate), represented as a function f:[n]â†’â„:ğ‘“â†’delimited-[]ğ‘›â„f\colon[n]\to{\mathbb{R}}italic_f : [ italic_n ] â†’ blackboard_R. The four full points form a copy of the permutation pattern 1432143214321432. In the permutation pattern matching (PPM) problem,111We shall interchangeably use the terms â€œpattern matchingâ€ and â€œpattern detectionâ€ to refer to this problem. the task is to determine whether fğ‘“fitalic_f contains at least one copy of the pattern Ïƒğœ\sigmaitalic_Ïƒ. In the counting variant, the goal is to return the exact or approximate number of Ïƒğœ\sigmaitalic_Ïƒ-copies in fğ‘“fitalic_f. Recent years have seen several breakthroughs in both detection and counting, revealing important implications in parameterized and fine-grained complexity. Of most importance is the case where kğ‘˜kitalic_k is a small constant, which has a large number of diverse applications and interesting connections: â€¢ Counting inversions, which are 21212121-patterns, that is, k=2ğ‘˜2k=2italic_k = 2, is of fundamental importance for ranking applications [DKNS01]. It has thus attracted significant attention from the algorithmic community for the last several decades, for both exact counting [CP10, Die89, FS89] and approximate counting [CP10, AP98]. â€¢ Counting 4444-patterns222We henceforth use the abbreviation â€œkğ‘˜kitalic_k-patternâ€ to refer to a permutation pattern of length kğ‘˜kitalic_k. is equivalent, by a bidirectional reduction, to counting 4444-cycles in sparse graphs. The latter is a fundamental problem in algorithmic graph theory (e.g., [AYZ97, DKS17]) and fine-grained complexity (e.g., [WWWY15, ABKZ22, ABF23, JX23]). This equivalence was shown by Dudek and Gawrychowski [DG20]. â€¢ Pattern counting for fixed kğ‘˜kitalic_k (especially kâ‰¤5ğ‘˜5k\leq 5italic_k â‰¤ 5) has deep and intricate connections to (bivariate) independece testing, a fundamental question in nonparametric statistics that asks the following. Given nğ‘›nitalic_n pairs of samples (x1,y1),â€¦,(xn,yn)subscriptğ‘¥1subscriptğ‘¦1â€¦subscriptğ‘¥ğ‘›subscriptğ‘¦ğ‘›(x_{1},y_{1}),\ldots,(x_{n},y_{n})( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , â€¦ , ( italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) from two real continuous random variables Xğ‘‹Xitalic_X and Yğ‘ŒYitalic_Y, should we deduce that Xğ‘‹Xitalic_X and Yğ‘ŒYitalic_Y are independent? This question has seen a long line of work in nonparametric statistics (e.g., [EZ20, BD14, Yan70, Cha21, BKR61]). A line of work that started by Hoeffding in the 1940â€™s [Hoe48] and is still very active to this day establishes distribution-free methods to test independence by (i) ordering the sample pairs according to the values of the xisubscriptğ‘¥ğ‘–x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTâ€™s, effectively treating the yisubscriptğ‘¦ğ‘–y_{i}italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTâ€™s as a length-nğ‘›nitalic_n sequence; and (ii) deciding whether Xğ‘‹Xitalic_X and Yğ‘ŒYitalic_Y are independent based on the kğ‘˜kitalic_k-profile of yisubscriptğ‘¦ğ‘–y_{i}italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTâ€™s, for kâ‰¤5ğ‘˜5k\leq 5italic_k â‰¤ 5. This is a special case of the much broader notion of Uğ‘ˆUitalic_U-statistics [Lee90, KB94]. See [EZ20, GrÃ¼23] for more details on this fascinating connection. â€¢ A family of length-nğ‘›nitalic_n permutations is considered quasirandom if, roughly speaking, the number of occurrences of every pattern in the family (of any length) is asymptotically similar to that of a random permutation. Quasirandomness turns out to be quite closely related to independence testing, discussed above, and it is known that the counts of patterns of length up to four suffice to determine quasirandomness, see, e.g., [CDN23, GrÃ¼23]. â€¢ Permutation pattern matching allows one to deduce whether an input fğ‘“fitalic_f is free from some pattern Ïƒğœ\sigmaitalic_Ïƒ, and consequently run much faster algorithms tailored to Ïƒğœ\sigmaitalic_Ïƒ-free instances. Indeed, many classical optimization tasks, such as binary search trees, kğ‘˜kitalic_k-server, and Euclidean TSP [BKO24] become much faster on Ïƒğœ\sigmaitalic_Ïƒ-free inputs. For example, a recent fascintating result by Opler [Opl24] shows that sorting can be done in linear time in pattern-avoiding sequences. Pattern matching itself sometimes also becomes faster in classes of Ïƒğœ\sigmaitalic_Ïƒ-free permutations [JK17, JOP21, BBL98]. Consequently, there has been a long line of computational work on pattern matching and counting, e.g., [BL12, BD14, JK17, BKM21, EZ20, JOP21, Cha21, GR22]. Here, we focus on the most relevant results in the constant kğ‘˜kitalic_k case. Notably, the version of the problem where kğ‘˜kitalic_k is large (linear in nğ‘›nitalic_n) is NP-hard [BBL98]. Both matching and counting admit a trivial algorithm with running time Oâ¢(kâ¢nk)ğ‘‚ğ‘˜superscriptğ‘›ğ‘˜O(kn^{k})italic_O ( italic_k italic_n start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ): the idea is to enumerate over all kğ‘˜kitalic_k-tuples of indices in fğ‘“fitalic_f, and check if each such tuple in fğ‘“fitalic_f induces a copy of the pattern. But can these algorithmic tasks be solved in time substantially smaller than nksuperscriptğ‘›ğ‘˜n^{k}italic_n start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT? Pattern matching: a linear-time algorithm, and the twin-width connection. In the matching case, the answer is resoundingly positive. The seminal work of Guillemot and Marx [GM14] shows that PPM is a fixed parameter tractable (FPT) problem that takes Oâ¢(n)ğ‘‚ğ‘›O(n)italic_O ( italic_n ) time for fixed kğ‘˜kitalic_k.333Unless mentioned otherwise, the computational model is Word RAM, that allows querying a single function value or comparing two values in constant time. Their running time is of the form 2Oâ¢(k2â¢logâ¡k)â‹…nâ‹…superscript2ğ‘‚superscriptğ‘˜2ğ‘˜ğ‘›2^{O(k^{2}\log k)}\cdot n2 start_POSTSUPERSCRIPT italic_O ( italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT roman_log italic_k ) end_POSTSUPERSCRIPT â‹… italic_n; the bound was slightly improved by Fox to 2Oâ¢(k2)â‹…nâ‹…superscript2ğ‘‚superscriptğ‘˜2ğ‘›2^{O(k^{2})}\cdot n2 start_POSTSUPERSCRIPT italic_O ( italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_POSTSUPERSCRIPT â‹… italic_n [Fox13]. The technical argument of [GM14] relies on two main ingredients: the first is the celebrated result of Marcus and Tardos [MT04] in their proof of the Stanley-Wilf conjecture [FH92, Kla00], while the second is a novel width notion for permutations suggested in their work. The latter subsequently led to the development of the very wide and useful notion of twin-width, which has revolutionized parametrized complexity in recent years. Indeed, the work of Bonnet, Kim, ThomassÃ©, and Watrigant [BKTW21], which originally defined twin-width, begins with the following statement: â€œInspired by a width invariant defined on permutations by Guillemot and Marx [GM14], we introduce the notion of twin-width on graphs and on matrices.â€ Pattern counting: algorithms and hardness. Exact counting, meanwhile, is unlikely to admit very efficient algorithms. A series of works from the last two decades has gradually improved the nksuperscriptğ‘›ğ‘˜n^{k}italic_n start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT upper bound, obtaining bounds of the form n(c+oâ¢(1))â¢ksuperscriptğ‘›ğ‘ğ‘œ1ğ‘˜n^{(c+o(1))k}italic_n start_POSTSUPERSCRIPT ( italic_c + italic_o ( 1 ) ) italic_k end_POSTSUPERSCRIPT for constant c<1ğ‘1c<1italic_c < 1 [AAAH01, AR08]. The current state of the art, proved by Bernedsohn, Kozma, and Marx [BKM21] is of the form nk/4+oâ¢(k)superscriptğ‘›ğ‘˜4ğ‘œğ‘˜n^{k/4+o(k)}italic_n start_POSTSUPERSCRIPT italic_k / 4 + italic_o ( italic_k ) end_POSTSUPERSCRIPT. The same work shows, however, that noâ¢(k/logâ¡k)superscriptğ‘›ğ‘œğ‘˜ğ‘˜n^{o(k/\log k)}italic_n start_POSTSUPERSCRIPT italic_o ( italic_k / roman_log italic_k ) end_POSTSUPERSCRIPT-time algorithms for exact counting cannot exist unless the exponential-time hypothesis (ETH) is false. The above results treat kğ‘˜kitalic_k as a variable; we next focus on the case where kğ‘˜kitalic_k is very small, given the myriad of applications discussed before. In the case k=2ğ‘˜2k=2italic_k = 2, it is easy to obtain an exact counting algorithm in time Oâ¢(nâ¢logâ¡n)ğ‘‚ğ‘›ğ‘›O(n\log n)italic_O ( italic_n roman_log italic_n ) (in the Word RAM model), via a variant of merge sort. A line of work [Die89, FS89, AP98, CP10] sought to obtain improved algorithms for both exact and approximate counting (to within a 1+Ïµ1italic-Ïµ1+\epsilon1 + italic_Ïµ multiplicative factor).444Formally, a (1+Ïµ)1italic-Ïµ(1+\epsilon)( 1 + italic_Ïµ )-approximate counting algorithm is required, given access to a pattern Ïƒğœ\sigmaitalic_Ïƒ and a function fğ‘“fitalic_f, to return a value between X/(1+Îµ)ğ‘‹1ğœ€X/(1+\varepsilon)italic_X / ( 1 + italic_Îµ ) and (1+Îµ)â¢X1ğœ€ğ‘‹(1+\varepsilon)X( 1 + italic_Îµ ) italic_X, where Xğ‘‹Xitalic_X is the number of Ïƒğœ\sigmaitalic_Ïƒ-copies in fğ‘“fitalic_f. The best known exact and approximate upper bounds for k=2ğ‘˜2k=2italic_k = 2 are Oâ¢(nâ¢logâ¡n)ğ‘‚ğ‘›ğ‘›O(n\sqrt{\log n})italic_O ( italic_n square-root start_ARG roman_log italic_n end_ARG ) and Oâ¢(n)ğ‘‚ğ‘›O(n)italic_O ( italic_n ), respectively, both proved by Chan and PÄƒtraÅŸcu [CP10]. The cases of k=3ğ‘˜3k=3italic_k = 3 and k=4ğ‘˜4k=4italic_k = 4 have been the subject of multiple recent works. Even-Zohar and Leng [EZL21] developed an object called corner tree to count a family of patterns (that slightly differ from permutation patterns) in near-linear time. Using linear combinations of corner tree formulas, they obtained near-linear time algorithm for all patterns of length 3333 and some (8 out of 24) length-4444 patterns. For the remaining ones of length 4444, the same work obtains an Oâ¢(n3/2)ğ‘‚superscriptğ‘›32O(n^{3/2})italic_O ( italic_n start_POSTSUPERSCRIPT 3 / 2 end_POSTSUPERSCRIPT ) time algorithm using different techniques. This interesting dichotomy between â€œeasyâ€ and â€œhardâ€ 4444-patterns raises an interesting question: is the dichotomy an artifact of the specific technique, or is there an inherent computational barrier? Dudek and Gawrychowski [DG20] proved that the latter is true: exact counting of any â€œhardâ€ 4444-pattern is equivalent (via bidirectional reductions) to exact counting of 4444-cycles in graphs, a central and very well studied problem in algorithmic graph theory. The concrete equivalence stated in their paper (see Theorem 1 there) is that an O~â¢(mÎ³)~ğ‘‚superscriptğ‘šğ›¾\tilde{O}(m^{\gamma})over~ start_ARG italic_O end_ARG ( italic_m start_POSTSUPERSCRIPT italic_Î³ end_POSTSUPERSCRIPT )-time algorithm for counting 4444-cycles in mğ‘šmitalic_m-edge graphs implies an O~â¢(nÎ³)~ğ‘‚superscriptğ‘›ğ›¾\tilde{O}(n^{\gamma})over~ start_ARG italic_O end_ARG ( italic_n start_POSTSUPERSCRIPT italic_Î³ end_POSTSUPERSCRIPT ) time algorithm for counting â€œhardâ€ 4-patterns, and vice versa. While this has led to a slightly improved Oâ¢(n1.48)ğ‘‚superscriptğ‘›1.48O(n^{1.48})italic_O ( italic_n start_POSTSUPERSCRIPT 1.48 end_POSTSUPERSCRIPT ) upper bound based on best known results for counting 4444-cycles in sparse graphs [WWWY15], the more interesting direction to us is the lower bound side. A line of recent works obtains conditional lower bounds on 4444-cycle counting, that apply already for the easier task of 4444-cycle detection [ABKZ22, ABF23, JX23]. These works imply that conditioning on the Strong 3-SUM conjecture, detecting whether a (sufficiently sparse) graph with mğ‘šmitalic_m edges contains a 4444-cycle requires m1+Î©â¢(1)superscriptğ‘š1Î©1m^{1+\Omega(1)}italic_m start_POSTSUPERSCRIPT 1 + roman_Î© ( 1 ) end_POSTSUPERSCRIPT time (see, e.g., the discussion after Theorem 1.14 in [JX23]), which translates to an n1+Î©â¢(1)superscriptğ‘›1Î©1n^{1+\Omega(1)}italic_n start_POSTSUPERSCRIPT 1 + roman_Î© ( 1 ) end_POSTSUPERSCRIPT lower bound for exact counting 4444-patterns, via [DG20]. 1.1 Our results Given the separation between the Oâ¢(n)ğ‘‚ğ‘›O(n)italic_O ( italic_n ) complexity of pattern detection and the n1+Î©â¢(1)superscriptğ‘›1Î©1n^{1+\Omega(1)}italic_n start_POSTSUPERSCRIPT 1 + roman_Î© ( 1 ) end_POSTSUPERSCRIPT conditional lower bound for pattern counting already for k=4ğ‘˜4k=4italic_k = 4, and the importance of counting in the constant-kğ‘˜kitalic_k regime, we ask whether approximate counting can be performed in time substantially (polynomially) faster than exact counting. What is the computational landscape of (1+Îµ)1ğœ€(1+\varepsilon)( 1 + italic_Îµ )-approximate counting of kğ‘˜kitalic_k-patterns, for small fixed kğ‘˜kitalic_k, as compared to exact counting and matching? Is approximate counting much faster than exact counting? The only case where the best known (1+Îµ)1ğœ€(1+\varepsilon)( 1 + italic_Îµ )-approximate algorithm is faster than the best known exact algorithm is when k=2ğ‘˜2k=2italic_k = 2 [CP10], but the gap is only of order logâ¡nğ‘›\sqrt{\log n}square-root start_ARG roman_log italic_n end_ARG (i.e., between Oâ¢(n)ğ‘‚ğ‘›O(n)italic_O ( italic_n ) and Oâ¢(nâ¢logâ¡n)ğ‘‚ğ‘›ğ‘›O(n\sqrt{\log n})italic_O ( italic_n square-root start_ARG roman_log italic_n end_ARG )), and no nontrivial exact counting lower bounds are known. Thus, it remains unknown whether exact counting is harder than approximate counting even for k=2ğ‘˜2k=2italic_k = 2, and even if it is, the gap would be of lower order. Our main contribution, stated below, is a near-linear time approximate counting algorithm for kâ‰¤5ğ‘˜5k\leq 5italic_k â‰¤ 5. Theorem 1.1. For every permutation pattern Ïƒğœ\sigmaitalic_Ïƒ of length kâ‰¤5ğ‘˜5k\leq 5italic_k â‰¤ 5 and every Îµ>0ğœ€0\varepsilon>0italic_Îµ > 0, the following holds. There exists a deterministic algorithm that, given access to a function f:[n]â†’â„:ğ‘“â†’delimited-[]ğ‘›â„f\colon[n]\to{\mathbb{R}}italic_f : [ italic_n ] â†’ blackboard_R, returns the number of Ïƒğœ\sigmaitalic_Ïƒ-copies in fğ‘“fitalic_f, up to a multiplicative error of 1+Îµ1ğœ€1+\varepsilon1 + italic_Îµ, in time nâ‹…(Îµâˆ’1â¢logâ¡n)Oâ¢(1)â‹…ğ‘›superscriptsuperscriptğœ€1ğ‘›ğ‘‚1n\cdot\left(\varepsilon^{-1}\log n\right)^{O(1)}italic_n â‹… ( italic_Îµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log italic_n ) start_POSTSUPERSCRIPT italic_O ( 1 ) end_POSTSUPERSCRIPT. Combined with the n1+Î©â¢(1)superscriptğ‘›1Î©1n^{1+{\Omega(1)}}italic_n start_POSTSUPERSCRIPT 1 + roman_Î© ( 1 ) end_POSTSUPERSCRIPT lower bound for counting â€œhardâ€ 4444-patterns (e.g., 2413241324132413), our result implies a polynomial separation between exact and (1+Îµ)1ğœ€(1+\varepsilon)( 1 + italic_Îµ )-approximate algorithms for 4-patterns and 5-patterns. The paper includes a full, self-contained proof for k=4ğ‘˜4k=4italic_k = 4; recall that for kâ‰¤3ğ‘˜3k\leq 3italic_k â‰¤ 3, even exact counting algorithms have near-linear time complexity [EZL21]. For k=5ğ‘˜5k=5italic_k = 5, our proof is computer-assisted: the algorithm enumerates over multiple parameter choices and techniques, heavily depending on the pattern structure. Verifying that the algorithm works for all patterns requires a tedious case analysis for k=5ğ‘˜5k=5italic_k = 5, involving 512 cases, each of which is straightforward to verify based on the output from our code. In this paper, we describe the set of techniques used, establish how they can be combined, provide examples of typical use cases, and delegate the full enumeration to the software. The source code for the enumeration and the full output (including for k=5ğ‘˜5k=5italic_k = 5) are provided here: https://github.com/omribene/approx-counting. Our proof can be immediately adapted to provide an algorithm for enumerating (or listing) copies of the pattern. In the enumeration problem, we are given f,Ïƒğ‘“ğœf,\sigmaitalic_f , italic_Ïƒ, and an integer tğ‘¡titalic_t, and are required to provide a list of tğ‘¡titalic_t copies of Ïƒğœ\sigmaitalic_Ïƒ in fğ‘“fitalic_f (or the full list if there are less than tğ‘¡titalic_t copies). We obtain the following result. Theorem 1.2. For every permutation pattern Ïƒğœ\sigmaitalic_Ïƒ of length kâ‰¤5ğ‘˜5k\leq 5italic_k â‰¤ 5 and every tâˆˆâ„•ğ‘¡â„•t\in{\mathbb{N}}italic_t âˆˆ blackboard_N, the following holds. There exists a deterministic algorithm that, given access to a function f:[n]â†’â„:ğ‘“â†’delimited-[]ğ‘›â„f\colon[n]\to{\mathbb{R}}italic_f : [ italic_n ] â†’ blackboard_R, returns a list of tğ‘¡titalic_t copies of Ïƒğœ\sigmaitalic_Ïƒ in fğ‘“fitalic_f (or all such copies, if there are fewer than tğ‘¡titalic_t), in time (n+t)â‹…logOâ¢(1)â¡nâ‹…ğ‘›ğ‘¡superscriptğ‘‚1ğ‘›(n+t)\cdot\log^{O(1)}n( italic_n + italic_t ) â‹… roman_log start_POSTSUPERSCRIPT italic_O ( 1 ) end_POSTSUPERSCRIPT italic_n. Our results further highlight the contrasting behavior between 4444-cycles in sparse graphs and 4444-patterns in sequences. The exact counting complexities for these objects are equal, due to the linear-size bidirectional reductions between these problems [DG20]. Meanwhile, for detection we have a separation between the Oâ¢(n)ğ‘‚ğ‘›O(n)italic_O ( italic_n ) algorithm for patterns [GM14] and the n1+Î©â¢(1)superscriptğ‘›1Î©1n^{1+\Omega(1)}italic_n start_POSTSUPERSCRIPT 1 + roman_Î© ( 1 ) end_POSTSUPERSCRIPT conditional lower bound for cycles in sparse graphs [ABKZ22, ABF23, JX23]. Since the same lower bound also applies to approximate counting of 4444-cycles, this implies a separation for approximate counting. Finally, for enumeration the lower bounds of [ABF23, JX23] are stronger (and in fact tight), of order Î©â¢(minâ¡{n2âˆ’oâ¢(1),m4/3âˆ’oâ¢(1)})Î©superscriptğ‘›2ğ‘œ1superscriptğ‘š43ğ‘œ1\Omega(\min\{n^{2-o(1)},m^{4/3-o(1)}\})roman_Î© ( roman_min { italic_n start_POSTSUPERSCRIPT 2 - italic_o ( 1 ) end_POSTSUPERSCRIPT , italic_m start_POSTSUPERSCRIPT 4 / 3 - italic_o ( 1 ) end_POSTSUPERSCRIPT } ), conditioning on the 3-SUM conjecture. Again, since enumeration of pattern detection is near-linear in nğ‘›nitalic_n and tğ‘¡titalic_t, we get a separation here for sufficiently small values of tğ‘¡titalic_t. 1.2 Our techniques Our approach to approximate pattern counting is based on a novel application of a known tool in distribution testing, and on several new techniques. Each of these techniques contributes to efficient approximate counting for small fixed patterns. Here, we outline three main ideas central to our work: (i) the BirgÃ© technique for exploiting structural monotonicity; (ii) using separators to impose additional structure on pattern instances; and (iii) a specialized data structure for approximating the counts of 12121212 copies within axis-parallel rectangles.555Throughout our work, we assume the input is a permutation. Nevertheless, our proofs also handle inputs/functions that contain points with the same yğ‘¦yitalic_y-coordinate, i.e., the proofs tolerate fâ¢(i)=fâ¢(j)ğ‘“ğ‘–ğ‘“ğ‘—f(i)=f(j)italic_f ( italic_i ) = italic_f ( italic_j ) for iâ‰ jğ‘–ğ‘—i\neq jitalic_i â‰  italic_j. Also, without loss of generality, for the problem of counting patterns, it can be assumed that fâ¢(i)âˆˆ{0,1,â€¦,n}ğ‘“ğ‘–01â€¦ğ‘›f(i)\in\{0,1,\ldots,n\}italic_f ( italic_i ) âˆˆ { 0 , 1 , â€¦ , italic_n }. 1.2.1 Leveraging the BirgÃ© decomposition for monotonicity-based counting (Section 3) Our proof makes crucial use of the BirgÃ© approximation method. This is a simple method to approximate monotone distributions using a step function with few steps. It was developed by Lucien BirgÃ© in the 1980â€™s [Bir87] and popularized in a number of distribution testing works, e.g., [DDS+13, DDS14] and the survey [Can20] (see Section 2.1.2 for more details). In our context, this method implies the following: to approximate the sum of a (weakly) monotone sequence x1â‰¥x2â‰¥â€¦â‰¥xnsubscriptğ‘¥1subscriptğ‘¥2â€¦subscriptğ‘¥ğ‘›x_{1}\geq x_{2}\geq\ldots\geq x_{n}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT â‰¥ italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT â‰¥ â€¦ â‰¥ italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT to within a 1+Îµ1ğœ€1+\varepsilon1 + italic_Îµ multiplicative factor, one only needs to query a sublinear number, Oâ¢(Îµâˆ’1â¢logâ¡n)ğ‘‚superscriptğœ€1ğ‘›O(\varepsilon^{-1}\log n)italic_O ( italic_Îµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log italic_n ), of the elements in the sequence. We illustrate the idea of using the BirgÃ© decomposition in counting 4444-patterns. Our approach to approximating the count of patterns like 1324132413241324 starts by fixing a value of the â€œ3â€. Specifically, we divide the set of all 1324132413241324 copies in the permutation based on the position of â€œ3â€, creating subsets C1,C2,â€¦,Cnsubscriptğ¶1subscriptğ¶2â€¦subscriptğ¶ğ‘›C_{1},C_{2},\ldots,C_{n}italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_C start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT where each Cisubscriptğ¶ğ‘–C_{i}italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT â€œ3â€ appears at the iğ‘–iitalic_i-th location. Once â€œ3â€ is fixed to a certain position, we look at the possible positions for â€œ4â€. Fixing â€œ4â€ further organizes Cisubscriptğ¶ğ‘–C_{i}italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT into smaller groups based on the placement of â€œ4â€ relative to â€œ3â€. One such scenario is illustrated in Figure 2. Figure 2: The illustration corresponds to permutation Ï€=136548279ğœ‹136548279\pi=136548279italic_Ï€ = 136548279, depicted in a plane at points (i,Ï€i)ğ‘–subscriptğœ‹ğ‘–(i,\pi_{i})( italic_i , italic_Ï€ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ). The key insight is that each position of â€œ4â€ constrains the remaining elements of the 1324132413241324 patterns in a monotone way. For example, after fixing â€œ3â€ to a specific position in the permutation, we can identify all positions of â€œ4â€ that can extend this configuration into valid 1324132413241324 copies. Within this subset, the positions of â€œ4â€ exhibit a specific ordering: if â€œ4â€ appears at a given position in the sequence, any more-to-the-right occurrence of â€œ4â€ will continue to yield valid 1324132413241324 copies! Similarly, we fix â€œ2â€ and then count the relevant candidates for â€œ1â€. In Section 3, we show that fixing â€œ2â€ also exhibits a certain monotonicity. We use the BirgÃ© decomposition to take advantage of this structure. The decomposition allows us to break down each subset Cisubscriptğ¶ğ‘–C_{i}italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT into manageable, monotone classes and then efficiently approximate the count of each class in polylogarithmic time. By structuring the count around this monotonicity, we can approximately compute each |Ci|subscriptğ¶ğ‘–|C_{i}|| italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | without directly enumerating all possibilities, which would be computationally expensive. So, by fixing values like â€œ3â€, then â€œ4â€, and then â€œ2â€, and using the BirgÃ© decomposition to handle the emerging monotonic structures, we reduce the complexity of counting 1324132413241324 patterns to a series of fast approximations, leading to Oâ¢(nâ‹…polyâ¢(n,Îµâˆ’1))ğ‘‚â‹…ğ‘›polyğ‘›superscriptğœ€1O(n\cdot\mathrm{poly}(n,\varepsilon^{-1}))italic_O ( italic_n â‹… roman_poly ( italic_n , italic_Îµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) ) running time. 1.2.2 Imposing structure through separators for 4444-patterns (Section 4) While the BirgÃ© decomposition effectively handles some patterns, others (such as 2413241324132413) do not exhibit the same straightforward monotonic structure. For these patterns, we introduce separators to impose additional structural constraints. Consider the 4444-pattern 2413241324132413. Unlike 1324132413241324, this pattern does not naturally exhibit a straightforward monotonic structure. If we fix â€œ4â€ to a particular position, we would ideally like the positions of other elements â€“ â€œ2â€, â€œ1â€, and â€œ3â€ â€“ to show some consistent ordering so that we can apply an efficient counting method. However, without further structuring, the placements of â€œ1â€ and â€œ3â€ relative to â€œ4â€ do not seem to reveal any particular order. Figure 3: An illustration of the idea of using separators to split the candidates for â€œ1â€ and â€œ3â€ into disjoint but neighboring regions based on their position. To handle this, we introduce a separator to divide the possible positions of elements in 2413241324132413 based on their relative positions to â€œ4â€. For instance, after fixing â€œ4â€, we introduce a position-based separator sğ‘ sitalic_s that splits the plane into two regions. We then require that â€œ1â€ appears to the left of sğ‘ sitalic_s while â€œ3â€ appears to the right of sğ‘ sitalic_s. This allows us to approximate the count of 2413241324132413 copies within each configuration independently. We illustrate such a separator in Figure 3. With this separator in place, the counts of 2413241324132413 copies become monotone again, enabling us to apply the BirgÃ© decomposition to each subset created by the separator. The complete analysis is presented in Section 4. 1.2.3 Global separators for 5555-patterns (Section 5) When extending our approach to 5555-patterns, we introduce an enhanced separator structure, which we refer to as global separators. This structure is specifically designed for handling the additional complexity that arises when counting 5-patterns, such as 24135241352413524135. These separators are easiest to describe using the language of two-dimensional segment trees. Consider a two-dimensional segment tree Sğ‘†Sitalic_S built over the plane. The outer segment tree divides the space along the xğ‘¥xitalic_x-axis, while each vertex in this tree contains an inner segment tree that further partitions the range along the yğ‘¦yitalic_y-axis. For each vertex vğ‘£vitalic_v in the outer segment tree, we want to count all copies of a given 5555-pattern, e.g., 24135241352413524135, that exist within vğ‘£vitalic_v but do not appear in any of its child vertices. This setup naturally leads to the concept of vertical separators. Given that vğ‘£vitalic_v corresponds to an interval [a,b]ğ‘ğ‘[a,b][ italic_a , italic_b ] along the xğ‘¥xitalic_x-axis, we define a vertical separator at the midpoint (a+b)/2ğ‘ğ‘2(a+b)/2( italic_a + italic_b ) / 2. Any copy that spans both sides of this vertical separator is counted within vğ‘£vitalic_v but not in any of vğ‘£vitalic_vâ€™s children. Figure 4: This sketch depicts the notion of vertical and horizontal global separators. In this example, the vertical dashed (blue) line is a vertical separator, splitting the range [a,b]ğ‘ğ‘[a,b][ italic_a , italic_b ] into two equal-sized halves. The horizontal dashed (red) line is a horizontal separator. The example also shows a (24135)24135(24135)( 24135 ) copy. This copy is counted only if (i) the â€œ2â€ is to the left and the â€œ5â€ is to the right of the vertical separator, and, (ii) if the â€œ1â€ is below and the â€œ5â€ is above the horizontal separator. In addition to vertical separators, we introduce horizontal separators that further partition each vğ‘£vitalic_v based on the yğ‘¦yitalic_y-axis. This second layer of separation divides the region into four distinct quadrants. We refer to Figure 8 for an illustration. In addition, we consider all valid configurations of 24135241352413524135 copies relative to these quadrants. For instance, we can enforce that specific elements (e.g., â€œ2â€ and â€œ5â€) fall on opposite sides of the vertical separator and that others (e.g., â€œ1â€ and â€œ5â€) fall on opposite sides of the horizontal separator. This structure ensures that each copy of the pattern is counted exactly once within a unique configuration. Crucially, it turns out that this structure also induces monotonicity and allows for using the BirgÃ© decomposition for efficient approximate counting. 1.2.4 A Primitive for Counting 12121212 Copies within Axis-Parallel Rectangles (Section 5.1) Our final technique introduces a data structure for counting simple 12121212 patterns (increasing pairs) within arbitrary axis-aligned rectangles. This primitive allows us to query the approximate number of 12121212 copies within any subregion of the input permutation. We employ this data structure to count 5555-patterns. To develop this 12121212-copy counting data structure, we employ a two-dimensional segment tree described in the previous subsection. With this tree, we pre-process the points in a bottom-up manner in Oâ¢(nâ‹…polyâ¢(logâ¡n,Îµâˆ’1))ğ‘‚â‹…ğ‘›polyğ‘›superscriptğœ€1O(n\cdot\mathrm{poly}(\log n,\varepsilon^{-1}))italic_O ( italic_n â‹… roman_poly ( roman_log italic_n , italic_Îµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) ) time. Section 5.1 details the implementation of this bottom-up pre-processing. This pre-processing computes an approximate number of 12121212 copies within each vertex of the segment tree. These pre-computed values are later used to answer queries for approximating the number of 12121212 copies within arbitrary rectangles, each answered in polylogarithmic time. 1.3 Open problems Our results and techniques open several interesting follow-up questions, both as the first approximate counting results for k>2ğ‘˜2k>2italic_k > 2 and due to the novel use of the BirgÃ© decomposition. The main open question is on the complexity of approximate counting for general (small) kğ‘˜kitalic_k. As discussed, there are complexity separations between detection and exact counting of permutation patterns: detection takes Oâ¢(n)ğ‘‚ğ‘›O(n)italic_O ( italic_n ) time for any fixed length kğ‘˜kitalic_k, while exact counting requires n1+Î©â¢(1)superscriptğ‘›1Î©1n^{1+\Omega(1)}italic_n start_POSTSUPERSCRIPT 1 + roman_Î© ( 1 ) end_POSTSUPERSCRIPT time for k=4ğ‘˜4k=4italic_k = 4, assuming Strong 3-SUM, and nÎ©â¢(k/logâ¡k)superscriptğ‘›Î©ğ‘˜ğ‘˜n^{\Omega(k/\log k)}italic_n start_POSTSUPERSCRIPT roman_Î© ( italic_k / roman_log italic_k ) end_POSTSUPERSCRIPT time when kğ‘˜kitalic_k is a parameter, assuming ETH. Approximate counting lies between exact counting and detection, and it is a priori unclear where its complexity sits between linear in nğ‘›nitalic_n (for detection) and nearly worst-possible (for exact counting). Question 1.3 (Complexity of approximate counting). What is the time complexity of approximating the number of Ïƒğœ\sigmaitalic_Ïƒ-patterns in an input sequence f:[n]â†’â„:ğ‘“â†’delimited-[]ğ‘›â„f\colon[n]\to{\mathbb{R}}italic_f : [ italic_n ] â†’ blackboard_R to within a (1+Ïµ)1italic-Ïµ(1+\epsilon)( 1 + italic_Ïµ )-multiplicative error, as a function of nğ‘›nitalic_n and k=|Ïƒ|ğ‘˜ğœk=|\sigma|italic_k = | italic_Ïƒ |? Establishing tight upper and lower bounds for 1.3 appears to be challenging. Even for exact pattern counting, a more extensively studied problem, there remains a gap between the best known upper bound of nk/4+oâ¢(k)superscriptğ‘›ğ‘˜4ğ‘œğ‘˜n^{k/4+o(k)}italic_n start_POSTSUPERSCRIPT italic_k / 4 + italic_o ( italic_k ) end_POSTSUPERSCRIPT and the conditional lower bound of nÎ©â¢(k/logâ¡k)superscriptğ‘›Î©ğ‘˜ğ‘˜n^{\Omega(k/\log k)}italic_n start_POSTSUPERSCRIPT roman_Î© ( italic_k / roman_log italic_k ) end_POSTSUPERSCRIPT, both attained by Berendsohn, Kozma and Marx [BKM21]. Nevertheless, given the separation we establish for k=4ğ‘˜4k=4italic_k = 4 and k=5ğ‘˜5k=5italic_k = 5 (along with the new techniques which are specially suited for approximate computation) it is tempting to conjecture that the complexity of approximate counting in the general case, as a function of nğ‘›nitalic_n and kğ‘˜kitalic_k, is fundamentally lower than that of exact counting. We make the following conjecture. Conjecture 1.4. The time complexity of approximate counting Ïƒğœ\sigmaitalic_Ïƒ-copies in a length-nğ‘›nitalic_n sequence, as a function of nğ‘›nitalic_n and k=|Ïƒ|ğ‘˜ğœk=|\sigma|italic_k = | italic_Ïƒ |, is asymptotically smaller than that of exact counting for the same parameters. Proving any bound of the form noâ¢(k/logâ¡k)superscriptğ‘›ğ‘œğ‘˜ğ‘˜n^{o(k/\log k)}italic_n start_POSTSUPERSCRIPT italic_o ( italic_k / roman_log italic_k ) end_POSTSUPERSCRIPT would affirm this conjecture. But even improving upon the state of the art for exact counting would be interesting. The current best known approach of [BKM21] formulates the pattern matching instance as a constraint satisfaction problem (CSP) with binary constraints. The complexity of solving this CSP is Oâ¢(nt+1)ğ‘‚superscriptğ‘›ğ‘¡1O(n^{t+1})italic_O ( italic_n start_POSTSUPERSCRIPT italic_t + 1 end_POSTSUPERSCRIPT ), where tğ‘¡titalic_t is the treewidth of the incidence graph of the pattern Ï€ğœ‹\piitalic_Ï€ (see also the work of Ahal and Rabinovich [AR08] for an earlier investigation of the role of treewidth in this context). The basic constraint graph has treewidth bounded by k/3+oâ¢(k)ğ‘˜3ğ‘œğ‘˜k/3+o(k)italic_k / 3 + italic_o ( italic_k ); Berendsohn et al. combine the tree-width based approach with a gridding technique based on ideas of Cygan, Kowalik, and SocaÅ‚a [CKS19] to reduce the exponent to k/4+oâ¢(k)ğ‘˜4ğ‘œğ‘˜k/4+o(k)italic_k / 4 + italic_o ( italic_k ). As we see here, algorithmic results for both detection and exact counting make use of central width notions from the parametrized complexity literature: the former gave rise to twin-width [GM14, BKTW21] and the latter makes heavy use of tree-width [AR08, BKM21]. It would be very intriguing to explore what role such width notions may play in the approximate version of pattern counting. The fact that approximate counting (in the small kğ‘˜kitalic_k case) admits techniques that go beyond the exact case may suggest that either a complexity notion other than tree-width is at play here, or we can use the new techniques to bound the tree-width of an easier subproblem (with more of the values constrained due to the use of, say, substructure monotonicity and BirgÃ© approximation). From the lower bound side, essentially no nontrivial (superlinear) results are known for the Word RAM model, and proving any Ï‰â¢(n)ğœ”ğ‘›\omega(n)italic_Ï‰ ( italic_n ) lower bound that applies to the approximate counting of some fixed-length patterns would be interesting. We further conjecture that for large enough (constant) kğ‘˜kitalic_k, there should be a strongly superlinear bound. Conjecture 1.5. There exists a pattern Ïƒğœ\sigmaitalic_Ïƒ of constant length for which approximate counting of Ïƒğœ\sigmaitalic_Ïƒ in length-nğ‘›nitalic_n sequences requires n1+Î©â¢(1)superscriptğ‘›1Î©1n^{1+\Omega(1)}italic_n start_POSTSUPERSCRIPT 1 + roman_Î© ( 1 ) end_POSTSUPERSCRIPT time. For k=3,4,5ğ‘˜345k=3,4,5italic_k = 3 , 4 , 5, the existing algorithms for, say, 2-approximate counting (and exact counting, for k=3ğ‘˜3k=3italic_k = 3) have time complexity nâ¢logOâ¢(1)â¡nğ‘›superscriptğ‘‚1ğ‘›n\log^{O(1)}nitalic_n roman_log start_POSTSUPERSCRIPT italic_O ( 1 ) end_POSTSUPERSCRIPT italic_n. This raises the question of whether the polylogarithmic dependence is necessary (for k=2ğ‘˜2k=2italic_k = 2 it is not necessary [CP10]). We conjecture that the answer is positive already for k=4ğ‘˜4k=4italic_k = 4. Finally, the use of BirgÃ© decomposition in this paper seems to be novel in the context of pattern counting and, perhaps more generally, in combinatorial contexts beyond the scope of distribution testing. This decomposition is very useful in our setting as many sequences of quantities turn out to be monotone. It would be interesting to find other counting problems in low-dimensional geometric settings where this technique, of finding and exploiting monotone subsequences, may be useful."
https://arxiv.org/html/2411.04979v1,Quantum speedups in solving near-symmetric optimization problems by low-depth QAOA,"We present new advances in achieving exponential quantum speedups for solving optimization problems by low-depth quantum algorithms. Specifically, we focus on families of combinatorial optimization problems that exhibit symmetry and contain planted solutions. We rigorously prove that the 1-step Quantum Approximate Optimization Algorithm (QAOA) can achieve a success probability of Î©â¢(1/n)Î©1ğ‘›\Omega(1/\sqrt{n})roman_Î© ( 1 / square-root start_ARG italic_n end_ARG ), and sometimes Î©â¢(1)Î©1\Omega(1)roman_Î© ( 1 ), for finding the exact solution in many cases. Furthermore, we construct near-symmetric optimization problems by randomly sampling the individual clauses of symmetric problems, and prove that the QAOA maintains a strong success probability in this setting even when the symmetry is broken. Finally, we construct various families of near-symmetric Max-SAT problems and benchmark state-of-the-art classical solvers, discovering instances where all known classical algorithms require exponential time. Therefore, our results indicate that low-depth QAOA could achieve an exponential quantum speedup for optimization problems.","Optimization problems are critical to a wide range of real-world applications, and efficiently solving these problems is of great practical importance across many fields in science and industry. Quantum computers hold the promise of solving certain optimization problems faster than classical algorithms, offering potential breakthroughs in speed and efficiency. However, while there is hope for quantum speedups, we currently lack strong evidence that near-term quantum computers with limited circuit depth can achieve a substantial advantage over classical methods. In this work, we provide new evidence that a low-depth quantum algorithm, the Quantum Approximate Optimization Algorithm (QAOA) [1], can solve families of near-symmetric optimization problems exponentially faster than the best known classical algorithms. The QAOA has been proposed as a general-purpose quantum optimization algorithm that can be run on near-term quantum computers, and has seen implementation across a variety of experimental platforms [2, 3, 4, 5]. In the low-depth regime, however, limitations of the QAOA has been proven for various problems [6, 7, 8, 9, 10, 11, 12]. While there has been some evidence that shows low-depth QAOA can still provide a quantum speedup for both approximate [13] and exact optimization [14], they appear to be only small polynomial speedups. In contrast, our results demonstrate that for problems possessing some level of symmetry, an exponential speedup with the QAOA is possible even in the low-depth regime. The symmetric optimization problems we consider are maximum constraint satisfaction problems (Max-CSPs) that are defined with a planted nğ‘›nitalic_n-bit string and has a cost function that exhibit certain symmetry. For example, when the symmetry is the symmetric group Snsubscriptğ‘†ğ‘›S_{n}italic_S start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT permuting all nğ‘›nitalic_n bits, the cost function depends only on the Hamming distance to the planted bit string. Similar families of problems have been previously studied in the context of quantum annealing, which may take polynomial or exponential time to find the solution depending on the problem [15, 16, 17, 18]. Although a quantum speedup relative to certain general-purpose classical algorithms may be obtained in some cases, these symmetric problems are susceptible to attacks by tailored classical algorithms that know and take advantage of the symmetry. In this work, however, we take a step further to consider the situation where the symmetry is broken by randomly sampling the clauses in the cost function, and we call these problems â€œnear-symmetric.â€ We also instantiate these problem explicitly in the form of Boolean satisfiability problems and benchmark them with state-of-the-art classical algorithms. As our main result, we prove that the 1-step QAOA can solve many symmetric and near-symmetric problem in polynomial time. Our findings are based on analytically deriving the success probability of 1-step QAOA in finding the planted solution using combinatorial calculations and a rigorous application of the saddle-point method. For example, given any Snsubscriptğ‘†ğ‘›S_{n}italic_S start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT-symmetric cost function that takes on n+1ğ‘›1n+1italic_n + 1 distinct values for the different Hamming distances, we show the 1-step QAOA has Î©â¢(1/n)Î©1ğ‘›\Omega(1/\sqrt{n})roman_Î© ( 1 / square-root start_ARG italic_n end_ARG ) success probability in finding the solution (Theorem 3.3). We also consider various problems with either Snsubscriptğ‘†ğ‘›S_{n}italic_S start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT or (Sn/2)2superscriptsubscriptğ‘†ğ‘›22(S_{n/2})^{2}( italic_S start_POSTSUBSCRIPT italic_n / 2 end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT symmetry and multiple local minima that confuse classical algorithms, and prove that 1-step QAOA succeeds in finding the global minima with Î©â¢(1)Î©1\Omega(1)roman_Î© ( 1 ) probability (Theorems 3.4 and 3.7). Furthermore, leveraging the high success probability from a low-depth QAOA, we show that the QAOA also succeed with similar probability even when the symmetry is broken by random sparsification of the cost function (Theorem 4.1). Hence, using repetitions of the 1-step QAOA, one can find the solution in polynomial time with Oâ¢(n)ğ‘‚ğ‘›O(\sqrt{n})italic_O ( square-root start_ARG italic_n end_ARG ) or Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 ) queries to the cost function for these symmetric and near-symmetric problems. On the other hand, we show that any classical algorithm requires at least Î©â¢(n/logâ¡n)Î©ğ‘›ğ‘›\Omega(n/\log n)roman_Î© ( italic_n / roman_log italic_n ) queries to the cost function to solve any Snsubscriptğ‘†ğ‘›S_{n}italic_S start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT-symmetric problem, even if the symmetry is known in advance (Claim 5.1). To explore quantum speedups, we explicitly construct instances of symmetric and near-symmetric Max-SAT problems, and study the performance of practical classical optimization algorithm such as simulated annealing and state-of-the-art classical SAT and Max-SAT solvers. In particular, we consider algorithms that are front-runners from recent SAT and Max-SAT competitions, and perform numerical experiments up to hundreds of bits to extract their run time scaling. For some instances, we show that all classical solvers known to us take exponential time to find the solution. Since one can solve these problems in polynomial time using 1-step QAOA, we have what appears to be an exponential quantum speedup over general-purpose classical algorithms by low-depth QAOA. Our results highlight the potential for significant quantum speedups even with constrained capabilities of near-term quantum devices, bringing us closer to realizing practical quantum advantages in optimization."
https://arxiv.org/html/2411.04564v1,A Generalisation of Voter Model: Influential Nodes and Convergence Properties,"Consider an undirected graph GğºGitalic_G, representing a social network, where each node is blue or red, corresponding to positive or negative opinion on a topic. In the voter model, in discrete time rounds, each node picks a neighbour uniformly at random and adopts its colour. Despite its significant popularity, this model does not capture some fundamental real-world characteristics such as the difference in the strengths of individuals connections, individuals with neutral opinion on a topic, and individuals who are reluctant to update their opinion. To address these issues, we introduce and study a generalisation of the voter model.Motivating by campaigning strategies, we study the problem of selecting a set of seeds blue nodes to maximise the expected number of blue nodes after some rounds. We prove that the problem is NP-hard and provide a polynomial time approximation algorithm with the best possible approximation guarantee. Our experiments on real-world and synthetic graph data demonstrate that the proposed algorithm outperforms other algorithms.We also investigate the convergence properties of the model. We prove that the process could take an exponential number of rounds to converge. However, if we limit ourselves to strongly connected graphs, the convergence time is polynomial and the period (the number of states in convergence) divides the length of all cycles in the graph.","Humans constantly form and update their opinions on different topics, from minor subjects such as which movie to watch and which new cafÃ© to try to major matters such as which political party to vote for and which company to invest in. In the process of making such decisions, we tend to rely not only on our own personal judgment and knowledge, but also that of others, especially those whose opinion we value and trust. As a result, opinion diffusion, influence propagation, and (mis)-information spreading can affect different aspects of our lives from economy and defense to fashion and personal affairs. Recent years have witnessed a booming development of online social networking platforms like Facebook, WeChat, and Instagram. The enormous popularity of these platforms has led to fundamental changes in how humans share and form opinions. Social phenomena such as disagreement and polarisation that have existed in human societies for millennia, are now taking place in an online virtual world and are tightly woven into everyday life, with a substantial impact on society. There has been a growing demand for a quantitative understanding of how opinions form and diffuse because of the existence of social ties among a communityâ€™s members and how the underlying structure of a social network can influence this process, cf. Faliszewski et al. (2022); Elkind et al. (2009); Brill et al. (2016); Bredereck and Elkind (2017); GÃ¤rtner and N. Zehmakan (2017). This would enable us to obtain better predictions of electoral results, control the effect of marketing and political campaigns, and in general advance our knowledge of the cognitive processes behind social influence. The motive to gain insights on how opinions are shaped and evolved in multi-agent systems has been the driving force behind an interdisciplinary research effort in diverse areas such as sociology MoussaÃ¯d et al. (2013), economics Jackson (2011), mathematics Balogh et al. (2012), physics Galam (2008), and computer science Chistikov et al. (2020). Within the field of computer science, especially computational social choice and algorithmic game theory, there has been a rising interest in developing and analyzing mathematical models which simulate the opinion diffusion in a network of individuals, cf. Bredereck and Elkind (2017); Out and Zehmakan (2021). Of course, in reality the opinion formation processes are too complex to be expressed in purely theoretical terms, but the goal is to shed some light on their general principles, which are otherwise hidden by the convolution of the full phenomenon. The following generic and abstract model is the foundation for most of the proposed opinion diffusion models. Consider a graph where each node has a colour and the nodes change their colour according to an updating rule, which is a function of the colour of the neighbouring nodes, in a defined order. The graph is meant to represent a social network, where the individuals are modeled as nodes and edges indicate relations between them, e.g., friendship, common interests, or advice. The colour of a node stands for its preference about a certain topic, e.g., an order over a set of candidates. In the very popular Voter Model, initially all nodes are either blue or red and then in each round, every node picks a random neighbour and adopts its colour. We generalise the voter model to capture several important real-world characteristics: â€¢ We relax the constraints of the graph being undirected and unweighted imposed in the original voter model. Directions are important for a realistic modelling since, for example in online social platforms, it is possible that one user follows the other user, but not the other way around. Furthermore, weights are utilised to model the strength of the relationships among individuals. â€¢ We allow uncoloured nodes in the initial colourings, which are nodes who are neither positive nor negative about the topic/product. â€¢ We allow some nodes to keep their colour unchanged. This accounts for stubborn or loyal individuals, who are not influenced by the opinion of their peers. Political parties and corporations frequently employ diverse strategies to persuade a specific segment of consumers on social media platforms to adopt a positive opinion about a specific product or topic. By harnessing the influence of these individuals on their social circles, a chain reaction of influence can be created, cf. Lin and Lui (2015); Myers and Leskovec (2012). This technique has emerged as a prominent method for promoting new ideas, products, and services, as it enables marketing or political campaigns to achieve extensive reach and exposure while keeping costs low. The question then becomes how to choose an initial subset of so-called early adopters to maximise the number of people that will eventually be reached, given some fixed budget. Motivated by this application, we study the problem of maximizing the expected number of blue nodes after some rounds by selecting a fixed number of initial blue nodes. We prove that the problem cannot be approximated better than (1âˆ’1/e)11ğ‘’(1-1/e)( 1 - 1 / italic_e ), unless P=NP, and provide a polynomial time algorithm with such approximation ratio. In addition to the theoretical guarantee, the proposed algorithm outperforms centrality based algorithms on real-world graph data. It is worth to emphasise that the red nodes can be seen as nodes who have a negative opinion about the political party or the product or prefer the competitor party or company. In either case, the voter (and consequently our model) captures the setup where switching colours is free or inexpensive, for example changing opinions about a controversial topic or switching from one grocery store chain to another. Another important problem in the area of opinion diffusion models is determining the convergence properties of the opinion dynamics: Is convergence to stable states guaranteed and if yes, what are the upper and lower bounds on the convergence time? Since in most cases, such as ours, the opinion dynamics can be modelled as a Markov process, this problem is usually equivalent to determining the stationary distribution and convergence time of the corresponding Markov chain, cf. Frischknecht et al. (2013). We prove that the convergence time can be exponential in the general case, but it becomes polynomial for strongly connected graphs. Outline. We provide some basic definitions and problem formulations in Section 2. Then, a short overview of prior work and our contributions are given in Sections 3 and 4. The complexity and algorithms results on maximum adoption problem, plus the experimental findings, are provided in Section 5. Finally, the convergence properties of the model are analyzed in Section 6."
https://arxiv.org/html/2411.04454v1,Mixing time of quantum Gibbs samplingfor random sparse Hamiltonians,"Providing evidence that quantum computers can efficiently prepare low-energy or thermal states of physically relevant interacting quantum systems is a major challenge in quantum information science. A newly developed quantum Gibbs sampling algorithm [CKG23] provides an efficient simulation of the detailed-balanced dissipative dynamics of non-commutative quantum systems. The running time of this algorithm depends on the mixing time of the corresponding quantum Markov chain, which has not been rigorously bounded except in the high-temperature regime. In this work, we establish a polylogâ¢(n)polylogğ‘›\mathrm{polylog}(n)roman_polylog ( italic_n ) upper bound on its mixing time for various families of random nÃ—nğ‘›ğ‘›n\times nitalic_n Ã— italic_n sparse Hamiltonians at any constant temperature. We further analyze how the choice of the jump operators for the algorithm and the spectral properties of these sparse Hamiltonians influence the mixing time. Our result places this method for Gibbs sampling on par with other efficient algorithms for preparing low-energy states of quantumly easy Hamiltonians.","One of the main anticipated applications of quantum computers is the simulation and characterization of quantum systems in condensed matter physics [WHW+15], quantum chemistry [MEAG+20], and high-energy physics [Pre18, BDB+23]. The problem of simulating the dynamics (time evolution) of an interacting quantum system under a local or sparse Hamiltonian ğ‘¯ğ‘¯\bm{H}bold_italic_H has largely been addressed, with efficient algorithms [HHKL18, LC17, BCC+15, LC19, GSLW19] that scale well with the number of particles, simulation time, and required precision. However, the ability of quantum computers to evaluate the static features of quantum systems, such as their ground state or thermal properties, is less understood. In this work, we focus on preparing the Gibbs (thermal) state ğ†Î²=eâˆ’Î²â¢ğ‘¯Trâ¢(eâˆ’Î²â¢ğ‘¯)subscriptğ†ğ›½superscriptğ‘’ğ›½ğ‘¯Trsuperscriptğ‘’ğ›½ğ‘¯\bm{\rho}_{\beta}=\frac{e^{-\beta\bm{H}}}{\mathrm{Tr}(e^{-\beta\bm{H}})}bold_italic_Ï start_POSTSUBSCRIPT italic_Î² end_POSTSUBSCRIPT = divide start_ARG italic_e start_POSTSUPERSCRIPT - italic_Î² bold_italic_H end_POSTSUPERSCRIPT end_ARG start_ARG roman_Tr ( italic_e start_POSTSUPERSCRIPT - italic_Î² bold_italic_H end_POSTSUPERSCRIPT ) end_ARG of a quantum system, which represents the equilibrium state when the system is in contact with a thermal bath at a fixed temperature Î²âˆ’1superscriptğ›½1\beta^{-1}italic_Î² start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT. This computational problem, known as Gibbs sampling or â€œcooling,â€ is valuable not only for simulating thermodynamic properties but also as a subroutine in quantum algorithms for optimization and learning [BS17, vAG19, BKL+19]. However, to prepare the Gibbs state, quantum computers face challenges. In general, it is not believed that estimating the low-temperature properties of quantum systems can be solved efficiently by a quantum computer in the worst-case [KKR05]. Fortunately, it has been hypothesized that this worst-case hardness of finding low-temperature states implied by arguments from complexity theory is due to pathological Hamiltonians, which are not apparent in many physical systems that normally occur in nature. This hypothesis is substantiated by the empirical success of natural cooling, such as using refrigerators, in reaching thermal equilibrium. Quantum Gibbs sampling. Aiming to mimic natureâ€™s cooling processes, a series of recent works have introduced quantum Markov Chain Monte Carlo (MCMC) algorithms, or quantum Gibbs samplers [CKG23, CKBG23, SM21, WT23, RWW23, JI24, ZBC23, DLL24, GCDK24], as promising alternatives for tackling a range of classically intractable low-temperature simulation tasks on quantum computers. These algorithms are designed to replicate the success of classical Markov chains in preparing Gibbs states for classical Hamiltonians. The analysis of classical MCMC algorithms relies on the principle of detailed balance; however, achieving this in the quantum setting has been challenging and was only recently addressed by an algorithm in [CKG23]. Part of the difficulty arises from a conflict between the finite energy resolution ÏƒEsubscriptğœğ¸\sigma_{E}italic_Ïƒ start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT achievable by efficient quantum algorithms and the seemingly strict requirement to precisely distinguish energy levels to satisfy detailed balance. In this work, we focus primarily on this algorithm, referring to it as the CKG algorithm or the quantum Gibbs sampler when the context is clear. We give a detailed review of this algorithm in Section 4.1.3 and Appendix 7.1.1. The Gibbs sampling algorithm provides a fully general method for preparing Gibbs states by evolving an initial state ğ†0subscriptğ†0\bm{\rho}_{0}bold_italic_Ï start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT under a Lindbladian â„’Î²subscriptâ„’ğ›½\mathcal{L}_{\beta}caligraphic_L start_POSTSUBSCRIPT italic_Î² end_POSTSUBSCRIPT, which is efficiently implementable on a quantum computer and produces the state ğ†t=eâ„’Î²â¢tâ¢[ğ†0]subscriptğ†ğ‘¡superscriptğ‘’subscriptâ„’ğ›½ğ‘¡delimited-[]subscriptğ†0\bm{\rho}_{t}=e^{\mathcal{L}_{\beta}t}[\bm{\rho}_{0}]bold_italic_Ï start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_e start_POSTSUPERSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_Î² end_POSTSUBSCRIPT italic_t end_POSTSUPERSCRIPT [ bold_italic_Ï start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ] after time tğ‘¡titalic_t. The runtime of the quantum Gibbs sampler is governed by the mixing time of the corresponding quantum Markov chain, which is roughly the time required for ğ†tsubscriptğ†ğ‘¡\bm{\rho}_{t}bold_italic_Ï start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT to approach the Gibbs state ğ†Î²subscriptğ†ğ›½\bm{\rho}_{\beta}bold_italic_Ï start_POSTSUBSCRIPT italic_Î² end_POSTSUBSCRIPT. This in turn is bounded by the spectral gap Î»gapâ¢(â„’Î²)subscriptğœ†gapsubscriptâ„’ğ›½\lambda_{\text{gap}}(\mathcal{L}_{\beta})italic_Î» start_POSTSUBSCRIPT gap end_POSTSUBSCRIPT ( caligraphic_L start_POSTSUBSCRIPT italic_Î² end_POSTSUBSCRIPT ) of the Lindbladian by tmixâ¢(â„’Î²)â‰¤ğ’ªâ¢(Î²â¢â€–ğ‘¯â€–+logâ¡(n))Î»gapâ¢(â„’Î²).subscriptğ‘¡mixsubscriptâ„’ğ›½ğ’ªğ›½normğ‘¯ğ‘›subscriptğœ†gapsubscriptâ„’ğ›½t_{\text{mix}}(\mathcal{L}_{\beta})\leq\frac{\mathcal{O}(\beta\|{\bm{H}}\|+% \log(n))}{\lambda_{\text{gap}}(\mathcal{L}_{\beta})}.italic_t start_POSTSUBSCRIPT mix end_POSTSUBSCRIPT ( caligraphic_L start_POSTSUBSCRIPT italic_Î² end_POSTSUBSCRIPT ) â‰¤ divide start_ARG caligraphic_O ( italic_Î² âˆ¥ bold_italic_H âˆ¥ + roman_log ( italic_n ) ) end_ARG start_ARG italic_Î» start_POSTSUBSCRIPT gap end_POSTSUBSCRIPT ( caligraphic_L start_POSTSUBSCRIPT italic_Î² end_POSTSUBSCRIPT ) end_ARG . The mixing time varies based on the quantum system in question. Bounding this mixing time is challenging without access to fault-tolerant quantum computers, as we cannot run and benchmark the algorithm directly, making theoretical analysis essential. However, such analysis is hindered by a lack of technical tools for two key reasons. Firstly, the theory of convergence of quantum Markov chains is new, unlike the very mature twin field for classical Markov chains. Secondly, the Markov chain described by the algorithm is considerably complex, and depends on several parameters that we will discuss in more detail shortly: an energy resolution ÏƒEsubscriptğœğ¸\sigma_{E}italic_Ïƒ start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT, a series of jump operators ğ‘¨asuperscriptğ‘¨ğ‘\bm{A}^{a}bold_italic_A start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT for aâˆˆ[M]ğ‘delimited-[]ğ‘€a\in[M]italic_a âˆˆ [ italic_M ], and the inverse temperature Î²ğ›½\betaitalic_Î². The space of possibilities makes the algorithmâ€™s performance more difficult to characterize. This motivates the identification of quantum systems whose mixing times are tractable for analysis yet exhibit rich features that provide insights into the performance of the quantum Gibbs sampler for more general non-commuting Hamiltonians. In line with this, the mixing time of the CKG algorithm has recently been bounded for local Hamiltonians, showing a polynomial scaling with system size at high enough temperatures [RFA24]. Mixing time of sparse Hamiltonians. In this work, we consider an alternative approach by characterizing the mixing time of a family of sparse Hamiltonians of the following form: which can be understood as the Hamiltonian on a graph G=(V,E)ğºğ‘‰ğ¸G=(V,E)italic_G = ( italic_V , italic_E ) with n=|V|ğ‘›ğ‘‰n=|V|italic_n = | italic_V | vertices indexed by basis states \ketâ¢ei\ketsubscriptğ‘’ğ‘–\ket{e_{i}}italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, iâˆˆ[n]ğ‘–delimited-[]ğ‘›i\in[n]italic_i âˆˆ [ italic_n ] and a set of edges Eğ¸Eitalic_E connecting vertices with Hiâ¢jâ‰ 0subscriptğ»ğ‘–ğ‘—0H_{ij}\neq 0italic_H start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT â‰  0. When non-zero entries Hiâ¢jsubscriptğ»ğ‘–ğ‘—H_{ij}italic_H start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT are all equal to 1111, the Hamiltonian ğ‘¯ğ‘¯\bm{H}bold_italic_H corresponds to the nÃ—nğ‘›ğ‘›n\times nitalic_n Ã— italic_n adjacency matrix of the nğ‘›nitalic_n-vertex graph. We define the degree dğ‘‘ditalic_d of the graph GğºGitalic_G as the sparsity of the underlying Hamiltonian and refer to Hamiltonians with constant or slowly increasing degrees d=polylogâ¢(n)ğ‘‘polylogğ‘›d=\mathrm{polylog}(n)italic_d = roman_polylog ( italic_n ) as sparse. Note that any logâ¡(n)ğ‘›\log(n)roman_log ( italic_n )-qubit Hamiltonian that consists of m=polylogâ¢(n)ğ‘špolylogğ‘›m=\mathrm{polylog}(n)italic_m = roman_polylog ( italic_n ) terms each acting locally on Îº=Oâ¢(1)ğœ…ğ‘‚1\kappa=O(1)italic_Îº = italic_O ( 1 ) qubits is a sparse Hamiltonian with degree dâ‰¤mâ¢2Îºâ‰¤polylogâ¢(n)ğ‘‘ğ‘šsuperscript2ğœ…polylogğ‘›d\leq m2^{\kappa}\leq\mathrm{polylog}(n)italic_d â‰¤ italic_m 2 start_POSTSUPERSCRIPT italic_Îº end_POSTSUPERSCRIPT â‰¤ roman_polylog ( italic_n ). However, not all sparse Hamiltonians admit local qubit encodings. Having defined sparse Hamiltonians, we now consider the dissipative dynamics of this system induced by a set of Mğ‘€Mitalic_M jump operators expressed as follows: ğ‘¨a=âˆ‘i,jâˆˆ[n]Aiâ¢jaâ¢\ketâ¢eiâ¢\braâ¢ej,aâˆˆ[M].formulae-sequencesuperscriptğ‘¨ğ‘subscriptğ‘–ğ‘—delimited-[]ğ‘›superscriptsubscriptğ´ğ‘–ğ‘—ğ‘\ketsubscriptğ‘’ğ‘–\brasubscriptğ‘’ğ‘—ğ‘delimited-[]ğ‘€\displaystyle\bm{A}^{a}=\sum_{i,j\in[n]}A_{ij}^{a}\ket{e_{i}}\bra{e_{j}},\quad% \quad a\in[M].bold_italic_A start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT = âˆ‘ start_POSTSUBSCRIPT italic_i , italic_j âˆˆ [ italic_n ] end_POSTSUBSCRIPT italic_A start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_e start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_a âˆˆ [ italic_M ] . (1) We will soon explain how the jump operators ğ‘¨asuperscriptğ‘¨ğ‘\bm{A}^{a}bold_italic_A start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT relate to the Lindbladian â„’Î²subscriptâ„’ğ›½\mathcal{L}_{\beta}caligraphic_L start_POSTSUBSCRIPT italic_Î² end_POSTSUBSCRIPT. Briefly, the resulting dynamics can be understood as a combination of two processes: a continuous-time quantum walk of a single particle on the graph of states due to the coherent evolution of the Hamiltonian ğ‘¯ğ‘¯\bm{H}bold_italic_H, which is combined with stochastic jumps on the graph determined by the jump operators ğ‘¨asuperscriptğ‘¨ğ‘\bm{A}^{a}bold_italic_A start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT. Our interest in bounding the mixing time of the sparse Hamiltonians is multifaceted: (1) Single-particle dynamics. As stated earlier, bounding the mixing time of general interacting multipartite Hamiltonians is a challenging task. However, for simple choices of graphs GğºGitalic_G, the mixing time of the quantum Gibbs sampler may be easier to analyze, potentially leading to relevant techniques for tackling the case of interacting particles. In fact, we can think of the dynamics induced by the Hamiltonian ğ‘¯ğ‘¯\bm{H}bold_italic_H (LABEL:eq:sparseH) as the dynamics of a single-particle hopping on the graph GğºGitalic_G. This single-particle evolution on path graphs or grids is commonly analyzed in the tight-binding model in condensed matter physics. That being said, even in the simplified case of a single particle, the Hamiltonian ğ‘¯ğ‘¯\bm{H}bold_italic_H is non-commuting, characterizing a continuous-time quantum walk that can yield exponential quantum advantage for certain oracular problem on graphs such as the glued tress [CCD+03]. (2) Chaotic Hamiltonians. Our additional motivation for studying random sparse Hamiltonians stems from the fact that their spectra exhibit many of the same characteristics as chaotic Hamiltonians, such as the SYK model [SY93, Kit15a, Kit15b] and random pğ‘pitalic_p-spin models [SW24, WBB+22]. Understanding whether chaotic Hamiltonians have a fast mixing time as they approach their thermal and low-energy states is a fundamental question in the study of quantum chaos [CB21, ACKK24]. As a concrete step toward addressing this problem, we identify key spectral properties of random sparse Hamiltonians that can ensure a fast mixing time. (3) Algorithmic applications. Preparing quantum Gibbs states, and more broadly computing the matrix exponential of sparse matrices such as the adjacency or Laplacian of a graph, is a fundamental subroutine in solving various graph and optimization problems. For instance, the Estrada indexâ€”defined as the trace of the matrix exponential of a graphâ€™s adjacency matrixâ€”measures subgraph centrality and provides structural insights [ERV05]. Computing the matrix exponential is also related to matrix inversion and linear system solvers [SV13]. Moreover, quantum Gibbs sampling has been applied to solving semidefinite programs (SDPs) in optimization problems [GLBKSF22, BS17, BKL+19, vAGGdW20], offering quantum speedups for these problems."
https://arxiv.org/html/2411.04394v1,Statistical-Computational Trade-offs for Greedy Recursive Partitioning Estimators,"Models based on recursive partitioning such as decision trees and their ensembles are popular for high-dimensional regression as they can potentially avoid the curse of dimensionality. Because empirical risk minimization (ERM) is computationally infeasible, these models are typically trained using greedy algorithms. Although effective in many cases, these algorithms have been empirically observed to get stuck at local optima. We explore this phenomenon in the context of learning sparse regression functions over dğ‘‘ditalic_d binary features, showing that when the true regression function fâˆ—superscriptğ‘“f^{*}italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT does not satisfy the so-called Merged Staircase Property (MSP), greedy training requires expâ¡(Î©â¢(d))Î©ğ‘‘\exp(\Omega(d))roman_exp ( roman_Î© ( italic_d ) ) to achieve low estimation error. Conversely, when fâˆ—superscriptğ‘“f^{*}italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT does satisfy MSP, greedy training can attain small estimation error with only Oâ¢(logâ¡d)ğ‘‚ğ‘‘O(\log d)italic_O ( roman_log italic_d ) samples. This performance mirrors that of two-layer neural networks trained with stochastic gradient descent (SGD) in the mean-field regime, thereby establishing a head-to-head comparison between SGD-trained neural networks and greedy recursive partitioning estimators. Furthermore, ERM-trained recursive partitioning estimators achieve low estimation error with Oâ¢(logâ¡d)ğ‘‚ğ‘‘O(\log d)italic_O ( roman_log italic_d ) samples irrespective of whether fâˆ—superscriptğ‘“f^{*}italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT satisfies MSP, thereby demonstrating a statistical-computational trade-off for greedy training. Our proofs are based on a novel interpretation of greedy recursive partitioning using stochastic process theory and a coupling technique that may be of independent interest.","Decision tree models are piecewise constant supervised learning models obtained by recursive partitioning of the covariate space. Although classical, they remain among the most important supervised learning models because they are highly interpretable (Rudin et al., 2021; Murdoch et al., 2019) and yet are flexible enough to afford the potential for high prediction accuracy. This potential is maximized when decision trees are combined in ensembles via random forests (RFs) (Breiman, 2001) or gradient boosting (Friedman, 2001). These algorithms are widely recognized as having state-of-the-art performance on moderately-sized tabular datasets (Caruana et al., 2008; FernÃ¡ndez-Delgado et al., 2014; Olson et al., 2018), even outperforming state-of-the-art deep learning methods (Grinsztajn et al., 2022), despite the amount of attention lavished on the latter. Such datasets are common in many settings such as bioinformatics, healthcare, economics, and social sciences. Naturally, decision trees and their ensembles receive widespread use via their implementation in popular machine learning packages such as ranger (Wright et al., 2017), scikit-learn (Pedregosa et al., 2011), xgboost (Chen and Guestrin, 2016), and lgbm (Ke et al., 2017). Decision trees have also been adapted to a variety of tasks beyond regression and classification, including survival analysis (Ishwaran et al., 2008), heterogeneous treatment effect estimation (Athey and Imbens, 2016), time series analysis, and multi-task learning. While alternatives exist, most decision tree models used in practice make binary, axis-aligned splits at each partitioning stage. In this paper, we study the statistical-computational trade-offs of these objects, focusing on regression trees. We assume a nonparametric regression model under random design: Yi=fâˆ—â¢(ğ—i)+Îµi,i=1,2,â€¦,n.formulae-sequencesubscriptğ‘Œğ‘–superscriptğ‘“subscriptğ—ğ‘–subscriptğœ€ğ‘–ğ‘–12â€¦ğ‘›Y_{i}=f^{*}(\mathbf{X}_{i})+\varepsilon_{i},\quad i=1,2,\ldots,n.italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) + italic_Îµ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_i = 1 , 2 , â€¦ , italic_n . (1) Here, the covariate space ğ’³ğ’³\mathcal{X}caligraphic_X is a compact subset of â„dsuperscriptâ„ğ‘‘\mathbb{R}^{d}roman_â„ start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT, ğ—1,ğ—2,â€¦â¢ğ—nsubscriptğ—1subscriptğ—2â€¦subscriptğ—ğ‘›\mathbf{X}_{1},\mathbf{X}_{2},\ldots\mathbf{X}_{n}bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_X start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ bold_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT are drawn i.i.d. from a distribution Î½ğœˆ\nuitalic_Î½ on ğ’³ğ’³\mathcal{X}caligraphic_X, Îµ1,Îµ2,â€¦,Îµnsubscriptğœ€1subscriptğœ€2â€¦subscriptğœ€ğ‘›\varepsilon_{1},\varepsilon_{2},\ldots,\varepsilon_{n}italic_Îµ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_Îµ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_Îµ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT are noise variables drawn i.i.d. from a zero-mean noise distribution on â„â„\mathbb{R}roman_â„, with ÎµiâŸ‚âŸ‚ğ—iâ‰•(Xiâ¢1,Xiâ¢2,â€¦,Xiâ¢d)\varepsilon_{i}\perp\!\!\!\perp\mathbf{X}_{i}\eqqcolon(X_{i1},X_{i2},\ldots,X_% {id})italic_Îµ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âŸ‚ âŸ‚ bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT â‰• ( italic_X start_POSTSUBSCRIPT italic_i 1 end_POSTSUBSCRIPT , italic_X start_POSTSUBSCRIPT italic_i 2 end_POSTSUBSCRIPT , â€¦ , italic_X start_POSTSUBSCRIPT italic_i italic_d end_POSTSUBSCRIPT ), and fâˆ—:ğ’³â†’â„:superscriptğ‘“â†’ğ’³â„f^{*}\colon\mathcal{X}\to\mathbb{R}italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT : caligraphic_X â†’ roman_â„ is the conditional expectation function of Yğ‘ŒYitalic_Y given ğ—=ğ±ğ—ğ±\mathbf{X}=\mathbf{x}bold_X = bold_x. The observed data is denoted as ğ’Ÿnâ‰”{(ğ—i,Yi):i=1,â€¦,n}â‰”subscriptğ’Ÿğ‘›conditional-setsubscriptğ—ğ‘–subscriptğ‘Œğ‘–ğ‘–1â€¦ğ‘›\mathcal{D}_{n}\coloneqq\left\{(\mathbf{X}_{i},Y_{i})\colon i=1,\ldots,n\right\}caligraphic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT â‰” { ( bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) : italic_i = 1 , â€¦ , italic_n }. We will compare and contrast various tree-based estimators for fâˆ—superscriptğ‘“f^{*}italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT , evaluating the accuracy of an estimate f^â¢(âˆ’;ğ’Ÿn)^ğ‘“subscriptğ’Ÿğ‘›\hat{f}(-;\mathcal{D}_{n})over^ start_ARG italic_f end_ARG ( - ; caligraphic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) using the L2superscriptğ¿2L^{2}italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT risk (or estimation error) Râ¢(f^â¢(âˆ’;ğ’Ÿn),fâˆ—)â‰”ğ”¼ğ—âˆ¼Î½â¢{(f^â¢(ğ—;ğ’Ÿn)âˆ’fâˆ—â¢(ğ—))2},â‰”ğ‘…^ğ‘“subscriptğ’Ÿğ‘›superscriptğ‘“subscriptğ”¼similar-toğ—ğœˆsuperscript^ğ‘“ğ—subscriptğ’Ÿğ‘›superscriptğ‘“ğ—2R(\hat{f}(-;\mathcal{D}_{n}),f^{*})\coloneqq\mathbb{E}_{\mathbf{X}\sim\nu}% \left\{\left(\hat{f}(\mathbf{X};\mathcal{D}_{n})-f^{*}(\mathbf{X})\right)^{2}% \right\},italic_R ( over^ start_ARG italic_f end_ARG ( - ; caligraphic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) , italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ) â‰” roman_ğ”¼ start_POSTSUBSCRIPT bold_X âˆ¼ italic_Î½ end_POSTSUBSCRIPT { ( over^ start_ARG italic_f end_ARG ( bold_X ; caligraphic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) - italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( bold_X ) ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT } , and the accuracy of an estimator f^â¢(âˆ’;âˆ’)^ğ‘“\hat{f}(-;-)over^ start_ARG italic_f end_ARG ( - ; - ) using the expected L2superscriptğ¿2L^{2}italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT risk, ğ”¼ğ’Ÿnâ¢{Râ¢(f^â¢(âˆ’;ğ’Ÿn),fâˆ—)}subscriptğ”¼subscriptğ’Ÿğ‘›ğ‘…^ğ‘“subscriptğ’Ÿğ‘›superscriptğ‘“\mathbb{E}_{\mathcal{D}_{n}}\left\{R\left(\hat{f}(-;\mathcal{D}_{n}),f^{*}% \right)\right\}roman_ğ”¼ start_POSTSUBSCRIPT caligraphic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_POSTSUBSCRIPT { italic_R ( over^ start_ARG italic_f end_ARG ( - ; caligraphic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) , italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ) }. Note that throughout this paper, we will use the convention that random variables are denoted in upper case while any fixed value they may attain is denoted in lower case. Furthermore, vectors will be denoted using boldface, whereas scalars will be denoted using regular font. We denote [d]â‰”{1,2,â€¦,d}â‰”delimited-[]ğ‘‘12â€¦ğ‘‘[d]\coloneqq\left\{1,2,\ldots,d\right\}[ italic_d ] â‰” { 1 , 2 , â€¦ , italic_d }. To supplement standard Big-Oğ‘‚Oitalic_O notation, we use O~~ğ‘‚\tilde{O}over~ start_ARG italic_O end_ARG to suppress poly-logarithmic factors. 1.1 Consistency of ERM trees, Greedy Trees, and Non-Adaptive Trees Machine learning models are often fit by solving empirical risk minimization (ERM). Solutions to ERM over appropriate classes of decision tree functions can be shown to be consistent, but unfortunately are computationally infeasible to obtain. For instance, ERM for classification trees is known to be NP-hard in the worst case (Hyafil and Rivest, 1976). As a result, practitioners make use of greedy algorithms such as CART (Breiman et al., 1984), ID3 (Quinlan, 1986), or C4.5 (Quinlan, 1993). Such algorithms grow a tree in a top-down manner, with the split for a given node chosen by optimizing a local node-specific objective. Once made, a split is never reversed.111Decision trees are sometimes pruned as a one-off post-processing procedure after they are fully grown. These heuristics seem to work well in practiceâ€”both RFs and gradient boosting are built out of CART treesâ€”but are notoriously difficult to analyze. Indeed, early efforts to prove consistency of greedy regression and classification trees under standard smoothness conditions on fâˆ—superscriptğ‘“f^{*}italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT had to further assume that the bias of the tree converged to zero (Breiman et al., 1984; Chaudhuri et al., 1995). More recent works were able to replace this assumption with more reasonable alternatives on fâˆ—superscriptğ‘“f^{*}italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT such as additivity (Scornet et al., 2015; Klusowski, 2020; Klusowski and Tian, 2024), a submodularity condition (Syrgkanis and Zampetakis, 2020), or the sufficient impurity decrease (SID) condition (Chi et al., 2022; Mazumder and Wang, 2024). In another line of work, researchers studied and were able to prove consistency for decision trees making splits that are non-adaptive (i.e., independent of the response). These algorithms include trees that make completely random splits (Geurts et al., 2006; Biau et al., 2008; Biau, 2012; Wager and Athey, 2018; Klusowski, 2021), Mondrian trees which make splits according to a Bayesian modeling of the covariate distribution (Roy and Teh, 2008; Lakshminarayanan et al., 2014; Mourtada et al., 2017; Cattaneo et al., 2023) and kğ‘˜kitalic_k-Potential Nearest Neighbors (Lin and Jeon, 2006; Biau and Devroye, 2010; Scornet, 2016; Biau and Scornet, 2016; Shi et al., 2024). Despite possessing theoretical guarantees requiring fewer assumptions, such algorithms exhibit performance gaps compared to greedy trees, in particular, in terms of adaptivity to heterogeneous truth. 1.2 High-Dimensional Consistency From the discussion thus far, we gather that considering nonparametric consistency alone does not explain why greedy trees have been so successful relative to non-adaptive trees, let alone other machine learning algorithms such as kğ‘˜kitalic_k-nearest neighbors or support vector machines. To explain this, we compare the performance of these algorithms in a high-dimensional setting (n,dâ†’âˆâ†’ğ‘›ğ‘‘n,d\to\inftyitalic_n , italic_d â†’ âˆ) with sparsity constraints on fâˆ—superscriptğ‘“f^{*}italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT, i.e., if fâˆ—superscriptğ‘“f^{*}italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT only depends on a constant number of sğ‘ sitalic_s covariates. Informally, we say that an estimator avoids the curse of dimensionality, or that it is high-dimensional consistent, if its expected L2superscriptğ¿2L^{2}italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT risk converges to zero under a scaling regime satisfying logâ¡n=oâ¢(d)ğ‘›ğ‘œğ‘‘\log n=o(d)roman_log italic_n = italic_o ( italic_d ). We first argue that non-adaptive trees are not high-dimensional consistent222See Appendix A for a formal definition of non-adaptive tree models and a rigorous proof of this statement.: Assuming relatively balanced splits, the average depth of a tree is Oâ¢(logâ¡n)ğ‘‚ğ‘›O(\log n)italic_O ( roman_log italic_n ). Hence, logâ¡n=oâ¢(d)ğ‘›ğ‘œğ‘‘\log n=o(d)roman_log italic_n = italic_o ( italic_d ) implies that not every covariate can receive a split, or in other words, that the tree needs to be able to selectively split on relevant covariates, which contradicts the definition of non-adaptivity. In contrast, several works (Syrgkanis and Zampetakis, 2020; Chi et al., 2022; Mazumder and Wang, 2024; Klusowski and Tian, 2024) have showed that CART and RFs are high-dimensional consistent given various condition on fâˆ—superscriptğ‘“f^{*}italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT (e.g., submodularity, SID, or additivity). This helps to explain the successful application of RFs to high-dimensional problems such as genomics in which nâ‰ªdmuch-less-thanğ‘›ğ‘‘n\ll ditalic_n â‰ª italic_d. On the other hand, these works were not able to show whether their assumed conditions were necessary for high-dimensional consistency. Syrgkanis and Zampetakis (2020) used the exclusive-or (XOR) function to argue heuristically that some condition is indeed necessary (see Section 5 therein). To understand this argument, we focus on the setting of binary covariates (ğ’³={Â±1}dğ’³superscriptplus-or-minus1ğ‘‘\mathcal{X}=\{\pm 1\}^{d}caligraphic_X = { Â± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT) under the uniform distribution. In this setting, the XOR function can be written as fâˆ—â¢(ğ±)=x1â¢x2superscriptğ‘“ğ±subscriptğ‘¥1subscriptğ‘¥2f^{*}(\mathbf{x})=x_{1}x_{2}italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( bold_x ) = italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. Meanwhile, every tree node corresponds to a subcube ğ’âŠ‚{Â±1}dğ’superscriptplus-or-minus1ğ‘‘\mathcal{C}\subset\{\pm 1\}^{d}caligraphic_C âŠ‚ { Â± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT and every potential split of ğ’ğ’\mathcal{C}caligraphic_C divides it evenly into two subcubes of lower dimension. With each split fully determined by the choice of covariate index k=1,â€¦,dğ‘˜1â€¦ğ‘‘k=1,\ldots,ditalic_k = 1 , â€¦ , italic_d, CART chooses the index of the covariate with the maximum plug-in squared correlation with the response within ğ’ğ’\mathcal{C}caligraphic_C: Corr^2â¢{Y,Xk|ğ—âˆˆğ’}=(âˆ‘ğ—iâˆˆğ’Xiâ¢kâ¢(Yiâˆ’YÂ¯ğ’))2âˆ‘ğ—iâˆˆğ’(Xiâ¢kâˆ’(XÂ¯k)ğ’)2â¢âˆ‘ğ—iâˆˆğ’(Yiâˆ’YÂ¯ğ’)2;superscript^Corr2conditional-setğ‘Œsubscriptğ‘‹ğ‘˜ğ—ğ’superscriptsubscriptsubscriptğ—ğ‘–ğ’subscriptğ‘‹ğ‘–ğ‘˜subscriptğ‘Œğ‘–subscriptÂ¯ğ‘Œğ’2subscriptsubscriptğ—ğ‘–ğ’superscriptsubscriptğ‘‹ğ‘–ğ‘˜subscriptsubscriptÂ¯ğ‘‹ğ‘˜ğ’2subscriptsubscriptğ—ğ‘–ğ’superscriptsubscriptğ‘Œğ‘–subscriptÂ¯ğ‘Œğ’2\widehat{\operatorname{Corr}}^{2}\left\{Y,X_{k}~{}|~{}\mathbf{X}\in\mathcal{C}% \right\}=\frac{\left(\sum_{\mathbf{X}_{i}\in\mathcal{C}}X_{ik}(Y_{i}-\bar{Y}_{% \mathcal{C}})\right)^{2}}{\sum_{\mathbf{X}_{i}\in\mathcal{C}}(X_{ik}-(\bar{X}_% {k})_{\mathcal{C}})^{2}\sum_{\mathbf{X}_{i}\in\mathcal{C}}(Y_{i}-\bar{Y}_{% \mathcal{C}})^{2}};over^ start_ARG roman_Corr end_ARG start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT { italic_Y , italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | bold_X âˆˆ caligraphic_C } = divide start_ARG ( âˆ‘ start_POSTSUBSCRIPT bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âˆˆ caligraphic_C end_POSTSUBSCRIPT italic_X start_POSTSUBSCRIPT italic_i italic_k end_POSTSUBSCRIPT ( italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT caligraphic_C end_POSTSUBSCRIPT ) ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG âˆ‘ start_POSTSUBSCRIPT bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âˆˆ caligraphic_C end_POSTSUBSCRIPT ( italic_X start_POSTSUBSCRIPT italic_i italic_k end_POSTSUBSCRIPT - ( overÂ¯ start_ARG italic_X end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT caligraphic_C end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT âˆ‘ start_POSTSUBSCRIPT bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âˆˆ caligraphic_C end_POSTSUBSCRIPT ( italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - overÂ¯ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT caligraphic_C end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ; (2) (see Klusowski and Tian (2021) and Klusowski (2020), Equation (5)). Unless ğ’ğ’\mathcal{C}caligraphic_C already depends on either x1subscriptğ‘¥1x_{1}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT or x2subscriptğ‘¥2x_{2}italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, however, we have zero correlation in the infinite-sample limit, i.e., Corr2â¡{fâˆ—â¢(ğ—),Xk|ğ—âˆˆğ’}=ğ”¼â¢{X1â¢X2â¢Xk|ğ—âˆˆğ’}2=0,superscriptCorr2superscriptğ‘“ğ—conditionalsubscriptğ‘‹ğ‘˜ğ—ğ’ğ”¼superscriptconditional-setsubscriptğ‘‹1subscriptğ‘‹2subscriptğ‘‹ğ‘˜ğ—ğ’20\operatorname{Corr}^{2}\left\{f^{*}(\mathbf{X}),X_{k}~{}|~{}\mathbf{X}\in% \mathcal{C}\right\}=\mathbb{E}\left\{X_{1}X_{2}X_{k}~{}|~{}\mathbf{X}\in% \mathcal{C}\right\}^{2}=0,roman_Corr start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT { italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( bold_X ) , italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | bold_X âˆˆ caligraphic_C } = roman_ğ”¼ { italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_X start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | bold_X âˆˆ caligraphic_C } start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 0 , (3) for k=1,â€¦,dğ‘˜1â€¦ğ‘‘k=1,\ldots,ditalic_k = 1 , â€¦ , italic_d. This implies that CART cannot differentiate between relevant and irrelevant covariates, instead tending to make non-adaptive random splits. Hence, it cannot avoid the curse of dimensionality. As an ensemble of CART trees, RF suffers from the same limitations. Beyond some heuristic calculations, Syrgkanis and Zampetakis (2020) were not able to make this argument rigorous, nor did they attempt to generalize beyond this example. In this paper, we seek to bridge this gap and establish a necessary and nearly sufficient condition on fâˆ—superscriptğ‘“f^{*}italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT that characterizes when CART is high-dimensional consistent, and, in doing so, explicitly compare its risk performance against that of ERM and two-layer neural networks trained with SGD. We do this in the context of binary covariates under the uniform distribution, which although unlikely to describe any real dataset, provides a useful setting where both results and calculations can be stated cleanly and precisely, thereby illustrating the essence of the strengths and limitations of using a greedy splitting rule. We will later also provide extensions of these results to more general settings. 1.3 â€œAlmostâ€ Characterization Using the Merged-Staircase Property For any subset SâŠ‚[d]ğ‘†delimited-[]ğ‘‘S\subset[d]italic_S âŠ‚ [ italic_d ] of the coordinate indices, let Ï‡Ssubscriptğœ’ğ‘†\chi_{S}italic_Ï‡ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT be the monomial function defined by Ï‡Sâ¢(ğ±)=âˆjâˆˆSxjsubscriptğœ’ğ‘†ğ±subscriptproductğ‘—ğ‘†subscriptğ‘¥ğ‘—\chi_{S}(\mathbf{x})=\prod_{j\in S}x_{j}italic_Ï‡ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( bold_x ) = âˆ start_POSTSUBSCRIPT italic_j âˆˆ italic_S end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT.333If |S|=sğ‘†ğ‘ |S|=s| italic_S | = italic_s, such a function is sometimes called an sğ‘ sitalic_s-sparse parity in the theoretical computer science literature. It is a classical result from Fourier analysis (Stein and Shakarchi, 2011) that every function fâˆ—:{Â±1}dâ†’â„:superscriptğ‘“â†’superscriptplus-or-minus1ğ‘‘â„f^{*}\colon\{\pm 1\}^{d}\to\mathbb{R}italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT : { Â± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT â†’ roman_â„ can be uniquely written in the form fâˆ—=âˆ‘i=1rÎ±Siâ¢Ï‡Si,superscriptğ‘“superscriptsubscriptğ‘–1ğ‘Ÿsubscriptğ›¼subscriptğ‘†ğ‘–subscriptğœ’subscriptğ‘†ğ‘–f^{*}=\sum_{i=1}^{r}\alpha_{S_{i}}\chi_{S_{i}},italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT = âˆ‘ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT italic_Î± start_POSTSUBSCRIPT italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_Ï‡ start_POSTSUBSCRIPT italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT , (4) for some nonnegative integer rğ‘Ÿritalic_r, subsets S1,S2,â€¦,Srsubscriptğ‘†1subscriptğ‘†2â€¦subscriptğ‘†ğ‘ŸS_{1},S_{2},\ldots,S_{r}italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_S start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT of {1,2,â€¦,d}12â€¦ğ‘‘\left\{1,2,\ldots,d\right\}{ 1 , 2 , â€¦ , italic_d } and nonzero coefficients Î±Sj,j=1,2,â€¦,dformulae-sequencesubscriptğ›¼subscriptğ‘†ğ‘—ğ‘—12â€¦ğ‘‘\alpha_{S_{j}},j=1,2,\ldots,ditalic_Î± start_POSTSUBSCRIPT italic_S start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT , italic_j = 1 , 2 , â€¦ , italic_d. We say that fâˆ—superscriptğ‘“f^{*}italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT satisfies the merged-staircase property (MSP) if the subsets S1,S2,â€¦,Srsubscriptğ‘†1subscriptğ‘†2â€¦subscriptğ‘†ğ‘ŸS_{1},S_{2},\ldots,S_{r}italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_S start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT can be reordered such that for any iâˆˆ{1,2,â€¦,r}ğ‘–12â€¦ğ‘Ÿi\in\{1,2,\ldots,r\}italic_i âˆˆ { 1 , 2 , â€¦ , italic_r }, we have |Si\âˆªj=1iâˆ’1Sj|â‰¤1.\left|S_{i}\backslash\cup_{j=1}^{i-1}S_{j}\right|\leq 1.| italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \ âˆª start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i - 1 end_POSTSUPERSCRIPT italic_S start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | â‰¤ 1 . (5) Note that any monomial of degree k>1ğ‘˜1k>1italic_k > 1 does not satisfy the MSP. In particular, the XOR function does not satisfy the MSP. On the other hand, a positive example is provided by the function fâˆ—â¢(ğ±)=x1+x2+x1â¢x2â¢x3.superscriptğ‘“ğ±subscriptğ‘¥1subscriptğ‘¥2subscriptğ‘¥1subscriptğ‘¥2subscriptğ‘¥3f^{*}(\mathbf{x})=x_{1}+x_{2}+x_{1}x_{2}x_{3}.italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( bold_x ) = italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT . (6) Our main theorem, stated informally, is the following: Theorem 1.1 (Informal). When ğ’³={Â±1}dğ’³superscriptplus-or-minus1ğ‘‘\mathcal{X}=\{\pm 1\}^{d}caligraphic_X = { Â± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT, Î½=Unifâ¡({Â±1}d)ğœˆUnifsuperscriptplus-or-minus1ğ‘‘\nu=\operatorname{Unif}\left(\{\pm 1\}^{d}\right)italic_Î½ = roman_Unif ( { Â± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ), and fâˆ—superscriptğ‘“f^{*}italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT depends only on sğ‘ sitalic_s covariates, then the expected L2superscriptğ¿2L^{2}italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT risk of the CART estimator f^CARTsubscript^ğ‘“CART\hat{f}_{\operatorname{CART}}over^ start_ARG italic_f end_ARG start_POSTSUBSCRIPT roman_CART end_POSTSUBSCRIPT satisfies: â€¢ (Necessity) If fâˆ—superscriptğ‘“f^{*}italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT does not satisfy MSP, then ğ”¼ğ’Ÿnâ¢{Râ¢(f^CARTâ¢(âˆ’;ğ’Ÿn),fâˆ—)}=Î©â¢(1)subscriptğ”¼subscriptğ’Ÿğ‘›ğ‘…subscript^ğ‘“CARTsubscriptğ’Ÿğ‘›superscriptğ‘“Î©1\mathbb{E}_{\mathcal{D}_{n}}\left\{R\left(\hat{f}_{\operatorname{CART}}(-;% \mathcal{D}_{n}),f^{*}\right)\right\}=\Omega(1)roman_ğ”¼ start_POSTSUBSCRIPT caligraphic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_POSTSUBSCRIPT { italic_R ( over^ start_ARG italic_f end_ARG start_POSTSUBSCRIPT roman_CART end_POSTSUBSCRIPT ( - ; caligraphic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) , italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ) } = roman_Î© ( 1 ) whenever n=expâ¡(Oâ¢(d))ğ‘›ğ‘‚ğ‘‘n=\exp(O(d))italic_n = roman_exp ( italic_O ( italic_d ) ). â€¢ (Near sufficiency) If fâˆ—superscriptğ‘“f^{*}italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT satisfies MSP and its Fourier coefficients {Î±Si}i=1rsuperscriptsubscriptsubscriptğ›¼subscriptğ‘†ğ‘–ğ‘–1ğ‘Ÿ\left\{\alpha_{S_{i}}\right\}_{i=1}^{r}{ italic_Î± start_POSTSUBSCRIPT italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT are generic, then ğ”¼ğ’Ÿnâ¢{Râ¢(f^CARTâ¢(âˆ’;ğ’Ÿn),fâˆ—)}=Oâ¢(2s/n)subscriptğ”¼subscriptğ’Ÿğ‘›ğ‘…subscript^ğ‘“CARTsubscriptğ’Ÿğ‘›superscriptğ‘“ğ‘‚superscript2ğ‘ ğ‘›\mathbb{E}_{\mathcal{D}_{n}}\left\{R\left(\hat{f}_{\operatorname{CART}}(-;% \mathcal{D}_{n}),f^{*}\right)\right\}=O(2^{s}/n)roman_ğ”¼ start_POSTSUBSCRIPT caligraphic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_POSTSUBSCRIPT { italic_R ( over^ start_ARG italic_f end_ARG start_POSTSUBSCRIPT roman_CART end_POSTSUBSCRIPT ( - ; caligraphic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) , italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ) } = italic_O ( 2 start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT / italic_n ) whenever n=Î©â¢(2sâ¢logâ¡d)ğ‘›Î©superscript2ğ‘ ğ‘‘n=\Omega(2^{s}\log d)italic_n = roman_Î© ( 2 start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT roman_log italic_d ). Furthermore, regardless of whether fâˆ—superscriptğ‘“f^{*}italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT satisfies MSP, the expected L2superscriptğ¿2L^{2}italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT risk of the ERM tree estimator satisfies ğ”¼ğ’Ÿnâ¢{Râ¢(f^ERMâ¢(âˆ’;ğ’Ÿn),fâˆ—)}=Oâ¢(2sâ¢logâ¡d/n)subscriptğ”¼subscriptğ’Ÿğ‘›ğ‘…subscript^ğ‘“ERMsubscriptğ’Ÿğ‘›superscriptğ‘“ğ‘‚superscript2ğ‘ ğ‘‘ğ‘›\mathbb{E}_{\mathcal{D}_{n}}\left\{R\left(\hat{f}_{\operatorname{ERM}}(-;% \mathcal{D}_{n}),f^{*}\right)\right\}=O(2^{s}\log d/n)roman_ğ”¼ start_POSTSUBSCRIPT caligraphic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_POSTSUBSCRIPT { italic_R ( over^ start_ARG italic_f end_ARG start_POSTSUBSCRIPT roman_ERM end_POSTSUBSCRIPT ( - ; caligraphic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) , italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ) } = italic_O ( 2 start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT roman_log italic_d / italic_n ). Here, genericity of {Î±Si}i=1rsuperscriptsubscriptsubscriptğ›¼subscriptğ‘†ğ‘–ğ‘–1ğ‘Ÿ\left\{\alpha_{S_{i}}\right\}_{i=1}^{r}{ italic_Î± start_POSTSUBSCRIPT italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT is satisfied with probability 1 whenever they are drawn from a distribution with a density. Up to this asterisk, we see that high-dimensional consistency for CART can be completely characterized by MSP, a combinatorial condition on Fourier coefficients. In addition, we see in fact that CART behaves similarly to ERM trees when MSP holds, and similarly to non-adaptive trees when it does not, thereby also establishing performance gaps: In the former case, CART sharply improves upon non-adaptive trees, and in the latter case, it performs much worse compared to ERM. Finally, we remark that our lower bounds when fâˆ—superscriptğ‘“f^{*}italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT does not satisfy MSP also hold more broadly for RFs, as well as for trees and ensembles grown with other greedy recursive partitioning strategies.444It is possible to extend the upper bound to various forms of RFs, but we leave this to future work. 1.4 Marginal Signal Bottleneck When a regression function fâˆ—superscriptğ‘“f^{*}italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT does not satisfy MSP, its Fourier decomposition must contain a term Î±Siâ¢Ï‡Sisubscriptğ›¼subscriptğ‘†ğ‘–subscriptğœ’subscriptğ‘†ğ‘–\alpha_{S_{i}}\chi_{S_{i}}italic_Î± start_POSTSUBSCRIPT italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_Ï‡ start_POSTSUBSCRIPT italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT that violates (5). This term creates problems for CART in a similar way as the XOR function. Let us call a covariate Xksubscriptğ‘‹ğ‘˜X_{k}italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT marginally undetectable in ğ’ğ’\mathcal{C}caligraphic_C if (3) holds. As argued before, when Xksubscriptğ‘‹ğ‘˜X_{k}italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT is relevant but marginally undetectable, CART cannot distinguish it from irrelevant covariates. One can show that there exists covariate indices k1,k2âˆˆSisubscriptğ‘˜1subscriptğ‘˜2subscriptğ‘†ğ‘–k_{1},k_{2}\in S_{i}italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT âˆˆ italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT such that Xk1subscriptğ‘‹subscriptğ‘˜1X_{k_{1}}italic_X start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT is marginally undetectable unless ğ’ğ’\mathcal{C}caligraphic_C has already has split on xk2subscriptğ‘¥subscriptğ‘˜2x_{k_{2}}italic_x start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT and Xk2subscriptğ‘‹subscriptğ‘˜2X_{k_{2}}italic_X start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT is marginally undetectable unless ğ’ğ’\mathcal{C}caligraphic_C has already has split on xk1subscriptğ‘¥subscriptğ‘˜1x_{k_{1}}italic_x start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT. We call this chicken and egg problem marginal signal bottleneck. It results in a highly probable event in which both these covariates remain marginally undetectable at every iteration of CART, leading to high-dimensional inconsistency. On the other hand, it is easy to show that MSP together with genericity implies the SID condition. In the setting of uniform binary features, this condition asserts a lower bound for what we call the marginal signal, max1â‰¤kâ‰¤dâ¡Corr2â¡{fâˆ—â¢(ğ—),Xk|ğ—âˆˆğ’}â‰¥Î»,subscript1ğ‘˜ğ‘‘superscriptCorr2superscriptğ‘“ğ—conditionalsubscriptğ‘‹ğ‘˜ğ—ğ’ğœ†\max_{1\leq k\leq d}\operatorname{Corr}^{2}\left\{f^{*}(\mathbf{X}),X_{k}~{}|~% {}\mathbf{X}\in\mathcal{C}\right\}\geq\lambda,roman_max start_POSTSUBSCRIPT 1 â‰¤ italic_k â‰¤ italic_d end_POSTSUBSCRIPT roman_Corr start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT { italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( bold_X ) , italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | bold_X âˆˆ caligraphic_C } â‰¥ italic_Î» , (7) that holds uniformly over all subcubes ğ’ğ’\mathcal{C}caligraphic_C on which Varâ¡{fâˆ—â¢(ğ—)|ğ—âˆˆğ’}>0Varconditionalsuperscriptğ‘“ğ—ğ—ğ’0\operatorname{Var}\left\{f^{*}(\mathbf{X})|\mathbf{X}\in\mathcal{C}\right\}>0roman_Var { italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( bold_X ) | bold_X âˆˆ caligraphic_C } > 0. When (7) holds for some ğ’ğ’\mathcal{C}caligraphic_C, then with enough samples, CART is able to identify and select a relevant covariate for splitting ğ’ğ’\mathcal{C}caligraphic_C. When (7) hold for all subcubes (i.e., SID), we are able to guarantee that CART is uniformly able to identify and select relevant covariates across every iteration loop of the algorithm, thereby yielding high-dimensional consistency. 1.5 Interpreting CART as a Stochastic Process Despite the simplicity of the lower bound argument outlined in the previous section, making it rigorous in finite samples, even in the special case of the XOR function, has proved elusive until now. To illustrate the difficulty, consider a fixed query point ğ±âˆˆ{Â±1}dğ±superscriptplus-or-minus1ğ‘‘\mathbf{x}\in\{\pm 1\}^{d}bold_x âˆˆ { Â± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT, and consider ğ’¬ğ’¬\mathcal{Q}caligraphic_Q, the root-to-leaf path taken by ğ±ğ±\mathbf{x}bold_x. We have to show that there is a highly probable event on which ğ’¬ğ’¬\mathcal{Q}caligraphic_Q does not split on x1subscriptğ‘¥1x_{1}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT or x2subscriptğ‘¥2x_{2}italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. The usual approach to such a task is to attempt to prove concentration of the split criteria (2) over all nodes (subcubes) along the path. However, because of the data adaptivity of CART splits, we have little control over which subcubes appear, and we would be forced to derive concentration bounds that are uniform over all subcubes. Unfortunately, this desired result is too strong and does not hold. To resolve this, we interpret ğ’¬ğ’¬\mathcal{Q}caligraphic_Q as a stochastic process. The measurement at â€œtime pointâ€ tğ‘¡titalic_t comprises the values Corr2â¡{fâˆ—â¢(ğ—),Xk|ğ—âˆˆğ’t}superscriptCorr2superscriptğ‘“ğ—conditionalsubscriptğ‘‹ğ‘˜ğ—subscriptğ’ğ‘¡\operatorname{Corr}^{2}\left\{f^{*}(\mathbf{X}),X_{k}~{}|~{}\mathbf{X}\in% \mathcal{C}_{t}\right\}roman_Corr start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT { italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( bold_X ) , italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | bold_X âˆˆ caligraphic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT }, k=1â¢â€¦,dğ‘˜1â€¦ğ‘‘k=1\ldots,ditalic_k = 1 â€¦ , italic_d, where ğ’tsubscriptğ’ğ‘¡\mathcal{C}_{t}caligraphic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is the node in ğ’¬ğ’¬\mathcal{Q}caligraphic_Q at depth tğ‘¡titalic_t. We next couple this stochastic process to one that makes totally random splits, which allows us to use symmetry to calculate the probability that it does not split on x1subscriptğ‘¥1x_{1}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT or x2subscriptğ‘¥2x_{2}italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. On the event that the two processes are coupled, this also holds for the CART path ğ’¬ğ’¬\mathcal{Q}caligraphic_Q. Although we have outlined a lower bound for the XOR function, it can be readily adapted to any function not satisfying MSP. Furthermore, there is nothing particularly special about the CART criterion. The proof relies on the fact that splits are determined by considering only the marginal distributions Xk|ğ’,Yconditionalsubscriptğ‘‹ğ‘˜ğ’ğ‘ŒX_{k}|~{}\mathcal{C},Yitalic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | caligraphic_C , italic_Y for k=1,â€¦,dğ‘˜1â€¦ğ‘‘k=1,\ldots,ditalic_k = 1 , â€¦ , italic_d (as opposed to the joint distribution ğ—|ğ’,Yconditionalğ—ğ’ğ‘Œ\mathbf{X}|~{}\mathcal{C},Ybold_X | caligraphic_C , italic_Y) and hence extends to a larger class of greedy recursive partitioning algorithms, which we formally define in Section 2. As far as we are aware, the proof technique is completely novel. 1.6 Robust Lower Bounds under â€œSoftâ€ Bottlenecks Thus far, we have seen that when fâˆ—superscriptğ‘“f^{*}italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT does not satisfy MSP, it experiences marginal signal bottleneck, which leads to high-dimensional inconsistency. In some sense, however, being non-MSP is not a robust property because a small perturbation of the zero coefficients of such a function results in one that does satisfy MSP. As such, our argument regarding the weaknesses of greedy recursive partitioning strategies would be stronger if we could show that their performance degrades smoothly as the regression function fâˆ—superscriptğ‘“f^{*}italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT approaches the class of non-MSP functions. Fortunately, this is indeed the case, as reflected by the following informal theorem. Theorem 1.2 (Informal). When ğ’³={Â±1}dğ’³superscriptplus-or-minus1ğ‘‘\mathcal{X}=\{\pm 1\}^{d}caligraphic_X = { Â± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT, Î½=Unifâ¡({Â±1}d)ğœˆUnifsuperscriptplus-or-minus1ğ‘‘\nu=\operatorname{Unif}\left(\{\pm 1\}^{d}\right)italic_Î½ = roman_Unif ( { Â± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ), fâˆ—superscriptğ‘“f^{*}italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT depends only on sğ‘ sitalic_s covariates, then the expected L2superscriptğ¿2L^{2}italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT risk of the CART estimator f^CARTsubscript^ğ‘“CART\hat{f}_{\operatorname{CART}}over^ start_ARG italic_f end_ARG start_POSTSUBSCRIPT roman_CART end_POSTSUBSCRIPT satisfies Eğ’Ÿnâ¢{Râ¢(f^â¢(âˆ’;ğ’Ÿn),fâˆ—)}=Î©â¢(1)subscriptğ¸subscriptğ’Ÿğ‘›ğ‘…^ğ‘“subscriptğ’Ÿğ‘›superscriptğ‘“Î©1E_{\mathcal{D}_{n}}\left\{R\left(\hat{f}(-;\mathcal{D}_{n}),f^{*}\right)\right% \}=\Omega(1)italic_E start_POSTSUBSCRIPT caligraphic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_POSTSUBSCRIPT { italic_R ( over^ start_ARG italic_f end_ARG ( - ; caligraphic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) , italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ) } = roman_Î© ( 1 ) whenever n=minâ¡{Oâ¢(1/minfâˆ‰MSPâ¡Râ¢(f,fâˆ—)),expâ¡(Oâ¢(d))}ğ‘›ğ‘‚1subscriptğ‘“MSPğ‘…ğ‘“superscriptğ‘“ğ‘‚ğ‘‘n=\min\left\{O\left(1/\min_{f\notin\textsf{MSP}}R(f,f^{*})\right),\exp(O(d))\right\}italic_n = roman_min { italic_O ( 1 / roman_min start_POSTSUBSCRIPT italic_f âˆ‰ MSP end_POSTSUBSCRIPT italic_R ( italic_f , italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ) ) , roman_exp ( italic_O ( italic_d ) ) }. The value, minfâˆ‰MSPâ¡Râ¢(f,fâˆ—)subscriptğ‘“MSPğ‘…ğ‘“superscriptğ‘“\min_{f\notin\textsf{MSP}}R(f,f^{*})roman_min start_POSTSUBSCRIPT italic_f âˆ‰ MSP end_POSTSUBSCRIPT italic_R ( italic_f , italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ), can be thought of as measuring the width of a â€œsoftâ€ bottleneck. For example, if we set fâˆ—â¢(ğ±)=Î±â¢x1+x2+x1â¢x2â¢x3superscriptğ‘“ğ±ğ›¼subscriptğ‘¥1subscriptğ‘¥2subscriptğ‘¥1subscriptğ‘¥2subscriptğ‘¥3f^{*}(\mathbf{x})=\alpha x_{1}+x_{2}+x_{1}x_{2}x_{3}italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( bold_x ) = italic_Î± italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT, then we have minfâˆ‰MSPâ¡Râ¢(f,fâˆ—)=Î±2subscriptğ‘“MSPğ‘…ğ‘“superscriptğ‘“superscriptğ›¼2\min_{f\notin\textsf{MSP}}R(f,f^{*})=\alpha^{2}roman_min start_POSTSUBSCRIPT italic_f âˆ‰ MSP end_POSTSUBSCRIPT italic_R ( italic_f , italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ) = italic_Î± start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. We thus see that the sample complexity lower bound scales inversely with the bottleneck width, thereby extending the performance gap between greedy and ERM trees to a larger range of settings. Finally, we remark that MSP can be re-interpreted in graph theoretic terms as form of connectedness. Under this interpretation, minfâˆ‰MSPâ¡Râ¢(f,fâˆ—)subscriptğ‘“MSPğ‘…ğ‘“superscriptğ‘“\min_{f\notin\textsf{MSP}}R(f,f^{*})roman_min start_POSTSUBSCRIPT italic_f âˆ‰ MSP end_POSTSUBSCRIPT italic_R ( italic_f , italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ) is equal to the weight of the minimum vertex cut. We elaborate upon this perspective in Section 5. 1.7 Comparisons with Neural Networks Trained by SGD In computational learning theory, the problem of â€œlearningâ€ (i.e., achieving small estimation error for) sparse Boolean monomials, also called parities, from i.i.d. noiseless observations (i.e., Îµi=0subscriptğœ€ğ‘–0\varepsilon_{i}=0italic_Îµ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 0 in (1)) is known to be statistically easy but computationally hard (Barak et al., 2022), and hence has served as a useful benchmark for learning algorithms and computational frameworks. Recently, this has become a benchmark for studying neural networks (NNs) trained using SGD, which combined with our results, allows us to perform, to our knowledge, the first theoretical head-to-head comparison between this class of algorithms and greedy regression trees and ensembles. Indeed, the MSP was introduced by Abbe et al. (2022) to generalize Boolean monomials. They showed that in the mean-field regime (very wide two-layer NNs with very small step size, see also Mei et al. (2018)), Câ¢dğ¶ğ‘‘Cditalic_C italic_d iterations of online SGD (one sample per iteration) are sufficient (for some Cğ¶Citalic_C) to achieve small estimation error whenever fâˆ—superscriptğ‘“f^{*}italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT satisfies MSP and is generic but is insufficient (for any Cğ¶Citalic_C) when fâˆ—superscriptğ‘“f^{*}italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT does not satisfy MSP. Intuitively, functions that do not satisfy MSP produce optimization landscapes with saddle points, which mean-field online SGD struggles to escape from. Comparing with Theorem 1.1, this creates an interesting analogy between mean-field online SGD and greedy regression trees and ensembles. On the other hand, later works inadvertently emphasized the differences between greedy regression trees and ensembles and NNs trained outside the mean-field regime and beyond the Oâ¢(d)ğ‘‚ğ‘‘O(d)italic_O ( italic_d ) iteration horizon. In the classification setting, Glasgow (2024) showed that online minibatch SGD on a two-layer NN can learn the XOR function with Î˜~â¢(d)~Î˜ğ‘‘\tilde{\Theta}(d)over~ start_ARG roman_Î˜ end_ARG ( italic_d ) samples and iterations. Kou et al. (2024) showed that for any kğ‘˜kitalic_k, online sign-SGD with a batch size of O~â¢(dkâˆ’1)~ğ‘‚superscriptğ‘‘ğ‘˜1\tilde{O}(d^{k-1})over~ start_ARG italic_O end_ARG ( italic_d start_POSTSUPERSCRIPT italic_k - 1 end_POSTSUPERSCRIPT ) can learn a kğ‘˜kitalic_k-parity (i.e., degree kğ‘˜kitalic_k monomial) within Oâ¢(logâ¡d)ğ‘‚ğ‘‘O(\log d)italic_O ( roman_log italic_d ) iterations, for a total sample complexity of O~â¢(dkâˆ’1)~ğ‘‚superscriptğ‘‘ğ‘˜1\tilde{O}(d^{k-1})over~ start_ARG italic_O end_ARG ( italic_d start_POSTSUPERSCRIPT italic_k - 1 end_POSTSUPERSCRIPT ). Since our lower bound in Theorem 1.1 holds even with noiseless observations, this establishes a rigorous performance gap between the two algorithm classes for some non-MSP functions. Of further interest is Abbe et al. (2023)â€™s conjecture characterizing the sample complexity required to learn any given Boolean function in terms of its â€œleap complexityâ€ (see Appendix J). If this conjecture is true, NNs trained with SGD outperform greedy regression trees and ensembles on all non-MSP functions but perform more poorly in comparison on MSP functions. We discuss these comparisons more elaborately in Appendix J."
https://arxiv.org/html/2411.03393v1,A refined graph container lemma and applications to the hard-core model on bipartite expanders,"We establish a refined version of a graph container lemma due to Galvin and discuss several applications related to the hard-core model on bipartite expander graphs. Given a graph GğºGitalic_G and Î»>0ğœ†0\lambda>0italic_Î» > 0, the hard-core model on GğºGitalic_G at activity Î»ğœ†\lambdaitalic_Î» is the probability distribution Î¼G,Î»subscriptğœ‡ğºğœ†\mu_{G,\lambda}italic_Î¼ start_POSTSUBSCRIPT italic_G , italic_Î» end_POSTSUBSCRIPT on independent sets in GğºGitalic_G given by Î¼G,Î»â¢(I)âˆÎ»|I|proportional-tosubscriptğœ‡ğºğœ†ğ¼superscriptğœ†ğ¼\mu_{G,\lambda}(I)\propto\lambda^{|I|}italic_Î¼ start_POSTSUBSCRIPT italic_G , italic_Î» end_POSTSUBSCRIPT ( italic_I ) âˆ italic_Î» start_POSTSUPERSCRIPT | italic_I | end_POSTSUPERSCRIPT. As one of our main applications, we show that the hard-core model at activity Î»ğœ†\lambdaitalic_Î» on the hypercube Qdsubscriptğ‘„ğ‘‘Q_{d}italic_Q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT exhibits a â€˜structured phaseâ€™ for Î»=Î©â¢(log2â¡d/d1/2)ğœ†Î©superscript2ğ‘‘superscriptğ‘‘12\lambda=\Omega(\log^{2}d/d^{1/2})italic_Î» = roman_Î© ( roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d / italic_d start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT ) in the following sense: in a typical sample from Î¼Qd,Î»subscriptğœ‡subscriptğ‘„ğ‘‘ğœ†\mu_{Q_{d},\lambda}italic_Î¼ start_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_Î» end_POSTSUBSCRIPT, most vertices are contained in one side of the bipartition of Qdsubscriptğ‘„ğ‘‘Q_{d}italic_Q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT. This improves upon a result of Galvin which establishes the same for Î»=Î©â¢(logâ¡d/d1/3)ğœ†Î©ğ‘‘superscriptğ‘‘13\lambda=\Omega(\log d/d^{1/3})italic_Î» = roman_Î© ( roman_log italic_d / italic_d start_POSTSUPERSCRIPT 1 / 3 end_POSTSUPERSCRIPT ). As another application, we establish a fully polynomial-time approximation scheme (FPTAS) for the hard-core model on a dğ‘‘ditalic_d-regular bipartite Î±ğ›¼\alphaitalic_Î±-expander, with Î±>0ğ›¼0\alpha>0italic_Î± > 0 fixed, when Î»=Î©â¢(log2â¡d/d1/2)ğœ†Î©superscript2ğ‘‘superscriptğ‘‘12\lambda=\Omega(\log^{2}d/d^{1/2})italic_Î» = roman_Î© ( roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d / italic_d start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT ). This improves upon the bound Î»=Î©â¢(logâ¡d/d1/4)ğœ†Î©ğ‘‘superscriptğ‘‘14\lambda=\Omega(\log d/d^{1/4})italic_Î» = roman_Î© ( roman_log italic_d / italic_d start_POSTSUPERSCRIPT 1 / 4 end_POSTSUPERSCRIPT ) due to the first author, Perkins and Potukuchi. We discuss similar improvements to results of Galvin-Tetali, Balogh-Garcia-Li and Kronenberg-Spinka.","Given a graph GğºGitalic_G, let â„â¢(G)â„ğº\mathcal{I}(G)caligraphic_I ( italic_G ) denote the collection of independent sets in GğºGitalic_G. The hard-core model on GğºGitalic_G at activity Î»>0ğœ†0\lambda>0italic_Î» > 0 is the probability distribution on â„â¢(G)â„ğº\mathcal{I}(G)caligraphic_I ( italic_G ) given by (1) Î¼G,Î»â¢(I)=Î»|I|ZGâ¢(Î»)subscriptğœ‡ğºğœ†ğ¼superscriptğœ†ğ¼subscriptğ‘ğºğœ†\displaystyle\mu_{G,\lambda}(I)=\frac{\lambda^{|I|}}{Z_{G}(\lambda)}italic_Î¼ start_POSTSUBSCRIPT italic_G , italic_Î» end_POSTSUBSCRIPT ( italic_I ) = divide start_ARG italic_Î» start_POSTSUPERSCRIPT | italic_I | end_POSTSUPERSCRIPT end_ARG start_ARG italic_Z start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_Î» ) end_ARG for Iâˆˆâ„â¢(G)ğ¼â„ğºI\in\mathcal{I}(G)italic_I âˆˆ caligraphic_I ( italic_G ), where the normalising constant (2) ZGâ¢(Î»)=âˆ‘Iâˆˆâ„â¢(G)Î»|I|subscriptğ‘ğºğœ†subscriptğ¼â„ğºsuperscriptğœ†ğ¼\displaystyle Z_{G}(\lambda)=\sum_{I\in\mathcal{I}(G)}\lambda^{|I|}italic_Z start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_Î» ) = âˆ‘ start_POSTSUBSCRIPT italic_I âˆˆ caligraphic_I ( italic_G ) end_POSTSUBSCRIPT italic_Î» start_POSTSUPERSCRIPT | italic_I | end_POSTSUPERSCRIPT is known as the hard-core model partition function. The hard-core model originated in statistical physics as a simple model of a gas. The vertices of the graph GğºGitalic_G are to be thought of as â€˜sitesâ€™ that can be occupied by particles, and neighbouring sites cannot both be occupied. This constraint models a system of particles with â€˜hard coresâ€™ that cannot overlap. In statistical physics, a major motivation for studying the hard-core model is that it provides a setting where the notion of phase transition can be rigorously investigated. In this context, the most common host graph of study is (the nearest neighbour graph on) the integer lattice â„¤dsuperscriptâ„¤ğ‘‘\mathbb{Z}^{d}blackboard_Z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT (see Section 7 for a more precise discussion of phase transitions and hard-core measures on â„¤dsuperscriptâ„¤ğ‘‘\mathbb{Z}^{d}blackboard_Z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT). For now a phase transition can be loosely thought of as follows: as Î»ğœ†\lambdaitalic_Î» increases, a typical sample from the hard-core model on (a large box in) â„¤dsuperscriptâ„¤ğ‘‘\mathbb{Z}^{d}blackboard_Z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT transitions from being disordered to being structured, in the sense that it prefers vertices from either the odd or even sublattice. Motivated by this phenomenon, Kahn [25] initiated the study of the hard-core model on the hypercube Qdsubscriptğ‘„ğ‘‘Q_{d}italic_Q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT (and regular bipartite graphs in general). Here Qdsubscriptğ‘„ğ‘‘Q_{d}italic_Q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT denotes the graph on vertex set {0,1}dsuperscript01ğ‘‘\{0,1\}^{d}{ 0 , 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT where two vertices are adjacent if and only if they have Hamming distance 1111. Qdsubscriptğ‘„ğ‘‘Q_{d}italic_Q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT is a bipartite graph with bipartition â„°âˆªğ’ªâ„°ğ’ª\mathcal{E}\cup\mathcal{O}caligraphic_E âˆª caligraphic_O, where â„°â„°\mathcal{E}caligraphic_E and ğ’ªğ’ª\mathcal{O}caligraphic_O consist of the vertices with even and odd Hamming weight, respectively. Kahn showed that for fixed Î»,Ïµ>0ğœ†italic-Ïµ0\lambda,\epsilon>0italic_Î» , italic_Ïµ > 0 and ğˆğˆ\mathbf{I}bold_I sampled according to Î¼Qd,Î»subscriptğœ‡subscriptğ‘„ğ‘‘ğœ†\mu_{Q_{d},\lambda}italic_Î¼ start_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_Î» end_POSTSUBSCRIPT both ||ğˆ|âˆ’Î»1+Î»â¢2dâˆ’1|â‰¤2dd1âˆ’Ïµğˆğœ†1ğœ†superscript2ğ‘‘1superscript2ğ‘‘superscriptğ‘‘1italic-Ïµ\left||\mathbf{I}|-\frac{\lambda}{1+\lambda}2^{d-1}\right|\leq\frac{2^{d}}{d^{% 1-\epsilon}}| | bold_I | - divide start_ARG italic_Î» end_ARG start_ARG 1 + italic_Î» end_ARG 2 start_POSTSUPERSCRIPT italic_d - 1 end_POSTSUPERSCRIPT | â‰¤ divide start_ARG 2 start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT end_ARG start_ARG italic_d start_POSTSUPERSCRIPT 1 - italic_Ïµ end_POSTSUPERSCRIPT end_ARG and minâ¡{|ğˆâˆ©â„°|,|ğˆâˆ©ğ’ª|}â‰¤2dd1/2âˆ’Ïµğˆâ„°ğˆğ’ªsuperscript2ğ‘‘superscriptğ‘‘12italic-Ïµ\min\{|\mathbf{I}\cap\mathcal{E}|,|\mathbf{I}\cap\mathcal{O}|\}\leq\frac{2^{d}% }{d^{1/2-\epsilon}}roman_min { | bold_I âˆ© caligraphic_E | , | bold_I âˆ© caligraphic_O | } â‰¤ divide start_ARG 2 start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT end_ARG start_ARG italic_d start_POSTSUPERSCRIPT 1 / 2 - italic_Ïµ end_POSTSUPERSCRIPT end_ARG hold whp (that is, with probability tending to 1111 as dâ†’âˆâ†’ğ‘‘d\to\inftyitalic_d â†’ âˆ). Roughly speaking, these results show that for Î»>0ğœ†0\lambda>0italic_Î» > 0 fixed, a sample from Î¼Qd,Î»subscriptğœ‡subscriptğ‘„ğ‘‘ğœ†\mu_{Q_{d},\lambda}italic_Î¼ start_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_Î» end_POSTSUBSCRIPT resembles a random subset of either â„°â„°\mathcal{E}caligraphic_E or ğ’ªğ’ª\mathcal{O}caligraphic_O where each element is chosen independently with probability Î»/(1+Î»)ğœ†1ğœ†\lambda/(1+\lambda)italic_Î» / ( 1 + italic_Î» ). In other words, samples from Î¼Qd,Î»subscriptğœ‡subscriptğ‘„ğ‘‘ğœ†\mu_{Q_{d},\lambda}italic_Î¼ start_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_Î» end_POSTSUBSCRIPT exhibit a significant degree of structure. This lies in stark contrast to the regime Î»â‰¤c/dğœ†ğ‘ğ‘‘\lambda\leq c/ditalic_Î» â‰¤ italic_c / italic_d (cğ‘citalic_c small) where a sample ğˆğˆ\mathbf{I}bold_I from Î¼Qd,Î»subscriptğœ‡subscriptğ‘„ğ‘‘ğœ†\mu_{Q_{d},\lambda}italic_Î¼ start_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_Î» end_POSTSUBSCRIPT resembles a Î»/(1+Î»)ğœ†1ğœ†\lambda/(1+\lambda)italic_Î» / ( 1 + italic_Î» )-random subset of â„°âˆªğ’ªâ„°ğ’ª\mathcal{E}\cup\mathcal{O}caligraphic_E âˆª caligraphic_O; in particular, |ğˆâˆ©â„°|=(1+oâ¢(1))â¢|ğˆâˆ©ğ’ª|ğˆâ„°1ğ‘œ1ğˆğ’ª|\mathbf{I}\cap\mathcal{E}|=(1+o(1))|\mathbf{I}\cap\mathcal{O}|| bold_I âˆ© caligraphic_E | = ( 1 + italic_o ( 1 ) ) | bold_I âˆ© caligraphic_O | whp â€“ see [38]. Galvin [14] later refined Kahnâ€™s results, showing that the structured regime holds all the way down to Î»=Î©~â¢(dâˆ’1/3)ğœ†~Î©superscriptğ‘‘13\lambda=\tilde{\Omega}(d^{-1/3})italic_Î» = over~ start_ARG roman_Î© end_ARG ( italic_d start_POSTSUPERSCRIPT - 1 / 3 end_POSTSUPERSCRIPT ). More precisely, he showed that there exists C>0ğ¶0C>0italic_C > 0 so that if Câ¢logâ¡d/d1/3â‰¤Î»â‰¤2âˆ’1ğ¶ğ‘‘superscriptğ‘‘13ğœ†21C\log d/d^{1/3}\leq\lambda\leq\sqrt{2}-1italic_C roman_log italic_d / italic_d start_POSTSUPERSCRIPT 1 / 3 end_POSTSUPERSCRIPT â‰¤ italic_Î» â‰¤ square-root start_ARG 2 end_ARG - 1 and ğˆğˆ\mathbf{I}bold_I is a sample from Î¼Qd,Î»subscriptğœ‡subscriptğ‘„ğ‘‘ğœ†\mu_{Q_{d},\lambda}italic_Î¼ start_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_Î» end_POSTSUBSCRIPT, then whp ||ğˆ|âˆ’Î»1+Î»â¢2dâˆ’1|â‰¤dâ¢logâ¡dâ¢(21+Î»)dğˆğœ†1ğœ†superscript2ğ‘‘1ğ‘‘ğ‘‘superscript21ğœ†ğ‘‘\left||\mathbf{I}|-\frac{\lambda}{1+\lambda}2^{d-1}\right|\leq d\log d\left(% \frac{2}{1+\lambda}\right)^{d}| | bold_I | - divide start_ARG italic_Î» end_ARG start_ARG 1 + italic_Î» end_ARG 2 start_POSTSUPERSCRIPT italic_d - 1 end_POSTSUPERSCRIPT | â‰¤ italic_d roman_log italic_d ( divide start_ARG 2 end_ARG start_ARG 1 + italic_Î» end_ARG ) start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT and 14â¢logâ¡mâ¢Î»2â¢(21+Î»)dâ‰¤minâ¡{|ğˆâˆ©â„°|,|ğˆâˆ©ğ’ª|}â‰¤eâ¢m2â¢Î»2â¢(21+Î»)d14ğ‘šğœ†2superscript21ğœ†ğ‘‘ğˆâ„°ğˆğ’ªğ‘’superscriptğ‘š2ğœ†2superscript21ğœ†ğ‘‘\frac{1}{4\log m}\frac{\lambda}{2}\left(\frac{2}{1+\lambda}\right)^{d}\leq\min% \{|\mathbf{I}\cap\mathcal{E}|,|\mathbf{I}\cap\mathcal{O}|\}\leq em^{2}\frac{% \lambda}{2}\left(\frac{2}{1+\lambda}\right)^{d}\,divide start_ARG 1 end_ARG start_ARG 4 roman_log italic_m end_ARG divide start_ARG italic_Î» end_ARG start_ARG 2 end_ARG ( divide start_ARG 2 end_ARG start_ARG 1 + italic_Î» end_ARG ) start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT â‰¤ roman_min { | bold_I âˆ© caligraphic_E | , | bold_I âˆ© caligraphic_O | } â‰¤ italic_e italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT divide start_ARG italic_Î» end_ARG start_ARG 2 end_ARG ( divide start_ARG 2 end_ARG start_ARG 1 + italic_Î» end_ARG ) start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT for some m=mâ¢(Î»,d)=oâ¢(d/logâ¡d)ğ‘šğ‘šğœ†ğ‘‘ğ‘œğ‘‘ğ‘‘m=m(\lambda,d)=o(d/\sqrt{\log d})italic_m = italic_m ( italic_Î» , italic_d ) = italic_o ( italic_d / square-root start_ARG roman_log italic_d end_ARG ). Similar bounds hold when Î»>2âˆ’1ğœ†21\lambda>\sqrt{2}-1italic_Î» > square-root start_ARG 2 end_ARG - 1, but they take a slightly different form (see [14, Theorem 1.1]). Galvinâ€™s proof is based on Sapozhenkoâ€™s graph container method [34] (see Section 1.1 for more on the container method). This influential method has now enjoyed numerous applications in the combinatorics literature. The restriction that Î»=Î©~â¢(dâˆ’1/3)ğœ†~Î©superscriptğ‘‘13\lambda=\tilde{\Omega}(d^{-1/3})italic_Î» = over~ start_ARG roman_Î© end_ARG ( italic_d start_POSTSUPERSCRIPT - 1 / 3 end_POSTSUPERSCRIPT ) in Galvinâ€™s result is an artifact of a graph container lemma and as a consequence, the same restriction appears in many other applications of similar lemmas [1, 17, 23, 24, 30]. Galvinâ€™s results [14] were extended by the first author and Perkins [22], who combined the graph container method with a method based on the theory of polymer models and cluster expansion from statistical physics. This allows for a very precise description of the hard-core measure on Qdsubscriptğ‘„ğ‘‘Q_{d}italic_Q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT as a â€˜perturbationâ€™ of the measure which selects a side â„°,ğ’ªâ„°ğ’ª\mathcal{E},\mathcal{O}caligraphic_E , caligraphic_O uniformly at random and then selects a Î»/(1+Î»)ğœ†1ğœ†\lambda/(1+\lambda)italic_Î» / ( 1 + italic_Î» )-subset of that side. As a consequence one can obtain detailed asymptotics for the partition function ZQdâ¢(Î»)subscriptğ‘subscriptğ‘„ğ‘‘ğœ†Z_{Q_{d}}(\lambda)italic_Z start_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_Î» ) and determine the asymptotic distribution of |ğˆ|ğˆ|\mathbf{I}|| bold_I | and minâ¡{|ğˆâˆ©â„°|,|ğˆâˆ©ğ’ª|}ğˆâ„°ğˆğ’ª\min\{|\mathbf{I}\cap\mathcal{E}|,|\mathbf{I}\cap\mathcal{O}|\}roman_min { | bold_I âˆ© caligraphic_E | , | bold_I âˆ© caligraphic_O | }. These results rely on Galvinâ€™s container lemma and are therefore also limited to the regime Î»=Î©~â¢(dâˆ’1/3)ğœ†~Î©superscriptğ‘‘13\lambda=\tilde{\Omega}(d^{-1/3})italic_Î» = over~ start_ARG roman_Î© end_ARG ( italic_d start_POSTSUPERSCRIPT - 1 / 3 end_POSTSUPERSCRIPT ). Here we prove a refined graph container lemma (Lemma 1.2 below), which allows us to extend these structure theorems to the range Î»=Î©~â¢(dâˆ’1/2)ğœ†~Î©superscriptğ‘‘12\lambda=\tilde{\Omega}(d^{-1/2})italic_Î» = over~ start_ARG roman_Î© end_ARG ( italic_d start_POSTSUPERSCRIPT - 1 / 2 end_POSTSUPERSCRIPT ). One can make similar improvements to several other applications of the graph container method and we discuss these further applications in Section 1.2. Theorem 1.1. There exists C>0ğ¶0C>0italic_C > 0 so that the following holds. Let Câ¢log2â¡d/d1/2â‰¤Î»â‰¤2âˆ’1ğ¶superscript2ğ‘‘superscriptğ‘‘12ğœ†21C\log^{2}d/d^{1/2}\leq\lambda\leq\sqrt{2}-1italic_C roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d / italic_d start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT â‰¤ italic_Î» â‰¤ square-root start_ARG 2 end_ARG - 1 and let ğˆğˆ\mathbf{I}bold_I be sampled according to Î¼Qd,Î»subscriptğœ‡subscriptğ‘„ğ‘‘ğœ†\mu_{Q_{d},\lambda}italic_Î¼ start_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_Î» end_POSTSUBSCRIPT. Then, with high probability, (3) ||ğˆ|âˆ’Î»1+Î»â¢2dâˆ’1|â‰¤Ï‰â¢(1)â‹…dâ¢Î»2â¢2d(1+Î»)dğˆğœ†1ğœ†superscript2ğ‘‘1â‹…ğœ”1ğ‘‘superscriptğœ†2superscript2ğ‘‘superscript1ğœ†ğ‘‘\left||\mathbf{I}|-\frac{\lambda}{1+\lambda}2^{d-1}\right|\leq\omega(1)\cdot% \frac{d\lambda^{2}2^{d}}{(1+\lambda)^{d}}| | bold_I | - divide start_ARG italic_Î» end_ARG start_ARG 1 + italic_Î» end_ARG 2 start_POSTSUPERSCRIPT italic_d - 1 end_POSTSUPERSCRIPT | â‰¤ italic_Ï‰ ( 1 ) â‹… divide start_ARG italic_d italic_Î» start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT 2 start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT end_ARG start_ARG ( 1 + italic_Î» ) start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT end_ARG and (4) minâ¡{|ğˆâˆ©â„°|,|ğˆâˆ©ğ’ª|}=(1+oâ¢(1))â¢Î»2â‹…(21+Î»)dğˆâ„°ğˆğ’ªâ‹…1ğ‘œ1ğœ†2superscript21ğœ†ğ‘‘\min\{|\mathbf{I}\cap\mathcal{E}|,|\mathbf{I}\cap\mathcal{O}|\}=(1+o(1))\frac{% \lambda}{2}\cdot\left(\frac{2}{1+\lambda}\right)^{d}roman_min { | bold_I âˆ© caligraphic_E | , | bold_I âˆ© caligraphic_O | } = ( 1 + italic_o ( 1 ) ) divide start_ARG italic_Î» end_ARG start_ARG 2 end_ARG â‹… ( divide start_ARG 2 end_ARG start_ARG 1 + italic_Î» end_ARG ) start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT where Ï‰â¢(1)ğœ”1\omega(1)italic_Ï‰ ( 1 ) is any function tending to infinity as dâ†’âˆâ†’ğ‘‘d\rightarrow\inftyitalic_d â†’ âˆ. We reiterate that the regime Î»â‰¥Câ¢logâ¡d/d1/3ğœ†ğ¶ğ‘‘superscriptğ‘‘13\lambda\geq C\log d/d^{1/3}italic_Î» â‰¥ italic_C roman_log italic_d / italic_d start_POSTSUPERSCRIPT 1 / 3 end_POSTSUPERSCRIPT was already treated in detail in [14, 22] and the regime Î»>2âˆ’1ğœ†21\lambda>\sqrt{2}-1italic_Î» > square-root start_ARG 2 end_ARG - 1 was treated in [14]. Theorem 1.1 shows that the â€˜structured regimeâ€™, where a typical independent set is imbalanced, persists all the way down to Î»=Î©~â¢(dâˆ’1/2)ğœ†~Î©superscriptğ‘‘12\lambda=\tilde{\Omega}(d^{-1/2})italic_Î» = over~ start_ARG roman_Î© end_ARG ( italic_d start_POSTSUPERSCRIPT - 1 / 2 end_POSTSUPERSCRIPT ). Theorem 1.1 is an easy consequence of a detailed description of Î¼Qd,Î»subscriptğœ‡subscriptğ‘„ğ‘‘ğœ†\mu_{Q_{d},\lambda}italic_Î¼ start_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_Î» end_POSTSUBSCRIPT (see Theorem 5.3 below) analogous to the one obtained in [22]. This structure theorem has several other consequences (many of which are elaborated upon in [22]), but here we focus on Theorem 1.1 as our main application for brevity. We describe our new graph container lemma (Lemma 1.2) in the following section, which is our main technical contribution. We then go on to discuss further applications of the lemma. 1.1. An improved graph container lemma The container method is a classical tool that has seen widespread use in the context of studying independent sets in graphs. Its roots can be traced back to the work of Kleitman and Winston [26] and Sapozhenko [34]. In recent years, the method has been generalized and developed into a powerful approach for studying independent sets in hypergraphs in the celebrated work of Balogh, Morris and Samotij [3] and Saxton and Thomason [35]. This method has enjoyed a wealth of applications in extremal, enumerative and probabilistic combinatorics and beyond. In this paper, we are interested in the graph container method specialized to the case of bipartite graphs Î£=XâˆªYÎ£ğ‘‹ğ‘Œ\Sigma=X\cup Yroman_Î£ = italic_X âˆª italic_Y. A first result of this type was established by Sapozhenko [34]. Sapozhenkoâ€™s method was elaborated upon by Galvin [15] and the form of our container lemma is closely modelled after his results (see also [14, 17]). These results have seen numerous applications which we discuss further in Section 1.2. We formulate our new container lemma in enough generality to encompass all of the applications outlined in Section 1.2 (not just Theorem 1.1) and with a view to future applications. The statement is somewhat technical and so we first set up some notation. For Î´â‰¥1ğ›¿1\delta\geq 1italic_Î´ â‰¥ 1 and dYâ‰¤dXsubscriptğ‘‘ğ‘Œsubscriptğ‘‘ğ‘‹d_{Y}\leq d_{X}italic_d start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT â‰¤ italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT, we say a bipartite graph Î£=XâŠ”YÎ£square-unionğ‘‹ğ‘Œ\Sigma=X\sqcup Yroman_Î£ = italic_X âŠ” italic_Y is Î´ğ›¿\deltaitalic_Î´-approximately (dX,dY)subscriptğ‘‘ğ‘‹subscriptğ‘‘ğ‘Œ(d_{X},d_{Y})( italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT )-biregular if it satisfies dâ¢(v)âˆˆ{[dX,Î´â¢dX]âˆ€vâˆˆX,[Î´âˆ’1â¢dY,dY]âˆ€vâˆˆYğ‘‘ğ‘£casessubscriptğ‘‘ğ‘‹ğ›¿subscriptğ‘‘ğ‘‹for-allğ‘£ğ‘‹superscriptğ›¿1subscriptğ‘‘ğ‘Œsubscriptğ‘‘ğ‘Œfor-allğ‘£ğ‘Œd(v)\in\begin{cases}[d_{X},\delta d_{X}]&\forall v\in X,\\ [\delta^{-1}d_{Y},d_{Y}]&\forall v\in Y\end{cases}italic_d ( italic_v ) âˆˆ { start_ROW start_CELL [ italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT , italic_Î´ italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT ] end_CELL start_CELL âˆ€ italic_v âˆˆ italic_X , end_CELL end_ROW start_ROW start_CELL [ italic_Î´ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT ] end_CELL start_CELL âˆ€ italic_v âˆˆ italic_Y end_CELL end_ROW where dâ¢(v)ğ‘‘ğ‘£d(v)italic_d ( italic_v ) denotes the degree of vğ‘£vitalic_v. For the rest of the paper, we assume Î£Î£\Sigmaroman_Î£ is Î´ğ›¿\deltaitalic_Î´-approximately (dX,dY)subscriptğ‘‘ğ‘‹subscriptğ‘‘ğ‘Œ(d_{X},d_{Y})( italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT )-biregular. For AâŠ†Xğ´ğ‘‹A\subseteq Xitalic_A âŠ† italic_X, the closure of Ağ´Aitalic_A is defined to be [A]:={xâˆˆX:Nâ¢(x)âŠ†Nâ¢(A)}.assigndelimited-[]ğ´conditional-setğ‘¥ğ‘‹ğ‘ğ‘¥ğ‘ğ´[A]:=\{x\in X:N(x)\subseteq N(A)\}.[ italic_A ] := { italic_x âˆˆ italic_X : italic_N ( italic_x ) âŠ† italic_N ( italic_A ) } . A set SâŠ†Vâ¢(Î£)ğ‘†ğ‘‰Î£S\subseteq V(\Sigma)italic_S âŠ† italic_V ( roman_Î£ ) is 2-linked if Î£2â¢[S]superscriptÎ£2delimited-[]ğ‘†\Sigma^{2}[S]roman_Î£ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT [ italic_S ] is connected, where Î£2superscriptÎ£2\Sigma^{2}roman_Î£ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT is the square of Î£Î£\Sigmaroman_Î£ (i.e. Vâ¢(Î£2)=Vâ¢(Î£)ğ‘‰superscriptÎ£2ğ‘‰Î£V(\Sigma^{2})=V(\Sigma)italic_V ( roman_Î£ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) = italic_V ( roman_Î£ ) and two vertices x,yğ‘¥ğ‘¦x,yitalic_x , italic_y are adjacent in Î£2superscriptÎ£2\Sigma^{2}roman_Î£ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT iff their distance in Î£Î£\Sigmaroman_Î£ is at most 2). Given a,gâˆˆâ„•ğ‘ğ‘”â„•a,g\in\mathbb{N}italic_a , italic_g âˆˆ blackboard_N, define (5) ğ’¢â¢(a,g)=ğ’¢â¢(a,g,Î£)={AâŠ†Xâ¢ 2-linked :|[A]|=aâ¢ and â¢|Nâ¢(A)|=g},ğ’¢ğ‘ğ‘”ğ’¢ğ‘ğ‘”Î£conditional-setğ´ğ‘‹ 2-linked delimited-[]ğ´ğ‘ and ğ‘ğ´ğ‘”\displaystyle\mathcal{G}(a,g)=\mathcal{G}(a,g,\Sigma)=\{A\subseteq X\text{ 2-% linked }:|[A]|=a\text{ and }|N(A)|=g\},caligraphic_G ( italic_a , italic_g ) = caligraphic_G ( italic_a , italic_g , roman_Î£ ) = { italic_A âŠ† italic_X 2-linked : | [ italic_A ] | = italic_a and | italic_N ( italic_A ) | = italic_g } , and set tâ‰”gâˆ’aâ‰”ğ‘¡ğ‘”ğ‘t\coloneqq g-aitalic_t â‰” italic_g - italic_a and wâ‰”gâ¢dYâˆ’aâ¢dXâ‰”ğ‘¤ğ‘”subscriptğ‘‘ğ‘Œğ‘subscriptğ‘‘ğ‘‹w\coloneqq gd_{Y}-ad_{X}italic_w â‰” italic_g italic_d start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT - italic_a italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT. The technical-looking definition below is a measurement of the expansion of Î£Î£\Sigmaroman_Î£. Given 1â‰¤Ï†â‰¤Î´âˆ’1â¢dYâˆ’11ğœ‘superscriptğ›¿1subscriptğ‘‘ğ‘Œ11\leq\varphi\leq\delta^{-1}d_{Y}-11 â‰¤ italic_Ï† â‰¤ italic_Î´ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT - 1, let mÏ†=mÏ†â¢(Î£)=minâ¡{|Nâ¢(K)|:yâˆˆY,KâŠ†Nâ¢(y),|K|>Ï†}.subscriptğ‘šğœ‘subscriptğ‘šğœ‘Î£:ğ‘ğ¾formulae-sequenceğ‘¦ğ‘Œformulae-sequenceğ¾ğ‘ğ‘¦ğ¾ğœ‘m_{\varphi}=m_{\varphi}(\Sigma)=\min\{|N(K)|:y\in Y,K\subseteq N(y),|K|>% \varphi\}.italic_m start_POSTSUBSCRIPT italic_Ï† end_POSTSUBSCRIPT = italic_m start_POSTSUBSCRIPT italic_Ï† end_POSTSUBSCRIPT ( roman_Î£ ) = roman_min { | italic_N ( italic_K ) | : italic_y âˆˆ italic_Y , italic_K âŠ† italic_N ( italic_y ) , | italic_K | > italic_Ï† } . The proof of Lemma 1.2 crucially relies on the expansion of Î£Î£\Sigmaroman_Î£, and the lower bounds on mÏ†subscriptğ‘šğœ‘m_{\varphi}italic_m start_POSTSUBSCRIPT italic_Ï† end_POSTSUBSCRIPT and gâˆ’ağ‘”ğ‘g-aitalic_g - italic_a below provide quantification of the expansion that we need. All the graphs that we consider in our applications will satisfy these lower bounds (except in Theorem 1.5, where we require a stronger bound on gâˆ’ağ‘”ğ‘g-aitalic_g - italic_a and we relax the assumption on mÏ†subscriptğ‘šğœ‘m_{\varphi}italic_m start_POSTSUBSCRIPT italic_Ï† end_POSTSUBSCRIPT). Lemma 1.2. Let dXsubscriptğ‘‘ğ‘‹d_{X}italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT and dYsubscriptğ‘‘ğ‘Œd_{Y}italic_d start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT be sufficiently large integers and let Î´â‰¥1ğ›¿1\delta\geq 1italic_Î´ â‰¥ 1, Î´â€²,Î´â€²â€²>0superscriptğ›¿â€²superscriptğ›¿â€²â€²0\delta^{\prime},\delta^{\prime\prime}>0italic_Î´ start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT , italic_Î´ start_POSTSUPERSCRIPT â€² â€² end_POSTSUPERSCRIPT > 0. Then there exist c=câ¢(Î´,Î´â€²,Î´â€²â€²)>0ğ‘ğ‘ğ›¿superscriptğ›¿â€²superscriptğ›¿â€²â€²0c=c(\delta,\delta^{\prime},\delta^{\prime\prime})>0italic_c = italic_c ( italic_Î´ , italic_Î´ start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT , italic_Î´ start_POSTSUPERSCRIPT â€² â€² end_POSTSUPERSCRIPT ) > 0 and C=Câ¢(Î´,Î´â€²,Î´â€²â€²)>0ğ¶ğ¶ğ›¿superscriptğ›¿â€²superscriptğ›¿â€²â€²0C=C(\delta,\delta^{\prime},\delta^{\prime\prime})>0italic_C = italic_C ( italic_Î´ , italic_Î´ start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT , italic_Î´ start_POSTSUPERSCRIPT â€² â€² end_POSTSUPERSCRIPT ) > 0 such that the following holds. Let Î£=XâŠ”YÎ£square-unionğ‘‹ğ‘Œ\Sigma=X\sqcup Yroman_Î£ = italic_X âŠ” italic_Y be Î´ğ›¿\deltaitalic_Î´-approximately (dX,dY)subscriptğ‘‘ğ‘‹subscriptğ‘‘ğ‘Œ(d_{X},d_{Y})( italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT )-biregular such that mÏ†â‰¥Î´â€²â€²â‹…(Ï†â¢dX)subscriptğ‘šğœ‘â‹…superscriptğ›¿â€²â€²ğœ‘subscriptğ‘‘ğ‘‹{m_{\varphi}}\geq\delta^{\prime\prime}\cdot(\varphi d_{X})italic_m start_POSTSUBSCRIPT italic_Ï† end_POSTSUBSCRIPT â‰¥ italic_Î´ start_POSTSUPERSCRIPT â€² â€² end_POSTSUPERSCRIPT â‹… ( italic_Ï† italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT ), where Ï†=dY/(2â¢Î´)ğœ‘subscriptğ‘‘ğ‘Œ2ğ›¿\varphi=d_{Y}/(2\delta)italic_Ï† = italic_d start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT / ( 2 italic_Î´ ). If a,gâˆˆâ„•ğ‘ğ‘”â„•a,g\in\mathbb{N}italic_a , italic_g âˆˆ blackboard_N satisfy gâˆ’aâ‰¥maxâ¡{Î´â€²â¢gdY,câ¢dX(logâ¡dX)2}ğ‘”ğ‘superscriptğ›¿â€²ğ‘”subscriptğ‘‘ğ‘Œğ‘subscriptğ‘‘ğ‘‹superscriptsubscriptğ‘‘ğ‘‹2g-a\geq\max\left\{\frac{\delta^{\prime}g}{d_{Y}},\frac{cd_{X}}{(\log d_{X})^{2% }}\right\}italic_g - italic_a â‰¥ roman_max { divide start_ARG italic_Î´ start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT italic_g end_ARG start_ARG italic_d start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT end_ARG , divide start_ARG italic_c italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT end_ARG start_ARG ( roman_log italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG } and Î»>Câ¢log2â¡dX(dX)1/2ğœ†ğ¶superscript2subscriptğ‘‘ğ‘‹superscriptsubscriptğ‘‘ğ‘‹12\lambda>\frac{C\log^{2}d_{X}}{(d_{X})^{1/2}}italic_Î» > divide start_ARG italic_C roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT end_ARG start_ARG ( italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT end_ARG, then (6) âˆ‘Aâˆˆğ’¢â¢(a,g)Î»|A|â‰¤|Y|â¢(1+Î»)gâ¢eâˆ’(gâˆ’a)â¢log2â¡dX/(6â¢dX).subscriptğ´ğ’¢ğ‘ğ‘”superscriptğœ†ğ´ğ‘Œsuperscript1ğœ†ğ‘”superscriptğ‘’ğ‘”ğ‘superscript2subscriptğ‘‘ğ‘‹6subscriptğ‘‘ğ‘‹\sum_{A\in\mathcal{G}(a,g)}\lambda^{|A|}\leq|Y|(1+\lambda)^{g}e^{-(g-a)\log^{2% }d_{X}/(6d_{X})}.âˆ‘ start_POSTSUBSCRIPT italic_A âˆˆ caligraphic_G ( italic_a , italic_g ) end_POSTSUBSCRIPT italic_Î» start_POSTSUPERSCRIPT | italic_A | end_POSTSUPERSCRIPT â‰¤ | italic_Y | ( 1 + italic_Î» ) start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT italic_e start_POSTSUPERSCRIPT - ( italic_g - italic_a ) roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT / ( 6 italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT ) end_POSTSUPERSCRIPT . We note that the main contribution of the above lemma is the improved lower bound on Î»ğœ†\lambdaitalic_Î», which was previously (essentially111Galvin considers regular graphs only, so that dX=dY=dsubscriptğ‘‘ğ‘‹subscriptğ‘‘ğ‘Œğ‘‘d_{X}=d_{Y}=ditalic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT = italic_d start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT = italic_d.) Î©~â¢(dXâˆ’1/3)~Î©superscriptsubscriptğ‘‘ğ‘‹13\tilde{\Omega}(d_{X}^{-1/3})over~ start_ARG roman_Î© end_ARG ( italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 / 3 end_POSTSUPERSCRIPT ) in [14]. As is typical with existing results of this type, the proof of Lemma 1.2 consists of two parts: an algorithmic procedure for constructing graph containers efficiently, and a â€˜reconstructionâ€™ argument that allows us to bound the sum on the left hand-side of (6) given the family of containers we have constructed. The main driving force behind our improvement in the range of Î»ğœ†\lambdaitalic_Î» is a novel approach to the container construction algorithm, Lemma 2.4. We conjecture that a bound of the type given in Lemma 1.2 should hold for Î»=Î©~â¢(1/dX)ğœ†~Î©1subscriptğ‘‘ğ‘‹\lambda=\tilde{\Omega}(1/d_{X})italic_Î» = over~ start_ARG roman_Î© end_ARG ( 1 / italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT ). Conjecture 1.3. There exists a constant Îºğœ…\kappaitalic_Îº and a function Î»âˆ—:â„•â†’â„:superscriptğœ†âˆ—â†’â„•â„\lambda^{\ast}:\mathbb{N}\to\mathbb{R}italic_Î» start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT : blackboard_N â†’ blackboard_R with Î»âˆ—â¢(d)=Î©~â¢(1/d)superscriptğœ†âˆ—ğ‘‘~Î©1ğ‘‘\lambda^{\ast}(d)=\tilde{\Omega}(1/d)italic_Î» start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_d ) = over~ start_ARG roman_Î© end_ARG ( 1 / italic_d ) as dâ†’âˆâ†’ğ‘‘d\to\inftyitalic_d â†’ âˆ such that, under the assumptions of Lemma 1.2, if Î»â‰¥Î»âˆ—â¢(dX)ğœ†superscriptğœ†âˆ—subscriptğ‘‘ğ‘‹\lambda\geq\lambda^{\ast}(d_{X})italic_Î» â‰¥ italic_Î» start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT ), then âˆ‘Aâˆˆğ’¢â¢(a,g)Î»|A|â‰¤|Y|â¢(1+Î»)gâ¢expâ¡{âˆ’(gâˆ’a)/dXÎº}.subscriptğ´ğ’¢ğ‘ğ‘”superscriptğœ†ğ´ğ‘Œsuperscript1ğœ†ğ‘”ğ‘”ğ‘superscriptsubscriptğ‘‘ğ‘‹ğœ…\sum_{A\in\mathcal{G}(a,g)}\lambda^{|A|}\leq|Y|(1+\lambda)^{g}\exp\left\{-(g-a% )/d_{X}^{\kappa}\right\}.âˆ‘ start_POSTSUBSCRIPT italic_A âˆˆ caligraphic_G ( italic_a , italic_g ) end_POSTSUBSCRIPT italic_Î» start_POSTSUPERSCRIPT | italic_A | end_POSTSUPERSCRIPT â‰¤ | italic_Y | ( 1 + italic_Î» ) start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT roman_exp { - ( italic_g - italic_a ) / italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_Îº end_POSTSUPERSCRIPT } . Remark 1.4. We note that the assumption gâˆ’a=Î©â¢(g/dY)ğ‘”ğ‘Î©ğ‘”subscriptğ‘‘ğ‘Œg-a=\Omega(g/d_{Y})italic_g - italic_a = roman_Î© ( italic_g / italic_d start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT ) in Lemma 1.2 can be relaxed to gâˆ’a=Î©â¢(gâ¢log2â¡dXdXâ¢dYâ¢Î»)ğ‘”ğ‘Î©ğ‘”superscript2subscriptğ‘‘ğ‘‹subscriptğ‘‘ğ‘‹subscriptğ‘‘ğ‘Œğœ†g-a=\Omega(\frac{g\log^{2}d_{X}}{\sqrt{d_{X}}d_{Y}\lambda})italic_g - italic_a = roman_Î© ( divide start_ARG italic_g roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT end_ARG italic_d start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT italic_Î» end_ARG ) (which is weaker for Î»â‰«log2â¡dX(dX)1/2much-greater-thanğœ†superscript2subscriptğ‘‘ğ‘‹superscriptsubscriptğ‘‘ğ‘‹12\lambda\gg\frac{\log^{2}d_{X}}{(d_{X})^{1/2}}italic_Î» â‰« divide start_ARG roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT end_ARG start_ARG ( italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT end_ARG) as long as Î»ğœ†\lambdaitalic_Î» grows at most polynomially fast in dXsubscriptğ‘‘ğ‘‹d_{X}italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT. We stick to the simpler lower bound g/dYğ‘”subscriptğ‘‘ğ‘Œg/d_{Y}italic_g / italic_d start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT for simplicity, as this lower bound is enough for all of the current applications. 1.2. Further applications Approximation algorithms for the hard-core model on expanders Given Z,Z^,Ïµ>0ğ‘^ğ‘italic-Ïµ0Z,\hat{Z},\epsilon>0italic_Z , over^ start_ARG italic_Z end_ARG , italic_Ïµ > 0, we say that Z^^ğ‘\hat{Z}over^ start_ARG italic_Z end_ARG is an Ïµitalic-Ïµ\epsilonitalic_Ïµ-relative approximation to Zğ‘Zitalic_Z if eâˆ’Ïµâ¢Zâ‰¤Z^â‰¤eÏµâ¢Zsuperscriptğ‘’italic-Ïµğ‘^ğ‘superscriptğ‘’italic-Ïµğ‘e^{-\epsilon}Z\leq\hat{Z}\leq e^{\epsilon}Zitalic_e start_POSTSUPERSCRIPT - italic_Ïµ end_POSTSUPERSCRIPT italic_Z â‰¤ over^ start_ARG italic_Z end_ARG â‰¤ italic_e start_POSTSUPERSCRIPT italic_Ïµ end_POSTSUPERSCRIPT italic_Z. Two natural computational tasks arise when considering the hard-core model on GğºGitalic_G at activity Î»ğœ†\lambdaitalic_Î»: (1) Compute an Ïµitalic-Ïµ\epsilonitalic_Ïµ-relative approximation to ZGâ¢(Î»)subscriptğ‘ğºğœ†Z_{G}(\lambda)italic_Z start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_Î» ); (2) Output an independent set with distribution Î¼^G,Î»subscript^ğœ‡ğºğœ†\hat{\mu}_{G,\lambda}over^ start_ARG italic_Î¼ end_ARG start_POSTSUBSCRIPT italic_G , italic_Î» end_POSTSUBSCRIPT such that â€–Î¼^G,Î»âˆ’Î¼G,Î»â€–Tâ¢Vâ‰¤Ïµsubscriptnormsubscript^ğœ‡ğºğœ†subscriptğœ‡ğºğœ†ğ‘‡ğ‘‰italic-Ïµ\|\hat{\mu}_{G,\lambda}-\mu_{G,\lambda}\|_{TV}\leq\epsilonâˆ¥ over^ start_ARG italic_Î¼ end_ARG start_POSTSUBSCRIPT italic_G , italic_Î» end_POSTSUBSCRIPT - italic_Î¼ start_POSTSUBSCRIPT italic_G , italic_Î» end_POSTSUBSCRIPT âˆ¥ start_POSTSUBSCRIPT italic_T italic_V end_POSTSUBSCRIPT â‰¤ italic_Ïµ. A deterministic algorithm which does Task 1 in time polynomial in nğ‘›nitalic_n and 1/Ïµ1italic-Ïµ1/\epsilon1 / italic_Ïµ is known as a fully polynomial time approximation scheme (FPTAS). An algorithm which does Task 2 is known as an efficient sampling scheme. Intuitively, one might expect the problem of approximating ZGâ¢(Î»)subscriptğ‘ğºğœ†Z_{G}(\lambda)italic_Z start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_Î» ) to be easier on the class of bipartite graphs; for one, there is a polynomial-time algorithm to find a maximum-size independent set in a bipartite graph while the corresponding problem is NP-hard for general graphs. The problem of approximating ZGâ¢(Î»)subscriptğ‘ğºğœ†Z_{G}(\lambda)italic_Z start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_Î» ) for bipartite GğºGitalic_G belongs to the complexity class #BIS introduced by Dyer, Goldberg, Greenhill, and Jerrum [9]. They showed that several natural combinatorial counting problems are as hard to approximate as #BIS. Resolving the complexity of #BIS remains a major open problem and in recent years there has been an effort to design approximation algorithms for ZGâ¢(Î»)subscriptğ‘ğºğœ†Z_{G}(\lambda)italic_Z start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_Î» ) that exploit bipartite structure. The line of work most relevant here is that which followed the breakthrough of Helmuth, Perkins and Regts [19] who designed efficient approximation algorithms for ZGâ¢(Î»)subscriptğ‘ğºğœ†Z_{G}(\lambda)italic_Z start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_Î» ) on â„¤dsuperscriptâ„¤ğ‘‘\mathbb{Z}^{d}blackboard_Z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT in the previously intractable â€˜low temperatureâ€™ (i.e. large Î»ğœ†\lambdaitalic_Î») regime. Their method was based on tools from statistical physics, namely Pirogov-Sinai Theory, polymer models and cluster expansions. Soon after the first author, Keevash and Perkins [20] gave an FPTAS and efficient sampling scheme for the low-temperature hard-core model in bounded-degree, bipartite expander graphs. This work was followed by several improvements, extensions, and generalizations including [5, 6, 7, 10, 11, 12, 31]. Many of these algorithms exploit the fact that on a bipartite graph GğºGitalic_G with sufficient expansion, typical samples from Î¼G,Î»subscriptğœ‡ğºğœ†\mu_{G,\lambda}italic_Î¼ start_POSTSUBSCRIPT italic_G , italic_Î» end_POSTSUBSCRIPT are imbalanced (preferring one side of the bipartition to the other). The container method is a particularly powerful tool for detecting such structure. This fact was exploited by the first author, Perkins and Potukuchi [24] who combined the cluster expansion method with graph containers to extend the range of Î»ğœ†\lambdaitalic_Î» for which efficient approximation algorithms on bipartite expanders were known to exist. Our new container lemma (Lemma 1.2) can be used to extend the range further still. Given Î±>0ğ›¼0\alpha>0italic_Î± > 0, we say that a bipartite graph with parts X,Yğ‘‹ğ‘ŒX,Yitalic_X , italic_Y is a bipartite Î±ğ›¼\alphaitalic_Î±-expander if |Nâ¢(A)|â‰¥(1+Î±)â¢|A|ğ‘ğ´1ğ›¼ğ´|N(A)|\geq(1+\alpha)|A|| italic_N ( italic_A ) | â‰¥ ( 1 + italic_Î± ) | italic_A | for all AâŠ†Xğ´ğ‘‹A\subseteq Xitalic_A âŠ† italic_X with |A|<|X|/2ğ´ğ‘‹2|A|<|X|/2| italic_A | < | italic_X | / 2 and AâŠ†Yğ´ğ‘ŒA\subseteq Yitalic_A âŠ† italic_Y with |A|<|Y|/2ğ´ğ‘Œ2|A|<|Y|/2| italic_A | < | italic_Y | / 2. Theorem 1.5. For every Î±>0ğ›¼0\alpha>0italic_Î± > 0 there exists a constant C>0ğ¶0C>0italic_C > 0 such that for all sufficiently large dğ‘‘ditalic_d and Î»â‰¥Câ¢log2â¡dd1/2ğœ†ğ¶superscript2ğ‘‘superscriptğ‘‘12\lambda\geq\frac{C\log^{2}d}{d^{1/2}}italic_Î» â‰¥ divide start_ARG italic_C roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d end_ARG start_ARG italic_d start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT end_ARG there is an FPTAS for ZGâ¢(Î»)subscriptğ‘ğºğœ†Z_{G}(\lambda)italic_Z start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_Î» ) and a polynomial-time sampling scheme for Î¼G,Î»subscriptğœ‡ğºğœ†\mu_{G,\lambda}italic_Î¼ start_POSTSUBSCRIPT italic_G , italic_Î» end_POSTSUBSCRIPT for the class of dğ‘‘ditalic_d-regular, bipartite Î±ğ›¼\alphaitalic_Î±-expanders. The above result extends [24, Theorem 2], which assumes that Î»=Î©~â¢(dâˆ’1/4)ğœ†~Î©superscriptğ‘‘14\lambda=\tilde{\Omega}(d^{-1/4})italic_Î» = over~ start_ARG roman_Î© end_ARG ( italic_d start_POSTSUPERSCRIPT - 1 / 4 end_POSTSUPERSCRIPT ). In the remainder of this section, we discuss further applications more briefly and informally for the sake of brevity and in order to avoid excessive repetition of previous work. For these applications we do not provide formal proofs, and only indicate where improvements can be made. Slow mixing of Glauber dynamics. Given a graph GğºGitalic_G, the Glauber dynamics for the hardcore model Î¼G,Î»subscriptğœ‡ğºğœ†\mu_{G,\lambda}italic_Î¼ start_POSTSUBSCRIPT italic_G , italic_Î» end_POSTSUBSCRIPT is the following Markov chain on state space â„â¢(G)â„ğº\mathcal{I}(G)caligraphic_I ( italic_G ), the family of independent sets of GğºGitalic_G: (1) Begin with an arbitrary Iâˆˆâ„â¢(G)ğ¼â„ğºI\in\mathcal{I}(G)italic_I âˆˆ caligraphic_I ( italic_G ), e.g. I=âˆ…ğ¼I=\emptysetitalic_I = âˆ…. (2) Choose a vertex vâˆˆVâ¢(G)ğ‘£ğ‘‰ğºv\in V(G)italic_v âˆˆ italic_V ( italic_G ) uniformly at random. (3) Sample Xâˆ¼Berâ¢(Î»/(1+Î»))similar-toğ‘‹Berğœ†1ğœ†X\sim\text{Ber}(\lambda/(1+\lambda))italic_X âˆ¼ Ber ( italic_Î» / ( 1 + italic_Î» ) ). If X=1ğ‘‹1X=1italic_X = 1 and vğ‘£vitalic_v has no neighbours in Iğ¼Iitalic_I then update Iâ†Iâˆª{v}â†ğ¼ğ¼ğ‘£I\leftarrow I\cup\{v\}italic_I â† italic_I âˆª { italic_v }. If X=0ğ‘‹0X=0italic_X = 0 update Iâ†I\{v}â†ğ¼\ğ¼ğ‘£I\leftarrow I\backslash\{v\}italic_I â† italic_I \ { italic_v }. This Markov chain has stationary distribution Î¼G,Î»subscriptğœ‡ğºğœ†\mu_{G,\lambda}italic_Î¼ start_POSTSUBSCRIPT italic_G , italic_Î» end_POSTSUBSCRIPT. If the chain mixes rapidly, it gives an efficient sampling scheme for the hard-core model on GğºGitalic_G. Galvin and Tetali in [17] investigated the mixing time Ï„â„³Î»â¢(G)subscriptğœsubscriptâ„³ğœ†ğº\tau_{\mathcal{M}_{\lambda}(G)}italic_Ï„ start_POSTSUBSCRIPT caligraphic_M start_POSTSUBSCRIPT italic_Î» end_POSTSUBSCRIPT ( italic_G ) end_POSTSUBSCRIPT of the Glauber dynamics for Î¼G,Î»subscriptğœ‡ğºğœ†\mu_{G,\lambda}italic_Î¼ start_POSTSUBSCRIPT italic_G , italic_Î» end_POSTSUBSCRIPT on a bipartite expander GğºGitalic_G. They showed that for Î»ğœ†\lambdaitalic_Î» sufficiently large the Glauber dynamics mixes slowly. The driving force behind this slow mixing is that, as alluded to in previous sections, a typical sample from Î¼G,Î»subscriptğœ‡ğºğœ†\mu_{G,\lambda}italic_Î¼ start_POSTSUBSCRIPT italic_G , italic_Î» end_POSTSUBSCRIPT is highly imbalanced. Balanced independent sets therefore create a small â€˜bottleneckâ€™ in the state space. As a concrete example, Galvin and Tetali prove the following. Theorem 1.6 ([17, Corollary 1.4]). There exists C>0ğ¶0C>0italic_C > 0 such that if dğ‘‘ditalic_d is sufficiently large and Câ¢log3/2â¡dd1/4â‰¤Î»â‰¤Oâ¢(1)ğ¶superscript32ğ‘‘superscriptğ‘‘14ğœ†ğ‘‚1\frac{C\log^{3/2}d}{d^{1/4}}\leq\lambda\leq O(1)divide start_ARG italic_C roman_log start_POSTSUPERSCRIPT 3 / 2 end_POSTSUPERSCRIPT italic_d end_ARG start_ARG italic_d start_POSTSUPERSCRIPT 1 / 4 end_POSTSUPERSCRIPT end_ARG â‰¤ italic_Î» â‰¤ italic_O ( 1 ), we have Ï„â„³Î»â¢(Qd)â‰¥expâ¡{Î©â¢(2dâ¢log3â¡(1+Î»)dâ¢log2â¡d)}.subscriptğœsubscriptâ„³ğœ†subscriptğ‘„ğ‘‘Î©superscript2ğ‘‘superscript31ğœ†ğ‘‘superscript2ğ‘‘\tau_{\mathcal{M}_{\lambda}(Q_{d})}\geq\exp\left\{\Omega\left(\frac{2^{d}\log^% {3}(1+\lambda)}{\sqrt{d}\log^{2}d}\right)\right\}.italic_Ï„ start_POSTSUBSCRIPT caligraphic_M start_POSTSUBSCRIPT italic_Î» end_POSTSUBSCRIPT ( italic_Q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT â‰¥ roman_exp { roman_Î© ( divide start_ARG 2 start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT roman_log start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ( 1 + italic_Î» ) end_ARG start_ARG square-root start_ARG italic_d end_ARG roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d end_ARG ) } . Our improved container lemma (Lemma 1.2) used in place of [17, Theorem 2.1] establishes the same slow mixing phenomenon all the way down to Î»=Î©~â¢(dâˆ’1/2)ğœ†~Î©superscriptğ‘‘12\lambda=\tilde{\Omega}(d^{-1/2})italic_Î» = over~ start_ARG roman_Î© end_ARG ( italic_d start_POSTSUPERSCRIPT - 1 / 2 end_POSTSUPERSCRIPT ). More generally, Galvin and Tetali establish slow mixing results for dğ‘‘ditalic_d-regular bipartite Î±ğ›¼\alphaitalic_Î±-expanders provided Î»ğœ†\lambdaitalic_Î» is sufficiently large as a function of Î±ğ›¼\alphaitalic_Î± and dğ‘‘ditalic_d. As above, Lemma 1.2 can be used to improve the range of Î»ğœ†\lambdaitalic_Î» for which these results hold. Antichains in the Boolean lattice. Given nâˆˆâ„•ğ‘›â„•n\in\mathbb{N}italic_n âˆˆ blackboard_N, let Bnsubscriptğµğ‘›B_{n}italic_B start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT denote the Boolean lattice of dimension nğ‘›nitalic_n, i.e. the power set of [n]={1,â€¦,n}delimited-[]ğ‘›1â€¦ğ‘›[n]=\{1,\ldots,n\}[ italic_n ] = { 1 , â€¦ , italic_n }. Recall that â„±âŠ†Bnâ„±subscriptğµğ‘›\mathcal{F}\subseteq B_{n}caligraphic_F âŠ† italic_B start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT is called an antichain if it is inclusion-free (i.e. AâŠˆBnot-subset-of-or-equalsğ´ğµA\not\subseteq Bitalic_A âŠˆ italic_B for all distinct A,Bâˆˆâ„±ğ´ğµâ„±A,B\in\mathcal{F}italic_A , italic_B âˆˆ caligraphic_F). Dedekindâ€™s problem, dating back to 1897, asks for the total number of antichains in Bnsubscriptğµğ‘›B_{n}italic_B start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT. In recent work [21], the current authors applied the graph container method to study Dedekindâ€™s problem in detail. As part of this study, the authors count and study the typical structure of antichains in Bnsubscriptğµğ‘›B_{n}italic_B start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT of a given size. For 0â‰¤kâ‰¤n0ğ‘˜ğ‘›0\leq k\leq n0 â‰¤ italic_k â‰¤ italic_n, we call the family Lk=([n]k)âŠ†Bnsubscriptğ¿ğ‘˜binomialdelimited-[]ğ‘›ğ‘˜subscriptğµğ‘›L_{k}=\binom{[n]}{k}\subseteq B_{n}italic_L start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = ( FRACOP start_ARG [ italic_n ] end_ARG start_ARG italic_k end_ARG ) âŠ† italic_B start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT the ktâ¢hsuperscriptğ‘˜ğ‘¡â„k^{th}italic_k start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT layer of Bnsubscriptğµğ‘›B_{n}italic_B start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT. We say that Lkâˆ’1,Lk,Lk+1subscriptğ¿ğ‘˜1subscriptğ¿ğ‘˜subscriptğ¿ğ‘˜1L_{k-1},L_{k},L_{k+1}italic_L start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT , italic_L start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_L start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT are three central layers if k=âŒŠn/2âŒ‹ğ‘˜ğ‘›2k=\lfloor n/2\rflooritalic_k = âŒŠ italic_n / 2 âŒ‹ or k=âŒˆn/2âŒ‰ğ‘˜ğ‘›2k=\lceil n/2\rceilitalic_k = âŒˆ italic_n / 2 âŒ‰. One of the main results of [21] is the following. Theorem 1.7 ([21, Theorem 1.5]). There exists C>0ğ¶0C>0italic_C > 0 such that if Câ¢log2â¡nn<Î²â‰¤1ğ¶superscript2ğ‘›ğ‘›ğ›½1\frac{C\log^{2}n}{\sqrt{n}}<\beta\leq 1divide start_ARG italic_C roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_n end_ARG start_ARG square-root start_ARG italic_n end_ARG end_ARG < italic_Î² â‰¤ 1, then almost all antichains of size Î²â¢(nâŒŠn/2âŒ‹)ğ›½binomialğ‘›ğ‘›2\beta\displaystyle\binom{n}{\lfloor n/2\rfloor}italic_Î² ( FRACOP start_ARG italic_n end_ARG start_ARG âŒŠ italic_n / 2 âŒ‹ end_ARG ) in Bnsubscriptğµğ‘›B_{n}italic_B start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT are a subset of three central layers. For the proof, we study the hard-core model on the graph Gnsubscriptğºğ‘›G_{n}italic_G start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT whose vertex set is Bnsubscriptğµğ‘›B_{n}italic_B start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT and uâˆ¼vsimilar-toğ‘¢ğ‘£u\sim vitalic_u âˆ¼ italic_v if and only if uâ‰ vğ‘¢ğ‘£u\neq vitalic_u â‰  italic_v and uğ‘¢uitalic_u is contained in vğ‘£vitalic_v or vice versa. Note that independent sets in Gnsubscriptğºğ‘›G_{n}italic_G start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT are precisely antichains in Bnsubscriptğµğ‘›B_{n}italic_B start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT. We use our container lemma, Lemma 1.2 (and its proof), to study the hard-core measure on Gnsubscriptğºğ‘›G_{n}italic_G start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT and to prove Theorem 1.7. Recently Balogh, Garcia and Li [1] studied the hard-core model on the subgraph MnâŠ†Gnsubscriptğ‘€ğ‘›subscriptğºğ‘›M_{n}\subseteq G_{n}italic_M start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT âŠ† italic_G start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT induced by the middle two layers L(nâˆ’1)/2,L(n+1)/2subscriptğ¿ğ‘›12subscriptğ¿ğ‘›12L_{(n-1)/2},L_{(n+1)/2}italic_L start_POSTSUBSCRIPT ( italic_n - 1 ) / 2 end_POSTSUBSCRIPT , italic_L start_POSTSUBSCRIPT ( italic_n + 1 ) / 2 end_POSTSUBSCRIPT when nğ‘›nitalic_n is odd. In particular, they obtain detailed asymptotics for the partition function ZMnâ¢(Î»)subscriptğ‘subscriptğ‘€ğ‘›ğœ†Z_{M_{n}}(\lambda)italic_Z start_POSTSUBSCRIPT italic_M start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_Î» ) when Î»â‰¥Câ¢logâ¡n/n1/3ğœ†ğ¶ğ‘›superscriptğ‘›13\lambda\geq C\log n/n^{1/3}italic_Î» â‰¥ italic_C roman_log italic_n / italic_n start_POSTSUPERSCRIPT 1 / 3 end_POSTSUPERSCRIPT (see [1, Theorem 5.1]). The now familiar Î©~â¢(nâˆ’1/3)~Î©superscriptğ‘›13\tilde{\Omega}(n^{-1/3})over~ start_ARG roman_Î© end_ARG ( italic_n start_POSTSUPERSCRIPT - 1 / 3 end_POSTSUPERSCRIPT ) bottleneck comes from the graph container method. Using Lemma 1.2 one can easily extend [1, Theorem 5.1] to the regime Î»=Î©~â¢(nâˆ’1/2)ğœ†~Î©superscriptğ‘›12\lambda=\tilde{\Omega}(n^{-1/2})italic_Î» = over~ start_ARG roman_Î© end_ARG ( italic_n start_POSTSUPERSCRIPT - 1 / 2 end_POSTSUPERSCRIPT ). Independent sets in the percolated hypercube. Given dâˆˆâ„•ğ‘‘â„•d\in\mathbb{N}italic_d âˆˆ blackboard_N and pâˆˆ[0,1]ğ‘01p\in[0,1]italic_p âˆˆ [ 0 , 1 ] we let Qd,psubscriptğ‘„ğ‘‘ğ‘Q_{d,p}italic_Q start_POSTSUBSCRIPT italic_d , italic_p end_POSTSUBSCRIPT denote the random subgraph of the hypercube Qdsubscriptğ‘„ğ‘‘Q_{d}italic_Q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT where each edge is retained independently with probability pğ‘pitalic_p. Recently, Kronenberg and Spinka [30] studied the hard-core model on Qd,psubscriptğ‘„ğ‘‘ğ‘Q_{d,p}italic_Q start_POSTSUBSCRIPT italic_d , italic_p end_POSTSUBSCRIPT. In particular, they find asymptotic formulae for the expected value (and higher moments) of the partition function ZQd,pâ¢(Î»)subscriptğ‘subscriptğ‘„ğ‘‘ğ‘ğœ†Z_{Q_{d,p}}(\lambda)italic_Z start_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT italic_d , italic_p end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_Î» ). For example, they prove the following. Theorem 1.8 ([30, Theorem 1.1]). For dğ‘‘ditalic_d sufficiently large and pâ‰¥Câ¢logâ¡dd1/3ğ‘ğ¶ğ‘‘superscriptğ‘‘13p\geq\frac{C\log d}{d^{1/3}}italic_p â‰¥ divide start_ARG italic_C roman_log italic_d end_ARG start_ARG italic_d start_POSTSUPERSCRIPT 1 / 3 end_POSTSUPERSCRIPT end_ARG, ğ”¼â¢|â„â¢(Qd,p)|=2â‹…22dâˆ’1â¢expâ¡[12â¢(2âˆ’p)d+(aâ¢(p)â¢(d2)âˆ’14)â¢2dâ¢(1âˆ’p2)2â¢d+Oâ¢(d4â¢2dâ¢(1âˆ’p2)3â¢d)],ğ”¼â„subscriptğ‘„ğ‘‘ğ‘â‹…2superscript2superscript2ğ‘‘112superscript2ğ‘ğ‘‘ğ‘ğ‘binomialğ‘‘214superscript2ğ‘‘superscript1ğ‘22ğ‘‘ğ‘‚superscriptğ‘‘4superscript2ğ‘‘superscript1ğ‘23ğ‘‘\mathbb{E}|\mathcal{I}(Q_{d,p})|=2\cdot 2^{2^{d-1}}\exp\left[\tfrac{1}{2}(2-p)% ^{d}+\left(a(p)\tbinom{d}{2}-\tfrac{1}{4}\right)2^{d}(1-\tfrac{p}{2})^{2d}+O% \left(d^{4}2^{d}(1-\tfrac{p}{2})^{3d}\right)\right],blackboard_E | caligraphic_I ( italic_Q start_POSTSUBSCRIPT italic_d , italic_p end_POSTSUBSCRIPT ) | = 2 â‹… 2 start_POSTSUPERSCRIPT 2 start_POSTSUPERSCRIPT italic_d - 1 end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT roman_exp [ divide start_ARG 1 end_ARG start_ARG 2 end_ARG ( 2 - italic_p ) start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT + ( italic_a ( italic_p ) ( FRACOP start_ARG italic_d end_ARG start_ARG 2 end_ARG ) - divide start_ARG 1 end_ARG start_ARG 4 end_ARG ) 2 start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ( 1 - divide start_ARG italic_p end_ARG start_ARG 2 end_ARG ) start_POSTSUPERSCRIPT 2 italic_d end_POSTSUPERSCRIPT + italic_O ( italic_d start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT 2 start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ( 1 - divide start_ARG italic_p end_ARG start_ARG 2 end_ARG ) start_POSTSUPERSCRIPT 3 italic_d end_POSTSUPERSCRIPT ) ] , where aâ¢(p):=(1+(1âˆ’p)2)2(2âˆ’p)4âˆ’14.assignğ‘ğ‘superscript1superscript1ğ‘22superscript2ğ‘414a(p):=\frac{(1+(1-p)^{2})^{2}}{(2-p)^{4}}-\frac{1}{4}.italic_a ( italic_p ) := divide start_ARG ( 1 + ( 1 - italic_p ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG ( 2 - italic_p ) start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT end_ARG - divide start_ARG 1 end_ARG start_ARG 4 end_ARG . The restriction on pğ‘pitalic_p here arises from an application of the graph container method. Our refined container method can be used to extend the above theorem to the range p=Î©~â¢(dâˆ’1/2)ğ‘~Î©superscriptğ‘‘12p=\tilde{\Omega}(d^{-1/2})italic_p = over~ start_ARG roman_Î© end_ARG ( italic_d start_POSTSUPERSCRIPT - 1 / 2 end_POSTSUPERSCRIPT ). Kronenberg and Spinka prove several results about the hard-core model on Qd,psubscriptğ‘„ğ‘‘ğ‘Q_{d,p}italic_Q start_POSTSUBSCRIPT italic_d , italic_p end_POSTSUBSCRIPT at activity Î»ğœ†\lambdaitalic_Î». The range of Î»ğœ†\lambdaitalic_Î» and pğ‘pitalic_p for which these results hold may similarly be extended using the refined container methods of this paper. 1.3. Organization We collect preliminary results in Section 2 and prove our core result, Lemma 2.4, in Section 3. We then give the reconstruction argument that enables us to deduce Lemma 1.2 in Section 4. We prove Theorem 1.1 in Section 5 and Theorem 1.5 in Section 6. Finally, we conclude with some brief remarks in Section 7. 1.4. Notation and usage We use Î£=XâŠ”YÎ£square-unionğ‘‹ğ‘Œ\Sigma=X\sqcup Yroman_Î£ = italic_X âŠ” italic_Y for a bipartite graph with parts Xğ‘‹Xitalic_X and Yğ‘ŒYitalic_Y, and Vâ¢(Î£)ğ‘‰Î£V(\Sigma)italic_V ( roman_Î£ ) and Eâ¢(Î£)ğ¸Î£E(\Sigma)italic_E ( roman_Î£ ) for the set of vertices and edges of Î£Î£\Sigmaroman_Î£, respectively. As usual, we write Nâ¢(x)ğ‘ğ‘¥N(x)italic_N ( italic_x ) for {yâˆˆVâ¢(Î£):{x,y}âˆˆEâ¢(Î£)}conditional-setğ‘¦ğ‘‰Î£ğ‘¥ğ‘¦ğ¸Î£\{y\in V(\Sigma):\{x,y\}\in E(\Sigma)\}{ italic_y âˆˆ italic_V ( roman_Î£ ) : { italic_x , italic_y } âˆˆ italic_E ( roman_Î£ ) } and dAâ¢(x):=|Nâ¢(x)âˆ©A|assignsubscriptğ‘‘ğ´ğ‘¥ğ‘ğ‘¥ğ´d_{A}(x):=|N(x)\cap A|italic_d start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT ( italic_x ) := | italic_N ( italic_x ) âˆ© italic_A |. We use dâ¢(x)ğ‘‘ğ‘¥d(x)italic_d ( italic_x ) for dÎ£â¢(x)subscriptğ‘‘Î£ğ‘¥d_{\Sigma}(x)italic_d start_POSTSUBSCRIPT roman_Î£ end_POSTSUBSCRIPT ( italic_x ) for simplicity. For SâŠ†Vâ¢(Î£)ğ‘†ğ‘‰Î£S\subseteq V(\Sigma)italic_S âŠ† italic_V ( roman_Î£ ), set Nâ¢(S):=âˆªxâˆˆSNâ¢(x)assignğ‘ğ‘†subscriptğ‘¥ğ‘†ğ‘ğ‘¥N(S):=\cup_{x\in S}N(x)italic_N ( italic_S ) := âˆª start_POSTSUBSCRIPT italic_x âˆˆ italic_S end_POSTSUBSCRIPT italic_N ( italic_x ) and let Î£â¢[S]Î£delimited-[]ğ‘†\Sigma[S]roman_Î£ [ italic_S ] denote the induced subgraph of Î£Î£\Sigmaroman_Î£ on Sğ‘†Sitalic_S. For A,BâŠ†Vâ¢(Î£)ğ´ğµğ‘‰Î£A,B\subseteq V(\Sigma)italic_A , italic_B âŠ† italic_V ( roman_Î£ ), let âˆ‡(A,B)={{x,y}âˆˆEâ¢(Î£):xâˆˆA,yâˆˆB},âˆ‡ğ´ğµconditional-setğ‘¥ğ‘¦ğ¸Î£formulae-sequenceğ‘¥ğ´ğ‘¦ğµ\nabla(A,B)=\{\{x,y\}\in E(\Sigma):x\in A,y\in B\},âˆ‡ ( italic_A , italic_B ) = { { italic_x , italic_y } âˆˆ italic_E ( roman_Î£ ) : italic_x âˆˆ italic_A , italic_y âˆˆ italic_B } , and âˆ‡(A)=âˆ‡(A,Vâ¢(Î£)âˆ–A)âˆ‡ğ´âˆ‡ğ´ğ‘‰Î£ğ´\nabla(A)=\nabla(A,V(\Sigma)\setminus A)âˆ‡ ( italic_A ) = âˆ‡ ( italic_A , italic_V ( roman_Î£ ) âˆ– italic_A ). We write (nâ‰¤k)binomialğ‘›absentğ‘˜\binom{n}{\leq k}( FRACOP start_ARG italic_n end_ARG start_ARG â‰¤ italic_k end_ARG ) for âˆ‘iâ‰¤k(ni).subscriptğ‘–ğ‘˜binomialğ‘›ğ‘–\sum_{i\leq k}\binom{n}{i}.âˆ‘ start_POSTSUBSCRIPT italic_i â‰¤ italic_k end_POSTSUBSCRIPT ( FRACOP start_ARG italic_n end_ARG start_ARG italic_i end_ARG ) . We will make frequent use of the basic binomial estimate (7) (nâ‰¤k)â‰¤expâ¡{kâ¢logâ¡(eâ¢nk)} for â¢kâ‰¤n,formulae-sequencebinomialğ‘›absentğ‘˜ğ‘˜ğ‘’ğ‘›ğ‘˜ for ğ‘˜ğ‘›{n\choose\leq k}\leq\exp\left\{k\log\left(\frac{en}{k}\right)\right\}\quad% \text{ for }k\leq n,( binomial start_ARG italic_n end_ARG start_ARG â‰¤ italic_k end_ARG ) â‰¤ roman_exp { italic_k roman_log ( divide start_ARG italic_e italic_n end_ARG start_ARG italic_k end_ARG ) } for italic_k â‰¤ italic_n , where here and throughout the paper log\logroman_log is used for the natural logarithm. For two functions f,g:â„•â†’â„:ğ‘“ğ‘”â†’â„•â„f,g:\mathbb{N}\to\mathbb{R}italic_f , italic_g : blackboard_N â†’ blackboard_R we write fâ¢(d)=O~â¢(gâ¢(d))ğ‘“ğ‘‘~ğ‘‚ğ‘”ğ‘‘f(d)=\tilde{O}(g(d))italic_f ( italic_d ) = over~ start_ARG italic_O end_ARG ( italic_g ( italic_d ) ) if there exists Câˆˆâ„ğ¶â„C\in\mathbb{R}italic_C âˆˆ blackboard_R such that |fâ¢(d)|â‰¤(logâ¡d)Câ¢|gâ¢(d)|ğ‘“ğ‘‘superscriptğ‘‘ğ¶ğ‘”ğ‘‘|f(d)|\leq(\log d)^{C}|g(d)|| italic_f ( italic_d ) | â‰¤ ( roman_log italic_d ) start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPT | italic_g ( italic_d ) | for dğ‘‘ditalic_d sufficiently large. We use Î©~~Î©\tilde{\Omega}over~ start_ARG roman_Î© end_ARG analogously. Throughout the paper Î£Î£\Sigmaroman_Î£ will denote a Î´ğ›¿\deltaitalic_Î´-approximately (dX,dY)subscriptğ‘‘ğ‘‹subscriptğ‘‘ğ‘Œ(d_{X},d_{Y})( italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT )-biregular graph (for some choice of absolute constant Î´ğ›¿\deltaitalic_Î´) and all asymptotic notation is to be understood with respect to the limits dX,dYâ†’âˆâ†’subscriptğ‘‘ğ‘‹subscriptğ‘‘ğ‘Œd_{X},d_{Y}\to\inftyitalic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT â†’ âˆ. The cost of a choice means the logarithm (in base two) of the number of its possibilities. We employ a common abuse of notation by often omitting floor and ceiling symbols for notational convenience."
https://arxiv.org/html/2411.03871v1,Safe Paths and Sequences forScalable ILPs in RNA Transcript Assembly Problems,"A common step at the core of many RNA transcript assembly tools is to find a set of weighted paths that best explain the weights of a DAG. While such problems easily become NP-hard, scalable solvers exist only for a basic error-free version of this problem, namely minimally decomposing a network flow into weighted paths.The main result of this paper is to show that we can achieve speedups of two orders of magnitude also for path-finding problems in the realistic setting (i.e., the weights do not induce a flow). We obtain these by employing the safety information that is encoded in the graph structure inside Integer Linear Programming (ILP) solvers for these problems. We first characterize the paths that appear in all path covers of the DAG, generalizing a graph reduction commonly used in the error-free setting (e.g. by Kloster et al. [ALENEX 2018]). Secondly, following the work of Ma, Zheng and Kingsford [RECOMB 2021], we characterize the sequences of arcs that appear in all path covers of the DAG.We experiment with a path-finding ILP model (least squares) and with a more recent and accurate one. We use a variety of datasets originally created by Shao and Kingsford [TCBB, 2017], as well as graphs built from sequencing reads by the state-of-the-art tool for long-read transcript discovery, IsoQuant [Prjibelski et al., Nat. Biotechnology 2023]. The ILPs armed with safe paths or sequences exhibit significant speed-ups over the original ones. On graphs with a large width, average speed-ups are in the range 50âˆ’160Ã—50-160\times50 - 160 Ã— in the latter ILP model and in the range 100âˆ’1000Ã—100-1000\times100 - 1000 Ã— in the least squares model.Our scaling techniques apply to any ILP whose solution paths are a path cover of the arcs of the DAG. As such, they can become a scalable building block of practical RNA transcript assembly tools, avoiding heuristic trade-offs currently needed on complex graphs.","Background and motivation. The genome-guided RNA transcript assembly problem, one the most famous assembly problems in bioinformatics, can be succinctly described as follows. Given a set of RNA-seq reads, a directed acyclic graph (DAG) is constructed from their alignments to a reference genome. The graph nodes correspond to e.g. exons, the arcs correspond to reads overlapping two consecutive exons, and the node or arc weights corresponding their read coverage. The RNA transcripts then correspond to a set of source-to-sink weighted paths in the DAG that â€œbest explainâ€ the nodes, arcs and their weights [tomescu2015explaining], under various definitions of optimality. On perfect, error-free data, the arc weights satisfy flow conservation. In this setting, the most well-known definition of optimality is to require a minimum number of weighted paths whose superposition fully equals to the given flow weights (minimum flow decomposition, or MFD). This is a classical NP-hard problem [ahuja1988network, vatinlen2008simple], with many applications also in other fields, such as transportation [olsen2022study] and networking [hartman2012split, vatinlen2008simple]. On real data, the arc weights do not satisfy flow conservation because of errors in the reads and in their alignment, biases in sequencing coverage, and trimming in the reads, see e.g. [lrgasp]. As such, many practical tools model the RNA transcript assembly problem as a path-finding Integer Linear Program (ILP), for which they use a fast solver, such as Gurobi [gurobi] or CPLEX [cplex2009v12]. ILP is a powerful paradigm to model and efficiently solve NP-hard problems, including in bioinformatics, see e.g. [gusfield2019integer]. RNA transcript assembly based on ILP include CIDANE [cidane], CLASS2 [class2], TransLiG [translig], CLIIQ [cliiq], IsoInfer [isoinfer], IsoLasso [isolasso], MultiTrans [multitrans], NSMAP [nsmap], SSP [ssp], JUMPER [jumper]. Most of the existing ILP-based RNA transcript assembly tools (e.g. [cidane, class2, cliiq, isoinfer, isolasso, multitrans, nsmap, ssp, translig]) in principle do not scale with large graphs. The reason is that one first needs to enumerate all possible paths in the graphs, and then add an ILP variable for each path. This possibly leads to an exponential pre-processing time, and to an exponentially-sized ILP. Thus, many of these tools e.g. [cidane, cliiq, ireckon, isoinfer, multitrans, nsmap] use the heuristic of enumerating only some of all possible paths, potentially leaving some transcripts undiscovered, or leading to incorrect answers. Recently, [dias2022fast, jumper] observed that the enumeration step can be avoided by modeling the search for paths in the ILP itself, via only polynomially-many additional variables and constraints. However, some datasets still require tens of hours to solve [acceleratingILP, dias2024robust]. Despite this pressing need for fast solutions to path-finding ILPs modeling real-world data, most research effort has been put in the error-free setting, namely in the MFD problem. This includes fast heuristics [shao2017theory, vatinlen2008simple, hartman2012split, bernard2014efficient], fixed-parameter tractable algorithms [kloster2018practical], and approximation algorithms [hartman2012split, caceres2024width]. Recently, [acceleratingILP] showed that also the above-mentioned polynomially-sized ILP models for the MFD problem can be sped-up using some insight into the input flow structure, via the notion of safety [omnitigs_tomescu] (which we also review below). These optimizations also apply to an MFD variant where we are also given subpath constraints corresponding to long-reads aligned to the graph. On the hardest instances this leads to speedups of two orders of magnitude [acceleratingILP]. Contributions. In this paper we show that similar speedups can be obtained also for real-world inputs where the arc weights do not satisfy flow conservation, and for any â€œpath-findingâ€ problem formulation, as long as their solution paths are a path cover of the arcs of the DAG.111In this paper we assume that the input DAG has a unique source node sğ‘ sitalic_s and a unique sink node tğ‘¡titalic_t; if this is not the case, one can just add a new global source sğ‘ sitalic_s connected to all existing graph sources (and symmetrically for a global sink), and specially handle these extra arcs in the problem formulations. Moreover, by a path cover we mean a set of paths from sğ‘ sitalic_s to tğ‘¡titalic_t (sğ‘ sitalic_s-tğ‘¡titalic_t paths), such that every arc belongs to at least one path. We obtain these speedups by exploiting the graph structure of the DAG, in particular by exploiting the safe paths (and safe sequences of arcs) that appear in all path covers of the DAG. As such, we prove new results about the structure of directed acyclic graphs, which may also be of independent interest. More specifically, we give the following contributions. 1.0.1 1. Generalizing the Y-to-V reduction as finding safe paths for path covers. For the minimum flow decomposition problem, Kloster et al. [kloster2018practical] used a graph reduction operation that decreases the size of the DAG, while preserving all flow decompositions. Namely, as long as the graph has a node vğ‘£vitalic_v with only one in-neighbor uğ‘¢uitalic_u, one removes vğ‘£vitalic_v, and adds arcs from uğ‘¢uitalic_u to each out-neighbor wğ‘¤witalic_w of vğ‘£vitalic_v, with flow value fâ¢(u,w)=fâ¢(v,w)ğ‘“ğ‘¢ğ‘¤ğ‘“ğ‘£ğ‘¤f(u,w)=f(v,w)italic_f ( italic_u , italic_w ) = italic_f ( italic_v , italic_w ). This is a correct operation for MFD because of flow conservation: intuitively, the flow on any arc (v,w)ğ‘£ğ‘¤(v,w)( italic_v , italic_w ) must come to vğ‘£vitalic_v via its unique in-neighbor uğ‘¢uitalic_u. A symmetric operation applies to nodes with only one out-neighbor (see Figure 4 (a) in Appendix 0.B). These operations have later been used in other works on the MFD problem, see [acceleratingILP, dias2023safety]. Moreover, they are also common in the context of the genome assembly problem [medvedev2007computability, jackson2009parallel, kingsford2010assembly], where this operation was called the â€œY-to-V reductionâ€ [omnitigs_tomescu] because of the shape of the subgraphs before and after reduction. We will also use this name to refer to this operation in this paper. (a) Four safe paths, shown as colored lines, such that there is no sğ‘ sitalic_s-tğ‘¡titalic_t path containing any two of them. (b) Four safe sequences such that there is no sğ‘ sitalic_s-tğ‘¡titalic_t path containing any two of them. Dotted lines indicate gaps in the sequences. Figure 1: Example of safe paths and safe sequences for the sğ‘ sitalic_s-tğ‘¡titalic_t path covers in a DAG GğºGitalic_G with unique source sğ‘ sitalic_s and unique sink tğ‘¡titalic_t. That is, for every path cover of GğºGitalic_G (i.e. set of paths from sğ‘ sitalic_s to tğ‘¡titalic_t in GğºGitalic_G such that every arc of GğºGitalic_G appears in a path of the cover), there is a path in the cover containing the safe path or the safe sequence. We can see that safe sequences extend safe paths over complex subgraphs. If we have a set of safe path, or safe sequences, respectively, such that no two of them can appear on the same path of the DAG, then each of them must clearly appear in different paths of any path cover. Suppose that an ILP model has kğ‘˜kitalic_k binary variables xuâ¢vâ¢isubscriptğ‘¥ğ‘¢ğ‘£ğ‘–x_{uvi}italic_x start_POSTSUBSCRIPT italic_u italic_v italic_i end_POSTSUBSCRIPT, iâˆˆ{1,â€¦,k}ğ‘–1â€¦ğ‘˜i\in\{1,\dots,k\}italic_i âˆˆ { 1 , â€¦ , italic_k }, for every arc (u,v)ğ‘¢ğ‘£(u,v)( italic_u , italic_v ) of GğºGitalic_G, with the interpretation that (u,v)ğ‘¢ğ‘£(u,v)( italic_u , italic_v ) appears in solution path iğ‘–iitalic_i iff xuâ¢vâ¢i=1subscriptğ‘¥ğ‘¢ğ‘£ğ‘–1x_{uvi}=1italic_x start_POSTSUBSCRIPT italic_u italic_v italic_i end_POSTSUBSCRIPT = 1. Then, using such set of safe paths or sequences we can fix to 1 some of these binary variables, as in [acceleratingILP]. For example, we can assign the blue, orange, green and violet safe paths to solution paths 1,2,3 and 4, respectively (we show this assignment only for the blue and violet safe paths). We can proceed in a similar manner for safe sequences, now fixing more variables because sequences are overall longer (we show this assignment only for the blue and violet safe sequences). However, when flow conservation does not hold, and when considering other path-finding problems than MFD, these are invalid operations. As such, we generalize the Y-to-V reduction so that we can use it for any problem whose solution is some path cover of the arcs. Specifically, we say that a path Pğ‘ƒPitalic_P is safe (with respect to the path covers of a DAG GğºGitalic_G) if for any path cover of GğºGitalic_G, there is a path in the path cover that contains Pğ‘ƒPitalic_P as subpath (see Figure 1(a) for an example). Using this framework, one can see the correctness of the Y-to-V reduction because the paths (u,v,w)ğ‘¢ğ‘£ğ‘¤(u,v,w)( italic_u , italic_v , italic_w ), for any out-neighbor wğ‘¤witalic_w of vğ‘£vitalic_v are safe. However, by repeatedly applying the Y-to-V reduction, one misses safe paths (see Figure 4), and thus this reduction is not a correct algorithm for finding all safe paths. In this paper, we characterize all the safe paths with respect to the path covers of a DAG GğºGitalic_G (Section 2). Using this characterization, we show that all maximal safe paths (i.e., those that contain all other safe paths as subpaths) can be computed in optimal linear time: {restatable*} [Maximal safe paths enumeration]theoremmaxsafepathsenum Given a DAG GğºGitalic_G with mğ‘šmitalic_m arcs, there is an Oâ¢(m+o)ğ‘‚ğ‘šğ‘œO(m+o)italic_O ( italic_m + italic_o )-time algorithm computing all the maximal safe paths of GğºGitalic_G, where oğ‘œoitalic_o is the total length of the output, namely of all maximal safe paths. 1.0.2 2. Generalizing safe paths to safe sequences for path covers. While safe paths fully capture contiguous safety information, it may be that the only way to reach a safe path P2subscriptğ‘ƒ2P_{2}italic_P start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT is by passing through a safe path P1subscriptğ‘ƒ1P_{1}italic_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, with some â€œcomplexâ€ subgraph between P1subscriptğ‘ƒ1P_{1}italic_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and P2subscriptğ‘ƒ2P_{2}italic_P start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. As such, the sequence P1,P2subscriptğ‘ƒ1subscriptğ‘ƒ2P_{1},P_{2}italic_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_P start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT appears in some path of any path cover. Therefore, inspired by the results of Ma, Zheng and Kingsford [and_or_quant], we generalize the previous notion of safe paths to safe sequences of arcs that appears in all paths covers of a DAG GğºGitalic_G (see Figure 1(b) for an example). More specifically, an application of the AND-Quant problem from [and_or_quant] is to characterize when a sequence (e1,â€¦,et)subscriptğ‘’1â€¦subscriptğ‘’ğ‘¡(e_{1},\dots,e_{t})( italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) of arcs of GğºGitalic_G appears in all flow decompositions of GğºGitalic_G. Namely, when it holds that for any flow decomposition, there is a path Pisubscriptğ‘ƒğ‘–P_{i}italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT of the decomposition such that e1,â€¦,etsubscriptğ‘’1â€¦subscriptğ‘’ğ‘¡e_{1},\dots,e_{t}italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT appear in Pisubscriptğ‘ƒğ‘–P_{i}italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, in this order. This is based on solving a max flow problem from (e1,â€¦,et)subscriptğ‘’1â€¦subscriptğ‘’ğ‘¡(e_{1},\dots,e_{t})( italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) and GğºGitalic_G. In this paper we characterize safe sequences with respect to path covers of GğºGitalic_G. For this, we generalize our results for safe paths by using the notion of uğ‘¢uitalic_u-vğ‘£vitalic_v bridges (i.e. arcs belonging to all uğ‘¢uitalic_u-vğ‘£vitalic_v paths), for a suitable choice of nodes uğ‘¢uitalic_u and vğ‘£vitalic_v. {restatable*} [Maximal safe sequences enumeration]theoremmaxsafeseqsenum Given a DAG GğºGitalic_G with mğ‘šmitalic_m arcs, there is an Oâ¢(m+o2)ğ‘‚ğ‘šsuperscriptğ‘œ2O(m+o^{2})italic_O ( italic_m + italic_o start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )-time algorithm computing all the maximal safe sequences of GğºGitalic_G, where oğ‘œoitalic_o is the total length of the output, namely of all maximal safe sequences. 1.0.3 3. Scaling path-finding ILPs via safe paths and sequences. Safety characterizes the information that must appear in all solutions to a problem. If the solutions to a real-world path-finding problem are a subset of all the path covers of a DAG, then safe paths and safe sequences for path covers also appear in the solution paths of our path-finding problem. As such, we can use them to simplify a solver for the problem. For this, we use the approach of Grigorjew et al. [acceleratingILP]: if we are given a set of paths (or sequences, in our case) that must each be used by different solution paths, then we can fix some binary variables in the ILP model that encode the solution paths. See Figure 1 for an illustration. We apply safe paths and safe sequences in this manner for two path-finding ILPs. In the first one (LeastSquares), we need to find a set of paths minimizing the sum of the squared errors between the weight of each arc and the weight of the solutions paths going through the arc. This is at the core of several RNA assembly tools e.g. [isolasso, cidane, ryuto, slide, ireckon, traph]. The second one (MinPathError) was recently introduced in [dias2024robust] and shown to be more accurate than LeastSquares (and than other ones, such as minimum inexact flow decomposition [inexact], see [dias2024robust]). The goal here is to account for errors not at the level of individual arcs, but at the level of solution paths, and minimize the sum of the errors of the paths. We describe all the above in detail in Appendix 0.B. The ILPs optimized with safe paths or sequences exhibit significant speed-ups over the original ones, with average speed-up of at least 10Ã—10\times10 Ã— on many types of graphs. On graphs with a large width, average speed-ups are in the range 50âˆ’150Ã—50-150\times50 - 150 Ã— for MinPathError, and in the range 100âˆ’1000Ã—100-1000\times100 - 1000 Ã— for LeastSquares. As such, our optimizations can become a scalable building block of practical RNA transcript assembly tools, avoiding heuristic trade-offs on complex graphs currently needed."
https://arxiv.org/html/2411.03784v1,Optimal prefix-suffix queries with applications,"We revisit the classic border tree data structure [Gu, Farach, Beigel, SODA 1994] that answers the following prefix-suffix queries on a string Tğ‘‡Titalic_T of length nğ‘›nitalic_n over an integer alphabet Î£=[0,Ïƒ)Î£0ğœ\Sigma=[0,\sigma)roman_Î£ = [ 0 , italic_Ïƒ ): for any i,jâˆˆ[0,n)ğ‘–ğ‘—0ğ‘›i,j\in[0,n)italic_i , italic_j âˆˆ [ 0 , italic_n ) return all occurrences of Tğ‘‡Titalic_T in Tâ¢[0â¢..â¢i]â¢Tâ¢[jâ¢..â¢nâˆ’1]T[0\mathinner{.\,.}i]T[j\mathinner{.\,.}n-1]italic_T [ 0 start_ATOM . . end_ATOM italic_i ] italic_T [ italic_j start_ATOM . . end_ATOM italic_n - 1 ]. The border tree of Tğ‘‡Titalic_T can be constructed in ğ’ªâ¢(n)ğ’ªğ‘›\mathcal{O}(n)caligraphic_O ( italic_n ) time and answers prefix-suffix queries in ğ’ªâ¢(logâ¡n+Occ)ğ’ªğ‘›Occ\mathcal{O}(\log n+\textsf{Occ})caligraphic_O ( roman_log italic_n + Occ ) time, where Occ is the number of occurrences of Tğ‘‡Titalic_T in Tâ¢[0â¢..â¢i]â¢Tâ¢[jâ¢..â¢nâˆ’1]T[0\mathinner{.\,.}i]T[j\mathinner{.\,.}n-1]italic_T [ 0 start_ATOM . . end_ATOM italic_i ] italic_T [ italic_j start_ATOM . . end_ATOM italic_n - 1 ]. Our contribution here is the following. We present a completely different and remarkably simple data structure that can be constructed in the optimal ğ’ªâ¢(n/logÏƒâ¡n)ğ’ªğ‘›subscriptğœğ‘›\mathcal{O}(n/\log_{\sigma}n)caligraphic_O ( italic_n / roman_log start_POSTSUBSCRIPT italic_Ïƒ end_POSTSUBSCRIPT italic_n ) time and supports queries in the optimal ğ’ªâ¢(1)ğ’ª1\mathcal{O}(1)caligraphic_O ( 1 ) time. Our result is based on a new structural lemma that lets us encode the output of any query in constant time and space. We also show a new direct application of our result in pattern matching on node-labeled graphs.","Let T=T[0..nâˆ’1]=T[0..n)T=T[0\mathinner{.\,.}n-1]=T[0\mathinner{.\,.}n)italic_T = italic_T [ 0 start_ATOM . . end_ATOM italic_n - 1 ] = italic_T [ 0 start_ATOM . . end_ATOM italic_n ) be a string of length nğ‘›nitalic_n over an integer alphabet Î£=[0,Ïƒ)Î£0ğœ\Sigma=[0,\sigma)roman_Î£ = [ 0 , italic_Ïƒ ) with Ïƒ=nğ’ªâ¢(1)ğœsuperscriptğ‘›ğ’ª1\sigma=n^{\mathcal{O}(1)}italic_Ïƒ = italic_n start_POSTSUPERSCRIPT caligraphic_O ( 1 ) end_POSTSUPERSCRIPT. We would like to preprocess Tğ‘‡Titalic_T in order to answer the following type of queries: for any i,jâˆˆ[0,n)ğ‘–ğ‘—0ğ‘›i,j\in[0,n)italic_i , italic_j âˆˆ [ 0 , italic_n ), return all occurrences of Tğ‘‡Titalic_T in Tâ¢[0â¢..â¢i]â¢Tâ¢[jâ¢..â¢nâˆ’1]T[0\mathinner{.\,.}i]T[j\mathinner{.\,.}n-1]italic_T [ 0 start_ATOM . . end_ATOM italic_i ] italic_T [ italic_j start_ATOM . . end_ATOM italic_n - 1 ]. This type of query, which we denote here by PrefSufâ¢(i,j)PrefSufğ‘–ğ‘—\textsf{PrefSuf}(i,j)PrefSuf ( italic_i , italic_j ), is supported by the classic border tree data structure in ğ’ªâ¢(logâ¡n+Occ)ğ’ªğ‘›Occ\mathcal{O}(\log n+\textsf{Occ})caligraphic_O ( roman_log italic_n + Occ ) time after an ğ’ªâ¢(n)ğ’ªğ‘›\mathcal{O}(n)caligraphic_O ( italic_n )-time preprocessing [17]. We show the following optimal data structure for PrefSufâ¢(i,j)PrefSufğ‘–ğ‘—\textsf{PrefSuf}(i,j)PrefSuf ( italic_i , italic_j ) queries. Theorem 1. For any string Tğ‘‡Titalic_T of length nğ‘›nitalic_n over an alphabet Î£=[0,Ïƒ)Î£0ğœ\Sigma=[0,\sigma)roman_Î£ = [ 0 , italic_Ïƒ ) with Ïƒ=nğ’ªâ¢(1)ğœsuperscriptğ‘›ğ’ª1\sigma=n^{\mathcal{O}(1)}italic_Ïƒ = italic_n start_POSTSUPERSCRIPT caligraphic_O ( 1 ) end_POSTSUPERSCRIPT, we can answer PrefSufâ¢(i,j)PrefSufğ‘–ğ‘—\textsf{PrefSuf}(i,j)PrefSuf ( italic_i , italic_j ) queries, for any i,jâˆˆ[0,n)ğ‘–ğ‘—0ğ‘›i,j\in[0,n)italic_i , italic_j âˆˆ [ 0 , italic_n ), in ğ’ªâ¢(1)ğ’ª1\mathcal{O}(1)caligraphic_O ( 1 ) time after an ğ’ªâ¢(n/logÏƒâ¡n)ğ’ªğ‘›subscriptğœğ‘›\mathcal{O}(n/\log_{\sigma}n)caligraphic_O ( italic_n / roman_log start_POSTSUBSCRIPT italic_Ïƒ end_POSTSUBSCRIPT italic_n )-time preprocessing. The data structure size is ğ’ªâ¢(n/logÏƒâ¡n)ğ’ªğ‘›subscriptğœğ‘›\mathcal{O}(n/\log_{\sigma}n)caligraphic_O ( italic_n / roman_log start_POSTSUBSCRIPT italic_Ïƒ end_POSTSUBSCRIPT italic_n ) and the output is given as a compact representation of ğ’ªâ¢(1)ğ’ª1\mathcal{O}(1)caligraphic_O ( 1 ) size. In the word RAM model with wğ‘¤witalic_w-bit machine words and w=Î©â¢(logâ¡n)ğ‘¤Î©ğ‘›w=\Omega(\log n)italic_w = roman_Î© ( roman_log italic_n ), Tğ‘‡Titalic_T is represented as an array: each letter occupies one machine word. However, a single letter can be represented using âŒˆlogâ¡ÏƒâŒ‰ğœ\lceil\log\sigma\rceilâŒˆ roman_log italic_Ïƒ âŒ‰ bits (i.e., packed representation), which could be (significantly) less than wğ‘¤witalic_w (e.g., for a constant-sized alphabet). Thus, in the word RAM model, it takes ğ’ªâ¢(n/logÏƒâ¡n)ğ’ªğ‘›subscriptğœğ‘›\mathcal{O}(n/\log_{\sigma}n)caligraphic_O ( italic_n / roman_log start_POSTSUBSCRIPT italic_Ïƒ end_POSTSUBSCRIPT italic_n ) words to store Tğ‘‡Titalic_T and ğ’ªâ¢(n/logÏƒâ¡n)ğ’ªğ‘›subscriptğœğ‘›\mathcal{O}(n/\log_{\sigma}n)caligraphic_O ( italic_n / roman_log start_POSTSUBSCRIPT italic_Ïƒ end_POSTSUBSCRIPT italic_n ) time to read it. Hence Theorem 1 is optimal with respect to the construction time and the query time. Border tree. We start with an informal description of the border tree data structure. The classic KMP algorithm [20] constructs an automaton over string T=Tâ¢[0â¢..â¢nâˆ’1]T=T[0\mathinner{.\,.}n-1]italic_T = italic_T [ 0 start_ATOM . . end_ATOM italic_n - 1 ]. The automaton consists of an initial state and: (1) one state per prefix of Tğ‘‡Titalic_T numbered from 00 to nâˆ’1ğ‘›1n-1italic_n - 1; (2) a success transition from state iğ‘–iitalic_i to state i+1ğ‘–1i+1italic_i + 1; and (3) a failure transition from state iğ‘–iitalic_i to the state representing the longest string that is both a prefix and a suffix (known as border) of Tâ¢[0â¢..â¢i]T[0\mathinner{.\,.}i]italic_T [ 0 start_ATOM . . end_ATOM italic_i ]. The failure transitions form the failure tree. Thus any path in the failure tree from the root to a state iğ‘–iitalic_i specifies all the borders of Tâ¢[0â¢..â¢i]T[0\mathinner{.\,.}i]italic_T [ 0 start_ATOM . . end_ATOM italic_i ]. Since the number of distinct borders of Tâ¢[0â¢..â¢i]T[0\mathinner{.\,.}i]italic_T [ 0 start_ATOM . . end_ATOM italic_i ] can be Î˜â¢(n)Î˜ğ‘›\Theta(n)roman_Î˜ ( italic_n ), we apply a grouping of the borders based on periodicity that results in the border tree: a compacted version of the failure tree with ğ’ªâ¢(n)ğ’ªğ‘›\mathcal{O}(n)caligraphic_O ( italic_n ) states and ğ’ªâ¢(logâ¡n)ğ’ªğ‘›\mathcal{O}(\log n)caligraphic_O ( roman_log italic_n ) depth. The construction time is ğ’ªâ¢(n)ğ’ªğ‘›\mathcal{O}(n)caligraphic_O ( italic_n ) and the size of the data structure is ğ’ªâ¢(n)ğ’ªğ‘›\mathcal{O}(n)caligraphic_O ( italic_n ). To answer a prefix-suffix query PrefSufâ¢(i,j)PrefSufğ‘–ğ‘—\textsf{PrefSuf}(i,j)PrefSuf ( italic_i , italic_j ), we use the border tree ğ’¯ğ’¯\mathcal{T}caligraphic_T of Tğ‘‡Titalic_T and the border tree ğ’¯Rsuperscriptğ’¯ğ‘…\mathcal{T}^{R}caligraphic_T start_POSTSUPERSCRIPT italic_R end_POSTSUPERSCRIPT of TR=Tâ¢[nâˆ’1]â¢â€¦â¢Tâ¢[0]superscriptğ‘‡ğ‘…ğ‘‡delimited-[]ğ‘›1â€¦ğ‘‡delimited-[]0T^{R}=T[n-1]\ldots T[0]italic_T start_POSTSUPERSCRIPT italic_R end_POSTSUPERSCRIPT = italic_T [ italic_n - 1 ] â€¦ italic_T [ 0 ]. Given that Tğ‘‡Titalic_T is of a fixed length nğ‘›nitalic_n and the border tree has ğ’ªâ¢(logâ¡n)ğ’ªğ‘›\mathcal{O}(\log n)caligraphic_O ( roman_log italic_n ) depth, we need to check ğ’ªâ¢(logâ¡n)ğ’ªğ‘›\mathcal{O}(\log n)caligraphic_O ( roman_log italic_n ) pairs of states: one from ğ’¯ğ’¯\mathcal{T}caligraphic_T and one from ğ’¯Rsuperscriptğ’¯ğ‘…\mathcal{T}^{R}caligraphic_T start_POSTSUPERSCRIPT italic_R end_POSTSUPERSCRIPT. For each pair, we solve one linear equation in ğ’ªâ¢(1)ğ’ª1\mathcal{O}(1)caligraphic_O ( 1 ) time to check for the length constraints. The query time is ğ’ªâ¢(logâ¡n+Occ)ğ’ªğ‘›Occ\mathcal{O}(\log n+\textsf{Occ})caligraphic_O ( roman_log italic_n + Occ ). Gu, Farach, and Beigel [17] used prefix-suffix queries in their dynamic text indexing algorithm to efficiently locate the occurrences of a pattern spanning an edit operation in the text by maintaining the longest prefix and the longest suffix of the pattern occurring right before and right after the edit, respectively. Since its introduction [17], the border tree has been used for several pattern matching tasks (e.g., [14, 3, 4, 6]). Our contribution. We present a completely different and remarkably simple data structure that can be constructed in the optimal ğ’ªâ¢(n/logÏƒâ¡n)ğ’ªğ‘›subscriptğœğ‘›\mathcal{O}(n/\log_{\sigma}n)caligraphic_O ( italic_n / roman_log start_POSTSUBSCRIPT italic_Ïƒ end_POSTSUBSCRIPT italic_n ) time in the word RAM model and answers PrefSufâ¢(i,j)PrefSufğ‘–ğ‘—\textsf{PrefSuf}(i,j)PrefSuf ( italic_i , italic_j ) queries in the optimal ğ’ªâ¢(1)ğ’ª1\mathcal{O}(1)caligraphic_O ( 1 ) time. We remark that it is quite standard to represent the set of occurrences of a string Xğ‘‹Xitalic_X in another string Yğ‘ŒYitalic_Y with |Y|<2â¢|X|ğ‘Œ2ğ‘‹|Y|<2|X|| italic_Y | < 2 | italic_X | in ğ’ªâ¢(1)ğ’ª1\mathcal{O}(1)caligraphic_O ( 1 ) space due to the following folklore fact: Fact 1 ([25]). Let Xğ‘‹Xitalic_X and Yğ‘ŒYitalic_Y be strings with |Y|<2â¢|X|ğ‘Œ2ğ‘‹|Y|<2|X|| italic_Y | < 2 | italic_X |. The set of occurrences (starting positions) of Xğ‘‹Xitalic_X in Yğ‘ŒYitalic_Y forms a single arithmetic progression. However, to the best of our knowledge, the set of occurrences of Tğ‘‡Titalic_T in Tâ¢[0â¢..â¢i]â¢Tâ¢[jâ¢..â¢nâˆ’1]T[0\mathinner{.\,.}i]T[j\mathinner{.\,.}n-1]italic_T [ 0 start_ATOM . . end_ATOM italic_i ] italic_T [ italic_j start_ATOM . . end_ATOM italic_n - 1 ], for all i,jâˆˆ[0,n)ğ‘–ğ‘—0ğ‘›i,j\in[0,n)italic_i , italic_j âˆˆ [ 0 , italic_n ), has not been characterized before. To arrive at Theorem 1, we prove a structural lemma that lets us encode the output of PrefSufâ¢(i,j)PrefSufğ‘–ğ‘—\textsf{PrefSuf}(i,j)PrefSuf ( italic_i , italic_j ) queries for any i,jâˆˆ[0,n)ğ‘–ğ‘—0ğ‘›i,j\in[0,n)italic_i , italic_j âˆˆ [ 0 , italic_n ) in constant time and space. The border tree, and, in particular prefix-suffix queries, have been mainly used in dynamic text indexing algorithms, which however involve many other crucial primitives to arrive at their final query time. We show here instead a new direct application of our data structure in pattern matching on node-labeled graphs, a very active topic of research [12, 10, 6, 2]. In particular, we formalize bipartite pattern matching as a core problem that underlies any algorithm for pattern matching on node-labeled graphs. For intuition, consider two nodes uğ‘¢uitalic_u and vğ‘£vitalic_v in a directed graph where nodes are labeled by strings. Some suffixes of node uğ‘¢uitalic_u match some prefixes of a given pattern Pğ‘ƒPitalic_P, and some prefixes of node vğ‘£vitalic_v match some suffixes of Pğ‘ƒPitalic_P. We would like to have a data structure, constructed over Pğ‘ƒPitalic_P, that takes ğ’ªâ¢(1)ğ’ª1\mathcal{O}(1)caligraphic_O ( 1 ) time to process the directed edge (u,v)ğ‘¢ğ‘£(u,v)( italic_u , italic_v ). Namely, in this setting, we find all occurrences of Pğ‘ƒPitalic_P spanning at most two nodes of the graph. (It is trivial to find the occurrences of Pğ‘ƒPitalic_P in a single node using any linear-time pattern matching algorithm [20].) Indeed, the bipartite pattern matching problem has been (implicitly) introduced by Ascone et al. [6] for pattern matching on block graphs (a restricted version of node-labeled graphs [22, 13, 26]), and the border tree [17] was used to solve it. If we apply Theorem 1 on Pğ‘ƒPitalic_P, we can answer any such query in ğ’ªâ¢(1)ğ’ª1\mathcal{O}(1)caligraphic_O ( 1 ) time instead of ğ’ªâ¢(logâ¡|P|+Occ)ğ’ªğ‘ƒOcc\mathcal{O}(\log|P|+\textsf{Occ})caligraphic_O ( roman_log | italic_P | + Occ ) time. Paper organization. In Section 2, we present the proof of Theorem 1. In Section 3, we present the application of Theorem 1 on bipartite pattern matching. We conclude this paper in Section 4."
https://arxiv.org/html/2411.03570v1,Learning Constant-Depth Circuits inMalicious Noise Models,"The seminal work of Linial, Mansour, and Nisan gave a quasipolynomial-time algorithm for learning constant-depth circuits (ğ– ğ–¢0superscriptğ– ğ–¢0\mathsf{AC}^{0}sansserif_AC start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT) with respect to the uniform distribution on the hypercube. Extending their algorithm to the setting of malicious noise, where both covariates and labels can be adversarially corrupted, has remained open. Here we achieve such a result, inspired by recent work on learning with distribution shift. Our running time essentially matches their algorithm, which is known to be optimal assuming various cryptographic primitives.Our proof uses a simple outlier-removal method combined with Bravermanâ€™s theorem for fooling constant-depth circuits. We attain the best possible dependence on the noise rate and succeed in the harshest possible noise model (i.e., contamination or so-called â€œnasty noiseâ€).","In their famous paper, Linial, Mansour, and Nisan [LMN93] introduced the â€œlow-degreeâ€ algorithm for learning Boolean functions with respect to the uniform distribution on {Â±1}dsuperscriptplus-or-minus1ğ‘‘\{\pm 1\}^{d}{ Â± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT. The running time and sample complexity of their algorithm scales in terms of the Fourier concentration of the underlying concept class, and, using this framework, they obtained a quasipolynomial-time algorithm for learning constant-depth, polynomial-size circuits (ğ– ğ–¢0superscriptğ– ğ–¢0\mathsf{AC}^{0}sansserif_AC start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT). Prior work [KKMS08] had extended their result to the agnostic setting, where the labels can be adversarially corrupted, but the marginal distribution on inputs must still be uniform over {Â±1}dsuperscriptplus-or-minus1ğ‘‘\{\pm 1\}^{d}{ Â± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT. Remarkably, there had been no progress on this problem in the last three decades for malicious noise models where both covariates and labels can be adversarially corrupted [Val85, KL93]. In this paper, we completely resolve this problem and obtain a quasipolynomial-time algorithm for learning ğ– ğ–¢0superscriptğ– ğ–¢0\mathsf{AC}^{0}sansserif_AC start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT in the harshest possible noise model, the so-called â€œnasty noiseâ€ model of [BEK02]. We define this model below and refer to it simply as learning with contamination, in line with recent work in computationally efficient robust statistics (see e.g., [DK23]). Definition 1.1 (Learning from Contaminated Samples). A set of Nğ‘Nitalic_N labeled examples SÂ¯inpsubscriptÂ¯ğ‘†inp\bar{S}_{\mathrm{inp}}overÂ¯ start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_inp end_POSTSUBSCRIPT is an Î·ğœ‚\etaitalic_Î·-contaminated (uniform) sample with respect to some class ğ’âŠ†{{Â±1}dâ†’{Â±1}}ğ’â†’superscriptplus-or-minus1ğ‘‘plus-or-minus1\mathcal{C}\subseteq\{\{\pm 1\}^{d}\to\{\pm 1\}\}caligraphic_C âŠ† { { Â± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT â†’ { Â± 1 } }, where Nâˆˆâ„•ğ‘â„•N\in\mathbb{N}italic_N âˆˆ blackboard_N and Î·âˆˆ(0,1)ğœ‚01\eta\in(0,1)italic_Î· âˆˆ ( 0 , 1 ), if it is formed by an adversary as follows. 1. The adversary receives a set of Nğ‘Nitalic_N clean i.i.d. labeled examples SÂ¯clnsubscriptÂ¯ğ‘†cln\bar{S}_{\mathrm{cln}}overÂ¯ start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_cln end_POSTSUBSCRIPT, drawn from the uniform distribution over {Â±1}dsuperscriptplus-or-minus1ğ‘‘\{\pm 1\}^{d}{ Â± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT and labeled by some unknown concept fâˆ—superscriptğ‘“f^{*}italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT in ğ’ğ’\mathcal{C}caligraphic_C. 2. The adversary removes an arbitrary set SÂ¯remsubscriptÂ¯ğ‘†rem\bar{S}_{\mathrm{rem}}overÂ¯ start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_rem end_POSTSUBSCRIPT of âŒŠÎ·â¢NâŒ‹ğœ‚ğ‘\lfloor\eta N\rfloorâŒŠ italic_Î· italic_N âŒ‹ labeled examples from SÂ¯clnsubscriptÂ¯ğ‘†cln\bar{S}_{\mathrm{cln}}overÂ¯ start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_cln end_POSTSUBSCRIPT and substitutes it with an adversarial set of âŒŠÎ·â¢NâŒ‹ğœ‚ğ‘\lfloor\eta N\rfloorâŒŠ italic_Î· italic_N âŒ‹ labeled examples SÂ¯advsubscriptÂ¯ğ‘†adv\bar{S}_{\mathrm{adv}}overÂ¯ start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_adv end_POSTSUBSCRIPT. Namely, SÂ¯inp=(SÂ¯clnâˆ–SÂ¯rem)âˆªSÂ¯advsubscriptÂ¯ğ‘†inpsubscriptÂ¯ğ‘†clnsubscriptÂ¯ğ‘†remsubscriptÂ¯ğ‘†adv\bar{S}_{\mathrm{inp}}=(\bar{S}_{\mathrm{cln}}\setminus\bar{S}_{\mathrm{rem}})% \cup\bar{S}_{\mathrm{adv}}overÂ¯ start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_inp end_POSTSUBSCRIPT = ( overÂ¯ start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_cln end_POSTSUBSCRIPT âˆ– overÂ¯ start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_rem end_POSTSUBSCRIPT ) âˆª overÂ¯ start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_adv end_POSTSUBSCRIPT. For the corresponding unlabeled set Sinpsubscriptğ‘†inpS_{\mathrm{inp}}italic_S start_POSTSUBSCRIPT roman_inp end_POSTSUBSCRIPT, we say that it is an Î·ğœ‚\etaitalic_Î·-contaminated (uniform) sample. In this model, the goal of the learner is to output (with probability 1âˆ’Î´1ğ›¿1-\delta1 - italic_Î´) a hypothesis h:{Â±1}dâ†’{Â±1}:â„â†’superscriptplus-or-minus1ğ‘‘plus-or-minus1h:\{\pm 1\}^{d}\to\{\pm 1\}italic_h : { Â± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT â†’ { Â± 1 } such that â„™ğ±âˆ¼Unifâ¢({Â±1}d)[hâ¢(ğ±)â‰ fâˆ—â¢(ğ±)]â‰¤2â¢Î·+Ïµsubscriptâ„™similar-toğ±Unifsuperscriptplus-or-minus1ğ‘‘â„ğ±superscriptğ‘“ğ±2ğœ‚italic-Ïµ\operatorname*{\mathbb{P}}_{\mathbf{x}\sim\mathrm{Unif}(\{\pm 1\}^{d})}[h(% \mathbf{x})\neq f^{*}(\mathbf{x})]\leq 2\eta+\epsilonblackboard_P start_POSTSUBSCRIPT bold_x âˆ¼ roman_Unif ( { Â± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) end_POSTSUBSCRIPT [ italic_h ( bold_x ) â‰  italic_f start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( bold_x ) ] â‰¤ 2 italic_Î· + italic_Ïµ. The factor 2222 is known to be the best possible constant achievable by any algorithm [BEK02]. Although there is now a long line of research giving computationally efficient algorithms for learning Boolean function classes in malicious noise models, these algorithms primarily apply to geometric concept classes and continuous marginal distributions, such as halfspaces or intersections of halfspaces with respect to Gaussian or log-concave densities [KKMS08, KLS09, ABL17, DKS18, SZ21]. In particular, nothing was known for the case of ğ– ğ–¢ğŸ¢superscriptğ– ğ–¢0\mathsf{AC^{0}}sansserif_AC start_POSTSUPERSCRIPT sansserif_0 end_POSTSUPERSCRIPT. Our main theorem is as follows: Theorem 1.2. For any s,â„“,dâˆˆâ„•ğ‘ â„“ğ‘‘â„•s,\ell,d\in{\mathbb{N}}italic_s , roman_â„“ , italic_d âˆˆ blackboard_N, and Ïµ,Î´âˆˆ(0,1)italic-Ïµğ›¿01\epsilon,\delta\in(0,1)italic_Ïµ , italic_Î´ âˆˆ ( 0 , 1 ), there is an algorithm that learns the class of ğ– ğ–¢0superscriptğ– ğ–¢0\mathsf{AC}^{0}sansserif_AC start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT circuits of size sğ‘ sitalic_s and depth â„“â„“\ellroman_â„“ and achieves error 2â¢Î·+Ïµ2ğœ‚italic-Ïµ2\eta+\epsilon2 italic_Î· + italic_Ïµ, with running time and sample complexity dOâ¢(k)â¢logâ¡(1/Î´)superscriptğ‘‘ğ‘‚ğ‘˜1ğ›¿d^{O(k)}\log(1/\delta)italic_d start_POSTSUPERSCRIPT italic_O ( italic_k ) end_POSTSUPERSCRIPT roman_log ( 1 / italic_Î´ ), where k=(logâ¡(s))Oâ¢(â„“)â¢logâ¡(1/Ïµ)ğ‘˜superscriptğ‘ ğ‘‚â„“1italic-Ïµk={(\log(s))^{O(\ell)}\log(1/\epsilon)}italic_k = ( roman_log ( italic_s ) ) start_POSTSUPERSCRIPT italic_O ( roman_â„“ ) end_POSTSUPERSCRIPT roman_log ( 1 / italic_Ïµ ), from contaminated samples of any noise rate Î·ğœ‚\etaitalic_Î·. Our running time essentially matches the Linial, Mansour, and Nisan result, which is known to be optimal assuming various cryptographic primitives [Kha95]. More generally, we prove that any concept class ğ’ğ’\mathcal{C}caligraphic_C that admits â„“1subscriptâ„“1\ell_{1}roman_â„“ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-sandwiching polynomials of degree kğ‘˜kitalic_k can be learned in time dOâ¢(k)superscriptğ‘‘ğ‘‚ğ‘˜d^{O(k)}italic_d start_POSTSUPERSCRIPT italic_O ( italic_k ) end_POSTSUPERSCRIPT from contaminated samples. Recent work due to [GSSV24] had obtained a similar result achieving the weaker bound of Oâ¢(Î·)+Ïµğ‘‚ğœ‚italic-ÏµO(\eta)+\epsilonitalic_O ( italic_Î· ) + italic_Ïµ for learning functions with â„“2subscriptâ„“2\ell_{2}roman_â„“ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT-sandwiching polynomials. Crucially, it remains unclear how to obtain such â„“2subscriptâ„“2\ell_{2}roman_â„“ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT sandwiching approximators for constant depth circuits 111Bravermanâ€™s celebrated result on ğ– ğ–¢ğŸ¢superscriptğ– ğ–¢0\mathsf{AC^{0}}sansserif_AC start_POSTSUPERSCRIPT sansserif_0 end_POSTSUPERSCRIPT [Bra08] obtains only â„“1subscriptâ„“1\ell_{1}roman_â„“ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-sandwiching., and so their result does not apply here. In 2005, Kalai et al. [KKMS08] showed that â„“1subscriptâ„“1\ell_{1}roman_â„“ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-approximation suffices for agnostic learning. Here we complete the analogy for malicious learning, showing that â„“1subscriptâ„“1\ell_{1}roman_â„“ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-sandwiching implies learnability with respect to contamination. Proof Overview. The input set SÂ¯inpsubscriptÂ¯ğ‘†inp\bar{S}_{\mathrm{inp}}overÂ¯ start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_inp end_POSTSUBSCRIPT is Î·ğœ‚\etaitalic_Î·-contaminated. This might make it hard to find a hypothesis with near-optimal error on SÂ¯inpsubscriptÂ¯ğ‘†inp\bar{S}_{\mathrm{inp}}overÂ¯ start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_inp end_POSTSUBSCRIPT. However, we are only interested in finding a hypothesis with error 2â¢Î·+Ïµ2ğœ‚italic-Ïµ2\eta+\epsilon2 italic_Î· + italic_Ïµ on the clean distribution, which is structured (in particular, the marginal distribution on the features is uniform over {Â±1}dsuperscriptplus-or-minus1ğ‘‘\{\pm 1\}^{d}{ Â± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT). In order to take advantage of the structure of the clean distribution despite only having access to the contaminated sample, we make use of the notion of sandwiching polynomials: Definition 1.3 (Sandwiching polynomials). Let f:{Â±1}dâ†’{Â±1}:ğ‘“â†’superscriptplus-or-minus1ğ‘‘plus-or-minus1f:\{\pm 1\}^{d}\to\{\pm 1\}italic_f : { Â± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT â†’ { Â± 1 }. We say that the (â„“1subscriptâ„“1\ell_{1}roman_â„“ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT) Ïµitalic-Ïµ\epsilonitalic_Ïµ-sandwiching degree of fğ‘“fitalic_f with respect to the uniform distribution over the hypercube {Â±1}dsuperscriptplus-or-minus1ğ‘‘\{\pm 1\}^{d}{ Â± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT is kğ‘˜kitalic_k if there are polynomials pup,pdown:{Â±1}dâ†’â„:subscriptğ‘upsubscriptğ‘downâ†’superscriptplus-or-minus1ğ‘‘â„p_{\mathrm{up}},p_{\mathrm{down}}:\{\pm 1\}^{d}\to{\mathbb{R}}italic_p start_POSTSUBSCRIPT roman_up end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT roman_down end_POSTSUBSCRIPT : { Â± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT â†’ blackboard_R of degree at most kğ‘˜kitalic_k such that (1) pdownâ¢(ğ±)â‰¤fâ¢(ğ±)â‰¤pupâ¢(ğ±)subscriptğ‘downğ±ğ‘“ğ±subscriptğ‘upğ±p_{\mathrm{down}}(\mathbf{x})\leq f(\mathbf{x})\leq p_{\mathrm{up}}(\mathbf{x})italic_p start_POSTSUBSCRIPT roman_down end_POSTSUBSCRIPT ( bold_x ) â‰¤ italic_f ( bold_x ) â‰¤ italic_p start_POSTSUBSCRIPT roman_up end_POSTSUBSCRIPT ( bold_x ) for all ğ±âˆˆ{Â±1}dğ±superscriptplus-or-minus1ğ‘‘\mathbf{x}\in\{\pm 1\}^{d}bold_x âˆˆ { Â± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT and (2) ğ”¼ğ±âˆ¼Unifâ¡({Â±1}d)[pupâ¢(ğ±)âˆ’pdownâ¢(ğ±)]â‰¤Ïµsubscriptğ”¼similar-toğ±Unifsuperscriptplus-or-minus1ğ‘‘subscriptğ‘upğ±subscriptğ‘downğ±italic-Ïµ\operatorname*{\mathbb{E}}_{\mathbf{x}\sim\operatorname{Unif}(\{\pm 1\}^{d})}[% p_{\mathrm{up}}(\mathbf{x})-p_{\mathrm{down}}(\mathbf{x})]\leq\epsilonblackboard_E start_POSTSUBSCRIPT bold_x âˆ¼ roman_Unif ( { Â± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) end_POSTSUBSCRIPT [ italic_p start_POSTSUBSCRIPT roman_up end_POSTSUBSCRIPT ( bold_x ) - italic_p start_POSTSUBSCRIPT roman_down end_POSTSUBSCRIPT ( bold_x ) ] â‰¤ italic_Ïµ. The sandwiching degree of size-sğ‘ sitalic_s depth-â„“â„“\ellroman_â„“ ğ– ğ–¢0superscriptğ– ğ–¢0\mathsf{AC}^{0}sansserif_AC start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT circuits is bounded by k=(logâ¡(s))Oâ¢(â„“)â¢logâ¡(1/Ïµ)ğ‘˜superscriptğ‘ ğ‘‚â„“1italic-Ïµk=(\log(s))^{O(\ell)}\log(1/\epsilon)italic_k = ( roman_log ( italic_s ) ) start_POSTSUPERSCRIPT italic_O ( roman_â„“ ) end_POSTSUPERSCRIPT roman_log ( 1 / italic_Ïµ ), due to the result of Braverman on fooling constant-depth circuits (see Theorem 4.2 from [Bra08, Tal17, HS19]). Suppose that SÂ¯Â¯ğ‘†\bar{S}overÂ¯ start_ARG italic_S end_ARG is a subset of SÂ¯inpsubscriptÂ¯ğ‘†inp\bar{S}_{\mathrm{inp}}overÂ¯ start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_inp end_POSTSUBSCRIPT that preserves the expectations of low-degree and non-negative polynomials (e.g., pupâˆ’pdownsubscriptğ‘upsubscriptğ‘downp_{\mathrm{up}}-p_{\mathrm{down}}italic_p start_POSTSUBSCRIPT roman_up end_POSTSUBSCRIPT - italic_p start_POSTSUBSCRIPT roman_down end_POSTSUBSCRIPT) compared to the uniform distribution. Under this condition, low-degree polynomial regression gives a hypothesis with near-optimal error on SÂ¯Â¯ğ‘†\bar{S}overÂ¯ start_ARG italic_S end_ARG (see Section 4). We show in Lemma 3.1 that a simple procedure that iteratively removes samples from SÂ¯inpsubscriptÂ¯ğ‘†inp\bar{S}_{\mathrm{inp}}overÂ¯ start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_inp end_POSTSUBSCRIPT can be used to form such a set SÂ¯Â¯ğ‘†\bar{S}overÂ¯ start_ARG italic_S end_ARG (that preserves the expectations of non-negative, degree-kğ‘˜kitalic_k and low-expectation polynomials) and, moreover, this procedure removes more contaminated points than clean points. The last property is important, because it implies that SÂ¯Â¯ğ‘†\bar{S}overÂ¯ start_ARG italic_S end_ARG is representative for the ground truth distribution, i.e., any near-optimal hypothesis for SÂ¯Â¯ğ‘†\bar{S}overÂ¯ start_ARG italic_S end_ARG will also have error 2â¢Î·+Ïµ2ğœ‚italic-Ïµ2\eta+\epsilon2 italic_Î· + italic_Ïµ on the ground truth. This is possible because the only way the adversary can significantly increase the expectation of a non-negative polynomial pğ‘pitalic_p is by inserting examples ğ±ğ±\mathbf{x}bold_x where pâ¢(ğ±)ğ‘ğ±p(\mathbf{x})italic_p ( bold_x ) is unreasonably large compared to the typical values of pğ‘pitalic_p over the uniform distribution. Our algorithm iteratively finds the non-negative polynomial qğ‘qitalic_q with the largest expectation over a given set through a simple linear program and then removes the points ğ±ğ±\mathbf{x}bold_x for which qâ¢(x)ğ‘ğ‘¥q(x)italic_q ( italic_x ) is large. Our iterative outlier removal procedure is inspired by prior work on TDS learning (Testable Learning with Distribution Shift) and PQ learning [KSV24, GSSV24] as well as the work of [DKS18] on learning geometric concepts from contaminated examples. Both of these works use outlier removal procedures that give bounds on the variance of polynomials rather than the expectation of non-negative polynomials and, instead of linear programming, they use spectral algorithms."
https://arxiv.org/html/2411.03451v1,Redundancy Is All You Need,"The seminal work of BenczÃºr and Karger demonstrated cut sparsifiers of near-linear size, with several applications throughout theoretical computer science. Subsequent extensions have yielded sparsifiers for hypergraph cuts and more recently linear codes over Abelian groups. A decade ago, Kogan and Krauthgamer asked about the sparsifiability of arbitrary constraint satisfaction problems (CSPs). For this question, a trivial lower bound is the size of a non-redundant CSP instance, which admits, for each constraint, an assignment satisfying only that constraint (so that no constraint can be dropped by the sparsifier). For instance, for graph cuts, spanning trees are non-redundant instances.Our main result is that redundant clauses are sufficient for sparsification: for any CSP predicate Rğ‘…Ritalic_R, every unweighted instance of CSPâ¡(R)CSPğ‘…\operatorname{CSP}(R)roman_CSP ( italic_R ) has a sparsifier of size at most its non-redundancy (up to polylog factors). For weighted instances, we similarly pin down the sparsifiability to the so-called chain length of the predicate. These results precisely determine the extent to which any CSP can be sparsified. A key technical ingredient in our work is a novel application of the entropy method from Gilmerâ€™s recent breakthrough on the union-closed sets conjecture.As an immediate consequence of our main theorem, a number of results in the non-redundancy literature immediately extend to CSP sparsification. We also contribute new techniques for understanding the non-redundancy of CSP predicates. In particular, we give an explicit family of predicates whose non-redundancy roughly corresponds to the structure of matching vector families in coding theory. By adapting methods from the matching vector codes literature, we are able to construct an explicit predicate whose non-redundancy lies between Î©â¢(n1.5)Î©superscriptğ‘›1.5\Omega(n^{1.5})roman_Î© ( italic_n start_POSTSUPERSCRIPT 1.5 end_POSTSUPERSCRIPT ) and O~â¢(n1.6)~ğ‘‚superscriptğ‘›1.6\widetilde{O}(n^{1.6})over~ start_ARG italic_O end_ARG ( italic_n start_POSTSUPERSCRIPT 1.6 end_POSTSUPERSCRIPT ), the first example with a provably non-integral exponent.","The broad goal in sparsification is to replace an object by a more compact surrogate, typically a carefully chosen subsample, that preserves the behavior of the object under some metric of interest. For instance, for preserving cuts in undirected graphs, the influential works of Karger [Kar93] and BenczÃºr and Karger [BK96] showed that every graph has an edge-weighted subgraph with near-linear number of edges that preserves the value of all (edge) cuts up to a (1Â±Ïµ)plus-or-minus1italic-Ïµ(1\pm\epsilon)( 1 Â± italic_Ïµ ) multiplicative factor. These papers have had a substantial impact in shaping the last thirty years of work in areas such as spectral sparsifiers [ST11, BSS12, LS18], clustering [KVV04, SPR11], hypergraph sparsifiers [KK15, CKN20, KKTY21, KK23, KPS24c], linear solvers [ST04, Vis13, KMP14], convex optimization [LS14, AK16, Tod16], sketching/streaming algorithms [AG09, AGM12b, AGM12a, ACK+16, McG14, KLM+17, BHM+21], max-flow/min-cut algorithms [LR99, CKM+11, KLOS14, CKL+22], machine learning [LCY+21, CSZ22, ZSW+23, GBY+24], submodular functions [KK23, Sch24, Raf24, Qua24], differential privacy [BBDS12, AU19], PageRank [Chu14], and even theoretical physics [HKTH16, Van18, TN22], among many other works. Among the multiple exciting dimensions in which cut sparsification has been generalized, we now highlight two which form the backdrop for our work. Note that the graph cut problem can be modeled by the arity-two Boolean constraint x+y=1(mod2)ğ‘¥ğ‘¦annotated1pmod2x+y=1\pmod{2}italic_x + italic_y = 1 start_MODIFIER ( roman_mod start_ARG 2 end_ARG ) end_MODIFIER. One can thus generalize cut sparsification by allowing for arbitrary constraints (of any arity over some finite domain) as considered in the field of constraint satisfaction problems (CSPs), leading to CSP sparsification. This direction was proposed by Kogan and Krauthgamer [KK15] in their work on hypergraph cut sparsifiers, where the not-all-equal constraint captures hypergraph cut. As as special case, arbitrary binary CSPs (where each constraint has two variables) were studied in [FK17] for the Boolean domain and in [BÅ½20] for general domains, leading to a dichotomy: either near-linear sized sparsifiers exist, or no improvement over quadratic is possible. In another direction, one can instead look toward more general structures to sparsify. For instance, a recent line of work by Khanna, Putterman, and Sudan turned toward sparsifying linear codes [KPS24a], or more generally subgroups of powers of Abelian groups [KPS24b]. Beyond being algorithmically efficient [KPS24b], these structural results have led to exciting new results in CSP sparsification by constructing optimal sparsifiers when the constraints can be embedded into linear/Abelian equations. In this work, we obtain sparsifiers encompassing both these generalizations via a unified approach to sparsification of non-linear codes. The resulting sparsifiers for CSPs have optimal asymptotic size up to polylogarithmic factors, for every choice of predicate defining the CSP. In other words, we pinpoint the optimal extent to which an arbitrary CSP can be sparsified.111In this work we focus on the existence of sparsifiers, which is already highly non-trivial (e.g., [KPS24a, BÅ½20] are also non-algorithmic). Future directions (and barriers) for algorithmic aspects are briefly discussed in Sections 1.7 and 9. 1.1 Non-linear code sparsification We first state our result for codes as it is very general and crisply stated, and then turn to the consequences and further new results for CSPs. For a non-linear code CâŠ†{0,1}mğ¶superscript01ğ‘šC\subseteq\{0,1\}^{m}italic_C âŠ† { 0 , 1 } start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT, an Îµğœ€\varepsilonitalic_Îµ-sparsifier (for a parameter Îµâˆˆ(0,1)ğœ€01\varepsilon\in(0,1)italic_Îµ âˆˆ ( 0 , 1 )) is a weight function w:[m]â†’â„â‰¥0:ğ‘¤â†’delimited-[]ğ‘šsubscriptâ„absent0w:[m]\to\mathbb{R}_{\geq 0}italic_w : [ italic_m ] â†’ blackboard_R start_POSTSUBSCRIPT â‰¥ 0 end_POSTSUBSCRIPT such that for every codeword cğ‘citalic_c, adding up the weights of its nonzero positions, i.e., âˆ‘iwâ¢(i)â¢cisubscriptğ‘–ğ‘¤ğ‘–subscriptğ‘ğ‘–\sum_{i}w(i)c_{i}âˆ‘ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_w ( italic_i ) italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, is an accurate estimate of the Hamming weight of cğ‘citalic_c (i.e., âˆ‘icisubscriptğ‘–subscriptğ‘ğ‘–\sum_{i}c_{i}âˆ‘ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT) to within a (1Â±Îµ)plus-or-minus1ğœ€(1\pm\varepsilon)( 1 Â± italic_Îµ ) multiplicative factor (Definition 2.8). The goal is to minimize the support of wğ‘¤witalic_w (i.e., the number of nonzero entries wâ¢(i)ğ‘¤ğ‘–w(i)italic_w ( italic_i )), and the minimum value is called Îµğœ€\varepsilonitalic_Îµ-sparsifiability of Cğ¶Citalic_C and is denoted SPRâ¡(C,Îµ)SPRğ¶ğœ€\operatorname{SPR}(C,\varepsilon)roman_SPR ( italic_C , italic_Îµ ). One of our main results is an upper bound on the sparsifiability in terms of a natural combinatorial parameter of the code called its non-redundancy NRDâ¡(C)NRDğ¶\operatorname{NRD}(C)roman_NRD ( italic_C ), defined as follows: NRDâ¡(C)NRDğ¶\operatorname{NRD}(C)roman_NRD ( italic_C ) is the size of the largest subset of indices IâŠ†[m]ğ¼delimited-[]ğ‘šI\subseteq[m]italic_I âŠ† [ italic_m ] such that for each iâˆˆIğ‘–ğ¼i\in Iitalic_i âˆˆ italic_I, there is a codeword câˆˆCğ‘ğ¶c\in Citalic_c âˆˆ italic_C with ci=1subscriptğ‘ğ‘–1c_{i}=1italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1 and ciâ€²=0subscriptğ‘superscriptğ‘–â€²0c_{i^{\prime}}=0italic_c start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT end_POSTSUBSCRIPT = 0 for iâ€²âˆˆIâˆ–{i}superscriptğ‘–â€²ğ¼ğ‘–i^{\prime}\in I\setminus\{i\}italic_i start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT âˆˆ italic_I âˆ– { italic_i }. In other words, if we imagine the code as a matrix whose rows are codewords, its non-redundancy is largest square submatrix which is a permutation matrix. Our result can then be stated compactly as follows. Theorem 1.1 (Main). For all CâŠ†{0,1}mğ¶superscript01ğ‘šC\subseteq\{0,1\}^{m}italic_C âŠ† { 0 , 1 } start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT and Îµâˆˆ(0,1)ğœ€01\varepsilon\in(0,1)italic_Îµ âˆˆ ( 0 , 1 ), SPRâ¡(C,Îµ)=Oâ¢(NRDâ¡(C)â¢(logâ¡m)6/Îµ2).SPRğ¶ğœ€ğ‘‚NRDğ¶superscriptğ‘š6superscriptğœ€2\operatorname{SPR}(C,\varepsilon)=O(\operatorname{NRD}(C)(\log m)^{6}/% \varepsilon^{2}).roman_SPR ( italic_C , italic_Îµ ) = italic_O ( roman_NRD ( italic_C ) ( roman_log italic_m ) start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT / italic_Îµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) . To see how our theorem generalizes linear code sparsification [KPS24a, KPS24b], let ğ”½ğ”½\mathbb{F}blackboard_F be a (finite) field and let VâŠ†ğ”½mğ‘‰superscriptğ”½ğ‘šV\subseteq\mathbb{F}^{m}italic_V âŠ† blackboard_F start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT be a subspace. Let C:={(ğŸâ¢[v1=0],â€¦,ğŸâ¢[vm=0]):vâˆˆV}assignğ¶conditional-set1delimited-[]subscriptğ‘£10â€¦1delimited-[]subscriptğ‘£ğ‘š0ğ‘£ğ‘‰C:=\{({\bf 1}[v_{1}=0],\ldots,{\bf 1}[v_{m}=0]):v\in V\}italic_C := { ( bold_1 [ italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0 ] , â€¦ , bold_1 [ italic_v start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT = 0 ] ) : italic_v âˆˆ italic_V } be the zero/non-zero pattern of Vğ‘‰Vitalic_V. Then, any Îµğœ€\varepsilonitalic_Îµ-sparsifier of Cğ¶Citalic_C is an Îµğœ€\varepsilonitalic_Îµ-sparsifier of Vğ‘‰Vitalic_V and NRDâ¡(C)=dimVNRDğ¶dimensionğ‘‰\operatorname{NRD}(C)=\dim Vroman_NRD ( italic_C ) = roman_dim italic_V. In fact, for any finite group GğºGitalic_G and subgroup Hâ‰¤Gmğ»superscriptğºğ‘šH\leq G^{m}italic_H â‰¤ italic_G start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT, we can show for the corresponding Cğ¶Citalic_C that NRDâ¡(C)â‰¤log2â¡|H|NRDğ¶subscript2ğ»\operatorname{NRD}(C)\leq\log_{2}|H|roman_NRD ( italic_C ) â‰¤ roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | italic_H | (see Theorem 5.1), matching [KPS24b]â€™s result for Abelian groups (modulo their efficiency). If we view CâŠ†{0,1}mğ¶superscript01ğ‘šC\subseteq\{0,1\}^{m}italic_C âŠ† { 0 , 1 } start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT as a set family over the universe [m]delimited-[]ğ‘š[m][ italic_m ], the above result in effect says that the non-redundancy NRDâ¡(C)NRDğ¶\operatorname{NRD}(C)roman_NRD ( italic_C ) plays the role of the VC dimension of Cğ¶Citalic_C when the goal is to estimate the size of the set câˆˆCğ‘ğ¶c\in Citalic_c âˆˆ italic_C rather than learn cğ‘citalic_c itself. In fact, it turns out that NRDâ¡(C)NRDğ¶\operatorname{NRD}(C)roman_NRD ( italic_C ) is precisely the VC dimension of the union-closure of Cğ¶Citalic_C. This connection to union-closed families plays a crucial role in the proof of Theorem 1.1. See the technical overview (Section 1.6) for more details, including discussion of a significantly simpler O~Îµâ¢(NRDâ¡(C)â¢logâ¡|C|)subscript~ğ‘‚ğœ€NRDğ¶ğ¶\widetilde{O}_{\varepsilon}(\operatorname{NRD}(C)\log|C|)over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_Îµ end_POSTSUBSCRIPT ( roman_NRD ( italic_C ) roman_log | italic_C | )-sized sparsifier. 1.2 CSP sparsification We now turn to (unweighted222The weighted case is discussed in Section 1.4.) CSP sparsification. For a relation RâŠ†Drğ‘…superscriptğ·ğ‘ŸR\subseteq D^{r}italic_R âŠ† italic_D start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT of arity rğ‘Ÿritalic_r over a finite domain Dğ·Ditalic_D, an instance Î¨Î¨\Psiroman_Î¨ of the CSPâ¡(R)CSPğ‘…\operatorname{CSP}(R)roman_CSP ( italic_R ) problem consists a variable set Xğ‘‹Xitalic_X and a constraint set YâŠ†Xrğ‘Œsuperscriptğ‘‹ğ‘ŸY\subseteq X^{r}italic_Y âŠ† italic_X start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT. An assignment Ïƒ:Xâ†’D:ğœâ†’ğ‘‹ğ·\sigma:X\to Ditalic_Ïƒ : italic_X â†’ italic_D satisfies a constraint y=(x1,x2,â€¦,xr)âˆˆYğ‘¦subscriptğ‘¥1subscriptğ‘¥2â€¦subscriptğ‘¥ğ‘Ÿğ‘Œy=(x_{1},x_{2},\dots,x_{r})\in Yitalic_y = ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_x start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ) âˆˆ italic_Y if (Ïƒâ¢(x1),Ïƒâ¢(x2),â€¦,Ïƒâ¢(xr))âˆˆRğœsubscriptğ‘¥1ğœsubscriptğ‘¥2â€¦ğœsubscriptğ‘¥ğ‘Ÿğ‘…(\sigma(x_{1}),\sigma(x_{2}),\dots,\sigma(x_{r}))\in R( italic_Ïƒ ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , italic_Ïƒ ( italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) , â€¦ , italic_Ïƒ ( italic_x start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ) ) âˆˆ italic_R. The value valâ¡(Î¨,Ïƒ)valÎ¨ğœ\operatorname{val}(\Psi,\sigma)roman_val ( roman_Î¨ , italic_Ïƒ ) of an assignment Ïƒğœ\sigmaitalic_Ïƒ is the number of constraints yâˆˆYğ‘¦ğ‘Œy\in Yitalic_y âˆˆ italic_Y that it satisfies. Similarly, for a weight function w:Yâ†’â„â‰¥0:ğ‘¤â†’ğ‘Œsubscriptâ„absent0w:Y\to\mathbb{R}_{\geq 0}italic_w : italic_Y â†’ blackboard_R start_POSTSUBSCRIPT â‰¥ 0 end_POSTSUBSCRIPT, the weighted value valâ¡(Î¨,w,Ïƒ)valÎ¨ğ‘¤ğœ\operatorname{val}(\Psi,w,\sigma)roman_val ( roman_Î¨ , italic_w , italic_Ïƒ ) is the sum of weights wâ¢(y)ğ‘¤ğ‘¦w(y)italic_w ( italic_y ) of all constraints yâˆˆYğ‘¦ğ‘Œy\in Yitalic_y âˆˆ italic_Y that Ïƒğœ\sigmaitalic_Ïƒ satisfies. The goal in CSP sparsification is to output a weight function w:Yâ†’â„â‰¥0:ğ‘¤â†’ğ‘Œsubscriptâ„absent0w:Y\to\mathbb{R}_{\geq 0}italic_w : italic_Y â†’ blackboard_R start_POSTSUBSCRIPT â‰¥ 0 end_POSTSUBSCRIPT of small support, such that for every assignment Ïƒ:Xâ†’D:ğœâ†’ğ‘‹ğ·\sigma:X\to Ditalic_Ïƒ : italic_X â†’ italic_D, (1âˆ’Îµ)â¢valâ¡(Î¨,Ïƒ)â‰¤valâ¡(Î¨,w,Ïƒ)â‰¤(1+Îµ)â¢valâ¡(Î¨,Ïƒ),1ğœ€valÎ¨ğœvalÎ¨ğ‘¤ğœ1ğœ€valÎ¨ğœ(1-\varepsilon)\operatorname{val}(\Psi,\sigma)\leq\operatorname{val}(\Psi,w,% \sigma)\leq(1+\varepsilon)\operatorname{val}(\Psi,\sigma)\ ,( 1 - italic_Îµ ) roman_val ( roman_Î¨ , italic_Ïƒ ) â‰¤ roman_val ( roman_Î¨ , italic_w , italic_Ïƒ ) â‰¤ ( 1 + italic_Îµ ) roman_val ( roman_Î¨ , italic_Ïƒ ) , and minimum such support size is denoted SPRâ¡(Î¨,Îµ)SPRÎ¨ğœ€\operatorname{SPR}(\Psi,\varepsilon)roman_SPR ( roman_Î¨ , italic_Îµ ). The Îµğœ€\varepsilonitalic_Îµ-sparsifiability of the relation RâŠ†Drğ‘…superscriptğ·ğ‘ŸR\subseteq D^{r}italic_R âŠ† italic_D start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT, as a function of number of variables, is defined to the maximum (i.e., worst-case) value of SPRâ¡(Î¨,Îµ)SPRÎ¨ğœ€\operatorname{SPR}(\Psi,\varepsilon)roman_SPR ( roman_Î¨ , italic_Îµ ) over all nğ‘›nitalic_n-variables instances Î¨Î¨\Psiroman_Î¨ of CSPâ¡(R)CSPğ‘…\operatorname{CSP}(R)roman_CSP ( italic_R ). We denote it by SPRâ¡(R,n,Îµ)SPRğ‘…ğ‘›ğœ€\operatorname{SPR}(R,n,\varepsilon)roman_SPR ( italic_R , italic_n , italic_Îµ ) and it is the chief object of our study. Note that this is for the unweighted case, see Section 1.4 how this result can be (tightly) applied to the weighted case. Let us note an obvious obstruction to sparsification. Suppose we have an instance Î¨=(X,Y)Î¨ğ‘‹ğ‘Œ\Psi=(X,Y)roman_Î¨ = ( italic_X , italic_Y ) of CSPâ¡(R)CSPğ‘…\operatorname{CSP}(R)roman_CSP ( italic_R ) such that for each of its constraints yâˆˆYğ‘¦ğ‘Œy\in Yitalic_y âˆˆ italic_Y, there is an assignment Ïƒy:Xâ†’D:subscriptğœğ‘¦â†’ğ‘‹ğ·\sigma_{y}:X\to Ditalic_Ïƒ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT : italic_X â†’ italic_D that satisfies only yğ‘¦yitalic_y and no other constraint. Then clearly Î¨Î¨\Psiroman_Î¨ cannot be sparsified at allâ€”dropping any constraint yğ‘¦yitalic_y would make the value of Ïƒysubscriptğœğ‘¦\sigma_{y}italic_Ïƒ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT drop from 1111 to 00. We call such an instance a non-redundant instance of CSPâ¡(RÂ¯)CSPÂ¯ğ‘…\operatorname{CSP}(\overline{R})roman_CSP ( overÂ¯ start_ARG italic_R end_ARG ), where RÂ¯=Drâˆ–RÂ¯ğ‘…superscriptğ·ğ‘Ÿğ‘…\overline{R}=D^{r}\setminus RoverÂ¯ start_ARG italic_R end_ARG = italic_D start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT âˆ– italic_R (cf., [BCH+13, BCK20]).333We use RÂ¯Â¯ğ‘…\overline{R}overÂ¯ start_ARG italic_R end_ARG rather than Rğ‘…Ritalic_R due to the conventions of each community. See Remark 2.6 for deeper technical reasons. As introduced by Bessiere, Carbonnel, and Katsirelos [BCK20], we denote the size of the largest such non-redundant instance of CSPâ¡(RÂ¯)CSPÂ¯ğ‘…\operatorname{CSP}(\overline{R})roman_CSP ( overÂ¯ start_ARG italic_R end_ARG ) on nğ‘›nitalic_n-variables by NRDâ¡(RÂ¯,n)NRDÂ¯ğ‘…ğ‘›\operatorname{NRD}(\overline{R},n)roman_NRD ( overÂ¯ start_ARG italic_R end_ARG , italic_n ) and call it the non-redundancy of RÂ¯Â¯ğ‘…\overline{R}overÂ¯ start_ARG italic_R end_ARG. Thus a trivial lower bound on sparsifiability of CSPâ¡(R)CSPğ‘…\operatorname{CSP}(R)roman_CSP ( italic_R ), regardless of the choice of Îµâˆˆ(0,1)ğœ€01\varepsilon\in(0,1)italic_Îµ âˆˆ ( 0 , 1 ), is given by SPRâ¡(R,n,Îµ)â‰¥NRDâ¡(RÂ¯,n),SPRğ‘…ğ‘›ğœ€NRDÂ¯ğ‘…ğ‘›\displaystyle\operatorname{SPR}(R,n,\varepsilon)\geq\operatorname{NRD}(% \overline{R},n)\ ,roman_SPR ( italic_R , italic_n , italic_Îµ ) â‰¥ roman_NRD ( overÂ¯ start_ARG italic_R end_ARG , italic_n ) , (1) and this holds even if the goal is merely to preserve which assignments have nonzero value. Rather remarkably, this simplistic lower bound can be met and one can sparsify all the way down to NRDâ¡(RÂ¯,n)NRDÂ¯ğ‘…ğ‘›\operatorname{NRD}(\overline{R},n)roman_NRD ( overÂ¯ start_ARG italic_R end_ARG , italic_n ) times polylogarithmic factors! In fact, this turns out to be an easy corollary of Theorem 1.1. One can associate a canonical code CÎ¨âŠ†{0,1}Ysubscriptğ¶Î¨superscript01ğ‘ŒC_{\Psi}\subseteq\{0,1\}^{Y}italic_C start_POSTSUBSCRIPT roman_Î¨ end_POSTSUBSCRIPT âŠ† { 0 , 1 } start_POSTSUPERSCRIPT italic_Y end_POSTSUPERSCRIPT with any CSPâ¡(R)CSPğ‘…\operatorname{CSP}(R)roman_CSP ( italic_R ) instance Î¨=(X,Y)Î¨ğ‘‹ğ‘Œ\Psi=(X,Y)roman_Î¨ = ( italic_X , italic_Y ) whose codewords cÏƒsubscriptğ‘ğœc_{\sigma}italic_c start_POSTSUBSCRIPT italic_Ïƒ end_POSTSUBSCRIPT correspond to the assignments Ïƒ:Xâ†’D:ğœâ†’ğ‘‹ğ·\sigma:X\to Ditalic_Ïƒ : italic_X â†’ italic_D, and cÏƒ,ysubscriptğ‘ğœğ‘¦c_{\sigma,y}italic_c start_POSTSUBSCRIPT italic_Ïƒ , italic_y end_POSTSUBSCRIPT is 1111 precisely when Ïƒğœ\sigmaitalic_Ïƒ satisfies yğ‘¦yitalic_y. It is easy to check that CSP sparsification of Î¨Î¨\Psiroman_Î¨ reduces to code sparsification of CÎ¨subscriptğ¶Î¨C_{\Psi}italic_C start_POSTSUBSCRIPT roman_Î¨ end_POSTSUBSCRIPT, and the non-redundancy of Cğ¶Citalic_C equals the size of the largest non-redundant sub-instance of Î¨Î¨\Psiroman_Î¨ (viewed as an instance of CSPâ¡(RÂ¯)CSPÂ¯ğ‘…\operatorname{CSP}(\overline{R})roman_CSP ( overÂ¯ start_ARG italic_R end_ARG )). Combining Theorem 1.1 and (1), we therefore have our main result pinning down the sparsifiability of every CSP up to polylogarithmic factors. Theorem 1.2. For every nonempty RâŠŠDrğ‘…superscriptğ·ğ‘ŸR\subsetneq D^{r}italic_R âŠŠ italic_D start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT and Îµâˆˆ(0,1)ğœ€01\varepsilon\in(0,1)italic_Îµ âˆˆ ( 0 , 1 ), we have that NRDâ¡(RÂ¯,n)â‰¤SPRâ¡(R,n,Îµ)â‰¤Oâ¢(NRDâ¡(RÂ¯,n)â¢(râ¢logâ¡n)6/Îµ2).NRDÂ¯ğ‘…ğ‘›SPRğ‘…ğ‘›ğœ€ğ‘‚NRDÂ¯ğ‘…ğ‘›superscriptğ‘Ÿğ‘›6superscriptğœ€2\operatorname{NRD}(\overline{R},n)\leq\operatorname{SPR}(R,n,\varepsilon)\leq O% (\operatorname{NRD}(\overline{R},n)(r\log n)^{6}/\varepsilon^{2}).roman_NRD ( overÂ¯ start_ARG italic_R end_ARG , italic_n ) â‰¤ roman_SPR ( italic_R , italic_n , italic_Îµ ) â‰¤ italic_O ( roman_NRD ( overÂ¯ start_ARG italic_R end_ARG , italic_n ) ( italic_r roman_log italic_n ) start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT / italic_Îµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) . 1.3 Non-redundancy of specific relations The non-redundancy of relations is readily computed in some simple cases. For example, for the relation ORr:=Drâˆ–{0r}assignsubscriptORğ‘Ÿsuperscriptğ·ğ‘Ÿsuperscript0ğ‘Ÿ\operatorname{OR}_{r}:=D^{r}\setminus\{0^{r}\}roman_OR start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT := italic_D start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT âˆ– { 0 start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT }, we have that NRDâ¡(ORr,n)=Î˜â¢(nr)NRDsubscriptORğ‘Ÿğ‘›Î˜superscriptğ‘›ğ‘Ÿ\operatorname{NRD}(\operatorname{OR}_{r},n)=\Theta(n^{r})roman_NRD ( roman_OR start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT , italic_n ) = roman_Î˜ ( italic_n start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ). Indeed Y=(Xr)ğ‘Œbinomialğ‘‹ğ‘ŸY=\binom{X}{r}italic_Y = ( FRACOP start_ARG italic_X end_ARG start_ARG italic_r end_ARG ) is a non-redundant instance because setting all but rğ‘Ÿritalic_r variables to 1111 fails to satisfy exactly that rğ‘Ÿritalic_r-tuple (see [FK17, Car22, KPS24b]). When Rğ‘…Ritalic_R is affine, NRDâ¡(R,n)=Î˜â¢(n)NRDğ‘…ğ‘›Î˜ğ‘›\operatorname{NRD}(R,n)=\Theta(n)roman_NRD ( italic_R , italic_n ) = roman_Î˜ ( italic_n ), and when Rğ‘…Ritalic_R is defined as the zero set of a degree kğ‘˜kitalic_k polynomial, NRDâ¡(R,n)=Oâ¢(nk)NRDğ‘…ğ‘›ğ‘‚superscriptğ‘›ğ‘˜\operatorname{NRD}(R,n)=O(n^{k})roman_NRD ( italic_R , italic_n ) = italic_O ( italic_n start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ); these follow from simple rank arguments (e.g., [LW20]). Via Theorem 1.2, these special cases (plus simple gadget reductions) already capture all the previously known upper and lower bounds for CSP sparsification (see Section 1.5 for more details on the CSP sparsification literature). Furthermore, there are also some non-trivial upper bounds known on NRD in the literature, which we can now import to sparsifiability for free courtesy Theorem 1.2. For instance, the so-called Malâ€™tsev relations, which generalize affine predicates (i.e., cosets) over Abelian groups, have been shown to have ODâ¢(n)subscriptğ‘‚ğ·ğ‘›O_{D}(n)italic_O start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT ( italic_n ) non-redundancy [LW20, BCK20], and therefore by Theorem 1.2 their complements have near-linear sparsifiability. Carbonnel [Car22] showed that if Rğ‘…Ritalic_R is an arity rğ‘Ÿritalic_r relation that doesnâ€™t contain444See Theorem 5.2 for a precise definition. any copy of ORrsubscriptORğ‘Ÿ\operatorname{OR}_{r}roman_OR start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT, then NRDâ¡(R,n)â‰¤Oâ¢(nrâˆ’Î´r)NRDğ‘…ğ‘›ğ‘‚superscriptğ‘›ğ‘Ÿsubscriptğ›¿ğ‘Ÿ\operatorname{NRD}(R,n)\leq O(n^{r-\delta_{r}})roman_NRD ( italic_R , italic_n ) â‰¤ italic_O ( italic_n start_POSTSUPERSCRIPT italic_r - italic_Î´ start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ) for Î´r=21âˆ’rsubscriptğ›¿ğ‘Ÿsuperscript21ğ‘Ÿ\delta_{r}=2^{1-r}italic_Î´ start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT = 2 start_POSTSUPERSCRIPT 1 - italic_r end_POSTSUPERSCRIPT (the specific bound arises from a classic hypergraph TurÃ¡n result [Erd64]). By Theorem 1.2 this immediately implies SPRâ¡(RÂ¯,n,Îµ)â‰¤O~Îµâ¢(nrâˆ’Î´r)SPRÂ¯ğ‘…ğ‘›ğœ€subscript~ğ‘‚ğœ€superscriptğ‘›ğ‘Ÿsubscriptğ›¿ğ‘Ÿ\operatorname{SPR}(\overline{R},n,\varepsilon)\leq\widetilde{O}_{\varepsilon}(% n^{r-\delta_{r}})roman_SPR ( overÂ¯ start_ARG italic_R end_ARG , italic_n , italic_Îµ ) â‰¤ over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_Îµ end_POSTSUBSCRIPT ( italic_n start_POSTSUPERSCRIPT italic_r - italic_Î´ start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ), where O~â¢(â‹…)~ğ‘‚â‹…\widetilde{O}(\cdot)over~ start_ARG italic_O end_ARG ( â‹… ) hides polylogarithmic factors in nğ‘›nitalic_n, yielding an Î©â¢(nr)Î©superscriptğ‘›ğ‘Ÿ\Omega(n^{r})roman_Î© ( italic_n start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ) vs O~â¢(nrâˆ’Î´r)~ğ‘‚superscriptğ‘›ğ‘Ÿsubscriptğ›¿ğ‘Ÿ\widetilde{O}(n^{r-\delta_{r}})over~ start_ARG italic_O end_ARG ( italic_n start_POSTSUPERSCRIPT italic_r - italic_Î´ start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ) dichotomy for sparsification of arity rğ‘Ÿritalic_r CSPs. (This was known for the Boolean case [KPS24b]; see the related work subsection.) The non-redundancy of a relation can in general be difficult to estimate. Thus while in principle Theorem 1.2 pins down the sparisifiability of every CSP, for specific relations, it can still be non-trivial to actually determine the asymptotic behavior of its sparsifiability. Our next set of results makes progress in this direction via novel methods to bound non-redundancy. Given that the non-redundancy of linear predicates is easy to pin down, we consider a natural family of relations which are very close to being linear. Specifically, let 3â¢Lâ¢Iâ¢NG={(x,y,z)âˆ£x+y+z=0}subscript3LINğºconditional-setğ‘¥ğ‘¦ğ‘§ğ‘¥ğ‘¦ğ‘§0\operatorname{3LIN}_{G}=\{(x,y,z)\mid x+y+z=0\}start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT = { ( italic_x , italic_y , italic_z ) âˆ£ italic_x + italic_y + italic_z = 0 } over an Abelian group GğºGitalic_G, and consider 3â¢Lâ¢Iâ¢NGâˆ—=3â¢Lâ¢Iâ¢NGâˆ–{(0,0,0)}subscriptsuperscript3LINğºsubscript3LINğº000\operatorname{3LIN}^{*}_{G}=\operatorname{3LIN}_{G}\setminus\{(0,0,0)\}start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT = start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT âˆ– { ( 0 , 0 , 0 ) }. (We pick arity 3333 since the arity 2222 case is already fully resolved [FK17, BÅ½20].) Being defined by a linear equation over an Abelian group, we already know that NRDâ¡(3â¢Lâ¢Iâ¢NG,n)=Î˜Gâ¢(n)NRDsubscript3LINğºğ‘›subscriptÎ˜ğºğ‘›\operatorname{NRD}(\operatorname{3LIN}_{G},n)=\Theta_{G}(n)roman_NRD ( start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT , italic_n ) = roman_Î˜ start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_n ). However the non-redundancy of 3â¢Lâ¢Iâ¢NGâˆ—subscriptsuperscript3LINğº\operatorname{3LIN}^{*}_{G}start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT seems challenging to understand. Existing methods in the literature only yield NRDâ¡(3â¢Lâ¢Iâ¢NGâˆ—,n)âˆˆ[Î©Gâ¢(n),OGâ¢(n2)]NRDsubscriptsuperscript3LINğºğ‘›subscriptÎ©ğºğ‘›subscriptğ‘‚ğºsuperscriptğ‘›2\operatorname{NRD}(\operatorname{3LIN}^{*}_{G},n)\in[\Omega_{G}(n),O_{G}(n^{2})]roman_NRD ( start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT , italic_n ) âˆˆ [ roman_Î© start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_n ) , italic_O start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) ]. We introduce a new method for bounding the non-redundancy of predicates like 3â¢Lâ¢Iâ¢NGâˆ—subscriptsuperscript3LINğº\operatorname{3LIN}^{*}_{G}start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT by connecting them to the theory of matching vector (MV) families [Yek08, DGY11] that have been used in the construction of locally decodable codes. Exploiting this connection, we construct a non-redundant instance to establish that NRDâ¡(3â¢Lâ¢Iâ¢NG,n)â‰¥Î©â¢(n1.5)NRDsubscript3LINğºğ‘›Î©superscriptğ‘›1.5\operatorname{NRD}(\operatorname{3LIN}_{G},n)\geq\Omega(n^{1.5})roman_NRD ( start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT , italic_n ) â‰¥ roman_Î© ( italic_n start_POSTSUPERSCRIPT 1.5 end_POSTSUPERSCRIPT ) for all Abelian groups of order â‰¥3absent3\geq 3â‰¥ 3. Adapting ideas from the analysis of MV families together with some combinatorial ideas, we also prove an upper bound NRDâ¡(3â¢Lâ¢Iâ¢Nâ„¤/pâ¢â„¤,n)=O~pâ¢(n2âˆ’Îµp)NRDsubscript3LINâ„¤ğ‘â„¤ğ‘›subscript~ğ‘‚ğ‘superscriptğ‘›2subscriptğœ€ğ‘\operatorname{NRD}(\operatorname{3LIN}_{\mathbb{Z}/p\mathbb{Z}},n)=\widetilde{% O}_{p}(n^{2-\varepsilon_{p}})roman_NRD ( start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUBSCRIPT blackboard_Z / italic_p blackboard_Z end_POSTSUBSCRIPT , italic_n ) = over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_n start_POSTSUPERSCRIPT 2 - italic_Îµ start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ) for Îµp=22â¢pâˆ’1subscriptğœ€ğ‘22ğ‘1\varepsilon_{p}=\tfrac{2}{2p-1}italic_Îµ start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT = divide start_ARG 2 end_ARG start_ARG 2 italic_p - 1 end_ARG and pğ‘pitalic_p prime. Specializing for p=3ğ‘3p=3italic_p = 3, we have the following result, which also gives the first examples of relations whose non-redundancy and sparsifiability have a non-integral exponent. Theorem 1.3. We have NRDâ¡(3â¢Lâ¢Iâ¢Nâ„¤/3â¢â„¤âˆ—,n)NRDsubscriptsuperscript3LINâ„¤3â„¤ğ‘›\displaystyle\operatorname{NRD}(\operatorname{3LIN}^{*}_{\mathbb{Z}/3\mathbb{Z% }},n)roman_NRD ( start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT start_POSTSUBSCRIPT blackboard_Z / 3 blackboard_Z end_POSTSUBSCRIPT , italic_n ) âˆˆ[Î©â¢(n1.5),O~â¢(n1.6)], andabsentÎ©superscriptğ‘›1.5~ğ‘‚superscriptğ‘›1.6 and\displaystyle\in[\Omega(n^{1.5}),\widetilde{O}(n^{1.6})],\ \ \ \text{ and }âˆˆ [ roman_Î© ( italic_n start_POSTSUPERSCRIPT 1.5 end_POSTSUPERSCRIPT ) , over~ start_ARG italic_O end_ARG ( italic_n start_POSTSUPERSCRIPT 1.6 end_POSTSUPERSCRIPT ) ] , and SPRâ¡(3â¢Lâ¢Iâ¢Nâ„¤/3â¢â„¤âˆ—Â¯,n,Îµ)SPRÂ¯subscriptsuperscript3LINâ„¤3â„¤ğ‘›ğœ€\displaystyle\operatorname{SPR}(\overline{\operatorname{3LIN}^{*}_{\mathbb{Z}/% 3\mathbb{Z}}},n,\varepsilon)roman_SPR ( overÂ¯ start_ARG start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT start_POSTSUBSCRIPT blackboard_Z / 3 blackboard_Z end_POSTSUBSCRIPT end_ARG , italic_n , italic_Îµ ) âˆˆ[Î©â¢(n1.5),O~â¢(n1.6/Îµ2)].absentÎ©superscriptğ‘›1.5~ğ‘‚superscriptğ‘›1.6superscriptğœ€2\displaystyle\in[\Omega(n^{1.5}),\widetilde{O}(n^{1.6}/\varepsilon^{2})].âˆˆ [ roman_Î© ( italic_n start_POSTSUPERSCRIPT 1.5 end_POSTSUPERSCRIPT ) , over~ start_ARG italic_O end_ARG ( italic_n start_POSTSUPERSCRIPT 1.6 end_POSTSUPERSCRIPT / italic_Îµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) ] . 1.4 Weighted CSP sparsification The discussion so far has focused on unweighted CSP instances, and we now shift our focus to the weighted case, where each constraint of Yğ‘ŒYitalic_Y comes with a weight. We also get a tight characterization of weighted CSP sparsifiablity, in terms of a parameter called the chain length, which was defined by Lagerkvist and WahlstrÃ¶m [LW17, LW20] in the context of CSP kernelization and later utilized by Bessiere, Carbonnel, and Katsirelos [BCK20] in the context of learning CSPs in a certain query model (see Section 1.5 for more details on these connections). As before, the result is obtained in the setting of weighted non-linear codes, with the consequence for weighted CSPs being an easy corollary. We just state the result for codes here (see Section 8 for the full treatment of weighted CSPs). For weighted sparsification of a code CâŠ†{0,1}mğ¶superscript01ğ‘šC\subseteq\{0,1\}^{m}italic_C âŠ† { 0 , 1 } start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT, we might have an arbitrary input weighting Î¶:[m]â†’â„â‰¥0:ğœâ†’delimited-[]ğ‘šsubscriptâ„absent0\zeta:[m]\to\mathbb{R}_{\geq 0}italic_Î¶ : [ italic_m ] â†’ blackboard_R start_POSTSUBSCRIPT â‰¥ 0 end_POSTSUBSCRIPT of its coordinates, and we must find a sparsifier w~:[n]â†’â„â‰¥0:~ğ‘¤â†’delimited-[]ğ‘›subscriptâ„absent0\widetilde{w}:[n]\to\mathbb{R}_{\geq 0}over~ start_ARG italic_w end_ARG : [ italic_n ] â†’ blackboard_R start_POSTSUBSCRIPT â‰¥ 0 end_POSTSUBSCRIPT of low support that sparsifies Cğ¶Citalic_C with respect to the weighting Î¶ğœ\zetaitalic_Î¶, i.e., âŸ¨w~,câŸ©âˆˆ(1Â±Îµ)â¢âŸ¨Î¶,câŸ©~ğ‘¤ğ‘plus-or-minus1ğœ€ğœğ‘\langle\widetilde{w},c\rangle\in(1\pm\varepsilon)\langle\zeta,c\rangleâŸ¨ over~ start_ARG italic_w end_ARG , italic_c âŸ© âˆˆ ( 1 Â± italic_Îµ ) âŸ¨ italic_Î¶ , italic_c âŸ©. The minimum possible support of sparsifiers over all weightings Î¶ğœ\zetaitalic_Î¶ is called the weighted Îµğœ€\varepsilonitalic_Îµ-sparsity wSPRâ¡(C,Îµ)wSPRğ¶ğœ€\operatorname{wSPR}(C,\varepsilon)roman_wSPR ( italic_C , italic_Îµ ). Now we define chain length. If we line up the codewords of Cğ¶Citalic_C as rows of an |C|Ã—mğ¶ğ‘š|C|\times m| italic_C | Ã— italic_m matrix and allow arbitrary column permutations, the chain length of Cğ¶Citalic_C, denoted CLâ¡(C)CLğ¶\operatorname{CL}(C)roman_CL ( italic_C ), is the dimension of the largest upper triangular square submatrix with 1111â€™s on the diagonal.555In this view NRDâ¡(C)NRDğ¶\operatorname{NRD}(C)roman_NRD ( italic_C ) is the dimension of the largest identity submatrix, so clearly NRDâ¡(C)â‰¤CLâ¡(C)NRDğ¶CLğ¶\operatorname{NRD}(C)\leq\operatorname{CL}(C)roman_NRD ( italic_C ) â‰¤ roman_CL ( italic_C ). The quantity CLâ¡(C)CLğ¶\operatorname{CL}(C)roman_CL ( italic_C ) was called visible rank in [AG21] and served as a field independent lower bound on the rank of Cğ¶Citalic_C. In our main result for the weighted setting, we pin the sparsifiability of a weighted code to its chain length. Note that in the weighted case CLâ¡(C)CLğ¶\operatorname{CL}(C)roman_CL ( italic_C ) is also a lower bound. Theorem 1.4. For all CâŠ†{0,1}mğ¶superscript01ğ‘šC\subseteq\{0,1\}^{m}italic_C âŠ† { 0 , 1 } start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT and Îµâˆˆ(0,1)ğœ€01\varepsilon\in(0,1)italic_Îµ âˆˆ ( 0 , 1 ), we have CLâ¡(C)â‰¤wSPRâ¡(C,Îµ)=Oâ¢(CLâ¡(C)â¢(logâ¡m)6/Îµ2).CLğ¶wSPRğ¶ğœ€ğ‘‚CLğ¶superscriptğ‘š6superscriptğœ€2\operatorname{CL}(C)\leq\operatorname{wSPR}(C,\varepsilon)=O(\operatorname{CL}% (C)(\log m)^{6}/\varepsilon^{2}).roman_CL ( italic_C ) â‰¤ roman_wSPR ( italic_C , italic_Îµ ) = italic_O ( roman_CL ( italic_C ) ( roman_log italic_m ) start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT / italic_Îµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) . The upper bound proceeds by using Theorem 1.1 as a black-box together with a geometric weight bucketing technique from [KPS24b]. The lower bound proceeds by applying an exponential sequence of weights to the indices i1,â€¦,iCLâ¡(C)âˆˆ[m]subscriptğ‘–1â€¦subscriptğ‘–CLğ¶delimited-[]ğ‘ši_{1},\ldots,i_{\operatorname{CL}{(C)}}\in[m]italic_i start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_i start_POSTSUBSCRIPT roman_CL ( italic_C ) end_POSTSUBSCRIPT âˆˆ [ italic_m ] forming a maximal chain. Of note, if for a particular set of weights, the ratio between maximum and minimal weights is Î»â‰ªexpâ¡(CLâ¡(C)/NRDâ¡(C))much-less-thanğœ†CLğ¶NRDğ¶\lambda\ll\exp(\operatorname{CL}(C)/\operatorname{NRD}(C))italic_Î» â‰ª roman_exp ( roman_CL ( italic_C ) / roman_NRD ( italic_C ) ), we get a sharper upper bound of O~Îµâ¢(NRDâ¡(C)â¢logâ¡Î»)subscript~ğ‘‚ğœ€NRDğ¶ğœ†\widetilde{O}_{\varepsilon}(\operatorname{NRD}(C)\log\lambda)over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_Îµ end_POSTSUBSCRIPT ( roman_NRD ( italic_C ) roman_log italic_Î» ) (see Corollary 8.18). We now transition to discussing the broader context of our work in the literature. 1.5 Related Work Our results and techniques have connections to many areas including computational complexity theory, extremal combinatorics, coding theory, and learning theory. We now give a general overview of these connections. CSP Sparsification. Since we already discussed the history of CSP sparsification, we give a comprehensive list of known results about CSP sparsification (up to polylog factors). â€¢ The case of binary CSPs (r=2ğ‘Ÿ2r=2italic_r = 2) is fully classified. In particular, for every finite domain Dğ·Ditalic_D and RâŠ†D2ğ‘…superscriptğ·2R\subseteq D^{2}italic_R âŠ† italic_D start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, we either have that SPRâ¡(R,n,Îµ)=Oâ¢(n/Îµ2)SPRğ‘…ğ‘›ğœ€ğ‘‚ğ‘›superscriptğœ€2\operatorname{SPR}(R,n,\varepsilon)=O(n/\varepsilon^{2})roman_SPR ( italic_R , italic_n , italic_Îµ ) = italic_O ( italic_n / italic_Îµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) or SPRâ¡(R,n,Îµ)=Î©â¢(n2)SPRğ‘…ğ‘›ğœ€Î©superscriptğ‘›2\operatorname{SPR}(R,n,\varepsilon)=\Omega(n^{2})roman_SPR ( italic_R , italic_n , italic_Îµ ) = roman_Î© ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) [BÅ½20]. However, the sparsification routine is only efficient in the Boolean case [FK17]. Of note, SPRâ¡(R,n,Îµ)=Î©â¢(n2)SPRğ‘…ğ‘›ğœ€Î©superscriptğ‘›2\operatorname{SPR}(R,n,\varepsilon)=\Omega(n^{2})roman_SPR ( italic_R , italic_n , italic_Îµ ) = roman_Î© ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) if and only if there exist D1,D2âŠ†Dsubscriptğ·1subscriptğ·2ğ·D_{1},D_{2}\subseteq Ditalic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT âŠ† italic_D of size exactly 2222 such that |Râˆ©(D1Ã—D2)|=1ğ‘…subscriptğ·1subscriptğ·21|R\cap(D_{1}\times D_{2})|=1| italic_R âˆ© ( italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT Ã— italic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) | = 1 (informally Rğ‘…Ritalic_R has an â€œinduced copyâ€ of AND2subscriptAND2\operatorname{AND}_{2}roman_AND start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT). â€¢ For râ‰¥3ğ‘Ÿ3r\geq 3italic_r â‰¥ 3, much less is known. Kogan and Krauthgamer [KK15] contributed near-linear hypergraph cut sparsifiers (i.e., the predicate is NAEr:={0,1}râˆ–{0r,1r}assignsubscriptNAEğ‘Ÿsuperscript01ğ‘Ÿsuperscript0ğ‘Ÿsuperscript1ğ‘Ÿ\operatorname{NAE}_{r}:=\{0,1\}^{r}\setminus\{0^{r},1^{r}\}roman_NAE start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT := { 0 , 1 } start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT âˆ– { 0 start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT , 1 start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT }). Since then, there have been multiple improvements in efficiently constructing hypergraph sparsifiers/sketches (e.g., [CKN20, KKTY21, KPS24c]). â€¢ The breakthroughs of Khanna, Putterman, and Sudan [KPS24a, KPS24b] construct non-linear sparsifiers for any predicate which can defined by a system of linear (in)equations (possibly over a higher domain). For example NAEr={xâˆˆ{0,1}r:x1+â‹¯+xrâ‰¢0modr}subscriptNAEğ‘Ÿconditional-setğ‘¥superscript01ğ‘Ÿnot-equivalent-tosubscriptğ‘¥1â‹¯subscriptğ‘¥ğ‘Ÿmodulo0ğ‘Ÿ\operatorname{NAE}_{r}=\{x\in\{0,1\}^{r}:x_{1}+\cdots+x_{r}\not\equiv 0\mod r\}roman_NAE start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT = { italic_x âˆˆ { 0 , 1 } start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT : italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + â‹¯ + italic_x start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT â‰¢ 0 roman_mod italic_r }. Of note, their first paper [KPS24a] only proved the result over finite fields (and was nonalgorithmic), whereas their second paper [KPS24b] extended the result to all Abelian groups and was computationally efficient. â€¢ The framework of Khanna, Putterman, and Sudan [KPS24b] produced numerous corollaries. In particular, if a predicate can be expressed as the nonzero set of a degree kğ‘˜kitalic_k polynomial, then it has a sparsifier of size O~Îµâ¢(nk)subscript~ğ‘‚ğœ€superscriptğ‘›ğ‘˜\widetilde{O}_{\varepsilon}(n^{k})over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_Îµ end_POSTSUBSCRIPT ( italic_n start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ). Furthermore, they show if a predicate Rğ‘…Ritalic_R can express666More specifically, we say that RâŠ†{0,1}rğ‘…superscript01ğ‘ŸR\subseteq\{0,1\}^{r}italic_R âŠ† { 0 , 1 } start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT can express ANDksubscriptANDğ‘˜\operatorname{AND}_{k}roman_AND start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT if there exits a map z:[r]â†’{0,1,x1,â€¦,xk,x1Â¯,â€¦,xkÂ¯}:ğ‘§â†’delimited-[]ğ‘Ÿ01subscriptğ‘¥1â€¦subscriptğ‘¥ğ‘˜Â¯subscriptğ‘¥1â€¦Â¯subscriptğ‘¥ğ‘˜z:[r]\to\{0,1,x_{1},\ldots,x_{k},\overline{x_{1}},\ldots,\overline{x_{k}}\}italic_z : [ italic_r ] â†’ { 0 , 1 , italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , overÂ¯ start_ARG italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG , â€¦ , overÂ¯ start_ARG italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG } such that Râ¢(zâ¢(1),â€¦,zâ¢(r))=ANDkâ¡(x1,â€¦,xk)ğ‘…ğ‘§1â€¦ğ‘§ğ‘ŸsubscriptANDğ‘˜subscriptğ‘¥1â€¦subscriptğ‘¥ğ‘˜R(z(1),\ldots,z(r))=\operatorname{AND}_{k}(x_{1},\ldots,x_{k})italic_R ( italic_z ( 1 ) , â€¦ , italic_z ( italic_r ) ) = roman_AND start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ). We discuss a more general framework of gadget reductions in Section 5.4. ANDk:={1k}assignsubscriptANDğ‘˜superscript1ğ‘˜\operatorname{AND}_{k}:=\{1^{k}\}roman_AND start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT := { 1 start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT }, then SPRâ¡(R,n,Îµ)=Î©â¢(nk)SPRğ‘…ğ‘›ğœ€Î©superscriptğ‘›ğ‘˜\operatorname{SPR}(R,n,\varepsilon)=\Omega(n^{k})roman_SPR ( italic_R , italic_n , italic_Îµ ) = roman_Î© ( italic_n start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ). As a consequence, they also classify all ternary Boolean predicates (r=3ğ‘Ÿ3r=3italic_r = 3) as well as which Boolean predicates of arity rğ‘Ÿritalic_r cannot be sparsified below Î©â¢(nr)Î©superscriptğ‘›ğ‘Ÿ\Omega(n^{r})roman_Î© ( italic_n start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ) (just ANDrsubscriptANDğ‘Ÿ\operatorname{AND}_{r}roman_AND start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT and its bit flips), while also constructing a sparsifier of size O~Îµâ¢(nrâˆ’1)subscript~ğ‘‚ğœ€superscriptğ‘›ğ‘Ÿ1\widetilde{O}_{\varepsilon}(n^{r-1})over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_Îµ end_POSTSUBSCRIPT ( italic_n start_POSTSUPERSCRIPT italic_r - 1 end_POSTSUPERSCRIPT ) in the other cases. â€¢ It appears that lower bounds with a nontrivial dependence on Îµğœ€\varepsilonitalic_Îµ are only known for cut sparsifiers (and thus hypergraph cut sparsifiers via a simple gadget reduction). See [ACK+16, CKST19] as well as Section 9 for further discussion. CSP Kernelization. Another question similar in spirit to CSP sparsification is that of CSP kernelization.777More commonly, CSP kernelization is referred to as CSP sparsification (e.g., [DvM14, LW20]). However, we refer to this line of work by the former name to reduce ambiguity. This similarity in name has been noted before in the literature (e.g., [BÅ½20]), but we appear to be the first work to notice both variants of â€œCSP sparsificationâ€ can be analyzed with similar techniques. The basic question is to, given an instance Î¨Î¨\Psiroman_Î¨ of CSPâ¡(R)CSPğ‘…\operatorname{CSP}(R)roman_CSP ( italic_R ), efficiently find as small of an instance Î¨â€²superscriptÎ¨â€²\Psi^{\prime}roman_Î¨ start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT of CSPâ¡(R)CSPğ‘…\operatorname{CSP}(R)roman_CSP ( italic_R ) as possible (not necessarily a subinstance) such that Î¨Î¨\Psiroman_Î¨ and Î¨â€²superscriptÎ¨â€²\Psi^{\prime}roman_Î¨ start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT are either both satisfiable or both unsatisfiable. This particular question can be attributed to Dell and van Melkebeek [DvM14], who were particularly inspired Impagliazzo, Paturi, and Zaneâ€™s sparsification lemma [IPZ01] and Harnik and Naorâ€™s compression framework [HN10]. See the literature review in [DvM14] for further motivations. At first, the problem seems rather unrelated to CSP sparsification. For example, if CSPâ¡(R)CSPğ‘…\operatorname{CSP}(R)roman_CSP ( italic_R ) is polynomial-time tractable, then there trivially exists a kernel of size Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 ). When CSPâ¡(R)CSPğ‘…\operatorname{CSP}(R)roman_CSP ( italic_R ) is NP-hard, however, the size of the smallest possible kernelization seems to much more closely track with the non-redundancy of Rğ‘…Ritalic_R. In particular, Dell and van Melkebeek [DvM14], proved that assuming ğ–¼ğ—ˆğ–­ğ–¯âŠˆğ–­ğ–¯/ğ—‰ğ—ˆğ—…ğ—’not-subset-of-nor-equalsğ–¼ğ—ˆğ–­ğ–¯ğ–­ğ–¯ğ—‰ğ—ˆğ—…ğ—’\mathsf{coNP}\nsubseteq\mathsf{NP/poly}sansserif_coNP âŠˆ sansserif_NP / sansserif_poly, the problem kğ‘˜kitalic_k-SAT cannot be kernelized below Î©â¢(nkâˆ’Îµ)Î©superscriptğ‘›ğ‘˜ğœ€\Omega(n^{k-\varepsilon})roman_Î© ( italic_n start_POSTSUPERSCRIPT italic_k - italic_Îµ end_POSTSUPERSCRIPT ) for any constant Îµ>0ğœ€0\varepsilon>0italic_Îµ > 0, which is close to kğ‘˜kitalic_k-SATâ€™s non-redundancy of Î˜â¢(nk)Î˜superscriptğ‘›ğ‘˜\Theta(n^{k})roman_Î˜ ( italic_n start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ). Furthermore, most upper bounds on the kernelization of NP-hard predicates follow from upper bounds on non-redundancy (see [Car22]). For example the works of Chen, Jansen, and Pieterse [CJP20] as well as Lagerkvist and WahlstrÃ¶m [LW17, LW20] develop various kernelization methods that happen to just be â€œefficientâ€ non-redundancy upper bounds. For example, these works show that if the predicate Rğ‘…Ritalic_R can be expressed as the zero set of a polynomial of degree kğ‘˜kitalic_k, then there exist a kernel of size Oâ¢(nk)ğ‘‚superscriptğ‘›ğ‘˜O(n^{k})italic_O ( italic_n start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ). This kernel happens to preserve every solution to Rğ‘…Ritalic_R, so it is also a non-redundancy upper bound. Using techniques like these, they are able to prove a number of results similar to the state-of-the-art in CSP sparsification, such as a complete classification of ternary Boolean predicates and a Oâ¢(nrâˆ’1)ğ‘‚superscriptğ‘›ğ‘Ÿ1O(n^{r-1})italic_O ( italic_n start_POSTSUPERSCRIPT italic_r - 1 end_POSTSUPERSCRIPT ) vs Î©â¢(nrâˆ’Îµ)Î©superscriptğ‘›ğ‘Ÿğœ€\Omega(n^{r-\varepsilon})roman_Î© ( italic_n start_POSTSUPERSCRIPT italic_r - italic_Îµ end_POSTSUPERSCRIPT ) Boolean dichotomy [CJP20]. See [JP19, JW20, Jan20, Tak23, Beu21] and citations therein for related work. We seek to emphasize that any efficient CSP sparsification algorithm for CSPâ¡(R)CSPğ‘…\operatorname{CSP}(R)roman_CSP ( italic_R ) is by design a kernelization algorithm for CSPâ¡(RÂ¯)CSPÂ¯ğ‘…\operatorname{CSP}(\overline{R})roman_CSP ( overÂ¯ start_ARG italic_R end_ARG ) (since all codewords with weight 00 are preserved). As such, making Theorem 1.2 efficient would require explicitly proving that every CSP can be kernelized to (approximately) its non-redundancy, which is a significant open question in the CSP kernelization community (see [Car22]). See Section 1.7 and Section 9 for further discussion. The Union-closed Sets Conjecture. A family â„±â„±\mathcal{F}caligraphic_F of subsets of [n]delimited-[]ğ‘›[n][ italic_n ] is union-closed if A,Bâˆˆâ„±ğ´ğµâ„±A,B\in\mathcal{F}italic_A , italic_B âˆˆ caligraphic_F imply that AâˆªBâˆˆâ„±ğ´ğµâ„±A\cup B\in\mathcal{F}italic_A âˆª italic_B âˆˆ caligraphic_F. In 1979, Frankl [Fra95] conjectured that there always exists iâˆˆ[n]ğ‘–delimited-[]ğ‘›i\in[n]italic_i âˆˆ [ italic_n ] which appears in at least half of the sets of â„±â„±\mathcal{F}caligraphic_F. For decades, progress on the conjecture was minimal, with the best general result being that some iâˆˆ[n]ğ‘–delimited-[]ğ‘›i\in[n]italic_i âˆˆ [ italic_n ] appears in Î©â¢(1/log2â¡|â„±|)Î©1subscript2â„±\Omega(1/\log_{2}|\mathcal{F}|)roman_Î© ( 1 / roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | caligraphic_F | ) of the sets [Kni94, WÃ³j99, Gil22]. However, in 2022, Gilmer [Gil22] shocked the combinatorics community by using an entropy-based approach to prove that some iâˆˆ[n]ğ‘–delimited-[]ğ‘›i\in[n]italic_i âˆˆ [ italic_n ] appears in 1/10011001/1001 / 100 of the sets. This immediately led to a large number of follow-up works refining Gilmerâ€™s entropy method [AHS22, CL22, Peb22, Saw23, Yu23, Cam22]. In particular, we can now replace â€˜1/10011001/1001 / 100â€™ with â€˜0.382â¢â€¦0.382â€¦0.382\ldots0.382 â€¦â€™, leaving Franklâ€™s conjecture (technically) still open. For our application to CSP sparsification, the entropy method used by Gilmer (and its subsequent refinements by many other reseachers) is the key idea needed to show that non-redundancy is essentially the optimal size for a CSP sparsifier. In particular, the improvement from 1/log2â¡|â„±|1subscript2â„±1/\log_{2}|\mathcal{F}|1 / roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | caligraphic_F | to Î©â¢(1)Î©1\Omega(1)roman_Î© ( 1 ) is precisely the same â€œgainâ€ we utilize to go from a very simple O~Îµâ¢(NRDâ¡(C)â‹…log2â¡|C|)subscript~ğ‘‚ğœ€â‹…NRDğ¶subscript2ğ¶\widetilde{O}_{\varepsilon}(\operatorname{NRD}(C)\cdot\log_{2}|C|)over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_Îµ end_POSTSUBSCRIPT ( roman_NRD ( italic_C ) â‹… roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | italic_C | ) sparsifier (see Section 3) to our O~Îµâ¢(NRDâ¡(C))subscript~ğ‘‚ğœ€NRDğ¶\widetilde{O}_{\varepsilon}(\operatorname{NRD}(C))over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_Îµ end_POSTSUBSCRIPT ( roman_NRD ( italic_C ) ) sparsifier. See the technical overview (Section 1.6) for more details. To the best of our knowledge, our work is the first application of Gilmerâ€™s entropy method to sparsification.888Gilmerâ€™s breakthough is cited in the literature review of [CDL+24], but the property-testing question they study on union-closed families has no technical connection to Gilmerâ€™s entropy method. See also [Wak24] for applications of the entropy method to learning theory and statistical physics. Matching Vector Families and Locally Decodable Codes. In coding theory, locally decodable codes (LDCs) are a class of codes which allow for jthe reliable recovery of any message symbol based on a small sample of codeword symbols, even in the presence of a constant fraction of errors. A particularly interesting familiy of constructions of LDCs has arisen out of a theory of matching vector codes [Yek08] and follow-ups [Rag07, Gop09, Efr09, DGY11]. See [DGY11] for a literature survey. Simply stated, a matching vector (MV) family over a (finite) ring â„›â„›\mathcal{R}caligraphic_R is a pair of lists of vectors u1,â€¦,uk,v1,â€¦,vkâˆˆâ„›dsubscriptğ‘¢1â€¦subscriptğ‘¢ğ‘˜subscriptğ‘£1â€¦subscriptğ‘£ğ‘˜superscriptâ„›ğ‘‘u_{1},\ldots,u_{k},v_{1},\ldots,v_{k}\in\mathcal{R}^{d}italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_u start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_v start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT âˆˆ caligraphic_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT such that the inner products âŸ¨ui,vjâŸ©subscriptğ‘¢ğ‘–subscriptğ‘£ğ‘—\langle u_{i},v_{j}\rangleâŸ¨ italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT âŸ© are nonzero999Or, more generally the inner products lie in some restricted subset of â„›â„›\mathcal{R}caligraphic_R. if and only iâ‰ jğ‘–ğ‘—i\neq jitalic_i â‰  italic_j. Informally, the uisubscriptğ‘¢ğ‘–u_{i}italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTâ€™s play a role in the encoding of the iğ‘–iitalic_iâ€™th message symbol, with the matching vector visubscriptğ‘£ğ‘–v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT helping with its local decoding. Given a choice of â„›â„›\mathcal{R}caligraphic_R and dğ‘‘ditalic_d, the primary question of interest is to find the maximal possible value of kğ‘˜kitalic_k. This â€œspin offâ€ question about LDCs has become a topic of interest in its own right [DGY11, Yek12, GHSY12, BDL13]. In this work, we demonstrate a novel application of matching vector families to the study of non-redundancy and thus (by Theorem 1.2) sparsification. In particular, we construct an explicit family of predicates such that their non-redundant instances can be viewed as a generalized MV family. We then use techniques developed for MV families to given nontrivial bounds on the non-redundancy of the predicates. See Section 6 and the technical overview (Section 1.6) for more details. Extremal Combinatorics. Computing the non-redundancy of a predicate can be viewed as a problem in extremal combinatorics known as a hypergraph TurÃ¡n problem. In particular, for an instance of a CSP to be non-redundant, every instance induced by a subset of the variables must also be non-redundant. In particular, if â„±â„±\mathcal{F}caligraphic_F is a family of hypergraphs which can never appear in non-redundant instances of CSPâ¡(R)CSPğ‘…\operatorname{CSP}(R)roman_CSP ( italic_R ), then NRDâ¡(R,n)â‰¤exrâ¡(n,â„±)NRDğ‘…ğ‘›subscriptexğ‘Ÿğ‘›â„±\operatorname{NRD}(R,n)\leq\operatorname{ex}_{r}(n,\mathcal{F})roman_NRD ( italic_R , italic_n ) â‰¤ roman_ex start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ( italic_n , caligraphic_F ), where the hypergraph TurÃ¡n number exrâ¡(n,â„±)subscriptexğ‘Ÿğ‘›â„±\operatorname{ex}_{r}(n,\mathcal{F})roman_ex start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ( italic_n , caligraphic_F ) is the size of the largest rğ‘Ÿritalic_r-uniform hypergraph on nğ‘›nitalic_n vertices without any Fâˆˆâ„±ğ¹â„±F\in\mathcal{F}italic_F âˆˆ caligraphic_F as a subgraph. This observation was first made explicit by Carbonnel [Car22] although the technique was also used in earlier work [BCK20]. As far as we are aware, ours is the first work to observe that these insights can also benefit the study of CSP sparsification. The literature on hypergraph TurÃ¡n numbers is quite rich. For instance, Keevash [Kee11] surveys the vast body of work on the â€œnon-degenerateâ€ case in which exrâ¡(n,â„±)=Î©râ¢(nr)subscriptexğ‘Ÿğ‘›â„±subscriptÎ©ğ‘Ÿsuperscriptğ‘›ğ‘Ÿ\operatorname{ex}_{r}(n,\mathcal{F})=\Omega_{r}(n^{r})roman_ex start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ( italic_n , caligraphic_F ) = roman_Î© start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ( italic_n start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ). However, for our applications, we are mostly interested in the â€œdenegerateâ€ case in which exrâ¡(n,â„±)=Oâ¢(nc)subscriptexğ‘Ÿğ‘›â„±ğ‘‚superscriptğ‘›ğ‘\operatorname{ex}_{r}(n,\mathcal{F})=O(n^{c})roman_ex start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ( italic_n , caligraphic_F ) = italic_O ( italic_n start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ) for some câˆˆ[1,r)ğ‘1ğ‘Ÿc\in[1,r)italic_c âˆˆ [ 1 , italic_r ). The works [BCK20, Car22] apply some of the most well-known works in this setting [Erd64, SEB73, RS78] to get some nontrivial results such as classifying precisely which predicates Rğ‘…Ritalic_R have NRDâ¡(R,n)=Î˜â¢(nr)NRDğ‘…ğ‘›Î˜superscriptğ‘›ğ‘Ÿ\operatorname{NRD}(R,n)=\Theta(n^{r})roman_NRD ( italic_R , italic_n ) = roman_Î˜ ( italic_n start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ), extending Chen, Jansen, and Pieterseâ€™s result for the Boolean case [CJP20]. See Sections 5.2, 7.1, and 8.4.2 for more details on specific applications. Query Complexity and Learning Theory. Rather surprisingly, the definition of non-redundancy appears to have come out of the artificial intelligence community [BCK20]. In particular, a rather broad and well-studied question (e.g., [FW02, PBS08, LLMV10, BK12, BCH+13, BCK20]) is that of constraint acquisition: how can an agent learn the constraints defining an instance of a constraint satisfaction problem? A model specifically relevant to our work is the partial membership queries model studied by Bessiere, Carbonnel, and Katsirelos [BCK20]. In this model, the domain Dğ·Ditalic_D, the constraint type Rğ‘…Ritalic_R (or types), and the set of variables Xğ‘‹Xitalic_X are known but the constraints are hidden. For each query, the agent picks some subset of variables Xâ€²âŠ†Xsuperscriptğ‘‹â€²ğ‘‹X^{\prime}\subseteq Xitalic_X start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT âŠ† italic_X as well as a partial assignment Ïƒ:Xâ€²â†’D:ğœâ†’superscriptğ‘‹â€²ğ·\sigma:X^{\prime}\to Ditalic_Ïƒ : italic_X start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT â†’ italic_D. The response to the query is â€˜YESâ€™ if Ïƒğœ\sigmaitalic_Ïƒ satisfies every constraint induced by Xâ€²superscriptğ‘‹â€²X^{\prime}italic_X start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT, and â€˜NOâ€™ otherwise. The goal is to construct an instance of CSPâ¡(R)CSPğ‘…\operatorname{CSP}(R)roman_CSP ( italic_R ) with the same solution set as the hidden CSP. For every CSP predicate Rğ‘…Ritalic_R, they prove that the query complexity of an instance of CSPâ¡(R)CSPğ‘…\operatorname{CSP}(R)roman_CSP ( italic_R ) on nğ‘›nitalic_n variables is bounded between Î©â¢(NRDâ¡(R,n))Î©NRDğ‘…ğ‘›\Omega(\operatorname{NRD}(R,n))roman_Î© ( roman_NRD ( italic_R , italic_n ) ) and Oâ¢(CLâ¡(R,n)â‹…logâ¡n)ğ‘‚â‹…CLğ‘…ğ‘›ğ‘›O(\operatorname{CL}(R,n)\cdot\log n)italic_O ( roman_CL ( italic_R , italic_n ) â‹… roman_log italic_n ). Notably, the lower bound is proved by showing that the VC dimension of the query complexity problem equals NRDâ¡(R,n)NRDğ‘…ğ‘›\operatorname{NRD}(R,n)roman_NRD ( italic_R , italic_n ).101010This observation is directly used in proving our main result, see Section 4.1. 1.6 Technical Overview We next describe the primary techniques we use to prove Theorem 1.1 and Theorem 1.3. A Simple Sparsifier. To begin, we discuss a warm-up version of Theorem 1.1 which proves a weaker upper bound of SPRâ¡(C,Îµ)â‰¤O~Îµâ¢(NRDâ¡(C)â‹…logâ¡|C|)SPRğ¶ğœ€subscript~ğ‘‚ğœ€â‹…NRDğ¶ğ¶\operatorname{SPR}(C,\varepsilon)\leq\widetilde{O}_{\varepsilon}(\operatorname% {NRD}(C)\cdot\log|C|)roman_SPR ( italic_C , italic_Îµ ) â‰¤ over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_Îµ end_POSTSUBSCRIPT ( roman_NRD ( italic_C ) â‹… roman_log | italic_C | ) (see Theorem 3.1), which for CSPs corresponds to an extra factor of the number of variables nğ‘›nitalic_n. The key technical insight (Lemma 3.3) is that for all dâˆˆ[m]ğ‘‘delimited-[]ğ‘šd\in[m]italic_d âˆˆ [ italic_m ], the set of codewords of Cğ¶Citalic_C with Hamming weight at most dğ‘‘ditalic_d (denoted by Câ‰¤dsubscriptğ¶absentğ‘‘C_{\leq d}italic_C start_POSTSUBSCRIPT â‰¤ italic_d end_POSTSUBSCRIPT) has total support size at most dâ‹…NRDâ¡(C)â‹…ğ‘‘NRDğ¶d\cdot\operatorname{NRD}(C)italic_d â‹… roman_NRD ( italic_C ). This can proved inductively by noticing that dropping a suitable non-redundant set of coordinates decreases the Hamming weight of every codeword of Cğ¶Citalic_C by at least one. With this lemma, we can recursively construct a sparsifier as follows, similar to the divide-and-conquer framework in [KPS24a, KPS24b] for linear codes. Pick dâ‰ˆÎ˜~Îµâ¢(logâ¡|C|)ğ‘‘subscript~Î˜ğœ€ğ¶d\approx\widetilde{\Theta}_{\varepsilon}(\log|C|)italic_d â‰ˆ over~ start_ARG roman_Î˜ end_ARG start_POSTSUBSCRIPT italic_Îµ end_POSTSUBSCRIPT ( roman_log | italic_C | ) and let IâŠ†[m]ğ¼delimited-[]ğ‘šI\subseteq[m]italic_I âŠ† [ italic_m ] be the support of Câ‰¤dsubscriptğ¶absentğ‘‘C_{\leq d}italic_C start_POSTSUBSCRIPT â‰¤ italic_d end_POSTSUBSCRIPT. Every iâˆˆIğ‘–ğ¼i\in Iitalic_i âˆˆ italic_I is given weight 1111 in our sparsifier. For the rest of [m]delimited-[]ğ‘š[m][ italic_m ], let JâŠ†[m]âˆ–Iğ½delimited-[]ğ‘šğ¼J\subseteq[m]\setminus Iitalic_J âŠ† [ italic_m ] âˆ– italic_I be a subsample where each iâˆˆ[m]âˆ–Iğ‘–delimited-[]ğ‘šğ¼i\in[m]\setminus Iitalic_i âˆˆ [ italic_m ] âˆ– italic_I is kept independently with probability 1/3131/31 / 3. Using a standard Chernoff bound, we can show that with positive111111We only need positive probability since we are focused on existence. This can easily be amplified to 1âˆ’1/mÎ©â¢(1)11superscriptğ‘šÎ©11-1/m^{\Omega(1)}1 - 1 / italic_m start_POSTSUPERSCRIPT roman_Î© ( 1 ) end_POSTSUPERSCRIPT probability by making dğ‘‘ditalic_d a factor of logâ¡mğ‘š\log mroman_log italic_m bigger. In applications to CSPs, the main algorithmic bottleneck is (approximately) finding Iğ¼Iitalic_I, which appears to be similar in difficulty to an open problem in CSP kernelization (see Section 1.7). probability the following holds for all câˆˆCğ‘ğ¶c\in Citalic_c âˆˆ italic_C: 3â¢Hamâ¡(c|J)+Hamâ¡(c|I)âˆˆ[1âˆ’Îµ2â¢log2â¡m,1+Îµ2â¢log2â¡m]â‹…Hamâ¡(c).3Hamevaluated-atğ‘ğ½Hamevaluated-atğ‘ğ¼â‹…1ğœ€2subscript2ğ‘š1ğœ€2subscript2ğ‘šHamğ‘3\operatorname{Ham}(c|_{J})+\operatorname{Ham}(c|_{I})\in\left[1-\frac{% \varepsilon}{2\log_{2}m},1+\frac{\varepsilon}{2\log_{2}m}\right]\cdot% \operatorname{Ham}(c).3 roman_Ham ( italic_c | start_POSTSUBSCRIPT italic_J end_POSTSUBSCRIPT ) + roman_Ham ( italic_c | start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT ) âˆˆ [ 1 - divide start_ARG italic_Îµ end_ARG start_ARG 2 roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_m end_ARG , 1 + divide start_ARG italic_Îµ end_ARG start_ARG 2 roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_m end_ARG ] â‹… roman_Ham ( italic_c ) . By induction, we can find a O~Îµâ€²â¢(NRDâ¡(Câ€²)â‹…logâ¡|Câ€²|)subscript~ğ‘‚superscriptğœ€â€²â‹…NRDsuperscriptğ¶â€²superscriptğ¶â€²\widetilde{O}_{\varepsilon^{\prime}}(\operatorname{NRD}(C^{\prime})\cdot\log|C% ^{\prime}|)over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_Îµ start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( roman_NRD ( italic_C start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) â‹… roman_log | italic_C start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT | ) Îµâ€²superscriptğœ€â€²\varepsilon^{\prime}italic_Îµ start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT-sparsifier for Câ€²:=C|Jassignsuperscriptğ¶â€²evaluated-atğ¶ğ½C^{\prime}:=C|_{J}italic_C start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT := italic_C | start_POSTSUBSCRIPT italic_J end_POSTSUBSCRIPT with Îµâ€²:=(1âˆ’1/log2â¡m)â¢Îµassignsuperscriptğœ€â€²11subscript2ğ‘šğœ€\varepsilon^{\prime}:=(1-1/\log_{2}m)\varepsilonitalic_Îµ start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT := ( 1 - 1 / roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_m ) italic_Îµ. Scaling this sparsifier by 3333 and adding weights for Iğ¼Iitalic_I gives us an Îµğœ€\varepsilonitalic_Îµ-sparsifier of Cğ¶Citalic_C. Entropy-based Sparsification. The key inefficiency of the O~Îµâ¢(NRDâ¡(C)â‹…logâ¡|C|)subscript~ğ‘‚ğœ€â‹…NRDğ¶ğ¶\widetilde{O}_{\varepsilon}(\operatorname{NRD}(C)\cdot\log|C|)over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_Îµ end_POSTSUBSCRIPT ( roman_NRD ( italic_C ) â‹… roman_log | italic_C | ) bound is that the use of Lemma 3.3 is too conservative. For the purposes of this overview, assume that all codewords of Cğ¶Citalic_C have the same Hamming weight dâ‰ˆNRDâ¡(C)ğ‘‘NRDğ¶d\approx\operatorname{NRD}(C)italic_d â‰ˆ roman_NRD ( italic_C ) as that is is the most representative case. Naively, Lemma 3.3 says we should set aside d2superscriptğ‘‘2d^{2}italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT coordinates of [m]delimited-[]ğ‘š[m][ italic_m ] to â€œsparsifyâ€ all codewords of weight dğ‘‘ditalic_d. However, we can give a heuristic argument that far fewer than d2superscriptğ‘‘2d^{2}italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT of these potential coordinates contain useful information for our sparsifier. Assume without loss of generality that the support of Cğ¶Citalic_C lies in [d2]delimited-[]superscriptğ‘‘2[d^{2}][ italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ]. For each iâˆˆ[d2]ğ‘–delimited-[]superscriptğ‘‘2i\in[d^{2}]italic_i âˆˆ [ italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ], let pisubscriptğ‘ğ‘–p_{i}italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT be the probability that a codeword câˆˆCğ‘ğ¶c\in Citalic_c âˆˆ italic_C selected uniformly at random has ci=1subscriptğ‘ğ‘–1c_{i}=1italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1. Since each codeword of Cğ¶Citalic_C has Hamming weight dğ‘‘ditalic_d, we have that p1+â‹¯+pd2=dsubscriptğ‘1â‹¯subscriptğ‘superscriptğ‘‘2ğ‘‘p_{1}+\cdots+p_{d^{2}}=ditalic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + â‹¯ + italic_p start_POSTSUBSCRIPT italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT = italic_d. Thus, the average value of pisubscriptğ‘ğ‘–p_{i}italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is 1/d1ğ‘‘1/d1 / italic_d. Consider the case in which each pi=Oâ¢(1/d)subscriptğ‘ğ‘–ğ‘‚1ğ‘‘p_{i}=O(1/d)italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_O ( 1 / italic_d ). In particular, no coordinate is distinguishing itself as a â€œmustâ€ to add to the sparsifier. A priori, the size of Cğ¶Citalic_C may be expâ¡(Î©~â¢(d))~Î©ğ‘‘\exp(\widetilde{\Omega}(d))roman_exp ( over~ start_ARG roman_Î© end_ARG ( italic_d ) ), so we cannot immediately use Chernoff bounds to analyze a random subsampling of the coordinates. To get around this issue, we need to prove a much stronger upper bound on the size of Cğ¶Citalic_C, similar to BenczÃºr and Kargerâ€™s cut-counting bound [BK96] and its adaptation to linear codes [KPS24a, KPS24b]. However, we use an entirely new method for proving such bounds based on the entropy method Gilmer [Gil22] developed to prove the union-closed sets conjecture up to a constant factor. In our context, pick t=Î˜~â¢(d)ğ‘¡~Î˜ğ‘‘t=\widetilde{\Theta}(d)italic_t = over~ start_ARG roman_Î˜ end_ARG ( italic_d ) and sample uniformly and independently tğ‘¡titalic_t codewords c1,â€¦,ctâˆˆCsubscriptğ‘1â€¦subscriptğ‘ğ‘¡ğ¶c_{1},\ldots,c_{t}\in Citalic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT âˆˆ italic_C. Let cğ‘citalic_c be the bitwise OR of these tğ‘¡titalic_t codewords, and let ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D be the distribution of cğ‘citalic_c over {0,1}d2superscript01superscriptğ‘‘2\{0,1\}^{d^{2}}{ 0 , 1 } start_POSTSUPERSCRIPT italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT (recall that the weight dğ‘‘ditalic_d codewords are supported on d2superscriptğ‘‘2d^{2}italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT coordinates). Since each pi=Oâ¢(1/d)subscriptğ‘ğ‘–ğ‘‚1ğ‘‘p_{i}=O(1/d)italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_O ( 1 / italic_d ), by adapting Gilmerâ€™s method (or more precisely, a refinement due to Sawin [Saw23]), we can show the entropy of ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D is at least Î˜~â¢(t)=Î˜~â¢(d)~Î˜ğ‘¡~Î˜ğ‘‘\widetilde{\Theta}(t)=\widetilde{\Theta}(d)over~ start_ARG roman_Î˜ end_ARG ( italic_t ) = over~ start_ARG roman_Î˜ end_ARG ( italic_d ) times the entropy of the uniform distribution over Cğ¶Citalic_C (i.e., log2â¡|C|subscript2ğ¶\log_{2}|C|roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | italic_C |)â€“a similar inequality appears in [Wak24]. To apply this fact, observe that each sample of ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D lies in the â€œOROR\operatorname{OR}roman_OR-closureâ€ of Cğ¶Citalic_C (denoted by spanORâ¡(C)subscriptspanORğ¶\operatorname{span}_{\operatorname{OR}}(C)roman_span start_POSTSUBSCRIPT roman_OR end_POSTSUBSCRIPT ( italic_C )). As such, the entropy of ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D is at most logâ¡|spanORâ¡(C)|subscriptspanORğ¶\log\lvert\operatorname{span}_{\operatorname{OR}}(C)\rvertroman_log | roman_span start_POSTSUBSCRIPT roman_OR end_POSTSUBSCRIPT ( italic_C ) |, which by the Sauer-Shelah-Peres lemma is at most (up to log factors) the VC dimension of spanORâ¡(C)subscriptspanORğ¶\operatorname{span}_{\operatorname{OR}}(C)roman_span start_POSTSUBSCRIPT roman_OR end_POSTSUBSCRIPT ( italic_C ). It is easily seen that the VC dimension of spanORâ¡(C)subscriptspanORğ¶\operatorname{span}_{\operatorname{OR}}(C)roman_span start_POSTSUBSCRIPT roman_OR end_POSTSUBSCRIPT ( italic_C ) equals the non-redundancy of Cğ¶Citalic_C [BCK20]. Therefore, we have proved that Î˜~â¢(t)â‹…log2â¡(C)â‰¤O~â¢(NRDâ¡(C))â‹…~Î˜ğ‘¡subscript2ğ¶~ğ‘‚NRDğ¶\widetilde{\Theta}(t)\cdot\log_{2}(C)\leq\widetilde{O}(\operatorname{NRD}(C))over~ start_ARG roman_Î˜ end_ARG ( italic_t ) â‹… roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_C ) â‰¤ over~ start_ARG italic_O end_ARG ( roman_NRD ( italic_C ) ). Since tâ‰ˆdâ‰ˆNRDâ¡(C)ğ‘¡ğ‘‘NRDğ¶t\approx d\approx\operatorname{NRD}(C)italic_t â‰ˆ italic_d â‰ˆ roman_NRD ( italic_C ), Cğ¶Citalic_C is actually at most quasipolynomial in size! Thus we can now use a Chernoff bound to prove that Cğ¶Citalic_C can be subsampled to O~Îµâ¢(d)subscript~ğ‘‚ğœ€ğ‘‘\widetilde{O}_{\varepsilon}(d)over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_Îµ end_POSTSUBSCRIPT ( italic_d ) coordinates while approximately preserving all Hamming weights. Recall this discussion was purely about the â€œuniformâ€ case pi=Oâ¢(1/d)subscriptğ‘ğ‘–ğ‘‚1ğ‘‘p_{i}=O(1/d)italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_O ( 1 / italic_d ). In general, we apply minimax theorem to prove the following â€œskewedâ€ versus â€œsparseâ€ dichotomy (see Proposition 4.14): for every code Cğ¶Citalic_C and parameter choice Î¸â‰¥1ğœƒ1\theta\geq 1italic_Î¸ â‰¥ 1 there is either a probability distribution ğ’«ğ’«\mathcal{P}caligraphic_P over Cğ¶Citalic_C for which each coordinate equals 1111 with probability at most 1/Î¸1ğœƒ1/\theta1 / italic_Î¸ (i.e., ğ’«ğ’«\mathcal{P}caligraphic_P is â€œÎ¸ğœƒ\thetaitalic_Î¸-sparseâ€); or, there is a probability distribution ğ’¬ğ’¬\mathcal{Q}caligraphic_Q over the coordinates of Cğ¶Citalic_C such that for every (nonzero) câˆˆCğ‘ğ¶c\in Citalic_c âˆˆ italic_C, we have that ğ’¬ğ’¬\mathcal{Q}caligraphic_Qâ€™s measure of suppâ¡(c)suppğ‘\operatorname{supp}(c)roman_supp ( italic_c ) is at least 1/Î¸1ğœƒ1/\theta1 / italic_Î¸ (i.e., ğ’¬ğ’¬\mathcal{Q}caligraphic_Q is a â€œÎ¸ğœƒ\thetaitalic_Î¸-cover.â€) For a suitable choice of Î¸ğœƒ\thetaitalic_Î¸, we repeatedly apply Proposition 4.14 to recursively build the sparsifier: in the Î¸ğœƒ\thetaitalic_Î¸-sparse case, we use the entropy method to prove that a â€œsmallâ€ number of codewords of Cğ¶Citalic_C can be removed to put us in the Î¸ğœƒ\thetaitalic_Î¸-cover case (see Lemma 4.15); and in the Î¸ğœƒ\thetaitalic_Î¸-cover case, we sample from the Î¸ğœƒ\thetaitalic_Î¸-cover to get a coordinate to add to our sparsifier. This procedure culminates in showing that we can set aside O~Îµâ¢(NRDâ¡(C))subscript~ğ‘‚ğœ€NRDğ¶\widetilde{O}_{\varepsilon}(\operatorname{NRD}(C))over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_Îµ end_POSTSUBSCRIPT ( roman_NRD ( italic_C ) ) coordinates to have weight 1111 in our sparsifier with the remainder of the code being sufficiently sparse that subsampling can be used (Theorem 4.16). Note that the statement of Theorem 4.16 resembles the analogous decompositions for linear codes [KPS24a, KPS24b]. However, their method found all the coordinates to set aside in â€œone pass,â€ whereas we iteratively understand the dense and sparse structure of our non-linear code. With Theorem 4.16 in hand, we construct the sparsifier with a recursive argument similar to that of Theorem 3.1. As mentioned earlier, extended these ideas to weighted sparsification (Theorem 1.4) is relatively straightforward. We adapt a weight-binning argument of [KPS24b] by essentially computing an (unweighted) sparsifier for each group of coordinates that is similar in weight (within polyâ¡(m)polyğ‘š\operatorname{poly}(m)roman_poly ( italic_m )). We then analyze the aggregated size of these sparsifiers by comparing the sum of the non-redundancies of the groups of coordinates to the chain length of the code. Connections to Matching Vector Families. We now switch gears to briefly discussing the key ideas behind Theorem 1.3. Let G:=â„¤/3â¢â„¤assignğºâ„¤3â„¤G:=\mathbb{Z}/3\mathbb{Z}italic_G := blackboard_Z / 3 blackboard_Z and recall that 3â¢Lâ¢Iâ¢NG={(x,y,z)âˆ£x+y+z=0}subscript3LINğºconditional-setğ‘¥ğ‘¦ğ‘§ğ‘¥ğ‘¦ğ‘§0\operatorname{3LIN}_{G}=\{(x,y,z)\mid x+y+z=0\}start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT = { ( italic_x , italic_y , italic_z ) âˆ£ italic_x + italic_y + italic_z = 0 } and 3â¢Lâ¢Iâ¢NGâˆ—=3â¢Lâ¢Iâ¢NGâˆ–{(0,0,0)}subscriptsuperscript3LINğºsubscript3LINğº000\operatorname{3LIN}^{*}_{G}=\operatorname{3LIN}_{G}\setminus\{(0,0,0)\}start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT = start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT âˆ– { ( 0 , 0 , 0 ) }. It is well-known that since 3â¢Lâ¢Iâ¢NGsubscript3LINğº\operatorname{3LIN}_{G}start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT is an affine predicate, we have that NRDâ¡(3â¢Lâ¢Iâ¢NG,n)=Î˜â¢(n)NRDsubscript3LINğºğ‘›Î˜ğ‘›\operatorname{NRD}(\operatorname{3LIN}_{G},n)=\Theta(n)roman_NRD ( start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT , italic_n ) = roman_Î˜ ( italic_n ), which is much smaller than our bound on NRDâ¡(3â¢Lâ¢Iâ¢NGâˆ—,n)NRDsubscriptsuperscript3LINğºğ‘›\operatorname{NRD}(\operatorname{3LIN}^{*}_{G},n)roman_NRD ( start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT , italic_n ). As such, we prove that to understand the asymptotics of NRDâ¡(3â¢Lâ¢Iâ¢NGâˆ—,n)NRDsubscriptsuperscript3LINğºğ‘›\operatorname{NRD}(\operatorname{3LIN}^{*}_{G},n)roman_NRD ( start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT , italic_n ) it suffices to look at specially-structured non-redundant instances. Recall that an instance Î¨:=(X,Y)assignÎ¨ğ‘‹ğ‘Œ\Psi:=(X,Y)roman_Î¨ := ( italic_X , italic_Y ) of CSPâ¡(3â¢Lâ¢Iâ¢NGâˆ—)CSPsubscriptsuperscript3LINğº\operatorname{CSP}(\operatorname{3LIN}^{*}_{G})roman_CSP ( start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ) is non redundant if for every clause yâˆˆYğ‘¦ğ‘Œy\in Yitalic_y âˆˆ italic_Y there is an assignment Ïƒysubscriptğœğ‘¦\sigma_{y}italic_Ïƒ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT which satisfies every clause of Î¨Î¨\Psiroman_Î¨ except yğ‘¦yitalic_y. We show that with at most an additive Î˜â¢(n)Î˜ğ‘›\Theta(n)roman_Î˜ ( italic_n ) change in size, we can assume that Ïƒysubscriptğœğ‘¦\sigma_{y}italic_Ïƒ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT maps yğ‘¦yitalic_y to (0,0,0)000(0,0,0)( 0 , 0 , 0 ). In other words, each Ïƒysubscriptğœğ‘¦\sigma_{y}italic_Ïƒ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT is a satisfying assignment to Î¨Î¨\Psiroman_Î¨ when viewed as an instance of CSPâ¡(3â¢Lâ¢Iâ¢NG)CSPsubscript3LINğº\operatorname{CSP}(\operatorname{3LIN}_{G})roman_CSP ( start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ) (see Proposition 6.3). This idea of â€œconditionalâ€ non-redundancy abstracts and generalizes an approach from [BCK20]. Since the set of solutions to an instance of CSPâ¡(3â¢Lâ¢Iâ¢NG)CSPsubscript3LINğº\operatorname{CSP}(\operatorname{3LIN}_{G})roman_CSP ( start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ) form a vector space (of some dimension, say dğ‘‘ditalic_d) over ğ”½3subscriptğ”½3\mathbb{F}_{3}blackboard_F start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT, we can think of each variable xâˆˆXğ‘¥ğ‘‹x\in Xitalic_x âˆˆ italic_X of Î¨Î¨\Psiroman_Î¨ as a vector vxâˆˆğ”½3dsubscriptğ‘£ğ‘¥superscriptsubscriptğ”½3ğ‘‘v_{x}\in\mathbb{F}_{3}^{d}italic_v start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT âˆˆ blackboard_F start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT and the assignments as linear maps on the vectors. Because we are studying satisfying assignment to CSPâ¡(3â¢Lâ¢Iâ¢NG)CSPsubscript3LINğº\operatorname{CSP}(\operatorname{3LIN}_{G})roman_CSP ( start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ), these vectors are highly structured: for each y:=(x1,x2,x3)âˆˆYassignğ‘¦subscriptğ‘¥1subscriptğ‘¥2subscriptğ‘¥3ğ‘Œy:=(x_{1},x_{2},x_{3})\in Yitalic_y := ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ) âˆˆ italic_Y, we have that vx1+vx2+vx3=0subscriptğ‘£subscriptğ‘¥1subscriptğ‘£subscriptğ‘¥2subscriptğ‘£subscriptğ‘¥30v_{x_{1}}+v_{x_{2}}+v_{x_{3}}=0italic_v start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT + italic_v start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT + italic_v start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = 0. Further, Ïƒysubscriptğœğ‘¦\sigma_{y}italic_Ïƒ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT can be viewed as a linear map taking each of vx1,vx2,vx3subscriptğ‘£subscriptğ‘¥1subscriptğ‘£subscriptğ‘¥2subscriptğ‘£subscriptğ‘¥3v_{x_{1}},v_{x_{2}},v_{x_{3}}italic_v start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT end_POSTSUBSCRIPT to 00, while mapping at least one vector in every other triple in Yğ‘ŒYitalic_Y to a nonzero value. We call this family of vectors together with these assignments a GğºGitalic_G-ensemble (Definition 6.5), and note that it bears a strong resemblance to matching vector families. In particular, we adapt techniques used by Dvir, Gopalan, and Yekhanin [DGY11] for constraining the size of matching vector families to give nontrivial upper and lower bounds on the size of GğºGitalic_G-ensembles. For the lower bound (Theorem 6.8), we directly construct a non-redundant instance with Î©â¢(n1.5)Î©superscriptğ‘›1.5\Omega(n^{1.5})roman_Î© ( italic_n start_POSTSUPERSCRIPT 1.5 end_POSTSUPERSCRIPT ) clauses. The proof is self-contained and elementary. The upper bound (Theorem 6.13) is slightly more technical. We break the proof into cases based on whether the embedding dimension dğ‘‘ditalic_d of the vectors is small (d=O~â¢(n0.4)ğ‘‘~ğ‘‚superscriptğ‘›0.4d=\widetilde{O}(n^{0.4})italic_d = over~ start_ARG italic_O end_ARG ( italic_n start_POSTSUPERSCRIPT 0.4 end_POSTSUPERSCRIPT )) or large (d=Î©~â¢(n0.4)ğ‘‘~Î©superscriptğ‘›0.4d=\widetilde{\Omega}(n^{0.4})italic_d = over~ start_ARG roman_Î© end_ARG ( italic_n start_POSTSUPERSCRIPT 0.4 end_POSTSUPERSCRIPT )). For small dğ‘‘ditalic_d, we adapt the polynomial method used in [DGY11] to prove there can be at most Oâ¢(d4)=O~â¢(n1.6)ğ‘‚superscriptğ‘‘4~ğ‘‚superscriptğ‘›1.6O(d^{4})=\widetilde{O}(n^{1.6})italic_O ( italic_d start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT ) = over~ start_ARG italic_O end_ARG ( italic_n start_POSTSUPERSCRIPT 1.6 end_POSTSUPERSCRIPT ) non-redundant clauses. On the other hand, when dğ‘‘ditalic_d is large, we ignore the assignments Ïƒysubscriptğœğ‘¦\sigma_{y}italic_Ïƒ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT and use a careful induction (Lemma 6.11) to show that the geometry of the vectors imply that some xâˆˆXğ‘¥ğ‘‹x\in Xitalic_x âˆˆ italic_X is a member of at most O~â¢(n/d)=O~â¢(n0.6)~ğ‘‚ğ‘›ğ‘‘~ğ‘‚superscriptğ‘›0.6\widetilde{O}(n/d)=\widetilde{O}(n^{0.6})over~ start_ARG italic_O end_ARG ( italic_n / italic_d ) = over~ start_ARG italic_O end_ARG ( italic_n start_POSTSUPERSCRIPT 0.6 end_POSTSUPERSCRIPT ) clauses, thereby leading to a bound of at most O~â¢(n1.6)~ğ‘‚superscriptğ‘›1.6\widetilde{O}(n^{1.6})over~ start_ARG italic_O end_ARG ( italic_n start_POSTSUPERSCRIPT 1.6 end_POSTSUPERSCRIPT ) clauses total. Closing the gap between Î©â¢(n1.5)Î©superscriptğ‘›1.5\Omega(n^{1.5})roman_Î© ( italic_n start_POSTSUPERSCRIPT 1.5 end_POSTSUPERSCRIPT ) and O~â¢(n1.6)~ğ‘‚superscriptğ‘›1.6\widetilde{O}(n^{1.6})over~ start_ARG italic_O end_ARG ( italic_n start_POSTSUPERSCRIPT 1.6 end_POSTSUPERSCRIPT ) for NRDâ¡(3â¢Lâ¢Iâ¢Nâ„¤/3â¢â„¤âˆ—,n)NRDsuperscriptsubscript3LINâ„¤3â„¤ğ‘›\operatorname{NRD}(\operatorname{3LIN}_{\mathbb{Z}/3\mathbb{Z}}^{*},n)roman_NRD ( start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUBSCRIPT blackboard_Z / 3 blackboard_Z end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT , italic_n ) is a tantalizing open question. 1.7 Open Questions We conclude the introduction with a few directions of further study. See Section 7 and Section 9 for a more thorough discussion of directions for future exploration. â€¢ Making Theorem 1.2 efficient. Note that the underlying construction for Theorem 1.1, if made algorithmic, runs in polynomial time with respect to the size of the code, yielding an expâ¡(Oâ¢(n))ğ‘‚ğ‘›\exp(O(n))roman_exp ( italic_O ( italic_n ) )-time algorithm121212This is already nontrivial, as a naive guess-and-check algorithm would require expâ¡(O~â¢(NRDâ¡(RÂ¯,n)))~ğ‘‚NRDÂ¯ğ‘…ğ‘›\exp(\widetilde{O}(\operatorname{NRD}(\overline{R},n)))roman_exp ( over~ start_ARG italic_O end_ARG ( roman_NRD ( overÂ¯ start_ARG italic_R end_ARG , italic_n ) ) ) time. for Theorem 1.2. The primary barrier in constructing our sparsifier in polyâ¡(n)polyğ‘›\operatorname{poly}(n)roman_poly ( italic_n ) time is the fact that an efficient sparsifier is also a kernelization algorithm, but kernelizing every CSP instance to its non-redundancy is a significant open question in the kernelization community [Car22]. â€¢ Computing NRDâ¡(R,n)NRDğ‘…ğ‘›\operatorname{NRD}(R,n)roman_NRD ( italic_R , italic_n ). For a general predicate RâŠ†Drğ‘…superscriptğ·ğ‘ŸR\subseteq D^{r}italic_R âŠ† italic_D start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT, there is no simple (even conjectured) expression for NRDâ¡(R,n)NRDğ‘…ğ‘›\operatorname{NRD}(R,n)roman_NRD ( italic_R , italic_n ). In fact, even determining when NRDâ¡(R,n)=Î˜â¢(n)NRDğ‘…ğ‘›Î˜ğ‘›\operatorname{NRD}(R,n)=\Theta(n)roman_NRD ( italic_R , italic_n ) = roman_Î˜ ( italic_n ) is an open question (e.g., [BCK20, Car22]). In Section 7, we explore a number of predicates from the various parts of the literature whose status is unresolved, including a predicate we categorize as the â€œsimplest unresolved predicate.â€ â€¢ Non-redundancy versus Chain Length. Recall we show that unweighted sparsification is closely tied to non-redundancy while weighted sparsification is closely tied to chain length. For non-linear codes, NRDNRD\operatorname{NRD}roman_NRD and CLCL\operatorname{CL}roman_CL can be very different (e.g., Example 8.8), but the relationship for CSPs is unknown [BCK20, Car22]. In particular, it seems quite possible that there exists a CSP predicate Rğ‘…Ritalic_R for which wSPRâ¡(R,n,Îµ)/SPRâ¡(R,n,Îµ)=nÎ©â¢(1).wSPRğ‘…ğ‘›ğœ€SPRğ‘…ğ‘›ğœ€superscriptğ‘›Î©1\operatorname{wSPR}(R,n,\varepsilon)/\operatorname{SPR}(R,n,\varepsilon)=n^{% \Omega(1)}.roman_wSPR ( italic_R , italic_n , italic_Îµ ) / roman_SPR ( italic_R , italic_n , italic_Îµ ) = italic_n start_POSTSUPERSCRIPT roman_Î© ( 1 ) end_POSTSUPERSCRIPT . â€¢ Average-case behavior. From Theorem 1.2, we know that every instance CSPâ¡(R)CSPğ‘…\operatorname{CSP}(R)roman_CSP ( italic_R ) has a sparsifier of size approximately its own non-redundancy, even if that value is much smaller than NRDâ¡(RÂ¯,n)NRDÂ¯ğ‘…ğ‘›\operatorname{NRD}(\overline{R},n)roman_NRD ( overÂ¯ start_ARG italic_R end_ARG , italic_n ). As such, it may be possible that â€˜averageâ€™ instances of CSPâ¡(R)CSPğ‘…\operatorname{CSP}(R)roman_CSP ( italic_R ) admit sparsifiers much smaller than the worst case. 1.8 Organization In Section 2, we prove some basic facts about non-redundancy, sparsification and their relationship. In Section 3, we give a straightforward proof that SPRâ¡(C,Îµ)=O~Îµâ¢(NRDâ¡(C)â¢log2â¡|C|)SPRğ¶ğœ€subscript~ğ‘‚ğœ€NRDğ¶subscript2ğ¶\operatorname{SPR}(C,\varepsilon)=\widetilde{O}_{\varepsilon}(\operatorname{% NRD}(C)\log_{2}|C|)roman_SPR ( italic_C , italic_Îµ ) = over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_Îµ end_POSTSUBSCRIPT ( roman_NRD ( italic_C ) roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | italic_C | ). In Section 4, we prove Theorem 1.2 by connecting CSP sparsification to non-redundancy via Gilmerâ€™s entropy method. In Section 5, we discuss the immediate applications of Theorem 1.2 based on what is known about non-redundancy in the literature. In Section 6, we bound the non-redundancy of a family of predicates via methods related to matching vector families. In Section 7, we give examples of CSP predicates in the literature whose non-redundancy is unresolved. In Section 8, we extend Theorem 1.2 to weighted instances. In Section 9, we wrap up with other directions of exploration. 1.9 Acknowledgments We thank Libor Barto, Dmitry Zhuk, Madhu Sudan, and Aaron Putterman for valuable conversations. This research was supported in part by a Simons Investigator award and NSF grant CCF-2211972."
https://arxiv.org/html/2411.04057v1,A unified approach to quantum de Finetti theorems and SoS rounding via geometric quantization,"The sum-of-squares hierarchy of semidefinite programs has become a common tool for algorithm design in theoretical computer science, including problems in quantum information. In this work we study a connection between a Hermitian version of the SoS hierarchy, related to the quantum de Finetti theorem, and geometric quantization of compact KÃ¤hler manifolds (such as complex projective space â„‚â¢Pdâ„‚superscriptğ‘ƒğ‘‘\mathbb{C}P^{d}blackboard_C italic_P start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT, the set of all pure states in a (d+1)ğ‘‘1(d+1)( italic_d + 1 )-dimensional Hilbert space). We show that previously known HSoS rounding algorithms can be recast as quantizing an objective function to obtain a finite-dimensional matrix, finding its top eigenvector, and then (possibly nonconstructively) rounding it by using a version of the Husimi quasiprobability distribution. Dually, we recover most known quantum de Finetti theorems by doing the same steps in the reverse order: a quantum state is first approximated by its Husimi distribution, and then quantized to obtain a separable state approximating the original one. In cases when there is a transitive group action on the manifold we give some new proofs of existing de Finetti theorems, as well as some applications including a new version of Rennerâ€™s exponential de Finetti theorem proven using the Borelâ€“Weilâ€“Bott theorem, and hardness of approximation results and optimal degree-2 integrality gaps for the basic SDP relaxation of Quantum Max-dğ‘‘ditalic_d-Cut (for arbitrary dğ‘‘ditalic_d). We also describe how versions of these results can be proven when there is no transitive group action. In these cases we can deduce some error bounds for the HSoS hierarchy on complex projective varieties which are smooth.","Quantum mechanics and quantum information are often thought of as being similar to classical probability theory, but the analogy is of course not exact and must break down at some point. The point where the analogy fails is often illustrated through the use of quasiprobability distributions, which associate a classical â€œdistributionâ€ (over phase space) to a quantum state in a way which preserves as many properties as possible. In the semiclassical limit â„â†’0â†’Planck-constant-over-2-pi0\hbar\to 0roman_â„ â†’ 0, the mapping must preserve essentially all properties of the state. A popular choice is the Wigner quasiprobability distribution, which preserves many properties of the original quantum state, but may take on negative values and is thus not a classical probability distribution. Elsewhere in computer science, negative probabilities have arisen in optimization. One technique for approximating solutions to non-convex problems involves relaxing a problem of the form maxxâˆˆDâ¡pâ¢(x)subscriptğ‘¥ğ·ğ‘ğ‘¥\max_{x\in D}p(x)roman_max start_POSTSUBSCRIPT italic_x âˆˆ italic_D end_POSTSUBSCRIPT italic_p ( italic_x ) for some domain Dğ·Ditalic_D and objective function pğ‘pitalic_p into a convex problem of the form maxÎ¼âˆˆğ’«â¢(D)â¡EÎ¼â¢[pâ¢(x)]subscriptğœ‡ğ’«ğ·subscriptğ¸ğœ‡delimited-[]ğ‘ğ‘¥\max_{\mu\in\mathcal{P}(D)}E_{\mu}[p(x)]roman_max start_POSTSUBSCRIPT italic_Î¼ âˆˆ caligraphic_P ( italic_D ) end_POSTSUBSCRIPT italic_E start_POSTSUBSCRIPT italic_Î¼ end_POSTSUBSCRIPT [ italic_p ( italic_x ) ], where ğ’«â¢(D)ğ’«ğ·\mathcal{P}(D)caligraphic_P ( italic_D ) is set of probability distributions over Dğ·Ditalic_D. Since this new convex problem often has dimension which is exponentially or infinitely large, the strategy of sum-of-squares (SoS) optimization is to further enlarge the set of distributions to degree-kğ‘˜kitalic_k pseudo-distributions, which have an efficient finite-dimensional representation. Such pseudo-distributions also have an interpretation in terms of negative probabilities: a degree-kğ‘˜kitalic_k pseudo-distribution can be defined as a density function P:Dâ†’â„:ğ‘ƒâ†’ğ·â„P:D\to\mathbb{R}italic_P : italic_D â†’ blackboard_R (relative to some base measure Î¼ğœ‡\muitalic_Î¼) such that âˆ«Dqâ¢(x)â¢Pâ¢(x)â¢ğ‘‘Î¼â¢(x)â‰¥0subscriptğ·ğ‘ğ‘¥ğ‘ƒğ‘¥differential-dğœ‡ğ‘¥0\int_{D}q(x)P(x)\,d\mu(x)\geq 0âˆ« start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT italic_q ( italic_x ) italic_P ( italic_x ) italic_d italic_Î¼ ( italic_x ) â‰¥ 0 for all degree-kğ‘˜kitalic_k polynomials qğ‘qitalic_q which are sums-of-squares. As the degree kâ†’âˆâ†’ğ‘˜k\to\inftyitalic_k â†’ âˆ, the value of the relaxed problem approaches that of the original one. It is natural to ask whether there is a connection between these two notions of negative probability, where where the inverse 1/â„1Planck-constant-over-2-pi1/\hbar1 / roman_â„ of the semiclassical parameter is analogous to the SoS degree kğ‘˜kitalic_k. In this work, we propose an affirmative answer to this question. 1.1. Approaches to nonlinear classical optimization For solving purely a classical problem which is nonlinear, such as optimization of a nonlinear polynomial function over a compact semialgebraic set, a common approach involves first constructing a finite-dimensional linear approximation to the problem, solving it, and then using the solution to approximate the original nonlinear problem. Indeed, the sum-of-squares optimization hierarchy does exactly this, where the finite-dimensional approximation is a semidefinite program with a linear objective function. Another common approach is to construct a finite set of reasonably separated points on which the objective function can be evaluated, and then approximating the optimum on the entire space by the optimum on the finite set. In either case, the approximation to the nonlinear problem must be finite in some way â€“ being finite-dimensional in the case of linearization, or having finite cardinality in the case of brute-forcing over a finite subset. Motivated by physics, one general approach to approximating a nonlinear function by a linear operator is quantization. In quantization one thinks of a nonlinear function as being a classical observable on some phase space, and the corresponding linear operator is the corresponding quantum observable acting on a Hilbert space, which should again correspond to the classical phase space in some way. The correspondence should depend on a parameter â„Planck-constant-over-2-pi\hbarroman_â„, and in the semiclassical limit â„â†’0â†’Planck-constant-over-2-pi0\hbar\to 0roman_â„ â†’ 0 the correspondence should become closer and closer. This approach suggests that there should be corresponding algorithms for various computational tasks, but for optimizing a nonlinear polynomial function in particular there is a clear analogy: one would first quantize the objective function, and then approximate its minimum or maximum by the minimum or maximum of the corresponding observable. One could even try to approximate the optimal solution itself by finding a classical distribution over the phase space which approximates the top eigenstate of the observable, and then sampling from it. One of the most common such quasiprobability distributions is the Husimi Q-function, which, unlike other common quasiprobability distributions, such as the Wigner function or Glauber-Sudarshan P-function, is nonnegative and integrates to 1, giving a well-defined classical probability distribution. The parameter â„Planck-constant-over-2-pi\hbarroman_â„ then allows one to tune the approximation, for which a smaller value of â„Planck-constant-over-2-pi\hbarroman_â„ would give a more accurate approximation but require more resources computationally. Conversely, one could also try to approximate a quantum state by a classical objective. Without additional assumptions one typically expects a quantum state on a large number of sites to have nontrivial entanglement. However, under symmetry conditions the principle of monogamy of entanglement suggests that no two sites should be strongly entangled with each other, implying that the overall state behaves classically. Indeed, the quantum de Finetti theorem [Christandl_Koenig_Mitchison_Renner_2007] formalizes this intuition with a quantitative error bound. The construction in the de Finetti theorem is quite similar to the rounding algorithm described above â€“ one can think of the Husimi quasiprobability distribution as a mapping from a quantum mixed state to a classical mixed state, and the Glauber-Sudharshan P-quantization as a mapping in the opposite direction, such that the two maps are approximate inverses. 1.2. Results in this paper As we will see, it is possible to make sense of some version of this quantization-based optimization algorithm. It will turn out to be exactly the same as a Hermitian version of the sum-of-squares hierarchy, but several assumptions will need to be made on the original problem. First, the domain Mğ‘€Mitalic_M of the optimization problem should be a compact semialgebraic set in order for the original optimization problem to make sense and for a Hermitian SoS relaxation to even be defined. To be able to think of Mğ‘€Mitalic_M as a classical phase space we will also need Mğ‘€Mitalic_M to be a smooth manifold, and have a Poisson bracket defined on pairs of classical observables, which is the minimum structure needed to define time-evolution of a system in classical mechanics. Because of the quantization procedure we use, we will actually need to further assume that the Poisson bracket comes from a symplectic structure on Mğ‘€Mitalic_M and that Mğ‘€Mitalic_M is also a polarized KÃ¤hler manifold (comes with a holomorphic line bundle â„’â†’Mâ†’â„’ğ‘€\mathcal{L}\to Mcaligraphic_L â†’ italic_M which defines an embedding into a projective space). 1.3. Content of this paper In Section 2, we give a more detailed technical overview and give a more precise definition of quantization. Then in Section 3 we describe one of the simplest examples of quantization, which is a single bosonic mode with phase space â„‚â‰…â„2â„‚superscriptâ„2\mathbb{C}\cong\mathbb{R}^{2}blackboard_C â‰… blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. (Despite the Hilbert space being infinite-dimensional in this case, it is overall simpler because there is no topology involved.) In Section 4 we describe geometric quantization, which is one standard approach to quantizing a compact phase space, which in general must be topologically nontrivial. In Section 5 we state and prove Theorem 19, which is a version of the de Finetti theorem generalized to the geometric quantization framework. Finally in Section 6 we show how to deduce various known quantum de Finetti theorems, including a version of Rennerâ€™s exponential de Finetti theorem and the de Finetti theorem for irreducible representations of Uâ¢(n)ğ‘ˆğ‘›U(n)italic_U ( italic_n ), from our general theorem, and how to deduce some new de Finetti theorems for other phase spaces, including smooth projective varities; in these cases we need to apply some recent results in geoemtric analysis and complex geometry to show that the assumptions of Theorem 19 can be satisfied, and also to explain how they can be used to deduce quantitative error bounds."
https://arxiv.org/html/2411.04013v1,kğ‘˜kitalic_kNN Attention Demystified: A Theoretical Exploration for Scalable Transformers,"Despite their power, Transformers [39] face challenges with long sequences due to the quadratic complexity of self-attention. To address this limitation, methods like kğ‘˜kitalic_k-Nearest-Neighbor (kğ‘˜kitalic_kNN) attention have been introduced [32], enabling each token to attend to only its kğ‘˜kitalic_k closest tokens. While kğ‘˜kitalic_kNN attention has shown empirical success in making Transformers more efficient, its exact approximation guarantees have not been theoretically analyzed. In this work, we establish a theoretical framework for kğ‘˜kitalic_kNN attention, reformulating self-attention as expectations over softmax distributions and leveraging lazy Gumbel sampling [28] with kğ‘˜kitalic_kNN indices for efficient approximation. Building on this framework, we also propose novel sub-quadratic algorithms that approximate self-attention gradients by leveraging efficient sampling techniques, such as Markov Chain-based estimation. Finally, we demonstrate the practical effectiveness of these algorithms through empirical experiments, showcasing their benefits in both training and inference.","Transformer models have become the dominant neural architecture across language, vision, and other domains [39, 13]. However, scaling them to handle larger input sequences remains a significant challenge [38], primarily due to the quadratic complexity of computing self-attention. Overcoming this limitation is crucial for advancing neural networks. Extending context length would enable Transformers to tackle complex tasks like book summarization [23] and time-series forecasting [43, 45, 47]. Furthermore, improving attention efficiency would reduce the computational burden of training, making these models more accessible. Bridging this â€œcompute divideâ€ is vital for democratizing AI [5]. Efficient computation of self-attention has been a focal point of research in recent years [15]. Flash Attention [14] and related work [36] optimize the exact calculation of attention by minimizing wasted computation during GPU I/O operations. However, most approaches focus on approximating the attention function. Sparse Transformers improve efficiency by allowing each token to attend to only a small subset of tokens [27]. These subsets are identified through deterministic methods [8, 18, 35, 26, 31, 7], randomized algorithms [22, 19, 46, 30], or adaptive techniques [11]. Additionally, self-attention is often approximated using low-rank matrices and kernel methods [40, 37, 44, 24, 10]. On the negative side, recent fine-grained complexity reductions indicate that achieving a good approximation with sub-quadratic time is not feasible across all scenarios [25, 3]. In this work, we focus on sparse attention methods where each token vector qiâˆˆâ„dsubscriptğ‘ğ‘–superscriptâ„ğ‘‘q_{i}\in\mathbb{R}^{d}italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT attends to the kğ‘˜kitalic_k tokens kjâˆˆâ„dsubscriptğ‘˜ğ‘—superscriptâ„ğ‘‘k_{j}\in\mathbb{R}^{d}italic_k start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT with the largest inner products qiTâ¢kjsuperscriptsubscriptğ‘ğ‘–ğ‘‡subscriptğ‘˜ğ‘—q_{i}^{T}k_{j}italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_k start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT [16, 42], a paradigm we refer to as kğ‘˜kitalic_kNN Attention. The Routing Transformer [32] was an early example, using kğ‘˜kitalic_k-means clustering to ensure each query only attends to keys within the same cluster. Memorizing Transformers [41] later extended this approach by leveraging kğ‘˜kitalic_kNN search within a stored memory, enabling models to memorize new data during inference. More recently, Unlimiformer models [6] have improved efficiency by using a single kğ‘˜kitalic_kNN data structure (or index) across all attention heads and layers. Previous works have empirically shown that kğ‘˜kitalic_kNN Attention not only improves computational efficiency, but also enhances model architectures and capabilities. However, a rigorous theoretical analysis of kğ‘˜kitalic_kNN Attention is still lacking. Key questions remain unresolved, including the precise approximation guarantees it offers, the optimal value of kğ‘˜kitalic_k, and how to extend the method to approximate the backward pass. Notation Let Q,K,Vâˆˆâ„nÃ—dğ‘„ğ¾ğ‘‰superscriptâ„ğ‘›ğ‘‘Q,K,V\in\mathbb{R}^{n\times d}italic_Q , italic_K , italic_V âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n Ã— italic_d end_POSTSUPERSCRIPT be our query, key and value matrices. Let qi=Qi,:âˆˆâ„dsubscriptğ‘ğ‘–subscriptğ‘„ğ‘–:superscriptâ„ğ‘‘q_{i}=Q_{i,:}\in\mathbb{R}^{d}italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_Q start_POSTSUBSCRIPT italic_i , : end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT be iğ‘–iitalic_i-th row of Qğ‘„Qitalic_Q written as a column vector. We will also denote the jğ‘—jitalic_j-th column of Qğ‘„Qitalic_Q by Q:,jsubscriptğ‘„:ğ‘—Q_{:,j}italic_Q start_POSTSUBSCRIPT : , italic_j end_POSTSUBSCRIPT. We define A:=Qâ¢KTâˆˆâ„nÃ—nassignğ´ğ‘„superscriptğ¾ğ‘‡superscriptâ„ğ‘›ğ‘›A:=QK^{T}\in\mathbb{R}^{n\times n}italic_A := italic_Q italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n Ã— italic_n end_POSTSUPERSCRIPT to be the attention matrix, and O=softmaxâ¢(A)â‹…Vâˆˆâ„nÃ—dğ‘‚â‹…softmaxğ´ğ‘‰superscriptâ„ğ‘›ğ‘‘O=\text{softmax}(A)\cdot V\in\mathbb{R}^{n\times d}italic_O = softmax ( italic_A ) â‹… italic_V âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n Ã— italic_d end_POSTSUPERSCRIPT to be the output of the attention function. The softmax function is applied row-wise to Ağ´Aitalic_A and is defined as a vector valued function Ïƒ:â„nâ†’â„n:ğœâ†’superscriptâ„ğ‘›superscriptâ„ğ‘›\sigma:\mathbb{R}^{n}\to\mathbb{R}^{n}italic_Ïƒ : blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT â†’ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT: Ïƒâ¢(y1,â€¦,yn)i=expâ¡(yi)âˆ‘s=1nexpâ¡(ys)ğœsubscriptsubscriptğ‘¦1â€¦subscriptğ‘¦ğ‘›ğ‘–subscriptğ‘¦ğ‘–superscriptsubscriptğ‘ 1ğ‘›subscriptğ‘¦ğ‘ \sigma(y_{1},...,y_{n})_{i}=\frac{\exp(y_{i})}{\sum_{s=1}^{n}\exp(y_{s})}italic_Ïƒ ( italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_y start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = divide start_ARG roman_exp ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG âˆ‘ start_POSTSUBSCRIPT italic_s = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT roman_exp ( italic_y start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) end_ARG We also let [n]:={1,2,â€¦,n}assigndelimited-[]ğ‘›12â€¦ğ‘›[n]:=\{1,2,...,n\}[ italic_n ] := { 1 , 2 , â€¦ , italic_n } and use the notation polylogâ¢(n)polylogğ‘›\text{polylog}(n)polylog ( italic_n ) as a substitute of logkâ¡(n)superscriptğ‘˜ğ‘›\log^{k}(n)roman_log start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ( italic_n ) for some arbitrary constant kâˆˆâ„¤+ğ‘˜superscriptâ„¤k\in\mathbb{Z}^{+}italic_k âˆˆ blackboard_Z start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT that is independent of nğ‘›nitalic_n. Finally, we use the O~~ğ‘‚\widetilde{O}over~ start_ARG italic_O end_ARG notation to hide polylogarithmic factors. We will often make use of the following boosting lemma: Lemma 1 (Median-Of-Means Boosting, [9]). If Q^^ğ‘„\widehat{Q}over^ start_ARG italic_Q end_ARG is an unbiased estimator of some statistic, then one can obtain an (Îµ,Î´)ğœ€ğ›¿(\varepsilon,\delta)( italic_Îµ , italic_Î´ )-multiplicative estimate of that statistic by suitably combining K:=CÎµ2â¢Varâ¢[Q^]ğ”¼â¢[Q^]2â¢lnâ¡2Î´assignğ¾ğ¶superscriptğœ€2Vardelimited-[]^ğ‘„ğ”¼superscriptdelimited-[]^ğ‘„22ğ›¿K:=\frac{C}{\varepsilon^{2}}\frac{\text{Var}[\widehat{Q}]}{\mathbb{E}[\widehat% {Q}]^{2}}\ln\frac{2}{\delta}italic_K := divide start_ARG italic_C end_ARG start_ARG italic_Îµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG divide start_ARG Var [ over^ start_ARG italic_Q end_ARG ] end_ARG start_ARG blackboard_E [ over^ start_ARG italic_Q end_ARG ] start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG roman_ln divide start_ARG 2 end_ARG start_ARG italic_Î´ end_ARG independent samples of Q^^ğ‘„\widehat{Q}over^ start_ARG italic_Q end_ARG, where Cğ¶Citalic_C is a universal constant. For a comprehensive outline of preliminary results and theory, please refer to Appendix A. 1.1 Our Contributions 1.1.1 A Theoretical Framework for kğ‘˜kitalic_kNN Attention Our work provides a theoretical framework to explain both the efficiency and effectiveness of kğ‘˜kitalic_kNN Attention. Our framework reformulates self-attention as expectations over softmax distributions. These expectations are approximated by sampling from each distribution in sublinear time using Lazy Gumbel Noise Sampling. By connecting kğ‘˜kitalic_kNN, kğ‘˜kitalic_k-Maximum Inner Product Search (MIPS), and Gumbel noise sampling, we develop a new sub-quadratic self-attention approximation algorithm aligning with the kğ‘˜kitalic_kNN Attention paradigm, as summarized in the following informal theorem: Theorem 2. Let Q,K,Vâˆˆâ„nÃ—dğ‘„ğ¾ğ‘‰superscriptâ„ğ‘›ğ‘‘Q,K,V\in\mathbb{R}^{n\times d}italic_Q , italic_K , italic_V âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n Ã— italic_d end_POSTSUPERSCRIPT and Îµ,Î´ğœ€ğ›¿\varepsilon,\deltaitalic_Îµ , italic_Î´ be positive constants. Assume â€–Vâ€–âˆ=Oâ¢(logâ¡n)subscriptnormğ‘‰ğ‘‚ğ‘›||V||_{\infty}=O(\log n)| | italic_V | | start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT = italic_O ( roman_log italic_n ). Then kğ‘˜kitalic_kNN-Attention as presented in Algorithm 2 with k=nğ‘˜ğ‘›k=\sqrt{n}italic_k = square-root start_ARG italic_n end_ARG outputs a matrix O^âˆˆâ„nÃ—d^ğ‘‚superscriptâ„ğ‘›ğ‘‘\widehat{O}\in\mathbb{R}^{n\times d}over^ start_ARG italic_O end_ARG âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n Ã— italic_d end_POSTSUPERSCRIPT such that: |O^iâ¢jâˆ’Oiâ¢j|â‰¤Îµâ¢Oiâ¢jsubscript^ğ‘‚ğ‘–ğ‘—subscriptğ‘‚ğ‘–ğ‘—ğœ€subscriptğ‘‚ğ‘–ğ‘—\displaystyle|\widehat{O}_{ij}-O_{ij}|\leq\varepsilon O_{ij}| over^ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT - italic_O start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT | â‰¤ italic_Îµ italic_O start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT (1) for all (i,j)âˆˆ[n]Ã—[d]ğ‘–ğ‘—delimited-[]ğ‘›delimited-[]ğ‘‘(i,j)\in[n]\times[d]( italic_i , italic_j ) âˆˆ [ italic_n ] Ã— [ italic_d ] with probability at least 1âˆ’Î´1ğ›¿1-\delta1 - italic_Î´ and in sub-quadratic time and space. 1.1.2 Approximating the Backward Pass Our framework can be extended to solve the problem of approximating attention gradients. Even though backpropagation is the main memory bottleneck for large models, few methods approximate attention gradients directly. Alman and Songâ€™s work () is most relevant, deriving inapproximability results for certain parameter regimes. We present new approximation algorithms for self-attention gradients using kğ‘˜kitalic_kNN search. A key challenge is the need to multiply by the transpose of a stochastic matrix, which disrupts our expectation-based reformulation. To address this, we use a Markov-Chain sampling technique, treating the attention matrix as a transition matrix and applying a single-step iteration. Our main theorem can be informally stated as follows: Theorem 3. Let Ï•italic-Ï•\phiitalic_Ï• be a scalar loss function and âˆ‚Ï•/âˆ‚Oâˆˆâ„nÃ—ditalic-Ï•ğ‘‚superscriptâ„ğ‘›ğ‘‘\partial\phi/\partial O\in\mathbb{R}^{n\times d}âˆ‚ italic_Ï• / âˆ‚ italic_O âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n Ã— italic_d end_POSTSUPERSCRIPT. Then, under certain assumptions on the ||â‹…||âˆ||\cdot||_{\infty}| | â‹… | | start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT norms of Q,K,Vğ‘„ğ¾ğ‘‰Q,K,Vitalic_Q , italic_K , italic_V, there exist sub-quadratic time algorithms that output estimates D^Q,D^K,D^Vâˆˆâ„nÃ—dsuperscript^ğ·ğ‘„superscript^ğ·ğ¾superscript^ğ·ğ‘‰superscriptâ„ğ‘›ğ‘‘\widehat{D}^{Q},\widehat{D}^{K},\widehat{D}^{V}\in\mathbb{R}^{n\times d}over^ start_ARG italic_D end_ARG start_POSTSUPERSCRIPT italic_Q end_POSTSUPERSCRIPT , over^ start_ARG italic_D end_ARG start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT , over^ start_ARG italic_D end_ARG start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n Ã— italic_d end_POSTSUPERSCRIPT for which with probability at least 1âˆ’Î´1ğ›¿1-\delta1 - italic_Î´ it holds that: â€–D^Qâˆ’âˆ‚Ï•/âˆ‚Qâ€–âˆâ‰¤eQ,â€–D^Kâˆ’âˆ‚Ï•/âˆ‚Kâ€–âˆâ‰¤eKâ¢ and â¢â€–D^Vâˆ’âˆ‚Ï•/âˆ‚Vâ€–âˆâ‰¤eVformulae-sequencesubscriptnormsuperscript^ğ·ğ‘„italic-Ï•ğ‘„subscriptğ‘’ğ‘„subscriptnormsuperscript^ğ·ğ¾italic-Ï•ğ¾subscriptğ‘’ğ¾ and subscriptnormsuperscript^ğ·ğ‘‰italic-Ï•ğ‘‰subscriptğ‘’ğ‘‰\displaystyle||\widehat{D}^{Q}-\partial\phi/\partial Q||_{\infty}\leq e_{Q},\,% ||\widehat{D}^{K}-\partial\phi/\partial K||_{\infty}\leq e_{K}\,\text{ and }\,% ||\widehat{D}^{V}-\partial\phi/\partial V||_{\infty}\leq e_{V}| | over^ start_ARG italic_D end_ARG start_POSTSUPERSCRIPT italic_Q end_POSTSUPERSCRIPT - âˆ‚ italic_Ï• / âˆ‚ italic_Q | | start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT â‰¤ italic_e start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT , | | over^ start_ARG italic_D end_ARG start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT - âˆ‚ italic_Ï• / âˆ‚ italic_K | | start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT â‰¤ italic_e start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT and | | over^ start_ARG italic_D end_ARG start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT - âˆ‚ italic_Ï• / âˆ‚ italic_V | | start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT â‰¤ italic_e start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT (2) where eQ,eK,eVsubscriptğ‘’ğ‘„subscriptğ‘’ğ¾subscriptğ‘’ğ‘‰e_{Q},e_{K},e_{V}italic_e start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT are explicit error parameters that can roughly be bounded by Oâ¢(Îµâ¢nâ‹…polylogâ¢(n))ğ‘‚â‹…ğœ€ğ‘›polylogğ‘›O(\varepsilon n\cdot\text{polylog}(n))italic_O ( italic_Îµ italic_n â‹… polylog ( italic_n ) ) Algorithm 4 computes D^Vsuperscript^ğ·ğ‘‰\widehat{D}^{V}over^ start_ARG italic_D end_ARG start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT, while Algorithms for D^Ksuperscript^ğ·ğ¾\widehat{D}^{K}over^ start_ARG italic_D end_ARG start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT and D^Qsuperscript^ğ·ğ‘„\widehat{D}^{Q}over^ start_ARG italic_D end_ARG start_POSTSUPERSCRIPT italic_Q end_POSTSUPERSCRIPT can be found in Appendices E and F."
https://arxiv.org/html/2411.03390v1,Six Candidates Suffice to Win a Voter Majority,"A cornerstone of social choice theory is Condorcetâ€™s paradox which says that in an election where nğ‘›nitalic_n voters rank mğ‘šmitalic_m candidates it is possible that, no matter which candidate is declared the winner, a majority of voters would have preferred an alternative candidate. Instead, can we always choose a small committee of winning candidates that is preferred to any alternative candidate by a majority of voters?Elkind, Lang, and Saffidine raised this question and called such a committee a Condorcet winning set. They showed that winning sets of size 2222 may not exist, but sets of size logarithmic in the number of candidates always do. In this work, we show that Condorcet winning sets of size 6666 always exist, regardless of the number of candidates or the number of voters. More generally, we show that if Î±1âˆ’lnâ¡Î±â‰¥2k+1ğ›¼1ğ›¼2ğ‘˜1\frac{\alpha}{1-\ln\alpha}\geq\frac{2}{k+1}divide start_ARG italic_Î± end_ARG start_ARG 1 - roman_ln italic_Î± end_ARG â‰¥ divide start_ARG 2 end_ARG start_ARG italic_k + 1 end_ARG, then there always exists a committee of size kğ‘˜kitalic_k such that less than an Î±ğ›¼\alphaitalic_Î± fraction of the voters prefer an alternate candidate. These are the first nontrivial positive results that apply for all kâ‰¥2ğ‘˜2k\geq 2italic_k â‰¥ 2.Our proof uses the probabilistic method and the minimax theorem, inspired by recent work on approximately stable committee selection. We construct a distribution over committees that performs sufficiently well (when compared against any candidate on any small subset of the voters) so that this distribution must contain a committee with the desired property in its support.","Voting is a versatile model for the aggregation of individual preferences to reach a collective decision. Disparate situations, such as constituents choosing representatives, organizations hiring employees, judges choosing prize winners, and even friends choosing games to play, can all be understood as a group of voters choosing from a pool of candidates. Voting theory seeks to understand how winning candidates can be selected in a fair and representative manner. One of the longest known challenges with voting is Condorcetâ€™s paradox, discovered by Nicolas de Condorcet around the French Revolution [dC85].111It is plausible that in early academic explorations of voting, 13th-century philosopher Ramon Llull had already discovered the possibility of this paradoxical situation [Llu83, HP00]. The paradox is that in an election where voters have ranked preferences over candidates, the preferences of the â€œmajorityâ€ can be contradictory â€” no matter which candidate is declared the winner, a majority of the voters would have preferred another candidate. In fact, the contradiction can be even more dramatic, with â€œmajorityâ€ replaced by a fraction arbitrarily close to 1. An illustrative example is when the voters have cyclic preferences as, for example, displayed in Table 1. v1subscriptğ‘£1v_{1}italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT v2subscriptğ‘£2v_{2}italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT v3subscriptğ‘£3v_{3}italic_v start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT v4subscriptğ‘£4v_{4}italic_v start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT v5subscriptğ‘£5v_{5}italic_v start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT v6subscriptğ‘£6v_{6}italic_v start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPT 1 2 3 4 5 6 2 3 4 5 6 1 3 4 5 6 1 2 4 5 6 1 2 3 5 6 1 2 3 4 6 1 2 3 4 5 Table 1: An election where voters have cyclic preferences. The column headed with visubscriptğ‘£ğ‘–v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT represents the iğ‘–iitalic_ith voterâ€™s ranking of the candidates (labeled 1,2,â€¦,612â€¦61,2,\dots,61 , 2 , â€¦ , 6 from top to bottom). For each candidate, another candidate is preferred by every voter except one. Though it is impossible to always find a single candidate that is always preferred over the others by a majority (called a Condorcet winner), one hope is that relaxations of this condition are still possible to achieve. A natural relaxation arises in the setting of committee selection, where rather than choosing a single winner, the goal is to choose a committee of kğ‘˜kitalic_k winners. For example, a political system may have districts with multiple representatives, organizations may make many hires at once, and friends might play more than one game in an evening. Another view is that committee selection can be used as an filtering step in a process with more than one round, like primaries or runoffs, choosing interviewees for a position, or nominations for a prize. In this context, Elkind, Lang, and Saffidine [ELS11, ELS15] asked: is it always possible to find a small committee of candidates such that no other candidate is preferred by a majority of voters over each member of the committee? They called this committee-analogue of a Condorcet winner a Condorcet winning set, and defined the Condorcet dimension of an election as the size of its smallest Condorcet winning set. For example, the election depicted in Table 1 has Condorcet dimension 2, since any pair of diametrically opposite candidates such as {3,6}36\{3,6\}{ 3 , 6 } would be a Condorcet winning set. More generally, [ELS15] raised the following question for an arbitrary threshold of Î±ğ›¼\alphaitalic_Î± in place of 1212\frac{1}{2}divide start_ARG 1 end_ARG start_ARG 2 end_ARG, and a target committee size kğ‘˜kitalic_k. Question 1 ([ELS15]). A committee Sğ‘†Sitalic_S is Î±ğ›¼\alphaitalic_Î±-undominated if for all candidates aâˆ‰Sğ‘ğ‘†a\notin Sitalic_a âˆ‰ italic_S, less than an Î±ğ›¼\alphaitalic_Î± fraction of voters prefer ağ‘aitalic_a over each member of Sğ‘†Sitalic_S. For what values of kâˆˆâ„¤+ğ‘˜superscriptâ„¤k\in\mathbb{Z}^{+}italic_k âˆˆ blackboard_Z start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT and Î±âˆˆ(0,1]ğ›¼01\alpha\in(0,1]italic_Î± âˆˆ ( 0 , 1 ] does every election have an Î±ğ›¼\alphaitalic_Î±-undominated committee of size kğ‘˜kitalic_k? In particular, we would like to know, for each kğ‘˜kitalic_k, what is the smallest Î±ğ›¼\alphaitalic_Î± for which Î±ğ›¼\alphaitalic_Î±-undominated committees of size kğ‘˜kitalic_k always exist (and, equivalently, for each Î±ğ›¼\alphaitalic_Î±, the smallest kğ‘˜kitalic_k such that these committees always exist). Condorcetâ€™s paradox (or rather, its aformentioned generalization) shows that for k=1ğ‘˜1k=1italic_k = 1 and any Î±ğ›¼\alphaitalic_Î± bounded away from 1, there are elections with no Î±ğ›¼\alphaitalic_Î±-undominated singleton candidates. For the threshold of Î±=12ğ›¼12\alpha=\frac{1}{2}italic_Î± = divide start_ARG 1 end_ARG start_ARG 2 end_ARG, [ELS15] constructed instances with Condorcet dimension 3 by taking the Kronecker product of two elections with cyclic preferences (see Table 3). This construction can be easily extended to give a lower bound of 2k+12ğ‘˜1\frac{2}{k+1}divide start_ARG 2 end_ARG start_ARG italic_k + 1 end_ARG on the smallest Î±ğ›¼\alphaitalic_Î± such that there always exists an Î±ğ›¼\alphaitalic_Î±-undominated committee of size kğ‘˜kitalic_k (see Appendix B). They also showed that an election with mğ‘šmitalic_m candidates has Condorcet dimension at most âŒˆlog2â¡mâŒ‰subscript2ğ‘š\lceil\log_{2}m\rceilâŒˆ roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_m âŒ‰; to see this, note that some candidate beats a majority of the other candidates, so we can iteratively add such a candidate to our committee and remove all the candidates that it beats. 1.1 Our Contributions We prove that every election has Condorcet dimension at most 6. This result is a corollary of our main theorem, which gives a nontrivial existence result for Î±ğ›¼\alphaitalic_Î±-undominated committees of size kâ‰¥2ğ‘˜2k\geq 2italic_k â‰¥ 2. We note that the final result we prove (Theorem 5) is stronger, but we start with the approximation below as it is easier to get a handle on. (For a comparison, see Table 2.) Theorem 1. If Î±1âˆ’lnâ¡Î±â‰¥2k+1ğ›¼1ğ›¼2ğ‘˜1\frac{\alpha}{1-\ln\alpha}\geq\frac{2}{k+1}divide start_ARG italic_Î± end_ARG start_ARG 1 - roman_ln italic_Î± end_ARG â‰¥ divide start_ARG 2 end_ARG start_ARG italic_k + 1 end_ARG, then in any election, there exists an Î±ğ›¼\alphaitalic_Î±-undominated committee of size kğ‘˜kitalic_k. For the specific threshold of Î±=12ğ›¼12\alpha=\frac{1}{2}italic_Î± = divide start_ARG 1 end_ARG start_ARG 2 end_ARG, Theorem 1 applies as long as kâ‰¥3+4â¢lnâ¡2â‰ˆ5.77ğ‘˜3425.77k\geq 3+4\ln 2\approx 5.77italic_k â‰¥ 3 + 4 roman_ln 2 â‰ˆ 5.77, and so any election has Condorcet dimension at most 6666 (which is not far from the lower bound of 3333). Taking k=2ğ‘˜2k=2italic_k = 2, Theorem 1 implies that there always exists a pair of candidates such that no third candidate is preferred by more than roughly 80%percent8080\%80 % of the voters. Even replacing 80%percent8080\%80 % with 99%percent9999\%99 %, this was previously unknown. These results show that just by having a few winners instead of one, the most dramatic failures of Condorcetâ€™s paradox are avoidable. We emphasize that these results hold for any election, regardless of the number of voters, the number of candidates, or the preferences that the voters have over candidates. Our starting point for proving Theorem 1 is the observation that 1 is closely linked to the problem of approximate stability in committee selection [JMW20]. The principle behind stability is that a subset of voters should have control over a subset of the committee of proportional size. That is, a committee of size kğ‘˜kitalic_k is stable (also referred to as in the core [Sca67, Fol70, FMS18]) if the fraction of voters that prefers any committee of size kâ€²superscriptğ‘˜â€²k^{\prime}italic_k start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT is less than kâ€²ksuperscriptğ‘˜â€²ğ‘˜\frac{k^{\prime}}{k}divide start_ARG italic_k start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT end_ARG start_ARG italic_k end_ARG. We note that in this setting, voters have preferences over committees rather than candidates. This more expressive space of preferences gives it the power to model a wide variety of preference structures, such as approval voting and participatory budgeting. Unfortunately, in many settings, stable committees do not always exist. To remedy this, [JMW20] put forth the following approximate notion of stability, and showed the surprising result that for any monotone preference structure and any kğ‘˜kitalic_k, a 32323232-stable committee of size kğ‘˜kitalic_k exists. Definition 1 (Approximately stable committees [JMW20]). A committee Sğ‘†Sitalic_S of kğ‘˜kitalic_k candidates is cğ‘citalic_c-stable if for any committee Sâ€²superscriptğ‘†â€²S^{\prime}italic_S start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT of size kâ€²superscriptğ‘˜â€²k^{\prime}italic_k start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT, the fraction of voters that prefers Sâ€²superscriptğ‘†â€²S^{\prime}italic_S start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT over Sğ‘†Sitalic_S is less than câ‹…kâ€²kâ‹…ğ‘superscriptğ‘˜â€²ğ‘˜c\cdot\frac{k^{\prime}}{k}italic_c â‹… divide start_ARG italic_k start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT end_ARG start_ARG italic_k end_ARG. Consider the natural preference order over committees induced by rankings over candidates, where vğ‘£vitalic_v prefers Sâ€²superscriptğ‘†â€²S^{\prime}italic_S start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT over Sğ‘†Sitalic_S if and only if she prefers her favorite candidate in Sâ€²superscriptğ‘†â€²S^{\prime}italic_S start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT over her favorite in Sğ‘†Sitalic_S. A simple observation (explained more fully in Appendix A) shows that a committee of size kğ‘˜kitalic_k is cğ‘citalic_c-stable if and only if it is ckğ‘ğ‘˜\frac{c}{k}divide start_ARG italic_c end_ARG start_ARG italic_k end_ARG-undominated. For this ranked preference structure, the constant of 32323232 in the result of [JMW20] can be improved to 16161616 using the existence of stable lotteries for these preferences [CJMW20]. Then, as a black box, [JMW20] implies that 16k16ğ‘˜\frac{16}{k}divide start_ARG 16 end_ARG start_ARG italic_k end_ARG-undominated committees of size kğ‘˜kitalic_k always exist, which in turn implies that we can always find Condorcet winning sets of size at most 32323232. Since this conclusion follows easily from [JMW20], we attribute the first constant upper bound on the size of Condorcet winning sets to their work. One can interpret the approximately stable committee problem as a version of 1 focused on the asymptotics of Î±ğ›¼\alphaitalic_Î± as the committee size kğ‘˜kitalic_k grows large. For this purpose, [JMW20] implies a result that is optimal up to a constant factor, but it says nothing nontrivial for committees of size at most 16161616. In contrast, Theorem 1 gives results even for k=2ğ‘˜2k=2italic_k = 2, and outperforms the bound implied by [JMW20] for kâ‰¤1.75Ã—104ğ‘˜1.75superscript104k\leq 1.75\times 10^{4}italic_k â‰¤ 1.75 Ã— 10 start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT, despite only implying the existence of Oâ¢(logâ¡k)ğ‘‚ğ‘˜O(\log k)italic_O ( roman_log italic_k )-stable committees. Nonetheless, we show that our techniques can be applied to the asymptotic setting as well, giving an improvement over [JMW20]. Theorem 2. In any election, there exists a 9.8217k9.8217ğ‘˜\frac{9.8217}{k}divide start_ARG 9.8217 end_ARG start_ARG italic_k end_ARG-undominated committee of size kğ‘˜kitalic_k. As a corollary, Theorem 2 implies the existence of 9.82179.82179.82179.8217-stable committees for preferences induced by rankings over candidates. We note that Theorem 2 improves Theorem 1 for kâ‰¥496ğ‘˜496k\geq 496italic_k â‰¥ 496. 1.2 Technical Overview Our approach, building on [JMW20], is to first construct a particular distribution over committees of size kğ‘˜kitalic_k, and then to show that by sampling from this distribution, the resulting committee is Î±ğ›¼\alphaitalic_Î±-undominated in expectation. In fact, [ELS15]â€™s proof that the existence of Oâ¢(logâ¡m)ğ‘‚ğ‘šO(\log m)italic_O ( roman_log italic_m ) size Condorcet winning committees in elections with mğ‘šmitalic_m candidates can also be viewed through this framework. There, we can consider the uniform distribution over candidates. To construct the committee, we sample from this distribution, remove the candidates that are beaten, and recurse on the remaining candidates. In expectation, half of the candidates are removed in each round, so the algorithm is likely to end with a committee of Oâ¢(logâ¡m)ğ‘‚ğ‘šO(\log m)italic_O ( roman_log italic_m ) candidates. The greedy algorithm of choosing the candidate that beats the most others in each round can be viewed as derandomization via conditional expectation. In this light, a natural approach to improving the Oâ¢(logâ¡m)ğ‘‚ğ‘šO(\log m)italic_O ( roman_log italic_m ) guarantee is to find a better distribution over committees. One of the insights in [JMW20] was to construct this distribution via the equilibrium of a zero-sum game. In the game, the defender chooses a committee Sğ‘†Sitalic_S of size kğ‘˜kitalic_k, and the attacker chooses a candidate ağ‘aitalic_a. After the choices are made, the defender pays the attacker a dollar for each voter that prefers ağ‘aitalic_a over all members of Sğ‘†Sitalic_S. The optimal strategy for the defender is to choose a committee randomly according to some distribution, which [JMW20] call the stable lottery. Then to create a committee of size kğ‘˜kitalic_k, [JMW20] take a recursive approach. First, they sample a committee Sğ‘†Sitalic_S of size k/2ğ‘˜2k/2italic_k / 2, and show that ignoring the 25% of voters that least like Sğ‘†Sitalic_S, any candidate ağ‘aitalic_a is preferred over Sğ‘†Sitalic_S by less than a 8k8ğ‘˜\frac{8}{k}divide start_ARG 8 end_ARG start_ARG italic_k end_ARG fraction of the voters (which are treated as an irrevocable loss). In the next step, they recurse on the ignored voters, sample a committee of size k/4ğ‘˜4k/4italic_k / 4, and lose less than another 4k4ğ‘˜\frac{4}{k}divide start_ARG 4 end_ARG start_ARG italic_k end_ARG fraction of the voters against any candidate ağ‘aitalic_a. The committee size and fraction of voters we lose continue to decrease exponentially, and in the end we have a committee of size kğ‘˜kitalic_k such that less than 16k16ğ‘˜\frac{16}{k}divide start_ARG 16 end_ARG start_ARG italic_k end_ARG voters prefer any candidate ağ‘aitalic_a. To prove Theorem 1, we introduce three twists into this framework. Two are part of how we set up the zero-sum game in order to construct a distribution over committees that individual candidates perform poorly against (Lemma 1), and one is in how we show that in expectation, a random committee sampled from the distribution performs well (Lemma 2 and 4). Improving the game by confining the adversary. First, we modify the setup of the game so that the adversary must choose both a candidate ağ‘aitalic_a and a subset Uğ‘ˆUitalic_U of an Î±ğ›¼\alphaitalic_Î± fraction of the voters. The adversary then only gets paid for the voters in Uğ‘ˆUitalic_U that prefer ağ‘aitalic_a over the committee Sğ‘†Sitalic_S. By tying the hands of the adversary in this way, we can drive down the value of the game, which gives a more favorable guarantee for the distribution over committees. Once we fix the distribution over committees (referred to by Î”Î”\Deltaroman_Î”), we measure the quality of a candidate ağ‘aitalic_a or committee Sğ‘†Sitalic_S with respect to a voter vğ‘£vitalic_v with a crucial notion that we call the rank, denoted rankvâ¡(a)subscriptrankğ‘£ğ‘\operatorname{rank}_{v}(a)roman_rank start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_a ) or rankvâ¡(S)subscriptrankğ‘£ğ‘†\operatorname{rank}_{v}(S)roman_rank start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_S ) (see Definition 3). Roughly speaking, this is simply the probability when we sample from Î”Î”\Deltaroman_Î” that we get a committee that is worse than ağ‘aitalic_a or Sğ‘†Sitalic_S in vğ‘£vitalic_vâ€™s preference. The activation function. The second twist is what we call the activation function gğ‘”gitalic_g, which allows us freedom in how we measure each voterâ€™s preferences for candidates and committees. This function may seem somewhat enigmatic in the proof, but here we try to give some rough intuition for the idea behind it. The initial observation is that by using versions of the zero-sum game with different committee sizes, we can construct distributions over committees of size kğ‘˜kitalic_k in a variety of ways. The simplest would be to take the optimal mixed strategy for the defender in the original game with committee size kğ‘˜kitalic_k, but we could also take the optimal strategy for size k/2ğ‘˜2k/2italic_k / 2, sample twice from it and take the union. These different ways of constructing the distributions can actually be interpreted as attaching different activation functions to the defenderâ€™s distribution in the payoffs of the original game. For example, sampling twice from the k/2ğ‘˜2k/2italic_k / 2 distribution is equivalent to attaching the function gâ¢(x)=xğ‘”ğ‘¥ğ‘¥g(x)=\sqrt{x}italic_g ( italic_x ) = square-root start_ARG italic_x end_ARG, and the reason corresponds to the fact that sampling two uniform reals from [0,1]01[0,1][ 0 , 1 ] and taking the max is equivalent to sampling one uniform real from [0,1]01[0,1][ 0 , 1 ] and taking the square root. In the end, thanks to the generality of the minimax theorem, the proof works for any non-constant, non-decreasing function g:[0,1]â†’â„â‰¥0:ğ‘”â†’01subscriptâ„absent0g\colon[0,1]\to\mathbb{R}_{\geq 0}italic_g : [ 0 , 1 ] â†’ blackboard_R start_POSTSUBSCRIPT â‰¥ 0 end_POSTSUBSCRIPT such that gâ¢(xk)ğ‘”superscriptğ‘¥ğ‘˜g(x^{k})italic_g ( italic_x start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ) is convex. These functions give a richer continuous space of options for modifying the game, some of which are not easily interpretable in terms of the intuition described above. Each choice of gğ‘”gitalic_g gives a different bound for Î±ğ›¼\alphaitalic_Î± as a function of kğ‘˜kitalic_k, and so we can simply choose the function that gives the best guarantee. A one-shot approach with finer accounting of all voters. Third, we use a more precise approach for showing that some committee in the support of our distribution performs well, by diligently accounting for the contributions of each voter. In each step of [JMW20]â€™s recursion, when they sample committee Sğ‘†Sitalic_S, they consider for each voter vğ‘£vitalic_v whether or not rankvâ¡(S)subscriptrankğ‘£ğ‘†\operatorname{rank}_{v}(S)roman_rank start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_S ) is above some threshold (called Î²ğ›½\betaitalic_Î², which is set to 1414\frac{1}{4}divide start_ARG 1 end_ARG start_ARG 4 end_ARG). The voters below the threshold are ignored, and then recursed on in the next iteration. There are two potential roadblocks in using this approach for small committee sizes. Intuitively, if we are aiming for a final committee size of around 6, the recursion cannot be very deep. Each iteration can only choose 2 or 3 candidates, for which the guarantee is insufficient. That is, the benefits of the recursion only kick in for sufficiently large committees, and for small committees, it is better to sample the whole committee in one shot (without recursion). Second, there is too much loss in evaluating each voter with a binary threshold, and without recursion, we need better accounting for voters with a low opinion of the committee. In Lemma 2 and 4, we give a smoother analysis, which allows us to more precisely account for the contributions of each voter. To give some rough intuition, what we would like to show is that there is some Sğ‘†Sitalic_S such that the total sum of rankvâ¡(S)subscriptrankğ‘£ğ‘†\operatorname{rank}_{v}(S)roman_rank start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_S ) is large for any subset of an Î±ğ›¼\alphaitalic_Î± fraction of the voters. If we fix Sğ‘†Sitalic_S and plot each rankvâ¡(S)subscriptrankğ‘£ğ‘†\operatorname{rank}_{v}(S)roman_rank start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_S ), ordered from smallest to largest, it suffices to bound the area under the bottom Î±ğ›¼\alphaitalic_Î± fraction. It turns out that the worst case for these ranks (that minimizes the area for all Sğ‘†Sitalic_S) is not a step function with a sharp threshold, but rather a linear function with slope 1111 (akin to the cyclic preferences depicted in Table 1). Finally, to prove Theorem 2, we use our modifications in tandem with the recursive approach. In the proof of this theorem, the idea above that does the heavy lifting is the use of the activation function gğ‘”gitalic_g. 1.3 Related Work Proportionality in committee selection. In the context of committee selection, the principle of proportionality says that large voter coalitions should have their preferences fairly represented â€” an idea that dates back to at least the 19th century [Dro81]. Since its advent, a substantial body of research has been dedicated to studying the possibility and implications of proportionality. One of the most widely studied models is approval voting, where voters express their preferences by selecting a subset of candidates they approve of. We refer the reader to a survey by Lackner and Skowron [LS23] for a detailed discussion on the topic. A key appeal of this model is that there are a wide variety of proportionality axioms such as justified representation (JR) [ABC+17] and its variants (for example, [FEL+17, BP23]) that are satisfied by natural rules (such as Proportional Approval Voting (Thieleâ€™s rule) [Thi95, Kil10, ABC+17, PS20], PhragmÃ©nâ€™s rule [Phr94, PS20], and the Method of Equal Shares [PS20, PPS21]). These ideas have also been impactful in practice, with for example, the historical use of Thieleâ€™s rule and PhragmÃ©nâ€™s rule [Jan16], and the recent successful implementation of the Method of Equal Shares for participatory budgeting in several European cities [PS]. Additionally, this line of work is driven forward by intriguing conjectures that even stronger axioms, such as core stability [ABC+17, FMS18], might be universally satisfiable as well. In comparison, proportionality in committee selection with ranked preferences is relatively under-explored. As [LS23] suggest, part of the challenge is that notions of proportionality in the approval setting do not always generalize to the ranking setting. (Or, like with core stability, the analogous axioms are not always satisfiable [CJMW20].) One particularly well studied class of rank-based committee selection rules is that of committee scoring rules [EFSS17]. These voting rules, which generalize scoring rules in the single-winner setting, capture several natural committee selection rules, and have been axiomatically characterized [FSST19, SFS19]. We refer the reader to [FSST17] for a more in-depth discussion. Committee analogues of Condorcet winners. Grappling with Condorcetâ€™s paradox has been a major driving force in social choice theory, and naturally, other attempts have been made to extend the notion of Condorcet winners to the multi-winner setting. Fishburn [Fis81b, Fis81a] introduced the idea of a majority committee, defined as a committee preferred by a majority of voters over any other committee of the same size. The Smith set [Goo71, Smi73] Sğ‘†Sitalic_S is defined as the minimal committee such that for any aâˆ‰Sğ‘ğ‘†a\notin Sitalic_a âˆ‰ italic_S and bâˆˆSğ‘ğ‘†b\in Sitalic_b âˆˆ italic_S, a majority of voters prefers bğ‘bitalic_b over ağ‘aitalic_a. Uncovered sets [Fis77, Mil80], bipartisan sets [LLLB93] (the support of maximal lotteries [Fis84]), and other tournament solutions [BBH16] can also be viewed as multi-winner generalizations of Condorcet winners. However, these notions face the same challenge as Condorcet winners: they either do not always exist or sometimes coincide with the entire (potentially large) set of candidates. As in the single-winner case, the goal shifts to identifying Condorcet-consistent rules, which select a Condorcet winner (or the analogous multi-winner notion) when one exists [Coe05, BC08]. In this context, Theorem 1 highlights a distinct advantage of the approach by Elkind, Lang, and Saffidine [ELS15]: small Condorcet-undominated sets are guaranteed to exist. Other explorations of 1. Lastly, we mention a few other interesting explorations of Condorcet winning sets, and more generally Î±ğ›¼\alphaitalic_Î±-undominated sets. [Gei14] used SAT solving to determine the largest Condorcet dimension in elections with a small number of voters and candidates. Their search did not turn up any instances with dimension larger than 3, but they found an election with just 6 voters and candidates with dimension 3, and they showed that this is the smallest possible. (We include one such instance in Table 4.) [Blo18] also explored whether elections with Condorcet dimension 4 could be constructed by exploring dominating sets in tourament graphs, but that approach did not yield any such elections. On the positive side, [LVvS24] very recently showed that in elections where the voters and candidates are embedded in a 2-dimensional space, and their preferences are defined by their distance according to the â„“1subscriptâ„“1\ell_{1}roman_â„“ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT or â„“âˆsubscriptâ„“\ell_{\infty}roman_â„“ start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT norm, the Condorcet dimension is at most 3. In a more informal setting, a question isomorphic to 1 has also been explored from a combinatorial perspective in a series of Math Overflow posts [PÃ¡l13, Spe13, Bra13]. These posts offer an intriguing window into different approaches to resolving the problem, including why some natural approaches fall short. In their formulation [PÃ¡l13], each candidate ağ‘aitalic_a is represented by a function fa:[n]â†’â„•:subscriptğ‘“ğ‘â†’delimited-[]ğ‘›â„•f_{a}\colon[n]\to\mathbb{N}italic_f start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT : [ italic_n ] â†’ blackboard_N, which can be thought of as a map from each voter vğ‘£vitalic_v to the rank of ağ‘aitalic_a in vğ‘£vitalic_vâ€™s preference order. They ask 1, with a particular focus on the case where k=2ğ‘˜2k=2italic_k = 2. The responses contain examples of elections with Condorcet dimension 3, including the general lower bound that Î±ğ›¼\alphaitalic_Î±-undominated committees of size kğ‘˜kitalic_k do not always exist when Î±<2k+1ğ›¼2ğ‘˜1\alpha<\frac{2}{k+1}italic_Î± < divide start_ARG 2 end_ARG start_ARG italic_k + 1 end_ARG [Zba14]. One natural approach towards positive results, suggested by Speyer [Spe13], is to solve the following graph theory question. Question 2. For what choices of â„“,kâˆˆâ„¤+â„“ğ‘˜superscriptâ„¤\ell,k\in\mathbb{Z}^{+}roman_â„“ , italic_k âˆˆ blackboard_Z start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT does there exist a directed graph with girth larger than â„“â„“\ellroman_â„“ such that every subset of kğ‘˜kitalic_k vertices has a common in-neighbor? If there does not exist such a graph for some choice of â„“â„“\ellroman_â„“ and kğ‘˜kitalic_k, then this implies that every election has a (1âˆ’1â„“)11â„“(1-\frac{1}{\ell})( 1 - divide start_ARG 1 end_ARG start_ARG roman_â„“ end_ARG )-undominated set of size kğ‘˜kitalic_k, by considering the graph on candidates where there is an edge (a,b)ğ‘ğ‘(a,b)( italic_a , italic_b ) whenever more than 1âˆ’1â„“11â„“1-\frac{1}{\ell}1 - divide start_ARG 1 end_ARG start_ARG roman_â„“ end_ARG fraction of the voters prefer ağ‘aitalic_a over bğ‘bitalic_b. In particular, if every triangle-free graph has a pair of vertices without a common in-neighbor (â„“=3â„“3\ell=3roman_â„“ = 3 and k=2ğ‘˜2k=2italic_k = 2), then this would imply that 2323\frac{2}{3}divide start_ARG 2 end_ARG start_ARG 3 end_ARG-undominated sets of size 2222 always exist, which would resolve 1 for k=2ğ‘˜2k=2italic_k = 2. Unfortunately, such graphs do exist. [AHL+15] gave a positive answer to 2 for every â„“,kâˆˆâ„¤+â„“ğ‘˜superscriptâ„¤\ell,k\in\mathbb{Z}^{+}roman_â„“ , italic_k âˆˆ blackboard_Z start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT, using a construction based on additive combinatorics."
https://arxiv.org/html/2411.03331v1,"Hypergraphs as Weighted Directed Self-Looped Graphs:Spectral Properties, Clustering, Cheeger Inequality","Hypergraphs naturally arise when studying group relations and have been widely used in the field of machine learning. There has not been a unified formulation of hypergraphs, yet the recently proposed edge-dependent vertex weights (EDVW) modeling [7] is one of the most generalized modeling methods of hypergraphs, i.e., most existing hypergraphs can be formulated as EDVW hypergraphs without any information loss to the best of our knowledge. However, the relevant algorithmic developments on EDVW hypergraphs remain nascent: compared to spectral graph theories, the formulations are incomplete, the spectral clustering algorithms are not well-developed, and one result regarding hypergraph Cheeger Inequality is even incorrect. To this end, deriving a unified random walk-based formulation, we propose our definitions of hypergraph Rayleigh Quotient, NCut, boundary/cut, volume, and conductance, which are consistent with the corresponding definitions on graphs. Then, we prove that the normalized hypergraph Laplacian is associated with the NCut value, which inspires our HyperClus-G algorithm for spectral clustering on EDVW hypergraphs. Finally, we prove that HyperClus-G can always find an approximately linearly optimal partitioning in terms of Both NCut111The NCut of the returned partition ğ’©ğ’©\mathcal{N}caligraphic_N and the optimal NCut of any partition ğ’©âˆ—superscriptğ’©\mathcal{N}^{*}caligraphic_N start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT satisfy ğ’©â‰¤Oâ¢(ğ’©âˆ—)ğ’©ğ‘‚superscriptğ’©\mathcal{N}\leq O(\mathcal{N}^{*})caligraphic_N â‰¤ italic_O ( caligraphic_N start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ). and conductance 222The conductance of the returned partition Î¦Î¦\Phiroman_Î¦ and the optimal conductance Î¦âˆ—superscriptÎ¦\Phi^{*}roman_Î¦ start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT satisfy Î¦â‰¤Oâ¢(Î¦âˆ—)Î¦ğ‘‚superscriptÎ¦\Phi\leq O(\Phi^{*})roman_Î¦ â‰¤ italic_O ( roman_Î¦ start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ). Additionally, we provide extensive experiments to validate our theoretical findings from an empirical perspective.","Higher-order relations are ubiquitous in nature, such as co-authorship [14, 48, 40], interactions between multiple proteins or chemicals [13, 47], items that are liked by the same person [49, 46], and interactions between multiple species in an ecosystem [20, 38]. Hypergraphs, extended from graphs, with the powerful capacity to model group interactions (i.e., higher-order relations), show extraordinary potential to be applied to many real-world tasks where the connections are beyond pair-wise. Therefore, hypergraphs have been used widely in recommendation systems [56, 36, 18], information retrieval [27, 53, 35] and link prediction [26, 12]. Hypergraphs modeled by edge-dependent vertex weights (EDVW) were necessitated in a recent work [7], with a motivating example that in citation networks, each scholar (i.e., vertex) may contribute differently to each co-authored publication (i.e., hyperedge). The authors show that hypergraphs with edge-independent vertex weights (EIVW) do not actually utilize the higher-order relations for the following two reasons. First, the hypergraph Laplacian matrix proposed by the seminal work [52], which serves as a basis of many follow-up algorithms, is equal to the Laplacian matrix of a closely related graph with only pair-wise relations. In this way, all the linear Laplacian operators utilize only pair-wise relationships between vertices [1]. Second, many hypergraph algorithms [37, 31, 4] are based on random walks [42, 9, 34], but it has been proved that for any EIVW hypergraph, there exists a weighted pair-wise graph on which a random walk is equivalent to that on the original hypergraph [7]. Table 1: Properties of graph models/formulations. EDVW hypergraphs generalized EIVW hypergraphs by allowing each hyperedge to distribute its vertex weights, bringing better formulation flexibility. Modeling/Formulation undirected graphs EIVW hypergraphs EDVW hypergraphs edge/hyperedge weights âˆšsquare-root\surdâˆš âˆšsquare-root\surdâˆš âˆšsquare-root\surdâˆš vertex weights âˆšsquare-root\surdâˆš âˆšsquare-root\surdâˆš âˆšsquare-root\surdâˆš hyperedges Ã—\timesÃ— âˆšsquare-root\surdâˆš âˆšsquare-root\surdâˆš edge-dependent vertex weights Ã—\timesÃ— Ã—\timesÃ— âˆšsquare-root\surdâˆš Figure 1: Undirected graphs âŠ‚\subsetâŠ‚ EIVW hypergraphs âŠ‚\subsetâŠ‚ EDVW hypergraphs. Each undirected graph can be reformulated to EIVW hypergraph by regarding each pair-wise edge as a hyperedge; each EIVW hypergraph can be reformulated to EDVW hypergraph by setting each vertexâ€™s weight to be the same across hyperedges, yet allowing different vertices to have different weights. In nature, â€EDVW hypergraphâ€ is not a special case of hypergraphs, but a more generalized way to model hypergraphs (Figure 1). Any algorithm designed for EDVW hypergraphs, taking EDVW inputs, also works on typical (EIVW) hypergraphs by setting all the EDVW to 1 (i.e., independent w.r.t. edge). In other words, the properties and algorithms on EDVW-formulated hypergraphs can be applied to most hypergraphs. In this paper, we focus on further developing the incomplete yet fundamental spectral theories for EDVW hypergraphs, with a straightforward application on spectral clustering, a.k.a., k-way global partitioning, where typically k=2ğ‘˜2k=2italic_k = 2. To be specific, k-way global partitioning aims to partition an entire graph into kğ‘˜kitalic_k clusters, where the vertices in one cluster are densely connected within this cluster while having sparser connections to vertices outside this cluster. On the one hand, although the spectral theories and spectral clustering on graphs have been well studied [10], converting the hypergraphs to graphs and applying those methods may ignore the higher-order relations and result in sub-optimal results [44]. On the other hand, despite the advantage of EDVW modeling in terms of utilizing high-order relations, directly developing a spectral clustering algorithm on EDVW hypergraphs is still an open question. To this end, for the first time, we propose a provably linearly optimal spectral clustering algorithm on EDVW hypergraphs, together with theoretical analysis concerning the Rayleigh Quotient, Normalized Cut (i.e., NCut), and conductance. In the context of EDVW hypergraphs, we bridge the eigensystem of Laplacian with the NCut value through our proposed Rayleigh Quotient. The proposed algorithm can also be applied to EIVW hypergraphs by setting all the vertex weights to 1, thus works generally for all hypergraphs. 1.1 Main Results In this paper, we further develop the spectral hypergraph theory for EDVW hypergraphs, and then study global partitioning on EDVW hypergraphs. Theorem 1. (algebraic connections between hypergraph NCut, Rayleigh Quotient and Laplacian) Given any hypergraph in the EDVW formatting â„‹=(ğ’±,â„°,Ï‰,Î³)â„‹ğ’±â„°ğœ”ğ›¾\mathcal{H}=(\mathcal{V},\mathcal{E},\omega,\gamma)caligraphic_H = ( caligraphic_V , caligraphic_E , italic_Ï‰ , italic_Î³ ) with positive edge weights Ï‰â¢(â‹…)>0ğœ”â‹…0\omega(\cdot)>0italic_Ï‰ ( â‹… ) > 0 and non-negative edge-dependent vertex weights Î³eâ¢(â‹…)subscriptğ›¾ğ‘’â‹…\gamma_{e}(\cdot)italic_Î³ start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ( â‹… ) for any eâˆˆâ„°ğ‘’â„°e\in\mathcal{E}italic_e âˆˆ caligraphic_E, define Normalized Cut Nâ¢Câ¢uâ¢tâ¢(â‹…)ğ‘ğ¶ğ‘¢ğ‘¡â‹…NCut(\cdot)italic_N italic_C italic_u italic_t ( â‹… ), Volume of a vertex set vâ¢oâ¢lâ¢(â‹…)ğ‘£ğ‘œğ‘™â‹…vol(\cdot)italic_v italic_o italic_l ( â‹… ), Rayleigh Quotient Râ¢(â‹…)ğ‘…â‹…R(\cdot)italic_R ( â‹… ), Laplacian Lğ¿Litalic_L, and stationary distribution matrix Î Î \Piroman_Î  as Definition 10, 8, 5, 13, and 4. For any vertex set ğ’®âŠ†ğ’±ğ’®ğ’±\mathcal{S}\subseteq\mathcal{V}caligraphic_S âŠ† caligraphic_V, we define a |ğ’±|ğ’±|\mathcal{V}|| caligraphic_V |-dimensional vector xğ‘¥xitalic_x such that, xâ¢(u)=vâ¢oâ¢lâ¢(ğ’®Â¯)vâ¢oâ¢lâ¢(ğ’®),âˆ€uâˆˆğ’®,xâ¢(uÂ¯)=âˆ’vâ¢oâ¢lâ¢(ğ’®)vâ¢oâ¢lâ¢(ğ’®Â¯),âˆ€uÂ¯âˆˆğ’®Â¯.formulae-sequenceğ‘¥ğ‘¢ğ‘£ğ‘œğ‘™Â¯ğ’®ğ‘£ğ‘œğ‘™ğ’®formulae-sequencefor-allğ‘¢ğ’®formulae-sequenceğ‘¥Â¯ğ‘¢ğ‘£ğ‘œğ‘™ğ’®ğ‘£ğ‘œğ‘™Â¯ğ’®for-allÂ¯ğ‘¢Â¯ğ’®\begin{split}x(u)&=\sqrt{\frac{vol(\bar{\mathcal{S}})}{vol(\mathcal{S})}},% \leavevmode\nobreak\ \forall\,\,u\in\mathcal{S},\\ x(\bar{u})&=-\sqrt{\frac{vol(\mathcal{S})}{vol(\bar{\mathcal{S}})}},% \leavevmode\nobreak\ \forall\,\,\bar{u}\in\bar{\mathcal{S}}.\\ \end{split}start_ROW start_CELL italic_x ( italic_u ) end_CELL start_CELL = square-root start_ARG divide start_ARG italic_v italic_o italic_l ( overÂ¯ start_ARG caligraphic_S end_ARG ) end_ARG start_ARG italic_v italic_o italic_l ( caligraphic_S ) end_ARG end_ARG , âˆ€ italic_u âˆˆ caligraphic_S , end_CELL end_ROW start_ROW start_CELL italic_x ( overÂ¯ start_ARG italic_u end_ARG ) end_CELL start_CELL = - square-root start_ARG divide start_ARG italic_v italic_o italic_l ( caligraphic_S ) end_ARG start_ARG italic_v italic_o italic_l ( overÂ¯ start_ARG caligraphic_S end_ARG ) end_ARG end_ARG , âˆ€ overÂ¯ start_ARG italic_u end_ARG âˆˆ overÂ¯ start_ARG caligraphic_S end_ARG . end_CELL end_ROW (1) then,â¢Nâ¢Câ¢uâ¢tâ¢(ğ’®,ğ’®Â¯)=12â¢Râ¢(x)=xTâ¢Lâ¢xxTâ¢Î â¢xthen,ğ‘ğ¶ğ‘¢ğ‘¡ğ’®Â¯ğ’®12ğ‘…ğ‘¥superscriptğ‘¥ğ‘‡ğ¿ğ‘¥superscriptğ‘¥ğ‘‡Î ğ‘¥\textit{then,}\,\,NCut(\mathcal{S},\bar{\mathcal{S}})=\frac{1}{2}R(x)=\frac{x^% {T}Lx}{x^{T}\Pi x}then, italic_N italic_C italic_u italic_t ( caligraphic_S , overÂ¯ start_ARG caligraphic_S end_ARG ) = divide start_ARG 1 end_ARG start_ARG 2 end_ARG italic_R ( italic_x ) = divide start_ARG italic_x start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_L italic_x end_ARG start_ARG italic_x start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT roman_Î  italic_x end_ARG (2) This is the first work regarding the Rayleigh Quotient on hypergraphs. Inspired by this Theorem, we develop a spectral clustering algorithm HyperClus-G to optimize the NCut value by loosing the combinatorial optimization constraint. Theorem 2. (Hypergraph Spectral Clustering Algorithm) There exists a algorithm for hypergraph spectral clustering that can be applied to EDVW-formatted hypergraphs, and always returns approximately linearly optimal clustering in terms of Normalized Cut and conductance. In other words, approximately, the NCut of the returned partition ğ’©ğ’©\mathcal{N}caligraphic_N and the optimal NCut of any partition ğ’©âˆ—superscriptğ’©\mathcal{N}^{*}caligraphic_N start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT satisfy ğ’©â‰¤Oâ¢(ğ’©âˆ—)ğ’©ğ‘‚superscriptğ’©\mathcal{N}\leq O(\mathcal{N}^{*})caligraphic_N â‰¤ italic_O ( caligraphic_N start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ). We name this algorithm as HyperClus-G. The pseudo code of HyperClus-G is given in Algorithm 1. Moreover, to extend the hypergraph spectral theory, for the first time we give a complete proof regarding the hypergraph Cheeger Inequality. In the mean time, by proving Theorem 3, the previous result on hypergraph Cheeger Inequality (Theorem 5.1 in [7]) is incorrect as it refers to the eigenvector of unnormalized hypergraph Laplacian. Theorem 3. (Hypergraph Cheeger Inequality) Let â„‹=(ğ’±,â„°,Ï‰,Î³)â„‹ğ’±â„°ğœ”ğ›¾\mathcal{H}=(\mathcal{V},\mathcal{E},\omega,\gamma)caligraphic_H = ( caligraphic_V , caligraphic_E , italic_Ï‰ , italic_Î³ ) be any hypergraph in the EDVW formatting with positive edge weights Ï‰â¢(â‹…)>0ğœ”â‹…0\omega(\cdot)>0italic_Ï‰ ( â‹… ) > 0 and non-negative edge-dependent vertex weights Î³eâ¢(â‹…)subscriptğ›¾ğ‘’â‹…\gamma_{e}(\cdot)italic_Î³ start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ( â‹… ) for any eâˆˆâ„°ğ‘’â„°e\in\mathcal{E}italic_e âˆˆ caligraphic_E. Define Î¦â¢(â„‹)=minğ’®âŠ†ğ’±Î¦â¢(ğ’®)Î¦â„‹subscriptğ’®ğ’±Î¦ğ’®\Phi(\mathcal{H})=\mathop{\min}_{\mathcal{S}\subseteq\mathcal{V}}{\Phi(% \mathcal{S})}roman_Î¦ ( caligraphic_H ) = roman_min start_POSTSUBSCRIPT caligraphic_S âŠ† caligraphic_V end_POSTSUBSCRIPT roman_Î¦ ( caligraphic_S ). Then the second smallest eigenvector Î»ğœ†\lambdaitalic_Î» of the normalized hypergraph Laplacian Î âˆ’12â¢Lâ¢Î âˆ’12superscriptÎ 12ğ¿superscriptÎ 12\Pi^{-\frac{1}{2}}L\Pi^{-\frac{1}{2}}roman_Î  start_POSTSUPERSCRIPT - divide start_ARG 1 end_ARG start_ARG 2 end_ARG end_POSTSUPERSCRIPT italic_L roman_Î  start_POSTSUPERSCRIPT - divide start_ARG 1 end_ARG start_ARG 2 end_ARG end_POSTSUPERSCRIPT satisfies Î¦â¢(â„‹)22â‰¤Î»â‰¤2â¢Î¦â¢(â„‹)Î¦superscriptâ„‹22ğœ†2Î¦â„‹\frac{\Phi(\mathcal{H})^{2}}{2}\leq\lambda\leq 2\Phi(\mathcal{H})divide start_ARG roman_Î¦ ( caligraphic_H ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG 2 end_ARG â‰¤ italic_Î» â‰¤ 2 roman_Î¦ ( caligraphic_H ) (3) In fact, this theorem shows that our HyperClus-G is also approximately linearly optimal in terms of conductance. In other words, the conductance of the returned cluster Î¦Î¦\Phiroman_Î¦ and the optimal conductance Î¦âˆ—superscriptÎ¦\Phi^{*}roman_Î¦ start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT satisfy Î¦â‰¤Oâ¢(Î¦âˆ—)Î¦ğ‘‚superscriptÎ¦\Phi\leq O(\Phi^{*})roman_Î¦ â‰¤ italic_O ( roman_Î¦ start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ). It is worth mentioning that the previous non-proved result in [7] regarding hypergraph Cheeger Inequality now can be proved by using normalized Laplacian instead of the conjecture of combinatorial Laplacian. Technical Overview. Given the EDVW modeling, the relevant algorithmic development still remains in a nascent stage, which hinders the application of hypergraphs in many real-world scenarios. To this end, we first re-analyze the random walks on EDVW hypergraphs, then propose the HyperClus-G for hypergraph partitioning. Finally, we prove the approximation of normalized cut, as well as the upper bound of NCut and conductance. The key insight from the previous work [7] is to model the hypergraphs similar to directed graphs through the equivalence of random walks. Unlike classical graph theory, such directed graphs are edge-weighted, node-weighted, and contain self-loops. In this work, inspired by the definitions of Rayleigh Quotient, NCut, boundary/cut, volume, and conductance in graphs, we develop these definitions in the context of EDVW hypergraphs. We show that Theorem 1 and Theorem 3, properties that hold for graphs, still hold for hypergraphs using our unified definitions. From Theorem 3, we can further prove that our proposed HyperClus-G is approximately linearly optimal in terms of both NCut and conductance. Our Appendix contains supplementary contents, such as trivial proofs and experimental details. Paper Organization. This paper is organized as follows. In Section 2, we introduce necessary notations and our definitions regarding hypergraphs. In Section 3, we introduce our definition of hypergraph Rayleigh Quotient and show its connection with the Laplacian and NCut. Then, we propose our HyperClus-G inspired from such connection. In section 4, we give complete proof regarding hypergraph Cheeger Inequality, then show the linear optimality of our HyperClus-G in terms of both NCut and conductance. In Section 5, we analyze the complexity of our algorithms. Finally, in Section 6, we prepare comprehensive experiments to validate our theoretical findings. 1.2 Other Related Works Early Hypergraphs [6] typically model graph structures and do not allow node or hyperedge weights. Later formulations [52] start to allow hyperedge weights. Among various choices of modeling hypergraphs [6, 52, 15, 33], EDVW modeling [7] shows great generalization because it allows both hyperedge weights and node weights. Many hypergraph-related techniques have been proposed [17, 3, 30], while very few of them involve EDVW hypergraphs. Inhomogeneous hypergraph partitioning was proposed in [32]. Later on, authors of [33] proposed submodular hypergraphs, a special group of weighted hypergraphs, and analyzed their spectral clustering. A recent work [23] demonstrates how random walks with EDVW are used to construct the EDVW hypergraph Laplacian. However, it stops at the construction of the Laplacian and only uses partial information encoded in the Laplacian for clustering. Some recent works [2, 43, 11] study partitioning edge-colored hypergraphs. There are several research works [57, 54, 55] targeting EDVW hypergraph global partitioning, but they do not actually directly work on EDVW hypergraphs, but are based on submodular hypergraphs; They propose to construct a submodular hypergraph from the given EDVW hypergraph [33], then apply learning-based approaches to optimize the global partitioning objective. Several works also study specific applications of hypergraph clustering [28, 5]."
https://arxiv.org/html/2411.03299v1,Concurrent Composition for Continual Mechanisms,"A series of recent works by Lyu, Wang, Vadhan, and Zhang (TCC â€˜21, NeurIPS â€˜22, STOC â€˜23) showed that composition theorems for non-interactive differentially private mechanisms extend to the concurrent composition of interactive differentially private mechanism, when differential privacy is measured using fğ‘“fitalic_f-DP and the adversary is adaptive. We extend their work to the continual observation setting, where the data is arriving online in a potentially adaptive manner. More specifically, we show that all composition theorems for non-interactive differentially private mechanisms extend to the concurrent composition of continual differentially private mechanism, where the adversary is adaptive. We show this result for fğ‘“fitalic_f-DP, which also implies the result for pure DP and (Ïµ,Î´)italic-Ïµğ›¿(\epsilon,\delta)( italic_Ïµ , italic_Î´ )-DP.","Differential privacy [7] is a popular measure of the privacy protection offered by an algorithm that performs statistical analysis on a sensitive dataset about individuals. While differential private mechanisms for the setting where the dataset is static (i.e., the batch setting) are well studied for a wide variety of problems, the setting where the dataset changes dynamically, i.e., where questions about the dataset and updates of the dataset are arbitrarily interleaved, has only recently received more attention. This setting was introduced in 2010 by Dwork, Naor, Pitassi, and Rothblum [8] and the corresponding privacy definition was called differential privacy under continual observation. These mechanisms are data structures (as they can (usually) process an arbitrary number of queries and updates) that are differentially private under continual observation. We call them continual mechanisms for short below. In recent years, mechanisms that are differential private under continual observation have been developed and analyzed for summing a sequence of (binary) numbers [8, 4, 9, 12, 1], weighted sums [2, 16], histograms and histogram-based queries [3, 14, 10], set cardinality [17, 15], various graph properties [6, 11, 12, 18], and clustering points in Euclidean space [20]. Some of these works are performed by reduction to another continual observation problem. That is, the continual mechanism â„³â„³\mathcal{M}caligraphic_M for the new problem uses a continual mechanism â„³â€²superscriptâ„³â€²\mathcal{M}^{\prime}caligraphic_M start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT for a previously studied problem mediate all of its access to the dataset. Thus, all steps taken by the mechanism can be seen as post-processing the output of â„³â€²superscriptâ„³â€²\mathcal{M}^{\prime}caligraphic_M start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT, leading immediately to the guarantee that â„³â„³\mathcal{M}caligraphic_M is differentially private under continual observation. However, for some of the above works (see e.g., [9, 14]), multiple continual mechanisms are used, i.e., â„³â„³\mathcal{M}caligraphic_M is interacting with multiple continual mechanisms Mâ€²,Mâ€²â€²,â€¦superscriptğ‘€â€²superscriptğ‘€â€²â€²â€¦M^{\prime},M^{\prime\prime},\dotsitalic_M start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT , italic_M start_POSTSUPERSCRIPT â€² â€² end_POSTSUPERSCRIPT , â€¦ in an arbitrary way. Thus, to guarantee privacy for â„³â„³\mathcal{M}caligraphic_M, the concurrent composition of the involved continual mechanisms needs to be analyzed. A series of recent works by Lyu, Wang, Vadhan, and Zhang ([21], [19], [21], [13]) analyzed the concurrent composition of interactive mechanisms in the setting where the dataset is static and unchanging (but there can be adaptive queries answered in a potentially stateful manner.) More specifically, they showed that all composition theorems for non-interactive differentially private mechanisms extend to the concurrent composition of interactive differentially private mechanism. The privacy definition used in their work encompasses fğ‘“fitalic_f-differential privacy [5], (Ïµ,Î´)italic-Ïµğ›¿(\epsilon,\delta)( italic_Ïµ , italic_Î´ )-differential privacy, and pure differential privacy. This paper analyzes the concurrent composition of continual mechanisms, where there are updates as well as queries, and shows the corresponding result: Composition theorems for non-interactive differentially private mechanisms extend to the concurrent composition of continual differentially private mechanism. As our theorem is based on their result, it applies to the same privacy definitions."
https://arxiv.org/html/2411.03227v1,Tight Sampling Bounds for Eigenvalue Approximation,"We consider the problem of estimating the spectrum of a symmetric bounded entry (not necessarily PSD) matrix via entrywise sampling. This problem was introduced by [Bhattacharjee, Dexter, Drineas, Musco, Ray â€™22], where it was shown that one can obtain an Ïµâ¢nitalic-Ïµğ‘›\epsilon nitalic_Ïµ italic_n additive approximation to all eigenvalues of Ağ´Aitalic_A by sampling a principal submatrix of dimension polyâ¢(logâ¡n)Ïµ3polyğ‘›superscriptitalic-Ïµ3\frac{\text{poly}(\log n)}{\epsilon^{3}}divide start_ARG poly ( roman_log italic_n ) end_ARG start_ARG italic_Ïµ start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT end_ARG. We improve their analysis by showing that it suffices to sample a principal submatrix of dimension O~â¢(1Ïµ2)~ğ‘‚1superscriptitalic-Ïµ2\tilde{O}(\frac{1}{\epsilon^{2}})over~ start_ARG italic_O end_ARG ( divide start_ARG 1 end_ARG start_ARG italic_Ïµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ) (with no dependence on nğ‘›nitalic_n). This matches known lower bounds and therefore resolves the sample complexity of this problem up to logâ¡1Ïµ1italic-Ïµ\log\frac{1}{\epsilon}roman_log divide start_ARG 1 end_ARG start_ARG italic_Ïµ end_ARG factors. Using similar techniques, we give a tight O~â¢(1Ïµ2)~ğ‘‚1superscriptitalic-Ïµ2\tilde{O}(\frac{1}{\epsilon^{2}})over~ start_ARG italic_O end_ARG ( divide start_ARG 1 end_ARG start_ARG italic_Ïµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ) bound for obtaining an additive Ïµâ¢â€–Aâ€–Fitalic-Ïµsubscriptnormğ´ğ¹\epsilon\|A\|_{F}italic_Ïµ âˆ¥ italic_A âˆ¥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT approximation to the spectrum of Ağ´Aitalic_A via squared row-norm sampling, improving on the previous best O~â¢(1Ïµ8)~ğ‘‚1superscriptitalic-Ïµ8\tilde{O}(\frac{1}{\epsilon^{8}})over~ start_ARG italic_O end_ARG ( divide start_ARG 1 end_ARG start_ARG italic_Ïµ start_POSTSUPERSCRIPT 8 end_POSTSUPERSCRIPT end_ARG ) bound. We also address the problem of approximating the top eigenvector for a bounded entry, PSD matrix A.ğ´A.italic_A . In particular, we show that sampling Oâ¢(1Ïµ)ğ‘‚1italic-ÏµO(\frac{1}{\epsilon})italic_O ( divide start_ARG 1 end_ARG start_ARG italic_Ïµ end_ARG ) columns of Ağ´Aitalic_A suffices to produce a unit vector uğ‘¢uitalic_u with uTâ¢Aâ¢uâ‰¥Î»1â¢(A)âˆ’Ïµâ¢nsuperscriptğ‘¢ğ‘‡ğ´ğ‘¢subscriptğœ†1ğ´italic-Ïµğ‘›u^{T}Au\geq\lambda_{1}(A)-\epsilon nitalic_u start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_A italic_u â‰¥ italic_Î» start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_A ) - italic_Ïµ italic_n. This matches what one could achieve via the sampling bound of [Musco, Muscoâ€™17] for the special case of approximating the top eigenvector, but does not require adaptivity.As additional applications, we observe that our sampling results can be used to design a faster eigenvalue estimation sketch for dense matrices resolving a question of [Swartworth, Woodruffâ€™23], and can also be combined with [Musco, Muscoâ€™17] to achieve Oâ¢(1/Ïµ3)ğ‘‚1superscriptitalic-Ïµ3O(1/\epsilon^{3})italic_O ( 1 / italic_Ïµ start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ) (adaptive) sample complexity for approximating the spectrum of a bounded entry PSD matrix to Ïµâ¢nitalic-Ïµğ‘›\epsilon nitalic_Ïµ italic_n additive error.","Computing the spectrum of a matrix is a fundamental problem with many applications. There are well-known high-precision algorithms that run in polynomial time [francis1962qr, golub2000eigenvalue], although any such algorithm is necessarily at least linear time in the input size. As data grows larger, even linear algorithms can be prohibitive. This has motivated a flurry of activity studying sublinear time estimation of problems in numerical linear algebra, for instance for low-rank approximation [musco2017recursive, musco2017sublinear, bakshi2018sublinear, bakshi2020robust], kernel density estimation [charikar2017hashing, siminelakis2019rehashing, charikar2020kernel], testing positive-semidefiniteness [bakshi2020testing], and matrix sparsification [bhattacharjee2023universal, drineas2011note]. For eigenvalue estimation, variants of the power method have long been known to give good approximations to the top eigenvalues and eigenvectors of Aâˆˆâ„nÃ—nğ´superscriptâ„ğ‘›ğ‘›A\in\mathbb{R}^{n\times n}italic_A âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n Ã— italic_n end_POSTSUPERSCRIPT while revealing sublinear information about Ağ´Aitalic_A, i.e., using oâ¢(n)ğ‘œğ‘›o(n)italic_o ( italic_n ) matrix-vector queries [rokhlin2010randomized, musco2015randomized]. However it was only recently asked in [bhattacharjee2024sublinear] whether there are spectral approximation algorithms for symmetric, but non-PSD matrices that run in sublinear time in the entry query model. This is perhaps the most natural model if one imagines having an extremely large matrix saved on disk for example. This may be in the form of a graph for instance, where one could be interested in obtaining spectral information about its Laplacian or adjacency matrix. One could also imagine having a large collection of data points with some kernel function that can be computed for pairs of points. Obtaining a rough spectral summary of the associated kernel matrix is a natural step for data analysis, for instance, to spot low-rank structure in the data. If data points are large or expensive to collect, or if kernel evaluation is expensive, it is natural to aim for minimizing entry queries to the kernel matrix. Of course, it is not reasonable to ask for sublinear time spectral approximation algorithms, without some additional assumptions. For instance, our matrix Ağ´Aitalic_A could contain all zeros but with a single large entry at indices (i,j)ğ‘–ğ‘—(i,j)( italic_i , italic_j ) and (j,i).ğ‘—ğ‘–(j,i).( italic_j , italic_i ) . Given only the ability to query entries, and no additional information, even distinguishing Ağ´Aitalic_A from the all zeros matrix would take Î©â¢(n2)Î©superscriptğ‘›2\Omega(n^{2})roman_Î© ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) queries. We consider two assumptions that allow for improved guarantees. The first is an assumption on the structure of Ağ´Aitalic_A called the bounded entry model, which assumes that Ağ´Aitalic_A has entries bounded by 1111 in magnitude. This condition was introduced in [balcan2019testing] and studied further by [bakshi2020testing] who showed that it was sufficient in order to test for positive semi-definiteness with sublinear entry queries. Motivated by this result, [bhattacharjee2024sublinear] showed that all eigenvalues of a symmetric bounded-entry matrix can be approximated using a sublinear number of queries, simply by sampling a polyâ¡(logâ¡nÏµ)polyğ‘›italic-Ïµ\operatorname{poly}(\frac{\log n}{\epsilon})roman_poly ( divide start_ARG roman_log italic_n end_ARG start_ARG italic_Ïµ end_ARG ) sized submatrix. Another way of getting sublinear sample complexity is to give the sampler additional power. In our case, as in [bhattacharjee2024sublinear], we consider having access to a sampler that can produce a row index with probability proportional to its squared row norm. Such samplers have been increasingly studied under the guise of â€œquantum-inspiredâ€ machine-learning algorithms [tang2019quantum, chepurko2022quantum, gilyen2018quantum, gilyen2022improved]. Such samplers are practical to maintain when Ağ´Aitalic_A is stored entrywise. For example by using an appropriate data structure, they can be built in nnzâ¡(A)nnzğ´\operatorname{nnz}(A)roman_nnz ( italic_A ) time, admit Oâ¢(logâ¡n)ğ‘‚ğ‘›O(\log n)italic_O ( roman_log italic_n ) time sampling, and can handle entry updates in Oâ¢(logâ¡n)ğ‘‚ğ‘›O(\log n)italic_O ( roman_log italic_n ) time. [bhattacharjee2024sublinear] showed that given a real symmetric matrix Ağ´Aitalic_A with entries bounded by 1111, one can sample a principal submatrix of Ağ´Aitalic_A of dimensions Oâ¢(polyâ¡logâ¡nÏµ3)Ã—Oâ¢(polyâ¡logâ¡nÏµ3)ğ‘‚polyğ‘›superscriptitalic-Ïµ3ğ‘‚polyğ‘›superscriptitalic-Ïµ3O(\frac{\operatorname{poly}\log n}{\epsilon^{3}})\times O(\frac{\operatorname{% poly}\log n}{\epsilon^{3}})italic_O ( divide start_ARG roman_poly roman_log italic_n end_ARG start_ARG italic_Ïµ start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT end_ARG ) Ã— italic_O ( divide start_ARG roman_poly roman_log italic_n end_ARG start_ARG italic_Ïµ start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT end_ARG ) and then output an additive Ïµâ¢nitalic-Ïµğ‘›\epsilon nitalic_Ïµ italic_n approximation to the entire spectrum, i.e., to all eigenvalues of A.ğ´A.italic_A . On the other hand, the best lower bound states that a principal-submatrix algorithm must sample at least Oâ¢(1Ïµ4)ğ‘‚1superscriptitalic-Ïµ4O(\frac{1}{\epsilon^{4}})italic_O ( divide start_ARG 1 end_ARG start_ARG italic_Ïµ start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT end_ARG ) entries of A.ğ´A.italic_A . There are two ways that one could hope to improve the sampling bound of [bhattacharjee2024sublinear]. First one could hope to improve the Ïµitalic-Ïµ\epsilonitalic_Ïµ dependence in the dimension from Oâ¢(1/Ïµ3)ğ‘‚1superscriptitalic-Ïµ3O(1/\epsilon^{3})italic_O ( 1 / italic_Ïµ start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ) to Oâ¢(1/Ïµ2).ğ‘‚1superscriptitalic-Ïµ2O(1/\epsilon^{2}).italic_O ( 1 / italic_Ïµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) . Several prior results suggested that this might be possible. For example [bhattacharjee2024sublinear] showed that their Ïµitalic-Ïµ\epsilonitalic_Ïµ dependence could be improved both when Ağ´Aitalic_A is PSD, and when the spectrum of Ağ´Aitalic_A is flat. Concurrently [swartworth2023optimal] showed that one can obtain an Ïµâ¢â€–Aâ€–Fitalic-Ïµsubscriptnormğ´ğ¹\epsilon\left\|A\right\|_{F}italic_Ïµ âˆ¥ italic_A âˆ¥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT additive approximation to the spectrum of Ağ´Aitalic_A by using a so-called bilinear sketch of Ağ´Aitalic_A of dimensions Oâ¢(1/Ïµ2)Ã—Oâ¢(1/Ïµ2).ğ‘‚1superscriptitalic-Ïµ2ğ‘‚1superscriptitalic-Ïµ2O(1/\epsilon^{2})\times O(1/\epsilon^{2}).italic_O ( 1 / italic_Ïµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) Ã— italic_O ( 1 / italic_Ïµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) . Such a sketch would give Ïµâ¢nitalic-Ïµğ‘›\epsilon nitalic_Ïµ italic_n additive error for approximating all eigenvalues when Ağ´Aitalic_A has bounded entries. Unfortunately this sketch is Gaussian, and it seems difficult to directly adapt its analysis to obtain a sampling bound instead. In this paper we close the gap between sketching and sampling for bounded entry matrices by showing that uniformly sampling an O~â¢(1/Ïµ2)Ã—O~â¢(1/Ïµ2)~ğ‘‚1superscriptitalic-Ïµ2~ğ‘‚1superscriptitalic-Ïµ2\tilde{O}(1/\epsilon^{2})\times\tilde{O}(1/\epsilon^{2})over~ start_ARG italic_O end_ARG ( 1 / italic_Ïµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) Ã— over~ start_ARG italic_O end_ARG ( 1 / italic_Ïµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) principal submatrix of Ağ´Aitalic_A suffices to approximate all eigenvalues of Ağ´Aitalic_A up to Ïµâ¢nitalic-Ïµğ‘›\epsilon nitalic_Ïµ italic_n additive error, even when Ağ´Aitalic_A is not necessarily PSD. In addition to obtaining an optimal Ïµitalic-Ïµ\epsilonitalic_Ïµ-dependence, we note that our uniform sampling bound contains no dependence on n.ğ‘›n.italic_n . We also address the squared-row norm sampling model. Here we improve the analysis of [bhattacharjee2024sublinear] to show that it suffices to query a principal submatrix of size O~â¢(polyâ¡logâ¡nÏµ2)~ğ‘‚polyğ‘›superscriptitalic-Ïµ2\tilde{O}(\frac{\operatorname{poly}\log n}{\epsilon^{2}})over~ start_ARG italic_O end_ARG ( divide start_ARG roman_poly roman_log italic_n end_ARG start_ARG italic_Ïµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ), compared to the O~â¢(polyâ¡logâ¡nÏµ8)~ğ‘‚polyğ‘›superscriptitalic-Ïµ8\tilde{O}(\frac{\operatorname{poly}\log n}{\epsilon^{8}})over~ start_ARG italic_O end_ARG ( divide start_ARG roman_poly roman_log italic_n end_ARG start_ARG italic_Ïµ start_POSTSUPERSCRIPT 8 end_POSTSUPERSCRIPT end_ARG ) dimensional principal submatrix required by [bhattacharjee2024sublinear]. Our approximation model. We note that all of the guarantees considered in our work and prior work focus on additive approximations to the spectrum. Ideally, one might like to aim for a relative error guarantee. However as pointed out, by [bhattacharjee2024sublinear] for example, this is not possible for entry queries. Such an algorithm would be able to distinguish the 00 matrix from a matrix with a single off-diagonal pair of nonzero entries, which clearly requires Î©â¢(n2)Î©superscriptğ‘›2\Omega(n^{2})roman_Î© ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) samples. Indeed even with squared row-norm sampling, relative error is still too much to hope for. In fact, even for sketches, approximating the top eigenvalue to within a constant factor requires Î©â¢(n2)Î©superscriptğ‘›2\Omega(n^{2})roman_Î© ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) sketching dimension in general [li2016tight, woodruff2014sketching]. One can always turn such a sketching lower bound into a sampling bound, even allowing for row-norm sampling - simply conjugate by a random orthogonal matrix to flatten all rows. Then squared row sampling is effectively uniform, and so a sampling algorithm could be used to construct a sketch of the same dimensions. These existing lower bounds are why we (as well as prior work) choose to focus on an additive approximation guarantee."
https://arxiv.org/html/2411.03121v1,"Fully Dynamickğ‘˜kitalic_k-Median with
Near-OptimalUpdate Time and Recourse","In metric kğ‘˜kitalic_k-clustering, we are given as input a set of nğ‘›nitalic_n points in a general metric space, and we have to pick kğ‘˜kitalic_k centers and cluster the input points around these chosen centers, so as to minimize an appropriate objective function. In recent years, significant effort has been devoted to the study of metric kğ‘˜kitalic_k-clustering problems in a dynamic setting, where the input keeps changing via updates (point insertions/deletions), and we have to maintain a good clustering throughout these updates [Fichtenberger, Lattanzi, Norouzi-Fard and Svensson, SODAâ€™21; Bateni, Esfandiari, Fichtenberger, Henzinger, Jayaram, Mirrokni and Weise, SODAâ€™23; Lacki, Haeupler, Grunau, Rozhon and Jayaram, SODAâ€™24; Bhattacharya, Costa, Garg, Lattanzi and Parotsidis, FOCSâ€™24; Forster and Skarlatos, SODAâ€™25]. The performance of such a dynamic algorithm is measured in terms of three parameters: (i) Approximation ratio, which signifies the quality of the maintained solution, (ii) Recourse, which signifies how stable the maintained solution is, and (iii) Update time, which signifies the efficiency of the algorithm.We consider a textbook metric kğ‘˜kitalic_k-clustering problem, metric kğ‘˜kitalic_k-median, where the objective is the sum of the distances of the points to their nearest centers. We design the first dynamic algorithm for this problem with near-optimal guarantees across all three performance measures (up to a constant factor in approximation ratio, and polylogarithmic factors in recourse and update time). Specifically, we obtain a Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 )-approximation algorithm for dynamic metric kğ‘˜kitalic_k-median with O~â¢(1)~ğ‘‚1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) recourse and O~â¢(k)~ğ‘‚ğ‘˜\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) update time. Prior to our work, the state-of-the-art here was the recent result of [Bhattacharya, Costa, Garg, Lattanzi and Parotsidis, FOCSâ€™24], who obtained Oâ¢(Ïµâˆ’1)ğ‘‚superscriptitalic-Ïµ1O(\epsilon^{-1})italic_O ( italic_Ïµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT )-approximation ratio with O~â¢(kÏµ)~ğ‘‚superscriptğ‘˜italic-Ïµ\tilde{O}(k^{\epsilon})over~ start_ARG italic_O end_ARG ( italic_k start_POSTSUPERSCRIPT italic_Ïµ end_POSTSUPERSCRIPT ) recourse and O~â¢(k1+Ïµ)~ğ‘‚superscriptğ‘˜1italic-Ïµ\tilde{O}(k^{1+\epsilon})over~ start_ARG italic_O end_ARG ( italic_k start_POSTSUPERSCRIPT 1 + italic_Ïµ end_POSTSUPERSCRIPT ) update time.We achieve our results by carefully synthesizing the concept of robust centers introduced in [Fichtenberger, Lattanzi, Norouzi-Fard and Svensson, SODAâ€™21] along with the randomized local search subroutine from [Bhattacharya, Costa, Garg, Lattanzi and Parotsidis, FOCSâ€™24], in addition to several key technical insights of our own.","Part I Extended Abstract 1 Introduction Consider a metric space (ğ’«,d)ğ’«ğ‘‘(\mathcal{P},d)( caligraphic_P , italic_d ) over a set ğ’«ğ’«\mathcal{P}caligraphic_P of nğ‘›nitalic_n points, with a distance function d:ğ’«Ã—ğ’«â†’â„â‰¥0:ğ‘‘â†’ğ’«ğ’«superscriptâ„absent0d:\mathcal{P}\times\mathcal{P}\to\mathbb{R}^{\geq 0}italic_d : caligraphic_P Ã— caligraphic_P â†’ blackboard_R start_POSTSUPERSCRIPT â‰¥ 0 end_POSTSUPERSCRIPT, and a positive integer kâ‰¤nğ‘˜ğ‘›k\leq nitalic_k â‰¤ italic_n. In the metric kğ‘˜kitalic_k-median problem, we have to pick a set ğ’°âŠ†ğ’«ğ’°ğ’«\mathcal{U}\subseteq\mathcal{P}caligraphic_U âŠ† caligraphic_P of kğ‘˜kitalic_k centers, so as to minimize the objective function Costâ¢(ğ’°,ğ’«):=âˆ‘pâˆˆğ’«dâ¢(p,ğ’°)assignCostğ’°ğ’«subscriptğ‘ğ’«ğ‘‘ğ‘ğ’°\textnormal{{Cost}}\left(\mathcal{U},\mathcal{P}\right):=\sum_{p\in\mathcal{P}% }d(p,\mathcal{U})Cost ( caligraphic_U , caligraphic_P ) := âˆ‘ start_POSTSUBSCRIPT italic_p âˆˆ caligraphic_P end_POSTSUBSCRIPT italic_d ( italic_p , caligraphic_U ), where dâ¢(p,ğ’°):=minqâˆˆğ’°â¡dâ¢(p,q)assignğ‘‘ğ‘ğ’°subscriptğ‘ğ’°ğ‘‘ğ‘ğ‘d(p,\mathcal{U}):=\min_{q\in\mathcal{U}}d(p,q)italic_d ( italic_p , caligraphic_U ) := roman_min start_POSTSUBSCRIPT italic_q âˆˆ caligraphic_U end_POSTSUBSCRIPT italic_d ( italic_p , italic_q ) denotes the distance between a point pğ‘pitalic_p and its nearest center in ğ’°ğ’°\mathcal{U}caligraphic_U. We assume that we have access to the function dğ‘‘ditalic_d via a distance oracle, which returns the value of dâ¢(p,q)ğ‘‘ğ‘ğ‘d(p,q)italic_d ( italic_p , italic_q ) for any two points p,qâˆˆğ’«ğ‘ğ‘ğ’«p,q\in\mathcal{P}italic_p , italic_q âˆˆ caligraphic_P in Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 ) time. We further assume that 1â‰¤dâ¢(p,q)â‰¤Î”1ğ‘‘ğ‘ğ‘Î”1\leq d(p,q)\leq\Delta1 â‰¤ italic_d ( italic_p , italic_q ) â‰¤ roman_Î” for all p,qâˆˆğ’«,pâ‰ qformulae-sequenceğ‘ğ‘ğ’«ğ‘ğ‘p,q\in\mathcal{P},p\neq qitalic_p , italic_q âˆˆ caligraphic_P , italic_p â‰  italic_q, where Î”Î”\Deltaroman_Î” is an upper bound on the aspect ratio of the metric space. Metric kğ‘˜kitalic_k-median is a foundational problem in clustering, is known to be NP-hard, and approximation algorithms for this problem are taught in standard textbooks [WS11]. In particular, it has a Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 )-approximation algorithm that runs in O~â¢(kâ¢n)~ğ‘‚ğ‘˜ğ‘›\tilde{O}(kn)over~ start_ARG italic_O end_ARG ( italic_k italic_n ) time [MP02],111Throughout this paper, we use the O~â¢(â‹…)~ğ‘‚â‹…\tilde{O}(\cdot)over~ start_ARG italic_O end_ARG ( â‹… ) notation to hide polylogarithmic factors in k,nğ‘˜ğ‘›k,nitalic_k , italic_n and Î”Î”\Deltaroman_Î”. and it is known that we cannot have any Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 )-approximation algorithm for metric kğ‘˜kitalic_k-median with oâ¢(kâ¢n)ğ‘œğ‘˜ğ‘›o(kn)italic_o ( italic_k italic_n ) runtime [BCIS05]. In recent years, substantive effort have been devoted to the study of this problem in a dynamic setting, when the underlying input changes over time [LV17, CHP+19, FLNS21, HK20, BCLP23, DHS24, BCG+24]. To be more specific, here the input changes by a sequence of updates; each update inserts/deletes a point in ğ’«ğ’«\mathcal{P}caligraphic_P. Throughout these updates, we have to maintain a set of kğ‘˜kitalic_k centers ğ’°âŠ†ğ’«ğ’°ğ’«\mathcal{U}\subseteq\mathcal{P}caligraphic_U âŠ† caligraphic_P which form an approximate kğ‘˜kitalic_k-median solution to the current input ğ’«ğ’«\mathcal{P}caligraphic_P. Such a dynamic algorithmâ€™s performance is measured in terms of its: (i) Approximation ratio, (ii) Recourse, which is the number of changes (i.e., point insertions/deletions) in the maintained solution ğ’°ğ’°\mathcal{U}caligraphic_U per update, and (iii) Update time, which is the time taken by the algorithm to process an update. In a sense, approximation ratio and recourse respectively measures the â€œqualityâ€ and the â€œstabilityâ€ of the maintained solution, whereas update time measures the â€œefficiencyâ€ of the algorithm. We design a dynamic algorithm for this problem with almost optimal performance guarantees with respect to all these measures. Our main result is summarized in the theorem below. Theorem 1.1. There is a randomized dynamic algorithm for the metric kğ‘˜kitalic_k-median problem that has Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 )-approximation ratio, Oâ¢(log2â¡Î”)ğ‘‚superscript2Î”O(\log^{2}\Delta)italic_O ( roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT roman_Î” ) recourse and O~â¢(k)~ğ‘‚ğ‘˜\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) update time, w.h.p.222Both our recourse and update time bounds are amortized. Throughout the paper, we do not make any distinction between amortized vs worst-case bounds. Remarks. A few important remarks are in order. First, note that there cannot exist a dynamic Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 )-approximation algorithm for our problem with oâ¢(k)ğ‘œğ‘˜o(k)italic_o ( italic_k ) update time, for otherwise we would get a static algorithm for metric kğ‘˜kitalic_k-median with Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 )-approximation ratio and oâ¢(kâ¢n)ğ‘œğ‘˜ğ‘›o(kn)italic_o ( italic_k italic_n ) runtime: Simply let the dynamic algorithm handle a sequence of nğ‘›nitalic_n insertions corresponding to the points in the static input, and return the solution maintained by the dynamic algorithm at the end of this update sequence. This would contradict the Î©â¢(kâ¢n)Î©ğ‘˜ğ‘›\Omega(kn)roman_Î© ( italic_k italic_n ) lower bound on the runtime of any such static algorithm, derived in [BCIS05]. Furthermore, it is easy to verify that we cannot achieve oâ¢(1)ğ‘œ1o(1)italic_o ( 1 ) recourse in the fully dynamic setting, and hence, our dynamic algorithm is almost optimal (up to a Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 ) factor in approximation ratio and polylogarithmic factors in recourse and update time). Second, in this extended abstract we focus the unweighted metric kğ‘˜kitalic_k-median problem, only to ease notations. In the full version (see Part III), we show that Theorem 1.1 seamlessly extends to the weighted setting, where each point pâˆˆğ’«ğ‘ğ’«p\in\mathcal{P}italic_p âˆˆ caligraphic_P has a weight wâ¢(p)>0ğ‘¤ğ‘0w(p)>0italic_w ( italic_p ) > 0 associated with it, and we have to maintain a set ğ’°ğ’°\mathcal{U}caligraphic_U of kğ‘˜kitalic_k centers that (approximately) minimizes âˆ‘pâˆˆğ’«wâ¢(p)â‹…dâ¢(p,ğ’°)subscriptğ‘ğ’«â‹…ğ‘¤ğ‘ğ‘‘ğ‘ğ’°\sum_{p\in\mathcal{P}}w(p)\cdot d(p,\mathcal{U})âˆ‘ start_POSTSUBSCRIPT italic_p âˆˆ caligraphic_P end_POSTSUBSCRIPT italic_w ( italic_p ) â‹… italic_d ( italic_p , caligraphic_U ). Moreover, our result extends to the related metric kğ‘˜kitalic_k-means problem as well, where we have to pick a set ğ’°âŠ†ğ’«ğ’°ğ’«\mathcal{U}\subseteq\mathcal{P}caligraphic_U âŠ† caligraphic_P of kğ‘˜kitalic_k centers so as to minimize âˆ‘pâˆˆğ’«(dâ¢(p,ğ’°))2subscriptğ‘ğ’«superscriptğ‘‘ğ‘ğ’°2\sum_{p\in\mathcal{P}}\left(d(p,\mathcal{U})\right)^{2}âˆ‘ start_POSTSUBSCRIPT italic_p âˆˆ caligraphic_P end_POSTSUBSCRIPT ( italic_d ( italic_p , caligraphic_U ) ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. We can get a dynamic Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 )-approximation algorithm for (weighted) metric kğ‘˜kitalic_k-means that has O~â¢(1)~ğ‘‚1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) recourse and O~â¢(k)~ğ‘‚ğ‘˜\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) update time, w.h.p. Finally, Table 1 compares our result against prior state-of-the-art. Until very recently, all known algorithms [CHP+19, HK20, BCLP23] for fully dynamic metric kğ‘˜kitalic_k-median had a trivial recourse bound of Î©â¢(k)Î©ğ‘˜\Omega(k)roman_Î© ( italic_k ), which can be obtained by computing a new set of kğ‘˜kitalic_k centers from scratch after every update (at the expense of Î©â¢(kâ¢n)Î©ğ‘˜ğ‘›\Omega(kn)roman_Î© ( italic_k italic_n ) update time). Then, in FOCS 2024, [BCG+24] took a major step towards designing an almost optimal algorithm for this problem, by achieving Oâ¢(Ïµâˆ’1)ğ‘‚superscriptitalic-Ïµ1O(\epsilon^{-1})italic_O ( italic_Ïµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT )-approximation ratio, O~â¢(k1+Ïµ)~ğ‘‚superscriptğ‘˜1italic-Ïµ\tilde{O}(k^{1+\epsilon})over~ start_ARG italic_O end_ARG ( italic_k start_POSTSUPERSCRIPT 1 + italic_Ïµ end_POSTSUPERSCRIPT ) update time and O~â¢(kÏµ)~ğ‘‚superscriptğ‘˜italic-Ïµ\tilde{O}(k^{\epsilon})over~ start_ARG italic_O end_ARG ( italic_k start_POSTSUPERSCRIPT italic_Ïµ end_POSTSUPERSCRIPT ) recourse. To achieve truly polylogarithmic recourse and O~â¢(k)~ğ‘‚ğ‘˜\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) update time using the algorithm of [BCG+24], we have to set Ïµ=Oâ¢(logâ¡logâ¡klogâ¡k)italic-Ïµğ‘‚ğ‘˜ğ‘˜\epsilon=O\left(\frac{\log\log k}{\log k}\right)italic_Ïµ = italic_O ( divide start_ARG roman_log roman_log italic_k end_ARG start_ARG roman_log italic_k end_ARG ). This, however, increases the approximation guarantee to Î©â¢(logâ¡klogâ¡logâ¡k)Î©ğ‘˜ğ‘˜\Omega\left(\frac{\log k}{\log\log k}\right)roman_Î© ( divide start_ARG roman_log italic_k end_ARG start_ARG roman_log roman_log italic_k end_ARG ). In contrast, we achieve Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 )-approximation ratio, O~â¢(k)~ğ‘‚ğ‘˜\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) update time and O~â¢(1)~ğ‘‚1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) recourse. Approximation Ratio Update Time Recourse Paper Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 ) O~â¢(n+k2)~ğ‘‚ğ‘›superscriptğ‘˜2\tilde{O}(n+k^{2})over~ start_ARG italic_O end_ARG ( italic_n + italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) Oâ¢(k)ğ‘‚ğ‘˜O(k)italic_O ( italic_k ) [CHP+19] Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 ) O~â¢(k2)~ğ‘‚superscriptğ‘˜2\tilde{O}(k^{2})over~ start_ARG italic_O end_ARG ( italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) Oâ¢(k)ğ‘‚ğ‘˜O(k)italic_O ( italic_k ) [HK20] Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 ) O~â¢(k2)~ğ‘‚superscriptğ‘˜2\tilde{O}(k^{2})over~ start_ARG italic_O end_ARG ( italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) Oâ¢(k)ğ‘‚ğ‘˜O(k)italic_O ( italic_k ) [BCLP23]333We remark that [BCLP23] actually maintain a Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 ) approximate â€œsparsifierâ€ of size O~â¢(k)~ğ‘‚ğ‘˜\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) in O~â¢(k)~ğ‘‚ğ‘˜\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) update time, and they need to run the static algorithm of [MP00] on top of this sparsifier after every update. This leads to an update time of O~â¢(k2)~ğ‘‚superscriptğ‘˜2\tilde{O}(k^{2})over~ start_ARG italic_O end_ARG ( italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) for the dynamic kğ‘˜kitalic_k-median and kğ‘˜kitalic_k-means problems. Oâ¢(Ïµâˆ’1)ğ‘‚superscriptitalic-Ïµ1O\left(\epsilon^{-1}\right)italic_O ( italic_Ïµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) O~â¢(k1+Ïµ)~ğ‘‚superscriptğ‘˜1italic-Ïµ\tilde{O}(k^{1+\epsilon})over~ start_ARG italic_O end_ARG ( italic_k start_POSTSUPERSCRIPT 1 + italic_Ïµ end_POSTSUPERSCRIPT ) O~â¢(kÏµ)~ğ‘‚superscriptğ‘˜italic-Ïµ\tilde{O}(k^{\epsilon})over~ start_ARG italic_O end_ARG ( italic_k start_POSTSUPERSCRIPT italic_Ïµ end_POSTSUPERSCRIPT ) [BCG+24] Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 ) O~â¢(k)~ğ‘‚ğ‘˜\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) Oâ¢(log2â¡Î”)ğ‘‚superscript2Î”O(\log^{2}\Delta)italic_O ( roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT roman_Î” ) Our Result Table 1: State-of-the-art for fully dynamic metric kğ‘˜kitalic_k-median. The table for fully dynamic metric kğ‘˜kitalic_k-means is identical, except that the approximation ratio of [BCG+24] is Oâ¢(Ïµâˆ’2)ğ‘‚superscriptitalic-Ïµ2O\left(\epsilon^{-2}\right)italic_O ( italic_Ïµ start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT ). Related Work. In addition to metric kğ‘˜kitalic_k-median, two related clustering problems have been extensively studied in the dynamic setting: (i) metric kğ‘˜kitalic_k-center [CGS18, BEF+23, LHG+24, BCG+24, FS25, BCLP24] and (ii) metric facility location [CHP+19, BLP22]. Both these problems are relatively well-understood by now. For example, it is known how to simultaneously achieve Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 )-approximation ratio, O~â¢(1)~ğ‘‚1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) recourse and O~â¢(k)~ğ‘‚ğ‘˜\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) update time for dynamic metric kğ‘˜kitalic_k-center [BCLP24]. They have also been studied under special classes of metrics, such as Euclidean spaces [GHL+21, BEF+23, BGJ+24], or shortest-path metrics in graphs undergoing edge-updates [CFG+24]. There is another line of work on dynamic metric kğ‘˜kitalic_k-center and metric kğ‘˜kitalic_k-median, which considers the incremental (insertion only) setting, and achieves total recourse guarantees that are sublinear in the total number of updates [LV17, FLNS21]. However, the update times of these algorithms are large polynomials in nğ‘›nitalic_n. Section 2.2 contains a detailed discussion on the algorithm of [FLNS21]. 2 Technical Overview In Sections 2.1 and 2.2, we summarize the technical contributions of two relevant papers [BCG+24, FLNS21]. We obtain our algorithm via carefully synthesizing the techniques from both these papers, along with some key, new insights of our own. In Section 2.3, we outline the major technical challenges we face while trying to prove Theorem 1.1, and how we overcome them. 2.1 The Fully Dynamic Algorithm of [BCG+24] There are two main technical contributions in [BCG+24]; we briefly review each of them below. 2.1.1 A Hierarchical Approach to Dynamic kğ‘˜kitalic_k-Median This approach allows the authors to obtain Oâ¢(Ïµâˆ’1)ğ‘‚superscriptitalic-Ïµ1O(\epsilon^{-1})italic_O ( italic_Ïµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT )-approximation ratio with O~â¢(kÏµ)~ğ‘‚superscriptğ‘˜italic-Ïµ\tilde{O}(k^{\epsilon})over~ start_ARG italic_O end_ARG ( italic_k start_POSTSUPERSCRIPT italic_Ïµ end_POSTSUPERSCRIPT ) recourse, and works as follows. We maintain a hierarchy of nested subsets of centers S0âŠ‡â‹¯âŠ‡Sâ„“+1superset-of-or-equalssubscriptğ‘†0â‹¯superset-of-or-equalssubscriptğ‘†â„“1S_{0}\supseteq\dots\supseteq S_{\ell+1}italic_S start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT âŠ‡ â‹¯ âŠ‡ italic_S start_POSTSUBSCRIPT roman_â„“ + 1 end_POSTSUBSCRIPT, where â„“=1/Ïµâ„“1italic-Ïµ\ell=1/\epsilonroman_â„“ = 1 / italic_Ïµ and |Si|=k+âŒŠk1âˆ’iâ¢ÏµâŒ‹subscriptğ‘†ğ‘–ğ‘˜superscriptğ‘˜1ğ‘–italic-Ïµ|S_{i}|=k+\lfloor k^{1-i\epsilon}\rfloor| italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | = italic_k + âŒŠ italic_k start_POSTSUPERSCRIPT 1 - italic_i italic_Ïµ end_POSTSUPERSCRIPT âŒ‹ for each iâˆˆ[0,â„“+1]ğ‘–0â„“1i\in[0,\ell+1]italic_i âˆˆ [ 0 , roman_â„“ + 1 ]. We refer to si:=âŒŠk1âˆ’iâ¢ÏµâŒ‹assignsubscriptğ‘ ğ‘–superscriptğ‘˜1ğ‘–italic-Ïµs_{i}:=\lfloor k^{1-i\epsilon}\rflooritalic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT := âŒŠ italic_k start_POSTSUPERSCRIPT 1 - italic_i italic_Ïµ end_POSTSUPERSCRIPT âŒ‹ as the slack at layer iğ‘–iitalic_i of the hierarchy. The set Sâ„“+1subscriptğ‘†â„“1S_{\ell+1}italic_S start_POSTSUBSCRIPT roman_â„“ + 1 end_POSTSUBSCRIPT has size exactly kğ‘˜kitalic_k and is the kğ‘˜kitalic_k-median solution maintained by the algorithm. We always maintain the following invariant. Invariant 2.1. Costâ¢(S0,ğ’«)=Oâ¢(1)â‹…OPTkâ¢(ğ’«)Costsubscriptğ‘†0ğ’«â‹…ğ‘‚1subscriptOPTğ‘˜ğ’«\textnormal{{Cost}}\left(S_{0},\mathcal{P}\right)=O(1)\cdot\textnormal{{OPT}}_% {k}(\mathcal{P})Cost ( italic_S start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , caligraphic_P ) = italic_O ( 1 ) â‹… OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ), and Costâ¢(Si,ğ’«)â‰¤Costâ¢(Siâˆ’1,ğ’«)+Oâ¢(1)â‹…OPTkâ¢(ğ’«)Costsubscriptğ‘†ğ‘–ğ’«Costsubscriptğ‘†ğ‘–1ğ’«â‹…ğ‘‚1subscriptOPTğ‘˜ğ’«\textnormal{{Cost}}\left(S_{i},\mathcal{P}\right)\leq\textnormal{{Cost}}\left(% S_{i-1},\mathcal{P}\right)+O(1)\cdot\textnormal{{OPT}}_{k}(\mathcal{P})Cost ( italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_P ) â‰¤ Cost ( italic_S start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT , caligraphic_P ) + italic_O ( 1 ) â‹… OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ) for each iâˆˆ[1,â„“]ğ‘–1â„“i\in[1,\ell]italic_i âˆˆ [ 1 , roman_â„“ ]. Here, OPTkâ¢(ğ’«)subscriptOPTğ‘˜ğ’«\textnormal{{OPT}}_{k}(\mathcal{P})OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ) is the optimal kğ‘˜kitalic_k-median objective w.r.t. the input point-set ğ’«ğ’«\mathcal{P}caligraphic_P. Given this invariant, we infer that Costâ¢(Sâ„“+1)=Oâ¢(â„“)â‹…OPTkâ¢(ğ’«)=Oâ¢(1/Ïµ)â‹…OPTkâ¢(ğ’«)Costsubscriptğ‘†â„“1â‹…ğ‘‚â„“subscriptOPTğ‘˜ğ’«â‹…ğ‘‚1italic-ÏµsubscriptOPTğ‘˜ğ’«\textnormal{{Cost}}\left(S_{\ell+1}\right)=O(\ell)\cdot\textnormal{{OPT}}_{k}(% \mathcal{P})=O(1/\epsilon)\cdot\textnormal{{OPT}}_{k}(\mathcal{P})Cost ( italic_S start_POSTSUBSCRIPT roman_â„“ + 1 end_POSTSUBSCRIPT ) = italic_O ( roman_â„“ ) â‹… OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ) = italic_O ( 1 / italic_Ïµ ) â‹… OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ). Thus, the approximation guarantee is proportional to the number of layers in this hierarchy. The hierarchy is maintained by periodically reconstructing each of the sets Sisubscriptğ‘†ğ‘–S_{i}italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. To be more specific, each set Sisubscriptğ‘†ğ‘–S_{i}italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT (along with the sets Si+1,â€¦,Sâ„“+1subscriptğ‘†ğ‘–1â€¦subscriptğ‘†â„“1S_{i+1},\dots,S_{\ell+1}italic_S start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT , â€¦ , italic_S start_POSTSUBSCRIPT roman_â„“ + 1 end_POSTSUBSCRIPT) is reconstructed from scratch every sisubscriptğ‘ ğ‘–s_{i}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT many updates, without modifying the sets S0,â€¦,Siâˆ’1subscriptğ‘†0â€¦subscriptğ‘†ğ‘–1S_{0},\dots,S_{i-1}italic_S start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , â€¦ , italic_S start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT. In between these updates, the subset Sisubscriptğ‘†ğ‘–S_{i}italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is maintained lazily: Whenever a point pğ‘pitalic_p is inserted into ğ’«ğ’«\mathcal{P}caligraphic_P, we set Siâ†Siâˆª{p}â†subscriptğ‘†ğ‘–subscriptğ‘†ğ‘–ğ‘S_{i}\leftarrow S_{i}\cup\{p\}italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT â† italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âˆª { italic_p }, and whenever a point is deleted from ğ’«ğ’«\mathcal{P}caligraphic_P, no changes are made to Sisubscriptğ‘†ğ‘–S_{i}italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT.444We can afford to handle the deletions in this lazy manner if we consider the improper kğ‘˜kitalic_k-median problem, where we are allowed to open a center at a point that got deleted. See the discussion in the beginning of Section 3. To analyze the recourse, consider the solution Sâ„“+1subscriptğ‘†â„“1S_{\ell+1}italic_S start_POSTSUBSCRIPT roman_â„“ + 1 end_POSTSUBSCRIPT before and after an update during which Sisubscriptğ‘†ğ‘–S_{i}italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is reconstructed. Let Sâ„“+1â€²superscriptsubscriptğ‘†â„“1â€²S_{\ell+1}^{\prime}italic_S start_POSTSUBSCRIPT roman_â„“ + 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT and Sâ„“+1â€²â€²superscriptsubscriptğ‘†â„“1â€²â€²S_{\ell+1}^{\prime\prime}italic_S start_POSTSUBSCRIPT roman_â„“ + 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT â€² â€² end_POSTSUPERSCRIPT denote the status of the solution before and after the update, respectively. Observe that the total recourse incurred in the solution Sâ„“+1subscriptğ‘†â„“1S_{\ell+1}italic_S start_POSTSUBSCRIPT roman_â„“ + 1 end_POSTSUBSCRIPT during this update, i.e. the value |Sâ„“+1â€²âŠ•Sâ„“+1â€²â€²|direct-sumsuperscriptsubscriptğ‘†â„“1â€²superscriptsubscriptğ‘†â„“1â€²â€²|S_{\ell+1}^{\prime}\oplus S_{\ell+1}^{\prime\prime}|| italic_S start_POSTSUBSCRIPT roman_â„“ + 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT âŠ• italic_S start_POSTSUBSCRIPT roman_â„“ + 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT â€² â€² end_POSTSUPERSCRIPT |, is Oâ¢(siâˆ’1)ğ‘‚subscriptğ‘ ğ‘–1O(s_{i-1})italic_O ( italic_s start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ). This follows immediately from the fact that Sâ„“+1â€²superscriptsubscriptğ‘†â„“1â€²S_{\ell+1}^{\prime}italic_S start_POSTSUBSCRIPT roman_â„“ + 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT and Sâ„“+1â€²â€²superscriptsubscriptğ‘†â„“1â€²â€²S_{\ell+1}^{\prime\prime}italic_S start_POSTSUBSCRIPT roman_â„“ + 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT â€² â€² end_POSTSUPERSCRIPT are both subsets of size kğ‘˜kitalic_k of the set Siâˆ’1subscriptğ‘†ğ‘–1S_{i-1}italic_S start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT, which has size â‰¤k+2â¢siâˆ’1absentğ‘˜2subscriptğ‘ ğ‘–1\leq k+2s_{i-1}â‰¤ italic_k + 2 italic_s start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT. We can amortize this recourse over the sisubscriptğ‘ ğ‘–s_{i}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT many lazy updates performed since the last time that Sisubscriptğ‘†ğ‘–S_{i}italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT was reconstructed. This implies that the amortized recourse of Sâ„“+1subscriptğ‘†â„“1S_{\ell+1}italic_S start_POSTSUBSCRIPT roman_â„“ + 1 end_POSTSUBSCRIPT which is caused by reconstructing Sisubscriptğ‘†ğ‘–S_{i}italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is Oâ¢(siâˆ’1/si)=Oâ¢(kÏµ)ğ‘‚subscriptğ‘ ğ‘–1subscriptğ‘ ğ‘–ğ‘‚superscriptğ‘˜italic-ÏµO(s_{i-1}/s_{i})=O(k^{\epsilon})italic_O ( italic_s start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT / italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = italic_O ( italic_k start_POSTSUPERSCRIPT italic_Ïµ end_POSTSUPERSCRIPT ). Summing over all iâˆˆ[0,â„“+1]ğ‘–0â„“1i\in[0,\ell+1]italic_i âˆˆ [ 0 , roman_â„“ + 1 ], we get an overall amortized recourse bound of Oâ¢((â„“+2)â‹…kÏµ)=Oâ¢(kÏµ/Ïµ)ğ‘‚â‹…â„“2superscriptğ‘˜italic-Ïµğ‘‚superscriptğ‘˜italic-Ïµitalic-ÏµO((\ell+2)\cdot k^{\epsilon})=O(k^{\epsilon}/\epsilon)italic_O ( ( roman_â„“ + 2 ) â‹… italic_k start_POSTSUPERSCRIPT italic_Ïµ end_POSTSUPERSCRIPT ) = italic_O ( italic_k start_POSTSUPERSCRIPT italic_Ïµ end_POSTSUPERSCRIPT / italic_Ïµ ). Barrier towards achieving Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 )-approximation with O~â¢(1)~ğ‘‚1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) recourse. If we want to use this hierarchy to obtain a recourse of O~â¢(1)~ğ‘‚1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ), then we need to ensure that si=Î©~â¢(siâˆ’1)subscriptğ‘ ğ‘–~Î©subscriptğ‘ ğ‘–1s_{i}=\tilde{\Omega}(s_{i-1})italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = over~ start_ARG roman_Î© end_ARG ( italic_s start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ) (i.e., we need the slacks at the layers to decrease by at most a O~â¢(1)~ğ‘‚1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) factor between the layers). If this is not the case, then the amortized recourse in Sâ„“+1subscriptğ‘†â„“1S_{\ell+1}italic_S start_POSTSUBSCRIPT roman_â„“ + 1 end_POSTSUBSCRIPT caused by reconstructing Sisubscriptğ‘†ğ‘–S_{i}italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT will be Î©â¢(siâˆ’1/si)=Ï‰~â¢(1)Î©subscriptğ‘ ğ‘–1subscriptğ‘ ğ‘–~ğœ”1\Omega(s_{i-1}/s_{i})=\tilde{\omega}(1)roman_Î© ( italic_s start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT / italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = over~ start_ARG italic_Ï‰ end_ARG ( 1 ). Unfortunately, to have such a guarantee, the number of layers needs to be Î©â¢(logâ¡k/logâ¡logâ¡k)Î©ğ‘˜ğ‘˜\Omega(\log k/\log\log k)roman_Î© ( roman_log italic_k / roman_log roman_log italic_k ); and since the approximation ratio of the algorithm is proportional to the number of layers (see 2.1), this leads to an approximation ratio of Ï‰â¢(1)ğœ”1\omega(1)italic_Ï‰ ( 1 ). Thus, it is not at all clear if this approach can be used to obtain Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 )-approximation and O~â¢(1)~ğ‘‚1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) recourse simultaneously. 2.1.2 Achieving O~â¢(k1+Ïµ)~ğ‘‚superscriptğ‘˜1italic-Ïµ\tilde{O}(k^{1+\epsilon})over~ start_ARG italic_O end_ARG ( italic_k start_POSTSUPERSCRIPT 1 + italic_Ïµ end_POSTSUPERSCRIPT ) Update Time via Randomized Local Search The second technical contribution in [BCG+24] is to show that the hierarchy from Section 2.1.1 can be maintained in O~â¢(k1+Ïµ)~ğ‘‚superscriptğ‘˜1italic-Ïµ\tilde{O}(k^{1+\epsilon})over~ start_ARG italic_O end_ARG ( italic_k start_POSTSUPERSCRIPT 1 + italic_Ïµ end_POSTSUPERSCRIPT ) update time, using a specific type of randomized local search. To be more specific, consider a set of nğ‘›nitalic_n points ğ’«ğ’«\mathcal{P}caligraphic_P, a set of kğ‘˜kitalic_k centers ğ’°ğ’°\mathcal{U}caligraphic_U, and an integer sâˆˆ[1,kâˆ’1]ğ‘ 1ğ‘˜1s\in[1,k-1]italic_s âˆˆ [ 1 , italic_k - 1 ]. Suppose that we want to compute a subset ğ’°â€²âŠ†ğ’°superscriptğ’°â€²ğ’°\mathcal{U}^{\prime}\subseteq\mathcal{U}caligraphic_U start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT âŠ† caligraphic_U of (kâˆ’s)ğ‘˜ğ‘ (k-s)( italic_k - italic_s ) centers, so as to minimize Costâ¢(ğ’°â€²,ğ’«)Costsuperscriptğ’°â€²ğ’«\textnormal{{Cost}}\left(\mathcal{U}^{\prime},\mathcal{P}\right)Cost ( caligraphic_U start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT , caligraphic_P ). In [BCG+24], the authors present a Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 )-approximation algorithm for this problem using randomized local search, that runs in only O~â¢(nâ¢s)~ğ‘‚ğ‘›ğ‘ \tilde{O}(ns)over~ start_ARG italic_O end_ARG ( italic_n italic_s ) time, assuming the algorithm has access to some â€œauxiliary data structuresâ€ to begin with (see Lemma 6.1). Morally, the important message here is that the runtime of randomized local search (when given access to some auxiliary data structures) is proportional to the slack sğ‘ sitalic_s, and independent of kğ‘˜kitalic_k. In [BCG+24], the authors call this procedure as a subroutine while reconstructing a set Sisubscriptğ‘†ğ‘–S_{i}italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT (along with the sets Si+1,â€¦,Sâ„“+1subscriptğ‘†ğ‘–1â€¦subscriptğ‘†â„“1S_{i+1},\dots,S_{\ell+1}italic_S start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT , â€¦ , italic_S start_POSTSUBSCRIPT roman_â„“ + 1 end_POSTSUBSCRIPT) in the hierarchy from scratch, after every sisubscriptğ‘ ğ‘–s_{i}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT many updates. Although our algorithm does not use a hierarchical approach while bounding the approximation ratio and recourse, we use randomized local search to achieve fast update time (see Section 2.3). 2.2 The Incremental Algorithm of [FLNS21] In [FLNS21], the authors obtain O~â¢(k)~ğ‘‚ğ‘˜\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) total recourse, while maintaining a Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 )-approximate kğ‘˜kitalic_k-median solution over a sequence of nğ‘›nitalic_n point insertions (starting from an empty input). Note that in this incremental setting, the total recourse is sublinear in the number of updates; this is achieved by using a technique known as Myerson sketch [Mey01]. Since it is not possible to achieve such a sublinear total recourse bound in the fully dynamic setting (the focus of our paper), in Theorem 2.2 we summarize the main result of [FLNS21] without invoking Myerson sketch. We emphasize that the update time in [FLNS21] is already prohibitively high (some large polynomial in nğ‘›nitalic_n) for our purpose. Accordingly, to highlight the main ideas in the rest of this section, we will outline a variant of the algorithm in [FLNS21] with exponential update time. Theorem 2.2 ([FLNS21]). Suppose that the input ğ’«ğ’«\mathcal{P}caligraphic_P undergoes a sequence of point-insertions. Then, we can maintain a Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 )-approximate kğ‘˜kitalic_k-median solution to ğ’«ğ’«\mathcal{P}caligraphic_P with O~â¢(1)~ğ‘‚1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) amortized recourse. A major technical insight in [FLNS21] was to introduce the notion of robust centers. Informally, a set of centers ğ’°ğ’°\mathcal{U}caligraphic_U is robust w.r.t. a point-set ğ’«ğ’«\mathcal{P}caligraphic_P, iff each uâˆˆğ’°ğ‘¢ğ’°u\in\mathcal{U}italic_u âˆˆ caligraphic_U is a good approximate 1111-median solution at every â€œdistance-scaleâ€ w.r.t. the points in ğ’«ğ’«\mathcal{P}caligraphic_P that are sufficiently close to uğ‘¢uitalic_u. See Section 3.2 for a formal definition. Below, we present our interpretation of the incremental algorithm in [FLNS21]. We start with a key lemma summarizing an important property of robust centers. Consider any integer 0â‰¤â„“â‰¤k0â„“ğ‘˜0\leq\ell\leq k0 â‰¤ roman_â„“ â‰¤ italic_k. We say that a set of kğ‘˜kitalic_k centers ğ’°ğ’°\mathcal{U}caligraphic_U is maximally â„“â„“\ellroman_â„“-stable w.r.t. a point-set ğ’«ğ’«\mathcal{P}caligraphic_P iff OPTkâˆ’â„“ğ’°â¢(ğ’«)â‰¤câ‹…Costâ¢(ğ’°,ğ’«)superscriptsubscriptOPTğ‘˜â„“ğ’°ğ’«â‹…ğ‘Costğ’°ğ’«\textnormal{{OPT}}_{k-\ell}^{\mathcal{U}}(\mathcal{P})\leq c\cdot\textnormal{{% Cost}}\left(\mathcal{U},\mathcal{P}\right)OPT start_POSTSUBSCRIPT italic_k - roman_â„“ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT caligraphic_U end_POSTSUPERSCRIPT ( caligraphic_P ) â‰¤ italic_c â‹… Cost ( caligraphic_U , caligraphic_P ) and OPTkâˆ’(â„“+1)ğ’°â¢(ğ’«)>câ‹…Costâ¢(ğ’°,ğ’«)superscriptsubscriptOPTğ‘˜â„“1ğ’°ğ’«â‹…ğ‘Costğ’°ğ’«\textnormal{{OPT}}_{k-(\ell+1)}^{\mathcal{U}}(\mathcal{P})>c\cdot\textnormal{{% Cost}}\left(\mathcal{U},\mathcal{P}\right)OPT start_POSTSUBSCRIPT italic_k - ( roman_â„“ + 1 ) end_POSTSUBSCRIPT start_POSTSUPERSCRIPT caligraphic_U end_POSTSUPERSCRIPT ( caligraphic_P ) > italic_c â‹… Cost ( caligraphic_U , caligraphic_P ), where c=456000ğ‘456000c=456000italic_c = 456000 is an absolute constant, and OPTtğ’°â¢(ğ’«):=minZâŠ†ğ’°:|Z|â‰¤tâ¡Costâ¢(Z,ğ’«)assignsuperscriptsubscriptOPTğ‘¡ğ’°ğ’«subscript:ğ‘ğ’°ğ‘ğ‘¡Costğ‘ğ’«\textnormal{{OPT}}_{t}^{\mathcal{U}}(\mathcal{P}):=\min_{Z\subseteq\mathcal{U}% :|Z|\leq t}\textnormal{{Cost}}\left(Z,\mathcal{P}\right)OPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT caligraphic_U end_POSTSUPERSCRIPT ( caligraphic_P ) := roman_min start_POSTSUBSCRIPT italic_Z âŠ† caligraphic_U : | italic_Z | â‰¤ italic_t end_POSTSUBSCRIPT Cost ( italic_Z , caligraphic_P ) is the objective of the optimal tğ‘¡titalic_t-median solution subject to the restriction that all the centers must be from the set ğ’°ğ’°\mathcal{U}caligraphic_U. This means that we can afford to remove â„“â„“\ellroman_â„“ centers from ğ’°ğ’°\mathcal{U}caligraphic_U without increasing the objective value by more than a Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 ) factor, but not more than â„“â„“\ellroman_â„“ centers. We defer the proof of Lemma 2.3 to Section 9. Lemma 2.3 ([FLNS21]). Consider any two point-sets ğ’«initsubscriptğ’«init\mathcal{P}_{\textnormal{{init}}}caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT and ğ’«finalsubscriptğ’«final\mathcal{P}_{\textnormal{{final}}}caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT with |ğ’«initâŠ•ğ’«final|â‰¤â„“+1direct-sumsubscriptğ’«initsubscriptğ’«finalâ„“1\left|\mathcal{P}_{\textnormal{{init}}}\oplus\mathcal{P}_{\textnormal{{final}}% }\right|\leq\ell+1| caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT âŠ• caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT | â‰¤ roman_â„“ + 1, for â„“âˆˆ[0,k]â„“0ğ‘˜\ell\in[0,k]roman_â„“ âˆˆ [ 0 , italic_k ].555Here, the notation âŠ•direct-sum\oplusâŠ• denotes the symmetric difference between two sets. W.r.t. ğ’«initsubscriptğ’«init\mathcal{P}_{\textnormal{{init}}}caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT, let ğ’°initsubscriptğ’°init\mathcal{U}_{\textnormal{{init}}}caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT be any set of kğ‘˜kitalic_k centers that is robust and maximally â„“â„“\ellroman_â„“-stable. Let ğ’±ğ’±\mathcal{V}caligraphic_V be any set of kğ‘˜kitalic_k centers such that Costâ¢(ğ’±,ğ’«init)â‰¤18â‹…Costâ¢(ğ’°init,ğ’«init)Costğ’±subscriptğ’«initâ‹…18Costsubscriptğ’°initsubscriptğ’«init\textnormal{{Cost}}\left(\mathcal{V},\mathcal{P}_{\textnormal{{init}}}\right)% \leq 18\cdot\textnormal{{Cost}}\left(\mathcal{U}_{\textnormal{{init}}},% \mathcal{P}_{\textnormal{{init}}}\right)Cost ( caligraphic_V , caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ) â‰¤ 18 â‹… Cost ( caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT , caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ). Then, there is a set of kğ‘˜kitalic_k centers ğ’²â‹†superscriptğ’²â‹†\mathcal{W}^{\star}caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT, such that |ğ’²â‹†âŠ•ğ’°init|â‰¤5â¢â„“+5direct-sumsuperscriptğ’²â‹†subscriptğ’°init5â„“5\left|\mathcal{W}^{\star}\oplus\mathcal{U}_{\textnormal{{init}}}\right|\leq 5% \ell+5| caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT âŠ• caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT | â‰¤ 5 roman_â„“ + 5 and Costâ¢(ğ’²â‹†,ğ’«final)â‰¤3â‹…Costâ¢(ğ’±,ğ’«final)Costsuperscriptğ’²â‹†subscriptğ’«finalâ‹…3Costğ’±subscriptğ’«final\textnormal{{Cost}}\left(\mathcal{W}^{\star},\mathcal{P}_{\textnormal{{final}}% }\right)\leq 3\cdot\textnormal{{Cost}}\left(\mathcal{V},\mathcal{P}_{% \textnormal{{final}}}\right)Cost ( caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT , caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT ) â‰¤ 3 â‹… Cost ( caligraphic_V , caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT ). The algorithm works in O~â¢(1)~ğ‘‚1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) many phases, where each phase consists of a sequence of consecutive updates (only insertions) such that the optimal objective value, given by OPTkâ¢(ğ’«)subscriptOPTğ‘˜ğ’«\textnormal{{OPT}}_{k}(\mathcal{P})OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ), does not increase by more than a factor of 18181818 within any given phase. The algorithm restarts whenever one phase terminates and the next phase begins, and computes a new kğ‘˜kitalic_k-median solution to the current input from scratch. Within each phase, the algorithm incurs O~â¢(n)~ğ‘‚ğ‘›\tilde{O}(n)over~ start_ARG italic_O end_ARG ( italic_n ) total recourse, and this implies the amortized recourse guarantee of O~â¢(1)~ğ‘‚1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ). Each phase is further partitioned into epochs, as follows. By induction hypothesis, we start an epoch with an 100100100100-approximate kğ‘˜kitalic_k-median solution ğ’°initsubscriptğ’°init\mathcal{U}_{\textnormal{{init}}}caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT that is robust and also maximally â„“â„“\ellroman_â„“-stable, for some â„“âˆˆ[0,k]â„“0ğ‘˜\ell\in[0,k]roman_â„“ âˆˆ [ 0 , italic_k ], w.r.t. the current input ğ’«initsubscriptğ’«init\mathcal{P}_{\textnormal{{init}}}caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT. Let Î»init:=OPTkâ¢(ğ’«init)assignsubscriptğœ†initsubscriptOPTğ‘˜subscriptğ’«init\lambda_{\textnormal{{init}}}:=\textnormal{{OPT}}_{k}(\mathcal{P}_{\textnormal% {{init}}})italic_Î» start_POSTSUBSCRIPT init end_POSTSUBSCRIPT := OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ) denote the optimal objective value at this point in time. We then compute a subset ğ’°âŠ†ğ’°initğ’°subscriptğ’°init\mathcal{U}\subseteq\mathcal{U}_{\textnormal{{init}}}caligraphic_U âŠ† caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT of (kâˆ’â„“)ğ‘˜â„“(k-\ell)( italic_k - roman_â„“ ) centers, such that Costâ¢(ğ’°,ğ’«init)â‰¤câ‹…Costâ¢(ğ’°init,ğ’«init)â‰¤100â¢câ‹…OPTkâ¢(ğ’«init)=100â¢câ‹…Î»initCostğ’°subscriptğ’«initâ‹…ğ‘Costsubscriptğ’°initsubscriptğ’«initâ‹…100ğ‘subscriptOPTğ‘˜subscriptğ’«initâ‹…100ğ‘subscriptğœ†init\textnormal{{Cost}}\left(\mathcal{U},\mathcal{P}_{\textnormal{{init}}}\right)% \leq c\cdot\textnormal{{Cost}}\left(\mathcal{U}_{\textnormal{{init}}},\mathcal% {P}_{\textnormal{{init}}}\right)\leq 100c\cdot\textnormal{{OPT}}_{k}(\mathcal{% P}_{\textnormal{{init}}})=100c\cdot\lambda_{\textnormal{{init}}}Cost ( caligraphic_U , caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ) â‰¤ italic_c â‹… Cost ( caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT , caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ) â‰¤ 100 italic_c â‹… OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ) = 100 italic_c â‹… italic_Î» start_POSTSUBSCRIPT init end_POSTSUBSCRIPT. The epoch lasts for the next (â„“+1)â„“1(\ell+1)( roman_â„“ + 1 ) updates. The algorithm lazily handles the first â„“â„“\ellroman_â„“ updates in the epoch, incurring a worst-case recourse of one per update: Whenever a point pğ‘pitalic_p gets inserted into ğ’«ğ’«\mathcal{P}caligraphic_P, it sets ğ’°â†ğ’°âˆª{p}â†ğ’°ğ’°ğ‘\mathcal{U}\leftarrow\mathcal{U}\cup\{p\}caligraphic_U â† caligraphic_U âˆª { italic_p }. Since initially |ğ’°|=kâˆ’â„“ğ’°ğ‘˜â„“|\mathcal{U}|=k-\ell| caligraphic_U | = italic_k - roman_â„“, the set ğ’°ğ’°\mathcal{U}caligraphic_U never grows large enough to contain more than kğ‘˜kitalic_k centers. Further, the objective of the maintained solution ğ’°ğ’°\mathcal{U}caligraphic_U does not increase due to these â„“â„“\ellroman_â„“ updates, and remains Costâ¢(ğ’°,ğ’«)â‰¤100â¢câ‹…Î»initâ‰¤200â¢câ‹…OPTkâ¢(ğ’«),Costğ’°ğ’«â‹…100ğ‘subscriptğœ†initâ‹…200ğ‘subscriptOPTğ‘˜ğ’«\textnormal{{Cost}}\left(\mathcal{U},\mathcal{P}\right)\leq 100c\cdot\lambda_{% \textnormal{{init}}}\leq 200c\cdot\textnormal{{OPT}}_{k}(\mathcal{P}),Cost ( caligraphic_U , caligraphic_P ) â‰¤ 100 italic_c â‹… italic_Î» start_POSTSUBSCRIPT init end_POSTSUBSCRIPT â‰¤ 200 italic_c â‹… OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ) , (1) where the last inequality holds because OPTkâ¢(ğ’«)subscriptOPTğ‘˜ğ’«\textnormal{{OPT}}_{k}(\mathcal{P})OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ) is almost monotone as ğ’«ğ’«\mathcal{P}caligraphic_P undergoes point-insertions (more precisely, it can decrease by at most a factor of 2222). While handling the last (i.e., (â„“+1)tâ¢hsuperscriptâ„“1ğ‘¡â„(\ell+1)^{th}( roman_â„“ + 1 ) start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT) update in the epoch, our goal is to come up with a kğ‘˜kitalic_k-median solution ğ’°finalsubscriptğ’°final\mathcal{U}_{\textnormal{{final}}}caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT such that: (i) the induction hypothesis holds w.r.t. ğ’°finalsubscriptğ’°final\mathcal{U}_{\textnormal{{final}}}caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT and ğ’«finalsubscriptğ’«final\mathcal{P}_{\textnormal{{final}}}caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT (where ğ’«finalsubscriptğ’«final\mathcal{P}_{\textnormal{{final}}}caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT is the state of the input at the end of the epoch), and (ii) the recourse remains small, i.e., |ğ’°finalâŠ•ğ’°init|=Oâ¢(â„“+1)direct-sumsubscriptğ’°finalsubscriptğ’°initğ‘‚â„“1\left|\mathcal{U}_{\textnormal{{final}}}\oplus\mathcal{U}_{\textnormal{{init}}% }\right|=O(\ell+1)| caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT âŠ• caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT | = italic_O ( roman_â„“ + 1 ). We can assume that OPTkâ¢(ğ’«final)â‰¤18â‹…Î»initsubscriptOPTğ‘˜subscriptğ’«finalâ‹…18subscriptğœ†init\textnormal{{OPT}}_{k}(\mathcal{P}_{\textnormal{{final}}})\leq 18\cdot\lambda_% {\textnormal{{init}}}OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT ) â‰¤ 18 â‹… italic_Î» start_POSTSUBSCRIPT init end_POSTSUBSCRIPT, for otherwise we would initiate a new phase at this point in time. We find the set ğ’°finalsubscriptğ’°final\mathcal{U}_{\textnormal{{final}}}caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT as follows. Let ğ’±ğ’±\mathcal{V}caligraphic_V be an optimal kğ‘˜kitalic_k-median solution w.r.t. ğ’«finalsubscriptğ’«final\mathcal{P}_{\textnormal{{final}}}caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT, so that: Costâ¢(ğ’±,ğ’«init)â‰¤Costâ¢(ğ’±,ğ’«final)=OPTkâ¢(ğ’«final)â‰¤18â‹…OPTkâ¢(ğ’«init)â‰¤18â‹…Costâ¢(ğ’°init,ğ’«init).Costğ’±subscriptğ’«initCostğ’±subscriptğ’«finalsubscriptOPTğ‘˜subscriptğ’«finalâ‹…18subscriptOPTğ‘˜subscriptğ’«initâ‹…18Costsubscriptğ’°initsubscriptğ’«init\textnormal{{Cost}}\left(\mathcal{V},\mathcal{P}_{\textnormal{{init}}}\right)% \leq\textnormal{{Cost}}\left(\mathcal{V},\mathcal{P}_{\textnormal{{final}}}% \right)=\textnormal{{OPT}}_{k}(\mathcal{P}_{\textnormal{{final}}})\leq 18\cdot% \textnormal{{OPT}}_{k}(\mathcal{P}_{\textnormal{{init}}})\leq 18\cdot% \textnormal{{Cost}}\left(\mathcal{U}_{\textnormal{{init}}},\mathcal{P}_{% \textnormal{{init}}}\right).Cost ( caligraphic_V , caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ) â‰¤ Cost ( caligraphic_V , caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT ) = OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT ) â‰¤ 18 â‹… OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ) â‰¤ 18 â‹… Cost ( caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT , caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ) . (2) Applying Lemma 2.3, we find a set of kğ‘˜kitalic_k centers ğ’²â‹†superscriptğ’²â‹†\mathcal{W}^{\star}caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT such that |ğ’²â‹†âŠ•ğ’°init|â‰¤5â¢â„“+5â¢ and Costâ¢(ğ’²â‹†,ğ’«final)â‰¤3â‹…Costâ¢(ğ’±,ğ’«final)=3â‹…OPTkâ¢(ğ’«final).direct-sumsuperscriptğ’²â‹†subscriptğ’°init5â„“5 and Costsuperscriptğ’²â‹†subscriptğ’«finalâ‹…3Costğ’±subscriptğ’«finalâ‹…3subscriptOPTğ‘˜subscriptğ’«final\left|\mathcal{W}^{\star}\oplus\mathcal{U}_{\textnormal{{init}}}\right|\leq 5% \ell+5\text{ and }\textnormal{{Cost}}\left(\mathcal{W}^{\star},\mathcal{P}_{% \textnormal{{final}}}\right)\leq 3\cdot\textnormal{{Cost}}\left(\mathcal{V},% \mathcal{P}_{\textnormal{{final}}}\right)=3\cdot\textnormal{{OPT}}_{k}(% \mathcal{P}_{\textnormal{{final}}}).| caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT âŠ• caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT | â‰¤ 5 roman_â„“ + 5 and sansserif_Cost ( caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT , caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT ) â‰¤ 3 â‹… Cost ( caligraphic_V , caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT ) = 3 â‹… OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT ) . (3) Next, we call a subroutine Robustify(ğ’²â‹†)superscriptğ’²â‹†(\mathcal{W}^{\star})( caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) which returns a set of kğ‘˜kitalic_k robust centers ğ’°finalsubscriptğ’°final\mathcal{U}_{\textnormal{{final}}}caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT such that Costâ¢(ğ’°final,ğ’«final)â‰¤(3/2)â‹…Costâ¢(ğ’²â‹†,ğ’«final)=(9/2)â‹…OPTkâ¢(ğ’«final)â‰¤100â‹…OPTkâ¢(ğ’«final)Costsubscriptğ’°finalsubscriptğ’«finalâ‹…32Costsuperscriptğ’²â‹†subscriptğ’«finalâ‹…92subscriptOPTğ‘˜subscriptğ’«finalâ‹…100subscriptOPTğ‘˜subscriptğ’«final\textnormal{{Cost}}\left(\mathcal{U}_{\textnormal{{final}}},\mathcal{P}_{% \textnormal{{final}}}\right)\leq(3/2)\cdot\textnormal{{Cost}}\left(\mathcal{W}% ^{\star},\mathcal{P}_{\textnormal{{final}}}\right)=(9/2)\cdot\textnormal{{OPT}% }_{k}(\mathcal{P}_{\textnormal{{final}}})\leq 100\cdot\textnormal{{OPT}}_{k}(% \mathcal{P}_{\textnormal{{final}}})Cost ( caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT , caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT ) â‰¤ ( 3 / 2 ) â‹… Cost ( caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT , caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT ) = ( 9 / 2 ) â‹… OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT ) â‰¤ 100 â‹… OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT ) (see Lemma 3.7). This restores the induction hypothesis for the next epoch, w.r.t. ğ’°finalsubscriptğ’°final\mathcal{U}_{\textnormal{{final}}}caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT and ğ’«finalsubscriptğ’«final\mathcal{P}_{\textnormal{{final}}}caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT. We can show that the subroutine Robustify works in such a manner that the step where we transition from ğ’²â‹†superscriptğ’²â‹†\mathcal{W}^{\star}caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT to ğ’°finalsubscriptğ’°final\mathcal{U}_{\textnormal{{final}}}caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT incurs at most O~â¢(1)~ğ‘‚1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) recourse, amortized over the entire sequence of updates within a phase (spanning across multiple epochs). This implies Theorem 2.2. 2.3 Our Approach At a high level, we achieve our result in two parts. First, we generalize the framework of [FLNS21] to achieve Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 )-approximation and O~â¢(1)~ğ‘‚1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) recourse in the fully dynamic setting. Second, we use the randomized local search procedure to implement our algorithm in O~â¢(k)~ğ‘‚ğ‘˜\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) update time. In addition, both these parts require us to come up with important and new technical insights of our own. Below, we explain three significant challenges and outline how we overcome them. Challenge I and Challenge II refers to the first part (approximation and recourse guarantees), whereas Challenge III refers to the third part (update time guarantee). 2.3.1 Challenge I: Double-sided Stability In Section 2.2, we crucially relied on the observation that the optimal kğ‘˜kitalic_k-median objective is (almost) monotonically increasing as more and more points get inserted into ğ’«ğ’«\mathcal{P}caligraphic_P. This allowed us to derive Equation 1, which guarantees that the maintained solution ğ’°ğ’°\mathcal{U}caligraphic_U remains Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 )-approximate while we lazily handle the first â„“â„“\ellroman_â„“ updates within the epoch. This guarantee, however, breaks down in the fully dynamic setting: If points can get deleted from ğ’«ğ’«\mathcal{P}caligraphic_P, then within an epoch we might end up in a situation where OPTkâ¢(ğ’«)â‰ªÎ»initmuch-less-thansubscriptOPTğ‘˜ğ’«subscriptğœ†init\textnormal{{OPT}}_{k}(\mathcal{P})\ll\lambda_{\textnormal{{init}}}OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ) â‰ª italic_Î» start_POSTSUBSCRIPT init end_POSTSUBSCRIPT. To address this issue, we derive a new double-sided stability property in the fully dynamic setting (see Lemma 4.1). Informally, this implies that if OPTkâˆ’â„“â¢(ğ’«init)=Î˜â¢(1)â‹…OPTkâ¢(ğ’«init)subscriptOPTğ‘˜â„“subscriptğ’«initâ‹…Î˜1subscriptOPTğ‘˜subscriptğ’«init\textnormal{{OPT}}_{k-\ell}(\mathcal{P}_{\textnormal{{init}}})=\Theta(1)\cdot% \textnormal{{OPT}}_{k}(\mathcal{P}_{\textnormal{{init}}})OPT start_POSTSUBSCRIPT italic_k - roman_â„“ end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ) = roman_Î˜ ( 1 ) â‹… OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ) (which follows from the hypothesis at the start of an epoch), then for some Î˜â¢(â„“)=râ‰¤â„“Î˜â„“ğ‘Ÿâ„“\Theta(\ell)=r\leq\ellroman_Î˜ ( roman_â„“ ) = italic_r â‰¤ roman_â„“ we have OPTk+râ¢(ğ’«init)=Î˜â¢(1)â‹…OPTkâ¢(ğ’«init)subscriptOPTğ‘˜ğ‘Ÿsubscriptğ’«initâ‹…Î˜1subscriptOPTğ‘˜subscriptğ’«init\textnormal{{OPT}}_{k+r}(\mathcal{P}_{\textnormal{{init}}})=\Theta(1)\cdot% \textnormal{{OPT}}_{k}(\mathcal{P}_{\textnormal{{init}}})OPT start_POSTSUBSCRIPT italic_k + italic_r end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ) = roman_Î˜ ( 1 ) â‹… OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ). Furthermore, we have |ğ’«âŠ•ğ’«init|â‰¤rdirect-sumğ’«subscriptğ’«initğ‘Ÿ\left|\mathcal{P}\oplus\mathcal{P}_{\textnormal{{init}}}\right|\leq r| caligraphic_P âŠ• caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT | â‰¤ italic_r throughout the first rğ‘Ÿritalic_r updates in the epoch, which gives us: OPTkâ¢(ğ’«)â‰¥OPTk+râ¢(ğ’«init)subscriptOPTğ‘˜ğ’«subscriptOPTğ‘˜ğ‘Ÿsubscriptğ’«init\textnormal{{OPT}}_{k}(\mathcal{P})\geq\textnormal{{OPT}}_{k+r}(\mathcal{P}_{% \textnormal{{init}}})OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ) â‰¥ OPT start_POSTSUBSCRIPT italic_k + italic_r end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ) (see Lemma 3.11). It follows that for the first rğ‘Ÿritalic_r updates in the epoch, we have OPTkâ¢(ğ’«)â‰¥OPTk+râ¢(ğ’«init)=Î˜â¢(1)â‹…OPTkâ¢(ğ’«init)=Î˜â¢(1)â‹…Î»initsubscriptOPTğ‘˜ğ’«subscriptOPTğ‘˜ğ‘Ÿsubscriptğ’«initâ‹…Î˜1subscriptOPTğ‘˜subscriptğ’«initâ‹…Î˜1subscriptğœ†init\textnormal{{OPT}}_{k}(\mathcal{P})\geq\textnormal{{OPT}}_{k+r}(\mathcal{P}_{% \textnormal{{init}}})=\Theta(1)\cdot\textnormal{{OPT}}_{k}(\mathcal{P}_{% \textnormal{{init}}})=\Theta(1)\cdot\lambda_{\textnormal{{init}}}OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ) â‰¥ OPT start_POSTSUBSCRIPT italic_k + italic_r end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ) = roman_Î˜ ( 1 ) â‹… OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ) = roman_Î˜ ( 1 ) â‹… italic_Î» start_POSTSUBSCRIPT init end_POSTSUBSCRIPT. Accordingly, we truncate the epoch to last for only r+1ğ‘Ÿ1r+1italic_r + 1 updates, and now we can rule out the scenario where OPTkâ¢(ğ’«)subscriptOPTğ‘˜ğ’«\textnormal{{OPT}}_{k}(\mathcal{P})OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ) drops significantly below Î»initsubscriptğœ†init\lambda_{\textnormal{{init}}}italic_Î» start_POSTSUBSCRIPT init end_POSTSUBSCRIPT during the epoch. But since r=Î©â¢(â„“)ğ‘ŸÎ©â„“r=\Omega(\ell)italic_r = roman_Î© ( roman_â„“ ), the epoch remains sufficiently long, so that we can still manage to generalize the recourse analysis from Section 2.2. 2.3.2 Challenge II: Getting Rid of the Phases To derive Equation 2, we need to have OPTkâ¢(ğ’«final)â‰¤18â‹…OPTkâ¢(ğ’«init)subscriptOPTğ‘˜subscriptğ’«finalâ‹…18subscriptOPTğ‘˜subscriptğ’«init\textnormal{{OPT}}_{k}(\mathcal{P}_{\textnormal{{final}}})\leq 18\cdot% \textnormal{{OPT}}_{k}(\mathcal{P}_{\textnormal{{init}}})OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT ) â‰¤ 18 â‹… OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ). This is precisely the reason why the algorithm in [FLNS21] works in phases, so that OPTkâ¢(ğ’«)subscriptOPTğ‘˜ğ’«\textnormal{{OPT}}_{k}(\mathcal{P})OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ) increases by at most a Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 ) factor within each phase. Further, the analysis in [FLNS21] crucially relies on showing that we incur O~â¢(n)~ğ‘‚ğ‘›\tilde{O}(n)over~ start_ARG italic_O end_ARG ( italic_n ) total recourse within a phase. This, combined with the fact that there are O~â¢(1)~ğ‘‚1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) phases overall in the incremental setting, implies the amortized recourse guarantee. From the preceding discussion, it becomes apparent that we cannot hope to extend such an argument in the fully dynamic setting, because it is not possible to argue that we have at most O~â¢(1)~ğ‘‚1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) phases when the value of OPTkâ¢(ğ’«)subscriptOPTğ‘˜ğ’«\textnormal{{OPT}}_{k}(\mathcal{P})OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ) can fluctuate in either direction (go up or down) over a sequence of fully dynamic updates. To circumvent this obstacle, we make the following subtle but important change to the framework of [FLNS21]. Recall Equation 3. Note that we can find the set ğ’²â‹†superscriptğ’²â‹†\mathcal{W}^{\star}caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT by solving the following computational task:666For now, we ignore any consideration about keeping the update time of our algorithm low, or even polynomial. Compute the set ğ’²â‹†superscriptğ’²â‹†\mathcal{W}^{\star}caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT of kğ‘˜kitalic_k centers, which minimizes the kğ‘˜kitalic_k-median objective w.r.t. ğ’«finalsubscriptğ’«final\mathcal{P}_{\textnormal{{final}}}caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT, subject to the constraint that it can be obtained by adding/removing Î˜â¢(â„“+1)Î˜â„“1\Theta(\ell+1)roman_Î˜ ( roman_â„“ + 1 ) points in ğ’°initsubscriptğ’°init\mathcal{U}_{\textnormal{{init}}}caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT. Informally, in our algorithm, we replace this task by three separate (and new) tasks, which we perform one after another (see Step 4 in Section 5.1). â€¢ Task (i). Find a set of k+Î˜â¢(â„“+1)ğ‘˜Î˜â„“1k+\Theta(\ell+1)italic_k + roman_Î˜ ( roman_â„“ + 1 ) centers ğ’°â‹†superscriptğ’°â‹†\mathcal{U}^{\star}caligraphic_U start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT, which minimizes the (k+â„“+1)ğ‘˜â„“1(k+\ell+1)( italic_k + roman_â„“ + 1 )-median objective w.r.t. ğ’«initsubscriptğ’«init\mathcal{P}_{\textnormal{{init}}}caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT, such that ğ’°â‹†superscriptğ’°â‹†\mathcal{U}^{\star}caligraphic_U start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT is obtained by adding Î˜â¢(â„“+1)Î˜â„“1\Theta(\ell+1)roman_Î˜ ( roman_â„“ + 1 ) many centers to ğ’°initsubscriptğ’°init\mathcal{U}_{\textnormal{{init}}}caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT. â€¢ Task (ii). Set ğ’±â‹†â†ğ’°â‹†âˆª(ğ’«finalâˆ–ğ’«init)â†superscriptğ’±â‹†superscriptğ’°â‹†subscriptğ’«finalsubscriptğ’«init\mathcal{V}^{\star}\leftarrow\mathcal{U}^{\star}\cup(\mathcal{P}_{\textnormal{% {final}}}\setminus\mathcal{P}_{\textnormal{{init}}})caligraphic_V start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT â† caligraphic_U start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT âˆª ( caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT âˆ– caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT ). â€¢ Task (iii). Find a set of kğ‘˜kitalic_k centers ğ’²â‹†superscriptğ’²â‹†\mathcal{W}^{\star}caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT, which minimizes the kğ‘˜kitalic_k-median objective w.r.t. ğ’«finalsubscriptğ’«final\mathcal{P}_{\textnormal{{final}}}caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT, such that ğ’²â‹†superscriptğ’²â‹†\mathcal{W}^{\star}caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT is obtained by removing Î˜â¢(â„“+1)Î˜â„“1\Theta(\ell+1)roman_Î˜ ( roman_â„“ + 1 ) many centers from ğ’±â‹†superscriptğ’±â‹†\mathcal{V}^{\star}caligraphic_V start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT. Thus, while adding centers in Task (i), we optimize the (k+â„“+1)ğ‘˜â„“1(k+\ell+1)( italic_k + roman_â„“ + 1 )-median objective w.r.t. ğ’«initsubscriptğ’«init\mathcal{P}_{\textnormal{{init}}}caligraphic_P start_POSTSUBSCRIPT init end_POSTSUBSCRIPT. On the other hand, while removing centers in Task (iii), we optimize the kğ‘˜kitalic_k-median objective w.r.t. ğ’«finalsubscriptğ’«final\mathcal{P}_{\textnormal{{final}}}caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT. This is in sharp contrast to the approach in [FLNS21], where we need to optimize the kğ‘˜kitalic_k-median objective w.r.t. ğ’«finalsubscriptğ’«final\mathcal{P}_{\textnormal{{final}}}caligraphic_P start_POSTSUBSCRIPT final end_POSTSUBSCRIPT, both while adding centers and while removing centers. Strikingly, we show that this modification allows us to get rid of the concept of phases altogether. In particular, our algorithm can be cast in the classical periodic recomputation framework: We work in epochs. Within an epoch we handle the updates lazily, and at the end of the epoch we reinitialize our maintained solution so that we get ready to handle the next epoch. See Section 5.3 for details. 2.3.3 Challenge III: Achieving Fast Update Time As mentioned previously, the update time of the algorithm in [FLNS21] is prohibitively large. This occurs because of the following computationally expensive steps at the start and at the end of an epoch.777Note that it is straightforward to lazily handle the updates within the epoch. (1) At the start of an epoch, [FLNS21] computes the value of â„“â„“\ellroman_â„“, by solving an LP for the (kâˆ’s)ğ‘˜ğ‘ (k-s)( italic_k - italic_s )-median problem with potential centers ğ’°initsubscriptğ’°init\mathcal{U}_{\textnormal{{init}}}caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT, for each sâˆˆ[0,kâˆ’1]ğ‘ 0ğ‘˜1s\in[0,k-1]italic_s âˆˆ [ 0 , italic_k - 1 ]. (2) Next, to initialize the subset ğ’°âŠ†ğ’°initğ’°subscriptğ’°init\mathcal{U}\subseteq\mathcal{U}_{\textnormal{{init}}}caligraphic_U âŠ† caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT of (kâˆ’â„“)ğ‘˜â„“(k-\ell)( italic_k - roman_â„“ ) centers at the start of the epoch, [FLNS21] again invokes an algorithm for the (kâˆ’â„“)ğ‘˜â„“(k-\ell)( italic_k - roman_â„“ )-median problem from scratch. (3) At the end of the epoch, [FLNS21] solves another LP and applies a rounding procedure, to get an approximation of the desired set ğ’²â‹†superscriptğ’²â‹†\mathcal{W}^{\star}caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT (see Equation 3). (4) Finally, at the end of the epoch, the call to the Robustify subroutine also takes a prohibitively long time for our purpose. In contrast, we take alternative approaches while performing the above steps. At the start of an epoch, we implement Steps (1) and (2) via randomized local search (see Section 2.1.2). For Step (3), we compute (an approximation) of the set ğ’²â‹†superscriptğ’²â‹†\mathcal{W}^{\star}caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT by solving the three tasks outlined in Section 2.3.2. One of our contributions is to design a new algorithm for Task (i) that runs in O~â¢(nâ‹…(â„“+1))~ğ‘‚â‹…ğ‘›â„“1\tilde{O}(n\cdot(\ell+1))over~ start_ARG italic_O end_ARG ( italic_n â‹… ( roman_â„“ + 1 ) ) time, assuming it has access to some auxiliary data structures (see Lemma 6.2). It is trivial to perform Task (ii). For Task (iii), we again invoke the randomized local search procedure (see Section 2.1.2). Finally, for step (4), we need to efficiently implement the calls to Robustifyâ¢(â‹…)Robustifyâ‹…\textsc{Robustify}(\cdot)Robustify ( â‹… ). See Section 6.2.1 for a more detailed discussion on this challenge, and how we overcome it (we defer the discussion to Section 6.2.1 because it requires an understanding of the inner workings of the Robustifyâ¢(â‹…)Robustifyâ‹…\textsc{Robustify}(\cdot)Robustify ( â‹… ) subroutine, which we have not described until now). 3 Preliminaries We now define some basic notations, and recall some relevant results from the existing literature. For the sake of completeness, we provide self-contained proofs for most of the lemmas stated in this section, but defer those proofs (since we do not take any credit for them) to Section 8. Consider a set of points ğğ\mathbf{P}bold_P and a distance function d:ğÃ—ğâ†’â„+:ğ‘‘â†’ğğsuperscriptâ„d:\mathbf{P}\times\mathbf{P}\rightarrow\mathbb{R}^{+}italic_d : bold_P Ã— bold_P â†’ blackboard_R start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT that together form a metric space. The input to our dynamic algorithm is a subset ğ’«âŠ†ğğ’«ğ\mathcal{P}\subseteq\mathbf{P}caligraphic_P âŠ† bold_P, which changes by means of updates. Let nğ‘›nitalic_n be an upper bound on the maximum size of ğ’«ğ’«\mathcal{P}caligraphic_P throughout these updates. Each update either inserts a point pâˆˆğâˆ–ğ’«ğ‘ğğ’«p\in\mathbf{P}\setminus\mathcal{P}italic_p âˆˆ bold_P âˆ– caligraphic_P into ğ’«ğ’«\mathcal{P}caligraphic_P, or deletes a point pâˆˆğ’«ğ‘ğ’«p\in\mathcal{P}italic_p âˆˆ caligraphic_P from ğ’«ğ’«\mathcal{P}caligraphic_P. At all times, we have to maintain a set ğ’°âŠ†ğğ’°ğ\mathcal{U}\subseteq\mathbf{P}caligraphic_U âŠ† bold_P of at most kğ‘˜kitalic_k â€œcentersâ€, so as to minimize the objective function Costâ¢(ğ’°,ğ’«):=âˆ‘pâˆˆğ’«dâ¢(p,ğ’°), where â¢dâ¢(p,ğ’°):=minqâˆˆğ’°â¡dâ¢(p,q)â¢ is the distance from â¢pâ¢ to the set â¢ğ’°.formulae-sequenceassignCostğ’°ğ’«subscriptğ‘ğ’«ğ‘‘ğ‘ğ’°assign where ğ‘‘ğ‘ğ’°subscriptğ‘ğ’°ğ‘‘ğ‘ğ‘ is the distance from ğ‘ to the set ğ’°\textnormal{{Cost}}\left(\mathcal{U},\mathcal{P}\right):=\sum_{p\in\mathcal{P}% }d(p,\mathcal{U}),\text{ where }d(p,\mathcal{U}):=\min_{q\in\mathcal{U}}d(p,q)% \text{ is the distance from }p\text{ to the set }\mathcal{U}.Cost ( caligraphic_U , caligraphic_P ) := âˆ‘ start_POSTSUBSCRIPT italic_p âˆˆ caligraphic_P end_POSTSUBSCRIPT italic_d ( italic_p , caligraphic_U ) , where italic_d ( italic_p , caligraphic_U ) := roman_min start_POSTSUBSCRIPT italic_q âˆˆ caligraphic_U end_POSTSUBSCRIPT italic_d ( italic_p , italic_q ) is the distance from italic_p to the set caligraphic_U . We will refer to this as the dynamic improper kğ‘˜kitalic_k-median problem. Our goal is to design an algorithm for this problem that has: (1) good approximation ratio, (2) small update time, which is the time it takes to process an update in ğ’«ğ’«\mathcal{P}caligraphic_P, and (3) small recourse, which is the number of changes (point insertions/deletions) in the maintained solution ğ’°ğ’°\mathcal{U}caligraphic_U per update. What makes this setting distinct from the standard kğ‘˜kitalic_k-median problem is this: Here, we are allowed to open centers at locations that are not part of the current input, i.e., we can have ğ’°âˆ©(ğâˆ–ğ’«)â‰ âˆ…ğ’°ğğ’«\mathcal{U}\cap(\mathbf{P}\setminus\mathcal{P})\neq\emptysetcaligraphic_U âˆ© ( bold_P âˆ– caligraphic_P ) â‰  âˆ…. Nevertheless, in a black-box manner we can convert any dynamic algorithm for improper kğ‘˜kitalic_k-median into a dynamic algorithm for kğ‘˜kitalic_k-median, with essentially the same guarantees (see Lemma 3.1). Accordingly, for the rest of this paper, we focus on designing a dynamic algorithm for improper kğ‘˜kitalic_k-median. Lemma 3.1 ([BCG+24]). Given an Î±ğ›¼\alphaitalic_Î±-approximation algorithm for dynamic improper kğ‘˜kitalic_k-median, we can get a 2â¢Î±2ğ›¼2\alpha2 italic_Î±-approximation algorithm for dynamic kğ‘˜kitalic_k-median, with an extra Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 ) multiplicative factor overhead in the recourse, and an extra O~â¢(n)~ğ‘‚ğ‘›\tilde{O}(n)over~ start_ARG italic_O end_ARG ( italic_n ) additive factor overhead in the update time. Remark. At this point, the reader might get alarmed by the fact that Lemma 3.1 incurs an additive overhead of O~â¢(n)~ğ‘‚ğ‘›\tilde{O}(n)over~ start_ARG italic_O end_ARG ( italic_n ) update time. To assuage this concern, in Section 6.3, we explain how to bring down the update time from O~â¢(n)~ğ‘‚ğ‘›\tilde{O}(n)over~ start_ARG italic_O end_ARG ( italic_n ) to O~â¢(k)~ğ‘‚ğ‘˜\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ), using standard sparsification techniques. 3.1 Basic Notations By a simple scaling, we can assume that all of the distances in the metric space lie in the range [1,Î”]1Î”[1,\Delta][ 1 , roman_Î” ], where Î”Î”\Deltaroman_Î” is the aspect ratio. Throughout the paper, we use the symbol ğğ\mathbf{P}bold_P to denote the underlying metric space with distance function d:ğÃ—ğâ†’â„â‰¥0:ğ‘‘â†’ğğsuperscriptâ„absent0d:\mathbf{P}\times\mathbf{P}\rightarrow\mathbb{R}^{\geq 0}italic_d : bold_P Ã— bold_P â†’ blackboard_R start_POSTSUPERSCRIPT â‰¥ 0 end_POSTSUPERSCRIPT, and ğ’«âŠ†ğğ’«ğ\mathcal{P}\subseteq\mathbf{P}caligraphic_P âŠ† bold_P to denote the current input. For simplicity, for each set Sğ‘†Sitalic_S and element pğ‘pitalic_p, we denote Sâˆª{p}ğ‘†ğ‘S\cup\{p\}italic_S âˆª { italic_p } and Sâˆ–{p}ğ‘†ğ‘S\setminus\{p\}italic_S âˆ– { italic_p } by S+pğ‘†ğ‘S+pitalic_S + italic_p and Sâˆ’pğ‘†ğ‘S-pitalic_S - italic_p respectively. For two sets of points Sğ‘†Sitalic_S and Sâ€²superscriptğ‘†â€²S^{\prime}italic_S start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT, we use SâŠ•Sâ€²direct-sumğ‘†superscriptğ‘†â€²S\oplus S^{\prime}italic_S âŠ• italic_S start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT to denote their symmetric difference. For each SâŠ†ğğ‘†ğS\subseteq\mathbf{P}italic_S âŠ† bold_P, we define Ï€S:ğâ†’S:subscriptğœ‹ğ‘†â†’ğğ‘†\pi_{S}:\mathbf{P}\rightarrow Sitalic_Ï€ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT : bold_P â†’ italic_S to be the projection function onto Sğ‘†Sitalic_S, i.e., Ï€Sâ¢(x):=argâ¡minsâˆˆSâ¡dâ¢(x,s)assignsubscriptğœ‹ğ‘†ğ‘¥subscriptğ‘ ğ‘†ğ‘‘ğ‘¥ğ‘ \pi_{S}(x):=\arg\min_{s\in S}d(x,s)italic_Ï€ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_x ) := roman_arg roman_min start_POSTSUBSCRIPT italic_s âˆˆ italic_S end_POSTSUBSCRIPT italic_d ( italic_x , italic_s ), breaking the ties arbitrarily. For each ğ’°âŠ†ğğ’°ğ\mathcal{U}\subseteq\mathbf{P}caligraphic_U âŠ† bold_P and SâŠ†ğ’«ğ‘†ğ’«S\subseteq\mathcal{P}italic_S âŠ† caligraphic_P, we also define AverageCostâ¢(ğ’°,S):=Costâ¢(ğ’°,S)|S|=âˆ‘pâˆˆSdâ¢(p,ğ’°)|S|=âˆ‘pâˆˆSminqâˆˆğ’°â¡dâ¢(p,q)|S|.assignAverageCostğ’°ğ‘†Costğ’°ğ‘†ğ‘†subscriptğ‘ğ‘†ğ‘‘ğ‘ğ’°ğ‘†subscriptğ‘ğ‘†subscriptğ‘ğ’°ğ‘‘ğ‘ğ‘ğ‘†\textnormal{{AverageCost}}\left(\mathcal{U},S\right):=\frac{\textnormal{{Cost}% }\left(\mathcal{U},S\right)}{|S|}=\frac{\sum_{p\in S}d(p,\mathcal{U})}{|S|}=% \frac{\sum_{p\in S}\min_{q\in\mathcal{U}}d(p,q)}{|S|}.AverageCost ( caligraphic_U , italic_S ) := divide start_ARG Cost ( caligraphic_U , italic_S ) end_ARG start_ARG | italic_S | end_ARG = divide start_ARG âˆ‘ start_POSTSUBSCRIPT italic_p âˆˆ italic_S end_POSTSUBSCRIPT italic_d ( italic_p , caligraphic_U ) end_ARG start_ARG | italic_S | end_ARG = divide start_ARG âˆ‘ start_POSTSUBSCRIPT italic_p âˆˆ italic_S end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT italic_q âˆˆ caligraphic_U end_POSTSUBSCRIPT italic_d ( italic_p , italic_q ) end_ARG start_ARG | italic_S | end_ARG . Consider any subset of points ğ’âŠ†ğğ’ğ\mathcal{C}\subseteq\mathbf{P}caligraphic_C âŠ† bold_P. For every kâ‰¥1ğ‘˜1k\geq 1italic_k â‰¥ 1, we let OPTkğ’â¢(ğ’«)superscriptsubscriptOPTğ‘˜ğ’ğ’«\textnormal{{OPT}}_{k}^{\mathcal{C}}(\mathcal{P})OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT caligraphic_C end_POSTSUPERSCRIPT ( caligraphic_P ) denote the cost of the optimum kğ‘˜kitalic_k-median solution for ğ’«âŠ†ğğ’«ğ\mathcal{P}\subseteq\mathbf{P}caligraphic_P âŠ† bold_P, where we can only open centers from ğ’ğ’\mathcal{C}caligraphic_C. Thus, we have OPTkğ’â¢(ğ’«)=minğ’°âŠ†ğ’,|ğ’°|â‰¤kâ¡Costâ¢(ğ’°,ğ’«).subscriptsuperscriptOPTğ’ğ‘˜ğ’«subscriptformulae-sequenceğ’°ğ’ğ’°ğ‘˜Costğ’°ğ’«\textnormal{{OPT}}^{\mathcal{C}}_{k}(\mathcal{P})=\min\limits_{\begin{subarray% }{c}\mathcal{U}\subseteq\mathcal{C},|\mathcal{U}|\leq k\end{subarray}}% \textnormal{{Cost}}\left(\mathcal{U},\mathcal{P}\right).OPT start_POSTSUPERSCRIPT caligraphic_C end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ) = roman_min start_POSTSUBSCRIPT start_ARG start_ROW start_CELL caligraphic_U âŠ† caligraphic_C , | caligraphic_U | â‰¤ italic_k end_CELL end_ROW end_ARG end_POSTSUBSCRIPT Cost ( caligraphic_U , caligraphic_P ) . When ğ’=ğğ’ğ\mathcal{C}=\mathbf{P}caligraphic_C = bold_P, we slightly abuse the notation and write OPTkâ¢(ğ’«)subscriptOPTğ‘˜ğ’«\textnormal{{OPT}}_{k}(\mathcal{P})OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ) instead of OPTkğ’â¢(ğ’«)superscriptsubscriptOPTğ‘˜ğ’ğ’«\textnormal{{OPT}}_{k}^{\mathcal{C}}(\mathcal{P})OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT caligraphic_C end_POSTSUPERSCRIPT ( caligraphic_P ). Next, for each ğ’°âŠ†ğğ’°ğ\mathcal{U}\subseteq\mathbf{P}caligraphic_U âŠ† bold_P and uâˆˆğ’°ğ‘¢ğ’°u\in\mathcal{U}italic_u âˆˆ caligraphic_U, we define Cuâ¢(ğ’°,ğ’«):={pâˆˆğ’«âˆ£Ï€ğ’°â¢(p)=u}assignsubscriptğ¶ğ‘¢ğ’°ğ’«conditional-setğ‘ğ’«subscriptğœ‹ğ’°ğ‘ğ‘¢C_{u}(\mathcal{U},\mathcal{P}):=\{p\in\mathcal{P}\mid\pi_{\mathcal{U}}(p)=u\}italic_C start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT ( caligraphic_U , caligraphic_P ) := { italic_p âˆˆ caligraphic_P âˆ£ italic_Ï€ start_POSTSUBSCRIPT caligraphic_U end_POSTSUBSCRIPT ( italic_p ) = italic_u } to be the set of points in ğ’«ğ’«\mathcal{P}caligraphic_P that are â€œassigned toâ€ the center uğ‘¢uitalic_u in the solution ğ’°ğ’°\mathcal{U}caligraphic_U (breaking ties arbitrarily). For each point pâˆˆğğ‘ğp\in\mathbf{P}italic_p âˆˆ bold_P and value râ‰¥0ğ‘Ÿ0r\geq 0italic_r â‰¥ 0, let Ballrğ’«â¢(p):={qâˆˆğ’«âˆ£dâ¢(p,q)â‰¤r}assignsubscriptsuperscriptBallğ’«ğ‘Ÿğ‘conditional-setğ‘ğ’«ğ‘‘ğ‘ğ‘ğ‘Ÿ\text{Ball}^{\mathcal{P}}_{r}(p):=\{q\in\mathcal{P}\mid d(p,q)\leq r\}Ball start_POSTSUPERSCRIPT caligraphic_P end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ( italic_p ) := { italic_q âˆˆ caligraphic_P âˆ£ italic_d ( italic_p , italic_q ) â‰¤ italic_r } denote the ball of radius rğ‘Ÿritalic_r around pğ‘pitalic_p. Note that if pâˆˆğâˆ–ğ’«ğ‘ğğ’«p\in\mathbf{P}\setminus\mathcal{P}italic_p âˆˆ bold_P âˆ– caligraphic_P, then pğ‘pitalic_p itself is not part of the ball Ballrğ’«â¢(p)subscriptsuperscriptBallğ’«ğ‘Ÿğ‘\text{Ball}^{\mathcal{P}}_{r}(p)Ball start_POSTSUPERSCRIPT caligraphic_P end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ( italic_p ). Finally, throughout the paper we use a sufficiently large constant parameter Î³=4000ğ›¾4000\gamma=4000italic_Î³ = 4000. 3.2 Robust Centers We will use the notion of robust centers [FLNS21]. Morally, a point pâˆˆğğ‘ğp\in\mathbf{P}italic_p âˆˆ bold_P is tğ‘¡titalic_t-robust for an integer tâ‰¥1ğ‘¡1t\geq 1italic_t â‰¥ 1 iff it satisfies the following condition for all iâˆˆ[1,t]ğ‘–1ğ‘¡i\in[1,t]italic_i âˆˆ [ 1 , italic_t ]: Let Bi=Ball10iğ’«â¢(p)subscriptğµğ‘–superscriptsubscriptBallsuperscript10ğ‘–ğ’«ğ‘B_{i}=\text{Ball}_{10^{i}}^{\mathcal{P}}(p)italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = Ball start_POSTSUBSCRIPT 10 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT caligraphic_P end_POSTSUPERSCRIPT ( italic_p ), and consider any point qâˆˆğ’«ğ‘ğ’«q\in\mathcal{P}italic_q âˆˆ caligraphic_P with dâ¢(p,q)â‰ª10imuch-less-thanğ‘‘ğ‘ğ‘superscript10ğ‘–d(p,q)\ll 10^{i}italic_d ( italic_p , italic_q ) â‰ª 10 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT, i.e., qğ‘qitalic_q is sufficiently close to pğ‘pitalic_p compared to the radius of Bisubscriptğµğ‘–B_{i}italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. Then Costâ¢(p,S)â‰¤Oâ¢(1)â‹…Costâ¢(q,S)Costğ‘ğ‘†â‹…ğ‘‚1Costğ‘ğ‘†\textnormal{{Cost}}\left(p,S\right)\leq O(1)\cdot\textnormal{{Cost}}\left(q,S\right)Cost ( italic_p , italic_S ) â‰¤ italic_O ( 1 ) â‹… Cost ( italic_q , italic_S ) for all BiâŠ†SâŠ†ğ’«subscriptğµğ‘–ğ‘†ğ’«B_{i}\subseteq S\subseteq\mathcal{P}italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âŠ† italic_S âŠ† caligraphic_P. In words, the point pğ‘pitalic_p is a good approximate 1111-median solution, compared to any other nearby point, at every â€œdistance scaleâ€ up to 10tsuperscript10ğ‘¡10^{t}10 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT. The above definition, however, is too strong, in the sense that there might not exist any tğ‘¡titalic_t-robust point under this definition.888For instance, it might be the case that there are (say) Î»ğœ†\lambdaitalic_Î» many points in Bisubscriptğµğ‘–B_{i}italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, and all of them (except pğ‘pitalic_p) are in the exact same location as qğ‘qitalic_q, i.e., dâ¢(pâ€²,q)=0ğ‘‘superscriptğ‘â€²ğ‘0d(p^{\prime},q)=0italic_d ( italic_p start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT , italic_q ) = 0 for all pâ€²âˆˆBiâˆ’psuperscriptğ‘â€²subscriptğµğ‘–ğ‘p^{\prime}\in B_{i}-pitalic_p start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT âˆˆ italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_p. Then, the condition Costâ¢(p,Bi)â‰¤Oâ¢(1)â‹…Costâ¢(q,Bi)Costğ‘subscriptğµğ‘–â‹…ğ‘‚1Costğ‘subscriptğµğ‘–\textnormal{{Cost}}\left(p,B_{i}\right)\leq O(1)\cdot\textnormal{{Cost}}\left(% q,B_{i}\right)Cost ( italic_p , italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) â‰¤ italic_O ( 1 ) â‹… Cost ( italic_q , italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) clearly does not hold, since the LHS is (Î»âˆ’1)â‹…dâ¢(p,q)â‹…ğœ†1ğ‘‘ğ‘ğ‘(\lambda-1)\cdot d(p,q)( italic_Î» - 1 ) â‹… italic_d ( italic_p , italic_q ), whereas the RHS is only dâ¢(p,q)ğ‘‘ğ‘ğ‘d(p,q)italic_d ( italic_p , italic_q ). Instead, the actual definition that we will use is stated below (see Definition 3.2), along with the relevant properties that follow from it (see Lemma 3.3 and Lemma 3.4). Conceptually, here the key difference from the idealized definition is that the balls {Bi}isubscriptsubscriptğµğ‘–ğ‘–\{B_{i}\}_{i}{ italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT are centered around different points {pi}isubscriptsubscriptğ‘ğ‘–ğ‘–\{p_{i}\}_{i}{ italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, with p0=psubscriptğ‘0ğ‘p_{0}=pitalic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = italic_p and dâ¢(p,pi)â‰ª10imuch-less-thanğ‘‘ğ‘subscriptğ‘ğ‘–superscript10ğ‘–d(p,p_{i})\ll 10^{i}italic_d ( italic_p , italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) â‰ª 10 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT for all iâˆˆ[1,t]ğ‘–1ğ‘¡i\in[1,t]italic_i âˆˆ [ 1 , italic_t ]. Definition 3.2. Let (p0,p1,â€¦,pt)subscriptğ‘0subscriptğ‘1â€¦subscriptğ‘ğ‘¡(p_{0},p_{1},\ldots,p_{t})( italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) be a sequence of t+1ğ‘¡1t+1italic_t + 1 points in ğğ\mathbf{P}bold_P, and let Bi=Ball10iğ’«â¢(pi)subscriptğµğ‘–superscriptsubscriptBallsuperscript10ğ‘–ğ’«subscriptğ‘ğ‘–B_{i}=\text{Ball}_{10^{i}}^{\mathcal{P}}(p_{i})italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = Ball start_POSTSUBSCRIPT 10 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT caligraphic_P end_POSTSUPERSCRIPT ( italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) for each iâˆˆ[0,t]ğ‘–0ğ‘¡i\in[0,t]italic_i âˆˆ [ 0 , italic_t ]. We refer to (p0,p1,â€¦,pt)subscriptğ‘0subscriptğ‘1â€¦subscriptğ‘ğ‘¡(p_{0},p_{1},\ldots,p_{t})( italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) as a tğ‘¡titalic_t-robust sequence iff for every iâˆˆ[1,t]ğ‘–1ğ‘¡i\in[1,t]italic_i âˆˆ [ 1 , italic_t ]: piâˆ’1={piifâ¢AverageCostâ¢(pi,Bi)â‰¥10i/5;qiotherwise, where â¢qi=argâ¡minqâˆˆBi+piâ¡Costâ¢(q,Bi).subscriptğ‘ğ‘–1casessubscriptğ‘ğ‘–ifAverageCostsubscriptğ‘ğ‘–subscriptğµğ‘–superscript10ğ‘–5subscriptğ‘ğ‘–otherwise, where subscriptğ‘ğ‘–subscriptğ‘subscriptğµğ‘–subscriptğ‘ğ‘–Costğ‘subscriptğµğ‘–\displaystyle p_{i-1}=\begin{cases}p_{i}\quad&\text{if}\ \textnormal{{% AverageCost}}\left(p_{i},B_{i}\right)\geq 10^{i}/5;\\ q_{i}\quad&\text{otherwise, where }q_{i}=\arg\min\limits_{q\in B_{i}+p_{i}}% \textnormal{{Cost}}\left(q,B_{i}\right).\end{cases}italic_p start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT = { start_ROW start_CELL italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_CELL start_CELL if AverageCost ( italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) â‰¥ 10 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT / 5 ; end_CELL end_ROW start_ROW start_CELL italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_CELL start_CELL otherwise, where italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = roman_arg roman_min start_POSTSUBSCRIPT italic_q âˆˆ italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT Cost ( italic_q , italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) . end_CELL end_ROW We say that a point pâˆˆğğ‘ğp\in\mathbf{P}italic_p âˆˆ bold_P is tğ‘¡titalic_t-robust iff there exists a tğ‘¡titalic_t-robust sequence (p0,p1,â€¦,pt)subscriptğ‘0subscriptğ‘1â€¦subscriptğ‘ğ‘¡(p_{0},p_{1},\ldots,p_{t})( italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) with p0=psubscriptğ‘0ğ‘p_{0}=pitalic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = italic_p. Lemma 3.3 ([FLNS21]). Let (p0,p1,â€¦,pt)subscriptğ‘0subscriptğ‘1â€¦subscriptğ‘ğ‘¡(p_{0},p_{1},\ldots,p_{t})( italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) be a tğ‘¡titalic_t-robust sequence and let Bi=Ball10iğ’«â¢(pi)subscriptğµğ‘–subscriptsuperscriptBallğ’«superscript10ğ‘–subscriptğ‘ğ‘–B_{i}=\text{Ball}^{\mathcal{P}}_{10^{i}}(p_{i})italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = Ball start_POSTSUPERSCRIPT caligraphic_P end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 10 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) for all iâˆˆ[0,t]ğ‘–0ğ‘¡i\in[0,t]italic_i âˆˆ [ 0 , italic_t ]. Then, for all iâˆˆ[1,t]ğ‘–1ğ‘¡i\in[1,t]italic_i âˆˆ [ 1 , italic_t ], we have dâ¢(piâˆ’1,pi)â‰¤10i/2ğ‘‘subscriptğ‘ğ‘–1subscriptğ‘ğ‘–superscript10ğ‘–2d(p_{i-1},p_{i})\leq 10^{i}/2italic_d ( italic_p start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) â‰¤ 10 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT / 2, Biâˆ’1âŠ†Bisubscriptğµğ‘–1subscriptğµğ‘–B_{i-1}\subseteq B_{i}italic_B start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT âŠ† italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and dâ¢(p0,pi)â‰¤10i/2ğ‘‘subscriptğ‘0subscriptğ‘ğ‘–superscript10ğ‘–2d(p_{0},p_{i})\leq 10^{i}/2italic_d ( italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) â‰¤ 10 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT / 2. Lemma 3.4 ([FLNS21]). Let (p0,p1,â€¦,pt)subscriptğ‘0subscriptğ‘1â€¦subscriptğ‘ğ‘¡(p_{0},p_{1},\ldots,p_{t})( italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) be a tğ‘¡titalic_t-robust sequence and let Bi=Ball10iğ’«â¢(pi)subscriptğµğ‘–subscriptsuperscriptBallğ’«superscript10ğ‘–subscriptğ‘ğ‘–B_{i}=\text{Ball}^{\mathcal{P}}_{10^{i}}(p_{i})italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = Ball start_POSTSUPERSCRIPT caligraphic_P end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 10 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) for all iâˆˆ[0,t]ğ‘–0ğ‘¡i\in[0,t]italic_i âˆˆ [ 0 , italic_t ]. Then, for every iâˆˆ[0,t]ğ‘–0ğ‘¡i\in[0,t]italic_i âˆˆ [ 0 , italic_t ] and every BiâŠ†SâŠ†ğ’«subscriptğµğ‘–ğ‘†ğ’«B_{i}\subseteq S\subseteq\mathcal{P}italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âŠ† italic_S âŠ† caligraphic_P, we have Costâ¢(p0,S)â‰¤(3/2)â‹…Costâ¢(pi,S)Costsubscriptğ‘0ğ‘†â‹…32Costsubscriptğ‘ğ‘–ğ‘†\textnormal{{Cost}}\left(p_{0},S\right)\leq(3/2)\cdot\textnormal{{Cost}}\left(% p_{i},S\right)Cost ( italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_S ) â‰¤ ( 3 / 2 ) â‹… Cost ( italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_S ). We next define the concept of a robust collection of centers. Definition 3.5. A set of centers ğ’²âŠ†ğğ’²ğ\mathcal{W}\subseteq\mathbf{P}caligraphic_W âŠ† bold_P is robust iff the following holds for every wâˆˆğ’²ğ‘¤ğ’²w\in\mathcal{W}italic_w âˆˆ caligraphic_W: wâ¢isâ¢tâ¢-robust, whereâ¢tâ¢is the smallest integer satisfyingâ¢ 10tâ‰¥dâ¢(w,ğ’²âˆ’w)/200.ğ‘¤isğ‘¡-robust, whereğ‘¡is the smallest integer satisfyingsuperscript10ğ‘¡ğ‘‘ğ‘¤ğ’²ğ‘¤200w\ \text{is}\ t\text{-robust, where}\ t\ \text{is the smallest integer % satisfying}\ 10^{t}\geq d(w,\mathcal{W}-w)/200.italic_w is italic_t -robust, where italic_t is the smallest integer satisfying 10 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT â‰¥ italic_d ( italic_w , caligraphic_W - italic_w ) / 200 . (4) Suppose that we have a set of centers ğ’²âŠ†ğğ’²ğ\mathcal{W}\subseteq\mathbf{P}caligraphic_W âŠ† bold_P that is not robust. A natural way to convert them into a robust set of centers is to call the subroutine Robustify(ğ’²)ğ’²(\mathcal{W})( caligraphic_W ), as described below. 1 while there exist a wâˆˆğ’²ğ‘¤ğ’²w\in\mathcal{W}italic_w âˆˆ caligraphic_W violating (4) do 2 tâ†â†ğ‘¡absentt\leftarrowitalic_t â† Smallest integer satisfying 10tâ‰¥dâ¢(w,ğ’²âˆ’w)/100superscript10ğ‘¡ğ‘‘ğ‘¤ğ’²ğ‘¤10010^{t}\geq d(w,\mathcal{W}-w)/10010 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT â‰¥ italic_d ( italic_w , caligraphic_W - italic_w ) / 100. 3 w0â†â†subscriptğ‘¤0absentw_{0}\leftarrowitalic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT â† Make-Robust(w,t)ğ‘¤ğ‘¡(w,t)( italic_w , italic_t ). 4 ğ’²â†ğ’²âˆ’w+w0â†ğ’²ğ’²ğ‘¤subscriptğ‘¤0\mathcal{W}\leftarrow\mathcal{W}-w+w_{0}caligraphic_W â† caligraphic_W - italic_w + italic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. Algorithm 1 Robustify(ğ’²)ğ’²(\mathcal{W})( caligraphic_W ) 1 ptâ†pâ†subscriptğ‘ğ‘¡ğ‘p_{t}\leftarrow pitalic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT â† italic_p. 2for i=tğ‘–ğ‘¡i=titalic_i = italic_t down to 1111 do 3 Biâ†Ball10iğ’«â¢(pi)â†subscriptğµğ‘–superscriptsubscriptBallsuperscript10ğ‘–ğ’«subscriptğ‘ğ‘–B_{i}\leftarrow\text{Ball}_{10^{i}}^{\mathcal{P}}(p_{i})italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT â† Ball start_POSTSUBSCRIPT 10 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT caligraphic_P end_POSTSUPERSCRIPT ( italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ). 4 if AverageCostâ¢(pi,Bi)â‰¥10i/5AverageCostsubscriptğ‘ğ‘–subscriptğµğ‘–superscript10ğ‘–5\textnormal{{AverageCost}}\left(p_{i},B_{i}\right)\geq 10^{i}/5AverageCost ( italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) â‰¥ 10 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT / 5 then 5 piâˆ’1â†piâ†subscriptğ‘ğ‘–1subscriptğ‘ğ‘–p_{i-1}\leftarrow p_{i}italic_p start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT â† italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. 6 else 7 piâˆ’1â†argâ¡minqâˆˆBi+piâ¡Costâ¢(q,Bi)â†subscriptğ‘ğ‘–1subscriptğ‘subscriptğµğ‘–subscriptğ‘ğ‘–Costğ‘subscriptğµğ‘–p_{i-1}\leftarrow\arg\min\limits_{q\in B_{i}+p_{i}}\textnormal{{Cost}}\left(q,% B_{i}\right)italic_p start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT â† roman_arg roman_min start_POSTSUBSCRIPT italic_q âˆˆ italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT Cost ( italic_q , italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ). return p0subscriptğ‘0p_{0}italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. Algorithm 2 Make-Robust(p,t)ğ‘ğ‘¡(p,t)( italic_p , italic_t ) During a call to Make-Robust(p,t)ğ‘ğ‘¡(p,t)( italic_p , italic_t ), we simply apply the rule from Definition 3.2 to obtain a tğ‘¡titalic_t-robust sequence (p0,p1,â€¦,pt)subscriptğ‘0subscriptğ‘1â€¦subscriptğ‘ğ‘¡(p_{0},p_{1},\dots,p_{t})( italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) with pt=psubscriptğ‘ğ‘¡ğ‘p_{t}=pitalic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_p, and then return the point p0subscriptğ‘0p_{0}italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. Further, Line 1 of the subroutine Robustify(ğ’²)ğ’²(\mathcal{W})( caligraphic_W ) considers the inequality 10tâ‰¥dâ¢(w,ğ’²âˆ’w)/100superscript10ğ‘¡ğ‘‘ğ‘¤ğ’²ğ‘¤10010^{t}\geq d(w,\mathcal{W}-w)/10010 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT â‰¥ italic_d ( italic_w , caligraphic_W - italic_w ) / 100, whereas (4) refers to the inequality 10tâ‰¥dâ¢(w,ğ’²âˆ’w)/200superscript10ğ‘¡ğ‘‘ğ‘¤ğ’²ğ‘¤20010^{t}\geq d(w,\mathcal{W}-w)/20010 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT â‰¥ italic_d ( italic_w , caligraphic_W - italic_w ) / 200. This discrepancy in the constants on the right hand side (100100100100 vs 200200200200) of these two inequalities is intentional, and plays a crucial role in deriving Lemma 3.6. Lemma 3.6 ([FLNS21]). Consider any call to Robustify(ğ’²)ğ’²(\mathcal{W})( caligraphic_W ), and suppose that it sets w0â†Make-Robustâ¢(w,t)â†subscriptğ‘¤0Make-Robustğ‘¤ğ‘¡w_{0}\leftarrow\textsc{Make-Robust}(w,t)italic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT â† Make-Robust ( italic_w , italic_t ) during some iteration of the while loop. Then in subsequent iterations of the while loop in the same call to Robustify(ğ’²)ğ’²(\mathcal{W})( caligraphic_W ), we will not make any call to Make-Robust(w0,â‹…)subscriptğ‘¤0â‹…(w_{0},\cdot)( italic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , â‹… ). Lemma 3.7 ([FLNS21]). If ğ’°ğ’°\mathcal{U}caligraphic_U is the output of Robustify(ğ’²)ğ’²(\mathcal{W})( caligraphic_W ), then Costâ¢(ğ’°,ğ’«)â‰¤32â‹…Costâ¢(ğ’²,ğ’«)Costğ’°ğ’«â‹…32Costğ’²ğ’«\textnormal{{Cost}}\left(\mathcal{U},\mathcal{P}\right)\leq\frac{3}{2}\cdot% \textnormal{{Cost}}\left(\mathcal{W},\mathcal{P}\right)Cost ( caligraphic_U , caligraphic_P ) â‰¤ divide start_ARG 3 end_ARG start_ARG 2 end_ARG â‹… Cost ( caligraphic_W , caligraphic_P ). 3.3 Well-Separated Pairs We will also use the notion of a well-separated pair of points [FLNS21], defined as follows. Definition 3.8. Consider any ğ’°,ğ’±âŠ†ğğ’°ğ’±ğ\mathcal{U},\mathcal{V}\subseteq\mathbf{P}caligraphic_U , caligraphic_V âŠ† bold_P. A pair (u,v)âˆˆğ’°Ã—ğ’±ğ‘¢ğ‘£ğ’°ğ’±(u,v)\in\mathcal{U}\times\mathcal{V}( italic_u , italic_v ) âˆˆ caligraphic_U Ã— caligraphic_V is well-separated w.r.t. (ğ’°,ğ’±)ğ’°ğ’±(\mathcal{U},\mathcal{V})( caligraphic_U , caligraphic_V ) iff dâ¢(u,ğ’°âˆ’u)â‰¥Î³â‹…dâ¢(u,v)â¢ and â¢dâ¢(v,ğ’±âˆ’v)â‰¥Î³â‹…dâ¢(u,v).ğ‘‘ğ‘¢ğ’°ğ‘¢â‹…ğ›¾ğ‘‘ğ‘¢ğ‘£ and ğ‘‘ğ‘£ğ’±ğ‘£â‹…ğ›¾ğ‘‘ğ‘¢ğ‘£d(u,\mathcal{U}-u)\geq\gamma\cdot d(u,v)\text{ and }d(v,\mathcal{V}-v)\geq% \gamma\cdot d(u,v).italic_d ( italic_u , caligraphic_U - italic_u ) â‰¥ italic_Î³ â‹… italic_d ( italic_u , italic_v ) and italic_d ( italic_v , caligraphic_V - italic_v ) â‰¥ italic_Î³ â‹… italic_d ( italic_u , italic_v ) . Using triangle inequality, it is easy to verify that each point uâˆˆğ’°ğ‘¢ğ’°u\in\mathcal{U}italic_u âˆˆ caligraphic_U either forms a well-separated pair with a unique vâˆˆğ’±ğ‘£ğ’±v\in\mathcal{V}italic_v âˆˆ caligraphic_V, or it does not form a well-separated pair with any vâˆˆğ’±ğ‘£ğ’±v\in\mathcal{V}italic_v âˆˆ caligraphic_V. The next lemma implies that if ğ’°ğ’°\mathcal{U}caligraphic_U is robust, then we can replace every center vâˆˆğ’±ğ‘£ğ’±v\in\mathcal{V}italic_v âˆˆ caligraphic_V that is well-separated by its counterpart in ğ’°ğ’°\mathcal{U}caligraphic_U, and this will increase the cost of the solution ğ’±ğ’±\mathcal{V}caligraphic_V by at most a constant factor. Lemma 3.9 ([FLNS21]). Consider any two sets of centers ğ’°,ğ’±âŠ†ğğ’°ğ’±ğ\mathcal{U},\mathcal{V}\subseteq\mathbf{P}caligraphic_U , caligraphic_V âŠ† bold_P such that ğ’°ğ’°\mathcal{U}caligraphic_U is robust. Then, for every well-separated pair (u,v)âˆˆğ’°Ã—ğ’±ğ‘¢ğ‘£ğ’°ğ’±(u,v)\in\mathcal{U}\times\mathcal{V}( italic_u , italic_v ) âˆˆ caligraphic_U Ã— caligraphic_V, we have Costâ¢(u,Cvâ¢(ğ’±,ğ’«))â‰¤3â‹…Costâ¢(v,Cvâ¢(ğ’±,ğ’«))Costğ‘¢subscriptğ¶ğ‘£ğ’±ğ’«â‹…3Costğ‘£subscriptğ¶ğ‘£ğ’±ğ’«\textnormal{{Cost}}\left(u,C_{v}(\mathcal{V},\mathcal{P})\right)\leq 3\cdot% \textnormal{{Cost}}\left(v,C_{v}(\mathcal{V},\mathcal{P})\right)Cost ( italic_u , italic_C start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( caligraphic_V , caligraphic_P ) ) â‰¤ 3 â‹… Cost ( italic_v , italic_C start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( caligraphic_V , caligraphic_P ) ). 3.4 Projection Lemma and Lazy-Updates Lemma We conclude by recalling two lemmas that are folklore in the literature on clustering [BCG+24]. Intuitively, the projection lemma says that if we have a set ğ’°ğ’°\mathcal{U}caligraphic_U of more than kğ‘˜kitalic_k centers, then the cost of the best possible kğ‘˜kitalic_k-median solution, subject to the constraint that all of the kğ‘˜kitalic_k centers must be picked from ğ’°ğ’°\mathcal{U}caligraphic_U, is not too large compared to Costâ¢(ğ’°,ğ’«)Costğ’°ğ’«\textnormal{{Cost}}\left(\mathcal{U},\mathcal{P}\right)Cost ( caligraphic_U , caligraphic_P ). Lemma 3.10 (Projection Lemma [BCG+24]). Consider any set of centers ğ’°âŠ†ğğ’°ğ\mathcal{U}\subseteq\mathbf{P}caligraphic_U âŠ† bold_P of size |ğ’°|â‰¥kğ’°ğ‘˜|\mathcal{U}|\geq k| caligraphic_U | â‰¥ italic_k, where kğ‘˜kitalic_k is a positive integer. Then we have OPTkğ’°â¢(ğ’«)â‰¤Costâ¢(ğ’°,ğ’«)+2â‹…OPTkâ¢(ğ’«)superscriptsubscriptOPTğ‘˜ğ’°ğ’«Costğ’°ğ’«â‹…2subscriptOPTğ‘˜ğ’«\textnormal{{OPT}}_{k}^{\mathcal{U}}(\mathcal{P})\leq\textnormal{{Cost}}\left(% \mathcal{U},\mathcal{P}\right)+2\cdot\textnormal{{OPT}}_{k}(\mathcal{P})OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT caligraphic_U end_POSTSUPERSCRIPT ( caligraphic_P ) â‰¤ Cost ( caligraphic_U , caligraphic_P ) + 2 â‹… OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ). The lazy updates lemma, stated below, is derived from the following observation. Suppose that whenever a new point gets inserted into ğ’«ğ’«\mathcal{P}caligraphic_P, we create a center at the position of the newly inserted point; and whenever a point gets deleted from ğ’«ğ’«\mathcal{P}caligraphic_P, we do not make any changes to the set of centers. Then this lazy rule for handling updates ensures that the cost of the solution we maintain does not increase over time (although the solution might consist of more than kğ‘˜kitalic_k centers). Lemma 3.11 (Lazy-Updates Lemma [BCG+24]). Consider any two sets of input points ğ’«,ğ’«â€²âŠ†ğğ’«superscriptğ’«â€²ğ\mathcal{P},\mathcal{P}^{\prime}\subseteq\mathbf{P}caligraphic_P , caligraphic_P start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT âŠ† bold_P such that |ğ’«âŠ•ğ’«â€²|â‰¤sdirect-sumğ’«superscriptğ’«â€²ğ‘ |\mathcal{P}\oplus\mathcal{P}^{\prime}|\leq s| caligraphic_P âŠ• caligraphic_P start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT | â‰¤ italic_s. Then for every kâ‰¥1ğ‘˜1k\geq 1italic_k â‰¥ 1, we have OPTk+sâ¢(ğ’«â€²)â‰¤OPTkâ¢(ğ’«)subscriptOPTğ‘˜ğ‘ superscriptğ’«â€²subscriptOPTğ‘˜ğ’«\textnormal{{OPT}}_{k+s}(\mathcal{P}^{\prime})\leq\textnormal{{OPT}}_{k}(% \mathcal{P})OPT start_POSTSUBSCRIPT italic_k + italic_s end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) â‰¤ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ). 4 Two Key Lemmas We now state two key lemmas that will be used in the design and analysis of our dynamic algorithm. We defer the formal proofs of these two lemmas to Section 7. Lemma 4.1 (Double-Sided Stability Lemma). Consider any râˆˆ[0,kâˆ’1]ğ‘Ÿ0ğ‘˜1r\in[0,k-1]italic_r âˆˆ [ 0 , italic_k - 1 ] and any Î·â‰¥1ğœ‚1\eta\geq 1italic_Î· â‰¥ 1. If OPTkâˆ’râ¢(ğ’«)â‰¤Î·â‹…OPTkâ¢(ğ’«)subscriptOPTğ‘˜ğ‘Ÿğ’«â‹…ğœ‚subscriptOPTğ‘˜ğ’«\textnormal{{OPT}}_{k-r}(\mathcal{P})\leq\eta\cdot\textnormal{{OPT}}_{k}(% \mathcal{P})OPT start_POSTSUBSCRIPT italic_k - italic_r end_POSTSUBSCRIPT ( caligraphic_P ) â‰¤ italic_Î· â‹… OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ), then we must have OPTkâ¢(ğ’«)â‰¤4â‹…OPTk+âŒŠr/(12â¢Î·)âŒ‹â¢(ğ’«)subscriptOPTğ‘˜ğ’«â‹…4subscriptOPTğ‘˜ğ‘Ÿ12ğœ‚ğ’«\textnormal{{OPT}}_{k}(\mathcal{P})\leq 4\cdot\textnormal{{OPT}}_{k+\lfloor r/% (12\eta)\rfloor}(\mathcal{P})OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ) â‰¤ 4 â‹… OPT start_POSTSUBSCRIPT italic_k + âŒŠ italic_r / ( 12 italic_Î· ) âŒ‹ end_POSTSUBSCRIPT ( caligraphic_P ). To interpret Lemma 4.1, first note that OPTkâ¢(ğ’«)subscriptOPTğ‘˜ğ’«\textnormal{{OPT}}_{k}(\mathcal{P})OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ) is a monotonically non-increasing function of kğ‘˜kitalic_k, since the objective value can only drop if we open extra centers. Now, suppose there is a sufficiently large integer râˆˆ[0,kâˆ’1]ğ‘Ÿ0ğ‘˜1r\in[0,k-1]italic_r âˆˆ [ 0 , italic_k - 1 ] such that OPTkâˆ’râ¢(ğ’«)â‰¤Î˜â¢(1)â‹…OPTkâ¢(ğ’«)subscriptOPTğ‘˜ğ‘Ÿğ’«â‹…Î˜1subscriptOPTğ‘˜ğ’«\textnormal{{OPT}}_{k-r}(\mathcal{P})\leq\Theta(1)\cdot\textnormal{{OPT}}_{k}(% \mathcal{P})OPT start_POSTSUBSCRIPT italic_k - italic_r end_POSTSUBSCRIPT ( caligraphic_P ) â‰¤ roman_Î˜ ( 1 ) â‹… OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ). Then, Lemma 4.1 guarantees that OPTkâ¢(ğ’«)â‰¤Î˜â¢(1)â‹…OPTk+râ€²â¢(ğ’«)subscriptOPTğ‘˜ğ’«â‹…Î˜1subscriptOPTğ‘˜superscriptğ‘Ÿâ€²ğ’«\textnormal{{OPT}}_{k}(\mathcal{P})\leq\Theta(1)\cdot\textnormal{{OPT}}_{k+r^{% \prime}}(\mathcal{P})OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ) â‰¤ roman_Î˜ ( 1 ) â‹… OPT start_POSTSUBSCRIPT italic_k + italic_r start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( caligraphic_P ) for some integer râ€²=Î˜â¢(r)superscriptğ‘Ÿâ€²Î˜ğ‘Ÿr^{\prime}=\Theta(r)italic_r start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT = roman_Î˜ ( italic_r ). In other words, if the optimal objective remains stable as we decrease the number of centers by some additive rğ‘Ÿritalic_r, then it also remains stable as we increase the number of centers by roughly the same amount. Conceptually, this holds because of two reasons: (i) the optimal objective of the fractional version of the kğ‘˜kitalic_k-median problem (encoded by its standard LP-relaxation) is convex as a function of kğ‘˜kitalic_k, and (ii) the concerned LP-relaxation has Î˜â¢(1)Î˜1\Theta(1)roman_Î˜ ( 1 )-integrality gap. Lemma 4.2 (Generalization of Lemma 7.3 in the arXiv version of [FLNS21]). Let râ‰¥0ğ‘Ÿ0r\geq 0italic_r â‰¥ 0 and mâˆˆ[0,k]ğ‘š0ğ‘˜m\in[0,k]italic_m âˆˆ [ 0 , italic_k ]. Consider any two sets of centers ğ’°,ğ’±âŠ†ğğ’°ğ’±ğ\mathcal{U},\mathcal{V}\subseteq\mathbf{P}caligraphic_U , caligraphic_V âŠ† bold_P such that |ğ’°|=kğ’°ğ‘˜|\mathcal{U}|=k| caligraphic_U | = italic_k and |ğ’±|=k+rğ’±ğ‘˜ğ‘Ÿ|\mathcal{V}|=k+r| caligraphic_V | = italic_k + italic_r. If the number of well-separated pairs w.r.t. (ğ’°,ğ’±)ğ’°ğ’±(\mathcal{U},\mathcal{V})( caligraphic_U , caligraphic_V ) is kâˆ’mğ‘˜ğ‘šk-mitalic_k - italic_m, then there exists a subset ğ’°~âŠ†ğ’°~ğ’°ğ’°\tilde{\mathcal{U}}\subseteq\mathcal{U}over~ start_ARG caligraphic_U end_ARG âŠ† caligraphic_U of size at most kâˆ’âŒŠ(mâˆ’r)/4âŒ‹ğ‘˜ğ‘šğ‘Ÿ4k-\lfloor(m-r)/4\rflooritalic_k - âŒŠ ( italic_m - italic_r ) / 4 âŒ‹ such that Costâ¢(ğ’°~,ğ’«)â‰¤6â¢Î³â‹…(Costâ¢(ğ’°,ğ’«)+Costâ¢(ğ’±,ğ’«))Cost~ğ’°ğ’«â‹…6ğ›¾Costğ’°ğ’«Costğ’±ğ’«\textnormal{{Cost}}\left(\tilde{\mathcal{U}},\mathcal{P}\right)\leq 6\gamma% \cdot\left(\textnormal{{Cost}}\left(\mathcal{U},\mathcal{P}\right)+\textnormal% {{Cost}}\left(\mathcal{V},\mathcal{P}\right)\right)Cost ( over~ start_ARG caligraphic_U end_ARG , caligraphic_P ) â‰¤ 6 italic_Î³ â‹… ( Cost ( caligraphic_U , caligraphic_P ) + Cost ( caligraphic_V , caligraphic_P ) ). Intuitively, think of ğ’°ğ’°\mathcal{U}caligraphic_U as the kğ‘˜kitalic_k-median solution maintained by our algorithm, and let ğ’±ğ’±\mathcal{V}caligraphic_V be another set of k+rğ‘˜ğ‘Ÿk+ritalic_k + italic_r centers such that Costâ¢(ğ’±,ğ’«)â‰¤Î˜â¢(1)â‹…OPTkâ¢(ğ’«)Costğ’±ğ’«â‹…Î˜1subscriptOPTğ‘˜ğ’«\textnormal{{Cost}}\left(\mathcal{V},\mathcal{P}\right)\leq\Theta(1)\cdot% \textnormal{{OPT}}_{k}(\mathcal{P})Cost ( caligraphic_V , caligraphic_P ) â‰¤ roman_Î˜ ( 1 ) â‹… OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ). The above lemma implies that if mâ‰«rmuch-greater-thanğ‘šğ‘Ÿm\gg ritalic_m â‰« italic_r (i.e., the number of well-separated pairs w.r.t. (ğ’°,ğ’±)ğ’°ğ’±(\mathcal{U},\mathcal{V})( caligraphic_U , caligraphic_V ) is sufficiently small), then we can delete âŒŠ(mâˆ’r)/4âŒ‹=Î©â¢(r)ğ‘šğ‘Ÿ4Î©ğ‘Ÿ\lfloor(m-r)/4\rfloor=\Omega(r)âŒŠ ( italic_m - italic_r ) / 4 âŒ‹ = roman_Î© ( italic_r ) centers from ğ’°ğ’°\mathcal{U}caligraphic_U without significantly increasing the objective Costâ¢(ğ’°,ğ’«)Costğ’°ğ’«\textnormal{{Cost}}\left(\mathcal{U},\mathcal{P}\right)Cost ( caligraphic_U , caligraphic_P ). 5 Achieving Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 ) Approximation Ratio and Oâ¢(log2â¡Î”)ğ‘‚superscript2Î”O(\log^{2}\Delta)italic_O ( roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT roman_Î” ) Recourse In this section, we focus only on achieving good approximation ratio and recourse bounds. We prove the following theorem, without any concern for the update time of the algorithm. In particular, to keep the exposition as simple as possible, we present an algorithm with exponential update time. Theorem 5.1. There is a deterministic Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 )-approximation algorithm for dynamic metric kğ‘˜kitalic_k-median with Oâ¢(log2â¡Î”)ğ‘‚superscript2Î”O(\log^{2}\Delta)italic_O ( roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT roman_Î” ) recourse. 5.1 Description of the Algorithm Our algorithm works in epochs; each epoch lasts for some consecutive updates in ğ’«ğ’«\mathcal{P}caligraphic_P. Let ğ’°âŠ†ğğ’°ğ\mathcal{U}\subseteq\mathbf{P}caligraphic_U âŠ† bold_P denote the maintained solution (set of kğ‘˜kitalic_k centers). We satisfy the following invariant. Invariant 5.2. At the start of an epoch, the set ğ’°ğ’°\mathcal{U}caligraphic_U is robust and Costâ¢(ğ’°,ğ’«)â‰¤8â‹…OPTkâ¢(ğ’«)Costğ’°ğ’«â‹…8subscriptOPTğ‘˜ğ’«\textnormal{{Cost}}\left(\mathcal{U},\mathcal{P}\right)\leq 8\cdot\textnormal{% {OPT}}_{k}(\mathcal{P})Cost ( caligraphic_U , caligraphic_P ) â‰¤ 8 â‹… OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ). We now describe how our dynamic algorithm works in a given epoch, in four steps. Step 1: Determining the length of the epoch. At the start of an epoch, we compute the maximum â„“â‹†â‰¥0superscriptâ„“â‹†0\ell^{\star}\geq 0roman_â„“ start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT â‰¥ 0 such that OPTkâˆ’â„“â‹†â¢(ğ’«)â‰¤54â¢Î³â‹…OPTkâ¢(ğ’«)subscriptOPTğ‘˜superscriptâ„“â‹†ğ’«â‹…54ğ›¾subscriptOPTğ‘˜ğ’«\textnormal{{OPT}}_{k-\ell^{\star}}(\mathcal{P})\leq 54\gamma\cdot\textnormal{% {OPT}}_{k}(\mathcal{P})OPT start_POSTSUBSCRIPT italic_k - roman_â„“ start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( caligraphic_P ) â‰¤ 54 italic_Î³ â‹… OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ),999Recall Î³ğ›¾\gammaitalic_Î³ from Section 3.1 and set â„“â†âŒŠâ„“â‹†/(12â‹…54â¢Î³)âŒ‹â†â„“superscriptâ„“â‹†â‹…1254ğ›¾\ell\leftarrow\lfloor\ell^{\star}/(12\cdot 54\gamma)\rfloorroman_â„“ â† âŒŠ roman_â„“ start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT / ( 12 â‹… 54 italic_Î³ ) âŒ‹. Since â„“â‰¤â„“â‹†â„“superscriptâ„“â‹†\ell\leq\ell^{\star}roman_â„“ â‰¤ roman_â„“ start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT, it follows that OPTkâˆ’â„“â¢(ğ’«)â‰¤OPTkâˆ’â„“â‹†â¢(ğ’«)subscriptOPTğ‘˜â„“ğ’«subscriptOPTğ‘˜superscriptâ„“â‹†ğ’«\textnormal{{OPT}}_{k-\ell}(\mathcal{P})\leq\textnormal{{OPT}}_{k-\ell^{\star}% }(\mathcal{P})OPT start_POSTSUBSCRIPT italic_k - roman_â„“ end_POSTSUBSCRIPT ( caligraphic_P ) â‰¤ OPT start_POSTSUBSCRIPT italic_k - roman_â„“ start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( caligraphic_P ). Thus, by setting Î·=54â¢Î³ğœ‚54ğ›¾\eta=54\gammaitalic_Î· = 54 italic_Î³ and r=â„“â‹†ğ‘Ÿsuperscriptâ„“â‹†r=\ell^{\star}italic_r = roman_â„“ start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT in Lemma 4.1, at the start of the epoch we have: OPTkâˆ’â„“â¢(ğ’«)54â¢Î³â‰¤OPTkâ¢(ğ’«)â‰¤4â‹…OPTk+â„“â¢(ğ’«).subscriptOPTğ‘˜â„“ğ’«54ğ›¾subscriptOPTğ‘˜ğ’«â‹…4subscriptOPTğ‘˜â„“ğ’«\frac{\textnormal{{OPT}}_{k-\ell}(\mathcal{P})}{54\gamma}\leq\textnormal{{OPT}% }_{k}(\mathcal{P})\leq 4\cdot\textnormal{{OPT}}_{k+\ell}(\mathcal{P}).divide start_ARG OPT start_POSTSUBSCRIPT italic_k - roman_â„“ end_POSTSUBSCRIPT ( caligraphic_P ) end_ARG start_ARG 54 italic_Î³ end_ARG â‰¤ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ) â‰¤ 4 â‹… OPT start_POSTSUBSCRIPT italic_k + roman_â„“ end_POSTSUBSCRIPT ( caligraphic_P ) . (5) The epoch will last for the next â„“+1â„“1\ell+1roman_â„“ + 1 updates.101010Note that we might very well have â„“=0â„“0\ell=0roman_â„“ = 0. From now on, we will use the superscript tâˆˆ[0,â„“+1]ğ‘¡0â„“1t\in[0,\ell+1]italic_t âˆˆ [ 0 , roman_â„“ + 1 ] to denote the status of some object after our algorithm has finished processing the ttâ¢hsuperscriptğ‘¡ğ‘¡â„t^{th}italic_t start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT update in the epoch. For example, at the start of the epoch we have ğ’«=ğ’«(0)ğ’«superscriptğ’«0\mathcal{P}=\mathcal{P}^{(0)}caligraphic_P = caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT. Step 2: Preprocessing at the start of the epoch. Let ğ’°initâ†ğ’°â†subscriptğ’°initğ’°\mathcal{U}_{\textnormal{{init}}}\leftarrow\mathcal{U}caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT â† caligraphic_U be the solution maintained by the algorithm after it finished processing the last update in the previous epoch. Before handling the very first update in the current epoch, we initialize the maintained solution by setting ğ’°(0)â†argâ¡minğ’°â€²âŠ†ğ’°init:|ğ’°â€²|=kâˆ’â„“â¡Costâ¢(ğ’°â€²,ğ’«(0)).â†superscriptğ’°0subscript:superscriptğ’°â€²subscriptğ’°initsuperscriptğ’°â€²ğ‘˜â„“Costsuperscriptğ’°â€²superscriptğ’«0\mathcal{U}^{(0)}\leftarrow\arg\min_{\mathcal{U}^{\prime}\subseteq\mathcal{U}_% {\textnormal{{init}}}\,:\,|\mathcal{U}^{\prime}|=k-\ell}\textnormal{{Cost}}% \left(\mathcal{U}^{\prime},\mathcal{P}^{(0)}\right).caligraphic_U start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT â† roman_arg roman_min start_POSTSUBSCRIPT caligraphic_U start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT âŠ† caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT : | caligraphic_U start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT | = italic_k - roman_â„“ end_POSTSUBSCRIPT Cost ( caligraphic_U start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) . (6) Thus, at this point in time, we have Costâ¢(ğ’°(0),ğ’«(0))=OPTkâˆ’â„“ğ’°initâ¢(ğ’«(0))â‰¤Costâ¢(ğ’°init,ğ’«(0))+2â‹…OPTkâˆ’â„“â¢(ğ’«(0))â‰¤8â‹…OPTkâ¢(ğ’«(0))+2â‹…OPTkâˆ’â„“â¢(ğ’«(0))â‰¤(32+432â¢Î³)â‹…OPTk+â„“â¢(ğ’«(0))Costsuperscriptğ’°0superscriptğ’«0superscriptsubscriptOPTğ‘˜â„“subscriptğ’°initsuperscriptğ’«0Costsubscriptğ’°initsuperscriptğ’«0â‹…2subscriptOPTğ‘˜â„“superscriptğ’«0â‹…8subscriptOPTğ‘˜superscriptğ’«0â‹…2subscriptOPTğ‘˜â„“superscriptğ’«0â‹…32432ğ›¾subscriptOPTğ‘˜â„“superscriptğ’«0\textnormal{{Cost}}\left(\mathcal{U}^{(0)},\mathcal{P}^{(0)}\right)=% \textnormal{{OPT}}_{k-\ell}^{\mathcal{U}_{\textnormal{{init}}}}\left(\mathcal{% P}^{(0)}\right)\leq\textnormal{{Cost}}\left(\mathcal{U}_{\textnormal{{init}}},% \mathcal{P}^{(0)}\right)+2\cdot\textnormal{{OPT}}_{k-\ell}\left(\mathcal{P}^{(% 0)}\right)\leq 8\cdot\textnormal{{OPT}}_{k}\left(\mathcal{P}^{(0)}\right)+2% \cdot\textnormal{{OPT}}_{k-\ell}\left(\mathcal{P}^{(0)}\right)\leq(32+432% \gamma)\cdot\textnormal{{OPT}}_{k+\ell}\left(\mathcal{P}^{(0)}\right)Cost ( caligraphic_U start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) = OPT start_POSTSUBSCRIPT italic_k - roman_â„“ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) â‰¤ Cost ( caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) + 2 â‹… OPT start_POSTSUBSCRIPT italic_k - roman_â„“ end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) â‰¤ 8 â‹… OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) + 2 â‹… OPT start_POSTSUBSCRIPT italic_k - roman_â„“ end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) â‰¤ ( 32 + 432 italic_Î³ ) â‹… OPT start_POSTSUBSCRIPT italic_k + roman_â„“ end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ), where the first inequality follows from Lemma 3.10, the second inequality follows from Invariant 5.2, and the last inequality follows from (5). To summarize, we get: Costâ¢(ğ’°(0),ğ’«(0))â‰¤(32+432â¢Î³)â‹…OPTk+â„“â¢(ğ’«(0))â¢ and â¢|ğ’°(0)|â‰¤kâˆ’â„“.Costsuperscriptğ’°0superscriptğ’«0â‹…32432ğ›¾subscriptOPTğ‘˜â„“superscriptğ’«0 and superscriptğ’°0ğ‘˜â„“\textnormal{{Cost}}\left(\mathcal{U}^{(0)},\mathcal{P}^{(0)}\right)\leq(32+432% \gamma)\cdot\textnormal{{OPT}}_{k+\ell}\left(\mathcal{P}^{(0)}\right)\text{ % and }\left|\mathcal{U}^{(0)}\right|\leq k-\ell.Cost ( caligraphic_U start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) â‰¤ ( 32 + 432 italic_Î³ ) â‹… OPT start_POSTSUBSCRIPT italic_k + roman_â„“ end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) and | caligraphic_U start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT | â‰¤ italic_k - roman_â„“ . (7) In words, before we deal with the very first update in the epoch, the maintained solution ğ’°(0)superscriptğ’°0\mathcal{U}^{(0)}caligraphic_U start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT is a (32+432â¢Î³)=Î˜â¢(1)32432ğ›¾Î˜1(32+432\gamma)=\Theta(1)( 32 + 432 italic_Î³ ) = roman_Î˜ ( 1 )-approximation of OPTk+â„“â¢(ğ’«(0))subscriptOPTğ‘˜â„“superscriptğ’«0\textnormal{{OPT}}_{k+\ell}(\mathcal{P}^{(0)})OPT start_POSTSUBSCRIPT italic_k + roman_â„“ end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ), and consists of at most (kâˆ’â„“)ğ‘˜â„“(k-\ell)( italic_k - roman_â„“ ) centers. Both these properties will be crucially exploited while handling the updates within the epoch. Step 3: Handling the updates within the epoch. Consider the ttâ¢hsuperscriptğ‘¡ğ‘¡â„t^{th}italic_t start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT update in the epoch, for tâˆˆ[1,â„“+1]ğ‘¡1â„“1t\in[1,\ell+1]italic_t âˆˆ [ 1 , roman_â„“ + 1 ]. We handle this update in a lazy manner, as follows. If the update involves the deletion of a point from ğ’«ğ’«\mathcal{P}caligraphic_P, then we do not change our maintained solution, and set ğ’°(t)â†ğ’°(tâˆ’1)â†superscriptğ’°ğ‘¡superscriptğ’°ğ‘¡1\mathcal{U}^{(t)}\leftarrow\mathcal{U}^{(t-1)}caligraphic_U start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT â† caligraphic_U start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT. (The maintained solution remains valid, since we are considering the improper kğ‘˜kitalic_k-median problem). In contrast, if the update involves the insertion of a point pğ‘pitalic_p into ğ’«ğ’«\mathcal{P}caligraphic_P, then we set ğ’°(t)â†ğ’°(tâˆ’1)+pâ†superscriptğ’°ğ‘¡superscriptğ’°ğ‘¡1ğ‘\mathcal{U}^{(t)}\leftarrow\mathcal{U}^{(t-1)}+pcaligraphic_U start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT â† caligraphic_U start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT + italic_p. It is easy to verify that this lazy way of dealing with an update does not increase the objective, and increases the number of centers in the maintained solution by at most one. Thus, we have Costâ¢(ğ’°(t),ğ’«(t))â‰¤Costâ¢(ğ’°(tâˆ’1),ğ’«(tâˆ’1))Costsuperscriptğ’°ğ‘¡superscriptğ’«ğ‘¡Costsuperscriptğ’°ğ‘¡1superscriptğ’«ğ‘¡1\textnormal{{Cost}}\left(\mathcal{U}^{(t)},\mathcal{P}^{(t)}\right)\leq% \textnormal{{Cost}}\left(\mathcal{U}^{(t-1)},\mathcal{P}^{(t-1)}\right)Cost ( caligraphic_U start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT ) â‰¤ Cost ( caligraphic_U start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) and |ğ’°(t)|â‰¤|ğ’°(tâˆ’1)|+1superscriptğ’°ğ‘¡superscriptğ’°ğ‘¡11\left|\mathcal{U}^{(t)}\right|\leq\left|\mathcal{U}^{(t-1)}\right|+1| caligraphic_U start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT | â‰¤ | caligraphic_U start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT | + 1. From (7), we now derive that Costâ¢(ğ’°(t),ğ’«(t))â‰¤Costâ¢(ğ’°(0),ğ’«(0))â¢ and â¢|ğ’°(t)|â‰¤kâˆ’â„“+t, for all â¢tâˆˆ[1,â„“+1].formulae-sequenceCostsuperscriptğ’°ğ‘¡superscriptğ’«ğ‘¡Costsuperscriptğ’°0superscriptğ’«0 and superscriptğ’°ğ‘¡ğ‘˜â„“ğ‘¡ for all ğ‘¡1â„“1\textnormal{{Cost}}\left(\mathcal{U}^{(t)},\mathcal{P}^{(t)}\right)\leq% \textnormal{{Cost}}\left(\mathcal{U}^{(0)},\mathcal{P}^{(0)}\right)\text{ and % }\left|\mathcal{U}^{(t)}\right|\leq k-\ell+t,\text{ for all }t\in[1,\ell+1].Cost ( caligraphic_U start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT ) â‰¤ Cost ( caligraphic_U start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) and | caligraphic_U start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT | â‰¤ italic_k - roman_â„“ + italic_t , for all italic_t âˆˆ [ 1 , roman_â„“ + 1 ] . (8) Step 4: Post-processing at the end of the epoch. By (8), the set ğ’°(t)superscriptğ’°ğ‘¡\mathcal{U}^{(t)}caligraphic_U start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT remains a valid solution for the improper kğ‘˜kitalic_k-median problem, for all tâˆˆ[1,â„“]ğ‘¡1â„“t\in[1,\ell]italic_t âˆˆ [ 1 , roman_â„“ ]. After the very last update in the epoch, however, the set ğ’°(â„“+1)superscriptğ’°â„“1\mathcal{U}^{(\ell+1)}caligraphic_U start_POSTSUPERSCRIPT ( roman_â„“ + 1 ) end_POSTSUPERSCRIPT might have more than kğ‘˜kitalic_k centers. At this point in time, we do some post-processing, and compute another set ğ’°finalâŠ†ğsubscriptğ’°finalğ\mathcal{U}_{\textnormal{{final}}}\subseteq\mathbf{P}caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT âŠ† bold_P of at most kğ‘˜kitalic_k centers (i.e., |ğ’°final|â‰¤ksubscriptğ’°finalğ‘˜|\mathcal{U}_{\textnormal{{final}}}|\leq k| caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT | â‰¤ italic_k) that satisfies Invariant 5.2. We then initiate the next epoch, with ğ’°â†ğ’°finalâ†ğ’°subscriptğ’°final\mathcal{U}\leftarrow\mathcal{U}_{\textnormal{{final}}}caligraphic_U â† caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT being the current solution. The post-processing is done as follows. We first add Oâ¢(â„“+1)ğ‘‚â„“1O(\ell+1)italic_O ( roman_â„“ + 1 ) extra centers to the set ğ’°initsubscriptğ’°init\mathcal{U}_{\textnormal{{init}}}caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT, while minimizing the cost of the resulting solution w.r.t. ğ’«(0)superscriptğ’«0\mathcal{P}^{(0)}caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT. This gives us the set of centers ğ’°â‹†superscriptğ’°â‹†\mathcal{U}^{\star}caligraphic_U start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT. Note that |ğ’°â‹†|=k+Oâ¢(â„“+1)superscriptğ’°â‹†ğ‘˜ğ‘‚â„“1\left|\mathcal{U}^{\star}\right|=k+O(\ell+1)| caligraphic_U start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT | = italic_k + italic_O ( roman_â„“ + 1 ). â„±â‹†â†argâ¡minâ„±âŠ†ğ:|â„±|â‰¤2700â¢Î³â‹…(â„“+1)â¡Costâ¢(ğ’°init+â„±,ğ’«(0)), and â¢ğ’°â‹†â†ğ’°init+â„±â‹†.formulae-sequenceâ†superscriptâ„±â‹†subscript:â„±ğâ„±â‹…2700ğ›¾â„“1Costsubscriptğ’°initâ„±superscriptğ’«0â† and superscriptğ’°â‹†subscriptğ’°initsuperscriptâ„±â‹†\mathcal{F}^{\star}\leftarrow\arg\min\limits_{\begin{subarray}{c}\mathcal{F}% \subseteq\mathbf{P}:|\mathcal{F}|\leq 2700\gamma\cdot(\ell+1)\end{subarray}}% \textnormal{{Cost}}\left(\mathcal{U}_{\textnormal{{init}}}+\mathcal{F},% \mathcal{P}^{(0)}\right),\text{ and }\mathcal{U}^{\star}\leftarrow\mathcal{U}_% {\textnormal{{init}}}+\mathcal{F}^{\star}.caligraphic_F start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT â† roman_arg roman_min start_POSTSUBSCRIPT start_ARG start_ROW start_CELL caligraphic_F âŠ† bold_P : | caligraphic_F | â‰¤ 2700 italic_Î³ â‹… ( roman_â„“ + 1 ) end_CELL end_ROW end_ARG end_POSTSUBSCRIPT Cost ( caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT + caligraphic_F , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) , and caligraphic_U start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT â† caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT + caligraphic_F start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT . (9) We next add the newly inserted points within the epoch to the set of centers, so as to obtain the set ğ’±â‹†superscriptğ’±â‹†\mathcal{V}^{\star}caligraphic_V start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT. Since the epoch lasts for â„“+1â„“1\ell+1roman_â„“ + 1 updates, we have |ğ’±â‹†|=k+Oâ¢(â„“+1)superscriptğ’±â‹†ğ‘˜ğ‘‚â„“1|\mathcal{V}^{\star}|=k+O(\ell+1)| caligraphic_V start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT | = italic_k + italic_O ( roman_â„“ + 1 ). Next, we identify the subset ğ’²â‹†âŠ†ğ’±â‹†superscriptğ’²â‹†superscriptğ’±â‹†\mathcal{W}^{\star}\subseteq\mathcal{V}^{\star}caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT âŠ† caligraphic_V start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT of kğ‘˜kitalic_k centers that minimizes the kğ‘˜kitalic_k-median objective w.r.t. ğ’«(â„“+1)superscriptğ’«â„“1\mathcal{P}^{(\ell+1)}caligraphic_P start_POSTSUPERSCRIPT ( roman_â„“ + 1 ) end_POSTSUPERSCRIPT. ğ’±â‹†â†ğ’°â‹†+(ğ’«(â„“+1)âˆ’ğ’«(0)), and â¢ğ’²â‹†â†argâ¡minğ’²âŠ†ğ’±â‹†:|ğ’²|=kâ¡Costâ¢(ğ’²,ğ’«(â„“+1)).formulae-sequenceâ†superscriptğ’±â‹†superscriptğ’°â‹†superscriptğ’«â„“1superscriptğ’«0â† and superscriptğ’²â‹†subscript:ğ’²superscriptğ’±â‹†ğ’²ğ‘˜Costğ’²superscriptğ’«â„“1\mathcal{V}^{\star}\leftarrow\mathcal{U}^{\star}+\left(\mathcal{P}^{(\ell+1)}-% \mathcal{P}^{(0)}\right),\text{ and }\mathcal{W}^{\star}\leftarrow\arg\min% \limits_{\begin{subarray}{c}\mathcal{W}\subseteq\mathcal{V}^{\star}\,:\,|% \mathcal{W}|=k\end{subarray}}\textnormal{{Cost}}\left(\mathcal{W},\mathcal{P}^% {(\ell+1)}\right).caligraphic_V start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT â† caligraphic_U start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT + ( caligraphic_P start_POSTSUPERSCRIPT ( roman_â„“ + 1 ) end_POSTSUPERSCRIPT - caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) , and caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT â† roman_arg roman_min start_POSTSUBSCRIPT start_ARG start_ROW start_CELL caligraphic_W âŠ† caligraphic_V start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT : | caligraphic_W | = italic_k end_CELL end_ROW end_ARG end_POSTSUBSCRIPT Cost ( caligraphic_W , caligraphic_P start_POSTSUPERSCRIPT ( roman_â„“ + 1 ) end_POSTSUPERSCRIPT ) . (10) Finally, we call Robustify(ğ’²â‹†)superscriptğ’²â‹†(\mathcal{W}^{\star})( caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) and let ğ’°finalsubscriptğ’°final\mathcal{U}_{\textnormal{{final}}}caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT be the set of kğ‘˜kitalic_k centers returned by this subroutine. Before starting the next epoch, we set ğ’°â†ğ’°finalâ†ğ’°subscriptğ’°final\mathcal{U}\leftarrow\mathcal{U}_{\textnormal{{final}}}caligraphic_U â† caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT. ğ’°finalâ†Robustifyâ¢(ğ’²â‹†).â†subscriptğ’°finalRobustifysuperscriptğ’²â‹†\mathcal{U}_{\textnormal{{final}}}\leftarrow\textsc{Robustify}(\mathcal{W}^{% \star}).caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT â† Robustify ( caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) . (11) It is easy to verify that we always maintain a set ğ’°âŠ†ğğ’°ğ\mathcal{U}\subseteq\mathbf{P}caligraphic_U âŠ† bold_P of kğ‘˜kitalic_k centers. In Section 5.2, we show that ğ’°=ğ’°finalğ’°subscriptğ’°final\mathcal{U}=\mathcal{U}_{\textnormal{{final}}}caligraphic_U = caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT satisfies Invariant 5.2 at the end of Step 4, and analyze the approximation ratio of the overall algorithm. Finally, Section 5.3 bounds the recourse of the algorithm. We conclude this section with a corollary that will play an important role in our recourse analysis. Corollary 5.3. We have |ğ’²â‹†âŠ•ğ’°init|=Oâ¢(â„“+1)direct-sumsuperscriptğ’²â‹†subscriptğ’°initğ‘‚â„“1\left|\mathcal{W}^{\star}\oplus\mathcal{U}_{\textnormal{{init}}}\right|=O(\ell% +1)| caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT âŠ• caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT | = italic_O ( roman_â„“ + 1 ). Proof. From (9) and (10), we infer that |ğ’±â‹†âŠ•ğ’°init|â‰¤|ğ’±â‹†âŠ•ğ’°â‹†|+|ğ’°â‹†âŠ•ğ’°init|â‰¤|ğ’«(â„“+1)âˆ’ğ’«(0)|+|â„±â‹†|â‰¤(2700â¢Î³â‹…(â„“+1))+(â„“+1)=Oâ¢(â„“+1)direct-sumsuperscriptğ’±â‹†subscriptğ’°initdirect-sumsuperscriptğ’±â‹†superscriptğ’°â‹†direct-sumsuperscriptğ’°â‹†subscriptğ’°initsuperscriptğ’«â„“1superscriptğ’«0superscriptâ„±â‹†â‹…2700ğ›¾â„“1â„“1ğ‘‚â„“1\left|\mathcal{V}^{\star}\oplus\mathcal{U}_{\textnormal{{init}}}\right|\leq% \left|\mathcal{V}^{\star}\oplus\mathcal{U}^{\star}\right|+\left|\mathcal{U}^{% \star}\oplus\mathcal{U}_{\textnormal{{init}}}\right|\leq\left|\mathcal{P}^{(% \ell+1)}-\mathcal{P}^{(0)}\right|+\left|\mathcal{F}^{\star}\right|\leq(2700% \gamma\cdot(\ell+1))+(\ell+1)=O(\ell+1)| caligraphic_V start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT âŠ• caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT | â‰¤ | caligraphic_V start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT âŠ• caligraphic_U start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT | + | caligraphic_U start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT âŠ• caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT | â‰¤ | caligraphic_P start_POSTSUPERSCRIPT ( roman_â„“ + 1 ) end_POSTSUPERSCRIPT - caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT | + | caligraphic_F start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT | â‰¤ ( 2700 italic_Î³ â‹… ( roman_â„“ + 1 ) ) + ( roman_â„“ + 1 ) = italic_O ( roman_â„“ + 1 ). Next, recall that |ğ’±â‹†|=k+Oâ¢(â„“+1)superscriptğ’±â‹†ğ‘˜ğ‘‚â„“1\left|\mathcal{V}^{\star}\right|=k+O(\ell+1)| caligraphic_V start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT | = italic_k + italic_O ( roman_â„“ + 1 ), and ğ’²â‹†superscriptğ’²â‹†\mathcal{W}^{\star}caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT is a subset of ğ’±â‹†superscriptğ’±â‹†\mathcal{V}^{\star}caligraphic_V start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT of size kğ‘˜kitalic_k. Thus, we get: |ğ’²â‹†âŠ•ğ’°init|â‰¤|ğ’²â‹†âŠ•ğ’±â‹†|+|ğ’±â‹†âŠ•ğ’°init|=Oâ¢(â„“+1)+Oâ¢(â„“+1)=Oâ¢(â„“+1)direct-sumsuperscriptğ’²â‹†subscriptğ’°initdirect-sumsuperscriptğ’²â‹†superscriptğ’±â‹†direct-sumsuperscriptğ’±â‹†subscriptğ’°initğ‘‚â„“1ğ‘‚â„“1ğ‘‚â„“1\left|\mathcal{W}^{\star}\oplus\mathcal{U}_{\textnormal{{init}}}\right|\leq% \left|\mathcal{W}^{\star}\oplus\mathcal{V}^{\star}\right|+\left|\mathcal{V}^{% \star}\oplus\mathcal{U}_{\textnormal{{init}}}\right|=O(\ell+1)+O(\ell+1)=O(% \ell+1)| caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT âŠ• caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT | â‰¤ | caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT âŠ• caligraphic_V start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT | + | caligraphic_V start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT âŠ• caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT | = italic_O ( roman_â„“ + 1 ) + italic_O ( roman_â„“ + 1 ) = italic_O ( roman_â„“ + 1 ). This concludes the proof. âˆ 5.2 Analyzing the Approximation Ratio Consider any tâˆˆ[0,â„“]ğ‘¡0â„“t\in[0,\ell]italic_t âˆˆ [ 0 , roman_â„“ ], and note that |ğ’«(t)âŠ•ğ’«(0)|â‰¤tâ‰¤â„“direct-sumsuperscriptğ’«ğ‘¡superscriptğ’«0ğ‘¡â„“\left|\mathcal{P}^{(t)}\oplus\mathcal{P}^{(0)}\right|\leq t\leq\ell| caligraphic_P start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT âŠ• caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT | â‰¤ italic_t â‰¤ roman_â„“. Thus, by Lemma 3.11, we have: OPTk+â„“â¢(ğ’«(0))â‰¤OPTkâ¢(ğ’«(t)).subscriptOPTğ‘˜â„“superscriptğ’«0subscriptOPTğ‘˜superscriptğ’«ğ‘¡\textnormal{{OPT}}_{k+\ell}\left(\mathcal{P}^{(0)}\right)\leq\textnormal{{OPT}% }_{k}\left(\mathcal{P}^{(t)}\right).OPT start_POSTSUBSCRIPT italic_k + roman_â„“ end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) â‰¤ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT ) . (12) From (7), (8) and (12), we now infer that: Costâ¢(ğ’°(t),ğ’«(t))â‰¤(32+432â¢Î³)â‹…OPTkâ¢(ğ’«(t))â¢ and â¢|ğ’°(t)|â‰¤k, for all â¢tâˆˆ[0,â„“].formulae-sequenceCostsuperscriptğ’°ğ‘¡superscriptğ’«ğ‘¡â‹…32432ğ›¾subscriptOPTğ‘˜superscriptğ’«ğ‘¡ and superscriptğ’°ğ‘¡ğ‘˜ for all ğ‘¡0â„“\textnormal{{Cost}}\left(\mathcal{U}^{(t)},\mathcal{P}^{(t)}\right)\leq(32+432% \gamma)\cdot\textnormal{{OPT}}_{k}\left(\mathcal{P}^{(t)}\right)\text{ and }% \left|\mathcal{U}^{(t)}\right|\leq k,\text{ for all }t\in[0,\ell].Cost ( caligraphic_U start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT ) â‰¤ ( 32 + 432 italic_Î³ ) â‹… OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT ) and | caligraphic_U start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT | â‰¤ italic_k , for all italic_t âˆˆ [ 0 , roman_â„“ ] . (13) In other words, at all times within an epoch, the set ğ’°(t)superscriptğ’°ğ‘¡\mathcal{U}^{(t)}caligraphic_U start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT maintained by our algorithm remains a valid Î˜â¢(1)Î˜1\Theta(1)roman_Î˜ ( 1 )-approximate solution to the improper kğ‘˜kitalic_k-median problem on the current input ğ’«(t)superscriptğ’«ğ‘¡\mathcal{P}^{(t)}caligraphic_P start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT. It remains to show that the algorithm successfully restores Invariant 5.2 when the epoch terminates after the (â„“+1)tâ¢hsuperscriptâ„“1ğ‘¡â„(\ell+1)^{th}( roman_â„“ + 1 ) start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT update. Accordingly, we devote the rest of this section to the proof of Lemma 5.4. Lemma 5.4. At the end of Step 4 in Section 5.1, the set ğ’°=ğ’°finalğ’°subscriptğ’°final\mathcal{U}=\mathcal{U}_{\textnormal{{final}}}caligraphic_U = caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT satisfies Invariant 5.2. The claim below bounds the cost of the solution ğ’°â‹†superscriptğ’°â‹†\mathcal{U}^{\star}caligraphic_U start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT w.r.t. the point-set ğ’«(0)superscriptğ’«0\mathcal{P}^{(0)}caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT. Claim 5.5. We have Costâ¢(ğ’°â‹†,ğ’«(0))â‰¤3â‹…OPTk+â„“+1â¢(ğ’«(0))Costsuperscriptğ’°â‹†superscriptğ’«0â‹…3subscriptOPTğ‘˜â„“1superscriptğ’«0\textnormal{{Cost}}\left(\mathcal{U}^{\star},\mathcal{P}^{(0)}\right)\leq 3% \cdot\textnormal{{OPT}}_{k+\ell+1}\left(\mathcal{P}^{(0)}\right)Cost ( caligraphic_U start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) â‰¤ 3 â‹… OPT start_POSTSUBSCRIPT italic_k + roman_â„“ + 1 end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ). Before proving Claim 5.5, we explain how it implies Lemma 5.4. Towards this end, note that: Costâ¢(ğ’°final,ğ’«(â„“+1))Costsubscriptğ’°finalsuperscriptğ’«â„“1\displaystyle\textnormal{{Cost}}\left(\mathcal{U}_{\textnormal{{final}}},% \mathcal{P}^{(\ell+1)}\right)Cost ( caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( roman_â„“ + 1 ) end_POSTSUPERSCRIPT ) â‰¤\displaystyle\leqâ‰¤ 32â‹…Costâ¢(ğ’²â‹†,ğ’«(â„“+1))=32â‹…OPTkğ’±â‹†â¢(ğ’«(â„“+1))â‹…32Costsuperscriptğ’²â‹†superscriptğ’«â„“1â‹…32superscriptsubscriptOPTğ‘˜superscriptğ’±â‹†superscriptğ’«â„“1\displaystyle\frac{3}{2}\cdot\textnormal{{Cost}}\left(\mathcal{W}^{\star},% \mathcal{P}^{(\ell+1)}\right)=\frac{3}{2}\cdot\textnormal{{OPT}}_{k}^{\mathcal% {V}^{\star}}\left(\mathcal{P}^{(\ell+1)}\right)divide start_ARG 3 end_ARG start_ARG 2 end_ARG â‹… Cost ( caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( roman_â„“ + 1 ) end_POSTSUPERSCRIPT ) = divide start_ARG 3 end_ARG start_ARG 2 end_ARG â‹… OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT caligraphic_V start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( roman_â„“ + 1 ) end_POSTSUPERSCRIPT ) (14) â‰¤\displaystyle\leqâ‰¤ 32â‹…Costâ¢(ğ’±â‹†,ğ’«(â„“+1))+3â‹…OPTkâ¢(ğ’«(â„“+1))â‹…32Costsuperscriptğ’±â‹†superscriptğ’«â„“1â‹…3subscriptOPTğ‘˜superscriptğ’«â„“1\displaystyle\frac{3}{2}\cdot\textnormal{{Cost}}\left(\mathcal{V}^{\star},% \mathcal{P}^{(\ell+1)}\right)+3\cdot\textnormal{{OPT}}_{k}\left(\mathcal{P}^{(% \ell+1)}\right)divide start_ARG 3 end_ARG start_ARG 2 end_ARG â‹… Cost ( caligraphic_V start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( roman_â„“ + 1 ) end_POSTSUPERSCRIPT ) + 3 â‹… OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( roman_â„“ + 1 ) end_POSTSUPERSCRIPT ) (15) â‰¤\displaystyle\leqâ‰¤ 32â‹…Costâ¢(ğ’°â‹†,ğ’«(0))+3â‹…OPTkâ¢(ğ’«(â„“+1))â‹…32Costsuperscriptğ’°â‹†superscriptğ’«0â‹…3subscriptOPTğ‘˜superscriptğ’«â„“1\displaystyle\frac{3}{2}\cdot\textnormal{{Cost}}\left(\mathcal{U}^{\star},% \mathcal{P}^{(0)}\right)+3\cdot\textnormal{{OPT}}_{k}\left(\mathcal{P}^{(\ell+% 1)}\right)divide start_ARG 3 end_ARG start_ARG 2 end_ARG â‹… Cost ( caligraphic_U start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) + 3 â‹… OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( roman_â„“ + 1 ) end_POSTSUPERSCRIPT ) (16) â‰¤\displaystyle\leqâ‰¤ 92â‹…OPTk+â„“+1â¢(ğ’«(0))+3â‹…OPTkâ¢(ğ’«(â„“+1))â‹…92subscriptOPTğ‘˜â„“1superscriptğ’«0â‹…3subscriptOPTğ‘˜superscriptğ’«â„“1\displaystyle\frac{9}{2}\cdot\textnormal{{OPT}}_{k+\ell+1}\left(\mathcal{P}^{(% 0)}\right)+3\cdot\textnormal{{OPT}}_{k}\left(\mathcal{P}^{(\ell+1)}\right)divide start_ARG 9 end_ARG start_ARG 2 end_ARG â‹… OPT start_POSTSUBSCRIPT italic_k + roman_â„“ + 1 end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) + 3 â‹… OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( roman_â„“ + 1 ) end_POSTSUPERSCRIPT ) (17) â‰¤\displaystyle\leqâ‰¤ 92â‹…OPTkâ¢(ğ’«(â„“+1))+3â‹…OPTkâ¢(ğ’«(â„“+1))â‹…92subscriptOPTğ‘˜superscriptğ’«â„“1â‹…3subscriptOPTğ‘˜superscriptğ’«â„“1\displaystyle\frac{9}{2}\cdot\textnormal{{OPT}}_{k}\left(\mathcal{P}^{(\ell+1)% }\right)+3\cdot\textnormal{{OPT}}_{k}\left(\mathcal{P}^{(\ell+1)}\right)divide start_ARG 9 end_ARG start_ARG 2 end_ARG â‹… OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( roman_â„“ + 1 ) end_POSTSUPERSCRIPT ) + 3 â‹… OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( roman_â„“ + 1 ) end_POSTSUPERSCRIPT ) (18) â‰¤\displaystyle\leqâ‰¤ 8â‹…OPTkâ¢(ğ’«(â„“+1)).â‹…8subscriptOPTğ‘˜superscriptğ’«â„“1\displaystyle 8\cdot\textnormal{{OPT}}_{k}\left(\mathcal{P}^{(\ell+1)}\right).8 â‹… OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( roman_â„“ + 1 ) end_POSTSUPERSCRIPT ) . (19) In the above derivation, the first step (14) follows from (10), (11) and Lemma 3.7. The second step (15) follows from Lemma 3.10. The third step (16) follows from (10). The fourth step (17) follows from Claim 5.5. The fifth step (18) follows from Lemma 3.11 and the observation that |ğ’«(â„“+1)âŠ•ğ’«(0)|â‰¤â„“+1direct-sumsuperscriptğ’«â„“1superscriptğ’«0â„“1\left|\mathcal{P}^{(\ell+1)}\oplus\mathcal{P}^{(0)}\right|\leq\ell+1| caligraphic_P start_POSTSUPERSCRIPT ( roman_â„“ + 1 ) end_POSTSUPERSCRIPT âŠ• caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT | â‰¤ roman_â„“ + 1. From (19), we infer that at the start of the next epoch Costâ¢(ğ’°,ğ’«)â‰¤8â‹…OPTkâ¢(ğ’«)Costğ’°ğ’«â‹…8subscriptOPTğ‘˜ğ’«\textnormal{{Cost}}\left(\mathcal{U},\mathcal{P}\right)\leq 8\cdot\textnormal{% {OPT}}_{k}(\mathcal{P})Cost ( caligraphic_U , caligraphic_P ) â‰¤ 8 â‹… OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ), and the set ğ’°ğ’°\mathcal{U}caligraphic_U is robust because of (11). This implies Lemma 5.4. 5.2.1 Proof of Claim 5.5 Let ğ’±âŠ†ğğ’±ğ\mathcal{V}\subseteq\mathbf{P}caligraphic_V âŠ† bold_P be an optimal improper (k+â„“+1)ğ‘˜â„“1(k+\ell+1)( italic_k + roman_â„“ + 1 )-median solution for the point-set ğ’«(0)superscriptğ’«0\mathcal{P}^{(0)}caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT, i.e., |ğ’±|=k+â„“+1ğ’±ğ‘˜â„“1\left|\mathcal{V}\right|=k+\ell+1| caligraphic_V | = italic_k + roman_â„“ + 1 and Costâ¢(ğ’±,ğ’«(0))=OPTk+â„“+1â¢(ğ’«(0))Costğ’±superscriptğ’«0subscriptOPTğ‘˜â„“1superscriptğ’«0\textnormal{{Cost}}\left(\mathcal{V},\mathcal{P}^{(0)}\right)=\textnormal{{OPT% }}_{k+\ell+1}\left(\mathcal{P}^{(0)}\right)Cost ( caligraphic_V , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) = OPT start_POSTSUBSCRIPT italic_k + roman_â„“ + 1 end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ). Let mâˆˆ[0,k]ğ‘š0ğ‘˜m\in[0,k]italic_m âˆˆ [ 0 , italic_k ] be the unique integer such that there are (kâˆ’m)ğ‘˜ğ‘š(k-m)( italic_k - italic_m ) well-separated pairs w.r.t. (ğ’°init,ğ’±)subscriptğ’°initğ’±\left(\mathcal{U}_{\textnormal{{init}}},\mathcal{V}\right)( caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT , caligraphic_V ). Let {(u1,v1),(u2,v2),â€¦,(ukâˆ’m,vkâˆ’m)}âŠ†ğ’°initÃ—ğ’±subscriptğ‘¢1subscriptğ‘£1subscriptğ‘¢2subscriptğ‘£2â€¦subscriptğ‘¢ğ‘˜ğ‘šsubscriptğ‘£ğ‘˜ğ‘šsubscriptğ’°initğ’±\{(u_{1},v_{1}),(u_{2},v_{2}),\ldots,(u_{k-m},v_{k-m})\}\subseteq\mathcal{U}_{% \textnormal{{init}}}\times\mathcal{V}{ ( italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , ( italic_u start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) , â€¦ , ( italic_u start_POSTSUBSCRIPT italic_k - italic_m end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_k - italic_m end_POSTSUBSCRIPT ) } âŠ† caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT Ã— caligraphic_V denote the collection of (kâˆ’m)ğ‘˜ğ‘š(k-m)( italic_k - italic_m ) well-separated pairs w.r.t. (ğ’°init,ğ’±)subscriptğ’°initğ’±\left(\mathcal{U}_{\textnormal{{init}}},\mathcal{V}\right)( caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT , caligraphic_V ). Define the set â„±~:=ğ’±âˆ–{v1,â€¦,vkâˆ’m}assign~â„±ğ’±subscriptğ‘£1â€¦subscriptğ‘£ğ‘˜ğ‘š\tilde{\mathcal{F}}:=\mathcal{V}\setminus\{v_{1},\ldots,v_{k-m}\}over~ start_ARG caligraphic_F end_ARG := caligraphic_V âˆ– { italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_v start_POSTSUBSCRIPT italic_k - italic_m end_POSTSUBSCRIPT }. It is easy to verify that: |â„±~|=|ğ’±|âˆ’(kâˆ’m)=m+â„“+1.~â„±ğ’±ğ‘˜ğ‘šğ‘šâ„“1\left|\tilde{\mathcal{F}}\right|=\left|\mathcal{V}\right|-(k-m)=m+\ell+1.| over~ start_ARG caligraphic_F end_ARG | = | caligraphic_V | - ( italic_k - italic_m ) = italic_m + roman_â„“ + 1 . (20) Claim 5.6. We have mâ‰¤2600â¢Î³â‹…(â„“+1)ğ‘šâ‹…2600ğ›¾â„“1m\leq 2600\gamma\cdot(\ell+1)italic_m â‰¤ 2600 italic_Î³ â‹… ( roman_â„“ + 1 ). Claim 5.7. We have Costâ¢(ğ’°init+â„±~,ğ’«(0))â‰¤3â‹…OPTk+â„“+1â¢(ğ’«(0))Costsubscriptğ’°init~â„±superscriptğ’«0â‹…3subscriptOPTğ‘˜â„“1superscriptğ’«0\textnormal{{Cost}}\left(\mathcal{U}_{\textnormal{{init}}}+\tilde{\mathcal{F}}% ,\mathcal{P}^{(0)}\right)\leq 3\cdot\textnormal{{OPT}}_{k+\ell+1}\left(% \mathcal{P}^{(0)}\right)Cost ( caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT + over~ start_ARG caligraphic_F end_ARG , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) â‰¤ 3 â‹… OPT start_POSTSUBSCRIPT italic_k + roman_â„“ + 1 end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ). By (20), Claim 5.6 and Claim 5.7, there exists a set â„±~âŠ†ğ~â„±ğ\tilde{\mathcal{F}}\subseteq\mathbf{P}over~ start_ARG caligraphic_F end_ARG âŠ† bold_P of m+â„“+1â‰¤2700â¢Î³â‹…(â„“+1)ğ‘šâ„“1â‹…2700ğ›¾â„“1m+\ell+1\leq 2700\gamma\cdot(\ell+1)italic_m + roman_â„“ + 1 â‰¤ 2700 italic_Î³ â‹… ( roman_â„“ + 1 ) centers such that Costâ¢(ğ’°init+â„±~,ğ’«(0))â‰¤3â‹…OPTk+â„“+1â¢(ğ’«(0))Costsubscriptğ’°init~â„±superscriptğ’«0â‹…3subscriptOPTğ‘˜â„“1superscriptğ’«0\textnormal{{Cost}}\left(\mathcal{U}_{\textnormal{{init}}}+\tilde{\mathcal{F}}% ,\mathcal{P}^{(0)}\right)\leq 3\cdot\textnormal{{OPT}}_{k+\ell+1}\left(% \mathcal{P}^{(0)}\right)Cost ( caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT + over~ start_ARG caligraphic_F end_ARG , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) â‰¤ 3 â‹… OPT start_POSTSUBSCRIPT italic_k + roman_â„“ + 1 end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ). Accordingly, from (9), we get Costâ¢(ğ’°init+â„±â‹†,ğ’«(0))â‰¤Costâ¢(ğ’°init+â„±~,ğ’«(0))â‰¤3â‹…OPTk+â„“+1â¢(ğ’«(0))Costsubscriptğ’°initsuperscriptâ„±â‹†superscriptğ’«0Costsubscriptğ’°init~â„±superscriptğ’«0â‹…3subscriptOPTğ‘˜â„“1superscriptğ’«0\textnormal{{Cost}}\left(\mathcal{U}_{\textnormal{{init}}}+\mathcal{F}^{\star}% ,\mathcal{P}^{(0)}\right)\leq\textnormal{{Cost}}\left(\mathcal{U}_{\textnormal% {{init}}}+\tilde{\mathcal{F}},\mathcal{P}^{(0)}\right)\leq 3\cdot\textnormal{{% OPT}}_{k+\ell+1}\left(\mathcal{P}^{(0)}\right)Cost ( caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT + caligraphic_F start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) â‰¤ Cost ( caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT + over~ start_ARG caligraphic_F end_ARG , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) â‰¤ 3 â‹… OPT start_POSTSUBSCRIPT italic_k + roman_â„“ + 1 end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ), which implies Claim 5.5. It now remains to prove Claim 5.6 and Claim 5.7. Proof of Claim 5.6 We apply Lemma 4.2, by setting r=â„“+1ğ‘Ÿâ„“1r=\ell+1italic_r = roman_â„“ + 1, ğ’°=ğ’°initğ’°subscriptğ’°init\mathcal{U}=\mathcal{U}_{\textnormal{{init}}}caligraphic_U = caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT and ğ’«=ğ’«(0)ğ’«superscriptğ’«0\mathcal{P}=\mathcal{P}^{(0)}caligraphic_P = caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT. This implies the existence of a set ğ’°~âŠ†ğ’°init~ğ’°subscriptğ’°init\tilde{\mathcal{U}}\subseteq\mathcal{U}_{\textnormal{{init}}}over~ start_ARG caligraphic_U end_ARG âŠ† caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT of at most (kâˆ’b)ğ‘˜ğ‘(k-b)( italic_k - italic_b ) centers, with b=âŒŠ(mâˆ’â„“âˆ’1)/4âŒ‹ğ‘ğ‘šâ„“14b=\lfloor(m-\ell-1)/4\rflooritalic_b = âŒŠ ( italic_m - roman_â„“ - 1 ) / 4 âŒ‹, such that Costâ¢(ğ’°~,ğ’«(0))Cost~ğ’°superscriptğ’«0\displaystyle\textnormal{{Cost}}\left(\tilde{\mathcal{U}},\mathcal{P}^{(0)}\right)Cost ( over~ start_ARG caligraphic_U end_ARG , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) â‰¤\displaystyle\leqâ‰¤ 6â¢Î³â‹…(Costâ¢(ğ’°init,ğ’«(0))+Costâ¢(ğ’±,ğ’«(0)))â‹…6ğ›¾Costsubscriptğ’°initsuperscriptğ’«0Costğ’±superscriptğ’«0\displaystyle 6\gamma\cdot\left(\textnormal{{Cost}}\left(\mathcal{U}_{% \textnormal{{init}}},\mathcal{P}^{(0)}\right)+\textnormal{{Cost}}\left(% \mathcal{V},\mathcal{P}^{(0)}\right)\right)6 italic_Î³ â‹… ( Cost ( caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) + Cost ( caligraphic_V , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) ) â‰¤\displaystyle\leqâ‰¤ 6â¢Î³â‹…(8â‹…OPTkâ¢(ğ’«(0))+OPTk+â„“+1â¢(ğ’«(0)))â‰¤54â¢Î³â‹…OPTkâ¢(ğ’«(0)).â‹…6ğ›¾â‹…8subscriptOPTğ‘˜superscriptğ’«0subscriptOPTğ‘˜â„“1superscriptğ’«0â‹…54ğ›¾subscriptOPTğ‘˜superscriptğ’«0\displaystyle 6\gamma\cdot\left(8\cdot\textnormal{{OPT}}_{k}\left(\mathcal{P}^% {(0)}\right)+\textnormal{{OPT}}_{k+\ell+1}\left(\mathcal{P}^{(0)}\right)\right% )\leq 54\gamma\cdot\textnormal{{OPT}}_{k}\left(\mathcal{P}^{(0)}\right).6 italic_Î³ â‹… ( 8 â‹… OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) + OPT start_POSTSUBSCRIPT italic_k + roman_â„“ + 1 end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) ) â‰¤ 54 italic_Î³ â‹… OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) . In the above derivation, the second inequality follows from Invariant 5.2, and the last inequality holds because OPTk+â„“+1â¢(ğ’«(0))â‰¤OPTkâ¢(ğ’«(0))subscriptOPTğ‘˜â„“1superscriptğ’«0subscriptOPTğ‘˜superscriptğ’«0\textnormal{{OPT}}_{k+\ell+1}\left(\mathcal{P}^{(0)}\right)\leq\textnormal{{% OPT}}_{k}(\mathcal{P}^{(0)})OPT start_POSTSUBSCRIPT italic_k + roman_â„“ + 1 end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) â‰¤ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ). Since OPTkâˆ’bâ¢(ğ’«(0))â‰¤Costâ¢(ğ’°~,ğ’«(0))subscriptOPTğ‘˜ğ‘superscriptğ’«0Cost~ğ’°superscriptğ’«0\textnormal{{OPT}}_{k-b}\left(\mathcal{P}^{(0)}\right)\leq\textnormal{{Cost}}% \left(\tilde{\mathcal{U}},\mathcal{P}^{(0)}\right)OPT start_POSTSUBSCRIPT italic_k - italic_b end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) â‰¤ Cost ( over~ start_ARG caligraphic_U end_ARG , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ), we get OPTkâˆ’bâ¢(ğ’«(0))â‰¤54â¢Î³â‹…OPTkâ¢(ğ’«(0)).subscriptOPTğ‘˜ğ‘superscriptğ’«0â‹…54ğ›¾subscriptOPTğ‘˜superscriptğ’«0\textnormal{{OPT}}_{k-b}\left(\mathcal{P}^{(0)}\right)\leq 54\gamma\cdot% \textnormal{{OPT}}_{k}\left(\mathcal{P}^{(0)}\right).OPT start_POSTSUBSCRIPT italic_k - italic_b end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) â‰¤ 54 italic_Î³ â‹… OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) . (21) Recall the way we defined â„“,â„“â‹†â„“superscriptâ„“â‹†\ell,\ell^{\star}roman_â„“ , roman_â„“ start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT at Step 1 in Section 5.1. From (21), it follows that bâ‰¤â„“â‹†ğ‘superscriptâ„“â‹†b\leq\ell^{\star}italic_b â‰¤ roman_â„“ start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT. Since bâ‰¥mâˆ’â„“âˆ’14âˆ’1=mâˆ’â„“âˆ’54ğ‘ğ‘šâ„“141ğ‘šâ„“54b\geq\frac{m-\ell-1}{4}-1=\frac{m-\ell-5}{4}italic_b â‰¥ divide start_ARG italic_m - roman_â„“ - 1 end_ARG start_ARG 4 end_ARG - 1 = divide start_ARG italic_m - roman_â„“ - 5 end_ARG start_ARG 4 end_ARG and â„“â‰¥â„“â‹†12â‹…54â¢Î³âˆ’1â„“superscriptâ„“â‹†â‹…1254ğ›¾1\ell\geq\frac{\ell^{\star}}{12\cdot 54\gamma}-1roman_â„“ â‰¥ divide start_ARG roman_â„“ start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT end_ARG start_ARG 12 â‹… 54 italic_Î³ end_ARG - 1, we get mâˆ’â„“âˆ’54â‰¤12â‹…54â¢Î³â‹…(â„“+1), and hence â¢mâ‰¤(2592â¢Î³+1)â¢(â„“+1)+4â‰¤2600â¢Î³â‹…(â„“+1)formulae-sequenceğ‘šâ„“54â‹…â‹…1254ğ›¾â„“1 and hence ğ‘š2592ğ›¾1â„“14â‹…2600ğ›¾â„“1\frac{m-\ell-5}{4}\leq 12\cdot 54\gamma\cdot(\ell+1),\text{ and hence }m\leq(2% 592\gamma+1)(\ell+1)+4\leq 2600\gamma\cdot(\ell+1)divide start_ARG italic_m - roman_â„“ - 5 end_ARG start_ARG 4 end_ARG â‰¤ 12 â‹… 54 italic_Î³ â‹… ( roman_â„“ + 1 ) , and hence italic_m â‰¤ ( 2592 italic_Î³ + 1 ) ( roman_â„“ + 1 ) + 4 â‰¤ 2600 italic_Î³ â‹… ( roman_â„“ + 1 ). This concludes the proof of the claim. Proof of Claim 5.7 We define assignment Ïƒ:ğ’«(0)â†’ğ’°init+â„±~:ğœâ†’superscriptğ’«0subscriptğ’°init~â„±\sigma:\mathcal{P}^{(0)}\rightarrow\mathcal{U}_{\textnormal{{init}}}+\tilde{% \mathcal{F}}italic_Ïƒ : caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT â†’ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT + over~ start_ARG caligraphic_F end_ARG, as follows.111111Recall the notations Cuâ¢(ğ’°,ğ’«)subscriptğ¶ğ‘¢ğ’°ğ’«C_{u}(\mathcal{U},\mathcal{P})italic_C start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT ( caligraphic_U , caligraphic_P ) and Ï€ğ’°â¢(p)subscriptğœ‹ğ’°ğ‘\pi_{\mathcal{U}}(p)italic_Ï€ start_POSTSUBSCRIPT caligraphic_U end_POSTSUBSCRIPT ( italic_p ) from Section 3.1. Consider any point pâˆˆğ’«(0)ğ‘superscriptğ’«0p\in\mathcal{P}^{(0)}italic_p âˆˆ caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT. â€¢ If pâˆˆCviâ¢(ğ’±,ğ’«(0))ğ‘subscriptğ¶subscriptğ‘£ğ‘–ğ’±superscriptğ’«0p\in C_{v_{i}}\left(\mathcal{V},\mathcal{P}^{(0)}\right)italic_p âˆˆ italic_C start_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( caligraphic_V , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) for some iâˆˆ[1,kâˆ’m]ğ‘–1ğ‘˜ğ‘ši\in[1,k-m]italic_i âˆˆ [ 1 , italic_k - italic_m ], then Ïƒâ¢(p):=uiassignğœğ‘subscriptğ‘¢ğ‘–\sigma(p):=u_{i}italic_Ïƒ ( italic_p ) := italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. â€¢ Otherwise, Ïƒâ¢(p):=Ï€ğ’±â¢(p)assignğœğ‘subscriptğœ‹ğ’±ğ‘\sigma(p):=\pi_{\mathcal{V}}(p)italic_Ïƒ ( italic_p ) := italic_Ï€ start_POSTSUBSCRIPT caligraphic_V end_POSTSUBSCRIPT ( italic_p ). In words, for every well-separated pair (ui,vi)âˆˆğ’°initÃ—ğ’±subscriptğ‘¢ğ‘–subscriptğ‘£ğ‘–subscriptğ’°initğ’±(u_{i},v_{i})\in\mathcal{U}_{\textnormal{{init}}}\times\mathcal{V}( italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) âˆˆ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT Ã— caligraphic_V all the points in the cluster of visubscriptğ‘£ğ‘–v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT get reassigned to the center uisubscriptğ‘¢ğ‘–u_{i}italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, and the assignment of all other points remain unchanged (note that their assigned centers are present in ğ’°init+â„±~subscriptğ’°init~â„±\mathcal{U}_{\textnormal{{init}}}+\tilde{\mathcal{F}}caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT + over~ start_ARG caligraphic_F end_ARG as well as ğ’±ğ’±\mathcal{V}caligraphic_V). Now, recall that the set of centers ğ’°initsubscriptğ’°init\mathcal{U}_{\textnormal{{init}}}caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT is robust (see Invariant 5.2). Hence, by applying Lemma 3.9, we infer that Costâ¢(ğ’°init+â„±~,ğ’«(0))â‰¤âˆ‘pâˆˆğ’«(0)dâ¢(p,Ïƒâ¢(p))â‰¤3â‹…Costâ¢(ğ’±,ğ’«(0)).Costsubscriptğ’°init~â„±superscriptğ’«0subscriptğ‘superscriptğ’«0ğ‘‘ğ‘ğœğ‘â‹…3Costğ’±superscriptğ’«0\textnormal{{Cost}}\left(\mathcal{U}_{\textnormal{{init}}}+\tilde{\mathcal{F}}% ,\mathcal{P}^{(0)}\right)\leq\sum_{p\in\mathcal{P}^{(0)}}d(p,\sigma(p))\leq 3% \cdot\textnormal{{Cost}}\left(\mathcal{V},\mathcal{P}^{(0)}\right).Cost ( caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT + over~ start_ARG caligraphic_F end_ARG , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) â‰¤ âˆ‘ start_POSTSUBSCRIPT italic_p âˆˆ caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_d ( italic_p , italic_Ïƒ ( italic_p ) ) â‰¤ 3 â‹… Cost ( caligraphic_V , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) . The claim follows since Costâ¢(ğ’±,ğ’«(0))=OPTk+â„“+1â¢(ğ’«(0))Costğ’±superscriptğ’«0subscriptOPTğ‘˜â„“1superscriptğ’«0\textnormal{{Cost}}\left(\mathcal{V},\mathcal{P}^{(0)}\right)=\textnormal{{OPT% }}_{k+\ell+1}\left(\mathcal{P}^{(0)}\right)Cost ( caligraphic_V , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) = OPT start_POSTSUBSCRIPT italic_k + roman_â„“ + 1 end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) by definition. 5.3 Analyzing the Recourse Recall the description of our algorithm from Section 5.1, and consider a given epoch (say) â„°â„°\mathcal{E}caligraphic_E of length (â„“+1)â„“1(\ell+1)( roman_â„“ + 1 ). The total recourse incurred by the algorithm during this epoch is Râ„°â‰¤|ğ’°initâŠ•ğ’°(0)|+(âˆ‘t=1â„“+1|ğ’°(t)âŠ•ğ’°(tâˆ’1)|)+|ğ’°(â„“+1)âŠ•ğ’°final|.subscriptğ‘…â„°direct-sumsubscriptğ’°initsuperscriptğ’°0superscriptsubscriptğ‘¡1â„“1direct-sumsuperscriptğ’°ğ‘¡superscriptğ’°ğ‘¡1direct-sumsuperscriptğ’°â„“1subscriptğ’°finalR_{\mathcal{E}}\leq\left|\mathcal{U}_{\textnormal{{init}}}\oplus\mathcal{U}^{(% 0)}\right|+\left(\sum_{t=1}^{\ell+1}\left|\mathcal{U}^{(t)}\oplus\mathcal{U}^{% (t-1)}\right|\right)+\left|\mathcal{U}^{(\ell+1)}\oplus\mathcal{U}_{% \textnormal{{final}}}\right|.italic_R start_POSTSUBSCRIPT caligraphic_E end_POSTSUBSCRIPT â‰¤ | caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT âŠ• caligraphic_U start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT | + ( âˆ‘ start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_â„“ + 1 end_POSTSUPERSCRIPT | caligraphic_U start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT âŠ• caligraphic_U start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT | ) + | caligraphic_U start_POSTSUPERSCRIPT ( roman_â„“ + 1 ) end_POSTSUPERSCRIPT âŠ• caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT | . (22) We will now bound each term on the right hand side of (22). Towards this end, recall that ğ’°(0)superscriptğ’°0\mathcal{U}^{(0)}caligraphic_U start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT is obtained by deleting â„“â„“\ellroman_â„“ centers from ğ’°initsubscriptğ’°init\mathcal{U}_{\textnormal{{init}}}caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT, and hence we have |ğ’°initâŠ•ğ’°(0)|=â„“direct-sumsubscriptğ’°initsuperscriptğ’°0â„“\left|\mathcal{U}_{\textnormal{{init}}}\oplus\mathcal{U}^{(0)}\right|=\ell| caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT âŠ• caligraphic_U start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT | = roman_â„“. Next, it is easy to verify that in Step 3 (see Section 5.1) we incur a worst-case recourse of at most one per update. Specifically, we have |ğ’°(t)âŠ•ğ’°(tâˆ’1)|â‰¤1direct-sumsuperscriptğ’°ğ‘¡superscriptğ’°ğ‘¡11\left|\mathcal{U}^{(t)}\oplus\mathcal{U}^{(t-1)}\right|\leq 1| caligraphic_U start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT âŠ• caligraphic_U start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT | â‰¤ 1 for all tâˆˆ[1,â„“+1]ğ‘¡1â„“1t\in[1,\ell+1]italic_t âˆˆ [ 1 , roman_â„“ + 1 ], and hence |ğ’°(â„“+1)âŠ•ğ’°init|â‰¤|ğ’°(â„“+1)âŠ•ğ’°(0)|+|ğ’°(0)âŠ•ğ’°init|â‰¤(â„“+1)+â„“=2â¢â„“+1direct-sumsuperscriptğ’°â„“1subscriptğ’°initdirect-sumsuperscriptğ’°â„“1superscriptğ’°0direct-sumsuperscriptğ’°0subscriptğ’°initâ„“1â„“2â„“1\left|\mathcal{U}^{(\ell+1)}\oplus\mathcal{U}_{\textnormal{{init}}}\right|\leq% \left|\mathcal{U}^{(\ell+1)}\oplus\mathcal{U}^{(0)}\right|+\left|\mathcal{U}^{% (0)}\oplus\mathcal{U}_{\textnormal{{init}}}\right|\leq(\ell+1)+\ell=2\ell+1| caligraphic_U start_POSTSUPERSCRIPT ( roman_â„“ + 1 ) end_POSTSUPERSCRIPT âŠ• caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT | â‰¤ | caligraphic_U start_POSTSUPERSCRIPT ( roman_â„“ + 1 ) end_POSTSUPERSCRIPT âŠ• caligraphic_U start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT | + | caligraphic_U start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT âŠ• caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT | â‰¤ ( roman_â„“ + 1 ) + roman_â„“ = 2 roman_â„“ + 1. Thus, from (22) we get: Râ„°subscriptğ‘…â„°\displaystyle R_{\mathcal{E}}italic_R start_POSTSUBSCRIPT caligraphic_E end_POSTSUBSCRIPT â‰¤\displaystyle\leqâ‰¤ â„“+(â„“+1)+|ğ’°(â„“+1)âŠ•ğ’°final|â„“â„“1direct-sumsuperscriptğ’°â„“1subscriptğ’°final\displaystyle\ell+(\ell+1)+\left|\mathcal{U}^{(\ell+1)}\oplus\mathcal{U}_{% \textnormal{{final}}}\right|roman_â„“ + ( roman_â„“ + 1 ) + | caligraphic_U start_POSTSUPERSCRIPT ( roman_â„“ + 1 ) end_POSTSUPERSCRIPT âŠ• caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT | (23) â‰¤\displaystyle\leqâ‰¤ (2â¢â„“+1)+|ğ’°(â„“+1)âŠ•ğ’°init|+|ğ’°initâŠ•ğ’°final|2â„“1direct-sumsuperscriptğ’°â„“1subscriptğ’°initdirect-sumsubscriptğ’°initsubscriptğ’°final\displaystyle(2\ell+1)+\left|\mathcal{U}^{(\ell+1)}\oplus\mathcal{U}_{% \textnormal{{init}}}\right|+\left|\mathcal{U}_{\textnormal{{init}}}\oplus% \mathcal{U}_{\textnormal{{final}}}\right|( 2 roman_â„“ + 1 ) + | caligraphic_U start_POSTSUPERSCRIPT ( roman_â„“ + 1 ) end_POSTSUPERSCRIPT âŠ• caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT | + | caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT âŠ• caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT | â‰¤\displaystyle\leqâ‰¤ (4â¢â„“+2)+|ğ’°initâŠ•ğ’²â‹†|+|ğ’²â‹†âŠ•ğ’°final|4â„“2direct-sumsubscriptğ’°initsuperscriptğ’²â‹†direct-sumsuperscriptğ’²â‹†subscriptğ’°final\displaystyle(4\ell+2)+\left|\mathcal{U}_{\textnormal{{init}}}\oplus\mathcal{W% }^{\star}\right|+\left|\mathcal{W}^{\star}\oplus\mathcal{U}_{\textnormal{{% final}}}\right|( 4 roman_â„“ + 2 ) + | caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT âŠ• caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT | + | caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT âŠ• caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT | =\displaystyle== Oâ¢(â„“+1)+|ğ’²â‹†âŠ•ğ’°final|.ğ‘‚â„“1direct-sumsuperscriptğ’²â‹†subscriptğ’°final\displaystyle O(\ell+1)+\left|\mathcal{W}^{\star}\oplus\mathcal{U}_{% \textnormal{{final}}}\right|.italic_O ( roman_â„“ + 1 ) + | caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT âŠ• caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT | . In the above derivation, the last step follows from Corollary 5.3. Since the epoch lasts for â„“+1â„“1\ell+1roman_â„“ + 1 updates, the Oâ¢(â„“+1)ğ‘‚â„“1O(\ell+1)italic_O ( roman_â„“ + 1 ) term in the right hand side of (23) contributes an amortized recourse of Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 ). Moreover, the term |ğ’²â‹†âŠ•ğ’°final|direct-sumsuperscriptğ’²â‹†subscriptğ’°final\left|\mathcal{W}^{\star}\oplus\mathcal{U}_{\textnormal{{final}}}\right|| caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT âŠ• caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT | is proportional to the number of Make-Robustâ¢(â‹…,â‹…)Make-Robustâ‹…â‹…\textsc{Make-Robust}(\cdot\,,\cdot)Make-Robust ( â‹… , â‹… ) calls made while computing ğ’°finalâ†Robustifyâ¢(ğ’²â‹†)â†subscriptğ’°finalRobustifysuperscriptğ’²â‹†\mathcal{U}_{\textnormal{{final}}}\leftarrow\textsc{Robustify}\left(\mathcal{W% }^{\star}\right)caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT â† Robustify ( caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ). So, the recourse of our algorithm is dominated by the number of calls made to the Make-Robustâ¢(â‹…,â‹…)Make-Robustâ‹…â‹…\textsc{Make-Robust}(\cdot\,,\cdot)Make-Robust ( â‹… , â‹… ) subroutine, and Lemma 5.8 implies that our algorithm has an amortized recourse of Oâ¢(log2â¡Î”)ğ‘‚superscript2Î”O(\log^{2}\Delta)italic_O ( roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT roman_Î” ). Lemma 5.8. The dynamic algorithm from Section 5.1 makes at most Oâ¢(log2â¡Î”)ğ‘‚superscript2Î”O(\log^{2}\Delta)italic_O ( roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT roman_Î” ) many calls to Make-Robustâ¢(â‹…,â‹…)Make-Robustâ‹…â‹…\textsc{Make-Robust}(\cdot\,,\cdot)Make-Robust ( â‹… , â‹… ), amortized over the entire sequence of updates (spanning multiple epochs). We devote the rest of this section towards proving Lemma 5.8. Contaminated vs Clean Centers. Focus on the scenario at the start of a given epoch (see Section 5.1). By Invariant 5.2, the set ğ’°initsubscriptğ’°init\mathcal{U}_{\textnormal{{init}}}caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT is robust w.r.t. ğ’«(0)superscriptğ’«0\mathcal{P}^{(0)}caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT. For each center uâˆˆğ’°initğ‘¢subscriptğ’°initu\in\mathcal{U}_{\textnormal{{init}}}italic_u âˆˆ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT, we maintain an integer tâ¢[u]ğ‘¡delimited-[]ğ‘¢t[u]italic_t [ italic_u ] such that: (i) uğ‘¢uitalic_u is tâ¢[u]ğ‘¡delimited-[]ğ‘¢t[u]italic_t [ italic_u ] robust w.r.t. ğ’«(0)superscriptğ’«0\mathcal{P}^{(0)}caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT, and (ii) 10tâ¢[u]â‰¥dâ¢(u,ğ’°initâˆ’u)/200superscript10ğ‘¡delimited-[]ğ‘¢ğ‘‘ğ‘¢subscriptğ’°initğ‘¢20010^{t[u]}\geq d(u,\mathcal{U}_{\textnormal{{init}}}-u)/20010 start_POSTSUPERSCRIPT italic_t [ italic_u ] end_POSTSUPERSCRIPT â‰¥ italic_d ( italic_u , caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT - italic_u ) / 200. The existence of tâ¢[u]ğ‘¡delimited-[]ğ‘¢t[u]italic_t [ italic_u ] is guaranteed by Definition 3.5.121212We use tâ¢[u]ğ‘¡delimited-[]ğ‘¢t[u]italic_t [ italic_u ] only for the sake of analyzing recourse. Here, the actual algorithm remains the same as in Section 5.1. But, we make use of these integers in Section 6 to get fast update time. Let (p0â¢(u),p1â¢(u),â€¦,ptâ¢[u]â¢(u))subscriptğ‘0ğ‘¢subscriptğ‘1ğ‘¢â€¦subscriptğ‘ğ‘¡delimited-[]ğ‘¢ğ‘¢(p_{0}(u),p_{1}(u),\ldots,p_{t[u]}(u))( italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( italic_u ) , italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_u ) , â€¦ , italic_p start_POSTSUBSCRIPT italic_t [ italic_u ] end_POSTSUBSCRIPT ( italic_u ) ) be the tâ¢[u]ğ‘¡delimited-[]ğ‘¢t[u]italic_t [ italic_u ]-robust sequence w.r.t. ğ’«(0)superscriptğ’«0\mathcal{P}^{(0)}caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT corresponding to uğ‘¢uitalic_u (i.e., u=p0â¢(u)ğ‘¢subscriptğ‘0ğ‘¢u=p_{0}(u)italic_u = italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( italic_u )), and for each iâˆˆ[1,tâ¢[u]]ğ‘–1ğ‘¡delimited-[]ğ‘¢i\in[1,t[u]]italic_i âˆˆ [ 1 , italic_t [ italic_u ] ], let Biâ¢(u)=Ball10iğ’«(0)â¢(piâ¢(u))subscriptğµğ‘–ğ‘¢superscriptsubscriptBallsuperscript10ğ‘–superscriptğ’«0subscriptğ‘ğ‘–ğ‘¢B_{i}(u)=\text{Ball}_{10^{i}}^{\mathcal{P}^{(0)}}(p_{i}(u))italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_u ) = Ball start_POSTSUBSCRIPT 10 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT ( italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_u ) ). Recall that by Lemma 3.3, we have B1â¢(u)âŠ†B2â¢(u)âŠ†â‹¯âŠ†Btâ¢[u]â¢(u)subscriptğµ1ğ‘¢subscriptğµ2ğ‘¢â‹¯subscriptğµğ‘¡delimited-[]ğ‘¢ğ‘¢B_{1}(u)\subseteq B_{2}(u)\subseteq\cdots\subseteq B_{t[u]}(u)italic_B start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_u ) âŠ† italic_B start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_u ) âŠ† â‹¯ âŠ† italic_B start_POSTSUBSCRIPT italic_t [ italic_u ] end_POSTSUBSCRIPT ( italic_u ). Now, consider any jâˆˆ[1,â„“+1]ğ‘—1â„“1j\in[1,\ell+1]italic_j âˆˆ [ 1 , roman_â„“ + 1 ], and let qjâˆˆğsubscriptğ‘ğ‘—ğq_{j}\in\mathbf{P}italic_q start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT âˆˆ bold_P denote the point being inserted/deleted in ğ’«ğ’«\mathcal{P}caligraphic_P during the jtâ¢hsuperscriptğ‘—ğ‘¡â„j^{th}italic_j start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT update in the epoch, i.e., ğ’«(j)âŠ•ğ’«(jâˆ’1)={qj}direct-sumsuperscriptğ’«ğ‘—superscriptğ’«ğ‘—1subscriptğ‘ğ‘—\mathcal{P}^{(j)}\oplus\mathcal{P}^{(j-1)}=\{q_{j}\}caligraphic_P start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT âŠ• caligraphic_P start_POSTSUPERSCRIPT ( italic_j - 1 ) end_POSTSUPERSCRIPT = { italic_q start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT }. We say that this jtâ¢hsuperscriptğ‘—ğ‘¡â„j^{th}italic_j start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT update contaminates a center uâˆˆğ’°initğ‘¢subscriptğ’°initu\in\mathcal{U}_{\textnormal{{init}}}italic_u âˆˆ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT iff dâ¢(qj,ptâ¢[u]â¢(u))â‰¤10tâ¢[u]ğ‘‘subscriptğ‘ğ‘—subscriptğ‘ğ‘¡delimited-[]ğ‘¢ğ‘¢superscript10ğ‘¡delimited-[]ğ‘¢d(q_{j},p_{t[u]}(u))\leq 10^{t[u]}italic_d ( italic_q start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT italic_t [ italic_u ] end_POSTSUBSCRIPT ( italic_u ) ) â‰¤ 10 start_POSTSUPERSCRIPT italic_t [ italic_u ] end_POSTSUPERSCRIPT. Intuitively, this means that if this jtâ¢hsuperscriptğ‘—ğ‘¡â„j^{th}italic_j start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT update were taken into account while defining the balls {Biâ¢(u)}isubscriptsubscriptğµğ‘–ğ‘¢ğ‘–\{B_{i}(u)\}_{i}{ italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_u ) } start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT at the start of the epoch, then it might have impacted our decision to classify uğ‘¢uitalic_u as being tâ¢[u]ğ‘¡delimited-[]ğ‘¢t[u]italic_t [ italic_u ]-robust at that time. Furthermore, we say that the center uâˆˆğ’°initğ‘¢subscriptğ’°initu\in\mathcal{U}_{\textnormal{{init}}}italic_u âˆˆ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT is clean at the end of the epoch if no update jâˆˆ[1,â„“+1]ğ‘—1â„“1j\in[1,\ell+1]italic_j âˆˆ [ 1 , roman_â„“ + 1 ] contaminated it (i.e. Btâ¢[u]â¢(u)subscriptğµğ‘¡delimited-[]ğ‘¢ğ‘¢B_{t[u]}(u)italic_B start_POSTSUBSCRIPT italic_t [ italic_u ] end_POSTSUBSCRIPT ( italic_u ) and accordingly all of the balls Biâ¢(u)subscriptğµğ‘–ğ‘¢B_{i}(u)italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_u ) remain intact during the updates in this epoch); otherwise we say that the center uğ‘¢uitalic_u is contaminated at the end of the epoch. In the two claims below, we summarize a few key properties of clean and contaminated centers. We defer the proof of Claim 5.10 to Section 5.3.1. Claim 5.9. If uâˆˆğ’°initğ‘¢subscriptğ’°initu\in\mathcal{U}_{\textnormal{{init}}}italic_u âˆˆ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT is clean at the end of the epoch, then uğ‘¢uitalic_u is tâ¢[u]ğ‘¡delimited-[]ğ‘¢t[u]italic_t [ italic_u ]-robust w.r.t. ğ’«(â„“+1)superscriptğ’«â„“1\mathcal{P}^{(\ell+1)}caligraphic_P start_POSTSUPERSCRIPT ( roman_â„“ + 1 ) end_POSTSUPERSCRIPT. Proof. Let uâˆˆğ’°initğ‘¢subscriptğ’°initu\in\mathcal{U}_{\textnormal{{init}}}italic_u âˆˆ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT be a clean center at the end of the epoch. So, during the epoch no point was inserted into/deleted from Btâ¢[u]subscriptğµğ‘¡delimited-[]ğ‘¢B_{t[u]}italic_B start_POSTSUBSCRIPT italic_t [ italic_u ] end_POSTSUBSCRIPT. As B1â¢(u)âŠ†B2â¢(u)âŠ†â‹¯âŠ†Btâ¢[u]â¢(u)subscriptğµ1ğ‘¢subscriptğµ2ğ‘¢â‹¯subscriptğµğ‘¡delimited-[]ğ‘¢ğ‘¢B_{1}(u)\subseteq B_{2}(u)\subseteq\cdots\subseteq B_{t[u]}(u)italic_B start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_u ) âŠ† italic_B start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_u ) âŠ† â‹¯ âŠ† italic_B start_POSTSUBSCRIPT italic_t [ italic_u ] end_POSTSUBSCRIPT ( italic_u ) by Lemma 3.3, this implies that during the epoch no point was inserted into/deleted from any of the balls B1â¢(u),B2â¢(u),â€¦,Btâ¢[u]â¢(u)subscriptğµ1ğ‘¢subscriptğµ2ğ‘¢â€¦subscriptğµğ‘¡delimited-[]ğ‘¢ğ‘¢B_{1}(u),B_{2}(u),\ldots,B_{t[u]}(u)italic_B start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_u ) , italic_B start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_u ) , â€¦ , italic_B start_POSTSUBSCRIPT italic_t [ italic_u ] end_POSTSUBSCRIPT ( italic_u ). Thus, we have Ball10iğ’«(0)â¢(piâ¢(u))=Ball10iğ’«(â„“+1)â¢(piâ¢(u))superscriptsubscriptBallsuperscript10ğ‘–superscriptğ’«0subscriptğ‘ğ‘–ğ‘¢superscriptsubscriptBallsuperscript10ğ‘–superscriptğ’«â„“1subscriptğ‘ğ‘–ğ‘¢\text{Ball}_{10^{i}}^{\mathcal{P}^{(0)}}(p_{i}(u))=\text{Ball}_{10^{i}}^{% \mathcal{P}^{(\ell+1)}}(p_{i}(u))Ball start_POSTSUBSCRIPT 10 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT ( italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_u ) ) = Ball start_POSTSUBSCRIPT 10 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT caligraphic_P start_POSTSUPERSCRIPT ( roman_â„“ + 1 ) end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT ( italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_u ) ) for all iâˆˆ[0,â„“+1]ğ‘–0â„“1i\in[0,\ell+1]italic_i âˆˆ [ 0 , roman_â„“ + 1 ]. The claim now follows from Definition 3.2. âˆ Claim 5.10. Each update during the epoch contaminates at most Oâ¢(logâ¡Î”)ğ‘‚Î”O(\log\Delta)italic_O ( roman_log roman_Î” ) centers in ğ’°initsubscriptğ’°init\mathcal{U}_{\textnormal{{init}}}caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT. At the end of the epoch, we set ğ’°finalâ†Robustifyâ¢(ğ’²â‹†)â†subscriptğ’°finalRobustifysuperscriptğ’²â‹†\mathcal{U}_{\textnormal{{final}}}\leftarrow\textsc{Robustify}\left(\mathcal{W% }^{\star}\right)caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT â† Robustify ( caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ). By Lemma 3.6, the subroutine Robustifyâ¢(ğ’²â‹†)Robustifysuperscriptğ’²â‹†\textsc{Robustify}\left(\mathcal{W}^{\star}\right)Robustify ( caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) makes at most one call to Make-Robustâ¢(w,â‹…)Make-Robustğ‘¤â‹…\textsc{Make-Robust}(w,\cdot)Make-Robust ( italic_w , â‹… ) for each point wâˆˆğ’²â‹†ğ‘¤superscriptğ’²â‹†w\in\mathcal{W}^{\star}italic_w âˆˆ caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT, and zero call to Make-Robustâ¢(w,â‹…)Make-Robustğ‘¤â‹…\textsc{Make-Robust}(w,\cdot)Make-Robust ( italic_w , â‹… ) for each point wâˆˆğâˆ–ğ’²â‹†ğ‘¤ğsuperscriptğ’²â‹†w\in\mathbf{P}\setminus\mathcal{W}^{\star}italic_w âˆˆ bold_P âˆ– caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT. Accordingly, we can partition the calls to Make-Robustâ¢(â‹…,â‹…)Make-Robustâ‹…â‹…\textsc{Make-Robust}(\cdot\,,\cdot)Make-Robust ( â‹… , â‹… ) that are made by Robustifyâ¢(ğ’²â‹†)Robustifysuperscriptğ’²â‹†\textsc{Robustify}\left(\mathcal{W}^{\star}\right)Robustify ( caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) into the following three types. Type I. A call to Make-Robustâ¢(w,â‹…)Make-Robustğ‘¤â‹…\textsc{Make-Robust}(w,\cdot)Make-Robust ( italic_w , â‹… ) for some wâˆˆğ’²â‹†âˆ–ğ’°initğ‘¤superscriptğ’²â‹†subscriptğ’°initw\in\mathcal{W}^{\star}\setminus\mathcal{U}_{\textnormal{{init}}}italic_w âˆˆ caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT âˆ– caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT. The total number of such calls is at most |ğ’²â‹†âˆ–ğ’°init|â‰¤|ğ’²â‹†âŠ•ğ’°init|=Oâ¢(â„“+1)superscriptğ’²â‹†subscriptğ’°initdirect-sumsuperscriptğ’²â‹†subscriptğ’°initğ‘‚â„“1\left|\mathcal{W}^{\star}\setminus\mathcal{U}_{\textnormal{{init}}}\right|\leq% \left|\mathcal{W}^{\star}\oplus\mathcal{U}_{\textnormal{{init}}}\right|=O(\ell% +1)| caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT âˆ– caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT | â‰¤ | caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT âŠ• caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT | = italic_O ( roman_â„“ + 1 ) (see Corollary 5.3). Since the epoch lasts for (â„“+1)â„“1(\ell+1)( roman_â„“ + 1 ) updates, the amortized number of Type I calls to Make-Robustâ¢(â‹…,â‹…)Make-Robustâ‹…â‹…\textsc{Make-Robust}(\cdot\,,\cdot)Make-Robust ( â‹… , â‹… ), per update, is Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 ). Type II. A call to Make-Robustâ¢(w,â‹…)Make-Robustğ‘¤â‹…\textsc{Make-Robust}(w,\cdot)Make-Robust ( italic_w , â‹… ) for some wâˆˆğ’²â‹†âˆ©ğ’°initğ‘¤superscriptğ’²â‹†subscriptğ’°initw\in\mathcal{W}^{\star}\cap\mathcal{U}_{\textnormal{{init}}}italic_w âˆˆ caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT âˆ© caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT that is contaminated at the end of the epoch. By Claim 5.10, the amortized number of such Type II Make-Robustâ¢(â‹…,â‹…)Make-Robustâ‹…â‹…\textsc{Make-Robust}(\cdot\,,\cdot)Make-Robust ( â‹… , â‹… ) calls, per update, is Oâ¢(logâ¡Î”)ğ‘‚Î”O(\log\Delta)italic_O ( roman_log roman_Î” ). Type III. A call to Make-Robustâ¢(w,â‹…)Make-Robustğ‘¤â‹…\textsc{Make-Robust}(w,\cdot)Make-Robust ( italic_w , â‹… ) for some wâˆˆğ’²â‹†âˆ©ğ’°initğ‘¤superscriptğ’²â‹†subscriptğ’°initw\in\mathcal{W}^{\star}\cap\mathcal{U}_{\textnormal{{init}}}italic_w âˆˆ caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT âˆ© caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT that is clean at the end of the epoch. Recall that the center wğ‘¤witalic_w was tâ¢[w]ğ‘¡delimited-[]ğ‘¤t[w]italic_t [ italic_w ]-robust w.r.t. ğ’«(0)superscriptğ’«0\mathcal{P}^{(0)}caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT at the start of the epoch, and by Claim 5.9 it remains tâ¢[w]ğ‘¡delimited-[]ğ‘¤t[w]italic_t [ italic_w ]-robust w.r.t. ğ’«(â„“+1)superscriptğ’«â„“1\mathcal{P}^{(\ell+1)}caligraphic_P start_POSTSUPERSCRIPT ( roman_â„“ + 1 ) end_POSTSUPERSCRIPT at the end of the epoch. Furthermore, note that if a center is tâ€²superscriptğ‘¡â€²t^{\prime}italic_t start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT-robust, then it is also tâ€²â€²superscriptğ‘¡â€²â€²t^{\prime\prime}italic_t start_POSTSUPERSCRIPT â€² â€² end_POSTSUPERSCRIPT-robust for all tâ€²â€²â‰¤tâ€²superscriptğ‘¡â€²â€²superscriptğ‘¡â€²t^{\prime\prime}\leq t^{\prime}italic_t start_POSTSUPERSCRIPT â€² â€² end_POSTSUPERSCRIPT â‰¤ italic_t start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT. Thus, at the end of the epoch, the call to Make-Robustâ¢(w,t)Make-Robustğ‘¤ğ‘¡\textsc{Make-Robust}(w,t)Make-Robust ( italic_w , italic_t ) could have been made for only one reason: The subroutine Robustifyâ¢(ğ’²â‹†)Robustifysuperscriptğ’²â‹†\textsc{Robustify}\left(\mathcal{W}^{\star}\right)Robustify ( caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) wanted to ensure that wğ‘¤witalic_w was tğ‘¡titalic_t-robust w.r.t. ğ’«(â„“+1)superscriptğ’«â„“1\mathcal{P}^{(\ell+1)}caligraphic_P start_POSTSUPERSCRIPT ( roman_â„“ + 1 ) end_POSTSUPERSCRIPT for some t>tâ¢[w]ğ‘¡ğ‘¡delimited-[]ğ‘¤t>t[w]italic_t > italic_t [ italic_w ], but it was not the case. Suppose that wâ€²â†Make-Robustâ¢(w,t)â†superscriptğ‘¤â€²Make-Robustğ‘¤ğ‘¡w^{\prime}\leftarrow\textsc{Make-Robust}(w,t)italic_w start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT â† Make-Robust ( italic_w , italic_t ) was the center returned by this call to Make-Robustâ¢(â‹…,â‹…)Make-Robustâ‹…â‹…\textsc{Make-Robust}(\cdot\,,\cdot)Make-Robust ( â‹… , â‹… ). Then at the end of this call, we set tâ¢[wâ€²]â†tâ†ğ‘¡delimited-[]superscriptğ‘¤â€²ğ‘¡t[w^{\prime}]\leftarrow titalic_t [ italic_w start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ] â† italic_t. Clearly, we have tâ¢[wâ€²]>tâ¢[w]ğ‘¡delimited-[]superscriptğ‘¤â€²ğ‘¡delimited-[]ğ‘¤t[w^{\prime}]>t[w]italic_t [ italic_w start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ] > italic_t [ italic_w ]. To bound the amortized number of Type III calls, we need to invoke a more â€œglobalâ€ argument, that spans across multiple epochs. Consider a maximal â€œchainâ€ of jğ‘—jitalic_j many Type III calls (possibly spanning across multiple different epochs), in increasing order of time: w1â†Make-Robustâ¢(w0,tâ¢[w1]),w2â†Make-Robustâ¢(w1,tâ¢[w2]),formulae-sequenceâ†subscriptğ‘¤1Make-Robustsubscriptğ‘¤0ğ‘¡delimited-[]subscriptğ‘¤1â†subscriptğ‘¤2Make-Robustsubscriptğ‘¤1ğ‘¡delimited-[]subscriptğ‘¤2\displaystyle w_{1}\leftarrow\textsc{Make-Robust}(w_{0},t[w_{1}]),w_{2}% \leftarrow\textsc{Make-Robust}(w_{1},t[w_{2}]),italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT â† Make-Robust ( italic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_t [ italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ] ) , italic_w start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT â† Make-Robust ( italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_t [ italic_w start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ] ) , â€¦,wjâ†Make-Robustâ¢(wjâˆ’1,tâ¢[wj]).â†â€¦subscriptğ‘¤ğ‘—Make-Robustsubscriptğ‘¤ğ‘—1ğ‘¡delimited-[]subscriptğ‘¤ğ‘—\displaystyle\ldots,w_{j}\leftarrow\textsc{Make-Robust}(w_{j-1},t[w_{j}]).â€¦ , italic_w start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT â† Make-Robust ( italic_w start_POSTSUBSCRIPT italic_j - 1 end_POSTSUBSCRIPT , italic_t [ italic_w start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ] ) . Note that the calls in the above chain can be interspersed with other Type I, Type II or Type III calls that are not part of the chain. Still, from the above discussion, we get 0â‰¤tâ¢[w0]<tâ¢[w1]<â‹¯<tâ¢[wj]â‰¤âŒˆlogâ¡Î”âŒ‰0ğ‘¡delimited-[]subscriptğ‘¤0ğ‘¡delimited-[]subscriptğ‘¤1â‹¯ğ‘¡delimited-[]subscriptğ‘¤ğ‘—Î”0\leq t[w_{0}]<t[w_{1}]<\cdots<t[w_{j}]\leq\lceil\log\Delta\rceil0 â‰¤ italic_t [ italic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ] < italic_t [ italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ] < â‹¯ < italic_t [ italic_w start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ] â‰¤ âŒˆ roman_log roman_Î” âŒ‰. So, the chain has length at most Oâ¢(logâ¡Î”)ğ‘‚Î”O(\log\Delta)italic_O ( roman_log roman_Î” ). Also, for the chain to start in the first place, we must have had a Type I or Type II call to Make-Robustâ¢(â‹…,â‹…)Make-Robustâ‹…â‹…\textsc{Make-Robust}(\cdot\,,\cdot)Make-Robust ( â‹… , â‹… ) which returned the center w0subscriptğ‘¤0w_{0}italic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. We can thus â€œchargeâ€ the length (total number of Type III calls) in this chain to the Type I or Type II call that triggered it (by returning the center w0subscriptğ‘¤0w_{0}italic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT). In other words, the total number of Type III calls ever made is at most Oâ¢(logâ¡Î”)ğ‘‚Î”O(\log\Delta)italic_O ( roman_log roman_Î” ) times the total number of Type I plus Type II calls. Since the amortized number of Type I and Type II calls per update is Oâ¢(logâ¡Î”)ğ‘‚Î”O(\log\Delta)italic_O ( roman_log roman_Î” ), the amortized number of Type III calls per update is Oâ¢(log2â¡Î”)ğ‘‚superscript2Î”O(\log^{2}\Delta)italic_O ( roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT roman_Î” ). This concludes the proof of Lemma 5.8. 5.3.1 Proof of Claim 5.10 Assume pâˆˆğ’«(0)âŠ•ğ’«(â„“+1)ğ‘direct-sumsuperscriptğ’«0superscriptğ’«â„“1p\in\mathcal{P}^{(0)}\oplus\mathcal{P}^{(\ell+1)}italic_p âˆˆ caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT âŠ• caligraphic_P start_POSTSUPERSCRIPT ( roman_â„“ + 1 ) end_POSTSUPERSCRIPT is an updated point during the epoch. Let uâˆˆğ’°initğ‘¢subscriptğ’°initu\in\mathcal{U}_{\textnormal{{init}}}italic_u âˆˆ caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT be contaminated by pğ‘pitalic_p, i.e., dâ¢(p,ptâ¢[u]â¢(u))â‰¤10tâ¢[u]ğ‘‘ğ‘subscriptğ‘ğ‘¡delimited-[]ğ‘¢ğ‘¢superscript10ğ‘¡delimited-[]ğ‘¢d(p,p_{t[u]}(u))\leq 10^{t[u]}italic_d ( italic_p , italic_p start_POSTSUBSCRIPT italic_t [ italic_u ] end_POSTSUBSCRIPT ( italic_u ) ) â‰¤ 10 start_POSTSUPERSCRIPT italic_t [ italic_u ] end_POSTSUPERSCRIPT. According to Lemma 3.3, dâ¢(ptâ¢[u]â¢(u),u)=dâ¢(ptâ¢[u]â¢(u),p0â¢(u))â‰¤10tâ¢[u]/2ğ‘‘subscriptğ‘ğ‘¡delimited-[]ğ‘¢ğ‘¢ğ‘¢ğ‘‘subscriptğ‘ğ‘¡delimited-[]ğ‘¢ğ‘¢subscriptğ‘0ğ‘¢superscript10ğ‘¡delimited-[]ğ‘¢2d(p_{t[u]}(u),u)=d(p_{t[u]}(u),p_{0}(u))\leq 10^{t[u]}/2italic_d ( italic_p start_POSTSUBSCRIPT italic_t [ italic_u ] end_POSTSUBSCRIPT ( italic_u ) , italic_u ) = italic_d ( italic_p start_POSTSUBSCRIPT italic_t [ italic_u ] end_POSTSUBSCRIPT ( italic_u ) , italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( italic_u ) ) â‰¤ 10 start_POSTSUPERSCRIPT italic_t [ italic_u ] end_POSTSUPERSCRIPT / 2, which concludes dâ¢(p,u)â‰¤dâ¢(p,ptâ¢[u]â¢(u))+dâ¢(ptâ¢[u]â¢(u),u)â‰¤10tâ¢[u]+10tâ¢[u]/2â‰¤2â‹…10tâ¢[u].ğ‘‘ğ‘ğ‘¢ğ‘‘ğ‘subscriptğ‘ğ‘¡delimited-[]ğ‘¢ğ‘¢ğ‘‘subscriptğ‘ğ‘¡delimited-[]ğ‘¢ğ‘¢ğ‘¢superscript10ğ‘¡delimited-[]ğ‘¢superscript10ğ‘¡delimited-[]ğ‘¢2â‹…2superscript10ğ‘¡delimited-[]ğ‘¢d(p,u)\leq d(p,p_{t[u]}(u))+d(p_{t[u]}(u),u)\leq 10^{t[u]}+10^{t[u]}/2\leq 2% \cdot 10^{t[u]}.italic_d ( italic_p , italic_u ) â‰¤ italic_d ( italic_p , italic_p start_POSTSUBSCRIPT italic_t [ italic_u ] end_POSTSUBSCRIPT ( italic_u ) ) + italic_d ( italic_p start_POSTSUBSCRIPT italic_t [ italic_u ] end_POSTSUBSCRIPT ( italic_u ) , italic_u ) â‰¤ 10 start_POSTSUPERSCRIPT italic_t [ italic_u ] end_POSTSUPERSCRIPT + 10 start_POSTSUPERSCRIPT italic_t [ italic_u ] end_POSTSUPERSCRIPT / 2 â‰¤ 2 â‹… 10 start_POSTSUPERSCRIPT italic_t [ italic_u ] end_POSTSUPERSCRIPT . (24) Let {u1,u2,â€¦,uÎ¼}âŠ†ğ’°initsubscriptğ‘¢1subscriptğ‘¢2â€¦subscriptğ‘¢ğœ‡subscriptğ’°init\{u_{1},u_{2},\ldots,u_{\mu}\}\subseteq\mathcal{U}_{\textnormal{{init}}}{ italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_u start_POSTSUBSCRIPT italic_Î¼ end_POSTSUBSCRIPT } âŠ† caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT be all centers contaminated by pğ‘pitalic_p ordered in decreasing order by the time they were added to the main solution via a call to Make-Robust(â‹…,â‹…)â‹…â‹…(\cdot,\cdot)( â‹… , â‹… ). So, when uisubscriptğ‘¢ğ‘–u_{i}italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is added to the main solution ğ’°ğ’°\mathcal{U}caligraphic_U, ui+1subscriptğ‘¢ğ‘–1u_{i+1}italic_u start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT was already present in ğ’°ğ’°\mathcal{U}caligraphic_U, which concludes 10tâ¢[ui]â‰¤dâ¢(ui,ui+1)/10,superscript10ğ‘¡delimited-[]subscriptğ‘¢ğ‘–ğ‘‘subscriptğ‘¢ğ‘–subscriptğ‘¢ğ‘–11010^{t[u_{i}]}\leq d(u_{i},u_{i+1})/10,10 start_POSTSUPERSCRIPT italic_t [ italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ] end_POSTSUPERSCRIPT â‰¤ italic_d ( italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT ) / 10 , for every iâˆˆ[1,Î¼âˆ’1]ğ‘–1ğœ‡1i\in[1,\mu-1]italic_i âˆˆ [ 1 , italic_Î¼ - 1 ] (by the choice of tâ¢[ui]ğ‘¡delimited-[]subscriptğ‘¢ğ‘–t[u_{i}]italic_t [ italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ] in Make-Robust at that time). Hence, 10tâ¢[ui+1]â‰¥dâ¢(p,ui+1)/2â‰¥(dâ¢(ui,ui+1)âˆ’dâ¢(p,ui))/2â‰¥5â‹…10tâ¢[ui]âˆ’10tâ¢[ui]=4â‹…10tâ¢[ui].superscript10ğ‘¡delimited-[]subscriptğ‘¢ğ‘–1ğ‘‘ğ‘subscriptğ‘¢ğ‘–12ğ‘‘subscriptğ‘¢ğ‘–subscriptğ‘¢ğ‘–1ğ‘‘ğ‘subscriptğ‘¢ğ‘–2â‹…5superscript10ğ‘¡delimited-[]subscriptğ‘¢ğ‘–superscript10ğ‘¡delimited-[]subscriptğ‘¢ğ‘–â‹…4superscript10ğ‘¡delimited-[]subscriptğ‘¢ğ‘–10^{t[u_{i+1}]}\geq d(p,u_{i+1})/2\geq(d(u_{i},u_{i+1})-d(p,u_{i}))/2\geq 5% \cdot 10^{t[u_{i}]}-10^{t[u_{i}]}=4\cdot 10^{t[u_{i}]}.10 start_POSTSUPERSCRIPT italic_t [ italic_u start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT ] end_POSTSUPERSCRIPT â‰¥ italic_d ( italic_p , italic_u start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT ) / 2 â‰¥ ( italic_d ( italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT ) - italic_d ( italic_p , italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) / 2 â‰¥ 5 â‹… 10 start_POSTSUPERSCRIPT italic_t [ italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ] end_POSTSUPERSCRIPT - 10 start_POSTSUPERSCRIPT italic_t [ italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ] end_POSTSUPERSCRIPT = 4 â‹… 10 start_POSTSUPERSCRIPT italic_t [ italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ] end_POSTSUPERSCRIPT . The first and last inequalities hold by (24) for u=ui+1,uiğ‘¢subscriptğ‘¢ğ‘–1subscriptğ‘¢ğ‘–u=u_{i+1},u_{i}italic_u = italic_u start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. Finally, this concludes tâ¢[uÎ¼]>â‹¯>tâ¢[u2]>tâ¢[u1]ğ‘¡delimited-[]subscriptğ‘¢ğœ‡â‹¯ğ‘¡delimited-[]subscriptğ‘¢2ğ‘¡delimited-[]subscriptğ‘¢1t[u_{\mu}]>\cdots>t[u_{2}]>t[u_{1}]italic_t [ italic_u start_POSTSUBSCRIPT italic_Î¼ end_POSTSUBSCRIPT ] > â‹¯ > italic_t [ italic_u start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ] > italic_t [ italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ]. Since distances between any two point in the space is between 1111 and Î”Î”\Deltaroman_Î”, we know 0â‰¤tâ¢[ui]â‰¤âŒˆlogâ¡Î”âŒ‰0ğ‘¡delimited-[]subscriptğ‘¢ğ‘–Î”0\leq t[u_{i}]\leq\lceil\log\Delta\rceil0 â‰¤ italic_t [ italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ] â‰¤ âŒˆ roman_log roman_Î” âŒ‰ for each iâˆˆ[1,Î¼]ğ‘–1ğœ‡i\in[1,\mu]italic_i âˆˆ [ 1 , italic_Î¼ ], which concludes Î¼=Oâ¢(logâ¡Î”)ğœ‡ğ‘‚Î”\mu=O(\log\Delta)italic_Î¼ = italic_O ( roman_log roman_Î” ). 6 Achieving O~â¢(k)~ğ‘‚ğ‘˜\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) Update Time We first outline how to implement the algorithm from Section 5.1 in O~â¢(n)~ğ‘‚ğ‘›\tilde{O}(n)over~ start_ARG italic_O end_ARG ( italic_n ) update time, by incurring only a Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 ) multiplicative overhead in approximation ratio and recourse (nğ‘›nitalic_n is an upper bound on the size of the input ğ’«âŠ†ğğ’«ğ\mathcal{P}\subseteq\mathbf{P}caligraphic_P âŠ† bold_P, throughout the sequence of updates). In Section 6.3, we show how to further improve the update time from O~â¢(n)~ğ‘‚ğ‘›\tilde{O}(n)over~ start_ARG italic_O end_ARG ( italic_n ) to O~â¢(k)~ğ‘‚ğ‘˜\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) using standard sparsification techniques. Disclaimer. In this section, we often use asymptotic notations and informal arguments without proofs. The reader can find the correct values of the parameters together with complete formal proofs for the statements of this section in Part III (full version). 6.1 Auxiliary Data Structure and Randomized Local Search Recall that ğ’°âŠ†ğğ’°ğ\mathcal{U}\subseteq\mathbf{P}caligraphic_U âŠ† bold_P is the solution (set of kğ‘˜kitalic_k centers) maintained by our algorithm, and ğ’«âŠ†ğğ’«ğ\mathcal{P}\subseteq\mathbf{P}caligraphic_P âŠ† bold_P denotes the current input. For each point pâˆˆğ’«âˆªğ’°ğ‘ğ’«ğ’°p\in\mathcal{P}\cup\mathcal{U}italic_p âˆˆ caligraphic_P âˆª caligraphic_U, we maintain a BST (balanced search tree) ğ’¯psubscriptğ’¯ğ‘\mathcal{T}_{p}caligraphic_T start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT that stores the centers ğ’°ğ’°\mathcal{U}caligraphic_U in increasing order of their distances to pğ‘pitalic_p. Note that after every change (insertion/deletion of a center) in the set ğ’°ğ’°\mathcal{U}caligraphic_U, we can update all these BSTs in O~â¢(|ğ’«|+|ğ’°|)=O~â¢(n+k)=O~â¢(n)~ğ‘‚ğ’«ğ’°~ğ‘‚ğ‘›ğ‘˜~ğ‘‚ğ‘›\tilde{O}(\left|\mathcal{P}\right|+\left|\mathcal{U}\right|)=\tilde{O}(n+k)=% \tilde{O}(n)over~ start_ARG italic_O end_ARG ( | caligraphic_P | + | caligraphic_U | ) = over~ start_ARG italic_O end_ARG ( italic_n + italic_k ) = over~ start_ARG italic_O end_ARG ( italic_n ) time. Similarly, after the insertion/deletion of a point pâˆˆğ’«ğ‘ğ’«p\in\mathcal{P}italic_p âˆˆ caligraphic_P, we can construct/destroy the relevant BST ğ’¯psubscriptğ’¯ğ‘\mathcal{T}_{p}caligraphic_T start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT in O~â¢(|ğ’°|)=O~â¢(k)=O~â¢(n)~ğ‘‚ğ’°~ğ‘‚ğ‘˜~ğ‘‚ğ‘›\tilde{O}(\left|\mathcal{U}\right|)=\tilde{O}(k)=\tilde{O}(n)over~ start_ARG italic_O end_ARG ( | caligraphic_U | ) = over~ start_ARG italic_O end_ARG ( italic_k ) = over~ start_ARG italic_O end_ARG ( italic_n ) time. In other words, if the algorithm from Section 5.1 incurs a total recourse of Ï„ğœ\tauitalic_Ï„ while handling a sequence of Î¼ğœ‡\muitalic_Î¼ updates, then we spend O~â¢(nâ‹…(Ï„+Î¼))~ğ‘‚â‹…ğ‘›ğœğœ‡\tilde{O}(n\cdot(\tau+\mu))over~ start_ARG italic_O end_ARG ( italic_n â‹… ( italic_Ï„ + italic_Î¼ ) ) total time on maintaining this auxiliary data structure (collection of BSTs) over the same update-sequence. Since Ï„=Oâ¢(Î¼â‹…log2â¡Î”)ğœğ‘‚â‹…ğœ‡superscript2Î”\tau=O(\mu\cdot\log^{2}\Delta)italic_Ï„ = italic_O ( italic_Î¼ â‹… roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT roman_Î” ) (see Section 5.3), this incurs an amortized update time of O~â¢(n)~ğ‘‚ğ‘›\tilde{O}(n)over~ start_ARG italic_O end_ARG ( italic_n ), which is within our budget. We will use the randomized local search algorithm, developed in [BCG+24] and summarized in the lemma below, as a crucial subroutine. Lemma 6.1 (Randomized Local Search [BCG+24]). Suppose that we have access to the auxiliary data structure described above. Then, given any integer sâˆˆ[0,kâˆ’1]ğ‘ 0ğ‘˜1s\in[0,k-1]italic_s âˆˆ [ 0 , italic_k - 1 ], in O~â¢(nâ¢s)~ğ‘‚ğ‘›ğ‘ \tilde{O}(ns)over~ start_ARG italic_O end_ARG ( italic_n italic_s ) time we can find a subset ğ’°â‹†âŠ†ğ’°superscriptğ’°â‹†ğ’°\mathcal{U}^{\star}\subseteq\mathcal{U}caligraphic_U start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT âŠ† caligraphic_U of (kâˆ’s)ğ‘˜ğ‘ (k-s)( italic_k - italic_s ) centers such that Costâ¢(ğ’°â‹†,ğ’«)â‰¤Oâ¢(1)â‹…OPTkâˆ’sğ’°â¢(ğ’«)Costsuperscriptğ’°â‹†ğ’«â‹…ğ‘‚1superscriptsubscriptOPTğ‘˜ğ‘ ğ’°ğ’«\textnormal{{Cost}}\left(\mathcal{U}^{\star},\mathcal{P}\right)\leq O(1)\cdot% \textnormal{{OPT}}_{k-s}^{\mathcal{U}}\left(\mathcal{P}\right)Cost ( caligraphic_U start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT , caligraphic_P ) â‰¤ italic_O ( 1 ) â‹… OPT start_POSTSUBSCRIPT italic_k - italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT caligraphic_U end_POSTSUPERSCRIPT ( caligraphic_P ). 6.2 Implementing Our Dynamic Algorithm Henceforth, we focus on a given epoch of our dynamic algorithm that lasts for (â„“+1)â„“1(\ell+1)( roman_â„“ + 1 ) updates (see Section 5.1), and outline how to implement the algorithm in such a manner that it spends O~â¢(nâ‹…(â„“+1))~ğ‘‚â‹…ğ‘›â„“1\tilde{O}(n\cdot(\ell+1))over~ start_ARG italic_O end_ARG ( italic_n â‹… ( roman_â„“ + 1 ) ) total time during the whole epoch, except the call to Robustify (see Equation 11). For Robustify, we provide an implementation that takes an amortized time of O~â¢(n)~ğ‘‚ğ‘›\tilde{O}(n)over~ start_ARG italic_O end_ARG ( italic_n ), over the entire sequence of updates (spanning multiple epochs). This implies an overall amortized update time of O~â¢(n)~ğ‘‚ğ‘›\tilde{O}(n)over~ start_ARG italic_O end_ARG ( italic_n ). Below, we first show how to implement each of Steps 1 - 4, as described in Section 5.1, one after another. Then, we provide the implementation of Robustify in Section 6.2.1. Implementing Step 1. Our task here is to compute an estimate of the value of â„“â‹†superscriptâ„“â‹†\ell^{\star}roman_â„“ start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT. For each iâˆˆ[0,log2â¡k]ğ‘–0subscript2ğ‘˜i\in[0,\log_{2}k]italic_i âˆˆ [ 0 , roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_k ] define si:=2iassignsubscriptğ‘ ğ‘–superscript2ğ‘–s_{i}:=2^{i}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT := 2 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT, and let sâˆ’1:=0assignsubscriptğ‘ 10s_{-1}:=0italic_s start_POSTSUBSCRIPT - 1 end_POSTSUBSCRIPT := 0. We now run a for loop, as described below. 1 for i=0ğ‘–0i=0italic_i = 0 to log2â¡ksubscript2ğ‘˜\log_{2}kroman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_k do 2 Using Lemma 6.1, in O~â¢(nâ¢si)~ğ‘‚ğ‘›subscriptğ‘ ğ‘–\tilde{O}(ns_{i})over~ start_ARG italic_O end_ARG ( italic_n italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) time compute a subset ğ’°iâ‹†âŠ†ğ’°initsubscriptsuperscriptğ’°â‹†ğ‘–subscriptğ’°init\mathcal{U}^{\star}_{i}\subseteq\mathcal{U}_{\textnormal{{init}}}caligraphic_U start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âŠ† caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT of (kâˆ’si)ğ‘˜subscriptğ‘ ğ‘–(k-s_{i})( italic_k - italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) centers, 3 that is a Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 )-approximation to OPTkâˆ’siğ’°initâ¢(ğ’«(0))superscriptsubscriptOPTğ‘˜subscriptğ‘ ğ‘–subscriptğ’°initsuperscriptğ’«0\textnormal{{OPT}}_{k-s_{i}}^{\mathcal{U}_{\textnormal{{init}}}}\left(\mathcal% {P}^{(0)}\right)OPT start_POSTSUBSCRIPT italic_k - italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ). 4 if Costâ¢(ğ’°iâ‹†,ğ’«(0))â‰¥Î˜â¢(Î³)â‹…Costâ¢(ğ’°init,ğ’«(0))Costsubscriptsuperscriptğ’°â‹†ğ‘–superscriptğ’«0â‹…Î˜ğ›¾Costsubscriptğ’°initsuperscriptğ’«0\textnormal{{Cost}}\left(\mathcal{U}^{\star}_{i},\mathcal{P}^{(0)}\right)\geq% \Theta(\gamma)\cdot\textnormal{{Cost}}\left(\mathcal{U}_{\textnormal{{init}}},% \mathcal{P}^{(0)}\right)Cost ( caligraphic_U start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) â‰¥ roman_Î˜ ( italic_Î³ ) â‹… Cost ( caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) then 5 return â„“^:=siâˆ’1assign^â„“subscriptğ‘ ğ‘–1\hat{\ell}:=s_{i-1}over^ start_ARG roman_â„“ end_ARG := italic_s start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT. Algorithm 3 Computing an estimate â„“^^â„“\hat{\ell}over^ start_ARG roman_â„“ end_ARG of the value of â„“â‹†superscriptâ„“â‹†\ell^{\star}roman_â„“ start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT. After finding â„“^^â„“\hat{\ell}over^ start_ARG roman_â„“ end_ARG, we set the length of the epoch to be â„“+1â„“1\ell+1roman_â„“ + 1 where â„“â†âŒŠâ„“^12â‹…Î˜â¢(Î³)âŒ‹â†â„“^â„“â‹…12Î˜ğ›¾\ell\leftarrow\left\lfloor\frac{\hat{\ell}}{12\cdot\Theta(\gamma)}\right\rfloorroman_â„“ â† âŒŠ divide start_ARG over^ start_ARG roman_â„“ end_ARG end_ARG start_ARG 12 â‹… roman_Î˜ ( italic_Î³ ) end_ARG âŒ‹. With some extra calculations, we can show that â„“+1=Î©â¢(â„“^+1)=Î©â¢(â„“â‹†+1)â„“1Î©^â„“1Î©superscriptâ„“â‹†1\ell+1=\Omega(\hat{\ell}+1)=\Omega(\ell^{\star}+1)roman_â„“ + 1 = roman_Î© ( over^ start_ARG roman_â„“ end_ARG + 1 ) = roman_Î© ( roman_â„“ start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT + 1 ) and OPTkâˆ’â„“â¢(ğ’«(0))Î˜â¢(Î³)â‰¤OPTkâ¢(ğ’«(0))â‰¤4â‹…OPTk+â„“â¢(ğ’«(0)).subscriptOPTğ‘˜â„“superscriptğ’«0Î˜ğ›¾subscriptOPTğ‘˜superscriptğ’«0â‹…4subscriptOPTğ‘˜â„“superscriptğ’«0\frac{\textnormal{{OPT}}_{k-{\ell}}(\mathcal{P}^{(0)})}{\Theta(\gamma)}\leq% \textnormal{{OPT}}_{k}(\mathcal{P}^{(0)})\leq 4\cdot\textnormal{{OPT}}_{k+{% \ell}}(\mathcal{P}^{(0)}).divide start_ARG OPT start_POSTSUBSCRIPT italic_k - roman_â„“ end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) end_ARG start_ARG roman_Î˜ ( italic_Î³ ) end_ARG â‰¤ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) â‰¤ 4 â‹… OPT start_POSTSUBSCRIPT italic_k + roman_â„“ end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) . The running time of this for loop is O~â¢(nâ¢âˆ‘j=0iâ‹†sj)=O~â¢(nâ¢siâ‹†)~ğ‘‚ğ‘›superscriptsubscriptğ‘—0superscriptğ‘–â‹†subscriptğ‘ ğ‘—~ğ‘‚ğ‘›subscriptğ‘ superscriptğ‘–â‹†\tilde{O}\left(n\sum_{j=0}^{i^{\star}}s_{j}\right)=\tilde{O}(ns_{i^{\star}})over~ start_ARG italic_O end_ARG ( italic_n âˆ‘ start_POSTSUBSCRIPT italic_j = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT italic_s start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) = over~ start_ARG italic_O end_ARG ( italic_n italic_s start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ), where iâ‹†superscriptğ‘–â‹†i^{\star}italic_i start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT is the index s.t. siâ‹†âˆ’1=â„“^subscriptğ‘ superscriptğ‘–â‹†1^â„“s_{i^{\star}-1}=\hat{\ell}italic_s start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT - 1 end_POSTSUBSCRIPT = over^ start_ARG roman_â„“ end_ARG. Thus, we have siâ‹†=Oâ¢(â„“^)=Oâ¢(â„“+1)subscriptğ‘ superscriptğ‘–â‹†ğ‘‚^â„“ğ‘‚â„“1s_{i^{\star}}=O(\hat{\ell})=O(\ell+1)italic_s start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT end_POSTSUBSCRIPT = italic_O ( over^ start_ARG roman_â„“ end_ARG ) = italic_O ( roman_â„“ + 1 ), and hence we can implement Step 1 in O~â¢(nâ‹…(â„“+1))~ğ‘‚â‹…ğ‘›â„“1\tilde{O}(n\cdot(\ell+1))over~ start_ARG italic_O end_ARG ( italic_n â‹… ( roman_â„“ + 1 ) ) time. Implementing Step 2. Instead of finding the optimum set of (kâˆ’â„“)ğ‘˜â„“(k-\ell)( italic_k - roman_â„“ ) centers within ğ’°initsubscriptğ’°init\mathcal{U}_{\textnormal{{init}}}caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT, we approximate it using Lemma 6.1: We compute a set of (kâˆ’â„“)ğ‘˜â„“(k-\ell)( italic_k - roman_â„“ ) centers ğ’°(0)âŠ†ğ’°initsuperscriptğ’°0subscriptğ’°init\mathcal{U}^{(0)}\subseteq\mathcal{U}_{\textnormal{{init}}}caligraphic_U start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT âŠ† caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT such that Costâ¢(ğ’°(0),ğ’«(0))â‰¤Oâ¢(1)â‹…OPTkâˆ’â„“ğ’°initâ¢(ğ’«(0))Costsuperscriptğ’°0superscriptğ’«0â‹…ğ‘‚1superscriptsubscriptOPTğ‘˜â„“subscriptğ’°initsuperscriptğ’«0\textnormal{{Cost}}\left(\mathcal{U}^{(0)},\mathcal{P}^{(0)}\right)\leq O(1)% \cdot\textnormal{{OPT}}_{k-\ell}^{\mathcal{U}_{\textnormal{{init}}}}(\mathcal{% P}^{(0)})Cost ( caligraphic_U start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) â‰¤ italic_O ( 1 ) â‹… OPT start_POSTSUBSCRIPT italic_k - roman_â„“ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ). With the same arguments as before, we get Costâ¢(ğ’°(0),ğ’«(0))â‰¤Oâ¢(1)â‹…OPTk+â„“â¢(ğ’«(0))Costsuperscriptğ’°0superscriptğ’«0â‹…ğ‘‚1subscriptOPTğ‘˜â„“superscriptğ’«0\textnormal{{Cost}}\left(\mathcal{U}^{(0)},\mathcal{P}^{(0)}\right)\leq O(1)% \cdot\textnormal{{OPT}}_{k+\ell}(\mathcal{P}^{(0)})Cost ( caligraphic_U start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) â‰¤ italic_O ( 1 ) â‹… OPT start_POSTSUBSCRIPT italic_k + roman_â„“ end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ). Note that the running time for this step is also O~â¢(nâ‹…(â„“+1))~ğ‘‚â‹…ğ‘›â„“1\tilde{O}(n\cdot(\ell+1))over~ start_ARG italic_O end_ARG ( italic_n â‹… ( roman_â„“ + 1 ) ). Implementing Step 3. Trivially, we can implement each of these updates in constant time. Implementing Step 4. We first need to add Oâ¢(â„“+1)ğ‘‚â„“1O(\ell+1)italic_O ( roman_â„“ + 1 ) centers to ğ’°initsubscriptğ’°init\mathcal{U}_{\textnormal{{init}}}caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT, while minimizing the cost of the solution w.r.t. ğ’«(0)superscriptğ’«0\mathcal{P}^{(0)}caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT. We compute an approximation of ğ’°â‹†superscriptğ’°â‹†\mathcal{U}^{\star}caligraphic_U start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT, by setting ğ’°=ğ’°initğ’°subscriptğ’°init\mathcal{U}=\mathcal{U}_{\textnormal{{init}}}caligraphic_U = caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT, ğ’«=ğ’«(0)ğ’«superscriptğ’«0\mathcal{P}=\mathcal{P}^{(0)}caligraphic_P = caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT and s=Î˜â¢(Î³â‹…(â„“+1))ğ‘ Î˜â‹…ğ›¾â„“1s=\Theta(\gamma\cdot(\ell+1))italic_s = roman_Î˜ ( italic_Î³ â‹… ( roman_â„“ + 1 ) ) in Lemma 6.2 below (see Equation 9). This also takes O~â¢(nâ‹…(â„“+1))~ğ‘‚â‹…ğ‘›â„“1\tilde{O}(n\cdot(\ell+1))over~ start_ARG italic_O end_ARG ( italic_n â‹… ( roman_â„“ + 1 ) ) time. We defer the proof sketch of Lemma 6.2 to Section 6.2.2. Lemma 6.2. Suppose that we have access to the auxiliary data structure described above (see Section 6.1). Then, given any integer sâ‰¥1ğ‘ 1s\geq 1italic_s â‰¥ 1, in O~â¢(nâ¢s)~ğ‘‚ğ‘›ğ‘ \tilde{O}(ns)over~ start_ARG italic_O end_ARG ( italic_n italic_s ) time we can find a superset ğ’°â‹†âŠ‡ğ’°ğ’°superscriptğ’°â‹†\mathcal{U}^{\star}\supseteq\mathcal{U}caligraphic_U start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT âŠ‡ caligraphic_U of (k+s)ğ‘˜ğ‘ (k+s)( italic_k + italic_s ) centers such that Costâ¢(ğ’°â‹†,ğ’«)â‰¤Oâ¢(1)â‹…minâ„±âŠ†ğ:|â„±|â‰¤sâ¡Costâ¢(ğ’°+â„±,ğ’«)Costsuperscriptğ’°â‹†ğ’«â‹…ğ‘‚1subscript:â„±ğâ„±ğ‘ Costğ’°â„±ğ’«\textnormal{{Cost}}\left(\mathcal{U}^{\star},\mathcal{P}\right)\leq O(1)\cdot% \min\limits_{\mathcal{F}\subseteq\mathbf{P}:|\mathcal{F}|\leq s}\textnormal{{% Cost}}\left(\mathcal{U}+\mathcal{F},\mathcal{P}\right)Cost ( caligraphic_U start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT , caligraphic_P ) â‰¤ italic_O ( 1 ) â‹… roman_min start_POSTSUBSCRIPT caligraphic_F âŠ† bold_P : | caligraphic_F | â‰¤ italic_s end_POSTSUBSCRIPT Cost ( caligraphic_U + caligraphic_F , caligraphic_P ). At this stage, we compute ğ’±â‹†:=ğ’°â‹†+(ğ’«(â„“+1)âˆ’ğ’«(0))assignsuperscriptğ’±â‹†superscriptğ’°â‹†superscriptğ’«â„“1superscriptğ’«0\mathcal{V}^{\star}:=\mathcal{U}^{\star}+\left(\mathcal{P}^{(\ell+1)}-\mathcal% {P}^{(0)}\right)caligraphic_V start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT := caligraphic_U start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT + ( caligraphic_P start_POSTSUPERSCRIPT ( roman_â„“ + 1 ) end_POSTSUPERSCRIPT - caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) as in Equation 10, in only O~â¢(â„“+1)~ğ‘‚â„“1\tilde{O}(\ell+1)over~ start_ARG italic_O end_ARG ( roman_â„“ + 1 ) time. Next, we compute an approximation of ğ’²â‹†superscriptğ’²â‹†\mathcal{W}^{\star}caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT (see Equation 10) using Lemma 6.1, which again takes O~â¢(nâ‹…(â„“+1))~ğ‘‚â‹…ğ‘›â„“1\tilde{O}(n\cdot(\ell+1))over~ start_ARG italic_O end_ARG ( italic_n â‹… ( roman_â„“ + 1 ) ) time. It follows that Costâ¢(ğ’²â‹†,ğ’«(â„“+1))â‰¤Oâ¢(1)â‹…OPTkâ¢(ğ’«(â„“+1))Costsuperscriptğ’²â‹†superscriptğ’«â„“1â‹…ğ‘‚1subscriptOPTğ‘˜superscriptğ’«â„“1\textnormal{{Cost}}\left(\mathcal{W}^{\star},\mathcal{P}^{(\ell+1)}\right)\leq O% (1)\cdot\textnormal{{OPT}}_{k}(\mathcal{P}^{(\ell+1)})Cost ( caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT , caligraphic_P start_POSTSUPERSCRIPT ( roman_â„“ + 1 ) end_POSTSUPERSCRIPT ) â‰¤ italic_O ( 1 ) â‹… OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT ( roman_â„“ + 1 ) end_POSTSUPERSCRIPT ). Finally, we explain below how we implement the call to Robustifyâ¢(ğ’²â‹†)Robustifysuperscriptğ’²â‹†\textsc{Robustify}(\mathcal{W}^{\star})Robustify ( caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) (see Equation 11). 6.2.1 Implementing the calls to Robustifyâ¢(â‹…)Robustifyâ‹…\textsc{Robustify}(\cdot)Robustify ( â‹… ) subroutine Recall Algorithm 1 and Algorithm 2 from Section 3.2. In the static setting, there are known Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 )-approximation algorithms for 1111-median with O~â¢(n)~ğ‘‚ğ‘›\tilde{O}(n)over~ start_ARG italic_O end_ARG ( italic_n ) runtime [MP02]. Using any such 1111-median algorithm, it is relatively straightforward to (approximately) implement a call to Make-Robustâ¢(â‹…,â‹…)Make-Robustâ‹…â‹…\textsc{Make-Robust}(\cdot\,,\cdot)Make-Robust ( â‹… , â‹… ) in O~â¢(n)~ğ‘‚ğ‘›\tilde{O}(n)over~ start_ARG italic_O end_ARG ( italic_n ) time (see Algorithm 1 in Algorithm 1). Using the auxiliary data structure (see Section 6.1), it is easy to implement each invocation of Algorithm 1 in Algorithm 1 in O~â¢(1)~ğ‘‚1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) time, whereas Algorithm 1 in Algorithm 1 trivially takes O~â¢(1)~ğ‘‚1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) time. By Lemma 5.8, our dynamic algorithm makes O~â¢(1)~ğ‘‚1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) many amortized calls to Make-Robustâ¢(â‹…,â‹…)Make-Robustâ‹…â‹…\textsc{Make-Robust}(\cdot\,,\cdot)Make-Robust ( â‹… , â‹… ), per update. Thus, it follows that we spend O~â¢(n)~ğ‘‚ğ‘›\tilde{O}(n)over~ start_ARG italic_O end_ARG ( italic_n ) amortized time per update on implementing Algorithms 1, 1 and 1 in Algorithm 1. There remains a significant challenge: We might have to iterate over |ğ’²|=kğ’²ğ‘˜\left|\mathcal{W}\right|=k| caligraphic_W | = italic_k centers in Algorithm 1 in Algorithm 1, before we find a center wâˆˆğ’²ğ‘¤ğ’²w\in\mathcal{W}italic_w âˆˆ caligraphic_W that violates Equation 4. Let us refer to this operation as â€œtesting a center wğ‘¤witalic_wâ€; this occurs when we check whether wğ‘¤witalic_w violates Equation 4. In other words, we need to perform Oâ¢(k)ğ‘‚ğ‘˜O(k)italic_O ( italic_k ) many such tests in Algorithm 1, before we execute a Make-Robustâ¢(â‹…,â‹…)Make-Robustâ‹…â‹…\textsc{Make-Robust}(\cdot\,,\cdot)Make-Robust ( â‹… , â‹… ) call in Algorithm 1 in Algorithm 1. Note that because of Lemma 5.8, we perform O~â¢(k)~ğ‘‚ğ‘˜\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) many tests, on average, per update. Thus, if we could hypothetically perform each test in O~â¢(1)~ğ‘‚1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) time, then we would incur an additive overhead of O~â¢(k)~ğ‘‚ğ‘˜\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) in our amortized update time, and everything would be fine. The issue, however, is that testing a center can be prohibitively expensive. This is because Equation 4 consists of two conditions. The second condition (which finds the value of tğ‘¡titalic_t) requires us to know the value of dâ¢(w,ğ’²âˆ’w)ğ‘‘ğ‘¤ğ’²ğ‘¤d(w,\mathcal{W}-w)italic_d ( italic_w , caligraphic_W - italic_w ), and this can indeed be implemented in O~â¢(1)~ğ‘‚1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) time using our auxiliary data structure (see Section 6.1). The first condition asks us to check whether wğ‘¤witalic_w is tğ‘¡titalic_t-robust, and there does not seem to be any efficient way in which we can implement this check (see Definition 3.2). To address this significant challenge, we modify the execution of a call to Robustifyâ¢(ğ’²â‹†)Robustifysuperscriptğ’²â‹†\textsc{Robustify}(\mathcal{W}^{\star})Robustify ( caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) at the end of an epoch, as described below. Modified version of the call to Robustifyâ¢(ğ’²â‹†)Robustifysuperscriptğ’²â‹†\textsc{Robustify}(\mathcal{W}^{\star})Robustify ( caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ). At the end of an epoch, the call to Robustifyâ¢(ğ’²â‹†)Robustifysuperscriptğ’²â‹†\textsc{Robustify}(\mathcal{W}^{\star})Robustify ( caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) is supposed to return the set ğ’°finalsubscriptğ’°final\mathcal{U}_{\textnormal{{final}}}caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT (see Equation 11). We replace this call to Robustifyâ¢(ğ’²â‹†)Robustifysuperscriptğ’²â‹†\textsc{Robustify}(\mathcal{W}^{\star})Robustify ( caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) by the procedure in Algorithm 4 below. To appreciate what Algorithm 4 does, recall the recourse analysis in Section 5.3; in particular, the distinction between contaminated vs clean centers in ğ’°initsubscriptğ’°init\mathcal{U}_{\textnormal{{init}}}caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT, and the three types of calls to the Make-Robustâ¢(â‹…,â‹…)Make-Robustâ‹…â‹…\textsc{Make-Robust}(\cdot\,,\cdot)Make-Robust ( â‹… , â‹… ) subroutine. Note that in Algorithms 4, 4 and 4 in Algorithm 4, the sets ğ’²1,ğ’²2subscriptğ’²1subscriptğ’²2\mathcal{W}_{1},\mathcal{W}_{2}caligraphic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , caligraphic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT and ğ’²3subscriptğ’²3\mathcal{W}_{3}caligraphic_W start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT respectively correspond to those centers wğ‘¤witalic_w that might potentially be the sources of Type I, Type II and Type III calls to Make-Robustâ¢(w,â‹…)Make-Robustğ‘¤â‹…\textsc{Make-Robust}(w,\cdot)Make-Robust ( italic_w , â‹… ). We refer to the centers in ğ’²1,ğ’²2subscriptğ’²1subscriptğ’²2\mathcal{W}_{1},\mathcal{W}_{2}caligraphic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , caligraphic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT and ğ’²3subscriptğ’²3\mathcal{W}_{3}caligraphic_W start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT respectively as Type I, Type II and Type III centers. The key difference between Algorithm 4 and the previous version of Robustifyâ¢(ğ’²â‹†)Robustifysuperscriptğ’²â‹†\textsc{Robustify}(\mathcal{W}^{\star})Robustify ( caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) is this: In Algorithm 4, we proactively make calls to Make-Robustâ¢(w,â‹…)Make-Robustğ‘¤â‹…\textsc{Make-Robust}(w\,,\cdot)Make-Robust ( italic_w , â‹… ) without even checking the first condition in Equation 4, which was the main bottleneck in achieving efficient update time. To be more specific, we proactively call Make-Robustâ¢(w,â‹…)Make-Robustğ‘¤â‹…\textsc{Make-Robust}(w,\cdot)Make-Robust ( italic_w , â‹… ) for every Type I and Type II center wğ‘¤witalic_w (see Algorithm 4). In contrast, for a Type III center wğ‘¤witalic_w, we call Make-Robustâ¢(w,â‹…)Make-Robustğ‘¤â‹…\textsc{Make-Robust}(w,\cdot)Make-Robust ( italic_w , â‹… ) whenever we observe that t>tâ¢[w]ğ‘¡ğ‘¡delimited-[]ğ‘¤t>t[w]italic_t > italic_t [ italic_w ], where tğ‘¡titalic_t is the smallest integer satisfying 10tâ‰¥dâ¢(w,ğ’²âˆ’w)/200superscript10ğ‘¡ğ‘‘ğ‘¤ğ’²ğ‘¤20010^{t}\geq d(w,\mathcal{W}-w)/20010 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT â‰¥ italic_d ( italic_w , caligraphic_W - italic_w ) / 200 (see Algorithms 4 and 4). Note that if tâ‰¤tâ¢[w]ğ‘¡ğ‘¡delimited-[]ğ‘¤t\leq t[w]italic_t â‰¤ italic_t [ italic_w ] in Algorithm 4, then by 5.9 the center wğ‘¤witalic_w does not violate Equation 4. 1 ğ’²1â†ğ’²â‹†âˆ–ğ’°initâ†subscriptğ’²1superscriptğ’²â‹†subscriptğ’°init\mathcal{W}_{1}\leftarrow\mathcal{W}^{\star}\setminus\mathcal{U}_{\textnormal{% {init}}}caligraphic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT â† caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT âˆ– caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT 2 ğ’²2â†{wâˆˆğ’²â‹†âˆ©ğ’°init:wâ¢ is contaminated}â†subscriptğ’²2conditional-setğ‘¤superscriptğ’²â‹†subscriptğ’°initğ‘¤ is contaminated\mathcal{W}_{2}\leftarrow\{w\in\mathcal{W}^{\star}\cap\mathcal{U}_{\textnormal% {{init}}}:w\text{ is contaminated}\}caligraphic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT â† { italic_w âˆˆ caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT âˆ© caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT : italic_w is contaminated } 3 ğ’²3â†{wâˆˆğ’²â‹†âˆ©ğ’°init:wâ¢ is clean}â†subscriptğ’²3conditional-setğ‘¤superscriptğ’²â‹†subscriptğ’°initğ‘¤ is clean\mathcal{W}_{3}\leftarrow\{w\in\mathcal{W}^{\star}\cap\mathcal{U}_{\textnormal% {{init}}}:w\text{ is clean}\}caligraphic_W start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT â† { italic_w âˆˆ caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT âˆ© caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT : italic_w is clean } // The set ğ’²â‹†superscriptğ’²â‹†\mathcal{W}^{\star}caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT is partitioned into the subsets ğ’²1,ğ’²2,ğ’²3subscriptğ’²1subscriptğ’²2subscriptğ’²3\mathcal{W}_{1},\mathcal{W}_{2},\mathcal{W}_{3}caligraphic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , caligraphic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , caligraphic_W start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT 4 ğ’²â†ğ’²â‹†â†ğ’²superscriptğ’²â‹†\mathcal{W}\leftarrow\mathcal{W}^{\star}caligraphic_W â† caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT 5 for each center wâˆˆğ’²1âˆªğ’²2ğ‘¤subscriptğ’²1subscriptğ’²2w\in\mathcal{W}_{1}\cup\mathcal{W}_{2}italic_w âˆˆ caligraphic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT âˆª caligraphic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT do 6 tâ†â†ğ‘¡absentt\leftarrowitalic_t â† Smallest integer satisfying 10tâ‰¥dâ¢(w,ğ’²âˆ’w)/100superscript10ğ‘¡ğ‘‘ğ‘¤ğ’²ğ‘¤10010^{t}\geq d(w,\mathcal{W}-w)/10010 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT â‰¥ italic_d ( italic_w , caligraphic_W - italic_w ) / 100 7 w0â†â†subscriptğ‘¤0absentw_{0}\leftarrowitalic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT â† Make-Robust(w,t)ğ‘¤ğ‘¡(w,t)( italic_w , italic_t ) 8 ğ’²â†ğ’²âˆ’w+w0â†ğ’²ğ’²ğ‘¤subscriptğ‘¤0\mathcal{W}\leftarrow\mathcal{W}-w+w_{0}caligraphic_W â† caligraphic_W - italic_w + italic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT 9 Save tâ¢[w0]â†tâ†ğ‘¡delimited-[]subscriptğ‘¤0ğ‘¡t[w_{0}]\leftarrow titalic_t [ italic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ] â† italic_t and ptâ¢[w0]â¢(w0)â†wâ†subscriptğ‘ğ‘¡delimited-[]subscriptğ‘¤0subscriptğ‘¤0ğ‘¤p_{t[w_{0}]}(w_{0})\leftarrow witalic_p start_POSTSUBSCRIPT italic_t [ italic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ] end_POSTSUBSCRIPT ( italic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) â† italic_w (see Section 5.3) together with w0subscriptğ‘¤0w_{0}italic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT 10 11while true do 12 for each center wâˆˆğ’²3ğ‘¤subscriptğ’²3w\in\mathcal{W}_{3}italic_w âˆˆ caligraphic_W start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT do 13 tâ†â†ğ‘¡absentt\leftarrowitalic_t â† Smallest integer satisfying 10tâ‰¥dâ¢(w,ğ’²âˆ’w)/200superscript10ğ‘¡ğ‘‘ğ‘¤ğ’²ğ‘¤20010^{t}\geq d(w,\mathcal{W}-w)/20010 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT â‰¥ italic_d ( italic_w , caligraphic_W - italic_w ) / 200 14 if t>tâ¢[w]ğ‘¡ğ‘¡delimited-[]ğ‘¤t>t[w]italic_t > italic_t [ italic_w ] then 15 tâ€²â†â†superscriptğ‘¡â€²absentt^{\prime}\leftarrowitalic_t start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT â† Smallest integer satisfying 10tâ‰¥dâ¢(w,ğ’²âˆ’w)/100superscript10ğ‘¡ğ‘‘ğ‘¤ğ’²ğ‘¤10010^{t}\geq d(w,\mathcal{W}-w)/10010 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT â‰¥ italic_d ( italic_w , caligraphic_W - italic_w ) / 100 16 w0â†â†subscriptğ‘¤0absentw_{0}\leftarrowitalic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT â† Make-Robust(w,tâ€²)ğ‘¤superscriptğ‘¡â€²(w,t^{\prime})( italic_w , italic_t start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) 17 ğ’²â†ğ’²âˆ’w+w0â†ğ’²ğ’²ğ‘¤subscriptğ‘¤0\mathcal{W}\leftarrow\mathcal{W}-w+w_{0}caligraphic_W â† caligraphic_W - italic_w + italic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT 18 Save tâ¢[w0]â†tâ€²â†ğ‘¡delimited-[]subscriptğ‘¤0superscriptğ‘¡â€²t[w_{0}]\leftarrow t^{\prime}italic_t [ italic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ] â† italic_t start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT and ptâ¢[w0]â¢(w0)â†wâ†subscriptğ‘ğ‘¡delimited-[]subscriptğ‘¤0subscriptğ‘¤0ğ‘¤p_{t[w_{0}]}(w_{0})\leftarrow witalic_p start_POSTSUBSCRIPT italic_t [ italic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ] end_POSTSUBSCRIPT ( italic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) â† italic_w (see Section 5.3) together with w0subscriptğ‘¤0w_{0}italic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT 19 go back to Line 4. 20 return ğ’°finalâ†ğ’²â†subscriptğ’°finalğ’²\mathcal{U}_{\textnormal{{final}}}\leftarrow\mathcal{W}caligraphic_U start_POSTSUBSCRIPT final end_POSTSUBSCRIPT â† caligraphic_W . Algorithm 4 Modified version of the call to Robustifyâ¢(ğ’²â‹†)Robustifysuperscriptğ’²â‹†\textsc{Robustify}(\mathcal{W}^{\star})Robustify ( caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) at the end of an epoch. Lemma 6.3 (Informal). Lemma 5.8 continues to hold even after the call to Robustifyâ¢(ğ’²â‹†)Robustifysuperscriptğ’²â‹†\textsc{Robustify}(\mathcal{W}^{\star})Robustify ( caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) at the end of every epoch is replaced by the procedure in Algorithm 4. Proof. (Sketch) The lemma holds because the procedure in Algorithm 4 is perfectly aligned with the recourse analysis in Section 5.3. In other words, the recourse analysis accounts for the scenario where we make proactive calls to Make-Robustâ¢(â‹…,â‹…)Make-Robustâ‹…â‹…\textsc{Make-Robust}(\cdot\,,\cdot)Make-Robust ( â‹… , â‹… ), precisely in the manner specified by Algorithm 4. For example, the recourse analysis bounds the number of Type I and Type II calls to Make-Robustâ¢(â‹…,â‹…)Make-Robustâ‹…â‹…\textsc{Make-Robust}(\cdot\,,\cdot)Make-Robust ( â‹… , â‹… ), by pretending that every Type I or Type II center makes such a call, regardless of whether or not it violates Equation 4. âˆ Bounding the Update Time. First, note that we can implement each invocation of Algorithms 4, 4, 4 and 4 in Algorithm 4 in O~â¢(kâ‹…(â„“+1))~ğ‘‚â‹…ğ‘˜â„“1\tilde{O}(k\cdot(\ell+1))over~ start_ARG italic_O end_ARG ( italic_k â‹… ( roman_â„“ + 1 ) ) time, which gets amortized over the length of the epoch, as an epoch lasts for (â„“+1)â„“1(\ell+1)( roman_â„“ + 1 ) updates. Specifically, to compute the sets ğ’²2subscriptğ’²2\mathcal{W}_{2}caligraphic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT and ğ’²3subscriptğ’²3\mathcal{W}_{3}caligraphic_W start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT, we iterate over all wâˆˆğ’²â‹†âˆ©ğ’°initğ‘¤superscriptğ’²â‹†subscriptğ’°initw\in\mathcal{W}^{\star}\cap\mathcal{U}_{\textnormal{{init}}}italic_w âˆˆ caligraphic_W start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT âˆ© caligraphic_U start_POSTSUBSCRIPT init end_POSTSUBSCRIPT and qâˆˆğ’«(0)âŠ•ğ’«(â„“+1)ğ‘direct-sumsuperscriptğ’«0superscriptğ’«â„“1q\in\mathcal{P}^{(0)}\oplus\mathcal{P}^{(\ell+1)}italic_q âˆˆ caligraphic_P start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT âŠ• caligraphic_P start_POSTSUPERSCRIPT ( roman_â„“ + 1 ) end_POSTSUPERSCRIPT, and check whether the update involving qğ‘qitalic_q contaminates wğ‘¤witalic_w, using the value tâ¢[w]ğ‘¡delimited-[]ğ‘¤t[w]italic_t [ italic_w ] and the point ptâ¢[w]â¢(w)subscriptğ‘ğ‘¡delimited-[]ğ‘¤ğ‘¤p_{t[w]}(w)italic_p start_POSTSUBSCRIPT italic_t [ italic_w ] end_POSTSUBSCRIPT ( italic_w ). Moreover, Algorithms 4 and 4 trivially take Oâ¢(k)ğ‘‚ğ‘˜O(k)italic_O ( italic_k ) time. Using the auxiliary data structure (see Section 6.1), we can implement each invocation of Algorithms 4, 4 and 4 in O~â¢(1)~ğ‘‚1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) time. Next, say that an iteration of the for loop in Algorithm 4 is uninterrupted if we find that tâ‰¤tâ¢[w]ğ‘¡ğ‘¡delimited-[]ğ‘¤t\leq t[w]italic_t â‰¤ italic_t [ italic_w ] in Algorithm 4 (and accordingly do not execute any line within the if block) and interrupted otherwise. Each uninterrupted iteration of the for loop takes O~â¢(1)~ğ‘‚1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) time. Furthermore, there can be at most |ğ’²3|=Oâ¢(k)subscriptğ’²3ğ‘‚ğ‘˜|\mathcal{W}_{3}|=O(k)| caligraphic_W start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT | = italic_O ( italic_k ) many consecutive uninterrupted iterations of the for loop in Algorithm 4: Any such chain of uninterrupted iterations is broken (i) either by an interrupted iteration, which involves a call to Make-Robustâ¢(w,tâ€²)Make-Robustğ‘¤superscriptğ‘¡â€²\textsc{Make-Robust}(w,t^{\prime})Make-Robust ( italic_w , italic_t start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) in Algorithm 4, (ii) or by the termination of the procedure in Algorithm 4 (this can happen only once in an epoch). From the preceding discussion, it follows that total time spent on the remaining lines in Algorithm 4 is dominated by the time spent on the calls to Make-Robustâ¢(â‹…,â‹…)Make-Robustâ‹…â‹…\textsc{Make-Robust}(\cdot\,,\cdot)Make-Robust ( â‹… , â‹… ). Recall that at the start of Section 6.2.1, we have already explained that we spend O~â¢(n)~ğ‘‚ğ‘›\tilde{O}(n)over~ start_ARG italic_O end_ARG ( italic_n ) time to implement each call to Make-Robustâ¢(â‹…,â‹…)Make-Robustâ‹…â‹…\textsc{Make-Robust}(\cdot\,,\cdot)Make-Robust ( â‹… , â‹… ). Finally, Lemmas 6.3 and 5.8 imply that we make O~â¢(1)~ğ‘‚1\tilde{O}(1)over~ start_ARG italic_O end_ARG ( 1 ) amortized calls to Make-Robustâ¢(â‹…,â‹…)Make-Robustâ‹…â‹…\textsc{Make-Robust}(\cdot\,,\cdot)Make-Robust ( â‹… , â‹… ), per update. This gives us an overall amortized update time of O~â¢(n)~ğ‘‚ğ‘›\tilde{O}(n)over~ start_ARG italic_O end_ARG ( italic_n ). 6.2.2 Algorithm for Lemma 6.2 Since we have a set of fixed kğ‘˜kitalic_k centers ğ’°ğ’°\mathcal{U}caligraphic_U that must be contained in ğ’°â‹†superscriptğ’°â‹†\mathcal{U}^{\star}caligraphic_U start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT, if we can treat these fixed centers as a single center instead of a set of kğ‘˜kitalic_k centers, we might be able to reduce the problem to a (s+1)ğ‘ 1(s+1)( italic_s + 1 )-median problem. So, we contract all of the points ğ’°ğ’°\mathcal{U}caligraphic_U to a single point uâ‹†superscriptğ‘¢â‹†u^{\star}italic_u start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT and define a new space ğ’«â€²=(ğ’«âˆ’ğ’°)+uâ‹†superscriptğ’«â€²ğ’«ğ’°superscriptğ‘¢â‹†\mathcal{P}^{\prime}=(\mathcal{P}-\mathcal{U})+u^{\star}caligraphic_P start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT = ( caligraphic_P - caligraphic_U ) + italic_u start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT with a new metric dâ€²superscriptğ‘‘â€²d^{\prime}italic_d start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT as follows. {dâ€²â¢(x,uâ‹†):=dâ¢(x,ğ’°)âˆ€xâˆˆğ’«dâ€²â¢(x,y):=minâ¡{dâ€²â¢(x,uâ‹†)+dâ€²â¢(y,uâ‹†),dâ¢(x,y)}âˆ€x,yâˆˆğ’«casesassignsuperscriptğ‘‘â€²ğ‘¥superscriptğ‘¢â‹†ğ‘‘ğ‘¥ğ’°for-allğ‘¥ğ’«assignsuperscriptğ‘‘â€²ğ‘¥ğ‘¦superscriptğ‘‘â€²ğ‘¥superscriptğ‘¢â‹†superscriptğ‘‘â€²ğ‘¦superscriptğ‘¢â‹†ğ‘‘ğ‘¥ğ‘¦for-allğ‘¥ğ‘¦ğ’«\begin{cases}d^{\prime}(x,u^{\star}):=d(x,\mathcal{U})&\forall x\in\mathcal{P}% \\ d^{\prime}(x,y):=\min\{d^{\prime}(x,u^{\star})+d^{\prime}(y,u^{\star}),d(x,y)% \}&\forall x,y\in\mathcal{P}\end{cases}{ start_ROW start_CELL italic_d start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ( italic_x , italic_u start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) := italic_d ( italic_x , caligraphic_U ) end_CELL start_CELL âˆ€ italic_x âˆˆ caligraphic_P end_CELL end_ROW start_ROW start_CELL italic_d start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ( italic_x , italic_y ) := roman_min { italic_d start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ( italic_x , italic_u start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) + italic_d start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ( italic_y , italic_u start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) , italic_d ( italic_x , italic_y ) } end_CELL start_CELL âˆ€ italic_x , italic_y âˆˆ caligraphic_P end_CELL end_ROW (25) This function dâ€²superscriptğ‘‘â€²d^{\prime}italic_d start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT defines a metric on ğ’«â€²superscriptğ’«â€²\mathcal{P}^{\prime}caligraphic_P start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT. A simple way to verify this is that dâ€²superscriptğ‘‘â€²d^{\prime}italic_d start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT is the metric derived from the shortest path in a complete graph where weight of edges between any two nodes in ğ’°ğ’°\mathcal{U}caligraphic_U is zero and the weight of the other edges is the dğ‘‘ditalic_d distance of their endpoints (you can find a complete proof in 11.2). We also define weights for each xâˆˆğ’«â€²ğ‘¥superscriptğ’«â€²x\in\mathcal{P}^{\prime}italic_x âˆˆ caligraphic_P start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT. We define the weight of all xâˆˆğ’«â€²âˆ’uâ‹†ğ‘¥superscriptğ’«â€²superscriptğ‘¢â‹†x\in\mathcal{P}^{\prime}-u^{\star}italic_x âˆˆ caligraphic_P start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT - italic_u start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT to be 1111 and the weight of uâ‹†superscriptğ‘¢â‹†u^{\star}italic_u start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT to be very large denoted by âˆ\inftyâˆ to enforce any constant approximate solution for (s+1)ğ‘ 1(s+1)( italic_s + 1 )-median in ğ’«â€²superscriptğ’«â€²\mathcal{P}^{\prime}caligraphic_P start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT to open center uâ‹†superscriptğ‘¢â‹†u^{\star}italic_u start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT. In order to have access to metric dâ€²superscriptğ‘‘â€²d^{\prime}italic_d start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT, we can simply construct an oracle Dâ€²superscriptğ·â€²D^{\prime}italic_D start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT to compute it. This is because we have access to sorted distances of any pâˆˆğ’«ğ‘ğ’«p\in\mathcal{P}italic_p âˆˆ caligraphic_P to ğ’°ğ’°\mathcal{U}caligraphic_U through ğ’¯psubscriptğ’¯ğ‘\mathcal{T}_{p}caligraphic_T start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT in our auxiliary data structure. Combining with the definition of dâ€²superscriptğ‘‘â€²d^{\prime}italic_d start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT, it is easy to see that we can compute dâ€²â¢(x,y)superscriptğ‘‘â€²ğ‘¥ğ‘¦d^{\prime}(x,y)italic_d start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ( italic_x , italic_y ) in Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 ) time for each x,yâˆˆğ’«â€²ğ‘¥ğ‘¦superscriptğ’«â€²x,y\in\mathcal{P}^{\prime}italic_x , italic_y âˆˆ caligraphic_P start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT. Next, we run the algorithm of [MP02] for (s+1)ğ‘ 1(s+1)( italic_s + 1 )-median problem on ğ’«â€²superscriptğ’«â€²\mathcal{P}^{\prime}caligraphic_P start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT w.r.t. metric dâ€²superscriptğ‘‘â€²d^{\prime}italic_d start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT to find a â„±â„±\mathcal{F}caligraphic_F of size at most (s+1)ğ‘ 1(s+1)( italic_s + 1 ) which is a constant approximation for OPTs+1â¢(ğ’«â€²)subscriptOPTğ‘ 1superscriptğ’«â€²\textnormal{{OPT}}_{s+1}(\mathcal{P}^{\prime})OPT start_POSTSUBSCRIPT italic_s + 1 end_POSTSUBSCRIPT ( caligraphic_P start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) in a total of O~â¢(|ğ’«â€²|â‹…(s+1))=O~â¢(nâ‹…(â„“+1))~ğ‘‚â‹…superscriptğ’«â€²ğ‘ 1~ğ‘‚â‹…ğ‘›â„“1\tilde{O}(|\mathcal{P}^{\prime}|\cdot(s+1))=\tilde{O}(n\cdot(\ell+1))over~ start_ARG italic_O end_ARG ( | caligraphic_P start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT | â‹… ( italic_s + 1 ) ) = over~ start_ARG italic_O end_ARG ( italic_n â‹… ( roman_â„“ + 1 ) ) time. Finally, we let ğ’°â‹†â†ğ’°+(â„±âˆ’uâ‹†)â†superscriptğ’°â‹†ğ’°â„±superscriptğ‘¢â‹†\mathcal{U}^{\star}\leftarrow\mathcal{U}+(\mathcal{F}-u^{\star})caligraphic_U start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT â† caligraphic_U + ( caligraphic_F - italic_u start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ). Note that â„±âˆ’uâ‹†âŠ†ğ’«â„±superscriptğ‘¢â‹†ğ’«\mathcal{F}-u^{\star}\subseteq\mathcal{P}caligraphic_F - italic_u start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT âŠ† caligraphic_P and its size is at most sğ‘ sitalic_s since uâ‹†âˆˆâ„±superscriptğ‘¢â‹†â„±u^{\star}\in\mathcal{F}italic_u start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT âˆˆ caligraphic_F. This set ğ’°+(â„±âˆ’uâ‹†)ğ’°â„±superscriptğ‘¢â‹†\mathcal{U}+(\mathcal{F}-u^{\star})caligraphic_U + ( caligraphic_F - italic_u start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) is going to be a good solution w.r.t. metric dğ‘‘ditalic_d as well as dâ€²superscriptğ‘‘â€²d^{\prime}italic_d start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT (see Lemma 11.3). 1 ğ’«â€²â†(ğ’«âˆ’ğ’°)+uâ‹†â†superscriptğ’«â€²ğ’«ğ’°superscriptğ‘¢â‹†\mathcal{P}^{\prime}\leftarrow(\mathcal{P}-\mathcal{U})+u^{\star}caligraphic_P start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT â† ( caligraphic_P - caligraphic_U ) + italic_u start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT. 2wâ¢(uâ‹†)â†âˆâ†ğ‘¤superscriptğ‘¢â‹†w(u^{\star})\leftarrow\inftyitalic_w ( italic_u start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) â† âˆ, wâ¢(x)â†1â¢âˆ€xâˆˆğ’«â€²âˆ’uâ‹†â†ğ‘¤ğ‘¥1for-allğ‘¥superscriptğ’«â€²superscriptğ‘¢â‹†w(x)\leftarrow 1\ \forall x\in\mathcal{P}^{\prime}-u^{\star}italic_w ( italic_x ) â† 1 âˆ€ italic_x âˆˆ caligraphic_P start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT - italic_u start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT. 3Consider Dâ€²superscriptğ·â€²D^{\prime}italic_D start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT as an oracle to the metric dâ€²superscriptğ‘‘â€²d^{\prime}italic_d start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT defined in Equation 25. 4Compute any Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 ) approximate solution â„±âŠ†ğ’«â€²â„±superscriptğ’«â€²\mathcal{F}\subseteq\mathcal{P}^{\prime}caligraphic_F âŠ† caligraphic_P start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT for (s+1)ğ‘ 1(s+1)( italic_s + 1 )-median problem on weighted metric space ğ’«â€²superscriptğ’«â€²\mathcal{P}^{\prime}caligraphic_P start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT in O~â¢(|ğ’«â€²|â‹…(s+1))~ğ‘‚â‹…superscriptğ’«â€²ğ‘ 1\tilde{O}(|\mathcal{P}^{\prime}|\cdot(s+1))over~ start_ARG italic_O end_ARG ( | caligraphic_P start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT | â‹… ( italic_s + 1 ) ) time using [MP02] with access to distance oracle Dâ€²superscriptğ·â€²D^{\prime}italic_D start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT. 5return â†ğ’°+(â„±âˆ’uâ‹†)â†absentğ’°â„±superscriptğ‘¢â‹†\leftarrow\mathcal{U}+(\mathcal{F}-u^{\star})â† caligraphic_U + ( caligraphic_F - italic_u start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ). Algorithm 5 Computing an approximation of the optimum ğ’°â‹†âŠ‡ğ’°ğ’°superscriptğ’°â‹†\mathcal{U}^{\star}\supseteq\mathcal{U}caligraphic_U start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT âŠ‡ caligraphic_U of size (k+s)ğ‘˜ğ‘ (k+s)( italic_k + italic_s ). 6.3 Improving the Update Time to O~â¢(k)~ğ‘‚ğ‘˜\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) Extension to Weighted Case. First, we argue that our algorithm can be extended to the weighted case defined as follows. We have a metric space ğ’«ğ’«\mathcal{P}caligraphic_P with positive weights wâ¢(p)ğ‘¤ğ‘w(p)italic_w ( italic_p ) for each pâˆˆğ’«ğ‘ğ’«p\in\mathcal{P}italic_p âˆˆ caligraphic_P and distance function dğ‘‘ditalic_d. Denote this weighted space by (ğ’«,w,d)ğ’«ğ‘¤ğ‘‘(\mathcal{P},w,d)( caligraphic_P , italic_w , italic_d ). The cost of a collection ğ’°ğ’°\mathcal{U}caligraphic_U of kğ‘˜kitalic_k centers is defined as Costâ¢(ğ’°,ğ’«)=âˆ‘pâˆˆğ’«wâ¢(p)â‹…dâ¢(p,ğ’°)Costğ’°ğ’«subscriptğ‘ğ’«â‹…ğ‘¤ğ‘ğ‘‘ğ‘ğ’°\textnormal{{Cost}}\left(\mathcal{U},\mathcal{P}\right)=\sum_{p\in\mathcal{P}}% w(p)\cdot d(p,\mathcal{U})Cost ( caligraphic_U , caligraphic_P ) = âˆ‘ start_POSTSUBSCRIPT italic_p âˆˆ caligraphic_P end_POSTSUBSCRIPT italic_w ( italic_p ) â‹… italic_d ( italic_p , caligraphic_U ) and subsequently AverageCostâ¢(ğ’°,S)=Costâ¢(ğ’°,S)/(âˆ‘pâˆˆSwâ¢(p))AverageCostğ’°ğ‘†Costğ’°ğ‘†subscriptğ‘ğ‘†ğ‘¤ğ‘\textnormal{{AverageCost}}\left(\mathcal{U},S\right)=\textnormal{{Cost}}\left(% \mathcal{U},S\right)/(\sum_{p\in S}w(p))AverageCost ( caligraphic_U , italic_S ) = Cost ( caligraphic_U , italic_S ) / ( âˆ‘ start_POSTSUBSCRIPT italic_p âˆˆ italic_S end_POSTSUBSCRIPT italic_w ( italic_p ) ) for all SâŠ†ğ’«ğ‘†ğ’«S\subseteq\mathcal{P}italic_S âŠ† caligraphic_P. We can extend our algorithm and all of the arguments for weighted case. Sparsification. Note that parameter nğ‘›nitalic_n in our algorithm is the maximum size of the space at any time during the total sequence of updates (it is not the size of the underlying ground metric space ğğ\mathbf{P}bold_P). As a result, if we make sure that the size of the space ğ’«ğ’«\mathcal{P}caligraphic_P is at most O~â¢(k)~ğ‘‚ğ‘˜\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) at any point in time, then the amortized running time of the algorithm would be O~â¢(k)~ğ‘‚ğ‘˜\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) as desired. We use the result of [BCLP23] to sparsify the input. A simple generalization of this result is presented in Section 10 of [BCG+24] which extends this sparsifier to weighted metric spaces. The authors provided an algorithm to sparsify the space to O~â¢(k)~ğ‘‚ğ‘˜\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) weighted points. More precisely, given a dynamic metric space (ğ’«,w,d)ğ’«ğ‘¤ğ‘‘(\mathcal{P},w,d)( caligraphic_P , italic_w , italic_d ) and parameter kâˆˆâ„•ğ‘˜â„•k\in\mathbb{N}italic_k âˆˆ blackboard_N, there is an algorithm that maintains a dynamic metric space (ğ’«â€²,wâ€²,d)superscriptğ’«â€²superscriptğ‘¤â€²ğ‘‘(\mathcal{P}^{\prime},w^{\prime},d)( caligraphic_P start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT , italic_w start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT , italic_d ) in O~â¢(k)~ğ‘‚ğ‘˜\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ) amortized update time such that the following holds. â€¢ ğ’«â€²âŠ†ğ’«superscriptğ’«â€²ğ’«\mathcal{P}^{\prime}\subseteq\mathcal{P}caligraphic_P start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT âŠ† caligraphic_P and the size of ğ’«â€²superscriptğ’«â€²\mathcal{P}^{\prime}caligraphic_P start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT at any time is O~â¢(k)~ğ‘‚ğ‘˜\tilde{O}(k)over~ start_ARG italic_O end_ARG ( italic_k ). â€¢ A sequence of Tğ‘‡Titalic_T updates in (ğ’«,w,d)ğ’«ğ‘¤ğ‘‘(\mathcal{P},w,d)( caligraphic_P , italic_w , italic_d ), leads to a sequence of Oâ¢(T)ğ‘‚ğ‘‡O(T)italic_O ( italic_T ) updates in (ğ’«â€²,wâ€²,d)superscriptğ’«â€²superscriptğ‘¤â€²ğ‘‘(\mathcal{P}^{\prime},w^{\prime},d)( caligraphic_P start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT , italic_w start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT , italic_d ).131313This guarantee follows from a slightly more refined analysis of the recourse of this sparsifier which is presented in [BCLP24] (see Lemma 3.4 of [BCLP24]). â€¢ Every Î±ğ›¼\alphaitalic_Î± approximate solution to the kğ‘˜kitalic_k-median problem in the metric space (ğ’«â€²,wâ€²,d)superscriptğ’«â€²superscriptğ‘¤â€²ğ‘‘(\mathcal{P}^{\prime},w^{\prime},d)( caligraphic_P start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT , italic_w start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT , italic_d ) is also a Oâ¢(Î±)ğ‘‚ğ›¼O(\alpha)italic_O ( italic_Î± ) approximate solution to the kğ‘˜kitalic_k-median problem in the metric space (ğ’«,w,d)ğ’«ğ‘¤ğ‘‘(\mathcal{P},w,d)( caligraphic_P , italic_w , italic_d ) with probability at least 1âˆ’O~â¢(1/nc)1~ğ‘‚1superscriptğ‘›ğ‘1-\tilde{O}(1/n^{c})1 - over~ start_ARG italic_O end_ARG ( 1 / italic_n start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ). Now, suppose that we are given a sequence of updates Ïƒ1,Ïƒ2,â€¦,ÏƒTsubscriptğœ1subscriptğœ2â€¦subscriptğœğ‘‡\sigma_{1},\sigma_{2},\ldots,\sigma_{T}italic_Ïƒ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_Ïƒ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_Ïƒ start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT in a dynamic metric space (ğ’«,w,d)ğ’«ğ‘¤ğ‘‘(\mathcal{P},w,d)( caligraphic_P , italic_w , italic_d ). Instead of feeding the metric space (ğ’«,w,d)ğ’«ğ‘¤ğ‘‘(\mathcal{P},w,d)( caligraphic_P , italic_w , italic_d ) directly to our algorithm, we can perform this dynamic sparsification to obtain a sequence of updates Ïƒ1â€²,Ïƒ2â€²,â€¦,ÏƒTâ€²â€²subscriptsuperscriptğœâ€²1subscriptsuperscriptğœâ€²2â€¦subscriptsuperscriptğœâ€²superscriptğ‘‡â€²\sigma^{\prime}_{1},\sigma^{\prime}_{2},\ldots,\sigma^{\prime}_{T^{\prime}}italic_Ïƒ start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_Ïƒ start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_Ïƒ start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_T start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT end_POSTSUBSCRIPT for a metric space (ğ’«â€²,wâ€²,d)superscriptğ’«â€²superscriptğ‘¤â€²ğ‘‘(\mathcal{P}^{\prime},w^{\prime},d)( caligraphic_P start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT , italic_w start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT , italic_d ), where Tâ€²=Oâ¢(T)superscriptğ‘‡â€²ğ‘‚ğ‘‡T^{\prime}=O(T)italic_T start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT = italic_O ( italic_T ), and feed the metric space (ğ’«â€²,wâ€²,d)superscriptğ’«â€²superscriptğ‘¤â€²ğ‘‘(\mathcal{P}^{\prime},w^{\prime},d)( caligraphic_P start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT , italic_w start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT , italic_d ) to our dynamic algorithm instead. Since our algorithm maintains a Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 ) approximate solution ğ’°ğ’°\mathcal{U}caligraphic_U to the kğ‘˜kitalic_k-median problem in (ğ’«â€²,wâ€²,d)superscriptğ’«â€²superscriptğ‘¤â€²ğ‘‘(\mathcal{P}^{\prime},w^{\prime},d)( caligraphic_P start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT , italic_w start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT , italic_d ), then ğ’°ğ’°\mathcal{U}caligraphic_U is also a Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 ) approximate solution for the kğ‘˜kitalic_k-median problem in (ğ’«,w,d)ğ’«ğ‘¤ğ‘‘(\mathcal{P},w,d)( caligraphic_P , italic_w , italic_d ) with high probability. Since the length of the stream is multiplied by Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 ), we would have a multiplicative overhead of Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 ) in the amortized update time and recourse (amortized w.r.t. the original input stream). 7 Missing Proofs From Section 4 7.1 Proof of Lemma 4.1: Double-Sided Stability Lemma Consider the LP relaxation for improper kğ‘˜kitalic_k-median problem on ğ’«ğ’«\mathcal{P}caligraphic_P for each kğ‘˜kitalic_k as follows. min\displaystyle\minroman_min âˆ‘pâˆˆğ’«âˆ‘câˆˆğdâ¢(c,p)â‹…xcâ¢psubscriptğ‘ğ’«subscriptğ‘ğâ‹…ğ‘‘ğ‘ğ‘subscriptğ‘¥ğ‘ğ‘\displaystyle\sum_{p\in\mathcal{P}}\sum_{c\in\mathbf{P}}d(c,p)\cdot x_{cp}âˆ‘ start_POSTSUBSCRIPT italic_p âˆˆ caligraphic_P end_POSTSUBSCRIPT âˆ‘ start_POSTSUBSCRIPT italic_c âˆˆ bold_P end_POSTSUBSCRIPT italic_d ( italic_c , italic_p ) â‹… italic_x start_POSTSUBSCRIPT italic_c italic_p end_POSTSUBSCRIPT s.t. xcâ¢pâ‰¤ycsubscriptğ‘¥ğ‘ğ‘subscriptğ‘¦ğ‘\displaystyle\quad x_{cp}\leq y_{c}italic_x start_POSTSUBSCRIPT italic_c italic_p end_POSTSUBSCRIPT â‰¤ italic_y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT âˆ€câˆˆğ,pâˆˆğ’«formulae-sequencefor-allğ‘ğğ‘ğ’«\displaystyle\forall c\in\mathbf{P},p\in\mathcal{P}âˆ€ italic_c âˆˆ bold_P , italic_p âˆˆ caligraphic_P âˆ‘câˆˆğxcâ¢pâ‰¥1subscriptğ‘ğsubscriptğ‘¥ğ‘ğ‘1\displaystyle\sum_{c\in\mathbf{P}}x_{cp}\geq 1âˆ‘ start_POSTSUBSCRIPT italic_c âˆˆ bold_P end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_c italic_p end_POSTSUBSCRIPT â‰¥ 1 âˆ€pâˆˆğ’«for-allğ‘ğ’«\displaystyle\forall p\in\mathcal{P}âˆ€ italic_p âˆˆ caligraphic_P âˆ‘câˆˆğycâ‰¤ksubscriptğ‘ğsubscriptğ‘¦ğ‘ğ‘˜\displaystyle\sum_{c\in\mathbf{P}}y_{c}\leq kâˆ‘ start_POSTSUBSCRIPT italic_c âˆˆ bold_P end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT â‰¤ italic_k xcâ¢p,ycâ‰¥0subscriptğ‘¥ğ‘ğ‘subscriptğ‘¦ğ‘0\displaystyle x_{cp},y_{c}\geq 0italic_x start_POSTSUBSCRIPT italic_c italic_p end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT â‰¥ 0 âˆ€câˆˆğ,pâˆˆğ’«formulae-sequencefor-allğ‘ğğ‘ğ’«\displaystyle\forall c\in\mathbf{P},p\in\mathcal{P}âˆ€ italic_c âˆˆ bold_P , italic_p âˆˆ caligraphic_P Denote the cost of the optimal fractional solution for this LP by FOPTksubscriptFOPTğ‘˜\textnormal{{FOPT}}_{k}FOPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT. Since space ğ’«ğ’«\mathcal{P}caligraphic_P is fixed here, we denote OPTkâ¢(ğ’«)subscriptOPTğ‘˜ğ’«\textnormal{{OPT}}_{k}(\mathcal{P})OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( caligraphic_P ) by OPTksubscriptOPTğ‘˜\textnormal{{OPT}}_{k}OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT. It is known that the integrality gap of this relaxation is at most 3333 [CS11]. So, for every kğ‘˜kitalic_k we have FOPTkâ‰¤OPTkâ‰¤3â‹…FOPTk.subscriptFOPTğ‘˜subscriptOPTğ‘˜â‹…3subscriptFOPTğ‘˜\textnormal{{FOPT}}_{k}\leq\textnormal{{OPT}}_{k}\leq 3\cdot\textnormal{{FOPT}% }_{k}.FOPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT â‰¤ OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT â‰¤ 3 â‹… FOPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT . (26) Claim 7.1. For every k1subscriptğ‘˜1k_{1}italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, k2subscriptğ‘˜2k_{2}italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT and 0â‰¤Î±,Î²â‰¤1formulae-sequence0ğ›¼ğ›½10\leq\alpha,\beta\leq 10 â‰¤ italic_Î± , italic_Î² â‰¤ 1 such that Î±+Î²=1ğ›¼ğ›½1\alpha+\beta=1italic_Î± + italic_Î² = 1, we have FOPTÎ±â¢k1+Î²â¢k2â‰¤Î±â‹…FOPTk1+Î²â‹…FOPTk2.subscriptFOPTğ›¼subscriptğ‘˜1ğ›½subscriptğ‘˜2â‹…ğ›¼subscriptFOPTsubscriptğ‘˜1â‹…ğ›½subscriptFOPTsubscriptğ‘˜2\textnormal{{FOPT}}_{\alpha k_{1}+\beta k_{2}}\leq\alpha\cdot\textnormal{{FOPT% }}_{k_{1}}+\beta\cdot\textnormal{{FOPT}}_{k_{2}}.FOPT start_POSTSUBSCRIPT italic_Î± italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_Î² italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT â‰¤ italic_Î± â‹… FOPT start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT + italic_Î² â‹… FOPT start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT . Proof. Assume optimal fractional solutions (x1âˆ—,y1âˆ—)subscriptsuperscriptğ‘¥1subscriptsuperscriptğ‘¦1(x^{*}_{1},y^{*}_{1})( italic_x start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_y start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) and (x2âˆ—,y2âˆ—)subscriptsuperscriptğ‘¥2subscriptsuperscriptğ‘¦2(x^{*}_{2},y^{*}_{2})( italic_x start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_y start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) to be an optimal solution for above LP for fractional k1subscriptğ‘˜1k_{1}italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and k2subscriptğ‘˜2k_{2}italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT-median problems respectively. It is easy to verify that their convex combination (Î±â¢x1âˆ—+Î²â¢x2âˆ—,Î±â¢y1âˆ—+Î²â¢y2âˆ—)ğ›¼subscriptsuperscriptğ‘¥1ğ›½subscriptsuperscriptğ‘¥2ğ›¼subscriptsuperscriptğ‘¦1ğ›½subscriptsuperscriptğ‘¦2(\alpha x^{*}_{1}+\beta x^{*}_{2},\alpha y^{*}_{1}+\beta y^{*}_{2})( italic_Î± italic_x start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_Î² italic_x start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_Î± italic_y start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_Î² italic_y start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) is a feasible solution for LP relaxation of (Î±â¢k1+Î²â¢k2)ğ›¼subscriptğ‘˜1ğ›½subscriptğ‘˜2(\alpha k_{1}+\beta k_{2})( italic_Î± italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_Î² italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT )-median problem whose cost is Î±â¢FOPTk1+Î²â¢FOPTk2ğ›¼subscriptFOPTsubscriptğ‘˜1ğ›½subscriptFOPTsubscriptğ‘˜2\alpha\ \textnormal{{FOPT}}_{k_{1}}+\beta\ \textnormal{{FOPT}}_{k_{2}}italic_Î± FOPT start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT + italic_Î² FOPT start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT which concludes the claim. âˆ Now, plug k1=kâˆ’rsubscriptğ‘˜1ğ‘˜ğ‘Ÿk_{1}=k-ritalic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = italic_k - italic_r, k2=k+r/(12â¢Î·)subscriptğ‘˜2ğ‘˜ğ‘Ÿ12ğœ‚k_{2}=k+r/(12\eta)italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = italic_k + italic_r / ( 12 italic_Î· ), Î±=1/(12â¢Î·)ğ›¼112ğœ‚\alpha=1/(12\eta)italic_Î± = 1 / ( 12 italic_Î· ) and Î²=1âˆ’Î±ğ›½1ğ›¼\beta=1-\alphaitalic_Î² = 1 - italic_Î± in the claim. We have Î±â¢k1+Î²â¢k2=112â¢Î·â¢(kâˆ’r)+(1âˆ’112â¢Î·)â¢(k+r12â¢Î·)=kâˆ’r(12â¢Î·)2â‰¤k.ğ›¼subscriptğ‘˜1ğ›½subscriptğ‘˜2112ğœ‚ğ‘˜ğ‘Ÿ1112ğœ‚ğ‘˜ğ‘Ÿ12ğœ‚ğ‘˜ğ‘Ÿsuperscript12ğœ‚2ğ‘˜\alpha k_{1}+\beta k_{2}=\frac{1}{12\eta}(k-r)+\left(1-\frac{1}{12\eta}\right)% \left(k+\frac{r}{12\eta}\right)=k-\frac{r}{(12\eta)^{2}}\leq k.italic_Î± italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_Î² italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG 12 italic_Î· end_ARG ( italic_k - italic_r ) + ( 1 - divide start_ARG 1 end_ARG start_ARG 12 italic_Î· end_ARG ) ( italic_k + divide start_ARG italic_r end_ARG start_ARG 12 italic_Î· end_ARG ) = italic_k - divide start_ARG italic_r end_ARG start_ARG ( 12 italic_Î· ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG â‰¤ italic_k . As a result, FOPTkâ‰¤FOPTÎ±â¢k1+Î²â¢k2â‰¤Î±â¢FOPTk1+Î²â¢FOPTk2subscriptFOPTğ‘˜subscriptFOPTğ›¼subscriptğ‘˜1ğ›½subscriptğ‘˜2ğ›¼subscriptFOPTsubscriptğ‘˜1ğ›½subscriptFOPTsubscriptğ‘˜2\textnormal{{FOPT}}_{k}\leq\textnormal{{FOPT}}_{\alpha k_{1}+\beta k_{2}}\leq% \alpha\ \textnormal{{FOPT}}_{k_{1}}+\beta\ \textnormal{{FOPT}}_{k_{2}}FOPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT â‰¤ FOPT start_POSTSUBSCRIPT italic_Î± italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_Î² italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT â‰¤ italic_Î± FOPT start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT + italic_Î² FOPT start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT. Together with Equation 26, we have OPTkâ‰¤3â¢Î±â‹…OPTk1+3â¢Î²â‹…OPTk2subscriptOPTğ‘˜â‹…3ğ›¼subscriptOPTsubscriptğ‘˜1â‹…3ğ›½subscriptOPTsubscriptğ‘˜2\textnormal{{OPT}}_{k}\leq 3\alpha\cdot\textnormal{{OPT}}_{k_{1}}+3\beta\cdot% \textnormal{{OPT}}_{k_{2}}OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT â‰¤ 3 italic_Î± â‹… OPT start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT + 3 italic_Î² â‹… OPT start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT. We also have the assumption that OPTk1=OPTkâˆ’râ‰¤Î·â‹…OPTksubscriptOPTsubscriptğ‘˜1subscriptOPTğ‘˜ğ‘Ÿâ‹…ğœ‚subscriptOPTğ‘˜\textnormal{{OPT}}_{k_{1}}=\textnormal{{OPT}}_{k-r}\leq\eta\cdot\textnormal{{% OPT}}_{k}OPT start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = OPT start_POSTSUBSCRIPT italic_k - italic_r end_POSTSUBSCRIPT â‰¤ italic_Î· â‹… OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, which implies OPTkâ‰¤3â¢Î±â¢Î·â‹…OPTk+3â¢Î²â‹…OPTk2subscriptOPTğ‘˜â‹…3ğ›¼ğœ‚subscriptOPTğ‘˜â‹…3ğ›½subscriptOPTsubscriptğ‘˜2\textnormal{{OPT}}_{k}\leq 3\alpha\eta\cdot\textnormal{{OPT}}_{k}+3\beta\cdot% \textnormal{{OPT}}_{k_{2}}OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT â‰¤ 3 italic_Î± italic_Î· â‹… OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + 3 italic_Î² â‹… OPT start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT. Finally, OPTkâ‰¤(3â¢Î²1âˆ’3â¢Î±â¢Î·)â‹…OPTk2â‰¤4â‹…OPTk2â‰¤4â‹…OPTk+âŒŠr/(12â¢Î·)âŒ‹.subscriptOPTğ‘˜â‹…3ğ›½13ğ›¼ğœ‚subscriptOPTsubscriptğ‘˜2â‹…4subscriptOPTsubscriptğ‘˜2â‹…4subscriptOPTğ‘˜ğ‘Ÿ12ğœ‚\textnormal{{OPT}}_{k}\leq\left(\frac{3\beta}{1-3\alpha\eta}\right)\cdot% \textnormal{{OPT}}_{k_{2}}\leq 4\cdot\textnormal{{OPT}}_{k_{2}}\leq 4\cdot% \textnormal{{OPT}}_{k+\lfloor r/(12\eta)\rfloor}.OPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT â‰¤ ( divide start_ARG 3 italic_Î² end_ARG start_ARG 1 - 3 italic_Î± italic_Î· end_ARG ) â‹… OPT start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT â‰¤ 4 â‹… OPT start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT â‰¤ 4 â‹… OPT start_POSTSUBSCRIPT italic_k + âŒŠ italic_r / ( 12 italic_Î· ) âŒ‹ end_POSTSUBSCRIPT . The second inequality holds since 3â¢Î²1âˆ’3â¢Î±â¢Î·â‰¤31âˆ’3â¢Î±â¢Î·=31âˆ’1/4=4.3ğ›½13ğ›¼ğœ‚313ğ›¼ğœ‚31144\frac{3\beta}{1-3\alpha\eta}\leq\frac{3}{1-3\alpha\eta}=\frac{3}{1-1/4}=4.divide start_ARG 3 italic_Î² end_ARG start_ARG 1 - 3 italic_Î± italic_Î· end_ARG â‰¤ divide start_ARG 3 end_ARG start_ARG 1 - 3 italic_Î± italic_Î· end_ARG = divide start_ARG 3 end_ARG start_ARG 1 - 1 / 4 end_ARG = 4 . 7.2 Proof of Lemma 4.2 Consider the standard LP relaxation for the weighted kğ‘˜kitalic_k-median problem as follows. min\displaystyle\minroman_min âˆ‘pâˆˆğ’«âˆ‘uâˆˆğ’°dâ¢(u,p)â‹…xuâ¢psubscriptğ‘ğ’«subscriptğ‘¢ğ’°â‹…ğ‘‘ğ‘¢ğ‘subscriptğ‘¥ğ‘¢ğ‘\displaystyle\sum_{p\in\mathcal{P}}\sum_{u\in\mathcal{U}}d(u,p)\cdot x_{up}âˆ‘ start_POSTSUBSCRIPT italic_p âˆˆ caligraphic_P end_POSTSUBSCRIPT âˆ‘ start_POSTSUBSCRIPT italic_u âˆˆ caligraphic_U end_POSTSUBSCRIPT italic_d ( italic_u , italic_p ) â‹… italic_x start_POSTSUBSCRIPT italic_u italic_p end_POSTSUBSCRIPT s.t. xuâ¢pâ‰¤yusubscriptğ‘¥ğ‘¢ğ‘subscriptğ‘¦ğ‘¢\displaystyle\quad x_{up}\leq y_{u}italic_x start_POSTSUBSCRIPT italic_u italic_p end_POSTSUBSCRIPT â‰¤ italic_y start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT âˆ€uâˆˆğ’°,pâˆˆğ’«formulae-sequencefor-allğ‘¢ğ’°ğ‘ğ’«\displaystyle\forall u\in\mathcal{U},p\in\mathcal{P}âˆ€ italic_u âˆˆ caligraphic_U , italic_p âˆˆ caligraphic_P âˆ‘uâˆˆğ’°xuâ¢pâ‰¥1subscriptğ‘¢ğ’°subscriptğ‘¥ğ‘¢ğ‘1\displaystyle\sum_{u\in\mathcal{U}}x_{up}\geq 1âˆ‘ start_POSTSUBSCRIPT italic_u âˆˆ caligraphic_U end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_u italic_p end_POSTSUBSCRIPT â‰¥ 1 âˆ€pâˆˆğ’«for-allğ‘ğ’«\displaystyle\forall p\in\mathcal{P}âˆ€ italic_p âˆˆ caligraphic_P âˆ‘uâˆˆğ’°yuâ‰¤kâˆ’(mâˆ’r)/4subscriptğ‘¢ğ’°subscriptğ‘¦ğ‘¢ğ‘˜ğ‘šğ‘Ÿ4\displaystyle\sum_{u\in\mathcal{U}}y_{u}\leq k-(m-r)/4âˆ‘ start_POSTSUBSCRIPT italic_u âˆˆ caligraphic_U end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT â‰¤ italic_k - ( italic_m - italic_r ) / 4 xuâ¢p,yuâ‰¥0subscriptğ‘¥ğ‘¢ğ‘subscriptğ‘¦ğ‘¢0\displaystyle x_{up},y_{u}\geq 0italic_x start_POSTSUBSCRIPT italic_u italic_p end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT â‰¥ 0 âˆ€uâˆˆğ’°,pâˆˆğ’«formulae-sequencefor-allğ‘¢ğ’°ğ‘ğ’«\displaystyle\forall u\in\mathcal{U},p\in\mathcal{P}âˆ€ italic_u âˆˆ caligraphic_U , italic_p âˆˆ caligraphic_P We consider the set of potential centers to open is ğ’°ğ’°\mathcal{U}caligraphic_U, and we want to open at most kâˆ’(mâˆ’r)/4ğ‘˜ğ‘šğ‘Ÿ4k-(m-r)/4italic_k - ( italic_m - italic_r ) / 4 many centers. Since the integrality gap of this LP is known to be at most 3333 [CS11], it suffices to show the existence of a fractional solution whose cost is at most 2â¢Î³â‹…(Costâ¢(ğ’°,ğ’«)+Costâ¢(ğ’±,ğ’«))â‹…2ğ›¾Costğ’°ğ’«Costğ’±ğ’«2\gamma\cdot\left(\textnormal{{Cost}}\left(\mathcal{U},\mathcal{P}\right)+% \textnormal{{Cost}}\left(\mathcal{V},\mathcal{P}\right)\right)2 italic_Î³ â‹… ( Cost ( caligraphic_U , caligraphic_P ) + Cost ( caligraphic_V , caligraphic_P ) ) and this solution opens at most kâˆ’(mâˆ’r)/4ğ‘˜ğ‘šğ‘Ÿ4k-(m-r)/4italic_k - ( italic_m - italic_r ) / 4 centers. Now, we explain how to construct a fractional solution for this LP. Fractional Opening of Centers. Consider the projection Ï€ğ’°:ğ’±â†’ğ’°:subscriptğœ‹ğ’°â†’ğ’±ğ’°\pi_{\mathcal{U}}:\mathcal{V}\rightarrow\mathcal{U}italic_Ï€ start_POSTSUBSCRIPT caligraphic_U end_POSTSUBSCRIPT : caligraphic_V â†’ caligraphic_U function. Assume ğ’°=ğ’°I+ğ’°Fğ’°subscriptğ’°ğ¼subscriptğ’°ğ¹\mathcal{U}=\mathcal{U}_{I}+\mathcal{U}_{F}caligraphic_U = caligraphic_U start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT + caligraphic_U start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT is a partition of ğ’°ğ’°\mathcal{U}caligraphic_U where ğ’°Isubscriptğ’°ğ¼\mathcal{U}_{I}caligraphic_U start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT contains those centers uâˆˆğ’°ğ‘¢ğ’°u\in\mathcal{U}italic_u âˆˆ caligraphic_U satisfying at least one of the following conditions: â€¢ uğ‘¢uitalic_u forms a well-separated pair with one center in ğ’±ğ’±\mathcal{V}caligraphic_V. â€¢ |Ï€ğ’°âˆ’1â¢(u)|â‰¥2subscriptsuperscriptğœ‹1ğ’°ğ‘¢2|\pi^{-1}_{\mathcal{U}}(u)|\geq 2| italic_Ï€ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT caligraphic_U end_POSTSUBSCRIPT ( italic_u ) | â‰¥ 2. For every uâˆˆğ’°Iğ‘¢subscriptğ’°ğ¼u\in\mathcal{U}_{I}italic_u âˆˆ caligraphic_U start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT, set yu=1subscriptğ‘¦ğ‘¢1y_{u}=1italic_y start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT = 1 and for every uâˆˆğ’°Fğ‘¢subscriptğ’°ğ¹u\in\mathcal{U}_{F}italic_u âˆˆ caligraphic_U start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT set yu=1/2subscriptğ‘¦ğ‘¢12y_{u}=1/2italic_y start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT = 1 / 2. Each center uâˆˆğ’°ğ‘¢ğ’°u\in\mathcal{U}italic_u âˆˆ caligraphic_U that forms a well-separated pair with a center vâˆˆğ’±ğ‘£ğ’±v\in\mathcal{V}italic_v âˆˆ caligraphic_V has |Ï€ğ’°âˆ’1â¢(u)|â‰¥1subscriptsuperscriptğœ‹1ğ’°ğ‘¢1|\pi^{-1}_{\mathcal{U}}(u)|\geq 1| italic_Ï€ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT caligraphic_U end_POSTSUBSCRIPT ( italic_u ) | â‰¥ 1 since uğ‘¢uitalic_u must be the closest center to vğ‘£vitalic_v in ğ’°ğ’°\mathcal{U}caligraphic_U. Since the number of well-separated pairs is kâˆ’mğ‘˜ğ‘šk-mitalic_k - italic_m, we have k+r=|ğ’±|=âˆ‘uâˆˆğ’°|Ï€ğ’°âˆ’1â¢(u)|â‰¥âˆ‘uâˆˆğ’°I|Ï€ğ’°âˆ’1â¢(u)|â‰¥1â‹…(kâˆ’m)+2â‹…(|ğ’°I|âˆ’(kâˆ’m)).ğ‘˜ğ‘Ÿğ’±subscriptğ‘¢ğ’°subscriptsuperscriptğœ‹1ğ’°ğ‘¢subscriptğ‘¢subscriptğ’°ğ¼subscriptsuperscriptğœ‹1ğ’°ğ‘¢â‹…1ğ‘˜ğ‘šâ‹…2subscriptğ’°ğ¼ğ‘˜ğ‘šk+r=|\mathcal{V}|=\sum_{u\in\mathcal{U}}|\pi^{-1}_{\mathcal{U}}(u)|\geq\sum_{u% \in\mathcal{U}_{I}}|\pi^{-1}_{\mathcal{U}}(u)|\geq 1\cdot(k-m)+2\cdot\left(|% \mathcal{U}_{I}|-(k-m)\right).italic_k + italic_r = | caligraphic_V | = âˆ‘ start_POSTSUBSCRIPT italic_u âˆˆ caligraphic_U end_POSTSUBSCRIPT | italic_Ï€ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT caligraphic_U end_POSTSUBSCRIPT ( italic_u ) | â‰¥ âˆ‘ start_POSTSUBSCRIPT italic_u âˆˆ caligraphic_U start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT end_POSTSUBSCRIPT | italic_Ï€ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT caligraphic_U end_POSTSUBSCRIPT ( italic_u ) | â‰¥ 1 â‹… ( italic_k - italic_m ) + 2 â‹… ( | caligraphic_U start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT | - ( italic_k - italic_m ) ) . Hence, |ğ’°I|â‰¤k+r+(kâˆ’m)2=kâˆ’mâˆ’r2subscriptğ’°ğ¼ğ‘˜ğ‘Ÿğ‘˜ğ‘š2ğ‘˜ğ‘šğ‘Ÿ2|\mathcal{U}_{I}|\leq\frac{k+r+(k-m)}{2}=k-\frac{m-r}{2}| caligraphic_U start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT | â‰¤ divide start_ARG italic_k + italic_r + ( italic_k - italic_m ) end_ARG start_ARG 2 end_ARG = italic_k - divide start_ARG italic_m - italic_r end_ARG start_ARG 2 end_ARG. Finally, we conclude âˆ‘uâˆˆğ’°yu=âˆ‘uâˆˆğ’°Iyu+âˆ‘uâˆˆğ’°Fyuâ‰¤1â‹…(kâˆ’mâˆ’r2)+12â‹…mâˆ’r2=kâˆ’mâˆ’r4.subscriptğ‘¢ğ’°subscriptğ‘¦ğ‘¢subscriptğ‘¢subscriptğ’°ğ¼subscriptğ‘¦ğ‘¢subscriptğ‘¢subscriptğ’°ğ¹subscriptğ‘¦ğ‘¢â‹…1ğ‘˜ğ‘šğ‘Ÿ2â‹…12ğ‘šğ‘Ÿ2ğ‘˜ğ‘šğ‘Ÿ4\displaystyle\sum_{u\in\mathcal{U}}y_{u}=\sum_{u\in\mathcal{U}_{I}}y_{u}+\sum_% {u\in\mathcal{U}_{F}}y_{u}\leq 1\cdot\left(k-\frac{m-r}{2}\right)+\frac{1}{2}% \cdot\frac{m-r}{2}=k-\frac{m-r}{4}.âˆ‘ start_POSTSUBSCRIPT italic_u âˆˆ caligraphic_U end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT = âˆ‘ start_POSTSUBSCRIPT italic_u âˆˆ caligraphic_U start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT + âˆ‘ start_POSTSUBSCRIPT italic_u âˆˆ caligraphic_U start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT â‰¤ 1 â‹… ( italic_k - divide start_ARG italic_m - italic_r end_ARG start_ARG 2 end_ARG ) + divide start_ARG 1 end_ARG start_ARG 2 end_ARG â‹… divide start_ARG italic_m - italic_r end_ARG start_ARG 2 end_ARG = italic_k - divide start_ARG italic_m - italic_r end_ARG start_ARG 4 end_ARG . Fractional Assignment of Points. For every pâˆˆğ’«ğ‘ğ’«p\in\mathcal{P}italic_p âˆˆ caligraphic_P, assume vp=Ï€ğ’±â¢(p)subscriptğ‘£ğ‘subscriptğœ‹ğ’±ğ‘v_{p}=\pi_{\mathcal{V}}(p)italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT = italic_Ï€ start_POSTSUBSCRIPT caligraphic_V end_POSTSUBSCRIPT ( italic_p ) is the closest center to pğ‘pitalic_p in ğ’±ğ’±\mathcal{V}caligraphic_V and up=Ï€ğ’°â¢(vp)subscriptğ‘¢ğ‘subscriptğœ‹ğ’°subscriptğ‘£ğ‘u_{p}=\pi_{\mathcal{U}}(v_{p})italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT = italic_Ï€ start_POSTSUBSCRIPT caligraphic_U end_POSTSUBSCRIPT ( italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) is the closest center in ğ’°ğ’°\mathcal{U}caligraphic_U to vpsubscriptğ‘£ğ‘v_{p}italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT. We have three cases: â€¢ If yup=1subscriptğ‘¦subscriptğ‘¢ğ‘1y_{u_{p}}=1italic_y start_POSTSUBSCRIPT italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT end_POSTSUBSCRIPT = 1, then set xupâ¢p=1subscriptğ‘¥subscriptğ‘¢ğ‘ğ‘1x_{u_{p}p}=1italic_x start_POSTSUBSCRIPT italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT = 1. The cost of this assignment would be dâ¢(up,p)ğ‘‘subscriptğ‘¢ğ‘ğ‘d(u_{p},p)italic_d ( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_p ). â€¢ If yup=1/2subscriptğ‘¦subscriptğ‘¢ğ‘12y_{u_{p}}=1/2italic_y start_POSTSUBSCRIPT italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT end_POSTSUBSCRIPT = 1 / 2 and there is a center upâ€²âˆˆğ’°âˆ’upsubscriptsuperscriptğ‘¢â€²ğ‘ğ’°subscriptğ‘¢ğ‘u^{\prime}_{p}\in\mathcal{U}-u_{p}italic_u start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT âˆˆ caligraphic_U - italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT such that dâ¢(up,upâ€²)â‰¤Î³â‹…dâ¢(up,vp)ğ‘‘subscriptğ‘¢ğ‘subscriptsuperscriptğ‘¢â€²ğ‘â‹…ğ›¾ğ‘‘subscriptğ‘¢ğ‘subscriptğ‘£ğ‘d(u_{p},u^{\prime}_{p})\leq\gamma\cdot d(u_{p},v_{p})italic_d ( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_u start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) â‰¤ italic_Î³ â‹… italic_d ( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ), then set xupâ¢p=xupâ€²â¢p=1/2subscriptğ‘¥subscriptğ‘¢ğ‘ğ‘subscriptğ‘¥superscriptsubscriptğ‘¢ğ‘â€²ğ‘12x_{u_{p}p}=x_{u_{p}^{\prime}p}=1/2italic_x start_POSTSUBSCRIPT italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT italic_p end_POSTSUBSCRIPT = 1 / 2. Note that upâ€²â‰ upsuperscriptsubscriptğ‘¢ğ‘â€²subscriptğ‘¢ğ‘u_{p}^{\prime}\neq u_{p}italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT â‰  italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT which means this solution is feasible to the LP. The cost of this assignment would be 12â‹…(dâ¢(p,up)+dâ¢(p,upâ€²))â‹…12ğ‘‘ğ‘subscriptğ‘¢ğ‘ğ‘‘ğ‘superscriptsubscriptğ‘¢ğ‘â€²\displaystyle\frac{1}{2}\cdot\left(d(p,u_{p})+d(p,u_{p}^{\prime})\right)divide start_ARG 1 end_ARG start_ARG 2 end_ARG â‹… ( italic_d ( italic_p , italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) + italic_d ( italic_p , italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) ) â‰¤\displaystyle\leqâ‰¤ 12â‹…(dâ¢(p,up)+dâ¢(p,up)+dâ¢(upâ€²,up))â‹…12ğ‘‘ğ‘subscriptğ‘¢ğ‘ğ‘‘ğ‘subscriptğ‘¢ğ‘ğ‘‘subscriptsuperscriptğ‘¢â€²ğ‘subscriptğ‘¢ğ‘\displaystyle\frac{1}{2}\cdot\left(d(p,u_{p})+d(p,u_{p})+d(u^{\prime}_{p},u_{p% })\right)divide start_ARG 1 end_ARG start_ARG 2 end_ARG â‹… ( italic_d ( italic_p , italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) + italic_d ( italic_p , italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) + italic_d ( italic_u start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) ) â‰¤\displaystyle\leqâ‰¤ dâ¢(p,up)+Î³2â‹…dâ¢(up,vp).ğ‘‘ğ‘subscriptğ‘¢ğ‘â‹…ğ›¾2ğ‘‘subscriptğ‘¢ğ‘subscriptğ‘£ğ‘\displaystyle d(p,u_{p})+\frac{\gamma}{2}\cdot d(u_{p},v_{p}).italic_d ( italic_p , italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) + divide start_ARG italic_Î³ end_ARG start_ARG 2 end_ARG â‹… italic_d ( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) . â€¢ If yup=1/2subscriptğ‘¦subscriptğ‘¢ğ‘12y_{u_{p}}=1/2italic_y start_POSTSUBSCRIPT italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT end_POSTSUBSCRIPT = 1 / 2 and the previous case does not hold, then since (up,vp)subscriptğ‘¢ğ‘subscriptğ‘£ğ‘(u_{p},v_{p})( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) is not a well-separated pair, there is a center vpâ€²âˆˆğ’±âˆ’vpsubscriptsuperscriptğ‘£â€²ğ‘ğ’±subscriptğ‘£ğ‘v^{\prime}_{p}\in\mathcal{V}-v_{p}italic_v start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT âˆˆ caligraphic_V - italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT such that dâ¢(vp,vpâ€²)â‰¤Î³â‹…dâ¢(up,vp)ğ‘‘subscriptğ‘£ğ‘subscriptsuperscriptğ‘£â€²ğ‘â‹…ğ›¾ğ‘‘subscriptğ‘¢ğ‘subscriptğ‘£ğ‘d(v_{p},v^{\prime}_{p})\leq\gamma\cdot d(u_{p},v_{p})italic_d ( italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_v start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) â‰¤ italic_Î³ â‹… italic_d ( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ). Let upâ€²=Ï€ğ’°â¢(vpâ€²)superscriptsubscriptğ‘¢ğ‘â€²subscriptğœ‹ğ’°subscriptsuperscriptğ‘£â€²ğ‘u_{p}^{\prime}=\pi_{\mathcal{U}}(v^{\prime}_{p})italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT = italic_Ï€ start_POSTSUBSCRIPT caligraphic_U end_POSTSUBSCRIPT ( italic_v start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) and set xupâ¢p=xupâ€²â¢p=1/2subscriptğ‘¥subscriptğ‘¢ğ‘ğ‘subscriptğ‘¥subscriptsuperscriptğ‘¢â€²ğ‘ğ‘12x_{u_{p}p}=x_{u^{\prime}_{p}p}=1/2italic_x start_POSTSUBSCRIPT italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT italic_u start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT = 1 / 2. First, we show that upâ€²â‰ upsubscriptsuperscriptğ‘¢â€²ğ‘subscriptğ‘¢ğ‘u^{\prime}_{p}\neq u_{p}italic_u start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT â‰  italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT. Since yup=1/2subscriptğ‘¦subscriptğ‘¢ğ‘12y_{u_{p}}=1/2italic_y start_POSTSUBSCRIPT italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT end_POSTSUBSCRIPT = 1 / 2, we have upâˆˆğ’°Fsubscriptğ‘¢ğ‘subscriptğ’°ğ¹u_{p}\in\mathcal{U}_{F}italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT âˆˆ caligraphic_U start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT which concludes |Ï€ğ’°âˆ’1â¢(up)|â‰¤1subscriptsuperscriptğœ‹1ğ’°subscriptğ‘¢ğ‘1|\pi^{-1}_{\mathcal{U}}(u_{p})|\leq 1| italic_Ï€ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT caligraphic_U end_POSTSUBSCRIPT ( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) | â‰¤ 1. We also know that Ï€ğ’°â¢(vp)=upsubscriptğœ‹ğ’°subscriptğ‘£ğ‘subscriptğ‘¢ğ‘\pi_{\mathcal{U}}(v_{p})=u_{p}italic_Ï€ start_POSTSUBSCRIPT caligraphic_U end_POSTSUBSCRIPT ( italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) = italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT. So, vpsubscriptğ‘£ğ‘v_{p}italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT is the only center in ğ’±ğ’±\mathcal{V}caligraphic_V mapped to upsubscriptğ‘¢ğ‘u_{p}italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT which implies Ï€ğ’°â¢(vpâ€²)â‰ upsubscriptğœ‹ğ’°subscriptsuperscriptğ‘£â€²ğ‘subscriptğ‘¢ğ‘\pi_{\mathcal{U}}(v^{\prime}_{p})\neq u_{p}italic_Ï€ start_POSTSUBSCRIPT caligraphic_U end_POSTSUBSCRIPT ( italic_v start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) â‰  italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT or upâ€²â‰ upsuperscriptsubscriptğ‘¢ğ‘â€²subscriptğ‘¢ğ‘u_{p}^{\prime}\neq u_{p}italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT â‰  italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT (note that vpâ€²â‰ vpsuperscriptsubscriptğ‘£ğ‘â€²subscriptğ‘£ğ‘v_{p}^{\prime}\neq v_{p}italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT â‰  italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT). So, point pğ‘pitalic_p is assigned one unit to centers. The cost of this assignment would be 12â‹…(dâ¢(p,up)+dâ¢(p,upâ€²))â‹…12ğ‘‘ğ‘subscriptğ‘¢ğ‘ğ‘‘ğ‘subscriptsuperscriptğ‘¢â€²ğ‘\displaystyle\frac{1}{2}\cdot\left(d(p,u_{p})+d(p,u^{\prime}_{p})\right)divide start_ARG 1 end_ARG start_ARG 2 end_ARG â‹… ( italic_d ( italic_p , italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) + italic_d ( italic_p , italic_u start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) ) â‰¤\displaystyle\leqâ‰¤ 12â‹…(dâ¢(p,up)+dâ¢(p,up)+dâ¢(up,vpâ€²)+dâ¢(vpâ€²,upâ€²))â‹…12ğ‘‘ğ‘subscriptğ‘¢ğ‘ğ‘‘ğ‘subscriptğ‘¢ğ‘ğ‘‘subscriptğ‘¢ğ‘subscriptsuperscriptğ‘£â€²ğ‘ğ‘‘subscriptsuperscriptğ‘£â€²ğ‘subscriptsuperscriptğ‘¢â€²ğ‘\displaystyle\frac{1}{2}\cdot\left(d(p,u_{p})+d(p,u_{p})+d(u_{p},v^{\prime}_{p% })+d(v^{\prime}_{p},u^{\prime}_{p})\right)divide start_ARG 1 end_ARG start_ARG 2 end_ARG â‹… ( italic_d ( italic_p , italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) + italic_d ( italic_p , italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) + italic_d ( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_v start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) + italic_d ( italic_v start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_u start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) ) â‰¤\displaystyle\leqâ‰¤ 12â‹…(dâ¢(p,up)+dâ¢(p,up)+dâ¢(up,vpâ€²)+dâ¢(vpâ€²,up))â‹…12ğ‘‘ğ‘subscriptğ‘¢ğ‘ğ‘‘ğ‘subscriptğ‘¢ğ‘ğ‘‘subscriptğ‘¢ğ‘subscriptsuperscriptğ‘£â€²ğ‘ğ‘‘subscriptsuperscriptğ‘£â€²ğ‘subscriptğ‘¢ğ‘\displaystyle\frac{1}{2}\cdot\left(d(p,u_{p})+d(p,u_{p})+d(u_{p},v^{\prime}_{p% })+d(v^{\prime}_{p},u_{p})\right)divide start_ARG 1 end_ARG start_ARG 2 end_ARG â‹… ( italic_d ( italic_p , italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) + italic_d ( italic_p , italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) + italic_d ( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_v start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) + italic_d ( italic_v start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) ) =\displaystyle== dâ¢(p,up)+dâ¢(up,vpâ€²)ğ‘‘ğ‘subscriptğ‘¢ğ‘ğ‘‘subscriptğ‘¢ğ‘superscriptsubscriptğ‘£ğ‘â€²\displaystyle d(p,u_{p})+d(u_{p},v_{p}^{\prime})italic_d ( italic_p , italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) + italic_d ( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) â‰¤\displaystyle\leqâ‰¤ dâ¢(p,up)+dâ¢(up,vp)+dâ¢(vp,vpâ€²)ğ‘‘ğ‘subscriptğ‘¢ğ‘ğ‘‘subscriptğ‘¢ğ‘subscriptğ‘£ğ‘ğ‘‘subscriptğ‘£ğ‘superscriptsubscriptğ‘£ğ‘â€²\displaystyle d(p,u_{p})+d(u_{p},v_{p})+d(v_{p},v_{p}^{\prime})italic_d ( italic_p , italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) + italic_d ( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) + italic_d ( italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) â‰¤\displaystyle\leqâ‰¤ dâ¢(p,up)+(Î³+1)â‹…dâ¢(up,vp).ğ‘‘ğ‘subscriptğ‘¢ğ‘â‹…ğ›¾1ğ‘‘subscriptğ‘¢ğ‘subscriptğ‘£ğ‘\displaystyle d(p,u_{p})+(\gamma+1)\cdot d(u_{p},v_{p}).italic_d ( italic_p , italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) + ( italic_Î³ + 1 ) â‹… italic_d ( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) . The second inequality is because of the choice of upâ€²=Ï€ğ’°â¢(vpâ€²)subscriptsuperscriptğ‘¢â€²ğ‘subscriptğœ‹ğ’°superscriptsubscriptğ‘£ğ‘â€²u^{\prime}_{p}=\pi_{\mathcal{U}}(v_{p}^{\prime})italic_u start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT = italic_Ï€ start_POSTSUBSCRIPT caligraphic_U end_POSTSUBSCRIPT ( italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) and the last inequality is because dâ¢(vp,vpâ€²)â‰¤Î³â‹…dâ¢(up,vp)ğ‘‘subscriptğ‘£ğ‘superscriptsubscriptğ‘£ğ‘â€²â‹…ğ›¾ğ‘‘subscriptğ‘¢ğ‘subscriptğ‘£ğ‘d(v_{p},v_{p}^{\prime})\leq\gamma\cdot d(u_{p},v_{p})italic_d ( italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) â‰¤ italic_Î³ â‹… italic_d ( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ). Bounding the Cost. Assume upâˆ—=Ï€ğ’°â¢(p)superscriptsubscriptğ‘¢ğ‘subscriptğœ‹ğ’°ğ‘u_{p}^{*}=\pi_{\mathcal{U}}(p)italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT = italic_Ï€ start_POSTSUBSCRIPT caligraphic_U end_POSTSUBSCRIPT ( italic_p ) for each pâˆˆğ’«ğ‘ğ’«p\in\mathcal{P}italic_p âˆˆ caligraphic_P. We have dâ¢(up,vp)â‰¤dâ¢(upâˆ—,vp)â‰¤dâ¢(upâˆ—,p)+dâ¢(p,vp).ğ‘‘subscriptğ‘¢ğ‘subscriptğ‘£ğ‘ğ‘‘superscriptsubscriptğ‘¢ğ‘subscriptğ‘£ğ‘ğ‘‘superscriptsubscriptğ‘¢ğ‘ğ‘ğ‘‘ğ‘subscriptğ‘£ğ‘d(u_{p},v_{p})\leq d(u_{p}^{*},v_{p})\leq d(u_{p}^{*},p)+d(p,v_{p}).italic_d ( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) â‰¤ italic_d ( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT , italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) â‰¤ italic_d ( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT , italic_p ) + italic_d ( italic_p , italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) . The first inequality is by the choice of up=Ï€ğ’°â¢(vp)subscriptğ‘¢ğ‘subscriptğœ‹ğ’°subscriptğ‘£ğ‘u_{p}=\pi_{\mathcal{U}}(v_{p})italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT = italic_Ï€ start_POSTSUBSCRIPT caligraphic_U end_POSTSUBSCRIPT ( italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ). As a result, âˆ‘pâˆˆğ’«dâ¢(up,vp)â‰¤âˆ‘pâˆˆğ’«(dâ¢(upâˆ—,p)+dâ¢(p,vp))=Costâ¢(ğ’°,ğ’«)+Costâ¢(ğ’±,ğ’«).subscriptğ‘ğ’«ğ‘‘subscriptğ‘¢ğ‘subscriptğ‘£ğ‘subscriptğ‘ğ’«ğ‘‘subscriptsuperscriptğ‘¢ğ‘ğ‘ğ‘‘ğ‘subscriptğ‘£ğ‘Costğ’°ğ’«Costğ’±ğ’«\sum_{p\in\mathcal{P}}d(u_{p},v_{p})\leq\sum_{p\in\mathcal{P}}\left(d(u^{*}_{p% },p)+d(p,v_{p})\right)=\textnormal{{Cost}}\left(\mathcal{U},\mathcal{P}\right)% +\textnormal{{Cost}}\left(\mathcal{V},\mathcal{P}\right).âˆ‘ start_POSTSUBSCRIPT italic_p âˆˆ caligraphic_P end_POSTSUBSCRIPT italic_d ( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) â‰¤ âˆ‘ start_POSTSUBSCRIPT italic_p âˆˆ caligraphic_P end_POSTSUBSCRIPT ( italic_d ( italic_u start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_p ) + italic_d ( italic_p , italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) ) = Cost ( caligraphic_U , caligraphic_P ) + Cost ( caligraphic_V , caligraphic_P ) . (27) In each of the cases of fractional assignments of points to centers, the cost of assigning a point pâˆˆCvâ¢(ğ’±,ğ’«)ğ‘subscriptğ¶ğ‘£ğ’±ğ’«p\in C_{v}(\mathcal{V},\mathcal{P})italic_p âˆˆ italic_C start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( caligraphic_V , caligraphic_P ) is at most dâ¢(p,up)+(Î³+1)â‹…dâ¢(up,vp)ğ‘‘ğ‘subscriptğ‘¢ğ‘â‹…ğ›¾1ğ‘‘subscriptğ‘¢ğ‘subscriptğ‘£ğ‘d(p,u_{p})+(\gamma+1)\cdot d(u_{p},v_{p})italic_d ( italic_p , italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) + ( italic_Î³ + 1 ) â‹… italic_d ( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ). As a result, the total cost of this assignment is upper bounded by âˆ‘pâˆˆP(dâ¢(p,up)+(Î³+1)â‹…dâ¢(up,vp))subscriptğ‘ğ‘ƒğ‘‘ğ‘subscriptğ‘¢ğ‘â‹…ğ›¾1ğ‘‘subscriptğ‘¢ğ‘subscriptğ‘£ğ‘\displaystyle\sum_{p\in P}\left(d(p,u_{p})+(\gamma+1)\cdot d(u_{p},v_{p})\right)âˆ‘ start_POSTSUBSCRIPT italic_p âˆˆ italic_P end_POSTSUBSCRIPT ( italic_d ( italic_p , italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) + ( italic_Î³ + 1 ) â‹… italic_d ( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) ) â‰¤\displaystyle\leqâ‰¤ âˆ‘pâˆˆğ’«(dâ¢(p,vp)+(Î³+2)â‹…dâ¢(up,vp))subscriptğ‘ğ’«ğ‘‘ğ‘subscriptğ‘£ğ‘â‹…ğ›¾2ğ‘‘subscriptğ‘¢ğ‘subscriptğ‘£ğ‘\displaystyle\sum_{p\in\mathcal{P}}\left(d(p,v_{p})+(\gamma+2)\cdot d(u_{p},v_% {p})\right)âˆ‘ start_POSTSUBSCRIPT italic_p âˆˆ caligraphic_P end_POSTSUBSCRIPT ( italic_d ( italic_p , italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) + ( italic_Î³ + 2 ) â‹… italic_d ( italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) ) â‰¤\displaystyle\leqâ‰¤ Costâ¢(ğ’±,ğ’«)+(Î³+2)â‹…(Costâ¢(ğ’°,ğ’«)+Costâ¢(ğ’±,ğ’«))Costğ’±ğ’«â‹…ğ›¾2Costğ’°ğ’«Costğ’±ğ’«\displaystyle\textnormal{{Cost}}\left(\mathcal{V},\mathcal{P}\right)+(\gamma+2% )\cdot\left(\textnormal{{Cost}}\left(\mathcal{U},\mathcal{P}\right)+% \textnormal{{Cost}}\left(\mathcal{V},\mathcal{P}\right)\right)Cost ( caligraphic_V , caligraphic_P ) + ( italic_Î³ + 2 ) â‹… ( Cost ( caligraphic_U , caligraphic_P ) + Cost ( caligraphic_V , caligraphic_P ) ) â‰¤\displaystyle\leqâ‰¤ 2â¢Î³â‹…(Costâ¢(ğ’°,ğ’«)+Costâ¢(ğ’±,ğ’«)).â‹…2ğ›¾Costğ’°ğ’«Costğ’±ğ’«\displaystyle 2\gamma\cdot\left(\textnormal{{Cost}}\left(\mathcal{U},\mathcal{% P}\right)+\textnormal{{Cost}}\left(\mathcal{V},\mathcal{P}\right)\right).2 italic_Î³ â‹… ( Cost ( caligraphic_U , caligraphic_P ) + Cost ( caligraphic_V , caligraphic_P ) ) . The second inequality follows by Equation 27."
https://arxiv.org/html/2411.03071v1,Multi-dimensional Approximate Countingâ€ â€ thanks:This work was supported by NSF Grant CCF-2221980.,"The celebrated Morris counter uses log2â¡log2â¡n+Oâ¢(log2â¡Ïƒâˆ’1)subscript2subscript2ğ‘›ğ‘‚subscript2superscriptğœ1\log_{2}\log_{2}n+O(\log_{2}\sigma^{-1})roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_n + italic_O ( roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_Ïƒ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) bits to count up to nğ‘›nitalic_n with a relative error Ïƒğœ\sigmaitalic_Ïƒ, where if Î»^^ğœ†\hat{\lambda}over^ start_ARG italic_Î» end_ARG is the estimate of the current count Î»ğœ†\lambdaitalic_Î», then ğ”¼â¢|Î»^âˆ’Î»|2<Ïƒ2â¢Î»2ğ”¼superscript^ğœ†ğœ†2superscriptğœ2superscriptğœ†2\mathbb{E}|\hat{\lambda}-\lambda|^{2}<\sigma^{2}\lambda^{2}blackboard_E | over^ start_ARG italic_Î» end_ARG - italic_Î» | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT < italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_Î» start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. The Morris counter was proved to be optimal in space complexity by Nelson and Yu [14], even when considering the error tails. A natural generalization is multi-dimensional approximate counting. Let dâ‰¥1ğ‘‘1d\geq 1italic_d â‰¥ 1 be the dimension. The count vector xâˆˆâ„•dğ‘¥superscriptâ„•ğ‘‘x\in\mathbb{N}^{d}italic_x âˆˆ blackboard_N start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT is incremented entry-wisely over a stream of coordinates (w1,â€¦,wn)âˆˆ[d]nsubscriptğ‘¤1â€¦subscriptğ‘¤ğ‘›superscriptdelimited-[]ğ‘‘ğ‘›(w_{1},\ldots,w_{n})\in[d]^{n}( italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_w start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) âˆˆ [ italic_d ] start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, where upon receiving wkâˆˆ[d]subscriptğ‘¤ğ‘˜delimited-[]ğ‘‘w_{k}\in[d]italic_w start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT âˆˆ [ italic_d ], xwkâ†xwk+1â†subscriptğ‘¥subscriptğ‘¤ğ‘˜subscriptğ‘¥subscriptğ‘¤ğ‘˜1x_{w_{k}}\leftarrow x_{w_{k}}+1italic_x start_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT â† italic_x start_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT + 1. A dğ‘‘ditalic_d-dimensional approximate counter is required to count dğ‘‘ditalic_d coordinates simultaneously and return an estimate x^^ğ‘¥\hat{x}over^ start_ARG italic_x end_ARG of the count vector xğ‘¥xitalic_x. Aden-Ali, Han, Nelson, and Yu [1] showed that the trivial solution of using dğ‘‘ditalic_d Morris counters that track dğ‘‘ditalic_d coordinates separately is already optimal in space, if each entry only allows error relative to itself, i.e., ğ”¼â¢|x^jâˆ’xj|2<Ïƒ2â¢|xj|2ğ”¼superscriptsubscript^ğ‘¥ğ‘—subscriptğ‘¥ğ‘—2superscriptğœ2superscriptsubscriptğ‘¥ğ‘—2\mathbb{E}|\hat{x}_{j}-x_{j}|^{2}<\sigma^{2}|x_{j}|^{2}blackboard_E | over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT - italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT < italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT | italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT for each jâˆˆ[d]ğ‘—delimited-[]ğ‘‘j\in[d]italic_j âˆˆ [ italic_d ]. However, for another natural error metricâ€”the Euclidean mean squared error ğ”¼â¢|x^âˆ’x|2ğ”¼superscript^ğ‘¥ğ‘¥2\mathbb{E}|\hat{x}-x|^{2}blackboard_E | over^ start_ARG italic_x end_ARG - italic_x | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPTâ€”we show that using dğ‘‘ditalic_d separate Morris counters is sub-optimal.In this work, we present a simple and optimal dğ‘‘ditalic_d-dimensional counter with Euclidean relative error Ïƒğœ\sigmaitalic_Ïƒ, i.e., ğ”¼â¢|x^âˆ’x|2<Ïƒ2â¢|x|2ğ”¼superscript^ğ‘¥ğ‘¥2superscriptğœ2superscriptğ‘¥2\mathbb{E}|\hat{x}-x|^{2}<\sigma^{2}|x|^{2}blackboard_E | over^ start_ARG italic_x end_ARG - italic_x | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT < italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT | italic_x | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT where |x|=âˆ‘j=1dxj2ğ‘¥superscriptsubscriptğ‘—1ğ‘‘superscriptsubscriptğ‘¥ğ‘—2|x|=\sqrt{\sum_{j=1}^{d}x_{j}^{2}}| italic_x | = square-root start_ARG âˆ‘ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG, with a matching lower bound. We prove the following.There exists a (log2â¡log2â¡n+Oâ¢(dâ¢log2â¡Ïƒâˆ’1))subscript2subscript2ğ‘›ğ‘‚ğ‘‘subscript2superscriptğœ1(\log_{2}\log_{2}n+O(d\log_{2}\sigma^{-1}))( roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_n + italic_O ( italic_d roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_Ïƒ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) )-bit dğ‘‘ditalic_d-dimensional counter with relative error Ïƒğœ\sigmaitalic_Ïƒ.Any dğ‘‘ditalic_d-dimensional counter with relative error Ïƒğœ\sigmaitalic_Ïƒ takes at least (log2â¡log2â¡n+Î©â¢(dâ¢log2â¡Ïƒâˆ’1))subscript2subscript2ğ‘›Î©ğ‘‘subscript2superscriptğœ1(\log_{2}\log_{2}n+\Omega(d\log_{2}\sigma^{-1}))( roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_n + roman_Î© ( italic_d roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_Ïƒ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) ) bits of space.The upper and lower bounds are proved with ideas that are strikingly simple. The upper bound is constructed with a certain variable-length integer encoding and the lower bound is derived from a straightforward volumetric estimation of sphere covering.","In 1978, Morris [13] invented the classic approximate counter which can count up to nğ‘›nitalic_n with log2â¡log2â¡n+Oâ¢(log2â¡Ïƒâˆ’1)subscript2subscript2ğ‘›ğ‘‚subscript2superscriptğœ1\log_{2}\log_{2}n+O(\log_{2}\sigma^{-1})roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_n + italic_O ( roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_Ïƒ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) bits, returning an estimate of the current count with a relative error Ïƒğœ\sigmaitalic_Ïƒ. Such approximate counters are invented mainly to save space. However, it was noticed quite recently that approximate counters can also be much faster on modern hardware in comparison to the deterministic counter due to their low write complexity [18, 17, 9]. In the original paper [13], Morris analyzed the mean and variance of the estimates. Flajolet [6] analyzed the mean and variance of the index111Roughly speaking, Morris counter stores an index vğ‘£vitalic_v to represent a count around 2vsuperscript2ğ‘£2^{v}2 start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT. Therefore, to count up to nğ‘›nitalic_n, the index increases to logâ¡nğ‘›\log nroman_log italic_n and thus it takes logâ¡logâ¡nğ‘›\log\log nroman_log roman_log italic_n to store the index. of the Morris counter, from which a quantified space bound can be derived with Chebyshevâ€™s bound. Nelson and Yu [14] analyzed the tail of the index, obtaining a sharper dependence on the failure probability. Specifically, for increments up to Î»ğœ†\lambdaitalic_Î» and Ïµ,Î´>0italic-Ïµğ›¿0\epsilon,\delta>0italic_Ïµ , italic_Î´ > 0, if an estimate Î»^^ğœ†\hat{\lambda}over^ start_ARG italic_Î» end_ARG is produced with â„™â¢(|Î»^âˆ’Î»|<Ïµâ¢Î»)â‰¥1âˆ’Î´â„™^ğœ†ğœ†italic-Ïµğœ†1ğ›¿\mathbb{P}(|\hat{\lambda}-\lambda|<\epsilon\lambda)\geq 1-\deltablackboard_P ( | over^ start_ARG italic_Î» end_ARG - italic_Î» | < italic_Ïµ italic_Î» ) â‰¥ 1 - italic_Î´ then the Morris counter needs Oâ¢(logâ¡logâ¡n+logâ¡Ïµâˆ’1+logâ¡logâ¡Î´âˆ’1)ğ‘‚ğ‘›superscriptitalic-Ïµ1superscriptğ›¿1O(\log\log n+\log\epsilon^{-1}+\log\log\delta^{-1})italic_O ( roman_log roman_log italic_n + roman_log italic_Ïµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT + roman_log roman_log italic_Î´ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) space, and this is proved to be optimal [14].222Technically, one needs to keep a separate deterministic counter for small Î»=Oâ¢(a)ğœ†ğ‘‚ğ‘\lambda=O(a)italic_Î» = italic_O ( italic_a ) to obtain the optimal failure rate [14]. It is the common scenario in real-world applications that many different counters are maintained simultaneously. For example, the original motivation of Morris is to count the number of each trigram in texts [12, 14] where d=263ğ‘‘superscript263d=26^{3}italic_d = 26 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT different trigrams are counted simultaneously. Aden-Ali, Han, Nelson, and Yu [1] showed that, essentially, dğ‘‘ditalic_d independent counters cannot be compressed as long as each counter is required to report an estimate with an error relative to itself. However, as we will later show, the space can be compressed if the error metric is the dğ‘‘ditalic_d-dimensional Euclidean norm. While the per-entry relative error requirement considered in [1] is reasonable when an approximation for each individual entry is needed, we remark that in applications where an approximation of the whole count vector is needed, the Euclidean error metric is more natural. We now formally define the problem of dğ‘‘ditalic_d-dimensional counting in terms of Euclidean mean squared error. Let (e1,e2,â€¦,ed)subscriptğ‘’1subscriptğ‘’2â€¦subscriptğ‘’ğ‘‘(e_{1},e_{2},\ldots,e_{d})( italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_e start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) be the standard basis of â„dsuperscriptâ„ğ‘‘\mathbb{R}^{d}blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT where eksubscriptğ‘’ğ‘˜e_{k}italic_e start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT has its kğ‘˜kitalic_kth component being one and other components being zero. Definition 1 ((Euclidean) dğ‘‘ditalic_d-dimensional counting). Let nâ‰¥1ğ‘›1n\geq 1italic_n â‰¥ 1 be the maximum stream length, dâ‰¥1ğ‘‘1d\geq 1italic_d â‰¥ 1 be the dimension, and Ïƒ>0ğœ0\sigma>0italic_Ïƒ > 0 be the relative error. An (n,d,Ïƒ)ğ‘›ğ‘‘ğœ(n,d,\sigma)( italic_n , italic_d , italic_Ïƒ )-counter is a randomized data structure that, for any input sequence (w1,â€¦,wk)âˆˆ[d]ksubscriptğ‘¤1â€¦subscriptğ‘¤ğ‘˜superscriptdelimited-[]ğ‘‘ğ‘˜(w_{1},\ldots,w_{k})\in[d]^{k}( italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_w start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) âˆˆ [ italic_d ] start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT with kâ‰¤nğ‘˜ğ‘›k\leq nitalic_k â‰¤ italic_n, returns an estimate x^^ğ‘¥\hat{x}over^ start_ARG italic_x end_ARG of x=âˆ‘j=1kewjğ‘¥superscriptsubscriptğ‘—1ğ‘˜subscriptğ‘’subscriptğ‘¤ğ‘—x=\sum_{j=1}^{k}e_{w_{j}}italic_x = âˆ‘ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_e start_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT with ğ”¼â¢|x^âˆ’x|2<Ïƒ2â¢|x|2ğ”¼superscript^ğ‘¥ğ‘¥2superscriptğœ2superscriptğ‘¥2\mathbb{E}|\hat{x}-x|^{2}<\sigma^{2}|x|^{2}blackboard_E | over^ start_ARG italic_x end_ARG - italic_x | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT < italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT | italic_x | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, where |x|=âˆ‘j=1dxj2ğ‘¥superscriptsubscriptğ‘—1ğ‘‘superscriptsubscriptğ‘¥ğ‘—2|x|=\sqrt{\sum_{j=1}^{d}x_{j}^{2}}| italic_x | = square-root start_ARG âˆ‘ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG is the Euclidean length. We denote the state space of the counter as Î©Î©\Omegaroman_Î© with |Î©|â‰¥1Î©1|\Omega|\geq 1| roman_Î© | â‰¥ 1. One counter thus uses log2â¡|Î©|subscript2Î©\log_{2}|\Omega|roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | roman_Î© | bits of space. Recall that the original Morris counter [13] with parameter aâ‰¥1ğ‘1a\geq 1italic_a â‰¥ 1, denoted as Morrisâ¢(a)Morrisğ‘\mathrm{Morris}(a)roman_Morris ( italic_a ), returns an unbiased estimate of the count with relative variance Oâ¢(1/a)ğ‘‚1ğ‘O(1/a)italic_O ( 1 / italic_a ). Thus by definition, Morrisâ¢(a)Morrisğ‘\mathrm{Morris}(a)roman_Morris ( italic_a ) is an (n,1,Oâ¢(1/a))ğ‘›1ğ‘‚1ğ‘(n,1,O(\sqrt{1/a}))( italic_n , 1 , italic_O ( square-root start_ARG 1 / italic_a end_ARG ) )-counter. The error here is measured by the mean squared error ğ”¼â¢|x^âˆ’x|2ğ”¼superscript^ğ‘¥ğ‘¥2\mathbb{E}|\hat{x}-x|^{2}blackboard_E | over^ start_ARG italic_x end_ARG - italic_x | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, which is equivalent to measure the variance if the estimator is unbiased. We note that there are, of course, many other reasonable error metrics to consider, depending on the specific applications. The mean squared error is a good starting point for a new algorithmic problem due to its simple probabilistic/geometric structures. One natural way to construct a dğ‘‘ditalic_d-dimensional counter is to simply use dğ‘‘ditalic_d separate Morris counters, which is the original method that Morris used to count trigrams [12]. Suppose now we have dğ‘‘ditalic_d (n,1,Ïƒ)ğ‘›1ğœ(n,1,\sigma)( italic_n , 1 , italic_Ïƒ )-counters to count x1,â€¦,xdsubscriptğ‘¥1â€¦subscriptğ‘¥ğ‘‘x_{1},\ldots,x_{d}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT. Let the estimates be x1^,â€¦,xd^^subscriptğ‘¥1â€¦^subscriptğ‘¥ğ‘‘\widehat{x_{1}},\ldots,\widehat{x_{d}}over^ start_ARG italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG , â€¦ , over^ start_ARG italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_ARG respectively. Then we have ğ”¼â¢|x^âˆ’x|2ğ”¼superscript^ğ‘¥ğ‘¥2\displaystyle\mathbb{E}|\hat{x}-x|^{2}blackboard_E | over^ start_ARG italic_x end_ARG - italic_x | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT =ğ”¼â¢âˆ‘j=1d|xj^âˆ’xj|2=âˆ‘j=1dğ”¼â¢|xj^âˆ’xj|2absentğ”¼superscriptsubscriptğ‘—1ğ‘‘superscript^subscriptğ‘¥ğ‘—subscriptğ‘¥ğ‘—2superscriptsubscriptğ‘—1ğ‘‘ğ”¼superscript^subscriptğ‘¥ğ‘—subscriptğ‘¥ğ‘—2\displaystyle=\mathbb{E}\sum_{j=1}^{d}|\widehat{x_{j}}-x_{j}|^{2}=\sum_{j=1}^{% d}\mathbb{E}|\widehat{x_{j}}-x_{j}|^{2}= blackboard_E âˆ‘ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT | over^ start_ARG italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_ARG - italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = âˆ‘ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT blackboard_E | over^ start_ARG italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_ARG - italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT Since for any jğ‘—jitalic_j, xj^^subscriptğ‘¥ğ‘—\widehat{x_{j}}over^ start_ARG italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_ARG is an (n,1,Ïƒ)ğ‘›1ğœ(n,1,\sigma)( italic_n , 1 , italic_Ïƒ )-counter for xjsubscriptğ‘¥ğ‘—x_{j}italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT, we have ğ”¼â¢|xj^âˆ’xj|2<Ïƒ2â¢xj2ğ”¼superscript^subscriptğ‘¥ğ‘—subscriptğ‘¥ğ‘—2superscriptğœ2superscriptsubscriptğ‘¥ğ‘—2\mathbb{E}|\widehat{x_{j}}-x_{j}|^{2}<\sigma^{2}x_{j}^{2}blackboard_E | over^ start_ARG italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_ARG - italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT < italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. Thus, ğ”¼â¢|x^âˆ’x|2ğ”¼superscript^ğ‘¥ğ‘¥2\displaystyle\mathbb{E}|\hat{x}-x|^{2}blackboard_E | over^ start_ARG italic_x end_ARG - italic_x | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT <âˆ‘j=1dÏƒ2â¢xj2=Ïƒ2â¢|x|2.absentsuperscriptsubscriptğ‘—1ğ‘‘superscriptğœ2superscriptsubscriptğ‘¥ğ‘—2superscriptğœ2superscriptğ‘¥2\displaystyle<\sum_{j=1}^{d}\sigma^{2}x_{j}^{2}=\sigma^{2}|x|^{2}.< âˆ‘ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT | italic_x | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . We thus see dğ‘‘ditalic_d (n,1,Ïƒ)ğ‘›1ğœ(n,1,\sigma)( italic_n , 1 , italic_Ïƒ )-counters do form an (n,d,Ïƒ)ğ‘›ğ‘‘ğœ(n,d,\sigma)( italic_n , italic_d , italic_Ïƒ )-counter (Definition 1). Each (n,1,Ïƒ)ğ‘›1ğœ(n,1,\sigma)( italic_n , 1 , italic_Ïƒ )-counter can be implemented with a Morrisâ¢(Oâ¢(Ïƒâˆ’2))Morrisğ‘‚superscriptğœ2\mathrm{Morris}(O(\sigma^{-2}))roman_Morris ( italic_O ( italic_Ïƒ start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT ) ) counter, taking log2â¡log2â¡n+Oâ¢(log2â¡Ïƒâˆ’1)subscript2subscript2ğ‘›ğ‘‚subscript2superscriptğœ1\log_{2}\log_{2}n+O(\log_{2}\sigma^{-1})roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_n + italic_O ( roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_Ïƒ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) bits. Thus the total space needed is dâ¢log2â¡log2â¡n+Oâ¢(dâ¢log2â¡Ïƒâˆ’1)ğ‘‘subscript2subscript2ğ‘›ğ‘‚ğ‘‘subscript2superscriptğœ1d\log_{2}\log_{2}n+O(d\log_{2}\sigma^{-1})italic_d roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_n + italic_O ( italic_d roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_Ïƒ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) bits, which is sub-optimal for this task. The main contribution of this work is the construction of a simple and optimal (n,d,Ïƒ)ğ‘›ğ‘‘ğœ(n,d,\sigma)( italic_n , italic_d , italic_Ïƒ )-counter, with a matching lower bound. Theorem 1. The following statements are true. Upper bound For any nâ‰¥2ğ‘›2n\geq 2italic_n â‰¥ 2, dâ‰¥1ğ‘‘1d\geq 1italic_d â‰¥ 1, and Ïƒâˆˆ(0,1/3)ğœ013\sigma\in(0,1/3)italic_Ïƒ âˆˆ ( 0 , 1 / 3 ), there exists an (n,d,Ïƒ)ğ‘›ğ‘‘ğœ(n,d,\sigma)( italic_n , italic_d , italic_Ïƒ )-counter with space size log2â¡|Î©|=log2â¡log2â¡n+Oâ¢(dâ¢log2â¡Ïƒâˆ’1)subscript2Î©subscript2subscript2ğ‘›ğ‘‚ğ‘‘subscript2superscriptğœ1\log_{2}|\Omega|=\log_{2}\log_{2}n+O(d\log_{2}\sigma^{-1})roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | roman_Î© | = roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_n + italic_O ( italic_d roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_Ïƒ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ). Lower bound For any nâ‰¥e2ğ‘›superscriptğ‘’2n\geq e^{2}italic_n â‰¥ italic_e start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, dâ‰¥1ğ‘‘1d\geq 1italic_d â‰¥ 1, and Ïƒâˆˆ(0,1/16)ğœ0116\sigma\in(0,1/16)italic_Ïƒ âˆˆ ( 0 , 1 / 16 ), if there is an (n,d,Ïƒ)ğ‘›ğ‘‘ğœ(n,d,\sigma)( italic_n , italic_d , italic_Ïƒ )-counter with state space Î©Î©\Omegaroman_Î©, then log2â¡|Î©|â‰¥log2â¡log2â¡n+Î©â¢(dâ¢log2â¡Ïƒâˆ’1)subscript2Î©subscript2subscript2ğ‘›Î©ğ‘‘subscript2superscriptğœ1\log_{2}|\Omega|\geq\log_{2}\log_{2}n+\Omega(d\log_{2}\sigma^{-1})roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | roman_Î© | â‰¥ roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_n + roman_Î© ( italic_d roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_Ïƒ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ). A related data structure is the Count-Min sketch [3] by Cormode and Muthukrishnan. Given a count vector xâˆˆâ„•dğ‘¥superscriptâ„•ğ‘‘x\in\mathbb{N}^{d}italic_x âˆˆ blackboard_N start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT with âˆ‘j=1dxj=nsuperscriptsubscriptğ‘—1ğ‘‘subscriptğ‘¥ğ‘—ğ‘›\sum_{j=1}^{d}x_{j}=nâˆ‘ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = italic_n, Count-Min is able to return an estimate xj^^subscriptğ‘¥ğ‘—\widehat{x_{j}}over^ start_ARG italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_ARG of xjsubscriptğ‘¥ğ‘—x_{j}italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT, for any jâˆˆ[d]ğ‘—delimited-[]ğ‘‘j\in[d]italic_j âˆˆ [ italic_d ], such that xj^âˆˆ[xj,xj+Ïµâ¢n]^subscriptğ‘¥ğ‘—subscriptğ‘¥ğ‘—subscriptğ‘¥ğ‘—italic-Ïµğ‘›\widehat{x_{j}}\in[x_{j},x_{j}+\epsilon n]over^ start_ARG italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_ARG âˆˆ [ italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT + italic_Ïµ italic_n ] with probability 1âˆ’Î´1ğ›¿1-\delta1 - italic_Î´, using only Oâ¢(Ïµâˆ’1â¢logâ¡Î´âˆ’1â¢logâ¡n)ğ‘‚superscriptitalic-Ïµ1superscriptğ›¿1ğ‘›O(\epsilon^{-1}\log\delta^{-1}\log n)italic_O ( italic_Ïµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log italic_Î´ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log italic_n ) bits of space. It may seem that Count-Min with the power of hash functions has a chance to beat the lower bound in Theorem 1 by combining the estimates for each coordinate. However, it is hopeless to use Count-Min as a dğ‘‘ditalic_d-dimensional counter unless Ïµ=Oâ¢(1/d)italic-Ïµğ‘‚1ğ‘‘\epsilon=O(1/d)italic_Ïµ = italic_O ( 1 / italic_d ), in which case it still lies above the lower bound in Theorem 1. Indeed, suppose xj^âˆˆ[xj,xj+Ïµâ¢n]^subscriptğ‘¥ğ‘—subscriptğ‘¥ğ‘—subscriptğ‘¥ğ‘—italic-Ïµğ‘›\widehat{x_{j}}\in[x_{j},x_{j}+\epsilon n]over^ start_ARG italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_ARG âˆˆ [ italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT + italic_Ïµ italic_n ] holds for all jğ‘—jitalic_j. The vector estimate x^=(x1^,â€¦,xd^)^ğ‘¥^subscriptğ‘¥1â€¦^subscriptğ‘¥ğ‘‘\hat{x}=(\widehat{x_{1}},\ldots,\widehat{x_{d}})over^ start_ARG italic_x end_ARG = ( over^ start_ARG italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG , â€¦ , over^ start_ARG italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_ARG ) has error |x^âˆ’x|2â‰¤dâ¢Ïµ2â¢n2â‰¤d2â¢Ïµ2â¢|x|2,superscript^ğ‘¥ğ‘¥2ğ‘‘superscriptitalic-Ïµ2superscriptğ‘›2superscriptğ‘‘2superscriptitalic-Ïµ2superscriptğ‘¥2\displaystyle|\hat{x}-x|^{2}\leq d\epsilon^{2}n^{2}\leq d^{2}\epsilon^{2}|x|^{% 2},| over^ start_ARG italic_x end_ARG - italic_x | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT â‰¤ italic_d italic_Ïµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT â‰¤ italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_Ïµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT | italic_x | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , where we use the bound |x|2â‰¥(âˆ‘j=1dxj)2/d=n2/dsuperscriptğ‘¥2superscriptsuperscriptsubscriptğ‘—1ğ‘‘subscriptğ‘¥ğ‘—2ğ‘‘superscriptğ‘›2ğ‘‘|x|^{2}\geq(\sum_{j=1}^{d}x_{j})^{2}/d=n^{2}/d| italic_x | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT â‰¥ ( âˆ‘ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT / italic_d = italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT / italic_d. Therefore Ïµitalic-Ïµ\epsilonitalic_Ïµ has to be Oâ¢(1/d)ğ‘‚1ğ‘‘O(1/d)italic_O ( 1 / italic_d ) for x^^ğ‘¥\hat{x}over^ start_ARG italic_x end_ARG to be a multiplicative estimate of xğ‘¥xitalic_x. 1.1 Technical Overview 1.1.1 Upper Bound The upper bound in Theorem 1 uses the natural idea of maintaining a common scale counter Uğ‘ˆUitalic_U for all dğ‘‘ditalic_d dimensions and tracking the relative magnitude of each coordinate with a vector Vâˆˆâ„•dğ‘‰superscriptâ„•ğ‘‘V\in\mathbb{N}^{d}italic_V âˆˆ blackboard_N start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT. The estimate x^^ğ‘¥\hat{x}over^ start_ARG italic_x end_ARG is equal to 2Uâ¢Vsuperscript2ğ‘ˆğ‘‰2^{U}V2 start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT italic_V. For demonstration, we will first describe a very simple but sub-optimal solution and then discuss how to modify it to reach optimal space-accuracy tradeoff. The transition of the states is designed similarly with the design of the Morris counter, in which the estimate is maintained unbiased at any moment (ğ”¼â¢2Uâ¢V=xğ”¼superscript2ğ‘ˆğ‘‰ğ‘¥\mathbb{E}2^{U}V=xblackboard_E 2 start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT italic_V = italic_x). Initially we set U=0ğ‘ˆ0U=0italic_U = 0 and V=(0,â€¦,0)ğ‘‰0â€¦0V=(0,\ldots,0)italic_V = ( 0 , â€¦ , 0 ). â€¢ When the jğ‘—jitalic_jth coordinate is incremented (i.e., xjâ†xj+1â†subscriptğ‘¥ğ‘—subscriptğ‘¥ğ‘—1x_{j}\leftarrow x_{j}+1italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT â† italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT + 1), we update Vjâ†Vj+1â†subscriptğ‘‰ğ‘—subscriptğ‘‰ğ‘—1V_{j}\leftarrow V_{j}+1italic_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT â† italic_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT + 1 with probability 2âˆ’Usuperscript2ğ‘ˆ2^{-U}2 start_POSTSUPERSCRIPT - italic_U end_POSTSUPERSCRIPT, ensuring the unbiasedness of the estimate. â€¢ Each coordinate Vjsubscriptğ‘‰ğ‘—V_{j}italic_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT is restricted to the range [0,a]0ğ‘[0,a][ 0 , italic_a ], so that the space to store Vğ‘‰Vitalic_V is dâ¢log2â¡(1+a)ğ‘‘subscript21ğ‘d\log_{2}(1+a)italic_d roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( 1 + italic_a ). Whenever some Vjsubscriptğ‘‰ğ‘—V_{j}italic_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT exceeds ağ‘aitalic_a, we have to scale up to keep Vjsubscriptğ‘‰ğ‘—V_{j}italic_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT in range. Again, we want to maintain the property that ğ”¼â¢2Uâ¢V=xğ”¼superscript2ğ‘ˆğ‘‰ğ‘¥\mathbb{E}2^{U}V=xblackboard_E 2 start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT italic_V = italic_x. Thus, the scale-up should be done as follows. â€“ Uâ†U+1â†ğ‘ˆğ‘ˆ1U\leftarrow U+1italic_U â† italic_U + 1. The scale counter is increased by 1. â€“ For the estimate to be unbiased, Vjsubscriptğ‘‰ğ‘—V_{j}italic_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT should become Vj/2subscriptğ‘‰ğ‘—2V_{j}/2italic_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT / 2 for every jğ‘—jitalic_j. Here we also want Vjsubscriptğ‘‰ğ‘—V_{j}italic_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT to always be an integer. Therefore, if Vjsubscriptğ‘‰ğ‘—V_{j}italic_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT is even, then Vjâ†Vj/2â†subscriptğ‘‰ğ‘—subscriptğ‘‰ğ‘—2V_{j}\leftarrow V_{j}/2italic_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT â† italic_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT / 2. If Vjsubscriptğ‘‰ğ‘—V_{j}italic_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT is odd, then Vjâ†(Vj+Î¾)/2â†subscriptğ‘‰ğ‘—subscriptğ‘‰ğ‘—ğœ‰2V_{j}\leftarrow(V_{j}+\xi)/2italic_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT â† ( italic_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT + italic_Î¾ ) / 2 where Î¾âˆˆ{âˆ’1,1}ğœ‰11\xi\in\{-1,1\}italic_Î¾ âˆˆ { - 1 , 1 } is a freshly sampled Rademacher random variable. The algorithm above is unbiased by design. However, the memory-accuracy tradeoff is not optimal. There is a fundamental problem of the design above that the parameter ağ‘aitalic_a has to be Î©â¢(d)Î©ğ‘‘\Omega(\sqrt{d})roman_Î© ( square-root start_ARG italic_d end_ARG ) (with Ïƒ=Oâ¢(1)ğœğ‘‚1\sigma=O(1)italic_Ïƒ = italic_O ( 1 )) for it to be a dğ‘‘ditalic_d-dimensional counter. Instead of analyzing the algorithm in details, this problem can be identified by simply checking the set of all possible estimates that are produced by the algorithm: ğ’Ÿ={2Uâ¢(V1,â€¦,Vd):Uâˆˆâ„•,âˆ€jâˆˆ[d],Vjâˆˆ[0,a]}.ğ’Ÿconditional-setsuperscript2ğ‘ˆsubscriptğ‘‰1â€¦subscriptğ‘‰ğ‘‘formulae-sequenceğ‘ˆâ„•formulae-sequencefor-allğ‘—delimited-[]ğ‘‘subscriptğ‘‰ğ‘—0ğ‘\displaystyle\mathcal{D}=\{2^{U}(V_{1},\ldots,V_{d}):U\in\mathbb{N},\forall j% \in[d],V_{j}\in[0,a]\}.caligraphic_D = { 2 start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT ( italic_V start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_V start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) : italic_U âˆˆ blackboard_N , âˆ€ italic_j âˆˆ [ italic_d ] , italic_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT âˆˆ [ 0 , italic_a ] } . It turns out that ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D is not dense enough for estimating some input xğ‘¥xitalic_x. Indeed, we choose x=2sâ¢(a,â€¦,aâŸr,1/2,â€¦,1/2âŸdâˆ’r),ğ‘¥superscript2ğ‘ subscriptâŸğ‘â€¦ğ‘ğ‘ŸsubscriptâŸ12â€¦12ğ‘‘ğ‘Ÿ\displaystyle x=2^{s}(\underbrace{a,\ldots,a}_{r},\underbrace{1/2,\ldots,1/2}_% {d-r}),italic_x = 2 start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ( underâŸ start_ARG italic_a , â€¦ , italic_a end_ARG start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT , underâŸ start_ARG 1 / 2 , â€¦ , 1 / 2 end_ARG start_POSTSUBSCRIPT italic_d - italic_r end_POSTSUBSCRIPT ) , for some râˆˆ[0,d]ğ‘Ÿ0ğ‘‘r\in[0,d]italic_r âˆˆ [ 0 , italic_d ] so that both y1=2sâ¢(a,â€¦,a,1,â€¦,1)subscriptğ‘¦1superscript2ğ‘ ğ‘â€¦ğ‘1â€¦1y_{1}=2^{s}(a,\ldots,a,1,\ldots,1)italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 2 start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ( italic_a , â€¦ , italic_a , 1 , â€¦ , 1 ) and y2=2sâˆ’1â¢(a,â€¦,a,1,â€¦,1)subscriptğ‘¦2superscript2ğ‘ 1ğ‘â€¦ğ‘1â€¦1y_{2}=2^{s-1}(a,\ldots,a,1,\ldots,1)italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 2 start_POSTSUPERSCRIPT italic_s - 1 end_POSTSUPERSCRIPT ( italic_a , â€¦ , italic_a , 1 , â€¦ , 1 ) are far from xğ‘¥xitalic_x. By construction, the nearest estimate of xğ‘¥xitalic_x in ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D has to be either y1subscriptğ‘¦1y_{1}italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT or y2subscriptğ‘¦2y_{2}italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT.333Note that y1subscriptğ‘¦1y_{1}italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT is one possible probabilistic rounding of xğ‘¥xitalic_x. Other roundings {2sâ¢(a,â€¦,a,z1,â€¦,zdâˆ’r)âˆ£zjâˆˆ{0,1}}conditional-setsuperscript2ğ‘ ğ‘â€¦ğ‘subscriptğ‘§1â€¦subscriptğ‘§ğ‘‘ğ‘Ÿsubscriptğ‘§ğ‘—01\{2^{s}(a,\ldots,a,z_{1},\ldots,z_{d-r})\mid z_{j}\in\{0,1\}\}{ 2 start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ( italic_a , â€¦ , italic_a , italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_z start_POSTSUBSCRIPT italic_d - italic_r end_POSTSUBSCRIPT ) âˆ£ italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT âˆˆ { 0 , 1 } } all have the same distance to xğ‘¥xitalic_x. By symmetry, we only need to consider y1subscriptğ‘¦1y_{1}italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. The vector y2subscriptğ‘¦2y_{2}italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT is produced by decreasing the scale to 2sâˆ’1superscript2ğ‘ 12^{s-1}2 start_POSTSUPERSCRIPT italic_s - 1 end_POSTSUPERSCRIPT, so the smaller entries can be represented precisely with the cost of underestimating the large entries. We choose rğ‘Ÿritalic_r so that the two choices have equal distances to xğ‘¥xitalic_x. A simple calculation shows that we need to set r=d/(1+4â¢a2)ğ‘Ÿğ‘‘14superscriptğ‘2r=d/(1+4a^{2})italic_r = italic_d / ( 1 + 4 italic_a start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ).444For simplicity, we only require r>1ğ‘Ÿ1r>1italic_r > 1 and omit the details of rounding rğ‘Ÿritalic_r to an integer. Assume now a<d/3ğ‘ğ‘‘3a<\sqrt{d}/3italic_a < square-root start_ARG italic_d end_ARG / 3 and then r>1ğ‘Ÿ1r>1italic_r > 1. Thus minyâˆˆğ’Ÿâ¡|xâˆ’y|2=22â¢sâ¢dâ¢a21+4â¢a2subscriptğ‘¦ğ’Ÿsuperscriptğ‘¥ğ‘¦2superscript22ğ‘ ğ‘‘superscriptğ‘214superscriptğ‘2\min_{y\in\mathcal{D}}|x-y|^{2}=2^{2s}\frac{da^{2}}{1+4a^{2}}roman_min start_POSTSUBSCRIPT italic_y âˆˆ caligraphic_D end_POSTSUBSCRIPT | italic_x - italic_y | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 2 start_POSTSUPERSCRIPT 2 italic_s end_POSTSUPERSCRIPT divide start_ARG italic_d italic_a start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG 1 + 4 italic_a start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG, while |x|2=22â¢sâ‹…2â¢a2â¢d/(1+4â¢a2)superscriptğ‘¥2â‹…superscript22ğ‘ 2superscriptğ‘2ğ‘‘14superscriptğ‘2|x|^{2}=2^{2s}\cdot 2a^{2}d/(1+4a^{2})| italic_x | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 2 start_POSTSUPERSCRIPT 2 italic_s end_POSTSUPERSCRIPT â‹… 2 italic_a start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d / ( 1 + 4 italic_a start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ). Therefore, if the algorithm is an (n,d,Ïƒ)ğ‘›ğ‘‘ğœ(n,d,\sigma)( italic_n , italic_d , italic_Ïƒ )-counter, then ğ”¼â¢|x^âˆ’x|2<Ïƒ2â¢|x|2ğ”¼superscript^ğ‘¥ğ‘¥2superscriptğœ2superscriptğ‘¥2\mathbb{E}|\hat{x}-x|^{2}<\sigma^{2}|x|^{2}blackboard_E | over^ start_ARG italic_x end_ARG - italic_x | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT < italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT | italic_x | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, which means there is at least one xâˆ—âˆˆğ’Ÿsubscriptğ‘¥ğ’Ÿx_{*}\in\mathcal{D}italic_x start_POSTSUBSCRIPT âˆ— end_POSTSUBSCRIPT âˆˆ caligraphic_D such that |xâˆ—âˆ’x|2<Ïƒ2â¢|x|2superscriptsubscriptğ‘¥ğ‘¥2superscriptğœ2superscriptğ‘¥2|x_{*}-x|^{2}<\sigma^{2}|x|^{2}| italic_x start_POSTSUBSCRIPT âˆ— end_POSTSUBSCRIPT - italic_x | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT < italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT | italic_x | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. This suggests that 22â¢sâ¢a2â¢d/(1+4â¢a2)<Ïƒ2â¢22â¢sâ‹…2â¢a2â¢d/(1+4â¢a2)superscript22ğ‘ superscriptğ‘2ğ‘‘14superscriptğ‘2â‹…superscriptğœ2superscript22ğ‘ 2superscriptğ‘2ğ‘‘14superscriptğ‘22^{2s}a^{2}d/(1+4a^{2})<\sigma^{2}2^{2s}\cdot 2a^{2}d/(1+4a^{2})2 start_POSTSUPERSCRIPT 2 italic_s end_POSTSUPERSCRIPT italic_a start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d / ( 1 + 4 italic_a start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) < italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT 2 start_POSTSUPERSCRIPT 2 italic_s end_POSTSUPERSCRIPT â‹… 2 italic_a start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d / ( 1 + 4 italic_a start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) and therefore Ïƒ>1/2ğœ12\sigma>1/\sqrt{2}italic_Ïƒ > 1 / square-root start_ARG 2 end_ARG. In other words, if a<d/3ğ‘ğ‘‘3a<\sqrt{d}/3italic_a < square-root start_ARG italic_d end_ARG / 3, then any (n,d,Ïƒ)ğ‘›ğ‘‘ğœ(n,d,\sigma)( italic_n , italic_d , italic_Ïƒ )-counter must have Ïƒ>1/2ğœ12\sigma>1/\sqrt{2}italic_Ïƒ > 1 / square-root start_ARG 2 end_ARG. We conclude that ağ‘aitalic_a needs to be Î©â¢(d)Î©ğ‘‘\Omega(\sqrt{d})roman_Î© ( square-root start_ARG italic_d end_ARG ) when Ïƒâ‰¤1/2ğœ12\sigma\leq 1/\sqrt{2}italic_Ïƒ â‰¤ 1 / square-root start_ARG 2 end_ARG, indicating an additional Oâ¢(dâ¢logâ¡d)ğ‘‚ğ‘‘ğ‘‘O(d\log d)italic_O ( italic_d roman_log italic_d ) space overhead in comparison to the optimal space. From the discussion above, we see that one has to have a dense enough set of estimates ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D for a correct (n,d,Ïƒ)ğ‘›ğ‘‘ğœ(n,d,\sigma)( italic_n , italic_d , italic_Ïƒ )-counter. Roughly speaking, one wants to spend more bits on large coordinates and fewer bits on small coordinates. This can be done with a simple and natural algorithmic idea: variable-length integer encoding. The usual binary encoding of non-negative integers automatically uses more bits on large numbers and fewer bits on small numbers. We will prove the following set of estimates is dense enough. ğ’Ÿcompressed={2Uâ¢(V1,â€¦,Vd):Uâˆˆâ„•,the encoding length of V is at most Oâ¢(dâ¢log2â¡Ïƒâˆ’1)},subscriptğ’Ÿcompressedconditional-setsuperscript2ğ‘ˆsubscriptğ‘‰1â€¦subscriptğ‘‰ğ‘‘ğ‘ˆâ„•the encoding length of V is at most Oâ¢(dâ¢log2â¡Ïƒâˆ’1)\displaystyle\mathcal{D}_{\text{compressed}}=\{2^{U}(V_{1},\ldots,V_{d}):U\in% \mathbb{N},\text{the encoding length of $V$ is at most $O(d\log_{2}\sigma^{-1}% )$}\},caligraphic_D start_POSTSUBSCRIPT compressed end_POSTSUBSCRIPT = { 2 start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT ( italic_V start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_V start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) : italic_U âˆˆ blackboard_N , the encoding length of italic_V is at most italic_O ( italic_d roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_Ïƒ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) } , This leads to our upper bound in Theorem 1. One may observe how this encoding idea better handles the previous counterexample x=2sâ¢(a,â€¦,a,1/2,â€¦,1/2)ğ‘¥superscript2ğ‘ ğ‘â€¦ğ‘12â€¦12x=2^{s}(a,\ldots,a,1/2,\ldots,1/2)italic_x = 2 start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ( italic_a , â€¦ , italic_a , 1 / 2 , â€¦ , 1 / 2 ). An estimate of 2sâˆ’1â¢(2â¢a,â€¦,2â¢a,1,â€¦,1)superscript2ğ‘ 12ğ‘â€¦2ğ‘1â€¦12^{s-1}(2a,\ldots,2a,1,\ldots,1)2 start_POSTSUPERSCRIPT italic_s - 1 end_POSTSUPERSCRIPT ( 2 italic_a , â€¦ , 2 italic_a , 1 , â€¦ , 1 ) can now be returned because though each entry with value 2â¢a2ğ‘2a2 italic_a needs above average space to encode, each entry with value 1111 uses less than average space to encode, saving the memory space for the large entries. 1.1.2 Lower Bound As we have discussed above, the set of all estimates produced by the counter has to be dense enough for the task of dğ‘‘ditalic_d-dimensional counting to be possible. The lower bound in Theorem 1 thus arises from estimating the size of multiplicative space coverings. Definition 2 (multiplicative space covering). For any subsets A,RâŠ‚â„dğ´ğ‘…superscriptâ„ğ‘‘A,R\subset\mathbb{R}^{d}italic_A , italic_R âŠ‚ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT and Ïƒ>0ğœ0\sigma>0italic_Ïƒ > 0, we say Rğ‘…Ritalic_R is a Ïƒğœ\sigmaitalic_Ïƒ-multiplicative covering of Ağ´Aitalic_A if for any xâˆˆAğ‘¥ğ´x\in Aitalic_x âˆˆ italic_A there exists yâˆˆRğ‘¦ğ‘…y\in Ritalic_y âˆˆ italic_R such that |xâˆ’y|<Ïƒâ¢|x|ğ‘¥ğ‘¦ğœğ‘¥|x-y|<\sigma|x|| italic_x - italic_y | < italic_Ïƒ | italic_x |. Clearly, the set of all estimates of an (n,d,Ïƒ)ğ‘›ğ‘‘ğœ(n,d,\sigma)( italic_n , italic_d , italic_Ïƒ )-counter will form a Ïƒğœ\sigmaitalic_Ïƒ-multiplicative space covering of ğ’³={xâˆˆâ„•dâˆ£x1+â‹¯+xdâ‰¤n}ğ’³conditional-setğ‘¥superscriptâ„•ğ‘‘subscriptğ‘¥1â‹¯subscriptğ‘¥ğ‘‘ğ‘›\mathcal{X}=\{x\in\mathbb{N}^{d}\mid x_{1}+\cdots+x_{d}\leq n\}caligraphic_X = { italic_x âˆˆ blackboard_N start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT âˆ£ italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + â‹¯ + italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT â‰¤ italic_n }. The lower bound is thus proved by estimating the covering size of ğ’³ğ’³\mathcal{X}caligraphic_X, which can be further reduced to the classic problem of covering spheres with smaller spheres [16]. 1.2 Related Work There have been many variants and analysis of Morris counters before [10, 11, 8, 7, 14]. Nevertheless, they have only considered the one-dimensional case. Tracking a multi-dimensional count vector (with a vector norm) is a new problem to the best of our knowledge. In the one-dimensional case (d=1ğ‘‘1d=1italic_d = 1), our algorithm is equivalent to the variantâ€”floating-point counterâ€”by CsÅ±rÃ¶s [4], except for that the floating-point counter restricts ağ‘aitalic_a to be a power of two so no probabilistic rounding is needed when halving. Nevertheless, when dâ‰¥2ğ‘‘2d\geq 2italic_d â‰¥ 2, probabilistic rounding is inevitable. In comparison to the classic Morris counter, the floating-point variant is much more convenient to implement on binary machines, where only simple integer operations and bit operations are needed [4]. Another advantage is that the number of random bits needed per increment is at most 2 in expectation, since the probability 2âˆ’Usuperscript2ğ‘ˆ2^{-U}2 start_POSTSUPERSCRIPT - italic_U end_POSTSUPERSCRIPT can be simulated by generating random bits sequentially, looking for Uğ‘ˆUitalic_U consecutive ones [4] (stop generating random bits when a zero shows up). Such advantages are inherited by our dğ‘‘ditalic_d-dimensional counter as well. 1.3 Organization We will construct the upper bound in Â§2 and prove the lower bound in Â§3. The main theorem (Theorem 1) follows directly from Corollary 1 and 3. We have included a sample run in Appendix A to help illustrate the patterns of the new algorithm."
https://arxiv.org/html/2411.03037v1,Top-k Stabbing Interval Queries,"We investigate a weighted variant of the interval stabbing problem, where the goal is to design an efficient data structure for a given set â„â„\mathcal{I}caligraphic_I of weighted intervals such that, for a query point qğ‘qitalic_q and an integer k>0ğ‘˜0k>0italic_k > 0, we can report the kğ‘˜kitalic_k intervals with largest weights among those stabbed by qğ‘qitalic_q. In this paper, we present a linear space solution with Oâ¢(logâ¡n+k)ğ‘‚ğ‘›ğ‘˜O(\log n+k)italic_O ( roman_log italic_n + italic_k ) query time. Moreover, we also present another trade-off for the problem.","The interval stabbing problem is an important problem in computational geometry [2, 1, 4], which asks to compute all intervals stabbed by a query point. In this paper, we study a natural variant of this problem where, instead of computing all stabbed intervals, the goal is to find a subset of the stabbed intervals that â€œbestâ€ represent them [1]. Formally, the problem is defined as follows. Let â„â„\mathcal{I}caligraphic_I be a set of nğ‘›nitalic_n intervals on the real line, each with a real valued weight. The set â„â„\mathcal{I}caligraphic_I is static. The goal is to preprocess â„â„\mathcal{I}caligraphic_I so that, for a query value qğ‘qitalic_q and a positive integer kğ‘˜kitalic_k, the kğ‘˜kitalic_k intervals stabbed by qğ‘qitalic_q with largest weights can be reported efficiently. An interval [si,ei]âˆˆâ„subscriptğ‘ ğ‘–subscriptğ‘’ğ‘–â„[s_{i},e_{i}]\in\mathcal{I}[ italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ] âˆˆ caligraphic_I is stabbed by qğ‘qitalic_q if and only if siâ‰¤qâ‰¤eisubscriptğ‘ ğ‘–ğ‘subscriptğ‘’ğ‘–s_{i}\leq q\leq e_{i}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT â‰¤ italic_q â‰¤ italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. The problem is useful in cryptocurrency and stock applications [6, 7]. We will refer this variant as the top-kğ‘˜kitalic_k interval stabbing problem. In this paper, we present a linear space solution with Oâ¢(logâ¡n+k)ğ‘‚ğ‘›ğ‘˜O(\log n+k)italic_O ( roman_log italic_n + italic_k ) query time. In the rank space, the query time can be reduced to Oâ¢(k)ğ‘‚ğ‘˜O(k)italic_O ( italic_k ). Moreover, we also present a solution that uses only segment tree structure. It takes Oâ¢(nâ¢logâ¡n)ğ‘‚ğ‘›ğ‘›O(n\log n)italic_O ( italic_n roman_log italic_n ) space and time to build, and Oâ¢(logâ¡n+kâ¢logâ¡logâ¡n)ğ‘‚ğ‘›ğ‘˜ğ‘›O(\log n+k\log\log n)italic_O ( roman_log italic_n + italic_k roman_log roman_log italic_n ) time to answer a query. In rank space, the query time can be improved to Oâ¢(logâ¡n+k)ğ‘‚ğ‘›ğ‘˜O(\log n+k)italic_O ( roman_log italic_n + italic_k ). Amagata et al. [1] has proposed two algorithms for the problem; one algorithm uses interval tree structure while the other one employs segment tree structure. The algorithm based on interval tree structure takes linear space and Oâ¢(nâ¢logâ¡n+k)ğ‘‚ğ‘›ğ‘›ğ‘˜O(\sqrt{n}\log n+k)italic_O ( square-root start_ARG italic_n end_ARG roman_log italic_n + italic_k ) time to answer a query. The preprocessing takes Oâ¢(nâ¢logâ¡n)ğ‘‚ğ‘›ğ‘›O(n\log n)italic_O ( italic_n roman_log italic_n ) time. The other algorithm based on segment tree achieves Oâ¢(logâ¡n+k)ğ‘‚ğ‘›ğ‘˜O(\log n+k)italic_O ( roman_log italic_n + italic_k ) query time, the preprocessing takes Oâ¢(nâ¢logâ¡nâ¢logâ¡logâ¡n)ğ‘‚ğ‘›ğ‘›ğ‘›O(n\log n\log\log n)italic_O ( italic_n roman_log italic_n roman_log roman_log italic_n ) time and Oâ¢(nâ¢log2â¡n)ğ‘‚ğ‘›superscript2ğ‘›O(n\log^{2}n)italic_O ( italic_n roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_n ) space."
https://arxiv.org/html/2411.02845v1,Max-Distance Sparsification for Diversification and Clustering,"Let ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D be a set family that is the solution domain of some combinatorial problem. The max-min diversification problem on ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D is the problem to select kğ‘˜kitalic_k sets from ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D such that the Hamming distance between any two selected sets is at least dğ‘‘ditalic_d. FPT algorithms parameterized by k,l:=maxDâˆˆğ’Ÿâ¡|D|assignğ‘˜ğ‘™subscriptğ·ğ’Ÿğ·k,l:=\max_{D\in\mathcal{D}}|D|italic_k , italic_l := roman_max start_POSTSUBSCRIPT italic_D âˆˆ caligraphic_D end_POSTSUBSCRIPT | italic_D | and k,dğ‘˜ğ‘‘k,ditalic_k , italic_d have been actively studied recently for several specific domains.This paper provides unified algorithmic frameworks to solve this problem. Specifically, for each parameterization k,lğ‘˜ğ‘™k,litalic_k , italic_l and k,dğ‘˜ğ‘‘k,ditalic_k , italic_d, we provide an FPT oracle algorithm for the max-min diversification problem using oracles related to ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D. We then demonstrate that our frameworks generalize most of the existing domain-specific tractability results and provide the first FPT algorithms for several domains.Our main technical breakthrough is introducing the notion of max-distance sparsifier of ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D, a domain on which the max-min diversification problem is equivalent to the same problem on the original domain ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D. The core of our framework is to design FPT oracle algorithms that construct a constant-size max-distance sparsifier of ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D. Using max-distance sparsifiers, we provide FPT algorithms for the max-min and max-sum diversification problems on ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D, as well as kğ‘˜kitalic_k-center and kğ‘˜kitalic_k-sum-of-radii clustering problems on ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D, which are also natural problems in the context of diversification and have their own interests.","1.1 Background and Motivation The procedure for approaching real-world problems with optimization algorithms involves formulating the real-world motivations as mathematical problems and then solving them. However, real-world problems are complex, and the idea of a â€œgoodâ€ solution cannot always be correctly formulated. Attempting to impose a formulation may ignore important perspectives such as human sensibilities and ethical considerations. The paradigm of diversification, introduced by [baste2019fpt] and [baste2022diversity], is a â€œformulation of the unformulatable problemsâ€, which formulates diversity measures for a set of multiple solutions, rather than attempting to formulate the â€œgoodnessâ€ of a single solution. By computing a set of solutions that maximize this measure, the algorithm provides effective options to evaluators who have the correct criteria for judging the â€œgoodnessâ€ of a solution. Let Uğ‘ˆUitalic_U be a finite set, kâˆˆâ„¤â‰¥1ğ‘˜subscriptâ„¤absent1k\in\mathbb{Z}_{\geq 1}italic_k âˆˆ blackboard_Z start_POSTSUBSCRIPT â‰¥ 1 end_POSTSUBSCRIPT and dâˆˆâ„¤â‰¥0ğ‘‘subscriptâ„¤absent0d\in\mathbb{Z}_{\geq 0}italic_d âˆˆ blackboard_Z start_POSTSUBSCRIPT â‰¥ 0 end_POSTSUBSCRIPT. Let ğ’ŸâŠ†2Uğ’Ÿsuperscript2ğ‘ˆ\mathcal{D}\subseteq 2^{U}caligraphic_D âŠ† 2 start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT be the feasible domain of some combinatorial problem. The following problem frameworks, defined by two types of diversity measures, have been studied extensively. Max-Min Diversification Problem on ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D: Does there exist a kğ‘˜kitalic_k-tuple (D1,â€¦,Dk)âˆˆğ’Ÿksubscriptğ·1â€¦subscriptğ·ğ‘˜superscriptğ’Ÿğ‘˜(D_{1},\dots,D_{k})\in\mathcal{D}^{k}( italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) âˆˆ caligraphic_D start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT of sets in ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D such that min1â‰¤i<jâ‰¤kâ¡|Diâ¢â–³â¢Dj|â‰¥dsubscript1ğ‘–ğ‘—ğ‘˜subscriptğ·ğ‘–â–³subscriptğ·ğ‘—ğ‘‘\min_{1\leq i<j\leq k}|D_{i}\triangle D_{j}|\geq droman_min start_POSTSUBSCRIPT 1 â‰¤ italic_i < italic_j â‰¤ italic_k end_POSTSUBSCRIPT | italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT â–³ italic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | â‰¥ italic_d?111We define Z1â¢â–³â¢Z2:=(Z1âˆ–Z2)âˆª(Z2âˆ–Z1)assignsubscriptğ‘1â–³subscriptğ‘2subscriptğ‘1subscriptğ‘2subscriptğ‘2subscriptğ‘1Z_{1}\triangle Z_{2}:=(Z_{1}\setminus Z_{2})\cup(Z_{2}\setminus Z_{1})italic_Z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT â–³ italic_Z start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT := ( italic_Z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT âˆ– italic_Z start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) âˆª ( italic_Z start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT âˆ– italic_Z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ). Max-Sum Diversification Problem on ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D: Does there exist a kğ‘˜kitalic_k-tuple (D1,â€¦,Dk)âˆˆğ’Ÿksubscriptğ·1â€¦subscriptğ·ğ‘˜superscriptğ’Ÿğ‘˜(D_{1},\dots,D_{k})\in\mathcal{D}^{k}( italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) âˆˆ caligraphic_D start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT of sets in ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D such that âˆ‘1â‰¤i<jâ‰¤k|Diâ¢â–³â¢Dj|â‰¥dsubscript1ğ‘–ğ‘—ğ‘˜subscriptğ·ğ‘–â–³subscriptğ·ğ‘—ğ‘‘\sum_{1\leq i<j\leq k}|D_{i}\triangle D_{j}|\geq dâˆ‘ start_POSTSUBSCRIPT 1 â‰¤ italic_i < italic_j â‰¤ italic_k end_POSTSUBSCRIPT | italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT â–³ italic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | â‰¥ italic_d? These problems ensure diversity by aiming to output solutions that are as dissimilar as possible in terms of Hamming distance. Parameterized algorithms for diversification problems have been actively studied. Particularly, FPT algorithms for the max-min diversification problems parameterized by kğ‘˜kitalic_k and l:=maxDâˆˆğ’Ÿâ¡|D|assignğ‘™subscriptğ·ğ’Ÿğ·l:=\max_{D\in\mathcal{D}}|D|italic_l := roman_max start_POSTSUBSCRIPT italic_D âˆˆ caligraphic_D end_POSTSUBSCRIPT | italic_D | [baste2022diversity, baste2019fpt, fomin2024diversecollection, hanaka2021finding, shida2024finding], as well as by kğ‘˜kitalic_k and dğ‘‘ditalic_d [eiben2024determinantal, fomin2024diversepair, fomin2024diversecollection, funayama2024parameterized], have been the focus of research. Since assuming dâ‰¤2â¢lğ‘‘2ğ‘™d\leq 2litalic_d â‰¤ 2 italic_l does not lose generality in the max-min diversification problem, the latter addresses a more general situation than the former. The max-sum diversification problem is empirically more tractable than the max-min diversification problem, and most of the existing algorithms for the max-min diversification problems can be straightforwardly converted to solve the max-sum diversification problems on the corresponding domain. Therefore, for some time hereafter, we will restrict our discussion to the max-min diversification problem. The most successful technical framework in this field is color-coding [alon1995color]. [hanaka2021finding] constructed a color-coding based framework designing FPT algorithms parameterized by k,lğ‘˜ğ‘™k,litalic_k , italic_l for the max-min diversification problem on several domains. [shida2024finding] provided the same result for the case where ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D is the family of longest common subsequences. Color-coding has also been successful for the case where the parameter is k,dğ‘˜ğ‘‘k,ditalic_k , italic_d. [fomin2024diversepair] provided a color-coding based FPT algorithm for the max-min diversification problem on ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D parameterized by dğ‘‘ditalic_d, where ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D is the family of maximum matchings and k=2ğ‘˜2k=2italic_k = 2. Furthermore, [fomin2024diversecollection] provided a color-coding based FPT algorithm parameterized by k,dğ‘˜ğ‘‘k,ditalic_k , italic_d for the family of perfect matchings, and [funayama2024parameterized] provided the same result for the family of shortest paths. All of these algorithms use a subroutine to find a colorful set that belongs to a domain related to ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D for a given coloring of Uğ‘ˆUitalic_U. This research provides general algorithmic frameworks for FPT algorithms addressing max-min (and max-sum) diversification problems for both parameterizations k,lğ‘˜ğ‘™k,litalic_k , italic_l and k,dğ‘˜ğ‘‘k,ditalic_k , italic_d. Our framework is very general and can be applied to all domains [bang2021k, bang2016parameterized, baste2022diversity, baste2019fpt, eiben2024determinantal, fomin2024diversepair, fomin2024diversecollection, funayama2024parameterized, gutin2018k, hanaka2021finding, shida2024finding] for which FPT algorithms parameterized by k,lğ‘˜ğ‘™k,litalic_k , italic_l and k,dğ‘˜ğ‘‘k,ditalic_k , italic_d are currently known for the case that diversity measure is defined using an unweighted Hamming distance and several domains where such algorithms were previously unknown. Unlike existing studies, our framework is not based on color-coding. Very roughly speaking, our framework solves the max-min diversification problem on ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D by utilizing an oracle that determines whether there exists a set of a specified size in the domain related to ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D that satisfies additional simple constraints. The design of this oracle is generally much simpler than the design of an oracle searching for colorful sets in the same domain, making our framework easy-to-use and applicable for a broader domain ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D. Our main technical breakthrough is introducing a notion of max-distance sparsifier as an intermediate step for solving the max-min diversification problem, whose precise definition is given in Section 1.3. The most critical fact is that when ğ’¦ğ’¦\mathcal{K}caligraphic_K is a max-distance sparsifier of ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D, the max-min diversification problem on ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D is equivalent to the same problem on ğ’¦ğ’¦\mathcal{K}caligraphic_K. Our framework constructs a max-distance sparsifier ğ’¦ğ’¦\mathcal{K}caligraphic_K of ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D with a constant size, enabling us to solve the max-min diversification problems on ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D by brute-force search on ğ’¦ğ’¦\mathcal{K}caligraphic_K. The power of max-distance sparsification is not limited to solving max-min (and max-sum) diversification problems. Specifically, the following kğ‘˜kitalic_k-center [hsu1979easy] and kğ‘˜kitalic_k-sum-of-radii clustering problems on ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D [charikar2001clustering] can also be solved via max-distance sparsification. kğ‘˜kitalic_k-Center Clustering Problem on ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D: Does there exist a kğ‘˜kitalic_k-tuple of subsets (D1,â€¦,Dk)âˆˆğ’Ÿksubscriptğ·1â€¦subscriptğ·ğ‘˜superscriptğ’Ÿğ‘˜(D_{1},\dots,D_{k})\in\mathcal{D}^{k}( italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) âˆˆ caligraphic_D start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT such that for all Dâˆˆğ’Ÿğ·ğ’ŸD\in\mathcal{D}italic_D âˆˆ caligraphic_D, there exists an iâˆˆ{1,â€¦,k}ğ‘–1â€¦ğ‘˜i\in\{1,\dots,k\}italic_i âˆˆ { 1 , â€¦ , italic_k } satisfying |Diâ¢â–³â¢D|â‰¤dsubscriptğ·ğ‘–â–³ğ·ğ‘‘|D_{i}\triangle D|\leq d| italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT â–³ italic_D | â‰¤ italic_d? kğ‘˜kitalic_k-Sum-of-Radii Clustering Problem on ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D: Does there exist a kğ‘˜kitalic_k-tuple of subsets (D1,â€¦,Dk)âˆˆğ’Ÿksubscriptğ·1â€¦subscriptğ·ğ‘˜superscriptğ’Ÿğ‘˜(D_{1},\dots,D_{k})\in\mathcal{D}^{k}( italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) âˆˆ caligraphic_D start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT and a kğ‘˜kitalic_k-tuple of non-negative integers (d1,â€¦,dk)âˆˆâ„¤â‰¥0ksubscriptğ‘‘1â€¦subscriptğ‘‘ğ‘˜superscriptsubscriptâ„¤absent0ğ‘˜(d_{1},\dots,d_{k})\in\mathbb{Z}_{\geq 0}^{k}( italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) âˆˆ blackboard_Z start_POSTSUBSCRIPT â‰¥ 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT with âˆ‘iâˆˆ{1,â€¦,k}diâ‰¤dsubscriptğ‘–1â€¦ğ‘˜subscriptğ‘‘ğ‘–ğ‘‘\sum_{i\in\{1,\dots,k\}}d_{i}\leq dâˆ‘ start_POSTSUBSCRIPT italic_i âˆˆ { 1 , â€¦ , italic_k } end_POSTSUBSCRIPT italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT â‰¤ italic_d such that for all Dâˆˆğ’Ÿğ·ğ’ŸD\in\mathcal{D}italic_D âˆˆ caligraphic_D, there exists an iâˆˆ{1,â€¦,k}ğ‘–1â€¦ğ‘˜i\in\{1,\dots,k\}italic_i âˆˆ { 1 , â€¦ , italic_k } satisfying |Diâ¢â–³â¢D|â‰¤disubscriptğ·ğ‘–â–³ğ·subscriptğ‘‘ğ‘–|D_{i}\triangle D|\leq d_{i}| italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT â–³ italic_D | â‰¤ italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT? When ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D is an explicitly given set of points, parameterized algorithms for these problems have been extensively studied in the area of clustering [amir2014efficiency, bandyapadhyayL023a, chen2024parameterized, demaine2005fixed, eiben2023parameterized, feldmann2020parameterized, inamdar2020capacitated]. This research initiates the study of clustering problems when ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D is implicitly given as the solution domain of combinatorial problems. These problems are also natural in the context of diversification, since in real situations, the concept of diversity often means that the extracted elements cover the entire space comprehensively rather than being mutually dissimilar. This motivation is formulated by clustering problems, which extract a list of sets in ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D such that for each set in ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D, there is an extracted set near to it. 1.2 Our Results This paper consists of two parts. In the first part, we design general frameworks for solving diversification and clustering problems. In the second part, we apply our frameworks to several specific domains ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D. 1.2.1 The Frameworks We define the following (âˆ’1,1)11(-1,1)( - 1 , 1 )-optimization oracle on ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D and the exact extension oracle on ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D. (âˆ’1,1)11(-1,1)( - 1 , 1 )-Optimization Oracle on ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D: Let Uğ‘ˆUitalic_U be a finite set, ğ’ŸâŠ†2Uğ’Ÿsuperscript2ğ‘ˆ\mathcal{D}\subseteq 2^{U}caligraphic_D âŠ† 2 start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT, and wâˆˆ{âˆ’1,1}Uğ‘¤superscript11ğ‘ˆw\in\{-1,1\}^{U}italic_w âˆˆ { - 1 , 1 } start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT be a weight vector. Return a set Dâˆˆğ’Ÿğ·ğ’ŸD\in\mathcal{D}italic_D âˆˆ caligraphic_D that maximizes âˆ‘eâˆˆDwesubscriptğ‘’ğ·subscriptğ‘¤ğ‘’\sum_{e\in D}w_{e}âˆ‘ start_POSTSUBSCRIPT italic_e âˆˆ italic_D end_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT. Exact Extension Oracle on ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D: Let Uğ‘ˆUitalic_U be a finite set, râˆˆâ„¤â‰¥0ğ‘Ÿsubscriptâ„¤absent0r\in\mathbb{Z}_{\geq 0}italic_r âˆˆ blackboard_Z start_POSTSUBSCRIPT â‰¥ 0 end_POSTSUBSCRIPT, ğ’ŸâŠ†2Uğ’Ÿsuperscript2ğ‘ˆ\mathcal{D}\subseteq 2^{U}caligraphic_D âŠ† 2 start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT, and Câˆˆğ’Ÿğ¶ğ’ŸC\in\mathcal{D}italic_C âˆˆ caligraphic_D. Let X,YâŠ†Uğ‘‹ğ‘Œğ‘ˆX,Y\subseteq Uitalic_X , italic_Y âŠ† italic_U be two disjoint subsets of Uğ‘ˆUitalic_U. If there exists a set Dâˆˆğ’Ÿğ·ğ’ŸD\in\mathcal{D}italic_D âˆˆ caligraphic_D such that |Dâ¢â–³â¢C|=rğ·â–³ğ¶ğ‘Ÿ|D\triangle C|=r| italic_D â–³ italic_C | = italic_r, XâŠ†Dğ‘‹ğ·X\subseteq Ditalic_X âŠ† italic_D, and Yâˆ©D=âˆ…ğ‘Œğ·Y\cap D=\emptysetitalic_Y âˆ© italic_D = âˆ…, return one such set. If no such set exists, return âŠ¥bottom\botâŠ¥. When C=âˆ…ğ¶C=\emptysetitalic_C = âˆ… and X=âˆ…ğ‘‹X=\emptysetitalic_X = âˆ…, we specifically call the exact extension oracle on ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D the exact empty extension oracle on ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D. Exact Empty Extension Oracle on ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D: Let Uğ‘ˆUitalic_U be a finite set, râˆˆâ„¤â‰¥0ğ‘Ÿsubscriptâ„¤absent0r\in\mathbb{Z}_{\geq 0}italic_r âˆˆ blackboard_Z start_POSTSUBSCRIPT â‰¥ 0 end_POSTSUBSCRIPT, ğ’ŸâŠ†2Uğ’Ÿsuperscript2ğ‘ˆ\mathcal{D}\subseteq 2^{U}caligraphic_D âŠ† 2 start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT, and YâŠ†Uğ‘Œğ‘ˆY\subseteq Uitalic_Y âŠ† italic_U. If there exists a set Dâˆˆğ’Ÿğ·ğ’ŸD\in\mathcal{D}italic_D âˆˆ caligraphic_D such that |D|=rğ·ğ‘Ÿ|D|=r| italic_D | = italic_r and Yâˆ©D=âˆ…ğ‘Œğ·Y\cap D=\emptysetitalic_Y âˆ© italic_D = âˆ…, return one such set. If no such set exists, return âŠ¥bottom\botâŠ¥. Let ğ’«ğ’Ÿsubscriptğ’«ğ’Ÿ\mathcal{P}_{\mathcal{D}}caligraphic_P start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT be the max-min/max-sum diversification problem or the kğ‘˜kitalic_k-center/kğ‘˜kitalic_k-sum-of-radii clustering problem on ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D. Our main result is FPT algorithms for solving ğ’«ğ’Ÿsubscriptğ’«ğ’Ÿ\mathcal{P}_{\mathcal{D}}caligraphic_P start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT using these oracles. We construct frameworks for both types of parameterizations, k,lğ‘˜ğ‘™k,litalic_k , italic_l and k,dğ‘˜ğ‘‘k,ditalic_k , italic_d. The result for the parameterization by k,lğ‘˜ğ‘™k,litalic_k , italic_l is as follows. Theorem 1.1. There exists an oracle algorithm solving ğ’«ğ’Ÿsubscriptğ’«ğ’Ÿ\mathcal{P}_{\mathcal{D}}caligraphic_P start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT using the exact empty extension oracle on ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D, where the number of oracle calls and time complexity are both FPT parameterized by k,lğ‘˜ğ‘™k,litalic_k , italic_l and for each call of an oracle, rğ‘Ÿritalic_r and |Y|ğ‘Œ|Y|| italic_Y | are bounded by constants that depend only on k,lğ‘˜ğ‘™k,litalic_k , italic_l. The result for the parameterization by k,dğ‘˜ğ‘‘k,ditalic_k , italic_d is as follows. Theorem 1.2. There exists a randomized oracle algorithm solving ğ’«ğ’Ÿsubscriptğ’«ğ’Ÿ\mathcal{P}_{\mathcal{D}}caligraphic_P start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT using the (âˆ’1,1)11(-1,1)( - 1 , 1 )-optimization oracle on ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D and the exact extension oracle on ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D, where the number of oracle calls and time complexity are both FPT parameterized by k,dğ‘˜ğ‘‘k,ditalic_k , italic_d and for each call of the exact extension oracle, r,|X|,|Y|ğ‘Ÿğ‘‹ğ‘Œr,|X|,|Y|italic_r , | italic_X | , | italic_Y | are bounded by constants that depend only on k,dğ‘˜ğ‘‘k,ditalic_k , italic_d. 1.2.2 Applications of Theorem 1.1 For most domains ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D, the exact empty extension oracle on ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D can be straightforwardly designed by using an algorithm for extracting a single element from ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D. Combining Theorem 1.1 with this empirical fact, we can claim the following surprising fact: For most domain ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D, the diversification and clustering problems parameterized by k,lğ‘˜ğ‘™k,litalic_k , italic_l on ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D are as easy as determining the non-emptiness of ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D. We demonstrate this by providing an FPT algorithm for the exact empty extension oracle on several domains ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D on which an FPT algorithm for the max-min diversification problem parameterized by k,lğ‘˜ğ‘™k,litalic_k , italic_l is known. Specifically, we design FPT algorithms for ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D being the vertex cover [baste2022diversity, baste2019fpt], tğ‘¡titalic_t-hitting set [baste2022diversity], feedback vertex set [baste2022diversity], and common independent set of two matroids [fomin2024diversecollection]. Furthermore, we prove that our framework generalizes existing frameworks provided by [baste2019fpt] and [hanaka2021finding] in both the broader applicability to more general domains ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D and the applicability to clustering problems. 1.2.3 Applications of Theorem 1.2 FPT algorithms for the max-min diversification problems on ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D parameterized by k,dğ‘˜ğ‘‘k,ditalic_k , italic_d are known for the cases where ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D is the family of matroid bases [fomin2024diversecollection], perfect matchings [fomin2024diversecollection], and shortest paths [funayama2024parameterized]. Additionally, FPT algorithms parameterized by k,lğ‘˜ğ‘™k,litalic_k , italic_l are known for the cases where ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D is the family of interval schedulings [hanaka2021finding] and the longest common subsequences of absolute constant number of strings [shida2024finding]. We consider the following domains ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D, which include, generalize, or are entirely novel compared to, the cases mentioned above. Matroid Base Domain: Let M=(U,â„)ğ‘€ğ‘ˆâ„M=(U,\mathcal{I})italic_M = ( italic_U , caligraphic_I ) be a matroid. ğ’ŸâŠ†2Uğ’Ÿsuperscript2ğ‘ˆ\mathcal{D}\subseteq 2^{U}caligraphic_D âŠ† 2 start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT is the family of bases of Mğ‘€Mitalic_M. Branching Domain: Let G=(V,E)ğºğ‘‰ğ¸G=(V,E)italic_G = ( italic_V , italic_E ) be a directed graph and râˆˆVğ‘Ÿğ‘‰r\in Vitalic_r âˆˆ italic_V. ğ’ŸâŠ†2Eğ’Ÿsuperscript2ğ¸\mathcal{D}\subseteq 2^{E}caligraphic_D âŠ† 2 start_POSTSUPERSCRIPT italic_E end_POSTSUPERSCRIPT is the family of edge sets of branchings with root rğ‘Ÿritalic_r in GğºGitalic_G. Matching Domain: Let G=(V,E)ğºğ‘‰ğ¸G=(V,E)italic_G = ( italic_V , italic_E ) be an undirected graph and lâˆˆâ„¤â‰¥0ğ‘™subscriptâ„¤absent0l\in\mathbb{Z}_{\geq 0}italic_l âˆˆ blackboard_Z start_POSTSUBSCRIPT â‰¥ 0 end_POSTSUBSCRIPT. ğ’ŸâŠ†2Eğ’Ÿsuperscript2ğ¸\mathcal{D}\subseteq 2^{E}caligraphic_D âŠ† 2 start_POSTSUPERSCRIPT italic_E end_POSTSUPERSCRIPT is the family of matchings of size lğ‘™litalic_l in GğºGitalic_G. Minimum Edge s,tğ‘ ğ‘¡s,titalic_s , italic_t-Flow Domain: Let G=(V,E)ğºğ‘‰ğ¸G=(V,E)italic_G = ( italic_V , italic_E ) be a directed graph, bâˆˆâ„¤â‰¥0ğ‘subscriptâ„¤absent0b\in\mathbb{Z}_{\geq 0}italic_b âˆˆ blackboard_Z start_POSTSUBSCRIPT â‰¥ 0 end_POSTSUBSCRIPT, and s,tâˆˆVğ‘ ğ‘¡ğ‘‰s,t\in Vitalic_s , italic_t âˆˆ italic_V. ğ’ŸâŠ†2Eğ’Ÿsuperscript2ğ¸\mathcal{D}\subseteq 2^{E}caligraphic_D âŠ† 2 start_POSTSUPERSCRIPT italic_E end_POSTSUPERSCRIPT is the family of the edge sets of s,tğ‘ ğ‘¡s,titalic_s , italic_t-flows of amount bğ‘bitalic_b using minimum number of edges. Minimum Steiner Tree Domain: Let G=(V,E)ğºğ‘‰ğ¸G=(V,E)italic_G = ( italic_V , italic_E ) be an undirected graph and TâŠ†Vğ‘‡ğ‘‰T\subseteq Vitalic_T âŠ† italic_V. ğ’ŸâŠ†2Eğ’Ÿsuperscript2ğ¸\mathcal{D}\subseteq 2^{E}caligraphic_D âŠ† 2 start_POSTSUPERSCRIPT italic_E end_POSTSUPERSCRIPT is the family of edge sets of minimum Steiner trees in GğºGitalic_G that connects the terminal set Tğ‘‡Titalic_T. Minimum s,tğ‘ ğ‘¡s,titalic_s , italic_t-Cut Domain: Let G=(V,E)ğºğ‘‰ğ¸G=(V,E)italic_G = ( italic_V , italic_E ) be a directed graph and s,tâˆˆVğ‘ ğ‘¡ğ‘‰s,t\in Vitalic_s , italic_t âˆˆ italic_V. ğ’ŸâŠ†2Vğ’Ÿsuperscript2ğ‘‰\mathcal{D}\subseteq 2^{V}caligraphic_D âŠ† 2 start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT is the family of vertex sets ZâŠ†Vğ‘ğ‘‰Z\subseteq Vitalic_Z âŠ† italic_V that include sğ‘ sitalic_s, exclude tğ‘¡titalic_t, and minimize the number of edges between Zğ‘Zitalic_Z and Vâˆ–Zğ‘‰ğ‘V\setminus Zitalic_V âˆ– italic_Z. Edge Bipartization Domain: Let G=(V,E)ğºğ‘‰ğ¸G=(V,E)italic_G = ( italic_V , italic_E ) be an undirected graph. ğ’ŸâŠ†2Vğ’Ÿsuperscript2ğ‘‰\mathcal{D}\subseteq 2^{V}caligraphic_D âŠ† 2 start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT is the family of vertex sets ZâŠ†Vğ‘ğ‘‰Z\subseteq Vitalic_Z âŠ† italic_V such that the total number of edges either within Zğ‘Zitalic_Z or within Vâˆ–Zğ‘‰ğ‘V\setminus Zitalic_V âˆ– italic_Z takes the minimum value sğ‘ sitalic_s. Dynamic Programming Problems Domain: Let Uğ‘ˆUitalic_U be a finite set, G=(V,E)ğºğ‘‰ğ¸G=(V,E)italic_G = ( italic_V , italic_E ) be a directed acyclic graph, and qâˆˆVUğ‘superscriptğ‘‰ğ‘ˆq\in V^{U}italic_q âˆˆ italic_V start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT be a labeling of vertices such that no path in GğºGitalic_G passes through multiple vertices with the same label. ğ’ŸâŠ†2Uğ’Ÿsuperscript2ğ‘ˆ\mathcal{D}\subseteq 2^{U}caligraphic_D âŠ† 2 start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT is the family of subsets DâŠ†Uğ·ğ‘ˆD\subseteq Uitalic_D âŠ† italic_U such that there exists a longest path in GğºGitalic_G where the set of labels of the vertices on the path is Dğ·Ditalic_D. See Section 5.2 for the more precise definitions of each domain. By applying Theorem 1.2, we provide FPT algorithms for diversification and clustering problems on all those domains ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D, where the parameterization is kğ‘˜kitalic_k and dğ‘‘ditalic_d except for the minimum Steiner tree domain, which is parameterized by k,d,|T|ğ‘˜ğ‘‘ğ‘‡k,d,|T|italic_k , italic_d , | italic_T |, and the edge bipartization domain, which is parameterized by k,d,sğ‘˜ğ‘‘ğ‘ k,d,sitalic_k , italic_d , italic_s. This result generalizes several existing results in both the broader applicability to more general domain ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D and the applicability to clustering problems. Specifically, for the matching domain, our result for the case of |V|=2â¢lğ‘‰2ğ‘™|V|=2l| italic_V | = 2 italic_l corresponds to the result in [fomin2024diversecollection] for perfect matchings, and for the minimum edge s,tğ‘ ğ‘¡s,titalic_s , italic_t-flow domain and minimum Steiner tree domain, our results for the cases of b=1ğ‘1b=1italic_b = 1 and |T|=2ğ‘‡2|T|=2| italic_T | = 2, respectively, correspond to the results in [funayama2024parameterized] for the shortest paths. Our result for the dynamic programming problems domain implies that for a wide range of problems solvable by dynamic programming, the corresponding diversification and clustering problems have FPT algorithms parameterized by kğ‘˜kitalic_k and dğ‘‘ditalic_d. Particularly, this improves the parameterization of the result in [hanaka2021finding] for the interval schedulings and the result in [shida2024finding] for the longest common subsequences from k,lğ‘˜ğ‘™k,litalic_k , italic_l to k,dğ‘˜ğ‘‘k,ditalic_k , italic_d. Furthermore, the technique for the branching domain leads to another FPT algorithm for the dğ‘‘ditalic_d-distinct branchings problem [bang2021k, bang2016parameterized, gutin2018k], which asks to find a pair consisting of a branching and an in-branching such that the Hamming distance between them is at least dğ‘‘ditalic_d. We also generalize this algorithm to the version of choosing multiple branchings and in-branchings, which is a new result in the literature of the dğ‘‘ditalic_d-distinct branchings problem. The rest of our results are entirely new, even for the max-min diversification problem. 1.3 Framework Overview In this section, we provide an overview of the entire flow of our frameworks. 1.3.1 From dğ‘‘ditalic_d-Limited kğ‘˜kitalic_k-Max-Distance Sparsifier to Diversification and Clustering The key idea for Theorem 1.1 is to design the following kğ‘˜kitalic_k-max-distance sparsifier of ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D. Definition 1.3 (kğ‘˜kitalic_k-max-distance sparsifier). Let kâˆˆâ„¤â‰¥1ğ‘˜subscriptâ„¤absent1k\in\mathbb{Z}_{\geq 1}italic_k âˆˆ blackboard_Z start_POSTSUBSCRIPT â‰¥ 1 end_POSTSUBSCRIPT. Let Uğ‘ˆUitalic_U be a finite set and ğ’Ÿ,â„±âŠ†2Uğ’Ÿâ„±superscript2ğ‘ˆ\mathcal{D},\mathcal{F}\subseteq 2^{U}caligraphic_D , caligraphic_F âŠ† 2 start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT. We say that ğ’¦âŠ†ğ’Ÿğ’¦ğ’Ÿ\mathcal{K}\subseteq\mathcal{D}caligraphic_K âŠ† caligraphic_D is a kğ‘˜kitalic_k-max-distance sparsifier of ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D with respect to â„±â„±\mathcal{F}caligraphic_F if for any (F1,â€¦,Fk)âˆˆâ„±ksubscriptğ¹1â€¦subscriptğ¹ğ‘˜superscriptâ„±ğ‘˜(F_{1},\dots,F_{k})\in\mathcal{F}^{k}( italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_F start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) âˆˆ caligraphic_F start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT and (z1,â€¦,zk)âˆˆâ„¤â‰¥0ksubscriptğ‘§1â€¦subscriptğ‘§ğ‘˜superscriptsubscriptâ„¤absent0ğ‘˜(z_{1},\dots,z_{k})\in\mathbb{Z}_{\geq 0}^{k}( italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) âˆˆ blackboard_Z start_POSTSUBSCRIPT â‰¥ 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT, the two conditions â€¢ There exists Dâˆˆğ’Ÿğ·ğ’ŸD\in\mathcal{D}italic_D âˆˆ caligraphic_D such that for each iâˆˆ{1,â€¦,k}ğ‘–1â€¦ğ‘˜i\in\{1,\dots,k\}italic_i âˆˆ { 1 , â€¦ , italic_k }, |Fiâ¢â–³â¢D|â‰¥zisubscriptğ¹ğ‘–â–³ğ·subscriptğ‘§ğ‘–|F_{i}\triangle D|\geq z_{i}| italic_F start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT â–³ italic_D | â‰¥ italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. â€¢ There exists Kâˆˆğ’¦ğ¾ğ’¦K\in\mathcal{K}italic_K âˆˆ caligraphic_K such that for each iâˆˆ{1,â€¦,k}ğ‘–1â€¦ğ‘˜i\in\{1,\dots,k\}italic_i âˆˆ { 1 , â€¦ , italic_k }, |Fiâ¢â–³â¢K|â‰¥zisubscriptğ¹ğ‘–â–³ğ¾subscriptğ‘§ğ‘–|F_{i}\triangle K|\geq z_{i}| italic_F start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT â–³ italic_K | â‰¥ italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. are equivalent. Unless specifically noted, when we write kğ‘˜kitalic_k-max-distance sparsifier of ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D, we mean the case where ğ’Ÿ=â„±ğ’Ÿâ„±\mathcal{D}=\mathcal{F}caligraphic_D = caligraphic_F. Similarly, the key idea for Theorem 1.2 is to design the following dğ‘‘ditalic_d-limited kğ‘˜kitalic_k-max-distance sparsifier of ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D. Definition 1.4 (dğ‘‘ditalic_d-limited kğ‘˜kitalic_k-max-distance sparsifier). Let kâˆˆâ„¤â‰¥1ğ‘˜subscriptâ„¤absent1k\in\mathbb{Z}_{\geq 1}italic_k âˆˆ blackboard_Z start_POSTSUBSCRIPT â‰¥ 1 end_POSTSUBSCRIPT and dâˆˆâ„¤â‰¥0ğ‘‘subscriptâ„¤absent0d\in\mathbb{Z}_{\geq 0}italic_d âˆˆ blackboard_Z start_POSTSUBSCRIPT â‰¥ 0 end_POSTSUBSCRIPT. Let Uğ‘ˆUitalic_U be a finite set and ğ’Ÿ,â„±âŠ†2Uğ’Ÿâ„±superscript2ğ‘ˆ\mathcal{D},\mathcal{F}\subseteq 2^{U}caligraphic_D , caligraphic_F âŠ† 2 start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT. We say that ğ’¦âŠ†ğ’Ÿğ’¦ğ’Ÿ\mathcal{K}\subseteq\mathcal{D}caligraphic_K âŠ† caligraphic_D is a dğ‘‘ditalic_d-limited kğ‘˜kitalic_k-max-distance sparsifier of ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D with respect to â„±â„±\mathcal{F}caligraphic_F if for any (F1,â€¦,Fk)âˆˆâ„±ksubscriptğ¹1â€¦subscriptğ¹ğ‘˜superscriptâ„±ğ‘˜(F_{1},\dots,F_{k})\in\mathcal{F}^{k}( italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_F start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) âˆˆ caligraphic_F start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT and (z1,â€¦,zk)âˆˆ{0,â€¦,d}ksubscriptğ‘§1â€¦subscriptğ‘§ğ‘˜superscript0â€¦ğ‘‘ğ‘˜(z_{1},\dots,z_{k})\in\{0,\dots,d\}^{k}( italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) âˆˆ { 0 , â€¦ , italic_d } start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT, the two conditions â€¢ There exists Dâˆˆğ’Ÿğ·ğ’ŸD\in\mathcal{D}italic_D âˆˆ caligraphic_D such that for each iâˆˆ{1,â€¦,k}ğ‘–1â€¦ğ‘˜i\in\{1,\dots,k\}italic_i âˆˆ { 1 , â€¦ , italic_k }, |Fiâ¢â–³â¢D|â‰¥zisubscriptğ¹ğ‘–â–³ğ·subscriptğ‘§ğ‘–|F_{i}\triangle D|\geq z_{i}| italic_F start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT â–³ italic_D | â‰¥ italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. â€¢ There exists Kâˆˆğ’¦ğ¾ğ’¦K\in\mathcal{K}italic_K âˆˆ caligraphic_K such that for each iâˆˆ{1,â€¦,k}ğ‘–1â€¦ğ‘˜i\in\{1,\dots,k\}italic_i âˆˆ { 1 , â€¦ , italic_k }, |Fiâ¢â–³â¢K|â‰¥zisubscriptğ¹ğ‘–â–³ğ¾subscriptğ‘§ğ‘–|F_{i}\triangle K|\geq z_{i}| italic_F start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT â–³ italic_K | â‰¥ italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. are equivalent. Unless specifically noted, when we write dğ‘‘ditalic_d-limited kğ‘˜kitalic_k-max-distance sparsifier of ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D, we mean the case where ğ’Ÿ=â„±ğ’Ÿâ„±\mathcal{D}=\mathcal{F}caligraphic_D = caligraphic_F. The difference between the two sparsifiers is that the domain of (z1,â€¦,zk)subscriptğ‘§1â€¦subscriptğ‘§ğ‘˜(z_{1},\dots,z_{k})( italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) is â„¤â‰¥0ksuperscriptsubscriptâ„¤absent0ğ‘˜\mathbb{Z}_{\geq 0}^{k}blackboard_Z start_POSTSUBSCRIPT â‰¥ 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT in the former case, while it is {0,â€¦,d}ksuperscript0â€¦ğ‘‘ğ‘˜\{0,\dots,d\}^{k}{ 0 , â€¦ , italic_d } start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT in the latter. By definition, any kğ‘˜kitalic_k-max-distance sparsifier is also a dğ‘‘ditalic_d-limited kğ‘˜kitalic_k-max-distance sparsifier for any dâˆˆâ„¤â‰¥0ğ‘‘subscriptâ„¤absent0d\in\mathbb{Z}_{\geq 0}italic_d âˆˆ blackboard_Z start_POSTSUBSCRIPT â‰¥ 0 end_POSTSUBSCRIPT. We can prove that given a dğ‘‘ditalic_d-limited (kâˆ’1)ğ‘˜1(k-1)( italic_k - 1 )-max-distance sparsifier of ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D with size bounded by a constant that depends only on kğ‘˜kitalic_k and dğ‘‘ditalic_d, we can construct FPT algorithms parameterized by kğ‘˜kitalic_k and dğ‘‘ditalic_d for the max-min/max-sum diversification problems on ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D. Similarly, we can prove that given a (d+1)ğ‘‘1(d+1)( italic_d + 1 )-limited kğ‘˜kitalic_k-max-distance sparsifier of ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D with size bounded by a constant that depends only on kğ‘˜kitalic_k and dğ‘‘ditalic_d, we can construct FPT algorithms parameterized by kğ‘˜kitalic_k and dğ‘‘ditalic_d for the kğ‘˜kitalic_k-center/kğ‘˜kitalic_k-sum-of-radii clustering problems on ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D. Therefore, to prove Theorems 1.1 and 1.2, it suffices to construct FPT oracle algorithms for designing kğ‘˜kitalic_k-max-distance sparsifiers and dğ‘‘ditalic_d-limited kğ‘˜kitalic_k-max-distance sparsifiers, respectively. 1.3.2 Computing kğ‘˜kitalic_k-Max-Distance Sparsifier The remaining task towards Theorem 1.1 is to provide an FPT algorithm parameterized by k,lğ‘˜ğ‘™k,litalic_k , italic_l that constructs a kğ‘˜kitalic_k-max-distance sparsifier of ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D with size bounded by a constant that depends only on kğ‘˜kitalic_k and lğ‘™litalic_l. Our algorithm starts with ğ’¦=âˆ…ğ’¦\mathcal{K}=\emptysetcaligraphic_K = âˆ… and exhaustively adds sets of ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D to ğ’¦ğ’¦\mathcal{K}caligraphic_K until ğ’¦ğ’¦\mathcal{K}caligraphic_K becomes a kğ‘˜kitalic_k-max-distance sparsifier. We can prove that if adding a set to ğ’¦ğ’¦\mathcal{K}caligraphic_K would cause a sufficiently large sunflower (see Section 3 for the definition) consisting of sets of the same size to appear in ğ’¦ğ’¦\mathcal{K}caligraphic_K, then we do not need to add that set to ğ’¦ğ’¦\mathcal{K}caligraphic_K (in fact, for the sake of simplifying the framework, we prove a slightly stronger statement). By using the well-known sunflower lemma (Lemma 3.1), we can bound the number of repetitions by a constant. To choose a set to be added, we design an FPT algorithm using calls of the exact empty extension oracle. 1.3.3 Computing dğ‘‘ditalic_d-Limited kğ‘˜kitalic_k-Max-Distance Sparsifier The algorithm in the previous section alone is insufficient to prove Theorem 1.2 since lğ‘™litalic_l is unbounded and the bound for the number of repetitions cannot be used. Our algorithm divides ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D into at most kğ‘˜kitalic_k clusters, computes a dğ‘‘ditalic_d-limited kğ‘˜kitalic_k-max-distance sparsifier for each cluster, and outputs their union. Let p>2â¢dğ‘2ğ‘‘p>2ditalic_p > 2 italic_d be a constant that depends only on kğ‘˜kitalic_k and dğ‘‘ditalic_d. We first find ğ’âŠ†ğ’Ÿğ’ğ’Ÿ\mathcal{C}\subseteq\mathcal{D}caligraphic_C âŠ† caligraphic_D satisfying the following properties. â€¢ |ğ’|â‰¤kğ’ğ‘˜|\mathcal{C}|\leq k| caligraphic_C | â‰¤ italic_k. â€¢ For all distinct C,Câ€²âˆˆğ’ğ¶superscriptğ¶â€²ğ’C,C^{\prime}\in\mathcal{C}italic_C , italic_C start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT âˆˆ caligraphic_C, |Câ¢â–³â¢Câ€²|>2â¢dğ¶â–³superscriptğ¶â€²2ğ‘‘|C\triangle C^{\prime}|>2d| italic_C â–³ italic_C start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT | > 2 italic_d. â€¢ For all Dâˆˆğ’Ÿğ·ğ’ŸD\in\mathcal{D}italic_D âˆˆ caligraphic_D, there exists Câˆˆğ’ğ¶ğ’C\in\mathcal{C}italic_C âˆˆ caligraphic_C such that |Dâ¢â–³â¢C|â‰¤pğ·â–³ğ¶ğ‘|D\triangle C|\leq p| italic_D â–³ italic_C | â‰¤ italic_p. If such a family does not exist, a trivial dğ‘‘ditalic_d-limited kğ‘˜kitalic_k-max-distance sparsifier of ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D will be found, and we output it and terminate. We provide an algorithm for computing ğ’ğ’\mathcal{C}caligraphic_C. Our algorithm starts with ğ’=âˆ…ğ’\mathcal{C}=\emptysetcaligraphic_C = âˆ… and exhaustively adds sets in ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D to ğ’ğ’\mathcal{C}caligraphic_C until ğ’ğ’\mathcal{C}caligraphic_C satisfies the above conditions or its size exceeds kğ‘˜kitalic_k. To choose the elements to be added, we randomly sample wâˆˆ{âˆ’1,1}Uğ‘¤superscript11ğ‘ˆw\in\{-1,1\}^{U}italic_w âˆˆ { - 1 , 1 } start_POSTSUPERSCRIPT italic_U end_POSTSUPERSCRIPT and call a (âˆ’1,1)11(-1,1)( - 1 , 1 )-optimization oracle. We can prove for sufficiently large constant pğ‘pitalic_p that if there exists Dâˆˆğ’Ÿğ·ğ’ŸD\in\mathcal{D}italic_D âˆˆ caligraphic_D such that |Dâ¢â–³â¢C|>pğ·â–³ğ¶ğ‘|D\triangle C|>p| italic_D â–³ italic_C | > italic_p for any Câˆˆğ’ğ¶ğ’C\in\mathcal{C}italic_C âˆˆ caligraphic_C, with a constant probability, the (âˆ’1,1)11(-1,1)( - 1 , 1 )-optimization oracle will find a Dâˆˆğ’Ÿğ·ğ’ŸD\in\mathcal{D}italic_D âˆˆ caligraphic_D such that |Dâ¢â–³â¢C|>2â¢dğ·â–³ğ¶2ğ‘‘|D\triangle C|>2d| italic_D â–³ italic_C | > 2 italic_d for any Câˆˆğ’ğ¶ğ’C\in\mathcal{C}italic_C âˆˆ caligraphic_C. Thus, if ğ’ğ’\mathcal{C}caligraphic_C does not meet the conditions, by calling the (âˆ’1,1)11(-1,1)( - 1 , 1 )-optimization oracle a sufficient number of times, we can find a set to add to ğ’ğ’\mathcal{C}caligraphic_C with high probability. Here, we provide an algorithm for computing a dğ‘‘ditalic_d-limited kğ‘˜kitalic_k-max-distance sparsifier of ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D using ğ’ğ’\mathcal{C}caligraphic_C. For each cluster ğ’ŸC:={Dâˆˆğ’Ÿ:|Dâ¢â–³â¢C|â‰¤p}assignsubscriptğ’Ÿğ¶conditional-setğ·ğ’Ÿğ·â–³ğ¶ğ‘\mathcal{D}_{C}:=\{D\in\mathcal{D}\colon|D\triangle C|\leq p\}caligraphic_D start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT := { italic_D âˆˆ caligraphic_D : | italic_D â–³ italic_C | â‰¤ italic_p }, let ğ’ŸCâˆ—:={Dâ¢â–³â¢C:Dâˆˆğ’ŸC}assignsubscriptsuperscriptğ’Ÿğ¶conditional-setğ·â–³ğ¶ğ·subscriptğ’Ÿğ¶\mathcal{D}^{*}_{C}:=\{D\triangle C\colon D\in\mathcal{D}_{C}\}caligraphic_D start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT := { italic_D â–³ italic_C : italic_D âˆˆ caligraphic_D start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT }. The algorithm computes a kğ‘˜kitalic_k-max-distance sparsifier of each ğ’ŸCâˆ—subscriptsuperscriptğ’Ÿğ¶\mathcal{D}^{*}_{C}caligraphic_D start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT and outputs their union. For technical reasons, we actually compute a slightly more general object, but we will not delve into the details here. Since each ğ’ŸCâˆ—subscriptsuperscriptğ’Ÿğ¶\mathcal{D}^{*}_{C}caligraphic_D start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT consists only of sets whose size is at most pğ‘pitalic_p, the kğ‘˜kitalic_k-max-distance sparsifier of ğ’ŸCâˆ—subscriptsuperscriptğ’Ÿğ¶\mathcal{D}^{*}_{C}caligraphic_D start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT can be constructed using the algorithm in Section 1.3.2. The exact empty extension oracle on ğ’ŸCâˆ—subscriptsuperscriptğ’Ÿğ¶\mathcal{D}^{*}_{C}caligraphic_D start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT corresponds to the exact extension oracle on ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D. Here, we note the difference between our framework and that used by [fomin2024diversecollection] and [funayama2024parameterized] to provide FPT algorithms for the max-min diversification problem on ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D when ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D is the family of perfect matchings and shortest paths, respectively. Their algorithms also start by dividing ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D into clusters. However, their algorithms perform stricter clustering than ours. Specifically, in their clustering, clusters ğ’ŸCsubscriptğ’Ÿğ¶\mathcal{D}_{C}caligraphic_D start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT corresponding to different Câˆˆğ’ğ¶ğ’C\in\mathcal{C}italic_C âˆˆ caligraphic_C must be well-separated. In contrast, we allow clusters to overlap. This simplifies the clustering step compared to their approach at the cost of a more challenging task afterward. We resolve this more challenging task by introducing and designing the dğ‘‘ditalic_d-limited kğ‘˜kitalic_k-max-distance sparsifier. 1.4 Further Related Work 1.4.1 Max-Sum Diversification Problems The max-sum diversification problem is often more tractable compared to the max-min diversification problem, and polynomial-time algorithms are known for multiple domains ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D. [hanaka2021finding] provided a polynomial-time algorithm for the case where ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D is the base family of a matroid. [hanaka2022computing] provided polynomial-time algorithms for the cases where ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D is the family of shortest paths, branchings, and bipartite matchings. [BergMS23] provided a polynomial-time algorithm for the case where ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D is the family of edge sets of minimum s,tğ‘ ğ‘¡s,titalic_s , italic_t-cuts. There has also been active research on approximation algorithms for the max-sum diversification problem. [hanaka2023framework] proposed a generic framework that provides local search-based approximation algorithms for max-sum diversification problems. [gao2022obtaining] provided a framework for bicriteria approximation algorithms for the case where ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D is a family of (not necessarily optimal) solutions of an optimization problem. [do2023diverse] discussed the tradeoff between solution quality and diversity for submodular maximization on matroids. 1.4.2 Further Parameterized Algorithms for Max-Min Diversification Problems There are several other research directions on parameterized algorithms for max-min diversification problems. [eiben2024determinantal] introduced a technique called determinantal sieving and improved the time complexity of the algorithms by [fomin2024diversecollection] for the cases where ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D is the base of a matroid, a matching, and the common independent set of two matroids. [drabik2024finding] provided an FPT algorithm for diversification problems on domains expressible by MSO1subscriptMSO1\mathrm{MSO}_{1}roman_MSO start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT formulas on graphs with bounded cliquewidth, parameterized by kğ‘˜kitalic_k, cliquewidth, and the length of the MSO1subscriptMSO1\mathrm{MSO}_{1}roman_MSO start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT formula. [arrighi2023synchronization] proposed an FPT algorithm for the max-min diversification problem on the set of synchronizing words for a deterministic finite automaton, where the distance between two words is measured by the edit distance. 1.4.3 Parameterized Algorithms for Clustering Problems For the case where the point set is explicitly given, several researches have been conducted on parameterized algorithms for clustering problems. It is easy to see that when parameterized solely by kğ‘˜kitalic_k and the metric is general, the kğ‘˜kitalic_k-center clustering problem includes the dominating set problem, which is known to be W[2]-hard. Therefore, the study of FPT approximation algorithms is very active recently [bandyapadhyayL023a, chen2024parameterized, eiben2023parameterized, feldmann2020parameterized, inamdar2020capacitated]. Research on providing exact parameterized algorithms on restricted metrics is also being conducted [demaine2005fixed]. Probably, the research most closely related to our situation is the extension of the closest string problem by [amir2014efficiency]. They defined the kğ‘˜kitalic_k-center and kğ‘˜kitalic_k-sum-of-radii clustering problems for sets of strings and investigated their parameterized complexity. When the alphabet is binary, they consider the same problem as ours. However, their results do not directly imply our results since, in our setting, the points are implicitly given as the solution domain of combinatorial problems, which are generally of exponential size. 1.5 Organization The rest of this paper is organized as follows. In Section 2, we provide FPT algorithms for solving diversification and clustering problems on ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D using a dğ‘‘ditalic_d-limited kğ‘˜kitalic_k-max-distance sparsifier of ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D of constant size. In Section 3, we complete the proof of Theorem 1.1 by providing an FPT oracle algorithm parameterized by k,lğ‘˜ğ‘™k,litalic_k , italic_l that computes a kğ‘˜kitalic_k-max-distance sparsifier of ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D with size bounded by a constant that depends only on k,lğ‘˜ğ‘™k,litalic_k , italic_l. In Section 4, we complete the proof of Theorem 1.2 by providing an FPT oracle algorithm parameterized by k,dğ‘˜ğ‘‘k,ditalic_k , italic_d that computes a dğ‘‘ditalic_d-limited kğ‘˜kitalic_k-max-distance sparsifier of ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D with size bounded by a constant that depends only on k,dğ‘˜ğ‘‘k,ditalic_k , italic_d. The discussion in Section 4 internally uses the results from Section 3. Finally, in Section 5, we apply the results of Theorems 1.1 and 1.2 to several domains ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D to obtain FPT algorithms for diversification and clustering problems."
https://arxiv.org/html/2411.02821v1,Faster Exact and Parameterized Algorithm for Feedback Vertex Set in Bipartite Tournaments,"A bipartite tournament is a directed graph T:=(AâˆªB,E)assignğ‘‡ğ´ğµğ¸T:=(A\cup B,E)italic_T := ( italic_A âˆª italic_B , italic_E ) such that every pair of vertices (a,b),aâˆˆA,bâˆˆBformulae-sequenceğ‘ğ‘ğ‘ğ´ğ‘ğµ(a,b),a\in A,b\in B( italic_a , italic_b ) , italic_a âˆˆ italic_A , italic_b âˆˆ italic_B are connected by an arc, and no arc connects two vertices of Ağ´Aitalic_A or two vertices of BğµBitalic_B. A feedback vertex set is a set Sğ‘†Sitalic_S of vertices in Tğ‘‡Titalic_T such that Tâˆ’Sğ‘‡ğ‘†T-Sitalic_T - italic_S is acyclic. In this article we consider the Feedback Vertex Set problem in bipartite tournaments. Here the input is a bipartite tournament Tğ‘‡Titalic_T on nğ‘›nitalic_n vertices together with an integer kğ‘˜kitalic_k, and the task is to determine whether Tğ‘‡Titalic_T has a feedback vertex set of size at most kğ‘˜kitalic_k. We give a new algorithm for Feedback Vertex Set in Bipartite Tournaments. The running time of our algorithm is upper-bounded by Oâ¢(1.6181k+nOâ¢(1))ğ‘‚superscript1.6181ğ‘˜superscriptğ‘›ğ‘‚1O(1.6181^{k}+n^{O(1)})italic_O ( 1.6181 start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT + italic_n start_POSTSUPERSCRIPT italic_O ( 1 ) end_POSTSUPERSCRIPT ), improving over the previously best known algorithm with running time 2kâ¢kOâ¢(1)+nOâ¢(1)superscript2ğ‘˜superscriptğ‘˜ğ‘‚1superscriptğ‘›ğ‘‚12^{k}k^{O(1)}+n^{O(1)}2 start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_k start_POSTSUPERSCRIPT italic_O ( 1 ) end_POSTSUPERSCRIPT + italic_n start_POSTSUPERSCRIPT italic_O ( 1 ) end_POSTSUPERSCRIPT [Hsiao, ISAAC 2011]. As a by-product, we also obtain the fastest currently known exact exponential-time algorithm for the problem, with running time Oâ¢(1.3820n)ğ‘‚superscript1.3820ğ‘›O(1.3820^{n})italic_O ( 1.3820 start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ).","A feedback vertex set in a graph GğºGitalic_G is a vertex set whose removal makes the graph acyclic. The Feedback Vertex Set problem is a well-studied graph problem where input is a graph GğºGitalic_G (directed or undirected) and the task is to find a smallest possible feedback vertex set. Finding such an optimal feedback vertex set turns out to be NP-complete [22], indeed the problem is one of the very first to be shown NP-complete in the influential paper of Karp [26]. Since, polynomial time algorithms are highly unlikely, Feedback Vertex Set on general directed and undirected graphs has been extensively studied from the perspective of approximation algorithms [2, 15], parameterized algorithms [6, 10, 27], exact exponential-time algorithms [29, 35] as well as graph theory [14, 30]. This paper belongs to a long line of work studying the complexity of Feedback Vertex Set on restricted classes of graphs. On one hand Feedback Vertex Set remains NP-complete on tournaments and bipartite tournaments [5], planar undirected graphs [22], planar directed graphs with in-degree and out-degree at most 3333 [22] as well as directed graphs with in-degree and out-degree at most 2222 [22]. On the other hand the problem is polynomial time solvable on undirected graphs of maximum degree 3333 [33], chordal graphs [16] and weakly chordal graphs [20], indeed on any class of graphs with polynomially many potential maximal cliques [20]. Being a problem of fundamental importance, Feedback Vertex Set has been approached algorithmically even on the classes of graphs where it remains NP-complete. For example the problem admits (efficient) polynomial time approximation schemes [8, 12, 18], sub-exponential time parameterized algorithms [11] and linear kernels [19] on classes of graphs excluding a fixed graph Hğ»Hitalic_H as a minor. In this paper we study the problem on bipartite tournaments. A tournament is a subclass of directed graphs where every pair of vertices are connected by an arc. A bipartite tournament is a directed graph where the vertices are partitioned into two sets Ağ´Aitalic_A and BğµBitalic_B, there is an arc connecting every vertex in Ağ´Aitalic_A with every vertex in BğµBitalic_B, and there are no edges between vertices of Ağ´Aitalic_A and vertices of BğµBitalic_B. Tournaments arise naturally from round-robin competitions whereas bipartite tournaments model a two-team competition in which every player in one team plays against every player of the other team. Here arcs are drawn from the winning to the losing player, and often one seeks to rank the players from â€œbestâ€ to â€œworstâ€ such that players that appear higher in the ranking beat all lower ranked players they played against. Such an absolute ranking possible only if there are no cycles in the tournament. The size of the smallest feedback vertex set then becomes a measure of how far the tournament is from admitting a consistent ranking. For this reason the structure of cycles and feedback vertex sets in (bipartite) tournaments has been studied both from the perspective of graph theory [3, 7, 21] and algorithms. For bipartite tournaments, finding a feedback vertex set reduces to hitting all cycles of length 4444. For this reason the Feedback Vertex Set problem is more computationally tractable on bipartite tournaments than on general directed graphs. Specifically the best known approximation algorithm for Feedback Vertex Set on directed graphs has an approximation factor of Oâ¢(logâ¡nâ‹…logâ¡logâ¡n)ğ‘‚â‹…ğ‘›ğ‘›O(\log n\cdot\log\log n)italic_O ( roman_log italic_n â‹… roman_log roman_log italic_n ) [15], and the problem does not admit a constant factor approximation assuming the Unique Games Conjecture [23]. On bipartite tournaments it is easy to obtain a 4444-approximation (see Lemma 2.2). Further, an improved approximation algorithm with ratio 2222 was obtained by Zuylen. [34]. Similarly, it was open for a long time whether Feedback Vertex Set on general directed graphs admits an FPT algorithm, that is an algorithm that determines whether there exists a solution of size at most kğ‘˜kitalic_k in time fâ¢(k)â¢nOâ¢(1)ğ‘“ğ‘˜superscriptğ‘›ğ‘‚1f(k)n^{O(1)}italic_f ( italic_k ) italic_n start_POSTSUPERSCRIPT italic_O ( 1 ) end_POSTSUPERSCRIPT. In 2008, Chen et al. [6] gave an algorithm with running time Oâ¢(4kâ¢kOâ¢(1)â¢k!â¢nâ¢m)ğ‘‚superscript4ğ‘˜superscriptğ‘˜ğ‘‚1ğ‘˜ğ‘›ğ‘šO(4^{k}k^{O(1)}k!nm)italic_O ( 4 start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_k start_POSTSUPERSCRIPT italic_O ( 1 ) end_POSTSUPERSCRIPT italic_k ! italic_n italic_m ), and it is an outstanding open problem whether there exists an algorithm with running time 2Oâ¢(k)â¢nOâ¢(1)superscript2ğ‘‚ğ‘˜superscriptğ‘›ğ‘‚12^{O(k)}n^{O(1)}2 start_POSTSUPERSCRIPT italic_O ( italic_k ) end_POSTSUPERSCRIPT italic_n start_POSTSUPERSCRIPT italic_O ( 1 ) end_POSTSUPERSCRIPT. For bipartite tournaments, the realization that it is necessary and sufficient to hit all cycles of length 4444 yields a simple 4kâ¢nOâ¢(1)superscript4ğ‘˜superscriptğ‘›ğ‘‚14^{k}n^{O(1)}4 start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_n start_POSTSUPERSCRIPT italic_O ( 1 ) end_POSTSUPERSCRIPT time parameterized algorithm: recursively branch on vertices of a cycle of length 4444. TruÃŸ [32] gave an improved algorithm with running time 3.12kâ¢nOâ¢(1)superscript3.12ğ‘˜superscriptğ‘›ğ‘‚13.12^{k}n^{O(1)}3.12 start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_n start_POSTSUPERSCRIPT italic_O ( 1 ) end_POSTSUPERSCRIPT, Sasatte [31] further improved the running time to 3kâ¢nOâ¢(1)superscript3ğ‘˜superscriptğ‘›ğ‘‚13^{k}n^{O(1)}3 start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_n start_POSTSUPERSCRIPT italic_O ( 1 ) end_POSTSUPERSCRIPT, while Hsiao [25] gave an algorithm with running time 2kâ¢nOâ¢(1)superscript2ğ‘˜superscriptğ‘›ğ‘‚12^{k}n^{O(1)}2 start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_n start_POSTSUPERSCRIPT italic_O ( 1 ) end_POSTSUPERSCRIPT. Prior to this work, this was the fastest known parameterized algorithm for Feedback Vertex Set on bipartite tournaments. Our main result is an algorithm with running time Oâ¢(1.6181k+nOâ¢(1))ğ‘‚superscript1.6181ğ‘˜superscriptğ‘›ğ‘‚1O(1.6181^{k}+n^{O(1)})italic_O ( 1.6181 start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT + italic_n start_POSTSUPERSCRIPT italic_O ( 1 ) end_POSTSUPERSCRIPT ). Using the recent black-box reduction from parameterized to exact exponential time algorithms of Fomin et al. [17] we also obtain an exponential-time algorithm running in Oâ¢(1.3820n)ğ‘‚superscript1.3820ğ‘›O(1.3820^{n})italic_O ( 1.3820 start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ) time. Methods. Our algorithm is based on the recent parameterized algorithm with running time Oâ¢(1.6181k+nOâ¢(1))ğ‘‚superscript1.6181ğ‘˜superscriptğ‘›ğ‘‚1O(1.6181^{k}+n^{O(1)})italic_O ( 1.6181 start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT + italic_n start_POSTSUPERSCRIPT italic_O ( 1 ) end_POSTSUPERSCRIPT ) by the authors [28] for Feedback Vertex Set in tournaments. The main idea of this algorithm is that tournaments are very rigid. Given as input a tournament Tğ‘‡Titalic_T, by obtaining a large set Mğ‘€Mitalic_M of vertices that is disjoint from the feedback vertex set Hğ»Hitalic_H sought for, we can get a rough sketch of the rigid structure of Tâˆ’Hğ‘‡ğ»T-Hitalic_T - italic_H. This structure is then very useful for recovering the solution Hğ»Hitalic_H. Indeed, the only way that vertices that are â€œfar apartâ€ in the approximate sketch of the structure of Tâˆ’Hğ‘‡ğ»T-Hitalic_T - italic_H can interact with each other is by being â€œin conflictâ€. Out of two vertices that are in conflict, one of them has to be deleted. Thus, dealing with conflicts can be done in a similar fashion as with edges in the Vertex Cover problem. For any vertex vğ‘£vitalic_v appearing in at least two conflicts, branch into two sub-problems. In the first sub-problem vğ‘£vitalic_v is deleted, in the second all vertices in conflict with vğ‘£vitalic_v are deleted. If there are no conflicts it is sufficient to solve the Feedback Vertex Set problem â€œlocallyâ€. If every vertex appears in at most one conflict a divide and conquer approach can be taken. Because bipartite tournaments are also quite â€œrigidâ€, we expected that the same approach would easily give an algorithm for Feedback Vertex Set on bipartite tournaments with the same running time. Our expectations were both wrong and correct; indeed we do obtain an algorithm for Feedback Vertex Set on bipartite tournaments with the same template and the same running time as the algorithm for tournaments [28], yet the adaptation turned out to be anything but easy. Specifically, in virtually every step of the algorithm, the lack of a unique topological sort of acyclic bipartite tournaments presented significant challenges. The fact that these challenges still could be overcome by sub-exponential time cleaning procedures gives hope that the same template could be applicable in several situations where one seeks a â€œsmallâ€ set of vertices or edges to delete in order to modify the input graph to a â€œrigidâ€ structure; such as Cluster Vertex Deletion, Cograph Vertex Deletion and Feedback Vertex Set in the more general setting when the input graph is a multi-partite tournament [24]. Organization of the paper. In Section 2 we set up definitions and notation, and state a few useful preliminary results. The standard graph notation and parameterized complexity terminology is set up in the appendix. In Section 3 we define and prove some properties of Mğ‘€Mitalic_M-sequence. In Section 4 we define and give an algorithm for Constrained Feedback Vertex Set problem."
https://arxiv.org/html/2411.02764v1,"Fast, robust approximate message passing","We give a fast, spectral procedure for implementing approximate-message passing (AMP) algorithms robustly. For any quadratic optimization problem over symmetric matrices Xğ‘‹Xitalic_X with independent subgaussian entries, and any separable AMP algorithm ğ’œğ’œ\mathcal{A}caligraphic_A, our algorithm performs a spectral pre-processing step and then mildly modifies the iterates of ğ’œğ’œ\mathcal{A}caligraphic_A. If given the perturbed input X+Eâˆˆâ„nÃ—nğ‘‹ğ¸superscriptâ„ğ‘›ğ‘›X+E\in\mathbb{R}^{n\times n}italic_X + italic_E âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n Ã— italic_n end_POSTSUPERSCRIPT for any Eğ¸Eitalic_E supported on a Îµâ¢nÃ—Îµâ¢nğœ€ğ‘›ğœ€ğ‘›\varepsilon n\times\varepsilon nitalic_Îµ italic_n Ã— italic_Îµ italic_n principal minor, our algorithm outputs a solution v^^ğ‘£\hat{v}over^ start_ARG italic_v end_ARG which is guaranteed to be close to the output of ğ’œğ’œ\mathcal{A}caligraphic_A on the uncorrupted Xğ‘‹Xitalic_X, with â€–ğ’œâ¢(X)âˆ’v^â€–2â©½fâ¢(Îµ)â¢â€–ğ’œâ¢(X)â€–2subscriptnormğ’œğ‘‹^ğ‘£2ğ‘“ğœ€subscriptnormğ’œğ‘‹2\|\mathcal{A}(X)-\hat{v}\|_{2}\leqslant f(\varepsilon)\|\mathcal{A}(X)\|_{2}âˆ¥ caligraphic_A ( italic_X ) - over^ start_ARG italic_v end_ARG âˆ¥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT â©½ italic_f ( italic_Îµ ) âˆ¥ caligraphic_A ( italic_X ) âˆ¥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT where fâ¢(Îµ)â†’0â†’ğ‘“ğœ€0f(\varepsilon)\to 0italic_f ( italic_Îµ ) â†’ 0 as Îµâ†’0â†’ğœ€0\varepsilon\to 0italic_Îµ â†’ 0 depending only on Îµğœ€\varepsilonitalic_Îµ.","Approximate Message Passing (AMP) is a family of algorithmic methods which generalize matrix power iteration. Suppose we are given a symmetric matrix Xâˆˆâ„nÃ—nğ‘‹superscriptâ„ğ‘›ğ‘›X\in\mathbb{R}^{n\times n}italic_X âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n Ã— italic_n end_POSTSUPERSCRIPT, and our goal is to maximize the quadratic form vâŠ¤â¢Xâ¢vsuperscriptğ‘£topğ‘‹ğ‘£v^{\top}Xvitalic_v start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT italic_X italic_v over vectors vğ‘£vitalic_v in some constraint set Kğ¾Kitalic_K. The basic AMP algorithm starts from some initialization x(0)âˆˆâ„nsuperscriptğ‘¥0superscriptâ„ğ‘›x^{(0)}\in\mathbb{R}^{n}italic_x start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT and computes iterates x(1),x(2),â€¦superscriptğ‘¥1superscriptğ‘¥2â€¦x^{(1)},x^{(2)},\ldotsitalic_x start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT , italic_x start_POSTSUPERSCRIPT ( 2 ) end_POSTSUPERSCRIPT , â€¦ by setting x(t+1)â‰ˆXâ¢fâ¢(x(t))superscriptğ‘¥ğ‘¡1ğ‘‹ğ‘“superscriptğ‘¥ğ‘¡x^{(t+1)}\approx Xf(x^{(t)})italic_x start_POSTSUPERSCRIPT ( italic_t + 1 ) end_POSTSUPERSCRIPT â‰ˆ italic_X italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT ),111The â‰ˆ\approxâ‰ˆ relation hides a lower-order additive term, the â€œOnsager correction,â€ which depends on x(t)superscriptğ‘¥ğ‘¡x^{(t)}italic_x start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT. For the sake of simplicity we ignore this in the present discussion. where the â€œdenoiserâ€ fğ‘“fitalic_f is a function (of the algorithm designersâ€™ choosing) from â„â†’â„â†’â„â„\mathbb{R}\to\mathbb{R}blackboard_R â†’ blackboard_R applied coordinate-wise. The goal of the â€œpoweringâ€ action, Xâ¢x(t)ğ‘‹superscriptğ‘¥ğ‘¡Xx^{(t)}italic_X italic_x start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT, is to increase the quadratic form, while the denoiser fğ‘“fitalic_f is chosen to bring fâ¢(x(t))ğ‘“superscriptğ‘¥ğ‘¡f(x^{(t)})italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT ) close to the constraint set Kğ¾Kitalic_K. AMP algorithms are extremely popular in high-dimensional statistics. In this context, given a prior distribution over the matrix Xğ‘‹Xitalic_X, it is often possible to optimize the design of the denoisers fğ‘“fitalic_f in such a way that AMP gives an FPTAS, in that x(t)superscriptğ‘¥ğ‘¡x^{(t)}italic_x start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT obtains an (1âˆ’Îµ)1ğœ€(1-\varepsilon)( 1 - italic_Îµ )-optimal solution for tğ‘¡titalic_t large enough as a function of Îµğœ€\varepsilonitalic_Îµ. Introduced initially as a generalization of Belief Propagation methods from statistical physics [Bol14, DMM09, BM11], AMP algorithms are now state-of-the-art for a variety of average-case optimization problems, including compressed sensing [DMM09], sparse Principal Components Analysis (PCA) [DM14], linear regression [DMM09, BM11, KMS+12], non-negative PCA [MR15], and more (many additional examples may be found in the surveys [Mon12, FVRS22]). One especially notable recent application is the breakthrough work of Montanari for optimizing the Sherrington-Kirkpatrick Hamiltonian, an average-case version of max-cut [Mon21]. One major drawback of AMP algorithms is that they are not robust. The NP-hardness of quadratic optimization means that, obviously, one cannot hope for the optimality of AMP on average-case inputs to generalize to arbitrary inputs Xğ‘‹Xitalic_X. But even structured perturbations can throw AMP off [CZK14, RSFS19]; for example, an additive perturbation to Xğ‘‹Xitalic_X by a rank-1111 matrix of large norm, or planting a principal minor of uniform sign (as described in [IS24]). Our prior work addressing this issue [IS24] shows that for a certain class of adversarial corruptions, AMP can be simulated robustly by polynomial-sized semidefinite programming relaxations in the â€œlocal statistics hierarchy.â€ While this result is a proof of concept that a robust version of AMP is possible, it is perhaps more interesting from a complexity-theoretic perspective than an algorithmic one: the semidefinite programs are of size nexpâ¡(t)superscriptğ‘›ğ‘¡n^{\exp(t)}italic_n start_POSTSUPERSCRIPT roman_exp ( italic_t ) end_POSTSUPERSCRIPT, where tğ‘¡titalic_t is the number of AMP iterations. When AMP is an FPTAS, the algorithm of [IS24] gives a robust PTAS, but the running time is too slow to feasibly implement on any computer. In the present work, we obtain simple and fast spectral algorithms which run in time Oâ¢(n3)ğ‘‚superscriptğ‘›3O(n^{3})italic_O ( italic_n start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ), while not just matching but even improving on the robustness guarantees of [IS24]. In the â€œspectral algorithms from sum-of-squares analysesâ€ line of work (initiated in [HSSS16]), our result stands out as giving a particularly dramatic reduction in running time, as well as in yielding a significantly simpler analysis. 1.1 Setup and definitions We give some necessary definitions of AMP and the noise model that we consider. Definition 1.1 (AMP algorithm). An Approximate Message Passing algorithm is specified by a sequence of denoiser functions â„±=f0,f1,f2,â€¦â„±subscriptğ‘“0subscriptğ‘“1subscriptğ‘“2â€¦\mathcal{F}=f_{0},f_{1},f_{2},\ldotscaligraphic_F = italic_f start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦, with ft:â„t+1â†’â„:subscriptğ‘“ğ‘¡â†’superscriptâ„ğ‘¡1â„f_{t}:\mathbb{R}^{t+1}\to\mathbb{R}italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT : blackboard_R start_POSTSUPERSCRIPT italic_t + 1 end_POSTSUPERSCRIPT â†’ blackboard_R for each tâˆˆâ„•ğ‘¡â„•t\in\mathbb{N}italic_t âˆˆ blackboard_N. It takes as input a symmetric nÃ—nğ‘›ğ‘›n\times nitalic_n Ã— italic_n matrix Xğ‘‹Xitalic_X, a number of iterations Tâˆˆâ„•ğ‘‡â„•T\in\mathbb{N}italic_T âˆˆ blackboard_N, and produces a sequence of iterates x(0),x(1),â€¦,x(T)superscriptğ‘¥0superscriptğ‘¥1â€¦superscriptğ‘¥ğ‘‡x^{(0)},x^{(1)},\ldots,x^{(T)}italic_x start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT , italic_x start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT , â€¦ , italic_x start_POSTSUPERSCRIPT ( italic_T ) end_POSTSUPERSCRIPT, with x(0)=1â†’superscriptğ‘¥0â†’1x^{(0)}=\vec{1}italic_x start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT = overâ†’ start_ARG 1 end_ARG and x(t+1)=Xâ¢ftâ¢(x(t),x(tâˆ’1),â€¦,x(0))âˆ’Î”tâ¢(x(t),x(tâˆ’1),â€¦,x(0)),superscriptğ‘¥ğ‘¡1ğ‘‹subscriptğ‘“ğ‘¡superscriptğ‘¥ğ‘¡superscriptğ‘¥ğ‘¡1â€¦superscriptğ‘¥0subscriptÎ”ğ‘¡superscriptğ‘¥ğ‘¡superscriptğ‘¥ğ‘¡1â€¦superscriptğ‘¥0x^{(t+1)}=Xf_{t}(x^{(t)},x^{(t-1)},\ldots,x^{(0)})-\Delta_{t}(x^{(t)},x^{(t-1)% },\ldots,x^{(0)}),italic_x start_POSTSUPERSCRIPT ( italic_t + 1 ) end_POSTSUPERSCRIPT = italic_X italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_x start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT , italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT , â€¦ , italic_x start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) - roman_Î” start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_x start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT , italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT , â€¦ , italic_x start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) , where ftsubscriptğ‘“ğ‘¡f_{t}italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is applied coordinate-wise, and Î”tsubscriptÎ”ğ‘¡\Delta_{t}roman_Î” start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is the Onsager correction term for decreasing correlations between iterates and is fully determined by â„±â„±\mathcal{F}caligraphic_F (see Definition 2.1). AMP algorithms often also come with a rounding procedure which is applied to the final iterate, in order to ensure it satisfies the optimization constraints. We note that we are considering separable AMP algorithms (where the denoisers are applied coordinate-wise) with fixed starting point x(0)=1â†’superscriptğ‘¥0â†’1x^{(0)}=\vec{1}italic_x start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT = overâ†’ start_ARG 1 end_ARG. In full generality AMP may relax both of these criteria, but the majority of AMP analyses are compatible with these assumptions. Example 1.2 (non-negative PCA). In the non-negative principal components analysis (PCA) problem, one is given a matrix Xâˆˆâ„nÃ—nğ‘‹superscriptâ„ğ‘›ğ‘›X\in\mathbb{R}^{n\times n}italic_X âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n Ã— italic_n end_POSTSUPERSCRIPT and asked to maximize vâŠ¤â¢Xâ¢vsuperscriptğ‘£topğ‘‹ğ‘£v^{\top}Xvitalic_v start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT italic_X italic_v over non-negative unit vectors vâ©¾0ğ‘£0v\geqslant 0italic_v â©¾ 0. The AMP algorithm which starts from x(0)=1â†’superscriptğ‘¥0â†’1x^{(0)}=\vec{1}italic_x start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT = overâ†’ start_ARG 1 end_ARG and uniformly chooses the separable denoiser fsâ¢(x(s),â€¦,x(0))=fâ¢(x(s))subscriptğ‘“ğ‘ superscriptğ‘¥ğ‘ â€¦superscriptğ‘¥0ğ‘“superscriptğ‘¥ğ‘ f_{s}(x^{(s)},\ldots,x^{(0)})=f(x^{(s)})italic_f start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ( italic_x start_POSTSUPERSCRIPT ( italic_s ) end_POSTSUPERSCRIPT , â€¦ , italic_x start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) = italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_s ) end_POSTSUPERSCRIPT ), with fâ¢(x)=maxâ¡(x,0)ğ‘“ğ‘¥ğ‘¥0f(x)=\max(x,0)italic_f ( italic_x ) = roman_max ( italic_x , 0 ), is an FPTAS for non-negative PCA on Xğ‘‹Xitalic_X with i.i.d. subgaussian entries [MR15].222Technically x(t)superscriptğ‘¥ğ‘¡x^{(t)}italic_x start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT may not be a unit vector nor non-negative, but AMP algorithms such as this one usually include a final â€œroundingâ€ stepâ€”in this case, the rounding is just applying fâ¢(x)=maxâ¡(x,0)ğ‘“ğ‘¥ğ‘¥0f(x)=\max(x,0)italic_f ( italic_x ) = roman_max ( italic_x , 0 ) followed by projection to the unit ball. In this case, up to the Onsager correction, AMP coincides with projected gradient ascent with â€œinfiniteâ€ step size. We will allow adversarially-chosen perturbations in the following model. Definition 1.3 (Îµğœ€\varepsilonitalic_Îµ-principal minor corruption). Given matrices X,Yâˆˆâ„nÃ—nğ‘‹ğ‘Œsuperscriptâ„ğ‘›ğ‘›X,Y\in\mathbb{R}^{n\times n}italic_X , italic_Y âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n Ã— italic_n end_POSTSUPERSCRIPT, we say Yğ‘ŒYitalic_Y is an Îµğœ€\varepsilonitalic_Îµ-principal minor corruption of Xğ‘‹Xitalic_X if Yâˆ’Xğ‘Œğ‘‹Y-Xitalic_Y - italic_X is supported on an Îµâ¢nÃ—Îµâ¢nğœ€ğ‘›ğœ€ğ‘›\varepsilon n\times\varepsilon nitalic_Îµ italic_n Ã— italic_Îµ italic_n-principal minor. A mean-00 random variable ğ‘¿ğ‘¿\bm{X}bold_italic_X is said to be Ïƒğœ\sigmaitalic_Ïƒ-subgaussian if for each integer kâˆˆâ„•ğ‘˜â„•k\in\mathbb{N}italic_k âˆˆ blackboard_N, ğ„[|ğ‘¿|k]â©½Ïƒkâ¢kk/2ğ„superscriptğ‘¿ğ‘˜superscriptğœğ‘˜superscriptğ‘˜ğ‘˜2\operatorname*{\mathbf{E}}[|\bm{X}|^{k}]\leqslant\sigma^{k}k^{k/2}bold_E [ | bold_italic_X | start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ] â©½ italic_Ïƒ start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_k start_POSTSUPERSCRIPT italic_k / 2 end_POSTSUPERSCRIPT. For example, a mean-00 Gaussian with variance Ïƒ2superscriptğœ2\sigma^{2}italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT is Ïƒğœ\sigmaitalic_Ïƒ-subgaussian, and a uniformly random sign âˆˆ{Â±1}absentplus-or-minus1\in\{\pm 1\}âˆˆ { Â± 1 } is 1111-subgaussian. Note that rescaling a Ïƒğœ\sigmaitalic_Ïƒ-subgaussian variable ğ‘¿ğ‘¿\bm{X}bold_italic_X to Câ¢ğ‘¿ğ¶ğ‘¿C\bm{X}italic_C bold_italic_X for constant Cğ¶Citalic_C rescales the subgaussian parameter to Câ¢Ïƒğ¶ğœC\sigmaitalic_C italic_Ïƒ. 1.2 Results Our main theorem is the following. Theorem 1.4 (Informal version of Theorem 3.1). Suppose ğ’œğ’œ\mathcal{A}caligraphic_A is a Tğ‘‡Titalic_T-step AMP algorithm with Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 )-Lipschitz or polynomial denoiser functions. Let Xğ‘‹Xitalic_X be a symmetric nÃ—nğ‘›ğ‘›n\times nitalic_n Ã— italic_n matrix with i.i.d. Oâ¢(1)nğ‘‚1ğ‘›\frac{O(1)}{\sqrt{n}}divide start_ARG italic_O ( 1 ) end_ARG start_ARG square-root start_ARG italic_n end_ARG end_ARG-subgaussian entries having mean 00 and variance 1n1ğ‘›\frac{1}{n}divide start_ARG 1 end_ARG start_ARG italic_n end_ARG, and let vAMPâ¢(X)subscriptğ‘£AMPğ‘‹v_{\mathrm{AMP}}(X)italic_v start_POSTSUBSCRIPT roman_AMP end_POSTSUBSCRIPT ( italic_X ) be the output of ğ’œğ’œ\mathcal{A}caligraphic_A on Xğ‘‹Xitalic_X. Then there exists an algorithm which when given access to an Îµğœ€\varepsilonitalic_Îµ-principal minor corruption Yğ‘ŒYitalic_Y produces in time Oâ¢(Îµâ¢n3â¢logâ¡n)ğ‘‚ğœ€superscriptğ‘›3ğ‘›O(\varepsilon n^{3}\log n)italic_O ( italic_Îµ italic_n start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT roman_log italic_n ) a vector v^â¢(Y)^ğ‘£ğ‘Œ\hat{v}(Y)over^ start_ARG italic_v end_ARG ( italic_Y ) satisfying â€–v^â¢(Y)âˆ’vAMPâ¢(X)â€–2â©½Oâ¢(Îµâ¢logdâ¡1Îµ)â‹…â€–vAMPâ¢(X)â€–2,superscriptnorm^ğ‘£ğ‘Œsubscriptğ‘£AMPğ‘‹2â‹…ğ‘‚ğœ€superscriptğ‘‘1ğœ€superscriptnormsubscriptğ‘£AMPğ‘‹2\|\hat{v}(Y)-v_{\mathrm{AMP}}(X)\|^{2}\leqslant O(\varepsilon\log^{d}\tfrac{1}% {\varepsilon})\cdot\|v_{\mathrm{AMP}}(X)\|^{2},âˆ¥ over^ start_ARG italic_v end_ARG ( italic_Y ) - italic_v start_POSTSUBSCRIPT roman_AMP end_POSTSUBSCRIPT ( italic_X ) âˆ¥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT â©½ italic_O ( italic_Îµ roman_log start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT divide start_ARG 1 end_ARG start_ARG italic_Îµ end_ARG ) â‹… âˆ¥ italic_v start_POSTSUBSCRIPT roman_AMP end_POSTSUBSCRIPT ( italic_X ) âˆ¥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , with probability 1âˆ’oâ¢(1)1ğ‘œ11-o(1)1 - italic_o ( 1 ) over the randomness of Xğ‘‹Xitalic_X, where d=1ğ‘‘1d=1italic_d = 1 if the denoisers are Lipschitz, and d=kTğ‘‘superscriptğ‘˜ğ‘‡d=k^{T}italic_d = italic_k start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT if the denoisers are degree â©½kabsentğ‘˜\leqslant kâ©½ italic_k polynomials. In words, given access to an adversarially corrupted matrix Yğ‘ŒYitalic_Y, our algorithm can find a vector v^â¢(Y)^ğ‘£ğ‘Œ\hat{v}(Y)over^ start_ARG italic_v end_ARG ( italic_Y ) which is close to the output of AMP on the uncorrupted matrix Xğ‘‹Xitalic_X.333Since Xğ‘‹Xitalic_X has bounded operator norm, this implies that v^â¢(Y)^ğ‘£ğ‘Œ\hat{v}(Y)over^ start_ARG italic_v end_ARG ( italic_Y ) has objective value v^âŠ¤â¢Xâ¢v^superscript^ğ‘£topğ‘‹^ğ‘£\hat{v}^{\top}X\hat{v}over^ start_ARG italic_v end_ARG start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT italic_X over^ start_ARG italic_v end_ARG within an additive O~â¢(Îµ)~ğ‘‚ğœ€\tilde{O}(\sqrt{\varepsilon})over~ start_ARG italic_O end_ARG ( square-root start_ARG italic_Îµ end_ARG ) of the objective of vAMPâ¢(X)subscriptğ‘£AMPğ‘‹v_{\mathrm{AMP}}(X)italic_v start_POSTSUBSCRIPT roman_AMP end_POSTSUBSCRIPT ( italic_X ). The result improves on that of [IS24] in that it (1) runs in time Oâ¢(Îµâ¢n3â¢logâ¡n)ğ‘‚ğœ€superscriptğ‘›3ğ‘›O(\varepsilon n^{3}\log n)italic_O ( italic_Îµ italic_n start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT roman_log italic_n ) rather than nexpâ¡(T)superscriptğ‘›ğ‘‡n^{\exp(T)}italic_n start_POSTSUPERSCRIPT roman_exp ( italic_T ) end_POSTSUPERSCRIPT, and (2) guarantees that â€–ğ’œâ¢(X)âˆ’v^â¢(Y)â€–â©½fâ¢(Îµ)â¢â€–ğ’œâ¢(X)â€–normğ’œğ‘‹^ğ‘£ğ‘Œğ‘“ğœ€normğ’œğ‘‹\|\mathcal{A}(X)-\hat{v}(Y)\|\leqslant f(\varepsilon)\|\mathcal{A}(X)\|âˆ¥ caligraphic_A ( italic_X ) - over^ start_ARG italic_v end_ARG ( italic_Y ) âˆ¥ â©½ italic_f ( italic_Îµ ) âˆ¥ caligraphic_A ( italic_X ) âˆ¥ for a function fâ¢(Îµ)â†’Îµ0subscriptâ†’ğœ€ğ‘“ğœ€0f(\varepsilon)\to_{\varepsilon}0italic_f ( italic_Îµ ) â†’ start_POSTSUBSCRIPT italic_Îµ end_POSTSUBSCRIPT 0 which is independent of nğ‘›nitalic_n (but does depend on Tğ‘‡Titalic_T), whereas in [IS24] the function fâ¢(Îµ)ğ‘“ğœ€f(\varepsilon)italic_f ( italic_Îµ ) included a multiplicative factor of polyâ¢logâ¡(n)polyğ‘›\mathrm{poly}\log(n)roman_poly roman_log ( italic_n ), and thus was trivial unless Îµ=oâ¢(1)ğœ€ğ‘œ1\varepsilon=o(1)italic_Îµ = italic_o ( 1 ). As noted in [IS24], an equivalent result is information-theoretically impossible under the stronger corruption model in which Xâˆ’Yğ‘‹ğ‘ŒX-Yitalic_X - italic_Y is supported on Îµâ¢n2ğœ€superscriptğ‘›2\varepsilon n^{2}italic_Îµ italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT arbitrary entries (unless Îµ=oâ¢(nâˆ’1/2)ğœ€ğ‘œsuperscriptğ‘›12\varepsilon=o(n^{-1/2})italic_Îµ = italic_o ( italic_n start_POSTSUPERSCRIPT - 1 / 2 end_POSTSUPERSCRIPT )). As a direct corollary, we can robustly simulate Montanariâ€™s algorithm [Mon21] for finding the ground state of the Sherrington-Kirkpatrick Hamiltonianâ€”that is, an approximately optimal solution for Max-Cut with i.i.d. Gaussian edge weights. Corollary 1.5 (Fast, robust Sherrington Kirkpatrick). Suppose Xğ‘‹Xitalic_X is a symmetric matrix with entries sampled i.i.d. from ğ’©â¢(0,1n)ğ’©01ğ‘›\mathcal{N}(0,\frac{1}{n})caligraphic_N ( 0 , divide start_ARG 1 end_ARG start_ARG italic_n end_ARG ). Then there is an algorithm which when run on an Îµğœ€\varepsilonitalic_Îµ-principal minor corruption Yğ‘ŒYitalic_Y of Xğ‘‹Xitalic_X, with probability 1âˆ’oâ¢(1)1ğ‘œ11-o(1)1 - italic_o ( 1 ) produces in time Oâ¢(Îµâ¢n3â¢logâ¡n)ğ‘‚ğœ€superscriptğ‘›3ğ‘›O(\varepsilon n^{3}\log n)italic_O ( italic_Îµ italic_n start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT roman_log italic_n ) a unit vector v^â¢(Y)âˆˆ{Â±1/n}n^ğ‘£ğ‘Œsuperscriptplus-or-minus1ğ‘›ğ‘›\hat{v}(Y)\in\{\pm 1/\sqrt{n}\}^{n}over^ start_ARG italic_v end_ARG ( italic_Y ) âˆˆ { Â± 1 / square-root start_ARG italic_n end_ARG } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT achieving objective value v^â¢(Y)âŠ¤â¢Xâ¢v^â¢(Y)â©¾OBJAMPâˆ’Oâ¢(Îµâ¢logâ¡1Îµ)^ğ‘£superscriptğ‘Œtopğ‘‹^ğ‘£ğ‘ŒsubscriptOBJAMPğ‘‚ğœ€1ğœ€\hat{v}(Y)^{\top}X\hat{v}(Y)\geqslant\mathrm{OBJ_{AMP}}-O(\sqrt{\varepsilon% \log\frac{1}{\varepsilon}})over^ start_ARG italic_v end_ARG ( italic_Y ) start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT italic_X over^ start_ARG italic_v end_ARG ( italic_Y ) â©¾ roman_OBJ start_POSTSUBSCRIPT roman_AMP end_POSTSUBSCRIPT - italic_O ( square-root start_ARG italic_Îµ roman_log divide start_ARG 1 end_ARG start_ARG italic_Îµ end_ARG end_ARG ). The value OBJAMPsubscriptOBJAMP\mathrm{OBJ_{AMP}}roman_OBJ start_POSTSUBSCRIPT roman_AMP end_POSTSUBSCRIPT is the objective value achieved by Montanariâ€™s AMP algorithm; modulo a widely-believed conjecture in statistical physics, OBJAMPsubscriptOBJAMP\mathrm{OBJ_{AMP}}roman_OBJ start_POSTSUBSCRIPT roman_AMP end_POSTSUBSCRIPT approaches OPT=maxvâˆˆ{Â±1/nâ¡vâŠ¤â¢Xâ¢vâ‰ˆ1.52\mathrm{OPT}=\max_{v\in\{\pm 1/\sqrt{n}}v^{\top}Xv\approx 1.52roman_OPT = roman_max start_POSTSUBSCRIPT italic_v âˆˆ { Â± 1 / square-root start_ARG italic_n end_ARG end_POSTSUBSCRIPT italic_v start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT italic_X italic_v â‰ˆ 1.52 as Tâ†’âˆâ†’ğ‘‡T\to\inftyitalic_T â†’ âˆ. The corollary follows from Theorem 1.4 because Montanariâ€™s denoisers are Lipschitz, and the rounding scheme applied to place the final iterate in the hypercube is also Lipschitz. In Section 4, we give a simple proof (along similar lines as the proof of Theorem 1.4) that AMP is robust to adversarial perturbations of small spectral norm. This fact is folklore, but we feel our proof is quite simple and may be of interest. 1.3 Experiments Our algorithm is fast enough that it can be easily implemented and run on a laptop. We have run some experiments to demonstrate the utility of our method. We consider the non-negative PCA objective described in Example 1.2. In [MR15], it was shown that AMP with denoiser function fâ¢(x)=maxâ¡(0,x)ğ‘“ğ‘¥0ğ‘¥f(x)=\max(0,x)italic_f ( italic_x ) = roman_max ( 0 , italic_x ) is an FPTAS for OPT=maxvâ©¾0,â€–vâ€–=1â¡vâŠ¤â¢Xâ¢v=2OPTsubscriptformulae-sequenceğ‘£0normğ‘£1superscriptğ‘£topğ‘‹ğ‘£2\mathrm{OPT}=\max_{v\geqslant 0,\|v\|=1}v^{\top}Xv=\sqrt{2}roman_OPT = roman_max start_POSTSUBSCRIPT italic_v â©¾ 0 , âˆ¥ italic_v âˆ¥ = 1 end_POSTSUBSCRIPT italic_v start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT italic_X italic_v = square-root start_ARG 2 end_ARG. In Figure 1, we show the result for n=3000,Îµ=0.02formulae-sequenceğ‘›3000ğœ€0.02n=3000,\varepsilon=0.02italic_n = 3000 , italic_Îµ = 0.02, with the adversarial corruption given by perturbing an Îµâ¢nÃ—Îµâ¢nğœ€ğ‘›ğœ€ğ‘›\varepsilon n\times\varepsilon nitalic_Îµ italic_n Ã— italic_Îµ italic_n principal minor by sampling two independent rank 50=56â¢Îµâ¢n5056ğœ€ğ‘›50=\frac{5}{6}\varepsilon n50 = divide start_ARG 5 end_ARG start_ARG 6 end_ARG italic_Îµ italic_n Wishart matrices, each normalized to have expected Frobenius norm 100100100100, and adding one and subtracting the other. Without having taken pains to optimize the running time, the implementation in Python on a laptop takes less than 5 minutes. We have plotted (1) the correlation of our algorithmâ€™s output, v^â¢(Y)^ğ‘£ğ‘Œ\hat{v}(Y)over^ start_ARG italic_v end_ARG ( italic_Y ), with vAMPâ¢(X)subscriptğ‘£AMPğ‘‹v_{\mathrm{AMP}}(X)italic_v start_POSTSUBSCRIPT roman_AMP end_POSTSUBSCRIPT ( italic_X ), and (2) the objective value of the output for the uncorrupted matrix Xğ‘‹Xitalic_X, v^â¢(Y)âŠ¤â¢Xâ¢v^â¢(Y)^ğ‘£superscriptğ‘Œtopğ‘‹^ğ‘£ğ‘Œ\hat{v}(Y)^{\top}X\hat{v}(Y)over^ start_ARG italic_v end_ARG ( italic_Y ) start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT italic_X over^ start_ARG italic_v end_ARG ( italic_Y ), as a function of the number of iterations. For comparison, we plot in Figure 1 the performance of (a) AMP on the corrupt matrix, vAMPâ¢(Y)subscriptğ‘£AMPğ‘Œv_{\mathrm{AMP}}(Y)italic_v start_POSTSUBSCRIPT roman_AMP end_POSTSUBSCRIPT ( italic_Y ), and (b) AMP on a â€œnaiveâ€ spectral cleaning Y~~ğ‘Œ\tilde{Y}over~ start_ARG italic_Y end_ARG of Yğ‘ŒYitalic_Y, given by deleting all larger-than-expected eigenvalues. Our procedure performs much better than AMP on the corrupt input. Empirically, the naive cleaning performance is comparable to ours, but unlike our algorithm, the naive procedure does not come with provable guarantees for arbitrary perturbations (and we suspect the naive procedure may be succeeding due to a small-nğ‘›nitalic_n effect). Figure 1: Plot of the correlation of the vector v^â¢(Y)^ğ‘£ğ‘Œ\hat{v}(Y)over^ start_ARG italic_v end_ARG ( italic_Y ) with the output of AMP on the â€œcleanâ€ matrix Xğ‘‹Xitalic_X, and of the objective value attained by v^â¢(Y)^ğ‘£ğ‘Œ\hat{v}(Y)over^ start_ARG italic_v end_ARG ( italic_Y ) on the clean matrix Xğ‘‹Xitalic_X. 1.4 Discussion We give a fast spectral algorithm for simulating AMP under adversarial principal minor corruptions. Our algorithm is an implementation of the â€œspectral algorithms from sum-of-squares (SoS) analysesâ€ strategy introduced in [HSSS16]. We find it to be a particularly striking example of this strategyâ€”not only was the running time reduced from nexpâ¡(T)superscriptğ‘›ğ‘‡n^{\exp(T)}italic_n start_POSTSUPERSCRIPT roman_exp ( italic_T ) end_POSTSUPERSCRIPT to Oâ¢(n3)ğ‘‚superscriptğ‘›3O(n^{3})italic_O ( italic_n start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ), but also, the analysis very transparently mimics/distills that of [IS24] to yield a much cleaner argument. We draw a comparison to previous spectral-to-SoS analyses in robust statistics, most of which have been based on a â€œfilteringâ€ approach (e.g. [JLST21, DKK+19]); in the filtering algorithms, the non-SoS analysis required significant additional tools. Another fitting comparison is to recent works obtaining robust spectral algorithms for community recovery in the stochastic block model [MRW24, DdHS23, DdNS22], where it was important to have a very fine-grained understanding of the spectrum of specific matrices. In our case, we are able to get away with a much simpler analysis. Though we have improved on the result in [IS24] in terms of running time and the robustness-accuracy tradeoff, we differ from our prior work in one aspect: we require a description of the denoisers â„±â„±\mathcal{F}caligraphic_F used in the AMP algorithm ğ’œğ’œ\mathcal{A}caligraphic_A, whereas the algorithm in [IS24] has access only to the low-degree moments of the joint distribution over X,ğ’œâ¢(X)ğ‘‹ğ’œğ‘‹X,\mathcal{A}(X)italic_X , caligraphic_A ( italic_X ). We find it unlikely that a fast algorithm could succeed without a description of â„±â„±\mathcal{F}caligraphic_F, but we pose this as a question nonetheless. Another question is whether our error guarantees are optimal, as a function of the number of AMP iterations Tğ‘‡Titalic_T. In our theorem, the O~â¢(Îµ)~ğ‘‚ğœ€\tilde{O}(\sqrt{\varepsilon})over~ start_ARG italic_O end_ARG ( square-root start_ARG italic_Îµ end_ARG ) hides factors that grow with the number of AMP iterations; however our experiments (Figure 1) seem to suggest that the error stabilizesâ€”is this a small nğ‘›nitalic_n effect? Or perhaps an artifact of the specific perturbation from our experiments? One clear direction for future work is making AMP robust when the input matrix Xğ‘‹Xitalic_X has planted structure, rather than just having i.i.d. subgaussian entries. For example, AMP has been a successful algorithm for â€œspiked matrix modelsâ€ in which X=G+Î»â¢uâ¢uâŠ¤ğ‘‹ğºğœ†ğ‘¢superscriptğ‘¢topX=G+\lambda uu^{\top}italic_X = italic_G + italic_Î» italic_u italic_u start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT with GğºGitalic_G a Gaussian matrix and uâ¢uâŠ¤ğ‘¢superscriptğ‘¢topuu^{\top}italic_u italic_u start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT a rank-1 spike, the goal often being to find uğ‘¢uitalic_u given Xğ‘‹Xitalic_X. In this case, it is not completely clear which noise model to study. In some cases (e.g. when uğ‘¢uitalic_u is sparse) a principal minor corruption could simply erase the spike uâ¢uâŠ¤ğ‘¢superscriptğ‘¢topuu^{\top}italic_u italic_u start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT. However, it is an interesting question whether our techniques can be extended to this caseâ€”currently, our algorithm incorporates information about i.i.d. subgaussian variables, which makes it inappropriate for planted models (the same is true of [IS24]). Finally, it is interesting to consider alternative corruption models. The principal minor corruption is tractable to study, and the fact that it is adversarial makes it a powerful model. We know from [IS24] that a similar result is information-theoretically impossible under the strongest sparse adversarial corruption model, in which an arbitrary subset of Îµâ¢n2ğœ€superscriptğ‘›2\varepsilon n^{2}italic_Îµ italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT entries is perturbed. However, it would be interesting to consider alternative corruption models that more faithfully model the distribution shift one expects to see in practice, for example in the application of compressed sensing. 1.5 Technical overview Though the proof of Theorem 1.4 is not long, we briefly summarize the main ideas here. For the sake of simplicity, in this technical overview we pretend that the AMP iteration has the form x(t)=Xâ¢fâ¢(x(tâˆ’1))superscriptğ‘¥ğ‘¡ğ‘‹ğ‘“superscriptğ‘¥ğ‘¡1x^{(t)}=Xf(x^{(t-1)})italic_x start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT = italic_X italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ), ignoring the Onsager correction and the dependence on more than one prior iterate. Recall that we are given an Îµğœ€\varepsilonitalic_Îµ-principal minor corruption Yğ‘ŒYitalic_Y of Xğ‘‹Xitalic_X. The fact that Xğ‘‹Xitalic_X has i.i.d. subgaussian entries of variance 1n1ğ‘›\frac{1}{n}divide start_ARG 1 end_ARG start_ARG italic_n end_ARG implies that with high probability, â€–Xâ€–ğ—ˆğ—‰=Oâ¢(1)subscriptnormğ‘‹ğ—ˆğ—‰ğ‘‚1\|X\|_{\operatorname{\mathsf{op}}}=O(1)âˆ¥ italic_X âˆ¥ start_POSTSUBSCRIPT sansserif_op end_POSTSUBSCRIPT = italic_O ( 1 ). The first step of our algorithm is a spectral procedure which removes Oâ¢(Îµâ¢n)ğ‘‚ğœ€ğ‘›O(\varepsilon n)italic_O ( italic_Îµ italic_n ) rows and columns of Yğ‘ŒYitalic_Y, producing a matrix Y^^ğ‘Œ\hat{Y}over^ start_ARG italic_Y end_ARG with â€–Y^â€–ğ—ˆğ—‰=Oâ¢(1)subscriptnorm^ğ‘Œğ—ˆğ—‰ğ‘‚1\|\hat{Y}\|_{\operatorname{\mathsf{op}}}=O(1)âˆ¥ over^ start_ARG italic_Y end_ARG âˆ¥ start_POSTSUBSCRIPT sansserif_op end_POSTSUBSCRIPT = italic_O ( 1 ). Then, we run a modified version of the AMP algorithm on the cleaned input matrix Y^^ğ‘Œ\hat{Y}over^ start_ARG italic_Y end_ARG, producing iterates y(1),y(2),â€¦superscriptğ‘¦1superscriptğ‘¦2â€¦y^{(1)},y^{(2)},\ldotsitalic_y start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT , italic_y start_POSTSUPERSCRIPT ( 2 ) end_POSTSUPERSCRIPT , â€¦ just as the original AMP algorithm would have, except that at each iteration we clip the entries y(t)=Y^â¢fâ¢(ğ–¼ğ—…ğ—‚ğ—‰â¡(y(tâˆ’1)))superscriptğ‘¦ğ‘¡^ğ‘Œğ‘“ğ–¼ğ—…ğ—‚ğ—‰superscriptğ‘¦ğ‘¡1y^{(t)}=\hat{Y}f(\operatorname{\mathsf{clip}}(y^{(t-1)}))italic_y start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT = over^ start_ARG italic_Y end_ARG italic_f ( sansserif_clip ( italic_y start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) ), so that the magnitude of all entries of ğ–¼ğ—…ğ—‚ğ—‰â¡(y(tâˆ’1))ğ–¼ğ—…ğ—‚ğ—‰superscriptğ‘¦ğ‘¡1\operatorname{\mathsf{clip}}(y^{(t-1)})sansserif_clip ( italic_y start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) does not exceed the Îµğœ€\varepsilonitalic_Îµ-quantile444In the proof we choose the threshold to not exactly correspond to the Îµğœ€\varepsilonitalic_Îµ-quantile, but this choice would have also worked and is simpler for the sake of this overview. value Oâ¢(polyâ¢logâ¡1Îµ)ğ‘‚poly1ğœ€O(\mathrm{poly}\log\frac{1}{\varepsilon})italic_O ( roman_poly roman_log divide start_ARG 1 end_ARG start_ARG italic_Îµ end_ARG ) of the entries in a typical iterate x(tâˆ’1)superscriptğ‘¥ğ‘¡1x^{(t-1)}italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT from a clean input matrix. We argue that â€–y(t)âˆ’x(t)â€–â©½O~â¢(Îµ)â¢â€–x(t)â€–normsuperscriptğ‘¦ğ‘¡superscriptğ‘¥ğ‘¡~ğ‘‚ğœ€normsuperscriptğ‘¥ğ‘¡\|y^{(t)}-x^{(t)}\|\leqslant\tilde{O}(\sqrt{\varepsilon})\|x^{(t)}\|âˆ¥ italic_y start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT - italic_x start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT âˆ¥ â©½ over~ start_ARG italic_O end_ARG ( square-root start_ARG italic_Îµ end_ARG ) âˆ¥ italic_x start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT âˆ¥ by induction on tğ‘¡titalic_t; In the base case, t=0ğ‘¡0t=0italic_t = 0, the iterates are identical as x(t)=1â†’=y(t)superscriptğ‘¥ğ‘¡â†’1superscriptğ‘¦ğ‘¡x^{(t)}=\vec{1}=y^{(t)}italic_x start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT = overâ†’ start_ARG 1 end_ARG = italic_y start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT. Now for tâ©¾1ğ‘¡1t\geqslant 1italic_t â©¾ 1, suppose that x(t)superscriptğ‘¥ğ‘¡x^{(t)}italic_x start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT is the (unobserved) iterate AMP would have produced on Xğ‘‹Xitalic_X. Then â€–y(t)âˆ’x(t)â€–normsuperscriptğ‘¦ğ‘¡superscriptğ‘¥ğ‘¡\displaystyle\left\|y^{(t)}-x^{(t)}\right\|âˆ¥ italic_y start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT - italic_x start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT âˆ¥ =â€–Y^â¢fâ¢(ğ–¼ğ—…ğ—‚ğ—‰â¡(y(tâˆ’1)))âˆ’Xâ¢fâ¢(x(tâˆ’1))â€–absentnorm^ğ‘Œğ‘“ğ–¼ğ—…ğ—‚ğ—‰superscriptğ‘¦ğ‘¡1ğ‘‹ğ‘“superscriptğ‘¥ğ‘¡1\displaystyle=\left\|\hat{Y}f(\operatorname{\mathsf{clip}}(y^{(t-1)}))-Xf(x^{(% t-1)})\right\|= âˆ¥ over^ start_ARG italic_Y end_ARG italic_f ( sansserif_clip ( italic_y start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) ) - italic_X italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) âˆ¥ â©½â€–Y^â¢(fâ¢(ğ–¼ğ—…ğ—‚ğ—‰â¡(y(tâˆ’1)))âˆ’fâ¢(x(tâˆ’1)))â€–+â€–(Y^âˆ’X)â¢fâ¢(x(tâˆ’1))â€–absentnorm^ğ‘Œğ‘“ğ–¼ğ—…ğ—‚ğ—‰superscriptğ‘¦ğ‘¡1ğ‘“superscriptğ‘¥ğ‘¡1norm^ğ‘Œğ‘‹ğ‘“superscriptğ‘¥ğ‘¡1\displaystyle\leqslant\left\|\hat{Y}(f(\operatorname{\mathsf{clip}}(y^{(t-1)})% )-f(x^{(t-1)}))\right\|+\left\|(\hat{Y}-X)f(x^{(t-1)})\right\|â©½ âˆ¥ over^ start_ARG italic_Y end_ARG ( italic_f ( sansserif_clip ( italic_y start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) ) - italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) ) âˆ¥ + âˆ¥ ( over^ start_ARG italic_Y end_ARG - italic_X ) italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) âˆ¥ â©½â€–Y^â€–ğ—ˆğ—‰â¢â€–fâ¢(ğ–¼ğ—…ğ—‚ğ—‰â¡(y(tâˆ’1)))âˆ’fâ¢(x(tâˆ’1))â€–+â€–(Y^âˆ’X)â¢fâ¢(x(tâˆ’1))â€–absentsubscriptnorm^ğ‘Œğ—ˆğ—‰normğ‘“ğ–¼ğ—…ğ—‚ğ—‰superscriptğ‘¦ğ‘¡1ğ‘“superscriptğ‘¥ğ‘¡1norm^ğ‘Œğ‘‹ğ‘“superscriptğ‘¥ğ‘¡1\displaystyle\leqslant\left\|\hat{Y}\right\|_{\operatorname{\mathsf{op}}}\left% \|f(\operatorname{\mathsf{clip}}(y^{(t-1)}))-f(x^{(t-1)})\right\|+\left\|(\hat% {Y}-X)f(x^{(t-1)})\right\|â©½ âˆ¥ over^ start_ARG italic_Y end_ARG âˆ¥ start_POSTSUBSCRIPT sansserif_op end_POSTSUBSCRIPT âˆ¥ italic_f ( sansserif_clip ( italic_y start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) ) - italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) âˆ¥ + âˆ¥ ( over^ start_ARG italic_Y end_ARG - italic_X ) italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) âˆ¥ (1) The spectral cleaning ensures that â€–Yâ€–ğ—ˆğ—‰=Oâ¢(1)subscriptnormğ‘Œğ—ˆğ—‰ğ‘‚1\|Y\|_{\operatorname{\mathsf{op}}}=O(1)âˆ¥ italic_Y âˆ¥ start_POSTSUBSCRIPT sansserif_op end_POSTSUBSCRIPT = italic_O ( 1 ). To further bound the first term in (1), consider the illustrative case of the denoiser fâ¢(x)=x2ğ‘“ğ‘¥superscriptğ‘¥2f(x)=x^{2}italic_f ( italic_x ) = italic_x start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. Then for any vectors a,bğ‘ğ‘a,bitalic_a , italic_b, fâ¢(a)âˆ’fâ¢(b)=(a+b)âˆ˜(aâˆ’b)ğ‘“ğ‘ğ‘“ğ‘ğ‘ğ‘ğ‘ğ‘f(a)-f(b)=(a+b)\circ(a-b)italic_f ( italic_a ) - italic_f ( italic_b ) = ( italic_a + italic_b ) âˆ˜ ( italic_a - italic_b ), for âˆ˜\circâˆ˜ the entrywise product. Thus we have â€–fâ¢(ğ–¼ğ—…ğ—‚ğ—‰â¡(y(tâˆ’1)))âˆ’fâ¢(x(tâˆ’1))â€–normğ‘“ğ–¼ğ—…ğ—‚ğ—‰superscriptğ‘¦ğ‘¡1ğ‘“superscriptğ‘¥ğ‘¡1\displaystyle\left\|f(\operatorname{\mathsf{clip}}(y^{(t-1)}))-f(x^{(t-1)})\right\|âˆ¥ italic_f ( sansserif_clip ( italic_y start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) ) - italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) âˆ¥ =â€–(ğ–¼ğ—…ğ—‚ğ—‰â¡(y(tâˆ’1))+x(tâˆ’1))âˆ˜(ğ–¼ğ—…ğ—‚ğ—‰â¡(y(tâˆ’1))âˆ’x(tâˆ’1))â€–absentnormğ–¼ğ—…ğ—‚ğ—‰superscriptğ‘¦ğ‘¡1superscriptğ‘¥ğ‘¡1ğ–¼ğ—…ğ—‚ğ—‰superscriptğ‘¦ğ‘¡1superscriptğ‘¥ğ‘¡1\displaystyle=\left\|(\operatorname{\mathsf{clip}}(y^{(t-1)})+x^{(t-1)})\circ(% \operatorname{\mathsf{clip}}(y^{(t-1)})-x^{(t-1)})\right\|= âˆ¥ ( sansserif_clip ( italic_y start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) + italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) âˆ˜ ( sansserif_clip ( italic_y start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) - italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) âˆ¥ â©½â€–ğ–¼ğ—…ğ—‚ğ—‰â¡(y(tâˆ’1))â€–âˆâ‹…â€–ğ–¼ğ—…ğ—‚ğ—‰â¡(y(tâˆ’1))âˆ’x(tâˆ’1)â€–+â€–x(tâˆ’1)âˆ˜(ğ–¼ğ—…ğ—‚ğ—‰â¡(y(tâˆ’1))âˆ’x(tâˆ’1))â€–absentâ‹…subscriptnormğ–¼ğ—…ğ—‚ğ—‰superscriptğ‘¦ğ‘¡1normğ–¼ğ—…ğ—‚ğ—‰superscriptğ‘¦ğ‘¡1superscriptğ‘¥ğ‘¡1normsuperscriptğ‘¥ğ‘¡1ğ–¼ğ—…ğ—‚ğ—‰superscriptğ‘¦ğ‘¡1superscriptğ‘¥ğ‘¡1\displaystyle\leqslant\left\|\operatorname{\mathsf{clip}}(y^{(t-1)})\right\|_{% \infty}\cdot\left\|\operatorname{\mathsf{clip}}(y^{(t-1)})-x^{(t-1)}\right\|+% \left\|x^{(t-1)}\circ(\operatorname{\mathsf{clip}}(y^{(t-1)})-x^{(t-1)})\right\|â©½ âˆ¥ sansserif_clip ( italic_y start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) âˆ¥ start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT â‹… âˆ¥ sansserif_clip ( italic_y start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) - italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT âˆ¥ + âˆ¥ italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT âˆ˜ ( sansserif_clip ( italic_y start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) - italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) âˆ¥ (2) The first and second terms of (2) are bounded in a similar manner, we begin by explaining the first. Because of the clipping procedure, â€–ğ–¼ğ—…ğ—‚ğ—‰â¡(y(tâˆ’1))â€–âˆ=Oâ¢(polyâ¢logâ¡1Îµ)subscriptnormğ–¼ğ—…ğ—‚ğ—‰superscriptğ‘¦ğ‘¡1ğ‘‚poly1ğœ€\|\operatorname{\mathsf{clip}}(y^{(t-1)})\|_{\infty}=O(\mathrm{poly}\log\frac{% 1}{\varepsilon})âˆ¥ sansserif_clip ( italic_y start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) âˆ¥ start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT = italic_O ( roman_poly roman_log divide start_ARG 1 end_ARG start_ARG italic_Îµ end_ARG ). Further, by the triangle inequality, â€–ğ–¼ğ—…ğ—‚ğ—‰â¡(y(tâˆ’1))âˆ’x(tâˆ’1)â€–â©½â€–ğ–¼ğ—…ğ—‚ğ—‰â¡(y(tâˆ’1))âˆ’ğ–¼ğ—…ğ—‚ğ—‰â¡(x(tâˆ’1))â€–+â€–ğ–¼ğ—…ğ—‚ğ—‰â¡(x(tâˆ’1))âˆ’x(tâˆ’1)â€–.normğ–¼ğ—…ğ—‚ğ—‰superscriptğ‘¦ğ‘¡1superscriptğ‘¥ğ‘¡1normğ–¼ğ—…ğ—‚ğ—‰superscriptğ‘¦ğ‘¡1ğ–¼ğ—…ğ—‚ğ—‰superscriptğ‘¥ğ‘¡1normğ–¼ğ—…ğ—‚ğ—‰superscriptğ‘¥ğ‘¡1superscriptğ‘¥ğ‘¡1\|\operatorname{\mathsf{clip}}(y^{(t-1)})-x^{(t-1)}\|\leqslant\|\operatorname{% \mathsf{clip}}(y^{(t-1)})-\operatorname{\mathsf{clip}}(x^{(t-1)})\|+\|% \operatorname{\mathsf{clip}}(x^{(t-1)})-x^{(t-1)}\|.âˆ¥ sansserif_clip ( italic_y start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) - italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT âˆ¥ â©½ âˆ¥ sansserif_clip ( italic_y start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) - sansserif_clip ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) âˆ¥ + âˆ¥ sansserif_clip ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) - italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT âˆ¥ . (3) The first term on the right of (3) can be bounded by O~â¢(Îµ)â‹…â€–x(tâˆ’1)â€–â‹…~ğ‘‚ğœ€normsuperscriptğ‘¥ğ‘¡1\tilde{O}(\sqrt{\varepsilon})\cdot\|x^{(t-1)}\|over~ start_ARG italic_O end_ARG ( square-root start_ARG italic_Îµ end_ARG ) â‹… âˆ¥ italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT âˆ¥ from the inductive hypothesis, because the ğ–¼ğ—…ğ—‚ğ—‰ğ–¼ğ—…ğ—‚ğ—‰\operatorname{\mathsf{clip}}sansserif_clip function is 1111-Lipschitz. The second term in (3) can be bounded by O~â¢(Îµ)â‹…â€–x(tâˆ’1)â€–â‹…~ğ‘‚ğœ€normsuperscriptğ‘¥ğ‘¡1\tilde{O}(\sqrt{\varepsilon})\cdot\|x^{(t-1)}\|over~ start_ARG italic_O end_ARG ( square-root start_ARG italic_Îµ end_ARG ) â‹… âˆ¥ italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT âˆ¥, because the distribution of x(tâˆ’1)superscriptğ‘¥ğ‘¡1x^{(t-1)}italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPTâ€™s entries is known, and is roughly that of independent polynomials in Gaussian random variables. To bound the second term from (2), we separate the contribution of the entries of x(tâˆ’1)superscriptğ‘¥ğ‘¡1x^{(t-1)}italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT which are bounded by Oâ¢(polyâ¢logâ¡1Îµ)ğ‘‚poly1ğœ€O(\mathrm{poly}\log\frac{1}{\varepsilon})italic_O ( roman_poly roman_log divide start_ARG 1 end_ARG start_ARG italic_Îµ end_ARG ), to which we can apply an identical argument, and the entries which exceed this threshold, and then appeal to the fact that these integrate to a small total. A similar argument can be used for arbitrary polynomial fğ‘“fitalic_f (for Lipschitz fğ‘“fitalic_f, (1) can be bounded directly and the clipping is not necessary). To bound the second term in (1), we use the fact that Y^âˆ’X^ğ‘Œğ‘‹\hat{Y}-Xover^ start_ARG italic_Y end_ARG - italic_X can be written as the sum of a matrix Eğ¸Eitalic_E, supported on an Îµâ¢nÃ—Îµâ¢nğœ€ğ‘›ğœ€ğ‘›\varepsilon n\times\varepsilon nitalic_Îµ italic_n Ã— italic_Îµ italic_n principal minor, and a matrix Fğ¹Fitalic_F which is equal to the support of âˆ’Xğ‘‹-X- italic_X on at most Oâ¢(Îµâ¢n)ğ‘‚ğœ€ğ‘›O(\varepsilon n)italic_O ( italic_Îµ italic_n ) rows/columnsâ€”these are precisely the rows/columns of Yğ‘ŒYitalic_Y which were removed to form Y^^ğ‘Œ\hat{Y}over^ start_ARG italic_Y end_ARG, but were not involved in the initial principal minor corruption. So, â€–(Y^âˆ’X)â¢fâ¢(x(tâˆ’1))â€–â©½â€–Eâ¢fâ¢(x(tâˆ’1))â€–+â€–Fâ¢fâ¢(x(tâˆ’1))â€–norm^ğ‘Œğ‘‹ğ‘“superscriptğ‘¥ğ‘¡1normğ¸ğ‘“superscriptğ‘¥ğ‘¡1normğ¹ğ‘“superscriptğ‘¥ğ‘¡1\|(\hat{Y}-X)f(x^{(t-1)})\|\leqslant\|Ef(x^{(t-1)})\|+\|Ff(x^{(t-1)})\|âˆ¥ ( over^ start_ARG italic_Y end_ARG - italic_X ) italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) âˆ¥ â©½ âˆ¥ italic_E italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) âˆ¥ + âˆ¥ italic_F italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) âˆ¥. Since Eğ¸Eitalic_E is supported on Îµâ¢nğœ€ğ‘›\varepsilon nitalic_Îµ italic_n columns, â€–Eâ¢fâ¢(x(tâˆ’1))â€–â©½â€–Eâ€–ğ—ˆğ—‰â‹…maxIâŠ‚[n],|I|=Îµâ¢nâ¢âˆ‘iâˆˆIfâ¢(x(tâˆ’1))i2.normğ¸ğ‘“superscriptğ‘¥ğ‘¡1â‹…subscriptnormğ¸ğ—ˆğ—‰subscriptformulae-sequenceğ¼delimited-[]ğ‘›ğ¼ğœ€ğ‘›subscriptğ‘–ğ¼ğ‘“superscriptsubscriptsuperscriptğ‘¥ğ‘¡1ğ‘–2\|Ef(x^{(t-1)})\|\leqslant\|E\|_{\operatorname{\mathsf{op}}}\cdot\max_{I% \subset[n],|I|=\varepsilon n}\sum_{i\in I}f(x^{(t-1)})_{i}^{2}.âˆ¥ italic_E italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) âˆ¥ â©½ âˆ¥ italic_E âˆ¥ start_POSTSUBSCRIPT sansserif_op end_POSTSUBSCRIPT â‹… roman_max start_POSTSUBSCRIPT italic_I âŠ‚ [ italic_n ] , | italic_I | = italic_Îµ italic_n end_POSTSUBSCRIPT âˆ‘ start_POSTSUBSCRIPT italic_i âˆˆ italic_I end_POSTSUBSCRIPT italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . Here again, because we know the order statistics of x(tâˆ’1)superscriptğ‘¥ğ‘¡1x^{(t-1)}italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT, and because fğ‘“fitalic_f is required to be a well-behaved function, the maximum norm of fâ¢(x(tâˆ’1))ğ‘“superscriptğ‘¥ğ‘¡1f(x^{(t-1)})italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) when restricted to a subset of Îµâ¢nğœ€ğ‘›\varepsilon nitalic_Îµ italic_n coordinates is on the order of O~â¢(Îµ)â¢â€–x(tâˆ’1)â€–~ğ‘‚ğœ€normsuperscriptğ‘¥ğ‘¡1\tilde{O}(\sqrt{\varepsilon})\|x^{(t-1)}\|over~ start_ARG italic_O end_ARG ( square-root start_ARG italic_Îµ end_ARG ) âˆ¥ italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT âˆ¥. Also, since Eğ¸Eitalic_E is a submatrix of Y^âˆ’X^ğ‘Œğ‘‹\hat{Y}-Xover^ start_ARG italic_Y end_ARG - italic_X, â€–Eâ€–ğ—ˆğ—‰â©½â€–Y^â€–ğ—ˆğ—‰+â€–Xâ€–ğ—ˆğ—‰â©½12subscriptnormğ¸ğ—ˆğ—‰subscriptnorm^ğ‘Œğ—ˆğ—‰subscriptnormğ‘‹ğ—ˆğ—‰12\|E\|_{\operatorname{\mathsf{op}}}\leqslant\|\hat{Y}\|_{\operatorname{\mathsf{% op}}}+\|X\|_{\operatorname{\mathsf{op}}}\leqslant 12âˆ¥ italic_E âˆ¥ start_POSTSUBSCRIPT sansserif_op end_POSTSUBSCRIPT â©½ âˆ¥ over^ start_ARG italic_Y end_ARG âˆ¥ start_POSTSUBSCRIPT sansserif_op end_POSTSUBSCRIPT + âˆ¥ italic_X âˆ¥ start_POSTSUBSCRIPT sansserif_op end_POSTSUBSCRIPT â©½ 12. The matrix Fğ¹Fitalic_F can be split into the part F1subscriptğ¹1F_{1}italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT supported on Oâ¢(Îµâ¢n)ğ‘‚ğœ€ğ‘›O(\varepsilon n)italic_O ( italic_Îµ italic_n ) columns, for which the argument is identical to the case of Ef(x(tâˆ’1)Ef(x^{(t-1)}italic_E italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT above. But there is also a part F2subscriptğ¹2F_{2}italic_F start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT supported on Oâ¢(Îµâ¢n)ğ‘‚ğœ€ğ‘›O(\varepsilon n)italic_O ( italic_Îµ italic_n ) rows. Here, we have to take a different perspective: since F2subscriptğ¹2F_{2}italic_F start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT is a restriction of âˆ’Xğ‘‹-X- italic_X to the rows indexed by some set TâŠ‚[n]ğ‘‡delimited-[]ğ‘›T\subset[n]italic_T âŠ‚ [ italic_n ] with |T|=Îµâ¢nğ‘‡ğœ€ğ‘›|T|=\varepsilon n| italic_T | = italic_Îµ italic_n, we have that F2â¢fâ¢(x(tâˆ’1))=(âˆ’Xâ¢fâ¢(x(tâˆ’1)))Tsubscriptğ¹2ğ‘“superscriptğ‘¥ğ‘¡1subscriptğ‘‹ğ‘“superscriptğ‘¥ğ‘¡1ğ‘‡F_{2}f(x^{(t-1)})=(-Xf(x^{(t-1)}))_{T}italic_F start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) = ( - italic_X italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) ) start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT, which is an Îµâ¢nğœ€ğ‘›\varepsilon nitalic_Îµ italic_n-sparse subset of the vector âˆ’Xf(x(tâˆ’1)-Xf(x^{(t-1)}- italic_X italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT. But we understand the order statistics of this vector too! Hence we have that â€–F2â¢fâ¢(x(tâˆ’1))â€–=O~â¢(Îµ)â¢â€–x(tâˆ’1)â€–normsubscriptğ¹2ğ‘“superscriptğ‘¥ğ‘¡1~ğ‘‚ğœ€normsuperscriptğ‘¥ğ‘¡1\|F_{2}f(x^{(t-1)})\|=\tilde{O}(\sqrt{\varepsilon})\|x^{(t-1)}\|âˆ¥ italic_F start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_f ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT ) âˆ¥ = over~ start_ARG italic_O end_ARG ( square-root start_ARG italic_Îµ end_ARG ) âˆ¥ italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT âˆ¥ as desired. Putting everything together, we have that â€–y(t)âˆ’x(t)â€–â©½O~â¢(Îµ)â‹…â€–x(tâˆ’1)â€–normsuperscriptğ‘¦ğ‘¡superscriptğ‘¥ğ‘¡â‹…~ğ‘‚ğœ€normsuperscriptğ‘¥ğ‘¡1\|y^{(t)}-x^{(t)}\|\leqslant\tilde{O}(\sqrt{\varepsilon})\cdot\|x^{(t-1)}\|âˆ¥ italic_y start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT - italic_x start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT âˆ¥ â©½ over~ start_ARG italic_O end_ARG ( square-root start_ARG italic_Îµ end_ARG ) â‹… âˆ¥ italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT âˆ¥. The argument is now finished by again using our knowledge of the distribution of x(tâˆ’1)superscriptğ‘¥ğ‘¡1x^{(t-1)}italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT to conclude that â€–x(tâˆ’1)â€–normsuperscriptğ‘¥ğ‘¡1\|x^{(t-1)}\|âˆ¥ italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT âˆ¥ and â€–x(t)â€–normsuperscriptğ‘¥ğ‘¡\|x^{(t)}\|âˆ¥ italic_x start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT âˆ¥ are within constant scalings of each other. Much of this analysis mirrors and simplifies the analysis in [IS24]. There, a semidefinite program is used to obtain a pseudoexpectation of a â€œcleanedâ€ version X^^ğ‘‹\hat{X}over^ start_ARG italic_X end_ARG of Yğ‘ŒYitalic_Y. The semidefinite program has formal variables for low-degree symmetric polynomials of X^^ğ‘‹\hat{X}over^ start_ARG italic_X end_ARG. It adds constraints to try to enforce that â€–X^â€–ğ—ˆğ—‰=Oâ¢(1)subscriptnorm^ğ‘‹ğ—ˆğ—‰ğ‘‚1\|\hat{X}\|_{\operatorname{\mathsf{op}}}=O(1)âˆ¥ over^ start_ARG italic_X end_ARG âˆ¥ start_POSTSUBSCRIPT sansserif_op end_POSTSUBSCRIPT = italic_O ( 1 ), that X^âˆ’Y^ğ‘‹ğ‘Œ\hat{X}-Yover^ start_ARG italic_X end_ARG - italic_Y be supported on a principal minor (by introducing indicator variables for â€œcleanâ€ rows and columns), as well as the constraint that some symmetric vector-valued polynomials in the entries of X^^ğ‘‹\hat{X}over^ start_ARG italic_X end_ARG have entries which are no larger than corresponding polynomials in Xğ‘‹Xitalic_X. The high-level sequence of arguments mirrors those outlined in (1) and the subsequent lines. We introduce some additional structure/arguments because our spectral cleaning step (for which we design a natural-in-hindsight spectral cleaning algorithm) deletes rows and columns. One advantage of the present argument over that in [IS24] is that it is unclear how to make a semidefinite program leverage the order statistics of vector-valued polynomials, so in our prior work we crudely enforce a bound on the infinity norm of the vectors, which gives rise to polyâ¢logâ¡npolyğ‘›\mathrm{poly}\log nroman_poly roman_log italic_n factors. Here we are able to circumvent this because we clip our iterates by hand."
https://arxiv.org/html/2411.02585v1,A Linear Time Gap-ETH-Tight Approximation Scheme forTSP in the Euclidean Plane,"The Traveling Salesman Problem (TSP) in the two-dimensional Euclidean plane is among the oldest and most famous NP-hard optimization problems. In breakthrough works, Arora [J. ACM 1998] and Mitchell [SICOMP 1999] gave the first polynomial time approximation schemes. The running time of their approximation schemes was improved by Rao and Smith [STOC 1998] to (1/Îµ)Oâ¢(1/Îµ)â¢nâ¢logâ¡nsuperscript1ğœ€ğ‘‚1ğœ€ğ‘›ğ‘›(1/\varepsilon)^{O(1/\varepsilon)}n\log n( 1 / italic_Îµ ) start_POSTSUPERSCRIPT italic_O ( 1 / italic_Îµ ) end_POSTSUPERSCRIPT italic_n roman_log italic_n. Bartal and Gottlieb [FOCS 2013] gave an approximation scheme of running time 2(1/Îµ)Oâ¢(1)â¢nsuperscript2superscript1ğœ€ğ‘‚1ğ‘›2^{(1/\varepsilon)^{O(1)}}n2 start_POSTSUPERSCRIPT ( 1 / italic_Îµ ) start_POSTSUPERSCRIPT italic_O ( 1 ) end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT italic_n, which is optimal in nğ‘›nitalic_n. Recently, Kisfaludi-Bak, Nederlof, and WÄ™grzycki [FOCS 2021] gave a 2Oâ¢(1/Îµ)â¢nâ¢logâ¡nsuperscript2ğ‘‚1ğœ€ğ‘›ğ‘›2^{O(1/\varepsilon)}n\log n2 start_POSTSUPERSCRIPT italic_O ( 1 / italic_Îµ ) end_POSTSUPERSCRIPT italic_n roman_log italic_n time approximation scheme, achieving the optimal running time in Îµğœ€\varepsilonitalic_Îµ under the Gap-ETH conjecture. In our work, we give a 2Oâ¢(1/Îµ)â¢nsuperscript2ğ‘‚1ğœ€ğ‘›2^{O(1/\varepsilon)}n2 start_POSTSUPERSCRIPT italic_O ( 1 / italic_Îµ ) end_POSTSUPERSCRIPT italic_n time approximation scheme, achieving the optimal running time both in nğ‘›nitalic_n and in Îµğœ€\varepsilonitalic_Îµ under the Gap-ETH conjecture.","The Traveling Salesman Problem (TSP) in the two-dimensional Euclidean plane is among the oldest and most famous NP-hard optimization problems. The problem is to find a shortest tour on a given set of nğ‘›nitalic_n points in â„2superscriptâ„2\mathbb{R}^{2}blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. The GÃ¶del-prize-winning approximation schemes for this problem due to Arora [Aro98] and Mitchell [Mit99] are among the most prominent results in approximation algorithms. They are classics in advanced algorithmic courses as well as in textbooks on approximation algorithms, optimization, and geometric algorithms [Vaz10, WS11, KV12, HP11, NS07]. The methods of Arora [Aro98] and Mitchell [Mit99] have numerous applications, see, e.g., [Aro03] for a survey. The running times of the approximation schemes in [Aro98] and [Mit99] are nâ¢(logâ¡n)Oâ¢(1/Îµ)ğ‘›superscriptğ‘›ğ‘‚1ğœ€n(\log n)^{O(1/\varepsilon)}italic_n ( roman_log italic_n ) start_POSTSUPERSCRIPT italic_O ( 1 / italic_Îµ ) end_POSTSUPERSCRIPT and nOâ¢(1/Îµ)superscriptğ‘›ğ‘‚1ğœ€n^{O(1/\varepsilon)}italic_n start_POSTSUPERSCRIPT italic_O ( 1 / italic_Îµ ) end_POSTSUPERSCRIPT, respectively. To improve their running times, Rao and Smith [RS98] designed a (1/Îµ)Oâ¢(1/Îµ)â¢nâ¢logâ¡nsuperscript1ğœ€ğ‘‚1ğœ€ğ‘›ğ‘›(1/\varepsilon)^{O(1/\varepsilon)}n\log n( 1 / italic_Îµ ) start_POSTSUPERSCRIPT italic_O ( 1 / italic_Îµ ) end_POSTSUPERSCRIPT italic_n roman_log italic_n approximation scheme using geometric spanners. Bartal and Gottlieb [BG13] gave a 2(1/Îµ)Oâ¢(1)â¢nsuperscript2superscript1ğœ€ğ‘‚1ğ‘›2^{(1/\varepsilon)^{O(1)}}n2 start_POSTSUPERSCRIPT ( 1 / italic_Îµ ) start_POSTSUPERSCRIPT italic_O ( 1 ) end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT italic_n time approximation scheme in the real-RAM model with atomic floor or mod operations. That running time is optimal in nğ‘›nitalic_n, though the dependency on Îµğœ€\varepsilonitalic_Îµ is not as good as in [RS98]. Recently, Kisfaludi-Bak, Nederlof, and WÄ™grzycki [KNW21] gave a 2Oâ¢(1/Îµ)â¢nâ¢logâ¡nsuperscript2ğ‘‚1ğœ€ğ‘›ğ‘›2^{O(1/\varepsilon)}n\log n2 start_POSTSUPERSCRIPT italic_O ( 1 / italic_Îµ ) end_POSTSUPERSCRIPT italic_n roman_log italic_n time approximation scheme, achieving the optimal running time in Îµğœ€\varepsilonitalic_Îµ under the Gap-ETH conjecture, though the dependency in nğ‘›nitalic_n is within a factor logâ¡nğ‘›\log nroman_log italic_n to the optimal. Among the above mentioned works, [Aro98, RS98, BG13, KNW21] generalize to dğ‘‘ditalic_d dimensions, for any fixed constant dâ‰¥2ğ‘‘2d\geq 2italic_d â‰¥ 2. The main open question in the above line of works is to achieve an approximation scheme with an optimal running time both in nğ‘›nitalic_n and in Îµğœ€\varepsilonitalic_Îµ. That question is central in many contexts. For example, for planar graphs, Klein [Kle08] gave a 2Oâ¢(1/Îµ)â¢nsuperscript2ğ‘‚1ğœ€ğ‘›2^{O(1/\varepsilon)}n2 start_POSTSUPERSCRIPT italic_O ( 1 / italic_Îµ ) end_POSTSUPERSCRIPT italic_n time approximation scheme for planar TSP, and Marx [Mar07] showed that the dependence on Îµğœ€\varepsilonitalic_Îµ in Kleinâ€™s algorithm is conditionally near-optimal. In our work, we settle the above open question for TSP in the Euclidean plane. Our main result is an approximation scheme whose running time is linear in nğ‘›nitalic_n and has an optimal dependence on Îµğœ€\varepsilonitalic_Îµ under the Gap-ETH conjecture. Theorem 1 (main theorem). There is a randomized (1+Îµ)1ğœ€(1+\varepsilon)( 1 + italic_Îµ )-approximation scheme for the Euclidean TSP in â„2superscriptâ„2\mathbb{R}^{2}blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT that runs in time 2Oâ¢(1/Îµ)â¢nsuperscript2ğ‘‚1ğœ€ğ‘›2^{O(1/\varepsilon)}n2 start_POSTSUPERSCRIPT italic_O ( 1 / italic_Îµ ) end_POSTSUPERSCRIPT italic_n in the real-RAM model with atomic floor operations. Remark. The choice of the model of computation needs care in order to achieve an algorithm with running time linear in nğ‘›nitalic_n. Indeed, Euclidean TSP requires Î©â¢(nâ¢logâ¡n)Î©ğ‘›ğ‘›\Omega(n\log n)roman_Î© ( italic_n roman_log italic_n ) time in the decision tree model, see, e.g., [DKS97, RS98]. Thus we use the real RAM model with an additional floor function as atomic operation. The same model was assumed in the open problem stated by Rao and Smith [RS98] regarding linear time computation and in the linear time algorithm of Bartal and Gottlieb [BG13].111Theorem I.1 of [BG13] uses the integer RAM model but requires additional assumptions. Similar to [BG13], our algorithm uses only a limited amount of the power offered by the model. The Gap-ETH hardness due to Kisfaludi-Bak, Nederlof, and WÄ™grzycki [KNW21] extends to that model. See Appendix A for details. 1.1 Preliminaries and Previous Techniques Let PâŠ†â„2ğ‘ƒsuperscriptâ„2P\subseteq\mathbb{R}^{2}italic_P âŠ† blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT denote a set of nğ‘›nitalic_n points. By preprocessing the input instance, we may assume that PâŠ†{0,â€¦,L}2ğ‘ƒsuperscript0â€¦ğ¿2P\subseteq\{0,\dots,L\}^{2}italic_P âŠ† { 0 , â€¦ , italic_L } start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT for some integer L=Oâ¢(n/Îµ)ğ¿ğ‘‚ğ‘›ğœ€L=O(n/\varepsilon)italic_L = italic_O ( italic_n / italic_Îµ ) that is a power of 2. This preprocessing step can be done in Oâ¢(n)ğ‘‚ğ‘›O(n)italic_O ( italic_n ) time [BG13]. For a segment Iğ¼Iitalic_I in â„2superscriptâ„2\mathbb{R}^{2}blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, let lengthâ¢(I)lengthğ¼\mathrm{length}(I)roman_length ( italic_I ) denote the length of Iğ¼Iitalic_I. For any path Ï€ğœ‹\piitalic_Ï€ of points x1,x2,â€¦,xmsubscriptğ‘¥1subscriptğ‘¥2â€¦subscriptğ‘¥ğ‘šx_{1},x_{2},\dots,x_{m}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_x start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT in â„2superscriptâ„2\mathbb{R}^{2}blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT where mâˆˆâ„•ğ‘šâ„•m\in\mathbb{N}italic_m âˆˆ blackboard_N, define costâ¢(Ï€)=âˆ‘i=1mâˆ’1distâ¢(xi,xi+1)costğœ‹superscriptsubscriptğ‘–1ğ‘š1distsubscriptğ‘¥ğ‘–subscriptğ‘¥ğ‘–1\mathrm{cost}(\pi)=\sum_{i=1}^{m-1}\mathrm{dist}(x_{i},x_{i+1})roman_cost ( italic_Ï€ ) = âˆ‘ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m - 1 end_POSTSUPERSCRIPT roman_dist ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT ). We say that Ï€ğœ‹\piitalic_Ï€ is a tour if x1=xmsubscriptğ‘¥1subscriptğ‘¥ğ‘šx_{1}=x_{m}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT. Let OPTOPT\mathrm{OPT}roman_OPT denote an optimal tour on Pğ‘ƒPitalic_P. Let optopt\mathrm{opt}roman_opt denote costâ¢(OPT)costOPT\mathrm{cost}(\mathrm{OPT})roman_cost ( roman_OPT ). 1.1.1 Dissection and Quadtree We follow the notations in [KNW21]. Let dâ‰¥2ğ‘‘2d\geq 2italic_d â‰¥ 2 be a constant. We pick a1,â€¦,adâˆˆ{1,â€¦,L}subscriptğ‘1â€¦subscriptğ‘ğ‘‘1â€¦ğ¿a_{1},\dots,a_{d}\in\{1,\dots,L\}italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_a start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT âˆˆ { 1 , â€¦ , italic_L } independently and uniformly at random and define ğ’‚:=(a1,â€¦,ad)âˆˆ{0,â€¦,L}dassignğ’‚subscriptğ‘1â€¦subscriptğ‘ğ‘‘superscript0â€¦ğ¿ğ‘‘\bm{a}:=(a_{1},\ldots,a_{d})\in\{0,\dots,L\}^{d}bold_italic_a := ( italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_a start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) âˆˆ { 0 , â€¦ , italic_L } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT. Consider the hypercube C(ğ’‚):=Ã—i=1d[âˆ’ai+1/2,2Lâˆ’ai+1/2].C(\bm{a}):=\bigtimes\limits_{i=1}^{d}[-a_{i}+1/2,2L-a_{i}+1/2].italic_C ( bold_italic_a ) := Ã— start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT [ - italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + 1 / 2 , 2 italic_L - italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + 1 / 2 ] . Note that Câ¢(ğ’‚)ğ¶ğ’‚C(\bm{a})italic_C ( bold_italic_a ) has side length 2â¢L2ğ¿2L2 italic_L and each point from Pğ‘ƒPitalic_P is contained in Câ¢(ğ’‚)ğ¶ğ’‚C(\bm{a})italic_C ( bold_italic_a ) since PâŠ†{0,â€¦,L}dğ‘ƒsuperscript0â€¦ğ¿ğ‘‘P\subseteq\{0,\dots,L\}^{d}italic_P âŠ† { 0 , â€¦ , italic_L } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT. We define the dissection Dâ¢(ğ’‚)ğ·ğ’‚D(\bm{a})italic_D ( bold_italic_a ) of Câ¢(ğ’‚)ğ¶ğ’‚C(\bm{a})italic_C ( bold_italic_a ) to be a tree constructed recursively, where each vertex is associated with a hypercube in â„dsuperscriptâ„ğ‘‘\mathbb{R}^{d}blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT. The root of the tree is associated with Câ¢(ğ’‚)ğ¶ğ’‚C(\bm{a})italic_C ( bold_italic_a ). Each non-leaf vertex of the tree that is associated with a hypercube Ã—i=1d[li,ui]superscriptsubscriptğ‘–1ğ‘‘absentsubscriptğ‘™ğ‘–subscriptğ‘¢ğ‘–\bigtimes_{i=1}^{d}[l_{i},u_{i}]Ã— start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT [ italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ] has 2dsuperscript2ğ‘‘2^{d}2 start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT children with which we associate hypercubes Ã—i=1dIisuperscriptsubscriptğ‘–1ğ‘‘absentsubscriptğ¼ğ‘–\bigtimes_{i=1}^{d}I_{i}Ã— start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, where Iisubscriptğ¼ğ‘–I_{i}italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is either [li,(li+ui)/2]subscriptğ‘™ğ‘–subscriptğ‘™ğ‘–subscriptğ‘¢ğ‘–2[l_{i},(l_{i}+u_{i})/2][ italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , ( italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) / 2 ] or [(li+ui)/2,ui]subscriptğ‘™ğ‘–subscriptğ‘¢ğ‘–2subscriptğ‘¢ğ‘–[(l_{i}+u_{i})/2,u_{i}][ ( italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) / 2 , italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ]. Each leaf vertex of the tree is associated with a hypercube of unit length. A quadtree is obtained from Dâ¢(ğ’‚)ğ·ğ’‚D(\bm{a})italic_D ( bold_italic_a ), except we stop the recursive partitioning as soon as the associated hypercube of a vertex contains at most one point from Pğ‘ƒPitalic_P. Each hypercube associated with a vertex in the quadtree is called a cell. We say that a cell Cğ¶Citalic_C is redundant if it has a child that contains the same set of input points as the parent of Cğ¶Citalic_C. A redundant path is a maximal ancestor-descendant path in the tree whose internal vertices are redundant. The compressed quadtree is obtained from the quadtree by removing all the empty children of redundant cells and replacing the redundant paths with edges. In the resulting tree some internal cells may have a single child; we call these compressed cells. It is well-known that compressed quadtrees have Oâ¢(n)ğ‘‚ğ‘›O(n)italic_O ( italic_n ) vertices, see, e.g., [BG13]. 1.1.2 Approach of Arora [Aro98] In the quadtree decomposition, each side of a cell has Oâ¢((logâ¡n)/Îµ)ğ‘‚ğ‘›ğœ€O((\log n)/\varepsilon)italic_O ( ( roman_log italic_n ) / italic_Îµ ) equidistant portals. Lemma 2 (Aroraâ€™s Patching Lemma, [Aro98]). Let Iğ¼Iitalic_I be any line segment and Ï€ğœ‹\piitalic_Ï€ be a closed path that crosses Iğ¼Iitalic_I at least thrice. Then there exist line segments on Iğ¼Iitalic_I whose total length is at most 6â‹…lengthâ¢(I)â‹…6lengthğ¼6\cdot\mathrm{length}(I)6 â‹… roman_length ( italic_I ) and whose addition to Ï€ğœ‹\piitalic_Ï€ changes it into a closed path Ï€â€²superscriptğœ‹â€²\pi^{\prime}italic_Ï€ start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT that crosses Iğ¼Iitalic_I at most twice. Using Lemma 2, Arora prove a structure theorem, which states that there is a near-optimal tour that crosses each side of a cell Oâ¢(1/Îµ)ğ‘‚1ğœ€O(1/\varepsilon)italic_O ( 1 / italic_Îµ ) times and only through portals. While such a near-optimal tour does not necessarily exist for a fixed quadtree, a randomly shifted quadtree works with high probability. Thus the algorithm first computes a randomly shifted quadtree, and then apply a dynamic program to compute the best solution satisfying the properties in the structure theorem. 1.1.3 Approach of Kisfaludi-Bak, Nederlof, and WÄ™grzycki [KNW21] To achieve Gap-ETH tight running time, Kisfaludi-Bak, Nederlof, and WÄ™grzycki introduce a powerful technique of the sparsity-sensitive patching: for a side of a cell that is crossed by a tour at 1<kâ‰¤Oâ¢(1/Îµ)1ğ‘˜ğ‘‚1ğœ€1<k\leq O(1/\varepsilon)1 < italic_k â‰¤ italic_O ( 1 / italic_Îµ ) crossings, modify the tour by mapping each crossing to the nearest portal from the set of gâ¢(k)ğ‘”ğ‘˜g(k)italic_g ( italic_k ) equidistant portals. Here gâ¢(k)=Î˜â¢(1/(Îµ2â¢k))ğ‘”ğ‘˜Î˜1superscriptğœ€2ğ‘˜g(k)=\Theta(1/(\varepsilon^{2}k))italic_g ( italic_k ) = roman_Î˜ ( 1 / ( italic_Îµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_k ) ) is a granularity parameter that depends on kğ‘˜kitalic_k. Kisfaludi-Bak, Nederlof, and WÄ™grzycki analyze the patching cost Î±iâ¢(x)subscriptğ›¼ğ‘–ğ‘¥\alpha_{i}(x)italic_Î± start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_x ) for a crossing xğ‘¥xitalic_x on a line hâ„hitalic_h of level iğ‘–iitalic_i as follows: Î±iâ¢(x)={proâ¢(x),if proâ¢(x)â‰¤L2iâ¢r(L2iâ¢r)2â¢1proâ¢(x),if proâ¢(x)>L2iâ¢r,subscriptğ›¼ğ‘–ğ‘¥casesproğ‘¥if proâ¢(x)â‰¤L2iâ¢rsuperscriptğ¿superscript2ğ‘–ğ‘Ÿ21proğ‘¥if proâ¢(x)>L2iâ¢r\alpha_{i}(x)=\begin{cases}\mathrm{pro}(x),&\text{if $\mathrm{pro}(x)\leq\frac% {L}{2^{i}r}$}\\ \left(\frac{L}{2^{i}r}\right)^{2}\frac{1}{\mathrm{pro}(x)},&\text{if $\mathrm{% pro}(x)>\frac{L}{2^{i}r}$},\end{cases}italic_Î± start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_x ) = { start_ROW start_CELL roman_pro ( italic_x ) , end_CELL start_CELL if roman_pro ( italic_x ) â‰¤ divide start_ARG italic_L end_ARG start_ARG 2 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT italic_r end_ARG end_CELL end_ROW start_ROW start_CELL ( divide start_ARG italic_L end_ARG start_ARG 2 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT italic_r end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT divide start_ARG 1 end_ARG start_ARG roman_pro ( italic_x ) end_ARG , end_CELL start_CELL if roman_pro ( italic_x ) > divide start_ARG italic_L end_ARG start_ARG 2 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT italic_r end_ARG , end_CELL end_ROW where r=Oâ¢(1/Îµ)ğ‘Ÿğ‘‚1ğœ€r=O(1/\varepsilon)italic_r = italic_O ( 1 / italic_Îµ ) and proâ¢(x)proğ‘¥\mathrm{pro}(x)roman_pro ( italic_x ) is the distance between xğ‘¥xitalic_x and the previous crossing on line hâ„hitalic_h. The above property on Î±iâ¢(x)subscriptğ›¼ğ‘–ğ‘¥\alpha_{i}(x)italic_Î± start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_x ) is key to show that the sparsity-sensitive patching leads to a near-optimal solution. Applying the sparsity-sensitive patching has a condition: the number of crossings kğ‘˜kitalic_k is such that kâ‰ 1ğ‘˜1k\neq 1italic_k â‰  1. This condition is necessary: when k=1ğ‘˜1k=1italic_k = 1, Kisfaludi-Bak, Nederlof, and WÄ™grzycki provide an example showing that moving the single crossing to the closest portal does not lead to a near-optimal solution. To deal with single crossings, their approach is to make use of a spanner. The Oâ¢(nâ¢logâ¡n)ğ‘‚ğ‘›ğ‘›O(n\log n)italic_O ( italic_n roman_log italic_n ) running time in [KNW21] is due to the computation of the spanner. Kisfaludi-Bak, Nederlof, and WÄ™grzycki raised an open question whether it is possible to improve the running time by a factor of logâ¡nğ‘›\log nroman_log italic_n by using some new ideas to handle single crossings. In our work, we answer their open question positively for TSP in â„2superscriptâ„2\mathbb{R}^{2}blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. See Section 1.2 for an overview of our approach. 1.1.4 Linear-time 2-approximation by Bartal and Gottlieb [BG13] One step in our approach requires computing a 2-approximate solution. This can be achieved in Oâ¢(n)ğ‘‚ğ‘›O(n)italic_O ( italic_n ) time, by setting Îµ=1ğœ€1\varepsilon=1italic_Îµ = 1 in the main result of Bartal and Gottlieb [BG13]: Lemma 3 (corollary of Bartal and Gottlieb [BG13]). There is randomized 2-approximation algorithm for the TSP in â„2superscriptâ„2\mathbb{R}^{2}blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT that runs in time Oâ¢(n)ğ‘‚ğ‘›O(n)italic_O ( italic_n ) in the real-RAM model with atomic floor or mod operations. 1.1.5 Portals Definition 4 (grid). For a segment Fğ¹Fitalic_F and an integer mğ‘šmitalic_m, let gridâ¢(F,m)gridğ¹ğ‘š\mathrm{grid}(F,m)roman_grid ( italic_F , italic_m ) denote an mğ‘šmitalic_m-regular set of portals on Fğ¹Fitalic_F. Note that gridâ¢(F,m)gridğ¹ğ‘š\mathrm{grid}(F,m)roman_grid ( italic_F , italic_m ) contains the two endpoints of Fğ¹Fitalic_F. The following function gâ¢(k)ğ‘”ğ‘˜g(k)italic_g ( italic_k ) computes the number of portals to place on a side, given the number kğ‘˜kitalic_k of crossing points on that side. Definition 5 (adaptation from [KNW21]). Let rğ‘Ÿritalic_r be a positive integer. For a positive integer kğ‘˜kitalic_k, let qğ‘qitalic_q denote the smallest integer such that qâ‰¥r2/(4â¢k)ğ‘superscriptğ‘Ÿ24ğ‘˜q\geq r^{2}/(4k)italic_q â‰¥ italic_r start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT / ( 4 italic_k ). Let pğ‘pitalic_p denote the smallest integer power of 2 such that pâ‰¥qğ‘ğ‘p\geq qitalic_p â‰¥ italic_q. Then we define gâ¢(k):=passignğ‘”ğ‘˜ğ‘g(k):=pitalic_g ( italic_k ) := italic_p. 1.2 Overview of Our Methods In this section, we give an overview of our methods in the proof of Theorem 1. Our approach is based on the approach of Kisfaludi-Bak, Nederlof, and WÄ™grzycki [KNW21] and uses several new ideas. To improve upon [KNW21], a main difficulty is to deal with the single crossing case, i.e., when a side has a single crossing with a tour, see Section 1.1.3. That is the reason why the running time in [KNW21] requires a factor logâ¡(n)ğ‘›\log(n)roman_log ( italic_n ). In order to get rid of the logâ¡(n)ğ‘›\log(n)roman_log ( italic_n ) factor, we deal with the single crossing case in a new way. We start by computing a 2-approximate solution Tğ‘‡Titalic_T in linear time, using an algorithm of Bartal and Gottlieb [BG13], see Section 1.1.4. The solution Tğ‘‡Titalic_T helps us in dealing with single crossings. 1.2.1 New set of portals We define a new set of portals for each side Fğ¹Fitalic_F, consisting of the portals in [KNW21], plus possibly an additional point, which is a crossing between Tğ‘‡Titalic_T and Fğ¹Fitalic_F if exists.222When Tğ‘‡Titalic_T and Fğ¹Fitalic_F has multiple crossings, an arbitrary crossing would work. See Definition 6. Definition 6. Let r=Oâ¢(1/Îµ)ğ‘Ÿğ‘‚1ğœ€r=O(1/\varepsilon)italic_r = italic_O ( 1 / italic_Îµ ). We say that a side Fğ¹Fitalic_F of a cell is Tğ‘‡Titalic_T-crossing if Tğ‘‡Titalic_T crosses Fğ¹Fitalic_F through at least one point. If Fğ¹Fitalic_F is Tğ‘‡Titalic_T-crossing, we let xFsubscriptğ‘¥ğ¹x_{F}italic_x start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT denote an arbitrary crossing point between Fğ¹Fitalic_F and Tğ‘‡Titalic_T. For a positive integer kğ‘˜kitalic_k, define Zâ¢(F,k):={gridâ¢(F,gâ¢(k))âˆª{xF},if F is T-crossing;gridâ¢(F,gâ¢(k)),otherwise,assignğ‘ğ¹ğ‘˜casesgridğ¹ğ‘”ğ‘˜subscriptğ‘¥ğ¹if F is T-crossing;gridğ¹ğ‘”ğ‘˜otherwise,Z(F,k):=\begin{cases}\mathrm{grid}(F,g(k))\cup\{x_{F}\},&\text{if $F$ is $T$-% crossing;}\\ \mathrm{grid}(F,g(k)),&\text{otherwise,}\end{cases}italic_Z ( italic_F , italic_k ) := { start_ROW start_CELL roman_grid ( italic_F , italic_g ( italic_k ) ) âˆª { italic_x start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT } , end_CELL start_CELL if italic_F is italic_T -crossing; end_CELL end_ROW start_ROW start_CELL roman_grid ( italic_F , italic_g ( italic_k ) ) , end_CELL start_CELL otherwise, end_CELL end_ROW where the function gğ‘”gitalic_g and gridgrid\mathrm{grid}roman_grid are defined in Section 1.1.5. 1.2.2 rğ‘Ÿritalic_r-basic Tour We introduce a new notion of rğ‘Ÿritalic_r-basic tour (Definition 7), which is analogous to rğ‘Ÿritalic_r-simple tour in [KNW21]. At a high level, a tour that respects the portals in Definition 6 is called rğ‘Ÿritalic_r-basic. For purely technical reasons, we will define \ACâ¢(Ï€,C)\ACğœ‹ğ¶\AC(\pi,C)( italic_Ï€ , italic_C ) to be a subset of the crossings between a tour Ï€ğœ‹\piitalic_Ï€ and the boundary of a cell Cğ¶Citalic_C as follows. For a tour Ï€ğœ‹\piitalic_Ï€ and a cell Cğ¶Citalic_C, let I0subscriptğ¼0I_{0}italic_I start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT denote the set of curves obtained by restricting Ï€ğœ‹\piitalic_Ï€ to the interior of Cğ¶Citalic_C. Let IâŠ†I0ğ¼subscriptğ¼0I\subseteq I_{0}italic_I âŠ† italic_I start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT be the set of curves QâˆˆI0ğ‘„subscriptğ¼0Q\in I_{0}italic_Q âˆˆ italic_I start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT such that Qğ‘„Qitalic_Q visits at least one input point from Pğ‘ƒPitalic_P. Let \ACâ¢(Ï€,C)\ACğœ‹ğ¶\AC(\pi,C)( italic_Ï€ , italic_C ) denote the (multi-)set of points consisting of the two endpoints of all curves in Iğ¼Iitalic_I. Note that when every curve in I0subscriptğ¼0I_{0}italic_I start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT visits some point from Pğ‘ƒPitalic_P, then \ACâ¢(Ï€,C)\ACğœ‹ğ¶\AC(\pi,C)( italic_Ï€ , italic_C ) is simply the set of crossings between Ï€ğœ‹\piitalic_Ï€ and the boundary of Cğ¶Citalic_C. Definition 7 (rğ‘Ÿritalic_r-basic). We say that a tour Ï€ğœ‹\piitalic_Ï€ is rğ‘Ÿritalic_r-basic if for every side Fğ¹Fitalic_F of a cell Cğ¶Citalic_C in Dâ¢(ğ’‚)ğ·ğ’‚D(\bm{a})italic_D ( bold_italic_a ), we have \ACâ¢(Ï€,C)âˆ©FâŠ†Zâ¢(F,|\ACâ¢(Ï€,C)âˆ©F|)\ACğœ‹ğ¶ğ¹ğ‘ğ¹\ACğœ‹ğ¶ğ¹\AC(\pi,C)\cap F\subseteq Z(F,|\AC(\pi,C)\cap F|)( italic_Ï€ , italic_C ) âˆ© italic_F âŠ† italic_Z ( italic_F , | ( italic_Ï€ , italic_C ) âˆ© italic_F | ). Moreover, every element in \ACâ¢(Ï€,C)\ACğœ‹ğ¶\AC(\pi,C)( italic_Ï€ , italic_C ) has multiplicity at most 2. 1.2.3 Structure Theorem We prove a structure theorem (Theorem 8), which shows the existence of an rğ‘Ÿritalic_r-basic solution that is near optimal. Theorem 8 (Structure Theorem). Let Pğ‘ƒPitalic_P be a set of nğ‘›nitalic_n points in â„2superscriptâ„2\mathbb{R}^{2}blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. Let ğšğš\bm{a}bold_italic_a be a random shift. For any large enough integer rğ‘Ÿritalic_r, there is an rğ‘Ÿritalic_r-basic tour OPTâ€²superscriptOPTâ€²\mathrm{OPT}^{\prime}roman_OPT start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT on PâŠ†â„2ğ‘ƒsuperscriptâ„2P\subseteq\mathbb{R}^{2}italic_P âŠ† blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT such that ğ”¼ğ’‚â¢[costâ¢(OPTâ€²)]â‰¤(1+Oâ¢(1/r))â‹…opt.subscriptğ”¼ğ’‚delimited-[]costsuperscriptOPTâ€²â‹…1ğ‘‚1ğ‘Ÿopt\mathbb{E}_{\bm{a}}[\mathrm{cost}(\mathrm{OPT}^{\prime})]\leq(1+O(1/r))\cdot% \mathrm{opt}.blackboard_E start_POSTSUBSCRIPT bold_italic_a end_POSTSUBSCRIPT [ roman_cost ( roman_OPT start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) ] â‰¤ ( 1 + italic_O ( 1 / italic_r ) ) â‹… roman_opt . The proof of the Structure Theorem is a main novelty in this paper. In the rest of this section, we sketch high level ideas of the proof. To begin with, when Fğ¹Fitalic_F has a single crossing with OPTOPT\mathrm{OPT}roman_OPT and a crossing with Tğ‘‡Titalic_T, then we move the crossing of OPTOPT\mathrm{OPT}roman_OPT to the crossing of Tğ‘‡Titalic_T. We can show that this cost is negligible (Lemma 12). Therefore, the non-trivial remaining case is when Fğ¹Fitalic_F has a single crossing with OPTOPT\mathrm{OPT}roman_OPT but no crossing with Tğ‘‡Titalic_T. We call such a side Fğ¹Fitalic_F interesting. A crucial observation is that, when Fğ¹Fitalic_F is interesting, we are able to afford moving the single crossing between Fğ¹Fitalic_F and OPTOPT\mathrm{OPT}roman_OPT to the closest portal. This is proved in Lemma 14, which is our main technical contribution. To show Lemma 14, for each edge eâˆˆOPTğ‘’OPTe\in\mathrm{OPT}italic_e âˆˆ roman_OPT, we analyze the cost due to its crossings with interesting sides. For interesting sides that are shorter than the longest side crossing eğ‘’eitalic_e, we analyze the cost using the notion of active crossings and by considering horizontal connections and vertical connections simultaneously, see Section 2.4. For the longest interesting side crossing eğ‘’eitalic_e, we bound the connection cost in a creative way. To begin with, we define badly cut edges (Definition 16). This definition is inspired by Cohen-Addad [CA20]. Our analysis on badly cut edges is completely different from [CA20]. We now sketch high level ideas of our analysis. The non-trivial case is when a badly cut edge eğ‘’eitalic_e crosses an interesting side. In this case, we distinguish whether eğ‘’eitalic_e is critical (Fig. 3) or non-critical (Fig. 4), depending on which sides of the cell has connections in Tğ‘‡Titalic_T with the endpoints of eğ‘’eitalic_e. For critical edges, our analysis (Lemma 19) is based on [KNW21] and uses several new ingredients. To prove Lemma 19, first, we observe that, in [KNW21], a single crossing is problematic because the proximity proâ¢(x)proğ‘¥\mathrm{pro}(x)roman_pro ( italic_x ) of that crossing may be arbitrarily large. To overcome that difficulty, we show that for a critical edge, the proximity of a crossing on a perpendicular side is bounded. Cares are needed, because in [KNW21], for a single crossing xğ‘¥xitalic_x along a side, the value Î±iâ¢(x)subscriptğ›¼ğ‘–ğ‘¥\alpha_{i}(x)italic_Î± start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_x ) is undefined. We define that value to be the interportal distance with small probability, by analyzing horizontal and vertical lines simultaneously. This is in contrast to the analysis in [KNW21] that is only on horizontal lines or only on vertical lines. Finally, for non-critical edges, we bound their connection cost using an elementary but delicate argument, see Lemma 20. The complete proof of the Structure Theorem is given in Section 2 . 1.2.4 Algorithm From the Structure Theorem, in order to compute a near-optimal solution, it suffices to compute an optimal rğ‘Ÿritalic_r-basic solution using a standard dynamic program discussed in Section 3. The description of our algorithm for Theorem 1 is given in Algorithm 1. Algorithm 1 Algorithm for TSP in the Euclidean plane 1:Compute a 2-approximate solution Tğ‘‡Titalic_T according to Lemma 3 2:Pick a random shift ğ’‚ğ’‚\bm{a}bold_italic_a and compute a compressed quadtree from the dissection Dâ¢(ğ’‚)Dğ’‚\mathrm{D}(\bm{a})roman_D ( bold_italic_a ) 3:for each side Fğ¹Fitalic_F of each cell Cğ¶Citalic_C do 4: if Fğ¹Fitalic_F is Tğ‘‡Titalic_T-crossing then 5: xFâ†â†subscriptğ‘¥ğ¹absentx_{F}\leftarrowitalic_x start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT â† an arbitrary crossing point between Tğ‘‡Titalic_T and Fğ¹Fitalic_F 6:Use a dynamic program to compute an optimal rğ‘Ÿritalic_r-basic solution. The running time analysis of Algorithm 1 is given in Section 4. Theorem 1 follows. Open problem. It is an interesting open question whether our approach can be extended to achieve a linear-time Gap-ETH tight approximation scheme in dğ‘‘ditalic_d dimension, for fixed constant dğ‘‘ditalic_d."
https://arxiv.org/html/2411.02422v1,About the Kannanâ€“Bachem algorithm,"The Smith reduction is a basic tool when analyzing integer matrices up to equivalence, and the Kannan-Bachem (KB) algorithm is the first polynomial algorithm computing such a reduction. Using this algorithm in complicated situations where the rank of the studied matrix is not maximal revealed an unexpected obstacle in the algorithm. This difficulty is described, analyzed, a simple solution is given to overcome it, finally leading to a general organization of the KB algorithm, simpler than the original one, efficient and having a general scope.An equivalent algorithm is used by the Magma program, without any detailed explanation, without any reference. The present text could so be useful.","The Smith reduction of an nÃ—mğ‘›ğ‘šn\times mitalic_n Ã— italic_m integer matrix d:â„¤mâ†’â„¤n:ğ‘‘â†’superscriptâ„¤ğ‘šsuperscriptâ„¤ğ‘›d:\mathbb{Z}^{m}\rightarrow\mathbb{Z}^{n}italic_d : blackboard_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT â†’ blackboard_Z start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT is a diagonal matrix s:â„¤mâ†’â„¤n:ğ‘ â†’superscriptâ„¤ğ‘šsuperscriptâ„¤ğ‘›s:\mathbb{Z}^{m}\rightarrow\mathbb{Z}^{n}italic_s : blackboard_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT â†’ blackboard_Z start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT satisfying the following conditions: â€¢ The matrices sğ‘ sitalic_s and dğ‘‘ditalic_d are equivalent, that is, there exist two invertible matrices u:â„¤mâ†’â„¤m:ğ‘¢â†’superscriptâ„¤ğ‘šsuperscriptâ„¤ğ‘šu:\mathbb{Z}^{m}\rightarrow\mathbb{Z}^{m}italic_u : blackboard_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT â†’ blackboard_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT and v:â„¤nâ†’â„¤n:ğ‘£â†’superscriptâ„¤ğ‘›superscriptâ„¤ğ‘›v:\mathbb{Z}^{n}\rightarrow\mathbb{Z}^{n}italic_v : blackboard_Z start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT â†’ blackboard_Z start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT with s=vâ¢dâ¢uğ‘ ğ‘£ğ‘‘ğ‘¢s=vduitalic_s = italic_v italic_d italic_u. â€¢ The non-null entries d1,â€¦,dksubscriptğ‘‘1â€¦subscriptğ‘‘ğ‘˜d_{1},\ldots,d_{k}italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT of the sğ‘ sitalic_s-diagonal are positive and satisfy the divisor condition: every entry disubscriptğ‘‘ğ‘–d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT divides the next one di+1subscriptğ‘‘ğ‘–1d_{i+1}italic_d start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT; in particular the last one dksubscriptğ‘‘ğ‘˜d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT divides the possible 0 in position k+1ğ‘˜1k+1italic_k + 1 on the diagonal if the rank kğ‘˜kitalic_k is less than mğ‘šmitalic_m and nğ‘›nitalic_n. The Smith reduction of integer matrices is used in many domains, in particular intensively used in computational Algebraic Topology. Calculating a homology group often consists in determining two boundary integer matrices dğ‘‘ditalic_d and dâ€²superscriptğ‘‘â€²d^{\prime}italic_d start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT satisfying dâ€²â¢d=0superscriptğ‘‘â€²ğ‘‘0d^{\prime}d=0italic_d start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT italic_d = 0 and the looked-for homology group is the quotient kerâ¡dâ€²/imâ¡dkernelsuperscriptğ‘‘â€²imğ‘‘\ker d^{\prime}/\operatorname{im}droman_ker italic_d start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT / roman_im italic_d. Then the Smith reductions of the matrices dğ‘‘ditalic_d and dâ€²superscriptğ‘‘â€²d^{\prime}italic_d start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT directly give the corresponding homology group. In Constructive Algebraic Topology [12], sophisticated computations often needing several weeks or months of runtime on powerful computers finally produce such matrices dğ‘‘ditalic_d and dâ€²superscriptğ‘‘â€²d^{\prime}italic_d start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT, the corresponding homology group kerâ¡dâ€²/imâ¡dkernelsuperscriptğ‘‘â€²imğ‘‘\ker d^{\prime}/\operatorname{im}droman_ker italic_d start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT / roman_im italic_d being some homology or homotopy group unreachable by other means. Recently, such a calculation produced an integer matrix T8subscriptğ‘‡8T_{8}italic_T start_POSTSUBSCRIPT 8 end_POSTSUBSCRIPT of size 684Ã—19956841995684\times 1995684 Ã— 1995 and a matrix T9subscriptğ‘‡9T_{9}italic_T start_POSTSUBSCRIPT 9 end_POSTSUBSCRIPT of size 1995Ã—5796199557961995\times 57961995 Ã— 5796 with T8â¢T9=0subscriptğ‘‡8subscriptğ‘‡90T_{8}T_{9}=0italic_T start_POSTSUBSCRIPT 8 end_POSTSUBSCRIPT italic_T start_POSTSUBSCRIPT 9 end_POSTSUBSCRIPT = 0, the final hoped-for result being the homology group kerâ¡T8/imâ¡T9kernelsubscriptğ‘‡8imsubscriptğ‘‡9\ker T_{8}/\operatorname{im}T_{9}roman_ker italic_T start_POSTSUBSCRIPT 8 end_POSTSUBSCRIPT / roman_im italic_T start_POSTSUBSCRIPT 9 end_POSTSUBSCRIPT. A careful implementation of the KB algorithm was available in our environment, giving for example the Smith reduction of T8subscriptğ‘‡8T_{8}italic_T start_POSTSUBSCRIPT 8 end_POSTSUBSCRIPT in 4 seconds. But the same algorithm used for T9subscriptğ‘‡9T_{9}italic_T start_POSTSUBSCRIPT 9 end_POSTSUBSCRIPT failed. After a few days of runtime without any output, it was obvious a devil was hidden somewhere. It is well known that the first naive algorithms computing the Smith reductions very quickly generate huge intermediate integers with thousands of digits, often making it impossible for the calculation to terminate in reasonable time. The Smith reduction consists in using elementary operations on the studied matrix, without changing its equivalence class, to cancel the non-diagonal entries of the matrix. Kannan and Bachem in their article [8] showed how a careful and simple organization of the order of the entries to be cancelled gives a polynomial algorithm, avoiding the combinatorial explosion of the naive methods. The Hermite and Smith reductions in [8] are obtained only for square invertible matrices, but obvious adaptations extend the scope of the KB algorithm to the most general situation, for rectangular matrices of arbitrary rank. A careful tracing of our implementation of the KB algorithm, detailed in this text, revealed in fact the devil was hidden in these â€œobviousâ€ adaptations to extend the KB organisation of [8] from the square non-singular case to arbitrary matrices, rectangular and arbitrary rank. Once this point is identified, it is easy (obvious!) to add a small complement in the organization of the â€œgeneralizedâ€ KB algorithm to dramatically improve our implementation in the general case. For example for our matrix T9subscriptğ‘‡9T_{9}italic_T start_POSTSUBSCRIPT 9 end_POSTSUBSCRIPT, then the result is obtained in a few minutes. Two symmetric Hermite reductions are defined. For a square matrix, let us call HNF-1 the column-style reduction giving a lower triangular matrix, and HNF-2 the row-style reduction giving an upper triangular matrix. In the original KB algorithm for the Smith reduction, the HNF-1 reduction was privileged, with a sort of minimal potion of HNF-2. It happens the gap in our first erroneous generalization of the KB algorithm was fixed thanks to an extra dose of HNF-2. This relatively complicated mixture of HNF-1 and HNF-2 gives another idea. Starting with a rectangular matrix M0subscriptğ‘€0M_{0}italic_M start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT of arbitrary rank, the HNF-1 reduction produces a first matrix M1subscriptğ‘€1M_{1}italic_M start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, lower triangular above a rectangle. Now let us simply apply the HNF-2 reduction to M1subscriptğ‘€1M_{1}italic_M start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, this produces a matrix M2subscriptğ‘€2M_{2}italic_M start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, upper triangular. Experience shows it is in general much more â€œreducedâ€ than M1subscriptğ‘€1M_{1}italic_M start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. Why not continue this game? We apply now HNF-1 to the last matrix M2subscriptğ‘€2M_{2}italic_M start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, obtaining M3subscriptğ‘€3M_{3}italic_M start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT, and so on. We prove that this process converges toward a diagonal matrix immediately giving the Smith reduction. The same proof like in [8] establishes this version of the KB algorithm is also polynomial, and experience shows after applying this method to numerous examples that it is the fastest one. Furthermore programming this version is very simple. Allowing easy extensions to other situations, for example for matrices of polynomials. This organization of the KB algorithm does not use any modular technology, it uses only left and right multiplications by unimodular matrices, mainly some appropriate BÃ©zout matrices, so that if the matrices uğ‘¢uitalic_u and vğ‘£vitalic_v satisfying s=vâ¢dâ¢uğ‘ ğ‘£ğ‘‘ğ‘¢s=vduitalic_s = italic_v italic_d italic_u are desired, they can be easily determined along this version of the KB algorithm, like in the original KB algorithm. Cf in [5, Section IV-2] the complementary calculations which are necessary if the Smith form has been obtained via modular reduction. Along the same lines, see the article [7] how the research of the right modulus can be sophisticated, even in favorable situations, in case of small valence, which cannot be applied here. See also the comments of the author after [4, Algorithm 2.4.8]. This point is important in constructive homology, when an explicit â„¤â„¤\mathbb{Z}blackboard_Z-cycle is required for some homology class, the very basis of constructive Algebraic Topology, see Section 4.4. About our â€œnumerousâ€ examples, an error would consist in testing the various versions of the algorithm with banal random matrices. Such matrices are generally too simple with respect to the possible difficulties. The same for the matrices coming from elementary contexts in algebraic topology, typically the homology groups of simplicial complexes; even when these matrices are geant, the Smith reduction of these matrices is easy. Our difficult matrix T9subscriptğ‘‡9T_{9}italic_T start_POSTSUBSCRIPT 9 end_POSTSUBSCRIPT was the result of a sophisticated work in Algebraic Topology, explained in the text, and was not at all arbitrary, allowing us to identify a severe drawback if the KB algorithm is too lazily extended for the rectangular matrices of arbitrary rank. But how it is possible to generate â€œinterestingâ€ difficult matrices with respect to the Smith reduction? A section of this text is devoted to this question, allowing us to easily generate â€œdifficultâ€ matrices and to present a statistical study of the results of the various versions of the KB algorithm with respect to these matrices. This generation process could be used to obtain good benchmarks for other Smith reduction algorithms. As explained in the abstract, an equivalent algorithm is in fact used by the program Magma, see[9]. Just a few lines are given in this program handbook, so that the present text, including detailed explanations, could be useful. Plan: Section 2 recalls the key tool of the reduction, known as the BÃ©zout matrices. Section 3 explains the original KB algorithm, defined only for the non-singular square matrices. Section 4 gives the obvious complements extending the KB algorithm to the general case of a rectangular matrix of arbitrary rank. Section 5 describes the problem of Algebraic Topology producing, via our constructive methods, boundary matrices giving the homology group H8subscriptğ»8H_{8}italic_H start_POSTSUBSCRIPT 8 end_POSTSUBSCRIPT of a relatively complicated topological space. Section 6 explains the observed â€œaccidentâ€ when we tried to apply our extended KB algorithm to the matrix T9subscriptğ‘‡9T_{9}italic_T start_POSTSUBSCRIPT 9 end_POSTSUBSCRIPT, a matrix 1995Ã—5796199557961995\times 57961995 Ã— 5796. The reason of this accident is described and a simple solution to overcome it is given, a successful one. Section 7 observes the solution so obtained is nothing but a mixture a little complicated of the two classical Hermite reductions HNF-1 and HNF-2. This gives the idea of a direct iterative combination of HNF-1 and HNF-2. The algorithm then obtained is quite simple. The last Section 8 relates various tests illustrating that this last version of the KB algorithm is the fastest one. Designing interesting benchmark matrices is not simple, we explain how to obtain such matrices."
https://arxiv.org/html/2411.03255v1,Error Interference in Quantum Simulation,"Understanding algorithmic error accumulation in quantum simulation is crucial due to its fundamental significance and practical applications in simulating quantum many-body system dynamics. Conventional theories typically apply the triangle inequality to provide an upper bound for the error. However, these often yield overly conservative and inaccurate estimates as they neglect error interference â€” a phenomenon where errors in different segments can destructively interfere. Here, we introduce a novel method that directly estimates the long-time algorithmic errors with multiple segments, thereby establishing a comprehensive framework for characterizing algorithmic error interference. We identify the sufficient and necessary condition for strict error interference and introduce the concept of approximate error interference, which is more broadly applicable to scenarios such as power-law interaction models, the Fermi-Hubbard model, and higher-order Trotter formulas. Our work demonstrates significant improvements over prior ones and opens new avenues for error analysis in quantum simulation, offering potential advancements in both theoretical algorithm design and experimental implementation of Hamiltonian simulation.","Simulating quantum dynamics is a central application of quantum computation [1, 2]. Efficient quantum algorithms have been proposed for realizing the SchrÃ¶dinger equation of a general quantum system [3, 4, 5], a notoriously difficult task for classical computers. These algorithms have been applied for various tasks, such as in quantum chemistry [6, 7, 8, 9] and quantum field theories [10, 11, 12, 13], and also serve as an indispensable subroutine for other fundamental quantum algorithms, such as quantum phase estimation [14] and the HHL algorithm for solving linear systems [15]. Since Lloydâ€™s proposal of the first digital quantum simulation algorithm based on the product formula (PF), also known as the Trotter-Suzuki formula, novel techniques such as Linear Combination of Unitaries (LCU) [16, 17, 18] and Quantum Signal Processing (QSP) [19, 20, 21] have been developed. These advancements have led to algorithms with optimal or nearly optimal dependence on several key parameters [22, 23, 24, 19, 20, 21, 21]. Nevertheless, variants of the product formula are still promising and popular candidates for near-term quantum devices [25] or intermediate problems [26] due to their simplicity, mild hardware requirements, and decent performance in practice. For a given Hamiltonian H=âˆ‘l=1LHlğ»superscriptsubscriptğ‘™1ğ¿subscriptğ»ğ‘™H=\sum_{l=1}^{L}H_{l}italic_H = âˆ‘ start_POSTSUBSCRIPT italic_l = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT italic_H start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT with Lğ¿Litalic_L terms of Hlsubscriptğ»ğ‘™H_{l}italic_H start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT, the key idea of the product formula method is to divide the long-time evolution eâˆ’iâ¢Hâ¢tsuperscriptğ‘’iğ»ğ‘¡e^{-\textup{i}Ht}italic_e start_POSTSUPERSCRIPT - i italic_H italic_t end_POSTSUPERSCRIPT into several segments rğ‘Ÿritalic_r and approximate the short-time evolution eâˆ’iâ¢Hâ¢t/rsuperscriptğ‘’iğ»ğ‘¡ğ‘Ÿe^{-\textup{i}Ht/r}italic_e start_POSTSUPERSCRIPT - i italic_H italic_t / italic_r end_POSTSUPERSCRIPT using the pğ‘pitalic_pth-order product formula (PFpğ‘pitalic_p) denoted ğ’°pâ¢(t)subscriptğ’°ğ‘ğ‘¡\mathscr{U}_{p}(t)script_U start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_t ) [27]. For example, the PF1 is defined as ğ’°1â¢(t/r):=eâˆ’iâ¢H1â¢t/râ¢eâˆ’iâ¢H2â¢t/râ¢â‹¯â¢eâˆ’iâ¢HLâ¢t/rassignsubscriptğ’°1ğ‘¡ğ‘Ÿsuperscriptğ‘’isubscriptğ»1ğ‘¡ğ‘Ÿsuperscriptğ‘’isubscriptğ»2ğ‘¡ğ‘Ÿâ‹¯superscriptğ‘’isubscriptğ»ğ¿ğ‘¡ğ‘Ÿ\mathscr{U}_{1}(t/r):=e^{-\textup{i}H_{1}t/r}e^{-\textup{i}H_{2}t/r}\cdots e^{% -\textup{i}H_{L}t/r}script_U start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_t / italic_r ) := italic_e start_POSTSUPERSCRIPT - i italic_H start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_t / italic_r end_POSTSUPERSCRIPT italic_e start_POSTSUPERSCRIPT - i italic_H start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_t / italic_r end_POSTSUPERSCRIPT â‹¯ italic_e start_POSTSUPERSCRIPT - i italic_H start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT italic_t / italic_r end_POSTSUPERSCRIPT and the algorithmic (Trotter) error of each segment is given by Ïµ=â€–ğ’°1â¢(t/r)âˆ’eâˆ’iâ¢Hâ¢t/râ€–italic-Ïµnormsubscriptğ’°1ğ‘¡ğ‘Ÿsuperscriptğ‘’iğ»ğ‘¡ğ‘Ÿ\epsilon=\|\mathscr{U}_{1}(t/r)-e^{-\textup{i}Ht/r}\|italic_Ïµ = âˆ¥ script_U start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_t / italic_r ) - italic_e start_POSTSUPERSCRIPT - i italic_H italic_t / italic_r end_POSTSUPERSCRIPT âˆ¥. Understanding the Trotter error is an important problem, providing the basis for analyzing and optimizing the performance of the algorithm. A tighter error analysis can help save gate costs in both Hamiltonian simulation [26, 24, 28] and other product-formula-related tasks, such as imaginary time evolution [29], quantum Monte Carlo [30, 31], quantum adiabatic algorithms [32], and quantum phase estimation [33]. Many efforts [34, 35, 36, 37, 38, 39, 40] have been devoted to have tighter upper bounds of Ïµitalic-Ïµ\epsilonitalic_Ïµ. However, the total Trotter error estimation is occasionally loose, as it is crudely upper-bounded by râ¢Ïµğ‘Ÿitalic-Ïµr\epsilonitalic_r italic_Ïµ through the triangle inequality, which overlooks destructive error interference between different segments. Interestingly, it has been recently found that Trotter errors from different segments do destructively interfere in the PF1. Thus, the total error may not increase linearly with the number of segments rğ‘Ÿritalic_r [39]. This error interference phenomenon can be explained from a PF2 perspective [41] and find various applications, e.g., quantum adiabatic algorithms [32] and quantum phase estimation [33]. However, this interesting phenomenon has only been investigated in a highly specific case, i.e., the two-term Hamiltonian H=A+Bğ»ğ´ğµH=A+Bitalic_H = italic_A + italic_B with PF1, and its generalization and systematic understanding remain limited. In reality, error interference is more prevalent, even in simple cases such as simulating power-law decaying interaction Hamiltonians with PF1 [42]. In this work, we establish a general framework for analyzing algorithmic Trotter error interference in quantum simulation. The framework is based on a novel method that directly estimates the algorithmic Trotter error for a long time evolution, allowing us to derive the necessary and sufficient condition for strict error interference. Next, we give a general theoretical lower bound for algorithmic error accumulation, which in various cases excludes the possibility of error interference, e.g., 1D nearest-neighbor Heisenberg model with PF2, and generic Hamiltonians with polynomial series expansion algorithms. As applications, we provide more examples with (approximate) error interference phenomena beyond the H=A+Bğ»ğ´ğµH=A+Bitalic_H = italic_A + italic_B case, including H=A+B+Cğ»ğ´ğµğ¶H=A+B+Citalic_H = italic_A + italic_B + italic_C cases in the Heisenberg model, Fermi-Hubbard model, and power-law interaction models. Our results yield significantly improved error bounds compared to the triangle bounds in these cases. Interestingly, approximate error interference also exists in higher-order product formula approximation of perturbative evolution and can lead to speed-ups in some regions when implementing digital adiabatic algorithms. Our results are fundamentally interesting â€” furthering our understanding of the product formula theory, and practically useful â€” directly applicable to the implementation of Hamiltonian simulation using realist quantum hardware. Leveraging our error interference bounds, our results may also inspire better algorithm designs to reduce the total error [40]."
https://arxiv.org/html/2411.03253v1,Discovering Data Structures:Nearest Neighbor Search and Beyond,"We propose a general framework for end-to-end learning of data structures. Our framework adapts to the underlying data distribution and provides fine-grained control over query and space complexity. Crucially, the data structure is learned from scratch, and does not require careful initialization or seeding with candidate data structures. We first apply this framework to the problem of nearest neighbor search. In several settings, we are able to reverse-engineer the learned data structures and query algorithms. For 1D nearest neighbor search, the model discovers optimal distribution (in)dependent algorithms such as binary search and variants of interpolation search. In higher dimensions, the model learns solutions that resemble k-d trees in some regimes, while in others, elements of locality-sensitive hashing emerge. Additionally, the model learns useful representations of high-dimensional data and exploits them to design effective data structures. We also adapt our framework to the problem of estimating frequencies over a data stream, and believe it could be a powerful discovery tool for new problems.","Can deep learning models be trained to discover data structures from scratch? There are several motivations for this question. The first is scientific. Deep learning models are increasingly performing tasks once considered exclusive to humans, from image recognition and mastering the game of Go to engaging in natural language conversations. Designing data structures and algorithms, along with solving complex math problems, are particularly challenging tasks. They require searching through a vast combinatorial space with a difficult to define structure. It is therefore natural to ask what it would take for deep learning models to solve such problems. There are already promising signs: these models have discovered fast matrix-multiplication algorithms (Fawzi et al., 2022), solved SAT problems (Selsam et al., 2018), and learned optimization algorithms for various learning tasks (Garg et al., 2022; AkyÃ¼rek et al., 2022; Fu et al., 2023; Von Oswald et al., 2023). In this work, we investigate the problem of data structure discovery, with a focus on nearest neighbor search. The second motivation is practical. Data structures are ubiquitous objects that enable efficient querying. Traditionally, they have been designed to be worst-case optimal and therefore agnostic to the underlying data and query distributions. However, in many applications there are patterns in these distributions that can be exploited to design more efficient data structures. This has motivated recent work on learning-augmented data structures which leverages knowledge of the data distribution to modify existing data structures with predictions (Lykouris & Vassilvitskii, 2018; Ding et al., 2020; Lin et al., 2022a; Mitzenmacher & Vassilvitskii, 2022). In much of this work, the goal of the learning algorithm is to learn distributional properties of the data, while the underlying query algorithm/data structure is hand-designed. Though this line of work clearly demonstrates the potential of leveraging distributional information, it still relies on expert knowledge to incorporate learning into such structures. In our work, we ask if it is possible to go one step further and let deep learning models discover entire data structures and query algorithms in an end-to-end manner. 1.1 Framework for data structure discovery Figure 1: Our model has two components: 1) A data-processing network that transforms raw data into structured data, arranging it for efficient querying and generating additional statistics when given extra space (not shown in the figure). 2) A query-execution network that performs Mğ‘€Mitalic_M lookups into the output of the data-processing network in order to retrieve the answer to some query qğ‘qitalic_q. Each lookup iğ‘–iitalic_i is managed by a separate query model Qisuperscriptğ‘„ğ‘–Q^{i}italic_Q start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT, which takes qğ‘qitalic_q and the lookup history Hisubscriptğ»ğ‘–H_{i}italic_H start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, and outputs a one-hot lookup vector misubscriptğ‘šğ‘–m_{i}italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT indicating the position to query. Data structure problems can often be decomposed into two steps: 1) data structure construction and 2) query execution. The first step transforms a raw dataset Dğ·Ditalic_D into a structured database D^^ğ·\hat{D}over^ start_ARG italic_D end_ARG, while query-execution performs lookups into D^^ğ·\hat{D}over^ start_ARG italic_D end_ARG to retrieve the answer for some query qğ‘qitalic_q. The performance of a data structure is typically quantified in terms of two measures: space complexityâ€”how much memory is required to store the data structure, and query complexityâ€”how many lookups into the data structure are required to answer some query. One can typically tradeoff larger space complexity for smaller query complexity, and vice versa. We focus on these criteria as they are widely studied and have clear practical connections to efficiency. To learn such data structures, we have a data-processing network which learns how to map a raw dataset to a data structure, and a query network which learns an algorithm for using the data structure to answer queries (Figure 1). In order to learn efficient data structures and query algorithms we impose constraints on the size of the data structures and on the number of lookups that the query network can make into the data structure. Crucially, we propose end-to-end training of both networks such that the learned data structure and query algorithm are optimized for one another. Moreover, in settings where it is beneficial to learn lower-dimensional representations from high-dimensional data, end-to-end training encourages the representations to capture features of the problem that the data structure can exploit. On the one hand, learning the data-processing network and query network jointly in an end-to-end fashion seems obvious, especially given the many successes of end-to-end learning over the past decade. On the other hand, it might be hard to imagine such learning getting off the ground. For instance, if the data-processing network produces a random garbled function of the dataset, we cannot hope the query model to do anything meaningful. This is further complicated by the fact that these data structure tasks are more discrete and combinatorial in terms of how the query model accesses the data structure. 1.2 Summary of Results We apply this framework to the problem of nearest neighbor (NN) search in both low and high dimensions. Given the extensive theoretical work on this topic, along with its widespread practical applications, NN search is an ideal starting point for understanding the landscape of end-to-end data structure discovery. Beyond NN search, we explore the problem of frequency estimation in streaming data and discuss other potential applications of this framework. Our findings are: Sorting and searching in 1D (Section 2.2) For 1D nearest neighbor search, the data-processing network learns to sort, while the query network simultaneously learns to search over the sorted data. When the data follows a uniform or Zipfian distribution, the query network exploits this structure to outperform binary search. On harder distributions lacking structure, the network adapts by discovering binary search, which is worst-case optimal. Importantly, the model discovers that sorting followed by the appropriate search algorithm is effective for NN search in 1D without explicit supervision for these primitives. K-d trees in 2D (Section 2.3) In 2D, when the data is drawn from a uniform distribution, the model discovers a data structure that outperforms k-d trees. On harder distributions, the learned data structure shows surprising resemblance to a k-d tree. This is striking as a k-d tree is a non-trivial data structure, constructed by recursively partitioning the data and finding the median along alternating dimensions. Useful representations in high dimensions (Section 2.4) For high-dimensional data, the model learns representations that make NN search efficient. For example, with data from a uniform distribution on a 30-dimensional hypersphere, the model partitions the space by projecting onto a pair of vectors, similar to locality-sensitive hashing. When trained on an extended 3-digit MNIST dataset, the model finds features that capture the relative ordering of the digits, sorts the images using these features, and performs a search on the sorted imagesâ€”all of which is learned jointly from scratch. Trading off space and query efficiency (Section 2.5) An ideal data structure can use extra space to improve query efficiency by storing additional statistics. The learned model demonstrates this behavior, with performance improving monotonically as more space is provided, in both low and high dimensions. Thus, the model learns to effectively trade off space for query efficiency. Beyond nearest neighbor search (Section 3) We also explore the classical problem of frequency estimation, where a memory-constrained model observes a stream of items and must approximate the frequency of a query item. The learned structure exploits the underlying data distribution to outperform baselines like CountMin sketch, demonstrating the broader applicability of the framework beyond nearest neighbor search."
https://arxiv.org/html/2411.03133v1,Reconstructing edge-deleted unicyclic graphs,"The Harary reconstruction conjecture states that any graph with more than four edges can be uniquely reconstructed from its set of maximal edge-deleted subgraphs [harary1965reconstruction]. In 1977, MÃ¼ller verified the conjecture for graphs with nğ‘›nitalic_n vertices and nâ¢log2â¡(n)ğ‘›subscript2ğ‘›n\log_{2}(n)italic_n roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_n ) edges, improving on LovÃ¡zâ€™s bound of (n2âˆ’n)/4superscriptğ‘›2ğ‘›4\nicefrac{{(n^{2}-n)}}{{4}}/ start_ARG ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - italic_n ) end_ARG start_ARG 4 end_ARG [Muller_1977]. Here, we show that the reconstruction conjecture holds for graphs which have exactly one cycle and three non-isomorphic subtrees.","A graph GğºGitalic_G is a pair G=(V,E)ğºğ‘‰ğ¸G=(V,E)italic_G = ( italic_V , italic_E ) of vertices Vğ‘‰Vitalic_V and edges EâŠ†VÃ—Vğ¸ğ‘‰ğ‘‰E\subseteq V\times Vitalic_E âŠ† italic_V Ã— italic_V. A graph is simple when (u,v)=(v,u)ğ‘¢ğ‘£ğ‘£ğ‘¢(u,v)=(v,u)( italic_u , italic_v ) = ( italic_v , italic_u ) for any edge (u,v)ğ‘¢ğ‘£(u,v)( italic_u , italic_v ), and no edge is of the form (u,u)ğ‘¢ğ‘¢(u,u)( italic_u , italic_u ). A (length-nğ‘›nitalic_n) path in GğºGitalic_G is a sequence of edges p=(e1,â€¦,en)ğ‘subscriptğ‘’1â€¦subscriptğ‘’ğ‘›p=(e_{1},\dots,e_{n})italic_p = ( italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_e start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) such that: eisubscriptğ‘’ğ‘–e_{i}italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and ejsubscriptğ‘’ğ‘—e_{j}italic_e start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT share a vertex if and only if j=(i+1)â¢(modâ¢n)ğ‘—ğ‘–1modğ‘›j=(i+1)~{}(\mathrm{mod}\ n)italic_j = ( italic_i + 1 ) ( roman_mod italic_n ); any vertex is shared by exactly two edges in pğ‘pitalic_p. A graph GğºGitalic_G is connected if and only if there exists a path beginning at uğ‘¢uitalic_u and terminating at vğ‘£vitalic_v for any pair of distinct vertices uâ‰ vğ‘¢ğ‘£u\neq vitalic_u â‰  italic_v. A cycle is a path which begins and ends at the same vertex, and a graph is unicyclic when it has exactly one cycle, as in Figure 1(a); a graph without cycles is called forest, and a connected forest graph is called a tree. For two graphs GğºGitalic_G and Hğ»Hitalic_H, a bijective map Ï†:VGâ†’VH:ğœ‘â†’subscriptğ‘‰ğºsubscriptğ‘‰ğ»\varphi:V_{G}\to V_{H}italic_Ï† : italic_V start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT â†’ italic_V start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT where (u,v)âˆˆEGâ‡”(Ï†â¢(u),Ï†â¢(v))âˆˆEHiffğ‘¢ğ‘£subscriptğ¸ğºğœ‘ğ‘¢ğœ‘ğ‘£subscriptğ¸ğ»(u,v)\in E_{G}\iff(\varphi(u),\varphi(v))\in E_{H}( italic_u , italic_v ) âˆˆ italic_E start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT â‡” ( italic_Ï† ( italic_u ) , italic_Ï† ( italic_v ) ) âˆˆ italic_E start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT for all pairs of distinct vertices uğ‘¢uitalic_u and vğ‘£vitalic_v is called a graph isomorphism; if such a Ï†ğœ‘\varphiitalic_Ï† exists, GğºGitalic_G and Hğ»Hitalic_H are isomorphic. From here, all graphs are finite and simple. In 1965, Harary posited in [harary1965reconstruction] that Conjecture (Harary). For G=(V,E)ğºğ‘‰ğ¸G=(V,E)italic_G = ( italic_V , italic_E ) a graph with |E|>4ğ¸4|E|>4| italic_E | > 4, let Gisubscriptğºğ‘–G_{i}italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT denote the maximal subgraph of GğºGitalic_G with the iğ‘–iitalic_ith edge deleted, and Deckâ¡(G)â‰”{Gi}i=1mâ‰”Deckğºsuperscriptsubscriptsubscriptğºğ‘–ğ‘–1ğ‘š\operatorname{\mathrm{Deck}}(G)\coloneqq\{G_{i}\}_{i=1}^{m}roman_Deck ( italic_G ) â‰” { italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT the set of maximal edge-deleted subgraphs of GğºGitalic_G. If Deckâ¡(G)=Deckâ¡(H)DeckğºDeckğ»\operatorname{\mathrm{Deck}}(G)=\operatorname{\mathrm{Deck}}(H)roman_Deck ( italic_G ) = roman_Deck ( italic_H ) for some graph Hğ»Hitalic_H, then Gâ‰…Hğºğ»G\cong Hitalic_G â‰… italic_H. Here, we show the conjecture holds for the class ğ’°ğ’°\mathscr{U}script_U of unicyclic graphs with at least three non-isomorphic subtrees."
https://arxiv.org/html/2411.02942v1,Constant Approximation for Weighted Nash Social Welfare with Submodular Valuations,"We study the problem of assigning items to agents so as to maximize the weighted Nash Social Welfare (NSW) under submodular valuations. The best-known result for the problem is an Oâ¢(nâ¢wmax)ğ‘‚ğ‘›subscriptğ‘¤O(nw_{\max})italic_O ( italic_n italic_w start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT )-approximation due to Garg, Husic, Li, Vega, and Vondrak [13], where wmaxsubscriptğ‘¤w_{\max}italic_w start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT is the maximum weight over all agents. Obtaining a constant approximation algorithm is an open problem in the field that has recently attracted considerable attention.We give the first such algorithm for the problem, thus solving the open problem in the affirmative. Our algorithm is based on the natural Configuration LP for the problem, which was introduced recently by Feng and Li [11] for the additive valuation case. Our rounding algorithm is similar to that of Li [25] developed for the unrelated machine scheduling problem to minimize weighted completion time. Roughly speaking, we designate the largest item in each configuration as a large item and the remaining items as small items. So, every agent gets precisely 1 fractional large item in the configuration LP solution. With the rounding algorithm in [25], we can ensure that in the obtained solution, every agent gets precisely 1 large item, and the assignments of small items are negatively correlated.","We study the problem of allocating a set Mğ‘€Mitalic_M of indivisible items among a set Nğ‘Nitalic_N of agents, where each agent iâˆˆNğ‘–ğ‘i\in Nitalic_i âˆˆ italic_N has a monotone non-negative submodular valuation vi:2Mâ†’â„â‰¥0:subscriptğ‘£ğ‘–â†’superscript2ğ‘€subscriptâ„absent0v_{i}:2^{M}\to\mathbb{R}_{\geq 0}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT : 2 start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT â†’ blackboard_R start_POSTSUBSCRIPT â‰¥ 0 end_POSTSUBSCRIPT and a weight wiâˆˆ(0,1)subscriptğ‘¤ğ‘–01w_{i}\in(0,1)italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âˆˆ ( 0 , 1 ) with âˆ‘iâˆˆNwi=1subscriptğ‘–ğ‘subscriptğ‘¤ğ‘–1\sum_{i\in N}w_{i}=1âˆ‘ start_POSTSUBSCRIPT italic_i âˆˆ italic_N end_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1. The weighted Nash Social Welfare (NSW) problem under submodular valuations asks for partition ğ’®:=(Si)iâˆˆNassignğ’®subscriptsubscriptğ‘†ğ‘–ğ‘–ğ‘{\cal S}:=(S_{i})_{i\in N}caligraphic_S := ( italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i âˆˆ italic_N end_POSTSUBSCRIPT of Mğ‘€Mitalic_M that maximizes the weighted geometric mean of the agentsâ€™ valuations: ğ–­ğ–²ğ–¶â¢(ğ’®)=âˆiâˆˆN(viâ¢(Si))wi.ğ–­ğ–²ğ–¶ğ’®subscriptproductğ‘–ğ‘superscriptsubscriptğ‘£ğ‘–subscriptğ‘†ğ‘–subscriptğ‘¤ğ‘–\mathsf{NSW}({\cal S})=\prod_{i\in N}\left(v_{i}(S_{i})\right)^{w_{i}}.sansserif_NSW ( caligraphic_S ) = âˆ start_POSTSUBSCRIPT italic_i âˆˆ italic_N end_POSTSUBSCRIPT ( italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) start_POSTSUPERSCRIPT italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT . The case when all wisubscriptğ‘¤ğ‘–w_{i}italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTâ€™s are equal to 1/n1ğ‘›1/n1 / italic_n is called the unweighted Nash Social Welfare problem. As usual, we assume we are given a value oracle for each visubscriptğ‘£ğ‘–v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. W.l.o.g, we assume viâ¢(âˆ…)=0subscriptğ‘£ğ‘–0v_{i}(\emptyset)=0italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( âˆ… ) = 0 for every agent iâˆˆNğ‘–ğ‘i\in Nitalic_i âˆˆ italic_N 111If viâ¢(âˆ…)>0subscriptğ‘£ğ‘–0v_{i}(\emptyset)>0italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( âˆ… ) > 0 for some iâˆˆNğ‘–ğ‘i\in Nitalic_i âˆˆ italic_N, we can create a â€œprivateâ€ item jisubscriptğ‘—ğ‘–j_{i}italic_j start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT for iğ‘–iitalic_i which has 00-value to all agents other than iğ‘–iitalic_i. We replace the valuation of iğ‘–iitalic_i with viâ€²subscriptsuperscriptğ‘£â€²ğ‘–v^{\prime}_{i}italic_v start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, which is defined as follows: viâ€²â¢(S):=viâ¢(S)âˆ’viâ¢(âˆ…)assignsubscriptsuperscriptğ‘£â€²ğ‘–ğ‘†subscriptğ‘£ğ‘–ğ‘†subscriptğ‘£ğ‘–v^{\prime}_{i}(S):=v_{i}(S)-v_{i}(\emptyset)italic_v start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_S ) := italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_S ) - italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( âˆ… ) if jiâˆ‰Ssubscriptğ‘—ğ‘–ğ‘†j_{i}\notin Sitalic_j start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âˆ‰ italic_S and viâ€²â¢(S)=viâ¢(Sâˆ–ji)subscriptsuperscriptğ‘£â€²ğ‘–ğ‘†subscriptğ‘£ğ‘–ğ‘†subscriptğ‘—ğ‘–v^{\prime}_{i}(S)=v_{i}(S\setminus j_{i})italic_v start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_S ) = italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_S âˆ– italic_j start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) if jiâˆˆSsubscriptğ‘—ğ‘–ğ‘†j_{i}\in Sitalic_j start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âˆˆ italic_S.. Fair and efficient allocation of resources is a central problem in computer science, game theory, and social choices, with applications across diverse domains [1, 3, 4, 22, 27, 31, 32, 37]. Three distinct communities independently discovered the notation of Nash social welfare: as a solution to the bargaining problem in classical game theory [28], as a well-established concept of proportional fairness in networking [23], and as the celebrated notion of competitive equilibrium with equal incomes in economics [35]. The unweighted case for the problem was introduced by Nash [28], and it was later extended to the weighted case [17, 21]. This extension has since been widely studied and applied across various fields, such as bargaining theory [7, 24, 34], water allocation [9, 19], climate agreements [38], and more. One of the most important features of the NSW objective is that it offers a tradeoff between the frequently conflicting demands of fairness and efficiency. A special case for the valuations visubscriptğ‘£ğ‘–v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is when they are additive. The unweighted NSW problem with additive valuations is an important topic in optimization and has received considerable interest. Barman, Krishnamurthy, and Vaish [2] developed a (ğ–¾1/ğ–¾â‰ˆ1.445)superscriptğ–¾1ğ–¾1.445(\mathsf{e}^{1/\mathsf{e}}\approx 1.445)( sansserif_e start_POSTSUPERSCRIPT 1 / sansserif_e end_POSTSUPERSCRIPT â‰ˆ 1.445 )-approximation algorithm that finds an allocation that is both Pareto-efficient and envy-free up to one item (EF1). They showed that this problem can be reduced to the case of identical valuations, where any EF1 allocation can achieve an approximation ratio of ğ–¾1/ğ–¾â‰ˆ1.445superscriptğ–¾1ğ–¾1.445\mathsf{e}^{1/\mathsf{e}}\approx 1.445sansserif_e start_POSTSUPERSCRIPT 1 / sansserif_e end_POSTSUPERSCRIPT â‰ˆ 1.445. On the negative side, Garg, Hoefer, and Mehlhorn [12] established a hardness of 8/787\sqrt{8/7}square-root start_ARG 8 / 7 end_ARG. For the weighted case with additive valuations, Brown, Laddha, Pittu, and Singh [5] introduced an approximation algorithm with a ratio of 5â‹…expâ¢(2â¢logâ¡n+2â¢âˆ‘iâˆˆAwiâ¢logâ¡wi)â‹…5exp2ğ‘›2subscriptğ‘–ğ´subscriptğ‘¤ğ‘–subscriptğ‘¤ğ‘–5\cdot\text{exp}(2\log n+2\sum_{i\in A}w_{i}\log w_{i})5 â‹… exp ( 2 roman_log italic_n + 2 âˆ‘ start_POSTSUBSCRIPT italic_i âˆˆ italic_A end_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT roman_log italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ). Later, Feng and Li [11] presented an elegant (ğ–¾1/ğ–¾+Ïµ)superscriptğ–¾1ğ–¾italic-Ïµ(\mathsf{e}^{1/\mathsf{e}}+\epsilon)( sansserif_e start_POSTSUPERSCRIPT 1 / sansserif_e end_POSTSUPERSCRIPT + italic_Ïµ )-approximation algorithm for the weighted case, using their novel configuration LP and the Shmoys-Tardos rounding procedure developed in the context of unrelated machine scheduling. The approximation ratio matches the best-known ratio for the unweighted case. When the nğ‘›nitalic_n valuation functions are additive and identical, Nguyen and Rothe [29] developed a PTAS for the unweighted NSW problem. Later, Inoue and Kobayashi [20] gave an additive PTAS for the problem, i.e., a polynomial-time algorithm that maximizes the Nash social welfare within an additive error of Ïµâ¢vmaxitalic-Ïµsubscriptğ‘£\epsilon v_{\max}italic_Ïµ italic_v start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT, where vmaxsubscriptğ‘£v_{\max}italic_v start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT is the maximum utility of an item. Li and Vondrak [26] developed the first constant approximation algorithm for unweighted NSW with submodular valuations using convex programming. The ratio has been improved by Garg, Husic, Li, Vega, and Vondrak [13] to (4+Ïµ)4italic-Ïµ(4+\epsilon)( 4 + italic_Ïµ ) using an elegant local-search-based algorithm. When additionally n=Oâ¢(1)ğ‘›ğ‘‚1n=O(1)italic_n = italic_O ( 1 ), by guessing the value and the Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 ) largest items for each agent, and using the multilinear extension of submodular functions, a ğ–¾/(ğ–¾âˆ’1)ğ–¾ğ–¾1\mathsf{e}/(\mathsf{e}-1)sansserif_e / ( sansserif_e - 1 )-approximation can be achieved [16]. In the same paper, [16] proved that unweighted NSW with submodular valuations is hard to approximate within ğ–¾/(ğ–¾âˆ’1)âˆ’Ïµğ–¾ğ–¾1italic-Ïµ\mathsf{e}/(\mathsf{e}-1)-\epsilonsansserif_e / ( sansserif_e - 1 ) - italic_Ïµ. The hardness holds even for the case n=Oâ¢(1)ğ‘›ğ‘‚1n=O(1)italic_n = italic_O ( 1 ). For the weighted NSW problem with submodular valuations, [13] showed that the approximation ratio of the local search algorithm becomes Oâ¢(nâ¢wmax)ğ‘‚ğ‘›subscriptğ‘¤O(nw_{\max})italic_O ( italic_n italic_w start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT ), where wmax:=maxiâˆˆ[n]â¡wiassignsubscriptğ‘¤subscriptğ‘–delimited-[]ğ‘›subscriptğ‘¤ğ‘–w_{\max}:=\max_{i\in[n]}w_{i}italic_w start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT := roman_max start_POSTSUBSCRIPT italic_i âˆˆ [ italic_n ] end_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is the maximum weight over the agents. In the new version [15] of the paper, the authors presented a (6â¢ğ–¾+Ïµ)6ğ–¾italic-Ïµ(6\mathsf{e}+\epsilon)( 6 sansserif_e + italic_Ïµ )-approximation algorithm with running time 2Oâ¢(nâ¢logâ¡n)â¢polyâ¢(m,1/Ïµ)superscript2ğ‘‚ğ‘›ğ‘›polyğ‘š1italic-Ïµ2^{O(n\log n)}\mathrm{poly}(m,1/\epsilon)2 start_POSTSUPERSCRIPT italic_O ( italic_n roman_log italic_n ) end_POSTSUPERSCRIPT roman_poly ( italic_m , 1 / italic_Ïµ ), which is polynomial when n=Oâ¢(1)ğ‘›ğ‘‚1n=O(1)italic_n = italic_O ( 1 ). For the more general setting where the valuations are subadditive, Dobzinski, Li, Rubinstein, and VondrÃ¡k [10] recently proposed a constant approximation algorithm when agents are unweighted, provided that we have access to demand oracles for the valuation functions. Our Result. In this paper, we give the first polynomial-time Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 )-approximation algorithm for weighted Nash social welfare under the submodular valuations. The best result prior to this work was the Oâ¢(nâ¢wmax)ğ‘‚ğ‘›subscriptğ‘¤O(nw_{\max})italic_O ( italic_n italic_w start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT )-approximation due to Garg, Husic, Li, Vega, and Vondrak [13]. Theorem 1.1. For any Ïµ>0italic-Ïµ0\epsilon>0italic_Ïµ > 0, there is a randomized (233+Ïµ)233italic-Ïµ(233+\epsilon)( 233 + italic_Ïµ )-approximation algorithm for the weighted Nash social welfare problem with submodular valuations, with running time polynomial in the size of the input and 1Ïµ1italic-Ïµ\frac{1}{\epsilon}divide start_ARG 1 end_ARG start_ARG italic_Ïµ end_ARG. For convenience, we list the known approximation results for the NSW problem in table 1. Additive Submodular Subadditive LB UB LB UB LB UB Unweighted 8787\sqrt{\frac{8}{7}}square-root start_ARG divide start_ARG 8 end_ARG start_ARG 7 end_ARG end_ARG [12] ğ–¾1/ğ–¾+Ïµsuperscriptğ–¾1ğ–¾italic-Ïµ\mathsf{e}^{1/\mathsf{e}}+\epsilonsansserif_e start_POSTSUPERSCRIPT 1 / sansserif_e end_POSTSUPERSCRIPT + italic_Ïµ [2] ğ–¾ğ–¾âˆ’1ğ–¾ğ–¾1\frac{\mathsf{e}}{\mathsf{e}-1}divide start_ARG sansserif_e end_ARG start_ARG sansserif_e - 1 end_ARG [16] 4+Ïµ4italic-Ïµ4+\epsilon4 + italic_Ïµ [13] Oâ¢(1)âˆ—ğ‘‚superscript1O(1)^{*}italic_O ( 1 ) start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT [10] Weighted ğ–¾1/ğ–¾+Ïµsuperscriptğ–¾1ğ–¾italic-Ïµ\mathsf{e}^{1/\mathsf{e}}+\epsilonsansserif_e start_POSTSUPERSCRIPT 1 / sansserif_e end_POSTSUPERSCRIPT + italic_Ïµ [11] 233+Ïµ233italic-Ïµ233+\epsilon233 + italic_Ïµ (theorem 1.1) Table 1: Known Results for Nash social welfare. LB and UB stand for lower and upper bounds, respectively. The result with âˆ— requires demand oracles for valuation functions. When the function is identical additive, the upper bound for the unweighted case is PTAS [20, 29]. When n=Oâ¢(1)ğ‘›ğ‘‚1n=O(1)italic_n = italic_O ( 1 ), the upper bounds for unweighted and weighted NSW with submodular valuations are respectively ğ–¾/(ğ–¾âˆ’1)ğ–¾ğ–¾1\mathsf{e}/(\mathsf{e}-1)sansserif_e / ( sansserif_e - 1 ) [16] and 6â¢ğ–¾+Ïµ6ğ–¾italic-Ïµ6\mathsf{e}+\epsilon6 sansserif_e + italic_Ïµ [15]. 1.1 Overview of Our Techniques Our algorithm leverages the configuration LP introduced in [11] for the additive valuation case. For each agent iâˆˆNğ‘–ğ‘i\in Nitalic_i âˆˆ italic_N and subset of items SâŠ†Mğ‘†ğ‘€S\subseteq Mitalic_S âŠ† italic_M, we define a variable yi,Ssubscriptğ‘¦ğ‘–ğ‘†y_{i,S}italic_y start_POSTSUBSCRIPT italic_i , italic_S end_POSTSUBSCRIPT to indicate whether the set of items assigned to iğ‘–iitalic_i is precisely Sğ‘†Sitalic_S. The objective of this LP is to minimize âˆ‘i,Syi,Sâ‹…wiâ‹…lnâ¡viâ¢(S)subscriptğ‘–ğ‘†â‹…subscriptğ‘¦ğ‘–ğ‘†subscriptğ‘¤ğ‘–subscriptğ‘£ğ‘–ğ‘†\sum_{i,S}y_{i,S}\cdot w_{i}\cdot\ln v_{i}(S)âˆ‘ start_POSTSUBSCRIPT italic_i , italic_S end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_i , italic_S end_POSTSUBSCRIPT â‹… italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT â‹… roman_ln italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_S ), the logarithm of the NSW objective. After solving the LP, we apply the rounding procedure from [25], developed for the weighted completion time minimization problem in the unrelated machine scheduling setting. Then we prove concentration bounds for the values obtained by each agent, using arguments developed for pipage rounding. To build intuition, let us focus on the unweighted case. For the special case where |M|=|N|ğ‘€ğ‘|M|=|N|| italic_M | = | italic_N |, the problem reduces to a maximum-weight bipartite matching problem with weights given by the logarithm of values. So, any general algorithm for the problem must capture the maximum weight of the bipartite matching algorithm as a special case. Interestingly, previous results showed that if one is given the largest (i.e., the most valuable) item assigned to every agent, then an Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 )-approximation algorithm is easy to obtain using local search [13] or LP rounding [14, 26]. For example, with this idea, Garg, Husic, Li, Vega, and Vondrak [13] designed an elegant 4-approximation local search algorithm. They first compute an initial matching of one item to every agent so as to maximize the NSW objective, then assign the remaining items using local search with an endowed valuation function, and finally rematch the initially assigned items to agents to maximize the final Nash social welfare. Unfortunately, their algorithm fails to give an Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 )-approximation when the agents are weighted. Our algorithm implements the idea of â€œmatching largest items to agentsâ€ using the configuration LP solution as a guide. We achieve a per-client guarantee, allowing us to give an Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 )-approximation for the weighted NSW problem with submodular valuations. After obtaining an LP solution (yi,Sâˆ—)i,Ssubscriptsubscriptsuperscriptğ‘¦ğ‘–ğ‘†ğ‘–ğ‘†(y^{*}_{i,S})_{i,S}( italic_y start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i , italic_S end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i , italic_S end_POSTSUBSCRIPT, for each agent iğ‘–iitalic_i and configuration Sğ‘†Sitalic_S, we designate the largest item in Sğ‘†Sitalic_S as a â€œlargeâ€ item for iğ‘–iitalic_i, while treating the remaining items as â€œsmallâ€. This creates a fractional assignment in which each agent receives exactly one fractional large item. While maintaining marginal probabilities in our rounding algorithm, we ensure that each agent gets exactly one large item, and the assignment of small items are negatively correlated. That is, we select a random matching for large items. If the large and small items were disjoint, the rounding algorithm would be straightforward. However, complications arise when an item may be large for one agent and small for another â€” or even for the same agent in different configurations. This necessitates a correlated assignment strategy for large and small items. This is where we employ the iterative rounding procedure of [25]. We construct a bipartite multi-graph between agents and items, with two edge types: marked edges for large items and unmarked edges for small items. During iterative rounding, we identify either a simple cycle of marked edges or a pseudo-marked path â€“ a simple path of marked edges with two unmarked edges at the ends â€“ and apply rotation or shifting operations on the cycle or path in each iteration. This process ultimately yields an integral assignment. To analyze the approximation ratio, we focus on each agent iğ‘–iitalic_i and analyze ğ”¼â¢[lnâ¡(viâ¢(T))]ğ”¼delimited-[]subscriptğ‘£ğ‘–ğ‘‡\mathbb{E}[\ln(v_{i}(T))]blackboard_E [ roman_ln ( italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_T ) ) ], where Tğ‘‡Titalic_T is the set of items assigned to iğ‘–iitalic_i. Note that Tğ‘‡Titalic_T includes exactly one large item, respecting the marginal probabilities. Let TSsuperscriptğ‘‡ST^{\mathrm{S}}italic_T start_POSTSUPERSCRIPT roman_S end_POSTSUPERSCRIPT denote the remaining items, i.e., the small items assigned to iğ‘–iitalic_i. The assignments of the large item and the small items may be positively correlated, so we analyze the worst-case scenario for this correlation. However, the assignments of the small items are negatively correlated; more precisely, they are determined through a pipage-rounding procedure. Using the concave pessimistic estimator technique from [18] and the submodularity of the function visubscriptğ‘£ğ‘–v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, we can establish concentration bounds for viâ¢(TS)subscriptğ‘£ğ‘–superscriptğ‘‡Sv_{i}(T^{\mathrm{S}})italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_T start_POSTSUPERSCRIPT roman_S end_POSTSUPERSCRIPT ). With the bounds, we can lower bound ğ”¼â¢[lnâ¡(viâ¢(T))]ğ”¼delimited-[]subscriptğ‘£ğ‘–ğ‘‡\mathbb{E}[\ln(v_{i}(T))]blackboard_E [ roman_ln ( italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_T ) ) ] by âˆ‘Syi,Sâˆ—â¢lnâ¡viâ¢(S)âˆ’Oâ¢(1)subscriptğ‘†subscriptsuperscriptğ‘¦ğ‘–ğ‘†subscriptğ‘£ğ‘–ğ‘†ğ‘‚1\sum_{S}y^{*}_{i,S}\ln v_{i}(S)-O(1)âˆ‘ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT italic_y start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i , italic_S end_POSTSUBSCRIPT roman_ln italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_S ) - italic_O ( 1 ). Organization. The rest of the paper is organized as follows. We introduce some preliminaries in section 2, describe our algorithm in section 3, and give its analysis in section 4. For a smoother flow in the main text, we defer some proofs to the appendix."
https://arxiv.org/html/2411.02148v1,Optimality of Frequency Moment Estimation,"Estimating the second frequency moment of a stream up to (1Â±Îµ)plus-or-minus1ğœ€(1\pm\varepsilon)( 1 Â± italic_Îµ ) multiplicative error requires at most Oâ¢(logâ¡n/Îµ2)ğ‘‚ğ‘›superscriptğœ€2O(\log n/\varepsilon^{2})italic_O ( roman_log italic_n / italic_Îµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) bits of space, due to a seminal result of Alon, Matias, and Szegedy. It is also known that at least Î©â¢(logâ¡n+1/Îµ2)Î©ğ‘›1superscriptğœ€2\Omega(\log n+1/\varepsilon^{2})roman_Î© ( roman_log italic_n + 1 / italic_Îµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) space is needed. We prove an optimal lower bound of Î©â¢(logâ¡(nâ¢Îµ2)/Îµ2)Î©ğ‘›superscriptğœ€2superscriptğœ€2\Omega\left(\log\left(n\varepsilon^{2}\right)/\varepsilon^{2}\right)roman_Î© ( roman_log ( italic_n italic_Îµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) / italic_Îµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) for all Îµ=Î©â¢(1/n)ğœ€Î©1ğ‘›\varepsilon=\Omega(1/\sqrt{n})italic_Îµ = roman_Î© ( 1 / square-root start_ARG italic_n end_ARG ). Note that when Îµ>nâˆ’1/2+cğœ€superscriptğ‘›12ğ‘\varepsilon>n^{-1/2+c}italic_Îµ > italic_n start_POSTSUPERSCRIPT - 1 / 2 + italic_c end_POSTSUPERSCRIPT, where c>0ğ‘0c>0italic_c > 0, our lower bound matches the classic upper bound of AMS. For smaller values of Îµğœ€\varepsilonitalic_Îµ we also introduce a revised algorithm that improves the classic AMS bound and matches our lower bound. Our lower bound holds also for the more general problem of pğ‘pitalic_p-th frequency moment estimation for the range of pâˆˆ(1,2]ğ‘12p\in(1,2]italic_p âˆˆ ( 1 , 2 ], giving a tight bound in the only remaining range to settle the optimal space complexity of estimating frequency moments.","An extensive body of literature is devoted to the streaming model of computation, which is important for the analysis of massive datasets and in network traffic monitoring. A central problem in this model is the frequency moment estimation problem: Elements from a universe Uğ‘ˆUitalic_U are given to the algorithm one-by-one, defining a vector of frequencies â€” that is, fxâˆˆâ„•subscriptğ‘“ğ‘¥â„•f_{x}\in\mathbb{N}italic_f start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT âˆˆ blackboard_N is the number of times the element xâˆˆUğ‘¥ğ‘ˆx\in Uitalic_x âˆˆ italic_U appeared in the stream; Finally, the algorithm has to return, with good probability, a (1Â±Îµ)plus-or-minus1ğœ€(1\pm\varepsilon)( 1 Â± italic_Îµ )-estimation of Fp:=âˆ‘xâˆˆUfxpassignsubscriptğ¹ğ‘subscriptğ‘¥ğ‘ˆsuperscriptsubscriptğ‘“ğ‘¥ğ‘F_{p}:=\sum_{x\in U}f_{x}^{p}italic_F start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT := âˆ‘ start_POSTSUBSCRIPT italic_x âˆˆ italic_U end_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT â€” the pğ‘pitalic_p-th frequency moment of the stream. We generally denote the length of the stream by nğ‘›nitalic_n and assume that |U|=polyâ¢(n)ğ‘ˆpolyğ‘›|U|=\text{poly}(n)| italic_U | = poly ( italic_n ). The main complexity parameter studied in this model is how much space is needed for the algorithm to succeed. The study of both the streaming model and of frequency moment estimation in it was initiated in the seminal 1996 work of Alon, Matias, and Szegedy [AMS96]. The case of p=2ğ‘2p=2italic_p = 2, or second moment estimation, is of particular importance. It is often called the repeat rate or surprise index, and is used in various tasks such as database query optimization [AGMS99], network traffic anomaly detection [KSZC03], approximate histogram maintenance [GGI+02] and more. Other moments of particular interest are p=1ğ‘1p=1italic_p = 1, corresponding to the approximate counting problem [Mor78, NY22], and p=0ğ‘0p=0italic_p = 0, corresponding to the distinct elements problem [FM85, IW03, KNW10b]. Among these special cases, only the space complexity of the first remains not fully understood. The original algorithm for F2subscriptğ¹2F_{2}italic_F start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT-estimation given by Alon, Matias, and Szegedy uses Oâ¢(logâ¡n/Îµ2)ğ‘‚ğ‘›superscriptğœ€2O(\log n/\varepsilon^{2})italic_O ( roman_log italic_n / italic_Îµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) bits of space; while the highest known lower bound due to Woodruff in 2004 [Woo04] is Î©â¢(logâ¡n+1/Îµ2)Î©ğ‘›1superscriptğœ€2\Omega(\log n+1/\varepsilon^{2})roman_Î© ( roman_log italic_n + 1 / italic_Îµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) â€” leaving up to a quadratic gap between the upper and lower bounds for certain choices of Îµğœ€\varepsilonitalic_Îµ. While Fpsubscriptğ¹ğ‘F_{p}italic_F start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT-estimation for pâ‰¤2ğ‘2p\leq 2italic_p â‰¤ 2 uses amount of space that is only logarithmic in the length of the stream, it was shown that for p>2ğ‘2p>2italic_p > 2 at least Î©â¢(n1âˆ’2/p/polyâ¢(Îµ))Î©superscriptğ‘›12ğ‘polyğœ€\Omega(n^{1-2/p}/\text{poly}(\varepsilon))roman_Î© ( italic_n start_POSTSUPERSCRIPT 1 - 2 / italic_p end_POSTSUPERSCRIPT / poly ( italic_Îµ ) ) space is needed [BYJKS04, CKS03b] â€” which is polynomial in the streamâ€™s length. A long list of works [IW05, BGKS06, MW10, AKO11, BO10, And17, Gan11b, WZ12, Gan11a, LW13] resulted in a nearly-tight bound of Î˜~â¢(n1âˆ’2/p/Îµ2)~Î˜superscriptğ‘›12ğ‘superscriptğœ€2\tilde{\Theta}\left(n^{1-2/p}/\varepsilon^{2}\right)over~ start_ARG roman_Î˜ end_ARG ( italic_n start_POSTSUPERSCRIPT 1 - 2 / italic_p end_POSTSUPERSCRIPT / italic_Îµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) for Fpsubscriptğ¹ğ‘F_{p}italic_F start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT-estimation for every p>2ğ‘2p>2italic_p > 2 (not necessarily an integer) and Îµğœ€\varepsilonitalic_Îµ, for some ranges of parameters the bounds are tight â€” in others there is a gap between the bounds that is poly-logarithmic in the bound itself. For pâ‰¤2ğ‘2p\leq 2italic_p â‰¤ 2 the space complexity is not as well understood. Woodruff [Woo04] showed a lower bound of Î©â¢(logâ¡n+1/Îµ2)Î©ğ‘›1superscriptğœ€2\Omega(\log n+1/\varepsilon^{2})roman_Î© ( roman_log italic_n + 1 / italic_Îµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) for every pâ‰ 1ğ‘1p\neq 1italic_p â‰  1, this is optimal in terms of Îµğœ€\varepsilonitalic_Îµ alone and is also known to be optimal for the distinct elements problem (that is, p=0ğ‘0p=0italic_p = 0). For the special case of approximate counting (that is, p=1ğ‘1p=1italic_p = 1), a tight bound of Î˜â¢(logâ¡logâ¡n+logâ¡Îµâˆ’1)Î˜ğ‘›superscriptğœ€1\Theta(\log\log n+\log\varepsilon^{-1})roman_Î˜ ( roman_log roman_log italic_n + roman_log italic_Îµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) is known [NY22]. For the range of pâˆˆ[0,1)ğ‘01p\in[0,1)italic_p âˆˆ [ 0 , 1 ), the upper bound of AMS was improved by Jayaram and Woodruff who presented a nearly-tight O~â¢(logâ¡n+1/Îµ2)~ğ‘‚ğ‘›1superscriptğœ€2\tilde{O}\left(\log n+1/\varepsilon^{2}\right)over~ start_ARG italic_O end_ARG ( roman_log italic_n + 1 / italic_Îµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) bound in that range [JW19]. This leaves pâˆˆ(1,2]ğ‘12p\in(1,2]italic_p âˆˆ ( 1 , 2 ] as the last remaining range within no nearly-tight bounds are known. For certain generalizations more is known: When the stream is randomly shuffled and given in random order, then in the range pâˆˆ(1,2)ğ‘12p\in(1,2)italic_p âˆˆ ( 1 , 2 ) (excluding p=2ğ‘2p=2italic_p = 2) [BVWY18] showed an improved upper bound of O~â¢(logâ¡n+1/Îµ2)~ğ‘‚ğ‘›1superscriptğœ€2\tilde{O}\left(\log n+1/\varepsilon^{2}\right)over~ start_ARG italic_O end_ARG ( roman_log italic_n + 1 / italic_Îµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ). When updates are allowed in the stream, that is, elements can also be deleted and not only added to it, then [KNW10a] showed that Î˜â¢(logâ¡n/Îµ2)Î˜ğ‘›superscriptğœ€2\Theta(\log n/\varepsilon^{2})roman_Î˜ ( roman_log italic_n / italic_Îµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) is optimal for pâ‰¤2ğ‘2p\leq 2italic_p â‰¤ 2. In this work, we settle the space complexity of frequency moment estimation in the entire remaining range of pâˆˆ(1,2]ğ‘12p\in(1,2]italic_p âˆˆ ( 1 , 2 ], including the special case of second frequency moment estimation. For p=2ğ‘2p=2italic_p = 2, we show that the AMS bound is essentially tight. Theorem. Let ğ’œğ’œ\mathcal{A}caligraphic_A be a streaming algorithm that gives an (1Â±Îµ)plus-or-minus1ğœ€\left(1\pm\varepsilon\right)( 1 Â± italic_Îµ ) multiplicative approximation to the F2subscriptğ¹2F_{2}italic_F start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT of its input stream and succeeds with probability â‰¥2/3absent23\geq 2/3â‰¥ 2 / 3, for some Îµ=Î©â¢(1/n)ğœ€Î©1ğ‘›\varepsilon=\Omega(1/\sqrt{n})italic_Îµ = roman_Î© ( 1 / square-root start_ARG italic_n end_ARG ). Then, the space used by ğ’œğ’œ\mathcal{A}caligraphic_A is Î©â¢(logâ¡(Îµ2â¢n)/Îµ2)Î©superscriptğœ€2ğ‘›superscriptğœ€2\Omega\left(\log\left(\varepsilon^{2}n\right)/\varepsilon^{2}\right)roman_Î© ( roman_log ( italic_Îµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_n ) / italic_Îµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ). Note that the range Îµ<1/nğœ€1ğ‘›\varepsilon<1/\sqrt{n}italic_Îµ < 1 / square-root start_ARG italic_n end_ARG is less interesting as Oâ¢(minâ¡{nâ¢logâ¡n,|U|})ğ‘‚ğ‘›ğ‘›ğ‘ˆO\left(\min\{n\log n,|U|\}\right)italic_O ( roman_min { italic_n roman_log italic_n , | italic_U | } ) space suffices for exactly maintaining the vector of frequencies. We observe that in the range where Îµğœ€\varepsilonitalic_Îµ is very close to 1/n1ğ‘›1/\sqrt{n}1 / square-root start_ARG italic_n end_ARG our lower bound is (slightly) lower than the AMS upper bound, we show that this is inherent by introducing a modification of the AMS algorithm that matches our lower bound in this range. Theorem. For Îµ=Î©â¢(1/n)ğœ€Î©1ğ‘›\varepsilon=\Omega(1/\sqrt{n})italic_Îµ = roman_Î© ( 1 / square-root start_ARG italic_n end_ARG ), we can get a (1Â±Îµ)plus-or-minus1ğœ€(1\pm\varepsilon)( 1 Â± italic_Îµ )-approximation of the F2subscriptğ¹2F_{2}italic_F start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT of a stream of length nğ‘›nitalic_n using Oâ¢(logâ¡(Îµ2â¢n)/Îµ2)ğ‘‚superscriptğœ€2ğ‘›superscriptğœ€2O\left(\log\left(\varepsilon^{2}n\right)/\varepsilon^{2}\right)italic_O ( roman_log ( italic_Îµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_n ) / italic_Îµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) space with success probability >2/3absent23>2/3> 2 / 3. 0â‰¤p<10ğ‘10\leq p<10 â‰¤ italic_p < 1 p=1ğ‘1p=1italic_p = 1 1<pâ‰¤21ğ‘21<p\leq 21 < italic_p â‰¤ 2 p>2ğ‘2p>2italic_p > 2 Î˜~â¢(logâ¡n+1/Îµ2)~Î˜ğ‘›1superscriptğœ€2\tilde{\Theta}(\log n+1/\varepsilon^{2})over~ start_ARG roman_Î˜ end_ARG ( roman_log italic_n + 1 / italic_Îµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) Î˜â¢(logâ¡logâ¡n+logâ¡Îµâˆ’1)Î˜ğ‘›superscriptğœ€1\Theta(\log\log n+\log\varepsilon^{-1})roman_Î˜ ( roman_log roman_log italic_n + roman_log italic_Îµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) Î˜â¢(logâ¡n/Îµ2)Î˜ğ‘›superscriptğœ€2\Theta(\log n/\varepsilon^{2})roman_Î˜ ( roman_log italic_n / italic_Îµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) Î˜~â¢(n1âˆ’2/p/Îµ2)~Î˜superscriptğ‘›12ğ‘superscriptğœ€2\tilde{\Theta}(n^{1-2/p}/\varepsilon^{2})over~ start_ARG roman_Î˜ end_ARG ( italic_n start_POSTSUPERSCRIPT 1 - 2 / italic_p end_POSTSUPERSCRIPT / italic_Îµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) Figure 1: Space complexity of Fpsubscriptğ¹ğ‘F_{p}italic_F start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT-estimation. We also extend our lower bound to the range pâˆˆ(1,2]ğ‘12p\in(1,2]italic_p âˆˆ ( 1 , 2 ], which settles the space complexity of Fpsubscriptğ¹ğ‘F_{p}italic_F start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT-estimation for all values of pğ‘pitalic_p. See Figure 1 for a summary of the space complexity of Fpsubscriptğ¹ğ‘F_{p}italic_F start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT-estimation for all pâ‰¥0ğ‘0p\geq 0italic_p â‰¥ 0. Theorem. Fix pâˆˆ(1,2]ğ‘12p\in(1,2]italic_p âˆˆ ( 1 , 2 ]. Let ğ’œğ’œ\mathcal{A}caligraphic_A be a streaming algorithm that gives an (1Â±Îµ)plus-or-minus1ğœ€\left(1\pm\varepsilon\right)( 1 Â± italic_Îµ ) approximation to the Fpsubscriptğ¹ğ‘F_{p}italic_F start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT of its input stream, for some Îµâˆˆ(Î©â¢(nâˆ’1/p),4âˆ’1/(pâˆ’1))ğœ€Î©superscriptğ‘›1ğ‘superscript41ğ‘1\varepsilon\in\left(\Omega\left(n^{-1/p}\right),{4^{-1/(p-1)}}\right)italic_Îµ âˆˆ ( roman_Î© ( italic_n start_POSTSUPERSCRIPT - 1 / italic_p end_POSTSUPERSCRIPT ) , 4 start_POSTSUPERSCRIPT - 1 / ( italic_p - 1 ) end_POSTSUPERSCRIPT ), and succeeds with probability â‰¥2/3absent23\geq 2/3â‰¥ 2 / 3. Then, the space used by ğ’œğ’œ\mathcal{A}caligraphic_A is Î©â¢(logâ¡(Îµ1/pâ¢n)/Îµ2)Î©superscriptğœ€1ğ‘ğ‘›superscriptğœ€2\Omega\left(\log\left(\varepsilon^{1/p}n\right)/\varepsilon^{2}\right)roman_Î© ( roman_log ( italic_Îµ start_POSTSUPERSCRIPT 1 / italic_p end_POSTSUPERSCRIPT italic_n ) / italic_Îµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ). Most of the lower bounds for streaming problems are based on reductions from communication complexity. In [JW19], a natural barrier to prove a better than Î©~â¢(1/Îµ2)~Î©1superscriptğœ€2\tilde{\Omega}(1/\varepsilon^{2})over~ start_ARG roman_Î© end_ARG ( 1 / italic_Îµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) lower bound was shown: Even in a very strong model of communication, Oâ¢(1/Îµ2â‹…(logâ¡logâ¡n+logâ¡d+logâ¡Îµâˆ’1))ğ‘‚â‹…1superscriptğœ€2ğ‘›ğ‘‘superscriptğœ€1O\left(1/\varepsilon^{2}\cdot\left(\log\log n+\log d+\log\varepsilon^{-1}% \right)\right)italic_O ( 1 / italic_Îµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT â‹… ( roman_log roman_log italic_n + roman_log italic_d + roman_log italic_Îµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) ) bits of communication suffice for the players to correctly produce a Fpsubscriptğ¹ğ‘F_{p}italic_F start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT estimation, where dğ‘‘ditalic_d is the diameter of the communication graph. This means that problems who reduce to Fpsubscriptğ¹ğ‘F_{p}italic_F start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT-estimation have a too low communication complexity to improve the existing lower bounds. To overcome this natural barrier, we present a new type of a direct sum theorem that takes place at the level of the streaming algorithm rather than the level of the communication model â€” informally, we pack many instances of problems with communication complexity Î˜â¢(1/Îµ2)Î˜1superscriptğœ€2\Theta(1/\varepsilon^{2})roman_Î˜ ( 1 / italic_Îµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) into a single stream, and then directly show that a successful streaming algorithm must solve them all. In Section 2 we give a detailed high-level overview of our proofs. The lower bound is presented in Section 4 and Section 5, and then extended from p=2ğ‘2p=2italic_p = 2 to pâˆˆ(1,2]ğ‘12p\in(1,2]italic_p âˆˆ ( 1 , 2 ] in Section 5.3. The improved upper bound is presented in Section 6. We conclude and present remaining open problems in Section 7."
https://arxiv.org/html/2411.01486v1,The Gap Between Greedy Algorithm and Minimum Multiplicative Spanner,"Given any undirected graph G=(V,E)ğºğ‘‰ğ¸G=(V,E)italic_G = ( italic_V , italic_E ) with nğ‘›nitalic_n vertices, if its subgraph HâŠ†Gğ»ğºH\subseteq Gitalic_H âŠ† italic_G satisfies ğ–½ğ—‚ğ—Œğ—Hâ¢(u,v)â‰¤kâ‹…ğ–½ğ—‚ğ—Œğ—Gâ¢(u,v)subscriptğ–½ğ—‚ğ—Œğ—ğ»ğ‘¢ğ‘£â‹…ğ‘˜subscriptğ–½ğ—‚ğ—Œğ—ğºğ‘¢ğ‘£\mathsf{dist}_{H}(u,v)\leq k\cdot\mathsf{dist}_{G}(u,v)sansserif_dist start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ( italic_u , italic_v ) â‰¤ italic_k â‹… sansserif_dist start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_u , italic_v ) for any two vertices (u,v)âˆˆVÃ—Vğ‘¢ğ‘£ğ‘‰ğ‘‰(u,v)\in V\times V( italic_u , italic_v ) âˆˆ italic_V Ã— italic_V, we call Hğ»Hitalic_H a kğ‘˜kitalic_k-spanner of GğºGitalic_G. The greedy algorithm adapted from Kruskalâ€™s algorithm is an efficient and folklore way to produce a kğ‘˜kitalic_k-spanner with girth at least k+2ğ‘˜2k+2italic_k + 2. The greedy algorithm has shown to be â€˜existentially optimalâ€™, while itâ€™s not â€˜universally optimalâ€™ for any constant kğ‘˜kitalic_k. Here, â€˜universal optimalityâ€™ means an algorithm can produce the smallest kğ‘˜kitalic_k-spanner Hğ»Hitalic_H given any nğ‘›nitalic_n-vertex input graph GğºGitalic_G.However, how well the greedy algorithm works compared to â€˜universal optimalityâ€™ is still unclear for superconstant k:=kâ¢(n)assignğ‘˜ğ‘˜ğ‘›k:=k(n)italic_k := italic_k ( italic_n ). In this paper, we aim to give a new and fine-grained analysis of this problem in undirected unweighted graph setting. Specifically, we show some bounds on this problem including the following twoOn the negative side, when k<13â¢nâˆ’Oâ¢(1)ğ‘˜13ğ‘›ğ‘‚1k<\frac{1}{3}n-O(1)italic_k < divide start_ARG 1 end_ARG start_ARG 3 end_ARG italic_n - italic_O ( 1 ), the greedy algorithm is not â€˜universally optimalâ€™.On the positive side, when k>23â¢n+Oâ¢(1)ğ‘˜23ğ‘›ğ‘‚1k>\frac{2}{3}n+O(1)italic_k > divide start_ARG 2 end_ARG start_ARG 3 end_ARG italic_n + italic_O ( 1 ), the greedy algorithm is â€˜universally optimalâ€™.We also introduce an appropriate notion for â€˜approximately universal optimalityâ€™. An algorithm is (Î±,Î²)ğ›¼ğ›½(\alpha,\beta)( italic_Î± , italic_Î² )-universally optimal iff given any nğ‘›nitalic_n-vertex input graph GğºGitalic_G, it can produce a kğ‘˜kitalic_k-spanner Hğ»Hitalic_H of GğºGitalic_G with size |H|â‰¤n+Î±â¢(|Hâˆ—|âˆ’n)+Î²ğ»ğ‘›ğ›¼superscriptğ»ğ‘›ğ›½|H|\leq n+\alpha(|H^{*}|-n)+\beta| italic_H | â‰¤ italic_n + italic_Î± ( | italic_H start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT | - italic_n ) + italic_Î², where Hâˆ—superscriptğ»H^{*}italic_H start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT is the smallest kğ‘˜kitalic_k-spanner of GğºGitalic_G. We show the following positive bounds.When k>47â¢n+Oâ¢(1)ğ‘˜47ğ‘›ğ‘‚1k>\frac{4}{7}n+O(1)italic_k > divide start_ARG 4 end_ARG start_ARG 7 end_ARG italic_n + italic_O ( 1 ), the greedy algorithm is (2,Oâ¢(1))2ğ‘‚1(2,O(1))( 2 , italic_O ( 1 ) )-universally optimal.When k>1223â¢n+Oâ¢(1)ğ‘˜1223ğ‘›ğ‘‚1k>\frac{12}{23}n+O(1)italic_k > divide start_ARG 12 end_ARG start_ARG 23 end_ARG italic_n + italic_O ( 1 ), the greedy algorithm is (18,Oâ¢(1))18ğ‘‚1(18,O(1))( 18 , italic_O ( 1 ) )-universally optimal.When k>12â¢n+Oâ¢(1)ğ‘˜12ğ‘›ğ‘‚1k>\frac{1}{2}n+O(1)italic_k > divide start_ARG 1 end_ARG start_ARG 2 end_ARG italic_n + italic_O ( 1 ), the greedy algorithm is (32,Oâ¢(1))32ğ‘‚1(32,O(1))( 32 , italic_O ( 1 ) )-universally optimal.All our proofs are constructive building on new structural analysis on spanners. We give some ideas about how to break small cycles in a spanner to increase the girth. These ideas may help us to understand the relation between girth and spanners.","A spanner of a graph is a subgraph that approximately preserves distances between all vertex pairs in the original graph. This notion was introduced by [PS89] and widely used in many areas such as graph algorithms [CKL+22, DJWW22], distributed networks [Awe85, BCLR86] and chip design [CKR+91, CKR+92]. In most applications, people aim to construct spanners as sparse as possible. The sparsity is measured by the number of edges in the spanner. In this paper, we will focus on multiplicative spanners for undirected unweighted graphs defined below. Unless otherwise stated, all graphs in this paper are undirected unweighted graphs. Definition 1.1. Given k:=kâ¢(n)assignğ‘˜ğ‘˜ğ‘›k:=k(n)italic_k := italic_k ( italic_n ) and an undirected unweighted graph G:=(V,E)assignğºğ‘‰ğ¸G:=(V,E)italic_G := ( italic_V , italic_E ) with nğ‘›nitalic_n vertices, a kğ‘˜kitalic_k-spanner of GğºGitalic_G is a subgraph H=(V,Eâ€²),Eâ€²âŠ†Eformulae-sequenceğ»ğ‘‰superscriptğ¸â€²superscriptğ¸â€²ğ¸H=(V,E^{\prime}),E^{\prime}\subseteq Eitalic_H = ( italic_V , italic_E start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) , italic_E start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT âŠ† italic_E of GğºGitalic_G such that for any vertex pair (u,v)âˆˆVÃ—Vğ‘¢ğ‘£ğ‘‰ğ‘‰(u,v)\in V\times V( italic_u , italic_v ) âˆˆ italic_V Ã— italic_V, there is ğ–½ğ—‚ğ—Œğ—Hâ¢(u,v)â‰¤kâ‹…ğ–½ğ—‚ğ—Œğ—Gâ¢(u,v)subscriptğ–½ğ—‚ğ—Œğ—ğ»ğ‘¢ğ‘£â‹…ğ‘˜subscriptğ–½ğ—‚ğ—Œğ—ğºğ‘¢ğ‘£\mathsf{dist}_{H}(u,v)\leq k\cdot\mathsf{dist}_{G}(u,v)sansserif_dist start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ( italic_u , italic_v ) â‰¤ italic_k â‹… sansserif_dist start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_u , italic_v ). We call kğ‘˜kitalic_k the stretch of Hğ»Hitalic_H. One of the simplest and most widely-used spanner construction algorithms is the greedy algorithm introduced by [ADD+93]. It is adapted from Kruskalâ€™s algorithm for computing the minimum spanning tree. In undirected unweighted settings, the greedy algorithm runs like this: Given any input graph G=(V,E)ğºğ‘‰ğ¸G=(V,E)italic_G = ( italic_V , italic_E ) and stretch kğ‘˜kitalic_k, we first choose an arbitrary total order Ïƒğœ\sigmaitalic_Ïƒ on its edges, denoted by Ïƒâ¢(E)=(e1<e2<â‹¯<em)ğœğ¸subscriptğ‘’1subscriptğ‘’2â‹¯subscriptğ‘’ğ‘š\sigma(E)=(e_{1}<e_{2}<\dots<e_{m})italic_Ïƒ ( italic_E ) = ( italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT < italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT < â‹¯ < italic_e start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ). Then, starting from empty subgraph H=(V,âˆ…)ğ»ğ‘‰H=(V,\varnothing)italic_H = ( italic_V , âˆ… ), we proceed edges {ei}iâˆˆ[m]subscriptsubscriptğ‘’ğ‘–ğ‘–delimited-[]ğ‘š\{e_{i}\}_{i\in[m]}{ italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i âˆˆ [ italic_m ] end_POSTSUBSCRIPT one by one. For each edge ei=(u,v)subscriptğ‘’ğ‘–ğ‘¢ğ‘£e_{i}=(u,v)italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = ( italic_u , italic_v ), if ğ–½ğ—‚ğ—Œğ—Hâ¢(u,v)>ksubscriptğ–½ğ—‚ğ—Œğ—ğ»ğ‘¢ğ‘£ğ‘˜\mathsf{dist}_{H}(u,v)>ksansserif_dist start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ( italic_u , italic_v ) > italic_k, we add eisubscriptğ‘’ğ‘–e_{i}italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT to edgeset of Hğ»Hitalic_H. Otherwise, we do nothing. After checking all mğ‘šmitalic_m edges, we output the latest Hğ»Hitalic_H as the spanner. [ADD+93] proved the following fact about Hğ»Hitalic_H Proposition 1.2. The subgraph Hğ»Hitalic_H outputted by the greedy algorithm with input âŸ¨G,kâŸ©ğºğ‘˜\langle G,k\rangleâŸ¨ italic_G , italic_k âŸ© is a kğ‘˜kitalic_k-spanner of GğºGitalic_G. Moreover, the girth of Hğ»Hitalic_H is at least k+2ğ‘˜2k+2italic_k + 2. Proposition 1.2 shows that for any graph G=(V,E)ğºğ‘‰ğ¸G=(V,E)italic_G = ( italic_V , italic_E ) with nğ‘›nitalic_n vertices and kğ‘˜kitalic_k, we can use the greedy algorithm to compute one of its kğ‘˜kitalic_k-spanners with girth at least k+2ğ‘˜2k+2italic_k + 2. A simple observation is that the backward direction also holds. Concretely, we have the following Proposition 1.3. Given any graph G=(V,E)ğºğ‘‰ğ¸G=(V,E)italic_G = ( italic_V , italic_E ) with nğ‘›nitalic_n vertices, stretch kğ‘˜kitalic_k and any of its kğ‘˜kitalic_k-spanner Hğ»Hitalic_H with girth at least k+2ğ‘˜2k+2italic_k + 2, there exists an edge ordering Ïƒğœ\sigmaitalic_Ïƒ of Eğ¸Eitalic_E such that the greedy algorithm outputs Hğ»Hitalic_H after running on Ïƒğœ\sigmaitalic_Ïƒ, with input âŸ¨G,kâŸ©ğºğ‘˜\langle G,k\rangleâŸ¨ italic_G , italic_k âŸ©. Proof. Let H=(V,Eâ€²âŠ†E)ğ»ğ‘‰superscriptğ¸â€²ğ¸H=(V,E^{\prime}\subseteq E)italic_H = ( italic_V , italic_E start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT âŠ† italic_E ). Consider the edge ordering Ïƒğœ\sigmaitalic_Ïƒ on Eğ¸Eitalic_E such that all edges in Eâ€²superscriptğ¸â€²E^{\prime}italic_E start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT appear earlier in Ïƒğœ\sigmaitalic_Ïƒ than any edge in E\Eâ€²\ğ¸superscriptğ¸â€²E\backslash E^{\prime}italic_E \ italic_E start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT. Let Hâ€²superscriptğ»â€²H^{\prime}italic_H start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT be the spanner outputted by the greedy algorithm running on Ïƒğœ\sigmaitalic_Ïƒ. Since we first proceed with edges in Eâ€²superscriptğ¸â€²E^{\prime}italic_E start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT, all edges in Eâ€²superscriptğ¸â€²E^{\prime}italic_E start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT will be in Hâ€²superscriptğ»â€²H^{\prime}italic_H start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT by the girth assumption. For any edge e=(u,v)âˆˆE\Eâ€²ğ‘’ğ‘¢ğ‘£\ğ¸superscriptğ¸â€²e=(u,v)\in E\backslash E^{\prime}italic_e = ( italic_u , italic_v ) âˆˆ italic_E \ italic_E start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT, since Hğ»Hitalic_H is a kğ‘˜kitalic_k-spanner of GğºGitalic_G, there must be ğ–½ğ—‚ğ—Œğ—Hâ¢(u,v)â‰¤ksubscriptğ–½ğ—‚ğ—Œğ—ğ»ğ‘¢ğ‘£ğ‘˜\mathsf{dist}_{H}(u,v)\leq ksansserif_dist start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ( italic_u , italic_v ) â‰¤ italic_k, and therefore eğ‘’eitalic_e wonâ€™t be added in Hâ€²superscriptğ»â€²H^{\prime}italic_H start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT after all edges in Eâ€²superscriptğ¸â€²E^{\prime}italic_E start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT have been added. We conclude Hâ€²=Hsuperscriptğ»â€²ğ»H^{\prime}=Hitalic_H start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT = italic_H. âˆ Proposition 1.3 shows that: Given âŸ¨G=(V,E),kâŸ©delimited-âŸ¨âŸ©ğºğ‘‰ğ¸ğ‘˜\langle G=(V,E),k\rangleâŸ¨ italic_G = ( italic_V , italic_E ) , italic_k âŸ©, the set of subgraphs the greedy algorithm can output (by choosing arbitrary edge ordering) is exactly the set of kğ‘˜kitalic_k-spanners with girth at least k+2ğ‘˜2k+2italic_k + 2. By the Moore bound [AHL02], all graphs Hğ»Hitalic_H with girth at least k+2ğ‘˜2k+2italic_k + 2 and nğ‘›nitalic_n vertices have at most m=Oâ¢(n1+2/k)ğ‘šğ‘‚superscriptğ‘›12ğ‘˜m=O(n^{1+2/k})italic_m = italic_O ( italic_n start_POSTSUPERSCRIPT 1 + 2 / italic_k end_POSTSUPERSCRIPT ) edges. Therefore, we can get a size upper bound of kğ‘˜kitalic_k-spanners possibly outputted by the greedy algorithm. There are also papers studying this upper bound on weighted graph setting [ADD+93, CDNS95, ENS14, Bod24]. Then, a natural question arises: How large is the gap between the minimum (optimal) spanners and spanners outputted by the greedy algorithm? Here, the minimum kğ‘˜kitalic_k-spanner of a graph GğºGitalic_G is its kğ‘˜kitalic_k-spanner with the fewest edges. In a seminal work [FS20], it is shown that the greedy algorithm is existentially optimal. Namely, for any n,kğ‘›ğ‘˜n,kitalic_n , italic_k, they show that there exists an nğ‘›nitalic_n-vertex graph GğºGitalic_G and its minimum kğ‘˜kitalic_k-spanner Hâˆ—superscriptğ»H^{*}italic_H start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT such that any kğ‘˜kitalic_k-spanner Hğ»Hitalic_H outputted by the greedy algorithm running on any nğ‘›nitalic_n-vertex input graph is no larger than Hâˆ—superscriptğ»H^{*}italic_H start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT. A more classical and difficult notion is universally optimal. It requires the greedy algorithm to output a minimum kğ‘˜kitalic_k-spanner for any nğ‘›nitalic_n-vertex graph. However, [FS20] also shows that for many parameter settings (n,k)ğ‘›ğ‘˜(n,k)( italic_n , italic_k ), the greedy algorithm for weighted graphs is â€˜far fromâ€™ universally optimal on stretch kğ‘˜kitalic_k and nğ‘›nitalic_n-vertex graphs. It means that the greedy algorithm cannot be universally optimal on general parameters. On the other hand, there is still hope that the greedy algorithm is universally optimal on some specific parameters (n,k)ğ‘›ğ‘˜(n,k)( italic_n , italic_k ). In fact, a simple observation is that when we set k=nâˆ’1ğ‘˜ğ‘›1k=n-1italic_k = italic_n - 1, the kğ‘˜kitalic_k-spanners of nğ‘›nitalic_n-vertex graphs are actually equivalent to their reachability preservers, and we can assert that the greedy algorithm is universally optimal on parameters (n,k=nâˆ’1)ğ‘›ğ‘˜ğ‘›1(n,k=n-1)( italic_n , italic_k = italic_n - 1 ) since reachability preservers are just spanning trees, which can be outputted by the greedy algorithm. It is natural to ask: for what parameters (n,k)ğ‘›ğ‘˜(n,k)( italic_n , italic_k ), the greedy algorithm is universally optimal? That is, it outputs a minimum kğ‘˜kitalic_k-spanner for any nğ‘›nitalic_n-vertex graphs. In this paper, we will try to partially answer this question. 1.1 Our Results We first define some terms, which is closely related to the performance of the greedy algorithm. Definition 1.4. For any pair (n,k)âˆˆâ„•Ã—â„•ğ‘›ğ‘˜â„•â„•(n,k)\in\mathbb{N}\times\mathbb{N}( italic_n , italic_k ) âˆˆ blackboard_N Ã— blackboard_N, we define: (a) If for any nğ‘›nitalic_n-vertex graph, all of its minimum kğ‘˜kitalic_k-spanners have girth at least k+2ğ‘˜2k+2italic_k + 2, we call (n,k)ğ‘›ğ‘˜(n,k)( italic_n , italic_k ) an extremely good pair. (b) If for any nğ‘›nitalic_n-vertex graph, at least one of its minimum kğ‘˜kitalic_k-spanners has girth at least k+2ğ‘˜2k+2italic_k + 2, we call (n,k)ğ‘›ğ‘˜(n,k)( italic_n , italic_k ) a good pair. (c) If for any nğ‘›nitalic_n-vertex graph GğºGitalic_G, all of its minimum kğ‘˜kitalic_k-spanners can be outputted by the greedy algorithm on some edge ordering Ïƒğœ\sigmaitalic_Ïƒ, we say the greedy algorithm is completely universally optimal on (n,k)ğ‘›ğ‘˜(n,k)( italic_n , italic_k ). (d) If for any nğ‘›nitalic_n-vertex graph GğºGitalic_G, there exists one of its minimum kğ‘˜kitalic_k-spanners that can be outputted by the greedy algorithm on some edge ordering Ïƒğœ\sigmaitalic_Ïƒ, we say the greedy algorithm is universally optimal on (n,k)ğ‘›ğ‘˜(n,k)( italic_n , italic_k ). From Proposition 1.3, we can connect the above notions Corollary 1.5. For any pair (n,k)âˆˆâ„•Ã—â„•ğ‘›ğ‘˜â„•â„•(n,k)\in\mathbb{N}\times\mathbb{N}( italic_n , italic_k ) âˆˆ blackboard_N Ã— blackboard_N, we have: If (n,k)ğ‘›ğ‘˜(n,k)( italic_n , italic_k ) is extremely good, then the greedy algorithm is completely universal optimal on (n,k)ğ‘›ğ‘˜(n,k)( italic_n , italic_k ). If (n,k)ğ‘›ğ‘˜(n,k)( italic_n , italic_k ) is good, then the greedy algorithm is universally optimal on (n,k)ğ‘›ğ‘˜(n,k)( italic_n , italic_k ). By Corollary 1.5, if we want to argue (completely) universal optimality on specific (n,k)ğ‘›ğ‘˜(n,k)( italic_n , italic_k ), it suffices to identify the category of (n,k)ğ‘›ğ‘˜(n,k)( italic_n , italic_k ): extremely good, good or â€˜not even goodâ€™. Lower Bound First, we give a lower (negative) bound. It asserts that if kğ‘˜kitalic_k is too small compared to nğ‘›nitalic_n, then (n,k)ğ‘›ğ‘˜(n,k)( italic_n , italic_k ) is not even a good pair. Theorem 1.6. For every sufficiently large nğ‘›nitalic_n and Oâ¢(1)<k<13â¢nâˆ’Oâ¢(1)ğ‘‚1ğ‘˜13ğ‘›ğ‘‚1O(1)<k<\frac{1}{3}n-O(1)italic_O ( 1 ) < italic_k < divide start_ARG 1 end_ARG start_ARG 3 end_ARG italic_n - italic_O ( 1 ), (n,k)ğ‘›ğ‘˜(n,k)( italic_n , italic_k ) is not a good pair. In fact, the proof of Theorem 1.6 is constructive: We can construct a graph GğºGitalic_G with nğ‘›nitalic_n vertices such that all of its minimum kğ‘˜kitalic_k-spanners have girth at most k+1ğ‘˜1k+1italic_k + 1. Upper Bound Extremely good (n,k)ğ‘›ğ‘˜(n,k)( italic_n , italic_k ) is the strongest definition, so we first find an upper (positive) bound about it. Our proof is algorithmic. Concretely, if kğ‘˜kitalic_k is large enough compared to nğ‘›nitalic_n, for any kğ‘˜kitalic_k-spanner Hğ»Hitalic_H of nğ‘›nitalic_n-vertex graph GğºGitalic_G with girth at most k+1ğ‘˜1k+1italic_k + 1, we can algorithmically remove some edges of Hğ»Hitalic_H such that Hğ»Hitalic_H is still a kğ‘˜kitalic_k-spanner after removal. Theorem 1.7. There is a deterministic polynomial time algorithm Ağ´Aitalic_A such that for all sufficiently large nğ‘›nitalic_n and any k>34â¢n+Oâ¢(1)ğ‘˜34ğ‘›ğ‘‚1k>\frac{3}{4}n+O(1)italic_k > divide start_ARG 3 end_ARG start_ARG 4 end_ARG italic_n + italic_O ( 1 ), given any nğ‘›nitalic_n-vertex graph GğºGitalic_G and its kğ‘˜kitalic_k-spanner HâŠ†Gğ»ğºH\subseteq Gitalic_H âŠ† italic_G, Aâ¢(G,H,k)ğ´ğºğ»ğ‘˜A(G,H,k)italic_A ( italic_G , italic_H , italic_k ) outputs a subgraph RâŠ†Hğ‘…ğ»R\subseteq Hitalic_R âŠ† italic_H of Hğ»Hitalic_H such that Rğ‘…Ritalic_R is a kğ‘˜kitalic_k-spanner of GğºGitalic_G and Rğ‘…Ritalic_R has girth at least k+2ğ‘˜2k+2italic_k + 2. This immediately gives us an upper bound for extremely good (n,k)ğ‘›ğ‘˜(n,k)( italic_n , italic_k ). Corollary 1.8. For all sufficiently large nğ‘›nitalic_n and any k>34â¢n+Oâ¢(1)ğ‘˜34ğ‘›ğ‘‚1k>\frac{3}{4}n+O(1)italic_k > divide start_ARG 3 end_ARG start_ARG 4 end_ARG italic_n + italic_O ( 1 ), (n,k)ğ‘›ğ‘˜(n,k)( italic_n , italic_k ) is extremely good. Proof. Suppose by contradiction that for some nğ‘›nitalic_n-vertex GğºGitalic_G, it has some minimum kğ‘˜kitalic_k-spanner Hğ»Hitalic_H with girth at most k+1ğ‘˜1k+1italic_k + 1. Then by Theorem 1.7 we can construct its subgraph RâŠ†Hğ‘…ğ»R\subseteq Hitalic_R âŠ† italic_H such that Rğ‘…Ritalic_R is also a kğ‘˜kitalic_k-spanner of GğºGitalic_G. Since Rğ‘…Ritalic_R has strictly larger girth than Hğ»Hitalic_H, Rğ‘…Ritalic_R must be a strictly smaller kğ‘˜kitalic_k-spanner of GğºGitalic_G than Hğ»Hitalic_H, which contradicts the assumption that Hğ»Hitalic_H is the minimum kğ‘˜kitalic_k-spanner. âˆ We can also give an upper bound using a similar but more complicated algorithm for the weaker notion, good pair (n,k)ğ‘›ğ‘˜(n,k)( italic_n , italic_k ). Theorem 1.9. There is a deterministic polynomial time algorithm Ağ´Aitalic_A such that for all sufficiently large nğ‘›nitalic_n and any k>23â¢n+Oâ¢(1)ğ‘˜23ğ‘›ğ‘‚1k>\frac{2}{3}n+O(1)italic_k > divide start_ARG 2 end_ARG start_ARG 3 end_ARG italic_n + italic_O ( 1 ), given any nğ‘›nitalic_n-vertex graph GğºGitalic_G and its kğ‘˜kitalic_k-spanner H=(V,EH)âŠ†Gğ»ğ‘‰subscriptğ¸ğ»ğºH=(V,E_{H})\subseteq Gitalic_H = ( italic_V , italic_E start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ) âŠ† italic_G, Aâ¢(G,H,k)ğ´ğºğ»ğ‘˜A(G,H,k)italic_A ( italic_G , italic_H , italic_k ) outputs R=(V,ER)âŠ†Gğ‘…ğ‘‰subscriptğ¸ğ‘…ğºR=(V,E_{R})\subseteq Gitalic_R = ( italic_V , italic_E start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT ) âŠ† italic_G such that Rğ‘…Ritalic_R is a kğ‘˜kitalic_k-spanner of GğºGitalic_G with girth at least k+2ğ‘˜2k+2italic_k + 2. Moreover, the size of Rğ‘…Ritalic_R satisfies |ER|â‰¤|EH|subscriptğ¸ğ‘…subscriptğ¸ğ»|E_{R}|\leq|E_{H}|| italic_E start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT | â‰¤ | italic_E start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT |. Similarly, this immediately gives us an upper bound for good pairs Corollary 1.10. For all sufficiently large nğ‘›nitalic_n and any k>23â¢n+Oâ¢(1)ğ‘˜23ğ‘›ğ‘‚1k>\frac{2}{3}n+O(1)italic_k > divide start_ARG 2 end_ARG start_ARG 3 end_ARG italic_n + italic_O ( 1 ), (n,k)ğ‘›ğ‘˜(n,k)( italic_n , italic_k ) is good. Approximately Universally Optimal By the previous results, we have known whether the greedy algorithm is universally optimal when k<13â¢nâˆ’Oâ¢(1)ğ‘˜13ğ‘›ğ‘‚1k<\frac{1}{3}n-O(1)italic_k < divide start_ARG 1 end_ARG start_ARG 3 end_ARG italic_n - italic_O ( 1 ) or k>23â¢n+Oâ¢(1)ğ‘˜23ğ‘›ğ‘‚1k>\frac{2}{3}n+O(1)italic_k > divide start_ARG 2 end_ARG start_ARG 3 end_ARG italic_n + italic_O ( 1 ). It remains open when 13â¢n<k<23â¢n13ğ‘›ğ‘˜23ğ‘›\frac{1}{3}n<k<\frac{2}{3}ndivide start_ARG 1 end_ARG start_ARG 3 end_ARG italic_n < italic_k < divide start_ARG 2 end_ARG start_ARG 3 end_ARG italic_n. To understand the gap between the greedy algorithm and minimum spanners in this range, we need to introduce a notion called â€˜approximately universally optimalâ€™. Since we actually have a polynomial time algorithm in Theorem 1.9, we can consider slacking the size requirement of outputted spanner Rğ‘…Ritalic_R. Concretely, rather than requiring |ER|â‰¤|EH|subscriptğ¸ğ‘…subscriptğ¸ğ»|E_{R}|\leq|E_{H}|| italic_E start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT | â‰¤ | italic_E start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT |, we can output a kğ‘˜kitalic_k-spanner Rğ‘…Ritalic_R with girth at least k+2ğ‘˜2k+2italic_k + 2 that is just â€˜slightlyâ€™ larger than Hğ»Hitalic_H. In the classical approximation algorithm setting, we use a multiplicative slack factor. Formally, Suppose Hğ»Hitalic_H is the minimum kğ‘˜kitalic_k-spanner, if our outputted large-girth kğ‘˜kitalic_k-spanner Rğ‘…Ritalic_R has size |ER|â‰¤Î±â¢|EH|subscriptğ¸ğ‘…ğ›¼subscriptğ¸ğ»|E_{R}|\leq\alpha|E_{H}|| italic_E start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT | â‰¤ italic_Î± | italic_E start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT |, we can call it Î±ğ›¼\alphaitalic_Î±-optimal. However, there is a critical issue about the multiplicative approximation factor in our setting. Since we are considering the case when k>Î©â¢(n)ğ‘˜Î©ğ‘›k>\Omega(n)italic_k > roman_Î© ( italic_n ), by Moore bound [AHL02] the classical greedy algorithm will always give a kğ‘˜kitalic_k-spanner with girth at least k+2ğ‘˜2k+2italic_k + 2 and size at most n1+Oâ¢(1/n)=n+Oâ¢(logâ¡n)superscriptğ‘›1ğ‘‚1ğ‘›ğ‘›ğ‘‚ğ‘›n^{1+O(1/n)}=n+O(\log{n})italic_n start_POSTSUPERSCRIPT 1 + italic_O ( 1 / italic_n ) end_POSTSUPERSCRIPT = italic_n + italic_O ( roman_log italic_n ). Since a trivial lower bound on the size of minimum kğ‘˜kitalic_k-spanner H=(V,EH)ğ»ğ‘‰subscriptğ¸ğ»H=(V,E_{H})italic_H = ( italic_V , italic_E start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ) of a connected graph is |EH|â‰¥nâˆ’1subscriptğ¸ğ»ğ‘›1|E_{H}|\geq n-1| italic_E start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT | â‰¥ italic_n - 1, any constant multiplicative approximation factor Î±=1+Î©â¢(1)ğ›¼1Î©1\alpha=1+\Omega(1)italic_Î± = 1 + roman_Î© ( 1 ) will become trivial since the well-known greedy algorithm always outputs a kğ‘˜kitalic_k-spanner with size n+oâ¢(n)â‰¤(1+oâ¢(1))â¢|EH|â‰¤Î±â¢|EH|ğ‘›ğ‘œğ‘›1ğ‘œ1subscriptğ¸ğ»ğ›¼subscriptğ¸ğ»n+o(n)\leq(1+o(1))|E_{H}|\leq\alpha|E_{H}|italic_n + italic_o ( italic_n ) â‰¤ ( 1 + italic_o ( 1 ) ) | italic_E start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT | â‰¤ italic_Î± | italic_E start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT |. These spanners are usually called ultrasparse spanners [Pet08, BEG+22]. Therefore, we must introduce a more refined notion of â€˜approximationâ€™. The first observation is that since different connected components are isolated when considering spanners, we can only focus on connected graphs without loss of generality. For any connected nğ‘›nitalic_n-vertex graph G=(V,E)ğºğ‘‰ğ¸G=(V,E)italic_G = ( italic_V , italic_E ) and its minimum kğ‘˜kitalic_k-spanner H=(V,EH)ğ»ğ‘‰subscriptğ¸ğ»H=(V,E_{H})italic_H = ( italic_V , italic_E start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ), since |EH|â‰¥nâˆ’1subscriptğ¸ğ»ğ‘›1|E_{H}|\geq n-1| italic_E start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT | â‰¥ italic_n - 1 is a trivial lower bound, we can use Î±=|ER|âˆ’n|EH|âˆ’nğ›¼subscriptğ¸ğ‘…ğ‘›subscriptğ¸ğ»ğ‘›\alpha=\frac{|E_{R}|-n}{|E_{H}|-n}italic_Î± = divide start_ARG | italic_E start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT | - italic_n end_ARG start_ARG | italic_E start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT | - italic_n end_ARG as a new definition for the approximation factor, where R=(V,ER)ğ‘…ğ‘‰subscriptğ¸ğ‘…R=(V,E_{R})italic_R = ( italic_V , italic_E start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT ) denotes the kğ‘˜kitalic_k-spanner of GğºGitalic_G outputted by some approximation algorithm. Definition 1.11. For any pair (n,k)ğ‘›ğ‘˜(n,k)( italic_n , italic_k ) and constants Î±>1,Î²â‰¥0formulae-sequenceğ›¼1ğ›½0\alpha>1,\beta\geq 0italic_Î± > 1 , italic_Î² â‰¥ 0, if any nğ‘›nitalic_n-vertex connected graph G=(V,E)ğºğ‘‰ğ¸G=(V,E)italic_G = ( italic_V , italic_E ) has a kğ‘˜kitalic_k-spanner H=(V,EH)ğ»ğ‘‰subscriptğ¸ğ»H=(V,E_{H})italic_H = ( italic_V , italic_E start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ) with girth at least k+2ğ‘˜2k+2italic_k + 2 such that |EH|âˆ’nâ‰¤Î±â¢(ğ–®ğ–¯ğ–³âˆ’n)+Î²subscriptğ¸ğ»ğ‘›ğ›¼ğ–®ğ–¯ğ–³ğ‘›ğ›½|E_{H}|-n\leq\alpha(\mathsf{OPT}-n)+\beta| italic_E start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT | - italic_n â‰¤ italic_Î± ( sansserif_OPT - italic_n ) + italic_Î², we call (n,k)ğ‘›ğ‘˜(n,k)( italic_n , italic_k ) an (Î±,Î²)ğ›¼ğ›½(\alpha,\beta)( italic_Î± , italic_Î² )-approx good pair. Here ğ–®ğ–¯ğ–³ğ–®ğ–¯ğ–³\mathsf{OPT}sansserif_OPT denotes the number of edges in any minimum kğ‘˜kitalic_k-spanner of GğºGitalic_G. We can observe that the fact (n,k)ğ‘›ğ‘˜(n,k)( italic_n , italic_k ) is (Î±,Î²)ğ›¼ğ›½(\alpha,\beta)( italic_Î± , italic_Î² )-approx good is equivalent to â€˜(Î±,Î²)ğ›¼ğ›½(\alpha,\beta)( italic_Î± , italic_Î² )-approximately universal optimalityâ€™ of the greedy algorithm. Namely, the greedy algorithm is â€˜(Î±,Î²)ğ›¼ğ›½(\alpha,\beta)( italic_Î± , italic_Î² )-approximately universally optimalâ€™ on (n,k)ğ‘›ğ‘˜(n,k)( italic_n , italic_k ) iff for any nğ‘›nitalic_n-vertex graph GğºGitalic_G, the greedy algorithm can output a (Î±,Î²)ğ›¼ğ›½(\alpha,\beta)( italic_Î± , italic_Î² )-approximately minimum kğ‘˜kitalic_k-spanner of GğºGitalic_G on some edge ordering Ïƒğœ\sigmaitalic_Ïƒ. Using the above definitions, we can try to understand the power of the greedy algorithm when 13â¢nâ‰¤kâ‰¤23â¢n13ğ‘›ğ‘˜23ğ‘›\frac{1}{3}n\leq k\leq\frac{2}{3}ndivide start_ARG 1 end_ARG start_ARG 3 end_ARG italic_n â‰¤ italic_k â‰¤ divide start_ARG 2 end_ARG start_ARG 3 end_ARG italic_n. The first bound is the following Theorem 1.12. There is a deterministic polynomial time algorithm Ağ´Aitalic_A such that for all sufficiently large nğ‘›nitalic_n and any k>47â¢n+Oâ¢(1)ğ‘˜47ğ‘›ğ‘‚1k>\frac{4}{7}n+O(1)italic_k > divide start_ARG 4 end_ARG start_ARG 7 end_ARG italic_n + italic_O ( 1 ), given any nğ‘›nitalic_n-vertex graph GğºGitalic_G and its kğ‘˜kitalic_k-spanner H=(V,EH)âŠ†Gğ»ğ‘‰subscriptğ¸ğ»ğºH=(V,E_{H})\subseteq Gitalic_H = ( italic_V , italic_E start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ) âŠ† italic_G, Aâ¢(G,H,k)ğ´ğºğ»ğ‘˜A(G,H,k)italic_A ( italic_G , italic_H , italic_k ) outputs a kğ‘˜kitalic_k-spanner R=(V,ER)ğ‘…ğ‘‰subscriptğ¸ğ‘…R=(V,E_{R})italic_R = ( italic_V , italic_E start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT ) of GğºGitalic_G with girth at least k+2ğ‘˜2k+2italic_k + 2. Moreover, the size of Rğ‘…Ritalic_R satisfies |ER|âˆ’nâ‰¤2â¢(Eâ¢(H)âˆ’n)+1subscriptğ¸ğ‘…ğ‘›2ğ¸ğ»ğ‘›1|E_{R}|-n\leq 2(E(H)-n)+1| italic_E start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT | - italic_n â‰¤ 2 ( italic_E ( italic_H ) - italic_n ) + 1. As Corollaries 1.5 and 1.8, we can use Theorem 1.12 to derive the following Corollary 1.13. For all sufficiently large nğ‘›nitalic_n and every k>47â¢n+Oâ¢(1)ğ‘˜47ğ‘›ğ‘‚1k>\frac{4}{7}n+O(1)italic_k > divide start_ARG 4 end_ARG start_ARG 7 end_ARG italic_n + italic_O ( 1 ), (n,k)ğ‘›ğ‘˜(n,k)( italic_n , italic_k ) is (2,Oâ¢(1))2ğ‘‚1(2,O(1))( 2 , italic_O ( 1 ) )-approx good. We also generalize Theorem 1.12 (but use a less fine-grained way to analyze it) to smaller kğ‘˜kitalic_k but with a larger approximation factor as follows: Theorem 1.14. For tâˆˆ{1,2,3,4}ğ‘¡1234t\in\{1,2,3,4\}italic_t âˆˆ { 1 , 2 , 3 , 4 }, there is a deterministic polynomial time algorithm Ağ´Aitalic_A such that for all sufficiently large nğ‘›nitalic_n and any k>4â¢t9â¢tâˆ’4â¢n+Oâ¢(1)ğ‘˜4ğ‘¡9ğ‘¡4ğ‘›ğ‘‚1k>\frac{4t}{9t-4}n+O(1)italic_k > divide start_ARG 4 italic_t end_ARG start_ARG 9 italic_t - 4 end_ARG italic_n + italic_O ( 1 ), given any nğ‘›nitalic_n-vertex graph GğºGitalic_G and its kğ‘˜kitalic_k-spanner H=(V,EH)âŠ†Gğ»ğ‘‰subscriptğ¸ğ»ğºH=(V,E_{H})\subseteq Gitalic_H = ( italic_V , italic_E start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ) âŠ† italic_G, Aâ¢(G,H,k)ğ´ğºğ»ğ‘˜A(G,H,k)italic_A ( italic_G , italic_H , italic_k ) outputs a kğ‘˜kitalic_k-spanner R=(V,ER)ğ‘…ğ‘‰subscriptğ¸ğ‘…R=(V,E_{R})italic_R = ( italic_V , italic_E start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT ) of GğºGitalic_G with girth at least k+2ğ‘˜2k+2italic_k + 2. Moreover, the size of Rğ‘…Ritalic_R satisfies |ER|âˆ’nâ‰¤2â¢t2â¢(Eâ¢(H)âˆ’n)+2â¢t2subscriptğ¸ğ‘…ğ‘›2superscriptğ‘¡2ğ¸ğ»ğ‘›2superscriptğ‘¡2|E_{R}|-n\leq 2t^{2}(E(H)-n)+2t^{2}| italic_E start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT | - italic_n â‰¤ 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_E ( italic_H ) - italic_n ) + 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. When t=2ğ‘¡2t=2italic_t = 2, the requirement for kğ‘˜kitalic_k is just k>47â¢n+Oâ¢(1)ğ‘˜47ğ‘›ğ‘‚1k>\frac{4}{7}n+O(1)italic_k > divide start_ARG 4 end_ARG start_ARG 7 end_ARG italic_n + italic_O ( 1 ) which is the same as Theorem 1.12, but the approximation factor 2â¢t2=82superscriptğ‘¡282t^{2}=82 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 8 is worse. Theorem 1.14 is useful when t={3,4}ğ‘¡34t=\{3,4\}italic_t = { 3 , 4 }, which gives us k>1223â¢n+Oâ¢(1)ğ‘˜1223ğ‘›ğ‘‚1k>\frac{12}{23}n+O(1)italic_k > divide start_ARG 12 end_ARG start_ARG 23 end_ARG italic_n + italic_O ( 1 ) and k>12â¢n+Oâ¢(1)ğ‘˜12ğ‘›ğ‘‚1k>\frac{1}{2}n+O(1)italic_k > divide start_ARG 1 end_ARG start_ARG 2 end_ARG italic_n + italic_O ( 1 ) bounds respectively. In fact, Theorem 1.14 is a â€˜bucket-decompositionâ€™ generalized version of Theorem 1.12, and tğ‘¡titalic_t is a decomposition parameter that can be set to any positive integer. However, setting tâ‰¥5ğ‘¡5t\geq 5italic_t â‰¥ 5 cannot derive new approximation bounds so larger tğ‘¡titalic_t is useless in the current technique. We give a detailed explanation in Remark 4.8. Corollary 1.15. For tâˆˆ{1,2,3,4}ğ‘¡1234t\in\{1,2,3,4\}italic_t âˆˆ { 1 , 2 , 3 , 4 }, for all sufficiently large nğ‘›nitalic_n and any k>4â¢t9â¢tâˆ’4â¢n+Oâ¢(1)ğ‘˜4ğ‘¡9ğ‘¡4ğ‘›ğ‘‚1k>\frac{4t}{9t-4}n+O(1)italic_k > divide start_ARG 4 italic_t end_ARG start_ARG 9 italic_t - 4 end_ARG italic_n + italic_O ( 1 ), (n,k)ğ‘›ğ‘˜(n,k)( italic_n , italic_k ) is (2â¢t2,Oâ¢(1))2superscriptğ‘¡2ğ‘‚1(2t^{2},O(1))( 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , italic_O ( 1 ) )-approx good. Remark 1.16. All proofs of our upper (positive) bounds are â€˜partially constructiveâ€™. Informally, Theorems 1.7, 1.9, 1.12 and 1.14 all provide algorithms to enlarge the girth of an input kğ‘˜kitalic_k-spanner Hğ»Hitalic_H to at least k+2ğ‘˜2k+2italic_k + 2, and guarantee the outputted graph is still a kğ‘˜kitalic_k-spanner whose size wonâ€™t become too large compared to the size of Hğ»Hitalic_H. These algorithms can be appended behind some other good algorithms that donâ€™t care about girth as â€˜girth enlargerâ€™. For example, suppose there is an algorithm BğµBitalic_B that constructs very sparse spanners but doesnâ€™t guarantee the girth of the outputted spanner, we can feed its output spanner into our algorithms Ağ´Aitalic_A such that the final spanner outputted by Ağ´Aitalic_A has large girth, and at the same time it is only â€˜slightlyâ€™ larger than BğµBitalic_Bâ€™s original output. Figure 1: Different Bounds To the best of our knowledge, our proofs are built upon new ideas of analyzing the structure of spanners using the girth information. We hope these ideas could help us to understand the relation between girth and sparse spanners. Figure 1 is a summary of our main results. 1.2 Notations For any nâˆˆâ„•ğ‘›â„•n\in\mathbb{N}italic_n âˆˆ blackboard_N, let [n]={1,â€¦,n}delimited-[]ğ‘›1â€¦ğ‘›[n]=\{1,\dots,n\}[ italic_n ] = { 1 , â€¦ , italic_n }. For any undirected graph GğºGitalic_G, we use VGsubscriptğ‘‰ğºV_{G}italic_V start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT and EGsubscriptğ¸ğºE_{G}italic_E start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT to denote its vertex-set and edge-set unless otherwise stated. For any (u,v)âˆˆVGÃ—VGğ‘¢ğ‘£subscriptğ‘‰ğºsubscriptğ‘‰ğº(u,v)\in V_{G}\times V_{G}( italic_u , italic_v ) âˆˆ italic_V start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT Ã— italic_V start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT, we use ğ–½ğ—‚ğ—Œğ—Gâ¢(u,v)subscriptğ–½ğ—‚ğ—Œğ—ğºğ‘¢ğ‘£\mathsf{dist}_{G}(u,v)sansserif_dist start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_u , italic_v ) to denote the length of the shortest path between u,vğ‘¢ğ‘£u,vitalic_u , italic_v in GğºGitalic_G. For any vertex-subset UâŠ†VGğ‘ˆsubscriptğ‘‰ğºU\subseteq V_{G}italic_U âŠ† italic_V start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT, we use Gâ¢[U]ğºdelimited-[]ğ‘ˆG[U]italic_G [ italic_U ] to denote the induced subgraph of GğºGitalic_G on Uğ‘ˆUitalic_U. For any two graphs G,Hğºğ»G,Hitalic_G , italic_H, we use G\H\ğºğ»G\backslash Hitalic_G \ italic_H to denote the graph (VG\VH,EG\EH)\subscriptğ‘‰ğºsubscriptğ‘‰ğ»\subscriptğ¸ğºsubscriptğ¸ğ»(V_{G}\backslash V_{H},E_{G}\backslash E_{H})( italic_V start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT \ italic_V start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT , italic_E start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT \ italic_E start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ). We can similarly define G\VH:=(VG\VH,EG)assign\ğºsubscriptğ‘‰ğ»\subscriptğ‘‰ğºsubscriptğ‘‰ğ»subscriptğ¸ğºG\backslash V_{H}:=(V_{G}\backslash V_{H},E_{G})italic_G \ italic_V start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT := ( italic_V start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT \ italic_V start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT , italic_E start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ) and G\EH:=(VG,EG\EH)assign\ğºsubscriptğ¸ğ»subscriptğ‘‰ğº\subscriptğ¸ğºsubscriptğ¸ğ»G\backslash E_{H}:=(V_{G},E_{G}\backslash E_{H})italic_G \ italic_E start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT := ( italic_V start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT , italic_E start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT \ italic_E start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ). For a simple path pğ‘pitalic_p and any two vertices s,tâˆˆVpğ‘ ğ‘¡subscriptğ‘‰ğ‘s,t\in V_{p}italic_s , italic_t âˆˆ italic_V start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT, we use pâ¢[s,t]ğ‘ğ‘ ğ‘¡p[s,t]italic_p [ italic_s , italic_t ] to denote the sub-path between sğ‘ sitalic_s and tğ‘¡titalic_t. pâ¢[s,t),pâ¢(s,t],pâ¢(s,t)ğ‘ğ‘ ğ‘¡ğ‘ğ‘ ğ‘¡ğ‘ğ‘ ğ‘¡p[s,t),p(s,t],p(s,t)italic_p [ italic_s , italic_t ) , italic_p ( italic_s , italic_t ] , italic_p ( italic_s , italic_t ) are similarly defined. We use Sâ¢CGğ‘†subscriptğ¶ğºSC_{G}italic_S italic_C start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT to denote an arbitrary smallest cycle of GğºGitalic_G, and LG:=|VSâ¢CG|assignsubscriptğ¿ğºsubscriptğ‘‰ğ‘†subscriptğ¶ğºL_{G}:=|V_{SC_{G}}|italic_L start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT := | italic_V start_POSTSUBSCRIPT italic_S italic_C start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT end_POSTSUBSCRIPT |. 1.3 Paper Organization and Overview In Section 2, we give a brief illustration of the negative lower bound regarding the greedy algorithm as stated in Theorem 1.6. Then, in Section 3 we give detailed proofs of positive upper bounds Theorem 1.7 and Theorem 1.9. Finally, we will prove Theorem 1.12 and Theorem 1.14 in Section 4. Letâ€™s give an overview of our proof strategy used in establishing positive upper bounds. Let Hğ»Hitalic_H be a minimum (in fact arbitrary) kğ‘˜kitalic_k-spanner of GğºGitalic_G and Sâ¢CHğ‘†subscriptğ¶ğ»SC_{H}italic_S italic_C start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT denote its smallest cycle. Let LH=|VSâ¢CH|subscriptğ¿ğ»subscriptğ‘‰ğ‘†subscriptğ¶ğ»L_{H}=|V_{SC_{H}}|italic_L start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT = | italic_V start_POSTSUBSCRIPT italic_S italic_C start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT end_POSTSUBSCRIPT |. If LHâ‰¥k+2subscriptğ¿ğ»ğ‘˜2L_{H}\geq k+2italic_L start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT â‰¥ italic_k + 2, our girth lower bound has been satisfied and we are done. Otherwise, we want to find a cycle-edge eâˆˆESâ¢CHğ‘’subscriptğ¸ğ‘†subscriptğ¶ğ»e\in E_{SC_{H}}italic_e âˆˆ italic_E start_POSTSUBSCRIPT italic_S italic_C start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT end_POSTSUBSCRIPT and at most mğ‘šmitalic_m edges e1,â€¦,emâˆˆEGsubscriptğ‘’1â€¦subscriptğ‘’ğ‘šsubscriptğ¸ğºe_{1},\dots,e_{m}\in E_{G}italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_e start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT âˆˆ italic_E start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT, such that we can break the smallest cycle Sâ¢CHğ‘†subscriptğ¶ğ»SC_{H}italic_S italic_C start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT by removing eğ‘’eitalic_e, and then preserve the property of kğ‘˜kitalic_k-spanner by adding these mğ‘šmitalic_m edges. Formally, let Hâ€²superscriptğ»â€²H^{\prime}italic_H start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT denote (H\{e})âˆª{e1,â€¦,em}\ğ»ğ‘’subscriptğ‘’1â€¦subscriptğ‘’ğ‘š\left(H\backslash\{e\}\right)\cup\{e_{1},\dots,e_{m}\}( italic_H \ { italic_e } ) âˆª { italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_e start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT }, we want â€¢ Hâ€²superscriptğ»â€²H^{\prime}italic_H start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT is a kğ‘˜kitalic_k-spanner. â€¢ e1,â€¦,emsubscriptğ‘’1â€¦subscriptğ‘’ğ‘še_{1},\dots,e_{m}italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_e start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT donâ€™t create any new cycle with length at most k+1ğ‘˜1k+1italic_k + 1 in Hâ€²superscriptğ»â€²H^{\prime}italic_H start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT compared to Hğ»Hitalic_H. If for some pair (n,k)ğ‘›ğ‘˜(n,k)( italic_n , italic_k ), the above condition can be met, then the greedy algorithm is (m,Oâ¢(m))ğ‘šğ‘‚ğ‘š(m,O(m))( italic_m , italic_O ( italic_m ) )-Approximately universally optimal on (n,k)ğ‘›ğ‘˜(n,k)( italic_n , italic_k ). (See the proof of Theorem 4.4 for details.) Therefore, our goal is to prove the above statement in different mğ‘šmitalic_m and (n,k)ğ‘›ğ‘˜(n,k)( italic_n , italic_k ) settings. First, in Section 3.1 we will show that as long as LH>2â¢(nâˆ’k)subscriptğ¿ğ»2ğ‘›ğ‘˜L_{H}>2(n-k)italic_L start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT > 2 ( italic_n - italic_k ), we can achieve m=0ğ‘š0m=0italic_m = 0 regardless. It suffices to consider the case when LHâ‰¤2â¢(nâˆ’k)subscriptğ¿ğ»2ğ‘›ğ‘˜L_{H}\leq 2(n-k)italic_L start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT â‰¤ 2 ( italic_n - italic_k ): In Section 3.2, we give a common structural framework required in all following proofs. Then, we work on different parameter settings as follows â€¢ In Section 3.3, we will prove when k>34â¢n+Oâ¢(1)ğ‘˜34ğ‘›ğ‘‚1k>\frac{3}{4}n+O(1)italic_k > divide start_ARG 3 end_ARG start_ARG 4 end_ARG italic_n + italic_O ( 1 ), we can achieve m=0ğ‘š0m=0italic_m = 0, and derive Theorem 1.7. â€¢ In Section 3.4, we will prove when k>23â¢n+Oâ¢(1)ğ‘˜23ğ‘›ğ‘‚1k>\frac{2}{3}n+O(1)italic_k > divide start_ARG 2 end_ARG start_ARG 3 end_ARG italic_n + italic_O ( 1 ), we can achieve m=1ğ‘š1m=1italic_m = 1, which confirms Theorem 1.9. â€¢ In Section 4.1, we will prove when k>47â¢n+Oâ¢(1)ğ‘˜47ğ‘›ğ‘‚1k>\frac{4}{7}n+O(1)italic_k > divide start_ARG 4 end_ARG start_ARG 7 end_ARG italic_n + italic_O ( 1 ), we can achieve m=2ğ‘š2m=2italic_m = 2 and derive Theorem 1.12. The proof strategy is a generalization of Section 3.4 â€¢ In Section 4.2, we will prove when k>1223â¢n+Oâ¢(1)ğ‘˜1223ğ‘›ğ‘‚1k>\frac{12}{23}n+O(1)italic_k > divide start_ARG 12 end_ARG start_ARG 23 end_ARG italic_n + italic_O ( 1 ), we can achieve m=18ğ‘š18m=18italic_m = 18, and when k>12â¢n+Oâ¢(1)ğ‘˜12ğ‘›ğ‘‚1k>\frac{1}{2}n+O(1)italic_k > divide start_ARG 1 end_ARG start_ARG 2 end_ARG italic_n + italic_O ( 1 ), we can achieve m=32ğ‘š32m=32italic_m = 32. These two results follow from a â€˜bucket-decompositionâ€™ generalized version of Theorem 1.12."
https://arxiv.org/html/2411.01384v1,Near-Optimal Relative Error Streaming Quantile Estimationvia Elastic Compactors,"Computing the approximate quantiles or ranks of a stream is a fundamental task in data monitoring. Given a stream of elements x1,x2,â€¦,xnsubscriptğ‘¥1subscriptğ‘¥2â€¦subscriptğ‘¥ğ‘›x_{1},x_{2},\dots,x_{n}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT and a query xğ‘¥xitalic_x, a relative-error quantile estimation algorithm can estimate the rank of xğ‘¥xitalic_x with respect to the stream, up to a multiplicative Â±Ïµâ‹…rankâ¢(x)plus-or-minusâ‹…italic-Ïµrankğ‘¥\pm\epsilon\cdot\mathrm{rank}(x)Â± italic_Ïµ â‹… roman_rank ( italic_x ) error. Notably, this requires the sketch to obtain more precise estimates for the ranks of elements on the tails of the distribution, as compared to the additive Â±Ïµâ¢nplus-or-minusitalic-Ïµğ‘›\pm\epsilon nÂ± italic_Ïµ italic_n error regime. This is particularly favorable for some practical applications, such as anomaly detection.Previously, the best known algorithms for relative error achieved space O~â¢(Ïµâˆ’1â¢log1.5â¡(Ïµâ¢n))~ğ‘‚superscriptitalic-Ïµ1superscript1.5italic-Ïµğ‘›\tilde{O}(\epsilon^{-1}\log^{1.5}(\epsilon n))over~ start_ARG italic_O end_ARG ( italic_Ïµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log start_POSTSUPERSCRIPT 1.5 end_POSTSUPERSCRIPT ( italic_Ïµ italic_n ) ) (Cormode, Karnin, Liberty, Thaler, Veselá»³, 2021) and O~â¢(Ïµâˆ’2â¢logâ¡(Ïµâ¢n))~ğ‘‚superscriptitalic-Ïµ2italic-Ïµğ‘›\tilde{O}(\epsilon^{-2}\log(\epsilon n))over~ start_ARG italic_O end_ARG ( italic_Ïµ start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT roman_log ( italic_Ïµ italic_n ) ) (Zhang, Lin, Xu, Korn, Wang, 2006). In this work, we present a nearly-optimal streaming algorithm for the relative-error quantile estimation problem using O~â¢(Ïµâˆ’1â¢logâ¡(Ïµâ¢n))~ğ‘‚superscriptitalic-Ïµ1italic-Ïµğ‘›\tilde{O}(\epsilon^{-1}\log(\epsilon n))over~ start_ARG italic_O end_ARG ( italic_Ïµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log ( italic_Ïµ italic_n ) ) space, which almost matches the trivial Î©â¢(Ïµâˆ’1â¢logâ¡(Ïµâ¢n))Î©superscriptitalic-Ïµ1italic-Ïµğ‘›\Omega(\epsilon^{-1}\log(\epsilon n))roman_Î© ( italic_Ïµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log ( italic_Ïµ italic_n ) ) space lower bound.To surpass the Î©â¢(Ïµâˆ’1â¢log1.5â¡(Ïµâ¢n))Î©superscriptitalic-Ïµ1superscript1.5italic-Ïµğ‘›\Omega(\epsilon^{-1}\log^{1.5}(\epsilon n))roman_Î© ( italic_Ïµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log start_POSTSUPERSCRIPT 1.5 end_POSTSUPERSCRIPT ( italic_Ïµ italic_n ) ) barrier of the previous approach, our algorithm crucially relies on a new data structure, called an elastic compactor, which can be dynamically resized over the course of the stream. Interestingly, we design a space allocation scheme which adaptively allocates space to each compactor based on the â€œhardnessâ€ of the input stream. This approach allows us to avoid using the maximal space simultaneously for every compactor and facilitates the improvement in the total space complexity.Along the way, we also propose and study a new problem called the Top Quantiles Problem, which only requires the sketch to provide estimates for the ranks of elements in a fixed-length tail of the distribution. This problem serves as an important subproblem in our algorithm, though it is also an interesting problem of its own right.","Learning the distribution of data that are represented as a stream is an important task in streaming data analysis. A concrete problem that captures this task is the streaming quantile estimation problem. Given a stream of elements Ï€=(x1,x2,â€¦,xn)ğœ‹subscriptğ‘¥1subscriptğ‘¥2â€¦subscriptğ‘¥ğ‘›\pi=(x_{1},x_{2},\ldots,x_{n})italic_Ï€ = ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ), the quantile estimation problem asks us to process the stream, while maintaining a small memory that stores a few input elements, such that at the end of the stream, for any given query yğ‘¦yitalic_y, the algorithm must output an approximation of the rank of yğ‘¦yitalic_y in Ï€ğœ‹\piitalic_Ï€ with high probability, i.e., an approximation of rankÏ€â¢(y):=|{iâˆˆ[n]:xi<y}|assignsubscriptrankğœ‹ğ‘¦conditional-setğ‘–delimited-[]ğ‘›subscriptğ‘¥ğ‘–ğ‘¦\mathrm{rank}_{\pi}(y):=\left|\left\{i\in[n]:x_{i}<y\right\}\right|roman_rank start_POSTSUBSCRIPT italic_Ï€ end_POSTSUBSCRIPT ( italic_y ) := | { italic_i âˆˆ [ italic_n ] : italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT < italic_y } |.555In this work, we consider algorithms in comparison-based model, wherein stream elements are drawn from a universe equipped with a total-ordering. At any time, the algorithm may only performed comparisons between any two elements stored in memory, and does not depend on the true value of each element. The problem has been extensively studied [9, 13, 1, 8, 14, 2, 16] when we allow additive error, i.e., the algorithm outputs an estimate rank^Ï€â¢(y)=rankÏ€â¢(y)Â±Ïµâ¢nsubscript^rankğœ‹ğ‘¦plus-or-minussubscriptrankğœ‹ğ‘¦italic-Ïµğ‘›\widehat{\mathrm{rank}}_{\pi}(y)=\mathrm{rank}_{\pi}(y)\pm\epsilon nover^ start_ARG roman_rank end_ARG start_POSTSUBSCRIPT italic_Ï€ end_POSTSUBSCRIPT ( italic_y ) = roman_rank start_POSTSUBSCRIPT italic_Ï€ end_POSTSUBSCRIPT ( italic_y ) Â± italic_Ïµ italic_n with high probability. Optimal bounds are known in this setting: Karnin, Lang, and Liberty proposed an algorithm with Oâ¢((1/Ïµ)â¢logâ¡logâ¡(1/Î´))ğ‘‚1italic-Ïµ1ğ›¿O((1/\epsilon)\log\log(1/\delta))italic_O ( ( 1 / italic_Ïµ ) roman_log roman_log ( 1 / italic_Î´ ) ) space, which matches the best space one can hope for even for offline algorithms when the failure probability Î´ğ›¿\deltaitalic_Î´ is a constant [13].666An offline algorithm sees the elements all at once and computes a small sketch that can answer rank queries approximately. On the other hand, oftentimes, the application needs to accurately learn the tail distribution of the data stream. For instance, this need arises when monitoring network latencies: the distribution of response times is often very long-tailed, and understanding the occasional, yet problematic, high response times is a key purpose of the task [4]. For such applications, algorithms that guarantee relative errors give high accuracy on the tail distribution, and are thus more aligned with this stricter requirement. That is, the algorithm must return rank^Ï€â¢(y)=(1Â±Ïµ)â¢rankÏ€â¢(y)subscript^rankğœ‹ğ‘¦plus-or-minus1italic-Ïµsubscriptrankğœ‹ğ‘¦\widehat{\mathrm{rank}}_{\pi}(y)=(1\pm\epsilon)\mathrm{rank}_{\pi}(y)over^ start_ARG roman_rank end_ARG start_POSTSUBSCRIPT italic_Ï€ end_POSTSUBSCRIPT ( italic_y ) = ( 1 Â± italic_Ïµ ) roman_rank start_POSTSUBSCRIPT italic_Ï€ end_POSTSUBSCRIPT ( italic_y ).777The definition as-is gives higher accuracy for queries with small ranks. By running the algorithm with a reversed total-ordering of stream elements, we can obtain high accuracy at the tail of the distribution. The relative-error quantile estimation task also arises when approximately counting the inversions in a stream [11]. The optimal bound for offline algorithms with relative error is Î˜â¢(Ïµâˆ’1â¢logâ¡(Ïµâ¢n))Î˜superscriptitalic-Ïµ1italic-Ïµğ‘›\Theta(\epsilon^{-1}\log(\epsilon n))roman_Î˜ ( italic_Ïµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log ( italic_Ïµ italic_n ) ), by simply storing elements with ranks {1,2,â€¦,Ïµâˆ’1}12â€¦superscriptitalic-Ïµ1\{1,2,\ldots,\epsilon^{-1}\}{ 1 , 2 , â€¦ , italic_Ïµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT } and {Ïµâˆ’1â¢(1+Ïµ),Ïµâˆ’1â¢(1+Ïµ)2,â€¦}superscriptitalic-Ïµ11italic-Ïµsuperscriptitalic-Ïµ1superscript1italic-Ïµ2â€¦\{\epsilon^{-1}(1+\epsilon),\epsilon^{-1}(1+\epsilon)^{2},\ldots\}{ italic_Ïµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( 1 + italic_Ïµ ) , italic_Ïµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( 1 + italic_Ïµ ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , â€¦ }. Best-known streaming algorithms are Multi-Layer Randomization (â€œMRâ€ algorithm) by Zhang, Lin, Xu, Korn and Wang [20] with Oâ¢(Ïµâˆ’2â¢logâ¡(Ïµ2â¢n))ğ‘‚superscriptitalic-Ïµ2superscriptitalic-Ïµ2ğ‘›O(\epsilon^{-2}\log(\epsilon^{2}n))italic_O ( italic_Ïµ start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT roman_log ( italic_Ïµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_n ) ) space, and a recent breakthrough [3] by Cormode, Karnin, Liberty, Thaler and Veselá»³ with Oâ¢(Ïµâˆ’1â¢log1.5â¡(Ïµâ¢n))ğ‘‚superscriptitalic-Ïµ1superscript1.5italic-Ïµğ‘›O(\epsilon^{-1}\log^{1.5}(\epsilon n))italic_O ( italic_Ïµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log start_POSTSUPERSCRIPT 1.5 end_POSTSUPERSCRIPT ( italic_Ïµ italic_n ) ) space (we will refer to it as the CKLTV algorithm below). The MR algorithm [20] maintains logarithmically many sketches of size Oâ¢(Ïµâˆ’2)ğ‘‚superscriptitalic-Ïµ2O(\epsilon^{-2})italic_O ( italic_Ïµ start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT ). Each sketch is responsible for queries with rank in [Ïµâˆ’2â¢2i,Ïµâˆ’2â¢2i+1)superscriptitalic-Ïµ2superscript2ğ‘–superscriptitalic-Ïµ2superscript2ğ‘–1[\epsilon^{-2}2^{i},\epsilon^{-2}2^{i+1})[ italic_Ïµ start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT 2 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , italic_Ïµ start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT 2 start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT ) for some iğ‘–iitalic_i. More recently, Cormode et al. [3] introduced â€œrelative compactorsâ€. Roughly speaking, a relative compactor takes a stream of elements as input and outputs a shorter stream such that the rank of any query in the input stream can be approximated with small relative error based on its rank in the output stream (see Section 5 for a more detailed overview). Then, the algorithm of [3] â€œconnectsâ€ logarithmically many relative compactors, i.e., the output stream of the previous relative compactor is fed (online, in the streaming sense) to the next relative compactor as its input stream. However, both of the aforementioned algorithms ([3], [20]) have the optimal dependence on one of the parameters Ïµitalic-Ïµ\epsilonitalic_Ïµ and nğ‘›nitalic_n, while are suboptimal by a polynomial factor on the other. A natural question is whether we can improve the sketch size in the MR algorithm, or improve the relative compactor space in CKLTV, so that the offline optimal space for this problem can also be achieved in streaming. It turns out that the answer to this question is yes and no. Neither of the two subroutines can be improved in general, due to a lower bound that we prove in Section 7. On the other hand, the â€œbadâ€ streams that lead to such lower bounds inherently cannot be combined â€“ the algorithm maintains logarithmically many sketches of relative compactors, there are bad streams that would force one of them to consume large space, but not all of them simultaneously. Based on this observation, we propose a new streaming algorithm for quantile estimation with relative errors using nearly optimal space O~â¢(Ïµâˆ’1â¢logâ¡Ïµâ¢n)~ğ‘‚superscriptitalic-Ïµ1italic-Ïµğ‘›\tilde{O}(\epsilon^{-1}\log\epsilon n)over~ start_ARG italic_O end_ARG ( italic_Ïµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log italic_Ïµ italic_n )888The O~~ğ‘‚\tilde{O}over~ start_ARG italic_O end_ARG hides the dominated logâ¡(1/Ïµ)1italic-Ïµ\log(1/\epsilon)roman_log ( 1 / italic_Ïµ ), logâ¡logâ¡nğ‘›\log\log nroman_log roman_log italic_n and logâ¡logâ¡(1/Î´)1ğ›¿\log\log(1/\delta)roman_log roman_log ( 1 / italic_Î´ ) terms. For a precise space bound, see 1., nearly matching the trivial Î©â¢(Ïµâˆ’1â¢logâ¡(Ïµâ¢n))Î©superscriptitalic-Ïµ1italic-Ïµğ‘›\Omega(\epsilon^{-1}\log(\epsilon n))roman_Î© ( italic_Ïµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log ( italic_Ïµ italic_n ) ) offline lower bound999The Î©â¢(Ïµâˆ’1â¢logâ¡(Ïµâ¢n))Î©superscriptitalic-Ïµ1italic-Ïµğ‘›\Omega(\epsilon^{-1}\log(\epsilon n))roman_Î© ( italic_Ïµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log ( italic_Ïµ italic_n ) ) lower bound can be shown by inserting Ïµâˆ’1â¢logâ¡(Ïµâ¢n)superscriptitalic-Ïµ1italic-Ïµğ‘›\epsilon^{-1}\log(\epsilon n)italic_Ïµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log ( italic_Ïµ italic_n ) many distinct elements x1<x2<â‹¯<xÏµâˆ’1â‹…logâ¡(Ïµâ¢n)subscriptğ‘¥1subscriptğ‘¥2â‹¯subscriptğ‘¥â‹…superscriptitalic-Ïµ1italic-Ïµğ‘›x_{1}<x_{2}<\dots<x_{\epsilon^{-1}\cdot\log(\epsilon n)}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT < italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT < â‹¯ < italic_x start_POSTSUBSCRIPT italic_Ïµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT â‹… roman_log ( italic_Ïµ italic_n ) end_POSTSUBSCRIPT where for any 1â‰¤iâ‰¤logâ¡(Ïµâ¢n)1ğ‘–italic-Ïµğ‘›1\leq i\leq\log(\epsilon n)1 â‰¤ italic_i â‰¤ roman_log ( italic_Ïµ italic_n ), the elements xÏµâˆ’1â¢(iâˆ’1)+1,â€¦,xÏµâˆ’1â¢isubscriptğ‘¥superscriptitalic-Ïµ1ğ‘–11â€¦subscriptğ‘¥superscriptitalic-Ïµ1ğ‘–x_{\epsilon^{-1}(i-1)+1},\dots,x_{\epsilon^{-1}i}italic_x start_POSTSUBSCRIPT italic_Ïµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( italic_i - 1 ) + 1 end_POSTSUBSCRIPT , â€¦ , italic_x start_POSTSUBSCRIPT italic_Ïµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT italic_i end_POSTSUBSCRIPT are inserted 2isuperscript2ğ‘–2^{i}2 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT times each. Any algorithm, even offline one that can see the entire stream, must keep all elements x1,x2,â€¦,xÏµâˆ’1â‹…logâ¡(Ïµâ¢n)subscriptğ‘¥1subscriptğ‘¥2â€¦subscriptğ‘¥â‹…superscriptitalic-Ïµ1italic-Ïµğ‘›x_{1},x_{2},\dots,x_{\epsilon^{-1}\cdot\log(\epsilon n)}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_x start_POSTSUBSCRIPT italic_Ïµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT â‹… roman_log ( italic_Ïµ italic_n ) end_POSTSUBSCRIPT in memory to satisfy the error guarantee.. We use the framework of MR, and maintain logarithmically many sketches such that each sketch is responsible for answering queries with rank in [Ïµâˆ’1â¢2i,Ïµâˆ’1â¢2i+1)superscriptitalic-Ïµ1superscript2ğ‘–superscriptitalic-Ïµ1superscript2ğ‘–1[\epsilon^{-1}2^{i},\epsilon^{-1}2^{i+1})[ italic_Ïµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT 2 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , italic_Ïµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT 2 start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT ). Now, each sketch is implemented using a collection of new data structures, which we call elastic compactors. Elastic compactors are inspired by relative compactors, but they have one crucial additional feature: they are resizable. Since our sketch for each of the (logarithmically-many) scales will be built using these elastic compactors, we will actually be able to resize the entire sketch for each scale as needed. Depending on how â€œdifficultâ€ the input stream for each sketch is, our algorithm dynamically allocates space to the sketches, and resizes them to the current space on-the-fly as the stream is observed. Whenever a piece of â€œbad input streamâ€ targeting a specific sketch occurs (i.e. the one that we construct in Section 7), the algorithm automatically allocates more space to that sketch temporarily, while still guaranteeing that the total space of all sketches is always bounded by O~â¢(Ïµâˆ’1â¢logâ¡Ïµâ¢n)~ğ‘‚superscriptitalic-Ïµ1italic-Ïµğ‘›\tilde{O}(\epsilon^{-1}\log\epsilon n)over~ start_ARG italic_O end_ARG ( italic_Ïµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log italic_Ïµ italic_n ) with high probability. Theorem 1. Let 0<Î´â‰¤0.50ğ›¿0.50<\delta\leq 0.50 < italic_Î´ â‰¤ 0.5 and 0<Ïµâ‰¤10italic-Ïµ10<\epsilon\leq 10 < italic_Ïµ â‰¤ 1. There is a randomized, comparison-based, one-pass streaming algorithm that, when processing a stream Ï€ğœ‹\piitalic_Ï€ consisting of nğ‘›nitalic_n elements, produces a sketch satisfying the following: for any query xâˆˆğ’°ğ‘¥ğ’°x\in\mathcal{U}italic_x âˆˆ caligraphic_U, the sketch returns an estimate rank^Ï€â¢(x)subscript^rankğœ‹ğ‘¥\widehat{\mathrm{rank}}_{\pi}(x)over^ start_ARG roman_rank end_ARG start_POSTSUBSCRIPT italic_Ï€ end_POSTSUBSCRIPT ( italic_x ) for rankÏ€â¢(x)subscriptrankğœ‹ğ‘¥\mathrm{rank}_{\pi}(x)roman_rank start_POSTSUBSCRIPT italic_Ï€ end_POSTSUBSCRIPT ( italic_x ) such that with probability 1âˆ’Î´1ğ›¿1-\delta1 - italic_Î´, |rank^Ï€â¢(x)âˆ’rankÏ€â¢(x)|â‰¤Ïµâ‹…rankÏ€â¢(x),subscript^rankğœ‹ğ‘¥subscriptrankğœ‹ğ‘¥â‹…italic-Ïµsubscriptrankğœ‹ğ‘¥|\widehat{\mathrm{rank}}_{\pi}(x)-\mathrm{rank}_{\pi}(x)|\leq\epsilon\cdot% \mathrm{rank}_{\pi}(x),| over^ start_ARG roman_rank end_ARG start_POSTSUBSCRIPT italic_Ï€ end_POSTSUBSCRIPT ( italic_x ) - roman_rank start_POSTSUBSCRIPT italic_Ï€ end_POSTSUBSCRIPT ( italic_x ) | â‰¤ italic_Ïµ â‹… roman_rank start_POSTSUBSCRIPT italic_Ï€ end_POSTSUBSCRIPT ( italic_x ) , where the probability is over the internal randomness of the streaming algorithm. Moreover, the total space used by the sketch is Oâ¢(Ïµâˆ’1â¢logâ¡(Ïµâ¢n)â‹…logâ¡(1/Ïµ)â‹…(logâ¡logâ¡n+logâ¡(1/Ïµ))â‹…(logâ¡logâ¡1/Î´)3).ğ‘‚â‹…superscriptitalic-Ïµ1italic-Ïµğ‘›1italic-Ïµğ‘›1italic-Ïµsuperscript1ğ›¿3O(\epsilon^{-1}\log(\epsilon n)\cdot\log(1/\epsilon)\cdot(\log\log n+\log(1/% \epsilon))\cdot(\log\log 1/\delta)^{3}).italic_O ( italic_Ïµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log ( italic_Ïµ italic_n ) â‹… roman_log ( 1 / italic_Ïµ ) â‹… ( roman_log roman_log italic_n + roman_log ( 1 / italic_Ïµ ) ) â‹… ( roman_log roman_log 1 / italic_Î´ ) start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ) . There are several consequences of our construction, which to recall, consists of logarithmically-many resizable sketches for each scale [Ïµâˆ’1â¢2i,Ïµâˆ’1â¢2i+1)superscriptitalic-Ïµ1superscript2ğ‘–superscriptitalic-Ïµ1superscript2ğ‘–1[\epsilon^{-1}2^{i},\epsilon^{-1}2^{i+1})[ italic_Ïµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT 2 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , italic_Ïµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT 2 start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT ). First, since the sketch can be easily resized, our algorithm actually does not need to know the stream length nğ‘›nitalic_n in advance, and the same algorithm works as the stream length increases. Another important feature in practice is mergeability, i.e. it is useful to be able to summarize two substreams Ï€1subscriptğœ‹1\pi_{1}italic_Ï€ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and Ï€2subscriptğœ‹2\pi_{2}italic_Ï€ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT separately into sketches â„³1,â„³2subscriptâ„³1subscriptâ„³2\mathcal{M}_{1},\mathcal{M}_{2}caligraphic_M start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , caligraphic_M start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, and then create a merged sketch â„³â„³\mathcal{M}caligraphic_M which applies to the combined stream Ï€=Ï€1â¢â¨†Ï€2ğœ‹subscriptğœ‹1square-unionsubscriptğœ‹2\pi=\pi_{1}\bigsqcup\pi_{2}italic_Ï€ = italic_Ï€ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT â¨† italic_Ï€ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT with similar error and space guarantees. Currently, it is not clear whether our relative-error quantiles sketch is fully-mergeable (See Section 8 for more discussion). All-quantiles estimation. As a straight-forward corollary of 1, we obtain a sketch that satisfies the all-quantiles guarantee, meaning that for all queries xâˆˆğ’°ğ‘¥ğ’°x\in\mathcal{U}italic_x âˆˆ caligraphic_U simultaneously, the sketch provides an accurate estimate with high probability. The proof proceeds by a standard union bound over an Ïµitalic-Ïµ\epsilonitalic_Ïµ-net, and is nearly identical to argument given in Appendix B of [3]. Corollary 2. Let 0<Î´â‰¤0.50ğ›¿0.50<\delta\leq 0.50 < italic_Î´ â‰¤ 0.5 and 0<Ïµâ‰¤10italic-Ïµ10<\epsilon\leq 10 < italic_Ïµ â‰¤ 1. There is a randomized, comparison-based, one pass streaming algorithm that, when processing a stream Ï€ğœ‹\piitalic_Ï€ consisting of nğ‘›nitalic_n elements, produces a sketch satisfying the all-quantiles guarantee: for all queries xâˆˆğ’°ğ‘¥ğ’°x\in\mathcal{U}italic_x âˆˆ caligraphic_U simultaneously, the sketch returns an estimate rank^Ï€â¢(x)subscript^rankğœ‹ğ‘¥\widehat{\mathrm{rank}}_{\pi}(x)over^ start_ARG roman_rank end_ARG start_POSTSUBSCRIPT italic_Ï€ end_POSTSUBSCRIPT ( italic_x ) such that with probability 1âˆ’Î´1ğ›¿1-\delta1 - italic_Î´, |rank^Ï€â¢(x)âˆ’rankÏ€â¢(x)|â‰¥Ïµâ‹…rankÏ€â¢(x),subscript^rankğœ‹ğ‘¥subscriptrankğœ‹ğ‘¥â‹…italic-Ïµsubscriptrankğœ‹ğ‘¥|\widehat{\mathrm{rank}}_{\pi}(x)-\mathrm{rank}_{\pi}(x)|\geq\epsilon\cdot% \mathrm{rank}_{\pi}(x),| over^ start_ARG roman_rank end_ARG start_POSTSUBSCRIPT italic_Ï€ end_POSTSUBSCRIPT ( italic_x ) - roman_rank start_POSTSUBSCRIPT italic_Ï€ end_POSTSUBSCRIPT ( italic_x ) | â‰¥ italic_Ïµ â‹… roman_rank start_POSTSUBSCRIPT italic_Ï€ end_POSTSUBSCRIPT ( italic_x ) , where the probability is over the internal randomness of the streaming algorithm. The total space used by the sketch is Oâ¢(Ïµâˆ’1â¢logâ¡(Ïµâ¢n)â‹…logâ¡(1/Ïµ)â‹…(logâ¡logâ¡n+logâ¡(1/Ïµ))â‹…(logâ¡logâ¡(logâ¡(Ïµâ¢n)Î´â¢Ïµ))3)ğ‘‚â‹…superscriptitalic-Ïµ1italic-Ïµğ‘›1italic-Ïµğ‘›1italic-Ïµsuperscriptitalic-Ïµğ‘›ğ›¿italic-Ïµ3O\left(\epsilon^{-1}\log(\epsilon n)\cdot\log(1/\epsilon)\cdot(\log\log n+\log% (1/\epsilon))\cdot\left(\log\log\left(\frac{\log(\epsilon n)}{\delta\epsilon}% \right)\right)^{3}\right)italic_O ( italic_Ïµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log ( italic_Ïµ italic_n ) â‹… roman_log ( 1 / italic_Ïµ ) â‹… ( roman_log roman_log italic_n + roman_log ( 1 / italic_Ïµ ) ) â‹… ( roman_log roman_log ( divide start_ARG roman_log ( italic_Ïµ italic_n ) end_ARG start_ARG italic_Î´ italic_Ïµ end_ARG ) ) start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ) 1.1 Further Related Works Deterministic Sketches. In the deterministic additive-error setting, Greenwald and Khanna constructed the GK sketch that stores Oâ¢(Ïµâˆ’1â¢logâ¡(Ïµâ¢n))ğ‘‚superscriptitalic-Ïµ1italic-Ïµğ‘›O(\epsilon^{-1}\log(\epsilon n))italic_O ( italic_Ïµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log ( italic_Ïµ italic_n ) ) elements [9]; more recently, [6] showed that the GK sketch is optimal and [10] gave a simplification of the GK sketch which still achieves optimal space. In the relative-error case, Zhang et al. [19] proposed a deterministic algorithm that uses Oâ¢(Ïµâˆ’1â¢log3â¡(Ïµâ¢n))ğ‘‚superscriptitalic-Ïµ1superscript3italic-Ïµğ‘›O(\epsilon^{-1}\log^{3}(\epsilon n))italic_O ( italic_Ïµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ( italic_Ïµ italic_n ) ) space. This algorithm maintains logarithmically many sketches based on the chronological order of the elements, and keeps merging sketches with similar sizes. Currently, the best known lower bound is Î©â¢(log2â¡(Ïµâ¢n)Ïµ)Î©superscript2italic-Ïµğ‘›italic-Ïµ\Omega\left(\frac{\log^{2}(\epsilon n)}{\epsilon}\right)roman_Î© ( divide start_ARG roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_Ïµ italic_n ) end_ARG start_ARG italic_Ïµ end_ARG ) [6]. Sketches with Known Universe. Additionally, some works focus on the case when the universe ğ’°ğ’°\mathcal{U}caligraphic_U is known in advance to the streaming algorithm [5, 17]. In the additive error regime, the classical qğ‘qitalic_q-digest algorithm gave an optimal deterministic quantile summary using Oâ¢(Ïµâˆ’1â¢logâ¡|ğ’°|)ğ‘‚superscriptitalic-Ïµ1ğ’°O(\epsilon^{-1}\log|\mathcal{U}|)italic_O ( italic_Ïµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log | caligraphic_U | ) words of memory. A recent work of [12] improved this bound to Oâ¢(Ïµâˆ’1)ğ‘‚superscriptitalic-Ïµ1O(\epsilon^{-1})italic_O ( italic_Ïµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) words, achieving an optimal space if the stream length nâ‰¤polyâ¢(|ğ’°|)ğ‘›polyğ’°n\leq\textrm{poly}(|\mathcal{U}|)italic_n â‰¤ poly ( | caligraphic_U | ). For the relative error setting, [5] designed a deterministic bq-summary algorithm using Oâ¢(logâ¡(Ïµâ¢n)â¢logâ¡|ğ’°|Ïµ)ğ‘‚italic-Ïµğ‘›ğ’°italic-ÏµO\left(\frac{\log(\epsilon n)\log|\mathcal{U}|}{\epsilon}\right)italic_O ( divide start_ARG roman_log ( italic_Ïµ italic_n ) roman_log | caligraphic_U | end_ARG start_ARG italic_Ïµ end_ARG ) words of memory, while the offline lower bound is only Î©â¢(logâ¡Ïµâ¢nÏµ)Î©italic-Ïµğ‘›italic-Ïµ\Omega\left(\frac{\log\epsilon n}{\epsilon}\right)roman_Î© ( divide start_ARG roman_log italic_Ïµ italic_n end_ARG start_ARG italic_Ïµ end_ARG ) words. Resizable Sketches. During the past ten years, there has been a flurry of works on â€œresizable sketches,â€ wherein the goal is to design sketches that provide a fixed guarantee on the accuracy while allowing the space allocation to be dynamically adjusted throughout the runtime of the algorithm. This is especially important in practice, where the sketch size may start out being very small, but may need to grow sublinearly until it reaches some fixed maximum size. In particular, resizable sketches have been designed for filters [15, 7] 101010In a talk on â€œResizable Sketchesâ€ at the Simons Institute Workshop on â€œSketching and Algorithm Designâ€ in October 2023, it was mentioned that there are also expandable sketches for the kğ‘˜kitalic_k-minimum values problem and the well-known Misra-Gries sketch for deterministic heavy-hitter detection [18].. Recently in [18], it was posed as an open question to design resizable sketches for the quantile estimation problem. In fact, we actually answer this question since our final sketch for the relative-error quantile estimation problem is actually resizable. Thus, we show that our sketch can achieve near-optimal space O~â¢(Ïµâˆ’1â¢logâ¡(Ïµâ¢n))~ğ‘‚superscriptitalic-Ïµ1italic-Ïµğ‘›\tilde{O}(\epsilon^{-1}\log(\epsilon n))over~ start_ARG italic_O end_ARG ( italic_Ïµ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT roman_log ( italic_Ïµ italic_n ) ) while also having this practically-favorable â€œresizabilityâ€ feature."
https://arxiv.org/html/2411.01191v1,Prophet Secretary and Matching: the Significance of the Largest Item,"The prophet secretary problem is a combination of the prophet inequality and the secretary problem, where elements are drawn from known independent distributions and arrive in uniformly random order. In this work, we design 1) a 0.6880.6880.6880.688-competitive algorithm, that breaks the 0.6750.6750.6750.675 barrier of blind strategies (Correa, Saona, Ziliotto, 2021), and 2) a 0.6410.6410.6410.641-competitive algorithm for the prophet secretary matching problem, that breaks the 1âˆ’1/eâ‰ˆ0.63211ğ‘’0.6321-1/e\approx 0.6321 - 1 / italic_e â‰ˆ 0.632 barrier for the first time. Our second result also applies to the query-commit model of weighted stochastic matching and improves the state-of-the-art ratio (Derakhshan and Farhadi, 2023).","The study of prophet inequality dates back to the 1970s [31, 32] from optimal stopping theory. Consider nğ‘›nitalic_n items with independent random values arriving one by one in an adversarial order. The value distribution Fisubscriptğ¹ğ‘–F_{i}italic_F start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT of each item iğ‘–iitalic_i is known upfront to the algorithm, but the realization of value viâˆ¼Fisimilar-tosubscriptğ‘£ğ‘–subscriptğ¹ğ‘–v_{i}\sim F_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âˆ¼ italic_F start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is only revealed on the itemâ€™s arrival. After seeing the itemâ€™s identity iğ‘–iitalic_i and value visubscriptğ‘£ğ‘–v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, the algorithm decides immediately whether to accept the item and collect its value; the algorithm can accept at most one item in this problem. The goal is to maximize the expected value of the accepted item and compete against the prophet, i.e., the expected maximum value ğ„â¡[maxiâ¡vi]ğ„subscriptğ‘–subscriptğ‘£ğ‘–\operatorname{\mathbf{E}}\mathchoice{\left[\max_{i}v_{i}\right]}{[\max_{i}v_{i% }]}{[\max_{i}v_{i}]}{[\max_{i}v_{i}]}bold_E [ roman_max start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ]. It is known that the optimal competitive ratio is 1212\frac{1}{2}divide start_ARG 1 end_ARG start_ARG 2 end_ARG for this problem. A fundamental extension of the prophet inequality is prophet matching. Consider an underlying bipartite graph with edge weights drawn from known distributions. The vertices on one side are known upfront and those on the other side arrive online. On the arrival of an online vertex visubscriptğ‘£ğ‘–v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, the weights of its incident edges are revealed and the algorithm decides whether to match visubscriptğ‘£ğ‘–v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and to which offline vertex. The classic prophet inequality is captured by this model with one offline vertex. Feldman, Gravin, and Lucier [19] gave a tight 1212\frac{1}{2}divide start_ARG 1 end_ARG start_ARG 2 end_ARG competitive algorithm for the matching setting, and their result was further generalized to settings when all vertices arrive online [17]. In this work, we consider the secretary variants (a.k.a. the random order variants) of prophet inequality and prophet matching, i.e., the setting where the arrival order of items (resp. vertices) is uniformly at random. The study of prophet secretary was initiated by Esfandiari et al. [16], who designed a 1âˆ’1/eâ‰ˆ0.63211ğ‘’0.6321-1/e\approx 0.6321 - 1 / italic_e â‰ˆ 0.632 competitive algorithm and provided an upper bound of 0.750.750.750.75.111The competitive ratio of an algorithm is a number between [0,1]01[0,1][ 0 , 1 ]. A lower bound corresponds to an algorithm and an upper bound corresponds to an impossibility result. Since then, a sequence of follow-up works [3, 10, 25, 7, 23] have focused on closing the gap. The state-of-the-art lower and upper bounds are 0.6720.6720.6720.672 by Harb [25] and 0.7230.7230.7230.723 by Giambartolomei et al. [23] respectively. Less progress has been made on the prophet secretary matching problem. Ehsani et al. [15] gave a 1âˆ’1/e11ğ‘’1-1/e1 - 1 / italic_e competitive algorithm. Very recently, the 1âˆ’1/e11ğ‘’1-1/e1 - 1 / italic_e barrier was surpassed in two special cases: 1) the i.i.d. setting studied by Yan [40] and Qiu et al. [37]; and 2) the query-commit setting studied by Derakhshan and Farhadi [12]. Beating 1âˆ’1/e11ğ‘’1-1/e1 - 1 / italic_e for the general case of prophet secretary matching remains one of the most intriguing open questions to the online algorithms community. 1.1 Our Contributions Result for Prophet Secretary. We design a 0.6880.6880.6880.688 competitive algorithm for the prophet secretary problem. Besides the improvement over the state-of-the-art 0.6720.6720.6720.672 ratio, our result further surpasses the 0.6750.6750.6750.675 barrier of blind strategies [10], the family of algorithms that Correa et al. [10] and Harb [25] focused on. Blind strategies rely on only the distribution of the maximum value, but not the fine-grained distributional information of individual itemsâ€™ values. Intuitively, such fine-grained information must be crucial because the items are heterogeneous in the prophet secretary problem. However, it is technically challenging to incorporate such information to design and analyze item-dependent strategies: changing the strategy for one item would unavoidably affect the probability of accepting other items since we can accept only one of them. Technique: Activation-Based Algorithms. We introduce two ideas to address this difficulty. First, we change our point of view from designing acceptance probabilities to choosing activation rates. In general, an online algorithm is defined by the probability of accepting an item based on its identity iğ‘–iitalic_i, value vğ‘£vitalic_v, and the set of future items that will arrive later. Following the conventional wisdom, the current itemâ€™s arrival time tğ‘¡titalic_t is a good surrogate for aggregating information about exponentially many possible sets of future items over their random arrivals. Here, we interpret the random order as having each item arrive within a time horizon from 00 to 1111 uniformly at random. In short, algorithms are represented by the acceptance probabilities for item iğ‘–iitalic_i when it has value vğ‘£vitalic_v and arrives at time tğ‘¡titalic_t. However, it is difficult to analyze the algorithms based on this representation. By contrast, we will consider the activation rates aivâ¢(t)superscriptsubscriptğ‘ğ‘–ğ‘£ğ‘¡a_{i}^{v}(t)italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT ( italic_t ) for item iğ‘–iitalic_i when it has value vğ‘£vitalic_v and arrives at time tğ‘¡titalic_t, and the overall activation rates Aiâ¢(t)=ğ„vâˆ¼Fiâ¡[aivâ¢(t)]subscriptğ´ğ‘–ğ‘¡subscriptğ„similar-toğ‘£subscriptğ¹ğ‘–superscriptsubscriptğ‘ğ‘–ğ‘£ğ‘¡A_{i}(t)=\operatorname{\mathbf{E}}_{v\sim F_{i}}\mathchoice{\left[a_{i}^{v}(t)% \right]}{[a_{i}^{v}(t)]}{[a_{i}^{v}(t)]}{[a_{i}^{v}(t)]}italic_A start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_t ) = bold_E start_POSTSUBSCRIPT italic_v âˆ¼ italic_F start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT ( italic_t ) ] of the item at time tğ‘¡titalic_t. We activate this item (and accept it if no item has been accepted yet) with probability: givâ¢(t)=aivâ¢(t)â‹…eâˆ’âˆ«0tAiâ¢(x)â¢dx.superscriptsubscriptğ‘”ğ‘–ğ‘£ğ‘¡â‹…superscriptsubscriptğ‘ğ‘–ğ‘£ğ‘¡superscriptğ‘’superscriptsubscript0ğ‘¡subscriptğ´ğ‘–ğ‘¥differential-dğ‘¥g_{i}^{v}(t)=a_{i}^{v}(t)\cdot e^{-\int_{0}^{t}A_{i}(x)\mathrm{d}x}~{}.italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT ( italic_t ) = italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT ( italic_t ) â‹… italic_e start_POSTSUPERSCRIPT - âˆ« start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT italic_A start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_x ) roman_d italic_x end_POSTSUPERSCRIPT . This new viewpoint offers two useful invariants. By definition, the probability that we activate item iğ‘–iitalic_i before time tğ‘¡titalic_t equals: âˆ«0tğ„vâˆ¼Fiâ¡[givâ¢(x)]â¢dx=1âˆ’eâˆ’âˆ«0tAiâ¢(x)â¢dx.superscriptsubscript0ğ‘¡subscriptğ„similar-toğ‘£subscriptğ¹ğ‘–superscriptsubscriptğ‘”ğ‘–ğ‘£ğ‘¥differential-dğ‘¥1superscriptğ‘’superscriptsubscript0ğ‘¡subscriptğ´ğ‘–ğ‘¥differential-dğ‘¥\int_{0}^{t}\operatorname{\mathbf{E}}_{v\sim F_{i}}\mathchoice{\left[g_{i}^{v}% (x)\right]}{[g_{i}^{v}(x)]}{[g_{i}^{v}(x)]}{[g_{i}^{v}(x)]}\mathrm{d}x~{}=~{}1% -e^{-\int_{0}^{t}A_{i}(x)\mathrm{d}x}~{}.âˆ« start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT bold_E start_POSTSUBSCRIPT italic_v âˆ¼ italic_F start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT ( italic_x ) ] roman_d italic_x = 1 - italic_e start_POSTSUPERSCRIPT - âˆ« start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT italic_A start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_x ) roman_d italic_x end_POSTSUPERSCRIPT . Hence, the activation events effectively follow a Poisson process with rates Aiâ¢(t)subscriptğ´ğ‘–ğ‘¡A_{i}(t)italic_A start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_t ). Accordingly, the probability that we activate item iğ‘–iitalic_i with value vğ‘£vitalic_v and arrival time tğ‘¡titalic_t is: ğğ«â¡[vi=v]â‹…aivâ¢(t)â‹…eâˆ’âˆ«0tâˆ‘j=1nAjâ¢(x)â¢dâ¢xâŸ(â‹†)â¢dâ¢t.â‹…â‹…ğğ«subscriptğ‘£ğ‘–ğ‘£superscriptsubscriptğ‘ğ‘–ğ‘£ğ‘¡subscriptâŸsuperscriptğ‘’superscriptsubscript0ğ‘¡superscriptsubscriptğ‘—1ğ‘›subscriptğ´ğ‘—ğ‘¥dğ‘¥â‹†dğ‘¡\operatorname{\mathbf{Pr}}\mathchoice{\left[v_{i}=v\right]}{[v_{i}=v]}{[v_{i}=% v]}{[v_{i}=v]}\cdot a_{i}^{v}(t)\cdot\underbrace{\vphantom{\big{|}}e^{-\int_{0% }^{t}\sum_{j=1}^{n}A_{j}(x)\mathrm{d}x}}_{(\star)}\mathrm{d}t~{}.bold_Pr [ italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_v ] â‹… italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT ( italic_t ) â‹… underâŸ start_ARG italic_e start_POSTSUPERSCRIPT - âˆ« start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT âˆ‘ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_A start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_x ) roman_d italic_x end_POSTSUPERSCRIPT end_ARG start_POSTSUBSCRIPT ( â‹† ) end_POSTSUBSCRIPT roman_d italic_t . Note that the second part (â‹†)â‹†(\star)( â‹† ) is independent of the itemâ€™s identity iğ‘–iitalic_i and value vğ‘£vitalic_v. Therefore, we can simplify the dependence of different itemsâ€™ strategies by introducing an upper bound on âˆ‘j=1nAjâ¢(t)superscriptsubscriptğ‘—1ğ‘›subscriptğ´ğ‘—ğ‘¡\sum_{j=1}^{n}A_{j}(t)âˆ‘ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_A start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_t ) for any time tğ‘¡titalic_t. Subject to this invariant, we can freely design the activation rates aivâ¢(t)superscriptsubscriptğ‘ğ‘–ğ‘£ğ‘¡a_{i}^{v}(t)italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT ( italic_t ) for each item iğ‘–iitalic_i and value vğ‘£vitalic_v to approximately match its contribution to the prophet benchmark. To further simplify the analysis, we focus on activation rates aivâ¢(t)superscriptsubscriptğ‘ğ‘–ğ‘£ğ‘¡a_{i}^{v}(t)italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT ( italic_t ) that are step functions that change their values at a common threshold time. We demonstrate the effectiveness of this viewpoint and such simple step activation rates in Section 2.4.2 by proving a 0.6940.6940.6940.694 competitive ratio when all items are small in the sense that each contributes only oâ¢(1)ğ‘œ1o(1)italic_o ( 1 ) to the prophet benchmark. Technique: Significance of the Largest Item. To further handle the general case of prophet secretary, we will focus on the item i0subscriptğ‘–0i_{0}italic_i start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT with the largest probability of being selected by the prophet. This is partly inspired by the existing hard instances (e.g., [10, 7, 23]), all of which involve one large item and many small items. The significance of the largest item is twofold: 1) we need to design a special strategy for it beyond the step-function activation rates, and 2) its characteristics provide sufficient information for selecting the invariants for the other itemsâ€™ activation rates. Why do we need a special strategy for this largest item? Consider the extreme case when it is the only item that matters. Intuitively, we would like to select it with certainty on its arrival. However, we cannot do that using the step-function activation rates. Within a time interval where the activation rates aivâ¢(t)superscriptsubscriptğ‘ğ‘–ğ‘£ğ‘¡a_{i}^{v}(t)italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT ( italic_t ) remain a constant, the eâˆ’âˆ«0tAiâ¢(x)â¢dxsuperscriptğ‘’superscriptsubscript0ğ‘¡subscriptğ´ğ‘–ğ‘¥differential-dğ‘¥e^{-\int_{0}^{t}A_{i}(x)\mathrm{d}x}italic_e start_POSTSUPERSCRIPT - âˆ« start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT italic_A start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_x ) roman_d italic_x end_POSTSUPERSCRIPT term decreases the activation probability over time. This decrease is mild for smaller items, but could be substantial for the largest item. Remarkably, this seemingly trivial instance plays an important role of establishing the 0.6750.6750.6750.675 barrier for blind strategies [10]. Motivated by this extreme case, we let the largest itemâ€™s activation probability rather than its activation rate be piece-wise constant. While it is difficult to analyze algorithms based on the representation by activation/acceptance probabilities in general, we show that it is manageable to do that for just one largest item. This special treatment of just one largest item is sufficient for breaking the 0.6750.6750.6750.675 barrier. We will consider two characteristics of this largest item: its probability of being selected by the prophet, i.e., x0=ğğ«â¡[vi0=maxjâ¡vj]subscriptğ‘¥0ğğ«subscriptğ‘£subscriptğ‘–0subscriptğ‘—subscriptğ‘£ğ‘—x_{0}=\operatorname{\mathbf{Pr}}\mathchoice{\left[v_{i_{0}}=\max_{j}v_{j}% \right]}{[v_{i_{0}}=\max_{j}v_{j}]}{[v_{i_{0}}=\max_{j}v_{j}]}{[v_{i_{0}}=\max% _{j}v_{j}]}italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = bold_Pr [ italic_v start_POSTSUBSCRIPT italic_i start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = roman_max start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ], and a quantity h0subscriptâ„0h_{0}italic_h start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT that measures the extent to which item i0subscriptğ‘–0i_{0}italic_i start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT would be selected by the prophet with probability more than half, over the randomness of the other itemsâ€™ values. Based on just x0subscriptğ‘¥0x_{0}italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT and h0subscriptâ„0h_{0}italic_h start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, we will choose the (1) invariants âˆ‘jâ‰ i0Ajâ¢(t)subscriptğ‘—subscriptğ‘–0subscriptğ´ğ‘—ğ‘¡\sum_{j\neq i_{0}}A_{j}(t)âˆ‘ start_POSTSUBSCRIPT italic_j â‰  italic_i start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_A start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_t ) for the activation rates of other items, (2) the shared threshold time for the step-function activation rates of other items, and (3) the three-stage step-function activation probabilities of the largest item i0subscriptğ‘–0i_{0}italic_i start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. It is surprising that these two characteristics of the largest item alone are sufficient for choosing all important invariants for our algorithm and analysis. Result for Prophet Secretary Matching. We design a 0.6410.6410.6410.641-competitive algorithm for the prophet secretary matching problem, breaking the 1âˆ’1/e11ğ‘’1-1/e1 - 1 / italic_e barrier for the first time. As a corollary of this result, we also improve the state-of-the-art ratio of the query-commit setting from 0.6330.6330.6330.633 [12] to 0.6410.6410.6410.641 through a reduction [22, 11] from the query-commit model to the secretary model. Summary of Techniques for Prophet Secretary Matching. We start by extending the activation-based framework to matching. First, let us consider a simple strategy: upon the arrival of an online vertex, assign it to an offline vertex with probability proportional to how likely the prophet would match them. We remark that the assignment is independent of the arrival of earlier vertices and their matching results. Then, from each offline vertexâ€™s viewpoint, it may treat the online vertices (more precisely, the corresponding edges) as online items in the prophet secretary problem, treating those not assigned to it as having zero values. However, the online stochastic matching literature suggests that we should not naÃ¯vely follow this approach and apply the two-stage step-function activation rates from prophet secretary, or we would miss the opportunity of exploiting second-chance (re-)assignments (a.k.a. the power of two choices). If an offline vertex is already matched, we should no longer assign online vertices to it, but instead redirect the opportunities to other unmatched offline vertices. Hence, we introduce a third stage into the activation-based algorithm, which has the same activation rates as the second stage, but takes into account the assignments redirected from the other offline vertices. This may be viewed as reinterpreting the three-stage algorithm by Yan [40] for the i.i.d. special case within the activation-based framework. By doing so, we achieve the same 0.6450.6450.6450.645 competitive ratio but more generally for all non-i.i.d. instances in which all edges are small, i.e., when each edge contributes only oâ¢(1)ğ‘œ1o(1)italic_o ( 1 ) to the prophet benchmark. On the other hand, the worst-case competitive ratio of this approach degenerates to 1âˆ’1/e11ğ‘’1-1/e1 - 1 / italic_e if there is a large edge adjacent to every offline vertex. To complement this scenario, we introduce a variant of the random order contention resolution scheme (RCRS) algorithm for matching [33, 21]. This may be viewed under the activation-based framework as follows. For each offline vertex uğ‘¢uitalic_u and its largest edge (u,v)ğ‘¢ğ‘£(u,v)( italic_u , italic_v ), let its adjacent edges other than (u,v)ğ‘¢ğ‘£(u,v)( italic_u , italic_v ) have constant activation rates; let edge (u,v)ğ‘¢ğ‘£(u,v)( italic_u , italic_v )â€™s activation probability be a 00-1111 step-function. This is consistent with our approach for the prophet secretary problem, but the design of activation rates and probabilities is simpler due to the complications in the analysis of the more general matching problem, and the fact that we only need to beat the 1âˆ’1e11ğ‘’1-\frac{1}{e}1 - divide start_ARG 1 end_ARG start_ARG italic_e end_ARG barrier in this case. We show that a hybrid algorithm that randomizes over the above two approaches achieves the stated 0.6410.6410.6410.641 competitive ratio. 1.2 Related Works DÃ¼tting et al. [14] studied the computational complexity of the optimal online algorithm for prophet secretary and gave a PTAS, though it does not imply any competitive ratio of the optimal online algorithm. Abolhassani et al. [1] and Liu et al. [34] studied the prophet secretary problem under small-item assumptions. They proved that if either 1) every distribution appears sufficiently many times [1, 34] or 2) every distribution has only a negligible probability of being non-zero [34], there exists a 0.7450.7450.7450.745-competitive algorithm, matching the optimal competitive ratio as in the i.i.d. setting. The order-selection prophet inequality lies between the i.i.d. setting and the secretary setting. In this variant, the algorithm is given the extra power of selecting the arrival order of the items. This is motivated by the application of prophet inequalities to sequential posted pricing mechanisms, and has been studied by [8, 5, 36, 7]. The current state-of-the-art competitive ratio is 0.7250.7250.7250.725 by Bubna and Chiplunkar [7]. Besides matching, the prophet inequality has also been generalized to other combinatorial settings, including matroids [30], combinatorial auctions [19, 13], and general downward-closed constraints [38]. Ehsani et al. [15] also studied the prophet secretary problem under matroid constraints and achieved a competitive ratio of 1âˆ’1/e11ğ‘’1-1/e1 - 1 / italic_e. Beating this ratio for general matroid constraints remains an important open question. Finally, the unweighted and vertex-weighted online stochastic matching problems have attracted a lot of attention in the online algorithms community [18, 4, 24, 35, 28, 6, 27, 39]. Most of these works assumed i.i.d. arrivals of online vertices, in order to surpass the optimal 1âˆ’1/e11ğ‘’1-1/e1 - 1 / italic_e competitive ratio of online (vertex-weighted) bipartite matching [29, 2]."
https://arxiv.org/html/2411.00819v1,A Bellman-Ford algorithm for the path-length-weighted distance in graphs,"Consider a finite directed graph without cycles in which the arrows are weighted. We present an algorithm for the computation of a new distance, called path-length-weighted distance, which has proven useful for graph analysis in the context of fraud detection. The idea is that the new distance explicitly takes into account the size of the paths in the calculations. Thus, although our algorithm is based on arguments similar to those at work for the Bellman-Ford and Dijkstra methods, it is in fact essentially different. We lay out the appropriate framework for its computation, showing the constraints and requirements for its use, along with some illustrative examples.","Algorithms for calculating the (weighted) path-distance between vertices in a graph appeared in the middle of the 20th century, motivated by the growing interest of the time in the applications of mathematical analysis of graphs. The Bellman-Ford algorithm is the main reference of these early studies [2]. Dijkstraâ€™s algorithm for solving the same problem appeared at about the same time [11], and differs from the other, being more efficient depending on the particular problem. After these original works, the growing interest in the subject (due to the numerous applications that graph theory has found in many fields) has given rise to a great deal of research on graph analysis, which often includes the study of these structures when considered as metric spaces. The idea of considering a graph also as a metric space goes back to the beginnings of the theory of graphs. Metric notions begin to appear explicitly in mathematical works in the second half of the last century. The main metric that was considered (and in a sense the only one until the latter part of the century) was the so-called path distance ([12, 15, 18]): for undirected and connected graphs, this metric evaluated between two vertices (nodes) is defined as the length of the shortest path between them (see for example [4, Â§.2.2.2]). One of the first advances in the metric analysis of graphs was the introduction of weights in the definition of the path distance, assigning weights to the individual paths connecting two consecutive nodes and calculating the infimum of the sum of these weights. Some recent papers on the subject using weighted distances that have inspired this paper are [8, 14]. As in the case of other notions of fundamental graph theory, the relevant theoretical ideas appeared together with research topics from other scientific fields, such as sociology [1, 16, 22]. The definition of different metrics and algorithms to compute them increased greatly in the last decade of the last century, often proposed by problems from other disciplines such as chemistry or crystallography (see [18, 19] and references therein). In this sense, there is a particular case that deserves attention, that is the resistance distance [9, 23]. Coming from some ideas in theoretical chemistry [19] and social network analysis [22], this definition turned out to be a useful tool in the study of molecular configurations in chemistry, although it has also been used in network analysis and other fields [24, 5, 3]. In general, metrics could play a relevant role when studying properties such as robustness (see for example the survey [21]; see also [20]). The interested reader can find more information on applications of metric graphs in the books [6, 4, 13]. In the same direction, in this paper we provide an algorithm to compute a new distance that also appeared in an applied context, in connection with the automatic analysis of fraud in economic networks (see [7]). In this paper, we show that this metric allows to consider vertices that are far away when the distance is measured using the path-distance, becoming close with respect to this metric. It is especially useful in the economic analysis of fraud in networks of companies, since very often the strategy used to hide such fraud is to use the path-distance. Technically, it is defined as a weighted metric, but by dividing the sums of weights appearing in the infimum that provides the value of the metric by a new term that depends on the number of steps involved in the summation. We will show that this change in the weighting process forces to radically change the algorithm to calculate it, since in this new case, longer paths could give shorter distances. However, in the present work we consider the case of acyclic directed graphs, in order to avoid restrictions that make it impossible to define a weighted path metric, since continuous passage through a cycle could always give a null value to the metric (which would mean that it is not a metric). There are other ways to avoid this (see for example Proposition 4.1 in [7]), but in our case we decided to compute the distance between vertices by restricting the set of possible paths in the infimum that gives it. The way to do this is to avoid cycles, and to consider directed graphs. As a result, what we compute is not a metric on the whole graph, but only a distance between two vertices chosen in it. Let us give now some basic definitions about graphs and metric spaces. Let us explain some concepts related to the general definition of what a metric is, which will adapted later to the graph theoretic framework. Let â„+superscriptâ„\mathbb{R}^{+}blackboard_R start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT be the non-negative real numbers. An (extended) quasi-metric on a set Î©Î©\Omegaroman_Î© is a function d:Î©Ã—Î©â†’â„+âˆª{âˆ}:ğ‘‘â†’Î©Î©superscriptâ„d:\Omega\times\Omega\to\mathbb{R}^{+}\cup\{\infty\}italic_d : roman_Î© Ã— roman_Î© â†’ blackboard_R start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT âˆª { âˆ } such that for all a,b,câˆˆÎ©ğ‘ğ‘ğ‘Î©a,b,c\in\Omegaitalic_a , italic_b , italic_c âˆˆ roman_Î©, the axioms (1) dâ¢(a,b)=0=dâ¢(b,a)ğ‘‘ğ‘ğ‘0ğ‘‘ğ‘ğ‘d(a,b)=0=d(b,a)italic_d ( italic_a , italic_b ) = 0 = italic_d ( italic_b , italic_a ) if and only if a=b,ğ‘ğ‘a=b,italic_a = italic_b , and (2) dâ¢(a,b)â‰¤dâ¢(a,c)+dâ¢(c,b)ğ‘‘ğ‘ğ‘ğ‘‘ğ‘ğ‘ğ‘‘ğ‘ğ‘d(a,b)\leq d(a,c)+d(c,b)italic_d ( italic_a , italic_b ) â‰¤ italic_d ( italic_a , italic_c ) + italic_d ( italic_c , italic_b ) hold. The resulting quasi-metric structure (Î©,d)Î©ğ‘‘(\Omega,d)( roman_Î© , italic_d ) is called a quasi-metric space. For the specific framework of this paper, a useful summary of the notions of distance in graphs, with sufficient explanation and many examples, is given in Chapter 15 in [10]. In this paper we will deal with the so called path-length-weighted distance, that was introduced in [7, Â§.4]. It should be noted that the version defined there is given for non-directed graphs, and so the definition we will use is slightly different. However, we will define an extended quasi-metric also in this case by giving the value dâ¢(a,b)=âˆğ‘‘ğ‘ğ‘d(a,b)=\inftyitalic_d ( italic_a , italic_b ) = âˆ when there is no path for going from ağ‘aitalic_a to b,ğ‘b,italic_b , and considering only allowed paths between vertices. Since we are interested in how to compute the distance, and not in theoretical questions about its metric space structure, we will focus attention on the computational algorithm. All the notions on graph theory that are needed can be found in books on this subject, as for example [4]. We will introduce some of them in the next section."
https://arxiv.org/html/2411.02298v1,Sample-Efficient Private Learning of Mixtures of Gaussians,"We study the problem of learning mixtures of Gaussians with approximate differential privacy. We prove that roughly kâ¢d2+k1.5â¢d1.75+k2â¢dğ‘˜superscriptğ‘‘2superscriptğ‘˜1.5superscriptğ‘‘1.75superscriptğ‘˜2ğ‘‘kd^{2}+k^{1.5}d^{1.75}+k^{2}ditalic_k italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_k start_POSTSUPERSCRIPT 1.5 end_POSTSUPERSCRIPT italic_d start_POSTSUPERSCRIPT 1.75 end_POSTSUPERSCRIPT + italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d samples suffice to learn a mixture of kğ‘˜kitalic_k arbitrary dğ‘‘ditalic_d-dimensional Gaussians up to low total variation distance, with differential privacy. Our work improves over the previous best result [AAL24b] (which required roughly k2â¢d4superscriptğ‘˜2superscriptğ‘‘4k^{2}d^{4}italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT samples) and is provably optimal when dğ‘‘ditalic_d is much larger than k2superscriptğ‘˜2k^{2}italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. Moreover, we give the first optimal bound for privately learning mixtures of kğ‘˜kitalic_k univariate (i.e., 1111-dimensional) Gaussians. Importantly, we show that the sample complexity for privately learning mixtures of univariate Gaussians is linear in the number of components kğ‘˜kitalic_k, whereas the previous best sample complexity [AAL21] was quadratic in kğ‘˜kitalic_k. Our algorithms utilize various techniques, including the inverse sensitivity mechanism [AD20b, AD20a, HKMN23], sample compression for distributions [ABDH+20], and methods for bounding volumes of sumsets.","Learning Gaussian Mixture Models (GMMs) is one of the most fundamental problems in algorithmic statistics. Gaussianity is a common data assumption, and the setting of Gaussian mixture models is motivated by heterogeneous data that can be split into numerous clusters, where each cluster follows a Gaussian distribution. Learning mixture models is among the most important problems in machine learning [Bis06], and is at the heart of several unsupervised and semi-supervised machine learning models. The study of Gaussian mixture models has had numerous scientific applications dating back to the 1890s [Pea94], and is a crucial tool in modern data analysis techniques in a variety of fields, including bioinformatics [LKWB22], anomaly detection [ZSM+18], and handwriting analysis [Bis06]. In this work, we study the problem of learning a GMM from samples. We focus on the density estimation setting, where the goal is to learn the overall mixture distribution up to low total variation distance. Unlike the parameter estimation setting for GMMs, density estimation can be done even without any boundedness or separation assumptions on the parameters of the components. In fact, it is known that mixtures of kğ‘˜kitalic_k Gaussians in dğ‘‘ditalic_d-dimensions can be learned up to total variation distance Î±ğ›¼\alphaitalic_Î± using O~â¢(kâ¢d2/Î±2)~ğ‘‚ğ‘˜superscriptğ‘‘2superscriptğ›¼2\widetilde{O}(kd^{2}/\alpha^{2})over~ start_ARG italic_O end_ARG ( italic_k italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT / italic_Î± start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) samples [ABH+18]. Ensuring data privacy has emerged as an increasingly important challenge in modern data analysis and statistics. Differential privacy (DP) [DMNS06] is a rigorous way of defining privacy, and is considered to be the gold standard both in theory and practice, with deployments by Apple [Tea17], Google [EPK14], Microsoft [DKY17], and the US Census Bureau [DLS+17]. As is the case for many data analysis tasks, standard algorithms for learning GMMs leak potentially sensitive information about the individuals who contributed data. This raises the question of whether we can do density estimation for GMMs under the constraint of differential privacy. Private density estimation for GMMs with unrestricted Gaussian components is a challenging task. In fact, privately learning a single unrestricted Gaussian has been the subject of multiple recent studies [AAK21, KMS+22b, AL22, KMV22, AKT+23, HKMN23]. Private learning of GMMs is significantly more challenging, because even without privacy constraints, parameter estimation for GMMs requires exponentially many samples in terms of the number of components [MV10]. Therefore, it is not clear how to use the typical recipe of â€œadding noiseâ€ to the estimated parameters or â€œprivately choosingâ€ from the finite-dimensional space of parameters. Consequently, the only known sample complexity bounds for privately learning unrestricted GMMs are loose [AAL24b, AAL21]. Let us first formally define the problem of learning GMMs. We represent a GMM ğ’Ÿ=âˆ‘i=1kwiâ¢ğ’©â¢(Î¼i,Î£i)ğ’Ÿsuperscriptsubscriptğ‘–1ğ‘˜subscriptğ‘¤ğ‘–ğ’©subscriptğœ‡ğ‘–subscriptÎ£ğ‘–\mathcal{D}=\sum_{i=1}^{k}w_{i}\mathcal{N}(\mu_{i},\Sigma_{i})caligraphic_D = âˆ‘ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT caligraphic_N ( italic_Î¼ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , roman_Î£ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) by its parameters, namely {(wi,Î¼i,Î£i)}i=1ksuperscriptsubscriptsubscriptğ‘¤ğ‘–subscriptğœ‡ğ‘–subscriptÎ£ğ‘–ğ‘–1ğ‘˜\{(w_{i},\mu_{i},\Sigma_{i})\}_{i=1}^{k}{ ( italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_Î¼ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , roman_Î£ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT, where wiâ‰¥0subscriptğ‘¤ğ‘–0w_{i}\geq 0italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT â‰¥ 0, âˆ‘iwi=1subscriptğ‘–subscriptğ‘¤ğ‘–1\sum_{i}w_{i}=1âˆ‘ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1, Î¼iâˆˆâ„dsubscriptğœ‡ğ‘–superscriptâ„ğ‘‘\mu_{i}\in\mathbb{R}^{d}italic_Î¼ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT, and Î£isubscriptÎ£ğ‘–\Sigma_{i}roman_Î£ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is a positive definite matrix. In the following, a GMM learning algorithm ğ’œğ’œ\mathcal{A}caligraphic_A receives a set of data points in â„dsuperscriptâ„ğ‘‘\mathbb{R}^{d}blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT and outputs a (representation of) a GMM. The total variation distance between two distributions is dTVâ¡(ğ’Ÿ~,ğ’Ÿ)=12â¢âˆ«â„d|ğ’Ÿâ¢(x)âˆ’ğ’Ÿ~â¢(x)|â¢ğ‘‘xsubscriptdTV~ğ’Ÿğ’Ÿ12subscriptsuperscriptâ„ğ‘‘ğ’Ÿğ‘¥~ğ’Ÿğ‘¥differential-dğ‘¥\operatorname{d_{TV}}(\tilde{\mathcal{D}},\mathcal{D})=\frac{1}{2}\int_{% \mathbb{R}^{d}}|\mathcal{D}(x)-\tilde{\mathcal{D}}(x)|dxstart_OPFUNCTION roman_d start_POSTSUBSCRIPT roman_TV end_POSTSUBSCRIPT end_OPFUNCTION ( over~ start_ARG caligraphic_D end_ARG , caligraphic_D ) = divide start_ARG 1 end_ARG start_ARG 2 end_ARG âˆ« start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT end_POSTSUBSCRIPT | caligraphic_D ( italic_x ) - over~ start_ARG caligraphic_D end_ARG ( italic_x ) | italic_d italic_x111We are slightly abusing the notation and using ğ’Ÿâ¢(x)ğ’Ÿğ‘¥\mathcal{D}(x)caligraphic_D ( italic_x ) as the pdf of ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D at points xğ‘¥xitalic_x.. Definition 1.1 (Learning GMMs). For Î±,Î²âˆˆ(0,1)ğ›¼ğ›½01\alpha,\beta\in(0,1)italic_Î± , italic_Î² âˆˆ ( 0 , 1 ), we say ğ’œğ’œ\mathcal{A}caligraphic_A learns GMMs with nğ‘›nitalic_n samples up to accuracy Î±ğ›¼\alphaitalic_Î± and failure probability Î²ğ›½\betaitalic_Î² if for every GMM ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D, given samples X1,â€¦,Xnâ¢âˆ¼i.i.d.â¢ğ’ŸX_{1},\dots,X_{n}\overset{i.i.d.}{\sim}\mathcal{D}italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_OVERACCENT italic_i . italic_i . italic_d . end_OVERACCENT start_ARG âˆ¼ end_ARG caligraphic_D, it outputs (a representation of) a GMM ğ’Ÿ~~ğ’Ÿ\tilde{\mathcal{D}}over~ start_ARG caligraphic_D end_ARG such that dTVâ¡(ğ’Ÿ~,ğ’Ÿ)â‰¤Î±subscriptdTV~ğ’Ÿğ’Ÿğ›¼\operatorname{d_{TV}}(\tilde{\mathcal{D}},\mathcal{D})\leq\alphastart_OPFUNCTION roman_d start_POSTSUBSCRIPT roman_TV end_POSTSUBSCRIPT end_OPFUNCTION ( over~ start_ARG caligraphic_D end_ARG , caligraphic_D ) â‰¤ italic_Î± with probability at least 1âˆ’Î²1ğ›½1-\beta1 - italic_Î². Î±ğ›¼\alphaitalic_Î± and Î²ğ›½\betaitalic_Î² are called the accuracy and failure probability, respectively. For clarity of presentation, we will typically fix the value of Î²ğ›½\betaitalic_Î² (e.g., Î²=1/3ğ›½13\beta=1/3italic_Î² = 1 / 3). The above definition does not enforce the constraint of differential privacy. The following definitions formalizes (approximate) differential privacy. Definition 1.2 (Differential Privacy (DP) [DMNS06, DKM+06]). Let Îµ,Î´â‰¥0ğœ€ğ›¿0\varepsilon,\delta\geq 0italic_Îµ , italic_Î´ â‰¥ 0. A randomized algorithm ğ’œ:ğ’³nâ†’ğ’ª:ğ’œâ†’superscriptğ’³ğ‘›ğ’ª\mathcal{A}:\mathcal{X}^{n}\to\mathcal{O}caligraphic_A : caligraphic_X start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT â†’ caligraphic_O is said to be (Îµ,Î´)ğœ€ğ›¿(\varepsilon,\delta)( italic_Îµ , italic_Î´ )-differentially private ((Îµ,Î´)ğœ€ğ›¿(\varepsilon,\delta)( italic_Îµ , italic_Î´ )-DP) if for any two neighboring datasets ğ—,ğ—â€²âˆˆğ’³nğ—superscriptğ—â€²superscriptğ’³ğ‘›\mathbf{X},\mathbf{X}^{\prime}\in\mathcal{X}^{n}bold_X , bold_X start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT âˆˆ caligraphic_X start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT and any measurable subset OâŠ‚ğ’ªğ‘‚ğ’ªO\subset\mathcal{O}italic_O âŠ‚ caligraphic_O, â„™â¢[ğ’œâ¢(ğ—â€²)âˆˆO]â‰¤eÎµâ‹…â„™â¢[ğ’œâ¢(ğ—)âˆˆO]+Î´.â„™delimited-[]ğ’œsuperscriptğ—â€²ğ‘‚â‹…superscriptğ‘’ğœ€â„™delimited-[]ğ’œğ—ğ‘‚ğ›¿\mathbb{P}[\mathcal{A}(\mathbf{X}^{\prime})\in O]\leq e^{\varepsilon}\cdot% \mathbb{P}[\mathcal{A}(\mathbf{X})\in O]+\delta.blackboard_P [ caligraphic_A ( bold_X start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) âˆˆ italic_O ] â‰¤ italic_e start_POSTSUPERSCRIPT italic_Îµ end_POSTSUPERSCRIPT â‹… blackboard_P [ caligraphic_A ( bold_X ) âˆˆ italic_O ] + italic_Î´ . If the GMM learner ğ’œğ’œ\mathcal{A}caligraphic_A of Definition 1.1 is (Îµ,Î´)ğœ€ğ›¿(\varepsilon,\delta)( italic_Îµ , italic_Î´ )-DP, we say that ğ’œğ’œ\mathcal{A}caligraphic_A privately learns GMMs. Formally, we have the following definition. Definition 1.3 (Privately learning GMMs). Fix the number of samples nğ‘›nitalic_n, dimension dğ‘‘ditalic_d, and number of mixture components kğ‘˜kitalic_k. For Î±,Î²âˆˆ(0,1)ğ›¼ğ›½01\alpha,\beta\in(0,1)italic_Î± , italic_Î² âˆˆ ( 0 , 1 ) and Îµ,Î´â‰¥0ğœ€ğ›¿0\varepsilon,\delta\geq 0italic_Îµ , italic_Î´ â‰¥ 0, a randomized algorithm ğ’œğ’œ\mathcal{A}caligraphic_A, that takes as input X1,â€¦,Xnâˆˆâ„dsubscriptğ‘‹1â€¦subscriptğ‘‹ğ‘›superscriptâ„ğ‘‘X_{1},\dots,X_{n}\in\mathbb{R}^{d}italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT, (Îµ,Î´)ğœ€ğ›¿(\varepsilon,\delta)( italic_Îµ , italic_Î´ )-privately learns GMMs up to accuracy Î±ğ›¼\alphaitalic_Î± and failure probability Î²ğ›½\betaitalic_Î², if: 1. For any GMM ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D that is a mixture of up to kğ‘˜kitalic_k Gaussians in dğ‘‘ditalic_d dimensions, if ğ—={X1,â€¦,Xn}â¢âˆ¼i.i.d.â¢ğ’Ÿ\mathbf{X}=\{X_{1},\dots,X_{n}\}\overset{i.i.d.}{\sim}\mathcal{D}bold_X = { italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } start_OVERACCENT italic_i . italic_i . italic_d . end_OVERACCENT start_ARG âˆ¼ end_ARG caligraphic_D, ğ’œâ¢(X1,â€¦,Xn)ğ’œsubscriptğ‘‹1â€¦subscriptğ‘‹ğ‘›\mathcal{A}(X_{1},\dots,X_{n})caligraphic_A ( italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) outputs a GMM ğ’Ÿ~~ğ’Ÿ\tilde{\mathcal{D}}over~ start_ARG caligraphic_D end_ARG such that dTVâ¡(ğ’Ÿ~,ğ’Ÿ)â‰¤Î±subscriptdTV~ğ’Ÿğ’Ÿğ›¼\operatorname{d_{TV}}(\tilde{\mathcal{D}},\mathcal{D})\leq\alphastart_OPFUNCTION roman_d start_POSTSUBSCRIPT roman_TV end_POSTSUBSCRIPT end_OPFUNCTION ( over~ start_ARG caligraphic_D end_ARG , caligraphic_D ) â‰¤ italic_Î± with probability at least 1âˆ’Î²1ğ›½1-\beta1 - italic_Î² (over the randomness of the data ğ—ğ—\mathbf{X}bold_X and the algorithm ğ’œğ’œ\mathcal{A}caligraphic_A). 2. For any neighboring datasets ğ—,ğ—â€²âˆˆ(â„d)nğ—superscriptğ—â€²superscriptsuperscriptâ„ğ‘‘ğ‘›\mathbf{X},\mathbf{X}^{\prime}\in(\mathbb{R}^{d})^{n}bold_X , bold_X start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT âˆˆ ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT (not necessarily drawn from any GMM) and any measurable subset OâŠ‚ğ’ªğ‘‚ğ’ªO\subset\mathcal{O}italic_O âŠ‚ caligraphic_O, â„™â¢[ğ’œâ¢(ğ—â€²)âˆˆO]â‰¤eÎµâ‹…â„™â¢[ğ’œâ¢(ğ—)âˆˆO]+Î´.â„™delimited-[]ğ’œsuperscriptğ—â€²ğ‘‚â‹…superscriptğ‘’ğœ€â„™delimited-[]ğ’œğ—ğ‘‚ğ›¿\mathbb{P}[\mathcal{A}(\mathbf{X}^{\prime})\in O]\leq e^{\varepsilon}\cdot% \mathbb{P}[\mathcal{A}(\mathbf{X})\in O]+\delta.blackboard_P [ caligraphic_A ( bold_X start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) âˆˆ italic_O ] â‰¤ italic_e start_POSTSUPERSCRIPT italic_Îµ end_POSTSUPERSCRIPT â‹… blackboard_P [ caligraphic_A ( bold_X ) âˆˆ italic_O ] + italic_Î´ . Finally, we assume a default value for Î²ğ›½\betaitalic_Î² of 1/3131/31 / 3, meaning that if not stated, the failure probability Î²ğ›½\betaitalic_Î² is assumed to equal 1/3131/31 / 3. Our main goal in this paper is to understand the number of samples (as a function of the dimension dğ‘‘ditalic_d, the number of mixture components kğ‘˜kitalic_k, the accuracy Î±ğ›¼\alphaitalic_Î±, and the privacy parameters Îµ,Î´ğœ€ğ›¿\varepsilon,\deltaitalic_Îµ , italic_Î´) that are needed to privately and accurately learn the GMM up to low total variation distance. 1.1 Results In this work, we provide improved sample complexity bounds for privately learning mixtures of arbitrary Gaussians, improving over previous work of [AAL21, AAL24b]. Moreover, our sample complexity bounds are optimal in certain regimes, when the dimension is either 1111 or a sufficiently large polynomial in kğ‘˜kitalic_k and logâ¡1Î´1ğ›¿\log\frac{1}{\delta}roman_log divide start_ARG 1 end_ARG start_ARG italic_Î´ end_ARG. For general dimension dğ‘‘ditalic_d, we prove the following theorem. Theorem 1.4. For any Î±,Îµ,Î´âˆˆ(0,1),k,dâˆˆâ„•formulae-sequenceğ›¼ğœ€ğ›¿01ğ‘˜ğ‘‘â„•\alpha,\varepsilon,\delta\in(0,1),k,d\in\mathbb{N}italic_Î± , italic_Îµ , italic_Î´ âˆˆ ( 0 , 1 ) , italic_k , italic_d âˆˆ blackboard_N, there exists an inefficient (Îµ,Î´)ğœ€ğ›¿(\varepsilon,\delta)( italic_Îµ , italic_Î´ )-DP algorithm that can learn a mixture of kğ‘˜kitalic_k arbitrary full-dimensional Gaussians in dğ‘‘ditalic_d dimensions up to accuracy Î±ğ›¼\alphaitalic_Î±, using the following number of samples: n=O~â¢(kâ¢d2Î±2+kâ¢d2+d1.75â¢k1.5â¢log0.5â¡(1/Î´)+k1.5â¢log1.5â¡(1/Î´)Î±â¢Îµ+k2â¢dÎ±).ğ‘›~ğ‘‚ğ‘˜superscriptğ‘‘2superscriptğ›¼2ğ‘˜superscriptğ‘‘2superscriptğ‘‘1.75superscriptğ‘˜1.5superscript0.51ğ›¿superscriptğ‘˜1.5superscript1.51ğ›¿ğ›¼ğœ€superscriptğ‘˜2ğ‘‘ğ›¼n=\widetilde{O}\left(\frac{kd^{2}}{\alpha^{2}}+\frac{kd^{2}+d^{1.75}k^{1.5}% \log^{0.5}(1/\delta)+k^{1.5}\log^{1.5}(1/\delta)}{\alpha\varepsilon}+\frac{k^{% 2}d}{\alpha}\right).italic_n = over~ start_ARG italic_O end_ARG ( divide start_ARG italic_k italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_Î± start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG + divide start_ARG italic_k italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_d start_POSTSUPERSCRIPT 1.75 end_POSTSUPERSCRIPT italic_k start_POSTSUPERSCRIPT 1.5 end_POSTSUPERSCRIPT roman_log start_POSTSUPERSCRIPT 0.5 end_POSTSUPERSCRIPT ( 1 / italic_Î´ ) + italic_k start_POSTSUPERSCRIPT 1.5 end_POSTSUPERSCRIPT roman_log start_POSTSUPERSCRIPT 1.5 end_POSTSUPERSCRIPT ( 1 / italic_Î´ ) end_ARG start_ARG italic_Î± italic_Îµ end_ARG + divide start_ARG italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d end_ARG start_ARG italic_Î± end_ARG ) . Notably, the mixing weights and the means can be arbitrary and the covariances of the Gaussians can be arbitrarily poorly conditioned, as long as the covariances are non-singular222For clarity of presentation, we assume the covariance matrices are not singular. However, extending our results to degenerate matrices is straightforward.. We remark that we omit the dependence on Î²ğ›½\betaitalic_Î² (and assume by default a failure probability of 1/3131/31 / 3). However, it is well-known that one can obtain failure probability Î²ğ›½\betaitalic_Î² with only a multiplicative Oâ¢(logâ¡1/Î²)ğ‘‚1ğ›½O(\log 1/\beta)italic_O ( roman_log 1 / italic_Î² ) blowup in sample complexity, in a black-box fashion333To obtain success probability Î²ğ›½\betaitalic_Î² with Oâ¢(nâ‹…logâ¡1/Î²)ğ‘‚â‹…ğ‘›1ğ›½O(n\cdot\log 1/\beta)italic_O ( italic_n â‹… roman_log 1 / italic_Î² ) samples, we repeat the procedure T=Oâ¢(logâ¡1/Î²)ğ‘‡ğ‘‚1ğ›½T=O(\log 1/\beta)italic_T = italic_O ( roman_log 1 / italic_Î² ) times on independent groups of nğ‘›nitalic_n samples each, to get Tğ‘‡Titalic_T estimates ğ’Ÿ~1,â€¦,ğ’Ÿ~Tsubscript~ğ’Ÿ1â€¦subscript~ğ’Ÿğ‘‡\tilde{\mathcal{D}}_{1},\dots,\tilde{\mathcal{D}}_{T}over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , over~ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT, and by a Chernoff bound, at least 51%percent5151\%51 % of the estimates are within total variation distance Î±ğ›¼\alphaitalic_Î± of the true mixture ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D. So, by choosing an estimate that is within 2â¢Î±2ğ›¼2\alpha2 italic_Î± of at least 51%percent5151\%51 % of the estimates, it is still within 3â¢Î±3ğ›¼3\alpha3 italic_Î± total variation distance of ğ’Ÿğ’Ÿ\mathcal{D}caligraphic_D.. In fact, our analysis can yield even better dependencies on Î²ğ›½\betaitalic_Î² in some regimes, though to avoid too much complication, we do not analyze this. For reasonably large dimension, i.e., dâ‰¥k2â¢log2â¡(1/Î´)ğ‘‘superscriptğ‘˜2superscript21ğ›¿d\geq k^{2}\log^{2}(1/\delta)italic_d â‰¥ italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT roman_log start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 / italic_Î´ ), this can be simplified to O~â¢(kâ¢d2Î±2+kâ¢d2Î±â¢Îµ)~ğ‘‚ğ‘˜superscriptğ‘‘2superscriptğ›¼2ğ‘˜superscriptğ‘‘2ğ›¼ğœ€\tilde{O}\left(\frac{kd^{2}}{\alpha^{2}}+\frac{kd^{2}}{\alpha\varepsilon}\right)over~ start_ARG italic_O end_ARG ( divide start_ARG italic_k italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_Î± start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG + divide start_ARG italic_k italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_Î± italic_Îµ end_ARG ), which is in fact optimal (see Theorem 1.6). Hence, we obtain the optimal sample complexity for sufficiently large dimension. Theorem 1.4 also improves over the previous best sample complexity upper bound of [AAL24b], which uses O~â¢(k2â¢d4+kâ¢d2â¢logâ¡(1/Î´)Î±2â¢Îµ+kâ¢dâ¢logâ¡(1/Î´)Î±3â¢Îµ+k2â¢d2Î±4â¢Îµ)~ğ‘‚superscriptğ‘˜2superscriptğ‘‘4ğ‘˜superscriptğ‘‘21ğ›¿superscriptğ›¼2ğœ€ğ‘˜ğ‘‘1ğ›¿superscriptğ›¼3ğœ€superscriptğ‘˜2superscriptğ‘‘2superscriptğ›¼4ğœ€\widetilde{O}\left(\frac{k^{2}d^{4}+kd^{2}\log(1/\delta)}{\alpha^{2}% \varepsilon}+\frac{kd\log(1/\delta)}{\alpha^{3}\varepsilon}+\frac{k^{2}d^{2}}{% \alpha^{4}\varepsilon}\right)over~ start_ARG italic_O end_ARG ( divide start_ARG italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT + italic_k italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT roman_log ( 1 / italic_Î´ ) end_ARG start_ARG italic_Î± start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_Îµ end_ARG + divide start_ARG italic_k italic_d roman_log ( 1 / italic_Î´ ) end_ARG start_ARG italic_Î± start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT italic_Îµ end_ARG + divide start_ARG italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_Î± start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT italic_Îµ end_ARG ) samples. Our results provide a polynomial improvement in all parameters, but to simplify the comparison, if we ignore dependencies in the error parameter Î±ğ›¼\alphaitalic_Î± and privacy parameters Îµ,Î´ğœ€ğ›¿\varepsilon,\deltaitalic_Îµ , italic_Î´, we improve the sample complexity from k2â¢d4superscriptğ‘˜2superscriptğ‘‘4k^{2}d^{4}italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT to kâ¢d2+k2â¢d+k1.5â¢d1.75ğ‘˜superscriptğ‘‘2superscriptğ‘˜2ğ‘‘superscriptğ‘˜1.5superscriptğ‘‘1.75kd^{2}+k^{2}d+k^{1.5}d^{1.75}italic_k italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d + italic_k start_POSTSUPERSCRIPT 1.5 end_POSTSUPERSCRIPT italic_d start_POSTSUPERSCRIPT 1.75 end_POSTSUPERSCRIPT: note that our result is quadratic in the dimension whereas [AAL24b] is quartic. When the dimension is d=1ğ‘‘1d=1italic_d = 1, we can provide an improved result, which is optimal for learning mixtures of univariate Gaussians (see Theorem 1.6 for a matching lower bound). Theorem 1.5. For any Î±,Îµ,Î´âˆˆ(0,1),kâˆˆâ„•formulae-sequenceğ›¼ğœ€ğ›¿01ğ‘˜â„•\alpha,\varepsilon,\delta\in(0,1),k\in\mathbb{N}italic_Î± , italic_Îµ , italic_Î´ âˆˆ ( 0 , 1 ) , italic_k âˆˆ blackboard_N, there exists an inefficient (Îµ,Î´)ğœ€ğ›¿(\varepsilon,\delta)( italic_Îµ , italic_Î´ )-DP algorithm that can learn a mixture of kğ‘˜kitalic_k arbitrary univariate Gaussians (of nonzero variance) up to accuracy Î±ğ›¼\alphaitalic_Î±, using the following number of samples: n=O~â¢(kÎ±2+kâ¢logâ¡(1/Î´)Î±â¢Îµ).ğ‘›~ğ‘‚ğ‘˜superscriptğ›¼2ğ‘˜1ğ›¿ğ›¼ğœ€n=\widetilde{O}\left(\frac{k}{\alpha^{2}}+\frac{k\log(1/\delta)}{\alpha% \varepsilon}\right).italic_n = over~ start_ARG italic_O end_ARG ( divide start_ARG italic_k end_ARG start_ARG italic_Î± start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG + divide start_ARG italic_k roman_log ( 1 / italic_Î´ ) end_ARG start_ARG italic_Î± italic_Îµ end_ARG ) . For privately learning mixtures of univariate Gaussians, the previous best-known result for arbitrary Gaussians required O~â¢(k2â¢log3/2â¡(1/Î´)Î±2â¢Îµ)~ğ‘‚superscriptğ‘˜2superscript321ğ›¿superscriptğ›¼2ğœ€\widetilde{O}\left(\frac{k^{2}\log^{3/2}(1/\delta)}{\alpha^{2}\varepsilon}\right)over~ start_ARG italic_O end_ARG ( divide start_ARG italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT roman_log start_POSTSUPERSCRIPT 3 / 2 end_POSTSUPERSCRIPT ( 1 / italic_Î´ ) end_ARG start_ARG italic_Î± start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_Îµ end_ARG ) samples [AAL21]. Importantly, we are the first paper to show that the sample complexity can be linear in the number of components. Our work purely focuses on sample complexity, and as noted in Theorems 1.4 and 1.5, they do not have polynomial time algorithms. We note that the previous works of [AAL21, AAL24b] also do not run in polynomial time. Indeed, there is reason to believe that even non-privately, it is impossible to learn GMMs in polynomial time (in terms of the optimal sample complexity) [DKS17, BRST21, GVV22]. Finally, we prove the following lower bound for learning GMMs in any fixed dimension dğ‘‘ditalic_d. Theorem 1.6. Fix any dimension dâ‰¥1ğ‘‘1d\geq 1italic_d â‰¥ 1 number of components kâ‰¥2ğ‘˜2k\geq 2italic_k â‰¥ 2, any Î±,Îµğ›¼ğœ€\alpha,\varepsilonitalic_Î± , italic_Îµ at most a sufficiently small constant câˆ—superscriptğ‘c^{*}italic_c start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT, and Î´â‰¤(Î±â¢Îµ/d)Oâ¢(1)ğ›¿superscriptğ›¼ğœ€ğ‘‘ğ‘‚1\delta\leq(\alpha\varepsilon/d)^{O(1)}italic_Î´ â‰¤ ( italic_Î± italic_Îµ / italic_d ) start_POSTSUPERSCRIPT italic_O ( 1 ) end_POSTSUPERSCRIPT. Then, any (Îµ,Î´)ğœ€ğ›¿(\varepsilon,\delta)( italic_Îµ , italic_Î´ )-DP algorithm that can learn a mixture of kğ‘˜kitalic_k arbitrary full-dimensional Gaussians in dğ‘‘ditalic_d dimensions up to total variation distance Î±ğ›¼\alphaitalic_Î±, with probability at least 2/3232/32 / 3, requires at least the following number of samples: Î©~â¢(kâ¢d2Î±2+kâ¢d2Î±â¢Îµ+kâ¢logâ¡(1/Î´)Î±â¢Îµ).~Î©ğ‘˜superscriptğ‘‘2superscriptğ›¼2ğ‘˜superscriptğ‘‘2ğ›¼ğœ€ğ‘˜1ğ›¿ğ›¼ğœ€\tilde{\Omega}\left(\frac{kd^{2}}{\alpha^{2}}+\frac{kd^{2}}{\alpha\varepsilon}% +\frac{k\log(1/\delta)}{\alpha\varepsilon}\right).over~ start_ARG roman_Î© end_ARG ( divide start_ARG italic_k italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_Î± start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG + divide start_ARG italic_k italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_Î± italic_Îµ end_ARG + divide start_ARG italic_k roman_log ( 1 / italic_Î´ ) end_ARG start_ARG italic_Î± italic_Îµ end_ARG ) . Note that for d=1ğ‘‘1d=1italic_d = 1, this matches the upper bound of Theorem 1.5, thus showing that our univariate result is near-optimal in all parameters k,Î±,Îµ,Î´ğ‘˜ğ›¼ğœ€ğ›¿k,\alpha,\varepsilon,\deltaitalic_k , italic_Î± , italic_Îµ , italic_Î´. Moreover, our lower bound refutes the conjecture of [AAL21], which conjectures that only Î˜â¢(kÎ±2+kÎ±â¢Îµ+logâ¡(1/Î´)Îµ)Î˜ğ‘˜superscriptğ›¼2ğ‘˜ğ›¼ğœ€1ğ›¿ğœ€\Theta\left(\frac{k}{\alpha^{2}}+\frac{k}{\alpha\varepsilon}+\frac{\log(1/% \delta)}{\varepsilon}\right)roman_Î˜ ( divide start_ARG italic_k end_ARG start_ARG italic_Î± start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG + divide start_ARG italic_k end_ARG start_ARG italic_Î± italic_Îµ end_ARG + divide start_ARG roman_log ( 1 / italic_Î´ ) end_ARG start_ARG italic_Îµ end_ARG ) samples are needed in the univariate case and Î˜â¢(kâ¢d2Î±2+kâ¢d2Î±â¢Îµ+logâ¡(1/Î´)Îµ)Î˜ğ‘˜superscriptğ‘‘2superscriptğ›¼2ğ‘˜superscriptğ‘‘2ğ›¼ğœ€1ğ›¿ğœ€\Theta\left(\frac{kd^{2}}{\alpha^{2}}+\frac{kd^{2}}{\alpha\varepsilon}+\frac{% \log(1/\delta)}{\varepsilon}\right)roman_Î˜ ( divide start_ARG italic_k italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_Î± start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG + divide start_ARG italic_k italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_Î± italic_Îµ end_ARG + divide start_ARG roman_log ( 1 / italic_Î´ ) end_ARG start_ARG italic_Îµ end_ARG ) samples are needed in the dğ‘‘ditalic_d-dimensional case. However, we note that our lower bound asymptotically differs from the conjectured bound in [AAL21] only when Î´ğ›¿\deltaitalic_Î´ is extremely small. 1.2 Related work In the non-private setting, the sample complexity of learning unrestricted GMMs with respect to total variation distance (a.k.a. density estimation) is known to be Î˜~â¢(kâ¢d2/Î±2)~Î˜ğ‘˜superscriptğ‘‘2superscriptğ›¼2\widetilde{\Theta}(kd^{2}/\alpha^{2})over~ start_ARG roman_Î˜ end_ARG ( italic_k italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT / italic_Î± start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) [ABM18, ABH+18], where the upper bound is obtained by the so-called distributional compression schemes. In the private setting, the only known sample complexity upper bound for unrestricted GMMs [AAL24b] is roughly k2â¢d4â¢logâ¡(1/Î´)/(Î±4â¢Îµ)superscriptğ‘˜2superscriptğ‘‘41ğ›¿superscriptğ›¼4ğœ€k^{2}d^{4}\log(1/\delta)/(\alpha^{4}\varepsilon)italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT roman_log ( 1 / italic_Î´ ) / ( italic_Î± start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT italic_Îµ ), which exhibits sub-optimal dependence on various parameters444More precisely, the upper bound is O~â¢(k2â¢d4Î±2â¢Îµ+kâ¢d2â¢logâ¡(1/Î´)Î±2â¢Îµ+kâ¢dâ¢logâ¡(1/Î´)Î±3â¢Îµ+k2â¢d2Î±4â¢Îµ)~ğ‘‚superscriptğ‘˜2superscriptğ‘‘4superscriptğ›¼2ğœ€ğ‘˜superscriptğ‘‘21ğ›¿superscriptğ›¼2ğœ€ğ‘˜ğ‘‘1ğ›¿superscriptğ›¼3ğœ€superscriptğ‘˜2superscriptğ‘‘2superscriptğ›¼4ğœ€\widetilde{O}\left(\frac{k^{2}d^{4}}{\alpha^{2}\varepsilon}+\frac{kd^{2}\log(1% /\delta)}{\alpha^{2}\varepsilon}+\frac{kd\log(1/\delta)}{\alpha^{3}\varepsilon% }+\frac{k^{2}d^{2}}{\alpha^{4}\varepsilon}\right)over~ start_ARG italic_O end_ARG ( divide start_ARG italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT end_ARG start_ARG italic_Î± start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_Îµ end_ARG + divide start_ARG italic_k italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT roman_log ( 1 / italic_Î´ ) end_ARG start_ARG italic_Î± start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_Îµ end_ARG + divide start_ARG italic_k italic_d roman_log ( 1 / italic_Î´ ) end_ARG start_ARG italic_Î± start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT italic_Îµ end_ARG + divide start_ARG italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_Î± start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT italic_Îµ end_ARG ). This bound is achieved by running multiple non-private list-decoders and then privately aggregating the results. For the special case of axis-aligned GMMs, an upper bound of k2dlog(1/Î´)3/2/(Î±2Îµ)k^{2}d\log(1/\delta)^{3/2}/(\alpha^{2}\varepsilon)italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d roman_log ( 1 / italic_Î´ ) start_POSTSUPERSCRIPT 3 / 2 end_POSTSUPERSCRIPT / ( italic_Î± start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_Îµ ) is known [AAL21]. These are the only known results even for privately learning (unbounded) univariate GMMs. In other words, the best known upper bound for sample complexity of privately learning univariate GMMs has quadratic dependence on kğ‘˜kitalic_k. In the related public-private setting [BKS22, BBC+23], it is assumed that the learner has access to some public data. In this setting, [BBC+23] show that unrestricted GMMs can be learned with a moderate amount of public and private data. Assuming the parameters of the Gaussian components (and the condition numbers of the covariance matrices) are bounded, one can create a cover for GMMs and use private hypothesis selection [BSKW19] or the private minimum distance estimator [AAK21] to learn the GMM. On the flip side, [ASZ21] prove a lower bound on the sample complexity of learning GMMs, though their lower bound is weaker than ours and is only against pure-DP algorithms. The focus of our work is on density estimation. A related problem is learning the parameters a GMM, which has received extensive attention in the (non-private) literature (e.g., [Das99, MV10, BS10, LM21, BDJ+22, LL22] among many other papers). To avoid identifiability issues, one has to assume that the Gaussian components are sufficiently separated and have large-enough weights. In the private setting, the early work of [NRS07] demonstrated a privatized version of [VW04] for learning GMMs with fixed (known) covariance matrices. The strong separation assumption (of Î©â¢(k1/4)Î©superscriptğ‘˜14\Omega(k^{1/4})roman_Î© ( italic_k start_POSTSUPERSCRIPT 1 / 4 end_POSTSUPERSCRIPT )) between the Gaussian components in [NRS07] was later relaxed to a weaker separation assumption [CCAd+23]. A substantially more general result for privately learning GMMs with unknown covariance matrices was established in [KSSU19], based on a privatized version of [AM05]. Yet, this approach also requires a polynomial separation (in terms of kğ‘˜kitalic_k) between the components, as well as a bound on the spectrum of the covariance matrices. [CKM+21] weakened the separation assumption of [KSSU19] and improved over their sample complexity. This result is based on a generic method that learns a GMM using a private learner for Gaussians and a non-private clustering method for GMMs. Finally, [AAL23] designed an efficient reduction from private learning of GMMs to its non-private counterpart, removing the boundedness assumptions on the parameters and achieving minimal separation (e.g., by reducing to [MV10]). Nevertheless, unlike density estimation, parameter estimation for unrestricted GMMs requires exponentially many samples in terms of kğ‘˜kitalic_k [MV10]. A final important question is that of efficient algorithms for learning GMMs. Much of the work on learning GMM parameters focuses on computational efficiency (e.g,. [MV10, BS10, LM21, BDJ+22, LL22]), as does some work on density estimation (e.g., [CDSS14, ADLS17]). However, under some standard hardness assumptions, it is known that even non-privately learning mixtures of kğ‘˜kitalic_k dğ‘‘ditalic_d-dimensional Gaussians with respect to total variation distance cannot be done in polynomial time as a function of kğ‘˜kitalic_k and dğ‘‘ditalic_d [DKS17, BRST21, GVV22]. Addendum. In a concurrent submission, [AAL24a] extended the result of [AAL24b] for learning unrestricted GMMs to the agnostic (i.e., robust) setting. In contrast, our algorithm works only in the realizable (non-robust) setting. Moreover, [AAL24a] slightly improved the sample complexity result of [AAL24b] from O~â¢(logâ¡(1/Î´)â¢k2â¢d4/(Îµâ¢Î±4))~ğ‘‚1ğ›¿superscriptğ‘˜2superscriptğ‘‘4ğœ€superscriptğ›¼4\widetilde{O}({\log(1/\delta)k^{2}d^{4}}/({\varepsilon\alpha^{4}}))over~ start_ARG italic_O end_ARG ( roman_log ( 1 / italic_Î´ ) italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT / ( italic_Îµ italic_Î± start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT ) ) to O~â¢(logâ¡(1/Î´)â¢k2â¢d4/(Îµâ¢Î±2))~ğ‘‚1ğ›¿superscriptğ‘˜2superscriptğ‘‘4ğœ€superscriptğ›¼2\widetilde{O}({\log(1/\delta)k^{2}d^{4}}/({\varepsilon\alpha^{2}}))over~ start_ARG italic_O end_ARG ( roman_log ( 1 / italic_Î´ ) italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT / ( italic_Îµ italic_Î± start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) ). The sample complexity of our approach is still significantly better than [AAL24a] in terms of all parametersâ€”similar to the way it improved over [AAL24b]."
https://arxiv.org/html/2411.02087v2,"An Exponential Separation Between Quantum and Quantum-Inspired
Classical Algorithms for Machine Learning","Achieving a provable exponential quantum speedup for an important machine learning task has been a central research goal since the seminal HHL quantum algorithm for solving linear systems and the subsequent quantum recommender systems algorithm by Kerenidis and Prakash. These algorithms were initially believed to be strong candidates for exponential speedups, but a lower bound ruling out similar classical improvements remained absent. In breakthrough work by Tang, it was demonstrated that this lack of progress in classical lower bounds was for good reasons. Concretely, she gave a classical counterpart of the quantum recommender systems algorithm, reducing the quantum advantage to a mere polynomial. Her approach is quite general and was named quantum-inspired classical algorithms. Since then, almost all the initially exponential quantum machine learning speedups have been reduced to polynomial via new quantum-inspired classical algorithms. From the current state-of-affairs, it is unclear whether we can hope for exponential quantum speedups for any natural machine learning task.In this work, we present the first such provable exponential separation between quantum and quantum-inspired classical algorithms. We prove the separation for the basic problem of solving a linear system when the input matrix is well-conditioned and has sparse rows and columns.","Demonstrating an exponential quantum advantage for a relevant machine learning task has been an important research goal since the promising quantum algorithm by Harrow, Hassidim and Lloyd [12] for solving linear systems. Ignoring a few details, the HHL algorithm (and later improvements [4, 9]) generates a quantum state âˆ‘i=1nxiâ¢|iâŸ©superscriptsubscriptğ‘–1ğ‘›subscriptğ‘¥ğ‘–ketğ‘–\sum_{i=1}^{n}x_{i}|i\rangleâˆ‘ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_i âŸ© corresponding to the solution x=Mâˆ’1â¢yğ‘¥superscriptğ‘€1ğ‘¦x=M^{-1}yitalic_x = italic_M start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT italic_y to an nÃ—nğ‘›ğ‘›n\times nitalic_n Ã— italic_n linear system of equations Mâ¢x=yğ‘€ğ‘¥ğ‘¦Mx=yitalic_M italic_x = italic_y in just polyâ¡(lnâ¡n)polyğ‘›\operatorname{poly}(\ln n)roman_poly ( roman_ln italic_n ) time. At first sight, this seems exponentially faster than any classic algorithm, which probably has to read the entire input matrix Mğ‘€Mitalic_M to solve the same problem. However, as pointed out e.g. by Aaronson [1], the analysis of the HHL algorithm assumes the input matrix is given in a carefully chosen input format. Taking this state preparation into consideration, it was initially unclear how the performance could be compared to a classical algorithm and whether any quantum advantage remained. The shortcoming of the HHL algorithm regarding state preparation was later addressed in seminal work by Kerenedis and Prakash [14], who gave an end-to-end analysis (i.e. including state preparation) that can be directly compared to a classical algorithm. Concretely, their framework assumes that the input matrices and vectors to a linear algebraic machine learning problem are given as simple classical data structures, but with quantum access to the memory representations. At the time, their new quantum algorithm (for recommender systems) was exponentially faster than the best classical counterpart (which is given the same classical data structures as input). Their work sparked a fruitful line of research, yielding exponential speedups for a host of important machine learning tasks, including solving linear systems [6], linear regression [6], PCA [6], recommender systems [14], supervised clustering [15] and Hamiltonian simulation [10]. Despite the exponential speedups over classical algorithms, a lower bound for classical algorithms ruling out a similar improvement via new algorithmic ideas remained illusive. It turned out that this was for good reasons: In breakthrough work by Tang [20], it was demonstrated that on all inputs where the recommender systems algorithm by Kerenedis and Prakash yielded an exponential speedup, a similar speedup could be obtained via a classical algorithmic approach that she dubbed quantum-inspired classical (QIC) algorithms. Since then, almost all the initially exponential speedups from quantum algorithms have been reduced to mere polynomial speedups through the development of new efficient QIC algorithms, see e.g. [5, 7, 19]. The disheartening state-of-affairs is thus that only a few machine learning problems remain where there is still an exponential gap between quantum and QIC algorithms. Based on Tangâ€™s work, it remains entirely plausible that new QIC algorithms may close these gaps as well. Our Contribution. In this work, we present the first provable exponential separation between quantum and quantum-inspired classical algorithms for a central machine learning problem. Concretely, we prove a lower bound for any QIC algorithm for solving linear systems with sparse rows and columns. The lower bound is exponentially higher than known quantum upper bounds [6] when the matrix is well-conditioned, thus establishing the separation. 1.1 Quantum-Inspired Classical Algorithms In the following, we formally introduce QIC algorithms, the linear system problem, our lower bound statement and previous work on proving separations between quantum and QIC algorithms. As mentioned earlier, the work by Kerenidis and Prakash [14] gave a rigorous framework for directly comparing a quantum algorithm for a machine learning task with a classical counterpart. Taking state preparation into account, they define a natural input format for matrices and vectors in linear algebraic problems. At a high level, they assume the input is presented as a classical binary tree based data structure over the entries of the rows and columns of a matrix. They then built their quantum recommender system algorithm assuming quantum access to the memory representation of this classical data structure. Follow-up works have used essentially the same input representation or equivalent formulations. In many cases, for sufficiently well-conditioned matrices, the obtained quantum algorithms run in just polyâ¡(lnâ¡n)polyğ‘›\operatorname{poly}(\ln n)roman_poly ( roman_ln italic_n ) time. Now to prove a separation between quantum and classical algorithms, any fair comparison should use the same input representation. Given the simplicity of the data structure by Kerenidis and Prakash for representing the input, it seemed reasonable to conjecture that any classical algorithm for e.g. recommender systems would need polynomial time even when given this data structure. This intuition was however proven false by Tang [20]. Her key insight was that the classical data structure allows efficient classical (i.e. polyâ¡(lnâ¡n)polyğ‘›\operatorname{poly}(\ln n)roman_poly ( roman_ln italic_n ) time) â„“22superscriptsubscriptâ„“22\ell_{2}^{2}roman_â„“ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT sampling (formally defined below) from the rows and columns of the input, as well as efficient reading of individual entries. Exploiting this sampling access, she gave a classical algorithm for recommender systems that runs in just polyâ¡(lnâ¡n)polyğ‘›\operatorname{poly}(\ln n)roman_poly ( roman_ln italic_n ) time on all matrices where the quantum algorithm by Kerenidis and Prakash does. She referred to such classical algorithms with â„“22superscriptsubscriptâ„“22\ell_{2}^{2}roman_â„“ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT sampling access to input matrices and vectors as quantum-inspired classical algorithms. This sampling access has since then proved extremely useful in other machine learning tasks, see e.g. [5, 7, 19]. Tang [20] summarized the above discussion as follows: â€œwhen quantum machine learning algorithms are compared to classical machine learning algorithms in the context of finding speedups, any state preparation assumptions in the quantum machine learning model should be matched with â„“22superscriptsubscriptâ„“22\ell_{2}^{2}roman_â„“ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT-norm sampling assumptions in the classical machine learning modelâ€™â€™. Using the notation of Mande and Shao [17], QIC algorithms formally have the following access to the input: Definition 1 (Query Access). For a vector vâˆˆâ„nğ‘£superscriptâ„ğ‘›v\in\mathbb{R}^{n}italic_v âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, we have Qâ¢(v)ğ‘„ğ‘£Q(v)italic_Q ( italic_v ), query access to vğ‘£vitalic_v, if for all iğ‘–iitalic_i, we can query visubscriptğ‘£ğ‘–v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. Likewise for a matrix Mâˆˆâ„mÃ—nğ‘€superscriptâ„ğ‘šğ‘›M\in\mathbb{R}^{m\times n}italic_M âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_m Ã— italic_n end_POSTSUPERSCRIPT, we have query access to Mğ‘€Mitalic_M if for all (i,j)âˆˆ[m]Ã—[n]ğ‘–ğ‘—delimited-[]ğ‘šdelimited-[]ğ‘›(i,j)\in[m]\times[n]( italic_i , italic_j ) âˆˆ [ italic_m ] Ã— [ italic_n ], we can query Mi,jsubscriptğ‘€ğ‘–ğ‘—M_{i,j}italic_M start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT. Definition 2 (Sampling and Query Access to a Vector). For a vector vâˆˆâ„nğ‘£superscriptâ„ğ‘›v\in\mathbb{R}^{n}italic_v âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, we have Sâ¢Qâ¢(v)ğ‘†ğ‘„ğ‘£SQ(v)italic_S italic_Q ( italic_v ), sampling and query access to vğ‘£vitalic_v, if we can â€¢ Query for entries of vğ‘£vitalic_v as in Qâ¢(v)ğ‘„ğ‘£Q(v)italic_Q ( italic_v ). â€¢ Obtain independent samples of indices iâˆˆ[n]ğ‘–delimited-[]ğ‘›i\in[n]italic_i âˆˆ [ italic_n ], each distributed as â„™â¢[i]=vi2/â€–vâ€–2â„™delimited-[]ğ‘–superscriptsubscriptğ‘£ğ‘–2superscriptnormğ‘£2\mathbb{P}[i]=v_{i}^{2}/\|v\|^{2}blackboard_P [ italic_i ] = italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT / âˆ¥ italic_v âˆ¥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. â€¢ Query for â€–vâ€–normğ‘£\|v\|âˆ¥ italic_v âˆ¥. Definition 3 (Sampling and Query Access to a Matrix). For a matrix Mâˆˆâ„mÃ—nğ‘€superscriptâ„ğ‘šğ‘›M\in\mathbb{R}^{m\times n}italic_M âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_m Ã— italic_n end_POSTSUPERSCRIPT, we have Sâ¢Qâ¢(M)ğ‘†ğ‘„ğ‘€SQ(M)italic_S italic_Q ( italic_M ) if we have Sâ¢Qâ¢(Mi,â‹†)ğ‘†ğ‘„subscriptğ‘€ğ‘–â‹†SQ(M_{i,\star})italic_S italic_Q ( italic_M start_POSTSUBSCRIPT italic_i , â‹† end_POSTSUBSCRIPT ), Sâ¢Qâ¢(Mâ‹†,j)ğ‘†ğ‘„subscriptğ‘€â‹†ğ‘—SQ(M_{\star,j})italic_S italic_Q ( italic_M start_POSTSUBSCRIPT â‹† , italic_j end_POSTSUBSCRIPT ), Sâ¢Qâ¢(r)ğ‘†ğ‘„ğ‘ŸSQ(r)italic_S italic_Q ( italic_r ) and Sâ¢Qâ¢(c)ğ‘†ğ‘„ğ‘SQ(c)italic_S italic_Q ( italic_c ) for all iâˆˆmğ‘–ğ‘ši\in mitalic_i âˆˆ italic_m and jâˆˆnğ‘—ğ‘›j\in nitalic_j âˆˆ italic_n where râ¢(M)=(â€–M1,â‹†â€–,â€¦,â€–Mm,â‹†â€–)ğ‘Ÿğ‘€normsubscriptğ‘€1â‹†â€¦normsubscriptğ‘€ğ‘šâ‹†r(M)=(\|M_{1,\star}\|,\dots,\|M_{m,\star}\|)italic_r ( italic_M ) = ( âˆ¥ italic_M start_POSTSUBSCRIPT 1 , â‹† end_POSTSUBSCRIPT âˆ¥ , â€¦ , âˆ¥ italic_M start_POSTSUBSCRIPT italic_m , â‹† end_POSTSUBSCRIPT âˆ¥ ) and câ¢(M)=(â€–Mâ‹†,1â€–,â€¦,â€–Mâ‹†,nâ€–)ğ‘ğ‘€normsubscriptğ‘€â‹†1â€¦normsubscriptğ‘€â‹†ğ‘›c(M)=(\|M_{\star,1}\|,\dots,\|M_{\star,n}\|)italic_c ( italic_M ) = ( âˆ¥ italic_M start_POSTSUBSCRIPT â‹† , 1 end_POSTSUBSCRIPT âˆ¥ , â€¦ , âˆ¥ italic_M start_POSTSUBSCRIPT â‹† , italic_n end_POSTSUBSCRIPT âˆ¥ ). Here Mi,â‹†subscriptğ‘€ğ‘–â‹†M_{i,\star}italic_M start_POSTSUBSCRIPT italic_i , â‹† end_POSTSUBSCRIPT is the iğ‘–iitalic_iâ€™th row of Mğ‘€Mitalic_M, Mâ‹†,jsubscriptğ‘€â‹†ğ‘—M_{\star,j}italic_M start_POSTSUBSCRIPT â‹† , italic_j end_POSTSUBSCRIPT is the jğ‘—jitalic_jâ€™th column, râ¢(M)ğ‘Ÿğ‘€r(M)italic_r ( italic_M ) is the vector of row-norms and câ¢(M)ğ‘ğ‘€c(M)italic_c ( italic_M ) is the vector of column-norms of Mğ‘€Mitalic_M. With the input representation defined, we proceed to present the problem of solving a linear system via a QIC algorithm. Here one again needs to be careful for a fair comparison between quantum and QIC algorithms. Concretely, the known quantum algorithms for solving a linear system Mâ¢x=yğ‘€ğ‘¥ğ‘¦Mx=yitalic_M italic_x = italic_y do not output the full solution xğ‘¥xitalic_x (which would take linear time), but instead a quantum state âˆ‘ix~iâ¢|iâŸ©subscriptğ‘–subscript~ğ‘¥ğ‘–ketğ‘–\sum_{i}\tilde{x}_{i}|i\rangleâˆ‘ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_i âŸ© for a x~~ğ‘¥\tilde{x}over~ start_ARG italic_x end_ARG approximating the solution xğ‘¥xitalic_x. Taking measurements on such a state allows one to sample an index iğ‘–iitalic_i with probability x~i2/â€–x~â€–2superscriptsubscript~ğ‘¥ğ‘–2superscriptnorm~ğ‘¥2\tilde{x}_{i}^{2}/\|\tilde{x}\|^{2}over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT / âˆ¥ over~ start_ARG italic_x end_ARG âˆ¥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. With this in mind, the classical analog of solving a linear system is as follows. Problem 1 (Linear Systems). Given Sâ¢Qâ¢(M)ğ‘†ğ‘„ğ‘€SQ(M)italic_S italic_Q ( italic_M ) and Sâ¢Qâ¢(y)ğ‘†ğ‘„ğ‘¦SQ(y)italic_S italic_Q ( italic_y ) for a symmetric and real matrix Mâˆˆâ„nÃ—nğ‘€superscriptâ„ğ‘›ğ‘›M\in\mathbb{R}^{n\times n}italic_M âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n Ã— italic_n end_POSTSUPERSCRIPT of full rank, a vector yâˆˆâ„nğ‘¦superscriptâ„ğ‘›y\in\mathbb{R}^{n}italic_y âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT and precision Îµ>0ğœ€0\varepsilon>0italic_Îµ > 0, the Linear Systems problem is to support sampling an index iğ‘–iitalic_i with probability x~i2/â€–x~â€–2superscriptsubscript~ğ‘¥ğ‘–2superscriptnorm~ğ‘¥2\tilde{x}_{i}^{2}/\|\tilde{x}\|^{2}over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT / âˆ¥ over~ start_ARG italic_x end_ARG âˆ¥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT from a vector x~~ğ‘¥\tilde{x}over~ start_ARG italic_x end_ARG satisfying that â€–x~âˆ’xâ€–â‰¤Îµâ¢â€–xâ€–norm~ğ‘¥ğ‘¥ğœ€normğ‘¥\|\tilde{x}-x\|\leq\varepsilon\|x\|âˆ¥ over~ start_ARG italic_x end_ARG - italic_x âˆ¥ â‰¤ italic_Îµ âˆ¥ italic_x âˆ¥ where x=Mâˆ’1â¢yğ‘¥superscriptğ‘€1ğ‘¦x=M^{-1}yitalic_x = italic_M start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT italic_y is the solution to the linear system of equations Mâ¢x=yğ‘€ğ‘¥ğ‘¦Mx=yitalic_M italic_x = italic_y. The query complexity of a QIC algorithm for solving a linear system, is the number of queries to Sâ¢Qâ¢(M)ğ‘†ğ‘„ğ‘€SQ(M)italic_S italic_Q ( italic_M ) and Sâ¢Qâ¢(y)ğ‘†ğ‘„ğ‘¦SQ(y)italic_S italic_Q ( italic_y ) necessary to sample one index iğ‘–iitalic_i from x~~ğ‘¥\tilde{x}over~ start_ARG italic_x end_ARG. We remark that the known QIC algorithms furthermore output the value x~isubscript~ğ‘¥ğ‘–\tilde{x}_{i}over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT upon sampling iğ‘–iitalic_i. Since we aim to prove a lower bound, our results are only stronger if we prove it for merely sampling iğ‘–iitalic_i. Quantum Benchmark. To prove our exponential separation, we first present the state-of-the-art performance of quantum algorithms for linear systems. Here we focus on the case where the input matrix Mğ‘€Mitalic_M has sparse rows and columns, i.e. every row and column has at most sğ‘ sitalic_s non-zero entries. The running time of the best known quantum algorithm depends on the condition number of Mğ‘€Mitalic_M, defined as Îº=Ïƒmax/Ïƒmin.ğœ…subscriptğœsubscriptğœ\kappa=\sigma_{\max}/\sigma_{\min}.italic_Îº = italic_Ïƒ start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT / italic_Ïƒ start_POSTSUBSCRIPT roman_min end_POSTSUBSCRIPT . Here Ïƒmaxsubscriptğœ\sigma_{\max}italic_Ïƒ start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT is the largest singular value of Mğ‘€Mitalic_M and Ïƒminsubscriptğœ\sigma_{\min}italic_Ïƒ start_POSTSUBSCRIPT roman_min end_POSTSUBSCRIPT is the smallest singular value. Note that for real symmetric Mğ‘€Mitalic_M of full rank, all eigenvalues Î»1â‰¥â‹¯â‰¥Î»nsubscriptğœ†1â‹¯subscriptğœ†ğ‘›\lambda_{1}\geq\cdots\geq\lambda_{n}italic_Î» start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT â‰¥ â‹¯ â‰¥ italic_Î» start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT of Mğ‘€Mitalic_M are real and non-zero, and the singular values Ïƒmax=Ïƒ1â‰¥â‹¯â‰¥Ïƒn=Ïƒmin>0subscriptğœsubscriptğœ1â‹¯subscriptğœğ‘›subscriptğœ0\sigma_{\max}=\sigma_{1}\geq\cdots\geq\sigma_{n}=\sigma_{\min}>0italic_Ïƒ start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT = italic_Ïƒ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT â‰¥ â‹¯ â‰¥ italic_Ïƒ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT = italic_Ïƒ start_POSTSUBSCRIPT roman_min end_POSTSUBSCRIPT > 0 are the absolute values of the eigenvalues {|Î»i|}i=1nsuperscriptsubscriptsubscriptğœ†ğ‘–ğ‘–1ğ‘›\{|\lambda_{i}|\}_{i=1}^{n}{ | italic_Î» start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT in sorted order. Given a precision Îµ>0ğœ€0\varepsilon>0italic_Îµ > 0, matrix Mğ‘€Mitalic_M and vector yğ‘¦yitalic_y as input (in the classical data structure format), the quantum algorithm by Chakraborty, GilyÃ©n and Jeffery [6] runs in time polyâ¡(s,Îº,lnâ¡(1/Îµ),lnâ¡n)polyğ‘ ğœ…1ğœ€ğ‘›\displaystyle\operatorname{poly}(s,\kappa,\ln(1/\varepsilon),\ln n)roman_poly ( italic_s , italic_Îº , roman_ln ( 1 / italic_Îµ ) , roman_ln italic_n ) (1) to produce a quantum state âˆ‘ix~iâ¢|iâŸ©subscriptğ‘–subscript~ğ‘¥ğ‘–ketğ‘–\sum_{i}\tilde{x}_{i}|i\rangleâˆ‘ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_i âŸ© for a x~~ğ‘¥\tilde{x}over~ start_ARG italic_x end_ARG with â€–x~âˆ’xâ€–â‰¤Îµâ¢â€–xâ€–norm~ğ‘¥ğ‘¥ğœ€normğ‘¥\|\tilde{x}-x\|\leq\varepsilon\|x\|âˆ¥ over~ start_ARG italic_x end_ARG - italic_x âˆ¥ â‰¤ italic_Îµ âˆ¥ italic_x âˆ¥ with x=Mâˆ’1â¢yğ‘¥superscriptğ‘€1ğ‘¦x=M^{-1}yitalic_x = italic_M start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT italic_y. We remark that to derive (1) from [6], one invokes their Lemma 11 (originating in [11]) to obtain a block-encoding of a sparse matrix and then invoke their Theorem 30. See also the recent work [16]. QIC Benchmark. The best QIC algorithm [19] for sparse linear systems instead has a query complexity (and running time) of polyâ¡(s,ÎºF,lnâ¡(1/Îµ),lnâ¡n),polyğ‘ subscriptğœ…ğ¹1ğœ€ğ‘›\displaystyle\operatorname{poly}(s,\kappa_{F},\ln(1/\varepsilon),\ln n),roman_poly ( italic_s , italic_Îº start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT , roman_ln ( 1 / italic_Îµ ) , roman_ln italic_n ) , (2) where ÎºF=â€–Mâ€–F/Ïƒmin=âˆ‘iÏƒi2Ïƒmin.subscriptğœ…ğ¹subscriptnormğ‘€ğ¹subscriptğœsubscriptğ‘–superscriptsubscriptğœğ‘–2subscriptğœ\kappa_{F}=\|M\|_{F}/\sigma_{\min}=\frac{\sqrt{\sum_{i}\sigma_{i}^{2}}}{\sigma% _{\min}}.italic_Îº start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT = âˆ¥ italic_M âˆ¥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT / italic_Ïƒ start_POSTSUBSCRIPT roman_min end_POSTSUBSCRIPT = divide start_ARG square-root start_ARG âˆ‘ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_Ïƒ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG end_ARG start_ARG italic_Ïƒ start_POSTSUBSCRIPT roman_min end_POSTSUBSCRIPT end_ARG . Since ÎºFsubscriptğœ…ğ¹\kappa_{F}italic_Îº start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT may be larger than Îºğœ…\kappaitalic_Îº by as much as a nğ‘›\sqrt{n}square-root start_ARG italic_n end_ARG factor, there are thus matrices with Îº,s=polyâ¡(lnâ¡n)ğœ…ğ‘ polyğ‘›\kappa,s=\operatorname{poly}(\ln n)italic_Îº , italic_s = roman_poly ( roman_ln italic_n ) where there is an exponential gap between (1) and (2). However, proving that a QIC algorithm with a performance matching (1) cannot be developed has so far remained out of reach. Our Result. We show the following strong lower bound for QIC algorithms Theorem 1. There is a constant c>0ğ‘0c>0italic_c > 0, such that for nâ‰¥cğ‘›ğ‘n\geq citalic_n â‰¥ italic_c and any precision Îµâ‰¤(câ¢ln2.5â¡n)âˆ’1ğœ€superscriptğ‘superscript2.5ğ‘›1\varepsilon\leq(c\ln^{2.5}n)^{-1}italic_Îµ â‰¤ ( italic_c roman_ln start_POSTSUPERSCRIPT 2.5 end_POSTSUPERSCRIPT italic_n ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT, it holds that for any QIC algorithm ğ’œğ’œ\mathcal{A}caligraphic_A for linear systems, there exists a full rank nÃ—nğ‘›ğ‘›n\times nitalic_n Ã— italic_n symmetric real matrix Mğ‘€Mitalic_M with condition number Îºâ‰¤câ¢ln2â¡nğœ…ğ‘superscript2ğ‘›\kappa\leq c\ln^{2}nitalic_Îº â‰¤ italic_c roman_ln start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_n and 4444-sparse rows and columns, such that ğ’œğ’œ\mathcal{A}caligraphic_A must make Î©â¢(n1/12)Î©superscriptğ‘›112\Omega(n^{1/12})roman_Î© ( italic_n start_POSTSUPERSCRIPT 1 / 12 end_POSTSUPERSCRIPT ) queries to Sâ¢Qâ¢(M)ğ‘†ğ‘„ğ‘€SQ(M)italic_S italic_Q ( italic_M ) on the linear system Mâ¢x=e1ğ‘€ğ‘¥subscriptğ‘’1Mx=e_{1}italic_M italic_x = italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. Observe that the complexity of the best quantum algorithm (1) for this setting of s,Îºğ‘ ğœ…s,\kappaitalic_s , italic_Îº and Îµğœ€\varepsilonitalic_Îµ is just polyâ¡(lnâ¡n)polyğ‘›\operatorname{poly}(\ln n)roman_poly ( roman_ln italic_n ), hence the claimed exponential separation. Furthermore, the matrix Mğ‘€Mitalic_M is extremely sparse, with only s=4ğ‘ 4s=4italic_s = 4 non-zeroes per row and column, and the vector yğ‘¦yitalic_y in the linear system Mâ¢x=yğ‘€ğ‘¥ğ‘¦Mx=yitalic_M italic_x = italic_y is simply the first standard unit vector e1subscriptğ‘’1e_{1}italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. Previous Separations. Finally, let us mention recent work by Mande and Shao [17] that also focuses on separating quantum and QIC algorithms. Using reductions from number-in-hand multiparty communication complexity [18], they prove a number of lower bounds for QIC algorithms for linear regression, supervised clustering, PCA, recommender systems and Hamiltonian simulation. Their lower bounds are of the form Î©~â¢(ÎºF2)~Î©superscriptsubscriptğœ…ğ¹2\tilde{\Omega}(\kappa_{F}^{2})over~ start_ARG roman_Î© end_ARG ( italic_Îº start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ), but only for problems where the best known quantum algorithms are no better than O~â¢(ÎºF)~ğ‘‚subscriptğœ…ğ¹\tilde{O}(\kappa_{F})over~ start_ARG italic_O end_ARG ( italic_Îº start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT ), thus establishing quadratic separations compared to our exponential separation. Let us also remark that our lower bound proof takes a completely different approach, instead reducing from a problem of random walks by Childs et al. [8]."
https://arxiv.org/html/2411.01246v1,"CAMP: A Cost Adaptive Multi-Queue Eviction Policy for Key-Value Storesâ€ â€ thanks:Sandy Irani and Jenny Lam are with the University of California, Irvine. Their research is supported in part by the NSF grant CCF-0916181. Shahram Ghandeharizadeh and Jason Yap are with the University of Southern California.111A shorter version of CAMP appeared in the Proceedings of the ACM/IFIP/USENIX
Middleware Conference, Bordeaux, France, December 2014. Seehttps://github.com/scdblab/CAMPfor an implementation.","Cost Adaptive Multi-queue eviction Policy (CAMP) is an algorithm for a general purpose key-value store (KVS) that manages key-value pairs computed by applications with different access patterns, key-value sizes, and varying costs for each key-value pair. CAMP is an approximation of the Greedy Dual Size (GDS) algorithm that can be implemented as efficiently as LRU. In particular, CAMPâ€™s eviction policies are as effective as those of GDS but require only a small fraction of the updates to an internal data structure in order to make those decisions. Similar to an implementation of LRU using queues, it adapts to changing workload patterns based on the history of requests for different key-value pairs. It is superior to LRU because it considers both the size and cost of key-value pairs to maximize the utility of the available memory across competing applications. We compare CAMP with both LRU and an alternative that requires human intervention to partition memory into pools and assign grouping of key-value pairs to different pools. The results demonstrate CAMP is as fast as LRU while outperforming both LRU and the pooled alternative. We also present results from an implementation of CAMP using Twitterâ€™s version of memcached.","Applications with a high read-to-write ratio augment their persistent infrastructure with an in-memory key-value store (KVS) to enhance performance. An example is memcached in use by popular Internet destinations such as Facebook, Twitter, and Wikipedia. Using a general purpose caching layer requires workloads to share infrastructure despite different access patterns, key-value sizes, and time required to compute a key-value pair [21]. An algorithm that considers only one factor may cause different application workloads to impact one another negatively, decreasing the overall effectiveness of the caching layer. As an example, consider two different applications of a social networking site: one shows the profile of members while a second determines the displayed advertisements. There may exist millions of key-value pairs corresponding to different member profiles, each computed using a simple database look-up that executed in a few milliseconds. The second application may consist of thousands of key-value pairs computed using a machine-learning algorithm that processed Terabytes of data and required hours of execution. This processing time is one definition of the cost of a key-value pair. With a limited memory size and a high frequency of access for member profile key-value pairs, a simple algorithm that manages memory using recency of references (LRU) may evict most of the key-value pairs of the second application, increasing the incurred cost. In general, reducing the incurred cost translates into a faster system that processes a larger number of requests per unit of time and may provide a better quality of service. The latter is due to availability of data (e.g., cache hit for a key-value computed using the machine learning algorithm) that enables the application to provide a user with more relevant content than content selected randomly. A possible approach is for a human expert to partition the available memory into disjoint pools with each pool managed using LRU. Next, the expert groups key-value pairs with similar costs together and assigns each group to a different pool [18]. With our example, the expert would construct two pools. One for the key-value pairs corresponding to members profiles and a second corresponding to advertisements. The primary limitation222Partitioning is known to reduce the utilization of resources by resulting in formation of hot spots and bottlenecks. One may address this limitation by over-provisioning resources. of this approach is that it requires a human familiar with the different classes of applications to identify the pools, construct grouping of key-value pairs, and assign each group to a pool. Over time, the service provider may either introduce a new application or discontinue an existing one. This means the human expert must again become involved to identify the pool for the key-value pairs of the new application and possibly rebalance memory across the pools once an application is discontinued. This paper introduces a novel caching method called Cost Adaptive Multi-queue eviction Policy (CAMP), that manages the available memory without partitioning it. CAMP is an approximation of the Greedy Dual Size (GDS) algorithm [4] that processes cache hits and misses more efficiently using queues. Hence, it is significantly faster than GDS and as fast as LRU. It is novel and different from LRU in that it constructs multiple LRU queues dynamically based on the size and cost of key-value pairs. The number of constructed LRU queues depends on the distribution of costs and sizes of the key-value pairs. CAMP manages these LRU queues without partitioning memory. Thus, there is no need for human involvement to construct groups of key-value pairs, dictate assignment of groups to the pools, or configure and adjust the memory pool characteristics. CAMP is robust enough to prevent an aged expensive key-value pair from occupying memory indefinitely. Such a key-value pair is evicted by CAMP as competing applications issue more requests. CAMP is parameterized by a variable that controls its precision. At the highest precision, CAMPâ€™s eviction decisions are essentially equivalent to those made by GDS. Our empirical results show that CAMP does not suffer any degradation in the quality of its eviction decisions at lower precisions. Moreover, it is able to make those decisions much more efficiently than GDS. GDS requires an internal priority queue to determine a key-value pair to evict from the cache. The time to maintain its data structures consistent in a thread-safe manner is expensive because it requires synchronization primitives [12] with multiple threads performing caching decisions. Moreover, CAMP performs a significantly fewer updates of its internal data structures than GDS, reducing the number of times it executes the thread-safe software dramatically. The rest of this paper is organized as follows. Section 2 starts with a description of GDS to motivate CAMP and details its design decisions. Section 3 presents a simulation study of CAMP and compares it with LRU and the pooled approach that partitions resources, demonstrating its superiority. Section 4 describes an implementation of CAMP using a variant of Twemcache and compares this implementation with the original that uses LRU. Obtained results demonstrate that CAMP is as fast as LRU and provides superior performance as it considers, in addition to recency of requests, the size and the cost of the key-value pairs. Section 5 describes related work. Section 6 provides brief words of conclusions and future research directions."
https://arxiv.org/html/2411.01115v1,Relax and Merge: A Simple Yet Effective Framework for Solving Fairkğ‘˜kitalic_k-Means andkğ‘˜kitalic_k-sparse Wasserstein Barycenter Problems,"The fairness of clustering algorithms has gained widespread attention across various areas in machine learning. In this paper, we study fair kğ‘˜kitalic_k-means clustering in Euclidean space. Given a dataset comprising several groups, the fairness constraint requires that each cluster should contain a proportion of points from each group within specified lower and upper bounds. Due to these fairness constraints, determining the locations of kğ‘˜kitalic_k centers and finding the induced partition are quite challenging tasks. We propose a novel â€œRelax and Mergeâ€ framework that returns a (1+4â¢Ï+Oâ¢(Ïµ))14ğœŒğ‘‚italic-Ïµ(1+4\rho+O(\epsilon))( 1 + 4 italic_Ï + italic_O ( italic_Ïµ ) )-approximate solution, where ÏğœŒ\rhoitalic_Ï is the approximate ratio of an off-the-shelf vanilla kğ‘˜kitalic_k-means algorithm and Oâ¢(Ïµ)ğ‘‚italic-ÏµO(\epsilon)italic_O ( italic_Ïµ ) can be an arbitrarily small positive number. If equipped with a PTAS of kğ‘˜kitalic_k-means, our solution can achieve an approximation ratio of (5+Oâ¢(Ïµ))5ğ‘‚italic-Ïµ(5+O(\epsilon))( 5 + italic_O ( italic_Ïµ ) ) with only a slight violation of the fairness constraints, which improves the current state-of-the-art approximation guarantee. Furthermore, using our framework, we can also obtain a (1+4â¢Ï+Oâ¢(Ïµ))14ğœŒğ‘‚italic-Ïµ(1+4\rho+O(\epsilon))( 1 + 4 italic_Ï + italic_O ( italic_Ïµ ) )-approximate solution for the kğ‘˜kitalic_k-sparse Wasserstein Barycenter problem, which is a fundamental optimization problem in the field of optimal transport, and a (2+6â¢Ï)26ğœŒ(2+6\rho)( 2 + 6 italic_Ï )-approximate solution for the strictly fair kğ‘˜kitalic_k-means clustering with no violation, both of which are better than the current state-of-the-art methods. In addition, the empirical results demonstrate that our proposed algorithm can significantly outperform baseline approaches in terms of clustering cost.","Clustering is one of the most fundamental problems in the area of machine learning. A wide range of practical applications rely on effective clustering algorithms, such as feature engineering (Glassman et al., 2014; Alelyani et al., 2018), image processing (Coleman and Andrews, 1979; Chang et al., 2017), and bioinformatics (Ronan et al., 2016; Nugent and Meila, 2010). In particular, the kğ‘˜kitalic_k-means clustering problem has been extensively studied in the past decades (Jain, 2010). Given an input dataset PâŠ‚â„dğ‘ƒsuperscriptâ„ğ‘‘P\subset\mathbb{R}^{d}italic_P âŠ‚ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT, the goal of the kğ‘˜kitalic_k-means problem is to find a set Sğ‘†Sitalic_S of at most kğ‘˜kitalic_k points for minimizing the clustering cost, which is the sum of the squared distances from every point of Pğ‘ƒPitalic_P to its nearest neighbor in Sğ‘†Sitalic_S. In recent years, motivated by various fields like education, social security, and cultural communication, the study on fairness of clustering has in particular attracted a great amount of attention (Chierichetti et al., 2017; Bera et al., 2019; Huang et al., 2019; Chen et al., 2019; Ghadiri et al., 2021). In this paper, we consider the problem of (Î±,Î²)Î±Î²(\alpha,\beta)( italic_Î± , italic_Î² )-fair kkkitalic_k-means clustering that was initially proposed by Chierichetti et al. (2017) and then generalized by Bera et al. (2019). Informally speaking, we assume that the given dataset Pğ‘ƒPitalic_P consists of mğ‘šmitalic_m groups of points, and the â€œfairnessâ€ constraint requires that in each obtained cluster, the points from each group should take a fraction between pre-specified lower and upper bounds. Bera et al. (2019) showed that a ÏğœŒ\rhoitalic_Ï-approximate algorithm for vanilla kğ‘˜kitalic_k-means can provide a (2+Ï)2superscript2ğœŒ2(2+\sqrt{\rho})^{2}( 2 + square-root start_ARG italic_Ï end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT- approximate solution 111In their paper, the approximate ratio is written as (2+Ï)2ğœŒ(2+\rho)( 2 + italic_Ï ) because they added a squared root to the kğ‘˜kitalic_k-means cost function. for (Î±,Î²)ğ›¼ğ›½(\alpha,\beta)( italic_Î± , italic_Î² )-fair kğ‘˜kitalic_k-clustering with a slight violation on the fairness constraints, where the â€œviolationâ€ is formally defined in Section 2. Furthermore, BÃ¶hm et al. (2021) studied the â€œstrictlyâ€ fair kğ‘˜kitalic_k-means clustering problem, where it requires that the number of points from each group should be uniform in every cluster; they obtained a (2+Ï)2superscript2ğœŒ2(2+\sqrt{\rho})^{2}( 2 + square-root start_ARG italic_Ï end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT approximate solution without violation. These fair kğ‘˜kitalic_k-means algorithms can also be accelerated by using the coreset techniques, such as (Huang et al., 2019; Braverman et al., 2022; Bandyapadhyay et al., 2024). There also exist polynomial-time approximation scheme (PTAS) for fair kğ‘˜kitalic_k-means, such as the algorithms proposed in (BÃ¶hm et al., 2021; Schmidt et al., 2020; Bandyapadhyay et al., 2024), but their methods have an exponential time complexity in kğ‘˜kitalic_k. We are also aware of several other different definitions of fairness for clustering problems, such as the proportionally fair clustering (Chen et al., 2019; Micha and Shah, 2020) and socially fair kğ‘˜kitalic_k-means clustering (Ghadiri et al., 2021). Another problem closely related to fair kğ‘˜kitalic_k-means is the so-called â€œkğ‘˜kitalic_k-sparse Wassertein Barycenter (WB)â€ (Agueh and Carlier, 2011) (the formal definition is shown in Section 2). The Wasserstein Barycenter is a fundamental concept in optimal transport theory, and it represents the â€œaverageâ€ or central distribution of a set of probability distributions. It plays a crucial role in various applications such as image processing (Bonneel et al., 2015; Cuturi and Doucet, 2014), data analysis (Rabin et al., 2012), and machine learning (Backhoff-Veraguas et al., 2022; Metelli et al., 2019). Given m>1ğ‘š1m>1italic_m > 1 discrete distributions, the goal of the kğ‘˜kitalic_k-sparse WB problem is to find a discrete distribution (i.e., the barycenter) that minimizes the sum of the Wasserstein distances (Villani, 2021) between itself to all the given distributions, and meanwhile the support size of the barycenter is restricted to be no larger than a given integer kâ‰¥1ğ‘˜1k\geq 1italic_k â‰¥ 1. If relaxing the â€œkğ‘˜kitalic_k-sparseâ€ constraint (i.e., the barycenter is allowed to take a support size larger than kğ‘˜kitalic_k), Altschuler and Boix-Adsera (2021) presented an algorithm based on linear programming, which can compute the WB within fixed dimensions in polynomial time. If the locations of the WB supports are given, the problem is called â€œfixed support WBâ€, which can be solved by using several existing algorithms (Claici et al., 2018; Cuturi and Doucet, 2014; Cuturi and PeyrÃ©, 2016; Lin et al., 2020). If we keep the â€œkğ‘˜kitalic_k-sparseâ€ constraint, it has been proved that the problem is NP-hard (Borgwardt and Patterson, 2021). To the best of our knowledge, the current lowest approximation ratio of k-sparse WB problem is also (2+Ï)2superscript2ğœŒ2(2+\sqrt{\rho})^{2}( 2 + square-root start_ARG italic_Ï end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT (same with the aforementioned approximation factor for fair kğ‘˜kitalic_k-means), as recently studied by Yang and Ding (2024). In fact, we can regard this problem as a special case of fair kğ‘˜kitalic_k-means clustering, where each input distribution is an individual group and the unique cost measured by â€œWasserstein distanceâ€ is implicitly endowed with a kind of fairness. This observation from Yang and Ding (2024) inspires us to consider solving the kğ‘˜kitalic_k-sparse WB problem under our framework. Why fair kğ‘˜kitalic_k-means is so challenging? Though the fair kğ‘˜kitalic_k-means clustering has been extensively studied in recent years, their current state-of-the-art approximation qualities are still not that satisfying. The major difficulty arises from the lack of â€œlocality propertyâ€ (Ding and Xu, 2020; Bhattacharya et al., 2018) caused by fair constraints. More precisely, in a clustering result of vanilla kğ‘˜kitalic_k-means, each client point obviously belongs to its closest center. That is, a kğ‘˜kitalic_k-means clustering implicitly forms a Voronoi diagram, where the cell centers are exactly the kğ‘˜kitalic_k cluster centers, and the client points in each Voronoi cell form a cluster. However, when we add some fair constraints, such as requiring that the proportion of points of each group should be equal in each cluster, the situation becomes more complicated. Given a set of cluster center locations, because the groups of client points within a Voronoi cell may not be equally distributed, some points are forced to be assigned to other Voronoi cells. This loss of locality introduces significant uncertainty for the selection of cluster center positions. The previous works (Bera et al., 2019; BÃ¶hm et al., 2021) do not pay much attention on how to handle this locality issue when searching for the cluster centers, instead, they directly apply vanilla kğ‘˜kitalic_k-means algorithms to the entire input dataset or a group, and use the obtained center locations as the center locations for fair kğ‘˜kitalic_k-means. It is easy to notice that their methods could result in a certain gap with the optimal fair kğ‘˜kitalic_k-means solution. To narrow this gap, we attempt to design some more effective way to determine the center locations, where the key part that we believe, should be how to encode the fair constraints into the searching algorithm. Our key ideas and main results. Our key idea relies on an important observation, where we find that the fair kğ‘˜kitalic_k-means problem is inherently related to a classic geometric structure, â€œÏµitalic-Ïµ\epsilonitalic_Ïµ-approximate centroid setâ€, which was firstly proposed by MatouÅ¡ek (2000). Roughly speaking, given a dataset, an Ïµitalic-Ïµ\epsilonitalic_Ïµ-approximate centroid set should contain at least one point that approximately represents the centroid location of any subset of this given dataset. It means that the Ïµitalic-Ïµ\epsilonitalic_Ïµ-approximate centroid set contains not only the approximate centroids based on the Voronoi diagram, but also the approximate centroids of those potential fairness-preserving clusters. Inspired by the above observation, we illustrate the relationship between fair kğ‘˜kitalic_k-means and Ïµitalic-Ïµ\epsilonitalic_Ïµ-approximate centroid set first, and then propose a novel Relax-and-Merge framework. In this framework, we relax the constraints on the number of clusters kğ‘˜kitalic_k; we focus on utilizing fair constraints to cluster the data into small and fair clusters, which are then merged together to determine the positions of kğ‘˜kitalic_k cluster centers. As shown in Table 1, our result is better than the state of the art works (Bera et al., 2019; BÃ¶hm et al., 2021). Equipped with a PTAS for kğ‘˜kitalic_k-means problem (e.g., the algorithm from Cohen-Addad et al. (2019)), our algorithm yields a 5+Oâ¢(Ïµ)5ğ‘‚italic-Ïµ{5+O(\epsilon)}5 + italic_O ( italic_Ïµ ) approximation factor. We also present two important extensions from our work. The first extension is an (1+4â¢Ï+Oâ¢(Ïµ))14ğœŒğ‘‚italic-Ïµ(1+4\rho+O(\epsilon))( 1 + 4 italic_Ï + italic_O ( italic_Ïµ ) ) solution for k-sparse Wasserstein Barycenter. The second one is about strictly fair kğ‘˜kitalic_k-means. We give a refined algorithm of Relax and Merge that yields a no-violation solution with a (2+6â¢Ï)26ğœŒ(2+6\rho)( 2 + 6 italic_Ï ) approximation factor, which is better than the state of the art work (BÃ¶hm et al., 2021). Table 1: Comparison of the approximation ratios for fair kğ‘˜kitalic_k-means and kğ‘˜kitalic_k-sparse WB. The â€œgeneral caseâ€ includes (Î±,Î²)ğ›¼ğ›½(\alpha,\beta)( italic_Î± , italic_Î² )-fair kğ‘˜kitalic_k-means, strictly (Î±,Î²)ğ›¼ğ›½(\alpha,\beta)( italic_Î± , italic_Î² )-fair kğ‘˜kitalic_k-means and kğ‘˜kitalic_k-sparse WB. Algorithms Approximation ratio When Ï=1+Oâ¢(Ïµ)ğœŒ1ğ‘‚italic-Ïµ\rho=1+O(\epsilon)italic_Ï = 1 + italic_O ( italic_Ïµ ) Note on the quality Bera et al. (2019) (2+Ï)2superscript2ğœŒ2(2+\sqrt{\rho})^{2}( 2 + square-root start_ARG italic_Ï end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT 9+Oâ¢(Ïµ)9ğ‘‚italic-Ïµ9+O(\epsilon)9 + italic_O ( italic_Ïµ ) general case Schmidt et al. (2020) 5.5â¢Ï+15.5ğœŒ15.5\rho+15.5 italic_Ï + 1 6.5+Oâ¢(Ïµ)6.5ğ‘‚italic-Ïµ6.5+O(\epsilon)6.5 + italic_O ( italic_Ïµ ) two groups only BÃ¶hm et al. (2021) (2+Ï)2superscript2ğœŒ2(2+\sqrt{\rho})^{2}( 2 + square-root start_ARG italic_Ï end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT 9+Oâ¢(Ïµ)9ğ‘‚italic-Ïµ9+O(\epsilon)9 + italic_O ( italic_Ïµ ) strictly only, no violation Yang and Ding (2024) (2+Ï)2superscript2ğœŒ2(2+\sqrt{\rho})^{2}( 2 + square-root start_ARG italic_Ï end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT 9+Oâ¢(Ïµ)9ğ‘‚italic-Ïµ9+O(\epsilon)9 + italic_O ( italic_Ïµ ) kğ‘˜kitalic_k-sparse WB Algorithm 1, now 1+4â¢Ï+Oâ¢(Ïµ)14ğœŒğ‘‚italic-Ïµ1+4\rho+O(\epsilon)1 + 4 italic_Ï + italic_O ( italic_Ïµ ) 5+Oâ¢(Ïµ)5ğ‘‚italic-Ïµ5+O(\epsilon)5 + italic_O ( italic_Ïµ ) general case Algorithm 2, now 2+6â¢Ï26ğœŒ2+6\rho2 + 6 italic_Ï 8+Oâ¢(Ïµ)8ğ‘‚italic-Ïµ8+O(\epsilon)8 + italic_O ( italic_Ïµ ) strictly only, no violation Other Related Works on kğ‘˜kitalic_k-Means The vanilla kğ‘˜kitalic_k-means problem is a topic that has been widely studied in both theory and practice. It has been proved that kğ‘˜kitalic_k-means clustering is NP-hard even in 2â¢D2ğ·2D2 italic_D if kğ‘˜kitalic_k is large (Mahajan et al., 2012). In high dimensions, even if kğ‘˜kitalic_k is fixed, say k=2ğ‘˜2k=2italic_k = 2, the kğ‘˜kitalic_k-means problem is still NP-hard (Drineas et al., 2004). Furthermore, Lee et al. (2017) proved the APX-hardness result for Euclidean kğ‘˜kitalic_k-means problem, which implies that it is impossible to approximate the optimal solution of kğ‘˜kitalic_k-means below a factor 1.0013 in polynomial time under the assumption of P â‰ \neqâ‰  NP. Therefore, a number of approximation algorithms have been proposed in theory. If the dimension dğ‘‘ditalic_d is fixed, Kanungo et al. (2002) obtained a (9+Oâ¢(Ïµ)9ğ‘‚italic-Ïµ9+O(\epsilon)9 + italic_O ( italic_Ïµ ))-approximate solution by using the local search technique. Roughly speaking, the idea of local search is swapping a small number of points in every iteration, so as to incrementally improve the solution until converging at some local optimum. Following this idea, Cohen-Addad et al. (2019) and Friggstad et al. (2019) proposed the PTAS for kğ‘˜kitalic_k-means in low dimensional space. For high-dimensional case with constant kğ‘˜kitalic_k, Kumar et al. (2010) proposed an elegant peeling algorithm that iteratively finds the kğ‘˜kitalic_k cluster centers and eventually obtain the PTAS."
https://arxiv.org/html/2411.00384v1,Perfect Matchings and Popularity in the Many-to-Many Settingâ€ â€ thanks:This result appeared in FSTTCSÂ 2023[29]. This version gives a simple proof that corrects a technical gap in the proof for the many-to-many setting in the conference version.,"We consider a matching problem in a bipartite graph GğºGitalic_G where every vertex has a capacity and a strict preference order on its neighbors. Furthermore, there is a cost function on the edge set. We assume GğºGitalic_G admits a perfect matching, i.e., one that fully matches all vertices. It is only perfect matchings that are feasible for us and we are interested in those perfect matchings that are popular within the set of perfect matchings. It is known that such matchings (called popular perfect matchings) always exist and can be efficiently computed. What we seek here is not any popular perfect matching, but a min-cost one. We show a polynomial-time algorithm for finding such a matching; this is via a characterization of popular perfect matchings in GğºGitalic_G in terms of stable matchings in a colorful auxiliary instance. This is a generalization of such a characterization that was known in the one-to-one setting.","Consider a matching problem in a bipartite graph G=(AâˆªB,E)ğºğ´ğµğ¸G=(A\cup B,E)italic_G = ( italic_A âˆª italic_B , italic_E ) where every vertex vâˆˆAâˆªBğ‘£ğ´ğµv\in A\cup Bitalic_v âˆˆ italic_A âˆª italic_B has a strict ranking of its neighbors. For convenience, vertices in Ağ´Aitalic_A will be called agents and those in BğµBitalic_B will be called jobs. Every agent/job vğ‘£vitalic_v has an integral capacity ğ–¼ğ–ºğ—‰â¢(v)â‰¥1ğ–¼ğ–ºğ—‰ğ‘£1\mathsf{cap}(v)\geq 1sansserif_cap ( italic_v ) â‰¥ 1 and seeks to be matched to ğ–¼ğ–ºğ—‰â¢(v)ğ–¼ğ–ºğ—‰ğ‘£\mathsf{cap}(v)sansserif_cap ( italic_v ) many neighbors. This is the bipartite bğ‘bitalic_b-matching framework. This model is also called the many-to-many setting in matchings under preferences. This is a well-studied model that is a natural generalization of several real-world settings such as matching students to schools and colleges [1, 3] and residents to hospitals [8, 35]. Note that agents (i.e., students/residents) in these applications have capacity 1 while in the many-to-many setting, agents are allowed to have capacity more than 1. For example, when Ağ´Aitalic_A is a set of students and BğµBitalic_B is a set of projects, the many-to-many setting allows for the flexibility that not only can several students work on one project, but a student can be part of several projects. One such application is a management system for a conference where program committee members have preferences over submissions and each submission has a ranking of PC members as per their suitability â€“ in terms of their areas of expertise â€“ to review this paper.111PC members typically have rankings with ties over papers; we can instead assume a project evaluation committee where each member has a strict ranking of the project proposals that he/she would like to evaluate and each proposal needs a certain number of independent evaluations. Every PC member (also, assume every paper) has a capacity. Definition 1 A matching Mğ‘€Mitalic_M in a many-to-many instance G=(AâˆªB,E)ğºğ´ğµğ¸G=(A\cup B,E)italic_G = ( italic_A âˆª italic_B , italic_E ) is a subset of the edge set Eğ¸Eitalic_E such that |Mâ¢(v)|â‰¤ğ–¼ğ–ºğ—‰â¢(v)ğ‘€ğ‘£ğ–¼ğ–ºğ—‰ğ‘£|M(v)|\leq\mathsf{cap}(v)| italic_M ( italic_v ) | â‰¤ sansserif_cap ( italic_v ) for each vertex vğ‘£vitalic_v, where Mâ¢(v)={u:(u,v)âˆˆM}ğ‘€ğ‘£conditional-setğ‘¢ğ‘¢ğ‘£ğ‘€M(v)=\{u:(u,v)\in M\}italic_M ( italic_v ) = { italic_u : ( italic_u , italic_v ) âˆˆ italic_M }. Though vertices in both the sets Ağ´Aitalic_A and BğµBitalic_B have capacities, note that Mğ‘€Mitalic_M can contain at most one copy of any edge since Mğ‘€Mitalic_M is a set (and not a multiset). Let us assume that the input instance admits a perfect matching, i.e., one that matches each vertex in AâˆªBğ´ğµA\cup Bitalic_A âˆª italic_B fully up to its capacity. Such a model occurs in the conference management system application mentioned above where PC members have to place bids and then they are explicitly asked to bid again for submissions that have received too few bids. Thus it will be ensured that the input instance admits a matching where each PC member ağ‘aitalic_a gets assigned to ğ–¼ğ–ºğ—‰â¢(a)ğ–¼ğ–ºğ—‰ğ‘\mathsf{cap}(a)sansserif_cap ( italic_a ) many papers and each paper bğ‘bitalic_b is assigned to ğ–¼ğ–ºğ—‰â¢(b)ğ–¼ğ–ºğ—‰ğ‘\mathsf{cap}(b)sansserif_cap ( italic_b ) many PC members. So we assume our input instance admits a perfect matching and we seek a best perfect matching as per vertex preferences. In the domain of matchings under preferences, stable matchings are usually regarded as the best matchings. Stable matchings. A matching Mğ‘€Mitalic_M is stable if there is no edge that blocks Mğ‘€Mitalic_M, where an edge (a,b)âˆ‰Mğ‘ğ‘ğ‘€(a,b)\notin M( italic_a , italic_b ) âˆ‰ italic_M blocks Mğ‘€Mitalic_M if (i) either ağ‘aitalic_a has less than ğ–¼ğ–ºğ—‰â¢(a)ğ–¼ğ–ºğ—‰ğ‘\mathsf{cap}(a)sansserif_cap ( italic_a ) partners in Mğ‘€Mitalic_M or ağ‘aitalic_a prefers bğ‘bitalic_b to its worst partner in Mğ‘€Mitalic_M and (ii) either bğ‘bitalic_b has less than ğ–¼ğ–ºğ—‰â¢(b)ğ–¼ğ–ºğ—‰ğ‘\mathsf{cap}(b)sansserif_cap ( italic_b ) partners in Mğ‘€Mitalic_M or bğ‘bitalic_b prefers ağ‘aitalic_a to its worst partner in Mğ‘€Mitalic_M. Stable matchings always exist in GğºGitalic_G and a natural modification of the Gale-Shapley algorithm [17] finds one. However stability is a strong and rather restrictive notion, e.g., stable matchings need not have maximum cardinality. Consider the following instance where A={a,aâ€²},B={b,bâ€²}formulae-sequenceğ´ğ‘superscriptğ‘â€²ğµğ‘superscriptğ‘â€²A=\{a,a^{\prime}\},B=\{b,b^{\prime}\}italic_A = { italic_a , italic_a start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT } , italic_B = { italic_b , italic_b start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT }, and all vertex capacities are 1. The preferences of agents are as follows: a:bâ‰»bâ€²:ğ‘succeedsğ‘superscriptğ‘â€²a:b\succ b^{\prime}italic_a : italic_b â‰» italic_b start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT and aâ€²:b:superscriptğ‘â€²ğ‘a^{\prime}:bitalic_a start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT : italic_b. That is, the top choice of ağ‘aitalic_a is bğ‘bitalic_b and its second choice is bâ€²superscriptğ‘â€²b^{\prime}italic_b start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT while aâ€²superscriptğ‘â€²a^{\prime}italic_a start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT has only one neighbor bğ‘bitalic_b. Similarly, the preferences of jobs are as follows: b:aâ‰»aâ€²:ğ‘succeedsğ‘superscriptğ‘â€²b:a\succ a^{\prime}italic_b : italic_a â‰» italic_a start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT and bâ€²:a:superscriptğ‘â€²ğ‘b^{\prime}:aitalic_b start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT : italic_a. This instance has only one stable matching S={(a,b)}ğ‘†ğ‘ğ‘S=\{(a,b)\}italic_S = { ( italic_a , italic_b ) } that leaves aâ€²superscriptğ‘â€²a^{\prime}italic_a start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT and bâ€²superscriptğ‘â€²b^{\prime}italic_b start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT unmatched. Observe that this instance has a perfect matching M={(a,bâ€²),(aâ€²,b)}ğ‘€ğ‘superscriptğ‘â€²superscriptğ‘â€²ğ‘M=\{(a,b^{\prime}),(a^{\prime},b)\}italic_M = { ( italic_a , italic_b start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) , ( italic_a start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT , italic_b ) } that matches all agents and jobs. However Mğ‘€Mitalic_M is not stable as the edge (a,b)ğ‘ğ‘(a,b)( italic_a , italic_b ) blocks Mğ‘€Mitalic_M. For the problem of selecting a best perfect matching, one with the least number of blocking edges would be a natural relaxation of stability. But finding such a matching is NP-hard [5] even in the one-to-one setting. A well-studied relaxation of stability that offers a meaningful and tractable solution to the problem of finding a â€œbest perfect matchingâ€ is the notion of popularity. Popularity. Popularity is based on voting by vertices on matchings. In the one-to-one setting, the preferences of a vertex over its neighbors extend naturally to preferences over matchingsâ€”so every vertex orders matchings in the order of its partners in these matchings. Popular matchings are weak Condorcet winners [13, 32] in this voting instance where vertices are voters and matchings are candidates. In other words, a popular matching Mğ‘€Mitalic_M does not lose a head-to-head election against any matching Nğ‘Nitalic_N where each vertex vğ‘£vitalic_v either casts a vote for the matching in {M,N}ğ‘€ğ‘\{M,N\}{ italic_M , italic_N } that it prefers or vğ‘£vitalic_v abstains from voting if its assignment is the same in Mğ‘€Mitalic_M and Nğ‘Nitalic_N. For any vertex vğ‘£vitalic_v and its neighbors uğ‘¢uitalic_u and uâ€²superscriptğ‘¢â€²u^{\prime}italic_u start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT, let ğ—ğ—ˆğ—ğ–¾vâ¢(u,uâ€²)subscriptğ—ğ—ˆğ—ğ–¾ğ‘£ğ‘¢superscriptğ‘¢â€²\mathsf{vote}_{v}(u,u^{\prime})sansserif_vote start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_u , italic_u start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) be vğ‘£vitalic_vâ€™s vote for uğ‘¢uitalic_u versus uâ€²superscriptğ‘¢â€²u^{\prime}italic_u start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT. More precisely, ğ—ğ—ˆğ—ğ–¾vâ¢(u,uâ€²)=1subscriptğ—ğ—ˆğ—ğ–¾ğ‘£ğ‘¢superscriptğ‘¢â€²1\mathsf{vote}_{v}(u,u^{\prime})=1sansserif_vote start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_u , italic_u start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) = 1 if vğ‘£vitalic_v prefers uğ‘¢uitalic_u to uâ€²superscriptğ‘¢â€²u^{\prime}italic_u start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT, it is âˆ’11-1- 1 if vğ‘£vitalic_v prefers uâ€²superscriptğ‘¢â€²u^{\prime}italic_u start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT to uğ‘¢uitalic_u, and it is 0 otherwise (so u=uâ€²ğ‘¢superscriptğ‘¢â€²u=u^{\prime}italic_u = italic_u start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT). Thus in the one-to-one setting, every vertex vğ‘£vitalic_v casts its vote which is ğ—ğ—ˆğ—ğ–¾vâ¢(u,uâ€²)âˆˆ{0,Â±1}subscriptğ—ğ—ˆğ—ğ–¾ğ‘£ğ‘¢superscriptğ‘¢â€²0plus-or-minus1\mathsf{vote}_{v}(u,u^{\prime})\in\{0,\pm 1\}sansserif_vote start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_u , italic_u start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) âˆˆ { 0 , Â± 1 }, where Mâ¢(v)={u}ğ‘€ğ‘£ğ‘¢M(v)=\{u\}italic_M ( italic_v ) = { italic_u } and Nâ¢(v)={uâ€²}ğ‘ğ‘£superscriptğ‘¢â€²N(v)=\{u^{\prime}\}italic_N ( italic_v ) = { italic_u start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT }, for Mğ‘€Mitalic_M versus Nğ‘Nitalic_N in their head-to-head election. Recall that we are in the many-to-many setting, i.e., vertices have capacities. So we need to specify how a vertex votes over different subsets of its neighbors. Thus we need to compare two subsets Mâ¢(v)ğ‘€ğ‘£M(v)italic_M ( italic_v ) and Nâ¢(v)ğ‘ğ‘£N(v)italic_N ( italic_v ) of ğ–­ğ–»ğ—‹â¢(v)ğ–­ğ–»ğ—‹ğ‘£\mathsf{Nbr}(v)sansserif_Nbr ( italic_v ) for any vertex vğ‘£vitalic_v, where ğ–­ğ–»ğ—‹â¢(v)ğ–­ğ–»ğ—‹ğ‘£\mathsf{Nbr}(v)sansserif_Nbr ( italic_v ) is the set of neighbors of vğ‘£vitalic_v. We will follow the method from [7] for this comparison where vğ‘£vitalic_v is allowed to cast up to ğ–¼ğ–ºğ—‰â¢(v)ğ–¼ğ–ºğ—‰ğ‘£\mathsf{cap}(v)sansserif_cap ( italic_v ) many votes. Any two subsets Sğ‘†Sitalic_S and Tğ‘‡Titalic_T of ğ–­ğ–»ğ—‹â¢(v)ğ–­ğ–»ğ—‹ğ‘£\mathsf{Nbr}(v)sansserif_Nbr ( italic_v ) are compared by vertex vğ‘£vitalic_v as follows: â€¢ a bijection Ïˆğœ“\psiitalic_Ïˆ is chosen from Sâ€²=Sâˆ–Tsuperscriptğ‘†â€²ğ‘†ğ‘‡S^{\prime}=S\setminus Titalic_S start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT = italic_S âˆ– italic_T to Tâ€²=Tâˆ–Ssuperscriptğ‘‡â€²ğ‘‡ğ‘†T^{\prime}=T\setminus Sitalic_T start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT = italic_T âˆ– italic_S;222If the two sets are not of equal size, then dummy vertices that are less preferred to all non-dummy vertices are added to the smaller set. â€¢ every neighbor uâˆˆSâ€²ğ‘¢superscriptğ‘†â€²u\in S^{\prime}italic_u âˆˆ italic_S start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT is compared with Ïˆâ¢(u)âˆˆTâ€²ğœ“ğ‘¢superscriptğ‘‡â€²\psi(u)\in T^{\prime}italic_Ïˆ ( italic_u ) âˆˆ italic_T start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT; â€¢ the number of wins minus the number of losses is vğ‘£vitalic_vâ€™s vote for Sğ‘†Sitalic_S versus Tğ‘‡Titalic_T. The bijection Ïˆğœ“\psiitalic_Ïˆ from Sâ€²superscriptğ‘†â€²S^{\prime}italic_S start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT to Tâ€²superscriptğ‘‡â€²T^{\prime}italic_T start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT that is chosen will be the one that minimizes vğ‘£vitalic_vâ€™s vote for Sğ‘†Sitalic_S versus Tğ‘‡Titalic_T. More formally, the vote of vğ‘£vitalic_v for Sğ‘†Sitalic_S versus Tğ‘‡Titalic_T, denoted by ğ—ğ—ˆğ—ğ–¾vâ¢(S,T)subscriptğ—ğ—ˆğ—ğ–¾ğ‘£ğ‘†ğ‘‡\mathsf{vote}_{v}(S,T)sansserif_vote start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_S , italic_T ), is defined as follows where |Sâ€²|=|Tâ€²|=ksuperscriptğ‘†â€²superscriptğ‘‡â€²ğ‘˜|S^{\prime}|=|T^{\prime}|=k| italic_S start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT | = | italic_T start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT | = italic_k and Î â¢[k]Î delimited-[]ğ‘˜\Pi[k]roman_Î  [ italic_k ] is the set of permutations on {1,â€¦,k}1â€¦ğ‘˜\{1,\ldots,k\}{ 1 , â€¦ , italic_k }: ğ—ğ—ˆğ—ğ–¾vâ¢(S,T)=minÏƒâˆˆÎ â¢[k]â¢âˆ‘i=1kğ—ğ—ˆğ—ğ–¾vâ¢(Sâ€²â¢[i],Tâ€²â¢[Ïƒâ¢(i)]).subscriptğ—ğ—ˆğ—ğ–¾ğ‘£ğ‘†ğ‘‡subscriptğœÎ delimited-[]ğ‘˜superscriptsubscriptğ‘–1ğ‘˜subscriptğ—ğ—ˆğ—ğ–¾ğ‘£superscriptğ‘†â€²delimited-[]ğ‘–superscriptğ‘‡â€²delimited-[]ğœğ‘–\mathsf{vote}_{v}(S,T)=\min_{\sigma\in\Pi[k]}\sum_{i=1}^{k}\mathsf{vote}_{v}(S% ^{\prime}[i],T^{\prime}[\sigma(i)]).sansserif_vote start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_S , italic_T ) = roman_min start_POSTSUBSCRIPT italic_Ïƒ âˆˆ roman_Î  [ italic_k ] end_POSTSUBSCRIPT âˆ‘ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT sansserif_vote start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_S start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT [ italic_i ] , italic_T start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT [ italic_Ïƒ ( italic_i ) ] ) . (1) Here Sâ€²â¢[i]superscriptğ‘†â€²delimited-[]ğ‘–S^{\prime}[i]italic_S start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT [ italic_i ] is the iğ‘–iitalic_i-th ranked resident in Sâ€²superscriptğ‘†â€²S^{\prime}italic_S start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT and Tâ€²â¢[Ïƒâ¢(i)]superscriptğ‘‡â€²delimited-[]ğœğ‘–T^{\prime}[\sigma(i)]italic_T start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT [ italic_Ïƒ ( italic_i ) ] is the Ïƒâ¢(i)ğœğ‘–\sigma(i)italic_Ïƒ ( italic_i )-th ranked resident in Tâ€²superscriptğ‘‡â€²T^{\prime}italic_T start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT. Consider the following example from [7] where ğ–­ğ–»ğ—‹â¢(v)={u1,u2,u3,u4,u5,u6}ğ–­ğ–»ğ—‹ğ‘£subscriptğ‘¢1subscriptğ‘¢2subscriptğ‘¢3subscriptğ‘¢4subscriptğ‘¢5subscriptğ‘¢6\mathsf{Nbr}(v)=\{u_{1},u_{2},u_{3},u_{4},u_{5},u_{6}\}sansserif_Nbr ( italic_v ) = { italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPT } and vğ‘£vitalic_vâ€™s preference order is u1â‰»u2â‰»u3â‰»u4â‰»u5â‰»u6succeedssubscriptğ‘¢1subscriptğ‘¢2succeedssubscriptğ‘¢3succeedssubscriptğ‘¢4succeedssubscriptğ‘¢5succeedssubscriptğ‘¢6u_{1}\succ u_{2}\succ u_{3}\succ u_{4}\succ u_{5}\succ u_{6}italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT â‰» italic_u start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT â‰» italic_u start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT â‰» italic_u start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT â‰» italic_u start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT â‰» italic_u start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPT. Let S={u1,u3,u5}ğ‘†subscriptğ‘¢1subscriptğ‘¢3subscriptğ‘¢5S=\{u_{1},u_{3},u_{5}\}italic_S = { italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT } and T={u2,u4,u6}ğ‘‡subscriptğ‘¢2subscriptğ‘¢4subscriptğ‘¢6T=\{u_{2},u_{4},u_{6}\}italic_T = { italic_u start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPT }. So Sâ€²=Ssuperscriptğ‘†â€²ğ‘†S^{\prime}=Sitalic_S start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT = italic_S and Tâ€²=Tsuperscriptğ‘‡â€²ğ‘‡T^{\prime}=Titalic_T start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT = italic_T. The bijection Ïˆ:Sâ€²â†’Tâ€²:ğœ“â†’superscriptğ‘†â€²superscriptğ‘‡â€²\psi:S^{\prime}\rightarrow T^{\prime}italic_Ïˆ : italic_S start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT â†’ italic_T start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT is Ïˆâ¢(u1)=u6ğœ“subscriptğ‘¢1subscriptğ‘¢6\psi(u_{1})=u_{6}italic_Ïˆ ( italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) = italic_u start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPT, Ïˆâ¢(u3)=u2ğœ“subscriptğ‘¢3subscriptğ‘¢2\psi(u_{3})=u_{2}italic_Ïˆ ( italic_u start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ) = italic_u start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, and Ïˆâ¢(u5)=u4ğœ“subscriptğ‘¢5subscriptğ‘¢4\psi(u_{5})=u_{4}italic_Ïˆ ( italic_u start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT ) = italic_u start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT as this is the most adversarial way of comparing Sâ€²superscriptğ‘†â€²S^{\prime}italic_S start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT with Tâ€²superscriptğ‘‡â€²T^{\prime}italic_T start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT. This results in ğ—ğ—ˆğ—ğ–¾vâ¢(S,T)=ğ—ğ—ˆğ—ğ–¾vâ¢(u1,u6)+ğ—ğ—ˆğ—ğ–¾vâ¢(u3,u2)+ğ—ğ—ˆğ—ğ–¾vâ¢(u5,u4)=1âˆ’1âˆ’1=âˆ’1subscriptğ—ğ—ˆğ—ğ–¾ğ‘£ğ‘†ğ‘‡subscriptğ—ğ—ˆğ—ğ–¾ğ‘£subscriptğ‘¢1subscriptğ‘¢6subscriptğ—ğ—ˆğ—ğ–¾ğ‘£subscriptğ‘¢3subscriptğ‘¢2subscriptğ—ğ—ˆğ—ğ–¾ğ‘£subscriptğ‘¢5subscriptğ‘¢41111\mathsf{vote}_{v}(S,T)=\mathsf{vote}_{v}(u_{1},u_{6})+\mathsf{vote}_{v}(u_{3},% u_{2})+\mathsf{vote}_{v}(u_{5},u_{4})=1-1-1=-1sansserif_vote start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_S , italic_T ) = sansserif_vote start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPT ) + sansserif_vote start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_u start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) + sansserif_vote start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_u start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT ) = 1 - 1 - 1 = - 1. Similarly, ğ—ğ—ˆğ—ğ–¾vâ¢(T,S)=ğ—ğ—ˆğ—ğ–¾vâ¢(u2,u1)+ğ—ğ—ˆğ—ğ–¾vâ¢(u4,u3)+ğ—ğ—ˆğ—ğ–¾vâ¢(u6,u5)=âˆ’3subscriptğ—ğ—ˆğ—ğ–¾ğ‘£ğ‘‡ğ‘†subscriptğ—ğ—ˆğ—ğ–¾ğ‘£subscriptğ‘¢2subscriptğ‘¢1subscriptğ—ğ—ˆğ—ğ–¾ğ‘£subscriptğ‘¢4subscriptğ‘¢3subscriptğ—ğ—ˆğ—ğ–¾ğ‘£subscriptğ‘¢6subscriptğ‘¢53\mathsf{vote}_{v}(T,S)=\mathsf{vote}_{v}(u_{2},u_{1})+\mathsf{vote}_{v}(u_{4},% u_{3})+\mathsf{vote}_{v}(u_{6},u_{5})=-3sansserif_vote start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_T , italic_S ) = sansserif_vote start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_u start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) + sansserif_vote start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_u start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ) + sansserif_vote start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_u start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT ) = - 3. For any pair of matchings M,Nğ‘€ğ‘M,Nitalic_M , italic_N and vertex vğ‘£vitalic_v, let ğ—ğ—ˆğ—ğ–¾vâ¢(M,N)=ğ—ğ—ˆğ—ğ–¾vâ¢(S,T)subscriptğ—ğ—ˆğ—ğ–¾ğ‘£ğ‘€ğ‘subscriptğ—ğ—ˆğ—ğ–¾ğ‘£ğ‘†ğ‘‡\mathsf{vote}_{v}(M,N)=\mathsf{vote}_{v}(S,T)sansserif_vote start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_M , italic_N ) = sansserif_vote start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_S , italic_T ) where S=Mâ¢(v)ğ‘†ğ‘€ğ‘£S=M(v)italic_S = italic_M ( italic_v ) and T=Nâ¢(v)ğ‘‡ğ‘ğ‘£T=N(v)italic_T = italic_N ( italic_v ). So ğ—ğ—ˆğ—ğ–¾vâ¢(M,N)subscriptğ—ğ—ˆğ—ğ–¾ğ‘£ğ‘€ğ‘\mathsf{vote}_{v}(M,N)sansserif_vote start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_M , italic_N ) counts the number of votes by vğ‘£vitalic_v for Mâ¢(v)ğ‘€ğ‘£M(v)italic_M ( italic_v ) versus Nâ¢(v)ğ‘ğ‘£N(v)italic_N ( italic_v ) when the two sets Mâ¢(v)âˆ–Nâ¢(v)ğ‘€ğ‘£ğ‘ğ‘£M(v)\setminus N(v)italic_M ( italic_v ) âˆ– italic_N ( italic_v ) and Nâ¢(v)âˆ–Mâ¢(v)ğ‘ğ‘£ğ‘€ğ‘£N(v)\setminus M(v)italic_N ( italic_v ) âˆ– italic_M ( italic_v ) are compared in the order that is most adversarial or negative for Mğ‘€Mitalic_M. The two matchings Mğ‘€Mitalic_M and Nğ‘Nitalic_N are compared using Î”â¢(M,N)=âˆ‘vâˆˆAâˆªBğ—ğ—ˆğ—ğ–¾vâ¢(Mâ¢(v),Nâ¢(v))Î”ğ‘€ğ‘subscriptğ‘£ğ´ğµsubscriptğ—ğ—ˆğ—ğ–¾ğ‘£ğ‘€ğ‘£ğ‘ğ‘£\Delta(M,N)=\sum_{v\in A\cup B}\mathsf{vote}_{v}(M(v),N(v))roman_Î” ( italic_M , italic_N ) = âˆ‘ start_POSTSUBSCRIPT italic_v âˆˆ italic_A âˆª italic_B end_POSTSUBSCRIPT sansserif_vote start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_M ( italic_v ) , italic_N ( italic_v ) ). We will say matching Mğ‘€Mitalic_M is more popular than matching Nğ‘Nitalic_N if Î”â¢(M,N)>0Î”ğ‘€ğ‘0\Delta(M,N)>0roman_Î” ( italic_M , italic_N ) > 0. Definition 2 A matching Mğ‘€Mitalic_M is popular if Î”â¢(M,N)â‰¥0Î”ğ‘€ğ‘0\Delta(M,N)\geq 0roman_Î” ( italic_M , italic_N ) â‰¥ 0 for all matchings Nğ‘Nitalic_N in GğºGitalic_G. Thus Mğ‘€Mitalic_M is popular if there is no matching more popular than Mğ‘€Mitalic_M. Recall our assumption that the set of feasible solutions is the set of perfect matchings. Hence the matchings of interest to us are popular perfect matchings, defined below. Definition 3 A perfect matching Mğ‘€Mitalic_M is a popular perfect matching if Î”â¢(M,N)â‰¥0Î”ğ‘€ğ‘0\Delta(M,N)\geq 0roman_Î” ( italic_M , italic_N ) â‰¥ 0 for all perfect matchings Nğ‘Nitalic_N in GğºGitalic_G. So a popular perfect matching need not be popularâ€”it may lose to a smaller size matching; nevertheless, it never loses to a perfect matching. Since weak Condorcet winners do not always exist, it is not a priori clear if a popular perfect matching always exists. It was shown in [27] that there always exists a popular perfect matching in the one-to-one setting;333It was shown in [27] that a popular maximum matching (i.e., a maximum matching that is popular among maximum matchings) always exists in the one-to-one setting. moreover, such a matching can be computed in polynomial time. The following problem in the many-to-many setting was considered in [33]: vertices have capacity lower bounds and feasible matchings are those that satisfy these lower bounds. It was shown that popular feasible matchings always exist and can be computed in polynomial time. Popular perfect matchings are a special case of this problem where every vertex has to be fully matched, i.e., every vertex vğ‘£vitalic_v has a lower bound of ğ–¼ğ–ºğ—‰â¢(v)ğ–¼ğ–ºğ—‰ğ‘£\mathsf{cap}(v)sansserif_cap ( italic_v ). Thus popular perfect matchings always exist in the many-to-many setting and can be computed in polynomial time. Our problem. We assume there is a function ğ–¼ğ—ˆğ—Œğ—:Eâ†’â„:ğ–¼ğ—ˆğ—Œğ—â†’ğ¸â„\mathsf{cost}:E\rightarrow\mathbb{R}sansserif_cost : italic_E â†’ blackboard_R, which is part of the input. Hence the cost of any matching Mğ‘€Mitalic_M is âˆ‘eâˆˆMğ–¼ğ—ˆğ—Œğ—â¢(e)subscriptğ‘’ğ‘€ğ–¼ğ—ˆğ—Œğ—ğ‘’\sum_{e\in M}\mathsf{cost}(e)âˆ‘ start_POSTSUBSCRIPT italic_e âˆˆ italic_M end_POSTSUBSCRIPT sansserif_cost ( italic_e ). There might be exponentially many popular perfect matchings in GğºGitalic_G, hence we would like to find an min-cost one, i.e., a popular perfect matching whose cost is least among all popular perfect matchings. Solving the min-cost popular perfect matching problem efficiently implies efficient algorithms for several desirable popular perfect matching problems such as finding one with the highest utility when every edge has an associated utility or one with forced/forbidden edges or an egalitarian one. In the conference management system application discussed earlier, we would like to find a popular perfect matching that matches as many PC members as possible along top ranked edges, subject to that, as many PC members as possible along second ranked edges, and so on [22]. Such a popular perfect matching is a min-cost popular perfect matching for an appropriate cost function. A polynomial-time algorithm for the min-cost popular perfect matching problem in the one-to-one setting was shown in [28]. The conference version of our paper [29] gave a polynomial-time algorithm for this problem in the many-to-one setting (i.e., ğ–¼ğ–ºğ—‰â¢(a)=1ğ–¼ğ–ºğ—‰ğ‘1\mathsf{cap}(a)=1sansserif_cap ( italic_a ) = 1 for all aâˆˆAğ‘ğ´a\in Aitalic_a âˆˆ italic_A) by reducing it to the min-cost popular perfect matching problem in the one-to-one setting and using the polynomial-time algorithm for computing a min-cost popular perfect matching in the one-to-one setting [28]. The hospitals/residents setting. The many-to-one setting is usually referred to as the hospitals/residents setting. The reduction in [29] from the hospitals/residents setting to the one-to-one setting is via the cloned instance Gâ€²=(AâˆªBâ€²,Eâ€²)superscriptğºâ€²ğ´superscriptğµâ€²superscriptğ¸â€²G^{\prime}=(A\cup B^{\prime},E^{\prime})italic_G start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT = ( italic_A âˆª italic_B start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT , italic_E start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) corresponding to the given hospitals/residents instance G=(AâˆªB,E)ğºğ´ğµğ¸G=(A\cup B,E)italic_G = ( italic_A âˆª italic_B , italic_E ). Every bâˆˆBğ‘ğµb\in Bitalic_b âˆˆ italic_B is replaced by ğ–¼ğ–ºğ—‰â¢(b)ğ–¼ğ–ºğ—‰ğ‘\mathsf{cap}(b)sansserif_cap ( italic_b ) many clones b1,b2,â€¦subscriptğ‘1subscriptğ‘2â€¦b_{1},b_{2},\ldotsitalic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ in Bâ€²superscriptğµâ€²B^{\prime}italic_B start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT and the preference order of every bisubscriptğ‘ğ‘–b_{i}italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is the same as the preference order of bğ‘bitalic_b. Furthermore, every aâˆˆAğ‘ğ´a\in Aitalic_a âˆˆ italic_A that is a neighbor of bğ‘bitalic_b in GğºGitalic_G replaces the occurrence of bğ‘bitalic_b in its preference order with b1â‰»â‹¯â‰»bğ–¼ğ–ºğ—‰â¢(b)succeedssubscriptğ‘1â‹¯succeedssubscriptğ‘ğ–¼ğ–ºğ—‰ğ‘b_{1}\succ\cdots\succ b_{\mathsf{cap}(b)}italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT â‰» â‹¯ â‰» italic_b start_POSTSUBSCRIPT sansserif_cap ( italic_b ) end_POSTSUBSCRIPT. There is a natural map fğ‘“fitalic_f from the set of popular perfect matchings in the one-to-one instance Gâ€²superscriptğºâ€²G^{\prime}italic_G start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT to the set of perfect matchings in the original instance GğºGitalic_G, where for any popular perfect matching Mâ€²superscriptğ‘€â€²M^{\prime}italic_M start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT in Gâ€²superscriptğºâ€²G^{\prime}italic_G start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT, the many-to-one matching fâ¢(Mâ€²)ğ‘“superscriptğ‘€â€²f(M^{\prime})italic_f ( italic_M start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) is obtained by replacing each edge (a,bi)ğ‘subscriptğ‘ğ‘–(a,b_{i})( italic_a , italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) in Mâ€²superscriptğ‘€â€²M^{\prime}italic_M start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT with the original edge (a,b)ğ‘ğ‘(a,b)( italic_a , italic_b ). It can be shown that fâ¢(Mâ€²)ğ‘“superscriptğ‘€â€²f(M^{\prime})italic_f ( italic_M start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) is a popular perfect matching in GğºGitalic_G. So we have f:{f:\{italic_f : {popular perfect matchings in Gâ€²}â†’G^{\prime}\}\rightarrowitalic_G start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT } â†’ {{\{{popular perfect matchings in G}G\}italic_G }. It was shown in [29] that the mapping fğ‘“fitalic_f is surjective. Hence solving the min-cost popular perfect matching problem in Gâ€²superscriptğºâ€²G^{\prime}italic_G start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT solves the min-cost popular perfect matching problem in GğºGitalic_G. Though it was claimed in [29] that the same approach can be generalized to the many-to-many setting, unfortunately it does not extend to the many-to-many setting. The many-to-many setting. Consider G=(AâˆªB,E)ğºğ´ğµğ¸G=(A\cup B,E)italic_G = ( italic_A âˆª italic_B , italic_E ) where A={a,aâ€²}ğ´ğ‘superscriptğ‘â€²A=\{a,a^{\prime}\}italic_A = { italic_a , italic_a start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT } and B={b,bâ€²}ğµğ‘superscriptğ‘â€²B=\{b,b^{\prime}\}italic_B = { italic_b , italic_b start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT } and every vertex has capacity 2. Vertex preferences are as follows: a:bâ‰»bâ€²:ğ‘succeedsğ‘superscriptğ‘â€²a\colon b\succ b^{\prime}italic_a : italic_b â‰» italic_b start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT and aâ€²:bâ€²â‰»b:superscriptğ‘â€²succeedssuperscriptğ‘â€²ğ‘a^{\prime}\colon b^{\prime}\succ bitalic_a start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT : italic_b start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT â‰» italic_b along with b:aâ‰»aâ€²:ğ‘succeedsğ‘superscriptğ‘â€²b\colon a\succ a^{\prime}italic_b : italic_a â‰» italic_a start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT and bâ€²:aâ€²â‰»a:superscriptğ‘â€²succeedssuperscriptğ‘â€²ğ‘b^{\prime}\colon a^{\prime}\succ aitalic_b start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT : italic_a start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT â‰» italic_a. There is only one perfect matching M={(a,b),(a,bâ€²),(aâ€²,b),(aâ€²,bâ€²)}ğ‘€ğ‘ğ‘ğ‘superscriptğ‘â€²superscriptğ‘â€²ğ‘superscriptğ‘â€²superscriptğ‘â€²M=\{(a,b),(a,b^{\prime}),(a^{\prime},b),(a^{\prime},b^{\prime})\}italic_M = { ( italic_a , italic_b ) , ( italic_a , italic_b start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) , ( italic_a start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT , italic_b ) , ( italic_a start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT , italic_b start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) } in GğºGitalic_G, hence Mğ‘€Mitalic_M is a popular perfect matching. But Mğ‘€Mitalic_M cannot be realized as a popular perfect matching in the corresponding one-to-one instance Gâ€²superscriptğºâ€²G^{\prime}italic_G start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT. The vertex set of Gâ€²=(Aâ€²âˆªBâ€²,Eâ€²)superscriptğºâ€²superscriptğ´â€²superscriptğµâ€²superscriptğ¸â€²G^{\prime}=(A^{\prime}\cup B^{\prime},E^{\prime})italic_G start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT = ( italic_A start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT âˆª italic_B start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT , italic_E start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) is given by Aâ€²={a1,a2,a1â€²,a2â€²}superscriptğ´â€²subscriptğ‘1subscriptğ‘2subscriptsuperscriptğ‘â€²1subscriptsuperscriptğ‘â€²2A^{\prime}=\{a_{1},a_{2},a^{\prime}_{1},a^{\prime}_{2}\}italic_A start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT = { italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_a start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_a start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT } and Bâ€²={b1,b2,b1â€²,b2â€²}superscriptğµâ€²subscriptğ‘1subscriptğ‘2subscriptsuperscriptğ‘â€²1subscriptsuperscriptğ‘â€²2B^{\prime}=\{b_{1},b_{2},b^{\prime}_{1},b^{\prime}_{2}\}italic_B start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT = { italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_b start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_b start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT }. Vertex preferences in Gâ€²superscriptğºâ€²G^{\prime}italic_G start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT are as follows. a1:b1â‰»b2â‰»b1â€²â‰»b2â€²:subscriptğ‘1succeedssubscriptğ‘1subscriptğ‘2succeedssubscriptsuperscriptğ‘â€²1succeedssubscriptsuperscriptğ‘â€²2\displaystyle a_{1}\colon b_{1}\succ b_{2}\succ b^{\prime}_{1}\succ b^{\prime}% _{2}\qquad\qquaditalic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT : italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT â‰» italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT â‰» italic_b start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT â‰» italic_b start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT b1:a1â‰»a2â‰»a1â€²â‰»a2â€²:subscriptğ‘1succeedssubscriptğ‘1subscriptğ‘2succeedssubscriptsuperscriptğ‘â€²1succeedssubscriptsuperscriptğ‘â€²2\displaystyle b_{1}\colon a_{1}\succ a_{2}\succ a^{\prime}_{1}\succ a^{\prime}% _{2}italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT : italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT â‰» italic_a start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT â‰» italic_a start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT â‰» italic_a start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT a2:b1â‰»b2â‰»b1â€²â‰»b2â€²:subscriptğ‘2succeedssubscriptğ‘1subscriptğ‘2succeedssubscriptsuperscriptğ‘â€²1succeedssubscriptsuperscriptğ‘â€²2\displaystyle a_{2}\colon b_{1}\succ b_{2}\succ b^{\prime}_{1}\succ b^{\prime}% _{2}\qquad\qquaditalic_a start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT : italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT â‰» italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT â‰» italic_b start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT â‰» italic_b start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT b2:a1â‰»a2â‰»a1â€²â‰»a2â€²:subscriptğ‘2succeedssubscriptğ‘1subscriptğ‘2succeedssubscriptsuperscriptğ‘â€²1succeedssubscriptsuperscriptğ‘â€²2\displaystyle b_{2}\colon a_{1}\succ a_{2}\succ a^{\prime}_{1}\succ a^{\prime}% _{2}italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT : italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT â‰» italic_a start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT â‰» italic_a start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT â‰» italic_a start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT a1â€²:b1â€²â‰»b2â€²â‰»b1â‰»b2:subscriptsuperscriptğ‘â€²1succeedssubscriptsuperscriptğ‘â€²1subscriptsuperscriptğ‘â€²2succeedssubscriptğ‘1succeedssubscriptğ‘2\displaystyle a^{\prime}_{1}\colon b^{\prime}_{1}\succ b^{\prime}_{2}\succ b_{% 1}\succ b_{2}\qquad\qquaditalic_a start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT : italic_b start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT â‰» italic_b start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT â‰» italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT â‰» italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT b1â€²:a1â€²â‰»a2â€²â‰»a1â‰»a2:subscriptsuperscriptğ‘â€²1succeedssubscriptsuperscriptğ‘â€²1subscriptsuperscriptğ‘â€²2succeedssubscriptğ‘1succeedssubscriptğ‘2\displaystyle b^{\prime}_{1}\colon a^{\prime}_{1}\succ a^{\prime}_{2}\succ a_{% 1}\succ a_{2}italic_b start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT : italic_a start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT â‰» italic_a start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT â‰» italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT â‰» italic_a start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT a2â€²:b1â€²â‰»b2â€²â‰»b1â‰»b2:subscriptsuperscriptğ‘â€²2succeedssubscriptsuperscriptğ‘â€²1subscriptsuperscriptğ‘â€²2succeedssubscriptğ‘1succeedssubscriptğ‘2\displaystyle a^{\prime}_{2}\colon b^{\prime}_{1}\succ b^{\prime}_{2}\succ b_{% 1}\succ b_{2}\qquad\qquaditalic_a start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT : italic_b start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT â‰» italic_b start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT â‰» italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT â‰» italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT b2â€²:a1â€²â‰»a2â€²â‰»a1â‰»a2:subscriptsuperscriptğ‘â€²2succeedssubscriptsuperscriptğ‘â€²1subscriptsuperscriptğ‘â€²2succeedssubscriptğ‘1succeedssubscriptğ‘2\displaystyle b^{\prime}_{2}\colon a^{\prime}_{1}\succ a^{\prime}_{2}\succ a_{% 1}\succ a_{2}italic_b start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT : italic_a start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT â‰» italic_a start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT â‰» italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT â‰» italic_a start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT Observe that Sâ€²={(a1,b1),(a2,b2),(a1â€²,b1â€²),(a2â€²,b2â€²)}superscriptğ‘†â€²subscriptğ‘1subscriptğ‘1subscriptğ‘2subscriptğ‘2subscriptsuperscriptğ‘â€²1subscriptsuperscriptğ‘â€²1subscriptsuperscriptğ‘â€²2subscriptsuperscriptğ‘â€²2S^{\prime}=\{(a_{1},b_{1}),(a_{2},b_{2}),(a^{\prime}_{1},b^{\prime}_{1}),(a^{% \prime}_{2},b^{\prime}_{2})\}italic_S start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT = { ( italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , ( italic_a start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) , ( italic_a start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_b start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , ( italic_a start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_b start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) } is a stable matching in Gâ€²superscriptğºâ€²G^{\prime}italic_G start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT. Hence it is popular [18], so Sâ€²superscriptğ‘†â€²S^{\prime}italic_S start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT is a popular perfect matching in Gâ€²superscriptğºâ€²G^{\prime}italic_G start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT. However the matching {(a,b),(aâ€²,bâ€²)}ğ‘ğ‘superscriptğ‘â€²superscriptğ‘â€²\{(a,b),(a^{\prime},b^{\prime})\}{ ( italic_a , italic_b ) , ( italic_a start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT , italic_b start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) } obtained by replacing edges in Gâ€²superscriptğºâ€²G^{\prime}italic_G start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT with original edges is not perfect in GğºGitalic_G. Note that we are not allowed to have two copies of any edge here, hence both (a,b)ğ‘ğ‘(a,b)( italic_a , italic_b ) and (aâ€²,bâ€²)superscriptğ‘â€²superscriptğ‘â€²(a^{\prime},b^{\prime})( italic_a start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT , italic_b start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) are present with multiplicity 1. Thus popular perfect matchings in GğºGitalic_G and Gâ€²superscriptğºâ€²G^{\prime}italic_G start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT are quite different from each other. Our approach. We need a different approach to solve the min-cost popular perfect matching problem in the many-to-many setting. Our approach is to reduce the min-cost popular perfect matching problem in G=(AâˆªB,E)ğºğ´ğµğ¸G=(A\cup B,E)italic_G = ( italic_A âˆª italic_B , italic_E ) to the min-cost stable matching problem in an auxiliary many-to-many instance Gâˆ—=(AâˆªB,Eâˆ—)superscriptğºğ´ğµsuperscriptğ¸G^{*}=(A\cup B,E^{*})italic_G start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT = ( italic_A âˆª italic_B , italic_E start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ). This instance Gâˆ—superscriptğºG^{*}italic_G start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT is a multigraph, i.e., parallel edges are present in Gâˆ—superscriptğºG^{*}italic_G start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT. A min-cost stable matching in a many-to-many instance can be computed in polynomial time [15, 20]. Furthermore, Fleinerâ€™s algorithm [15] works even in the case when the input instance is a multigraph. Hence a min-cost popular perfect matching in the original many-to-many instance GğºGitalic_G can be computed in polynomial time. Thus we show the following result. {reptheorem} restate_thm:many-many Let G=(AâˆªB,E)ğºğ´ğµğ¸G=(A\cup B,E)italic_G = ( italic_A âˆª italic_B , italic_E ) be a many-to-many matching instance where vertices have strict preferences (possibly incomplete) and there is a function ğ–¼ğ—ˆğ—Œğ—:Eâ†’â„:ğ–¼ğ—ˆğ—Œğ—â†’ğ¸â„\mathsf{cost}:E\rightarrow\mathbb{R}sansserif_cost : italic_E â†’ blackboard_R. If GğºGitalic_G admits a perfect matching then a min-cost popular perfect matching in GğºGitalic_G can be computed in polynomial time. 1.1 Background and Related Results The notion of popularity was proposed by GÃ¤rdenfors [18] in 1975 in the stable marriage problem (i.e., the one-to-one setting) where he observed that stable matchings are popular. It was shown in [4, 11] that when preferences include ties (even one-sided ties), it is NP-hard to decide if a popular matching exists or not. It was shown in [21] that every stable matching in a marriage instance is a min-size popular matching. Polynomial-time algorithms to find a max-size popular matching were shown in [21, 27]. We refer to [10] for a survey on results in popular matchings in the one-to-one setting. The notion of popularity was extended from one-to-one setting to many-to-many setting in [7] and [34], independently. A polynomial-time algorithm to compute a max-size popular matching in the many-to-many setting was given in [7]. It was also shown in [7] that every stable matching in the many-to-many setting is popular; so though a rather strong definition of popularity was adopted here, popular matchings always exist. The definition of popularity considered in [34] is weaker than the one in [7]; in order to compare a pair of matchings Mğ‘€Mitalic_M and Nğ‘Nitalic_N, every vertex vğ‘£vitalic_v uses the bijection that compares the top neighbor in Mâ¢(v)âˆ–Nâ¢(v)ğ‘€ğ‘£ğ‘ğ‘£M(v)\setminus N(v)italic_M ( italic_v ) âˆ– italic_N ( italic_v ) with the top neighbor in Nâ¢(v)âˆ–Mâ¢(v)ğ‘ğ‘£ğ‘€ğ‘£N(v)\setminus M(v)italic_N ( italic_v ) âˆ– italic_M ( italic_v ), and so on, i.e., the permutation Ïƒğœ\sigmaitalic_Ïƒ in Eq. (1) is the identity permutation. The max-size popular matching problem with matroid constraints (this model generalizes popular many-to-many matchings) was considered in [9, 26] and shown to be tractable. Strongly popular matchings (such a matching defeats every other matching) in many-to-many instances were studied in [31]. The stable matching problem has been extensively studied in the hospitals/residents and the many-to-many settings [2, 6, 15, 20, 19, 23, 24, 36, 38] and a min-cost stable matching in the one-to-one setting (and thus in the hospitals/residents setting) can be computed in polynomial time [37, 39]. Note that min-cost stable matching algorithms in the one-to-one setting do not generalize to the many-to-many setting. This is because (unlike the hospitals/residents setting) the many-to-many setting cannot be reduced to the one-to-one setting via cloning [20, Footnote 6]. The algorithms in [15, 20] solve the min-cost stable matching problem in the many-to-many setting. Fleinerâ€™s algorithm [15] solves the min-cost matroid kernel problem in the intersection of two strictly ordered matroids. This generalizes the min-cost stable matching problem in the many-to-many setting. Huangâ€™s algorithm [20] solves the min-cost classified stable matching problem when each vertex on one side of the graph has classifications with upper bounds on the number of partners it can have in each class. Note that this problem generalizes the min-cost stable matching problem in the many-to-many setting. The min-cost classified stable matching problem when vertices on both sides of the graph have classifications was solved in [16]. There is a polynomial-time algorithm to find a min-cost popular maximum matching in the one-to-one setting [28]. In contrast to this, finding a min-cost popular matching in the one-to-one setting is NP-hard [14]. Nevertheless, when preferences are complete, there is a polynomial-time algorithm to find a min-cost popular matching in the one-to-one setting [12]. This result was recently extended to the hospitals/residents setting in [30]; note that this is a non-trivial extension as the set of popular matchings in a hospitals/residents instance can be richer than those in the corresponding one-to-one instance. The main result in the conference version of our paper [29] (now included in [30]) showed that in contrast to popular matchings, the set of popular perfect matchings in a hospitals/residents instance is not richer than the set of popular perfect matchings in the corresponding one-to-one instance. That is, each popular perfect matching in a hospitals/residents instance can be realized as a popular perfect matching in the corresponding one-to-one instance. As seen in our example earlier, this is not the case for popular perfect matchings in the many-to-many setting. Interestingly, the tractability of a matching problem in the one-to-one setting does not always imply its tractability in the hospitals/residents setting. One such problem is that of finding a matching that maximizes Nash social welfare (i.e., the geometric mean of edge utilities). Such a matching can be easily found by the maximum weight matching algorithm in the one-to-one setting,444For any edge (a,b)ğ‘ğ‘(a,b)( italic_a , italic_b ), the weight of this edge is log\logroman_log of the product of utilities that ağ‘aitalic_a and bğ‘bitalic_b have for each other. however it is NP-hard to find such a matching in the hospitals/residents setting [25]. 1.2 Techniques We will use the characterization of popular perfect matchings in the one-to-one setting. It was shown in [27] that in order to find a popular maximum matching in a given one-to-one instance, it suffices to run the Gale-Shapley algorithm in an appropriate auxiliary instance (a multigraph). It was shown in [28] that this mapping (essentially, a projection) from the set of stable matchings in this auxiliary instance to the set of popular maximum matchings in the original instance is surjective. Thus for every popular maximum matching in the original instance, there exists a corresponding stable matching in the auxiliary instance. Hence in the one-to-one setting, the min-cost stable matching algorithm in the auxiliary instance solves the min-cost popular maximum matching problem in the original instance. Our problem is to find a min-cost popular perfect matching in a many-to-many instance GğºGitalic_G. We will show a many-to-many instance Gâˆ—superscriptğºG^{*}italic_G start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT (as before, a multigraph) such that there is a surjective mapping from the set of stable matchings in Gâˆ—superscriptğºG^{*}italic_G start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT to the set of popular perfect matchings in GğºGitalic_G. Let Mğ‘€Mitalic_M be any perfect matching in GğºGitalic_G. We will show in Section 2 that Mğ‘€Mitalic_M is a popular perfect matching in GğºGitalic_G if and only if there is a realization Mâ€²superscriptğ‘€â€²M^{\prime}italic_M start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT of Mğ‘€Mitalic_M such that Mâ€²superscriptğ‘€â€²M^{\prime}italic_M start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT is a popular perfect matching in an appropriate one-to-one instanceâ€”this instance is a subgraph of the one-to-one cloned instance Gâ€²superscriptğºâ€²G^{\prime}italic_G start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT described earlier. This subgraph depends on the matching Mâ€²superscriptğ‘€â€²M^{\prime}italic_M start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT, so let us call it GMâ€²â€²subscriptsuperscriptğºâ€²superscriptğ‘€â€²G^{\prime}_{M^{\prime}}italic_G start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_M start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT end_POSTSUBSCRIPT. Thus this reduction from the many-to-many setting to the one-to-one setting does not seem particularly helpful since it depends on the matching Mğ‘€Mitalic_M that we seek. We will overcome the above bottleneck in Section 3 by using the result for one-to-one instances in [28] to construct a one-to-one multigraph GM0subscriptsuperscriptğº0ğ‘€G^{0}_{M}italic_G start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT such that Mâ€²superscriptğ‘€â€²M^{\prime}italic_M start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT can be realized as a stable matching in GM0subscriptsuperscriptğº0ğ‘€G^{0}_{M}italic_G start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT. We will then use this to show a many-to-many multigraph Gâˆ—superscriptğºG^{*}italic_G start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT such that Mğ‘€Mitalic_M can be realized as a stable matching Mâˆ—superscriptğ‘€M^{*}italic_M start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT in the many-to-many instance Gâˆ—superscriptğºG^{*}italic_G start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT. Conversely, every stable matching in the instance Gâˆ—superscriptğºG^{*}italic_G start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT projects to a popular perfect matching in GğºGitalic_G. Thus finding a min-cost stable matching in Gâˆ—superscriptğºG^{*}italic_G start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT (by Fleinerâ€™s algorithm [15]) solves the min-cost popular perfect matching problem in GğºGitalic_G."
https://arxiv.org/html/2411.00255v1,Dynamic Accountable Storage: An Efficient Protocol for Real-time Cloud Storage Auditing,"Ateniese, Goodrich, Lekakis, Papamanthou, Paraskevas, and Tamassia introduced the Accountable Storage protocol, which is a way for a client to outsource their data to a cloud storage provider while allowing the client to periodically perform accountability challenges. An accountability challenge efficiently recovers any pieces of data the server has lost or corrupted, allowing the client to extract the original copies of the damaged or lost data objects. A severe limitation of the prior accountable storage scheme of Ateniese et al., however, is that it is not fully dynamic. That is, it does not allow a client to freely insert and delete data from the outsourced data set after initializing the protocol, giving the protocol limited practical use in the real world. In this paper, we present Dynamic Accountable Storage, which is an efficient way for a client to periodically audit their cloud storage while also supporting insert and delete operations on the data set. To do so, we introduce a data structure, the IBLT tree, which allows either the server or the client to reconstruct data the server has lost or corrupted in a space-efficient way.","Cloud storage providers often advertise the number of â€œninesâ€ of durability they achieve, such as the â€œ11 ninesâ€ in the durability probability of 99.999999999% advertised by Amazon S3 for not losing a given data object in a given year,111See, e.g., https://aws.amazon.com/s3/storage-classes/. which implies an expected loss of at most one object out of 100 billion per year. Such statements might seem at first to imply that cloud storage data loss is impossible, until one considers that there are hundreds of trillions of objects currently being stored in Amazon S3.222E.g., see https://aws.amazon.com/blogs/aws/welcome-to-aws-pi-day-2022/. Moreover, such durability statements do not address data corruption or data loss caused by misuse or misconfiguration, e.g., see [23, 25, 27]. Such durability statements also beg the question of how to determine whether one or more of a clientâ€™s data objects has been lost or corrupted. For example, when data is lost, a client or server may not even realize it and may also have no ability to recover from the damage. We propose an efficient way for a client and/or server to audit and recover client data. Our work is an extension of the Accountable Storage protocol first described by Ateniese et al. [4]. In this scheme, a client, Alice, outsources her data to a cloud storage provider, Bob, and she then stores a small sketch representation of her data set along with some metadata in a data structure called an Invertible Bloom Lookup Table (IBLT) [17, 20]. At any time, the client can issue an accountability challenge, requiring that the storage provider, Bob, send the client a similar encoding of the data set representing everything that has not been lost. The client can then compare the two IBLTs to peel out any data blocks that were lost, assess how much the blocks deviate from their original versions, and demand compensation accordingly. Furthermore, Ateniese et al. claim that in their scheme the server is forced to acknowledge and pay for lost data. However, we show that the server can easily thwart their scheme. Another caveat in the protocol by Ateniese et al. is that their scheme is static, i.e., it does not support insert or delete operations. This severely limits the usefulness of the scheme in the real world. Our work fixes this limitation; hence, we call our scheme â€œDynamic Accountable Storage.â€ Furthermore, our scheme does not rely on forcing the server to pay for lost data or store large amounts of metadata, as in the original (static) Accountable Storage scheme [4]. Instead, our protocol provides the server with an efficient low-overhead way to recover lost data for the client. 1.1 Other Related Work There are many existing schemes that allow a client to verify that a cloud storage provider is keeping their data intact, but not with the same levels of efficiency or dynamism as our scheme. For example, Provable Data Possession (PDP) schemes [2, 3, 16, 19, 31, 21] and Proofs of Retrievability (POR) schemes [10, 22, 28, 29, 1] use versions of cryptographic tags to verify that data is maintained correctly. A common technique used in these schemes is homomorphic tags [2, 5], which enable batch verification and reduces communication complexity from linear to constant in the size of the clientâ€™s data. Verifiable Database schemes [6, 11, 13, 14, 26, 12] do something similar, except use cryptographic commitment primitives rather than tags. Unfortunately, dynamic PDP or POR schemes, such as the one by Erway, Kupcu, Papamanthou, and Tamassia [19], stop at answering whether the clientâ€™s data has been corrupted by a cloud service provider. In contrast, our protocol goes further, by recovering the lost data. Other work, such as that by Ateniese, Di Pietro, Mancini, and Tsudik [2] or Shacham and Waters [28], implements a type of accountability challenge with dynamic data, but their schemes require the client to run Oâ¢(n)ğ‘‚ğ‘›O(n)italic_O ( italic_n ) expensive cryptographic operations per update, where nğ‘›nitalic_n is the size of the database. Jin et al. [21] solved this recomputation problem at the expense of having the client store Oâ¢(n)ğ‘‚ğ‘›O(n)italic_O ( italic_n ) extra metadata, somewhat defeating the purpose of the client outsourcing their data. 1.2 Our Contributions In this paper, we describe a scheme for Dynamic Accountable Storage, which supports insertions, deletions, and data recovery such that both the client and server have low overheads in terms of additional storage requirements and running times required to update metadata. For example, given a parameter, Î´ğ›¿\deltaitalic_Î´, for the number of blocks our scheme can efficiently tolerate being lost or corrupted, our scheme requires only oâ¢(n)ğ‘œğ‘›o(n)italic_o ( italic_n ) additional space at the server rather than the Î˜â¢(nâ¢Î´)Î˜ğ‘›ğ›¿\Theta(n\delta)roman_Î˜ ( italic_n italic_Î´ ) space required by the original Accountable Storage scheme [4]. Another difference of our approach from prior approaches is that our scheme does not assume the server is malicious, since any reputable cloud service company has an incentive to help the client maintain her data. As a result, our work investigates efficient ways for the server to maintain the clientâ€™s data with added reliability and to be able to detect and repair corruptions on the clientâ€™s behalf, assuming that the server is honest-but-curious; hence, we provide a way for the client, Alice, to encrypt both her keys and their values while still allowing her to outsource her data with low overheads for both the client and the server. We wish to emphasize that an honest-but-curious server does not preclude the need for accountability mechanisms. As described above, any cloud storage system of a sufficient size will inevitably produce errors. Even if the server is not maliciously corrupting data, identify errant data objects among billions of others is a non-trivial problem. In addition to cloud storage verification, schemes like ours have applications in version control systems [19], verifiably-secure logging [15, 30], and public data auditing [21, 24, 4]. We have two main contributions in this work. The first is to show how the the server in the original Accountable Storage protocol of Ateniese et al. [4] can thwart their original scheme so as to recover lost data without paying the client or revealing that there has been a data loss. Indeed, we see this as a feature, not a bug, and we build our scheme on the assumption that the honest-but-curious server, Bob, is motivated to recover the clientâ€™s data whenever this is possible. The second contribution of our work is that we describe an extension to the Accountable Storage protocol [4] that can support insertions and deletions of key-value pairs. To allow the server to efficiently detect and repair corruptions in the clientâ€™s data, we introduce a new data structure that is maintained at the server, which we call an IBLT tree. This data structure takes oâ¢(n)ğ‘œğ‘›o(n)italic_o ( italic_n ) extra space at the server, compared to the Oâ¢(n)ğ‘‚ğ‘›O(n)italic_O ( italic_n ) extra metadata required by the original Accountable Storage protocol. Although the server must already store nğ‘›nitalic_n data blocks for the client, the space savings across a large number of clients are significant. The remainder of the paper is organized as follows. In Section 2, we briefly review the original Accountable Storage protocol of Ateniese et al. [4], showing how the server can thwart the requirement to pay for lost data. In Section 3, we describe our new data structure, the IBLT tree. Lastly, in Section 4, we provide an overview of our protocol and a formal construction with an efficiency analysis."
https://arxiv.org/html/2411.00082v1,Testing and learning structured quantum Hamiltonians,"We consider the problems of testing and learning an nğ‘›nitalic_n-qubit Hamiltonian H=âˆ‘xÎ»xâ¢Ïƒxğ»subscriptğ‘¥subscriptğœ†ğ‘¥subscriptğœğ‘¥H=\sum_{x}\lambda_{x}\sigma_{x}italic_H = âˆ‘ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT italic_Î» start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT italic_Ïƒ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT expressed in its Pauli basis, from queries to its evolution operator U=eâˆ’iâ¢Hâ¢tğ‘ˆsuperscriptğ‘’ğ‘–ğ»ğ‘¡U=e^{-iHt}italic_U = italic_e start_POSTSUPERSCRIPT - italic_i italic_H italic_t end_POSTSUPERSCRIPT with respect the normalized Frobenius norm. To this end, we prove the following results (with and without quantum memory) for Hamiltonians whose Pauli spectrum involves only kğ‘˜kitalic_k-local terms or has sparsity at most sğ‘ sitalic_s:Local Hamiltonians: We give a tolerant testing protocol to decide if a Hamiltonian is Îµ1subscriptğœ€1\varepsilon_{1}italic_Îµ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-close to kğ‘˜kitalic_k-local or Îµ2subscriptğœ€2\varepsilon_{2}italic_Îµ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT-far from kğ‘˜kitalic_k-local, with Oâ¢(1/(Îµ2âˆ’Îµ1)4)ğ‘‚1superscriptsubscriptğœ€2subscriptğœ€14O(1/(\varepsilon_{2}-\varepsilon_{1})^{4})italic_O ( 1 / ( italic_Îµ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_Îµ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT ) queries, thereby solving two open questions posed in a recent work by Bluhm, Caro and Oufkir [5]. For learning a kğ‘˜kitalic_k-local Hamiltonian up to error Îµğœ€\varepsilonitalic_Îµ, we give a protocol with query complexity and total time evolution expâ¡(Oâ¢(k2+kâ¢logâ¡(1/Îµ)))ğ‘‚superscriptğ‘˜2ğ‘˜1ğœ€\exp(O(k^{2}+k\log(1/\varepsilon)))roman_exp ( italic_O ( italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_k roman_log ( 1 / italic_Îµ ) ) ). Our algorithm leverages the non-commutative Bohnenblust-Hille inequality in order to get a complexity independent of nğ‘›nitalic_n.Sparse Hamiltonians: We give a protocol for testing whether a Hamiltonian is Îµ1subscriptğœ€1\varepsilon_{1}italic_Îµ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-close to being sğ‘ sitalic_s-sparse or Îµ2subscriptğœ€2\varepsilon_{2}italic_Îµ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT-far from being sğ‘ sitalic_s-sparse, with Oâ¢(s6/(Îµ22âˆ’Îµ12)6)ğ‘‚superscriptğ‘ 6superscriptsuperscriptsubscriptğœ€22superscriptsubscriptğœ€126O(s^{6}/(\varepsilon_{2}^{2}-\varepsilon_{1}^{2})^{6})italic_O ( italic_s start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT / ( italic_Îµ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - italic_Îµ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT ) queries. For learning up to error Îµğœ€\varepsilonitalic_Îµ, we show that Oâ¢(s4/Îµ8)ğ‘‚superscriptğ‘ 4superscriptğœ€8O(s^{4}/\varepsilon^{8})italic_O ( italic_s start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT / italic_Îµ start_POSTSUPERSCRIPT 8 end_POSTSUPERSCRIPT ) queries suffices.Learning without quantum memory: The learning results stated above have no dependence on the system size nğ‘›nitalic_n, but require nğ‘›nitalic_n-qubit quantum memory. We give subroutines that allow us to reproduce all the above learning results without quantum memory; increasing the query complexity by a (logâ¡n)ğ‘›(\log n)( roman_log italic_n )-factor in the local case and an nğ‘›nitalic_n-factor in the sparse case.Testing without quantum memory: We give a new subroutine called Pauli hashing, which allows one to tolerantly test sğ‘ sitalic_s-sparse Hamiltonians using O~â¢(s14/(Îµ22âˆ’Îµ12)18)~ğ‘‚superscriptğ‘ 14superscriptsuperscriptsubscriptğœ€22superscriptsubscriptğœ€1218\tilde{O}(s^{14}/(\varepsilon_{2}^{2}-\varepsilon_{1}^{2})^{18})over~ start_ARG italic_O end_ARG ( italic_s start_POSTSUPERSCRIPT 14 end_POSTSUPERSCRIPT / ( italic_Îµ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - italic_Îµ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT 18 end_POSTSUPERSCRIPT ) query complexity. A key ingredient is showing that sğ‘ sitalic_s-sparse Pauli channels can be tested in a tolerant fashion as being Îµ1subscriptğœ€1\varepsilon_{1}italic_Îµ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-close to being sğ‘ sitalic_s-sparse or Îµ2subscriptğœ€2\varepsilon_{2}italic_Îµ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT-far under the diamond norm, using O~â¢(s2/(Îµ2âˆ’Îµ1)6)~ğ‘‚superscriptğ‘ 2superscriptsubscriptğœ€2subscriptğœ€16\tilde{O}(s^{2}/(\varepsilon_{2}-\varepsilon_{1})^{6})over~ start_ARG italic_O end_ARG ( italic_s start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT / ( italic_Îµ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_Îµ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT ) queries via Pauli hashing.In order to prove these results, we prove new structural theorems for local Hamiltonians, sparse Pauli channels and sparse Hamiltonians. We complement our learning algorithms with lower bounds that are polynomially weaker. Furthermore, our algorithms use short time evolutions and do not assume prior knowledge of the terms on which the Pauli spectrum is supported on, i.e., we do not require prior knowledge about the support of the Hamiltonian terms.","A fundamental and important challenge with building quantum devices is being able to characterize and calibrate its behavior. One approach to do so is Hamiltonian learning which seeks to learn the Hamiltonian governing the dynamics of a quantum system given finite classical and quantum resources. Beyond system characterization, it is also carried out during validation of physical systems and designing control strategies for implementing quantum gates [39]. However, learning an nğ‘›nitalic_n-qubit Hamiltonian is known to be difficult, requiring complexity that scales exponential in the number of qubits [12]. In practice, however, prior knowledge on the structure of Hamiltonians is available e.g., those of engineered quantum devices [53] where the underlying Hamiltonians primarily involve local interactions with few non-local interactions, and even naturally occurring physical quantum systems such as those with translationally invariant Hamiltonians. To highlight these structural properties, consider an nğ‘›nitalic_n-qubit Hamiltonian Hğ»Hitalic_H (which is a self-adjoint operator acting on (â„‚2)âŠ—nsuperscriptsuperscriptâ„‚2tensor-productabsentğ‘›(\mathbb{C}^{2})^{\otimes n}( blackboard_C start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT âŠ— italic_n end_POSTSUPERSCRIPT) expanded in terms of the nğ‘›nitalic_n-qubit Pauli operators: H=âˆ‘xâˆˆ{0,1}2â¢nÎ»xâ¢Ïƒx,ğ»subscriptğ‘¥superscript012ğ‘›subscriptğœ†ğ‘¥subscriptğœğ‘¥H=\sum_{x\in\{0,1\}^{2n}}\lambda_{x}\sigma_{x},italic_H = âˆ‘ start_POSTSUBSCRIPT italic_x âˆˆ { 0 , 1 } start_POSTSUPERSCRIPT 2 italic_n end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_Î» start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT italic_Ïƒ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT , where Î»xsubscriptğœ†ğ‘¥\lambda_{x}italic_Î» start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT are real-valued coefficients (also called interaction strengths) of the Pauli operators Ïƒxsubscriptğœğ‘¥\sigma_{x}italic_Ïƒ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT denoted by the string x=(a,b)âˆˆğ”½2nğ‘¥ğ‘ğ‘superscriptsubscriptğ”½2ğ‘›x=(a,b)\in\mathbb{F}_{2}^{n}italic_x = ( italic_a , italic_b ) âˆˆ blackboard_F start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT with Ïƒ(a,b)=iaâ‹…bâŠ—i=1nXaiâ¢Zbisubscriptğœğ‘ğ‘superscriptsubscripttensor-productğ‘–1ğ‘›superscriptğ‘–â‹…ğ‘ğ‘superscriptğ‘‹subscriptğ‘ğ‘–superscriptğ‘subscriptğ‘ğ‘–\sigma_{(a,b)}=i^{a\cdot b}\otimes_{i=1}^{n}X^{a_{i}}Z^{b_{i}}italic_Ïƒ start_POSTSUBSCRIPT ( italic_a , italic_b ) end_POSTSUBSCRIPT = italic_i start_POSTSUPERSCRIPT italic_a â‹… italic_b end_POSTSUPERSCRIPT âŠ— start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_X start_POSTSUPERSCRIPT italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT italic_Z start_POSTSUPERSCRIPT italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT. We call the set of Paulis with non-zero coefficients Î»xsubscriptğœ†ğ‘¥\lambda_{x}italic_Î» start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT as the Pauli spectrum of the Hamiltonian denoted by ğ’®={xâˆˆ{0,1}2â¢n:Î»xâ‰ 0}ğ’®conditional-setğ‘¥superscript012ğ‘›subscriptğœ†ğ‘¥0\mathcal{S}=\{x\in\{0,1\}^{2n}:\ \lambda_{x}\neq 0\}caligraphic_S = { italic_x âˆˆ { 0 , 1 } start_POSTSUPERSCRIPT 2 italic_n end_POSTSUPERSCRIPT : italic_Î» start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT â‰  0 }. Of particular relevance are kğ‘˜kitalic_k-local Hamiltonians which involve Pauli operators that act non-trivially on all but at most kğ‘˜kitalic_k qubits and sğ‘ sitalic_s-sparse Hamiltonians whose Pauli expansion contains at most sğ‘ sitalic_s non-zero Pauli operators i.e., |ğ’®|â‰¤sğ’®ğ‘ |\mathcal{S}|\leq s| caligraphic_S | â‰¤ italic_s. There has thus been a growing suite of Hamiltonian learning results that have shown that when the underlying nğ‘›nitalic_n-qubit Hamiltonian Hğ»Hitalic_H satisfies these structural properties, learning can be performed with only polyâ¡(n)polyğ‘›\operatorname{poly}(n)roman_poly ( italic_n ) query complexity, either by making â€œqueriesâ€ to the unitary evolution operator Uâ¢(t)=expâ¡(âˆ’iâ¢Hâ¢t)ğ‘ˆğ‘¡ğ‘–ğ»ğ‘¡U(t)=\exp(-iHt)italic_U ( italic_t ) = roman_exp ( - italic_i italic_H italic_t ) [23, 34, 62, 36, 59, 21, 38, 40, 52, 28], or by assuming one has access to Gibbs state [1, 36, 51, 48, 8, 28]. Notably, [9] considered the problem of learning Hamiltonians that are both local and sparse, without prior knowledge of the support. Several of the learning algorithms mentioned above however require assumptions on the support of the Hamiltonian beyond locality or sparsity, such as [38] which considers geometrically-local Hamiltonians (a subset of local Hamiltonians) and [59] which requires assumptions on the support. Moreover, before learning, it might be desirable to uncover what is the structure of an unknown Hamiltonian in order to choose specialized learning algorithms. Even deciding if a Hamiltonian has a particular structure is a fundamental challenge and constitutes the problem of testing if an unknown Hamiltonian satisfies a certain structural property. As far as we know, this line of investigation is nascent with only a few works on Hamiltonian property testing [54, 2, 41] with Blum et al. [6] having considered the problem of testing local Hamiltonians and the problem of testing sparse Hamiltonians yet to be tackled. This leads us to the motivating question of our work: What is the query complexity of learning and testing structured Hamiltonians? 1.1 Problem statement Before we state our results answering the question above, we clearly mention our learning and testing problems first. If Hğ»Hitalic_H is the Hamiltonian describing the dynamics of a certain physical system, then the state of that system evolves according to the time evolution operator Uâ¢(t)=eâˆ’iâ¢Hâ¢tğ‘ˆğ‘¡superscriptğ‘’ğ‘–ğ»ğ‘¡U(t)=e^{-iHt}italic_U ( italic_t ) = italic_e start_POSTSUPERSCRIPT - italic_i italic_H italic_t end_POSTSUPERSCRIPT. This means that if Ïâ¢(0)ğœŒ0\rho(0)italic_Ï ( 0 ) is the state at time 00, at time tğ‘¡titalic_t the state would have evolved to Ïâ¢(t)=Uâ¢(t)â¢Ïâ¢(0)â¢Uâ€ â¢(t)ğœŒğ‘¡ğ‘ˆğ‘¡ğœŒ0superscriptğ‘ˆâ€ ğ‘¡\rho(t)=U(t)\rho(0)U^{\dagger}(t)italic_Ï ( italic_t ) = italic_U ( italic_t ) italic_Ï ( 0 ) italic_U start_POSTSUPERSCRIPT â€  end_POSTSUPERSCRIPT ( italic_t ). Hence, to test and learn a Hamiltonian one can do the following: prepare a desired state, apply Uâ¢(t)ğ‘ˆğ‘¡U(t)italic_U ( italic_t ) or tensor products of Uâ¢(t)ğ‘ˆğ‘¡U(t)italic_U ( italic_t ) with identity to the state, and finally measure in a chosen basis. From here onwards, this is what we mean by querying the unitary Uâ¢(t)ğ‘ˆğ‘¡U(t)italic_U ( italic_t ). It is usual to impose the normalization condition âˆ¥Hâˆ¥âˆâ‰¤1subscriptdelimited-âˆ¥âˆ¥ğ»1\lVert H\rVert_{\infty}\leq 1âˆ¥ italic_H âˆ¥ start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT â‰¤ 1 (i.e., that the eigenvalues of Hğ»Hitalic_H are bounded in absolute value by 1111), as otherwise the complexities scale with the norm of the Hamiltonian. Throughout this paper, we will consider the normalized Frobenius norm as the distance between Hamiltonians, namely dâ¢(H,Hâ€²)=âˆ¥Hâˆ’Hâ€²âˆ¥2=Trâ¡[(Hâˆ’Hâ€²)2]2n,ğ‘‘ğ»superscriptğ»â€²subscriptdelimited-âˆ¥âˆ¥ğ»superscriptğ»â€²2Trsuperscriptğ»superscriptğ»â€²2superscript2ğ‘›d(H,H^{\prime})=\lVert H-H^{\prime}\rVert_{2}=\sqrt{\frac{\operatorname{Tr}[(H% -H^{\prime})^{2}]}{2^{n}}},italic_d ( italic_H , italic_H start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) = âˆ¥ italic_H - italic_H start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT âˆ¥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = square-root start_ARG divide start_ARG roman_Tr [ ( italic_H - italic_H start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] end_ARG start_ARG 2 start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_ARG end_ARG , which equals the 2-norm of the Pauli spectrum, dâ¢(H,Hâ€²)=âˆ‘|Î»xâˆ’Î»xâ€²|2ğ‘‘ğ»superscriptğ»â€²superscriptsubscriptğœ†ğ‘¥subscriptsuperscriptğœ†â€²ğ‘¥2d(H,H^{\prime})=\sqrt{\sum|\lambda_{x}-\lambda^{\prime}_{x}|^{2}}italic_d ( italic_H , italic_H start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) = square-root start_ARG âˆ‘ | italic_Î» start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT - italic_Î» start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG. A property of a Hamiltonian, denoted â„‹â„‹\mathcal{H}caligraphic_H is a class of Hamiltonians that satisfy the property (here we will be interested in sparse and local properties). We say that Hğ»Hitalic_H is Îµğœ€\varepsilonitalic_Îµ-far from having a property â„‹â„‹\mathcal{H}caligraphic_H if dâ¢(H,Hâ€²)>Îµğ‘‘ğ»superscriptğ»â€²ğœ€d(H,H^{\prime})>\varepsilonitalic_d ( italic_H , italic_H start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) > italic_Îµ for every Hâ€²âˆˆâ„‹superscriptğ»â€²â„‹H^{\prime}\in\mathcal{H}italic_H start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT âˆˆ caligraphic_H, and otherwise is Îµğœ€\varepsilonitalic_Îµ-close. Now, we are ready to state the testing and learning problems. Let â„‹â„‹\mathcal{H}caligraphic_H be a property and let Hğ»Hitalic_H be an unknown Hamiltonian with âˆ¥Hâˆ¥âˆâ‰¤1subscriptdelimited-âˆ¥âˆ¥ğ»1\lVert H\rVert_{\infty}\leq 1âˆ¥ italic_H âˆ¥ start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT â‰¤ 1 and Trâ¡[H]=0Trğ»0\operatorname{Tr}[H]=0roman_Tr [ italic_H ] = 0. Problem 1.1 (Tolerant testing). Promised Hğ»Hitalic_H is either Îµ1subscriptğœ€1\varepsilon_{1}italic_Îµ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-close or Îµ2subscriptğœ€2\varepsilon_{2}italic_Îµ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT-far from satisfying property â„‹â„‹\mathcal{H}caligraphic_H, decide which is the case by making queries to Uâ¢(t)ğ‘ˆğ‘¡U(t)italic_U ( italic_t ). Problem 1.2 (Hamiltonian learning). Promised Hâˆˆâ„‹ğ»â„‹H\in\mathcal{H}italic_H âˆˆ caligraphic_H, output a classical description of H~âˆˆâ„‹~ğ»â„‹\widetilde{H}\in\mathcal{H}over~ start_ARG italic_H end_ARG âˆˆ caligraphic_H such that â€–Hâˆ’H~â€–2â‰¤Îµsubscriptnormğ»~ğ»2ğœ€\|H-\widetilde{H}\|_{2}\leq\varepsilonâˆ¥ italic_H - over~ start_ARG italic_H end_ARG âˆ¥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT â‰¤ italic_Îµ by making queries to Uâ¢(t)ğ‘ˆğ‘¡U(t)italic_U ( italic_t ). 1.2 Summary of results The main results of this submission are query-efficient algorithms for testing and learning Hamiltonians that are local and/or sparse. We can reproduce these results without using quantum memory by increasing the number of queries. We summarize our results in the table below (for simplicity we state our results for constant accuracy). Testing Learning with memory w/o memory with memory w/o memory sğ‘ sitalic_s-sparse polyâ¡(s)polyğ‘ \operatorname{poly}(s)roman_poly ( italic_s ) polyâ¡(s)polyğ‘ \operatorname{poly}(s)roman_poly ( italic_s ) polyâ¡(s)polyğ‘ \operatorname{poly}(s)roman_poly ( italic_s ) nâ‹…polyâ¡(s)â‹…ğ‘›polyğ‘ n\cdot\operatorname{poly}(s)italic_n â‹… roman_poly ( italic_s ) kğ‘˜kitalic_k-local Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 ) Oâ¢(1)ğ‘‚1O(1)italic_O ( 1 ) [6] expâ¡(k2)superscriptğ‘˜2\exp(k^{2})roman_exp ( italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) (logâ¡n)â‹…expâ¡(k2)â‹…ğ‘›superscriptğ‘˜2(\log n)\cdot\exp(k^{2})( roman_log italic_n ) â‹… roman_exp ( italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) kğ‘˜kitalic_k-local & sğ‘ sitalic_s-sparse polyâ¡(s)polyğ‘ \operatorname{poly}(s)roman_poly ( italic_s ) polyâ¡(s)polyğ‘ \operatorname{poly}(s)roman_poly ( italic_s ) minâ¡{expâ¡(k2),polyâ¡(sâ¢k)}superscriptğ‘˜2polyğ‘ ğ‘˜\min\{\exp(k^{2}),\operatorname{poly}(sk)\}roman_min { roman_exp ( italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) , roman_poly ( italic_s italic_k ) } (logâ¡n)â‹…minâ¡{expâ¡(k2),polyâ¡(sâ¢k)}â‹…ğ‘›superscriptğ‘˜2polyğ‘ ğ‘˜(\log n)\cdot\min\{\exp(k^{2}),\operatorname{poly}(sk)\}( roman_log italic_n ) â‹… roman_min { roman_exp ( italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) , roman_poly ( italic_s italic_k ) } Table 1: Query complexity for learning and testing nğ‘›nitalic_n-qubit structured Hamiltonians. Dependence on nğ‘›nitalic_n and the structural property is shown for constant accuracy. Results are indicated with quantum memory (i.e., an nğ‘›nitalic_n-qubit ancillary system is available) and without quantum memory. Before we discuss our results in more detail, we make a few remarks about our main results. (i)ğ‘–(i)( italic_i ) As far as we know, this is the first work: (a)ğ‘(a)( italic_a ) with complexities that are independent of nğ‘›nitalic_n (with memory)111We remark that there are a few works that achieve nğ‘›nitalic_n-independent complexities for learning local Hamiltonians in the âˆ\inftyâˆ-norm of the Pauli coefficients, but when transformed into 2222-norm learners they yield complexities depending on nksuperscriptğ‘›ğ‘˜n^{k}italic_n start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT., and (b)ğ‘(b)( italic_b ) that does not assume knowledge of the support.222Soon after the third-authorâ€™s work [32], Bakshi et al. [9] presented a learning algorithm that does not require prior knowledge of the support, although their result achieve Heisenberg scaling in complexity using heavy machinery. (iâ¢v)ğ‘–ğ‘£(iv)( italic_i italic_v ) We give the first learning algorithm for Hamiltonians that are only promised to be sparse, and not necessarily local. Similarly, our local Hamiltonian learning problem doesnâ€™t assume geometric locality which was assumed in several prior works. (iâ¢iâ¢i)ğ‘–ğ‘–ğ‘–(iii)( italic_i italic_i italic_i ) Our testing algorithms are tolerant, i.e., they can handle the setting where Îµ1â‰ 0subscriptğœ€10\varepsilon_{1}\neq 0italic_Îµ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT â‰  0. As far as we know, there are only a handful of polynomial-time tolerant testers for quantum objects. (iâ¢v)ğ‘–ğ‘£(iv)( italic_i italic_v ) We show that all our all the learning protocols with quantum memory can be translated to ones which require no quantum memory. In the case of learning structured Hamiltonians, we obtain a protocol with only a factor logâ¡nğ‘›\log nroman_log italic_n overhead for local Hamiltonians and a protocol with a factor nğ‘›nitalic_n overhead for sparse Hamiltonians. (v)ğ‘£(v)( italic_v ) We also give a tolerant testing algorithm for sğ‘ sitalic_s-sparse Hamiltonians that requires no quantum memory based on a new subroutine called Pauli hashing. The query complexity is Oâ¢(polyâ¡(s))ğ‘‚polyğ‘ O(\operatorname{poly}(s))italic_O ( roman_poly ( italic_s ) ) and is notably independent of dimension nğ‘›nitalic_n. We remark that most previous work on Hamiltonian learning (that we highlighted earlier) are done under the distance induced by the supremum norm of the Pauli spectrum and with extra constraints apart from locality [23, 34, 62, 36, 58, 59, 12, 21, 38, 40, 42, 52, 28]. When transformed into learning algorithms under the finer distance induced by the 2-norm of the Pauli spectrum, these proposals yield complexities that depend polynomially on nksuperscriptğ‘›ğ‘˜n^{k}italic_n start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT and only work for a restricted family of kğ‘˜kitalic_k-local Hamiltonians. The works that explicitly consider the problem of learning under the 2-norm have complexities depending on nğ‘›nitalic_n and assume a stronger access model [19, 9]. 1.3 Results Local Hamiltonians. Recently, Bluhm, Caro and Oufkir proposed a non-tolerant testing algorithm, meaning that it only works for the case Îµ1=0,subscriptğœ€10\varepsilon_{1}=0,italic_Îµ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0 , whose query complexity is Oâ¢(n2â¢k+2/(Îµ2âˆ’Îµ1)4)ğ‘‚superscriptğ‘›2ğ‘˜2superscriptsubscriptğœ€2subscriptğœ€14O(n^{2k+2}/(\varepsilon_{2}-\varepsilon_{1})^{4})italic_O ( italic_n start_POSTSUPERSCRIPT 2 italic_k + 2 end_POSTSUPERSCRIPT / ( italic_Îµ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_Îµ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT ) and with total evolution time Oâ¢(nk+1/(Îµ2âˆ’Îµ1)3)ğ‘‚superscriptğ‘›ğ‘˜1superscriptsubscriptğœ€2subscriptğœ€13O(n^{k+1}/(\varepsilon_{2}-\varepsilon_{1})^{3})italic_O ( italic_n start_POSTSUPERSCRIPT italic_k + 1 end_POSTSUPERSCRIPT / ( italic_Îµ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_Îµ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ). They posed as open questions whether the dependence on nğ‘›nitalic_n could be removed and whether an efficient tolerant-tester was possible [5, Section 1.5]. Our first result gives positive answer to both questions. Result 1.3. There is an algorithm that solves 1.1 for kğ‘˜kitalic_k-local Hamiltonians by making polyâ¡(1/(Îµ2âˆ’Îµ1))poly1subscriptğœ€2subscriptğœ€1\operatorname{poly}(1/(\varepsilon_{2}-\varepsilon_{1}))roman_poly ( 1 / ( italic_Îµ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_Îµ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ) queries to the evolution operator and with polyâ¡(1/(Îµ2âˆ’Îµ1))poly1subscriptğœ€2subscriptğœ€1\operatorname{poly}(1/(\varepsilon_{2}-\varepsilon_{1}))roman_poly ( 1 / ( italic_Îµ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_Îµ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ) total evolution time. See Theorem 4.1 for a formal statement of this result. Our algorithm to test for locality is simple. It consists of repeating the following process 1/(Îµ2âˆ’Îµ1)41superscriptsubscriptğœ€2subscriptğœ€141/(\varepsilon_{2}-\varepsilon_{1})^{4}1 / ( italic_Îµ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_Îµ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT times: prepare nğ‘›nitalic_n EPR pairs, apply Uâ¢(Îµ2âˆ’Îµ1)âŠ—Id2ntensor-productğ‘ˆsubscriptğœ€2subscriptğœ€1subscriptIdsuperscript2ğ‘›U(\varepsilon_{2}-\varepsilon_{1})\otimes\mathop{\rm Id}\nolimits_{2^{n}}italic_U ( italic_Îµ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_Îµ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) âŠ— roman_Id start_POSTSUBSCRIPT 2 start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_POSTSUBSCRIPT to them and measure in the Bell basis. Each time that we repeat this process, we sample from the Pauli sprectrum of Uâ¢(Îµ2âˆ’Îµ1)ğ‘ˆsubscriptğœ€2subscriptğœ€1U(\varepsilon_{2}-\varepsilon_{1})italic_U ( italic_Îµ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_Îµ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ).333The Pauli spectrum of a unitary U=âˆ‘xU^xâ¢Ïƒxğ‘ˆsubscriptğ‘¥subscript^ğ‘ˆğ‘¥subscriptğœğ‘¥U=\sum_{x}\widehat{U}_{x}\sigma_{x}italic_U = âˆ‘ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT over^ start_ARG italic_U end_ARG start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT italic_Ïƒ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT determines a probability distribution because âˆ‘x|U^x|2=1subscriptğ‘¥superscriptsubscript^ğ‘ˆğ‘¥21\sum_{x}|\widehat{U}_{x}|^{2}=1âˆ‘ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT | over^ start_ARG italic_U end_ARG start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 1. As Îµ2âˆ’Îµ1subscriptğœ€2subscriptğœ€1\varepsilon_{2}-\varepsilon_{1}italic_Îµ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_Îµ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT is small, Taylor expansion ensures that Uâ¢(Îµ2âˆ’Îµ1)â‰ˆId2nâˆ’iâ¢(Îµ2âˆ’Îµ1)â¢Hğ‘ˆsubscriptğœ€2subscriptğœ€1subscriptIdsuperscript2ğ‘›ğ‘–subscriptğœ€2subscriptğœ€1ğ»U(\varepsilon_{2}-\varepsilon_{1})\approx\mathop{\rm Id}\nolimits_{2^{n}}-i(% \varepsilon_{2}-\varepsilon_{1})Hitalic_U ( italic_Îµ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_Îµ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) â‰ˆ roman_Id start_POSTSUBSCRIPT 2 start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_POSTSUBSCRIPT - italic_i ( italic_Îµ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_Îµ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) italic_H, so sampling from the Pauli spectrum of Uâ¢(Îµ2âˆ’Îµ1)ğ‘ˆsubscriptğœ€2subscriptğœ€1U(\varepsilon_{2}-\varepsilon_{1})italic_U ( italic_Îµ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_Îµ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) allows us to estimate the weight of the non-local terms of Hğ»Hitalic_H. If that weight is big, we output that Hğ»Hitalic_H is far from kğ‘˜kitalic_k-local, and otherwise we conclude that Hğ»Hitalic_H is close to kğ‘˜kitalic_k-local. Our second result is a learning algorithm for kğ‘˜kitalic_k-local Hamiltonians. Result 1.4. There is an algorithm that solves 1.2 for kğ‘˜kitalic_k-local Hamiltonians by making expâ¡(k2+kâ¢logâ¡(1/Îµ))superscriptğ‘˜2ğ‘˜1ğœ€\exp(k^{2}+k\log(1/\varepsilon))roman_exp ( italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_k roman_log ( 1 / italic_Îµ ) ) queries to the evolution operator with expâ¡(k2+kâ¢logâ¡(1/Îµ))superscriptğ‘˜2ğ‘˜1ğœ€\exp(k^{2}+k\log(1/\varepsilon))roman_exp ( italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_k roman_log ( 1 / italic_Îµ ) ) total evolution time. See Theorem 4.3 for a formal statement of this result. The learning algorithm has two stages. In the first stage one samples from the Pauli distribution of Uâ¢(Îµ)ğ‘ˆğœ€U(\varepsilon)italic_U ( italic_Îµ ), as in the testing algorithm, and from that one can detect which are the big Pauli coefficients of Hğ»Hitalic_H. In the second stage we learn the big Pauli coefficients the swap test. One can ensure that the coefficients not detected as big in the first stage of the algorithm can be neglected. To argue this formally, we use the non-commutative Bohnenblust-Hille inequality, which has been used recently for various quantum learning algorithms [35, 56]. Sparse Hamiltonians. Despite the numerous papers in the classical literature studying the problems of testing and learning sparse Boolean functions [31, 46, 61, 24], there are not many results on learning Hamiltonians that are sparse (and not necessarily local) and the only testing result that we are aware of requires Oâ¢(sâ¢n)ğ‘‚ğ‘ ğ‘›O(sn)italic_O ( italic_s italic_n ) queries [6, Remark B.2]. Here, we present the first sparsity testing algorithm whose complexity does not depend on nğ‘›nitalic_n and the first learning algorithm for sparse Hamiltonians which does not make any assumptions regarding the support of the Hamiltonian beyond sparsity. Result 1.5. There is an algorithm that solves 1.1 for sğ‘ sitalic_s-sparse Hamiltonians by making polyâ¡(s/(Îµ2âˆ’Îµ1))polyğ‘ subscriptğœ€2subscriptğœ€1\operatorname{poly}(s/(\varepsilon_{2}-\varepsilon_{1}))roman_poly ( italic_s / ( italic_Îµ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_Îµ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ) queries to the evolution operator and with polyâ¡(s/(Îµ2âˆ’Îµ1))polyğ‘ subscriptğœ€2subscriptğœ€1\operatorname{poly}(s/(\varepsilon_{2}-\varepsilon_{1}))roman_poly ( italic_s / ( italic_Îµ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_Îµ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ) total evolution time. See Theorem 4.5 for a formal statement. This testing algorithm consists on performing Pauli sampling of Uâ¢((Îµ22âˆ’Îµ12)/s)ğ‘ˆsuperscriptsubscriptğœ€22superscriptsubscriptğœ€12ğ‘ U(\sqrt{(\varepsilon_{2}^{2}-\varepsilon_{1}^{2})/s})italic_U ( square-root start_ARG ( italic_Îµ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - italic_Îµ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) / italic_s end_ARG ) a total of Oâ¢(s4/(Îµ22âˆ’Îµ12)4)ğ‘‚superscriptğ‘ 4superscriptsuperscriptsubscriptğœ€22superscriptsubscriptğœ€124O(s^{4}/(\varepsilon_{2}^{2}-\varepsilon_{1}^{2})^{4})italic_O ( italic_s start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT / ( italic_Îµ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - italic_Îµ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT ) times. From these samples one can estimate the sum of the squares of the top sğ‘ sitalic_s Pauli coefficients of Uğ‘ˆUitalic_U. If this quantity is big enough, we output that the Hamiltonian is close to sğ‘ sitalic_s-sparse, and otherwise that is far. Although from this high-level description the algorithm seems similar to the locality testing one, the analysis is more involved and requires taking the second order Taylor expansion, which is the reason why the dependence on (Îµ2âˆ’Îµ1)subscriptğœ€2subscriptğœ€1(\varepsilon_{2}-\varepsilon_{1})( italic_Îµ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_Îµ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) is worse in this case. Result 1.6. There is an algorithm that solves 1.2 for sğ‘ sitalic_s-sparse Hamiltonians by making polyâ¡(s/Îµ)polyğ‘ ğœ€\operatorname{poly}(s/\varepsilon)roman_poly ( italic_s / italic_Îµ ) queries to the evolution operator with polyâ¡(s/Îµ)polyğ‘ ğœ€\operatorname{poly}(s/\varepsilon)roman_poly ( italic_s / italic_Îµ ) total evolution time. See Theorem 4.6 for a formal statement. This learning algorithm begins by detecting which are the top sğ‘ sitalic_s Pauli coefficients, which can be done via Pauli sampling, and concludes by learning the top sğ‘ sitalic_s Pauli coefficients using a swap test, similarly to the locality learning algorithm. Learning and testing without quantum memory. Motivated by the limitations of current devices, there has been a series of recent works aiming to understand the power of quantum memory in testing and learning tasks, exhibiting exponential separations in some cases [14, 16, 17]. A natural question is, if the problems that we mentioned above become harder without quantum memory? Learning without memory. We surprisingly show that, the learning protocols that we mention above, can be implemented efficiently when one has no quantum memory. To this end, we provide two crucial subroutines for (i)ğ‘–(i)( italic_i ) estimating the Pauli spectrum of a unitary, (iâ¢i)ğ‘–ğ‘–(ii)( italic_i italic_i ) estimating a single Pauli coefficient to make our protocols work in the memory-less setting. Subroutine (iâ¢i)ğ‘–ğ‘–(ii)( italic_i italic_i ) incurs in no extra query-cost, and subroutine (i)ğ‘–(i)( italic_i ) only incurs in a factor-nğ‘›nitalic_n overhead in the case of learning sğ‘ sitalic_s-sparse Hamiltonians and a factor logâ¡(n)ğ‘›\log(n)roman_log ( italic_n ) in the case of learning kğ‘˜kitalic_k-local Hamiltonians. These subroutines can also be useful in other contexts. In particular, we propose tolerant tester to decide if an unknown unitary is a kğ‘˜kitalic_k-junta which uses Oâ¢(4k)ğ‘‚superscript4ğ‘˜O(4^{k})italic_O ( 4 start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ) queries (see Proposition 3.5), making progress on a question of Chen et al. [18, Section 1.3], and then we use subroutine (i)ğ‘–(i)( italic_i ) to turn it into a memory-less tester that only makes Oâ¢(4kâ¢n)ğ‘‚superscript4ğ‘˜ğ‘›O(4^{k}n)italic_O ( 4 start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_n ) queries. Testing sparse Pauli channels via Pauli hashing. In order to test for sparsity of Hamiltonian without memory we reduce to the problem of testing sparsity of a Pauli channel Î¦:Ïâ†¦âˆ‘xpâ¢(x)â¢Ïƒxâ¢Ïâ¢Ïƒx:Î¦maps-toğœŒsubscriptğ‘¥ğ‘ğ‘¥subscriptğœğ‘¥ğœŒsubscriptğœğ‘¥\Phi:\rho\mapsto\sum_{x}p(x)\sigma_{x}\rho\sigma_{x}roman_Î¦ : italic_Ï â†¦ âˆ‘ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT italic_p ( italic_x ) italic_Ïƒ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT italic_Ï italic_Ïƒ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT, which is of independent interest. To do that, we introduce a new technique called Pauli Hashing which allows to construct random partitions of Pauli operators. The high-level idea is to bucket the error rates pâ¢(x)ğ‘ğ‘¥p(x)italic_p ( italic_x ) and thereby the corresponding Pauli operators: for this, we choose a random subgroup GğºGitalic_G of the nğ‘›nitalic_n-qubit Pauli group with dimension t=Oâ¢(logâ¡s)ğ‘¡ğ‘‚ğ‘ t=O(\log s)italic_t = italic_O ( roman_log italic_s ). Pauli hashing allows us to partition all the Pauli operators into cosets the centralizer of GğºGitalic_G which contains all the Paulis that commute with the Paulis in GğºGitalic_G. The buckets are then the Oâ¢(s)ğ‘‚ğ‘ O(s)italic_O ( italic_s ) cosets of the centralizer of GğºGitalic_G. The main work then goes into arguing that the sum of the weights of the top sğ‘ sitalic_s buckets is a good estimate of the top sğ‘ sitalic_s error rates, and then a structural lemma we prove shows this is a good proxy for indicating whether the Pauli channel is close to being sğ‘ sitalic_s-sparse or not. Putting everything together, with some careful analysis, we get an efficient tolerant tester for sğ‘ sitalic_s-sparse Pauli channels. Result 1.7. There is an algorithm requiring no quantum memory that tests if a Pauli channel Îµ1subscriptğœ€1\varepsilon_{1}italic_Îµ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-close to or Îµ1subscriptğœ€1\varepsilon_{1}italic_Îµ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-far-from being sğ‘ sitalic_s-sparse in diamond norm by making O~â¢(s2/(Îµ2âˆ’Îµ1)6)~ğ‘‚superscriptğ‘ 2superscriptsubscriptğœ€2subscriptğœ€16\widetilde{O}(s^{2}/(\varepsilon_{2}-\varepsilon_{1})^{6})over~ start_ARG italic_O end_ARG ( italic_s start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT / ( italic_Îµ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_Îµ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT ) queries to the channel. See Theorem 5.3 for a formal statement. We remark that Pauli Hashing only requires the preparation of Pauli eigenstates and Pauli measurements, making it suitable for the near-term. Testing sparse Hamiltonians without memory. We provide a memory-less testing algorithm for sğ‘ sitalic_s-sparse Hamiltonians that uses Pauli hashing, that is completely independent of our tester with memory and only requires polyâ¡(s/Îµ)polyğ‘ ğœ€\operatorname{poly}(s/\varepsilon)roman_poly ( italic_s / italic_Îµ ) queries and total evolution time, notably avoiding any dependence on nğ‘›nitalic_n. To do this, we reduce the problem of testing Hamiltonian sparsity to testing the sparsity of an associated Pauli channel. To be precise, given the time evolution channel â„‹t:Ïâ†’Uâ¢(t)â¢Ïâ¢Uâ€ â¢(t):subscriptâ„‹ğ‘¡â†’ğœŒğ‘ˆğ‘¡ğœŒsuperscriptğ‘ˆâ€ ğ‘¡\mathcal{H}_{t}:\rho\to U(t)\rho U^{\dagger}(t)caligraphic_H start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT : italic_Ï â†’ italic_U ( italic_t ) italic_Ï italic_U start_POSTSUPERSCRIPT â€  end_POSTSUPERSCRIPT ( italic_t ), we define its Pauli-twirled channel via â„‹tğ’¯â¢(Ï)=ğ”¼xâ¢[Ïƒxâ¢â„‹tâ¢(Ïƒxâ¢Ïâ¢Ïƒx)â¢Ïƒx],superscriptsubscriptâ„‹ğ‘¡ğ’¯ğœŒsubscriptğ”¼ğ‘¥delimited-[]subscriptğœğ‘¥subscriptâ„‹ğ‘¡subscriptğœğ‘¥ğœŒsubscriptğœğ‘¥subscriptğœğ‘¥\mathcal{H}_{t}^{\mathcal{T}}(\rho)=\mathbb{E}_{x}[\sigma_{x}\mathcal{H}_{t}(% \sigma_{x}\rho\sigma_{x})\sigma_{x}],caligraphic_H start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT caligraphic_T end_POSTSUPERSCRIPT ( italic_Ï ) = blackboard_E start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT [ italic_Ïƒ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT caligraphic_H start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_Ïƒ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT italic_Ï italic_Ïƒ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT ) italic_Ïƒ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT ] , and we prove that â„‹tsubscriptâ„‹ğ‘¡\mathcal{H}_{t}caligraphic_H start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is sparse if and only if â„‹tğ’¯superscriptsubscriptâ„‹ğ‘¡ğ’¯\mathcal{H}_{t}^{\mathcal{T}}caligraphic_H start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT caligraphic_T end_POSTSUPERSCRIPT is sparse. Our result is then as follows. See Theorem 5.7 for a formal statement. Result 1.8. There is an algorithm requiring no quantum memory that solves 1.2 for sğ‘ sitalic_s-sparse Hamiltonians by making polyâ¡(s/Îµ)polyğ‘ ğœ€\operatorname{poly}(s/\varepsilon)roman_poly ( italic_s / italic_Îµ ) queries to the evolution operator with polyâ¡(s/Îµ)polyğ‘ ğœ€\operatorname{poly}(s/\varepsilon)roman_poly ( italic_s / italic_Îµ ) total evolution time. Lower bounds. One drawback of our learning and testing algorithms is the exponent of the sparsity parameter sğ‘ sitalic_s, locality parameter kğ‘˜kitalic_k and the tolerance (Îµ2âˆ’Îµ1)subscriptğœ€2subscriptğœ€1(\varepsilon_{2}-\varepsilon_{1})( italic_Îµ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_Îµ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ). Reducing to classical Boolean functions, we show lower bounds without memory that cerfity that the dependence on these parameters cannot be completely avoided, but an interesting and important future direction is to obtain the optimal results for these near-term relevant problems.444We remark that, Bakshi et al. [9] used highly non-trivial ideas to get Heisenberg scaling for their learning algorithm, and potentially similar ideas could be useful here. 1.4 Discussion and open questions Our work opens up several interesting directions which we state here and leave for future work. 1. Dependence on parameters Îµ1,Îµ2subscriptğœ€1subscriptğœ€2\varepsilon_{1},\varepsilon_{2}italic_Îµ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_Îµ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. Our main objective in this work was to obtain query complexities for testing and learning with good dependence on the structural parameters. It is natural to ask if we could improve the dependence on the error parameters and perhaps achieve Heisenberg limited scaling as has been shown to be possible in some particular cases for Hamiltonian learning [38, 9]. 2. Robustness to SPAM noise. It would be desirable to make the protocols introduced in this work to be robust to SPAM noise. A potential approach is to adapt strategies in [26]. 3. Adaptivity. For learning structured Hamiltonians, adaptive strategies [29, 22] can improve query complexity by shedding constant factors over baseline learning algorithms, thereby improving performance in practice. Another direction is to then explore adaptive protocols for testing structured Hamiltonians and the performance gains they may bring. 4. Testing and learning with limited quantum memory. For estimating properties of quantum states, Chen et al. [14] showcased the utility of the resource of quantum memory or a kğ‘˜kitalic_k-qubit ancillary system (k<nğ‘˜ğ‘›k<nitalic_k < italic_n). Large separations in query complexity when learning with memory (even for kâ‰ªnmuch-less-thanğ‘˜ğ‘›k\ll nitalic_k â‰ª italic_n) and without memory have been reported for learning Pauli channels [20, 15] and shadow tomography [16]. We could thus imagine having access to only limited quantum memory during learning or testing structured Hamiltonians as well. However, it should be noted that given the separation between the query complexities (see Table 1) with access to nğ‘›nitalic_n-qubit quantum memory and without any, only marginal gains in complexity are expected from having access to limited quantum memory. 5. Testing and learning Hamiltonians from Gibbs states. Another natural learning model is that of having access to copies of the Gibbs state of a quantum Hamiltonian at a certain inverse temperature. There has been a suite of work investigating learning local Hamiltonians from Gibbs states [1, 8] but answering the question of testing structured Hamiltonians given access to copies of the Gibbs state remains wide open. Note added. After sharing Theorem 4.1 with Bluhm et al., they independently improved the analysis of their testing algorithm and showed that it only requires Oâ¢(1/(Îµ2âˆ’Îµ1)3â¢Îµ2)ğ‘‚1superscriptsubscriptğœ€2subscriptğœ€13subscriptğœ€2O(1/(\varepsilon_{2}-\varepsilon_{1})^{3}\varepsilon_{2})italic_O ( 1 / ( italic_Îµ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_Îµ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT italic_Îµ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) queries and Oâ¢(1/(Îµ2âˆ’Îµ1)2.5â¢Îµ20.5)ğ‘‚1superscriptsubscriptğœ€2subscriptğœ€12.5superscriptsubscriptğœ€20.5O(1/(\varepsilon_{2}-\varepsilon_{1})^{2.5}\varepsilon_{2}^{0.5})italic_O ( 1 / ( italic_Îµ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_Îµ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2.5 end_POSTSUPERSCRIPT italic_Îµ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 0.5 end_POSTSUPERSCRIPT ) total evolution time, which is very similar to our Theorem 4.1 [6]. In addition, for a wide range of k=Oâ¢(n)ğ‘˜ğ‘‚ğ‘›k=O(n)italic_k = italic_O ( italic_n ), their algorithm does not require the use of auxiliary qubits. Acknowledgements. S.A. and A.D. thank the Institute for Pure and Applied Mathematics (IPAM) for its hospitality throughout the long program â€œMathematical and Computational Challenges in Quantum Computingâ€ in Fall 2023 during which part of this work was initiated. This work was done in part while S.A. was visiting the Simons Institute for the Theory of Computing, supported by DOE QSA grant #FP00010905. This research was supported by the Europea unionâ€™s Horizon 2020 research and innovation programme under the Marie SkÅ‚odowska-Curie grant agreement no. 945045, and by the NWO Gravitation project NETWORKS under grant no. 024.002.003. We thank Amira Abbas, Francesco Anna Mele, Andreas Bluhm, Jop BriÃ«t, Matthias Caro, Nunzia Cerrato, Aadil Oufkir, and Daniel Liang for useful comments and discussions. A.D. thanks Patrick Rall for multiple conversations on stabilizer subgroups and Pauli twirling. A.D. thanks Isaac Chuang for discussions on the problem of testing Hamiltonians. F.E.G. is funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germanyâ€™s Excellence Strategy â€“ EXC-2047/1 â€“ 390685813."
