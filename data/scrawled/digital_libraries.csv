URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.04825v1,VTechAGP: An Academic-to-General-Audience Text Paraphrase Dataset and Benchmark Models,"Existing text simplification or paraphrase datasets mainly focus on sentence-level text generation in a general domain. These datasets are typically developed without using domain knowledge. In this paper, we release a novel dataset, VTechAGP, which is the first academic-to-general-audience text paraphrase dataset consisting of 4,938 document-level these and dissertation academic and general-audience abstract pairs from 8 colleges authored over 25 years. We also propose a novel dynamic soft prompt generative language model, DSPT5. For training, we leverage a contrastive-generative loss function to learn the keyword vectors in the dynamic prompt. For inference, we adopt a crowd-sampling decoding strategy at both semantic and structural levels to further select the best output candidate. We evaluate DSPT5 and various state-of-the-art large language models (LLMs) from multiple perspectives. Results demonstrate that the SOTA LLMs does not provide satisfactory outcomes, while the lightweight DSPT5 can achieve competitive results. To the best of our knowledge, we are the first to build a benchmark dataset and solutions for academic-to-general-audience text paraphrase dataset.","Text generation aims to produce understandable text in human language from various sources of input data. Among them, text-to-text generation remains an important and challenging task with extensive applications such as language translation Ranathunga et al. (2023); Dabre et al. (2020), paraphrase generation Singh and Josan (2022), text simplification Martin et al. (2023), etc. Existing text simplification and paraphrase generation datasets (shown in Table 5 in the Appendix) mainly focus on sentence-level translation. The recent paragraph-level dataset WikiAuto Jiang et al. (2020), derived from WikiLarge Zhang and Lapata (2017) and Newsela Xu et al. (2015), still suffers from a lack of domain diversity and specification, which are sourced from Wikipedia and news articles. Besides, the objective of these studies on the aforementioned datasets is to simplify the text for children at lower grade levels. Furthermore, existing works on lay summarization involves brief summarization limited to medicine and biological domains Devaraj et al. (2021); Flores et al. (2023); Jiang et al. (2024); Zaman et al. . However, our proposed VTechAGP involves translation from technical language to general-audience language across a broader set of multiple domains. We aim to generate text from the domain-specific to a general level of understanding, while keeping it scientifically accurate and easy to understand, in order to encourage and facilitate interdisciplinary collaboration across different research fields. In this paper, we pioneer the research of academic-to-general-audience text generation by introducing a new benchmark dataset VTechAGP, which is derived from electronic theses and dissertations (ETDs) at Virginia Tech over twenty-five years. VTechAGP consists of 4,938 document-level abstract pairs (academic abstract and general-audience abstract). VTechAGP also provides other information such as title, discipline, degree level, etc. This auxiliary information shows the potential of VTechAGP for other tasks such as topic generation, etc. In addition, the abstracts in VTechAGP come from multiple domains (colleges) and VTechAGP is labeled with each specific domain and provides the domain knowledge keywords. More details about VTechAGP are presented in Sec. 3 and Table 6. Based on VTechAGP, we evaluate several SOTA pre-trained large language models (LLMs), such as LLaMA2 Touvron et al. (2023), Claude2 Anthropic (2023), ChatGPT Brown et al. (2020), etc., to establish the baseline performance. However, these SOTA pre-trained LLMs have demonstrated the following limitations: (1) Some LLMs do not provide public APIs, or the APIs are not free. Also, some LLMs (e.g., Claude2) do not provide fine-tuning, making them less adaptable to specific tasks. (2) The model size of LLMs is very large. For example, LLaMA2 has about 65 billion parameters. Fine-tuning these LLMs is resource-intensive in terms of memory and computation time. Even the inference implementation requires more memory and time. (3) The pre-trained LLMs do not show competitive performance for the academic-to-general-audience text paraphrasing task on VTechAGP in Sec. 5. To address the above challenges, we propose DSPT5, a dynamic soft prompt-based generative model with the crowd sampling decoding strategy during the inference stage. DSPT5 is built based on the pre-trained T5 Raffel et al. (2020), which has only about 220 million parameters. In particular, the dynamic soft prompt template in DSPT5 can automatically adapt to different academic domains by changing the keywords extracted from the academic abstract. The prompt encoder in DSPT5 is trained to generate and fine-tune keyword vectors combined with the dynamic prompt template. To this end, we design a hybrid loss function with generative language model loss and contrastive loss to jointly learn the generated text representations as well as the ability to distinguish technical keywords from non-technical keywords. During inference, DSPT5 employs two alignment functions at both the semantic and structural levels to select the best candidate for the final generated output. The contributions can be summarized as follows: (1) Dataset: We construct VTechAGP, the first academic-to-general-audience text paraphrase dataset. VTechAGP is a document-level text generation dataset with multiple technical domains. (2) Baselines: We implement several SOTA LLMs as benchmarks to compare the performance with our proposed model DSPT5. Experimental results show that there is still a huge room for further improvement of the existing LLMs. (3) Approach: We propose a lightweight model, DSPT5, which utilizes dynamic soft prompts with a hybrid loss function and a new crowd decoding strategy. Experimental results show that DSPT5 can achieve competitive results with SOTA LLMs. (4) Evaluation: We explore various evaluation metrics for the academic-to-general-audience text paraphrasing task on VTechAGP from different perspectives, including document-level embedding-based, word-based, and end-to-end metrics. In addition, simplicity, diversity, readability, and toxicity are also considered for the performance evaluation."
https://arxiv.org/html/2411.03417v1,Usefulness of LLMs as an Author Checklist Assistantfor Scientific Papers: NeurIPS’24 Experiment,"Large language models (LLMs) represent a promising, but controversial, tool in aiding scientific peer review. This study evaluates the usefulness of LLMs in a conference setting as a tool for vetting paper submissions against submission standards. We conduct an experiment at the 2024 Neural Information Processing Systems (NeurIPS) conference, where 234 papers were voluntarily submitted to an “LLM-based Checklist Assistant.” This assistant validates whether papers adhere to the author checklist used by NeurIPS, which includes questions to ensure compliance with research and manuscript preparation standards. Evaluation of the assistant by NeurIPS paper authors suggests that the LLM-based assistant was generally helpful in verifying checklist completion. In post-usage surveys, over 70% of authors found the assistant useful, and 70% indicate that they would revise their papers or checklist responses based on its feedback. While causal attribution to the assistant is not definitive, qualitative evidence suggests that the LLM contributed to improving some submissions. Survey responses and analysis of re-submissions indicate that authors made substantive revisions to their submissions in response to specific feedback from the LLM. The experiment also highlights common issues with LLMs—inaccuracy (20/52) and excessive strictness (14/52) were the most frequent issues flagged by authors. We also conduct experiments to understand potential gaming of the system, which reveal that the assistant could be manipulated to enhance scores through fabricated justifications, highlighting potential vulnerabilities of automated review tools.","Recent advancements in large language models (LLMs) have significantly enhanced their capabilities in areas such as question answering and text generation. One promising application of LLMs is in aiding the scientific peer-review process [Sha22, KAD+24]. However, the idea of using LLMs in peer review is contentious and fraught with potential issues [LS23]. LLMs can hallucinate, exhibit biases, and may compromise the fairness of the peer-review process. Despite these potential issues, LLMs may serve as useful analytical tools to scrutinize manuscripts and identify possible weaknesses or inaccuracies that need addressing. In this study, we take the first steps towards harnessing the power of LLMs in the application of conference peer review. We conduct an experiment the the Neural Information Processing Systems (NeurIPS) 2024 conference, a premier conference in the field of machine learning.111In computer science, unlike most other fields, conferences are a primary venue for publication, with the peer-review process evaluating entire manuscripts rather than just abstracts. While the wider ethical implications and appropriate use cases of LLMs remain unclear and must be a larger community discussion, here, we evaluate a relatively clear-cut and low-risk use case: vetting paper submissions against submission standards, with results shown only to the authors. Specifically, the NeurIPS peer-review process requires authors to submit a checklist appended to their manuscripts. Such author checklists, utilized in NeurIPS as well as in other peer-review venues [MSA01, VEA+07, MLT+09], contain a set of questions designed to ensure that authors follow appropriate research and manuscript preparation practices. The NeurIPS Paper Checklist is a series of yes/no questions that help authors check if their work meets reproducibility, transparency, and ethical research standards expected for papers at NeurIPS. The checklist is a critical component in maintaining standards of research presented at the conference. Adhering to the guidelines outlined by these checklists helps authors avoid mistakes that could lead to rejection during peer review. We deploy and evaluate a NeurIPS 2024 Checklist Assistant powered by LLMs. This assistant scrutinizes authors’ responses to the NeurIPS checklist, proposing enhancements for submissions to meet the conference’s requirements. To prevent any potential bias in the review process, we confine its usage exclusively to the authors of papers, so the checklist assistant is not accessible to reviewers. We then systematically evaluate the benefits and risks of LLMs by conducting a structured study to understand if LLMs can enhance research quality and improve efficiency by helping authors understand if their work meets research standards. Specifically, we administered surveys both before and after use of the Checklist Assistant asking authors about their expectations for and perceptions of the tool. We received 539 responses to the pre-usage survey, 234 submissions the the Checklist Assistant and 78 responses to the post-usage survey. Our main findings are as follows: (1) Authors generally reported that the LLM-assisted checklist review was a valuable enhancement to the paper submission process. • The majority of surveyed authors reported a positive experience using the LLM assistant. After using the assistant, over 70% of authors reported that they found the assistant useful and over 70% reported that they would modify their paper and/or checklist responses based on the feedback given (Section 4.1.3). • Authors’ expectations of the assistant’s effectiveness were even more positive before using it than their assessments after actually using it (Section 4.1.3). • Among the main issues reported by authors in qualitative feedback, the most frequently cited were inaccuracy (20/52 respondents) and that the LLM was too strict in its requirements (14/52 respondents) (Section 4.1.4). (2) While changes in NeurIPS paper submissions cannot be causally attributed to use of the checklist verification assistant, we find qualitative evidence that the checklist review meaningfully helped some authors to improve their submissions. • Analysis of the content of LLM feedback to authors indicates that the LLM provided granular feedback to authors, generally giving 4-6 distinct and specific points of feedback per question across the 15 questions (Section 4.2.1). • Survey responses reflect that some authors made meaningful changes to their submissions—35 survey respondents described specific modifications they would make to their submissions in response to the Checklist Assistant (Section 4.2.2). • In 40 instances, authors submitted their paper twice to the checklist verifier (accounting for 80 total paper submissions.) Between these two submissions, authors tended to increase the length of their checklist justifications significantly, suggesting that they may have added content in response to LLM feedback (Section 4.2.3). Finally, we investigate how LLM-based tools can be easily manipulated – specifically, we find that with AI-assisted re-writing of the justifications, an adversarial author can make the Checklist Assistant significantly more lenient (Section 5.1). In summary, the majority of authors found LLM assistance to be beneficial, highlighting the significant potential of LLMs to enhance scientific workflows—whether by serving as direct assistants to authors or helping journals and conferences verify guideline compliance. However, our findings also underscore that LLMs cannot fully replace human expertise in these contexts. A notable portion of users encountered inaccuracies, and the models were also vulnerable to adversarial manipulation. Our code, LLM prompts, and sample papers used for testing are available at: https://github.com/ihsaan-ullah/neurips-checklist-assistant"
https://arxiv.org/html/2411.03340v1,Unlocking the Archives: Large Language Models Achieve State-of-the-Art Performance on the Transcription of Handwritten Historical Documents,"This study demonstrates that Large Language Models (LLMs) can transcribe historical handwritten documents with significantly higher accuracy than specialized Handwritten Text Recognition (HTR) software, while being faster and more cost-effective. We introduce an open-source software tool called Transcription Pearl that leverages these capabilities to automatically transcribe and correct batches of handwritten documents using commercially available multimodal LLMs from OpenAI, Anthropic, and Google. In tests on a diverse corpus of 18th/19th century English language handwritten documents, LLMs achieved Character Error Rates (CER) of 5.7 to 7% and Word Error Rates (WER) of 8.9 to 15.9%, improvements of 14% and 32% respectively over specialized state-of-the-art HTR software like Transkribus. Most significantly, when LLMs were then used to correct those transcriptions as well as texts generated by conventional HTR software, they achieved near-human levels of accuracy, that is CERs as low as 1.8% and WERs of 3.5%. The LLMs also completed these tasks 50 times faster and at approximately 1/50th the cost of proprietary HTR programs. These results demonstrate that when LLMs are incorporated into software tools like Transcription Pearl, they provide an accessible, fast, and highly accurate method for mass transcription of historical handwritten documents, significantly streamlining the digitization process.","Introduction As historians and archivists digitize ever larger collections of handwritten records, accurate transcription remains a barrier to their systematic analysis, publication, and accessibility. It is also the most time-consuming and costly aspect of the digitization process. The field of Handwritten Text Recognition (HTR) attempts to automate the task through machine learning but it is a complex and technologically advanced subfield of computer vision and artificial intelligence research, an area in which few historians are comfortable operating. In recent years, programs like as Transkribus, run by the READ-COOP cooperative which includes more than 150 major universities and archives, has made HTR far more accessible through graphical, “drag and drop” interfaces (Kahle et al 2017; Nockels et al 2022). Transkribus largely automates the most complex image pre-processing and segmentation elements of the workflow and allows users to use pretrained neural network models to generate a rough transcription of handwritten documents. These models are highly accurate when they have been trained to recognize a specific individual’s handwriting, achieving word-level accuracy between 85 and 95% (Al Kendi, 2024). That said, they are also moderately expensive and time-consuming to use at roughly $0.27 USD per page with variable turnaround times. Although actual processing normally takes 15-20 minutes, depending on customer volumes transcription requests can be placed in a queue for a day or more. A more important issue is that because handwriting is unique, the most impressive results do not normally reflect most users’ experiences with HTR. Historians are often discouraged when they find that most HTR models have significant difficulty generalizing “out of the box” to new hands, document formats, or styles that they did not encounter in training. By “out-of-the-box”, we mean a model that can be employed “as is” without the user having to train the model on a specific handwriting style. On such tasks, they typically achieve word level accuracy of only 50-75% which make the results largely unusable without major editing. Teaching or fine-tuning models to recognize a specific, individual handwriting style requires users to generate perfect transcriptions of at least 75 handwritten pages (about 15,000 words)—referred to as “ground-truth” documents—which can then be used as training data. As a result, unless historians are working with hundreds or thousands of pages of documents written in a single hand, this fine-tuning process is rarely worth the effort. This means that for most historians, HTR remains a tantalizing but impractical solution to the transcription problem. In this paper, we introduce Transcription Pearl, an easy-to-use software tool that automatically transcribes batches of handwritten documents “out-of-the-box” using widely available multi-modal Large Language Models (LLMs) like GPT-4o, Claude Sonnet-3.5, and Gemini 1.5-Pro, quickly, cost effectively, and to a high degree of accuracy. Depending on how capitalization, punctuation, and historical spelling errors are evaluated, it achieves accuracy levels of between 84 and 93% on transcription tasks without the need for image pre-processing or fine-tuning. We also demonstrate that unlike conventional HTR software, LLMs can be used to correct transcriptions generated by other LLMs as well as Transkribus to levels of accuracy that would normally require extensive fine-tuning, that is above 96%. At this level of accuracy these transcriptions not only approach human levels of accuracy but are “good enough” for most day-to-day use cases such as full text keyword and semantic search as well as improving accessibility and readability. We thus show that the application of LLMs to HTR potentially provides an accessible, affordable, fast, and accurate pathway for both mass transcription projects and individual historians working on their own corpus of records."
https://arxiv.org/html/2411.00987v1,Identifying Telescope Usage in Astrophysics Publications: A Machine Learning Framework for Institutional Research Management at Observatories,"Large scientific institutions, such as the Space Telescope Science Institute, track the usage of their facilities to understand the needs of the research community. Astrophysicists incorporate facility usage data into their scientific publications, embedding this information in plain-text. Traditional automatic search queries prove unreliable for accurate tracking due to the misidentification of facility names in plain-text. As automatic search queries fail, researchers are required to manually classify publications for facility usage, which consumes valuable research time. In this work, we introduce a machine learning classification framework for the automatic identification of facility usage of observation sections in astrophysics publications. Our framework identifies sentences containing telescope mission keywords (e.g., Kepler and TESS) in each publication. Subsequently, the identified sentences are transformed using Term Frequency-Inverse Document Frequency and classified with a Support Vector Machine. The classification framework leverages the context surrounding the identified telescope mission keywords to provide relevant information to the classifier. The framework successfully classifies usage of MAST hosted missions with a 92.9% accuracy. Furthermore, our framework demonstrates robustness when compared to other approaches, considering common metrics and computational complexity. The framework’s interpretability makes it adaptable for use across observatories and other scientific facilities worldwide.","The global astronomical community identifies the scientific output of telescope facilities, which is essential to understand the needs of the community, effectively allocate resources, and plan current and future facility management. A primary metric for measuring this scientific output is tracking the usage of telescope facilities in scientific publications (see, e.g., Grothkopf et al., 2018). However, simple programmatic attempts at identifying telescope usage based on the appearance of mission keywords fail because the descriptions of telescope usage are embedded in an unstructured way throughout the plain-text of publications. Consequently, for now, researchers are required to manually identify publications and leverage specific subject knowledge needed for accurate identification. Figure 1: The filtered dataset comprises sentences that include MAST hosted mission keywords. For example, when filtering publications for the space telescope ’Kepler’ we identify three sentences shown in these two publications. Contextual cues guide the reader or classifier to distinguish that, on the left, one publication references the ’Kepler’ space telescope and is a MAST publication, while on the right, the other publication discusses ’Keplerian physics’ and the research does not utilize the Kepler space telescope. The exponential growth of the astronomical literature requires researchers to continually spend additional time manually classifying publications, a mundane task diverting researchers from their primary research (see, for example, Kerzendorf, 2019; Bornmann et al., 2021). At the Space Telescope Science Institute (STScI), a small team of researchers queries the SAO/NASA Astrophysics Data System (NASA/ADS) 111https://ui.adsabs.harvard.edu/ each month, dedicating part of their time to identifying and classifying as many relevant publications as possible, alongside their other responsibilities. For example, from 2021 to 2023, on average, ∼550similar-toabsent550\sim{550}∼ 550 new publications per month were identified for classification. Given workforce capacity and resource limitations, on average, 14% of these publications remain unclassified, contributing to a growing backlog (private communication, Brian Cherinka). The accurate and scalable tracking of telescope usage can potentially be met by automatic text classification. Simple automatic solutions, such as full-text searches, cannot accurately distinguish the nuances that indicate telescope usage in astrophysics publications (Barnett et al., 2009). Facility names (e.g., FUSE, Copernicus, and Kepler) can be misidentified owing to their lack of uniqueness and potential overlap with common words (see Figure 1). While the lack of structure in the plain-text of publications requires initially searching for telescope names, accurate classification requires leveraging the context to identify telescope usage. Text classification is a principle task in the field of natural language processing (NLP), with various recent advancements with the specific aim to allow for the inclusion of domain knowledge and feedback processes (e.g., fine-tuning; Beltagy et al., 2019). Accordingly, much research has been directed at presenting efficient, generalizable, and accurate methodologies with more applications appearing recently in the scientific domain (Khadhraoui et al., 2022; Farshid Danesh & Dastani, 2023). As simple automatic solutions fail, the advancements of computational techniques, like machine learning, are leveraged for the task of identifying telescope or data usage from publications (Chen et al., 2022, hereafter C22). In the domain of astrophysics, C22 presents a classification framework trained to categorize publications relevant to NASA/IPAC Extragalactic Database (NED) data products. Utilizing the Stanford NLP classifier, C22 identifies NED-relevant publications. Nevertheless, various limitations exist, specifically the resource-intensive data preprocessing and inclusion of less informative features. These limitations introduce challenges when adapting the classification framework presented in C22 for our objective of identifying telescope usage (refer to Section 3 for detailed insights). To overcome the limitations, we present a robust, interpretable, and scalable text classification framework using machine learning to automatically identify telescope usage in publications. We train and validate the text classification framework using labeled data from The Barbara A. Mikulski Archive for Space Telescopes (MAST)222https://archive.stsci.edu/ as a case study. In Section 2, we introduce the data used in the binary text classification task and outline the data preprocessing workflow. Additionally, we discuss the training process for the selected classifiers along with their hyperparameter tuning. In Section 3, we present the performance of the selected classifiers, utilizing various commonly used metrics, which we draw comparisons with C22. Finally, in Section 4, we draw conclusions from our findings and emphasize the effectiveness of the developed classification framework in correctly tracking the usage of MAST hosted missions. Figure 2: Our classification framework, presented here, follows a supervised training structure. First we identify relevant sentences in each publication containing mission keywords (e.g., Kepler and TESS) and then we vectorize and classify the text."
https://arxiv.org/html/2411.00041v1,NeuroSym-BioCAT: Leveraging Neuro-Symbolic Methods for Biomedical Scholarly Document Categorization and Question Answering,"The growing volume of biomedical scholarly document abstracts presents an increasing challenge in efficiently retrieving accurate and relevant information. To address this, we introduce a novel approach that integrates an optimized topic modelling framework, OVB-LDA, with the BI-POP CMA-ES optimization technique for enhanced scholarly document abstract categorization. Complementing this, we employ the distilled MiniLM model, fine-tuned on domain-specific data, for high-precision answer extraction. Our approach is evaluated across three configurations: scholarly document abstract retrieval, gold-standard scholarly documents abstract, and gold-standard snippets, consistently outperforming established methods such as RYGH and bio-answer finder. Notably, we demonstrate that extracting answers from scholarly documents abstracts alone can yield high accuracy, underscoring the sufficiency of abstracts for many biomedical queries. Despite its compact size, MiniLM exhibits competitive performance, challenging the prevailing notion that only large, resource-intensive models can handle such complex tasks. Our results, validated across various question types and evaluation batches, highlight the robustness and adaptability of our method in real-world biomedical applications. While our approach shows promise, we identify challenges in handling complex list-type questions and inconsistencies in evaluation metrics. Future work will focus on refining the topic model with more extensive domain-specific datasets, further optimizing MiniLM and utilizing large language models (LLM) to improve both precision and efficiency in biomedical question answering.","With around 2.5 million new research contributions every year [1] specifically the rapidly growing biomedical research, the need for efficient and accurate information retrieval methods has become increasingly critical. The total volume of scholarly documents and the complexity of biomedical queries present substantial challenges in extracting relevant answers from vast repositories of knowledge. As the field advances, researchers are often faced with the daunting task of sifting through an extensive array of documents to obtain precise information. This highlights the necessity for robust answer extraction and document categorization methods that can enhance the accessibility of vital information. To address these challenges, this research proposes a neuro-symbolic approach that combines optimized topic modelling with advanced machine learning techniques, effectively integrating symbolic reasoning with neural representations for enhanced document retrieval and answer extraction. Specifically, we explore the efficacy of our method through three distinct configurations: the utilization of scholarly document abstract retrieval methods, golden scholarly documents abstract, and golden snippets. Our method evaluations demonstrate that our topic model-based document categorization outperforms existing methods, such as RYGH and bio-answer finder, which utilize a complex blend of techniques like BM25 [2], ElasticSearch [3], and various transformer models [4]. This suggests that a simpler yet fine-tuned approach can lead to more effective and cost-efficient solutions for biomedical information retrieval. The primary research question in this investigation was: How can optimized scholarly document abstract categorization and answer extraction methodologies improve the accuracy and efficiency of information retrieval in the biomedical domain? Addressing this question is vital, as it not only enhances the precision of answer extraction but also reduces the cognitive load on researchers seeking relevant information. Furthermore, our findings reveal that even distilled smaller language models like MiniLM [5] can effectively extract answers when fine-tuned on domain-specific data, particularly when focused on scholarly document abstracts rather than complete documents. While the comparison with the use of Large Language Models [6] instead of the smaller MiniLM is also an interesting research avenue, it is deemed out of the scope of this research. Overall our promising results suggest a potential shift in focus for future biomedical information retrieval methods, advocating for strategies that emphasize the utility of concise scholarly abstracts. In summary, the contributions of this research comprise: 1. A novel neuro-symbolic answer extraction methodology that combines optimized topic modelling and advanced machine learning techniques, effectively addressing the challenges in biomedical information retrieval, particularly in extracting answers from an expanding corpus of scholarly documents abstract. 2. A novel answer extraction methodology that combines optimized topic modelling and advanced machine learning techniques, effectively addressing the challenges in biomedical information retrieval, particularly in extracting answers from an expanding corpus of scholarly documents abstract. 3. A comprehensive evaluation of the proposed method across three configurations— scholarly document abstract retrieval, golden scholarly documents abstract, and golden snippets—demonstrating its superior performance over existing methods like RYGH and bio-answer finder, while highlighting the advantages of simplicity and domain-specific fine-tuning. 4. Insights into the effective use of distilled models, specifically MiniLM, for accurate answer extraction when fine-tuned on domain-specific data, along with recommendations for future biomedical information retrieval methods to focus on concise scholarly document abstracts for enhanced efficiency and accuracy. methods Phase Approach bio-answerfinder A, B Bio-AnswerFinder, ElasticSearch, Bio-ELECTRA, ELECTRA, BioBERT, SQuAD, wRWMD, BM25, LSTM, T5 bioinfo A, B BM25, ElasticSearch, distant learning, DeepRank, universal weighting passage mechanism (UPWM), PARADE-CNN, PubMedBERT LaRSA A, B ElasticSearch, BM25, SQuAD, Macro Passage Ranking, BioBERT, BoolQA, BART ELECTROBERT A, B ELECTRA, ALBERT, BioELECTRA, BERT NEUROSYM-BIOCAT A, B OVB-LDA, CMA-ES, MiniLM RYGH A BM25, BioBERT, PubMedBERT, T5, BERTMeSH, SciBERT gsl A BM25, BERT, dual-encoder BioNIR B sBERT, distance metrics KU-methods B BioBERT, data augmentation MQ B tf-idf, sBERT, DistilBERT Ir_sys B BERT, SQuAD1.0, SpanBERT, XLNet, PubMedBERT, BioELECTRA, BioALBERT, BART UDEL-LAB B BioM-ALBERT, Bio-ELECTRA, SQuAD MQU B BART, summarization NCU-IISR/AS-GIS B BioBERT, BERTScore, SQuAD, logistic-regression Table 1: Approaches used by BioASQ 10b Participants."
