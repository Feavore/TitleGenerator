URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.04876v1,Non-Euclidean Mixture Model for Social Network Embedding,"It is largely agreed that social network links are formed due to either homophily or social influence. Inspired by this, we aim at understanding the generation of links via providing a novel embedding-based graph formation model. Different from existing graph representation learning, where link generation probabilities are defined as a simple function of the corresponding node embeddings, we model the link generation as a mixture model of the two factors. In addition, we model the homophily factor in spherical space and the influence factor in hyperbolic space to accommodate the fact that (1) homophily results in cycles and (2) influence results in hierarchies in networks. We also design a special projection to align these two spaces. We call this model Non-Euclidean Mixture Model, i.e., NMM. We further integrate NMM with our non-Euclidean graph variational autoencoder (VAE) framework, NMM-GNN. NMM-GNN learns embeddings through a unified framework which uses non-Euclidean GNN encoders, non-Euclidean Gaussian priors, a non-Euclidean decoder, and a novel space unification loss component to unify distinct non-Euclidean geometric spaces. Experiments on public datasets show NMM-GNN significantly outperforms state-of-the-art baselines on social network generation and classification tasks, demonstrating its ability to better explain how the social network is formed.","Social networks are omnipresent because they are used for modeling interactions among users on social platforms. Social network analysis plays a key role in several applications, including detecting underlying communities among users [1], classifying people into meaningful social classes [2], and predicting user connectivity [3]. Most existing embedding models are designed based on the homophily aspect of social networks [4, 5]. They utilize the intuition that associated nodes in a social network imply feature similarity, and an edge is usually generated between similar nodes. Prior works have used shallow embedding models to represent homophily, like matrix factorization and random-walk (Section 2), which are parameter intensive and do not employ message passing. As an improvement, graph neural network (GNN) models (Section 2) have been proposed to more effectively capture homophily by representing a node through its local neighborhood context. However, research of RaRE [6] and work of [7] show homophily is insufficient, and social influence is also critical in forming connections. This is due to popular nodes having direct influence in forming links [8]. For example, in Twitter network, users tend to follow celebrities in addition to users who share similar interests [9]. Though RaRE jointly models both factors, it has limitations in modeling capabilities. Specifically, it is parameter intensive as each node embedding is fully parameterized through a Bayesian framework. Further, RaRE assumes graphs are transductive, limiting its performance in the practical inductive setting where new links not seen during training must be predicted. Moreover, nearly all works embedding social networks utilize a single zero-curvature Euclidean space, when in reality, network factors may create different topologies. Specifically, edges generated by homophily tend to form cycles [10], while edges generated by social influence tend to form tree structures [11, 12]. From Riemannian geometry, people have found that networks with cycles are best represented by spherical space embeddings [13], while tree structured networks are best represented by hyperbolic space embeddings [14]. Thus, an end-to-end model to bridge social network embeddings of distinct non-Euclidean geometric spaces is a promising direction. Our motivation is two-fold: (1) We aim to understand how the social network is generated e.g., which factors affect node connectivity and what topological patterns emerge in the network as a result. (2) Using our learning from (1), we aim to design a more realistic deep learning model to explain how the network is generated (inferring new connections). We summarize our contributions as follows: ‚Ä¢ We propose Graph-based Non-Euclidean Mixture Model (NMM) to explain social network generation. NMM represents nodes via joint influence by homophily (modeled in spherical space) and social influence (modeled in hyperbolic space), while seamlessly unifying embeddings via our space unification loss. ‚Ä¢ To our knowledge, we are also the first to couple NMM with a graph-based VAE learning framework, NMM-GNN. Specifically, we introduce a novel non-Euclidean VAE framework where node embeddings are learned with a powerful encoder of GNNs using spherical and hyperbolic spaces, non-Euclidean Gaussian priors, and unified non-Euclidean optimization. ‚Ä¢ Extensive experiments on several real-world datasets on large-scale social networks, Wikipedia networks, and attributed graphs demonstrate effectiveness of NMM-GNN in social network generation and classification, which outperforms state-of-the-art (SOTA) network embedding models."
https://arxiv.org/html/2411.04564v1,A Generalisation of Voter Model: Influential Nodes and Convergence Properties,"Consider an undirected graph Gùê∫Gitalic_G, representing a social network, where each node is blue or red, corresponding to positive or negative opinion on a topic. In the voter model, in discrete time rounds, each node picks a neighbour uniformly at random and adopts its colour. Despite its significant popularity, this model does not capture some fundamental real-world characteristics such as the difference in the strengths of individuals connections, individuals with neutral opinion on a topic, and individuals who are reluctant to update their opinion. To address these issues, we introduce and study a generalisation of the voter model.Motivating by campaigning strategies, we study the problem of selecting a set of seeds blue nodes to maximise the expected number of blue nodes after some rounds. We prove that the problem is NP-hard and provide a polynomial time approximation algorithm with the best possible approximation guarantee. Our experiments on real-world and synthetic graph data demonstrate that the proposed algorithm outperforms other algorithms.We also investigate the convergence properties of the model. We prove that the process could take an exponential number of rounds to converge. However, if we limit ourselves to strongly connected graphs, the convergence time is polynomial and the period (the number of states in convergence) divides the length of all cycles in the graph.","Humans constantly form and update their opinions on different topics, from minor subjects such as which movie to watch and which new caf√© to try to major matters such as which political party to vote for and which company to invest in. In the process of making such decisions, we tend to rely not only on our own personal judgment and knowledge, but also that of others, especially those whose opinion we value and trust. As a result, opinion diffusion, influence propagation, and (mis)-information spreading can affect different aspects of our lives from economy and defense to fashion and personal affairs. Recent years have witnessed a booming development of online social networking platforms like Facebook, WeChat, and Instagram. The enormous popularity of these platforms has led to fundamental changes in how humans share and form opinions. Social phenomena such as disagreement and polarisation that have existed in human societies for millennia, are now taking place in an online virtual world and are tightly woven into everyday life, with a substantial impact on society. There has been a growing demand for a quantitative understanding of how opinions form and diffuse because of the existence of social ties among a community‚Äôs members and how the underlying structure of a social network can influence this process, cf. Faliszewski et al. (2022); Elkind et al. (2009); Brill et al. (2016); Bredereck and Elkind (2017); G√§rtner and N. Zehmakan (2017). This would enable us to obtain better predictions of electoral results, control the effect of marketing and political campaigns, and in general advance our knowledge of the cognitive processes behind social influence. The motive to gain insights on how opinions are shaped and evolved in multi-agent systems has been the driving force behind an interdisciplinary research effort in diverse areas such as sociology Moussa√Ød et al. (2013), economics Jackson (2011), mathematics Balogh et al. (2012), physics Galam (2008), and computer science Chistikov et al. (2020). Within the field of computer science, especially computational social choice and algorithmic game theory, there has been a rising interest in developing and analyzing mathematical models which simulate the opinion diffusion in a network of individuals, cf. Bredereck and Elkind (2017); Out and Zehmakan (2021). Of course, in reality the opinion formation processes are too complex to be expressed in purely theoretical terms, but the goal is to shed some light on their general principles, which are otherwise hidden by the convolution of the full phenomenon. The following generic and abstract model is the foundation for most of the proposed opinion diffusion models. Consider a graph where each node has a colour and the nodes change their colour according to an updating rule, which is a function of the colour of the neighbouring nodes, in a defined order. The graph is meant to represent a social network, where the individuals are modeled as nodes and edges indicate relations between them, e.g., friendship, common interests, or advice. The colour of a node stands for its preference about a certain topic, e.g., an order over a set of candidates. In the very popular Voter Model, initially all nodes are either blue or red and then in each round, every node picks a random neighbour and adopts its colour. We generalise the voter model to capture several important real-world characteristics: ‚Ä¢ We relax the constraints of the graph being undirected and unweighted imposed in the original voter model. Directions are important for a realistic modelling since, for example in online social platforms, it is possible that one user follows the other user, but not the other way around. Furthermore, weights are utilised to model the strength of the relationships among individuals. ‚Ä¢ We allow uncoloured nodes in the initial colourings, which are nodes who are neither positive nor negative about the topic/product. ‚Ä¢ We allow some nodes to keep their colour unchanged. This accounts for stubborn or loyal individuals, who are not influenced by the opinion of their peers. Political parties and corporations frequently employ diverse strategies to persuade a specific segment of consumers on social media platforms to adopt a positive opinion about a specific product or topic. By harnessing the influence of these individuals on their social circles, a chain reaction of influence can be created, cf. Lin and Lui (2015); Myers and Leskovec (2012). This technique has emerged as a prominent method for promoting new ideas, products, and services, as it enables marketing or political campaigns to achieve extensive reach and exposure while keeping costs low. The question then becomes how to choose an initial subset of so-called early adopters to maximise the number of people that will eventually be reached, given some fixed budget. Motivated by this application, we study the problem of maximizing the expected number of blue nodes after some rounds by selecting a fixed number of initial blue nodes. We prove that the problem cannot be approximated better than (1‚àí1/e)11ùëí(1-1/e)( 1 - 1 / italic_e ), unless P=NP, and provide a polynomial time algorithm with such approximation ratio. In addition to the theoretical guarantee, the proposed algorithm outperforms centrality based algorithms on real-world graph data. It is worth to emphasise that the red nodes can be seen as nodes who have a negative opinion about the political party or the product or prefer the competitor party or company. In either case, the voter (and consequently our model) captures the setup where switching colours is free or inexpensive, for example changing opinions about a controversial topic or switching from one grocery store chain to another. Another important problem in the area of opinion diffusion models is determining the convergence properties of the opinion dynamics: Is convergence to stable states guaranteed and if yes, what are the upper and lower bounds on the convergence time? Since in most cases, such as ours, the opinion dynamics can be modelled as a Markov process, this problem is usually equivalent to determining the stationary distribution and convergence time of the corresponding Markov chain, cf. Frischknecht et al. (2013). We prove that the convergence time can be exponential in the general case, but it becomes polynomial for strongly connected graphs. Outline. We provide some basic definitions and problem formulations in Section 2. Then, a short overview of prior work and our contributions are given in Sections 3 and 4. The complexity and algorithms results on maximum adoption problem, plus the experimental findings, are provided in Section 5. Finally, the convergence properties of the model are analyzed in Section 6."
https://arxiv.org/html/2411.04936v1,Fed-LDR: Federated Local Data-infused Graph Creation with Node-centric Model Refinement,"The rapid acceleration of global urbanization has introduced novel challenges in enhancing urban infrastructure and services. Spatio-temporal data, integrating spatial and temporal dimensions, has emerged as a critical tool for understanding urban phenomena and promoting sustainability. In this context, Federated Learning (FL) has gained prominence as a distributed learning paradigm aligned with the privacy requirements of urban IoT environments. However, integrating traditional and deep learning models into the FL framework poses significant challenges, particularly in capturing complex spatio-temporal dependencies and adapting to diverse urban conditions. To address these challenges, we propose the Federated Local Data-Infused Graph Creation with Node-centric Model Refinement (Fed-LDR) algorithm. Fed-LDR leverages FL and Graph Convolutional Networks (GCN) to enhance spatio-temporal data analysis in urban environments. The algorithm comprises two key modules: (1) the Local Data-Infused Graph Creation (LDIGC) module, which dynamically reconfigures adjacency matrices to reflect evolving spatial relationships within urban environments, and (2) the Node-centric Model Refinement (NoMoR) module, which customizes model parameters for individual urban nodes to accommodate heterogeneity. Evaluations on the PeMSD4 and PeMSD8 datasets demonstrate Fed-LDR‚Äôs superior performance over six baseline methods. Fed-LDR achieved the lowest Mean Absolute Error (MAE) values of 20.15 and 17.30, and the lowest Root Mean Square Error (RMSE) values of 32.30 and 27.15, respectively, while maintaining a high correlation coefficient of 0.96 across both datasets. Notably, on the PeMSD4 dataset, Fed-LDR reduced MAE and RMSE by up to 81% and 78%, respectively, compared to the best-performing baseline FedMedian.","Global urbanization is accelerating at an unprecedented rate, presenting complex challenges and opportunities for enhancing urban infrastructure and services. As cities expand and evolve, the environments within them become increasingly intricate, marked by dynamic interactions that traditional data types often fail to capture [1]. In this context, spatio-temporal data‚Äîwhich integrates both spatial and temporal dimensions‚Äîhas emerged as a critical tool for urban planners and policymakers. This data offers a nuanced understanding of urban phenomena, including traffic patterns, fluctuations in population density, and evolving environmental impacts over time [2, 3]. The effective utilization of spatio-temporal data is essential for informed decision-making aimed at promoting sustainability and resilience in urban development [4]. Federated Learning (FL) has emerged as a promising paradigm, particularly suited for the Internet of Things (IoT) environments prevalent in urban settings. FL facilitates decentralized model training across diverse geographical locations, allowing the integration of various data sources while preserving data privacy [5, 6, 7]. This decentralized approach not only enhances model flexibility but also enables personalized adjustments tailored to local conditions. Given the rapidly urbanizing environments, FL naturally aligns with the decentralized and heterogeneous nature of urban data, making it an ideal solution for developing adaptable and privacy-preserving models in real-world, complex urban landscapes. While traditional computational models like regression analysis and agent-based models have been instrumental in understanding urban dynamics, integrating these models into the FL framework presents significant challenges, particularly in the analysis of spatio-temporal data within urban IoT environments. These traditional models are typically designed for analyzing static or relatively simple datasets, often focusing on specific variables or isolated snapshots in time. Although effective in certain contexts, they often fail to capture the intricate and evolving interdependencies of time and space that are inherent in urban systems [8]. Moreover, their static structure makes them ill-equipped to adapt to the continuous flux of urban life, thereby limiting their effectiveness in real-world applications [9, 10, 11, 12]. Deep learning has revolutionized spatio-temporal data analysis by providing powerful tools for recognizing and learning complex patterns from large datasets [13]. However, significant challenges remain in integrating deep learning with FL for the analysis of spatio-temporal data within urban IoT environments. The primary challenge is developing models that not only achieve high accuracy but also adapt effectively to the unique characteristics of various urban nodes [14, 15]. These nodes often exhibit substantial heterogeneity and evolving temporal dynamics, particularly in diverse urban settings. Consequently, the variability in local conditions can lead to suboptimal model performance [16]. To address these challenges, we propose the Federated Local Data-Infused Graph Creation with Node-centric Model Refinement (Fed-LDR) algorithm. Fed-LDR combines the distributed learning strengths of FL with the spatial analysis capabilities of Graph Convolutional Networks (GCN), which excel at capturing spatial dependencies in spatio-temporal data [17]. As illustrated in Figure 3, Fed-LDR comprises two key components: the Local Data-Infused Graph Creation (LDIGC) module and the Node-centric Model Refinement (NoMoR) module. LDIGC dynamically reconfigures adjacency matrices to reflect the continuous evolution of spatial relationships within urban environments, while NoMoR tailors model parameters for individual nodes, accommodating the heterogeneous nature of urban elements. This approach overcomes the limitations of static and centralized models, aligning with the inherently distributed and dynamic nature of urban data to enhance adaptability and protect privacy. By continuously adapting its learning framework based on localized, time-varying data inputs, Fed-LDR offers a more nuanced and context-aware analysis of urban dynamics, advancing the state-of-the-art in modeling complex, heterogeneous urban spatio-temporal phenomena. Our comprehensive experiments on the PeMSD4 and PeMSD8 datasets demonstrate the superior performance of Fed-LDR compared to six baseline models, including traditional FL approaches and their variants enhanced with our proposed modules. Fed-LDR achieved the lowest Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) on both datasets: MAE of 20.15 and RMSE of 32.30 for PeMSD4, and MAE of 17.30 and RMSE of 27.15 for PeMSD8. Notably, Fed-LDR maintained a high correlation coefficient (CORR) of 0.96 across both datasets, underscoring its robust predictive capabilities. These results represent significant improvements over traditional FL methods, with reductions in MAE and RMSE of up to 81% and 78%, respectively, compared to the best-performing baseline (FedMedian) on PeMSD4. Figure 1: Client-side processing Figure 2: Central server aggregation Figure 3: Overview of our Proposed Fed-LDR."
https://arxiv.org/html/2411.04681v1,A dynamical model of platform choice and online segregation,"In order to truly understand how social media might shape online discourses or contribute to societal polarization, we need refined models of platform choice, that is: models that help us understand why users prefer one social media platform over another. This study develops a dynamic model of platform selection, extending Social Feedback Theory by incorporating multi-agent reinforcement learning to capture how user decisions are shaped by past rewards across different platforms. A key parameter (Œºùúá\muitalic_Œº) in the model governs users‚Äô tendencies to either seek approval from like-minded peers or engage with opposing views. Our findings reveal that online environments can evolve into suboptimal states characterized by polarized, strongly opinionated echo chambers, even when users prefer diverse perspectives. Interestingly, this polarizing state coexists with another equilibrium, where users gravitate toward a single dominant platform, marginalizing other platforms into extremity. Using agent-based simulations and dynamical systems analysis, our model underscores the complex interplay of user preferences and platform dynamics, offering insights into how digital spaces might be better managed to foster diverse discourse.","Digital platforms have a profound impact on society, influencing how people communicate, access information, and express opinions (Boyd,, 2014). While these platforms have become integral to daily life, they are also increasingly scrutinized for their role in fostering polarization, the spread of misinformation, and growing mistrust in democratic institutions (Sunstein,, 2018; Tucker et al.,, 2018). The recent turmoil surrounding Twitter (now X) and the emergence of conglomerates like Meta‚Äîencompassing Facebook, Instagram, WhatsApp, and Threads‚Äîhas marked a new phase of public skepticism toward social media platforms, their leadership, and the algorithms driving user experiences. In response, users are becoming more discerning, gravitating toward platforms that better align with their personal communication preferences (Brady et al.,, 2023). One of the primary concerns surrounding digital platforms is their role in reinforcing polarization and creating echo chambers, where users predominantly engage with like-minded individuals (Sunstein,, 2018). This selective exposure not only amplifies existing opinions but also fosters ideological divides and undermines cross-cutting discourse (Stroud,, 2010). A key driver of this phenomenon lies in the cultural norms and values embedded within each platform, which shape the kinds of opinions that are promoted or marginalized. Platforms differ in their normative structures ‚Äì ranging from rigid moderation policies to more open, decentralized environments ‚Äì resulting in distinct platform cultures (Scharlach and Hallinan,, 2023; Van Raemdonck and Pierson,, 2022). Users actively navigate these environments, often selecting online spaces that align with their own communicative preferences, values, and even political identities (Garrett,, 2009; Knobloch-Westerwick,, 2015). At the same time, the market dynamics of platform ecosystems enable the constant emergence of new platforms, giving users the option to switch when a platform‚Äôs technical features or culture no longer suit their needs. This fluid platform ecosystem raises important questions about how user choices, shaped by social feedback and platform-specific norms, contribute to the formation of either polarized spaces or diverse, open environments. In particular, by means of their specific technical and social affordances, social media platforms might support key functions of the public sphere in liberal democracies. They might (1) facilitate the circulation of information and provide public access to information, (2) facilitate rational-critical debate among the public, (3) foster the creation and expression of collective identities, and (4) allow actors to coordinate and collectively perform actions (Willaert and Olbrich,, 2024). When choosing which platforms to actively engage with, users might thus seek out those social media that best support the functions of the public sphere they value most highly. A user interested in belonging to a specific group or collective, might for instance look for those platforms that host like-minded individuals. On the other hand, users who wish to challenge their assumptions and expose themselves to new information, or wish to engage in rational debate with others, might be drawn to platforms that offer a diversity of perspectives and opinions. This dynamic selection process highlights the interplay between user motivations and platform affordances, underscoring how individual choices can shape the broader online discourse and, potentially, the structure of the digital public sphere. To better understand the complex interplay between user behavior and online platform ecologies, this paper extends social feedback theory (SFT) by incorporating reinforcement learning (RL) to model adaptive user-platform interactions (Banisch and Olbrich,, 2019; Banisch et al.,, 2022). In SFT, user behavior is shaped by the social rewards or feedback individuals receive from others, making it an ideal framework for understanding how social media platforms influence opinion expression and choice. From Jacob and Banisch, (2023) we borrow an explicit representation of virtual environments. We refer to these different environments as platforms throughout the paper noticing that they might also relate to online spaces within a single large social network service. By integrating RL, we allow users to adapt their platform choices based on the rewards they have experienced, such as the affirmation of like-minded peers or exposure to opposing views. This approach captures the dynamic, evolving nature of online environments, where user preferences for different platforms co-evolve with the composition of the platform ecology (see Figure 1). In contrast to models that assume static decision-making (Granovetter,, 1978; Schelling,, 1971), our approach emphasizes how feedback loops between many user actions and platforms can lead to different system-wide outcomes ‚Äì ranging from polarized echo chambers to diverse, open spaces ‚Äì depending on the balance between social approval and diversity-seeking behaviors. In this way, our paper provides a more focused understanding of how platform markets and user behavior influence the evolution of polarized environments or more inclusive spaces. Figure 1: Platform choice models address the co-evolutionary process of users‚Äô preferences and decisions to engage with different platforms, and the resulting platform ecology in which these decisions take place. The model simulates a population of users, each holding one of two opposing opinions, who choose between a set of platforms for expressing their views. Users are influenced by two primary rewards: social approval, which is derived from interacting with like-minded individuals, and diversity, which comes from engaging with opposing views. Each platform is characterized by the proportion of users holding each opinion and the level of activity on the platform, which affects user satisfaction. A key parameter Œºùúá\muitalic_Œº governs the relative importance users place on social approval versus diversity. Through RL, users adapt their platform choices over time, updating their platform preferences based on the rewards they experience. This leads to dynamic changes in platform composition as users gravitate toward platforms that maximize their satisfaction. As the simulation evolves, we observe the formation of two distinct platform configurations. On the one hand, the simulation forms polarized echo chambers, where users cluster around like-minded peers. On the other hand, we see the emergence of a single very large platform where different opinions co-exist and other strongly opinionated platforms are marginal. Remarkably, both configurations are stable in a specific parameter range Œºùúá\muitalic_Œº. Our model thus captures the feedback loops between individual behavior and platform dynamics, revealing how small changes in user preferences or platform features can lead to vastly different systemic outcomes. These results echo a key insight from Schelling‚Äôs seminal work on segregation (Schelling,, 1969, 1971), which demonstrated how individual motivations can aggregate into unintended macro-level outcomes. In Schelling‚Äôs model, even mild preferences for like-minded neighbors led to sharp spatial segregation, even though individuals did not explicitly seek such extremes. Like Schelling‚Äôs work, our model highlights how individual motivations ‚Äì in our case, for social approval or diversity ‚Äì aggregate to produce collective outcomes such as polarized platforms or diverse spaces. The model parameter, Œºùúá\muitalic_Œº, governs the balance between social approval and diversity rewards for the individual users. When Œº<0.5ùúá0.5\mu<0.5italic_Œº < 0.5, polarization is the dominant outcome as users prioritize social approval. As Œºùúá\muitalic_Œº increases beyond 0.5, the system favors global diversity as the optimal outcome, yet polarization persists as the only stable outcome for a range of Œºùúá\muitalic_Œº values between 0.5 and 0.64. Strong motivations for diversity (0.64<Œº<0.8750.64ùúá0.8750.64<\mu<0.8750.64 < italic_Œº < 0.875) eventually favor a globally optimal equilibrium, where a single mega-platform features both opinion groups and the remaining platforms are marginalized to extremity. Nevertheless, even in this setting suboptimal equilibria of online segregation remain a stable outcome of the model (for Œº<0.79ùúá0.79\mu<0.79italic_Œº < 0.79). These findings underscore how feedback loops between user behavior and platform characteristics can drive polarization, even when users genuinely value diversity, recalling Schelling‚Äôs demonstration of how small individual preferences for like-minded others can lead to unexpected systemic segregation. This paper makes several key contributions to the study of platform dynamics and user behavior on social media. First, this paper extends SFT to model how users adapt their communication choices in a dynamic platform environment. Unlike most models of polarization that focus on opinion dynamics within fixed networks (Flache et al.,, 2017; Banisch and Olbrich,, 2019; Lorenz et al.,, 2021), our model explicitly captures how users select platforms based on social feedback. By simulating the evolving interplay between user preferences and platform features, the model provides a novel framework for studying how online segregation emerges from individual communication decisions. Second, we conduct a rigorous mathematical analysis of the ""tipping points"" in the model using bifurcation analysis. This approach identifies critical transitions in system behavior, specifically how variations in the balance between social approval and diversity-seeking behavior (governed by parameter Œºùúá\muitalic_Œº) lead to different platform outcomes ‚Äì ranging from polarized echo chambers to diverse spaces. This analysis offers new insights into the parameter regimes where polarization persists, and where more diverse, inclusive environments can emerge, providing a theoretical basis for understanding platform segregation and diversity dynamics. Third, the model is highly flexible and can accommodate a wide range of attributes, beyond political or ideological stances. ""Opinion"" in the model can refer to cultural values, norms, or even preferences regarding communication standards, such as the toxicity of language. Furthermore, the model can be applied to posting decisions within platforms like Reddit, where users choose between different communities (called ""subreddits"") based on feedback such as upvotes or comments. This flexibility makes the modeling framework presented in this paper a versatile tool for fine-tuning and calibrating models to specific online settings and for exploring how platform design and user incentives interact across different contexts. In this paper, we focus on rigorous theoretical understanding of a simple model with fixed binary opinions and homogeneous motivations Œºùúá\muitalic_Œº to explain basic dynamics of online segregation. Future model iteration attuned to real platforms will provide actionable insights for platform designers and policymakers aiming to foster more inclusive digital spaces."
https://arxiv.org/html/2411.04655v1,Centrality Graph Shift Operatorsfor Graph Neural Networks,"Graph Shift Operators (GSOs), such as the adjacency and graph Laplacian matrices, play a fundamental role in graph theory and graph representation learning. Traditional GSOs are typically constructed by normalizing the adjacency matrix by the degree matrix, a local centrality metric. In this work, we instead propose and study Centrality GSOs (CGSOs), which normalize adjacency matrices by global centrality metrics such as the PageRank, kùëòkitalic_k-core or count of fixed length walks. We study spectral properties of the CGSOs, allowing us to get an understanding of their action on graph signals. We confirm this understanding by defining and running the spectral clustering algorithm based on different CGSOs on several synthetic and real-world datasets. We furthermore outline how our CGSO can act as the message passing operator in any Graph Neural Network and in particular demonstrate strong performance of a variant of the Graph Convolutional Network and Graph Attention Network using our CGSOs on several real-world benchmark datasets. Our code is publicly available at: https://github.com/abbahaddou/CGSO.","We propose and study a new family of operators defined on graphs that we call Centrality Graph Shift Operators (CGSOs). To insert these into the rich history of matrices representing graphs and centrality metrics, the two concepts married in CGSOs, we begin by recalling major advances in these two topics in turn (readers interested purely in recent developments in Graph Representation Learning and Graph Neural Networks are recommended to begin reading in Paragraph 3 of this section). The study of graph theory and with it the use of matrices to represent graphs have a long-standing history. Graph theory is often said to have its origins in 1736 when Leonard Euler posed and solved the K√∂nigsberg bridge problem (Euler, 1736). His solution did not involve any matrix calculus. In fact, it seems that the first matrix defined to represent graph structures is the incidence matrix defined by Henri Poincar√© in 1900 (Poincar√©, 1900). It is difficult to pinpoint the first definition of adjacency matrices, but by 1936 when the first book on the topic of graph theory was published by D√©nes K√∂nig adjacency matrices had certainly been defined and began to be used to solve graph theoretic problems (K√∂nig, 1936). Two seemingly concurrent works in 1973 defined an additional matrix structure to represent graphs that later became known as the unnormalized graph Laplacian (Donath & Hoffman, 1973; Fiedler, 1973). Then, it was Fan Chung in her book ‚ÄúSpectral Graph Theory‚Äù published in 1997 who extensively characterized the spectral properties of normalized Laplacians (Chung, 1997). In the emerging field of Graph Signal Processing (GSP) (Sandryhaila & Moura, 2013; Ortega et al., 2018) these different graph representation matrices were all defined to belong to a more general family of operators defined on graphs, the Graph Shift Operators (GSOs). GSOs currently play a crucial role in graph representation learning research, since the choice of GSO, used to represent a graph structure, corresponds to the choice of message passing function in the currently much-used Graph Neural Network (GNN) models. In parallel to advances in graph representation via matrices, centrality metrics have proved to be insightful in the study of graphs. Chief among them is the success of the PageRank centrality criterion revealing the significance of certain webpages (Brin & Page, 1998) and playing a role in the formation of what is now one of the largest companies worldwide. But also an even older metric, the kùëòkitalic_k-core centrality (Seidman, 1983; Malliaros et al., 2020), as well as the degree centrality, closeness centrality, and betweenness centrality, have proven to be impactful in revealing key structural properties of graphs (Freeman, 1977; Zhang & Luo, 2017). A commonality of the most frequently used GSOs is their property to encode purely local information in the graph, with the adjacency matrix encoding neighborhoods in the graph and the graph Laplacians relying on the node degree, a local centrality metric, to normalize the adjacency matrix. In this work, we study a novel class of GSOs, the Centrality GSOs (CGSOs) that arise from the normalization of the adjacency matrix by centrality metrics such as the PageRank, kùëòkitalic_k-core and the count of fixed length walks emanating from a given node. Our CGSOs introduce global information into the graph representation without altering the connectivity pattern encoded in the original GSO and therefore, maintain the sparsity of the adjacency matrix. We provide several theorems characterizing the spectral properties of our CGSOs. We confirm the intuition gained from our theoretical study by running the spectral clustering algorithm on the basis of our CGSOs on 1) synthetic graphs that are generated from a stochastic blockmodel in which each block is sampled from the Barrabasi-Albert model and 2) the real-world Cora graph in which we aim to recover the partition provided by the kùëòkitalic_k-core number of each node. We will furthermore describe how our CGSOs can be inserted as the message passing operator into any GNN and observe strong performance of the resulting GNNs on real-world benchmark datasets. In particular, our contributions can be summarized as follows, (i) we define Centrality GSOs, a novel class of GSOs based on the normalization of the adjacency matrix with different centrality metrics, such as the degree, PageRank score, kùëòkitalic_k-core number, and the count of walks of a fixed length, (ii) we conduct a comprehensive spectral analysis to unveil the fundamental properties of the CGSOs. Our gained understanding of the benefits of CGSOs is confirmed by running the spectral clustering algorithm using our CGSOs on synthetic and real-world graphs, (iii) we incorporate the proposed CGSOs within GNNs and evaluate performance of a Graph Convolutional Network and Graph Attention Network v2 with a CGSO message passing operator on several real-world datasets."
https://arxiv.org/html/2411.03333v1,"Analysis of Bipartite Networks in Anime Series:Textual Analysis, Topic Clustering, and Modeling","This article analyzes a specific bipartite network that shows the relationships between users and anime, examining how the descriptions of anime influence the formation of user communities. In particular, we introduce a new variable that quantifies the frequency with which words from a description appear in specific word clusters. These clusters are generated from a bigram analysis derived from all descriptions in the database. This approach fully characterizes the dynamics of these communities and shows how textual content affect the cohesion and structure of the social network among anime enthusiasts. Our findings suggest that there may be significant implications for the design of recommendation systems and the enhancement of user experience on anime platforms.","The analysis of bipartite networks is a valuable method for studying the relationships between two sets of individuals. For instance, this approach can be utilized on websites that reflect users‚Äô interests in books or music, as well as on entertainment platforms where there is a connection between users and the movies they have viewed. Furthermore, we can examine the relationships between actors and the films they have participated in, as outlined in studies on bipartite networks and their application in modeling complex and collaborative systems (Wasserman and Faust,, 1994). This approach is particularly useful for platforms that facilitate interactions between users and products, enabling the exploration of these relationships using statistical models. One example involves modeling the choices made by customers on a specific platform or in a particular market. For instance, a study conducted in Ghose and Ipeirotis, (2011) reexamined the impact of reviews on economic outcomes, such as product sales, and explored textual factors like subjectivity, readability, and spelling errors in the reviews to identify important textual characteristics. Descriptions represent a relevant source of information for all platforms concerning product consumption, offering valuable insights into the features and attributes of the items available. Composed of carefully selected words, such descriptions can be subjected to rigorous analysis using several text analysis techniques. By employing various statistical methods, researchers can identify distinctive patterns and trends emerging from the vocabulary and phrasing utilized in these descriptions. This analysis not only enhances the understanding of consumer perceptions but also facilitates the identification of key factors that influence purchasing decisions and overall user engagement (Manning et al.,, 2008). Descriptions, like any other nodal variable‚Äîreferring to attributes possessed by individuals within a network‚Äîcan influence the structural features of the network to which they are embedded. Research in social network analysis shows how nodal characteristics shape network structures and influence connection patterns (Jackson,, 2008). To assess the influence of a nodal variable within a network, one effective model is the Exponential Random Graph Model (ERGM; e.g., Frank and Strauss, 1986a , Wasserman and Pattison, 1996, Robins et al., 2007a ), which allows for inferences about these nodal variables (Hunter et al.,, 2008). Here, we analyze a dataset containing 16,214 anime series along with the preferences of 325,000 users, intentionally excluding those featuring adult content. Of these 16,214 records, 15,497 include complete descriptions. This publicly available data can be accessed at https://github.com/Hernan4444/MyAnimeList-Database and https://www.kaggle.com/datasets/hernan4444/anime-recommendation-database-2020. The primary objective of this article, in addition to addressing several technical challenges and computational issues, is to evaluate the influence of anime series descriptions on the formation of large user communities surrounding these series. This is particularly important as it helps us understand how anime series descriptions shape the formation of large user communities, providing valuable insights into crafting descriptions that effectively promote a series. Moreover, it is evident that this approach can be applied to any platform utilizing descriptions in a similar way. Understanding how descriptions impact user engagement and community formation enables platforms to optimize their content strategies. For instance, social media and streaming services can create more engaging descriptions that inform users and foster community interaction, ultimately enhancing user retention and experience This article is structured as follows: Section 2 presents the state of the art and relevant instances related to our research. Section 3 outlines the methodology and provides technical details necessary for conducting our analyses. Section 4 details the case study. Finally, Section 5 presents our main findings along with suggestions for future research directions."
https://arxiv.org/html/2411.03331v1,"Hypergraphs as Weighted Directed Self-Looped Graphs:Spectral Properties, Clustering, Cheeger Inequality","Hypergraphs naturally arise when studying group relations and have been widely used in the field of machine learning. There has not been a unified formulation of hypergraphs, yet the recently proposed edge-dependent vertex weights (EDVW) modeling [7] is one of the most generalized modeling methods of hypergraphs, i.e., most existing hypergraphs can be formulated as EDVW hypergraphs without any information loss to the best of our knowledge. However, the relevant algorithmic developments on EDVW hypergraphs remain nascent: compared to spectral graph theories, the formulations are incomplete, the spectral clustering algorithms are not well-developed, and one result regarding hypergraph Cheeger Inequality is even incorrect. To this end, deriving a unified random walk-based formulation, we propose our definitions of hypergraph Rayleigh Quotient, NCut, boundary/cut, volume, and conductance, which are consistent with the corresponding definitions on graphs. Then, we prove that the normalized hypergraph Laplacian is associated with the NCut value, which inspires our HyperClus-G algorithm for spectral clustering on EDVW hypergraphs. Finally, we prove that HyperClus-G can always find an approximately linearly optimal partitioning in terms of Both NCut111The NCut of the returned partition ùí©ùí©\mathcal{N}caligraphic_N and the optimal NCut of any partition ùí©‚àósuperscriptùí©\mathcal{N}^{*}caligraphic_N start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT satisfy ùí©‚â§O‚Å¢(ùí©‚àó)ùí©ùëÇsuperscriptùí©\mathcal{N}\leq O(\mathcal{N}^{*})caligraphic_N ‚â§ italic_O ( caligraphic_N start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ). and conductance 222The conductance of the returned partition Œ¶Œ¶\Phiroman_Œ¶ and the optimal conductance Œ¶‚àósuperscriptŒ¶\Phi^{*}roman_Œ¶ start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT satisfy Œ¶‚â§O‚Å¢(Œ¶‚àó)Œ¶ùëÇsuperscriptŒ¶\Phi\leq O(\Phi^{*})roman_Œ¶ ‚â§ italic_O ( roman_Œ¶ start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ). Additionally, we provide extensive experiments to validate our theoretical findings from an empirical perspective.","Higher-order relations are ubiquitous in nature, such as co-authorship [14, 48, 40], interactions between multiple proteins or chemicals [13, 47], items that are liked by the same person [49, 46], and interactions between multiple species in an ecosystem [20, 38]. Hypergraphs, extended from graphs, with the powerful capacity to model group interactions (i.e., higher-order relations), show extraordinary potential to be applied to many real-world tasks where the connections are beyond pair-wise. Therefore, hypergraphs have been used widely in recommendation systems [56, 36, 18], information retrieval [27, 53, 35] and link prediction [26, 12]. Hypergraphs modeled by edge-dependent vertex weights (EDVW) were necessitated in a recent work [7], with a motivating example that in citation networks, each scholar (i.e., vertex) may contribute differently to each co-authored publication (i.e., hyperedge). The authors show that hypergraphs with edge-independent vertex weights (EIVW) do not actually utilize the higher-order relations for the following two reasons. First, the hypergraph Laplacian matrix proposed by the seminal work [52], which serves as a basis of many follow-up algorithms, is equal to the Laplacian matrix of a closely related graph with only pair-wise relations. In this way, all the linear Laplacian operators utilize only pair-wise relationships between vertices [1]. Second, many hypergraph algorithms [37, 31, 4] are based on random walks [42, 9, 34], but it has been proved that for any EIVW hypergraph, there exists a weighted pair-wise graph on which a random walk is equivalent to that on the original hypergraph [7]. Table 1: Properties of graph models/formulations. EDVW hypergraphs generalized EIVW hypergraphs by allowing each hyperedge to distribute its vertex weights, bringing better formulation flexibility. Modeling/Formulation undirected graphs EIVW hypergraphs EDVW hypergraphs edge/hyperedge weights ‚àösquare-root\surd‚àö ‚àösquare-root\surd‚àö ‚àösquare-root\surd‚àö vertex weights ‚àösquare-root\surd‚àö ‚àösquare-root\surd‚àö ‚àösquare-root\surd‚àö hyperedges √ó\times√ó ‚àösquare-root\surd‚àö ‚àösquare-root\surd‚àö edge-dependent vertex weights √ó\times√ó √ó\times√ó ‚àösquare-root\surd‚àö Figure 1: Undirected graphs ‚äÇ\subset‚äÇ EIVW hypergraphs ‚äÇ\subset‚äÇ EDVW hypergraphs. Each undirected graph can be reformulated to EIVW hypergraph by regarding each pair-wise edge as a hyperedge; each EIVW hypergraph can be reformulated to EDVW hypergraph by setting each vertex‚Äôs weight to be the same across hyperedges, yet allowing different vertices to have different weights. In nature, ‚ÄùEDVW hypergraph‚Äù is not a special case of hypergraphs, but a more generalized way to model hypergraphs (Figure 1). Any algorithm designed for EDVW hypergraphs, taking EDVW inputs, also works on typical (EIVW) hypergraphs by setting all the EDVW to 1 (i.e., independent w.r.t. edge). In other words, the properties and algorithms on EDVW-formulated hypergraphs can be applied to most hypergraphs. In this paper, we focus on further developing the incomplete yet fundamental spectral theories for EDVW hypergraphs, with a straightforward application on spectral clustering, a.k.a., k-way global partitioning, where typically k=2ùëò2k=2italic_k = 2. To be specific, k-way global partitioning aims to partition an entire graph into kùëòkitalic_k clusters, where the vertices in one cluster are densely connected within this cluster while having sparser connections to vertices outside this cluster. On the one hand, although the spectral theories and spectral clustering on graphs have been well studied [10], converting the hypergraphs to graphs and applying those methods may ignore the higher-order relations and result in sub-optimal results [44]. On the other hand, despite the advantage of EDVW modeling in terms of utilizing high-order relations, directly developing a spectral clustering algorithm on EDVW hypergraphs is still an open question. To this end, for the first time, we propose a provably linearly optimal spectral clustering algorithm on EDVW hypergraphs, together with theoretical analysis concerning the Rayleigh Quotient, Normalized Cut (i.e., NCut), and conductance. In the context of EDVW hypergraphs, we bridge the eigensystem of Laplacian with the NCut value through our proposed Rayleigh Quotient. The proposed algorithm can also be applied to EIVW hypergraphs by setting all the vertex weights to 1, thus works generally for all hypergraphs. 1.1 Main Results In this paper, we further develop the spectral hypergraph theory for EDVW hypergraphs, and then study global partitioning on EDVW hypergraphs. Theorem 1. (algebraic connections between hypergraph NCut, Rayleigh Quotient and Laplacian) Given any hypergraph in the EDVW formatting ‚Ñã=(ùí±,‚Ñ∞,œâ,Œ≥)‚Ñãùí±‚Ñ∞ùúîùõæ\mathcal{H}=(\mathcal{V},\mathcal{E},\omega,\gamma)caligraphic_H = ( caligraphic_V , caligraphic_E , italic_œâ , italic_Œ≥ ) with positive edge weights œâ‚Å¢(‚ãÖ)>0ùúî‚ãÖ0\omega(\cdot)>0italic_œâ ( ‚ãÖ ) > 0 and non-negative edge-dependent vertex weights Œ≥e‚Å¢(‚ãÖ)subscriptùõæùëí‚ãÖ\gamma_{e}(\cdot)italic_Œ≥ start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ( ‚ãÖ ) for any e‚àà‚Ñ∞ùëí‚Ñ∞e\in\mathcal{E}italic_e ‚àà caligraphic_E, define Normalized Cut N‚Å¢C‚Å¢u‚Å¢t‚Å¢(‚ãÖ)ùëÅùê∂ùë¢ùë°‚ãÖNCut(\cdot)italic_N italic_C italic_u italic_t ( ‚ãÖ ), Volume of a vertex set v‚Å¢o‚Å¢l‚Å¢(‚ãÖ)ùë£ùëúùëô‚ãÖvol(\cdot)italic_v italic_o italic_l ( ‚ãÖ ), Rayleigh Quotient R‚Å¢(‚ãÖ)ùëÖ‚ãÖR(\cdot)italic_R ( ‚ãÖ ), Laplacian LùêøLitalic_L, and stationary distribution matrix Œ†Œ†\Piroman_Œ† as Definition 10, 8, 5, 13, and 4. For any vertex set ùíÆ‚äÜùí±ùíÆùí±\mathcal{S}\subseteq\mathcal{V}caligraphic_S ‚äÜ caligraphic_V, we define a |ùí±|ùí±|\mathcal{V}|| caligraphic_V |-dimensional vector xùë•xitalic_x such that, x‚Å¢(u)=v‚Å¢o‚Å¢l‚Å¢(ùíÆ¬Ø)v‚Å¢o‚Å¢l‚Å¢(ùíÆ),‚àÄu‚ààùíÆ,x‚Å¢(u¬Ø)=‚àív‚Å¢o‚Å¢l‚Å¢(ùíÆ)v‚Å¢o‚Å¢l‚Å¢(ùíÆ¬Ø),‚àÄu¬Ø‚ààùíÆ¬Ø.formulae-sequenceùë•ùë¢ùë£ùëúùëô¬ØùíÆùë£ùëúùëôùíÆformulae-sequencefor-allùë¢ùíÆformulae-sequenceùë•¬Øùë¢ùë£ùëúùëôùíÆùë£ùëúùëô¬ØùíÆfor-all¬Øùë¢¬ØùíÆ\begin{split}x(u)&=\sqrt{\frac{vol(\bar{\mathcal{S}})}{vol(\mathcal{S})}},% \leavevmode\nobreak\ \forall\,\,u\in\mathcal{S},\\ x(\bar{u})&=-\sqrt{\frac{vol(\mathcal{S})}{vol(\bar{\mathcal{S}})}},% \leavevmode\nobreak\ \forall\,\,\bar{u}\in\bar{\mathcal{S}}.\\ \end{split}start_ROW start_CELL italic_x ( italic_u ) end_CELL start_CELL = square-root start_ARG divide start_ARG italic_v italic_o italic_l ( over¬Ø start_ARG caligraphic_S end_ARG ) end_ARG start_ARG italic_v italic_o italic_l ( caligraphic_S ) end_ARG end_ARG , ‚àÄ italic_u ‚àà caligraphic_S , end_CELL end_ROW start_ROW start_CELL italic_x ( over¬Ø start_ARG italic_u end_ARG ) end_CELL start_CELL = - square-root start_ARG divide start_ARG italic_v italic_o italic_l ( caligraphic_S ) end_ARG start_ARG italic_v italic_o italic_l ( over¬Ø start_ARG caligraphic_S end_ARG ) end_ARG end_ARG , ‚àÄ over¬Ø start_ARG italic_u end_ARG ‚àà over¬Ø start_ARG caligraphic_S end_ARG . end_CELL end_ROW (1) then,‚Å¢N‚Å¢C‚Å¢u‚Å¢t‚Å¢(ùíÆ,ùíÆ¬Ø)=12‚Å¢R‚Å¢(x)=xT‚Å¢L‚Å¢xxT‚Å¢Œ†‚Å¢xthen,ùëÅùê∂ùë¢ùë°ùíÆ¬ØùíÆ12ùëÖùë•superscriptùë•ùëáùêøùë•superscriptùë•ùëáŒ†ùë•\textit{then,}\,\,NCut(\mathcal{S},\bar{\mathcal{S}})=\frac{1}{2}R(x)=\frac{x^% {T}Lx}{x^{T}\Pi x}then, italic_N italic_C italic_u italic_t ( caligraphic_S , over¬Ø start_ARG caligraphic_S end_ARG ) = divide start_ARG 1 end_ARG start_ARG 2 end_ARG italic_R ( italic_x ) = divide start_ARG italic_x start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_L italic_x end_ARG start_ARG italic_x start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT roman_Œ† italic_x end_ARG (2) This is the first work regarding the Rayleigh Quotient on hypergraphs. Inspired by this Theorem, we develop a spectral clustering algorithm HyperClus-G to optimize the NCut value by loosing the combinatorial optimization constraint. Theorem 2. (Hypergraph Spectral Clustering Algorithm) There exists a algorithm for hypergraph spectral clustering that can be applied to EDVW-formatted hypergraphs, and always returns approximately linearly optimal clustering in terms of Normalized Cut and conductance. In other words, approximately, the NCut of the returned partition ùí©ùí©\mathcal{N}caligraphic_N and the optimal NCut of any partition ùí©‚àósuperscriptùí©\mathcal{N}^{*}caligraphic_N start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT satisfy ùí©‚â§O‚Å¢(ùí©‚àó)ùí©ùëÇsuperscriptùí©\mathcal{N}\leq O(\mathcal{N}^{*})caligraphic_N ‚â§ italic_O ( caligraphic_N start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ). We name this algorithm as HyperClus-G. The pseudo code of HyperClus-G is given in Algorithm 1. Moreover, to extend the hypergraph spectral theory, for the first time we give a complete proof regarding the hypergraph Cheeger Inequality. In the mean time, by proving Theorem 3, the previous result on hypergraph Cheeger Inequality (Theorem 5.1 in [7]) is incorrect as it refers to the eigenvector of unnormalized hypergraph Laplacian. Theorem 3. (Hypergraph Cheeger Inequality) Let ‚Ñã=(ùí±,‚Ñ∞,œâ,Œ≥)‚Ñãùí±‚Ñ∞ùúîùõæ\mathcal{H}=(\mathcal{V},\mathcal{E},\omega,\gamma)caligraphic_H = ( caligraphic_V , caligraphic_E , italic_œâ , italic_Œ≥ ) be any hypergraph in the EDVW formatting with positive edge weights œâ‚Å¢(‚ãÖ)>0ùúî‚ãÖ0\omega(\cdot)>0italic_œâ ( ‚ãÖ ) > 0 and non-negative edge-dependent vertex weights Œ≥e‚Å¢(‚ãÖ)subscriptùõæùëí‚ãÖ\gamma_{e}(\cdot)italic_Œ≥ start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ( ‚ãÖ ) for any e‚àà‚Ñ∞ùëí‚Ñ∞e\in\mathcal{E}italic_e ‚àà caligraphic_E. Define Œ¶‚Å¢(‚Ñã)=minùíÆ‚äÜùí±Œ¶‚Å¢(ùíÆ)Œ¶‚ÑãsubscriptùíÆùí±Œ¶ùíÆ\Phi(\mathcal{H})=\mathop{\min}_{\mathcal{S}\subseteq\mathcal{V}}{\Phi(% \mathcal{S})}roman_Œ¶ ( caligraphic_H ) = roman_min start_POSTSUBSCRIPT caligraphic_S ‚äÜ caligraphic_V end_POSTSUBSCRIPT roman_Œ¶ ( caligraphic_S ). Then the second smallest eigenvector ŒªùúÜ\lambdaitalic_Œª of the normalized hypergraph Laplacian Œ†‚àí12‚Å¢L‚Å¢Œ†‚àí12superscriptŒ†12ùêøsuperscriptŒ†12\Pi^{-\frac{1}{2}}L\Pi^{-\frac{1}{2}}roman_Œ† start_POSTSUPERSCRIPT - divide start_ARG 1 end_ARG start_ARG 2 end_ARG end_POSTSUPERSCRIPT italic_L roman_Œ† start_POSTSUPERSCRIPT - divide start_ARG 1 end_ARG start_ARG 2 end_ARG end_POSTSUPERSCRIPT satisfies Œ¶‚Å¢(‚Ñã)22‚â§Œª‚â§2‚Å¢Œ¶‚Å¢(‚Ñã)Œ¶superscript‚Ñã22ùúÜ2Œ¶‚Ñã\frac{\Phi(\mathcal{H})^{2}}{2}\leq\lambda\leq 2\Phi(\mathcal{H})divide start_ARG roman_Œ¶ ( caligraphic_H ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG 2 end_ARG ‚â§ italic_Œª ‚â§ 2 roman_Œ¶ ( caligraphic_H ) (3) In fact, this theorem shows that our HyperClus-G is also approximately linearly optimal in terms of conductance. In other words, the conductance of the returned cluster Œ¶Œ¶\Phiroman_Œ¶ and the optimal conductance Œ¶‚àósuperscriptŒ¶\Phi^{*}roman_Œ¶ start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT satisfy Œ¶‚â§O‚Å¢(Œ¶‚àó)Œ¶ùëÇsuperscriptŒ¶\Phi\leq O(\Phi^{*})roman_Œ¶ ‚â§ italic_O ( roman_Œ¶ start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ). It is worth mentioning that the previous non-proved result in [7] regarding hypergraph Cheeger Inequality now can be proved by using normalized Laplacian instead of the conjecture of combinatorial Laplacian. Technical Overview. Given the EDVW modeling, the relevant algorithmic development still remains in a nascent stage, which hinders the application of hypergraphs in many real-world scenarios. To this end, we first re-analyze the random walks on EDVW hypergraphs, then propose the HyperClus-G for hypergraph partitioning. Finally, we prove the approximation of normalized cut, as well as the upper bound of NCut and conductance. The key insight from the previous work [7] is to model the hypergraphs similar to directed graphs through the equivalence of random walks. Unlike classical graph theory, such directed graphs are edge-weighted, node-weighted, and contain self-loops. In this work, inspired by the definitions of Rayleigh Quotient, NCut, boundary/cut, volume, and conductance in graphs, we develop these definitions in the context of EDVW hypergraphs. We show that Theorem 1 and Theorem 3, properties that hold for graphs, still hold for hypergraphs using our unified definitions. From Theorem 3, we can further prove that our proposed HyperClus-G is approximately linearly optimal in terms of both NCut and conductance. Our Appendix contains supplementary contents, such as trivial proofs and experimental details. Paper Organization. This paper is organized as follows. In Section 2, we introduce necessary notations and our definitions regarding hypergraphs. In Section 3, we introduce our definition of hypergraph Rayleigh Quotient and show its connection with the Laplacian and NCut. Then, we propose our HyperClus-G inspired from such connection. In section 4, we give complete proof regarding hypergraph Cheeger Inequality, then show the linear optimality of our HyperClus-G in terms of both NCut and conductance. In Section 5, we analyze the complexity of our algorithms. Finally, in Section 6, we prepare comprehensive experiments to validate our theoretical findings. 1.2 Other Related Works Early Hypergraphs [6] typically model graph structures and do not allow node or hyperedge weights. Later formulations [52] start to allow hyperedge weights. Among various choices of modeling hypergraphs [6, 52, 15, 33], EDVW modeling [7] shows great generalization because it allows both hyperedge weights and node weights. Many hypergraph-related techniques have been proposed [17, 3, 30], while very few of them involve EDVW hypergraphs. Inhomogeneous hypergraph partitioning was proposed in [32]. Later on, authors of [33] proposed submodular hypergraphs, a special group of weighted hypergraphs, and analyzed their spectral clustering. A recent work [23] demonstrates how random walks with EDVW are used to construct the EDVW hypergraph Laplacian. However, it stops at the construction of the Laplacian and only uses partial information encoded in the Laplacian for clustering. Some recent works [2, 43, 11] study partitioning edge-colored hypergraphs. There are several research works [57, 54, 55] targeting EDVW hypergraph global partitioning, but they do not actually directly work on EDVW hypergraphs, but are based on submodular hypergraphs; They propose to construct a submodular hypergraph from the given EDVW hypergraph [33], then apply learning-based approaches to optimize the global partitioning objective. Several works also study specific applications of hypergraph clustering [28, 5]."
https://arxiv.org/html/2411.03865v1,AdaSociety: An Adaptive Environment with Social Structures for Multi-Agent Decision-Making,"Traditional interactive environments limit agents‚Äô intelligence growth with fixed tasks. Recently, single-agent environments address this by generating new tasks based on agent actions, enhancing task diversity. We consider the decision-making problem in multi-agent settings, where tasks are further influenced by social connections, affecting rewards and information access. However, existing multi-agent environments lack a combination of adaptive physical surroundings and social connections, hindering the learning of intelligent behaviors. To address this, we introduce AdaSociety, a customizable multi-agent environment featuring expanding state and action spaces, alongside explicit and alterable social structures. As agents progress, the environment adaptively generates new tasks with social structures for agents to undertake. In AdaSociety, we develop three mini-games showcasing distinct social structures and tasks. Initial results demonstrate that specific social structures can promote both individual and collective benefits, though current reinforcement learning and LLM-based algorithms show limited effectiveness in leveraging social structures to enhance performance. Overall, AdaSociety serves as a valuable research platform for exploring intelligence in diverse physical and social settings. The code is available at https://github.com/bigai-ai/AdaSociety.","Classic learning environments [55, 41, 9, 42, 34] have agents trained in small and stationary worlds, which hinders the improvement of agents‚Äô intelligence. The learning process stagnates once the environments can no longer provide novel data for agents‚Äô explorations. Additionally, agents trained on a fixed task set may suffer from a loss of generalization ability [13]. Single-agent environments [18, 25, 61] set out to solve this problem by constructing adaptive environments that continuously generate new tasks based on agent actions, providing a multitudinous task set. In multi-agent settings, however, the task set is determined by not only physical surroundings but also social connections among agents. Social connections dramatically impact agents‚Äô decision-making by shaping their reward structures and information access [20], and different social structures endow the environments with radically different research problems. For example, centralized scenarios focus on issues like credit assignment and consensus establishment [21, 44], while decentralized settings require agents to address opponent modeling issues and non-stationarity [3, 21, 29, 33]. What makes the problem even more challenging is that social connections are not predefined but adaptive, which means there‚Äôs a dynamical interplay between the topology of social connections and agents‚Äô states [23]. The adaptive nature of social connections and physical surroundings requires agents to learn continuously, reason about other agents‚Äô policies, and balance between physical explorations and establishing social connections. While contemporary multi-agent decision-making environments [6, 2, 53, 66, 48] have achieved great progress in stimulating and testing capabilities of learning algorithms in fixed task sets, they fail to generate new tasks by concurrently considering expanding physical surroundings and adaptive social connections. To bridge this gap, we propose AdaSociety, a multi-agent environment with massive and diverse tasks generated by adaptive social connections and expanding physical surroundings, which are influenced by agents‚Äô behavior. In particular, to the best of our knowledge, AdaSociety first introduces social states (expressed as a multi-layer directed graph) to explicitly and quantitatively describe the adaptive and dynamic connections between entities, including agents and emerged organizations. This greatly enriches the diversity of tasks, supporting the establishment of stable and long-term relations between entities and the quantitative study of social intelligence, like coalition formation and the emergence of hierarchy. In such an environment, agents need to balance the exploration of physical surroundings and the alteration of social connections, leading to multiple possible victory paths and significant decision-making challenges. To stimulate algorithm design and theoretical analysis in AdaSociety, we provide a formulation of the multi-agent decision-making problems, named Growing-MG (Sec. 3). AdaSociety serves as a platform for researchers to customize the environment for diverse research needs. Specifically, a set of fundamental elements and mechanisms can be used, and interfaces are provided to set environment attributes and hyper-parameters. Moreover, AdaSociety exhibits its characteristics by offering three mini-games, where both tensor- and LLM-based methods are tested. In summary, this paper makes three contributions. 1) We introduce a novel multi-agent general-sum environment featuring expanding physical surroundings and adaptive social connections. 2) We offer a customizable environment with three built-in mini-games, supporting both tensor- and LLM-based methods. 3) We implement RL and LLM methods in these mini-games and provide preliminary results, laying the groundwork for further research in this environment. Figure 1: An overview of AdaSociety, composed of physical component and social component. Physical Component consists of diverse resources and events on the map and heterogeneous agents‚Äô inventories. Social Component describes the adaptive connections between agents and organizations, which shape information access and reward structure. Agents take social actions to alter their social connections. As shown in the rightmost flowchart, agents are initially independent and can establish individual connections (edges between nodes) and form groups (gray ovals)."
https://arxiv.org/html/2411.03859v1,UniTraj: Universal Human Trajectory Modeling from Billion-Scale Worldwide Traces,"Human trajectory modeling is essential for deciphering movement patterns and supporting advanced applications across various domains. However, existing methods are often tailored to specific tasks and regions, resulting in limitations related to task specificity, regional dependency, and data quality sensitivity. Addressing these challenges requires a universal human trajectory foundation model capable of generalizing and scaling across diverse tasks and geographic contexts. To this end, we propose UniTraj, a Universal human Trajectory foundation model that is task-adaptive, region-independent, and highly generalizable. To further enhance performance, we construct WorldTrace, the first large-scale, high-quality, globally distributed dataset sourced from open web platforms, encompassing 2.45 million trajectories with billions of points across 70 countries. Through multiple resampling and masking strategies designed for pre-training, UniTraj effectively overcomes geographic and task constraints, adapting to heterogeneous data quality. Extensive experiments across multiple trajectory analysis tasks and real-world datasets demonstrate that UniTraj consistently outperforms existing approaches in terms of scalability and adaptability. These results underscore the potential of UniTraj as a versatile, robust solution for a wide range of trajectory analysis applications, with WorldTrace serving as an ideal but non-exclusive foundation for training.","Human trajectory data, which captures the movement paths of individuals or groups over time, has become increasingly significant in various domains such as transportation management (Luca et al., 2021), logistics optimization (Guo et al., 2018), and web-based services (Zheng, 2015). With the widespread adoption of GPS-enabled devices and the integration of positioning technologies into numerous applications, vast amounts of trajectory data are generated daily from vehicles and mobile devices connected to the Internet (Wang et al., 2022; Lan et al., 2022; Liu et al., 2024). This type of data permits us an unprecedented opportunity to analyze movement patterns, traffic flow, and user mobility behaviors, supporting a range of applications from real-time traffic updates to location-based services and personalized content recommendations (Chen et al., 2024). \Description [] Figure 1. Overview of this work, we propose a trajectory foundation model and also collect a worldwide trajectory dataset. The pre-trained UniTraj can be used as a backbone while adapters are trained for different regions and tasks. To effectively harness this wealth of data, robust human trajectory modeling techniques are essential to extract meaningful insights. Modeling trajectories allows us to convert raw location data into actionable information, uncovering human mobility patterns across spatial and temporal dimensions to enable advanced applications in diverse fields (Chang et al., 2023). From a task-oriented perspective, existing methods often utilize various statistical and machine learning techniques (e.g., CNNs and RNNs) to capture detailed spatio-temporal features (Li et al., 2024). These models are typically optimized for specific tasks, with algorithms and architectures tailored to address distinct challenges such as trajectory prediction, anomaly detection, and activity recognition (Lin et al., 2024b). On the data side, researchers employ a wide array of trajectory datasets gathered from sources like vehicles, mobile devices, and other GPS-enabled equipment (Chen et al., 2024; Luca et al., 2021). These datasets vary significantly in size, geographic coverage, and quality, providing essential support for model development and evaluation. Collectively, these task-specific modeling efforts and diverse data sources have propelled advancements in human trajectory analysis, deepening our understanding of mobility behavior. Despite these advancements, existing methods face significant limitations that impede their generalizability and practical applicability: Despite these advancements, existing methods face (i) Task Specificity: Current approaches are typically designed and optimized for specific tasks, lacking the flexibility to adapt across different applications without extensive modifications. This task-centric focus restricts their reusability across a range of trajectory-related problems, including prediction, classification, and anomaly detection. (ii) Regional Dependency: Many models are developed and trained on data from specific geographic regions, limiting their effectiveness when applied to trajectories from diverse locations. Variations in infrastructure, traffic patterns, and behaviors across regions mean that models confined on narrow geographic data often fail to capture the diversity essential for global trajectory datasets, thus struggling to generalize to new environments. (iii) Data Quality Sensitivity: Real-world trajectory data is inherently heterogeneous, with variability in sampling rates, noise levels, and occasional missing data due to differences in data collection criteria and device capabilities. Existing models are typically sensitive to these inconsistencies, leading to degraded performance when faced with noisy or incomplete data. This sensitivity requires extensive data preprocessing and cleaning, which may not always be practical, reducing the robustness of these models in real-world scenarios. What measures can be taken to overcome these limitations? Empirically, developing a task-adaptive, region-independent, and scalable foundation model for universal trajectory modeling is both an emerging necessity and a promising trend (Zhang et al., 2024). As shown in Figure 1, such a model can generalize across various tasks without requiring specialized models for each application, thereby enhancing scalability and efficiency. Additionally, a foundation model can effectively handle diverse data qualities, making it adaptable to real-world scenarios where data variability is the norm. However, constructing a universal trajectory foundation model presents two primary challenges: ‚Ä¢ Data preparation: Constructing a foundation model requires the collection and integration of vast amounts of high-quality trajectory data, covering different geographic regions, sampling rates, and user behaviors. However, most existing datasets are primarily held by a limited number of companies or organizations with proprietary rights or restrictive access policies, hindering widespread usage and collaborative research. Furthermore, the labor and financial costs associated with data collection make obtaining large-scale, high-quality trajectory datasets particularly difficult. As a result, available datasets (such as GeoLife (Zheng et al., 2009) and Porto (Meghan O‚ÄôConnell, 2015)) are often restricted to specific regions or cities, reducing their generalizability and constraining research aimed at broader, global applications. ‚Ä¢ Model Design: A universal trajectory foundation model must be equipped several capabilities that current approaches lack. First, the model should be capable of generalizing across diverse spatio-temporal contexts, enabling it to serve as a backbone that can be adapted to a wide range of tasks without extensive modifications. Second, it must maintain robust representation capabilities to handle data with varying qualities, demonstrating resilience to noise, missing values, and inconsistent sampling rates. Finally, the model should balance complexity and computational efficiency, avoiding overfitting to specific data patterns while remaining scalable for large datasets. With these challenges in mind, we introduce WorldTrace, the first large-scale, high-quality, globally distributed trajectory dataset sourced from open platforms. Spanning 2.45 million trajectories with billions of points across 70 countries, WorldTrace overcomes the limitations of existing datasets by offering extensive geographic coverage, diverse sampling rates, and accessible data, thereby calling widespread use and collaboration. Meanwhile, we present UniTraj, a Universal human Trajectory foundation model designed to be task-adaptive, region-independent, and resilient to varying data quality. UniTraj can serve as a versatile backbone capable of supporting diverse trajectory analysis tasks without dependence on a specific dataset, though it achieves optimal performance when trained on high-quality, diverse data like WorldTrace. In addition, our approach employs advanced pre-training techniques, including multiple resampling and masking strategies, which enable UniTraj to capture complex spatio-temporal dependencies and adapt to heterogeneous data characteristics across regions and sampling frequencies. This design promotes robust generalization across tasks and regions, offering a scalable and efficient solution for a wide range of trajectory analysis applications. In summary, the contributions of our research are as follows: ‚Ä¢ We construct the first large-scale, high-quality, globally distributed trajectory dataset, called WorldTrace. This dataset overcomes the limitations of existing datasets by offering accessible data for widespread use and collaboration, facilitating research with a broader global perspective, and supporting the development of universal trajectory models. ‚Ä¢ We propose UniTraj, a universal human trajectory foundation model that leverages advanced techniques such as multiple resampling and masking strategies. UniTraj can serve as a backbone to captures complex spatio-temporal dependencies and adapts to heterogeneous data characteristics across different regions and sampling rates. ‚Ä¢ We conduct extensive experiments across diverse trajectory analysis tasks and real-world datasets, demonstrating the scalability and adaptability of UniTraj. Additionally, we validate the unique advantages of WorldTrace, highlighting its potential as an ideal dataset for building robust and generalizable trajectory models."
https://arxiv.org/html/2411.02866v1,Double Whammy: Stealthy Data Manipulation aided Reconstruction Attack on Graph Federated Learning,"Graph federated learning (GFL) is one of the effective distributed learning paradigms for training graph neural network (GNN) on isolated graph data. It perfectly addresses the issue that GNN requires a large amount of labeled graph without original data sharing. Unfortunately, recent research has constructed successful graph reconstruction attack (GRA) on GFL. But these attacks are still challenged in aspects of effectiveness and stealth. To address the issues, we propose the first Data Manipulation aided Reconstruction attack on GFL, dubbed as DMan4Rec. The malicious client is born to manipulate its locally collected data to enhance graph stealing privacy from benign ones, so as to construct double whammy on GFL. It differs from previous work in three terms: (i) effectiveness - to fully utilize the sparsity and feature smoothness of the graph, novel penalty terms are designed adaptive to diverse similarity functions for connected and unconnected node pairs, as well as incorporation label smoothing on top of the original cross-entropy loss. (ii) scalability - DMan4Rec is capable of both white-box and black-box attacks via training a supervised model to infer the posterior probabilities obtained from limited queries. (iii) stealthiness - by manipulating the malicious client‚Äôs node features, it can maintain the overall graph structure‚Äôs invariance and conceal the attack. Comprehensive experiments on four real datasets and three GNN models demonstrate that DMan4Rec achieves the state-of-the-art (SOTA) attack performance, e.g., the attack AUC and precision improved by 9.2% and 10.5% respectively compared with the SOTA baselines. Particularly, DMan4Rec achieves an AUC score and a precision score of up to 99.59% and 99.56%, respectively in black-box setting. Nevertheless, the complete overlap of the distribution graphs supports the stealthiness of the attack. Besides, DMan4Rec still beats the defensive GFL, which alarms a new threat to GFL.","During the last decade, graph neural network (GNN) has dominated the tasks for graph analysis, leading to its widespread application in various fields, such as recommendation systems [1], drug discovery [2], and etc [3, 4, 5]. Recently, due to privacy concerns, regulatory restrictions, and commercial competition, practical applications require the decentralization of graph data, and graph federated learning (GFL) [6, 7, 8] is coming up just in time, which is perfectly designed to train the global GNN on a large amount of labeled graph without raw data sharing. To cope with different data distribution applications [9], GFL is roughly categorized into graph horizontal federated learning (GHFL) [10, 11, 1, 12] and graph vertical federated learning (GVFL) [13, 14, 15, 16]. Since GHFL captures the mainstreaming attention in practice [17], thus we focus on GHFL and refer to it as GFL for short in this paper. Although GFL is proposed to train a global model with isolated data, unfortunately recent research [18, 19] has launched successful inference attack on GFL, which has revealed its vulnerability towards privacy leakage. Graph reconstruction attack (GRA) is one of the inference attacks that can compromise the intellectual property, confidentiality, or privacy of graphs. Therefore, we specifically focus on GRA against GFL [20]. In the case of Amazon‚Äôs shopping site, different stores on this platform sell various products, each maintaining its own graph networks, making it suitable for GFL scenarios. In this context, competing stores may sell similar types of products. Stores infer the complete graph network and obtain more product information through a federated training process as shown in Fig. 1. In this way, the malicious store can adjust its marketing strategy to enhance the exposure and recommendation level of their products in the recommendation system. More importantly, taking advantage of the inherent ease of manipulating local data in GFL, the malicious store can conveniently access and modify its locally collected data [21] without sharing it with other clients or the server. This enables a greater ability to steal more private information from benign clients. Figure 1: An example of the graph reconstruction attack against the Amazon shopping website based on data manipulation. Currently, numerous studies have focused on GRA within graph-related fields, but these attacks [20, 22, 23] fail to steal privacy in GFL scenarios since they are launched in assumption of centralized training. In this context, we summarized the main challenges of GRA on GFL, i.e., (i) effectiveness: due to the sparsity of graphs, joint training across clients exacerbates this vulnerability, thereby preventing the attack from being fully effective; (ii) scalability: in the GFL scenario, the malicious client lacks direct access to the server model, as well as they can only query the node posterior probability, resulting in difficulty to construct black-box attack; (iii) stealthiness: the existing methods of manipulated data, such as malicious node injection [24], rewiring [25], and link modification [26, 27, 28, 29] are purposely designed for backdoor injection in general, that is significantly increase the risk of being detected by defensive mechanism. In order to solve the aforementioned issues and uncover possible security vulnerabilities for GFL scenarios, we introduce a novel data manipulation aided reconstruction attack (DMan4Rec) for the first time. Specifically, to tackle the challenge of effectiveness and ensure that the manipulated graph enhances the similarity of GNN outputs for connected nodes while promoting dissimilarity for unconnected nodes, we design adaptive penalties for both connected and unconnected node pairs by employing various similarity functions. In addition, to improve the generalization performance of the model, we incorporate label smoothing into the original cross-entropy loss to fully leverage feature smoothness of the graph. In order to address the scalability challenge, a supervised model is trained using only the posterior probability obtained by querying the server model and the shadow dataset obtained using malicious nodes, thus converting the attack into a supervised classification problem. Since existing methods [24, 25, 26, 27, 28, 29] for manipulating data are specifically designed for backdoor injection, they significantly increase the risk of detection by defensive mechanisms. Therefore, we strategically manipulate node features to contaminate the graph while carefully preserving its overall structure, addressing the challenge of stealthiness. In summary, the main contributions are outlined as follows: ‚Ä¢ Attack scenario. Since the natural characteristics of distributed data in GFL, malicious client is easy to manipulate local data, we propose the first stealthy data manipulation aided graph reconstruction attack on GFL, dubbed as DMan4Rec. It outperforms previous work in aspects of effectiveness, scalability and stealthiness. ‚Ä¢ Attack framework. A novel penalty terms is proposed to adaptive to diverse similarity function, as well as incorporation label smoothing strategy for attack effectiveness. DMan4Rec is scalable of both white-box and black-box attacks by achieving the state-of-the-art (SOTA) attack performance. The malicous client manipulates limited node features to maintain the overall graph structure‚Äôs invariance to promise the stealthiness. ‚Ä¢ Attack performance. Extensive experiments are conducted on 4 general graph datasets, 3 GNN models and 5 baselines, and the results testify that DMan4Rec improves AUC and precision by 9.2% and 10.5% respectively compared with the SOTA baselines, without affecting classification performance. Particularly, in balck-box scenario, DMan4Rec can achieve an AUC score and a precision score of up to 99.72% and 99.96%, respectively, under the distribution graphs before and after data manipulation completely overlap, ensuring the concealment of the attack. Additionally, DMan4Rec can also beat the defensive GFLs. The rest of the paper is organized as follows. Related works are introduced in Section II, while the proposed method is detailed in Section III. Experimental results and discussion are showed in Section IV. At last, we conclude our work and point future work."
https://arxiv.org/html/2411.02434v1,Analysis of the inference of ratings and rankings on Higher Order Networks with complex topologies,"The inference of rankings plays a central role in the theory of social choice, which seeks to establish preferences from collectively generated data, such as pairwise comparisons. Examples include political elections, ranking athletes based on competition results, ordering web pages in search engines using hyperlink networks, and generating recommendations in online stores based on user behavior. Various methods have been developed to infer rankings from incomplete or conflicting data. One such method, HodgeRank, introduced by Jiang et al. [1], utilizes Hodge decomposition of cochains in Higher Order Networks to disentangle gradient and cyclical components contributing to rating scores, enabling a parsimonious inference of ratings and rankings for lists of items. This paper presents a systematic study of HodgeRank‚Äôs performance under the influence of quenched disorder and across networks with complex topologies generated by four different network models. The results reveal a transition from a regime of perfect trieval of true rankings to one of imperfect trieval as the strength of the quenched disorder increases. A range of observables are analyzed, and their scaling behavior with respect to the network model parameters is characterized. This work advances the understanding of social choice theory and the inference of ratings and rankings within complex network structures.","The rating and ranking candidates, agents, items or options based on collectively expressed preferences is deeply ingrained modern human civilization, making the study of inference methods for ratings and rankings under diverse conditions highly significant. Ratings and rankings are often derived from pairwise comparisons, which naturally form a network structure. Understanding how the topology of these networks influences the accuracy of rating and ranking inference is of critical importance. The modern theory of ratings and rankings has its origins in the field of social choice, with early foundations laid in the late 13th century by Ramon Llull in his Ars Electionis [2]. Llull‚Äôs work introduced the concept of pairwise comparisons, the majority principle, and fairness in voting systems. Unfortunately, his contributions were largely forgotten over time. A mathematical revival of these ideas emerged in the late 18th century with the works of Borda and Condorcet [3, 4], who laid the groundwork for modern voting and ranking theory. Later, in 1929, Zermelo proposed a model for ranking inference using ratings as latent variables within a probabilistic framework [5], but his work was also overlooked [6]. In the 1950s, Bradley and Terry reintroduced Zermelo‚Äôs model in a broader context [7]. In the 1960s, a model based on similar principles was introduced and promoted by √âl≈ë, eventually becoming the default rating system used by the World Chess Federation (FIDE) [8]. More recently, the advent of the World Wide Web and search engines highlighted the significance of network topology in ranking systems, exemplified by the success of PageRank, the original algorithm behind Google and introduced Brin and Page at the end of the 90‚Äôs [9]. The study of rating and ranking systems remains an active area of research [1, 10, 11, 12, 13], as collective decision-making plays a vital role in modern civilization, particularly in the information age. In 1736, Euler solved the Seven Bridges of K√∂nigsberg problem, establishing the foundations of graph theory and foreshadowing the development of topology. Graphs, or networks, can be viewed as a specific type of topological space. Within the theory of Higher Order Networks [14, 15, 16, 17, 18, 19, 20, 21], hypergraphs extend the concept of graphs by incorporating hyperlinks, that may be used to represent many-body interactions or multiary-relations. However, unlike graphs, hypergraphs are not necessarily topological spaces, as they may not satisfy the condition of topological closure. For example, a hypergraph may include a hyperlink {i,j,k}ùëñùëóùëò\{i,j,k\}{ italic_i , italic_j , italic_k } without containing the link {i,j}ùëñùëó\{i,j\}{ italic_i , italic_j }. When hypergraphs satisfy the closure condition, they form a special class of topological spaces known as abstract simplicial complexes, or simply, simplicial complexes. Although terminology and definitions may vary by context, many concepts from differential geometry on smooth manifolds can be adapted to the discrete or combinatorial setting of simplicial complexes [22, 23, 24, 25, 26, 14]. Namely, discrete or combinatorial analogous of forms, gradients, divergences and Laplacians can be defined over simplicial complexes. This is the reason for which simplicial complexes are often preferred over general hypergraphs. In particular, and thanks to the property of topological closure, Hodge theory can be applied to decompose cochains over simplicial complexes into exact, coexact and harmonic components, much like the analogous decomposition of tensor fields [23, 27]. This is the approach followed by Jian et al. [1] who developed HodgeRank, a method for the inference of ratings that leverages Hodge theory over simplicial complexes to disentangle different contributions to the inferred ratings. The present article takes advantage of HodgeRank to study the inference of ratings and rankings in complex topologies [28, 29]. Section II provides an introduction to the theory of rating and ranking inference, and focuses on HodgeRank. Section III presents the results of numerical experiments using HodgeRank applied to networks with complex topologies. Section IV discuss the results. Section V summarizes the findings and outlines directions for future research."
https://arxiv.org/html/2411.02542v1,Enhancing Graph Neural Networks in Large-scale Traffic Incident Analysis with Concurrency Hypothesis,"Despite recent progress in reducing road fatalities, the persistently high rate of traffic-related deaths highlights the necessity for improved safety interventions. Leveraging large-scale graph-based nationwide road network data across 49 states in the USA, our study first posits the Concurrency Hypothesis from intuitive observations, suggesting a significant likelihood of incidents occurring at neighboring nodes within the road network. To quantify this phenomenon, we introduce two novel metrics, Average Neighbor Crash Density (ANCD) and Average Neighbor Crash Continuity (ANCC), and subsequently employ them in statistical tests to validate the hypothesis rigorously. Building upon this foundation, we propose the Concurrency Prior (CP) method, a powerful approach designed to enhance the predictive capabilities of general Graph Neural Network (GNN) models in semi-supervised traffic incident prediction tasks. Our method allows GNNs to incorporate concurrent incident information, as mentioned in the hypothesis, via tokenization with negligible extra parameters. The extensive experiments, utilizing real-world data across states and cities in the USA, demonstrate that integrating CP into 12 state-of-the-art GNN architectures leads to significant improvements, with gains ranging from 3% to 13% in F1 score and 1.3% to 9% in AUC metrics. The code is publicly available at https://github.com/xiwenc1/Incident-GNN-CP111We tend to use the term incident rather than accident according to the preference of the Department of Transportation. They may be interchangeably used in our paper..","The significance of traffic safety is underscored by recent statistics, which highlight the ongoing challenges and successes in reducing road fatalities. In the early months of 2023, the National Highway Traffic Safety Administration (NHTSA) (National Highway Traffic Safety Administration, 2023) reported a decrease in traffic fatalities, estimating that 9,330 people lost their lives in traffic crashes in the first quarter, marking a 3.3% decline compared to the previous year. This trend continued into the first half of the year, with fatalities dropping to 19,515, also down by about 3.3% from the prior year. However, despite the positive trend in these numbers, they still reflect a high rate of traffic-related deaths, especially compared to the earlier years (2005-2019). This situation emphasizes the ongoing need for enhanced traffic safety measures and interventions to sustain and accelerate the reduction in road fatalities. The importance of modeling traffic incident risks is well recognized in the field of urban planning and public safety (Fadhel et al., 2024; Razi et al., 2023). Accurate predictions of where and when incidents are likely to occur can significantly contribute to the development of more effective traffic management strategies and infrastructure improvements. As urban areas continue to grow and also traffic volume increases, the need for powerful analytical tools to assess risk and prevent incidents becomes increasingly critical. It requires the integration of comprehensive datasets and advanced analytical techniques to understand the complex dynamics of road traffic and enhance safety measures. By leveraging detailed geospatial data and traffic incident records, researchers and city planners can identify high-risk areas and implement specific interventions to mitigate the possible risks, while enhancing road safety (Pei and Hou, 2024). Recently, many studies have analyzed the effect of road features for predicting incident occurrences, such as (Persaud and Dzbik, 1992; Oh et al., 2006; Caliendo et al., 2007; Najjar et al., 2017; Zhou et al., 2020a; Zheng et al., 2021). More recently, Deep Learning (DL)-based methods have gained significant attention in traffic safety analysis since their powerful ability to characterize the inherent complex features of large-scale data (Yin et al., 2021; Sarlak et al., 2023; Zhang et al., 2023; Razi et al., 2023). Due to the nature that both the road network and traffic flow can be viewed as graph structure data, Graph Neural Network (GNN) (Kipf and Welling, 2016; Veliƒçkoviƒá et al., 2018; Hamilton et al., 2017; Du et al., 2018; Xie and Grossman, 2018; Li et al., 2020; Liu et al., 2022; Zhang et al., 2024) is the rational choice to characterize the relations in a network and has been adopted in recent works (Li et al., 2017; Yu et al., 2021; Zhou et al., 2020b). Our work is motivated by two intuitive observations in traffic incident occurrences in road networks: When people are driving and notice a traffic incident, there is a high probability that they observe another incident has occurred nearby. Another observation is that there are always some accident-prone sections, meaning the continuous areas included in the sections are likely to have incidents even if they have not occurred necessarily at the same time. We then make a unified hypothesis for them: Concurrency Hypothesis. There is a high probability when a node has an incident occurred, some of its neighbors have an incident occurred. Subsequently, we propose two novel metrics, the Average Neighbor Crash Density (ANCD) and Average Neighbor Crash Continuity (ANCC), to quantify these observations, and apply standard statistical tests for these quantitative results to validate the proposed hypothesis. An exemplary visualization of ANCD for each state is shown in Fig. 1, which underscores the difference of the nodes between different categories for each state. We then conjecture that this hypothesis indicates that there may be some important but difficult-to-capture information and features that have not been fully collected by the general datasets. Accordingly, in this work, we proposed an enhancement method called Concurrency Prior (CP) that explores the hidden information beyond the common features from the crash label for semi-supervised traffic incident prediction. This problem is built on a single monolithic graph representing an entire state or city. Entire edge features, entire node features and partial nodes‚Äô labels are known. Our goal is to utilize the known information to learn a model and predict the label for the rest of the nodes with unknown labels. The formal problem description is in Section 4. Our method is compatible and complementary with general graph neural networks, such as Graph Convolutional Networks (Kipf and Welling, 2016), Graph Attention Networks (Veliƒçkoviƒá et al., 2018), and Graph Transformers (Shi et al., 2021). Our investigation is based on the nationwide real-world road network data provided by (Huang et al., 2023). This large-scale data source contains the incident record from 49 states of the USA and provides various edge features, such as length, type, number of lanes, max speed, and road direction and angular information. We provide the details of the data acquisition in Section 3. In summary, our contribution is two-fold: (i) We are the first to statistically validate the Concurrency Hypothesis in nationwide graph-based data by using our proposed metrics; and (ii) We propose an enhancement method called Concurrency Prior that enables boosting broad variations of graph neural networks in the semi-supervised traffic incident prediction task by introducing negligible parameters. Our intensive experiments on 12 state-of-the-art graph neural networks demonstrate a 3%-13% and 1.3%-9% gain in F1 and AUC, respectively."
https://arxiv.org/html/2411.02405v1,Accuracy nudges are not effective against non-harmful deepfakes,"I conducted a preregistered survey experiment (n=525) to assess the effectiveness of ‚Äúaccuracy nudges‚Äù against deepfakes (osf.io/69x17). The results, based on a sample of Colombian participants, replicated previous findings showing that prompting participants to assess the accuracy of a headline at the beginning of the survey significantly decreased their intention to share fake news. However, this effect was not significant when applied to a non-harmful AI-generated video. Keywords: Misinformation, Deepfakes, Artificial Intelligence, Nudges.","Shifting attention to accuracy through nudges has been proposed as a scalable intervention to reduce the spread of misinformation online. These interventions have proven effective in reducing the spread of false information related to health and political issues (Pennycook et al., 2021b, ; Pennycook and Rand,, 2019; Offer-Westort et al.,, 2023). However, recent advances in generative artificial intelligence have made these systems capable of creating increasingly credible messages, known as deepfakes, raising concerns about their potential use for influence operations (Goldstein et al.,, 2023; Dufour et al.,, 2023). How well do accuracy nudge interventions work on deepfake sharing intentions? This study aims to answer this question using a simplified version of the experimental method proposed by Pennycook et al., 2021a ."
https://arxiv.org/html/2411.01844v1,DeMod: A Holistic Tool with Explainable Detection and Personalized Modification for Toxicity Censorship,"Although there have been automated approaches and tools supporting toxicity censorship for social posts, most of them focus on detection. Toxicity censorship is a complex process, wherein detection is just an initial task and a user can have further needs such as rationale understanding and content modification. For this problem, we conduct a needfinding study to investigate people‚Äôs diverse needs in toxicity censorship and then build a ChatGPT-based censorship tool named DeMod accordingly. DeMod is equipped with the features of explainable Detection and personalized Modification, providing fine-grained detection results, detailed explanations, and personalized modification suggestions. We also implemented the tool and recruited 35 Weibo users for evaluation. The results suggest DeMod‚Äôs multiple strengths like the richness of functionality, the accuracy of censorship, and ease of use. Based on the findings, we further propose several insights into the design of content censorship systems.","Nowadays, social media sites have been popular mediums for self-disclosure. For example, hundreds of millions of people utilize Twitter (Jhaver et al., 2018), Facebook (Seering, 2020; Pan et al., 2022; Saha et al., 2021), and Weibo (Zhao, 2017) to record life events, express personal thoughts and opinions, and interact with friends every day. The openness of social media provides a spacious environment for content sharing while resulting in the disclosure of toxic content (toxicity), defined as ‚Äùa rude, disrespectful, or unreasonable comment that is likely to make someone leave a discussion‚Äù (Per, 2023), including hate speech (Kang et al., 2022), harassment (Jhaver et al., 2018; Cai and Wohn, 2019), insults and abuse (Beres et al., 2021), and offensive language (Deng et al., 2022), etc. Since the severe problem of context collapse (Misra and Such, 2017), social media users are usually unaware of the disclosure of toxic content. For example, the prior studies (Michael Wiegand, 2021; Maarten et al., 2020) found that about two-thirds of toxic content was implicit toxicity in online communities and the corresponding users were usually unaware of the content and the harm to others. Research revealed that 23.00% of users regret when they re-examine their shared content due to several reasons (Wang et al., 2011), such as lack of the consequence consideration of posts, culture misjudgment, unintended audience, misunderstanding of platform norms. To avoid toxic content disclosure, social media users generally conduct content censorship before publishing a post. The censorship procedure can be implemented by users themselves or by leveraging some automated tools. For example, several studies have found that individuals usually censored their content by checking, adjusting, or even deleting part of the content to make the content suitable to be published on social media (Wisniewski et al., 2012). Although there have been various censorship approaches, most of them focus on toxic content detection, e.g., toxicity score evaluation with Perspective API (Per, 2023) and toxic keywords identification (Wright et al., 2021). Toxic content censorship is a complex process, wherein detection is just an initial task, and a user can have diverse needs such as detection result understanding and content modification. For example, a user can identify toxic words in the content with the RECAST tool (Wright et al., 2021) while not knowing how to reduce its toxicity limited by her/his knowledge or experience. Therefore, there needs a holistic automated tool that can help social media users conduct multiple censorship tasks including toxic content detection, content modification, etc. Building a holistic tool for toxicity censorship faces several challenges. First, social media users‚Äô diverse needs for toxic content censorship remain unknown. As mentioned, social media users may have different function demands like enriching explanations and giving modifications. Therefore, a systematic investigation of toxic content censorship demands is needed when conducting research on a holistic censorship tool, aggravating the complexity of this study. Second, designing and implementing a toxicity censorship tool that meets the diverse needs of users is non-trivial. Such a tool should be characterized by multiple objectives like accurate detection, fine-grained results, and appropriate revisions. How to achieve different functions and integrate them efficiently is a challenging task. Third, extensive evaluations in practice are difficult to conduct. To demonstrate the tool‚Äôs performance in helping users censor toxic content, it needs to conduct long-term evaluations in real social media scenarios by using various measurements, while some of them like the modification effects, are difficult to measure. For the above problem and challenges, we explore to design a holistic automated tool for helping users conduct toxic content censorship on social media. First, we conduct a needfinding study on a popular Chinese social platform - Weibo to systematically understand users‚Äô current toxicity censorship practice, the problems encountered, and their corresponding expectations for system design. By combining a questionnaire survey and interviews, we uncover users‚Äô diverse demands for the design of toxicity censorship tools and propose five goals to guide our system design, including providing holistic censorship, offering fine-grained detection results, strengthening interpretability, giving personalized revising suggestions, and ensuring user-control. Second, according to these goals, we design and implement a holistic automated toxicity censorship tool named DeMod. It is essentially a ChatGPT-enhanced tool equipped with the modules of explainable Detector and personalized Modifier. The explainable Detector can detect toxic content by giving fine-grained results like keywords and providing immediate and dynamic explanations. The immediate explanation clarifies why the content is toxic, and the dynamic explanation simulates audiences‚Äô attitudes to the forthcoming post, helping a user know the content‚Äôs potential effects. Both explanations aim to enhance the user‚Äôs understanding of toxic content and encourage behavior regulation. After that, the modifier gives suggestions on how to revise the toxic content by considering multiple requirements, including detoxifying, reserving the original semantics, and revealing a user‚Äôs personalized language style. By taking advantage of these modules, social media users can conduct content censorship more efficiently and flexibly. Third, we implement DeMod as a third-party tool by setting Weibo as a research site and recruit 35 participants to conduct extensive evaluations. We adopt several metrics regarding our design goals. The evaluation results suggest DeMod‚Äôs capability in toxicity censorship and high acceptance among participants. Based on the above work and results, we also propose several insights into the design of content censorship tools, including enhancing censorship tools from the holistic perspective, emphasizing the interpretability of the process and results, and providing improvement measures to assist users in posting better. To conclude, our contributions can be summarized as: ‚Ä¢ We conduct a needfinding study to investigate social media users‚Äô current toxicity censorship practice, the problems encountered, and their corresponding expectations for system design, based on which five design goals are proposed to guide the improvement of toxicity censorship tools. ‚Ä¢ We propose a holistic automated tool based on ChatGPT for helping users conduct toxicity censorship. To the best of our knowledge, this is the first work that supports users‚Äô demands in multiple stages of toxicity censorship beyond detection. ‚Ä¢ We conduct extensive evaluations in real social media scenarios and validate DeMod‚Äôs strengths in toxicity censorship. ‚Ä¢ Several insights are proposed for the further improvement of content censorship system design. The rest of this paper is organized as follows. In Section 2, we review related research on content censorship and large language models. In Section 3, we introduce the procedure and results of our empirical study. The framework of DeMod and its implementation are given in Section 4. Section 5 exhibits our evaluation settings and results. Section 6 discusses our findings, and the limitations and future work are clarified in Section 7. Finally, conclusions are given in Section 8."
https://arxiv.org/html/2411.02268v2,Memory-Efficient Community Detection on Large GraphsUsing Weighted Sketches,"Community detection in graphs identifies groups of nodes with denser connections within the groups than between them, and while existing studies often focus on optimizing detection performance, memory constraints become critical when processing large graphs on shared-memory systems. We recently proposed efficient implementations of the Louvain, Leiden, and Label Propagation Algorithms (LPA) for community detection. However, these incur significant memory overhead from the use of collision-free per-thread hashtables. To address this, we introduce memory-efficient alternatives using weighted Misra-Gries (MG) sketches, which replace the per-thread hashtables, and reduce memory demands in Louvain, Leiden, and LPA implementations ‚Äî while incurring only a minor quality drop (up to 1%percent11\%1 %) and moderate runtime penalties. We believe that these approaches, though slightly slower, are well-suited for parallel processing and could outperform current memory-intensive techniques on systems with many threads.","Research on graph-structured data has seen rapid growth, driven by the capacity of graphs to represent complex, real-world interactions and capture intricate relationships between entities. At the core of this field is community detection, a technique that divides graphs into tightly connected subgroups or communities, thereby revealing the natural structure within the data. Community detection finds applications across a wide range of areas, including examining epidemic-prone group dynamics (Salath√© and Jones, 2010), studying zoonotic eco-epidemiology (Desvars-Larrive et al., 2024), detecting diseases like lung cancer (Bechtel et al., 2005), categorizing tumors via genomic data (Haq and Wang, 2016), aiding therapeutic discovery (Ma et al., 2019; Udrescu et al., 2020), mapping healthcare areas (Wang et al., 2021), analyzing retail patterns (Verhetsel et al., 2022), identifying transportation trends (Chen et al., 2023), unsupervised part-of-speech tagging (Das and Petrov, 2011), partitioning graphs for machine learning (Bai et al., 2024), automating microservice decomposition (Cao and Zhang, 2022), sectionalizing power systems (Aziz et al., 2023), characterizing polarized information ecosystems (Uyheng et al., 2021), identifying hidden social network groups (Blekanov et al., 2021; La Cava et al., 2022), detecting disinformation on Telegram (La Morgia et al., 2021), investigating restored Twitter accounts (Kapoor et al., 2021), mapping multi-step cyberattacks (Zang et al., 2023), detecting blockchain attacks (Erfan et al., 2023), studying cyber resilience (Chernikova et al., 2022), analyzing human brain networks (Bullmore and Sporns, 2009; He and Evans, 2010), and understanding metabolic network evolution (Pfeiffer et al., 2005; Kim et al., 2009). Community detection is also used for addressing other graph related problems, such as, finding connected components (Stergiou et al., 2018), graph partitioning (Meyerhenke et al., 2017; Slota et al., 2020), vertex reordering and graph compression (Boldi et al., 2011), and graph coarsening (Valejo et al., 2020). Community detection is challenging due to the lack of prior knowledge about the number of communities and their size distribution, a problem that has led to the development of various heuristic methods for identifying communities (Blondel et al., 2008; Gregory, 2010; Raghavan et al., 2007; Newman and Reinert, 2016; Ghoshal et al., 2019). A commonly used metric for assessing the quality of detected communities is the modularity score, introduced by Newman et al. (Newman, 2004). The Louvain method, introduced by Blondel et al. (Blondel et al., 2008), is a widely used community detection algorithm (Lancichinetti and Fortunato, 2009) that applies a two-phase approach consisting of an iterative local-moving phase and an aggregation phase to optimize the modularity metric across multiple passes. However, Traag et al. (Traag et al., 2019) found that the Louvain method can yield poorly connected and even internally disconnected communities. They proposed the Leiden algorithm, which introduces an additional refinement phase to address these shortcomings, enabling the algorithm to better detect well-connected communities (Traag et al., 2019). The Label Propagation Algorithm (LPA) is another method that outperforms the above algorithms in terms of speed and scalability, but yields communities with lower modularity scores. However, it has been observed to achieve high Normalized Mutual Information (NMI) score compared to the ground truth (Peng et al., 2014). Given the importance of the community detection problem, a number of existing studies have aimed at improving the performance of the above algorithms using various algorithmic optimizations (Rotta and Noack, 2011; Waltman and Eck, 2013; Gach and Hao, 2014; Traag, 2015; Lu et al., 2015; Ozaki et al., 2016; Naim et al., 2017; Halappanavar et al., 2017; Ghosh et al., 2018b; Traag et al., 2019; Shi et al., 2021; Xing et al., 2014; Berahmand and Bouyer, 2018; Sattari and Zamanifar, 2018; You et al., 2020; Liu et al., 2020) and parallelization techniques (Cheong et al., 2013; Wickramaarachchi et al., 2014; Lu et al., 2015; Naim et al., 2017; Fazlali et al., 2017; Halappanavar et al., 2017; Ghosh et al., 2018b; Bhowmik and Vadhiyar, 2019; Shi et al., 2021; Bhowmick et al., 2022; Staudt and Meyerhenke, 2015; Soman and Narang, 2011; Kuzmin et al., 2015; Traag and ≈†ubelj, 2023). Additionally, significant effort has gone into developing efficient parallel implementations for multicore CPUs (Staudt and Meyerhenke, 2015; Staudt et al., 2016; Fazlali et al., 2017; Halappanavar et al., 2017; Qie et al., 2022; Hu et al., [n. d.]), GPUs (Naim et al., 2017; Kang et al., 2023), CPU-GPU hybrids (Bhowmik and Vadhiyar, 2019; Mohammadi et al., 2020), multi-GPUs (Cheong et al., 2013; Kang et al., 2023; Chou and Ghosh, 2022; Gawande et al., 2022), and multi-node systems ‚Äî CPU only (Ghosh et al., 2018b, a; Sattar and Arifuzzaman, 2022; Hu et al., [n. d.]) / CPU-GPU hybrids (Bhowmick et al., 2022). However, these studies focus primarily on reducing the runtime of the algorithms. As network sizes grow, the memory footprint becomes a critical concern, particularly when processing large graphs on shared-memory systems. Recently, we proposed some of the most efficient implementations of Louvain (Sahu, 2023b), Leiden (Sahu, 2023a), and LPA (Sahu, 2023c). These implementations have a space complexity of O‚Å¢(T‚Å¢|V|+|E|)ùëÇùëáùëâùê∏O(T|V|+|E|)italic_O ( italic_T | italic_V | + | italic_E | ), where |V|ùëâ|V|| italic_V | is the number of vertices, |E|ùê∏|E|| italic_E | is the number of edges, and TùëáTitalic_T is the number of threads used. As a result, they also face similar memory constraints. In this work, we present a method based on the Misra-Gries heavy hitters algorithm (Misra and Gries, 1982) to significantly reduce memory usage in our Louvain,111https://github.com/puzzlef/louvain-lowmem-communities-openmp Leiden,222https://github.com/puzzlef/leiden-lowmem-communities-openmp and LPA333https://github.com/puzzlef/rak-lowmem-communities-openmp implementations, with minimal impact on community quality. While this approach introduces some runtime overhead, it is more parallelizabile, and by current trends, may eventually outperform existing memory-intensive methods."
https://arxiv.org/html/2411.01947v1,HACD: Harnessing Attribute Semantics and Mesoscopic Structure for Community Detection,"Community detection plays a pivotal role in uncovering closely connected subgraphs, aiding various real-world applications such as recommendation systems and anomaly detection. With the surge of rich information available for entities in real-world networks, the community detection problem in attributed networks has attracted widespread attention. While previous research has effectively leveraged network topology and attribute information for attributed community detection, these methods overlook two critical issues: (i) the semantic similarity between node attributes within the community, and (ii) the inherent mesoscopic structure, which differs from the pairwise connections of the micro-structure. To address these limitations, we propose HACD, a novel attributed community detection model based on heterogeneous graph attention networks. HACD treats node attributes as another type of node, constructs attributed networks into heterogeneous graph structures and employs attribute-level attention mechanisms to capture semantic similarity. Furthermore, HACD introduces a community membership function to explore mesoscopic community structures, enhancing the robustness of detected communities. Extensive experiments demonstrate the effectiveness and efficiency of HACD, outperforming state-of-the-art methods in attributed community detection tasks. Our code is publicly available at https://github.com/Anniran1/HACD1-wsdm.","Community detection (wu2022clare, ) is a fundamental problem in network analysis, seeking to unveil closely connected subgraphs (i.e., communities) within complex networks. Previous research has adeptly utilized network topology to discern communities (kang2021adversarial, ; hou2022meta, ). However, nodes in real-world networks typically possess rich attribute information. For example, in citation networks (ye2023top, ), papers are associated with specific keyword domains. Such networks, known as attributed graphs (yang2013community, ), introduce additional complexity for community detection algorithms. To harness the potential of topology and attribute information for attributed community detection (ACD), existing methods, e.g., CommDGI (zhang2020commdgi, ) and ACDM (cheng2023significant, ), map these dual information sources to low-dimensional continuous vector spaces by using embedding techniques. While these methods have demonstrated promising results, we contend that current solutions may not be optimal because they overlook two critical issues: Figure 1. Most studies treat AI (artificial intelligence), CV (computer vision), and ML (machine learning) as independent attributes. However, AI and CV are subfields within the broader domain of ML, implying that they share underlying semantic similarities. ‚Ä¢ Semantic similarity. Semantic similarity refers to the degree of semantic resemblance or the extent of correlation between attributes. For instance, as illustrated in Figure 1, the semantic similarity of attributes can reveal latent relationships between nodes and enhance the attribute cohesiveness of detected communities(jiang2021query, ). However, existing methods usually disregard the semantic similarity between node attributes within communities, leading to the omission of crucial nodes in the detected communities. ‚Ä¢ Mesoscopic community structure. Inherent community structure, serving as a crucial mesoscopic description of network topology, imposes constraints on node representation at a higher structural level. If the mesoscopic community structure is considered to guide network embedding, the results would remain robust against minor local changes in the network structure, such as node noise and the addition or deletion of edges or nodes (liu2022robust, ). However, existing methods primarily focus on the pairwise connections of micro-structure between nodes (zhang2021spectral, ), rendering the results overly sensitive to minor changes in microscopic structure. To address these limitations, we propose a novel attributed community detection model based on a heterogeneous graph attention network (HAN), termed HACD. To tackle the first issue, we initially treat node attributes as another type of node, transforming real-world attributed networks into a heterogeneous graph structure. Subsequently, we propose an attribute-level attention mechanism (A2M), which utilizes weighted aggregation based on attention coefficients to identify key attributes within each community and employs an attention-based similarity metric to compute the distance between the semantic meanings of different attributes. By embedding with A2M, the representation learns the importance of different attributes and captures the semantic similarity between node attributes. This semantic similarity fully reflects the latent relationships between nodes, achieving attribute cohesion within communities. Furthermore, to address sensitivity issues and enhance robustness, we introduce a community membership function (CMF). By encoding initial community membership information and introducing a new modularity function to formulate CMF as a modularity optimization problem, we guide network embedding to explore mesoscopic community structures, ensuring the structural cohesiveness of detected communities. Our principal contributions can be summarized as follows: ‚Ä¢ We first identify two critical problems affecting attributed community detection: semantic similarity and mesoscopic community structure. ‚Ä¢ We propose a novel attributed community detection model, HACD. We construct the attribute network as a heterogeneous graph structure and introduce the heterogeneous graph neural network into attributed community detection tasks. We propose an attribute-level attention method to explore the semantic similarity between node attributes, as well as design a community membership function to obtain the mesoscopic community structure. ‚Ä¢ We conduct extensive experiments demonstrating the effectiveness and efficiency of HACD, showing superiority over state-of-the-art community detection methods in attributed graph datasets."
https://arxiv.org/html/2411.01424v1,Effective Community Detection Over Streaming Bipartite Networks (Technical Report),"The streaming bipartite graph is extensively used to model the dynamic relationship between two types of entities in many real-world applications, such as movie recommendations, location-based services, and online shopping. Since it contains abundant information, discovering the dense subgraph with high structural cohesiveness (i.e., community detection) in the bipartite streaming graph is becoming a valuable problem. Inspired by this, in this paper, we study the structure of community on the butterfly motif in the bipartite graph. We propose a novel problem, named Community Detection over Streaming Bipartite Network (CD-SBN), which aims to retrieve qualified communities with user-specific query keywords and high structural cohesiveness at snapshot and continuous scenarios. In particular, we formulate the user relationship score in the weighted bipartite network via the butterfly pattern and define a novel (k,r,œÉ)ùëòùëüùúé(k,r,\sigma)( italic_k , italic_r , italic_œÉ )-bitruss as the community structure. To efficiently tackle the CD-SBN problem, we design effective pruning strategies to rule out false alarms of (k,r,œÉ)ùëòùëüùúé(k,r,\sigma)( italic_k , italic_r , italic_œÉ )-bitruss and propose a hierarchical synopsis to facilitate the CD-SBN processing. Due to the dynamic of streaming bipartite networks, we devise an efficient procedure for incremental graph maintenance. We develop an efficient algorithm to answer the snapshot and continuous CD-SBN query by traversing the synopsis and applying the pruning strategies. With extensive experiments, we demonstrate the efficiency and effectiveness of our proposed CD-SBN processing approach over real/synthetic streaming bipartite networks.","Nowadays, the management of bipartite graphs that involve two distinct types of nodes (e.g., customers and products, visitors and check-in locations, users and clicked websites) has been extensively studied in many real applications such as Online Recommendation from user-product transaction graphs (Fan et al., 2022; Weng et al., 2022; Kou et al., 2023), location-based services with visitor-location check-in graphs (Hang et al., 2018; Zhou et al., 2021a; Wang et al., 2017; Kim et al., 2022), and behavior analysis in user-Webpage click graphs (Sun et al., 2009; Neria et al., 2017). The community detection (CD) over such bipartite graphs has recently become increasingly important for bipartite graph analysis and mining (Barber, 2007; Bouguessa and Nouri, 2021; Yen and Larremore, 2020; Zhou and Amini, 2019), which uncovers hidden and structurally cohesive bipartite subgraphs (or groups). The CD problem over bipartite graphs has many useful real applications, such as online product recommendation and marketing, user behavior analysis, malicious user group identification, and so on. Although many prior works (Wang et al., 2023; Abidi et al., 2023a; Wang et al., 2021; Zhang et al., 2024) studied the CD problem in static bipartite graphs, real-world bipartite graphs are usually dynamically changing over time (e.g., updates of purchase transactions, check-ins, or website browsing). Therefore, in this paper, we will consider the Community Detection over Streaming Bipartite Network (CD-SBN), upon graph updates (e.g., edge insertions/deletions or edge weight changes). Below, we give a motivating example of our CD-SBN problem in the application of detecting malicious user groups (communities) over a user-website click bipartite graph. Example 0. (Anomaly Detection for the Cybersecurity) Figure 1(a) illustrates an example of a clickstream bipartite network Gt‚àí1subscriptùê∫ùë°1G_{t-1}italic_G start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT at timestamp (t‚àí1)ùë°1(t-1)( italic_t - 1 ), which contains 3 user vertices, u1‚àºu3similar-tosubscriptùë¢1subscriptùë¢3u_{1}\sim u_{3}italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ‚àº italic_u start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT, 2 website vertices, v1subscriptùë£1v_{1}italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and v2subscriptùë£2v_{2}italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, and 5 edges (ui,va)subscriptùë¢ùëñsubscriptùë£ùëé(u_{i},v_{a})( italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ) between two types of nodes, user uisubscriptùë¢ùëñu_{i}italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and website vasubscriptùë£ùëév_{a}italic_v start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT (for 1‚â§i‚â§31ùëñ31\leq i\leq 31 ‚â§ italic_i ‚â§ 3 and 1‚â§a‚â§21ùëé21\leq a\leq 21 ‚â§ italic_a ‚â§ 2). Here, each website vertex vasubscriptùë£ùëév_{a}italic_v start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT (for a=1,2ùëé12a=1,2italic_a = 1 , 2) has a set of keywords that represent the website‚Äôs features (e.g., {bank, finance} for a bank website v1subscriptùë£1v_{1}italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT). Moreover, each edge (ui,va)subscriptùë¢ùëñsubscriptùë£ùëé(u_{i},v_{a})( italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ) (for i=1‚àº3ùëñ1similar-to3i=1\sim 3italic_i = 1 ‚àº 3 and a=1,2ùëé12a=1,2italic_a = 1 , 2) is associated with an integer weight, indicating the frequency that user uisubscriptùë¢ùëñu_{i}italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT accessed website vasubscriptùë£ùëév_{a}italic_v start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT for the past five minutes. As shown in Figure 1(a), to detect malicious groups of users accessing sensitive/suspicious websites, the network security officer may want to specify some query keywords of websites (e.g., ‚Äúbank‚Äù and ‚Äúmalware‚Äù), and identify some group (community), g1subscriptùëî1g_{1}italic_g start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, of users (e.g., u2subscriptùë¢2u_{2}italic_u start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT and u3subscriptùë¢3u_{3}italic_u start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT) who frequently visit some common websites with query keywords (e.g., bank website v1subscriptùë£1v_{1}italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and hacking website v2subscriptùë£2v_{2}italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT). The officer will warn those suspicious users in the resulting community g1subscriptùëî1g_{1}italic_g start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and/or take immediate actions (e.g., recording the evidence for the police). Figure 1(b) shows the streaming bipartite network Gtsubscriptùê∫ùë°G_{t}italic_G start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT at timestamp tùë°titalic_t, with a newly inserted edge (u1,v1)subscriptùë¢1subscriptùë£1(u_{1},v_{1})( italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) and an expired edge (u3,v2)subscriptùë¢3subscriptùë£2(u_{3},v_{2})( italic_u start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) (i.e., user u3subscriptùë¢3u_{3}italic_u start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT has not visited website v2subscriptùë£2v_{2}italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT for the past five minutes). In this case, at timestamp tùë°titalic_t, the community in Gtsubscriptùê∫ùë°G_{t}italic_G start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is changed to g2subscriptùëî2g_{2}italic_g start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT (rather than g1subscriptùëî1g_{1}italic_g start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT at timestamp (t‚àí1)ùë°1(t-1)( italic_t - 1 )), as circled by the orange dashed line. ‚ñ†‚ñ†\blacksquare‚ñ† In addition to the example above, the CD-SBN problem has many other real applications. For example, in the customer-product transaction bipartite graph, we can use the CD-SBN results to find communities of customers who recently have a purchasing behaviors for online advertising and marketing. Similarly, in the visitor-location check-in bipartite graph (e.g., from Yelp (Inc., 2024)), we can identify a group of visitors who frequently check in at some common points of interest (POIs) and provide them with group buying coupons/discounts (e.g., Groupon (Groupon, 2024)). In this paper, we formulate and tackle the CD-SBN problem, which obtains all bipartite communities with the user-specified query keywords (e.g., POIs‚Äô features) and high structural cohesiveness (e.g., small distance between any two users and high structural score of the community). In particular, we formally define the community semantics in the context of bipartite graphs, that is, a keyword-aware and structurally dense bipartite subgraph (i.e., a so-called (k,r,œÉ)ùëòùëüùúé(k,r,\sigma)( italic_k , italic_r , italic_œÉ )-bitruss that contains query keywords, as will be described in Section 2.4). We consider both snapshot and continuous scenarios of our CD-SBN problem, which detect communities over a snapshot of streaming bipartite network Gtsubscriptùê∫ùë°G_{t}italic_G start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT at timestamp tùë°titalic_t, or continuously monitor the CD-SBN answer set for each registered community constraint upon streaming graph updates, respectively. Due to the large scale of bipartite graphs and rapid streaming graph changes, efficient processing of the snapshot and continuous CD-SBN is rather challenging. In order to tackle the challenges, in this paper, we propose effective pruning strategies with respect to the community constraints (e.g., query keywords, community radius, edge supports, and community scores) to significantly reduce the CD-SBN problem space. We design a hierarchical synopsis to effectively facilitate the candidate community search, and develop efficient snapshot and continuous algorithms to retrieve or incrementally maintain actual community answers (via our proposed synopsis and pruning methods), respectively. In this paper, we make the following major contributions. (1) We formally define the problem of the community detection over streaming bipartite network (CD-SBN) containing snapshot and continuous queries in Section 2. (2) We present a framework to efficiently process the two types of queries with incremental updates of the bipartite streaming graph in Section 3. (3) We design effective community-level pruning strategies to reduce the search space of the CD-SBN problem in Section 4. (4) We propose a novel hierarchical synopsis to facilitate CD-SBN query processing and devise an efficient procedure for incremental graph maintenance in Section 5. (5) We develop an efficient algorithm with effective synopsis-level pruning strategies to answer the snapshot CD-SBN query and another algorithm to maintain the result set of a continuous CD-SBN query with low computational cost in Section 6. (6) We demonstrate the efficiency and effectiveness of our CD-SBN processing approach through extensive experiments over synthetic/real-world graphs in Section 7. Section 8 reviews the related work on community search/ detection on unipartite graphs and static/streaming bipartite graphs. Finally, Section 9 concludes this paper."
https://arxiv.org/html/2411.01394v2,Centrality in Collaboration: A Novel Algorithm for Social Partitioning Gradients in Community Detection for Multiple Oncology Clinical Trial Enrollments,"Patients at a comprehensive cancer center who do not achieve cure or remission following standard treatments often become candidates for clinical trials. Patients who participate in a clinical trial may be suitable for other studies. A key factor influencing patient enrollment in subsequent clinical trials is the structured collaboration between oncologists and most responsible physicians. Possible identification of these collaboration networks can be achieved through the analysis of patient movements between clinical trial intervention types with social network analysis and community detection algorithms. In the detection of oncologist working groups, the present study evaluates three community detection algorithms: Girvan-Newman, Louvain and an algorithm developed by the author. Girvan-Newman identifies each intervention as their own community, while Louvain groups interventions in a manner that is difficult to interpret. In contrast, the author‚Äôs algorithm groups interventions in a way that is both intuitive and informative, with a gradient evident in social partitioning that is particularly useful for epidemiological research. This lays the groundwork for future subgroup analysis of clustered interventions.","When cancer patients complete standard treatments, and have not responded with being cured or in remission, they become candidates for clinical trials. These clinical trials are regulated studies registered by Health Canada111For more information, see https://www.canada.ca/en/health-canada/services/clinical-trials.html as opposed to quality assurance studies222Quality assurance studies in the context of medical studies are studies which look at drugs which are already approved for use, but the goals are focused on other aspects of care such as drug delivery or quality of care.. Patients who qualify may have been screen failures for other trials, have experienced progressive disease, or are receiving maintenance therapy and have been referred to a clinical trial by their oncologist or most responsible physician. Ground truth shows that collaboration networks between oncologists is a primary factor for further engagement in subsequent clinical trials by patients after completion of the given clinical trial that they are enrolled in. A possible approach to understanding the structure of these collaboration networks is through use of social network analysis (SNA) and community detection algorithms. Social network analysis examines individual entities and their relationships among them. The data is represented as a ‚Äúgraph‚Äù where individual entities are referred to as ‚Äúnodes‚Äù and their relationships between them as ‚Äúedges‚Äù, which may be directional if specified (see Figure 1). A primary area of study in SNA is the analysis of interconnectivity of nodes, called ‚Äùcommunities‚Äù and identification of clusters through the use of algorithms called ‚Äùcommunity detection algorithms‚Äù. Rostami et al1 (2023) note that there is no specific model which describes exactly what a ‚Äùcommunity‚Äù is. Generally, community detection algorithms employ specific optimization strategies to partition a large-scale complex network into a set of disjoint and compact subgroups, often (but not always) without prior knowledge regarding the number of subgroups and their sizes. Rostami et al further note that it is commonly acknowledged that there is no unique community detection algorithm that can accommodate all kinds of graphs, because of the inherent variability in network structures and their respective objective(s). Figure 1: Two simple graphs with directed and undirected edges. Direction is noted by arrowheads at the end of the edges. Application of community detection algorithms with oncology clinical trial data has been preformed in the past. Georgiev et al2 (2011) applied the Girvan-Newman3 (2002) algorithm and noted a lack of cohesion among researchers who studied treatments for multiple myeloma. Haq and Wang4 (2016) applied the Louvain algorithm (by Blondel et al5 (2008)) to identify communities of cancer patients with significantly different survival curves. The present study applies SNA, and compares multiple community detection algorithms to identify collaboration networks between oncologists through the interventions studied in clinical trials via enrollment data of patients in multiple, nonconcurrent clinical trials. Inspired by work from Gorgiev et al (2011), Haq and Wang (2016), Ostovari and Yu6 (2019) and Bissoyi and Patra7 (2020) this research considers the Girvan-Newman and Louvain algorithms and compares them to an author-developed algorithm, referred to as ‚ÄùSmith-Pittman‚Äù333Named after the author and his co-supervisor, Tyler Pittman., to identify collaboration networks between clinical trials classified by intervention."
https://arxiv.org/html/2411.01242v1,"Assessing the Impact of Sampling, Remixes, and Covers on Original Song Popularity","Music digitalization has introduced new forms of composition known as ‚Äúmusical borrowings‚Äù, where composers use elements of existing songs‚Äîsuch as melodies, lyrics, or beats‚Äîto create new songs. Using Who Sampled data and Google Trends, we examine how the popularity of a borrowing song affects the original. Employing Regression Discontinuity Design (RDD) for short-term effects and Granger Causality for long-term impacts, we find evidence of causal popularity boosts in some cases. Borrowee songs can revive interest in older tracks, underscoring economic dynamics that may support fairer compensation in the music industry.","Digitization has drastically transformed the production, distribution, and consumption of music. Advances in data storage, transmission, and processing technologies have brought about significant changes in the music industry, making digital music predominant. This shift has led to new forms of composition, known as ‚Äúmusical borrowings‚Äù, where composers and producers draw on existing musical influences to create new works, thereby expanding the boundaries of musical genres. An example of musical borrowing is Beyonc√©‚Äôs 2003 hit ‚ÄúCrazy in Love‚Äù one of her most iconic songs. This work calls this song a borrowee. The borrowee‚Äôs introduction, marked by triumphant brass, drew significant attention. The brass section was sampled from The Chi-Lites‚Äô 1970s song ‚ÄúAre You My Woman‚Äù, here called a borrowed song. Created in a pre-digital era, the borrowed‚Äôs influence on ‚ÄúCrazy in Love‚Äù is evident, although its precise contribution to the song‚Äôs success remains difficult to quantify. We explore three primary forms of musical borrowing: sampling, remixes, and covers. Sampling, a technique popularized in the late 1970s, involves using fragments of pre-existing songs. Widely used in hip-hop, electronic, and experimental music, sampling allows for creative expression through the manipulation and combination of different tracks. On the other hand, remixes involve transforming an existing song by altering its structure, rhythm, or effects, often revitalizing it for new audiences. Finally, covers involve re-recordings that usually offer a fresh interpretation and sometimes even surpass the original in popularity. Musical borrowings raise significant legal issues, particularly regarding the copyright laws that govern these practices ‚Äì debates around fair use often center on whether the borrowing negatively impacts the market for the original work. A study titled ‚ÄúSampling Increases Music Sales: An Empirical Copyright Study‚Äù highlights that, in the U.S., the Supreme Court considers the market impact on the original song as a key factor in determining fair use [1]. The research suggests that digital sampling can boost sales of the original songs, indicating that musical borrowings may, in some cases, qualify as fair use. Understanding the consequences of using samples in original songs can contribute to fairer legal discussions and more informed decisions by musicians and managers. Our research aligns with the above discussion, but it also presents counter-arguments, as we now detail. Specifically, we focus on how the popularity of a borrowee song (cover, sample, or remix) influences the popularity of the borrowed song. Data from WhoSampled111https://www.whosampled.com, a crowdsourced website cataloging samples, remixes, and covers, and Google Trends222https://trends.google.com, which measures relative search interest, provide the basis for this analysis. To effectively measure an effect, we employ two techniques. First, we explore Regression Discontinuity Design [2] (RDD) to measure the immediate causal impact of the borrowee‚Äôs release. Next, we make use of Granger Causality [3] as a means to understand the lasting impact of a borrower after release. As an example of a borrowee that impacts a borrowed song, we show in Figure 1 the song ‚ÄúSomebody‚Äù by Natalie La Rose (released in late 2014) that sampled ‚ÄúShots‚Äù from LMFAO. Here, we can see that ‚ÄúSomebody‚Äù renews interest in the LMFAO song. Figure 1: Search Interest Over Time for ‚ÄúShots‚Äù (LMFAO) and ‚ÄúSomebody‚Äù (Natalie La Rose). The release month is the shaded region. In general, it is important to note that establishing causality is challenging. Among the approximately 884 instances of musical borrowings analyzed, we identified 182 cases (20%) showing any causal evidence. From these, our results show that only a fraction of the borrowees (45% or 82 out of 182) have an immediate causal impact via our RDD study. Out of these, 64% (or 117 out of 182) of these cases show a lasting effect via Granger Causality. It should be emphasized that this analysis focuses on web search interest, rather than sales metrics. Nonetheless, even with a relatively limited dataset, we observed some influence of borrowees on the originals. Thus, we hope that our research sheds light on the actual implications of sampling. In the following sections, we begin with a review of related work (Section 2), followed by a description of our datasets and methodology (Section 3) and our results (Section 4). Finally, we conclude with a discussion of our findings and their broader implications (Section 5)."
https://arxiv.org/html/2411.01239v1,I‚Äôve heard this before: Initial Results on TikTok‚Äôs impact on the re-popularization of songs.,"With over a billion active users, TikTok‚Äôs video-sharing service is currently one of the largest social media websites. This rise in TikTok‚Äôs popularity has made the website a central platform for music discovery. In this paper, we analyze how TikTok helps to revitalize older songs. To do so, we use both the popularity of songs shared on TikTok and how the platform allows songs to propagate to other places on the Web. We analyze data from TokBoard, a website measuring such popularity over time, and Google Trends, which captures songs‚Äô overall Web search interest. Our analysis initially focuses on whether TokBoard can cause (Granger Causality) popularity on Google Trends. Next, we examine whether TikTok and Google Trends share the same virality patterns (via a Bass Model). To our knowledge, we are one of the first works to study song re-popularization via TikTok.","With the rise of novel social media websites, online music discovery becomes an ever changing ecosystem[1]. While major players such as Spotify and YouTube Music dominate the market, short video-sharing websites like TikTok, the focus of this work, also promote music to end users. As stated, TikTok is mostly a video-sharing service. After a user uploads a short video, other users can use these snippets to create their videos. In TikTok, songs are present in accompanying video memes or music videos. This phenomenon is a breeding ground for audio to become viral (and thus popular). Nevertheless, many audio snippets on TikTok are from major artists and record labels. With the website becoming a platform for discovering songs from such artists, a question arises: How do short video-sharing websites impact the revival of older songs? Figure 1: Popularity trend for the song ‚ÄùWhere is the Love?‚Äù from group ‚ÄùBlack Eyed Peas‚Äù on both Tok Board and Google Trends. The dates include the peak popularity for both platofms To tackle our research goal of understanding TikTok‚Äôs impact on the re-popularization of songs, we perform an analysis of songs that were (1) released on or before September 2016 (TikTok‚Äôs release date), and (2) trending on TikTok at some point in time. To define this second factor, we use data from TokBoard111tokboard.com ‚Äì A website aggregating popularity data from TikTok. To motivate our work, we cite examples such as ‚ÄúDreams‚Äù222https://pitchfork.com/news/watch-mick-fleetwood-recreate-viral-fleetwood-mac-dreams-tiktok/ by Fleetwood Mac which was released in 1977, and ‚ÄúWhere is the Love?‚Äù by The Black Eyed Peas333https://newsroom.tiktok.com/en-us/year-on-tiktok-music-2020. This second example is shown in Figure 1. As reported by news outlets, the viral trend of both songs on TikTok affected the song‚Äôs overall popularity online and offline. This is exemplified in the figure when we see the Tok Board curve in a rise-and-fall viral trend, whereas the GTrends curve increased after the song went viral. Motivated by such examples, our study will mainly focus on answering the two research questions described below: RQ1: Is it possible to predict Web search popularity based on TikTok popularity? Here, we shall employ the Granger Causality Test [2], in which we evaluate if the TokBoard popularity curve can predict the Google Trends curve. If this is so, we have evidence that TikTok is causing Web search popularity. While this question sheds light on causality and prediction, we still need to understand the viral patterns of both websites. This is why we complement this question with the next one. RQ2: Do the viral trends of TikTok transfer to Google Trends? In this question, we shall fit TokBoard and Google Trend curves with the Bass Model [3]. The Bass Model is a simple differential equation that captures how products get adopted by a population. It has two interpretable parameters to understand virality: (1) Innovation, or who are the adopters that consume a song without influence, and (2) Immitation, who are the adopters influenced by others. After our causal analysis, we aim to understand whether TikTok trends are reflected in Web searches. Overall, and to the best of our knowledge, ours is the first work to look into the popularity of both TikTok and Web Search based on these two aspects. We also release all code and data for this paper to foster reproducibility.444https://github.com/brenomatos/tiktok-lamir Before discussing our dataset and results, we take the time to summarize related work in the next section."
https://arxiv.org/html/2411.01143v1,A Large-scale Time-aware Agents Simulation forInfluencer Selection in Digital Advertising Campaigns,"In the digital world, influencers are pivotal as opinion leaders, shaping the views and choices of their influencees. Modern advertising often follows this trend, where marketers choose appropriate influencers for product endorsements, based on thorough market analysis. Previous studies on influencer selection have typically relied on numerical representations of individual opinions and interactions, a method that simplifies the intricacies of social dynamics. In this work, we first introduce a Time-aware Influencer Simulator (TIS), helping promoters identify and select the right influencers to market their products, based on LLM simulation. Specifically, after an advertisement is posted, we introduce a time simulation dimension into the process, which is rarely explored in previous simulation environments. This time-based simulation predicts user activity by modeling the user timeline and content lifecycle. It filters out most inactive users and contents, aligns with real-world behavior, and significantly reduces the scope of the simulation. Each active user is represented as an LLM-based agent, drawing from their interaction history to deduce their profile and interests. These user agents will predict their behavior in response to influencer advertising. After a period, we will obtain the interaction network generated from the advertisement‚Äôs dissemination. Subsequently, we develop a ranking metric aimed at identifying influencers who have the highest potential to drive product purchases, based on the derived interaction network. To validate our approach, we conduct experiments on the public advertising campaign dataset SAGraph which encompasses social relationships, posts, and user interactions. The results show that our method outperforms traditional numerical feature-based approaches and methods using limited LLM agents. Our research shows that simulating user timelines and content lifecycles over time simplifies scaling, allowing for large-scale agent simulations in social networks. Additionally, LLM-based agents for social recommendations and advertising offer substantial benefits for decision-making in promotional campaigns.","Social networks and social media platforms have revolutionized the way we interact and share information, creating vibrant online communities where influencers and their influencees engage in continuous dialogue. These influencers frequently make recommendations, whether for products, services, or lifestyle choices, which can have a considerable impact on their influencees‚Äô decisions and preferences. In this realm of digital advertising, promoters strategically select a limited number of influencers to maximize their product‚Äôs influence and visibility in the market (Zhang et al., 2015; Mallipeddi et al., 2022; Lenger, 2022; Qiu et al., 2018). Various methods have been developed for simulating opinions and selecting influencers in social networks. These approaches often reduce user opinions and interactions to numerical values and develop algorithms to predict changes in these numbers (Zhu et al., 2021; Zhou and Zhang, 2023, 2023; Dutta et al., 2020). For instance, (Zhu et al., 2021) focuses on the optimization problem of suggesting new links to reduce polarization and disagreement in a social network, without altering initial opinions. However, these models often oversimplify reality, as people‚Äôs opinions are diverse and nuanced, and the way others influence them is complex. For example, individuals may be influenced by an influencer‚Äôs perspective on one topic but not on another. Moreover, numerical representations fail to capture the nuances of textual dialogues, the reasons for opinion shifts, and the complexities of viewpoint evolution. (Wang et al., 2022; Chan, 2022; Gu et al., 2017). Nowadays, researchers have been employing LLMs as agents for simulations (Chen et al., 2023c, b; Gao et al., 2024; T√∂rnberg et al., 2023), and in this work, we utilize LLM simulation for the influencer selection task. Directly using an LLM for each user would result in tens of thousands of agents, leading to significant time and financial costs when deploying a large-scale agent society. Correspondingly, we note that although the network is large, the number of active users within a given time frame is limited. Figure 1 shows an example of a user interaction timeline with social media content, illustrating that users are inactive from 9:00 AM to 8:00 PM. The content also has a lifecycle in that it spreads quickly when current and engaging but as engagement decreases and the content ages, it fades from the public. With the above observations, in this work, we propose a Time-aware Influencer Simulator (TIS) for digital advertising campaigns, which aims to select suitable influencers for products by modeling LLM-based agents that track the temporal paths of both individuals and the content they generate. The TIS provides several advantages compared with existing methods. Firstly, TIS effectively manages the number of active agents and content on social networks within a given time frame, reducing the simulation complexity and enabling the creation of a large-scale agent society. Secondly, TIS constructs an interaction network for users at each time window, and as time progresses, this network continues to expand, accurately simulating the interactive behavior of users on social networks, which can be utilized for monitoring the progression of future events. Specifically, in TIS, we start by modeling the temporal trajectory, which includes modeling both the User Timeline (UT) and the Content Lifecycle (CL). UT is based on users‚Äô past interaction data, while CL relies on the interaction history of the content, i.e., the interaction time and counts towards the content. By modeling the temporal trajectory, we can accurately estimate the number of active users needed for behavior simulation during specific periods and determine when the simulated content begins to lose its dissemination power. Next, we defined the patterns for simulating user behavior with an LLM-based agent in the context of advertising recommendations. These patterns include self-awareness, social behavior prediction, and self-assessment. We anticipate that the agent will possess a clear understanding of itself, provide appropriate feedback on social discourse‚Äîeither by commenting or by directly ignoring it‚Äîand accurately assess the level of support that simulated behaviors offer for the ultimate purchasing behavior related to the published advertisement. Finally, in our simulation of influencer selection, we controlled the active agents based on the UT process and monitored their behaviors over a specified period. We focused on content identified by CL as being in its dissemination phase. By constructing a social network that evolves with time, we assessed the effectiveness of the advertisement‚Äôs spread during the specified time frame, thereby reflecting the influence of the influencers. We conducted experiments on the social advertising recommendation dataset SAGraph (Zhang et al., 2024), which encompasses a wide range of user interactions and behaviors, providing a rich ground for analyzing and modeling the spread of influence and the effectiveness of advertising campaigns within various domains. By leveraging this comprehensive dataset, we can delve into the intricacies of user engagement and the dynamics of social networks in a more realistic and nuanced manner. Our contribution can be summarized as follows: Firstly, we develop a temporal trajectory simulation mechanism, incorporating UT and CL modules, which provide a scalable solution for simulating large numbers of agents. Secondly, we propose a social network simulation framework driven by LLM agents that operate within specified time windows, enabling effective monitoring of information spread across social platforms. Finally, experiments in digital advertising campaigns demonstrate that our framework closely mirrors real-world influencer selection, reducing the need for extensive market surveys and aiding economic decision-makers in strategy development. Figure 2. (a) Temporal Trajectory Simulation identifies agents and content to simulate within specific time frames, focusing on the user timeline and content lifecycle. (b) Agent Simulation uses LLMs to simulate behaviors, including self-awareness, social behavior prediction, and self-assessment. (c) Social Network Simulation leverages the TIS framework to model social network dynamics post-advertisement, offering insights into campaign effectiveness over time."
https://arxiv.org/html/2411.00975v1,Analyzing Social Networks of Actors in Movies and TV Shows,"The paper offers a comprehensive analysis of social networks among movie actors and directors in the film industry. Utilizing data from IMDb and Netflix, we leverage Python and NetworkX to uncover valuable insights into the movie industry‚Äôs intricate web of collaborations. Key findings include identifying the top actors and directors in the OTT sector, tracking the rise of movies on OTT platforms, and analyzing centrality measures for actors. We also explore the hidden patterns within the movie data, unveiling the shortest paths between actors and predicting future collaborations. Cluster analysis categorizes movies based on various criteria, revealing the most insular and liberal clusters and identifying crossover actors bridging different segments of the industry. The study highlights that actors predominantly collaborate within language groups, transcending national boundaries. We investigate the degree of isolation of Bollywood from global cinema and identify actors working across world clusters. The project provides valuable insights into the evolving dynamics of the film industry and the impact of OTT platforms, benefiting industry professionals, scholars, and enthusiasts.","The rapid rise of Over-the-top (OTT) streaming services alongside the long-established film industry is driving a major and transformative shift in the entertainment landscape [1, 2, 3, 4]. A lot of research has been done already that studied the performance of OTT platforms and their impact on the market [5, 6, 7]. For performers and artists, this is a critical time that offers them a unique chance to engage with audiences around the world [8, 9]. In this digitally infused era, a strong online presence has become essential to success, and the OTT sector has become a vital platform for actors and filmmakers to flourish on. But in the fiercely competitive OTT space, the complex web of interactions and relationships between market players becomes a critical determinant of success [10, 11]. A thorough understanding of the interactions between actors, directors, producers, and the industry at large is imperative in this digital age. The implications of the entertainment industry‚Äôs ongoing digital age adaptation are wide-ranging, encompassing aspects such as cross-industry dynamics, collaboration patterns, and the changing nature of actor relationships. SNA, a data science discipline, provides a powerful framework for investigating these interactions by leveraging the power of networks and graph theory [12, 13, 14, 15]. SNA has a long history of revealing hidden patterns in a variety of fields, from determining how illnesses spread to tracking the dissemination of ideas. In order to investigate the intricate connections within the traditional film industry and the OTT industry, we are utilizing SNA for movie network analysis. Our data collection includes publicly available sources, with platforms like IMDb and Netflix functioning as crucial repositories. Using the NetworkX package and the Python programming language, the analysis allows us to carefully examine this data and reveal important insights. Additionally, our study explores the centrality metrics of different players, illuminating their roles within the networks and exposing the dominant figures that influence the sector. We investigate the idea of the shortest path between players, revealing hidden relationships that specify the paths leading to cooperation. Furthermore, in order to predict future partnerships, we utilize advanced link prediction approaches like resource allocation, Jaccard cosine similarity, common neighbour analysis, and Adamic Adar index. The study explores the fascinating world of clusters within the network, exposing subclusters within clusters, raising important questions about Bollywood‚Äôs level of exclusion from the international film industry, and identifying performers who work across different countries. This research project is motivated by the investigation of these interrelated networks and the complex relationships that characterize the cinema and over-the-top (OTT) industries. The following sections of the paper include a literature survey, methodology, results, and conclusion, as well as future work."
https://arxiv.org/html/2411.02003v1,Against Multifaceted Graph Heterogeneity via Asymmetric Federated Prompt Learning,"Federated Graph Learning (FGL) aims to collaboratively and privately optimize graph models on divergent data for different tasks. A critical challenge in FGL is to enable effective yet efficient federated optimization against multifaceted graph heterogeneity to enhance mutual performance. However, existing FGL works primarily address graph data heterogeneity and perform incapable of graph task heterogeneity. To address the challenge, we propose a Federated Graph Prompt Learning (FedGPL) framework to efficiently enable prompt-based asymmetric graph knowledge transfer between multifaceted heterogeneous federated participants. Generally, we establish a split federated framework to preserve universal and domain-specific graph knowledge, respectively. Moreover, we develop two algorithms to eliminate task and data heterogeneity for advanced federated knowledge preservation. First, a Hierarchical Directed Transfer Aggregator (HiDTA) delivers cross-task beneficial knowledge that is hierarchically distilled according to the directional transferability. Second, a Virtual Prompt Graph (VPG) adaptively generates graph structures to enhance data utility by distinguishing dominant subgraphs and neutralizing redundant ones. We conduct theoretical analyses and extensive experiments to demonstrate the significant accuracy and efficiency effectiveness of FedGPL against multifaceted graph heterogeneity compared to state-of-the-art baselines on large-scale federated graph datasets.","Federated Graph Learning (FGL) has become an attractive research direction for distributed Graph Neural Network (GNN) optimization on isolated graph data without explicit information exposure (Liu et al., 2024). Specifically, in FGL, several participants maintain their private graphs collected from different domains, and they aim to collaboratively train GNNs that can provide beneficial knowledge of raw graph data for particular downstream tasks, respectively. Recent FGL studies have achieved significant progress mainly in learning more effective graph representation to enhance downstream task prediction (Guo et al., 2024). For example, FedSage+ (Zhang et al., 2021b) is proposed based on a standard FGL pipeline where subgraphs from different silos are independently distributed, and improve the node classification via missing edge generation. Despite these advancements, the difficulty of graph data heterogeneity remains paramount. Graphs inherently differ in terms of node and edge types, structural configurations, and other characteristics. Traditional graph learning methodologies often operate under the assumption of homogeneity, which can lead to biased or suboptimal models when applied to heterogeneous graph domains. Addressing graph data heterogeneity is crucial for FGL, as it enables the model to capture diverse intrinsic graph characteristics and enhances overall performance. Therefore, FedStar (Tan et al., 2023b) investigates the data heterogeneity of graph features and shared knowledge, which attempts to achieve outperformance in a Non-Independent and Non-Identically Distributed (Non-IID) issue setting. In addition, FedLIT (Xie et al., 2023) detects link-type heterogeneity to decrease the harmful structure to allow more beneficial message passing. Generally, these works designed effective methods to tackle the graph data heterogeneity in terms of features and structures, where they demand participants to train their models for a consistent downstream task. However, while attempts have been made to address data heterogeneity, the critical issue of task heterogeneity remains unexplored. FGL participants originate from various sectors, each with its own set of tasks and data characteristics. For instance, healthcare providers may focus on patient data analysis, while financial institutions might prioritize fraud detection. Addressing task heterogeneity allows FGL to adapt to these varied needs, enhancing its applicability and relevance across different applications. Existing federated algorithms, which typically assume a uniform model structure across clients, cannot accommodate the need for diverse architectures in graph learning (Sun et al., 2023b). This limitation complicates parameters and knowledge sharing between clients working on different tasks, potentially causing inferior effectiveness and efficiency in collaborative optimization. To effectively optimize models against multifaceted heterogeneity, we necessitate a system not only to preserve common knowledge adaptable to various tasks but also to generalize across divergent graph data distributions. Therefore, the core challenge of this work is to simultaneously overcome the multifaceted heterogeneity for more effective and efficient FGL. To address the challenge, we propose a Federated Graph Prompt Learning (FedGPL) framework to federally fine-tune graph models on heterogeneous tasks and data via an efficient prompt-based asymmetric knowledge transfer. Overall, we establish a split framework to simultaneously achieve a universal graph representing and personalized graph prompting to preserve global and local knowledge, respectively. Then, we design tailored algorithms to disentangle and address task and data heterogeneity. On the server side, we develop a Hierarchical Directed Transfer Aggregator (HiDTA) to extract and share asymmetrically beneficial knowledge among task-heterogeneous participants via personalized federated aggregation. On the client side, we devise a lightweight prompting module, called Virtual Prompt Graph (VPG), to adaptively generate augmented graph data by distilling more dominant information with minor data heterogeneity. We provide theoretical analyses that demonstrate the effectiveness in reducing task and data heterogeneity, as well as the significant reduction of memory and communication costs. Extensive experiments validate that FedGPL outperforms baseline methods across three levels of tasks against multifaceted graph heterogeneity on five datasets. Notably, we evaluate FedGPL in a typical large-scale FGL system consisting of 1111 million node data. Our method achieves 5.3√ó‚àº6.0√ó5.3\times\sim 6.0\times5.3 √ó ‚àº 6.0 √ó GPU memory efficiency, 2.1√ó‚àº3.7√ó2.1\times\sim 3.7\times2.1 √ó ‚àº 3.7 √ó communication efficiency, and 1.3√ó‚àº1.9√ó1.3\times\sim 1.9\times1.3 √ó ‚àº 1.9 √ó training time efficiency. The efficiency superiority demonstrates its scalability for massive FGL participants and large-scale graph data. Our main contributions can be concluded as: (1) To our knowledge, our work is the first to study both task and data heterogeneity in FGL, addressing gaps overlooked by previous research. (2) We propose a federated graph prompt learning framework to effectively enable federated optimization of personalized models among task- and data- heterogeneous participants. (3) We develop an aggregation algorithm to deliver task-specific knowledge based on a transferability-aware hierarchy to asymmetrically enhance model performance. Moreover, we devise a virtual graph prompt to jointly highlight dominant graph structures and alleviate extensive data heterogeneity. (4) We theoretically analyze the mitigation of multifaceted heterogeneity by FedGPL. Besides, we conduct extensive experiments to prove the accuracy and efficiency superiority of FedGPL against FGL with multifaceted heterogeneity on federated graph datasets with million nodes."
https://arxiv.org/html/2411.01410v1,PageRank Bandits for Link Prediction,"Link prediction is a critical problem in graph learning with broad applications such as recommender systems and knowledge graph completion. Numerous research efforts have been directed at solving this problem, including approaches based on similarity metrics and Graph Neural Networks (GNN). However, most existing solutions are still rooted in conventional supervised learning, which makes it challenging to adapt over time to changing customer interests and to address the inherent dilemma of exploitation versus exploration in link prediction. To tackle these challenges, this paper reformulates link prediction as a sequential decision-making process, where each link prediction interaction occurs sequentially. We propose a novel fusion algorithm, PRB (PageRank Bandits), which is the first to combine contextual bandits with PageRank for collaborative exploitation and exploration. We also introduce a new reward formulation and provide a theoretical performance guarantee for PRB. Finally, we extensively evaluate PRB in both online and offline settings, comparing it with bandit-based and graph-based methods. The empirical success of PRB demonstrates the value of the proposed fusion approach. Our code is released at https://github.com/jiaruzouu/PRB","Link prediction is an essential problem in graph machine learning, focusing on predicting whether a link will exist between two nodes. Given the ubiquitous graph data in real-world applications, link prediction has become a powerful tool in domains such as recommender systems [72] and knowledge graph completion [49, 41]. Considerable research efforts have been dedicated to solving this problem. One type of classic research approaches is heuristic-based methods, which infer the likelihood of links based on node similarity metrics [43, 46]. Graph Neural Networks (GNNs) have been widely utilized for link prediction. For example, Graph Autoencoders leverage Message Passing Neural Network (MPNN) representations to predict links [29]. Recently, MPNNs have been combined with structural features to better explore pairwise relations between target nodes [73, 70, 18, 61]. Existing supervised-learning-based methods for link prediction are designed for either the static [73, 70, 18, 61] or relatively dynamic environment [64, 55, 62, 58, 69, 19, 27, 26, 75], they (chronologically) split the dataset into training and testing sets. Due to the dynamic and evolving nature of many real-world graphs, ideal link prediction methods should adapt over time to consistently meet the contexts and goals of the serving nodes. For instance, in short-video recommender systems, both video content and user preferences change dynamically over time [28]. Another significant challenge is the dilemma of exploitation and exploration in link prediction. The learner must not only exploit past collected data to predict links with high likelihood but also explore lower-confidence target nodes to acquire new knowledge for long-term benefits. For example, in social recommendations, it is necessary to prioritize popular users by ‚Äòexploiting‚Äô knowledge gained from previous interactions, while also‚Äòexploring‚Äô potential value from new or under-explored users to seek long-term benefits [7]. Furthermore, while existing works often analyze time and space complexity, they generally lack theoretical guarantees regarding the performance of link prediction. To address these challenges, in this paper, we make the following contributions: Problem Formulation and Algorithm. We formulate the task of link prediction as sequential decision-making under the framework of contextual bandits, where each interaction of link prediction is regarded as one round of decision-making. We introduce a pseudo-regret metric to evaluate the performance of this decision process. More specifically, we propose a fusion algorithm named PRB (PageRank Bandits), which combines the exploitation and exploration balance of contextual bandits with the graph structure utilization of PageRank [59, 42]. Compared to contextual bandit approaches, PRB leverages graph connectivity for an aggregated representation. In contrast to PageRank, it incorporates the principles of exploitation and exploration from contextual bandits to achieve a collaborative trade-off. Additionally, we extend PRB to node classification by introducing a novel transformation from node classification to link prediction, thereby broadening the applicability of PRB. Theoretical Analysis. We introduce a new formulation of the reward function to represent the mapping from both node contexts and graph connectivity to the reward. We provide one theoretical guarantee for the link prediction performance of the proposed algorithm, demonstrating that the cumulative regret induced by PRB can grow sub-linearly with respect to the number of rounds. This regret upper bound also provides insights into the relationship between the reward and damping factor, as well as the required realization complexity of the neural function class. Empirical Evaluation. We extensively evaluate PRB in two mainstream settings. (1) Online Link Prediction. In this setting, each link prediction is made sequentially. In each round, given a serving node, the model is required to choose one target node that has the highest likelihood of forming a link with the serving node. The model then observes feedback and performs corresponding optimizations. The goal is to minimize regret over TùëáTitalic_T rounds (e.g., T=10,000ùëá10000T=10,000italic_T = 10 , 000). We compare PRB with state-of-the-art (SOTA) bandit-based approaches (e.g., [76, 12]), which are designed for sequential decision-making. PRB significantly outperforms these bandit-based baselines, demonstrating the success of fusing contextual bandits with PageRank for collaborative exploitation and exploration. (2) Offline Link Prediction. In this setting, both training and testing data are provided, following the typical supervised learning process. Although PRB is designed for online learning, it can be directly applied to offline learning on the training data. We then use the trained model to perform link prediction on the testing data, comparing it with SOTA GNNs-based methods (e.g., [18, 61]). The superior performance of PRB indicates that principled exploitation and exploration can break the performance bottleneck in link prediction. Additionally, we conduct ablation and sensitivity studies for a comprehensive evaluation of PRB."
https://arxiv.org/html/2411.01329v1,Cloned Identity Detection in Social-Sensor Clouds based on Incomplete Profiles,"We propose a novel approach to effectively detect cloned identities of social-sensor cloud service providers (i.e. social media users) in the face of incomplete non-privacy-sensitive profile data. Named ICD-IPD, the proposed approach first extracts account pairs with similar usernames or screen names from a given set of user accounts collected from a social media. It then learns a multi-view representation associated with a given account and extracts two categories of features for every single account. These two categories of features include profile and Weighted Generalised Canonical Correlation Analysis (WGCCA)-based features that may potentially contain missing values. To counter the impact of such missing values, a missing value imputer will next impute the missing values of the aforementioned profile and WGCCA-based features. After that, the proposed approach further extracts two categories of augmented features for each account pair identified previously, namely, 1) similarity and 2) differences-based features. Finally, these features are concatenated and fed into a Light Gradient Boosting Machine classifier to detect identity cloning. We evaluated and compared the proposed approach against the existing state-of-the-art identity cloning approaches and other machine or deep learning models atop a real-world dataset. The experimental results show that the proposed approach outperforms the state-of-the-art approaches and models in terms of Precision, Recall and F1-score.","Social-sensor cloud services (SocSen services) refer to services whose functional (e.g. time and location) and non-functional (e.g. quality and trust) characteristics are abstracted from data (e.g. texts, images, videos, etc.) posted in social media [1]. These SocSen services can power numerous socially significant and influential applications such as scene reconstruction from social media images, etc. The identities of SocSen service providers (i.e., individuals that post social media data from social media) have increasingly become a target of the cybercriminals in the recent past [2, 3]. One such example of these crimes associated with SocSen service provider identities (i.e. social media users) is identity cloning, which is an attempt by an adversary to steal the identity information of SocSen service providers to register a fake profile. Many recent attempts for identity cloning in social media platforms aimed to exploit SocSen service provider identities via cloning for either theft for financial fraud or deceiving the public. Recent examples illustrate the severity of this problem: Facebook CEO Mark Zuckerberg‚Äôs account was cloned for financial theft111https://www.nytimes.com/2018/04/25/technology/fake-mark-zuckerberg-facebook.html, and a fake Twitter account impersonating Russian President Vladimir Putin gained over one million followers222https://www.abc.net.au/news/2018-11-29/twitter-suspends-account-impersonating-vladimir-putin/10569064. These incidents highlight the critical need for effective measures to detect and prevent identity cloning and other malicious activities. Ensuring the security of social media platforms is essential not only for protecting individual identities but also for maintaining the integrity and trustworthiness of online interactions. Therefore, it is imperative to put in place measures to detect such attempts to keep attackers at bay and make social media a more secure place for social media users. Despite its importance, most social media platforms do not offer automated and integrated identity cloning detection. For instance, Instagram and Twitter currently selectively evaluate identity cloning claims only upon receiving legitimate complaints from end-users 333https://help.instagram.com/446663175382270444https://help.twitter.com/en/rules-and-policies/twitter-impersonation-policy. However, given the rate at which identity cloning attacks occur, such selective approaches can be deemed inadequate to keep social media a safer environment for social media users. Therefore, it is vital to research more proactive and automated approaches that can also withstand the scale at which social media platforms operate. Most existing identity cloning detection approaches (such as [4, 5, 6, 7]) rely on complete SocSen service provider (i.e. social media user) profile data. The performance of these approaches often depends on the availability of comprehensive social media profile information. However, obtaining a comprehensive representation of such profile data is often infeasible due to various reasons. One of the major reasons is that SocSen clouds enable stronger privacy preservation measures not to disclose such information to third-party applications. For example, there has been a growing trend that more third-party websites/apps employ mainstream SocSen cloud APIs for authentication. These websites/apps can only access limited profile information authorized by SocSen clouds. This information is termed as non-privacy-sensitive profile information [8]. Our previous research [8, 9] focuses on developing identity cloning detection approaches based on SocSen service providers‚Äô non-privacy-sensitive profile information. However, SocSen service providers can even opt not to disclose part of the non-privacy-sensitive profile information. For example, during account registration, SocSen clouds such as Twitter have made it mandatory that users provide a username, screen name, email address and phone number, which are known as required fields555https://help.twitter.com/en/using-twitter/create-twitter-account. The users can still opt out of providing the other optional details, such as description, location, etc., which can be accessed by Twitter API. Under such circumstances, cloned user accounts might not expose their full profile information or non-privacy-sensitive profile information in order to reduce the risk of being detected. For example, an adversary can register a cloned profile without including a profile description or adding any post. Therefore, existing identity cloning detection approaches may either fail or perform less in the face of incomplete user profile data since most of the existing approaches are built based on the prerequisite of the existence of the complete profile information or non-privacy-sensitive profile information. According to our experiment results (see Table IX), all the existing identity cloning detection approaches are affected by incomplete profile information. All the existing approaches performed worse when there was incomplete profile information (missing value). Imputation is a technique used to handle missing or incomplete data by filling in the gaps with substitute values. Imputation can be performed using statistical or machine learning methods [10]. To address these issues, we use imputation methods to replace missing values with appropriate estimates. By applying this technique, we can improve the quality of the data and enhance the detection effectiveness. To address the above limitations, we propose a novel approach for SocSen service provider Identity Cloning Detection in the face of Incomplete Profile Data (ICD-IPD). ICD-IPD is specially designed to detect cloned identities based on incomplete non-privacy-sensitive profile information. ICD-IPD consists of five main components, namely, 1) account pair generator (APG), 2) a multi-view learner, 3) a missing value imputer, 4) an account pair feature generator and 5) a prediction model. From a given set of social media users, the APG generates account pairs that share similar screen names or usernames. The multi-view learner then combines multi-view information of an account to improve learning performance. More specifically, it extracts profile (i.e. friends and posts count etc.) and Weighted Generalised Canonical Correlation Analysis (WGCCA)-based features (i.e. combination of multi-view) from a SocSen service provider‚Äôs non-privacy-sensitive profile information. Next, the missing value imputer imputes the missing feature values associated with profile and WGCCA-based features. The account pair feature generator then extracts similarity and differences-based features for each account pair in terms of the imputed feature values. Finally, ICD-IPD utilises a Light Gradient Boosting Machine (LightGBM) model atop a concatenated form of the aforementioned features to predict whether a pair of accounts compared possibly consists of a cloned account and a victim account. Our main contributions can be summarized as follows: ‚Ä¢ We propose a novel approach to detect SocSen service providers‚Äô identity cloning based on incomplete non-privacy-sensitive profiles. To the best of our knowledge, this is the first work in the field of social media identity deception information that specifically works on user profiles with missing values (incomplete profile data). ‚Ä¢ We utilize an imputation approach to impute the missing value of incomplete non-privacy-sensitive profile data. The utilised imputation approach can substantially enhance the cloned identity prediction performance as shown in Section 4. ‚Ä¢ We adopt an effective prediction model for detecting cloned identities with missing non-privacy-sensitive profile information. The proposed prediction model shows better performance than the state-of-art cloned identity detection approaches as well as several other candidate machine and deep learning models. ‚Ä¢ We present the results of our extensive experiments carried out atop a real-world dataset. The experimental findings showed that ICD-IPD outperforms current cloned identity detection approaches on the Key Performance Indicators: Precision, Recall, and F1-score. The remainder of the paper is structured as follows. Section 2 reviews the related work on identity cloning detection. Section 3 elaborates our proposed approach to address the challenges outlined previously. Meanwhile, Section 4 provides comprehensive details on the methodology used to evaluate the proposed approach and outcomes. Section 5 concludes the paper."
https://arxiv.org/html/2411.01169v1,Bi-Level Graph Structure Learning forNext POI Recommendation,"Next point-of-interest (POI) recommendation aims to predict a user‚Äôs next destination based on sequential check-in history and a set of POI candidates. Graph neural networks (GNNs) have demonstrated a remarkable capability in this endeavor by exploiting the extensive global collaborative signals present among POIs. However, most of the existing graph-based approaches construct graph structures based on pre-defined heuristics, failing to consider inherent hierarchical structures of POI features such as geographical locations and visiting peaks, or suffering from noisy and incomplete structures in graphs. To address the aforementioned issues, this paper presents a novel Bi-level Graph Structure Learning (BiGSL) for next POI recommendation. BiGSL first learns a hierarchical graph structure to capture the fine-to-coarse connectivity between POIs and prototypes, and then uses a pairwise learning module to dynamically infer relationships between POI pairs and prototype pairs. Based on the learned bi-level graphs, our model then employs a multi-relational graph network that considers both POI- and prototype-level neighbors, resulting in improved POI representations. Our bi-level structure learning scheme is more robust to data noise and incompleteness, and improves the exploration ability for recommendation by alleviating sparsity issues. Experimental results on three real-world datasets demonstrate the superiority of our model over existing state-of-the-art methods, with a significant improvement in recommendation accuracy and exploration performance.","The emergence of location-based social networks has brought to light a subject of great interest to both researchers and service providers alike: next point-of-interest (POI) recommendation. This task seeks to comprehend the temporal nature of a user‚Äôs preferences by analyzing their historical check-in sequences and then make predictions about the next POIs that they are most likely to visit. Such insights can be used to improve both the user experience as well as the service provider‚Äôs services. Graph-based methods have been widely used in POI recommendation due to their capability of modeling global collaborative relationships of POIs across users. These methods typically involve two stages: (1) the construction of a topology graph based on POI features and (2) the learning of POI representations based on the constructed graph. Depending on the type of information to be used, such a graph may be built by taking into consideration the spatial information of POIs, such as distance intervals [1, 2, 3] or grid regions [4], as well as temporal features from users‚Äô sequential check-in data, such as the average time intervals between consecutive visits [1] or the Jaccard similarity of time slot sets [4]. Additionally, it is also common to model transitions between POIs based on the number or frequency of consecutive visits between each POI [5, 6, 3]. After the graph is built, graph neural networks (GNNs) are used to learn the POI representations by aggregating information from the neighborhood of the nodes. These POI representations are then used to further learn users‚Äô preferences from the sequences of visited POIs and rank candidate POIs for producing recommendations. Despite their success, existing graph-based methods for POI recommendation suffer from various limitations. Firstly, previous methods construct graphs based solely on local neighborhoods, disregarding the valuable hierarchical structures of POIs. Hierarchical structure means that fine-grained POIs can be divided into coarse-grained groups, and POIs within the same group have similar group characteristics in some aspects. As shown in Fig. 1, POIs could be grouped into the same group due to similar spatial locations, transition sequence patterns, temporal visiting peaks, or category descriptors. These hierarchical structures have been proven to improve recommendations by mitigating the sparsity issues [7, 8, 9] and improving the exploration ability [4, 10]. Some previous methods handle hierarchical information by employing multi-task learning [4] or designing hierarchical encoders [11], however, the ingenious combination of the advantages of GNNs and hierarchical structures remains unexplored. Secondly, the graph structure of these existing methods is usually fixed during training, determined by pre-defined rules or empirical laws [6, 3, 12, 13]. This lack of flexibility could lead to performance degradation due to the presence of noise or incompleteness in the graph structure, since GNNs heavily rely on the quality of graph structure [14]. For example, there could be noisy and missing edges in the graph structure, due to erroneous check-ins and missed check-ins by users, or even malicious attacks. Lastly, multiple POI features (e.g., spatial, transition, and temporal features) are usually leveraged to construct multiple graphs. In different graphs, the same POI may have common neighbors and graph-specific neighbors, so there is shared information and specific information contained in different POI features. However, existing approaches often simply linearly combine or concatenate the POI representations in each graph to fuse them [1, 4], which inadequately models the alignment and complementary relationships of POI representations in different graphs, and results in sub-optimal POI representations. Figure 1: Hierarchical structures in different POI features, i.e., fine-grained POIs can be divided into coarse-grained groups (corresponding to cyan and red triangles in the figure). We use prototypes to represent coarse-grained group information and introduce prototype nodes in the graph structure learning to construct hierarchical graphs. To overcome the aforementioned limitations, we introduce a novel Bi-level Graph Structure Learning method for next POI recommendation, BiGSL for brevity. Our proposed BiGSL model consists of two graph structure learning modules that adaptively capture coarse- and fine-grained connectivity structures of POIs. Specifically, we first map each POI to a node in the graph space and then resort to clustering on the POI features to discover the hierarchical structure. The resulting prototypes represent the coarse-grained clusters, which are then added to the graph to augment the neighborhood of POI nodes. Subsequently, we introduce a pairwise structure learning method to infer the connectivity between POI pairs and prototype pairs in an adaptive manner, the result of which represents fine-grained connections that supplement the coarse-grained information. This bi-level approach produces coarse-to-fine connectivities of POIs that can be learned in a data-driven manner, thereby alleviating the first two limitations. Based on the bi-level graphs, we propose a multi-relational graph attention network that considers two facets of local structures including POI- and prototype-level neighbors, which produces better POI representations as a result. Finally, to further boost the performance, we construct multiple views based on the original POIs and users‚Äô sequential data. To encourage the fusion of POI information from distinct views, we design a contrastive multiview fusion approach by mining view-shared and view-specific information, which better aligns complementary features in different views. The main contributions of our work are outlined as follows: ‚Ä¢ We propose a novel BiGSL model for next POI recommendation, which employs a bi-level graph structure learning method that adaptively infers hierarchical graph structures in a data-driven manner. ‚Ä¢ Based on the learned bi-level graphs, we design a multi-relational graph network to generate more informative POI representations, considering both POI- and prototype-level neighbors. ‚Ä¢ We further introduce a multiview contrastive learning strategy to integrate information from multiple views to improve the recommendation accuracy. ‚Ä¢ We conduct extensive experiments on three real-world datasets including two widely adopted benchmarks (Gowalla and Foursquare) and a new commercial dataset. The results show that BiGSL significantly outperforms state-of-the-art methods in recommendation accuracy and exploration performance."
https://arxiv.org/html/2411.00825v1,Transparent Tagging for Strategic Social Nudges on User-Generated Misinformation,"Social network platforms (SNP), such as X and TikTok, rely heavily on user-generated content to attract users and advertisers, yet they have limited control over content provision, which leads to the proliferation of misinformation across platforms. As countermeasures, SNPs have implemented various policies, such as tweet labeling, to notify users about potentially misleading information, influencing users‚Äô responses, either favorably or unfavorably, to the tagged contents. The population-level response creates a social nudge to the content provider that encourages it to supply more authentic content without exerting direct control over the provider. Yet, when designing such tagging policies to leverage social nudges, SNP must be cautious about the potential misdetection of misinformation (wrongly detecting factual content as misinformation and vice versa), which impairs its credibility to generic users and, hence, its ability to create social nudges. This work establishes a Bayesian persuaded branching process to study SNP‚Äôs tagging policy design under misdetection. Misinformation circulation is modeled by a multi-type branching process, where users are persuaded through tagging to give positive and negative comments that influence the spread of misinformation. When translated into posterior belief space, the SNP‚Äôs problem is reduced to an equality-constrained convex optimization, the optimal condition of which is given by the Lagrangian characterization. The key finding is that SNP‚Äôs optimal policy is simply transparent tagging, i.e., revealing the content‚Äôs authenticity to the user, albeit midsection, which nudges the provider not to generate misinformation. We corroborate our findings using numerical simulations.","Social network platforms (SNP), such as X and TikTok, where users create and consume content, play an increasingly important role in society. These platforms rely heavily on user-generated content (UGC) to engage and retain users to maintain high-level daily activity. Since users who generate original content(‚Äúcontent providers‚Äù) are not paid workers, platforms have limited control over the UGC, including misinformation. User-generated misinformation has become a growing concern on SNPs, as false information can spread rapidly and have significant consequences [1]. For instance, false stories about candidates were shared widely through SNPs during the 2016 US presidential election; misinformation about the virus, mask-wearing policies, and vaccine concerns spread through social networks during the COVID-19 pandemic. To address this issue, SNPs have implemented policies such as labeling, tagging, or notifying to alert users to potentially false or misleading information [2, 3]. Figure 1: An illustration of the proposed persuasion model, where the misinformation distribution Œ∏‚Å¢(Œª)ùúÉùúÜ\theta(\lambda)italic_Œ∏ ( italic_Œª ) is affected by the content provider and remains unknown to the user. The SNP‚Äôs misdetection of the underlying content is modeled by dùëëditalic_d. Previous studies have shown that these policies effectively (to some extent) curb the spread of misinformation [4]. One of the key reasons is that these platforms feature intensive social interactions among users, which can be leveraged to create social nudges in stimulating UGC supply [5]. For example, a post tagged as misleading will inflict users‚Äô negative comments. After circulation on social networks, the population response to the post creates pressure on the content provider that discourages it from generating misinformation. This work proposes a persuasion game model to provide theoretical underpinnings for the SNP‚Äôs tagging design, aiming to harness the power of social nudges to reduce user-generated misinformation. As illustrated in Figure 1, the strategic interactions among the SNP, the content provider, and the user unfold as below. The SNP designs a tagging policy whose realized tags indicate the content authenticity of an arbitrary post returned by a detection device. Of particular note is that the detection device, usually empowered by artificial intelligence methods [6, 7, 8], is often imperfect and may misclassify the post‚Äôs authenticity. Such a policy does not directly control the provider or user but influences others‚Äô behaviors through information provision. Hence, this tagging policy is referred to as the information structure [9]. Fully aware of this policy, the content provider exerts a private effort (unobservable to the SNP or user) in creating the content, assuming that the more effort exerted, the more authentic the content is. Finally, the user observes the tagging policy and the realized tags and then decides their views and comments that influence the online circulation modeled by a multi-type branching process. The proposed model differs from the seminal Bayesian persuasion game [10] in that the user cannot directly observe the prior distribution. Consequently, the user must form a conjecture about the content provider‚Äôs behavior to update their beliefs. This conjecture must be consistent with the provider‚Äôs equilibrium behavior, which leads to the concept of perfect Bayesian equilibrium (PBE) as the natural solution concept for our game. In our previous work [11], we addressed a special case where there was no detection error, allowing the SNP to identify misinformation in posts perfectly. However, in practical scenarios, detection errors are inevitable. In this work, the SNP‚Äôs design problem considers such misdetection, which leads to the SNP‚Äôs misperception of the game state that impairs tagging‚Äôs credibility and effectiveness in fostering social nudges. Our key finding is that transparent tagging, where the SNP honestly discloses the detection outcome to the content provider and user, is most effective in combating misinformation generation and circulation. Although the SNP may not have direct control over content generation, it can nudge user perceptions through tagging. The collective behaviors of users, under these perceptions, determine the content provider‚Äôs reputation, effectively making users the SNP‚Äôs proxy in terms of incentive provision, encouraging the provider to exert the best effort in reducing misinformation generation. Our contributions are summarized below. ‚Ä¢ We propose a three-player Bayesian persuasion game that studies the SNP‚Äôs tagging policy under the presence of misdetection and the content provider‚Äôs intention to uphold its reputation, with misinformation circulation among users modeled as a multi-type branching process; ‚Ä¢ We identify players‚Äô strategies under perfect Bayesian equilibrium by transforming the problem into the posterior belief space, reducing it to an equality-constrained convex optimization problem; ‚Ä¢ We characterize the optimal conditions using a Lagrangian approach, demonstrating that the SNP‚Äôs optimal policy is transparent tagging despite detection errors, incentivizing the content provider to exert maximum implementable effort."
https://arxiv.org/html/2411.00823v1,Mobility-LLM: Learning Visiting Intentions and Travel Preferences from Human Mobility Data with Large Language Models,"Location-based services (LBS) have accumulated extensive human mobility data on diverse behaviors through check-in sequences. These sequences offer valuable insights into users‚Äô intentions and preferences. Yet, existing models analyzing check-in sequences fail to consider the semantics contained in these sequences, which closely reflect human visiting intentions and travel preferences, leading to an incomplete comprehension. Drawing inspiration from the exceptional semantic understanding and contextual information processing capabilities of large language models (LLMs) across various domains, we present Mobility-LLM, a novel framework that leverages LLMs to analyze check-in sequences for multiple tasks. Since LLMs cannot directly interpret check-ins, we reprogram these sequences to help LLMs comprehensively understand the semantics of human visiting intentions and travel preferences. Specifically, we introduce a visiting intention memory network (VIMN) to capture the visiting intentions at each record, along with a shared pool of human travel preference prompts (HTPP) to guide the LLM in understanding users‚Äô travel preferences. These components enhance the model‚Äôs ability to extract and leverage semantic information from human mobility data effectively. Extensive experiments on four benchmark datasets and three downstream tasks demonstrate that our approach significantly outperforms existing models, underscoring the effectiveness of Mobility-LLM in advancing our understanding of human mobility data within LBS contexts.","Location-based services (LBS) such as Gowalla, Weeplace, and Foursquare enable users to share and discover location information and nearby services. This results in the collection of extensive human mobility data, often presented in the form of check-in sequences. These sequences record users‚Äô visits to different points of interest (POIs) like restaurants and hospitals at various times, reflecting significant semantics about their intentions and preferences. Analyzing these check-in sequences is crucial as it offers valuable information on human mobility data, which can positively impact individuals, businesses, and urban management. The key to effectively mining check-in sequences lies in understanding their rich semantics. Existing methods primarily focus on specific tasks, such as location prediction [10, 58, 51, 25], time prediction [39, 44], and trajectory user linking [33, 13, 61], rather than delving into the semantics of human behaviors. This narrow focus often results in limited optimization goals and a shallow understanding of the semantics contained in check-in sequences. Recently, large language models (LLMs) have demonstrated impressive capabilities in semantic understanding and contextual information processing, demonstrating successful adaptability across different domains. LLMs trained on extensive corpora surpass task-specific models in their potential to understand semantic information. Inspired by this, we aim to utilize pre-trained LLMs as powerful check-in sequence learners. Nevertheless, LLMs encounter a significant obstacle in their inability to directly interpret check-in sequences. As typical sequential data, check-in sequences contain a wealth of semantic information that reflects various near-term regularities and inherent characteristics. The future intention of an individual is prone to be dictated by near-term regularities that are close to recent visits, termed visiting intentions. Furthermore, an individual‚Äôs inherent characteristics tend to persist over time and determine their travel preferences, which is necessary to analyze them across multiple domains for a comprehensive understanding. Hence, our main challenge is to enable LLMs to effectively extract semantics from check-in sequences and comprehensively understand human visiting intentions and travel preferences. To address this challenge, we present a novel unified framework called Mobility-LLM for various check-in sequence analysis tasks. It leverages pre-trained LLMs for general check-in sequence analysis. Our contributions can be summarized as follows: ‚Ä¢ We propose a unified framework called Mobility-LLM that uses a pre-trained LLM to achieve a SOTA or comparable performance across various check-in analysis tasks including location prediction, trajectory user link, and time prediction. We extract the semantics of check-in sequences to enable LLMs to gain a comprehensive understanding of human visiting intentions and travel preferences. ‚Ä¢ A visiting intention memory network (VIMN) is proposed for capturing users‚Äô visiting intentions of users at each check-in record by prioritizing relevant check-in records. ‚Ä¢ A shared pool of human travel preference prompts (HTPP) in different domains is introduced, which enables a comprehensive understanding of human travel preferences and matches appropriate prompts from multiple domains. ‚Ä¢ Our model‚Äôs exceptional performance is validated through extensive experiments on four benchmark datasets involving three tasks. Our robust outcomes in cross-domain pre-training exhibit an average enhancement of 17.8% and an average of 23.6% to 38.3% on the few-shot scenario."
https://arxiv.org/html/2411.00813v1,Personality Analysis from Online Short Video Platforms with Multi-domain Adaptation,"Personality analysis from online short videos has gained prominence due to its applications in personalized recommendation systems, sentiment analysis, and human-computer interaction. Traditional assessment methods, such as questionnaires based on the Big Five Personality Framework, are limited by self-report biases and are impractical for large-scale or real-time analysis. Leveraging the rich, multi-modal data present in short videos offers a promising alternative for more accurate personality inference. However, integrating these diverse and asynchronous modalities poses significant challenges, particularly in aligning time-varying data and ensuring models generalize well to new domains with limited labeled data. In this paper, we propose a novel multi-modal personality analysis framework that addresses these challenges by synchronizing and integrating features from multiple modalities and enhancing model generalization through domain adaptation. We introduce a timestamp-based modality alignment mechanism that synchronizes data based on spoken word timestamps, ensuring accurate correspondence across modalities and facilitating effective feature integration. To capture temporal dependencies and inter-modal interactions, we employ Bidirectional Long Short-Term Memory networks and self-attention mechanisms, allowing the model to focus on the most informative features for personality prediction. Furthermore, we develop a gradient-based domain adaptation method that transfers knowledge from multiple source domains to improve performance in target domains with scarce labeled data. Extensive experiments on real-world datasets demonstrate that our framework significantly outperforms existing methods in personality prediction tasks, highlighting its effectiveness in capturing complex behavioral cues and robustness in adapting to new domains. We open our datasets and code at https://github.com/Anne6645/personality_analysis","Personality analysis has long been a central topic in psychological science and has gained increasing importance in recent years due to its wide-ranging applications. It plays a crucial role in various domains such as personalized recommendation systems [1, 2], sentiment analysis [3, 4], and human-computer interaction. Accurately identifying an individual‚Äôs personality can enable tailored experiences and services, enhancing user satisfaction and engagement. However, personality traits are inherently latent characteristics that are not directly observable, making the assessment of personality a challenging task. Traditionally, psychologists have employed structured methods to evaluate an individual‚Äôs personality. One of the most widely accepted models is the Big Five Personality Traits (as shown in Figure 1), which assesses personality across five key dimensions: openness, conscientiousness, extraversion, agreeableness, and neuroticism. To determine an individual‚Äôs position on these dimensions, conventional approaches often rely on well-designed questionnaires and psychological inventories that analyze self-reported responses. While these methods are grounded in rigorous psychometric principles, they have notable limitations. Self-reported data can be influenced by social desirability bias, where respondents tailor their answers to be viewed favorably. Additionally, administering and processing these surveys can be time-consuming and resource-intensive, making them less practical for large-scale or real-time applications. Figure 1: Big Five Personality Traits With the advent of online video social platforms like TikTok111https://www.tiktok.com/ and others, there is a growing opportunity to analyze personality traits through digital means. Users increasingly share selfie videos online, providing a wealth of data that captures not only their visual appearance but also their speech patterns, facial expressions, and environmental context. Compared to static questionnaires, these multi-modal data offer richer insights into an individual‚Äôs intrinsic traits. Unlike traditional social media platforms that primarily feature text or images, video platforms enable the observation of dynamic behaviors and interactions, which are crucial for understanding personality. This shift opens up new possibilities for applications such as online job interviews, remote education, and personalized content delivery, where assessing personality from videos can significantly enhance outcomes. Recent research has begun to explore the potential of analyzing personality traits through online media instead of traditional surveys. Behavioral observations from personal photographs [5, 6, 7] and short videos [8] have been utilized to glean personality insights. For instance, [5] analyzed Facebook profile pictures to infer personality traits, while [6] and [7] leveraged social media images for similar purposes. However, photograph-based approaches have limitations, as individuals often curate their online images, sharing selective moments that may not accurately represent their typical behaviors or personality, leading to biased data and potentially inaccurate predictions. In contrast, short videos provide a more comprehensive medium for personality analysis. They capture changes in facial expressions, body movements, speech patterns, and contextual scenes‚Äîall of which are significant indicators in psychological assessments of personality. Recognizing this, researchers have started to model the audio, visual, and textual features present in short videos [9, 10, 11]. For example, [9] developed a Deep Bimodal Regression model combining audio and visual modalities to predict scores on the Big Five personality traits. Similarly, [10] employed convolutional neural networks to extract visual features and linear regression for audio features, while [11] conducted an in-depth analysis using logistic regression on audio, video, and text features. Figure 2: Comparison of Existing Works Despite these advances, existing multi-modal personality prediction methods often rely on large volumes of high-quality short videos with high-resolution visuals and clear audio to achieve satisfactory performance. Moreover, many approaches depend heavily on supervised learning techniques that require extensive labeled datasets. Collecting and annotating such multi-modal data is both expensive and time-consuming. Manual annotation introduces the potential for subjectivity and inconsistency, which can affect the reliability of the analysis. Consequently, detecting personality traits from online video platforms presents significant challenges, particularly in the following areas: The First Challenge is identifying the most important features from multiple modalities to optimize the use of a limited number of short videos for accurate personality analysis. The complexity arises from the need to effectively integrate diverse data types‚Äîvisual cues, auditory signals, textual content, and contextual information‚Äîeach contributing uniquely to personality inference. Existing methods may not adequately address the alignment and synchronization of these modalities, leading to fragmented or incomplete representations that hinder predictive accuracy. The Second Challenge is effectively utilizing a small number of high-quality short videos to achieve strong generalization in personality analysis. Models trained on specific datasets may struggle to generalize across different domains due to variations in cultural contexts, linguistic expressions, and recording conditions. The scarcity of labeled data in new or underrepresented domains exacerbates this issue, limiting the applicability of the models in real-world scenarios where data diversity is the norm. In this paper, we propose an effective multi-modal personality analysis framework designed to overcome these challenges. To address the first challenge, we introduce a semantic unit method for feature extraction and alignment, which synchronizes multi-modal data based on spoken words. This ensures that features from different modalities correspond accurately at each moment in the video, facilitating effective integration. Within this module, we employ self-attention mechanisms to discern the significance of features across various modalities. By assigning weights to features based on their relevance to personality prediction, the model focuses on the most informative aspects of the data, enhancing analytical accuracy. To tackle the second challenge, we propose a multi-domain adaptation method that transfers domain knowledge across multiple domains to alleviate the data sparsity problem. This approach leverages information from data-rich source domains to enhance learning in data-scarce target domains. By computing gradient similarities between source and target domains, our model adapts to emphasize learning from source domains that are most relevant to the target domain. This method improves the model‚Äôs generalization capabilities, enabling more accurate predictions even when limited data is available in certain domains. Our main contributions are summarized as follows: ‚Ä¢ We propose an effective multi-modal personality analysis framework that effectively integrates facial expressions, audio signals, textual content, and background information from short videos for personality prediction. ‚Ä¢ We introduce a semantic unit modality alignment mechanism that synchronizes multi-modal data based on spoken word timestamps, ensuring accurate correspondence across modalities and enhancing feature representation. ‚Ä¢ We develop a gradient-based domain adaptation method that transfers knowledge from multiple source domains to target domains with limited labeled data, enhancing model generalization and performance in few-shot learning scenarios. ‚Ä¢ We validate the effectiveness of our proposed framework through extensive experiments on real-world datasets, demonstrating significant improvements over existing methods in personality prediction tasks. By addressing both the feature integration and domain adaptation challenges, our framework advances in personality analysis from online short videos. The rest of the paper is organized as follows. Section II introduces the related work of personality analysis and domain adaption. The problem definition is in Section III. Section IV elucidates the detailed methodology, and Section V presents the results of the experiments and the analysis. The last section is the conclusion of the paper."
https://arxiv.org/html/2411.00612v1,How to Bridge Structural and Temporal Heterogeneity in Link Prediction? A Contrastive Method,"Temporal Heterogeneous Networks play a crucial role in capturing the dynamics and heterogeneity inherent in various real-world complex systems, rendering them a noteworthy research avenue for link prediction. However, existing methods fail to capture the fine-grained differential distribution patterns and temporal dynamic characteristics, which we refer to as spatial heterogeneity and temporal heterogeneity. To overcome such limitations, we propose a novel Contrastive Learning-based Link Prediction model, CLP, which employs a multi-view hierarchical self-supervised architecture to encode spatial and temporal heterogeneity. Specifically, aiming at spatial heterogeneity, we develop a structural feature modeling layer to capture the fine-grained topological distribution patterns from node- and edge-level representations, respectively. Furthermore, aiming at temporal heterogeneity, we devise a temporal information modeling layer to perceive the evolutionary dependencies of dynamic graph topologies from time-level representations. Finally, we encode the structural and temporal distribution heterogeneity from a contrastive learning perspective, enabling a comprehensive self-supervised hierarchical relation modeling for the link prediction task. Extensive experiments conducted on four real-world dynamic heterogeneous network datasets verify that our CLP consistently outperforms the state-of-the-art models, demonstrating an average improvement of 10.10%, 13.44% in terms of AUC and AP, respectively.","Contemporary information networks such as social networks (Wang et al., 2024a) and biological systems (Chen and Liu, 2022) are becoming increasingly complex. These networks often comprise multi-typed nodes and connections, undergo continuous temporal evolution, making the link prediction in such complex networks a long-standing challenge. Specifically, the link prediction task aims to predict the likelihood of future connections between arbitrary nodes (Wang et al., 2024b; Yao and Li, 2024; Wu et al., 2022), which captures the evolution of heterogeneous networks and stores the temporal details of the node embeddings, simulating intricate and expressive semantics for real-world systems, including Social Recommendations (Wu et al., 2023a), Traffic Management (Shao et al., 2022; Jin et al., 2022), Medical Health (Wang et al., 2021b) and Network Biology (Guzzi and Zitnik, 2022). Aiming at modeling the dynamics and complex relationships between entities, link prediction models are primarily designed to portray the topological relationship between heterogeneous snapshots and the evolving progress along chronological order, revealing the distribution patterns within complex Temporal Heterogeneous Networks (THNs). The primary challenges in link prediction tasks revolve around heterogeneous entity relationship modeling and dynamic snapshot variability modeling. Current link prediction methods in literature typically segment dynamic snapshot sequences chronologically and address the complex entity relationships existing in each snapshot. We term such challenges as spatial complexity and temporal complexity. (1) Spatial complexity (Wang et al., 2019; Zhang et al., 2023d) highlights the complex heterogeneous static relationships between multi-typed entities in complex networks, primarily modeling the diverse co-occurrence paradigm through heterogeneous network embedding approaches. Specifically, Meta-Path-based approaches (Qiu et al., 2018; Dong et al., 2017; Fu et al., 2017; Zhao et al., 2023b) construct meta-paths within individual snapshots to excavate the heterogeneous information. On the other hand, Attribute-based methods (Hu et al., 2020a; Li et al., 2020; Zhang et al., 2023e) focus on incorporating multiple rich attributes (Xu et al., 2021; Peng et al., 2023) and merging neighbor attributes (Hu et al., 2020a; Li et al., 2020) to enhance the node embedding process. (2) Temporal complexity (Pareja et al., 2020; Pham et al., 2021; Fan et al., 2022; Layne et al., 2023) mainly exploits the dynamic distribution changes in snapshot sequences. Temporal approaches focus on tracking the continuous evolution across chronological snapshots, primarily classified into sequential and graph methods. Sequential methods (Hao et al., 2020; Jiang et al., 2023) learn from time-ordered snapshot sequences, capturing evolutionary dependencies between different snapshots based on Recurrent Neural Networks (RNNs) (Chen et al., 2019) and attention mechanism (Zhang et al., 2023c). Graph methods (Lei et al., 2019; Yang et al., 2019; Xu et al., 2020; Yang et al., 2022) aggregate embeddings of dynamic nodes, encoding the appearing or disappearing network features continuously over time through Graph Neural Networks (GNNs) (Zhao et al., 2023a; Gao et al., 2024). Figure 1. An Illustrative Example. Despite the effectiveness, a prominent drawback of these methods is that they model dynamic heterogeneous representations in a coarse-grained manner and only focus on the representation paradigm, but ignore the universally-distributed differential relations in THNs, thus resulting in the suboptimal performance in link prediction tasks. We reckon that by leveraging such differential relations and bridging temporal and spatial heterogeneity, it becomes feasible to portray the comprehensive and detailed dynamic and diversified characteristics, thereby enhancing link prediction performance. Specifically, focusing on the aforementioned temporal and spatial complexity, the fine-grained differential relations between different nodes and edges (spatial) and the evolution paradigm distinctions (temporal) play a crucial role in representation learning and significantly influence link prediction performance. We illustrate such fine-grained differential distributions at the node-, edge-, and time-level in Figure 1. From the node-level propagation, taking the static THN snapshot ùí¢1superscriptùí¢1\mathcal{G}^{1}caligraphic_G start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT as an example, different propagation and aggregation paradigms convey differential information. If we take the quadrangle () as the ego node, from the perspective of connection types, the propagation priority sequence for reveals: === === . Meanwhile, from the perspective of single-type neighbor numbers, the priority manifests: >>> === , reflecting significant distribution heterogeneity among different propagation patterns. Analogously, targeting edge-level propagation, different edge types form the holistic propagation paradigm, yet convey unique propagation information. However, the heterogeneity between different edge-level propagation lacks sufficient attention. Finally, from time-level propagation, significant variations occur due to diverse inspection degrees (e.g., number of nodes, variations of edges), highlighting the transformative information conveyed by different time-level propagation patterns. We term such differential distribution in THNs as ‚Äòspatial heterogeneity‚Äô and ‚Äòtemporal heterogeneity‚Äô, which are crucial factors for link prediction modeling but have been rarely addressed in related research. As is illustrated in the above example, different entities in complex networks possess variously-grained distribution differentiation, existing among nodes, edges and varying along chronological order. However, conventional methods ignore the modeling of such heterogeneity differentiation, which introduces the first challenge in this work, i.e., CH1: How to capture the heterogeneity differentiation existing in link relation networks? To represent such distribution heterogeneity in link prediction, it is essential to characterize the fine-grained intrinsic topological distribution in the link graph. Accordingly, we resort to Self-supervised Learning (Chen et al., 2020) to investigate the inherent inter-relation in temporal heterogeneous graphs. In addition, to bridge the discrepancy elimination and topological exploration module in link prediction, we need to resolve the second challenge, i.e., CH2: How to integrate different granularity of distribution discrepancy? To address this challenge, we first design a heterogeneous temporal graph to absorb both structural distribution patterns and sequential evolutionary paradigms. Subsequently, we propose a contrastive hierarchical heterogeneity differentiation module to absorb the intrinsic inter-relation from node-, edge-, and time-level, respectively. Leveraging such hierarchical contrastive module, we implement a fine-grained multi-view entity relation extraction functionality. To summarize, our main contributions are as follows: ‚Ä¢ Targeting CH1, we propose a three-layer hierarchical contrastive entity relation extraction module to enable multi-view discrepancy elimination functionality, thereby bridging the spatial and temporal heterogeneity in link prediction scenarios. ‚Ä¢ Targeting CH2, we design a heterogeneous temporal graph network to absorb sequential and structural distribution paradigms and comprehensively eliminate discrepancies from various perspectives. Specifically, we depict the structural distribution differentiation paradigms with node- and edge-level graph networks and propose a dual-channel sequential module to capture different sequential reliance among snapshots. ‚Ä¢ We conduct extensive experiments on four benchmark datasets to predict temporal links between two entities. The experimental results indicate that CLP achieves superior prediction performance compared to the existing state-of-the-art link prediction methods."
https://arxiv.org/html/2411.00606v1,Domain-Informed Negative Sampling Strategies for Dynamic Graph Embedding in Meme Stock-Related Social Networks,"Social network platforms like Reddit are increasingly impacting real-world economics. Meme stocks are a recent phenomena where price movements are driven by retail investors organising themselves via social networks. To study the impact of social networks on meme stocks, the first step is to analyse these networks. Going forward, predicting meme stocks‚Äô returns would require to predict dynamic interactions first. This is different from conventional link prediction, frequently applied in e.g. recommendation systems. For this task, it is essential to predict more complex interaction dynamics, such as the exact timing and interaction types like loops. These are crucial for linking the network to meme stock price movements. Dynamic graph embedding (DGE) has recently emerged as a promising approach for modeling dynamic graph-structured data. However, current negative sampling strategies, an important component of DGE, are designed for conventional dynamic link prediction and do not capture the specific patterns present in meme stock-related social networks. This limits the training and evaluation of DGE models in analysing such social networks. To overcome this drawback, we propose novel negative sampling strategies based on the analysis of real meme stock-related social networks and financial knowledge. Our experiments show that the proposed negative sampling strategy can better evaluate and train DGE models targeted at meme stock-related social networks compared to existing baselines.","Social networks play an ever increasing role in society (Rao and Kalyani, 2022; Bakshy et al., 2012). Various studies show that social networks, such as Twitter (Gu and Kurov, 2020) and Reddit (Umar et al., 2021), also influence financial markets. The research presented in this paper is motivated by Reddit and the GameStop (GME) market frenzy occurring around January 2021 when users on the subreddit ‚ÄòWallstreetbets‚Äô discussed GME and collectively caused a market frenzy (Trimborn and Zwetsloot, 2024). It has become clear that internet users are a notable group influencing stock prices specifically for so called ‚Äòmeme stocks‚Äô (Costola et al., 2021), stocks that receive significant attention on social media. To study the relationship between retail investors on social networks such as Reddit and the stock markets, a thorough understanding is needed of the network structure and behavior of the people posting on these networks. A first step to studying the relationship is in understanding changes in posting behavior over time which may trigger stock market action (Trimborn and Zwetsloot, 2024). For this purpose, a dynamic network model is needed which captures the dynamics of posting behavior at the individual node level. Such models need to be scalable due to the large number of users and high volume of interactions and posts in social networks. Dynamic graph embedding (DGE) has emerged as an effective tool for tacking these challenges (Xue et al., 2022; Xie et al., 2020). Graphs naturally describe social networks by representing individuals as nodes and their interactions as edges, providing a structured framework for analysis. DGE builds on this by transforming the nodes and edges into continuous vector representations (node embeddings), preserving both the network‚Äôs structural and temporal properties. This approach allows DGE to capture the dynamic evolution of social networks over time, enabling insights into complex user interactions and facilitating predictions of network behavior. For DGE, dynamic link prediction (DLP) is an important component which predicts if there is a link between two nodes based on their embeddings (Barros et al., 2021). From a technical perspective, DLP can evaluate the quality of generated embeddings and serve as the training objective. From an application perspective, predicting when two users will interact in the future based on embeddings can help identify stock market trends (Zhang et al., 2018; Lei and Ramos Salazar, 2022; Chen and Du, 2013), such as renewed interest in a stock, which may manifest in a new stock market frenzy. DGE models need to accurately predict both existing interactions (positive samples) and nonexistent connections (negative samples) (Leskovec et al., 2010; Daud et al., 2020). If the model only predicts that all interactions exist, it may achieve good performance on positive samples but will incorrectly identify nonexistent connections, leading to unreliable and misleading results. However, in meme stock-related social networks, the number of negative samples far exceeds that of positive samples. Due to the huge size of these social networks, users typically only communicate with a fraction of other users (Wang et al., 2019; Gonzalez-Bailon et al., 2010). Therefore, the majority of these negative samples provide little valuable information, as many users may never interact. We conjecture that using such obvious non-connections for model training and evaluation will focus the models prediction ability upon these obvious non-connections, whereas the real challenge lies in predicting negative samples which are difficult to predict in real social networks. This highlights the need to carefully select informative negative samples, a process known as negative sampling (Yang et al., 2020). Most existing negative sampling strategies for DLP are primarily based on random or heuristic approaches (Chen et al., 2024; Poursafaei et al., 2022; Poursafaei and Rabbany, 2023). For instance, random negative sampling is one of the most widely used strategies (Poursafaei et al., 2022). It generates one negative sample for each positive sample (u,v,t)ùë¢ùë£ùë°(u,v,t)( italic_u , italic_v , italic_t ), where u,v,tùë¢ùë£ùë°u,v,titalic_u , italic_v , italic_t is the sender, receiver, and occurrence time of the interaction, by replacing vùë£vitalic_v with a random user. Such strategy leads to many of the obvious non-connections to be part of the generated negative samples which results in deceivingly outstanding performance. State-of-the-art (SOTA) dynamic graph embedding (DGE) models can achieve the AUC (Area Under the Receiver Operating Characteristic Curve) over 0.9 on certain datasets when trained and evaluated using this negative sampling strategy (Rossi et al., 2020; Zhang et al., 2023; Gao et al., 2022). However, the practical use for real applications, such as meme stock-related social network prediction, is low. For illustration, consider the use case of predicting when the users who have already interacted (i.e., there are edges connecting two nodes) will interact again. This is important because repeated interactions often indicate renewed interest or users‚Äô joint and repeated interest in a stock, which can lead to price movements for meme stocks (Trimborn and Zwetsloot, 2024). To evaluate the model‚Äôs prediction ability in such a case, we generated three types of negative samples for each positive sample (u,v,t)ùë¢ùë£ùë°(u,v,t)( italic_u , italic_v , italic_t ): (u,v,t+6‚Å¢h)ùë¢ùë£ùë°6‚Ñé(u,v,t+6h)( italic_u , italic_v , italic_t + 6 italic_h ), (u,v,t+12‚Å¢h)ùë¢ùë£ùë°12‚Ñé(u,v,t+12h)( italic_u , italic_v , italic_t + 12 italic_h ), and (u,v,t+24‚Å¢h)ùë¢ùë£ùë°24‚Ñé(u,v,t+24h)( italic_u , italic_v , italic_t + 24 italic_h ). These samples test whether the model can correctly predict if nodes that have interacted will interact again after 6, 12 and 24 hours. We use the dataset AMC (see Section 4) and the SOTA DGE model Temporal Graph Networks (TGNs) (Rossi et al., 2020) as an example. The results in Table 1 show that the TGNs achieved an AUC of 0.9736 when trained and tested using random negative sampling, closely matching results reported in the original paper (Rossi et al., 2020). However, the performance dropped strongly when tested with the other three types of negative samples. This indicates that random negative sampling strategy limits the DGE model‚Äôs ability to accurately predict when previously interacting nodes will interact again. Table 1. Test AUC of TGN with various negative sampling strategies (Using AMC Dataset, January for Training and February for Validation and Testing) Strategy Random 6h 12h 24h AUC 0.9736 0.6041 0.6982 0.7681 With this example, we show that the design of the negative sampling strategy should be closely tied to domain knowledge. This has also been proved by a recent study study (Ma et al., 2023). In the settings like meme stock-related social networks, interactions between users are not random or uniform. A generic negative sampling strategy may miss some important information, leading to suboptimal performance in predictive tasks. By incorporating domain-specific knowledge, such as understanding the significance of predicting the exact time of repeated interactions, a more effective negative sampling strategy can be developed. In this paper, we analyzed three real-word meme stock-related social networked datasets containing interactions on Reddit related to three companies, GameStop (GME), American Multi-Cinema (AMC), and BlackBerry (BB), and identified several key characteristics of meme stock-related social networks, such as the frequency of interactions between users, and the presence of unique interaction types such as loops. Based on these insights, we developed several individual negative sampling strategies specifically tailored to these network properties. Each strategy captures a distinct aspect of the network dynamics. We also developed a joint negative sampling strategy, incorporating these individual negative sampling strategies. To overcome the complexity and imbalance between positive and negative samples caused by incorporating all of these negative samples into the training process, we implemented positive enhancement where additional positive samples are included during training to maintain a balanced ratio between positive and negative interactions. In summary, with this paper we make the following contributions: (1) We explored the application of DGE models to a special type of special social networks, meme stock-related social networks. We found that the current design of negative sampling strategies, an important component of dynamic graph embedding models, limits the performance of DGE models in this kind of social networks. (2) We proposed several individual negative sampling strategies based on the analysis of three real-word meme stock-related social networks and corresponding financial domain knowledge. Each of them evaluates a certain part of DGE models‚Äô prediction ability in meme stock-related social networks. We also proposed a negative sampling strategy named D(omain)I(nformed)N(egative)S(ampling) that combines these single strategies and further balances the positive and negative sample by positive enhancement. (3) We conducted extensive experiments to show the effect of negative sampling strategies in the evaluation and training of DGE models. The experimental results also show that our proposed negative sampling strategies can improve DGE model‚Äôs prediction performance in meme stock-related social networks."
https://arxiv.org/html/2411.00262v1,Content Aware Analysis of Scholarly Networks:A Case Study on CORD19 Dataset,"This paper investigates the relationships among key elements of scientific research network, namely articles, researchers, and journals. We introduce a novel approach to use semantic information through the HITS algorithm based propagation of topic information in the network. The topic information is derived by using the Named Entity Recognition and Entity Linkage. In our case, MedCAT is used to extract the topics from the CORD19 Dataset, which is a corpus of academic articles about COVID-19 and coronavirus scientific network. Our approach focuses on the COVID-19 domain, utilizing the CORD-19 dataset to demonstrate the efficacy of integrating topic-related information within the citation framework. Through the application of a hybrid HITS algorithm, we show that incorporating topic data significantly influences article rankings, revealing deeper insights into the structure of the academic community.","The core elements of scientific research include articles, researchers, and institutions. Since scientific research is the cumulative effort of researchers to increase the understanding of the world around us, the relationships between these elements are as important as the scientific results themselves. Gaining insight into the relationship between the core elements of scientific research can be useful for a variety of purposes, such as guiding scientific effort toward better use of resources, inferring comparative results between fields of research, better representing the importance of certain research fields and research groups. Current scientific literature continues to grow at a rapid pace every day. Let alone being able to follow the growth of communities in which we are not a part, it has become very difficult to even find conferences, journals or other prominent studies in our field. Naturally, examining the academic community in detail becomes a great burden for most young or experienced researchers, which results in missing out promising researchers and useful works. To overcome this problem, most researchers represent the scientific literature as a wide network consists of different entities such as researchers, institutes, etc. In this paper, we aimed to analyze current literature and demonstrate different approaches to this problem with some practical applications. The academic writers, their studies and the citation connection between them composes the scientific community, which forms a wide network of authors and articles. Authors are identified as the entities that creates the knowledge in the community through the articles they have published. The citation network which is derived from the published work is the most common representation of this knowledge, which is very simple yet effective to analyze the communities. Social Network Analysis is a way of measuring and mapping various aspects of relationships between different entities such as people, organizations and groups Sweet [2018]. At first step, we started our analysis from simple representation of the network which is a graph of authors and articles. Then, we focus on the possible interpretations of the centrality metrics, PageRank and its variations in the real world scenarios. Apart from the approach above, it is clear that the proposed citation network lacks of the semantic meaning of the published works. Also representation of the topics is missing in the constructed graph of authors and their articles. Even though some solutions based on Natural Language Processing are available in the literature, most of these works require to process and analyze the content of the published articles via their open access files or abstracts. So, we propose a new and robust method to represent and retrieve the data in scientific network by considering the topics as an entity in the research graph. The topics are derived from a pipeline based on Named Entity Recognition and Knowledge Base of the relevant graph. Naturally, we have to focus on a specific domain to use the knowledge base effectively. Eventually, we showed the applicability of our method on a chosen domain and dataset, which are COVID-19 and CORD19 Dataset. Our motivation in this paper is to develop a way to provide the researchers with quantifiable information about the relationships between these elements so that it can be used for such purposes. This quantifiable information includes graph measures of individual elements of a graph as well as the graph measures of the whole graph. Briefly, we propose a pipeline to create, analyze and store the research network which consists of authors, articles, named entities and relationships between them."
https://arxiv.org/html/2411.00249v1,GraphC: Parameter-free Hierarchical Clustering of Signed Graph Networks,"Spectral clustering methodologies, when extended to accommodate signed graphs, have encountered notable limitations in effectively encapsulating inherent grouping relationships. Recent findings underscore a substantial deterioration in the efficacy of spectral clustering methods when applied to expansive signed networks. We introduce a scalable hierarchical Graph Clustering algorithm denominated GraphC. This algorithm excels at discerning optimal clusters within signed networks of varying magnitudes. GraphC aims to preserve the positive edge fractions within communities during partitioning while concurrently maximizing the negative edge fractions between communities. Importantly, GraphC does not require a predetermined cluster count (denoted as k). Empirical substantiation of GraphC ‚Äôs efficacy is provided through a comprehensive evaluation involving fourteen datasets juxtaposed against ten baseline signed graph clustering algorithms. The algorithm‚Äôs scalability is demonstrated through its application to extensive signed graphs drawn from Amazon-sourced datasets, each comprising tens of millions of vertices and edges. A noteworthy accomplishment is evidenced, with an average cumulative enhancement of 18.64% (consisting of the summation of positive edge fractions within communities and negative edge fractions between communities) over the second-best baseline for each respective signed graph. It is imperative to note that this evaluation excludes instances wherein all baseline algorithms failed to execute comprehensively.","Communities within a network are sets of vertices characterized by denser interconnections among them than with the broader network. These communities may exhibit either distinct vertex sets or overlap, where vertices partake in multiple communities. The detection of community structure holds great importance, unveiling latent insights into relationships and dynamic mechanisms within complex networks spanning various domains, ranging from biological to social networks. The inherent complexity of these network systems frequently causes the substantial datasets to surpass individual systems‚Äô computational capacities, necessitating data partitioning for distributed processing. The efficiency of such partitioning schemes in community detection algorithms is contingent upon various factors. Numerous methodologies, encompassing local optima pursuit, statistical inference, and machine learning, have been posited to unveil community structure. While state-of-the-art community discovery algorithms in unsigned graphs adeptly handle vast networks comprising millions of vertices and edges [Traag and ≈†ubelj, 2023], the modeling of unsigned graphs falls short in capturing the complex relationships within networks. The ascendancy of signed graph modeling as a more fruitful and meaningful data representation is evident. Recent investigations demonstrate the struggles of contemporary signed graph methods in recovering communities within graphs featuring mere thousands of vertices and hundreds of thousands of edges [Maria Tomasso et al., 2022b]. The latest research underscores the challenges posed by spectral methods in recovering community structure within sparse networks, even with the incorporation of normalization techniques [Cucuringu et al., 2021]. This finding aligns with our conclusions: there is a significant degradation in the performance of spectral methods for clustering large, sparse signed networks derived from authentic data sources [Maria Tomasso et al., 2022a]. In the realm of signed networks, balance theory stands as a pivotal concept, explaining the evolution of attitudes within networks. Established by Heider [Abelson and Rosenberg, 1958] and subsequently formalized mathematically by Harary, who introduced k-way balance [Cartwright and Harary, 1956, Harary and Cartwright, 1968], balance theory has found applications in predicting edge sentiment, content and product recommendations, and anomaly detection in various domains [Derr et al., 2020, Garimella et al., 2021, Interian et al., 2022, Amelkin and Singh, 2019]. 1.1 Spectral Clustering of Real Signed Graphs Spectral clustering was proposed in 1973 by Donath and Hoffman [Donath and Hoffman, 1973], and the proposition is that the graph can be partitioned using the eigenvectors of the adjacency matrix. We outline the generic spectral clustering process in four steps: (1) computation of the Laplacian variant and identification of clusters (kùëòkitalic_k); (2) derivation of kùëòkitalic_k eigenvectors corresponding to the kùëòkitalic_k smallest eigenvalues; (3) formation of the eigenvector matrix U to reduce dimensionality; and (4) application of k-means++ to cluster features. The SigNeT package, employed for spectral methods in signed networks, encompasses multiple Laplacian variants, all presumed to be positive semi-definite [et al., 2018]. The scalability of spectral clustering algorithms is limited in the context of real signed networks: (i) the substantial time required for the solver to formulate eigenvectors with their associated eigenvalues, and (ii) the susceptibility to spectral pollution or eigenvalue pollution, manifesting as instability in the approximation implementations for large matrices and consequential error fluctuations [Boulton, 2016]. A recent survey has provided a proof-of-concept and practical evaluation of these limitations in actual signed graphs [Maria Tomasso et al., 2022a]. The survey falls short of delineating the breaking points of signed Laplacians concerning algorithmic assumptions (e.g., small world, diameter) and characteristics specific to signed networks (e.g., density, sparsity) [Maria Tomasso et al., 2022a]. Another significant drawback in contemporary signed graph clustering algorithms pertains to the challenging task of determining the suitable number of communities (kùëòkitalic_k) before algorithmic execution. A common approach for dealing with this is to utilize the elbow method. However, this method is subjective in nature because it introduces variability, rendering it dependent on individual perspectives. Furthermore, the decision-making process is derailed by various factors and network characteristics, including sparsity, average degree, and overall network structure, which collectively influence the selection of the optimal number of communities. The evaluation of why one community partitioning surpasses another is not clear in the absence of meaningful ground truth, as in [Maria Tomasso et al., 2022a]. 1.2 Research Contributions GraphC is a term contraction of a graph clustering phrase. GraphC is a parameter-free clustering algorithm for signed graphs. Gisubscriptùê∫ùëñG_{i}italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT notes the signed graph with an unsigned topology Gùê∫Gitalic_G (depicted in Figure 1), and Gi‚Å¢jsubscriptùê∫ùëñùëóG_{ij}italic_G start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT symbolize a collection of vertex sets discovered by clustering algorithm jùëójitalic_j. GraphC reformulates the clustering problem by shifting the emphasis from minimizing the combined count of positive edges between communities and negative edges within communities. The loss minimizes the fraction of positive edges between communities and the fraction of negative edges within communities for algorithm jùëójitalic_j applied to the signed graph Gisubscriptùê∫ùëñG_{i}italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. We quantify the positive edges outside communities as p‚Å¢o‚Å¢so‚Å¢u‚Å¢t‚Å¢(Gi‚Å¢j)ùëùùëúsubscriptùë†ùëúùë¢ùë°subscriptùê∫ùëñùëópos_{out}(G_{ij})italic_p italic_o italic_s start_POSTSUBSCRIPT italic_o italic_u italic_t end_POSTSUBSCRIPT ( italic_G start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ) (Eq. 4), and the negative edges within communities as n‚Å¢e‚Å¢gi‚Å¢n‚Å¢(Gi‚Å¢j)ùëõùëísubscriptùëîùëñùëõsubscriptùê∫ùëñùëóneg_{in}(G_{ij})italic_n italic_e italic_g start_POSTSUBSCRIPT italic_i italic_n end_POSTSUBSCRIPT ( italic_G start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ) (Eq. 5) for algorithm jùëójitalic_j operating on the signed graph Gisubscriptùê∫ùëñG_{i}italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. GraphC identifies optimal clusters without the need for a predefined kùëòkitalic_k and a spectral decomposition solver. Paper Contributions: 1. Spectral-Balance duality: we prove that Harary cuts from a balanced signed network are equivalent to an eigenvector with a 0 eigenvalue for that balanced network. 2. GraphC implements this theoretical novelty and directly performs Harary cuts on the balanced signed graph by purging the negative edges than it is using Laplacian. 3. GraphC algorithm can process signed graphs with tens of millions of vertices and edges. Parameterized trade-off between speed and the overall quality of the clustering that allows the algorithm to scale on a single workstation. 4. GraphC is kùëòkitalic_k-independent: it does not require a predefined number of clusters to produce a high-quality clustering. The parameters used in the algorithm are dependent on computing resources. 5. GraphC addresses the positive-negative edge imbalance that naturally occurs in most signed networks as it takes equally into account the contribution of positive edges and the negative edges in the graph for clustering optimization."
https://arxiv.org/html/2411.00714v1,Self-reinforcing cascades: A spreading model for beliefs or products of varying intensity or quality,"Models of how things spread often assume that transmission mechanisms are fixed over time. However, social contagions‚Äìthe spread of ideas, beliefs, innovations‚Äìcan lose or gain in momentum as they spread: ideas can get reinforced, beliefs strengthened, products refined. We study the impacts of such self-reinforcement mechanisms in cascade dynamics. We use different mathematical modeling techniques to capture the recursive, yet changing nature of the process. We find a critical regime with a range of power-law cascade size distributions with varying scaling exponents. This regime clashes with classic models, where criticality requires fine tuning at a precise critical point. Self-reinforced cascades produce critical-like behavior over a wide range of parameters, which may help explain the ubiquity of power-law distributions in empirical social data.","Introduction Cascades of beliefs, ideas, or news often show signs of criticality despite coming from various sources and spreading through different mechanisms Notarmuzi et al. (2022). This signature of criticality takes the form of a power-law tail in the cascade size distribution, scaling as s‚àíœÑsuperscriptùë†ùúès^{-\tau}italic_s start_POSTSUPERSCRIPT - italic_œÑ end_POSTSUPERSCRIPT. Cascade models predict this behavior at a precise critical point, the phase transition between a regime where all cascades eventually go extinct and another where they can grow infinitely. At this point, cascade models that follow a branching process structure universally predict a scaling exponent of œÑ=3/2ùúè32\tau=3/2italic_œÑ = 3 / 2 Harris et al. (1963). We call this critical exponent universal because, for a large family of spreading mechanisms, its value does not depend on the details of the model Radicchi et al. (2020). However, social media data show that cascade sizes can follow power-law distributions with scaling exponents much different from the prediction œÑ=3/2ùúè32\tau=3/2italic_œÑ = 3 / 2. The size of reply trees might decay faster with a scaling exponent of œÑ=4ùúè4\tau=4italic_œÑ = 4 Nishi et al. (2016), as do reposting cascades with an exponent œÑ=2.3ùúè2.3\tau=2.3italic_œÑ = 2.3 Wegrzycki et al. (2017), and many other data sources on platforms with exponents around œÑ=2ùúè2\tau=2italic_œÑ = 2 Notarmuzi et al. (2022). The difference between the universality observed in cascade models and the diversity of empirical results is yet unexplained. Although cascade models vary, the vast majority of them use fixed mechanisms such that the same rules apply at every step of the cascade. For example, a new case of a disease produces infections through the same mechanism as the previous cases do. However, cascades of beliefs and ideas might be different. Beliefs can be reinforced and strengthened when instilled by a passionate teacher. Ideas or products can be refined as they are transmitted from one person to the next."
https://arxiv.org/html/2411.00644v1,What can we learn from marketing skills as a bipartite network from accredited programs?,"The relationship between professional skills and higher education programs is modeled as a non-directed bipartite network with binary entries representing the links between 28 skills (as captured by the occupational information network, O*NET) and 258 graduate program summaries (as captured by commercial brochures of graduate programs in marketing with accreditation standards of the ‚ÄúAssociation to Advance Collegiate Schools of Business‚Äù). While descriptive analysis for skills suggests a qualitative lack of alignment between the job demands captured by O*NET, inferential analyses based on exponential random graph model estimates show that skills‚Äô popularity and homophily coexist with a systematic yet weak alignment to job demands for marketing managers.","A challenging decision for any manager is placing the right person in the right job. In search for predictors of job performance, the American psychologist David C. McClelland [29] argued that academic aptitude, knowledge from content tests, school grades, and credentials are often poor predictors of employee performance. McClelland‚Äôs research laid the groundwork for the ‚Äújob competence assessment‚Äù method, which became foundational to management literature in the latter half of the 20th century [38]. Contemporary data-driven research supports McClelland‚Äôs findings, suggesting that academic offerings often fall short of meeting companies‚Äô job requirements [12]. In this context, gaining a competitive advantage from a labor perspective increasingly relies on improving the educational-occupational match [19]. Some scholars argue that one way to enhance this education-job alignment is by estimating the value of employees‚Äô skills, especially given the unpredictable future [41]. From a social network perspective, a network researcher can consider that skills represent a set with connections to university programs that represent the academic offering [21]. The formalization of these connections is a bipartite network structure represented as a triple Y=(R,C,E)ùëåùëÖùê∂ùê∏Y=(R,C,E)italic_Y = ( italic_R , italic_C , italic_E ). In this triple, RùëÖRitalic_R and Cùê∂Citalic_C refer to two disjoint sets of nodes, with RùëÖRitalic_R being the first partition of nodes (i.e., rows in a bi-adjacency matrix) and Cùê∂Citalic_C being the second partition of nodes (i.e., columns in a bi-adjacency matrix), and Eùê∏Eitalic_E ‚äÜR√óCabsentùëÖùê∂\subseteq R\times C‚äÜ italic_R √ó italic_C is the set of edges of the network [23]. As per [21], this bipartite network facilitates a deeper understanding of skills‚Äô centrality in the landscape of the academic offering, which has conceptual roots with limited data-driven evidence. A conceptual root of the above-mentioned bipartite network goes back to the ‚Äúoccupational information network‚Äù (O*NET). According to [33], the goal of O*NET was to ‚Äúpromote the effective education, training, counseling, and employment of the American workforce. It should accomplish its purpose by providing a database system that identifies, defines, classifies, and describes economic occupations in an accessible and flexible manner‚Äù (p. 455). The data-driven evidence supporting the use of bipartite networks as a tool for modeling the relationship between skills and university programs is more recent than the conceptual roots just mentioned. The empirical work of [19] and [21] provide relevant pillars to the present work. Apart from these two references, the work of [13] deserves special mention for their initial insights into the modeling perspective. Let us review the goal of these previous works to argue why the present work offers an extension of their findings. [13] analyzed a case study on open-source software development by reconstructing the bipartite network structure generated by the association between 72 professionals (software developers) and 737 problems (software bugs) they had to solve during an entire development cycle of the free/open source software project Epiphany. Among their results, they found that problem solvers tend to distribute their activity over multiple software bugs. However, these bugs tend not to share multiple contributors, and this dual trend toward de-specialization and exclusivity relied on specific local network dependencies estimated by exponential random graph models. Although [13] did not consider university programs in their case study, their inferential approach based on exponential random graph allowed them to go beyond network-based descriptive statistics and contrast some hypotheses about the local network properties as predictors of the entire network. [19] proposed a bipartite network structure to analyze the educational-occupation match of 15,253 subjects who worked in 402 occupations and graduated from 398 education programs delivered by 52 institutions in Hungary. Their results showed that computer science, nursing, and engineering programs connected to occupations with a scarce source of workers. They also found five components or modules in their network, but the programs-occupation connectivity was stronger in three of these modules. The first module grouped teaching, humanity, and art programs (Germanistic studies), and half of their occupations did not require a higher education diploma, although a quarter of graduates from Germanistic studies worked in the manufacturing and IT sector due in part to their advanced knowledge of the German language. The second module showed connections between business, economics, finance, human resources, social work, nursing, and medical programs with health care, teaching, child care workers, medical technicians, and personal care jobs. The third module revealed the links between engineering, IT, physics, ecology, and earth sciences with production, manufacturing, information managers, life science, and engineering professionals. [21] proposed a bipartite network structure to estimate the importance of soft skills centrality by sampling 230 programs for three types of graduate studies (i.e., Specialization, Master, and Doctorate) with two quality accreditation standards offered by 49 universities in Colombia. Using text mining techniques focused on keyword-in-context search, they identified 31 soft skills (i.e., skills related to socio-emotional behavioral control) in programs‚Äô intended learning outcomes. Their results showed that skills‚Äô centrality varied as a function of the graduate program without revealing statistically significant differences. In addition, while most central skills tend to be those related to creativity, leadership, and analytical thinking, less central skills were those related to empathy, ethical thinking, and critical thinking. Based on these results, the authors claimed to what extent emphasizing the most visible skills might imply an imbalance in the opportunities to enhance other soft skills, such as ethical thinking. Compared with existing research, this work extends the evidence on the relationship between professional skills and university programs modeled as a bipartite network. Our contribution elaborates upon the case of marketing programs as they represent a niche of accredited business schools‚Äô academic offerings. The rationale behind this case is twofold. On the one hand, business school leaders have made crystal clear why radical innovations are essential for their educational practices, given the potential transformation associated with artificial intelligence and other emerging technologies [25, 36]. On the other hand, as the interaction between soft skills and training programs is vital to enhancing employee job performance [27], an updated perspective on professional training should consider that artificial intelligence technologies have entered into the classroom [17]. These technologies can serve as the building blocks for future educational innovations based on skills-programs networks."
https://arxiv.org/html/2411.00028v1,Synergizing LLM Agents and Knowledge Graph for Socioeconomic Prediction in LBSN,"The fast development of location-based social networks (LBSNs) has led to significant changes in society, resulting in popular studies of using LBSN data for socioeconomic prediction, e.g., regional population and commercial activity estimation. Existing studies design various graphs to model heterogeneous LBSN data, and further apply graph representation learning methods for socioeconomic prediction. However, these approaches heavily rely on heuristic ideas and expertise to extract task-relevant knowledge from diverse data, which may not be optimal for specific tasks. Additionally, they tend to overlook the inherent relationships between different indicators, limiting the prediction accuracy. Motivated by the remarkable abilities of large language models (LLMs) in commonsense reasoning, embedding, and multi-agent collaboration, in this work, we synergize LLM agents and knowledge graph for socioeconomic prediction. We first construct a location-based knowledge graph (LBKG) to integrate multi-sourced LBSN data. Then we leverage the reasoning power of LLM agent to identify relevant meta-paths in the LBKG for each type of socioeconomic prediction task, and design a semantic-guided attention module for knowledge fusion with meta-paths. Moreover, we introduce a cross-task communication mechanism to further enhance performance by enabling knowledge sharing across tasks at both LLM agent and KG levels. On the one hand, the LLM agents for different tasks collaborate to generate more diverse and comprehensive meta-paths. On the other hand, the embeddings from different tasks are adaptively merged for better socioeconomic prediction. Experiments on two datasets demonstrate the effectiveness of the synergistic design between LLM and KG, providing insights for information sharing across socioeconomic prediction tasks.","The development of location-based social networks (LBSNs) has significantly advanced socioeconomic prediction with rich web-sourced LBSN data such as user-generated content on review platforms. Socioeconomic indicators like regional population, user activity, and rating, provide a more comprehensive description of LBSN in turn, which can be leveraged by various web applications to offer enhanced services such as location recommendation, web page description, and personal assistants. As a result, socioeconomic prediction in the context of LBSN has become increasingly important, leading to a growing body of research in this field (Wang et al., 2016; Wang and Li, 2017; Yang et al., 2017; Yao et al., 2018; Dong et al., 2019; Xu et al., 2020; Zhang et al., 2021; Wu et al., 2022; Hou et al., 2022; Luo et al., 2022; Kim and Yoon, 2022; Zhou et al., 2023) Traditionally people collect socioeconomic indicators from surveys, which are costly and time-consuming. Recently, data-driven methods have become popular, which use machine learning models to predict the socioeconomic indicators based on various LBSN data. The LBSN data comes from various sources and is heterogeneous. Existing studies have widely used graph structure to model the complex relationships within LBSN data, and predict the socioeconomic indicators through graph representation learning methods. They construct either multi-view graphs (Zhang et al., 2021; Wu et al., 2022; Kim and Yoon, 2022) or knowledge graphs (KGs) (Zhou et al., 2023; Liu et al., 2023c) to model different factors in LBSN data like mobility, spatial proximity, and functionality. However, These approaches highly rely on heuristic ideas and expertise to extract knowledge related to the tasks from LBSN data, such as the construction of sub-graphs or definition of meta-structures, which may be sub-optimal for different indicator prediction tasks. Moreover, the intrinsic correlations and potential for knowledge sharing across different socioeconomic prediction tasks are often overlooked, limiting the overall prediction accuracy. The recently emerged large language models (LLMs) provide a promising solution to these limitations. LLMs have demonstrated several remarkable abilities which could help socioeconomic prediction (Wei et al., 2022a): (1) Latent semantic embedding. At the foundational level, LLMs are able to generate text embeddings with rich semantic information, which makes it possible to integrate LLM with deep learning models to improve the performance by leveraging the inherent semantic information in the LBSN data. (2) Explicit commonsense reasoning. Moreover, LLMs possess vast commonsense knowledge, based on which they can perform complex reasoning (Zhao et al., 2024). This capability facilitates the automatic extraction of task-relevant knowledge from LBSN data. (3) Multi-agent collaboration. At the highest level, LLM agents can communicate with other agents through natural language, and collaborate to solve complex tasks which is difficult for a single agent (Li et al., 2023a; Hong et al., 2023; Xiao et al., 2023). Such ability enables LLM to transfer knowledge across different socioeconomic prediction tasks via semantic-rich natural language, and collaboratively improve the performance. The recently emerged large language models (LLMs) provide a promising solution to these limitations. LLMs have demonstrated many remarkable abilities which could help socioeconomic prediction (Wei et al., 2022a): (1) Commonsense reasoning. LLMs possess vast commonsense knowledge, based on which they can perform complex reasoning (Zhao et al., 2024), thus enabling automatic task-relevant knowledge discovery from LBSN data. (2) Multi-agent collaboration. LLM agents can communicate with other agents through natural language, and collaborate to solve complex tasks which is difficult for a single agent (Li et al., 2023a; Hong et al., 2023; Xiao et al., 2023). Such ability enables LLM to transfer knowledge across different socioeconomic prediction tasks, and collaboratively improve the performance. (3) Semantic embedding. LLMs are able to generate text embeddings with rich semantic information, which makes it possible to integrate LLM with deep learning models and enhance the performance with semantic information in the LBSN data. Inspired by this, we propose a learning framework that Synergize LLM Agent and Knowledge Graph learning model (SLAK) for socioeconomic prediction. We first construct a location-based knowledge graph (LBKG) to comprehensively integrate LBSN data. To extract task-relevant knowledge, we construct an LLM agent to automatically discover meta-paths in the LBKG that help the prediction for specific indicators, and extract a corresponding meta-path-based sub-KG. Moreover, we leverage KG representation learning model to distil knowledge from each meta-path-based sub-KG, and design a semantic-enhanced knowledge fusion module to adaptively fuse the knowledge based on semantic embeddings of meta-paths obtained from LLM. In addition, we propose a cross-task communication mechanism to enable knowledge sharing across different socioeconomic prediction tasks at both the LLM agent and KG level. Specifically, the LLM agents collaborate to extract better meta-paths, and we also adaptively merge the KG embeddings from different tasks with the knowledge fusion module. Our contribution can be summarized as follows: ‚Ä¢ We propose a framework to synergize LLM agents and KG for socioeconomic prediction. We leverage the reasoning capability of LLM agent to find task-relevant meta-paths from LBKG, and further use the semantic information of the meta-paths to guide the knowledge fusion. ‚Ä¢ We design a cross-task communication mechanism to enable knowledge sharing across different socioeconomic prediction tasks at both the LLM agent level through multi-agent collaboration and the KG level through semantic attention, which further improves the accuracy. ‚Ä¢ Extensive experiments on two city-scale datasets show that our model surpasses existing methods by 2.9-74.2% in terms of R2superscriptùëÖ2R^{2}italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT on eight indicator prediction tasks, demonstrating the effectiveness of our synergistic model between LLM and KG. Several in-depth analyses further show the advantage of our model design, providing insights for web-based socioeconomic prediction in LBSN."
