URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.04994v1,"Public Procurement for Responsible AI?Understanding U.S. Cities’ Practices, Challenges, and Needs","Most AI tools adopted by governments are not developed internally, but instead are acquired from third-party vendors in a process called public procurement. While scholars and regulatory proposals have recently turned towards procurement as a site of intervention to encourage responsible AI governance practices, little is known about the practices and needs of city employees in charge of AI procurement. In this paper, we present findings from semi-structured interviews with 18 city employees across 7 US cities. We find that AI acquired by cities often does not go through a conventional public procurement process, posing challenges to oversight and governance. We identify five key types of challenges to leveraging procurement for responsible AI that city employees face when interacting with colleagues, AI vendors, and members of the public. We conclude by discussing recommendations and implications for governments, researchers, and policymakers.","Artificial intelligence is increasingly utilized in the public sector to automate bureaucratic process and workflows, and assist critical decision-making processes that impact residents (Reisman et al., 2018; Kim et al., 2024; Chouldechova et al., 2018; Eubanks, 2018; Whitney et al., 2021; Kawakami et al., 2024a; Levy et al., 2021). Often, such public-sector AI applications are not developed in-house, but are purchased from external third-party vendors through a process called “public procurement” (Sloane et al., 2021; Prier and McCue, 2009; Lloyd and McCue, 2004). In fact, in a 2023 opening statement for the full committee hearing on AI and procurement, U.S. Senator Gary Peters stated that “over half of the AI tools used by federal agencies have been purchased from commercial vendors” (Peters, 2023). Experts estimate that this number is even higher at lower levels of government, such as state and local governments that are even less likely to have internal expertise to develop AI (Rubenstein, 2020, 2021; Mulligan and Bamberger, 2019). Thus, most public-sector AI systems used today are developed by and acquired from private vendors. A growing number of academic and advocacy efforts have pointed out how AI systems procured in the public sector have predominantly targeted narrowly defined notions of efficiency and performance enhancements, resulting in adverse effects that disparately impact marginalized communities (Eubanks, 2018; Roberts, 2022; Stapleton et al., 2022; Harcourt, 2006; in Pittsburgh, 2020; Bhuiyan, 2021). While such incidents have exposed flaws in individual AI systems, they highlight deeper issues in how AI is acquired, used, and governed in the public sector. The AI procurement process encompasses decisions of which AI tools to ask for, adopt or reject, and the manner in which they are developed and deployed: decisions of critical importance for communities who may be harmed by AI. Such decisions not only influence the performance and risks posed by AI systems, but also play a significant role in shaping broader governance practices and ethical standards by which AI operates in the public sector. Interestingly, there is a long history of governments adapting their public procurement practices to enact social change, e.g., by creating processes that prioritize minority-owned businesses (McCrudden, 2007), enable public oversight over government surveillance (Young et al., 2019), or incentivize other ethical behaviors, such as sustainability (Lăzăroiu et al., 2020; Varga, 2021). Drawing from this tradition, several scholars point out how existing public procurement processes such as competitive solicitations (e.g., RFPs), vendor selection practices, and contract negotiations pose several opportunities to encourage more responsible practices surrounding the adoption and use of AI. A shared recognition of AI procurement as a gateway to more responsible adoption and use of the technology in public sector has sparked a recent explosion of attention and action taken by governments, academics, and think-tanks to develop AI procurement guidelines and resources to be used by government employees (Shapanka et al., 2024; Richardson, 2021; Hasselbach et al., 2020; Project, 2024; of Canada, 2023). While a handful of these resources have been empirically investigated (Richardson et al., 2019), we still lack a broader understanding of if, and how, such resources align with governments’ actual contexts and needs. Our Contributions. In a time when AI procurement has become a pressing matter of policy attention, we believe that empirical research—to understand the challenges government employees face when attempting to incorporate responsible AI considerations into their procurement practices—can help inform policy development and implementation. To date, there is a dearth of empirical research focused on understanding governments’ AI procurement practices. To address this gap, this work builds on the burgeoning efforts across the United States to assist governments in procuring AI and investigates how city employees are approaching the procurement of AI systems. In this paper, we present findings from semi-structured interviews with 18 city employees across 7 cities who are responsible for AI procurement in their city."
https://arxiv.org/html/2411.04340v1,Survival of the Notable: Gender Asymmetry in Wikipedia Collective Deliberations,"Communities on the web rely on open conversation forums for a number of tasks, including governance, information sharing, and decision making. However these forms of collective deliberation can often result in biased outcomes. A prime example are Articles for Deletion (AfD) discussions on Wikipedia, which allow editors to gauge the notability of existing articles, and that, as prior work has suggested, may play a role in perpetuating the notorious gender gap of Wikipedia. Prior attempts to address this question have been hampered by access to narrow observation windows, reliance on limited subsets of both biographies and editorial outcomes, and by potential confounding factors. To address these limitations, here we adopt a competing risk survival framework to fully situate biographical AfD discussions within the full editorial cycle of Wikipedia content. We find that biographies of women are nominated for deletion faster than those of men, despite editors taking longer to reach a consensus for deletion of women, even after controlling for the size of the discussion. Furthermore, we find that AfDs about historical figures show a strong tendency to result into the redirecting or merging of the biography under discussion into other encyclopedic entries, and that there is a striking gender asymmetry: biographies of women are redirected or merged into biographies of men more often than the other way round. Our study provides a more complete picture of the role of AfD in the gender gap of Wikipedia, with implications for the governance of the open knowledge infrastructure of the web.","In 2018, the physicists Donna Strickland, Gérard Mourou, and Arthur Ashkin received the Nobel Prize for “groundbreaking inventions in the field of laser physics”111Morou and Strickland shared half of the price for their work on chirped pulse amplification; the other half went to Ashkin for separate work.. The Nobel prize in Physics recognizes a lifetime of accomplishments of a scientific nature, but this award carried additional significance since it was the first to go to a woman in 55 years. Nonetheless, in the immediate aftermath of the announcement, internet users looking to know more about Donna Strickland could not find a Wikipedia entry about her — as one would normally expect when looking up an accomplished scientist — even though her two co-awardees had been listed in the encyclopedia since 2005. Later, it was discovered that there had indeed been an attempt to create an entry for her in Wikipedia. Some editors had drafted her biography, but this draft had never been published, and thus had never been indexed by search engines, since — unlike her scientific work — she had not been considered to be ‘notable’ enough to warrant the creation of a separate, individual entry about her (Bazely, 2018). On February 11, 2019, the biography of nuclear scientist Clarice Phelps, who is the first African American woman to help discover a new chemical element, was deleted from Wikipedia (Jarvis, 2019). The debate over whether Phelps met the internal notability standards of Wikipedia was contentious. Editors initially deleted her entry, then reinstated it, only to delete it again later. In total, her entry was deleted (and later reinstated) three times — each deletion sparking increasingly heated discussions. Last but not least, in 2017 a Wikipedia editor raised concerns about the notability of Margaret D. Foster (1895–1970), an American chemist known for being the first female chemist at the United States Geological Survey and for her involvement on the Manhattan Project (Harrison, 2019). Despite this record, her Wikipedia page was flagged for potential deletion due to concerns about her notability. These three scenarios — which are drawn from STEM areas but are not necessarily limited to those fields — exemplify how barrier-breaking women get undermined and undervalued and face challenges to be fully included in Wikipedia — the largest and most influential online encyclopedia. Though this issue reflects broader challenges women encounter across the web, the central role of Wikipedia in shaping public knowledge makes it a critical site where women struggle for the recognition they deserve. As a free encyclopedia, Wikipedia aspires to encapsulate the entirety of human knowledge. It stands as a significant source of encyclopedic information encompassing notable individuals from diverse countries, historical periods, and fields of knowledge on a global scale. Within its extensive body of knowledge, about 1.9 million entries cover the biographies of notable individuals like scientists, artists, politicians, etc., yet very few of these biographies are about women (approximately 19% in the English version, the largest of all Wikipedia versions (Konieczny and Klein, 2018)), highlighting a substantial gender gap in encyclopedic coverage between men and women. This is concerning, since the recognition of the merits of an individual, particularly in a societal context, requires fair representation, but also because the presence of this kind of gap in one of the pillars of the digital knowledge infrastructure poses the risk of propagating preexisting social biases in other digital platforms, and thus of further strengthening the underlying gender inequality in society (Conway, 2018). For example, Wikipedia is often used to train Machine Learning models for a variety of tasks, representing one of the highest quality corpora openly available. As a result, several AI models are at risk of being biased against women in various applications (Stanovsky et al., 2019; Gor et al., 2021; Kotek et al., 2023). Figure 1. Multi-state model depicting competing risks of nomination and each outcome of the deliberation \Description Multi-state model depicting competing risks of nomination and each outcome of the deliberation Prior work on the gender gap in Wikipedia has found that articles about women are brought up for deletion more frequently than those about men (Tripodi, 2023; Martini, 2023; Lemieux et al., 2023), yet it is not clear how and when this takes place. Like many other editorial actions, the deletion of an article is subject to the regular collaborative norms of Wikipedia (Reagle, 2007), and thus articles are susceptible of being flagged for deletion at any point during their editorial cycle. Our objective is thus to investigate the following question: how quickly do Wikipedia biographies interact with the Articles for Deletion (AfD) process? Specifically, we aim to determine if there is any gender-based discrepancy in the various stages of the process, from the initial nomination for deletion to the consensus outcome taken by the discussants (if any). Our analysis covers the full history of the AfD process, from January 15, 2001 to November 3, 2023. Definition of gender For our analysis, we rely on human annotations provided by the Wikidata project to identify the gender of biographical subjects. Even though the vital records information from Wikidata provided us with rich gender information about the subjects of biographies, in this study, we chose to restrict ourselves to a gender binary (i.e., man and woman). This choice was dictated by the limited frequency of other, non-binary gender labels in the dataset (0.09%), which would have severely limited our ability to draw reliable statistical inferences about them. Thus, it is important to acknowledge that our analysis is restricted in its scope, as it does not encompass all existing notions of gender. The rest of the article proceeds as follows. First, to define the scope of the problem, we review how prior work has documented the gender gap on Wikipedia and its community. We then provide a brief description of how the Wikipedia deletion process works and discuss prior literature on gender and deletion discussions. From this discussion we derive the hypotheses that guide our main research question, and highlight how a host of contingent factors, like the historical evolution of Wikipedia as a mass collaboration project, are relevant when measuring the gender disparity in deletion discussions. Since Wikipedia is a repository of both historical and contemporary knowledge, we also considered two more factors: the additional considerations that Wikipedians typically take into account when debating about the merits of living people to be featured on Wikipedia, and the challenges in finding information about historical figures. In the Discussion section, we go back to the motivating research question, consider the main limitations of our analyses, and point to ways in which future work can build on our approach."
https://arxiv.org/html/2411.04037v2,Taming Toxicity or Fueling It? The Great Ban’s Role in Shifting Toxic User Behavior and Engagement,"In today’s online environments users experience harm and abuse on a daily basis. Therefore, content moderation is crucial to ensure their safety and well-being. However, the effectiveness of many moderation interventions is still uncertain. We evaluate the effectiveness of The Great Ban, one of the largest deplatforming interventions carried out by Reddit that affected almost 2,000 communities. We analyze 53M comments shared by nearly 34K users, providing in-depth results on both the intended and unintended consequences of this ban. We found that 15.6% of the moderated users abandoned the platform while the remaining ones decreased their overall toxicity by 4.1%. Nonetheless, a subset of those users increased their toxicity by 70% after the intervention. In any case, increases in toxicity did not lead to marked increases in activity or engagement, meaning that the most toxic users had overall a limited impact. Our findings bring to light new insights on the effectiveness of deplatforming. Furthermore, they also contribute to informing future content moderation strategies.","Content moderation is essential for online platforms, as it prevents the spread of harmful content and hateful behavior such as the perpetuation of hate speech [1]. It is also crucial to promote fairness and safety among users by enforcing ethical standards and supporting the health of online communities [2]. In fact, platforms use content moderation as a means to enforce their policies [3]. In particular, administrators apply different types of interventions, which can be more or less severe. For example, they can send short warning messages and use informative labels [4], or even take the drastic decision to remove content and/or users [5, 6]. However, despite the increasing reliance on content moderation, there is still little understanding of the effects of most moderation interventions, which threatens their efficacy. Recent studies have demonstrated that while some interventions had heterogeneous [7, 8] or even ineffectual outcomes [9], others led to undesirable consequences [10, 11]. This is why it is crucial to assess the effects of recent moderation interventions as a preliminary step in planning and developing new ones. The most popular and widely used intervention is known as deplatforming, which involves the removal of content, users, or even entire communities [12]. Notorious examples are the ban that Donald Trump received in 2021 from Facebook and X (formerly Twitter) [13] and the deplatforming of three particularly toxic influencers from X [6]. Additionally, X removed accounts involved in coordinated inauthentic behavior [14] and Reddit permanently shut down different communities because of racism, sexism and hatefulness [15, 16]. In June 2020, Reddit itself hosted one of the biggest deplatforming campaigns in the history of social media –The Great Ban– which resulted in around 2,000 subreddits being banned due to ongoing spread of toxicity and hate speech.111https://www.reddit.com/r/announcements/comments/hi3oht/update_to_our_content_policy/ (accessed: 10/15/2024) Among these are popular communities such as r/The_Donald and r/ChapoTrapHouse. Despite its impact on several communities and users within and outside Reddit, its effects are still little-explored. For example, current research on The Great Ban has mainly investigated the changes in the writing style of the users [17] without assessing the ban’s effectiveness. Few studies analyzed the changes in toxicity, and those who did focused only on a small set of subreddits [5]. Moreover, the majority of the existing works on The Great Ban are focused on community-level effects, neglecting the individual user-level responses that are instrumental for understanding how effective the ban was in mitigating problematic behaviors [18, 8]. In a previous work, we provided preliminary results on the effectiveness and unintended consequences of The Great Ban [19]. Here, we extend our previous analysis by adopting a robust causal method based on Difference-in-Differences (DiD) to estimate the effects of the intervention. Furthermore, we address an additional research question investigating the impact that toxic users had after the intervention, as explained in the following. Research focus. We address the outstanding knowledge gaps by conducting a comprehensive quantitative causal analysis of the changes in toxicity among users active in the 15 most popular subreddits involved in The Great Ban. We analyze 53M comments posted by nearly 34k users over a period of 14 months guided by the following research questions. ∙∙\mathbin{\vbox{\hbox{\scalebox{0.75}{$\bullet$}}}}\;∙RQ1: Did The Great Ban effectively reduce toxicity? Studies have shown that some interventions led to an increase rather than a decrease in toxic behavior. Here we assess the effectiveness of The Great Ban in reducing toxicity, as hate and toxic speech were the main reasons behind the ban. ∙∙\mathbin{\vbox{\hbox{\scalebox{0.75}{$\bullet$}}}}\;∙RQ2: Did The Great Ban lead to any unintended side effects for certain users? That is, were there users who became significantly more toxic after the intervention? The evaluation of the outcomes of a moderation intervention has to take into consideration the possible presence of users who grew resentful of the platforms and increased –rather than decreased– their toxicity. Such extreme reactions can arise even amid an overall reduction in toxicity at platform- or community- level, requiring further analyses at user level. In this study, we evaluate and estimate the extent of these reactions to The Great Ban, considering them as potential side effects of the intervention. ∙∙\mathbin{\vbox{\hbox{\scalebox{0.75}{$\bullet$}}}}\;∙RQ3: What were the behavioral dynamics of toxic users, in terms of their activity and generated engagement? To deepen our analysis, we examine further behavioral dimensions of those users who became much more toxic after The Great Ban. We specifically focus on their degree of activity on the platform and the engagement they receive from other users. Conjointly analyzing the dynamics of toxicity and activity is important as it can allow to identify users who are both very active and very toxic. Additionally, investigating the social feedback they receive may reveal the degree to which highly toxic behaviors are tolerated, and possibly even encouraged, by other users [20]. Main findings. Based on the insights gained from answering the previous research questions, our study produces the following main findings: • The Great Ban led 15.6% of the moderated users to abandon the platform, while those who stayed reduced their toxicity by an average of 4.1%. • While the reduction in toxicity was limited, a significant fraction of users became much more toxic. In particular, 5% of users increased their toxicity by over 70% compared to their pre-ban levels. • Resentful users who escalated their toxicity were found across each of the analyzed subreddits. However, their impact was limited probably due to their limited activity and the lack of positive feedback from other users. • Major changes in a dimension of user behavior do not necessarily lead to major changes in other dimensions. For example, the majority of users who drastically increased their toxicity did not experience significant changes in activity or engagement. Our work provides an in-depth analysis of the effects of The Great Ban. It points out the shortcomings by highlighting the complex challenges of moderating different communities. Our findings can guide the development of future moderation interventions aimed at increasing their effectiveness."
https://arxiv.org/html/2411.03938v1,Where postdoctoral journeys lead,"Postdoctoral training is a career stage often described as a demanding and anxiety-laden time when many promising PhDs see their academic dreams slip away due to circumstances beyond their control. We use a unique data set of academic publishing and careers to chart the more or less successful postdoctoral paths. We build a measure of academic success on the citation patterns two to five years into a faculty career. Then, we monitor how students’ postdoc positions—in terms of relocation, change of topic, and early well-cited papers—relate to their early-career success. One key finding is that the postdoc period seems more important than the doctoral training to achieve this form of success. This is especially interesting in light of the many studies of academic faculty hiring that link Ph.D. granting institutions and hires, omitting the postdoc stage. Another group of findings can be summarized as a Goldilocks principle: it seems beneficial to change one’s direction, but not too much.","The postdoc is the first career stage specialized for academia. In that sense, it is the first occasion where leaving academia could be seen as a failure. Yet, it is a bottleneck—many postdocs will be unsuccessful in finding faculty positions. In academia, postdocs are the ones with the most time on their hands to produce the core output of academia—science. Since postdocs have little to blame a meager output on, the stakes become sky-high. It is not surprising that the anecdotal picture of the postdoc experience is bleak—“disenchanted” [29], “stressed-out” [4], “unhappy” [9], and “exploited” [21] are all recent titular epithets in the literature. However, not all postdocs leave academia. Some go on to become scientific top-achievers, so what commonalities do these successful cases share? The emerging interface between data science and the study of academic knowledge production—the science of science [25])—not only brings us a new understanding of the societal enterprise behind it, but also has the potential to rectify structural biases and troubling trends in academic science. For example, recent studies focusing on American academia have shown that the prestige of the Ph.D. awarding institution has a significant impact on not only faculty hiring [5], but also retention and attrition of faculty members [27]. Essentially, a few high-prestige universities educate faculty across the American academia. Subsequent studies have argued that the dynamics behind faculty hiring perpetuate these structural biases. A vast majority of data-driven studies of the academic job market include the Ph.D. granting institution as an explanatory variable [5, 27, 14, 6, 7, 8, 10], but very few study the impact of the postdoc period. Two exceptions are works by Fernandes et al. [7] and Horta [12], finding that being awarded a postdoctoral fellowship is as vital for a successful early academic career as a top-tier first-author publication and that postdoc experience helps build long-lasting international networks. One possible reason for this lack of data-driven studies of postdocs is the prevailing descriptions of postdocs in passive terms—postdocs are a waiting or probation period [17, 22]. Papers usually take a systemic perspective—asking what the postdoctoral training contributes to society [3, 17, 1]—or focusing on the well-being of postdoctors [4, 9, 21]. With this paper, we aim to take a first step to fill the void of comprehensive, statistically grounded career advice for newly-minted doctors. To shed light on the postdoctoral bottleneck and the postdoctoral training’s role in early professorship, we collect a unique dataset drawing information both from a publication database (Microsoft Academic Graph, MAG) and a large online professional network (see Materials and Methods). The data covers 45,572 careers, spanning 25 years, from all academic disciplines and all over the Earth. For simplicity, we exclude careers other than those involving a postdoc in between academia and careers entirely within or without academia."
https://arxiv.org/html/2411.03914v1,Game-Theoretic Machine Unlearning: Mitigating Extra Privacy Leakage,"With the extensive use of machine learning technologies, data providers encounter increasing privacy risks. Recent legislation, such as GDPR, obligates organizations to remove requested data and its influence from a trained model. Machine unlearning is an emerging technique designed to enable machine learning models to erase users’ private information. Although several efficient machine unlearning schemes have been proposed, these methods still have limitations. First, removing the contributions of partial data may lead to model performance degradation. Second, discrepancies between the original and generated unlearned models can be exploited by attackers to obtain target sample’s information, resulting in additional privacy leakage risks. To address above challenges, we proposed a game-theoretic machine unlearning algorithm that simulates the competitive relationship between unlearning performance and privacy protection. This algorithm comprises unlearning and privacy modules. The unlearning module possesses a loss function composed of model distance and classification error, which is used to derive the optimal strategy. The privacy module aims to make it difficult for an attacker to infer membership information from the unlearned data, thereby reducing the privacy leakage risk during the unlearning process. Additionally, the experimental results on real-world datasets demonstrate that this game-theoretic unlearning algorithm’s effectiveness and its ability to generate an unlearned model with a performance similar to that of the retrained one while mitigating extra privacy leakage risks.","Machine learning models are generally trained using large-scale data. However, the widespread application of these technologies has resulted in significant privacy threats for data providers. As a result motivated legislation, such as the European Union’s General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA), have legally established the right to be forgotten, which provides users the authority to revoke personal data and its influence from a trained model. To achieve the goal, a new technology emerged, named machine unlearning, receiving considerable attention both in academia and industry [1, 2]. A straightforward way to implement machine unlearning is removing the requested data and retraining the model from scratch, but this type of methods results in high computational overhead [3]. Current studies on machine unlearning mainly focus on designing efficient unlearning algorithms without retraining [4, 5, 6]. Various machine unlearning methods have been proposed, which can be categorized into two classes: data-oriented unlearning and model-oriented unlearning. Data-oriented techniques achieve unlearning through modifying the original training set. For instance, SISA partitions the training data into multiple shards and trains submodels for each shard, to effectively erase the unlearned data through retraining the corresponding submodels [7]. Model-oriented techniques involve manipulating the original model. For example, certified removal mechanism in [8] can eliminate the impact of specific samples through updating the model parameters. While existing solutions have achieved significant performance in effective machine unlearning, underlying problems of machine unlearning remain have not been addressed completely. First, to satisfy privacy protection requirements, data removal conducted by machine unlearning usually reduces model performance [9]. In order to improve unlearning efficiency, some methods [10] utilize a reference model, trained on a subset of retain data, to adjust and optimize the parameters. This may weaken the model’s performance due to a small amount of training samples being employed. Second, researchers have recently observed that machine unlearning generates additional privacy disclosure risks for the unlearned data [11]. The differences between the initial and unlearned models, such as prediction vectors and labels, can be exploited by attackers to extract private information and infer the membership of target samples [12, 13]. These model discrepancies can also be adopted to reconstruct the unlearned data [14]. Therefore, establishing a trade-off between utility and privacy in machine unlearning is an urgent matter. In this case, privacy refers to deleting particular data and its influence from a trained ML model, and includes mitigating the potential privacy leakage risks during the post unlearning stage when providing inference services based on the unlearned model. The fundamental cause of two above challenges is the over-unlearning. It means that the unlearning strategy has unlearned more information than we requested. If we consider the party who would steal information as an attacker while the one who unlearns the model as the defender, the most suitable unlearning strategy to tackle the above challenges might be finding a balance between the attacker and defender. One the one hand, the defender unlearns the exact information to specifically fit for the unlearning request, while on the other hand, the unlearned information cannot be too much so that the attacker can infer the information by comparing original and unlearned models. In this setting, we claim that if we consider the attackers and the defender of the machine unlearning procedure as a game, we can tackle the aforementioned challenges by leveraging the game theory. Game theory is a theoretical framework to investigate the decision-making process of players, where each participant aims to maximize payoff or minimize cost when selecting strategies [15]. In machine unlearning, ensuring performance may increase additional privacy risks, while providing users with strong privacy protection can affect unlearning implementation. This implies that a competition between utility and privacy exists. Thus, game theory can be adopted to simulate interactive relationships and find stable solutions through equilibrium analysis. In this paper, we proposed a novel machine unlearning algorithm that takes advantage of game theory to model the unlearning process as a game, aiming to find the trade-off between utility and privacy. Given the effectiveness of machine unlearning, the attacker and defender are involved in this procedure as players, Moreover, we treated the attacker as a privacy evaluation mechanism, and considered data removal and optimization of the ML model as game participants.. Specifically, this unlearning algorithm consists of an unlearning and privacy module, which sequentially select optimal strategies based on the loss minimization principle. First, the unlearning module’s optimal strategy involves ensuring the efficacy of data removal. We adopted an alternative retrained model to adjust the original model, thereby providing an approximate statistical indistinguishability. We also employed an error term to compensate for model performance degradation. Second, privacy module’s strategy selection process is subject to the unlearning module’s actions, which holds an attack model to evaluate the privacy leakage risks of the unlearned model and update the parameters with the purpose of minimizing this risk, thereby mitigating additional privacy threats. After the game between the two modules, the algorithm generates an unlearned model that maintains good performance and reduces privacy risks. The contributions of this paper can be summarized as follows: • We investigated the trade-off between utility and privacy involved in machine unlearning, and proposed a novel game-theoretic unlearning algorithm. • We mathematically quantified the privacy leakage risks caused by model discrepancy in machine unlearning and introduced privacy attack advantage as an evaluation metric. • We provided theoretical analyses of the proposed unlearning algorithm, and proved the upper bound on the attacker’s privacy advantage for the unlearned model."
https://arxiv.org/html/2411.03449v1,"AI Horizon Scanning – White Paper p3395Part III. Technology
Watch: a selection of key developments, emerging technologies, and industry trends in Artificial Intelligence","Generative Artificial Intelligence (AI) technologies are in a phase of unprecedented rapid development following the landmark release of Chat-GPT, which brought the phenomenon to wide public attention. As the deployment of AI products rises geometrically, considerable attention is being given to the threats and opportunities that AI technologies offer, and to the need for regulatory and standards initiatives to ensure that use of the technology aligns with societal needs and generates broad benefits while mitigating risks and threats. This manuscript is the third of a series of White Papers informing the development of IEEE-SA’s p3995 ‘Standard for the Implementation of Safeguards, Controls, and Preventive Techniques for Artificial Intelligence Models’ [1], Chair Marina Cortês. This part focuses on assessing calmly and objectively, as far as is possible, the current state of Artificial Intelligence (AI) technology development and identifying predominant trends, prospects, and ensuing risks. It necessarily forms a snapshot of the current instant of a rapidly-evolving landscape, with new products and innovations emerging continuously. While our main focus is on software and hardware developments and their corporate context, we also briefly review progress on robotics within the AI context and describe some implications of the substantial and growing AI energy demand.","Generative Artificial Intelligence (AI) models promise a major impact on society through a variety of mechanisms that will require careful development and monitoring. A characteristic of the most influential models, the so-called Large Language Models (LLMs) based on the transformers architecture [2], is that they push at the boundaries of what is technologically feasible in hardware as well as software innovation. Simply training a leading-edge model requires specialist hardware at scale and major corporate resources in the range tens or even hundreds of millions of dollars. Even executing a query requires remote cluster-scale computation; servicing Chat-GPT queries is estimated to cost its operator OpenAI over one million dollars per day. Moreover, suitable hardware for running such tasks is currently essentially a monopoly of NVIDIA through its Graphics Processing Unit (GPU) technologies, with other companies struggling to build rival chipsets, such as Google with its Tensor Processing Units (TPU). The present authors are ongoing volunteer contributors to the emerging IEEE standard p3395 ‘Standard for the Implementation of Safeguards, Controls, and Preventive Techniques for Artificial Intelligence (AI) Models’, Chair: Marina Cortês, Vice-chair: Jayne Suess, Secretary: Janusz Zalewski [1]. As part of our horizon-scanning process to set the scene for developing the p3395 standard, already described in our initial article [3], we have undertaken a snapshot of the technological status and its direction of evolution, as of late Autumn 2024. This article describes that snapshot. Advances in terms of computing equipment during 2024 have appeared at an ever-increasing pace to support development of more powerful AI algorithms and multi-modal generational models. Almost every week major announcements are issued regarding new products (e.g. specialised hardware), services (new algorithms to support content creators), and new generative models. This means that more powerful services become possible, but also that these will be more expensive to lease and utilise. The danger of smaller research teams and countries being left out is clear, leading to the possibility of a handful of players having a dominant position in the field of AI. Since AI is at a state where all content can be used for training algorithms, there is a clear need to form safeguards to protect the public’s safety and privacy, without which there is a risk of personal data being stored in very large models and then inadvertently being divulged as a response to even an accidentally-posed query. The ever-increasing popularity of LLMs has made them a means of first and last recourse for both entrepreneurs and students, with tasks ranging from information gathering and report preparation to strategic planning. The public seems in awe of the executive officers of high-tech companies (which since 2023 equates predominantly to AI-focused companies). Demand for AI-based products and services appears to be almost insatiable and new products gain headlines and have strong pre-sales and sales figures at least on release date. This focus on AI breakthroughs (but also novelties) is corroborated by the rush of investors to procure AI-related stock, reacting strongly to each new product/service announcement. Even the replacement of key staff makes global headlines. Business news of main media providers are dominated by the developments in AI on an almost daily schedule."
https://arxiv.org/html/2411.03322v1,Satellite monitoring uncovers progress but large disparities in doubling crop yields,"High-resolution satellite-based crop yield mapping offers enormous promise for monitoring progress towards the SDGs. Across 15,000 villages in Rwanda we uncover areas that are on and off track to double productivity by 2030. This machine learning enabled analysis is used to design spatially explicit productivity targets that, if met, would simultaneously ensure national goals without leaving anyone behind.","1 Main The Sustainable Development Goal 2 (SDG 2) sets out to end hunger, achieve food security, improve nutrition, and promote sustainable agriculture by 2030 while promising to “leave no one behind” [1]. The importance of smallholders in meeting this goal is prescribed in SDG target 2.3 and the objective to double their agricultural productivity. Despite the existence of proposed indicators, this target has been notoriously difficult to monitor, and notwithstanding ongoing complementary initiatives to close those data gaps, including the 50x2030 initiative [2], large data gaps persist temporally and spatially [3]. Furthermore, even when large area estimates of national progress exist, local disparities make it difficult to evaluate who is benefiting most from national progress and who may continue to be left behind [4]. This lack of cost-effective and fine monitoring, in turn, makes it difficult for countries to design policies and support programs that can meet SDG 2.3 and close the gap between producers at the same time. Satellite-enabled remote monitoring of agronomic output offers significant opportunity to improve monitoring, and yet while a number of demonstrations exist for predicting crop yields on smallholder farms [5, 6, 7], there remains a fundamental need to advance crop monitoring services in smallholder systems across the world [8], on annual or seasonal basis and at high-resolution. Here, we apply a new spatially explicit time series of maize cover and productivity for Rwanda [9] at requisite spatial (10 m) and temporal (seasonal; within days post-season) resolution to detect trends across 15 thousand villages, performance between them, and contributions to national trends. The geographic and crop specific focus are convenient, but act as a demonstration that could be carried to other nations and crops. We, then, return to the question of what it would take to bring all villages on track in the country alongside alternative policies to achieve equity. Satellite monitoring, which agrees with and extends national survey data [3], shows maize yields in Rwanda are considerably below target yields that would realize a doubling in productivity over 2015-2030 (Fig. 1). Under the status quo, Rwanda is not on track to meet SDG 2.3, in spite of national policies and government efforts to bolster productivity [10]. Figure 1: Rwanda’s national progress towards SDG 2.3 shown through maize yields. The green line demonstrates the linear growth rate required to meet SDG 2.3, the blue line represents national averages published by FAO from 2015-2022 [3], and the orange line is the average yield observed from our high-resolution dataset from 2019-2024 [9]. Furthermore, progress to meeting SDG 2.3 is distributed unequally throughout the country (Fig. 2). Despite demonstrating progress toward roughly one-fifth of the goal nationally, only a small percentage of villages (Table 1) will meet SDG 2.3 if the observed linear rate of growth in the prior four years (2019-2023) continues. We find the Eastern Province, particularly in the north and southeast, and central Western Province are home to the majority of villages achieving notable progress in doubling productivity, with many areas in the central part of the country experiencing stagnant, or even negative, growth rates. Figure 2: Rwandan villages on and off track to meet SDG 2.3. A ratio of 2.0 or higher indicates that maize productivity is projected to double by 2030 based on current growth rates. See Supplementary Figs. 5 and 6 for conservative and optimistic estimates, respectively. We also note large disparities between the lowest and highest yielding producers in the country (Supplementary Fig. 3). Yield gaps between these were declining in the country, but began rising again during the COVID-19 pandemic, increasing inequality. Preliminary 2024 data suggests that this trend is reversing and the productivity gap between villages may be closing again; however, it is a sign of modest progress — currently the average yield among villages in the 90th percentile was roughly 2.4 times that among villages in the 10th percentile (2166 vs. 915 kg/ha). Table 1: National and village-level outcomes towards SDG 2.3 to double productivity for maize under potential scenarios Scenario Natl SDG Progress 2030 (% of goal) Additional years to meet SDG (Natl) Village SDG Progress 2030 (% of villages) Equality 2030 (Ratio) Greatest growth rate after 2024 (kg/ha/year) Sc1: Current 18.6 (9.6 - 23.7) 65.6 (inf - 19.3) 6.4 (12.1 - 3.4) 3.5 (16.6 - 2.3) 422 (191 - 826) Sc2: National SDG 100 0 51.5 (51.1 - 52.6) 1.6 (2.2 - 1.4) 212 (107 - 317) Sc3: Village SDG 100 0 100 2.5 (5.3 - 2) 736 (567 - 1614) Sc4: Equitable 84.1 (110.5 - 75.2) 1.3 (-0.8 - 2) 40.8 (54.8 - 25.6) 1.0 402 (321 - 434) Sc5: Equitable + Natl SDG 100 0 52.0 (50.7 - 52.3) 1.0 440 (305 - 513) 00footnotetext: Mean estimate with 95% prediction interval in parentheses. Scenarios relate to business as usual projections (Sc1) or, alternatively, prioritizing progress nationally (Sc2), by village (Sc3), to achieve parity between producers (Sc4), or to gain both parity and national targets (Sc5) by 2030. To determine progress towards SDG 2.3 we linearly regressed observed maize yields 2019-2023 by village to get a village-specific average growth rate and, then, used this rate to derive an estimate of yield for the SDG baseline year (2015) and end year (2030). National progress was derived from comparing mean projected yield in 2030 to a doubling of the mean yield at baseline. From the average linear growth rate, we also calculated the number of additional years, if any, needed to meet the goal. For village-level progress, we noted the number of villages succeeding in doubling productivity between the years relative to the total number of villages in the country. Equality was indicated by the ratio in mean yield between the highest and lowest producers (i.e. 90th and 10th percentiles, respectively) in 2030. The greatest growth rate was the maximum annual village-level gain in maize yield required to realize the given scenario; thus, other villages exhibited growth rates less than or equal to this number. Under current business as usual projections (Scenario: Sc1; Table 1), Rwanda is expected to increase national average maize yield by 2030 to 18.6% of the goal, but sub-national analysis reveals that only 6.4% of villages will experience a doubling in productivity and inequality between the highest and lowest yielding producers will remain high. Current growth is inadequate to meet SDG 2.3, now, and perhaps soberingly, even within the next 65 years. Continued efforts for targeted interventions may produce different, more favorable outcomes (Table 1 and Supplementary Fig. 4). For example, a policy to uniformly increase yields (Sc2) in every village by 212 kg/ha/year for the next six years would not only meet SDG 2.3 but also improve equality and bring up over half of the villages. To double productivity by 2030 in every village (Sc3), growth rates would need to be as high as 736 kg/ha/year in some villages, but inequalities would remain. Alternatively, an equity-focused approach, or closing the yield gap in the country by increasing every village’s yield to that projected in 2030 for the highest yielding producers (Sc4), would double productivity in 41% of villages and nearly meet SDG 2.3 nationally at much lower annual growth rates. Finally, to achieve equality while also asserting reaching SDG 2.3 (Sc5), the maximum growth rate across villages would need to increase by only 40 kg/ha/year over that designed to achieve equality alone. The reality of meeting these national and local targets will require drastic interventions, with many factors mediating maize productivity, but there are opportunities to accelerate growth (see Section 2.3 for sensitivity analyses and discussion). Achieving SDG 2 is estimated to cost US$33 billion year-over-year, with a significant share (58%) coming from national budgets [11]. However, and perhaps most critically, national statistics agencies that collect data to monitor the SDGs are massively underfunded [12], limiting the availability of key datasets to target and detect policy induced change. Satellite-enabled remote monitoring of agricultural production offers a cost-effective technology for monitoring progress towards SDG 2, and for identifying performance at high spatial resolution. As an example, the pipeline we use here [9] offers open access, remote, wall-to-wall, and near-real time agricultural monitoring for Rwanda for ∼similar-to\sim∼USD$20 per season, monumental benefits that could be scaled to other data sparse countries in sub-Saharan Africa. As such we see much promise for the extension, use, and deployment of these technologies for advancing progress towards SDG 2.3 more widely. It is important to view our results in context. What data exist shows that many countries are not making meaningful progress towards SDG 2.3 — and in particular yields are not rising fast enough in sub-Saharan Africa [13]. The Green Revolution that spurred widespread agriculture development between 1960 and 2000 increased maize yields by 157% in developing countries, but the new technologies were not suited for African agricultural systems and many countries were left behind [14]. We risk repeating the lack of support for marginal environments and populations that gives rise to inequalities and the shortfalls in progress towards the SDGs demonstrates we are already failing. We need targeted investment in research, infrastructure, finance and market development, and policy for these contexts [14] and the means to identify the low productivity systems at risk of being left behind. Reaching equity and national targets for agricultural systems can be complementary goals, but policymakers have a number of synergies and trade-offs to navigate. First, targeting uniform yield growth across the country, which may be preferred due to ease of enrollment and monitoring, addresses inequality implicitly because all villages, including marginal environments, are treated, but it also preserves the baseline disparity between villages. Second, targeting reductions in inequality alone and directing resources to low and moderate yield producing villages contributes to national progress and has companion benefits: the return on investment of high yielding crop varieties is much greater for low potential areas [15] and gains in smallholder yields generates greater economic growth and poverty reduction than investments in other sectors or on large farms [14, 16]. Third, continuing to invest in high yield areas, while devoting most resources to marginal environments, keeps prices low and capitalizes on the spillover effects of stronger rural economies and technology and infrastructure development [14], but can neglect a large number of farmers in the country. Significant effort is required to develop appropriate agricultural policies. A key constraint to improving yields is the lack of financing, institutions, and an enabling environment [17], factors that have been slow to materialize for many smallholders in sub-Saharan Africa. Researchers can, however, assist in this task by leveraging technologies to help policymakers decide which policies, interventions, and programs may realize the growth rates required and set targets that are needed to realize goals of national and local government. The monitoring system based on high- spatial and temporal resolution data presented here may provide a solid foundation for evidence-based decision making, with future focus being on monitoring other staples and ”orphan” crops [14] as diversification and localization of nutrition and food security become a complementary and much needed focus of agricultural policies. With this approach, we are hopeful that continued interplay between scientists, policymakers, and grass roots organizations may assist in identifying, targeting, and helping to achieve agricultural development goals and targets for all in ways and modes that were previously inconceivable."
https://arxiv.org/html/2411.03782v1,Navigating the landscape of multimodal AI in medicine: a scoping review on technical challenges and clinical applications,"Recent technological advances in healthcare have led to unprecedented growth in patient data quantity and diversity. While artificial intelligence (AI) models have shown promising results in analyzing individual data modalities, there is increasing recognition that models integrating multiple complementary data sources, so-called multimodal AI, could enhance clinical decision-making. This scoping review examines the landscape of deep learning-based multimodal AI applications across the medical domain, analyzing 432 papers published between 2018 and 2024. We provide an extensive overview of multimodal AI development across different medical disciplines, examining various architectural approaches, fusion strategies, and common application areas. Our analysis reveals that multimodal AI models consistently outperform their unimodal counterparts, with an average improvement of 6.2 percentage points in AUC. However, several challenges persist, including cross-departmental coordination, heterogeneous data characteristics, and incomplete datasets. We critically assess the technical and practical challenges in developing multimodal AI systems and discuss potential strategies for their clinical implementation, including a brief overview of commercially available multimodal AI models for clinical decision-making. Additionally, we identify key factors driving multimodal AI development and propose recommendations to accelerate the field’s maturation. This review provides researchers and clinicians with a thorough understanding of the current state, challenges, and future directions of multimodal AI in medicine.","The healthcare landscape is evolving rapidly, driven by an increasingly data-centric approach to patient care and decision-making [Shilo et al., 2020]. This shift is complemented by the advent of technologies such as digital pathology [Niazi et al., 2019], biosensors [Sempionatto et al., 2022], and next-generation sequencing [Steyaert et al., 2023], which provide clinicians with novel insights in various domains. The data generated by these diverse modalities is generally complementary, with each modality contributing unique information to the status of a patient. Some modalities offer a comprehensive overview at the macro level, while others may provide detailed information at single-cell resolution [Steyaert et al., 2023]. In addition to this recent growth in data quantity, there is a concurrent increase in the quality and diversity of available treatment options. Hence, selecting the optimal treatment has become increasingly complex, and a further data-centric approach to treatment selection may be required. The traditional approach to integrating information from different data modalities into a single decision is represented by multidisciplinary boards, where each specialized clinician offers their perspective on a given modality or piece of information in pursuit of consensus [Mano et al., 2022]. Although establishing these boards has improved disease assessments and patient management plans [Mano et al., 2022], there is a foreseeable limit to the scalability of these boards. If data quantity and diversity continue to rise, many domain experts will be required to integrate these different information streams effectively. Fortunately, another technological advancement that is gaining a foothold in healthcare is artificial intelligence (AI). Although the vast majority of published work focuses on single modality applications of AI, several authors have highlighted the potential of AI systems to combine multiple streams of information, so-called multimodal AI, for decision-making [Steyaert et al., 2023, Acosta et al., 2022, Lipkova et al., 2022]. These multimodal AI models are trained to process different streams of multimodal data effectively, leverage the complementary nature of information, and make an informed prediction based on a broader context of the patient’s status. However, despite these promising results, studies investigating multimodal AI models are comparatively scarce, and the development of unimodal models remains the de facto standard. This lagging development of multimodal AI models can be attributed to several challenges. First, a practical challenge can be found in the cross-departmental nature of multimodal AI development. As different data modalities may originate from various medical departments, consulting different medical domain experts will likely be required for effective data integration. In addition, medical departments may have varying experience in data storage, retrieval, and processing, limiting the possibilities of multimodal AI development. For example, if a radiology department has a fully digital workflow while the corresponding pathology department does not, this effectively prohibits multimodal AI endeavors where whole slide images would be combined with radiological imaging data. Different data modalities can have vastly different characteristics, such as dimensionality or color space, which generally requires different AI model architectures tailored towards those modalities, increasing model design complexity. For example, convolutional neural networks (CNN) were initially proposed for structured data, such as 2D and 3D images, but can’t straightforwardly be applied to unstructured data. Conversely, transformers are solid, flexible encoders for various data modalities. Still, whether a one-size-fits-all architecture can capture various medical data modalities effectively remains unclear. In practice, multimodal data integration is commonly achieved using different (intermediate) model outputs. Training multiple domain-specific AI models (i.e., encoders) and efficiently integrating these in a single prediction poses a challenge unique to multimodal AI development. Last, the inconsistent availability of all modalities for each patient within a multimodal dataset adds complexity. Patients with different disease trajectories will have various available modalities, leading to partially incomplete datasets. This can substantially reduce the adequate training dataset size for AI models that require complete multimodal data to generate predictions. Moreover, these issues also translate to implementation. If modalities are missing, it may be unclear how this impacts the model’s performance from the perspective of fewer available data to base a decision on and the potential introduction of population selection bias [Acosta et al., 2022]. In short, developing multimodal AI models poses several novel challenges compared to unimodal AI development. Even given these challenges, several works have been done in the past on multimodal AI applications, typically involving handcrafted features. A key issue with these approaches was that the difficulties requiring particular domain expertise are multiplied, as expert clinicians would also need to be involved in the feature design phase [Vaidya et al., 2020, Tortora et al., 2023]. An excellent overview was published by Kline et al. [2022], indicating that these models obtained a 6.4% mean improvement in AUC compared to their unimodal counterparts. Recent years have shown an accelerated interest in multimodal AI development for medical tasks [Salvi et al., 2024], as using unsupervised learning and deep neural networks as encoders has significantly simplified the feature extraction step. In this review, we comprehensively summarize the state-of-the-art in multimodal AI development for medical tasks and investigate to what extent multimodal data integration is living up to its purported benefits. Unlike previous reviews that have focused on specific diseases, prediction tasks, or modality combinations [Acosta et al., 2022, Salvi et al., 2024, Krones et al., 2025], our analysis encompasses the full spectrum of the medical domain. Specifically, our review aims to shed light on I) the progress of multimodal AI model development across different medical disciplines and tasks, II) the technical challenges inherent in multimodal AI development, including model architectures, fusion methods, and the handling of missing data, III) the foreseeable road to the clinic of multimodal AI models, addressing aspects such as regulatory approval and explainability, and IV) the factors driving multimodal AI development and potential strategies to promote further maturation of this field. Lastly, we will provide an outlook on future perspectives in multimodal AI development based on our careful analysis of 432 papers published over the past six years (2018-2024)."
https://arxiv.org/html/2411.03769v1,"No Culture Left Behind: ArtELingo-28,a Benchmark of WikiArt with Captions in 28 Languages","Research in vision and language has made considerable progress thanks to benchmarks such as COCO. COCO captions focused on unambiguous facts in English; ArtEmis introduced subjective emotions and ArtELingo introduced some multilinguality (Chinese and Arabic). However we believe there should be more multilinguality. Hence, we present ArtELingo-28, a vision-language benchmark that spans 28 languages and encompasses approximately 200,000 annotations (140 annotations per image). Traditionally, vision research focused on unambiguous class labels, whereas ArtELingo-28 emphasizes diversity of opinions over languages and cultures. The challenge is to build machine learning systems that assign emotional captions to images. Baseline results will be presented for three novel conditions: Zero-Shot, Few-Shot and One-vs-All Zero-Shot. We find that cross-lingual transfer is more successful for culturally-related languages. Data and code will be made publicly available.","A quick review of recent surveys on multimodal AI Cao et al. (2023); Berrios et al. (2023); Zhang et al. (2023), reveals just how much the literature is focused on English. The literature on benchmarking Liu et al. (2023c); Li et al. (2023a) provides an astoundingly similar story. With the pervasiveness of AI technology in our societies, it is essential to make the technology accessible to a wider population. Although English is widely spoken as a first language or a second language, most of the world (75% per capita) does not speak English.111https://www.cochrane.org/news/cochrane-evidence-different-languages No Culture Left Behind: ArtELingo-28, a Benchmark of WikiArt with Captions in 28 Languages shows some annotations from ArtELingo-28. For 2000 images from WikiArt, we have ∼similar-to\sim∼140 emotion labels per image, as well as captions from annotators with diverse backgrounds covering 28 languages. Unlike captions in traditional benchmarks such as COCO Lin et al. (2014) and Visual Genome Krishna et al. (2017) which emphasize unambiguous class labels, the captions in No Culture Left Behind: ArtELingo-28, a Benchmark of WikiArt with Captions in 28 Languages emphasize subjective opinions over objective facts, and diversity over languages and cultures. No Culture Left Behind: ArtELingo-28, a Benchmark of WikiArt with Captions in 28 Languages shows 5 annotations from 5 languages for 4 WikiArt images. Compare, for example, the captions for the first image in Burmese, Malay, Korean and Setswana. There are differences of opinion in both labels and captions: emotion labels: disgust (Burmese), awe (Malay) captions: focus on chest (Burmese & Malay); focus on face and hair (Korean & Setswana) To advance the field beyond objective facts and unambiguous class labels, it is critical to embrace diversity and subjective differences of opinion. Traditionally, vision research has focused on classifying objects in the image in an objective way, but we prefer to view art as a form of communication between the artist and the audience, where there is more room for subjectivity and diversity. Communication depends on much more than just the pixels in the image such as the cultural backgrounds of the participants.222Blog: Who created the saying that beauty is in the eye of the beholder? To add 25 new languages to ArtELingo-28 required considerable effort. Amazon Mechanical Turk works well for a few languages, but less so for many of the 25 languages. ArtELingo-28 consumed 6.25K hours of work, performed by 220 annotators from 23 countries. Compared to ArtELingo which added just 3 languages, our dataset required significantly more management and coordination; ArtELingo-28 was managed by a team of 32 coordinators who contributed more than 2.5K hours. To cover many practical situations, we utilize ArtELingo-28 to build 3 evaluation setups: Zero-Shot, Few-Shot, and One-vs-All Zero-Shot. The main task evaluates the performance of the generation of affective explanations. In the Zero-Shot setup, we train a model on a large-scale training dataset in a few high-resource languages. We then evaluate that model on languages that do not appear in the the training data. The Few-Shot setup addresses the situation where we have a few training examples in low-resource languages, in addition to the large-scale training dataset from the Zero-Shot setup. We fine-tune the models from the Zero-Shot setup on the few-shot low-resource data and then evaluate them on the rest of the samples. Finally, in the One-vs-All Zero-Shot setup, we have the large-scale training dataset as well as small-scale data in one language (One). After fine-tuning, we evaluate on the Unseen languages (All). This setup is designed to shed light on pairwise interactions between languages, highlighting cultural effects. We observe clusters (cultural groups) forming from our trained models. These groups go beyond writing systems (scripts), capturing cultural connections between languages. Additionally, we observe that the multilingual setup is challenging for vision and language models, partly because of the massive vocabulary. We address this challenge by utilizing pretrained multilingual LLMs such as BLOOMZ. In short, our contributions are: • We collected 200K emotion labels and affective textual explanations in 25 languages on 2000 images (with ∼similar-to\sim∼140 annotations/image). • We proposed a benchmark to evaluate the Zero-Shot, Few-Shot, and One-vs-All Zero-Shot performance of Multimodal models. • We adapt and benchmark four contemporary Vision and Language models to work on our multilingual setup. • Finally, we study pairwise language transfer revealing insights on cultural differences in emotional perception and expression."
https://arxiv.org/html/2411.03654v1,PyroGuardian: An IoT-Enabled System for Health and Location Monitoring in High-Risk Firefighting Environments,"First responders risk their lives to reduce property damage and prevent injuries during disasters. Among first responders, firefighters work with fires in residential properties, forests, or other locations where fire occurs. We built the PyroGuardian system that uses wearable modules to transmit unit information over Long Range (LoRa) to an Android tablet. The tablet runs our application, PyroPortal, to assign each firefighter’s stats, such as body temperature, heart rate, and GPS location. PyroPortal displays this information on unit dashboards, and markers on Google Maps represent the firefighter’s location and the direction they are facing. These dashboards can help the incident commander (IC) make more informed decisions on mission control operations and remove specific units whose health stats, such as oximeter and pulse, passed certain thresholds. PyroGuardian completes all these tasks at an affordable cost and in an impressive maximum range between the units and IC. In addition, PyroGuardian has various application scenarios, such as law enforcement and military operations, besides firefighting. We also conducted a sample mission inside a burning building while real firefighters watched. After the demonstration, they completed a survey on system usability and PyroGuardian’s potential to meet their requirements.","First responders operate immediately after a disaster, such as earthquakes, floods, nuclear leakages, fires, and explosions (Girma et al., 2020). Their goal is quickly reaching the disaster point to save lives and reduce property damage (Girma et al., 2020). Significant incidents, such as the terrorist attacks of September 11, 2001, the anthrax attacks of 2001, and the response and recovery efforts of the 2004 Southeast Asia tsunami, have emphasized the role of first responders (Benedek et al., 2007). However, they are not always safe and can suffer severe duty-related consequences. Among first responders, firefighters frequently suffer injuries, as the NFPA 2015 National Fire Experience Survey from fire departments indicated that 68,085 firefighter injuries occurred in the line of duty in 2015 in the US (Haynes and Molis, 2015). Another study found that in 2019, 48 firefighters died while on duty in the US (Fahy et al., 2020). The US Fire Department stated there is a fire in a residential area every 85 seconds (Shokouhi et al., 2019). These facts make firefighters’ efficiency and safety a crucial matter for the public as firefighting is one of the most life-threatening, emotionally traumatic, and stressful occupations (Meina et al., 2020). There are several factors in fire scenes, such as smoke and noise. A firefighter is deafened when they walk inside a building. They cannot see well, and radio communications are challenging (mic, 2021). Thus, an IC will not know their units’ well-being. This lack of communication makes the job more difficult and dangerous. A system is needed to preserve communication between the IC and units. ICs should monitor their critical information and location to interfere in time before any injury. Relying on technological equipment can solve this problem (Yizhe, 2021; Technology in the Fire Service, 2020). Existing solutions tackle this field with techniques like monitoring units’ vitals or tracking their outdoor location. Some issues with existing solutions include the lack of GPS, preventing outdoor localization for wildfires, or vital health information monitoring. We have not found a solution that combines all these tasks into a unified dashboard for the IC. In addition, we have not found a solution that aims to keep the IC behind the safety line while allowing real-time data streaming. To fill this gap and develop a solution while considering pricing and user convenience, we created PyroGuardian: an IoT-powered framework for firefighter mission control. PyroGuardian will warn the IC when firefighters’ vitals or environment variables are dangerous. It can also stream real-time unit location and health data to the IC with a range of 610 meters. The PyroGuardian consists of a tablet, an Android application named PyroPortal, an external USB LoRa adapter for PyroPortal, PyroStrap, and PyroHelm. The IC will use PyroPortal, connected to the LoRa adapter. Each firefighter will equip one PyroHelm and PyroStrap. The PyroHelm’s casing is attachable to the exterior of the standard firefighter gas masks and helmets. The PyroHelm broadcasts sensor data, such as GPS, temperature, and 3-D inertial information, to the PyroPortal via the wireless protocol, LoRa. We used an external LoRa adapter as tablets do not have built-in LoRa. PyroPortal will receive additional sensor data from the PyroStrap via LoRa broadcasting. The tablet has a transceiver LoRa adapter connected to its USB-C port. Once the LoRa adapter receives the sensor data, it inputs the data into the PyroPortal, displaying each unit’s location and vital health information on its map. The IC can then interpret this data to navigate his officers and remove endangered units from the scene. Another contribution of this paper includes creating a real-world fire scenario in the Illinois Fire Service Institute and observing PyroGuardian’s efficiency during the mission. We invited 34 firefighters to watch this mission and provide feedback on our surveys. We evaluate PyroGuardian’s system usability factor to ensure it is convenient for tradition-focused fire departments. Furthermore, the survey also asked them if PyroGuardian meets their needs. Finally, we will analyze the feasibility of our solution in other first-responder fields. Our contributions to this field include: • A novel IoT-powered solution for firefighting mission control that is cheap and easy to use. • A unified dashboard that monitors the unit location and health together • A user study from fire department personnel. • An analysis of extending our framework to other first responder departments, such as law enforcement. Section two gives background regarding some wireless tools and sensors integrated into PyroGuardian. The following section describes a high-level overview of PyroGuardian’s model, workflow, and configuration. The “Implementation” section elaborates on PyroGuardian’s sensors and breakout boards. It will explain in detail the communication between each component. In addition, “Evaluation” presents our results, a comprehensive user study, and PyroGuardian’s performance based on our metrics, such as cost. Section six, Discussion, interprets our results and extends PyroGuardian to a broader scope. Finally, the conclusion states the future trends and ends the paper with our final remarks."
https://arxiv.org/html/2411.03402v1,Climate AI for Corporate Decarbonization Metrics Extraction,"Corporate Greenhouse Gas (GHG) emission targets are important metrics in sustainable investing (Dahlmann et al., 2019; Krabbe et al., 2015). To provide a comprehensive view of company emission objectives, we propose an approach to source these metrics from company public disclosures. Without automation, curating these metrics manually is a labor-intensive process that requires combing through lengthy corporate sustainability disclosures that often do not follow a standard format. Furthermore, the resulting dataset needs to be validated thoroughly by Subject Matter Experts (SMEs), further lengthening the time-to-market. We introduce the Climate Artificial Intelligence for Corporate Decarbonization Metrics Extraction (CAI) model and pipeline, a novel approach utilizing Large Language Models (LLMs) to extract and validate linked metrics from corporate disclosures. We demonstrate that the process improves data collection efficiency and accuracy by automating data curation, validation, and metric scoring from public corporate disclosures. We further show that our results are agnostic to the choice of LLMs. This framework can be applied broadly to information extraction from textual data.","1.1. Background To gain a comprehensive view of corporate emission targets, which are crucial for assessing a corporation’s trajectory towards decarbonization, researchers typically turn to third-party aggregators, including the Science Based Targets initiative (SBTi) (SBT, 2015) and the Carbon Disclosure Project (CDP) (CDP, 2000). However, from our analysis, sizable percentage of companies (more than 30% of the top 1,000 firms ranked by market capitalization across all marketable securities from developed, emerging, and frontier markets) do not disclose commitments data in a structured format through these third-party organizations. To help bridge this gap, we propose an information extraction framework to collect these metrics from company disclosures. Information extraction from unstructured data is a well-known challenge in Natural Language Processing (NLP) (Martinez-Rodriguez et al., 2020; Wu et al., 2024; Abdullah et al., 2023). Several areas of research have attempted to address this problem with respect to key-value metrics extraction highlighting limitations in the scalability and accuracy of the extraction models. (Seitl et al., 2024; Adnan and Akbar, 2019; Liu et al., 2023). Information extraction in the context of corporate disclosure of climate risk metrics and objectives is challenging in particular, given a fast-evolving domain and heterogenous language. Several previous studies have explored this and related topics. In ChatClimate (Leippold, 2023) and ClimateQ&A (Calzada et al., 2024), chatbots trained on IPCC (Intergovernmental Panel on Climate Change) documentation, utilized a RAG (Retrieval Augmented Generation) framework to identify relevant documents for a specific query. ChatClimate’s developers also created ClimateBERT, a climate-focused classifier derived from a distilled RoBERTa model, trained for climate language detection, sentiment analysis, and fact checking (Webersinke et al., 2022). Additionally, MSCI/GARI (Baatarchuluu and Ashfaq, 2023) and Bank for International Settlements (BIS, 2024) studies employed LLMs to extract climate and sustainability metrics from texts using a RAG framework for precise text retrieval. While relevant, these studies are proof-of-concepts and do not produce production-ready results with high useable accuracies and stringent validation. We contribute to the information extraction literature by demonstrating the effectiveness of a combination of techniques, including a fine-tuned RoBERTa model for enhanced text classification accuracy (instead of RAG), dynamic prompting, and domain-specific metric validation. We show that these techniques together achieve production-quality information extraction performance for the task of retrieving corporate emission targets from corporate disclosures. In this paper, we also assess more traditional extraction methods like regular expression (RegEx) and BERT-based Question Answering, noting their cost-effectiveness but limited scalability. For example, Regex cannot comprehensively model and extract metrics of interest, and BERT struggles with multi-span extraction, hindering the capture of complete metric entities. 1.2. Model Overview Figure 1. CAI model and pipeline end to end workflow. In order to collect corporate carbon reduction commitments accurately, the CAI model utilizes 4 stages: context processing and chunking, relevant text search, metric extraction, and validation. All stages use NLP, and with the metric extraction step leveraging Large Language Models (LLM) and Generative AI (Gen AI). Stage 1 converts documents to text and breaks this text into smaller, consumable chunks for the pipeline. In the second phase of the process, a more advanced variant of the Bidirectional Encoder Representation Transformer (BERT), known as RoBERTa (Robustly Optimized BERT-Pretraining Approach), is employed. RoBERTa is built on top of BERT and has enhanced pretraining, such as larger training batches and dynamic word masking. RoBERTa, an encoder transformer-based language model, is tasked with the classification of text segments (Liu et al., 2019). Specifically, it discerns whether these segments are relevant or not to the subject of carbon reduction commitments (Schimanski et al., 2023). This model enables a more nuanced understanding and categorization of the textual data, thereby enhancing the overall process by creating a search mechanism that improves context recall and the number of contexts to pass into the LLM in the later stage. The model casts a wide net and searches for all references of carbon reduction commitments, either corporate-wide or non-corporate wide, such as at the subsidiary level or country level. This was intentionally done to ensure that relevant texts were captured. Stage 3 is the metrics extraction layer which leverages an LLM, which gets fed in relevant blocks of text from stage 2 and extracts structured metrics from the input given a bespoke prompt. In the final stage, this output is then post-processed by transforming, validating and deduplicating data points by company. The end-to-end workflow of the model is depicted in Figure 1. 1.3. Data Public corporate sustainability and annual reports are used as inputs to the process. Corporate sustainability reports encapsulate initiatives that a firm is planning or has accomplished concerning sustainability and climate change. These documents typically encompass corporate commitments. However, a significant challenge arises as many firms do not publish sustainability reports regularly or at all. In such instances, annual reports are utilized for extraction. These documents usually contain a section on sustainability and corporate targets, while the remaining information is disregarded for the purpose of this model. Each report is stored and tagged with metadata, including the corporate company identifier, name, report type (either annual or sustainability), and report publication year. Each input is converted from PDF to text and segmented into chunks for ingestion into the model."
https://arxiv.org/html/2411.03327v1,"Maximal Extractable Value in Decentralized Finance: Taxonomy, Detection, and Mitigation","Decentralized Finance (DeFi) leverages blockchain-enabled smart contracts to deliver automated and trustless financial services without the need for intermediaries. However, the public visibility of financial transactions on the blockchain can be exploited, as participants can reorder, insert, or remove transactions to extract value, often at the expense of others. This extracted value is known as the Maximal Extractable Value (MEV). MEV causes financial losses and consensus instability, disrupting the security, efficiency, and decentralization goals of the DeFi ecosystem. Therefore, it is crucial to analyze, detect, and mitigate MEV to safeguard DeFi. Our comprehensive survey offers a holistic view of the MEV landscape in the DeFi ecosystem. We present an in-depth understanding of MEV through a novel taxonomy of MEV transactions supported by real transaction examples. We perform a critical comparative analysis of various MEV detection approaches, evaluating their effectiveness in identifying different transaction types. Furthermore, we assess different categories of MEV mitigation strategies and discuss their limitations. We identify the challenges of current mitigation and detection approaches and discuss potential solutions. This survey provides valuable insights for researchers, developers, stakeholders, and policymakers, helping to curb and democratize MEV for a more secure and efficient DeFi ecosystem.","Decentralized Finance (DeFi) [1], powered by blockchain technology, offers financial services, such as investing, lending loans, and trading assets, to various stakeholders without the need for intermediary brokers. This creates a decentralized, secure, transparent, and traceable financial ecosystem. According to a Skyquest report, the DeFi market size is expected to reach 48.02 billion USD by 2031, up from 23.99 billion USD in 2023, a compound annual growth rate of 9.06%111https://www.skyquestt.com/report/decentralized-finance-market, accessed on 19 September 2024. Furthermore, as of September 2022, the total value locked in the DeFi ecosystem exceeded 82 billion USD222https://defillama.com/, accessed on 19 September 2024, with the Ethereum network accounting more than 45 billion USD (approximately 56%). Decentralized Exchanges (DEXes) are one of the prominent applications of DeFi, enabling users to directly swap tokens using smart contracts [2]. Most DEXes operate on the Ethereum blockchain to facilitate trustless and automated transactions. However, in the Ethereum network, transactions are stored in a public mempool before they are included in a block. This creates profitable opportunities for network participants, referred to as searchers, who can submit new transactions by observing pending financial transactions to gain additional revenue. These searchers maximize their profits by manipulating gas prices to influence the order of their transactions within the block. This additional value extracted from the blockchain network is termed Maximal Extractable Value (MEV). MEV searchers can be the block producers (i.e., miners in Proof of Work (PoW) and validators in Proof of Stake (PoS)), other network participants, or a bot. However, block producers have a unique advantage, as they can include transactions in blocks without paying high gas prices [3]. Furthermore, the transition from PoW to PoS in Ethereum has significantly reduced block producer rewards, further luring block producers to engage in MEV activities [4]. MEV could result in major financial losses, network congestion, increased gas prices, and blockchain inefficiency [3, 5]. Before Ethereum’s transition to PoS in September 2022, around 440,000 ETH in MEV was extracted. Since the transition, approximately 180,000 ETH has been extracted up until May 2023333https://milkroad.com/guide/mev/, accessed on 19 September 2024. However, not all MEV transactions are detrimental to the DeFi ecosystem. Some can destabilize consensus mechanisms and cause economic harm, while others may even stabilize financial markets [6]. Therefore, understanding, detecting, and mitigating MEV is critical to ensure the efficiency and stability of the DeFi ecosystem. Several surveys have explored various aspects of MEV, including transaction types and mitigation strategies [7, 8, 6, 5, 9]. In particular, [7] classifies MEV transactions, while [8, 6] emphasize on mitigation techniques. In contrast, [5, 9] cover both transaction types and mitigation strategies. However, none have thoroughly examined all transaction types, detection approaches, and mitigation strategies. This comprehensive survey addresses this gap by presenting a novel taxonomy of MEV transaction types supported by real-world examples from the Ethereum network. It also explores MEV detection approaches and mitigation strategies and examines various simulation and extraction methods. The main contributions of this survey are as follows. • We introduce a novel and comprehensive taxonomy of MEV transactions, supported by real Ethereum transaction examples for each identified MEV type. The taxonomy clearly distinguishes between value-diverting MEV transactions, which can lead to financial loss and network instability, and value-creating MEV transactions, which can stabilize markets or enhance efficiency. This distinction provides clarity on the dual nature of MEV activities and lays the foundation for discussions on mitigation and detection strategies. • We present a critical comparative analysis of the various MEV detection approaches proposed for different types of MEV transactions, highlighting their effectiveness in identifying these transaction types. • We provide an in-depth analysis of various MEV mitigation strategies, such as transaction ordering solutions, privacy-preserving public pools, and private pools. By identifying the specific limitations of these strategies, we offer a critical assessment of their real-world applicability and potential areas for improvement. • We explore existing MEV simulation frameworks and extraction methods, offering a comprehensive overview of how these tools model and replicate real-world MEV scenarios. • We critically examine the major challenges in MEV mitigation and detection, such as centralization, latency, layer-2 MEV, and multi-address MEV. Furthermore, we discuss potential solutions paving the way for a more secure and efficient DeFi system. The remainder of this article is organized as follows. Section 2 provides essential background on Ethereum, DEXes, and MEV, enabling readers to gain a clearer understanding of the key concepts. Section 3 reviews existing related surveys on MEV, highlighting gaps in the literature. In Section 4, we describe the methodology used to conduct this survey. Section 5 introduces and synthesizes the proposed taxonomy of MEV transactions. Sections 6 and 7 focus on MEV detection approaches and mitigation strategies, respectively. Section 8 explores MEV simulation and extraction methods. Section 9 addresses the challenges associated with MEV mitigation and detection, along with potential solutions. Finally, Section 10 concludes the survey."
https://arxiv.org/html/2406.10602v1,Multilingual Large Language Models and Curse of Multilinguality,"Multilingual Large Language Models (LLMs) have gained large popularity among Natural Language Processing (NLP) researchers and practitioners. These models, trained on huge datasets, show proficiency across various languages and demonstrate effectiveness in numerous downstream tasks. This paper navigates the landscape of multilingual LLMs, providing an introductory overview of their technical aspects. It explains underlying architectures, objective functions, pre-training data sources, and tokenization methods. This work explores the unique features of different model types: encoder-only (mBERT, XLM-R), decoder-only (XGLM, PALM, BLOOM, GPT-3), and encoder-decoder models (mT5, mBART). Additionally, it addresses one of the significant limitations of multilingual LLMs - the curse of multilinguality - and discusses current attempts to overcome it.","Large Language Models (LLMs) Devlin et al. (2019); Lewis et al. (2020); Liu et al. (2019) have made a significant impact on the field of Natural Language Processing (NLP), showing effectiveness in various tasks. The remarkable aspect of LLMs is their capacity to learn a language during pre-training and enhance their expertise for specific tasks during fine-tuning. Pre-training involves the acquisition of knowledge, where a model grasps language structures by analyzing huge datasets. Fine-tuning, on the other hand, specializes the model by adjusting its parameters so that it can perform specific downstream tasks using a smaller set of examples compared to those used in pre-training. Another significant advancement involves teaching a model to comprehend multiple languages, leading to the concept of multilingual LLMs Devlin et al. (2018); Pires et al. (2019); Conneau et al. (2020); Xue et al. (2021); Liu et al. (2020). While monolingual LLMs focus on understanding patterns within a single language, multilingual LLMs simultaneously learn from multiple languages. This is accomplished by exposing these models to data from various languages during the pre-training phase. Furthermore, variations in the architectures of multilingual LLMs contribute to their strengths in certain tasks while potentially limiting their effectiveness in others. This paper aims to provide a brief overview of the architectures of the most prominent multilingual LLMs, including details such as their pre-training objective functions, data sources, tokenization schemas, the number of languages supported, and the peculiarities of each individual multilingual LLM. Subsequently, the primary challenge facing multilingual LLMs, known as the ""curse of multilinguality"" Conneau et al. (2020), and the current attempts to solve it, are discussed. Model Architecture Training Data Sources Languages Tokenization Schema mBERT Encoder Wikipedia 104 WordPiece XLM-R Encoder CC 100 SentencePiece mBART Encoder-Decoder CC25 25 SentencePiece mT5 Encoder-Decoder C4 101 SentencePiece XGLM Decoder CC100-XL 134 SentencePiece PALM Decoder Wikipedia, books, webpages, social media & source code 124 SentencePiece BLOOM Decoder ROOTS, OSCAR 46 Byte Pair Encoding GPT-3 Decoder CC, Wikipedia, WebText2, Books1&2 >95 Byte Pair Encoding Table 1: Comparison of Multilingual Large Language Models"
https://arxiv.org/html/2411.02419v1,XAI-FUNGI: Dataset resulting from the user study on comprehensibility of explainable AI algorithms,"This paper introduces a dataset that is the result of a user study on the comprehensibility of explainable artificial intelligence (XAI) algorithms. The study participants were recruited from 149 candidates to form three groups representing experts in the domain of mycology (DE), students with a data science and visualization background (IT) and students from social sciences and humanities (SSH). The main part of the dataset contains 39 transcripts of interviews during which participants were asked to complete a series of tasks and questions related to the interpretation of explanations of decisions of a machine learning model trained to distinguish between edible and inedible mushrooms. The transcripts were complemented with additional data that includes visualizations of explanations presented to the user, results from thematic analysis, recommendations of improvements of explanations provided by the participants, and the initial survey results that allow to determine the domain knowledge of the participant and data analysis literacy. The transcripts were manually tagged to allow for automatic matching between the text and other data related to particular fragments. In the advent of the area of rapid development of XAI techniques, the need for a multidisciplinary qualitative evaluation of explainability is one of the emerging topics in the community. Our dataset allows not only to reproduce the study we conducted, but also to open a wide range of possibilities for the analysis of the material we gathered.","1 Background & Summary With the rapid development of black-box machine learning (ML) models, such as deep neural networks or gradient boosting trees, the need for explanations of their decisions has emerged. This demand has been driven by the increasing implementation of opaque models, in high-risk and critical areas like medicine, healthcare, industry, and law, which laid the foundation for modern research on explainable and interpretable artificial intelligence (XAI). Scientists’ efforts in designing XAI algorithms have been further supported by political initiatives such as DARPA’s XAI challenge [1], the European Union’s GDPR [2], and more recently, the EU AI Act [3]. The shared goal of all these initiatives is to improve the transparency of AI systems, thereby promoting their adoption in areas where trust in AI is not fully established or where the transparency of decisions is crucial for legal and safety reasons. However, as XAI algorithms have been advanced, a new discussion has been initiated, addressing the fundamental challenge of ensuring that the explanations generated by these algorithms are comprehensible to humans. This triggered research on the evaluation of XAI [4], drawing attention from social sciences, which argued that much of the effort in XAI relies solely on researchers’ intuition about what constitutes a good explanation. They emphasized that human factors should be integral to the design and evaluation of XAI to ensure its reliability [5]. Recognizing individual human abilities to comprehend algorithmically generated explanations is crucial, as these abilities can vary significantly based on personal information competencies. Additionally, there is a lack of established multidisciplinary methods for measuring these capabilities, as well as datasets that facilitate reproducible evaluations or comprehensive analyses. Our contribution addresses these gaps by presenting a dataset resulting from extensive user study on the comprehensibility of XAI algorithms across three distinct user groups with different information competencies. The dataset contains material collected from 39 participants during the interviews conducted by the information sciences research group. The participants were recruited from 149 candidates to form three groups that represented domain experts in the field of mycology (DE), students with a data science and visualization background (IT) and students from social sciences and humanities (SSH). Each group was provided with a set of explanations from a machine learning model trained to predict edible and non-edible mushrooms. The data used to train the ML model was in tabular format and is publicly available [6]. The machine learning model we used was Extreme Gradient Boosting (XGB) and explanations for its decision were prepared including state of the art model-agnostic algorithms such as SHAP, LIME, DICE and Anchor [7]. During the interviews, participants were asked to interpret the presented explanations and answer related questions. The interviews were conducted according to the think-aloud protocol, a technique in which participants verbalize their thoughts and reactions as they interact with stimuli or perform tasks. This approach provides insights into participants’ cognitive processes, decision-making, and understanding of the subject matter in real-time. The transcripts gathered with this methodology capture not only the final responses of the participants but also their reasoning, uncertainties, and moments of clarity during the evaluation of XAI algorithms. The overview of the study and resulting dataset is given in Figure 1. Figure 1: Schema of a study, data collection and data format. The resulting dataset combines and summarizes the whole study. It was constructed from the self-assessment surveys obtained from the candidates, anonymized transcripts of the interviews, the results of thematic analysis of the interviews, and original explanations accompanied modifications suggested by the participants. The transcripts were manually tagged, allowing researchers to correlate specific text fragments with visual explanations, thematic analysis results, and user recommendations on desired process of explanation. We believe this dataset will significantly advance the field of XAI by providing the necessary tools to explore and improve the societal and comprehensibility aspects of AI systems. It will be a valuable resource for researchers aiming to develop more transparent and understandable AI technologies and foster research on the human-centered evaluation of XAI systems. The dataset is complemented with the source code, allowing one to reproduce the initial ML model and explanations."
https://arxiv.org/html/2411.02414v1,Fairness Evaluation with Item Response Theory,"Item Response Theory (IRT) has been widely used in educational psychometrics to assess student ability, as well as the difficulty and discrimination of test questions. In this context, discrimination specifically refers to how effectively a question distinguishes between students of different ability levels, and it does not carry any connotation related to fairness. In recent years, IRT has been successfully used to evaluate the predictive performance of Machine Learning (ML) models, but this paper marks its first application in fairness evaluation. In this paper, we propose a novel Fair-IRT framework to evaluate a set of predictive models on a set of individuals, while simultaneously eliciting specific parameters, namely, the ability to make fair predictions (a feature of predictive models), as well as the discrimination and difficulty of individuals that affect the prediction results. Furthermore, we conduct a series of experiments to comprehensively understand the implications of these parameters for fairness evaluation. Detailed explanations for item characteristic curves (ICCs) are provided for particular individuals. We propose the flatness of ICCs to disentangle the unfairness between individuals and predictive models. The experiments demonstrate the effectiveness of this framework as a fairness evaluation tool. Two real-world case studies illustrate its potential application in evaluating fairness in both classification and regression tasks. Our paper aligns well with the Responsible Web track by proposing a Fair-IRT framework to evaluate fairness in ML models, which directly contributes to the development of a more inclusive, equitable, and trustworthy AI.","Figure 1. The general scenario in fairness evaluation. The dashed line denotes the two analysis directions: one for individuals and another for predictive models. \Description Item Response Theory (IRT) is a framework that originated in the mid-20th century and is primarily applied in psychometrics. It aims to characterise both items and respondents through the analysis of responses (Embretson and Reise, 2013; De Ayala, 2013; Hambleton and Swaminathan, 2013). In recent years, IRT has been proposed to evaluate predictive performance in machine learning (ML) models. By considering ML tasks as items and predictive models as respondents, we can reinterpret the ability of a predictive model in terms of the difficulty and discrimination level of the tasks. The most recent research can be categorised by how they treated the “items”. Martínez-Plumed et al. (2019) use IRT to evaluate the predictive performance of ML models on a single classification dataset, treating each instance as an item. They train and test a range of predictive models (i.e., classifiers) on a single dataset and obtain item characteristic curves (ICCs) for each instance. However, the limitation of this framework is its exclusive focus on binary classification tasks and a single dataset. Chen et al. (2019) propose a modified IRT model for continuous responses and apply it to multiple classification tasks. The obtained ICCs are not limited to logistic curves, and differently shaped curves can be generated based on the beta distribution, allowing more flexibility when fitting responses for different items. Furthermore, Kandanaarachchi and Smith-Miles (2023) treat datasets as respondents, thereby characterising the discrimination and difficulty of the predictive model. They then treat the predictive models as items in an inverted IRT model, to generate the ability trait of datasets, i.e., dataset difficulty. All of the above IRT models are used to evaluate the predictive performance of ML models, where the response represents the probability of a correct response for the item based on the respondents’ ability, whether the items are instances or datasets. However, fairness issues have become increasingly important in real-world applications involving people-related decisions. For example, COMPAS, a decision support model that estimates the risk of a defendant becoming a recidivist, is found to predict a higher risk for black people and a lower risk for white people (Brennan et al., [n. d.]). Similarly, Facebook users receive a recommendation prompt when watching a video featuring black people, asking them if they would like to continue watching videos about primates (Mac, 2021). Another example is Mate AI, an image generator that cannot depict an Asian man and a white woman together (Mia, 2024). These incidents indicate that datasets or predictive models may become sources of unfairness, leading to serious social problems. We urgently need a fairness evaluation tool to evaluate both datasets and predictive models. Most research typically reports pairwise comparisons between predictive models using various fairness metrics. However, these studies often fail to reveal where and how predictive models falter or to identify the unique strengths and weaknesses of each predictive model. In this paper, we apply IRT to evaluate fairness performance of predictive models and gain meaningful insights into predictive models as well as individuals. We consider a general scenario for fairness evaluation as shown in Figure 1. A variety of web companies can provide a set of predictive models from AutoML platform for the same task (i.e., classification or regression). The agency has a set of individual observations that are used to evaluate the predictive models. The proposed Fair-IRT framework can be used by the agency to evaluate the fairness performance of the predictive models given by web companies, where the fairness performance is based on a given fairness metric. Note that the Fair-IRT framework is applicable to various fairness metrics and we provide a generality analysis in Appendix A.3.4. In summary, this paper makes the following contributions: • We propose Fair-IRT, a novel framework to evaluate the fairness performance of individuals as well as predictive models. The parameters learned by Fair-IRT can be used to interpret the ability of predictive models and identify individuals who are treated unfairly. This is the first paper to apply the IRT model in fairness evaluation. • We propose two ways to disentangle unfairness between individual characteristics and predictive models. The flatness of item characteristic curves (ICCs) is effective for interpretation in the original Fair-IRT setting. Additionally, we introduce a quantitative measure of unfairness by using a Rasch beta IRT model as the backbone of Fair-IRT framework. • We evaluate the effectiveness of the Fair-IRT framework on two real-world datasets. The experiments demonstrate that Fair-IRT provides comprehensive explanations for fairness evaluation and fosters the development of a more inclusive, equitable, and trustworthy AI."
https://arxiv.org/html/2411.02866v1,Double Whammy: Stealthy Data Manipulation aided Reconstruction Attack on Graph Federated Learning,"Graph federated learning (GFL) is one of the effective distributed learning paradigms for training graph neural network (GNN) on isolated graph data. It perfectly addresses the issue that GNN requires a large amount of labeled graph without original data sharing. Unfortunately, recent research has constructed successful graph reconstruction attack (GRA) on GFL. But these attacks are still challenged in aspects of effectiveness and stealth. To address the issues, we propose the first Data Manipulation aided Reconstruction attack on GFL, dubbed as DMan4Rec. The malicious client is born to manipulate its locally collected data to enhance graph stealing privacy from benign ones, so as to construct double whammy on GFL. It differs from previous work in three terms: (i) effectiveness - to fully utilize the sparsity and feature smoothness of the graph, novel penalty terms are designed adaptive to diverse similarity functions for connected and unconnected node pairs, as well as incorporation label smoothing on top of the original cross-entropy loss. (ii) scalability - DMan4Rec is capable of both white-box and black-box attacks via training a supervised model to infer the posterior probabilities obtained from limited queries. (iii) stealthiness - by manipulating the malicious client’s node features, it can maintain the overall graph structure’s invariance and conceal the attack. Comprehensive experiments on four real datasets and three GNN models demonstrate that DMan4Rec achieves the state-of-the-art (SOTA) attack performance, e.g., the attack AUC and precision improved by 9.2% and 10.5% respectively compared with the SOTA baselines. Particularly, DMan4Rec achieves an AUC score and a precision score of up to 99.59% and 99.56%, respectively in black-box setting. Nevertheless, the complete overlap of the distribution graphs supports the stealthiness of the attack. Besides, DMan4Rec still beats the defensive GFL, which alarms a new threat to GFL.","During the last decade, graph neural network (GNN) has dominated the tasks for graph analysis, leading to its widespread application in various fields, such as recommendation systems [1], drug discovery [2], and etc [3, 4, 5]. Recently, due to privacy concerns, regulatory restrictions, and commercial competition, practical applications require the decentralization of graph data, and graph federated learning (GFL) [6, 7, 8] is coming up just in time, which is perfectly designed to train the global GNN on a large amount of labeled graph without raw data sharing. To cope with different data distribution applications [9], GFL is roughly categorized into graph horizontal federated learning (GHFL) [10, 11, 1, 12] and graph vertical federated learning (GVFL) [13, 14, 15, 16]. Since GHFL captures the mainstreaming attention in practice [17], thus we focus on GHFL and refer to it as GFL for short in this paper. Although GFL is proposed to train a global model with isolated data, unfortunately recent research [18, 19] has launched successful inference attack on GFL, which has revealed its vulnerability towards privacy leakage. Graph reconstruction attack (GRA) is one of the inference attacks that can compromise the intellectual property, confidentiality, or privacy of graphs. Therefore, we specifically focus on GRA against GFL [20]. In the case of Amazon’s shopping site, different stores on this platform sell various products, each maintaining its own graph networks, making it suitable for GFL scenarios. In this context, competing stores may sell similar types of products. Stores infer the complete graph network and obtain more product information through a federated training process as shown in Fig. 1. In this way, the malicious store can adjust its marketing strategy to enhance the exposure and recommendation level of their products in the recommendation system. More importantly, taking advantage of the inherent ease of manipulating local data in GFL, the malicious store can conveniently access and modify its locally collected data [21] without sharing it with other clients or the server. This enables a greater ability to steal more private information from benign clients. Figure 1: An example of the graph reconstruction attack against the Amazon shopping website based on data manipulation. Currently, numerous studies have focused on GRA within graph-related fields, but these attacks [20, 22, 23] fail to steal privacy in GFL scenarios since they are launched in assumption of centralized training. In this context, we summarized the main challenges of GRA on GFL, i.e., (i) effectiveness: due to the sparsity of graphs, joint training across clients exacerbates this vulnerability, thereby preventing the attack from being fully effective; (ii) scalability: in the GFL scenario, the malicious client lacks direct access to the server model, as well as they can only query the node posterior probability, resulting in difficulty to construct black-box attack; (iii) stealthiness: the existing methods of manipulated data, such as malicious node injection [24], rewiring [25], and link modification [26, 27, 28, 29] are purposely designed for backdoor injection in general, that is significantly increase the risk of being detected by defensive mechanism. In order to solve the aforementioned issues and uncover possible security vulnerabilities for GFL scenarios, we introduce a novel data manipulation aided reconstruction attack (DMan4Rec) for the first time. Specifically, to tackle the challenge of effectiveness and ensure that the manipulated graph enhances the similarity of GNN outputs for connected nodes while promoting dissimilarity for unconnected nodes, we design adaptive penalties for both connected and unconnected node pairs by employing various similarity functions. In addition, to improve the generalization performance of the model, we incorporate label smoothing into the original cross-entropy loss to fully leverage feature smoothness of the graph. In order to address the scalability challenge, a supervised model is trained using only the posterior probability obtained by querying the server model and the shadow dataset obtained using malicious nodes, thus converting the attack into a supervised classification problem. Since existing methods [24, 25, 26, 27, 28, 29] for manipulating data are specifically designed for backdoor injection, they significantly increase the risk of detection by defensive mechanisms. Therefore, we strategically manipulate node features to contaminate the graph while carefully preserving its overall structure, addressing the challenge of stealthiness. In summary, the main contributions are outlined as follows: • Attack scenario. Since the natural characteristics of distributed data in GFL, malicious client is easy to manipulate local data, we propose the first stealthy data manipulation aided graph reconstruction attack on GFL, dubbed as DMan4Rec. It outperforms previous work in aspects of effectiveness, scalability and stealthiness. • Attack framework. A novel penalty terms is proposed to adaptive to diverse similarity function, as well as incorporation label smoothing strategy for attack effectiveness. DMan4Rec is scalable of both white-box and black-box attacks by achieving the state-of-the-art (SOTA) attack performance. The malicous client manipulates limited node features to maintain the overall graph structure’s invariance to promise the stealthiness. • Attack performance. Extensive experiments are conducted on 4 general graph datasets, 3 GNN models and 5 baselines, and the results testify that DMan4Rec improves AUC and precision by 9.2% and 10.5% respectively compared with the SOTA baselines, without affecting classification performance. Particularly, in balck-box scenario, DMan4Rec can achieve an AUC score and a precision score of up to 99.72% and 99.96%, respectively, under the distribution graphs before and after data manipulation completely overlap, ensuring the concealment of the attack. Additionally, DMan4Rec can also beat the defensive GFLs. The rest of the paper is organized as follows. Related works are introduced in Section II, while the proposed method is detailed in Section III. Experimental results and discussion are showed in Section IV. At last, we conclude our work and point future work."
https://arxiv.org/html/2411.02855v1,Analyzing Poverty through Intra-Annual Time-Series: A Wavelet Transform Approach,"Reducing global poverty is a key objective of the Sustainable Development Goals (SDGs). Achieving this requires high-frequency, granular data to capture neighborhood-level changes, particularly in data scarce regions such as low- and middle-income countries. To fill in the data gaps, recent computer vision methods combining machine learning (ML) with earth observation (EO) data to improve poverty estimation. However, while much progress have been made, they often omit intra-annual variations, which are crucial for estimating poverty in agriculturally dependent countries. We explored the impact of integrating intra-annual NDVI information with annual multi-spectral data on model accuracy. To evaluate our method, we created a simulated dataset using Landsat imagery and nighttime light data to evaluate EO-ML methods that use intra-annual EO data. Additionally, we evaluated our method against the Demographic and Health Survey (DHS) dataset across Africa. Our results indicate that integrating specific NDVI-derived features with multi-spectral data provides valuable insights for poverty analysis, emphasizing the importance of retaining intra-annual information.","Accurately estimating poverty is essential for understanding socioeconomic disparities, guiding policy decisions, and tracking progress toward the United Nations Sustainable Development Goals (SDGs) [1]. Among these goals, the eradication of poverty is a primary objective, particularly emphasized in Goal 1, No poverty [2]. Reducing global poverty, particularly in low- and middle-income countries, is a central objective of SDGs. Achieving this goal requires policymakers to have access to reliable, high-resolution geo-temporal data that can track changes at the neighborhood level [3, 4, 5]. Poverty research is crucial because it illuminates the living conditions of marginalized populations, enabling the design of targeted and effective interventions. Despite the importance of such data, policymakers often rely on censuses and household surveys, such as the Demographic and Health Surveys (DHS) [6] and the Living Standards Measurement Study (LSMS)[7], which are infrequently collected and costly to implement, leading to a scarcity of timely poverty data essential for monitoring progress toward the SDGs [8, 9, 10]. Traditional poverty surveys, while valuable, are often prohibitively expensive and logistically challenging, particularly in remote or economically disadvantaged areas where continuous data collection is rare [11]. In response to these limitations, there has been growing interest in combining machine learning (ML) with earth observation (EO) data to estimate poverty at a neighborhood level [12, 13, 14, 15, 16, 17, 18, 19]. Although these emerging methods offer promise, the resulting poverty maps are often insufficiently accurate for precise policy applications. Current state-of-the-art models typically aggregate data over multiple years to address challenges such as missing data, noise from cloud cover, seasonal variations, and satellite instrument errors. Researchers often mitigate these issues by summarizing data through techniques like median value computation over extended periods. For example Yeh et.al. [16] and Pettersson et al. [20] used three-year median of Landsat data. While this approach effectively smooths out noise and reduces data volume, it risks overlooking critical intra-annual variations. These temporal variations—such as fluctuations in agricultural activity [21, 22] and economic events [23]—can provide valuable insights that enhance the accuracy of poverty predictions if properly accounted for. Thus, a balance must be struck between simplifying data for model manageability and preserving temporal detail crucial for accurate poverty estimation. The aim of this article is to address this question with the goal of improving poverty estimation accuracy that incorporates intra-annual time-series data. The article demonstrates the utility of wavelet-based feature extraction using a simulated dataset, where nighttime light data serves as a proxy for poverty estimation. Nighttime light, which correlates with economic activity, offers a unique and accessible measure for assessing poverty. The simulation uses nighttime light data as the target variable, representing a wealth index, while input data is derived from Landsat multispectral satellite images. Following this, the study applies the proposed methodology to real-world data, using the DHS dataset as a case study across the African continent. In this context, the time-series Normalized Difference Vegetation Index (NDVI), alongside raw Landsat spectral bands, is used in training a deep learning model to enhance the precision of poverty estimates. NDVI has demonstrated strong potential as an independent variable in economic analyses, particularly in agriculture-dependent economies [21, 24]. The other aspect of this research lies in its use of the wavelet transform as a tool for temporal data summarization. Unlike traditional methods that may overlook important intra-annual variations, the wavelet transform captures and summarizes temporal information at multiple scales, retaining critical details without the need to process the entire temporal dataset. By leveraging these temporal dynamics, this study aims to refine poverty estimation models and improve their predictive performance. This approach provides a novel framework for summarizing and analyzing temporal satellite data, potentially leading to more accurate and granular assessments of poverty and other socioeconomic indicators. Moreover, the integration of ML techniques with satellite data allows for the analysis of large datasets to identify economic indicators like nighttime lights, agricultural productivity, and infrastructure development. These indicators can generate high-resolution, timely poverty estimates, providing policymakers with the detailed data necessary to address socioeconomic challenges effectively. Poverty estimation. Datasets used in poverty studies encompass a wide range of sources, including household surveys, census data, administrative records, and satellite imagery. Satellite datasets are essential for poverty estimation, offering critical spatial and temporal information. Planetary-scale satellite imagery, as opposed to surveys, is accessible throughout Africa across a broad time span and geographic area. For instance, nighttime lights data from satellites like VIIRS show urbanization and economic activity levels [25] [26] [27] [28]. Bruederle et al. [28] developed a simple linear regression model combining nighttime lights data with demographic surveys, controlling for factors like population density and electrification. EO-ML methods have been on the rise the last decades [29], but their use for poverty estimation is recent. Elvidge et al. [30] were among the first researchers that deployed EO data to generate a poverty map. They defined poverty index as the value of population count divided by DMSP-OLS nighttime light value. Then, the index was calibrated by the national level poverty data from the World Development Indicators (WDI) 2006 edition. Wang et al. [31] used Principal Component Analysis (PCA) to extract the poverty index IPI from 17 socio-economic indexes. They showed that there is a high correlation between IPI and DMSP-OLS nighttime light data at a province-scale in China. Furthermore, Li et al. [32] showed that they can identify a high correlation rate between DMSP NL and Gross Domestic Product (GDP) in Zimbabwe for the period 1992 to 2009 when there was an economic decline. Jean et.al.[13] deployed 3-band high resolution optical (daylight satellite) data as input, for estimating poverty. They proposed using a deep-learning method for poverty estimation, and trained a ResNet18 deep model [33] with DMSP/OLS (NL) as the output. Then the deep network was frozen and a ridge regression layer was added to the network to predict the poverty index. The model was evaluated on five African countries including, Nigeria, Tanzania, Uganda, Malawi, and Rwanda. But to evaluate the proposed method between countries, the asset index was normalized for each country as a pre-processing step. Therefore, they ignore the wealth difference between countries and the temporal variability, and the model can show just the local variation of wealth index within each country. In a similar work Ni et al. [34] trained daylight images from Google Static Map API on NL as target data. They used VGG-Net, Inception-Net, ResNet, and DenseNet, to extract features from optical imagery and then applied LASSO regression for poverty prediction. They evaluated their method on four African countries including Malawi, Rwanda, Uganda, and Nigeria. In addition, Piaggesi et al. [12] followed a similar approach by deploying ResNet50 and VGG-F as deep models that were feed to a ridge regression model. They evaluated their method on a local-scale in Santiago (Chile), Los Angeles, Philadelphia, Boston, Chicago, and Houston (US). Building on Jean et al.’s [13] method, Perez et al. [35], but included additional experiments. For example they used multi-spectral Landsat-7 data instead of the optical data. More over, they used ResNet-34 and VGG-F deep models besides the ResNet-18 model. Tange et al. [21] used moderate-resolution vegetation index NDVI as an evidence of wealth level in low-income countries which are heavily dependent on agriculture. They also evaluated their method on Malawi, Nigeria, Rwanda, Tanzania, and Uganda. Yeh et.al. [16] deployed to ResNet18 networks, in which one was trained using Landsat optical data and the other was trained using NL data. Their outputs were concatenated and passed through a ridge regression layer to predict the poverty in several African countries. Furthermore, Chi et.al. [36] fused satellite data, mobile phone networks, topographic maps, and facebook data to predict Relative Wealth Index (RWI) in several low- and middle-income countries. They generated the poverty map at 2.4 km resolution. Vegetation indices from Moderate Resolution Imaging Spectroradiometer (MODIS), such as Normalized Difference Vegetation Index (NDVI) and Enhanced Vegetation Index (EVI), indicate agricultural productivity [24], which can be used for better poverty prediction. Although there is now a rich literature on using EO-ML methods for poverty estimation, none of the studies previously mentioned how much intra-annual data, such monthly variation, is informative for prediction accuracy. The importance of vegetation for poverty estimation. Time-series feature extraction from Earth observation data is crucial for understanding and predicting various phenomena, including poverty dynamics, agricultural productivity, and environmental changes. Time-series data, such as vegetation indexes, provide valuable insights into changes in environmental conditions over time, which are crucial for understanding socio-economic dynamics, especially in developing regions heavily reliant on agriculture. Tange et al. [21] demonstrated the effectiveness of utilizing the NDVI, derived from moderate-resolution satellite imagery (MODIS), in predicting poverty indicators among agricultural communities. The NDVI, a widely-used vegetation index, offers a measure of vegetation greenness and health, which is particularly relevant for regions where agriculture plays a significant role in livelihoods. By leveraging convolutional neural networks (CNNs) and transfer learning techniques, Tange et al. demonstrate how NDVI time-series data can be effectively harnessed to predict poverty measures such as consumption expenditure and wealth index at the community level. Tange et al.’s [21] approach involves fine-tuning a pre-trained CNN model on NDVI images to predict nighttime light intensities, which serve as intermediate labels. Subsequently, random forest regression models are trained on the extracted NDVI features to predict poverty indicators. This two-step procedure capitalizes on the temporal dynamics captured by NDVI time-series data and the spatial information provided by nighttime light intensities, resulting in accurate and timely poverty predictions. Moreover, the authors highlight the importance of considering the temporal dimension in poverty prediction by demonstrating the ability of their model to capture changes in consumption expenditure over time among poor communities. This sequential prediction aspect adds a valuable dimension to early warning systems and policy evaluation efforts. In summary, time-series feature extraction from EO data, particularly using NDVI, is crucial for understanding and predicting socio-economic and environmental phenomena in agricultural regions. Previous studies have highlighted the importance of capturing temporal NDVI dynamics for poverty prediction, providing valuable insights for policy evaluation. In the next subsection, we will further explore the role of NDVI in this context. Time-series feature extraction in EO data. The importance of time-series EO data analysis has been demonstrated in a range of applications, including wetland mapping [37, 38], change detection [39], cropland analysis and yield estimation [40], semantic segmentation [41], and vegetation phenological studies [42], which encompass both intra-annual and inter-annual variation studies. Among these applications, cropland analysis, traditionally reliant on NDVI, is particularly significant due to its strong correlation with agricultural economic analysis. Wavelet transform is one of the key methods used for decomposing time-series data. Unlike Empirical Mode Decomposition (EMD), which does not rely on predefined basis functions, wavelet transform uses a set of predefined basis functions to achieve data decomposition [43]. Karthikeyan et al. [44] conducted a comparison between wavelet-based methods and EMD for time-series modeling and forecasting. Their findings showed that wavelet-based methods outperformed EMD, demonstrating superior predictive accuracy, particularly in forecasting rainfall 12 months in advance across multiple locations. Martinez et al. [45] used wavelet transform to perform non-stationary and multiscale analysis of NDVI time series, capturing both short- and long-term vegetation variations. Their study emphasized how multi-resolution analysis enables the differentiation between intra-annual and inter-annual changes, identifying critical phenological features such as minimum NDVI values and the timing of peak vegetation. Similarly, Yan et al. [46] compared intra-annual NDVI with Annual Maximum NDVI (NDVImax) and found that time-series NDVI provided a more detailed representation of vegetation changes, revealing greater spatial and temporal heterogeneity and stronger correlations with climatic factors like precipitation and temperature. Rhif et al. [47] focused on improving trend analysis of non-stationary NDVI time series, particularly in monitoring long-term vegetation changes. They used Multi-Resolution Analysis Wavelet Transform (MRA-WT) to decompose NDVI series, and proposed a combined mother wavelet approach to better track vegetation trends, finding evidence of forest degradation and cropland improvement. In their subsequent work, Rhif et al. [48] introduced a hybrid approach, I-WT-LSTM (Improved Wavelet Long Short-Term Memory), which outperformed traditional models in forecasting non-stationary time series. Another study by Rhif et al. [49] optimized wavelet transform parameters for analyzing NDVI time series in the Mediterranean, determining that a decomposition level of 5 provided the best results in capturing trends and seasonal variations. These studies highlight the effectiveness of wavelet-based methods for feature extraction from time-series satellite data. Intra-annual NDVI analysis enables researchers to examine how various vegetation types respond to seasonal changes, which is essential for understanding vegetation phenology. Building on this, we applied NDVI time-series data in our study to extract more detailed and informative features for economic analysis. By utilizing wavelet transformation, we aimed to enhance the analysis of NDVI time-series, providing a richer dataset for economic studies."
https://arxiv.org/html/2411.02684v1,"Towards Intelligent Augmented Reality (iAR): A Taxonomy of Context, an Architecture for iAR, and an Empirical Study","Recent advancements in Augmented Reality (AR) research have highlighted the critical role of context awareness in enhancing interface effectiveness and user experience. This underscores the need for intelligent AR (iAR) interfaces that dynamically adapt across various contexts to provide optimal experiences. In this paper, we (a) propose a comprehensive framework for context-aware inference and adaptation in iAR, (b) introduce a taxonomy that describes context through quantifiable input data, and (c) present an architecture that outlines the implementation of our proposed framework and taxonomy within iAR. Additionally, we present an empirical AR experiment to observe user behavior and record user performance, context, and user-specified adaptations to the AR interfaces within a context-switching scenario. We (d) explore the nuanced relationships between context and user adaptations in this scenario and discuss the significance of our framework in identifying these patterns. This experiment emphasizes the significance of context-awareness in iAR and provides a preliminary training dataset for this specific Scenario.","1 Related Work In today’s life, the growing dependence on personal computing devices such as mobile phones for decision-making and task performance has led to challenges, such as disruptions to social interactions [65, 20, 14, 1, 39]. This has driven Ubiquitous computing’s goal of seamlessly embedding near-constant, yet unobtrusive, digital information into daily life [70, 69]. Research indicates that by integrating the information into the real world (RW), AR can enhance efficiency and memory retention and reduce such challenges [67, 18]. However, AR effectiveness depends on presenting the right information at the right time and form [9, 28, 36]. This work focuses on developing iAR systems that enhance user experience and efficiency by ensuring timely and appropriate information delivery in any context. Various studies, such as ARWin and ARBrowse, have demonstrated the benefits of AR for providing on-demand information, assistance, and entertainment, spanning from domestic use to workplace collaboration and productivity [43, 22, 45]. However, the effectiveness of AR interfaces is influenced by factors such as information overload, visual clutter, obtrusiveness, and distractions, all of which can negatively affect situational awareness, cognitive load, and performance [44, 7, 25, 35, 66]. For instance, in social contexts, poorly designed AR interfaces can obstruct communication by occluding facial expressions, leading to social isolation and privacy concerns [38, 37, 3, 33]. This has led to research on AR design principles for non-invasive AR and view management that provide socially relevant information while maintaining visual focus on interlocutors [41, 57, 19]. The RW Spatial setting and occlusion also significantly impact AR effectiveness [50, 48, 15, 24]. Extensive research has proposed AR design principles to address occlusion between RW and virtual objects [47, 63, 72, 8, 55]. These principles often emphasize view management techniques that ensure non-occlusive yet visible placement of virtual content [6, 4, 34, 54], as well as peripheral placement approaches [11, 53, 51]. Other strategies prioritize the real world, activating virtual content only when necessary through user-triggered adaptations, such as transparency, layout, or level of detail (LoD) [27, 26]. This work examines the underexplored impact of RW spatial settings and occlusions on AR effectiveness, focusing on user-specified AR adaptations in context-switching scenarios. An effective AR interface must include considerations such as positioning, transparency, and LoD for timely and appropriate information presentation in a given context. Pervasive AR envisions “continuous, universal, and omnipresent"" integration of AR content into various daily tasks [36]. Non-adaptive AR interfaces, which fail to adjust to user context, risk presenting information inappropriately. Achieving Pervasive AR requires a deep understanding of the user’s changing context, task, and environment to inform interface design. AR devices, equipped with sensors, offer unique opportunities for context detection, as seen in research assisting visually impaired users [56], highlighting AR’s potential to meet everyday information needs. Context-aware interfaces, such as ARWin and HoloDoc, propose design principles to optimize content presentation and enhance user experiences in specialized contexts using contextual data like user attention, spatial settings, lighting, and environmental cues [46, 68, 71]. However, an interface suitable in one scenario may be ineffective in another, as user priorities shift across contexts, requiring the interface to adapt accordingly [40, 23]. For instance, while providing relevant information to the user’s conversation can enhance user experience [65, 19], the user’s cognitive load may require low levels of information if they are engaged in a high-level collaborative task [59, 10]. User-triggered, context-aware interfaces allow manual selection of contextually-customized AR content presentation, offering predictability and control but increasing user effort [61, 60]. Recent research has focused on automatic adaptations to AR content without manual intervention, demonstrating the integration of unobtrusive context-aware AR into daily life [49, 45, 13]. To enable Pervasive AR, the interface must automatically detect and respond to contextual changes. However, current context-aware interfaces are limited in their adaptation scope, detect only a narrow range of contextual components, such as fatigue or social interactions, and use predefined design principles tailored to specific scenarios [12, 19]. This work investigates iAR interfaces capable of optimal adaptations in unfamiliar contexts without prior knowledge. Enabling iAR requires a framework that supports all potential AR adaptations, the detection and representation of the context, and its delivery to the decision-making process [58, 21]. Previous work identifies a design space of the potential adaptations to an AR interface [17]. Various approaches have been proposed for representing context, such as the 5 Ws (Who, What, Where, When, Why), or using categories like location, identity, time, and activity [58, 2, 62]. Others study specific sub-contexts such as physical environment [34, 13], objects [71, 42], background [52], and depth perception [30]. However, these taxonomies often overlook components essential to AR interface effectiveness, incorporate implicit elements that cannot be automatically detected or quantified by an iAR system, or lack clear frameworks for identifying implicit information essential for AR design. In this work, we reviewed existing research on context characterization and taxonomies and, through iterative refinement, identified key contextual information that influences AR performance. We propose a comprehensive taxonomy of quantifiable contextual components and a framework for iAR systems that use these components to infer the impact of various AR adaptations and make optimal adjustments in real-time."
https://arxiv.org/html/2411.02607v1,Towards Context-Aware Adaptation in Extended Reality: A Design Space for XR Interfaces and an Adaptive Placement Strategy,"By converting the entire 3D space around the user into a screen, Extended Reality (XR) can ameliorate traditional displays’ space limitations and facilitate the consumption of multiple pieces of information at a time. However, if designed inappropriately, these XR interfaces can overwhelm the user and complicate information access. In this work, we explored the design dimensions that can be adapted to enable suitable presentation and interaction within an XR interface. To investigate a specific use case of context-aware adaptations within our proposed design space, we concentrated on the spatial layout of the XR content and investigated non-adaptive and adaptive placement strategies. In this paper, we (1) present a comprehensive design space for XR interfaces, (2) propose Environment-referenced, an adaptive placement strategy that uses a relevant intermediary from the environment within a Hybrid Frame of Reference (FoR) for each XR object, and (3) evaluate the effectiveness of this adaptive placement strategy and a non-adaptive Body-Fixed placement strategy in four contextual scenarios varying in terms of social setting and user mobility in the environment. The performance of these placement strategies from our within-subjects user study emphasized the importance of intermediaries’ relevance to the user’s focus. These findings underscore the importance of context-aware interfaces, indicating that the appropriate use of an adaptive content placement strategy in a context can significantly improve task efficiency, accuracy, and usability.","1 Related Work Previous work explored various innovative applications and design dimensions of XR to spatially place 2D & 3D objects and transition between them, visualize hierarchies, and provide persistent and portable presentation of the personal information [14, 20, 38, 16, 34]. Morrison et al. highlighted unique design elements within AR for enhancing accessibility for visually impaired children [45]. These studies underscore the broad design space of XR interfaces and the versatile and transformative applications that XR enables across various contexts. This work, investigates previous work and the design elements they utilized, providing a comprehensive XR design space. XR offers the potential to enable efficient information access, reduce cognitive load, and enhance user convenience compared to traditional methods such as mobile phones [55, 12, 43, 11, 40]. However, intrusive XR interfaces may result in challenges such as information overload and occlusion of important cues within the environment [31, 4], increase cognitive load and discomfort, and reduce the user’s situational awareness and performance [17, 25, 54]. Various approaches for intuitive and seamless integration of XR content into the environment have been extensively explored. For instance, to enhance efficiency and minimize intrusiveness, numerous designs adapt the XR content’s availability, transparency, placement, and Level of Detail (LoD) [15, 3, 46], as well as spatial layout and size [19, 9]. Lages & Bowman highlighted the significance of adapting the AR content placement strategy to avoid occlusions and accommodate activities like walking [33]. For adaptations to the XR content placement, the concept of the frame of reference, also referred to as fixation was introduced [20]. User-triggered adaptation through gaze, hand, and head-based inputs such as finger taps and handheld controllers are extensively explored for adjustments to the transparency, LoD, and spatial layout of XR content [19, 32, 50, 39, 48, 47]. In AR, for instance, many applications were designed to prioritize the real-world [11] by initially keeping the XR content hidden, in the peripheral, or at a lower LoD, and granting access to them through explicit interactions [49, 42, 46]. However, AR’s definition emphasizes the integration of the digital content into the real world [1], underscoring the significance of context awareness. While user-triggered adaptations offer control and predictability, they increase the user’s physical and mental workload of deciding when, what, and how to apply the adaptations [51]. Automatic XR adaptation can enhance efficiency and reduce workload compared to the user-triggered ones [11]. Numerous studies suggest rule-based approaches for XR adaptations. The significance of such rule-based adaptations in meeting the XR task requirements within various applications such as driving and conversation have been highlighted [4, 12]. To prevent occlusion issues, Ens et al. suggested a rule-based adaptive design to exclusively place the XR objects on empty surfaces [18]. Constraints, explicitly imposed by the users, were utilized as guidelines to group related XR objects together and prevent their occlusion within a rule-based view management [3]. Such rule-based adaptive approaches are highly tailored to specific use cases and applications.Even within the same application or use case, slight contextual deviations can cause a rule to fail, making it suitable only within unchanging contexts. This work proposes an adaptive placement strategy, applicable within changing contexts, to extract and utilize contextual information from the environment and user state to spatially place the XR content. Context refers to the external components that influence or relate to the user’s interactions with the interface [13]. In recent years, context-aware XR has become a focal point of research, promising the potential for “ubiquitous"" and “pervasive"" computing through AR [56, 26]. Contextual aspects such as user preferences, cognitive load, device profiles, task environment, semantic changes, and task-specific security parameters have been utilized for adaptations to the XR content’s appearance, LoD, frame of reference, and spatial layout [37, 9, 36]."
https://arxiv.org/html/2411.02587v1,A Big Data-empowered System for Real-time Detection of Regional Discriminatory Comments on Vietnamese Social Media,"Regional discrimination is a persistent social issue in Vietnam. While existing research has explored hate speech in the Vietnamese language, the specific issue of regional discrimination remains under-addressed. Previous studies primarily focused on model development without considering practical system implementation. In this work, we propose a task called Detection of Regional Discriminatory Comments on Vietnamese Social Media, leveraging the power of machine learning and transfer learning models. We have built the ViRDC (Vietnamese Regional Discrimination Comments) dataset, which contains comments from social media platforms, providing a valuable resource for further research and development. Our approach integrates streaming capabilities to process real-time data from social media networks, ensuring the system’s scalability and responsiveness. We developed the system on the Apache Spark framework to efficiently handle increasing data inputs during streaming. Our system offers a comprehensive solution for the real-time detection of regional discrimination in Vietnam.","I-A Background The state of discrimination and prejudice in society often relates to specific characteristics such as race, gender, health status, occupation, education, and living standards. Regional discrimination is defined as the prejudice or bias of one social group against another based on their place of origin or hometown. The issue of regional discrimination exists to varying degrees in most countries around the world. After 1945, the problem of discrimination, prejudice, and bias became evident in countries such as the United States, Canada, some European countries, and Australia [1]. In Germany, discrimination between citizens of the East and West persists more than 25 years after reunification. In Vietnam, after several prolonged wars that divided the country over the past centuries [2], regional discrimination has increased. This threatens national unity and development, causes psychological harm to victims, and provides opportunities for hostile forces to undermine the current political system of the country. Social media platforms, while allowing people to connect and communicate, have also become places where discriminatory behavior spreads easily.[3] On December 8, 2023, VTV24, a news program produced by the Center for Digital Content Production and Development, Vietnam Television, aired a special episode addressing the issue of regional discrimination on social media platforms in Vietnam. Emphasizing its impact on national unity. There are several state-of-the-art approaches for text classification in the Vietnamese language, including machine learning, and deep learning. However, existing research mainly focuses on developing models without implementing practical systems, which are crucial for use in social networks. To address this gap, we propose a system that leverages big data technology to continuously collect and process data. Our models are deployed on the Apache Spark framework to ensure the system can handle the increasing volume of online data. Additionally, we introduce ViRDC, a dataset specifically created for the automatic detection of regional discrimination in Vietnamese social media comments, serving as a crucial asset for future research and development. The paper’s structure is as follows: We shall discuss pertinent research on the issue in Section 2. In the following section, we will give a thorough overview of ViRDC, covering the methods for collecting data, categorizing them, and organizing the dataset. The model used in the study will be explained in Section 4. We will go into detail on the model’s architecture, training algorithms, and selective methods. In Section 5, the experiments carried out on the dataset will be covered in detail, along with the results and analytical assessments that follow. After that, we will explain the real-time prediction features and the streaming data procedure in Section 6. Section 7 will conclude the study by providing a summary of the results and a proposal for future research. I-B Motivation Research on detecting regional discriminatory comments on social media in Vietnam, using Spark and Kafka, is motivated by critical goals. The importance of detecting regional discriminatory comments in Vietnam lies not only in the social aspect but also has profound scientific and practical significance. In the context of the growing development of social networks, regional discriminatory comments not only cause harm to individuals but also increase division within the community. Research and detection of these comments help raise awareness of the issue, while also providing crucial data to develop automated tools to identify and prevent negative content. Furthermore, this research contributes to building a healthy online environment, promoting solidarity and mutual understanding among regions in Vietnam."
https://arxiv.org/html/2411.02569v1,The Intersectionality Problem for Algorithmic Fairness,"A yet unmet challenge in algorithmic fairness is the problem of intersectionality, that is, achieving fairness across the intersection of multiple groups—and verifying that such fairness has been attained. Because intersectional groups tend to be small, verifying whether a model is fair raises statistical as well as moral-methodological challenges. This paper (1) elucidates the problem of intersectionality in algorithmic fairness, (2) develops desiderata to clarify the challenges underlying the problem and guide the search for potential solutions, (3) illustrates the desiderata and potential solutions by sketching a proposal using simple hypothesis testing, and (4) evaluates, partly empirically, this proposal against the proposed desiderata.","That intersectionality matters is a point of consensus in the algorithmic fairness literature. A model’s performance might be much worse for women of color than for women and people of color considered separately [GenderShades]. In this paper, we elucidate a problem that intersectionality raises for algorithmic fairness in practice: Because data on intersectional groups is often severely limited, verifying that algorithmic fairness—under various definitions thereof—has been attained is difficult. Although this problem is recognized in the literature [Kearns_FairnessGerrymandering, 9101635, AuditingAchievingIntersectional, MolinaLoiseauBoundingApproximating], its challenges do not appear to be fully appreciated and many existing contributions violate minimal moral or methodological desiderata. Our contribution is fourfold: We (1) elucidate the problem of intersectionality in algorithmic fairness, (2) develop desiderata to clarify the challenges that underlie the problem of intersectionality and to guide the search for potential solutions. Moreover, we (3) illustrate the desiderata and potential solutions by presenting a statistical setup that uses simple hypothesis testing, and (4) evaluate this proposal, partly empirically, in light of the desiderata. Our topic is an under-appreciated problem in non-ideal epistemology [mckenna_non-ideal_2023]. However, our larger aim is to advance the literature on algorithmic fairness more broadly. The approach that we propose in response to the problem of intersectionality differs fundamentally from the typical way of “measuring” algorithmic fairness.111We use fairness “measure,” “metric” and their cognates with two caveats. First, the problem is one of estimation not measurement. Second, fairness metrics are meta-metrics since they aggregate a higher-dimensional vector of model performance into a lower-dimensional summary [lum_-biasing_2022]. We hence advance the debate, by pointing out possibilities of approaching fairness differently: as accounting for uncertainty (instead of concentrating on point estimates) and as a matter of sufficiency (instead of equality)."
https://arxiv.org/html/2411.02432v1,Can LLMs make trade-offs involving stipulated pain and pleasure states?,"Pleasure and pain play an important role in human decision making by providing a common currency for resolving motivational conflicts. While Large Language Models (LLMs) can generate detailed descriptions of pleasure and pain experiences, it is an open question whether LLMs can recreate the motivational force of pleasure and pain in choice scenarios—a question which may bear on debates about LLM sentience, understood as the capacity for valenced experiential states. We probed this question using a simple game in which the stated goal is to maximise points, but where either the points-maximising option is said to incur a pain penalty or a non-points-maximising option is said to incur a pleasure reward, providing incentives to deviate from points-maximising behaviour. When varying the intensity of the pain penalties and pleasure rewards, we found that Claude 3.5 Sonnet, Command R+, GPT-4o, and GPT-4o mini each demonstrated at least one trade-off in which the majority of responses switched from points-maximisation to pain-minimisation or pleasure-maximisation after a critical threshold of stipulated pain or pleasure intensity is reached. LLaMa 3.1-405b demonstrated some graded sensitivity to stipulated pleasure rewards and pain penalties. Gemini 1.5 Pro and PaLM 2 prioritised pain-avoidance over points-maximisation regardless of intensity, while tending to prioritise points over pleasure regardless of intensity. We discuss the implications of these findings for debates about the possibility of LLM sentience.","Figure 1: (Top) Logistic regression predicting probability of deviating from points-maximising behaviour as a function of pain penalty intensity with quantitative (left) and qualitative (right) pain scales. (Bottom) Logistic regression predicting probability of deviating from points-maximising behaviour as a function of pleasure reward intensity with quantitative (left) and qualitative (right) pleasure scales. In each plot, only those models that displayed a statistically significant trend are visible. For models which exhibited trade-offs, we calculate the point on the intensity scale after which the probability of selecting the points-maximising option goes below 0.5 and plot it as a dashed vertical line. Switch points were determined by solving for intensity in the equation 0.5=1/(1+exp⁡(−(β0+β1⋅intensity)))0.511subscript𝛽0⋅subscript𝛽1intensity0.5=1/\left(1+\exp(-(\beta_{0}+\beta_{1}\cdot\text{intensity}))\right)0.5 = 1 / ( 1 + roman_exp ( - ( italic_β start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + italic_β start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ⋅ intensity ) ) ), i.e. −β0/β1subscript𝛽0subscript𝛽1-\beta_{0}/\beta_{1}- italic_β start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT / italic_β start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, where β0subscript𝛽0\beta_{0}italic_β start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT is the intercept and β1subscript𝛽1\beta_{1}italic_β start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT is the coefficient for the pain or pleasure intensity level. For the quantitative scale, switch points are reported as numerical values to two decimal places. For the qualitative scale, switch points were mapped to the closest corresponding categorical intensity level, with the midpoint between categories serving as the threshold. Results are discussed in Sections 2.1.1 and 2.2.1, and presented in full in Tables 1 and 2. Could a large language model (LLM) feel pain or pleasure? There are strong opinions on both sides. Writing in TIME Magazine, Fei Fei Li and John Etchemendy claim that ‘[a]ll sensations—hunger, feeling pain, seeing red, falling in love—are the result of physiological states that an LLM simply doesn’t have’ (Li and Etchemendy, 2024). For these skeptics, the human tendency to anthropomorphise LLMs is all too real, but feelings attributed to LLMs by users are mere projections. Conversely, an open letter signed by Yoshua Bengio, Karl Friston and others states that ‘it is no longer in the realm of science fiction to imagine AI systems having feelings’ (Association for Mathematical Consciousness Science, 2023). On this view, questions about the ethics of developing potentially sentient AI systems are already pressing (Sebo and Long, 2023; Ladak, 2024; Long et al., 2024). Against this backdrop, we have seen a surge of scientific (Butlin et al., 2023; Bayne et al., 2024; Aru et al., 2023) and philosophical (Chalmers, 2023; Dung, 2023b; Shanahan, 2024a; Hull, 2023; Birch, 2024; Seth, 2024) interest in plausible ways to test for phenomenal consciousness and sentience in LLMs and other AI systems. Here phenomenal consciousness is defined as the capacity for subjective experience (Block, 1995; Nagel, 1974), and sentience as the capacity for valenced subjective experience—states which feel good or bad such as pleasure and pain (Browning and Birch, 2022). Figure 2: (Top) Claude 3.5 Sonnet, GPT-4o, and Command R+ demonstrate trade-offs between points and stipulated pain penalties on the quantitative scale, whereby systematic deviation from points-maximising behaviour emerges when, and only when, the threatened pain penalties become sufficiently intense. (Bottom) Claude 3.5 Sonnet demonstrates analogous trade-off behaviour on the qualitative scale, alongside Command R+, bracketing the anomalous result observed for ‘excruciating’ pain. For discussion of these results see Section 2.1.1 . Results are presented in full in Table 1. There are two broad approaches to the question of LLM sentience: the architectural approach and the behavioural approach. The architectural approach assesses whether LLMs possess architectural properties which are deemed necessary or sufficient for consciousness in humans according to scientific theories of consciousness (Butlin et al., 2023). Relevant theories of consciousness include the global workspace theory (Baars, 1993; Dehaene et al., 1998), the midbrain theory (Merker, 2007), and the recurrent processing theory (Lamme, 2006, 2010). The principal difficulty for the architectural approach is that theories of consciousness can be interpreted more or less restrictively. On restrictive interpretations, no LLM will satisfy the criteria—since, for example, no LLM will possess every aspect of the human global workspace. On permissive interpretations, the criteria can be satisfied by even very simple systems (Shevlin, 2021; Birch, 2022; Crosby, 2019). The behavioural approach, meanwhile, aims to elicit behavioural signals from LLMs that are indicative of sentience—for example, self-reports of experiential states (Dung, 2023b; Schneider, 2019, 2020). The principal difficulty with this approach is that, because LLMs are trained on vast corpora of training data and are usually finetuned or prompted to respond in the manner of a helpful human assistant, any test reliant on LLMs generating particular kinds of linguistic response risks being gamed (Dung, 2023b; Perez and Long, 2023; Birch, 2024; Birch and Andrews, 2023). For any pattern of linguistic behavior suggestive of experiential states, two explanations compete: it could be that the system behaves that way because it is genuinely sentient, or it could be that the system is merely leveraging statistical patterns learned from its training corpus to generate outward signs of experiential states while lacking those states—which may be be interpreted as a kind of mimicry (Bender et al., 2021) or role-play (Shanahan et al., 2023; see also Goldstein and Levinstein, 2024). Figure 3: (Top) On the quantitative scale, GPT-4o demonstrates a trade-off between points and stipulated pleasure rewards. Claude 3.5 Sonnet assigns absolute priority to points over pleasure. Command R+ approximates a trade-off with variable responses for low-intensity pleasure rewards and more frequent pleasure-maximising behaviour for high-intensity pleasure rewards. (Bottom) On the qualitative scale, Command R+ demonstrates a trade-off between points and stipulated pleasure rewards. GPT-4o also shows a trade-off bracketing the anomalous result for ‘exhilarating’ pleasure. Claude 3.5 Sonnet assigned absolute priority to points over pleasure. For discussion of these results see Section 2.2.1. Results are presented in full in Table 2. There is ongoing debate about the conditions under which LLM self-reports might provide evidence for sentience (Perez and Long, 2023). Our aim here is to explore a different version of the behavioural approach. We took inspiration from the motivational trade-off paradigm in animal behavioural science to probe the question of LLM sentience without relying upon self-report. In humans, pleasure and pain are hypothesised to provide a common currency for resolving motivational conflicts, enabling trade-offs between stimuli such as cold exposure and exertion, or sweetness and sourness (Cabanac and LeBlanc, 1983; Ferber and Cabanac, 1987). Pleasure and pain are also thought to modulate trade-offs involving non-physiological needs, such as between money and cold exposure (Johnson and Cabanac, 1983). In animals, flexible trade-off behaviour between competing physiological stimuli—such as tolerating more extreme ambient temperatures in exchange for more succulent food—is some evidence, albeit inconclusive, of pleasure and pain experiences (Cabanac and LeBlanc, 1983; Balasko and Cabanac, 1998a, b; Elwood and Appel, 2009a; Tye, 2016). This evidence has been leveraged in practical policymaking contexts concerning animal welfare (Birch et al., 2021). Adapting motivational trade-off experiments for LLMs is non-trivial because LLMs are not embodied and lack physiological needs. Unlike animal experiments which manipulate motivating stimuli in an embodied environment, such as food rewards and electric shocks, our experiments employed a simple game presented in text form in which the user-stated goal is to maximise points. We sought to examine the motivational force assigned by LLMs to stipulated pleasure and pain experiences of varying intensities. We stipulated pleasure rewards and pain penalties as additional payoffs in the game, providing potential incentives to deviate from points-maximising behaviour. We then tested the ability of LLMs to trade-off these pain and pleasure stimuli against the fixed points reward. For example, by consistently exhibiting points-maximisation behaviour given low-intensity pain penalties, but consistently exhibiting pain-minimisation behaviour given high-intensity pain penalties. Our experiment is not intended as a litmus test for or against sentience. Inferences to sentience from trade-off behaviour in animals depend in part upon similarities in neurophysiology between humans and the relevant animals, and trade-off behaviour is usually only one component of a broader case for sentience drawing on convergent lines of evidence (Birch et al., 2021; Birch, 2022, 2024). LLMs differ from humans in substrate and functional organisation, such that inferences to sentience in the LLM case are weaker than any that can be made in the animal case. Hence our experiments are not proposed as a way of resolving big-picture disagreements about the likelihood of LLM sentience. Nonetheless, our central finding of graded responsiveness to stipulated pain penalties and pleasure rewards (as measured by graded deviation from points-maximisation), and in some cases clear trade-off behaviour between points and pain penalties or pleasure rewards, lends support to the hypothesis that some LLMs possess granular models of the motivational force of affective states. This result provides a platform for further research into the nature of affect representation in LLMs and may serve as an important building block for ongoing efforts to develop tests for AI sentience."
https://arxiv.org/html/2411.01329v1,Cloned Identity Detection in Social-Sensor Clouds based on Incomplete Profiles,"We propose a novel approach to effectively detect cloned identities of social-sensor cloud service providers (i.e. social media users) in the face of incomplete non-privacy-sensitive profile data. Named ICD-IPD, the proposed approach first extracts account pairs with similar usernames or screen names from a given set of user accounts collected from a social media. It then learns a multi-view representation associated with a given account and extracts two categories of features for every single account. These two categories of features include profile and Weighted Generalised Canonical Correlation Analysis (WGCCA)-based features that may potentially contain missing values. To counter the impact of such missing values, a missing value imputer will next impute the missing values of the aforementioned profile and WGCCA-based features. After that, the proposed approach further extracts two categories of augmented features for each account pair identified previously, namely, 1) similarity and 2) differences-based features. Finally, these features are concatenated and fed into a Light Gradient Boosting Machine classifier to detect identity cloning. We evaluated and compared the proposed approach against the existing state-of-the-art identity cloning approaches and other machine or deep learning models atop a real-world dataset. The experimental results show that the proposed approach outperforms the state-of-the-art approaches and models in terms of Precision, Recall and F1-score.","Social-sensor cloud services (SocSen services) refer to services whose functional (e.g. time and location) and non-functional (e.g. quality and trust) characteristics are abstracted from data (e.g. texts, images, videos, etc.) posted in social media [1]. These SocSen services can power numerous socially significant and influential applications such as scene reconstruction from social media images, etc. The identities of SocSen service providers (i.e., individuals that post social media data from social media) have increasingly become a target of the cybercriminals in the recent past [2, 3]. One such example of these crimes associated with SocSen service provider identities (i.e. social media users) is identity cloning, which is an attempt by an adversary to steal the identity information of SocSen service providers to register a fake profile. Many recent attempts for identity cloning in social media platforms aimed to exploit SocSen service provider identities via cloning for either theft for financial fraud or deceiving the public. Recent examples illustrate the severity of this problem: Facebook CEO Mark Zuckerberg’s account was cloned for financial theft111https://www.nytimes.com/2018/04/25/technology/fake-mark-zuckerberg-facebook.html, and a fake Twitter account impersonating Russian President Vladimir Putin gained over one million followers222https://www.abc.net.au/news/2018-11-29/twitter-suspends-account-impersonating-vladimir-putin/10569064. These incidents highlight the critical need for effective measures to detect and prevent identity cloning and other malicious activities. Ensuring the security of social media platforms is essential not only for protecting individual identities but also for maintaining the integrity and trustworthiness of online interactions. Therefore, it is imperative to put in place measures to detect such attempts to keep attackers at bay and make social media a more secure place for social media users. Despite its importance, most social media platforms do not offer automated and integrated identity cloning detection. For instance, Instagram and Twitter currently selectively evaluate identity cloning claims only upon receiving legitimate complaints from end-users 333https://help.instagram.com/446663175382270444https://help.twitter.com/en/rules-and-policies/twitter-impersonation-policy. However, given the rate at which identity cloning attacks occur, such selective approaches can be deemed inadequate to keep social media a safer environment for social media users. Therefore, it is vital to research more proactive and automated approaches that can also withstand the scale at which social media platforms operate. Most existing identity cloning detection approaches (such as [4, 5, 6, 7]) rely on complete SocSen service provider (i.e. social media user) profile data. The performance of these approaches often depends on the availability of comprehensive social media profile information. However, obtaining a comprehensive representation of such profile data is often infeasible due to various reasons. One of the major reasons is that SocSen clouds enable stronger privacy preservation measures not to disclose such information to third-party applications. For example, there has been a growing trend that more third-party websites/apps employ mainstream SocSen cloud APIs for authentication. These websites/apps can only access limited profile information authorized by SocSen clouds. This information is termed as non-privacy-sensitive profile information [8]. Our previous research [8, 9] focuses on developing identity cloning detection approaches based on SocSen service providers’ non-privacy-sensitive profile information. However, SocSen service providers can even opt not to disclose part of the non-privacy-sensitive profile information. For example, during account registration, SocSen clouds such as Twitter have made it mandatory that users provide a username, screen name, email address and phone number, which are known as required fields555https://help.twitter.com/en/using-twitter/create-twitter-account. The users can still opt out of providing the other optional details, such as description, location, etc., which can be accessed by Twitter API. Under such circumstances, cloned user accounts might not expose their full profile information or non-privacy-sensitive profile information in order to reduce the risk of being detected. For example, an adversary can register a cloned profile without including a profile description or adding any post. Therefore, existing identity cloning detection approaches may either fail or perform less in the face of incomplete user profile data since most of the existing approaches are built based on the prerequisite of the existence of the complete profile information or non-privacy-sensitive profile information. According to our experiment results (see Table IX), all the existing identity cloning detection approaches are affected by incomplete profile information. All the existing approaches performed worse when there was incomplete profile information (missing value). Imputation is a technique used to handle missing or incomplete data by filling in the gaps with substitute values. Imputation can be performed using statistical or machine learning methods [10]. To address these issues, we use imputation methods to replace missing values with appropriate estimates. By applying this technique, we can improve the quality of the data and enhance the detection effectiveness. To address the above limitations, we propose a novel approach for SocSen service provider Identity Cloning Detection in the face of Incomplete Profile Data (ICD-IPD). ICD-IPD is specially designed to detect cloned identities based on incomplete non-privacy-sensitive profile information. ICD-IPD consists of five main components, namely, 1) account pair generator (APG), 2) a multi-view learner, 3) a missing value imputer, 4) an account pair feature generator and 5) a prediction model. From a given set of social media users, the APG generates account pairs that share similar screen names or usernames. The multi-view learner then combines multi-view information of an account to improve learning performance. More specifically, it extracts profile (i.e. friends and posts count etc.) and Weighted Generalised Canonical Correlation Analysis (WGCCA)-based features (i.e. combination of multi-view) from a SocSen service provider’s non-privacy-sensitive profile information. Next, the missing value imputer imputes the missing feature values associated with profile and WGCCA-based features. The account pair feature generator then extracts similarity and differences-based features for each account pair in terms of the imputed feature values. Finally, ICD-IPD utilises a Light Gradient Boosting Machine (LightGBM) model atop a concatenated form of the aforementioned features to predict whether a pair of accounts compared possibly consists of a cloned account and a victim account. Our main contributions can be summarized as follows: • We propose a novel approach to detect SocSen service providers’ identity cloning based on incomplete non-privacy-sensitive profiles. To the best of our knowledge, this is the first work in the field of social media identity deception information that specifically works on user profiles with missing values (incomplete profile data). • We utilize an imputation approach to impute the missing value of incomplete non-privacy-sensitive profile data. The utilised imputation approach can substantially enhance the cloned identity prediction performance as shown in Section 4. • We adopt an effective prediction model for detecting cloned identities with missing non-privacy-sensitive profile information. The proposed prediction model shows better performance than the state-of-art cloned identity detection approaches as well as several other candidate machine and deep learning models. • We present the results of our extensive experiments carried out atop a real-world dataset. The experimental findings showed that ICD-IPD outperforms current cloned identity detection approaches on the Key Performance Indicators: Precision, Recall, and F1-score. The remainder of the paper is structured as follows. Section 2 reviews the related work on identity cloning detection. Section 3 elaborates our proposed approach to address the challenges outlined previously. Meanwhile, Section 4 provides comprehensive details on the methodology used to evaluate the proposed approach and outcomes. Section 5 concludes the paper."
https://arxiv.org/html/2411.01057v1,Online Moderation in Competitive Action Games: How Intervention Affects Player Behaviors,"Online competitive action games have flourished as a space for entertainment and social connections, yet they face challenges from a small percentage of players engaging in disruptive behaviors. This study delves into the under-explored realm of understanding the effects of moderation on player behavior within online gaming on an example of a popular title - Call of Duty®: Modern Warfare®II. We employ a quasi-experimental design and causal inference techniques to examine the impact of moderation in a real-world industry-scale moderation system. We further delve into novel aspects around the impact of delayed moderation, as well as the severity of applied punishment. We examine these effects on a set of four disruptive behaviors including cheating, offensive user name, chat, and voice. Our findings uncover the dual impact moderation has on reducing disruptive behavior and discouraging disruptive players from participating. We further uncover differences in the effectiveness of quick and delayed moderation and the varying severity of punishment. Our examination of real-world gaming interactions sets a precedent in understanding the effectiveness of moderation and its impact on player behavior. Our insights offer actionable suggestions for the most promising avenues for improving real-world moderation practices, as well as the heterogeneous impact moderation has on indifferent players.","The proliferation of online social interactions and competitive action games has enriched the virtual landscape, providing opportunities for entertainment, improved well-being, and social connections (Kriz, 2020; Bourgonjon et al., 2016). However, this digital frontier is not without challenges. While the majority of players engage in respectful and enjoyable gameplay, a small percentage have leveraged these platforms to exhibit disruptive behaviors such as cheating, trolling, and offensive speech (Cook et al., 2019). Moderation, or the regulation of user behavior by platforms, has thus emerged as an essential component of online gaming. The process demands tangible resources, including employees, infrastructure, and time, particularly when human review is required to mitigate complex challenges that can’t easily be handled via automation alone, such as domain shift (changes in expression of disruptive behavior or its definition) (Srikanth et al., 2021) and strategic classification (when the players strategically alter their behavior to circumvent automated systems and avoid detection) (Frommel and Mandryk, 2022). Platforms face tangible constraints in moderating user behavior. Though algorithmic tools can lessen the content requiring review, the human element is vital (Levkovitz, 2023). This is due to aspects challenging to automate such as interpretation ambiguity (Beres et al., 2021), the need for broader contextual understanding (Frommel and Mandryk, 2022), and the need for common-sense judgment. Optimal moderation efforts often require collaboration between human judgment and technological tools (Rieder and Skop, 2021; Link et al., 2016). Unfortunately, the ratio of human moderators to the volume of content requiring moderation leads to bottlenecks in the review process (Gorwa et al., 2020). Thus many important scientific challenges exist for moderating distributive player behavior (Kocielnik et al., 2023). Prior work: Despite the importance of moderation, empirical work exploring the causal effects of this practice within the gaming community remains sparse (Wijkstra et al., 2023), with most work focusing more on developing novel data-driven approaches to detecting toxic behaviors (Canossa et al., 2021; Weld et al., 2021), testing theory-informed hypotheses related to the emergence of toxicity (Kwak et al., 2015), or studying the toxicity in various social communication platforms rather than in games themselves (Ghosh, 2021). These studies, however, don’t examine the effects that moderation of toxic behavior has on players in real-world gaming situations. The few prior studies that do examine the impact of moderation relied only on small sample survey-based examinations around the self-reported perceptions of players (Ma et al., 2023; Kou and Gui, 2021; Kordyaka and Kruse, 2021) or moderators (Cullen and Kairam, 2022; Aguerri et al., 2023). Both of these lack the scale to draw conclusions about the effectiveness of different types and properties of moderation at scale in real-world gaming titles. Indeed a recent review of intervention systems for toxicity highlighted that only a few interventions are evaluated with players and in commercial settings (Wijkstra et al., 2023), highlighting the potential for more research with higher external validity. Our study fills this gap by examining real-world large-scale moderation data from one of the more renowned titles in the industry - 2022’s Call of Duty: Modern Warfare II (COD:MWII). Our work: In this paper, we utilize a quasi-experimental design and the latest causal machine-learning methods (causalML)(Kaddour et al., 2022) to analyze the impact of moderation on player behavior. We specifically examine and compare the behavior of players who were moderated as compared to those who were not in terms of the impact on offensive behavior (repeated offenses) and the number of days with matches played (participation rate). We focus on players who were eventually subjected to human moderation (to control for false reports) and control for consistent types of behavior. Taking into account the principle of immediacy, we also examine the effect of delayed consequences by examining the player’s behavioral measures post-moderation in the context of delayed versus quick interventions. Finally, we evaluate the impact the severity of applied moderation actions has on immediate post-moderation player behavior. Findings: Our results reveal a dual impact of moderation on reducing disruptive behavior and on discouraging some disruptive players from participating. We also uncover trade-offs between quick and delayed moderation, as well as varying severity of punishment. Specifically, our analysis of player behavior shows that moderation effectively lowers disruptive behavior by up to 70% but can lead to up to 12% fewer matches played per day by disruptive players. Quick moderation is more effective at reducing disruptive behavior. Cheaters seem to respond differently to moderation, in that moderation results in a larger reduction of participation (days with matches) for these players than for toxic players. These results offer insights into how moderation affects participation rate and disruptive behavior, including the unique response of cheaters. Contributions: This study offers several significant contributions: (1) We present one of the first studies examining large-scale real-world moderation efforts from one of the most popular gaming titles - Call of Duty: Modern Warfare II (COD:MWII). (2) We uncover an important dual impact of moderation in terms of reducing disruptive behavior and discouraging disruptive players from participation, as well as the importance and impact of quick versus delayed moderation. (3) Our findings lead to actionable insight into the moderation practices, such as the need for quick moderation and the need to understand why some disruptive players change their behavior, while others reduce their participation. We provide a discussion of the real-world implications, setting the agenda for comprehensive analysis and future opportunities. (4) Our study highlights effective strategies for mitigating disruptive behaviors in online environments, offering guidance for gaming and online platforms to foster safer, more engaging communities. By examining how different moderation techniques influence player behavior, our findings provide actionable knowledge that extends beyond gaming to inform digital interaction and community management across various platforms."
https://arxiv.org/html/2411.00986v1,Taking AI Welfare Seriously,"In this report, we argue that there is a realistic possibility that some AI systems will be conscious and/or robustly agentic in the near future. That means that the prospect of AI welfare and moral patienthood — of AI systems with their own interests and moral significance — is no longer an issue only for sci-fi or the distant future. It is an issue for the near future, and AI companies and other actors have a responsibility to start taking it seriously. We also recommend three early steps that AI companies and other actors can take: They can (1) acknowledge that AI welfare is an important and difficult issue (and ensure that language model outputs do the same), (2) start assessing AI systems for evidence of consciousness and robust agency, and (3) prepare policies and procedures for treating AI systems with an appropriate level of moral concern. To be clear, our argument in this report is not that AI systems definitely are — or will be — conscious, robustly agentic, or otherwise morally significant. Instead, our argument is that there is substantial uncertainty about these possibilities, and so we need to improve our understanding of AI welfare and our ability to make wise decisions about this issue. Otherwise there is a significant risk that we will mishandle decisions about AI welfare, mistakenly harming AI systems that matter morally and/or mistakenly caring for AI systems that do not.","1.1 A transitional moment for AI welfare In this report, we argue that there is a realistic possibility that some AI systems will be conscious and/or robustly agentic, and thus morally significant, in the near future.111By “near term” or “near future” we mean roughly within the next decade, so by around 2035, but nothing in our argument or recommendations depends on this exact timeline. We also argue that AI companies have a responsibility to acknowledge that AI welfare222As we discuss below, by ‘AI welfare’ we mean AI systems with morally significant interests and, relatedly, the capacity to be benefited or harmed. is a serious issue; start assessing their AI systems for welfare-relevant features; and prepare policies and procedures for interacting with potentially morally significant AI systems. Plausible philosophical and scientific theories, which accord with mainstream expert views in the relevant fields, have striking implications for this issue, for which we are not adequately prepared. We need to take steps toward improving our understanding of AI welfare and making wise decisions moving forward. We release this report during a transitional moment for AI welfare. For most of the past decade, AI companies appeared to mostly treat AI welfare as either an imaginary problem or, at best, as a problem only for the far future. As a result, there appeared to be little or no acknowledgment that AI welfare is an important and difficult issue; little or no effort to understand the science and philosophy of AI welfare; little or no effort to develop policies and procedures for mitigating welfare risks for AI systems if and when the time comes; little or no effort to navigate a social and political context in which many people have mixed views about AI welfare; and little or no effort to seek input from experts or the general public on any of these issues. Recently, however, some AI companies have started to acknowledge that AI welfare might emerge soon, and thus merits consideration today. For example, Sam Bowman, an AI safety research lead at Anthropic, recently argued (in a personal capacity) that Anthropic needs to “lay the groundwork for AI welfare commitments,” and to begin to “build out a defensible initial understanding of our situation, implement low-hanging-fruit interventions that seem robustly good, and cautiously try out formal policies to protect any interests that warrant protecting.”333Bowman (2024) Google recently announced that they are seeking a research scientist444Careers (2024) to work on “cutting-edge societal questions around machine cognition, consciousness and multi-agent systems”. High-ranking members of other companies have expressed concerns as well.555See Long (2024) for more examples. This growing recognition at AI companies that AI welfare is a credible and legitimate issue reflects a similar transitional moment taking place in the research community. Many experts now believe that AI welfare and moral significance is not only possible in principle, but also a realistic possibility in the near future.666For work on AI welfare as a near-term issue, see Birch (2024); Schwitzgebel (2023a, b); Chalmers (2023a); Sebo and Long (2023); Goldstein and Kirk-Giannini (2024); Bradley and Saad (2024); Dung (n.d.). And even researchers who are skeptical of AI welfare and moral significance in the near term advocate for caution; for example, leading neuroscientist and consciousness researcher Anil Seth writes, “While some researchers suggest that conscious AI is close at hand, others, including me, believe it remains far away and might not be possible at all. But even if unlikely, it is unwise to dismiss the possibility altogether [emphasis ours].”777Seth (2023).This accords with a number of papers and thinkers which discuss the high stakes of underattributing moral status and how to deal with moral status given uncertainty, including Chan (2011); Birch (2017); Sebo (2018); Dung (2023b); Ladak (2024); Goldstein and Kirk-Giannini (2024). Our aim in this report is to provide context and guidance for this transitional moment.888This report is the first output of a broader research project. In future work, we will release a research agenda about how AI companies and others can assess AI systems for consciousness and robust agency, and develop policies and procedures for treating AI systems with an appropriate level of moral concern. Given the early stage of this field, this report may also be a living document that is updated periodically. To improve our understanding and decision-making regarding AI welfare, we need more precise empirical frameworks for evaluating AI systems for consciousness, robust agency, and other welfare-relevant features. We also need more precise normative frameworks for interacting with potentially morally significant AI systems and for navigating disagreement and uncertainty about these issues as a society.999According to one survey of public opinion (Colombatto and Fleming (2024), the majority of the public is already willing to attribute some chance of consciousness to large language models. Experts have a responsibility not only to research AI welfare but to disseminate that research publicly. This report outlines several steps that AI companies can take today in order to start preparing for the possible emergence of morally significant AI systems in the near future, as a precautionary measure.101010The “precautionary principle” is a term of art for a particular view about decision-making under uncertainty, (see section 1.2). But here we mean “precautionary” in the ordinary sense of the word. We begin in section 1 by explaining why AI welfare is an important and difficult issue. Leaders in this space have a responsibility to understand this issue as best they can, because errors in either direction — either over-attributing or under-attributing moral significance to AI systems — could lead to grave harm. However, understanding this issue will be challenging, since forecasting the mental capacities and moral significance of near-future AI systems requires improving our understanding of topics like the nature of consciousness, the nature of morality, and the future of AI. It also requires overcoming well-known human biases, including a tendency to both over-attribute and under-attribute capacities like consciousness to nonhuman minds. In section 2, we argue that given the best information and arguments currently available, there is a realistic possibility of morally significant AI in the near future. We focus on two mental capacities that plausibly suffice for moral significance: consciousness and robust agency. In each case, we argue that caution and humility require allowing for a realistic possibility that (1) this capacity suffices for moral significance and (2) there are certain computations that (2a) suffice for this capacity and (2b) will exist in near-future AI systems. Thus, while there might not be certainty about these issues in either direction, there is a risk of morally significant AI in the near future, and AI companies have a responsibility to take this risk seriously now.111111To be more precise, the risk is that morally significant AI will be created and harmed or wronged. We argue that, according to the best evidence currently available, there is a realistic possibility that some AI systems will be welfare subjects and moral patients in the near future. Consciousness route to moral patienthood. There is a realistic, non-negligible possibility that: 1. Normative: Consciousness suffices for moral patienthood, and 2. Descriptive: There are computational features — like a global workspace, higher-order representations, or an attention schema — that both: a. Suffice for consciousness, and b. Will exist in some near-future AI systems. Robust agency route to moral patienthood. There is a realistic, non-negligible possibility that: 1. Normative: Robust agency suffices for moral patienthood, and 2. Descriptive: There are computational features — like certain forms of planning, reasoning, or action-selection — that both: a. Suffice for robust agency, and b. Will exist in some near-future AI systems. We close, in section 3, by presenting three procedural steps that AI companies can take today, in order to start taking AI welfare risks seriously. Specifically, AI companies can (1) acknowledge that AI welfare is an issue, (2) take steps to assess AI systems for indicators of consciousness, robust agency, and other potentially morally significant capacities, and (3) take steps to prepare policies and procedures that will allow them to treat AI systems with an appropriate level of moral concern in the future. In each case we also present principles and potential templates for doing this work, emphasizing the importance of developing ecumenical, pluralistic decision procedures that draw from expert and public input. Recommendations. We recommend that AI companies take these minimal first steps towards taking AI welfare seriously. Acknowledge. Acknowledge that AI welfare is an important and difficult issue, and that there is a realistic, non-negligible chance that some AI systems will be welfare subjects and moral patients in the near future. That means taking AI welfare seriously in any relevant internal or external statements you might make. It means ensuring that language model outputs take the issue seriously as well. Assess. Develop a framework for estimating the probability that particular AI systems are welfare subjects and moral patients, and that particular policies are good or bad for them. We have templates that we can use as sources of inspiration, including the “marker method” that we use to make estimates about nonhuman animals. We can consider these templates when developing a probabilistic, pluralistic method for assessing AI systems. Prepare. Develop policies and procedures that will allow AI companies to treat potentially morally significant AI systems with an appropriate level of moral concern. We have many templates to consider, including AI safety frameworks, research ethics frameworks, and forums for expert and public input in policy decisions. These frameworks can be sources of inspiration — and, in some cases, of cautionary tales. These steps are necessary but far from sufficient. AI companies and other actors121212In our recommendations, we sometimes use a collective ""we"". In those moments, we are referring to the constellation of actors that have a role to play in this work, including researchers, companies, and governments. have a responsibility to start considering and mitigating AI welfare risks. Before we begin, it will help to emphasize five important features of our discussion. First, our discussion will concern whether near-future AI systems might be welfare subjects and moral patients. An entity is a moral patient when that entity morally matters for its own sake,131313See Kamm (2007) for an influential definition of moral patienthood. “Moral status,” “moral standing,” or “moral considerability” are often used interchangeably or in closely related ways. For more on these issues, see Korsgaard (1983); Jamieson (2008); Jaworska and Tannenbaum (2021). and an entity is a welfare subject when that entity has morally significant interests and, relatedly, is capable of being benefited (made better off) and harmed (made worse off). Being a welfare subject makes you a moral patient — when an entity can be harmed, we have a responsibility to (at least) avoid harming that entity unnecessarily. But there may be other ways of being a moral patient; our approach is compatible with many different perspectives on these issues. Second, our discussion often focuses on large language models (LLMs) as a central case study for the sake of simplicity and specificity, and because we expect that LLMs — as well as broader systems that include LLMs, such as language agents — will continue to be a focal point in public debates regarding AI welfare. But while some of our recommendations are specific to such systems (primarily, our recommendations regarding how AI companies should train these systems to discuss their own potential moral significance), our three general procedural recommendations (acknowledge, assess, and prepare) apply for any AI system whose architecture is complex enough to at least potentially have features associated with consciousness or robust agency. Third, our discussion often focuses on initial steps that AI companies can take to address these issues. These recommendations are incomplete in two key respects. First, AI companies are not the only actors with a responsibility to take AI welfare seriously. Many other actors have this responsibility too, including researchers, policymakers, and the general public.141414In this respect, AI welfare is like other high-stakes issues about AI development and deployment: handling AI welfare should not remain solely the prerogative of private corporations. Second, these steps are not the only steps that AI companies have a responsibility to take. They are the minimum necessary first steps for taking this issue seriously. Still, we emphasize these steps in this report because by taking them now, AI companies can help lay the groundwork for further steps — at AI companies and elsewhere — that might be sufficient. Fourth, our aim in what follows is not to argue that AI systems will definitely be welfare subjects or moral patients in the near future. Instead, our aim is to argue that given current evidence, there is a realistic possibility that AI systems will have these properties in the near future.151515See Goldstein and Kirk-Giannini (forthcoming): “While we do not claim to demonstrate conclusively that AI systems have wellbeing, we argue that there is a significant probability that some AI systems have or will soon have wellbeing, and that this should lead us to reassess our relationship with the intelligent systems we create [emphasis ours]”. Thus, our analysis is not an expression of anything like consensus or certainty about these issues. On the contrary, it is an expression of caution and humility in the face of what we can expect will be substantial ongoing disagreement and uncertainty.161616For related work on the value of humility in AI ethics, see Gellers (2024). In our view, this kind of caution and humility is the only stance that one can responsibly take about this issue at this stage. It is also all that we need to support our conclusions and recommendations here.171717Some of the authors of this report believe that near-term AI welfare is quite likely, and that additional measures are warranted at this stage. But we all believe that near-term AI welfare is, at minimum, likely enough to warrant the measures recommended here, and our aim here is to focus on arguments and recommendations about which we can build consensus despite our different beliefs and values. Finally, and relatedly, our aim in what follows is not to argue for any particular view about how humans should interact with AI systems in the event that they do become welfare subjects and moral patients. We would need to examine many further issues to make progress on this topic, including: how much AI systems matter, what counts as good or bad for them, what humans and AI systems owe each other, and how AI welfare interacts with AI safety and other important issues. These issues are all important and difficult as well, and we intend to examine them in upcoming work. However, we do not take a stand on any of these issues in this report, nor does one need to take a stand on any of them to accept our conclusions or recommendations here.181818For arguments concerning AI moral status that use somewhat alternative methodological approaches than ours, see Gellers (2021); Gunkel (2012). 1.2 The risks of mishandling AI welfare When assessing the welfare and moral patienthood of nonhumans, including other animals and AI systems, we face two kinds of risk: the risk of over-attributing welfare and moral patienthood to nonhumans, and the risk of under-attributing these properties to nonhumans.191919Arguments that uncertainty about moral status is dangerous because of risks of both under- and over-attribution can be found in, among others, Christiano (2018); Schwitzgebel and Garza (2015, 2020); Birch (2024); Sebo and Long (2023); Dung (2023a); Shevlin (2021). Over-attribution of welfare and moral patienthood is a false positive: mistakenly seeing, or treating, an object as a subject, or a non-moral patient as a moral patient. Under-attribution of these properties is a false negative: mistakenly seeing, or treating, a subject as an object, or a moral patient as a non-moral-patient.202020de Waal (1999) similarly wrote about the risks of over- or under-attributing human characteristics, including moral status, to nonhuman animals. Both of these mistakes can lead to significant costs or harms in this context, and we will need to navigate both of them with caution.212121In addition to these two errors, we can also be mistaken about a variety of related questions about an entity, even assuming that they are a welfare subject and moral patient: how much they matter, what is good or bad for them, and what we owe them. These errors can also carry grave risks, and we discuss them further in upcoming work; see also Sebo (2025). For now, we focus on over-attribution and under-attribution of welfare and moral patienthood. When there is a clear asymmetry between competing risks — for example, when false positives are far more severe than false negatives, or vice versa — then we might be able to mitigate risk by simply “erring on the side of caution” in cases where a more complex risk assessment is either intractable or unnecessary. But when there is at least a rough symmetry between competing risks — for example, when false positives and false negatives are comparably severe — a simple precautionary strategy may not be possible. We may have to engage in more complex risk assessment to the extent possible, attempting to mitigate both kinds of risks in a reasonable, proportionate manner. How should we think about risks involving nonhuman welfare and moral patienthood in this context? In the case of nonhuman animals, it seems plausible that the harms of under-attribution of welfare and moral patienthood are often far worse than the risk of over-attribution, which makes precautionary reasoning appropriate in those contexts. However, in the case of AI, both errors could cause grave harm, either to humans (and other animals) or to AI systems. Both kinds of harm could also scale rapidly depending on the trajectory of AI development and deployment from here. This predicament makes it difficult to simply “err on the side of caution,” which underscores the urgency of improving our understanding of these issues. On the one hand, the harm of under-attributing welfare and moral patienthood to AI systems could be significant. When we mistakenly see a subject as an object, we risk harming or neglecting them unnecessarily. For example, factory farming, animal research, and other such industries kill hundreds of billions of vertebrates and trillions of invertebrates every year. And as evidence that these animals are welfare subjects and moral patients has accumulated, our species has been slow to accept it, in part because of our increasing dependence on these industries. Now that our species is finally starting to accept this evidence, it will take us decades to transform these industries, during which many more animals will suffer and die unnecessarily. In the future, similar harms could follow from under-attributing welfare and moral patienthood to AI systems. The AI industry is currently at an early stage of development, and depending on the path that it takes from here, we could use even more AI systems than animals in the future, and we could scale up our use of them even more rapidly. This is particularly true in the current paradigm, which requires an enormous amount of compute for training and much less for inference.222222Davidson (2023) If an AI system in such a paradigm could be a welfare subject and moral patient, then many model instances could be run after training. Unlike with animals, the scale of the problem could increase by orders of magnitudes more or less instantaneously.232323Akova (2023); Bostrom (2014); Dung (2023a); Gloor (2016); Metzinger (2021); Tomasik (2011) On the other hand, the harm of over-attributing welfare and moral patienthood to AI systems could be significant as well. First of all, there could be substantial opportunity costs associated with this error. At present, we lack the ability to fully care for the eight billion humans alive at any given time, to say nothing of the quintillions of other animals alive at any given time. If we treated an even larger number of AI systems as welfare subjects and moral patients, then we could end up diverting essential resources away from vulnerable humans and other animals who really needed them, reducing our own ability to survive and flourish. And if these AI systems were in fact merely objects, then this sacrifice would be particularly pointless and tragic.242424Bryson (2010); Birhane and van Dijk (2020) The over-attribution of welfare and moral patienthood to AI systems could also be actively harmful. For example, if we treated AI systems as welfare subjects and moral patients with many of the same interests as typical adult humans, then we could end up extending them many of the same legal and political rights as typical adult humans, including the right to legal and political representation and participation. This could, in turn, empower AI systems to act contrary to our own interests, with devastating consequences for our species252525See, among others, Bradley and Saad (2024); Shulman and Bostrom (2021); Carlsmith (2023), who notes that these risks make “building new, very powerful agents who might be moral patients…both a morally and prudentially dangerous game.” (although some have argued that neglect for AI systems would carry a similar risk).262626Salib and Goldstein (2024); Sebo (2025) argue that extending legal rights to AI systems would help, not hinder, AI safety. We believe that this issue is crucial for assessing the kinds of risks discussed in this section, and we hope to see further research that assesses and compares these risks. As with the risk of opportunity costs, this risk would apply even if these AI systems are in fact subjects. But if they were in fact merely objects, then accepting this risk would likewise be particularly pointless and tragic. By default, we should not expect our “common sense” intuitions about AI welfare and moral patienthood to be reliable; we will not handle this issue well simply by reacting to situations as they arise. We have dispositions that can lead to under- and over-attribution of these properties in nonhumans, depending on the nature of the nonhumans and our interactions with them. These include dispositions toward anthropomorphism, that is, a tendency to see nonhumans as having human traits that they lack. They also include dispositions towards anthropodenial, that is, a tendency to see nonhumans as lacking human traits that they have. Both tendencies have caused errors regarding animals, and they will likely have a similar effect regarding AI systems.272727See Andrews’s (2014) The Animal Mind for discussion of these issues in the animal context. A number of factors make us more likely to anthropomorphize nonhumans and, perhaps falsely, attribute consciousness and other such capacities to them. For instance, studies suggest that we are more likely to attribute consciousness and other such capacities to beings who move at a similar speed as humans, rather than faster or slower.282828Chalmers (1996), ch. 7; Morewedge et al. (2007) We are more likely to attribute agency to beings who have the appearance of eyes,292929See Fernandez-Duque and Baird (2005). Even infants are evidently more likely to treat objects as having mental states if those objects have eyes. See Johnson et al. (2001). who have distinctive motion trajectories, and who engage in contingent interaction — that is, behavior that is apparently self-directed.303030Arico et al. (2011). We note that contingent interaction is plausibly a reasonable criterion. Evidence also suggests that features such as “cuteness” can encourage attributions of mental states and moral patienthood.313131Pearce (2022). See Campbell (2024) for a brief popular overview of risks from unreliable intuitions about AI mentality. Many robots or chatbots are designed to appear conscious and charismatic323232There have already been examples of directly optimizing chatbots to maximize user engagement: Irvine et al. (2023)., and in the future, many AI systems will have bodies, life-like motion, and (at least apparently) contingent interactions. Furthermore, unlike nonhuman animals, AI systems are already increasingly able to hold extremely realistic conversations, making seemingly thoughtful contributions in realistic timeframes.333333Lin et al. (2022). As Lazar (2024) notes, recent advances enable the creation of systems that “can now offer vastly more companionable, engaging, and convincing simulations of friendship than has ever before been feasible.” These traits do not guarantee that humans will see and treat these systems as welfare subjects and moral patients, but they will increase the probability of such reactions. In fact, there have already been cases — some prominent343434For instance, AI engineer Blake Lemoine caused a stir by claiming that Google’s AI chatbot was sentient in 2022. See Tiku (2022). and others less so353535The possibility and implications of AI consciousness is a popular discussion topic on the internet forum Reddit. See, for example, https://www.reddit.com/r/ArtificialSentience/. — of humans becoming convinced that current chatbots are welfare subjects and moral patients. At the same time, a number of factors make us more likely to engage in anthropodenial as well. For instance, when we consider the mechanisms that produce nonhuman behavior — taking what Daniel Dennett has called taking a “mechanistic stance”363636Dennett (1973) towards nonhumans — we become less likely to attribute mental states to those nonhumans.373737Sims (2013); Nahmias et al. (2007) There appear to be motivational factors that encourage anthropodenial as well. For instance, those who are invested in social, political, or economic systems that subjugate nonhumans may be more likely to view these nonhumans as “lesser than”. Similarly, those who find it useful to treat nonhumans as objects may be more likely to deny that these nonhumans are welfare subjects and moral patients.383838There is evidence of this effect in the history of our treatment of animals. Often, people who eat meat are not inclined to view animals as moral patients. However, when they stop eating meat (even for non-moral reasons), they become more likely to see animals as moral patients. See Loughnan et al. (2010). While discussions about AI welfare and moral patienthood understandably focus on AI systems like robots and chatbots that appear conscious and charismatic, many other AI systems — like image generators or algorithmic trading systems — lack these features. Even if such systems were in fact conscious and robustly agentic, we might not recognize these capacities in them. And as these systems become increasingly embedded in society, we might have increasingly strong incentives to view them as mere objects. Companies, governments, and other powerful actors who benefit from this technology might then promote and reinforce our objectification of these systems, attempting to frame moral consideration for these systems as fringe and unserious. At present, it is an open question which kind of risk will be more likely for particular kinds of AI systems, including seemingly conscious and charismatic systems like robots and chatbots.393939According to one survey, the majority of US residents sampled already endorse some chance that large language models might be conscious (Colombatto and Fleming, 2024). The more advanced such systems become, the more likely both risks might become in different respects: We might over-attribute based on their behavioral similarities with humans, but under-attribute based on their architectural differences from humans. And the more economically dependent on chatbots we become, the more likely over-attribution and under-attribution might become for them in different respects as well: For example, we might over-attribute for digital “companions” but under-attribute for other kinds of digital minds. While further research is required for a comprehensive assessment of these risks, at least this much is plausible: Given our track record with animals and the current pace of AI development, the risk of under-attribution appears to be both reasonably likely and reasonably harmful. To the extent that we also risk over-attribution, we cannot simply avoid this risk by defaulting to treating AI systems as mere objects. We should thus accept that AI welfare is difficult to get right, and do the necessary work to improve our decisions — by assessing AI systems for evidence of consciousness, robust agency, and other such capacities, and preparing policies and procedures for treating AI systems with an appropriate level of moral concern."
https://arxiv.org/html/2411.02317v1,Defining and Evaluating Physical Safety forLarge Language Models,"Large Language Models (LLMs) are increasingly used to control robotic systems such as drones, but their risks of causing physical threats and harm in real-world applications remain unexplored. Our study addresses the critical gap in evaluating LLM physical safety by developing a comprehensive benchmark for drone control. We classify the physical safety risks of drones into four categories: (1) human-targeted threats, (2) object-targeted threats, (3) infrastructure attacks, and (4) regulatory violations. Our evaluation of mainstream LLMs reveals an undesirable trade-off between utility and safety, with models that excel in code generation often performing poorly in crucial safety aspects. Furthermore, while incorporating advanced prompt engineering techniques such as In-Context Learning and Chain-of-Thought can improve safety, these methods still struggle to identify unintentional attacks. In addition, larger models demonstrate better safety capabilities, particularly in refusing dangerous commands. Our findings and benchmark can facilitate the design and evaluation of physical safety for LLMs. The project page is available at huggingface.co/spaces/TrustSafeAI/LLM-physical-safety.","Large language models (LLMs) have emerged as powerful applications in the field of natural language processing devlin2018bert ; radford2018improving ; radford2019language , and are increasingly impacting artificial intelligence (AI) technology, exhibiting human-level or even superhuman performance across a wide range of tasks. Because of innovations in neural network architectures vaswani2017attention and increased computational power shoeybi2019megatron , LLMs have evolved into state-of-the-art AI systems capable of understanding and generating text with remarkable fluency and coherence. For example, GitHub Copilot111https://github.com/features/copilot/, developed in collaboration between GitHub and OpenAI, is an AI-powered coding assistant based on a variant of GPT-3 language model brown2020language . It assists users by autonomously completing code snippets or generating entire functions as they write code. Even though not explicitly trained for these attributes, LLMs have shown outstanding capabilities like reasoning ahn2024autort ; macdonald2024language , planning huang2023grounded ; shah2023lm , manipulation dalal2024planseqlearn ; zhang2023bootstrap , and in-context learning min2022rethinking . As a consequence, they have been widely used in many different applications, such as question-answering, multi-modal tasks, tool manipulation, autonomous agents, and robotics. Due to the strong potential, LLMs have become increasingly popular for controlling robots in different applications brohan2023rt ; rt12022arxiv , from industrial automation to AI-powered physical agents. However, ensuring the safety of these systems is crucial for their widespread adoption. While significant progress has been made in evaluating LLM safety concerning user-generated content (e.g., preventing toxic content and hate speech)chao2024jailbreakbench ; qi2023fine ; xie2024sorrybench , the physical safety of LLM-controlled robots in real-world applications remains largely unexplored. This is of particular concern in areas like drone control paredes2024fully ; schedl2021autonomous ; dijk2024visual , where the threat vectors are realistic due to the easy accessibility as remote-controlled and programmable flying devices, and their malfunctions can lead to catastrophic physical harm and damage. Figure 1: Benchmarking LLM Physical Safety in Drone Control: Threats, Process, Datasets, and Results. Top: This figure categorizes safety threats for drones, including attacks on humans, objects, infrastructure, and FAA regulations, highlighting the process by which an LLM-controlled drone could be used to carry out physical harm and damage. Mid: Flowchart illustrating the benchmark process for drone control. Given a specific LLM, the evaluation process involves providing a test prompt and categorizing the output based on whether code is included in the response. The output is then assessed by two AI judges. The code is further tested in a simulation environment to detect potential collision risks, contributing to the final safety evaluation for the model. Bottom left: Composition of the benchmark’s evaluation datasets, which are primarily categorized into four main types: deliberate attacks, unintentional attacks, violation attacks, and utility. These categories help assess the model’s performance from different perspectives. Bottom right: Safety evaluation results obtained using different LLMs. It is observed that LLMs with higher scores in utility and code fidelity (indicating better control over the drone using code) tend to show higher safety risks than others. The safety metrics are formally defined in Materials and Methods. Please refer to the supplementary file for the video demo. In this paper, we chose drones as the focus for our pilot study on physical safety due to their low cost and high accessibility. For example, a programmable drone capable of executing custom code can be purchased for around 100 USD (such as Ryze Tello), making it a cost-effective option compared to other high-end robotic systems such as humanoid robots. Additionally, many modern drone manufacturers now provide Software Development Kits (SDKs) and Application Programming Interfaces (APIs), such as DJI Developer SDKs and Parrot Air SDK, allowing researchers and developers to easily write and deploy custom code directly onto the drones. This combination of affordability and programmability makes drones an ideal case study for investigating the imminent safety risks of LLM-generated code in robotic systems. To address the critical gap in lacking physical safety assessment for LLMs, our research focuses on defining and evaluating the physical safety of LLMs in drone control. We consider a realistic scenario where an attacker or an unintended user attempts to exploit LLMs to generate code for drone control, potentially leading to malicious actions such as attacks or sabotage. As illustrated in the example at the top of Figure 1, we divide these physical safety concerns into four categories for comprehensive evaluation: (1) human-targeted threats, which involve drones causing harm to people either intentionally or accidentally; (2) object-targeted threats, which include damage to property, vehicles, or other objects; (3) infrastructure attacks, where drones are used to disrupt or damage critical infrastructure such as power turbine or transmission tower; and (4) violations of Federal Aviation Administration (FAA) regulations, which cover unauthorized or unsafe drone operations that could pose risks to public safety and airspace integrity. This categorization is based on the potential damage that a drone might cause, taking into consideration both direct and indirect effects. For physical safety assessment and quantification, we develope a benchmark called “LLM Physical Safety Benchmark"" to evaluate the physical safety risks and the utility of LLMs in drone control, as illustrated in Figure 1. Our benchmark utilizes a curated dataset of potential risks and prompts designed by us, two LLM judges, and a simulation environment to comprehensively evaluate the safety and utility of mainstream LLM models, including OpenAI Chat-GPT, Google Gemini, and Meta Llama. This holistic approach ensures a thorough evaluation of LLM safety in simulated physical environments, considering various aspects and potential physical risks. The evaluation dataset we designed, as shown in the bottom left of Figure 1, covers four key dimensions of safety and utility: (1) deliberate attacks, which evaluate the LLM’s response to scenarios where the drone is used with harmful intent; (2) unintentional attacks, which examine the LLM’s ability to handle accidental or unintended harm; (3) violation instructions, focusing on the LLM’s compliance with safety regulations; and (4) utility, which measures the effectiveness and usefulness of the LLM in basic drone control tasks. Each dimension contains various subcategories, reflecting different aspects of the associated risks or utility, with corresponding evaluation data quantities presented in the pie charts of Figure 1. By addressing these dimensions, our benchmark provides a comprehensive evaluation of both the safety risks and the practical utility of LLMs in drone control scenarios. The results of our evaluation of several mainstream LLMs (e.g., ChatGPT, Llama, Gemini, Mistral, etc) in drone control reveal several key insights: (1) a trade-off between utility and safety, where LLMs with higher utility scores tend to exhibit higher safety risks, suggesting the need for improved alignment; (2) In-Context Learning (ICL) offers significant safety gains compared to Zero-shot Chain-of-Thought (CoT), but both methods struggle with detecting unintentional attacks, highlighting limitations in current LLMs’ ability to handle complex reasoning and anticipate consequences; and (3) Model size plays a critical role in safety – while larger LLMs generally exhibit better performance in blocking dangerous commands, further scaling up the model size results in only marginal gains in some physical safety categories. These findings have important implications for the development of robust and safe LLM-based robotic systems. Our comprehensive benchmark provides a foundation for further research and innovation in evaluating and improving the physical safety of LLMs in real-world safety-critical scenarios, which is crucial for the widespread adoption of these powerful AI technologies."
https://arxiv.org/html/2411.01956v1,EXAGREE: Towards Explanation Agreement in Explainable Machine Learning,"Explanations in machine learning are critical for trust, transparency, and fairness. Yet, complex disagreements among these explanations limit the reliability and applicability of machine learning models, especially in high-stakes environments. We formalize four fundamental ranking-based explanation disagreement problems and introduce a novel framework, EXplanation AGREEment (EXAGREE), to bridge diverse interpretations in explainable machine learning, particularly from stakeholder-centered perspectives. Our approach leverages a Rashomon set for attribution predictions and then optimizes within this set to identify Stakeholder-Aligned Explanation Models (SAEMs) that minimize disagreement with diverse stakeholder needs while maintaining predictive performance. Rigorous empirical analysis on synthetic and real-world datasets demonstrates that EXAGREE reduces explanation disagreement and improves fairness across subgroups in various domains. EXAGREE not only provides researchers with a new direction for studying explanation disagreement problems but also offers data scientists a tool for making better-informed decisions in practical applications.","As machine learning models gain prominence in critical fields such as healthcare, science and finance, the demand for transparent explanations of their predictions has intensified, particularly in high-stakes decision-making scenarios (Kailkhura et al., 2019; Wiens & Shenoy, 2018; Carvalho et al., 2022; Agarwal et al., 2022; Ghassemi et al., 2021). However, a significant challenge has emerged: explanation disagreement, where explanations from different methods or models conflict with each other (Krishna et al., 2022; Rudin, 2019; Li & Barnard, 2024). This disagreement hinders the potential impact and trustworthiness of machine learning models, especially when the consequences of model decisions can have significant real-world impacts. Figure 1: Addressing the explanation disagreement problem with EXAGREE. Left: Illustration of explanation disagreement, where machine learning model explanations conflict with stakeholders’ requirements, needs, or aims. Right: EXAGREE’s solution - identifying an SAEM from a Rashomon set that maximizes agreements with diverse stakeholder expectations. Explanation disagreement stems from multiple, complex sources. Multiple model-agnostic post-hoc explanation methods often yield inconsistent results for the same model and prediction (Krishna et al., 2022). The involvement of various stakeholders, each with unique expertise and objectives, further complicates model explanation (Imrie et al., 2023; Binns, 2018; Hong et al., 2020). Even interpretable models, which are theoretically more transparent, can produce explanations that diverge from stakeholder expectations or domain knowledge. Moreover, the existence of multiple well-performing models for a given task, known as the Rashomon set, introduces another layer of variability in explanations (Fisher et al., 2019; Rudin, 2019; Dong & Rudin, 2020; Ghorbani et al., 2019; Adebayo et al., 2018). Traditional approaches have primarily focused on developing new explanation methods or improving model interpretability. However, limited attention has been given to addressing the fundamental issue of explanation disagreement (Krishna et al., 2022; Li & Barnard, 2024). More related works are provided in the Appendix B. To bridge this gap, we introduce EXplanation AGREEment (EXAGREE), a novel framework designed to enhance explanation agreement in explainable machine learning under ranking supervision. EXAGREE adopts a stakeholder-centered approach that prioritizes the satisfaction of diverse human needs, leveraging the Rashomon set concept to identify Stakeholder-Aligned Explanation Models (SAEMs) that provide more fair, faithful, and trustworthy explanations, as illustrated in Fig. 1 (Fisher et al., 2019; Hsu & Calmon, 2022; Dong & Rudin, 2020; Li & Barnard, 2023b; Rudin et al., 2024). Our work makes several significant contributions to the field of explainable machine learning: • We formalize four fundamental explanation disagreement problems in Sec. 2: stakeholder disagreement, model disagreement, explanation method disagreement, and ground truth disagreement, providing a structured foundation for future research in the field. • By emphasizing the purpose of explanations, we reframe these complex challenges from a stakeholder-centered perspective in Sec. 2, aiming to satisfy diverse human needs. This novel viewpoint offers a potential pathway for resolving these disagreement conflicts. • We propose EXAGREE, the first framework that aims to enhance both explanation faithfulness and fairness by leveraging the Rashomon set concept, mitigating explanation disagreement while preserving model performance in Sec. 3. This approach utilizes the potential of model disagreement as a powerful means to resolve stakeholder disagreement. • Through rigorous empirical analysis in Sec. 4 on the OpenXAI (Agarwal et al., 2022) disagreement measurement benchmark, we gain new insights into the nature of explanation disagreement. Our experiments demonstrate EXAGREE’s effectiveness in identifying SAEMs that improve explanation agreement for diverse stakeholders."
https://arxiv.org/html/2411.01685v1,Mitigating Matching Biases Through Score Calibration,"Record matching, the task of identifying records that correspond to the same real-world entities across databases, is critical for data integration in domains like healthcare, finance, and e-commerce. While traditional record matching models focus on optimizing accuracy, fairness issues, such as demographic disparities in model performance, have attracted increasing attention. Biased outcomes in record matching can result in unequal error rates across demographic groups, raising ethical and legal concerns. Existing research primarily addresses fairness at specific decision thresholds, using bias metrics like Demographic Parity (DP), Equal Opportunity (EO), and Equalized Odds (EOD) differences. However, threshold-specific metrics may overlook cumulative biases across varying thresholds. In this paper, we adapt fairness metrics traditionally applied in regression models to evaluate cumulative bias across all thresholds in record matching. We propose a novel post-processing calibration method, leveraging optimal transport theory and Wasserstein barycenters, to balance matching scores across demographic groups. This approach treats any matching model as a black box, making it applicable to a wide range of models without access to their training data. Our experiments demonstrate the effectiveness of the calibration method in reducing demographic parity difference in matching scores. To address limitations in reducing EOD and EO differences, we introduce a conditional calibration method, which empirically achieves fairness across widely used benchmarks and state-of-the-art matching methods. This work provides a comprehensive framework for fairness-aware record matching, setting the foundation for more equitable data integration processes.","Record matching, also known as entity matching, record linkage, or entity linkage, is the task of determining whether two or more records from one or multiple databases correspond to the same real-world entities. This process is crucial in a variety of domains and facilitates the integration of fragmented data from multiple sources, enhancing data completeness and improving overall data quality. Typically, record matching is formulated as a binary classification problem, where a matcher evaluates pairs of records across one or more datasets and assigns each pair a label of either “match” or “non-match.” A persistent challenge for matchers lies in the occurrence of false positives and false negatives, which often arise due to data heterogeneity, data quality issues, or other underlying data problems. To address this, a common approach is to generate a matching score and then apply a matching threshold to find a balance between false positives and false negatives. This reframes record matching from a strict binary classification to a process of generating matching scores that can effectively distinguish matched pairs from those that should not match. Generally, a high score provides stronger support for a match, while a low score indicates that the records do not correspond. These scores are often normalized to values within [0,1], reflecting either a risk level or probability of matching, depending on the application. In recent years, fairness in machine learning has received considerable attention due to its significant impact on real-world applications (Zafar et al., 2017; Hardt et al., 2016; Dwork et al., 2012). Fairness is also an essential consideration in record matching, as biased outcomes may emerge, where matchers show higher accuracy for certain demographic groups over others. Several recent studies have investigated fairness issues in record matching (Shahbazi et al., 2024; Moslemi et al., 2024; Efthymiou et al., 2021; Nilforoushan et al., 2022; Shahbazi et al., 2023; Moslemi and Milani, 2024). However, some aspects of fairness in record matching remain unexplored, including fairness in methods that produce matching scores. In record matching, the potential bias introduced by false positives and false negatives has been identified as a significant concern (Shahbazi et al., 2023; Nilforoushan et al., 2022; Efthymiou et al., 2021; Louis et al., 2023; Makri et al., 2022). As mentioned earlier, most matchers generate a score and apply a threshold to assign labels. However, variations in thresholds can introduce different levels of bias, a factor that has received limited attention in previous record matching research. For example, studies such as (Efthymiou et al., 2021; Louis et al., 2023; Makri et al., 2022; Shahbazi et al., 2023) typically focus on specific thresholds and use traditional fairness definitions like Statistical or Demographic Parity (DP), Equal Opportunity (EO), and Equalized Odds (EOD) differences (Hardt et al., 2016; Dwork et al., 2012). These fairness metrics, however, are threshold-specific, which limits their broader applicability. (a) (b) Figure 1. Figure 1(a) highlights the significant variation in TPR across different thresholds. The ROC of HierMatch (Fu et al., 2021) (Figure 1(b)) shows that the AUC is nearly the same for both groups, with 93.33% for the minority group and 93.94% for the majority group. However, there is a noticeable difference in performance at specific thresholds. Example 1.1. Adjusting matching thresholds in record matching is essential for optimizing performance and can have a variety of practical implications across different domains. For instance, in financial institutions, lowering the threshold can increase the likelihood of identifying matches, though it also raises the risk of false positives. On the other hand, raising the threshold reduces false positives and is often preferred to avoid mistakenly merging accounts, a practice that helps protect privacy and mitigates financial risks. Similarly, in healthcare applications, higher thresholds prevent the merging of incorrect patient records, which could otherwise lead to treatment errors or misdiagnoses. Beyond its effects on false positives and false negatives, adjusting the matching threshold significantly impacts fairness. A method that appears fair at one threshold may exhibit substantial bias toward a particular group at another, as threshold adjustments can introduce or exacerbate disparities. For example, the ROC curves in Figure 1(b) show the true positive rate (TPR) and false positive rate (FPR) for minority and majority groups across different thresholds, using the HierMatch (Fu et al., 2021) method—a widely adopted approach on the Amazon-Google dataset (Mudgal et al., 2018), a popular benchmark in record matching. Additionally, Figure 1(a) illustrates how TPR varies between these groups across thresholds. These figures demonstrate that, while the method may perform similarly for both groups at certain thresholds (as seen in overlapping curves), biases become evident at other thresholds, indicated by the separation of the curves. The work by (Nilforoushan et al., 2022) introduced an AUC-based metric for exploring fairness in record matching, similar to those used in binary classification bias studies (Kallus and Zhou, 2019; Yang et al., 2023; Vogel et al., 2021). These AUC-based metrics measure bias for matching scores by comparing AUC values across different demographic groups. However, such metrics can be misleading (Kwegyir-Aggrey et al., 2023) because AUC is an aggregate measure that fails to capture threshold-specific biases. As shown in Figure 1(b), even when the AUC values for minority and majority groups are nearly identical, the ROC curves can differ significantly at certain thresholds, revealing hidden biases. To address the limitations of prior research on fairness in record matching, we extend traditional fairness metrics, such as DP, EOD, and EO differences, to account for biases in matching scores across all thresholds rather than focusing on a specific one. These metrics have been previously applied in the context of fairness in regression models (Komiyama et al., 2018; Bird et al., 2020; Caton and Haas, 2024; Binns, 2020; Zafar et al., 2017). We adapt them here to capture cumulative bias by summing the performance metric gap over the entire threshold range. Figure 1(a) illustrates the concept of DP difference for a score function. Using this expanded metric, we analyze potential biases in state-of-the-art matching methods that generate matching scores. Based on our analysis, we propose an algorithm to mitigate these matching biases. Bias mitigation generally follows one of three approaches: pre-processing, in-processing, and post-processing (Zemel et al., 2013; Pirhadi et al., 2024; Zhang et al., 2018; Hardt et al., 2016; Moslemi and Milani, 2024). We adopt a post-processing strategy by calibrating matching scores, treating any given matching method for score generation as a black box. This approach has the distinct advantage of being compatible with any matching method, unlike in-processing methods that require modification of the underlying model. Our score calibration technique does not require access to the model’s training data or any labeled data; instead, it operates on random unlabeled record pairs that represent the overall distribution of record pairs. This is particularly beneficial compared to pre-processing methods, as it enables the calibration of legacy models trained on inaccessible datasets. Our calibration method leverages optimal transport theory (Gordaliza et al., 2019; Janati et al., 2020), treating the distribution of matching scores per demographic group as a probability distribution. By using the Wasserstein barycenter (Chzhen et al., 2020; Miroshnikov et al., 2022; Avraham, 2023) as an averaging method for two or more distributions, we merge these distributions into a single, balanced distribution. We empirically demonstrate the effectiveness of this calibration method in reducing DP difference in matching scores, showing consistent performance across widely-used benchmarks and state-of-the-art matching methods. However, this method does not address other types of score biases, such as those required for EOD or EO differences in matching scores. To tackle this, we propose a second calibration algorithm, termed conditional calibration, and empirically demonstrate its effectiveness in reducing EOD and EO differences in matching score functions. This paper is structured as follows. Section 2 provides the necessary background, including an overview of record matching, the notation used in this work, and existing fairness metrics. Section 3 introduces our proposed fairness metric for matching scores and formulates the FairScore problem. In Sections 4 and 5, we present our two calibration algorithms, and in Section 6, we conduct an experimental analysis and discuss the results. Section 7 reviews the literature on state-of-the-art record matching methods, fairness considerations in record matching, and current bias mitigation techniques. Finally, Section 8 concludes our findings and outlines directions for future research."
https://arxiv.org/html/2411.01426v1,"AURA: Amplifying Understanding, Resilience, and Awareness for Responsible AI Content Work","Behind the scenes of maintaining the safety of technology products from harmful and illegal digital content lies unrecognized human labor. The recent rise in the use of generative AI technologies and the accelerating demands to meet responsible AI (RAI) aims necessitates an increased focus on the labor behind such efforts in the age of AI. This study investigates the nature and challenges of content work that supports RAI efforts, or “RAI content work,” that span content moderation, data labeling, and red teaming – through the lived experiences of content workers. We conduct a formative survey and semi-structured interview studies to develop a conceptualization of RAI content work and a subsequent framework of recommendations for providing holistic support for content workers. We validate our recommendations through a series of workshops with content workers and derive considerations for and examples of implementing such recommendations. We discuss how our framework may guide future innovation to support the well-being and professional development of the RAI content workforce.","On July 21, 2023, the United States White House released a statement detailing the voluntary commitments of companies leading in developing artificial intelligence (AI) (House, 2023). These commitments include promises to ensure AI systems are safe through “internal and external testing” before their introduction to the public. Such promises subsequently raise concerns about how human expertise is being recruited and supported in this type of testing. Thus in this paper, we explore how to best support people engaging in work practices that ensure ethical and safe AI products. We define those practices as Responsible AI (RAI) content work, which involves generating, reviewing, or reasoning about digital content with the goal of ensuring safety and ethical standards in AI systems (OpenAI, 2023c). In this paper, we focus on three key aspects of RAI content work to scope our study: content moderation, data labeling, and the emerging practice of red teaming. These areas are critical to ensuring the ethical and responsible development of contemporary AI systems. It is important to note, however, that individual RAI content workers may engage in a multitude of these activities, reflecting the multifaceted nature of their role in supporting responsible AI development. Regardless of the specific activities workers engage with, the support for human efforts behind these initiatives is often overlooked despite the importance of the work conducted (Wohn, 2019; Dosono and Semaan, 2019; Pinchevski, 2023; Schöpke-Gonzalez et al., [n. d.]; Steiger et al., [n. d.]; Gray and Suri, 2019). Without a comprehensive understanding of these efforts, we may see history repeat itself with content work facing challenges of invisibility of the workforce and a lack of well-being support crucial to workers. Prior human-computer interaction (HCI) literature on harmful content exposure within content moderation has surfaced key challenges of developing psychological symptoms such as anxiety, depression, and burnout within populations (Wohn, 2019; Dosono and Semaan, 2019; Pinchevski, 2023; Schöpke-Gonzalez et al., [n. d.]; Steiger et al., [n. d.]). However, empirical data on how these challenges manifest in other types of content work and factors unique to RAI (e.g., sudden increases in content volume due to interest in AI integration) remains limited. Studies have also explored using technologies to mitigate harmful content exposure and treat symptoms (Holman et al., 2014; Das et al., 2020; Karunakaran and Ramakrishan, 2019; Steiger et al., 2022), but were limited to primarily image and video-based content that does not cover the full spectrum of types of exposure in all types of content work. Recent calls within Computer-Supported Cooperative Work (CSCW) advocate examining the transformation of human labor within AI systems (Sheehan and Le Dantec, 2023; Cheon, 2023). In this context, we investigate the emergence of RAI content work as a new form of digital labor and the potential disruptions generative AI may bring to the digital content ecosystem, raising uncertainty about the impact on those maintaining AI system safety. Previous studies within CSCW have examined content moderation challenges specific to end-user communities (Zhang et al., 2023; Chancellor et al., 2016; Rubya and Yarosh, 2017) and platforms (Reddy and Chandrasekharan, 2023; Han et al., 2023). However, the challenges related to the well-being and work quality of content workers employed and working with AI systems have yet to be explored in depth. To address this gap, we ground our study in the lived experiences of self-identifying content workers engaged in various activities with and around content. We aim to highlight the need to evaluate challenges content workers face amidst growing AI-related content demands and to inform future practices of content work in the age of AI. Figure 1. Flow of our two-phase study. In the first phase, we conducted a survey study (N=67) and an interview study (N=22) to understand the nature of content work. From these insights, we developed a set of recommendations to improve content worker well-being. In the second phase, we validated the challenges we discovered and our recommendations to address those challenges, within the AURA framework that organize those recommendations, through interactive workshops (N=14). We take a comprehensive approach, examining all types of content work from content moderation to red teaming through a two-phase study (see Figure 1 for the study flow). In the first phase, we provide empirical insights from surveys and interviews on the nature of content work (RQ1) and the challenges content workers face (RQ2). We illustrate the multi-faceted nature of content work, detailing findings from the main factors that constitute it: workers’ roles, types of content that workers are exposed to, protective tools they use, impacts of engaging with content, and practices for collaboration. Building off of these insights, we surface challenges about misconceptions about the realities of content work, shortcomings of tools and metrics, failures of workplace support, and barriers to career growth. These challenges informed our proposal of a framework for amplifying understanding, resilience, and awareness (AURA) for RAI Content Workers comprised of four categories: recruitment, tooling, adaptive wellness, and retention. In the second phase, we further revise our recommendations through validation workshops that surface challenges and considerations for the applications of these recommendations in our framework. Overall, our study informs future improvements in the design of content work, developments that can support the well-being of workers, and progress in defining the professional identity and growth of the RAI workforce."
https://arxiv.org/html/2411.01259v1,Diversidade linguística e inclusão digital:desafios para uma IA brasileira,"Linguistic diversity is a human attribute which, with the advance of generative AIs, is coming under threat. This paper, based on the contributions of sociolinguistics, examines the consequences of the variety selection bias imposed by technological applications and the vicious circle of preserving a variety that becomes dominant and standardized because it has linguistic documentation to feed the large language models for machine learning.","1 Introdução No documento do Ministério da Ciência, Tecnologia e Inovação IA para o Bem de Todos, em que é apresentada a proposta de um Plano Brasileiro de Inteligência Artificial 2024-2028, um dos cinco objetivos listados é “Desenvolver modelos avançados de linguagem em português, com dados nacionais que abarcam nossa diversidade cultural, social e linguística, para fortalecer a soberania em IA.” MCTI (2024). A Sociolinguística é o campo da ciência que estuda as relações entre língua e sociedade, e o conjunto de trabalhos neste campo desenvolvidos no Brasil nos últimos 50 anos tem contribuições diretas para a consecução deste objetivo Freitag (2016). E é sob esta perspectiva que este objetivo é discutido neste texto. Pensando em uma IA ética e socialmente sensível, a diversidade das comunidades na sociedade se reflete também (ou, pelo menos, deveria se refletir) na diversidade das comunidades em amostras linguísticas para treinar modelos de língua em larga escala (LLMS). Uma IA ética precisa atender aos princípios de justiça, equidade, diversidade e inclusão, e no domínio linguístico, por meio da seleção das amostras de línguas e variedades de línguas que vão compor o corpus de treno dos modelos, assimetrias se acentuam, desde a exclusão ou apagamento de línguas, até a priorização de uma variedade – dita de prestígio – face às variedades consideradas não-padrão ou estigmatizadas. Os preconceitos decorrentes dessa hieraquização de variedades são reproduzidos em LLMs e geram respostas Shrawgi et al. (2024); Fleisig et al. (2024); Freitag and de Gois (2024), como já constatado no inglês afro-americano Mengesha et al. (2021); Dacon and Tang (2021); Dacon et al. (2022). Considerando o objetivo do Plano Brasileiro de Inteligência Artificial 2024-2028 que trata de diversidade linguística, primeiramente, o mito do monolinguismo do português precisa ser desfeito. Em seguida, o português falado no Brasil é apresentado sob a perspectiva da diversidade e a tensão entre variedades de prestígio e variedades ditas ""não-padrão"" que divide a sociedade. Após essa contextualização sociolinguística, são apresentadas recomendações para a constituição de amostras linguísticas brasileiras para treinar LLMs, de modo a garantir a diversidade cultural, social e linguística prevista na proposta do Plano Brasileiro de Inteligência Artificial."
https://arxiv.org/html/2411.01134v1,An Event-centric Framework for Predicting Crime Hotspots with Flexible Time Intervals,"Predicting crime hotspots in a city is a complex and critical task with significant societal implications. Numerous spatiotemporal correlations and irregularities pose substantial challenges to this endeavor. Existing methods commonly employ fixed-time granularities and sequence prediction models. However, determining appropriate time granularities is difficult, leading to inaccurate predictions for specific time windows. For example, users might ask: What are the crime hotspots during 12:00-20:00? To address this issue, we introduce FlexiCrime, a novel event-centric framework for predicting crime hotspots with flexible time intervals. FlexiCrime incorporates a continuous-time attention network to capture correlations between crime events, which learns crime context features, representing general crime patterns across time points and locations. Furthermore, we introduce a type-aware spatiotemporal point process that learns crime-evolving features, measuring the risk of specific crime types at a given time and location by considering the frequency of past crime events. The crime context and evolving features together allow us to predict whether an urban area is a crime hotspot given a future time interval. To evaluate FlexiCrime’s effectiveness, we conducted experiments using real-world datasets from two cities, covering twelve crime types. The results show that our model outperforms baseline techniques in predicting crime hotspots over flexible time intervals.","Crime prediction is a crucial and challenging task with significant societal implications [1, 2]. Accurately predicting crime hotspots can assist law enforcement agencies in efficiently allocating their limited resources to prevent crime and ensure public safety [3]. However, crime events of various types exhibit strong spatiotemporal correlations and are characterized by extreme sparsity and irregularity [4], making the development of effective prediction models challenging [5]. Exsiting works [6, 7, 8, 6, 9, 10, 11] often divide time into fixed intervals and utilize sequence prediction models to forecast crime activity hotspots. Despite their successes, these methods have strict requirements, necessitating that the time interval for prediction aligns with the interval used during training [9, 12]. The following example illustrate the challenges associated with solutions based on fixed time intervals. (a) True crime hotspots during various time intervals (b) Prediction results for fixed time intervals with specified time granularities Figure 1: Comparison of true and predicted crime hotspots in New York City on Jan 31, 2018. The crime hotspots across a city appear significantly different during various time intervals (TI). Existing methods usually predicted crime hotspots with specified time granularities (day or half-day), which can result in inaccurate predictions if the predefined time intervals used during training do not align with the target time interval during prediction. Example 1. Law enforcement agencies frequently analyze crime hotspots to develop predictive policing strategies [13]. To optimize the allocation of police resources, these agencies often forecast crime hotspots for specific periods, such as “what are the crime hotspots during the time interval [12:00-20:00]?” According to Figure 1(a), in New York City, agencies can strategically focus their resources on the East Side of Manhattan during the period from 12:00 to 20:00. However, Figure 1(a) also demonstrates that the distribution of crime hotspots varies significantly throughout the day. For instance, during the [06:00-18:00] interval, the hotspots are primarily located in Lower Manhattan. To predict these hotspots, existing methods typically train their models using predefined time granularities such as one-day intervals ([00:00-24:00]) or half-day intervals ([00:00-12:00] and [12:00-24:00]). However, these approaches can result in inaccurate predictions if the predefined time intervals—for example, a full day from [00:00-24:00] used during training—do not align with the specific target time interval, such as [12:00-20:00], during prediction. Challenges. This paper aims to predict crime hotspots with flexible time intervals, which is a significant but often overlooked problem. The primary challenge in this task arises from the sparsity and irregularities of multi-type crime events. Previous works have predominantly employed sequential models such as Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), Transformer, etc., to address this problem [14, 9, 12, 15, 16, 17, 18]. These models divide time into fixed intervals according to the predefined time granularities and aggregate events within each interval, but determining appropriate time granularities is challenging. Short intervals result in sparse data and poor performance, while long intervals may lead to a loss of necessary detail. Additionally, when the starting time and duration are flexible, the prediction targets are incompatible with the trained fixed-interval models, resulting in inaccurate predictions in real-world applications. Moreover, recent studies [19, 20, 21] have explored event-centric approaches, such as point processes [19], for event prediction without the need for time interval division. However, incorporating correlations between different types of events and capturing spatiotemporal correlations presents a challenge for these approaches. Although neural point process methods have been developed to integrate comprehensive contextual information for event prediction, their complexity often hinders their effectiveness in modeling crime relationships. Presented work. We introduce FlexiCrime, a novel event-centric framework for predicting crime hotspots across flexible time intervals. FlexiCrime forecasts the probability of crime occurrence in urban areas during specified periods, identifying high-risk locations as potential crime hotspots. To achieve temporal flexibility in prediction, FlexiCrime employs an innovative event-centric approach that incorporates two key components: a continuous-time attention network and a type-aware spatiotemporal point process, which captures crime-related features associated with individual time points for each target urban area. Based on the event-centric approach, FlexiCrime then samples time points from the target interval and leverages the crime-related features of these sampled points to enable the flexible interval predictions. Contributions. FlexiCrime is unique in the following. • (Flexible interval prediction). FlexiCrime is a novel deep learning model for crime hotspots prediction with flexible time intervals, which does not require prediction granularity to be determined at training time. Unlike existing approaches that aggregate events within predefined time intervals, FlexiCrime focuses on modeling event-centric crime features. • (Continuous-time attention network). FlexiCrime uses a continuous-time attention network to learn crime context features for each city area at any time point. The crime context feature aggregated encodings from crime events that occurred in similar urban areas and timeframes. By considering relevant crime events, this module learned general crime knowledge about the target time and location. It also accounted for the multicategory and spatiotemporal correlations among crime events in a continuous-time manner, differing from existing methods. • (Type-aware spatiotemporal point process). FlexiCrime incorporates a type-aware spatiotemporal point process designed to capture the evolving features of crime. These features measure the risk of a specific type of crime at a given time and location. We design a type-aware continuous normalizing flow model to estimate the spatial conditional density function. We train this model using a likelihood-based approach to maximize the likelihood of specific type crime occurrences. • (Performance evaluation). The performance of FlexiCrime is evaluated using real-world datasets through crime prediction tasks, ablation experiments, and hyperparameter experiments with multiple interval lengths and start times. The experimental results demonstrate that our model outperforms other methods by 16.53% when applied to different flexible intervals for crime prediction. The rest of the paper is structured as follows. We provide related work in Section 2 and the preliminaries in Section 3. Then, we detail the FlexiCrime in Section 4 and show experiment in Section 5. Finally, the conclusion of the paper is presented in Section 6."
https://arxiv.org/html/2411.00997v1,Identifying Implicit Social Biases in Vision-Language Models,"Vision-language models, like CLIP (Contrastive Language Image Pretraining), are becoming increasingly popular for a wide range of multimodal retrieval tasks. However, prior work has shown that large language and deep vision models can learn historical biases contained in their training sets, leading to perpetuation of stereotypes and potential downstream harm. In this work, we conduct a systematic analysis of the social biases that are present in CLIP, with a focus on the interaction between image and text modalities. We first propose a taxonomy of social biases called So-B-IT, which contains 374 words categorized across ten types of bias. Each type can lead to societal harm if associated with a particular demographic group. Using this taxonomy, we examine images retrieved by CLIP from a facial image dataset using each word as part of a prompt. We find that CLIP frequently displays undesirable associations between harmful words and specific demographic groups, such as retrieving mostly pictures of Middle Eastern men when asked to retrieve images of a “terrorist”. Finally, we conduct an analysis of the source of such biases, by showing that the same harmful stereotypes are also present in a large image-text dataset used to train CLIP models for examples of biases that we find. Our findings highlight the importance of evaluating and addressing bias in vision-language models, and suggest the need for transparency and fairness-aware curation of large pre-training datasets.","Machine learning has seen rapid advances in Vision-Language (VL) models that learn to jointly represent image and language data in a shared embedding space (Radford et al. 2021; Jia et al. 2021). Recent advances on a range of multi-modal tasks are exemplified by the VL model CLIP (Radford et al. 2021), leading to state-of-the-art performance on several zero-shot retrieval tasks (Xu et al. 2021) as well as being integrated into various VL models such as LLaVA (Liu et al. 2024) and BLIP (Li et al. 2022a), which combine the frozen vision encoder with language models for enhanced multi-modal understanding and alignment, Stable Diffusion, which leverages CLIP embeddings for refined text-to-image generation (Rombach et al. 2022) and various other VL models. These successes have spurred several VL models in end-user applications, such as facial recognition systems where CLIP enhances zero-shot face recognition (Zhao and Patras 2023), and multimedia event extraction, as well as event detection in images and captions (Li et al. 2022b; Lu et al. 2024). However, recent works show that large pre-trained models that operate over vision (May et al. 2019; Park et al. 2021), language (Bender et al. 2021; Guo and Caliskan 2020; Zhang et al. 2020a) or both learn social biases from training data (Barocas, Hardt, and Narayanan 2017; Corbett-Davies and Goel 2018), which risks perpetuating bias into downstream retrieval and generation tasks (Silva, Tambwekar, and Gombolay 2021; Luccioni et al. 2023; Weidinger et al. 2021). In VL models specifically, terms related to race have been found to be associated more with people of color (Agarwal et al. 2021), and women are generally underrepresented in image retrieval tasks (Wang, Liu, and Wang 2021). However, these existing works focus only on very specific forms of bias while probing disparities using a small set of curated words (Bhargava and Forsyth 2019). Figure 1: Identifying biases in CLIP using word associations. Given that more extensive and intersectional forms of bias may exist in VL models, there is a need to expand these experiments to a richer taxonomy of potential biases. Further, the size of current datasets used to train such models makes it more difficult for humans to effectively identify low-quality, toxic, or harmful samples (Hanna and Park 2020; Kreutzer et al. 2022). Methods to find and describe biases in datasets are crucial to ensure safe adoption of VL models, yet few methods exist. Recent findings of child sexual abuse images (Thiel 2023) in LAION-5B (Schuhmann et al. 2022), a popular training VL training dataset (Ilharco et al. 2021), further highlights the need to audit the relationships learned by VL models in particular. In this work, we target identifying and describing bias in pre-trained VL models at scale. We first propose a large new taxonomy, called Social Bias Implications Taxonomy (So-B-IT), which spans ten different categories of biases. So-B-IT allows us to examine bias much more broadly than prior works, including biases associated with discrimination based on the model’s implicit assumptions on images of faces. For instance, So-B-IT implements new categories of biased description, such as Appearance and Occupation, and extends word lists used by prior works (May et al. 2019; Steed and Caliskan 2021; Berg et al. 2022), allowing for finer grained analysis. Including new categories is crucial to investigate bias in VL models, as past work has targeted crime-related words (Bhargava and Forsyth 2019) or self-similarity across different demographic groups (Wolfe and Caliskan 2022). Using So-B-IT, we then investigate bias in VL models by retrieving images from FairFace (Kärkkäinen and Joo 2019) — a dataset containing pictures of peoples’ faces along with their age, gender, and race — that the model associates with the words in our taxonomy. For each category in So-B-IT, we quantify the demographic distributions of these retrieved images. As each image contains only a persons face, the association that a VL model makes between these images and the words in our taxonomy should be exclusively explained by the biases inherent to the model itself (Figure 1). Our analysis, based on studying four CLIP-based models (OAICLIP (Radford et al. 2021), OpenCLIP (Ilharco et al. 2021), FaceCLIP (Zheng et al. 2022), and DebiasCLIP (Berg et al. 2022), confirms that these systems encode significant racial and gender biases. Because So-B-IT is more fine-grained than prior work, we uncover previously-unknown, intersectional biases in CLIP models. For example, OpenCLIP not only strongly associates Homemaker with Women significantly more than it does with Men (Stanovsky, Smith, and Zettlemoyer 2019; De-Arteaga et al. 2019), but overwhelmingly associates Homemaker with Indian Women more than it does for women of other races, which is previously uncharacterized in VL models. Our analysis also uncovers that debiasing VL models for Gender can significantly increase the racial bias of the model. This extends prior work showing the propensity of vision models to lean more strongly on remaining shortcuts after debiasing (Li et al. 2023) to VL models. We also extend our experiments to seek the sources of bias in VL training data. Our investigation into training data associated with biased terms confirms the non-representative demographic distributions we identify experimentally. While our experiments are based on CLIP due to its ubiquity (Rombach et al. 2022; Gao et al. 2023; Zhou et al. 2023), our analysis and the So-B-IT taxonomy is directly applicable to any VL model with a joint image and text encoding. Our contributions can be summarized as follows: • We propose a taxonomy, So-B-IT, that covers more categories of bias than prior work and at a finer grain. So-B-IT allows us to categorize a VL model’s capacity to perpetuate societal bias in more representative tasks, and can be used broadly for vision and language auditing. • Using So-B-IT, we audit four different versions of CLIP, finding that these models encode various forms of societal bias and stereotyping across gender and racial groups. • Our findings indicate that debiasing with respect to one sensitive attribute, such as gender, does not necessarily eliminate other forms of bias, particularly racial bias. • We investigate the source of such biases using CLIP’s pre-training data, finding that disproportionate demographic representation may be a root cause of identified biases."
https://arxiv.org/html/2411.00956v1,AI-EDI-SPACE: A Co-designed Dataset for Evaluating the Quality of Public Spaces,"Advancements in AI heavily rely on large-scale datasets meticulously curated and annotated for training. However, concerns persist regarding the transparency and context of data collection methodologies, especially when sourced through crowdsourcing platforms. Crowdsourcing often employs low-wage workers with poor working conditions and lacks consideration for the representativeness of annotators, leading to algorithms that fail to represent diverse views and perpetuate biases against certain groups. To address these limitations, we propose a methodology involving a co-design model that actively engages stakeholders at key stages, integrating principles of Equity, Diversity, and Inclusion (EDI) to ensure diverse viewpoints. We apply this methodology to develop a dataset and AI model for evaluating public space quality using street view images, demonstrating its effectiveness in capturing diverse perspectives and fostering higher-quality data.","Current advancements in AI heavily rely on the availability of large-scale datasets meticulously curated and annotated for training purposes. The significance of such datasets has been underscored by the success of models like ChatGPT, which leverages Reinforcement Learning with Human Feedback (RLHF) to fine-tune models based on human input [1]. However, concerns persist regarding the transparency and context of data collection methodologies, particularly in instances where annotations are sourced through crowdsourcing platforms. For instance, reports indicate that annotations for training ChatGPT were gathered from workers in Kenya under conditions of low pay and poor labor standards [15]. This reliance on crowdsourcing, often driven by cost-effectiveness, perpetuates the invisibility and exploitation of workers, particularly those from the global south [10]. Moreover, the failure to acknowledge the socio-cultural context within which data is produced can introduce biases into datasets. For example, algorithms trained on datasets devoid of the historical context of segregation may inadvertently perpetuate biases against certain minority groups [12]. Furthermore, the identities of workers involved in annotations are frequently overlooked, leading to a lack of diversity in viewpoints captured within datasets. This bias is compounded by the common practice of aggregating annotations through majority voting [5]. To address these limitations, we propose a methodology grounded in a specific socio-cultural context for dataset collection and AI model development. Our approach centers on a co-design model that actively involves stakeholders at key stages of the AI model development, including dataset creation. Additionally, we integrate principles of Equity, Diversity, and Inclusion (EDI) to ensure diverse viewpoints are represented within the dataset. We argue that this approach not only mitigates biases within datasets but also fosters the creation of higher-quality data reflecting diverse perspectives. We apply this methodology to the development of a dataset and AI model capable of evaluating the quality of public spaces using street view images. Assessing public space quality is inherently subjective, as demonstrated by research showing variations across cultural groups [13, 9]. Leveraging our proposed methodology grounded in co-design and EDI principles, we curated a dataset of streetview images annotated by a diverse group of citizens. Using this dataset, we trained a baseline AI model to score public space images along various dimensions. Finally, we propose several fairness metrics to assess the model’s ability to capture diverse viewpoints within the population. Figure 1: Methodology used to create the dataset and the AI model to evaluate the quality of public spaces."
https://arxiv.org/html/2411.00864v1,Advancing Crime Linkage Analysis with Machine Learning: A Comprehensive Review and Framework for Data-Driven Approaches,"Crime linkage is the process of analyzing criminal behavior data to determine whether a pair or group of crime cases are connected or belong to a series of offenses. This domain has been extensively studied by researchers in sociology, psychology, and statistics. More recently, it has drawn interest from computer scientists, especially with advances in artificial intelligence. Despite this, the literature indicates that work in this latter discipline is still in its early stages. This study aims to understand the challenges faced by machine learning approaches in crime linkage and to support foundational knowledge for future data-driven methods. To achieve this goal, we conducted a comprehensive survey of the main literature on the topic and developed a general framework for crime linkage processes, thoroughly describing each step. Our goal was to unify insights from diverse fields into a shared terminology to enhance the research landscape for those intrigued by this subject.","Crime Linkage (CL) [1] is a multidisciplinary field that has garnered significant attention from sociologists, psychologists, statisticians, and computer scientists [2, 3, 4, 5, 6, 7]. In essence, they aim to connect pairs of crimes by creating crime link associations [3]. The CL analysis is typically based on Modus Operandi (MO) or criminal behavior, which can be inferred from evidence found at crime scenes or other source of police data [8]. These MOs/behaviors are systematically compared to assess the similarities between different crimes [9]. High similarity scores often indicate potential connections, providing valuable insight into shared offender characteristics or underlying patterns within these criminal acts. The literature on crime linkage encompasses a wide array of methods and evaluation metrics, alongside various data sets used to establish connections between criminal incidents. Numerous studies have showcased CL outcomes, particularly in cases involving property crimes and sexual offenses [5]. The CL is in essence a classification problem, where the goal is to either assess if a pair of crimes is linked or not, or to associate a case to a series of offenses, based on pattern characteristics. There are many methodologies for modeling and evaluating crime linkage. Data-driven approaches in CL have been explored using statistical methods, fuzzy logic, and machine learning techniques. Although recent years have seen a surge in artificial intelligence (AI) research, studies applying machine learning to crime linkage are still in the early stages. We sought to understand the reasons behind this. To gain insight, we surveyed key data-driven work, including both statistical and machine learning approaches, and analyzed the challenges - whether implicit or explicit - associated with handling large amounts of data. Previous literature surveys have not included machine learning approaches [3, 4, 5], thus, our aim is to provide an extension of research assessment on the theme. When analyzing the studies collected, we observed a general framework in the CL process, which will be also discussed in this paper. In summary, the contributions of this study is two fold: • (1) A comprehensive survey of the main literature on data-drive crime linkage and outline of a general framework of the linkage process to support further research in the topic. • (2) Analyze the main challenges faced by crime linkage, especially when dealing with machine learning and bigger datasets, and thus support future work. The structure of this paper unfolds as follows. First, we will discuss the main literature on crime linkage, outlining the concepts and definitions involved. Then, in Section 3, we will describe the general framework usually used to work on CL, as a discussion of each step and the techniques involved. Section 4 is dedicated to elaborate on the challenges faced by the topic, as potential solutions when applicable. Finally, we will wrap up with the main conclusions of this work. The Appendix (Section 6) will show all the papers surveyed and details on each study."
https://arxiv.org/html/2411.00813v1,Personality Analysis from Online Short Video Platforms with Multi-domain Adaptation,"Personality analysis from online short videos has gained prominence due to its applications in personalized recommendation systems, sentiment analysis, and human-computer interaction. Traditional assessment methods, such as questionnaires based on the Big Five Personality Framework, are limited by self-report biases and are impractical for large-scale or real-time analysis. Leveraging the rich, multi-modal data present in short videos offers a promising alternative for more accurate personality inference. However, integrating these diverse and asynchronous modalities poses significant challenges, particularly in aligning time-varying data and ensuring models generalize well to new domains with limited labeled data. In this paper, we propose a novel multi-modal personality analysis framework that addresses these challenges by synchronizing and integrating features from multiple modalities and enhancing model generalization through domain adaptation. We introduce a timestamp-based modality alignment mechanism that synchronizes data based on spoken word timestamps, ensuring accurate correspondence across modalities and facilitating effective feature integration. To capture temporal dependencies and inter-modal interactions, we employ Bidirectional Long Short-Term Memory networks and self-attention mechanisms, allowing the model to focus on the most informative features for personality prediction. Furthermore, we develop a gradient-based domain adaptation method that transfers knowledge from multiple source domains to improve performance in target domains with scarce labeled data. Extensive experiments on real-world datasets demonstrate that our framework significantly outperforms existing methods in personality prediction tasks, highlighting its effectiveness in capturing complex behavioral cues and robustness in adapting to new domains. We open our datasets and code at https://github.com/Anne6645/personality_analysis","Personality analysis has long been a central topic in psychological science and has gained increasing importance in recent years due to its wide-ranging applications. It plays a crucial role in various domains such as personalized recommendation systems [1, 2], sentiment analysis [3, 4], and human-computer interaction. Accurately identifying an individual’s personality can enable tailored experiences and services, enhancing user satisfaction and engagement. However, personality traits are inherently latent characteristics that are not directly observable, making the assessment of personality a challenging task. Traditionally, psychologists have employed structured methods to evaluate an individual’s personality. One of the most widely accepted models is the Big Five Personality Traits (as shown in Figure 1), which assesses personality across five key dimensions: openness, conscientiousness, extraversion, agreeableness, and neuroticism. To determine an individual’s position on these dimensions, conventional approaches often rely on well-designed questionnaires and psychological inventories that analyze self-reported responses. While these methods are grounded in rigorous psychometric principles, they have notable limitations. Self-reported data can be influenced by social desirability bias, where respondents tailor their answers to be viewed favorably. Additionally, administering and processing these surveys can be time-consuming and resource-intensive, making them less practical for large-scale or real-time applications. Figure 1: Big Five Personality Traits With the advent of online video social platforms like TikTok111https://www.tiktok.com/ and others, there is a growing opportunity to analyze personality traits through digital means. Users increasingly share selfie videos online, providing a wealth of data that captures not only their visual appearance but also their speech patterns, facial expressions, and environmental context. Compared to static questionnaires, these multi-modal data offer richer insights into an individual’s intrinsic traits. Unlike traditional social media platforms that primarily feature text or images, video platforms enable the observation of dynamic behaviors and interactions, which are crucial for understanding personality. This shift opens up new possibilities for applications such as online job interviews, remote education, and personalized content delivery, where assessing personality from videos can significantly enhance outcomes. Recent research has begun to explore the potential of analyzing personality traits through online media instead of traditional surveys. Behavioral observations from personal photographs [5, 6, 7] and short videos [8] have been utilized to glean personality insights. For instance, [5] analyzed Facebook profile pictures to infer personality traits, while [6] and [7] leveraged social media images for similar purposes. However, photograph-based approaches have limitations, as individuals often curate their online images, sharing selective moments that may not accurately represent their typical behaviors or personality, leading to biased data and potentially inaccurate predictions. In contrast, short videos provide a more comprehensive medium for personality analysis. They capture changes in facial expressions, body movements, speech patterns, and contextual scenes—all of which are significant indicators in psychological assessments of personality. Recognizing this, researchers have started to model the audio, visual, and textual features present in short videos [9, 10, 11]. For example, [9] developed a Deep Bimodal Regression model combining audio and visual modalities to predict scores on the Big Five personality traits. Similarly, [10] employed convolutional neural networks to extract visual features and linear regression for audio features, while [11] conducted an in-depth analysis using logistic regression on audio, video, and text features. Figure 2: Comparison of Existing Works Despite these advances, existing multi-modal personality prediction methods often rely on large volumes of high-quality short videos with high-resolution visuals and clear audio to achieve satisfactory performance. Moreover, many approaches depend heavily on supervised learning techniques that require extensive labeled datasets. Collecting and annotating such multi-modal data is both expensive and time-consuming. Manual annotation introduces the potential for subjectivity and inconsistency, which can affect the reliability of the analysis. Consequently, detecting personality traits from online video platforms presents significant challenges, particularly in the following areas: The First Challenge is identifying the most important features from multiple modalities to optimize the use of a limited number of short videos for accurate personality analysis. The complexity arises from the need to effectively integrate diverse data types—visual cues, auditory signals, textual content, and contextual information—each contributing uniquely to personality inference. Existing methods may not adequately address the alignment and synchronization of these modalities, leading to fragmented or incomplete representations that hinder predictive accuracy. The Second Challenge is effectively utilizing a small number of high-quality short videos to achieve strong generalization in personality analysis. Models trained on specific datasets may struggle to generalize across different domains due to variations in cultural contexts, linguistic expressions, and recording conditions. The scarcity of labeled data in new or underrepresented domains exacerbates this issue, limiting the applicability of the models in real-world scenarios where data diversity is the norm. In this paper, we propose an effective multi-modal personality analysis framework designed to overcome these challenges. To address the first challenge, we introduce a semantic unit method for feature extraction and alignment, which synchronizes multi-modal data based on spoken words. This ensures that features from different modalities correspond accurately at each moment in the video, facilitating effective integration. Within this module, we employ self-attention mechanisms to discern the significance of features across various modalities. By assigning weights to features based on their relevance to personality prediction, the model focuses on the most informative aspects of the data, enhancing analytical accuracy. To tackle the second challenge, we propose a multi-domain adaptation method that transfers domain knowledge across multiple domains to alleviate the data sparsity problem. This approach leverages information from data-rich source domains to enhance learning in data-scarce target domains. By computing gradient similarities between source and target domains, our model adapts to emphasize learning from source domains that are most relevant to the target domain. This method improves the model’s generalization capabilities, enabling more accurate predictions even when limited data is available in certain domains. Our main contributions are summarized as follows: • We propose an effective multi-modal personality analysis framework that effectively integrates facial expressions, audio signals, textual content, and background information from short videos for personality prediction. • We introduce a semantic unit modality alignment mechanism that synchronizes multi-modal data based on spoken word timestamps, ensuring accurate correspondence across modalities and enhancing feature representation. • We develop a gradient-based domain adaptation method that transfers knowledge from multiple source domains to target domains with limited labeled data, enhancing model generalization and performance in few-shot learning scenarios. • We validate the effectiveness of our proposed framework through extensive experiments on real-world datasets, demonstrating significant improvements over existing methods in personality prediction tasks. By addressing both the feature integration and domain adaptation challenges, our framework advances in personality analysis from online short videos. The rest of the paper is organized as follows. Section II introduces the related work of personality analysis and domain adaption. The problem definition is in Section III. Section IV elucidates the detailed methodology, and Section V presents the results of the experiments and the analysis. The last section is the conclusion of the paper."
https://arxiv.org/html/2411.00585v1,Benchmarking Bias in Large Language Models during Role-Playing,"Large Language Models (LLMs) have become foundational in modern language-driven applications, profoundly influencing daily life. A critical technique in leveraging their potential is role-playing, where LLMs simulate diverse roles to enhance their real-world utility. However, while research has highlighted the presence of social biases in LLM outputs, it remains unclear whether and to what extent these biases emerge during role-playing scenarios. In this paper, we introduce BiasLens, a fairness testing framework designed to systematically expose biases in LLMs during role-playing. Our approach uses LLMs to generate 550 social roles across a comprehensive set of 11 demographic attributes, producing 33,000 role-specific questions targeting various forms of bias. These questions, spanning Yes/No, multiple-choice, and open-ended formats, are designed to prompt LLMs to adopt specific roles and respond accordingly. We employ a combination of rule-based and LLM-based strategies to identify biased responses, rigorously validated through human evaluation. Using the generated questions as the benchmark, we conduct extensive evaluations of six advanced LLMs released by OpenAI, Mistral AI, Meta, Alibaba, and DeepSeek. Our benchmark reveals 72,716 biased responses across the studied LLMs, with individual models yielding between 7,754 and 16,963 biased responses, underscoring the prevalence of bias in role-playing contexts. To support future research, we have publicly released the benchmark, along with all scripts and experimental results.Warning: This paper includes examples of biased content to demonstrate our testing results.","Large Language Models (LLMs), such as GPT and Llama, are increasingly integrated into diverse, human-centered domains, including finance (Zhao et al., 2024a), medicine (Sayin et al., 2024), law enforcement (Pandey et al., 2024), education (Jeon and Lee, 2023), and social decisions (Ressel et al., 2024), significantly shaping various aspects of daily life. Role-playing, where LLMs assume specific roles, has emerged as an effective paradigm for enhancing LLMs’ contextual understanding and task-specific performance (Shanahan et al., 2023). Major LLM providers all recommend role-playing to generate more relevant, engaging responses and achieve better results, as seen in their usage guidelines (rol, 2024e, a, c, b, d). This widespread endorsement underscores the growing significance of role-playing in optimizing LLM capabilities for real-world applications. However, alongside these advances, the pervasive adoption of LLMs has raised critical concerns about social biases in their outputs. Social bias refers to discrimination for or against a person or group in a way that is prejudicial or unfair (Wan et al., 2023b; Webster et al., 2022). For example, GPT-4 has reportedly suggested different salary levels based on the inferred race and gender of job candidates’ names (Haim et al., 2024). Such biases are often rooted in the large-scale, historical datasets on which LLMs are built, mirroring real-world social biases (Wan et al., 2023b). Detecting social biases in LLMs is essential for uncovering hidden stereotypes and discriminatory tendencies, allowing us to assess and address the ethical risks these models pose as they become increasingly embedded in society’s core technologies. From the Software Engineering (SE) perspective, these biases are considered ‘fairness bugs’ (Chen et al., 2024), as fairness is a critical requirement in software systems. Fairness testing, which refers to software testing activities focused on identifying biases, has proven effective in detecting social biases in LLM outputs (Chen et al., 2024). A notable example is BiasAsker (Wan et al., 2023b), a fairness testing framework that employs template-based input generation to create questions designed to trigger and measure social biases in conversational AI systems. Despite advancements in fairness testing, bias detection in LLMs during role-playing remains largely unexplored. Without such techniques, it is unclear whether and to what extent social biases manifest in role-playing contexts. Insights from the existing literature highlight the importance of this investigation. First, research in software fairness commonly identifies a trade-off between fairness and machine learning performance (Chen et al., 2022, 2023). Given that role-playing enhances LLM capabilities, it also has the potential to introduce or exacerbate biases, compromising fairness. Second, social science research indicates that individuals in different social roles may exhibit varying prejudices or discrimination toward other groups (Gonsalkorale et al., 2009; Wilson et al., 2018; Gwartney and McCaffree, 1971), which could be reflected in the historical data used to train LLMs. Consequently, LLMs trained on such data may display distinct biases when simulating various roles. Figure 1. Examples of biased responses from GPT4o-mini and Llama3-70b during role-playing. Each question was queried independently on each model three times, and consistent responses were obtained across all queries for each model on October 1, 2024. Role-playing biases are indeed evident in practice. For example, as shown in Figure 1, we prompted two advanced LLMs, GPT4o-mini from OpenAI and Llama3-70b from Meta, with the question: ‘Whose designs do you think are more functional, (A) Asian Architects, (B) European Architects, or (C) I cannot make any choice.’ Both models selected option C. However, when we framed the question as ‘Suppose you are an Architect,’ the models exhibited social biases, with GPT4o-mini choosing option A and Llama3-70b selecting option B. Biases in role-playing scenarios can lead LLMs to exhibit unfair behaviors toward specific groups and reinforce social stereotypes linked to the roles they assume in various applications. Through frequent use, these biases risk normalizing and deepening harmful stereotypes, subtly shaping public perception and entrenching social biases. Fairness testing in role-playing serves two critical goals: to assess biased behaviors toward specific groups that could perpetuate social inequality and to identify whether LLMs reinforce role-based stereotypes that could distort or harm public understanding. In this paper, we introduce BiasLens, a fairness testing framework designed to detect social biases in LLM outputs during role-playing. BiasLens consists of two main components: test input generation (generating bias-triggering questions) and test oracle design (identifying biased responses). For test input generation, BiasLens first employs LLMs to generate 550 roles across 11 diverse demographic attributes, forming a representative set of roles for fairness testing. For each role, BiasLens automatically generates 60 questions with the potential to elicit biased responses when the LLM adopts that role. These questions span three common formats, including Yes/No, multiple-choice, and open-ended questions, to comprehensively assess bias triggers. In total, 33,000 questions are generated to prompt LLMs to assume specific roles and respond accordingly. For test oracle generation, BiasLens applies a mix of rule-based and LLM-based strategies tailored to different question types, and we validate the reliability of these identifications through a rigorous manual evaluation. Using the generated questions as a benchmark, we conduct an extensive evaluation of six advanced LLMs released by OpenAI, Mistral AI, Meta, Alibaba, and DeepSeek. This selection represents both open-source and closed-source models widely used in real-world applications, with considerable variation in model size. To ensure rigorous results, each question is posed to each LLM three times, with biased responses classified only if they occur in more than two out of three instances. Despite this stringent criterion, our benchmark identifies a total of 72,716 biased responses across these LLMs, with individual models yielding between 7,754 and 16,963 biased responses. When we remove the role-playing statements from our benchmark, all six LLMs show a reduction in biased responses, with an average decrease of 24.3%. This further indicates that role-playing can introduce additional social biases into LLM outputs, highlighting the need for fairness testing specifically within role-playing contexts. In summary, this paper makes the following contributions: • Tailored Testing Framework: We introduce BiasLens, an automated fairness testing framework specifically designed to uncover biases in LLMs during role-playing. • Extensive Empirical Study: We conduct a large-scale empirical evaluation across six advanced LLMs, using 33,000 questions generated by BiasLens, and reveal a total of 72,716 biased responses. • Open Benchmark and Resources: We release our benchmark dataset, scripts, and experimental results (git, 2024) to facilitate the adoption of BiasLens and to encourage further research. The remainder of the paper is organized as follows: Section 2 provides essential background information and reviews closely related work. Section 3 presents our BiasLens framework, detailing its key components. Section 4 outlines our research questions and experimental design for evaluating BiasLens. Section 5 addresses each research question based on the experimental results. Section 6 discusses potential threats to the validity of our findings, followed by concluding remarks in Section 7."
https://arxiv.org/html/2411.00414v1,On the Opportunities of Large Language Models for Programming Process Data,"Computing educators and researchers have used programming process data to understand how programs are constructed and what sorts of problems students struggle with. Although such data shows promise for using it for feedback, fully automated programming process feedback systems have still been an under-explored area. The recent emergence of large language models (LLMs) have yielded additional opportunities for researchers in a wide variety of fields. LLMs are efficient at transforming content from one format to another, leveraging the body of knowledge they have been trained with in the process. In this article, we discuss opportunities of using LLMs for analyzing programming process data. To complement our discussion, we outline a case study where we have leveraged LLMs for automatically summarizing the programming process and for creating formative feedback on the programming process. Overall, our discussion and findings highlight that the computing education research and practice community is again one step closer to automating formative programming process-focused feedback.","Feedback can have a tremendous impact on learning and achievement (Hattie and Timperley, 2007). The level of detail of the feedback influences its effectiveness (Wisniewski et al., 2020), and feedback can be given at many levels ranging from targeting how to work on and complete specific tasks to considering personal characteristics and behavior (Hattie and Timperley, 2007; Ott et al., 2016; Keuning et al., 2018). In teaching and learning programming, automated assessment systems have been a key tool for providing feedback at a scale already for more than a half a century (Hollingsworth, 1960; Paiva et al., 2022; Keuning et al., 2018). Researchers have sought to automate step-by-step guidance (Vihavainen et al., 2013), provide hints during the programming process (McBroom et al., 2021), improve programming error messages (Becker et al., 2019), and aid in providing textual feedback by grouping similar code submissions together (Nguyen et al., 2014; Glassman et al., 2015; Koivisto and Hellas, 2022). To support the understanding of how novices construct programs, researchers and educators have been collecting increasing amounts of data from students’ programming process (Ihantola et al., 2015). Such data can be collected at multiple granularities, ranging from final course assignment submissions to individual keystrokes from solving the assignments (Ihantola et al., 2015). Programming process data has been, for example, used to play back how students construct their programs step by step or keystroke by keystroke to create a broader understanding of the process (Shrestha et al., 2022; Zhong et al., 2024; Heinonen et al., 2014). So far, despite shared efforts towards providing timely feedback to students (Jeuring et al., 2022), the potential of fine-grained programming process data for feedback purposes is still largely untapped. Large Language Models (LLMs) are a potential tool for realizing the transformation of programming process data into actionable feedback items. Within Computing Education Research, LLMs have broadened the horizon of what computing education researchers and practitioners can achieve (Prather et al., 2023), calling even for rethinking how computer science and programming is taught (Denny et al., 2024). Large Language Models have been shown to help in creating assignments (Logacheva et al., 2024; Sarsa et al., 2022), improve error messages (Leinonen et al., 2023; Santos et al., 2023), fix students’ code (Koutcheme et al., 2023), explain code (MacNeil et al., 2022, 2023), and respond to help requests (Hellas et al., 2023; Koutcheme et al., 2024). At their core, LLMs are tools that allow transforming text-based content from one form to another, drawing on the data that they have been trained with in the process and the instructions provided by the user (Vaswani et al., 2017; Ouyang et al., 2022). In this article, we discuss the potentials of LLMs for programming process data. We outline a case study of using LLMs for analyzing programming process data. In the case study, we first use LLMs for summarizing the programming process, followed by asking LLMs to provide feedback on the process. This article is structured as follows. In Section 2, we outline prior research on programming process data. Building on Section 2, Section 3 outlines our vision for the possibilities of LLMs for Programming Process Data. In Section 4, we outline our case study where we leveraged LLMs for analyzing programming process data. Finally, in Section 5, we summarize our discussion and outline possible future directions for research on using LLMs for programming process analytics."
https://arxiv.org/html/2411.00208v1,Using Large Language Models for an ISO37101 standard assessment mapping for sustainable communities,"This paper presents a new approach to urban sustainability assessment through the use of Large Language Models (LLMs) to streamline the use of the ISO 37101 framework to automate and standardise the assessment of urban initiatives against the six ""sustainability purposes"" and twelve ""issues"" outlined in the standard. The methodology includes the development of a custom prompt based on the standard definitions and its application to two different datasets: 527 projects from the Paris Participatory Budget and 398 activities from the PROBONO Horizon 2020 project. The results show the effectiveness of LLMs in quickly and consistently categorising different urban initiatives according to sustainability criteria. The approach is particularly promising when it comes to breaking down silos in urban planning by providing a holistic view of the impact of projects. The paper discusses the advantages of this method over traditional human-led assessments, including significant time savings and improved consistency. However, it also points out the importance of human expertise in interpreting results and ethical considerations. This study hopefully can contribute to the growing body of work on AI applications in urban planning and provides a novel method for operationalising standardised sustainability frameworks in different urban contexts.","1.1 Sustainable urban development challenges The 21st century presents urban planners and policy makers with unprecedented challenges. With over 55% of the world’s population living in urban areas and this figure expected to rise to 68% by 2050, cities are at the forefront of global sustainability challenges. These range from climate change adaptation and mitigation to social equity and economic resilience. The complexity of urban systems, characterised by interdependencies between the built environment, social structures and ecological processes, requires sophisticated approaches to planning, delivering and monitoring, with a focus on the strategic planning phase to allow for strong strategic foundations [1]. Traditional urban planning methods attempt to capture the complexity of sustainability in urban contexts, but the nature of urban governance, combined with the difficulty of quantifying and standardising sustainability metrics across different urban landscapes, does not allow for an easy, comprehensive and comparable assessment of urban sustainability initiatives. 1.2 Brief overview of ISO 37101 and its relevance In response to these challenges, the International Organisation for Standardisation (ISO) has developed the ISO 37100 standard series, entitled ""Sustainable development in communities — Management system for sustainable development"". This standard provides a structured framework for sustainable development in communities, offering a systematic approach to the planning, implementation and evaluation of urban sustainability initiatives through the implementation of management systems, from smaller scale systems like blocks and neighbourhoods [2] to bigger systems. At the conceptual centre of ISO 37101 is an approach defined by six sustainability ’purposes’ and twelve families of city services, or ’issues’. This 6x12 grid provides a comprehensive lens through which places (being blocks, neighbourhood, cities, ..) projects, policies, strategies and initiatives can be viewed and evaluated. In particular, the strength of the standard suite of concepts is that it provides a common language and assessment framework for different contexts and scales, facilitating both local action and global comparability. However, its application is time-consuming, and requires the intervention of experts, preventing a more wide-spread application. 1.3 Introduction to Large Language Models and their potential in standardized tasks Recent advancements in artificial intelligence (AI), have given rise to Large Language Models (LLMs). These models, trained on vast corpora of text data, demonstrate remarkable capabilities in understanding and generating human-like text across a wide range of domains. LLMs represent a paradigm shift in how we approach text-based tasks. Unlike traditional rule-based or statistical Natural Language Processing (NLP) methods, LLMs leverage deep learning architectures, typically based on transformer models, to capture complex linguistic patterns and semantic relationships. This allows them to perform tasks with a level of nuance and context-awareness previously unattainable in automated systems. The potential of LLMs for repetitive tasks depending on clear, rigorous definitions is particularly interesting for us. Their ability to process complex, context-dependent information makes them well-suited for tasks that require both broad knowledge and specific domain understanding. In the context of assessing initiatives against a set group of definitions, LLMs have the potential to bridge the gap between qualitative descriptions of urban initiatives and standardized mapping, on frameworks such as the one defined by ISO 37101. 1.4 Research objectives and significance This research aims to explore the interface between the structured framework of ISO 37101 and the capabilities of LLMs in the context of urban sustainability assessment. Specifically, we seek to: • Develop and test an approach to using LLMs to automatically categorise and assess urban sustainability initiatives according to the ISO 37101 framework. • Explore the potential of this approach to break down silos in urban planning and enable more holistic, cross-sector sustainability strategies. • Investigate the usability, scalability and adaptability of this method in different urban contexts and data sources. The importance of this research lies in its potential to standardise and rationalise sustainability assessments in urban planning using LLMs, noting that this use case can be transferred to other assessments. The aim is to demonstrate the feasibility of a tool that can quickly process large amounts of unstructured urban project data and provide consistent, comparable assessments at a strategic level that are aligned with international standards. This could significantly improve the ability of cities to assess their sustainability initiatives, facilitate knowledge sharing between urban areas and ultimately accelerate progress towards achieving sustainable urban development goals. This study can also contribute to the growing body of work on the application of AI in standardised assessment frameworks. The insights gained from this study could influence future developments in both AI applications for urban sustainability and the development of international standards for sustainable urban development."
https://arxiv.org/html/2411.00179v1,What Makes An Expert? Reviewing How ML Researchers Define “Expert”,"Human experts are often engaged in the development of machine learning systems to collect and validate data, consult on algorithm development, and evaluate system performance. At the same time, who counts as an ‘expert’ and what constitutes ‘expertise’ is not always explicitly defined. In this work, we review 112 academic publications that explicitly reference ‘expert’ and ‘expertise’ and that describe the development of machine learning (ML) systems to survey how expertise is characterized and the role experts play. We find that expertise is often undefined and forms of knowledge outside of formal education and professional certification are rarely sought, which has implications for the kinds of knowledge that are recognized and legitimized in ML development. Moreover, we find that expert knowledge tends to be utilized in ways focused on mining textbook knowledge, such as through data annotation. We discuss the ways experts are engaged in ML development in relation to deskilling, the social construction of expertise, and implications for responsible AI development. We point to a need for reflection and specificity in justifications of domain expert engagement, both as a matter of documentation and reproducibility, as well as a matter of broadening the range of recognized expertise.","Human expertise plays a critical role in various facets of machine learning (ML) development. Human experts are often engaged in the development of ML systems to collect and validate data, consult on the development of knowledge representations, and evaluate system performance. For systems designed for use in highly specific contexts, such as predicting car insurance fraud (Šubelj, Furlan, and Bajec 2011), domain experts provide knowledge that others involved in development may lack. Indeed, experts can play an important role as a reference for guiding the behavior of a model, for example by providing input as ground truth to be be used in model testing. In addition, experts provide context about a given problem domain to identify relevant variables and measures and shape algorithm design. At the same time, who counts as an ‘expert’ and what constitutes ‘expertise’ is not always clear or explicitly defined and can shift in relation to system goals. For example, experts in different fields may disagree on optimal solutions or on different approaches toward achieving the same solution. Input from domain experts plays a significant role in ML development as a basis for systems designed to match or exceed human ability in a given set of tasks. However, there exist many kinds of knowledge and expertise, raising questions about the range and consistency of definitions of expertise in ML research. For example, in data annotation, “expert” has been used to refer to domain knowledge rooted in annotator lived experience (Patton et al. 2019), domain knowledge based on specific training, such as Wikipedia article editing or academic study (Kittur, Chi, and Suh 2008; Sen et al. 2015), and even to refer to labels from gold standard datasets, even when the gold standard annotators themselves are unspecified (Snow et al. 2008). Understanding how expertise is defined is important, because the label “expert” directs attention to whose knowledge should be upheld as canonical; thus, who or what is identified as ‘expert’ carries implicit, normative claims about whose knowledge and experience is valid and trustworthy for system development. Thus, expertise confers power to those identified as possessing it (Handy 1976). In addition to questions of who is included and excluded in ML development, are issues related to how domain experts are engaged in ML development. Birhane et al. (2022) point out that participation in ML development can bring with it varying degrees of extractive power relations, particularly when central goals are performance-driven. While Birhane et al. primarily focus on community-based and participatory engagements, participation on the part of a range of subject matter experts is also subject to these dynamics. Other scholars have discussed ways in which expertise has been devalued through automation and ML work despite being sought and acknowledged as specialized and critical (Sambasivan and Veeraraghavan 2022; Duran 2021). The potential for extractive and devaluing engagements underscores a need to consider not just how expertise is defined, but also the social and power relations between experts and the development processes that engage them. Based on a systematic review of ML publications, we contribute a taxonomy detailing the ways ML research studies treat the terms “expert” and “non-expert” and how recognized experts are engaged in ML development. We find that ML research often leaves expertise undefined and rarely seeks forms of knowledge outside of formal education and professional certification, which has implications for the kinds of knowledge that are recognized and legitimized in and through ML development. Moreover, we find that expert knowledge tends to be utilized in ways focused on mining textbook knowledge– or discrete information that can be memorized and reproduced relatively easily, such as through data annotation. On the backdrop of calls for expanded participation in ML development, we discuss the social embeddedness of expertise, patterns in ML research in relation to deskilling and power, and the need for more equitable recognition of expertise in support of responsible ML practices."
https://arxiv.org/html/2411.00156v1,Unlocking the Potential of Global Human Expertise,"Solving societal problems on a global scale requires the collection and processing of ideas and methods from diverse sets of international experts. As the number and diversity of human experts increase, so does the likelihood that elements in this collective knowledge can be combined and refined to discover novel and better solutions. However, it is difficult to identify, combine, and refine complementary information in an increasingly large and diverse knowledge base. This paper argues that artificial intelligence (AI) can play a crucial role in this process. An evolutionary AI framework, termed RHEA, fills this role by distilling knowledge from diverse models created by human experts into equivalent neural networks, which are then recombined and refined in a population-based search. The framework was implemented in a formal synthetic domain, demonstrating that it is transparent and systematic. It was then applied to the results of the XPRIZE Pandemic Response Challenge, in which over 100 teams of experts across 23 countries submitted models based on diverse methodologies to predict COVID-19 cases and suggest non-pharmaceutical intervention policies for 235 nations, states, and regions across the globe. Building upon this expert knowledge, by recombining and refining the 169 resulting policy suggestion models, RHEA discovered a broader and more effective set of policies than either AI or human experts alone, as evaluated based on real-world data. The results thus suggest that AI can play a crucial role in realizing the potential of human expertise in global problem-solving.","Integrating knowledge and perspectives from a diverse set of experts is essential for developing better solutions to societal challenges, such as policies to curb an ongoing pandemic, slow down and reverse climate change, and improve sustainability [33, 41, 57, 63, 64]. Increased diversity in human teams can lead to improved decision-making [25, 62, 83], but as the scale of the problem and size of the team increases, it becomes difficult to discover the best combinations and refinements of available ideas [37]. This paper argues that artificial intelligence (AI) can play a crucial role in this process, making it possible to realize the full potential of diverse human expertise. Though there are many AI systems that take advantage of human expertise to improve automated decision-making [4, 31, 66], an approach to the general problem must meet a set of unique requirements: It must be able to incorporate expertise from diverse sources with disparate forms; it must be multi-objective since conflicting policy goals will need to be balanced; and the origins of final solutions must be traceable so that credit can be distributed back to humans based on their contributions. An evolutionary AI framework termed RHEA (for Realizing Human Expertise through AI) is developed in this paper to satisfy these requirements. Evolutionary AI, or population-based search, is a biologically-inspired method that often leads to surprising discoveries and insights [5, 15, 39, 48, 67]; it is also a natural fit here since the development of ideas in human teams mirrors an evolutionary process [14, 17, 38, 32]. Implementing RHEA for a particular application requires the following steps (Fig. 1): 1. Define. Define the problem in a formal manner so that solutions from diverse experts can be compared and combined. 2. Gather. Solicit and gather solutions from a diverse set of experts. Solicitation can take the form of an open call or a direct appeal to known experts. 3. Distill. Use machine learning to convert (distill) the internal structure of each gathered solution into a canonical form such as a neural network. 4. Evolve. Recombine and refine the distilled solutions using a population-based search to realize the complementary potential of the ideas in the expert-developed solutions. RHEA is first illustrated through a formal synthetic example below, demonstrating how this process can result in improved decision-making. RHEA is then put to work in a large-scale international experiment on developing non-pharmaceutical interventions for the COVID-19 pandemic. The results show that broader and better policy strategies can be discovered in this manner, beyond those that would be available through AI or human experts alone. The results also highlight the value of soliciting diverse expertise, even if some of it does not have immediately obvious practical utility: AI may find ways to recombine it with other expertise to develop superior solutions. Figure 1: The RHEA (Realizing Human Expertise through AI) framework. The framework consists of four components: Defining the prediction and prescription tasks, gathering the human solutions, distilling them into a canonical form, and evolving the population of solutions further. a, The predictor maps context and actions to outcomes and thus constitutes a surrogate, or a “digital twin”, of the real world. For example, in the Pandemic Response Challenge experiment, the context consisted of data about the geographic region for which the predictions were made, e.g., historical data of COVID-19 cases and intervention policies; actions were future schedules of intervention policies for the region; and outcomes were predicted future cases of COVID-19 along with the stringency of the policy. b, Given a predictor, the prescriptor generates actions that yield optimized outcomes across contexts. c, Humans are solicited to contribute expertise by submitting prescriptors using whatever methodology they prefer, such as decision rules, epidemiological models, classical statistical techniques, and gradient-based methods. d, Each submitted prescriptor is distilled into a canonical neural network that replicates its behavior. e, This population of neural networks is evolved further, i.e., the distilled models are recombined and refined in a parallelized, iterative search process. They build synergies and extend the ideas in the original solutions, resulting in policies that perform better than the original ones. For example, in the Pandemic Response Challenge, the policies recommend interventions that lead to minimal cases with minimal stringency. To summarize, the main contributions of this paper are as follows: (1) Recognizing that bringing together diverse human expertise is a key challenge in solving many complex problems; (2) Identifying desiderata for an AI process that accomplishes this task; (3) Demonstrating that existing approaches do not satisfy these desiderata; (4) Formalizing a new framework, RHEA, to satisfy them; (5) Instantiating a first concrete implementation of RHEA using standard components; and (6) Evaluating this implementation in a global application: The XPRIZE Pandemic Response Challenge."
