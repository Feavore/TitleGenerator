URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.04658v1,Finding Strong Lottery Ticket Networks with Genetic Algorithms,"According to the Strong Lottery Ticket Hypothesis, every sufficiently large neural network with randomly initialized weights contains a sub-network which – still with its random weights – already performs as well for a given task as the trained super-network. We present the first approach based on a genetic algorithm to find such strong lottery ticket sub-networks without training or otherwise computing any gradient. We show that, for smaller instances of binary classification tasks, our evolutionary approach even produces smaller and better-performing lottery ticket networks than the state-of-the-art approach using gradient information.","A central aspect to the wide success of artificial neural networks (ANNs) is that they are usually designed to be overparametrized (Aggarwal et al.,, 2018). That means that they feature more parameters (weights) than are strictly necessary to represent the function they are meant to approximate. However, it is also that overparametrization that constructs a solution landscape that is friendly towards relatively simple optimization strategies like stochastic gradient descent (Shevchenko and Mondelli,, 2020), whose application is also enabled by the fact that neural networks are usually differentiable and can thus provide gradient information to the optimization algorithm. The Lottery Ticket Hypothesis (Frankle and Carbin,, 2018) and its variants (Ramanujan et al.,, 2020) have provided a different perspective on the properties of neural networks: Among the randomly initialized weights (before any optimization), some weights have already “won the lottery” by being easily trainable. Furthermore, in any sufficiently overparametrized network, there already exist — at the point of random initialization — certain subnetworks that (when unhinged from the rest of the network) approximates the desired function as accurately as the whole network would after optimization. Thus, if these subnetworks or strong lottery tickets could be found easily, the whole training process of neural networks could be skipped. Figure 1 illustrates a lottery ticket network evolved from a full network with much more active (i.e., non-zero) connections. Figure 1: Illustration of a lottery ticket network. Top: Full network graph. Red connections persist in most evolved lottery ticket networks in an example population (blue connections do not). Bottom: Example of an evolved lottery ticket subnetwork with only a fraction of active connections. Finding such subnetworks naturally requires a substantial computational load, as the number of possible combinations of connections to prune from the subnetwork grows exponentially with the network size. This makes it difficult for a lottery-ticket-based optimization alternative to succeed in practice. In fact, state-of-the-art methods for finding lottery tickets tend to utilize regular training steps of the full network (without changing the weights) to identify more important connections to be kept in the subnetwork. This paper presents a novel approach to finding strong lottery tickets based purely on combinatorial evolutionary optimization without training the weights or utilizing gradient information. To the best of our knowledge, this is the first approach in this direction. We summarize our contribution as follows: • We show that a basic genetic algorithm (GA) can already produce strong lottery ticket networks. • Our approach yields sparser and more accurate networks compared to the gradient-based state-of-the-art in exemplary binary classification tasks. • Uncovering scenarios where the utilized GA operations are insufficient, we hope to pave the way for further investigating the applicability of GAs for optimizing neural networks or similar entities."
https://arxiv.org/html/2411.04817v1,Harnessing the Power of Gradient-Based Simulations for Multi-Objective Optimization in Particle Accelerators,"Particle accelerator operation requires simultaneous optimization of multiple objectives. Multi-Objective Optimization (MOO) is particularly challenging due to trade-offs between the objectives. Evolutionary algorithms, such as genetic algorithm (GA), have been leveraged for many optimization problems, however, they do not apply to complex control problems by design. This paper demonstrates the power of differentiability for solving MOO problems using a Deep Differentiable Reinforcement Learning (DDRL) algorithm in particle accelerators. We compare DDRL algorithm with Model Free Reinforcement Learning (MFRL), GA and Bayesian Optimization (BO) for simultaneous optimization of heat load and trip rates in the Continuous Electron Beam Accelerator Facility (CEBAF). The underlying problem enforces strict constraints on both individual states and actions as well as cumulative (global) constraint for energy requirements of the beam. A physics-based surrogate model based on real data is developed. This surrogate model is differentiable and allows back-propagation of gradients. The results are evaluated in the form of a Pareto-front for two objectives. We show that the DDRL outperforms MFRL, BO, and GA on high dimensional problems.","Particle accelerators are intricate, high-energy machines comprised of numerous specialized components. To ensure efficient operation and precise control, accelerator operators must meticulously adjust multiple component settings to fine-tune the machine. Jefferson Laboratory’s primary particle accelerator, Continuous Electron Beam Accelerator Facility (CEBAF) [1], is comprised of two anti-parallel superconducting radiofrequency (SRF) linear accelerators (linac) to accelerate electrons. Each linac contains 25 cryomodules comprising of 200 SRF cavities. These cavities are kept at 2K temperature to maintain superconductivity. The cryomodules are filled with liquid helium regulated by a Central Helium Liquefier (CHL). These cavities are individually controlled and each has its own unique operating characteristics. An ever-present challenge faced by operations staff is determining the best way to distribute Radio Frequency (RF) gradients across these cavities to meet the required experiment energy gain and simultaneously minimize negative impacts to CEBAF and experiments. Two aspects of SRF operations that are directly controlled through the gradient distribution is the heat load imposed upon the cryogenics system through RF operations and the number of Fast Shut Down (FSD) trips initiated by the RF system. Optimizing the RF heat load and FSD trips is particularly important since it would lower the wear and tear in CHL and reduce machine downtime respectively. This is a multi-objective optimization (MOO) problem, in which there is a tradeoff between the heat load and the number of FSD trips. In MOO, the full optimal set of tradeoffs is known as the Pareto-optimal front (or Pareto front). This front defines the best quality solution that can be achieved for a given objective without reducing the quality of a competing objective. The problem we are considering thus becomes a question of how to efficiently find the Pareto-optimal front defining the tradeoff between the FSD trips and RF heat load, ideally in a way that will also translate from offline system analysis to real-world facility operation. In addition to RF heat load and FSD trips, the gradient distribution needs to produce an energy within a very small tolerance for proper steering of the beam. This introduces a global hard constraint on the optimal solution sets, making the problem very challenging especially when considering the large number of cavities in a linac. From an operational point of view, a quickly-converging algorithm on this high-dimensional MOO problem is desired. In this paper, we use a surrogate model of the CEBAF RF heat load and trip rates based on historical data to train and compare MFRL based Conditional Multi-Objective Twin Delayed Deep Deterministic Policy Gradient (CMO-TD3) [2], NSGA-II [3], Multi-Objective Bayesian Optimization (MOBO) [4], and Conditional Multi-Objective Deep Differentiable Reinforcement learning (CMO-DDRL) [5] algorithms in the offline multi-objective optimization task described above. We investigate the performance of these algorithms in terms of time- and sample- efficiency, as well as solution quality. We also assess how the performance changes with problem dimensionality (i.e. number of RF cavities used). Finally, embedded in this investigation, we quantitatively assess the impact of using a differentiable system model (as is done in DDRL), in contrast to algorithms that do not use such a model (NSGA-II, TD3, MOBO). We also highlight some advantages the DDRL approach has in terms of being able to actively scan the Pareto-optimal front in a control setting. This investigation highlights the importance of sample acquisition speed in determining appropriate algorithms to use in MOO. It also provides a useful case-study for the accelerator community, which will aid researchers in determining approaches to use for problems on other systems. The paper is organized as follows: Section 2 describes the latest work in AI-based optimization for particle accelerators, Section 3 describes the CEBAF optimization challenge, and the methods and results are described in Section 4 and Section 5 respectively. We close the discussion with some insights and future outlook in Section 6."
https://arxiv.org/html/2411.04728v1,Neuromorphic Wireless Split Computing with Multi-Level Spikes,"Inspired by biological processes, neuromorphic computing utilizes spiking neural networks (SNNs) to perform inference tasks, offering significant efficiency gains for workloads involving sequential data. Recent advances in hardware and software have demonstrated that embedding a few bits of payload in each spike exchanged between the spiking neurons can further enhance inference accuracy. In a split computing architecture, where the SNN is divided across two separate devices, the device storing the first layers must share information about the spikes generated by the local output neurons with the other device. Consequently, the advantages of multi-level spikes must be balanced against the challenges of transmitting additional bits between the two devices.This paper addresses these challenges by investigating a wireless neuromorphic split computing architecture employing multi-level SNNs. For this system, we present the design of digital and analog modulation schemes optimized for an orthogonal frequency division multiplexing (OFDM) radio interface. Simulation and experimental results using software-defined radios provide insights into the performance gains of multi-level SNN models and the optimal payload size as a function of the quality of the connection between a transmitter and receiver.","I-A Context and Motivation Current learning algorithms, computing primitives, and hardware platforms such as GPUs are widely expected to soon fall short in supporting scalable, energy-efficient artificial intelligence (AI) models, especially for edge deployments [1]. This motivates the ongoing exploration of alternative computing paradigms, including in-memory computing [2], neuromorphic computing [3, 4], and quantum computing [5, 6]. Advances in computing technologies are bound to affect a range of fields from the sciences [7] to engineering [8]. This work studies some of the implications of the emergence of neurormorphic computing for telecommunications engineering [9, 10, 11, 12, 13, 14, 15, 16]. As communication networks become increasingly softwarized [17], spiking neural networks (SNNs) present a promising option as co-processors for wireless transmitters and receivers, as explored in [15, 13]. Neuromorphic computing, therefore, can play an important role in enabling advanced communication functionalities. Conversely, communication networks can support the development of distributed computing architectures grounded in neuromorphic principles. In these architectures, communication protocols must be tailored to the unique nature of information exchanged between SNN neurons. Unlike conventional multi-bit clocked messages, spiking neurons encode and transmit information through the timing of individual spikes. As a result, partitioning an SNN across multiple devices requires these devices to share timing information to maintain the integrity of the neural computations [9, 18, 10, 12, 19, 16, 20]. Figure 1: (a) Neuromorphic wireless split computing architecture based on multi-level SNNs: Spikes exchanged between a transmitter and a receiver over a wireless channel include a payload of m𝑚mitalic_m bits. (b) While the accuracy of a centralized implementation increases monotonically with the spike payload m𝑚mitalic_m [21, 22], in the presence of communication constraints there is generally an optimized value of m𝑚mitalic_m that balances the informativeness of each spike with the reduced accuracy of higher-rate transmission. As shown in Fig. 1(a), in this paper, we focus on a basic distributed computing architecture [23] consisting of an SNN split between two devices, which are connected over a wireless channel. The transmitter-side SNN processes sequential data captured by a neuromorphic sensor, such as an event-driven camera [24, 25, 26, 27]. SNNs can natively process event-driven data via spiking neurons. The receiver-side SNN uses the received radio signal to produce a final inference decision. For example, in the set-up shown in Fig. 1, the transmitter’s sensor observes hand gestures, which are estimated at the receiver side. Conventional SNNs represent information solely in the timing of spikes. However, digital neuromorphic chips, such as Intel’s Loihi 2 supports multi-level, or graded, spikes with minimal additional energy cost [21, 22]. Multi-level spikes encode information both in the timing of the spikes and in their amplitude. There is evidence that biological brains may also leverage spike amplitude variability to encode additional information [28]. As illustrated in Fig. 1(a), multi-level spikes are assigned a payload of m𝑚mitalic_m bits, while conventional spike carry m=0𝑚0m=0italic_m = 0 additional bits of information. SNNs with multi-level spikes have been shown to improve the accuracy of conventional SNN deployments, particularly when the number of timesteps available for inference is limited [29, 30, 31]. In a split computing architecture, the introduction of multi-level spikes creates the challenge of transmitting a larger amount of information per spike on the wireless interface. As illustrated in Fig. 1(b), while in a centralized implementation, larger values of payload size m𝑚mitalic_m are generally beneficial in terms of inference accuracy, in a split computing system, an excessively large payload size can cause a performance degradation due to the lower fidelity of higher-rate transmissions on wireless channels. This work addresses this challenge by investigating the design of both analog and digital transmission schemes for neuromorphic wireless split computing systems with multi-level spikes. I-B Related Work Neuromorphic wireless split computing: Neuromorphic wireless split computing was first studied in [9], in which single-link neuromorphic sensing and computing were integrated with ultra-wideband (UWB) transmission to enable edge-based remote inference. This work was then extended in [10] to a multi-device scenario with frequency-selective channels, demonstrating IR transmission’s compatibility in multi-device environments. In [12], wake-up radios were incorporated into the system to further reduce the overall energy consumption of the system. The work reported in [19] analyzed how spike losses affect the inference accuracy and total neural activity when considering a distributed wireless SNN implementation. Another reference [14] studied a distributed system of edge nodes, each containing a subset of spiking neurons, that communicate with an access point via wireless channels using frequency division multiple access (FDMA) by allocating different frequency bands to different nodes. A neuromorphic integrated sensing and communications system was studied in [11], in which an SNN was deployed at the receiver to decode the transmitted information and detect the possible presence of a target simultaneously. There have been also several reported prototypes for neuromorphic split computing. The transmission model in [32] utilized neuromorphic principles, implemented on Intel’s Loihi chip combined with software-defined radio (SDR) hardware, to build a full-stack neuromorphic wireless communication system that considers both orthogonal frequency division multiplexing (OFDM) and UWB transmission. Another work [33] experimentally demonstrated a communication approach for large-scale wireless asynchronous microsensor networks, enabling the transmission of binary events from thousands of local nodes with high spectral efficiency and low error rates. Multi-level SNNs: A few studies have contributed to advances in multi-level SNNs in centralized implementations. For example, reference [31] proposed a multi-bit transmission mechanism that expands spike representation from a single bit to multiple bits, enriching the information content per spike. In [34], a ternary spiking neuron was introduced to increase information capacity while retaining event-driven, addition-only processing advantages. Additionally, reference [35] designed a spiking neuron that activates integer values during training and maintains spike-driven behavior by extending virtual time-steps during inference for object detection tasks. I-C Main Contributions This paper investigates for the first time neuromorphic wireless split computing with multi-level SNNs. Previous works [12, 9, 10, 11], which focused on conventional SNNs, adopted a UWB interface due to its low power consumption and compatibility with spike-based transmission. In contrast, in this paper we adopt the standard OFDM interface, which provides a more flexible modulation scheme to accommodate multi-level spikes and is more widely applicable and available. In particular, OFDM facilitates prototyping using conventional SDR platforms, such as the Universal Software Radio Peripheral (USRP) [36]. We design and evaluate both digital and analog modulation schemes, which are tested via simulation and via an experimental platform. Overall, the main contributions of this paper are summarized as follows. • We study for the first time a neuromorphic wireless split computing architecture based on multi-level SNNs. Unlike conventional SNNs with binary spikes, multi-level SNNs are able to process richer information by assigning a multi-bit payload to each spike. • We detail a digital modulation scheme based on the address-event representation (AER) of multi-level spikes [21, 22]. According to this implementation, spike addresses and payloads are channel-encoded and modulated on OFDM symbols. If the number of information bits exceeds the available capacity – which is more likely to occur for a larger value of the payload size m𝑚mitalic_m – spikes are dropped, causing a potential decrease in accuracy. Upon channel decoding, the transmitted spikes are reconstructed at the receiver and fed to the receiver-side SNN to produce the final inference decision. • We also detail an analog implementation whereby each output neuron of the transmitter-side SNN is assigned to a fixed subset of OFDM subcarriers and the spikes payloads are transmitted via pulse-amplitude modulation (PAM) on all the assigned subcarriers. This way, the addresses are implicitly transmitted via the location of the PAM symbols across the subcarrier indices. While no spikes are dropped as long as the number of subcarriers is large enough, analog transmission may degrade the quality of the reconstructed spikes due to the reliance of repetition coding. • We evaluate the performance of the proposed neuromorphic wireless split computing architecture based on multi-level SNNs both via simulations and via a basic prototype using a neuromorphic camera [37] and USRP boards. I-D Organization The remainder of the paper is organized as follows. Section II presents background information about multi-level SNN. Section III describes the neuromorphic wireless split computing system with multi-level spikes under study, while the proposed digital and analog transmission schemes are described in Section IV. Section V explains neuromorphic receiver processing, including channel estimation, equalization, and decoding SNN processing. Experimental setting and results are described in Section VI. Finally, Section VII concludes the paper."
https://arxiv.org/html/2411.04547v1,Dynamic Detection of Relevant Objectives and Adaptation to Preference Drifts in Interactive Evolutionary Multi-Objective Optimization,"Evolutionary Multi-Objective Optimization Algorithms (EMOAs) are widely employed to tackle problems with multiple conflicting objectives. Recent research indicates that not all objectives are equally important to the decision-maker (DM). In the context of interactive EMOAs, preference information elicited from the DM during the optimization process can be leveraged to identify and discard irrelevant objectives, a crucial step when objective evaluations are computationally expensive. However, much of the existing literature fails to account for the dynamic nature of DM preferences, which can evolve throughout the decision-making process and affect the relevance of objectives. This study addresses this limitation by simulating dynamic shifts in DM preferences within a ranking-based interactive algorithm. Additionally, we propose methods to discard outdated or conflicting preferences when such shifts occur. Building on prior research, we also introduce a mechanism to safeguard relevant objectives that may become trapped in local or global optima due to the diminished correlation with the DM-provided rankings. Our experimental results demonstrate that the proposed methods effectively manage evolving preferences and significantly enhance the quality and desirability of the solutions produced by the algorithm.","In many real-world optimization problems, candidate solutions often involve numerous numerical features that need simultaneous optimization. Each of these features holds varying degrees of importance for decision makers, making it challenging to prioritize objectives. This often leads to the inclination to model as many objectives as possible [1]. However, such an approach significantly increases computational time, which increases exponentially with the number of objectives [2, 3, 4]. On the other hand, multi-objective problems are characterized by conflicting objective functions, aiming to identify a set of solutions that offer compelling trade-offs, termed non-dominated or Pareto-optimal solutions. The count of Pareto-optimal solutions grows exponentially with the number of objectives [5, 6], further complicating the decision-making process [7]. Thus, it is essential to reduce the number of objectives, if possible. Previous studies on objective reduction have focused mainly on eliminating objectives that are highly correlated with others [7, 1] or those that do not significantly impact the dominance relations among solutions [8, 9]. However, irrespective of the problem structure, certain objectives may be “irrelevant” to the decision maker (DM) and can be excluded from the optimization model. In addition, there are instances where the DM preferences are influenced by factors beyond the optimized objectives, including numerical features observed by the DM but not explicitly targeted by the system. These features, which are “hidden” from the optimizer but relevant to the DM, can affect the satisfaction of the DM if neglected during optimization [10, 11]. The concept of hidden objectives was previously discussed under the term “unmodeled criteria” by Stewart [12] and was defined as objectives that exist within the internal utility function (UF) of the DM, but are not included in the preference model. Later, formal definitions of hidden and irrelevant objectives were proposed in [13]. The detection of hidden and irrelevant objectives is possible using the preference information obtained in interactive evolutionary multi-objective optimization algorithms (iEMOA) [14]. iEMOAs are designed to address the significant performance decline in traditional EMOAs caused by the increase in the number of objectives, which reduces the selection pressure [15, 16, 17]. iEMOAs overcome this issue by utilizing the DM’s preferences and discrimination information to develop a preference model. This preference model is used mainly to break the ties between solutions in the same rank and to generate only the parts of the Pareto front (PF) that are interesting to the DM [18, 19]. By alternating between decision-making and optimization phases and exploiting the DM’s preference information, iEMOAs minimize computational costs and support the DM in finding a desirable solution with minimal cognitive effort. It’s common for a decision maker (DM) to adjust preferences during the optimization process, often as a result of learning and exploring the solution space through interactions [20, 21, 22]. This phenomenon, known as ""preference drift""[23], affects the significance and relevance of objective functions, highlighting the need to account for the dynamic nature of DM preferences[24, 25]. Objective sets can also change due to external factors. This situation arises in dynamic multi-objective optimization problems (DMOPs), where constraints, objectives, and parameters evolve over time, potentially altering the problem’s Pareto front and even the number of objectives [26, 27]. This dynamic aspect is evident in various real-world scenarios, such as balancing project costs with minimizing the makespan in project scheduling as deadlines approach, incorporating the objective of reducing energy consumption when relying on battery power in supply systems, or incorporating flood-related cost reduction into water resource management systems that were initially designed to minimize construction costs, in response to the impacts of climate change [27, 28]. For the first time, Shavarani et al. [13] suggested using the valuable preference information not only for the direction of the search, but also for dynamic refinement of the set of objective functions. However, they did not address the dynamic nature of the DM’s preferences, which significantly contributes to the emergence of both irrelevant and hidden objectives, as these preferences evolve through interactions [22]. Based on [13], which served as a pilot study to detect hidden and irrelevant objectives in iEMOAs, the present work makes several significant contributions to the field considering the dynamic preference behavior of DMs. First, this study simulates various levels of preference drift and analyzes their effects on the performance of the detection technique. Second, it discusses a methodology aimed at mitigating the effects of preference changes when they occur. Third, a new method is proposed to detect preferences changes based on the information provided by a DM, which will be used to trigger appropriate responses to these changes. In [13], it was observed that while the proposed detection method successfully identifies relevant objectives, these objectives are sometimes replaced by irrelevant ones in subsequent iterations once they reach a local or global optimum and become fixed. In this study, we also investigate a method to prevent the algorithm from deactivating relevant objectives solely based on diminished correlation. This method can also be utilized for the early termination of the algorithm, thereby further reducing computational effort. To validate the performance of the proposed detection model and examine the effects of preference drift and the designed responses, we performed a comprehensive experimental study. This study uses a diverse set of UFs to simulate a wide range of DM behaviors, tackle problems of varying dimensions and complexities, and evaluate multiple aspects of the algorithms. In addition, it evaluates various parameters for the interactive method and the associated detection and mitigation strategies. In addition, a sensitivity analysis is conducted to assess the impact of various parameters within the algorithm developed. This comprehensive analysis evaluates the robustness and reliability of the proposed detection method. Experimental results demonstrate that integrating dynamic detection methods and responsive mechanisms significantly enhances the adaptability and efficiency of the optimization process. Our findings show that these methods successfully refine the set of objectives, manage preference changes, and improve the relevance of solutions, thereby reducing computational effort and improving overall performance in dynamic optimization environments. This comprehensive analysis confirms the effectiveness of our approach in addressing real-world complexities and dynamic preference behavior in multi-objective optimization. The remainder of the paper is structured as follows. Section II defines several key concepts that underlie this study. Section III provides a concise overview of previous efforts in objective reduction. In Section IV, the proposed method and its various iterations are expounded upon. Section LABEL:experiments outlines the experimental setup. The results of the experiments are analyzed in Section VI. Lastly, Section VII presents the conclusions drawn from the study and suggests avenues for future research."
https://arxiv.org/html/2411.03948v1,Long-Form Text-to-Music Generation with Adaptive Prompts: A Case of Study in Tabletop Role-Playing Games Soundtracks,"This paper investigates the capabilities of text-to-audio music generation models in producing long-form music with prompts that change over time, focusing on soundtrack generation for Tabletop Role-Playing Games (TRPGs). We introduce Babel Bardo, a system that uses Large Language Models (LLMs) to transform speech transcriptions into music descriptions for controlling a text-to-music model. Four versions of Babel Bardo were compared in two TRPG campaigns: a baseline using direct speech transcriptions, and three LLM-based versions with varying approaches to music description generation. Evaluations considered audio quality, story alignment, and transition smoothness. Results indicate that detailed music descriptions improve audio quality while maintaining consistency across consecutive descriptions enhances story alignment and transition smoothness.","Recent text-to-audio music generation models such as MusicLM [1] and MusicGen [2] are capable of producing high-quality music in the audio domain that aligns with a given textual description. These models typically generate music autoregressively by predicting the next token from a context window, which limits the size of the signal they can model. While the context size is limited, these models can generate longer signals by sliding a context window through time. Regardless of this capability, they have mainly been evaluated with a fixed prompt and for relatively short music durations. For instance, MusicGen [2] was evaluated considering 30-second music pieces, each generated from a single music description. In this paper, we are interested in evaluating whether text-to-music models can maintain music quality while generating long music pieces, where music descriptions change over time. It is important to evaluate text-to-music models considering long music pieces (greater than 30 seconds, for example) because many music production scenarios involve music durations longer than one can generate with a single short audio context window (e.g., pop music composition, jazz improvisation, soundtrack generation). One key problem of generating long sequences from a small context is that a model has to split the generation into multiple parts, ensuring that the independent parts are smoothly connected in the final composition. Moreover, one might change the initial prompt at any time step, steering the composition in a different direction, and the model must consider both the previous audio context and the new prompt. Figure 1: At every 30 seconds of gameplay, Babel Bardo transcribes the players’ speeches into a text sisubscript𝑠𝑖s_{i}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT using a Speech Recognition system and uses a Large Langue Model (LLM) to map sisubscript𝑠𝑖s_{i}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT into a music description disubscript𝑑𝑖d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT that matches the scene described by the players. This music description is given to a Text-to-Music system that generates a 30-second piece aisubscript𝑎𝑖a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT directly in the audio domain. In this paper, we investigate long generation with text-to-audio models in the context of Tabletop Role-Playing Games (TRPGs). In this scenario, a music generator takes speech as input and must generate music that matches the story being told by the players. We chose this problem because it inherently poses the challenge of long music generation, where prompts have to change over time to adjust for different story scenes. We also use TRPGs as a research object because TRPG players often enhance their gaming experience by manually selecting songs to play as background music [3], which allows us to compare the results of a generator against a human baseline. To investigate the capabilities of current text-to-music models in generating background music for TRPG stories, we’ve built a system called Babel Bardo, which is inspired by Bardo Composer[4], a system that generates symbolic music by transcribing players’ speeches into text and conditioning an autoregressive model with the emotional tone of this text, as given by an emotion classifier. Different than Bardo Composer, Babel Bardo composes music directly in the audio domain by leveraging a Large Language Model (LLM) to transform the speech transcriptions into music descriptions every 30 seconds of gameplay. These descriptions are then given to a text-to-music model to generate a piece of music for that current moment of the story. Figure 1 shows an overview of our system. Babel Bardo is inspired by Hermann1[5], which uses LLMs and text-to-music models to generate soundtracks for films. We compared four different versions of Babel Bardo in two TRPG campaigns played on YouTube: Call of the Wild (in English) and O Segredo na Ilha (in Brazilian Portuguese). The first version is our baseline and uses the speech transcriptions directly as prompts for a text-to-music model. All other versions use an LLM to transform the transcriptions into music descriptions. The second one follows the Bardo Composer approach and applies an LLM as an emotion classifier. The music description follows a template that is adjusted based on the emotion given by the LLM. The remaining two versions use the LLM to produce a complete music description; however, one generates a new description for every transcript, while the other can just continue the previously generated segment if the scene hasn’t changed. We evaluated our models according to audio quality, alignment with the story, and transition smoothness between transcriptions. Results suggest that while detailed music descriptions contribute to improved audio quality, maintaining consistency across consecutive descriptions helps achieve smoother transitions between musical segments. Furthermore, our findings indicate that emotion serves as an effective signal for aligning generated music with TRPG narratives."
https://arxiv.org/html/2411.03726v1,PropNEAT - Efficient GPU-Compatible Backpropagation over NeuroEvolutionary Augmenting Topology Networks,"We introduce PropNEAT, a fast backpropagation implementation of NEAT that uses a bidirectional mapping of the genome graph to a layer-based architecture that preserves the NEAT genomes whilst enabling efficient GPU backpropagation. We test PropNEAT on 58 binary classification datasets from the Penn Machine Learning Benchmarks database, comparing the performance against logistic regression, dense neural networks and random forests, as well as a densely retrained variant of the final PropNEAT model. PropNEAT had the second best overall performance, behind Random Forest, though the difference between the models was not statistically significant apart from between Random Forest in comparison with logistic regression and the PropNEAT retrain models. PropNEAT was substantially faster than a naive backpropagation method, and both were substantially faster and had better performance than the original NEAT implementation. We demonstrate that the per-epoch training time for PropNEAT scales linearly with network depth, and is efficient on GPU implementations for backpropagation. This implementation could be extended to support reinforcement learning or convolutional networks, and is able to find sparser and smaller networks with potential for applications in low-power contexts.","The NeuroEvolution of Augmenting Topologies (NEAT) algorithm [1] is a genetic algorithm for training sparse neural networks that has been used for a range of purposes, especially as a competitor to reinforcement learning in control systems [2, 3]. NEAT evolves complex networks by incrementally adding nodes and edges, optimizing weights through genetic techniques. Its relative simplicity and lack of need for advanced hardware or GPUs have driven a high level of interest, including demonstrations of AI on YouTube channels when applied to computer games [4]. However, NEAT has had limited application in tabular data due to slow convergence and subpar performance compared to alternatives such as dense neural networks and decision trees. The limitation derives from the use of genetic-based weight optimisation. Gradient-descent based methods have been used but have also been limited due to the inherently sequential implementation of the activation of nodes in NEAT which does not permit the efficiencies of GPU-based backpropagation to be applied. Here, we present the PropNEAT algorithm, first covering a naive implementation of backpropagation before presenting the PropNEAT algorithm itself. We cover the challenges that arise from the genetic algorithm, the details of the construction and mapping of the layer-based representation that allows for efficient linear-algebra operations, and the details of other secondary changes to NEAT including topological change rates that are subsequently required when using this method. We then present the details of the experiments run to evaluate this algorithm, namely the performance comparison against other predictive models, an ablation experiment against the naive and original implementations, and performance characterisation of the training time of the model. We present the full results and analysis of these experiments, discussing the implications and conclusions that we can draw."
https://arxiv.org/html/2411.03604v1,Temporal-Difference Learning Using Distributed Error Signals,"A computational problem in biological reward-based learning is how credit assignment is performed in the nucleus accumbens (NAc) to update synaptic weights. Much research suggests that NAc dopamine encodes temporal-difference (TD) errors for learning value predictions. However, dopamine is synchronously distributed in regionally homogeneous concentrations, which does not support explicit credit assignment (like used by backpropagation). It is unclear whether distributed errors alone are sufficient for synapses to make coordinated updates to learn complex, nonlinear reward-based learning tasks. We design a new deep Q-learning algorithm, Artificial Dopamine, to computationally demonstrate that synchronously distributed, per-layer TD errors may be sufficient to learn surprisingly complex RL tasks. We empirically evaluate our algorithm on MinAtar, the DeepMind Control Suite, and classic control tasks, and show it often achieves comparable performance to deep RL algorithms that use backpropagation.","Computer science and neuroscience have enjoyed a longstanding and mutually beneficial relationship. This synergy is exemplified by the inception of artificial neural networks, which drew inspiration from biological neural networks. Neuroscience also adopted temporal-difference (TD) learning [62] from reinforcement learning (RL) as a framework for biological reward-based learning in the midbrain [23, 59]. At the intersection of these ideas, deep RL has much benefited from and contributed to interdisciplinary progress between the two fields [45, 11]. An interesting problem raised in biological learning is how signals transmitted by the neuromodulator dopamine computationally induce coordinated reward-based learning. In the mesolimbic system, dopamine is synthesized by dopamine neurons in the ventral tegmental area (VTA) and transmitted through the mesolimbic pathway to several regions, including the nucleus accumbens (NAc). There, it is synchronously distributed in regionally homogeneous concentrations [58], and serves as a reward prediction error signal for synaptic adjustments via TD learning [9, 23].111NAc dopamine also serves many roles beyond signaling reward prediction errors [18]; its full responsibilities are an active area of research. We only focus on its role in error signaling, which is most pertinent to our problem. Figure 1 shows a conceptual illustration: the medium spiny neurons in the NAc receive error signals distributed locally in their region via dopamine. Computationally, however, this theory faces the credit assignment problem [25]: the individual synaptic updates using just local errors must somehow work in coordination to improve the collective prediction.222For clarity, this is different from the temporal credit assignment problem, oft discussed in RL literature. Are distributed error signals alone sufficient to coordinate neurons to learn complex reward-based learning tasks? Figure 1: Simplified illustration of dopamine distribution in the NAc. Dopamine is synthesized in the VTA and transported along axons to the NAc, where it is picked up by receptors in medium spiny neurons. Dopamine concentrations (error signals) are locally homogenous, but can vary across regions. Connections between NAc neurons not shown. Deep RL typically solves the credit assignment problem using backpropagation (BP) [57]. BP propagates the global error backwards through the network, and computes the gradient (w.r.t. the global error) of each layer’s synaptic weights sequentially via the chain rule. In contrast to the synchronously distributed errors in the NAc, BP involves neurons sequentially communicating error signals with each other. This sequential propagation explicitly coordinates learning, but also creates dependencies: each layer’s updates depend on the error of subsequent layers. This is known as the update locking problem, which is biologically implausible [52], limits parallelization, and cannot explain how distributed error signals may support coordinated learning. Recent ML research on more biologically plausible alternatives to BP may offer critical insights. PEPITA [21] and Forward-Forward (FF) [30] both replace BP’s backward learning pass with a second forward pass to address update locking. Most relevantly, Hinton [30] made a surprising discovery: layers can learn useful representations for subsequent layers even when trained independently of the errors of those subsequent layers. In FF, each layer generates its own prediction and error, and is only trained to learn hidden representations that minimize the local error. The subsequent layer takes these representations as input, and achieves better performance over training, despite being unable to send errors to the previous layer. This improves the collective global prediction without explicit, sequential coordination of error signals. To the best of our knowledge, these learning principles have not been explored in RL or biological reward-based learning. Drawing a novel connection, we hypothesize that the computational mechanisms that enable FF’s independent, per-layer training may also enable distributed error signals to support coordinated reward-based learning. To test our hypothesis, we design Artificial Dopamine (AD), a new deep Q-learning algorithm that trains RL agents using only synchronously distributed, per-layer TD errors, and evaluate its performance on a range of discrete and continuous RL tasks. This provides a potential explanation for credit assignment in NAc dopaminergic learning at the algorithmic level of analysis [42]. Our results show that AD can solve many common RL tasks often as well as deep RL algorithms that use backpropagation, despite not propagating error signals between layers. Thus, we computationally demonstrate that distributed errors alone may be sufficient for coordinated reward-based learning. AD networks inherit several ideas from FF, which differ from traditional neural networks in two significant ways. First, each layer in an AD network computes its own prediction and receives a corresponding error (Section 3.1). This per-layer error mirrors the locally homogenous distribution of dopamine, and the computation of error and updates can be synchronously parallelized across layers; there are no dependencies across layers. Second, we use forward333“Forward” and “backward” are widely used in deep learning literature both to describe direction in time and in the order of layers. This can be confusing. For the remainder of this paper, we use “forward/backward” when describing time, and “upper/lower” when describing position among layers, as shown in Figure 2. connections in time to send activations from upper to lower layers (Section 3.2). This provides an information pathway for upper layers to communicate with lower layers using activations, rather than error signals, and empirically improves performance. Figure 2 outlines our architecture, unfolded in time. The AD cell (Figure 3) is where we differ most significantly from FF. Its role is to compute the local Q-prediction and TD error. FF is designed to separate real from fake (generated) data, a binary classification task. But the NAc is theorized to predict value, a regression task, and therefore needs more precision. To achieve this, we introduce an attention-like mechanism for non-linear regression without using error propagation (Section 3.1). We evaluate AD on 14 discrete and continuous RL tasks from the MinAtar testbed [70], the DeepMind Control Suite (DMC) [64], and classic control environments implemented in Gymnasium [65]. MinAtar tasks are miniaturized versions of Atari games, and DMC contains continuous control tasks with simulated physics. These environments are complex enough to reflect many challenges in modern RL [14], yet remain adequately tractable so as not to necessitate extra components like convolutional layers, which may be confounding when attributing performance. We benchmark AD against DQN [45], SAC [26], and TD-MPC2 [28] baselines, and conduct ablation studies to examine the effects of the forward connections and additional layers. Our results in Figures 4 and 8 show that AD learns to solve many of these tasks with comparable performance to the baselines, using just per-layer TD errors. Our code is available at https://github.com/social-ai-uoft/ad-paper. To summarize our core contributions: • Are distributed TD error signals sufficient to solve credit assignment and learn complex reward-based learning tasks? We construct a computational example of an RL agent that learns using only distributed, per-layer TD errors. This provides evidence that dopamine-distributed signals alone may be enough to support reward-based learning in the nucleus accumbens. • We design a Q-learning algorithm, Artificial Dopamine, to train our agent. Like Forward-Forward, AD does not propagate error signals across layers. Unlike FF, we introduce a novel cell architecture to compute Q-value predictions, as Q-learning is a regression task. • We evaluate our agent on 14 common RL tasks in discrete and continuous control, and show that AD can often achieve comparable performance to deep RL algorithms, without backpropagation."
https://arxiv.org/html/2411.03588v1,An Experimental Study on Decomposition-Based Deep Ensemble Learning for Traffic Flow Forecasting,"Traffic flow forecasting is a crucial task in intelligent transport systems. Deep learning offers an effective solution, capturing complex patterns in time-series traffic flow data to enable the accurate prediction. However, deep learning models are prone to overfitting the intricate details of flow data, leading to poor generalisation. Recent studies suggest that decomposition-based deep ensemble learning methods may address this issue by breaking down a time series into multiple simpler signals, upon which deep learning models are built and ensembled to generate the final prediction. However, few studies have compared the performance of decomposition-based ensemble methods with non-decomposition-based ones which directly utilise raw time-series data. This work compares several decomposition-based and non-decomposition-based deep ensemble learning methods. Experimental results on three traffic datasets demonstrate the superiority of decomposition-based ensemble methods, while also revealing their sensitivity to aggregation strategies and forecasting horizons.","Traffic flow forecasting is one of its most important tasks for intelligent transport systems (ITS) in daily traffic management and operations [23]. Several operations, such as incident management, require reliable flow forecasting for a short horizon in future to support decision-making. However, accurate forecasting remains challenging due to the complex patterns in time-series traffic data. Factors like road congestion, vehicle breakdowns and traffic signal timing [17, 15] contribute to irregular and unpredictable patterns in traffic data, making accurate forecasting difficult to achieve. Traditional statistical and shallow machine learning techniques often struggle to capture the complex patterns within this data, rendering them less effective in this context. In contrast, deep learning techniques are better suited to adapt to the intricate nature of time-series traffic data [21]. However, these methods can be biased, overfitting the intricate details of flow data, leading to poor generalisation. Ensemble learning [7] is a potential solution to mitigate the limitations of deep learning by combining the outputs of multiple models [25]. Recent advances in ensemble learning have promoted various kinds of deep ensemble learning approaches, including using conventional ensemble methods [14, 28] and time-based ensemble methods [3]. Among these ensemble learning methods, the decomposition-based ensemble is a less explored category that transforms time-series into simple components for modelling [20]. The components extracted from decomposition-based methods may reduce the complexity of the data, and thus yield a more robust solution. However, there is a lack of comparative studies on whether this approach can better benefit deep learning models than non-decomposition-based ensemble methods. Currently, the comparison studies are mostly restricted to certain types of methods [20, 12, 9]. This paper compares several decomposition-based ensemble methods with conventional bagging and time domain multi-resolution ensemble methods [3] under the traffic flow forecasting tasks. The main contributions of this paper are: 1. An empirical study is conducted to assess the helpfulness of decomposition-based ensemble methods for deep learning models in traffic flow forecasting tasks. Results yield that decomposition-based methods better enhance the performance of deep learning models than baseline methods. 2. We explored the effectiveness of optimised aggregation for decomposition methods. The results show that decomposition methods are sensitive to the aggregation methods. 3. We investigated the impact of inputs and forecasting horizons for decomposition methods. Results indicate that these methods are sensitive to the input and do not always benefit from extensive data."
https://arxiv.org/html/2411.02842v1,"Metaheuristics for the Template Design Problem: Encoding, Symmetry and Hybridisation","The template design problem (TDP) is a hard combinatorial problem with a high number of symmetries which makes solving it more complicated. A number of techniques have been proposed in the literature to optimise its resolution, ranging from complete methods to stochastic ones. However, although metaheuristics are considered efficient methods that can find enough-quality solutions at a reasonable computational cost, these techniques have not proven to be truly efficient enough to deal with this problem. This paper explores and analyses a wide range of metaheuristics to tackle the problem with the aim of assessing their suitability for finding template designs.We tackle the problem using a wide set of metaheuristics whose implementation is guided by a number of issues such as problem formulation, solution encoding, the symmetrical nature of the problem, and distinct forms of hybridisation. For the TDP, we also propose a slot-based alternative problem formulation (distinct to other slot-based proposals), which represents another option other than the classical variation-based formulation of the problem.An empirical analysis, assessing the performance of all the metaheuristics (i.e., basic, integrative and collaborative algorithms working on different search spaces and with/without symmetry breaking) shows that some of our proposals can be considered the state-of-the-art when they are applied to specific problem instances.","Many problems in the area of manufacturing are related to reducing the waste of the raw material used in the production process (Kasemset et al., 2015; Wang et al., 2016). In general, achieving this objective requires a huge effort in the analysis of the problem in order to obtain a suitable model that allows greater production with minimum waste. The template design problem (TDP) is a challenging example of this. The TDP arises in industrial settings in which variations of a given product must be produced, each of them requiring a particular packaging (typically with different printing patterns). The production of these packages entails minimising the use of cardboard (or any other raw material used). Appropriate templates for printing these packages must therefore be designed, hence the TDP. The TDP was first described by Proll and Smith (1998) who observed this problem arising at a local colour printing firm. Roughly speaking, we can assume a certain product has to be manufactured with distinct variations (e.g., different flavours of cereal flakes), each one requiring a similar –but different– packaging. A printing machine is used to produce this packaging. This machine is configured with a given template, which is subsequently pressed on sheets of raw material (e.g., cardboard). Given the large number of items required, a template comprises several slots, each of them filled with a given variation of the product, which are printed on each pressing. In addition, there can be more than one such template. This means that the problem is twofold: (i) determine the design of each template, namely which variations are included in each slot, and (ii) determine the optimal usage of these templates. The latter requires a given criterion to be optimised, for example minimising the manufacturing time (i.e., minimising the number of pressings) or minimising the waste, given the known demands of each variation. We consider here the latter criterion (i.e., optimise the use of raw material). In the literature, one can find various proposals that deal with the TDP, including constraint programming techniques, mathematical programming and integer linear programming. Proll and Smith used an integer linear model to solve this problem. It must be noted that the problem is intrinsically symmetrical in nature, meaning that one solution can be represented in different ways. This can exert an influence on the way the search is conducted (and ultimately on the performance of the algorithm). Indeed, it has been shown that an adequate treatment of such symmetries with symmetry-breaking techniques can reduce the complexity of the search (Janßen, 2016). In this sense, one can consider the equivalence among solutions from different perspectives. For instance, with a numeric value (Benhamou, 1994) or with a geometric approach (Backofen and Will, 2002), just to name a couple In the last few decades, a number of methods have been applied to deal with this interesting issue (Benhamou, 1994; Fahle et al., 2001; Gent and Smith, 1999). The primary method, in constraint and integer programming, to cope with symmetries consists in breaking them, i.e., removing symmetries with the goal of reducing the search space of the problem. With regard to metaheuristics, there are not many proposals that deal with the TDP, although it has been proved that metaheuristics are efficient methods in the solving of manufacturing problems (e.g., (Meeran and Morshed, 2012; Lin and Chiu, 2018)). More recently, we handled the problem with some basic metaheuristics (i.e., local searches, and genetic algorithms) (Rodríguez et al., 2010, 2011). These techniques demonstrated a moderate success as they performed reasonably well for small instances of the problem but their performance worsened when they were applied to more complex instances. In fact, the problem formulation is ideal for the employment of integer linear programming (ILP) techniques, as it has been proven in the literature. However, metaheuristics can also be suitable for tackling the problem as they offer a worthwhile balance between the quality of the solutions found and the computational cost to find them. The main contribution of this paper is to explore this issue, that is to say, the goodness of metaheuristics to tackle the TDP. Thus, this paper contains the description of a wide range of metaheuristics, of distinct nature, to cope with the problem. Our proposals can also be viewed as an alternative mechanism to those already reported in the literature, to tackle the TDP. Thus, in general, this paper tries to shed light on the solving of the TDP using metaheuristics, and considers three main issues for the design of these techniques: problem symmetry, problem formulation (and related aspects such as the search space representation), and hybrid forms of collaboration between metaheuristics (i.e., integrative vs. cooperative schemes). So, this paper firstly considers a standard procedure, used in constraint and integer programming, for symmetry breaking, that is to say, the addition of new constraints to the problem with the aim of removing symmetries and ease its solving. Note however, that other mechanisms of dealing with symmetries have been proposed for genetic algorithms and local search; for instance, in Prügel-Bennett (2004) symmetry breaking is modelled using stochastic differential equations and their associated diffusion equations. Secondly, based on the assumption that the candidate encoding can drastically affect the search process, this paper also considers an alternative integer slot-based representation scheme for the TDP solutions (that we have called the alternative model, in response to the variation-based model that has been classically taken as reference). Note however that our proposal is not the first slot-based scheme proposed to deal with the problem; a 0/1 variable slot-based approach was employed in Prestwich et al. (2006). In this context, we describe a number of optimisation methods to deal with TDP that are derived from all the possible scenarios that arise from the combination of these two encodings and the decision on whether to apply symmetry breaking. Each scenario is firstly tackled with a number of basic metaheuristics, including local search (LS) and genetic algorithms (GA). In addition, it has been proven that the use of hybrid algorithms represents a very strong mechanism for improving the search capability of optimisation algorithms (Ting et al., 2015). Generally speaking, hybridisation can be viewed from two broad point of view (Raidl, 2006): integration and cooperation. Integration usually refers to the adding of one optimisation technique as a component of another optimisation method, whereas cooperation is generally related to the establishment of a way to exchange information between methods that are applied one after another or in parallel. Meanwhile, Crainic and Toulouse (2008), consider hybridisation basically as a synergistic union of different algorithmic approaches, such that at least one of them represents an exploitation mechanism of knowledge. In this paper, we also explore this path, and use an integrative mechanism which embeds a local search (LS) inside a genetic algorithm (GA) resulting in a memetic approach (MA) (Neri et al., 2012). We also develop cooperative algorithms in which both the basic and integrative metaheuristics (i.e., LSs, GAs and MAs) work independently to handle the problem and interchange information in certain synchronisation moments that have been previously preset. All the hybrid metaheuristics described in this paper are applied to solving the TDP for first time (to the best of our knowledge). One of the novelties of these hybrid methods is that they are allowed to link metaheustics that are very different to each other in the sense that the connected methods can vary in their encoding schemes, the problem formulation that is handled, the use or absence of constraints for symmetry breaking, and/or even the nature of the method (e.g., an LS or a MA). We have also conducted an experimental evaluation and have compared the performance of all the metaheuristics proposed here. The results show that some of our metaheuristics optimisation methods for the TDP can be considered the state-of-the-art when they are applied to specific problem instances of the problem. Thus, the main contribution of this paper is to show the goodness of metaheuristics to address the TDP. In addition, the paper suggests that other possible forms of hybrid metaheuristics could be suitable for the solving and optimisation of TDPs, problems that have traditionally been efficiently tackled by ILP methods."
https://arxiv.org/html/2411.03082v1,Self-supervised cross-modality learning for uncertainty-aware object detection and recognition in applications which lack pre-labelled training data,"This paper shows how an uncertainty-aware, deep neural network can be trained to detect, recognise and localise objects in 2D RGB images, in applications lacking annotated training datasets. We propose a self-supervising “teacher-student” pipeline, in which a relatively simple “teacher” classifier, trained with only a few labelled 2D thumbnails, automatically processes a larger body of unlabelled RGB-D data to teach a “student” network based on a modified YOLOv3 architecture. Firstly, 3D object detection with back projection is used to automatically extract and “teach” 2D detection and localisation information to the student network. Secondly, a weakly supervised 2D thumbnail classifier, with minimal training on a small number of hand-labelled images, is used to teach object category recognition. Thirdly, we use a Gaussian Process (GP) to encode and teach a robust uncertainty estimation functionality, so that the student can output confidence scores with each categorization. The resulting student significantly outperforms the same YOLO architecture trained directly on the same amount of labelled data. Our GP-based approach yields robust and meaningful uncertainty estimations for complex industrial object classifications. The end-to-end network is also capable of real-time processing, needed for robotics applications. Our method can be applied to many important industrial tasks, where labelled data-sets are typically unavailable. In this paper, we demonstrate an example of detection, localisation, and object category recognition of nuclear mixed-waste materials in highly cluttered and unstructured scenes. This is critical for robotic sorting and handling of legacy nuclear waste, which poses complex environmental remediation challenges in many nuclearised nations.","I-A Motivation This paper addresses the computer vision problems of detecting, recognising and localising objects. Our proposed method has broad potential to be used for many applications and object types. It is especially useful for industrial or applied problems, where large amounts of application-specific annotated training data are typically unavailable. We demonstrate such an application with a motivating example of robotics challenges in extreme environments, for example, robotic sorting of nuclear waste objects and materials, for the safe remediation of legacy nuclear facilities [1]. The UK alone contains an estimated 4.9 million tonnes of legacy nuclear waste [2], much of it dating back many decades. Waste items can comprise numerous objects, e.g. contaminated gloves, respirators, swabs, tools, containers, and pipework sections. At the Sellafield site (dating back to the 1940s), a new plant is being built which will use robot arms for the next 50 years. These will cut open old containers, for which there is some uncertainty about the contents. The robots must sort and identify waste items, separate them according to the estimated hazard level, and repackage them into safer modern containers. In addition to the potential for computer vision to help guide robots during e.g. pick and place operations, there is also a need to create inventory lists for the contents of the new containers. Since the quantities of waste are extremely large, automating such inventory generation will be necessary. It is also an essential requirement to estimate and document the uncertainty associated with the inventory for each storage container. This, and many other real-world industrial problems, pose particular challenges for modern computer vision approaches. Large, annotated, and ground-truthed data-sets are generally unavailable and may be prohibitively difficult, slow, or expensive to create. For example, it has been estimated that labelling the benchmark ImageNet dataset [3], with 14 million images, took approximately 22 human years of effort. Meanwhile, the objects and materials in industrial (or domestic) waste-handling problems are extremely diverse and unstructured, often appearing in arbitrary random heaps. For example, a contaminated rubber glove can appear in numerous different shapes and configurations. To incorporate such a perception system with robots, e.g. for autonomous grasping, relatively fast processing speeds are needed. Furthermore, for optimal robotic action planning [4, 5], the system needs to make explicit use of representations of uncertainty. I-B Background In recent years, modern computing hardware has enabled rapid advances in computer vision recognition tasks, via deep neural network structures. However, these methods are predominantly based on extensive supervised learning, depending on very large training data-sets, in which each image must be laboriously hand-labelled with ground-truth information. As a result, much of the deep learning computer vision literature is demonstrated on open-source benchmark data-sets. Many of these benchmark data-sets feature domestic objects, e.g. furniture, kitchen utensils etc., which do not readily transfer to practical industrial problems. In addition to the labour-intensive nature of collecting and hand-labelling data, such human labour can be prone to error. Sometimes an object may not be accurately bounded by bounding boxes or may be assigned a wrong class label. In some cases, it is difficult for a human annotator to categorize some images [6]. Many objects, e.g. a cat with a long tail, or a frying pan with a long handle, do not neatly fit within a bounding box. It is not clear what the correct definition of a bounding box should be, since a complete bounding box will contain large areas of non-object background pixels. Conversely, a box that is tightly fitted to the body of the cat or the frying pan, will omit key parts and features of these objects (the tail or handle). An uncertainty-aware approach to image-based learning, is valuable for such problems. State-of-the-art CNN based object detectors such as Mask Region-based convolutional neural network (Mask R-CNN) [7], Fast Region-based convolutional neural network (Fast R-CNN) [8] and Single Shot MultiBox Detector (SSD) [9] have demonstrated impressive object detection capabilities. However, most of these models are unable to estimate the uncertainty accompanying each detection or classification. More recently, object detection YOLOv3[10] network does assign a confidence estimate alongside its output detections. However, this capability is conventionally trained by inputting confidences that are derived from a simplistic calculation (essentially defining “confidence” as the proportion of the network’s output box which overlaps with the ground-truth bounding box). Figure 1 shows an image from our dataset [11], with detection results from a conventional YOLOv3 [10] network. It can be seen that this model was not successful in recognising the object categories accurately, with most of the objects labelled as “bottle”. Furthermore, the confidence scores assigned to each detection are questionable, e.g. a “plastic-pipe” is detected object as “bottle” with a high confidence score of 0.63. Figure 1: Detections on example image from our nuclear waste test dataset using the standard version of YOLOv3. Note how the conventional YOLOv3 can assign overly high confidence numbers to incorrect classifications. In this work, we have chosen to use YOLOv3 [12] over its more recent variants for the following reasons. You Only Look Once (YOLOv3) is a fast object detector that integrates the feature pyramids network and achieves a good balance between detection accuracy and detection speed, making it one of the most popular methods in this field. Redmon and Farhadi [10] proposed a balanced and optimised algorithm regarding the speed and accuracy of object detection. Later variants of YOLOv3 has been developed such as v4,v5,v6 and v7 [13]. New variants have developed an efficient backbone and a more understandable label assignment strategy and have minimal to no impact on calculation overhead. Despite that, YOLOv3 is still providing the base network to these variants. It is still very popular in the research community as it provides a simple implementation and deployment structure [14]. Ge et al. [15] articulate this perspective by stating that, while YOLOv4 and YOLOv5 have indeed made significant strides in object detection accuracy, they may potentially grapple with issues pertaining to over-optimization. The YOLOv3 algorithm is a popular choice in the industry for its high detection efficiency among the YOLO family, with a broad range of applications in various domains such as human nail abnormality detection [16], pavement distress detection [17], pedestrian detection [18], tracking smart robot car [19] apple growth stage detection [20], industrial distress detection [21], and perception systems for driver-less cars [22]. The main motivation for our use of YOLOv3 is that it incorporates functionality for explicitly encoding and outputting an estimate of confidence alongside its object categorization decisions. Figure 2: The outline of the proposed method for rapidly boot-strapping a learning system, in a semi-supervised manner, requiring relatively sparse data. This is accomplished by combining Gaussian Processes and YOLOv3 in a Knowledge Distillation paradigm. , Later versions of YOLO do not possess this functionality. In our work, we modify and enhance this uncertainty-awareness functionality by using a Gaussian Process to model uncertainty in a teacher classifier. The teacher then teaches robust uncertainty estimations to our modified YOLO3 classifier during teacher-student training. I-C Approach and novel contributions We use our previous work [23] as a baseline method, which also introduced our nuclear waste objects computer vision data-set. This method successfully detected objects, and accurately assigned category labels compared to contemporary methods from the literature. However, it was computationally expensive (execution time for detection was 100ms-200ms). It sometimes made false positive detections of background regions as objects, and object category assignment could be noisy and variable. It also struggled to detect small objects or partially occluded objects in cluttered scenes. Most importantly, this system also lacked an “uncertainty-aware” functionality. In this study, we address these problems. We describe a new approach which yields more accurate detections, with less computational complexity, while adding a new functionality enabling the system to output confidence estimates to accompany each detection. Common sense suggests that a robust model, with a meaningful and useful “uncertainty-awareness” capability, should output low confidence scores whenever it outputs false-positive detections or incorrect object category labels. In contrast, as seen in Fig. 1, in our example nuclear waste application we can see that the conventional approach to training confidence estimates in YOLOv3, often results in inappropriate output confidence scores during testing. To provide an improved uncertainty-awareness capability, this study proposes the fusion of a Gaussian Process (GPC) model for classification with a YOLOv3 detector, in a “teacher-student” paradigm, enabling real-time detection accompanied by robust and useful confidence scores. In contrast to previous methods for assigning confidences (discussed above), we adopt a “teacher-student” approach (related to “knowledge distillation” methods [24]. We use the GPC as the teacher and YOLOv3 as the student. The GPC proposes confidence scores associated with object image thumbnails and teaches these confidences to the YOLOV3 network during its object category recognition training. Figure 3: Deep kernel learning architecture with Stochastic variational inference procedure. Note that previously, “knowledge distillation” has been used in a very different way. Typically a complex (and computationally expensive) strong classifier is used as the “teacher”, and trains a simpler (and cheaper) classifier which serves as the “student” [25]. I.e. a large amount of knowledge, encoded in the large and complex teacher network, is “distilled” [26] into a much smaller and computationally cheaper student network. In contrast, a key novelty of our work is that we show how a relatively simple and cheap classifier can be bootstrapped as a “teacher”, which generates inputs to a much more complex and powerful “student” classifier during its training. The resulting strong classifier (student) then outperforms its teacher and also outperforms the same network structure when trained in a conventional way, without the teacher, on the same data-set. First, we use a 3D-detector from our previous work [23] to generate objectness proposals from RGB-D video streams, and generate corresponding 2D object thumbnails from the RGB-D data. We manually label a small number of these thumbnails. Some are retained for testing, and a few are used as a training input to a“weakly supervised” system. The system then bootstraps on this small input data, becoming “self-supervised’. I.e. based on this small labelled data, our system effectively creates and labels more training data, while training itself by using the teacher-student paradigm. We train the classic pre-trained Resnet-50v2 on this small labelled dataset, by using transfer learning [27]. Then we augment this Resnet network with a Gaussian Process (GP) model to provide a sophisticated functionality for learning uncertainty-awareness. The resulting “teacher” then generates a much larger scale of automatically labelled, or “self-labelled” data as inputs to the training of the YOLOV3 network. Meanwhile, the GP component of the teacher is used to provide input to the uncertainty-awareness learning component of our modified YOLOv3 network (in contrast to the more simplistic uncertainty learning approach of the original YOLOv3 as discussed above). The knowledge of the “teacher” network is thus “distilled” into a YOLOv3 “student” network, using the variation of loss for classification. This variation of loss is composed of knowledge distillation loss and the sum of squared loss. This technique improves the classification loss compared to the original YOLOV3 object detector method. The resulting network, informed by the GP component of the teacher during training, also generates significantly improved confidence/uncertainty values for each classification, compared to the original YOLOv3. The main contributions of this paper are as follows: Figure 4: Schematic of the Knowledge distillation pipeline for categorization. a) The transfer of knowledge from the teacher backbone, as shown in Figure 3, to the student backbone utilizing the YOLOv3 architecture. (b) Illustration of the YOLOv3 output structure, where bounding box coordinates are generated by a 3D detector, defining the spatial location and size of each detected object within the 3D space. The objectness score indicates the confidence level that the bounding box contains an object. The final part of the output comprises probabilistic class scores, which provide a probabilistic distribution over possible classes, thereby incorporating uncertainty in the classification process. 1. A self-supervised 2D objectness detection, trained by automatically extracting and labelling 2D RGB object thumbnails from 3D RGB-D data. We use 3D conditional clustering within the point clouds to automate the extraction and labelling of bounding boxes without human effort. This automatically generates 2D object bounding box annotations as inputs for training the YOLOV3 network, hence our term “self-supervised” learning. 2. A novel use of “teacher-student” and “knowledge distillation” concepts, to enable boot-strapping a weak classifier (based on a small amount of annotated training data) to train a more complex and strong classifier (by automatically generating and feeding it training examples). Not that this is significantly novel in contrast to conventional knowledge distillation methods. Such methods use complex, strong classifiers, to teach effective classification capabilities to a smaller, simpler classifier (e.g. for implementations on small processors). In contrast, we show how to invert this concept, using a weakly trained classifier to automatically generate large amounts of training data for teaching a larger and more complex student classifier, which eventually outperforms the teacher. 3. We propose a new way to enable a classifier network to learn uncertainty-awareness, i.e. the ability to output a confidence value alongside each object detection and classification decision. In contrast to the conventional YOLOv3 approach, by using items 1) and 2) we enable self-supervised training of a Gaussian Process Classifier (GPC) as part of the “teacher” in our teacher-student paradigm. The purpose of the GPC is to teach confidence/uncertainty scores to the YOLO student network, alongside its learning of objectness detection and object category values during teacher-student training. This yields significantly better quality confidence outputs than conventional approaches to YOLOV3 confidence training, in our example industrial waste objects application. 4. We redesign the loss function of classic knowledge distillation, which works more effectively with our waste object data-set and achieves SOTA performance, while reducing computational complexity. 5. Our semi-supervised and self-supervised methods can be readily applied to new industrial applications, where no large ground-truthed or annotated training data-sets exist. We demonstrate this capability by using our unique nuclear waste objects data-set, motivated by the robotics and AI challenges of environmental clean-up and remediation on legacy nuclear sites in hazardous environments,"
https://arxiv.org/html/2411.02791v1,Language Models and Cycle Consistency for Self-Reflective Machine Translation,"This paper introduces a novel framework that leverages large language models (LLMs) for machine translation (MT). We start with one conjecture: an ideal translation should contain complete and accurate information for a strong enough LLM to recover the original sentence. We generate multiple translation candidates from a source language A𝐴Aitalic_A to a target language B𝐵Bitalic_B, and subsequently translate these candidates back to the original language A𝐴Aitalic_A. By evaluating the cycle consistency between the original and back-translated sentences using metrics such as token-level precision and accuracy, we implicitly estimate the translation quality in language B𝐵Bitalic_B, without knowing its ground-truth. This also helps to evaluate the LLM translation capability, only with monolingual corpora. For each source sentence, we identify the translation candidate with optimal cycle consistency with the original sentence as the final answer. Our experiments demonstrate that larger LLMs, or the same LLM with more forward passes during inference, exhibit increased cycle consistency, aligning with the LLM model size scaling law [Kaplan et al. (2020)] and test-time computation scaling law [Snell et al. (2024)]. This work provide methods for, 1) to implicitly evaluate translation quality of a sentence in the target language, 2), to evaluate capability of LLM for any-to-any-language translation, and 3), how to generate a better translation for a specific LLM.","Machine Translation (MT) has been a cornerstone of natural language processing, facilitating globalization by cross-linguistic communication and democratizing newest information access to all population. In recent years, transformer-based large language models (LLMs) have fundamentally changed the field of natural language processing. Introduced by [Vaswani et al. (2017)], the transformer architecture facilitates parallel processing of input word tokens, significantly improving computational efficiency and successfully scales to unseen model size. Strong language capabilities beyond human imagination emerges from LLM scaling, and create an image of Silicon intelligence for the first time. Transformer-based LLMs can be categorized into several paradigms: encoder-only architectures, like BERT [Devlin (2018)], which focus on meaningful embeddings of input sequences, and don’t fit translation task; the rest two architectures, the encoder-decoder architectures, like T5 [Raffel et al. (2020)], which separately process input and output sequences, were born for translation; and decoder-only architectures, like GPT [Radford et al. (2019)], which generate text in an autoregressive manner. Although GPT is not initially trained specifically for translation tasks like T5, it is exceptionally well-suited for a wide range of natural language processing (NLP) tasks via supervised fine-tuning on downstream tasks, including translation, and provides applications to users via prompting [Brown et al. (2020)]. On a related subject, MT evaluation poses significant challenges as there is no unit test for human languages, like Python or Java. Machine-based evaluation metrics, such as BLEU [Papineni et al. (2002)], METEOR [Banerjee and Lavie (2005)], and TER [Snover et al. (2006)], provide quantitative assessments based on N-gram overlaps and edit distances. While these metrics offer consistency and objectivity, they often fail to fully capture the semantic adequacy and fluency of translations [Liu et al. (2022)], although not being a problem for LLMs as they in most cases generate fluent sentences. Higher-level criteria beyond these metrics, such as overall quality and asceticity, nuanced subtleties, professional terminologies and informal idioms, require evaluation from native speaker of the target language, which is expensive and impractical during online inference. Additionally, evaluation on low-resource languages [Luong et al. (2015)] poses further difficulties due to the data scarcity. The starting point of our work is a simple conjecture: a good translation, and the LLM who translated it, should be able to jointly recover the original sentence completely and accurately. This is natural from information theory view of point. For instance, if an English sentence is translated into French and then back to English, a high degree of similarity between the original and final English sentences indicates a more accurate and reliable translation. To prove this, we propose translation cycle consistency as a meaningful metric to evaluate translation quality without parallel corpora in source language A𝐴Aitalic_A and target language B𝐵Bitalic_B. By translating a sentence from language A𝐴Aitalic_A to language B𝐵Bitalic_B and then back to A𝐴Aitalic_A, we can quantitatively measure the alignment, similarity, or closeness, between the original and back-translated texts. This method not only economically scale MT assessments, it also streamlines the evaluation process, making it widely usable for offline or online evaluation. Cycle consistency brings chance for further improving MT through a self-reflective mechanism: to think ahead a few steps. If the evaluation of translation during inference is accurate enough, we can afford to generate multiple candidates and select the best one. Unlike LLM decoding techniques such as beam search, which select the most probable translation based on a search space over a few tokens, solely based the LLM itself, cycle consistency allows for a complete evaluation of all translated tokens, selecting the most coherent and accurate output, with a math formula-backed metric. This is analogous to AlphaGo, which simulates several future steps to determine the best possible action in the current move [Silver et al. (2016)]. In this paper, we formalize our idea, and empirically investigate the scaling effects, observing that larger models exhibit improved cycle consistency in translations [Chen et al. (2021)], which in turn proves that cycle consistency is a valid and novel metric for evaluation."
https://arxiv.org/html/2411.02250v1,"Memetic collaborative approaches for finding balanced incomplete block designs111This work is partially funded by Junta de Andalucía (project P10-TIC-6083, DNEMESIS –http://dnemesis.lcc.uma.es/wordpress/), Ministerio Español de Economía y Competitividad (projects TIN2014-56494-C4-1-P, UMA::EPHEMECH –https://ephemech.wordpress.com/and TIN2017-85727-C4-1-P, UMA::DeepBio
–http://deepbio.wordpress.com), and Universidad de Málaga, Campus de Excelencia Internacional Andalucía Tech.","The balanced incomplete block design (BIBD) problem is a difficult combinatorial problem with a large number of symmetries, which add complexity to its resolution. In this paper, we propose a dual (integer) problem representation that serves as an alternative to the classical binary formulation of the problem. We attack this problem incrementally: firstly, we propose basic algorithms (i.e. local search techniques and genetic algorithms) intended to work separately on the two different search spaces (i.e. binary and integer); secondly, we propose two hybrid schemes: an integrative approach (i.e. a memetic algorithm) and a collaborative model in which the previous methods work in parallel, occasionally exchanging information. Three distinct two-dimensional structures are proposed as communication topology among the algorithms involved in the collaborative model, as well as a number of migration and acceptance criteria for sending and receiving data. An empirical analysis comparing a large number of instances of our schemes (with algorithms possibly working on different search spaces and with/without symmetry breaking methods) shows that some of these algorithms can be considered the state of the art of the metaheuristic methods applied to finding BIBDs. Moreover, our cooperative proposal is a general scheme from which distinct algorithmic variants can be instantiated to handle symmetrical optimisation problems. For this reason, we have also analysed its key parameters, thereby providing general guidelines for the design of efficient/robust cooperative algorithms devised from our proposal.","The generation of block designs is a well-known combinatorial problem of enormous difficulty [1]. The problem has a number of variants [2, 3, 4, 5, 6], among which a popular one is the so-called balanced incomplete block design (BIBD). Basically, a BIBD is defined as an arrangement of v𝑣vitalic_v different objects into b𝑏bitalic_b blocks such that each block contains exactly k𝑘kitalic_k different objects, each object occurs in exactly r𝑟ritalic_r different blocks, and every two different objects occur together in exactly λ𝜆\lambdaitalic_λ blocks (for k,r,λ>0𝑘𝑟𝜆0k,r,\lambda>0italic_k , italic_r , italic_λ > 0). The construction of BIBDs was initially tackled in the area of experimental design [7, 8]; however, nowadays BIBDs are applied in a variety of fields such as cryptography [9], coding theory [10], food evaluation [11], load balance in distributed networks [12], and classification tasks [13], among others. BIBD generation is an NP-hard problem [14] that provides an excellent benchmark for optimisation algorithms since it is scalable and has a wide variety of problem instances ranging from easy instances to very difficult ones. As discussed in Sect. 2.2, complete methods (including exhaustive search) have been applied to the problem although it remains intractable even for designs of a relatively small size [15]. In fact, as proof of the difficulty of the problem, there are currently a number of open instances that have not yet been solved (although, it may be that there is no solution for them; then again, non-solvability cannot be established by complete methods). The application of metaheuristics thus seems to be appropriate to tackle larger problem instances due to the limitations of complete methods. Indeed, some approaches in this area have already provided evidence of the potential of metaheuristic approaches applied to this problem, e.g. [16, 17, 18, 19]. One of the most interesting features of the BIBD is its highly-symmetrical nature. This introduces a number of considerations that have to be taken into account. Firstly, the existence of solutions that are equivalent with respect to the same representation space generally increases the size of the search space and, as a direct consequence, the difficulty of finding solutions (i.e. the problem solving complexity). In the last few decades, a number of methods have been applied to deal with symmetries [20, 21, 22, 23]. The primary method of dealing with them consists in applying some symmetry breaking technique. This method basically imposes new constraints to remove symmetries with the goal of reducing the problem’s search space. Symmetry breaking can be applied in many diverse forms [24]. In connection with this, it is also well known that the encoding of solutions can drastically affect the search process, because it influences the underlying landscape and its navigability. This paper proposes an alternative –and novel, to the best of our knowledge– representation scheme for BIBD solutions that we call the dual (or decimal) formulation (see Sect. 4.1), in response to the ‘more natural’ primal (or binary) model considered in the scientific literature, cf. Sect 2.1. A number of algorithms to tackle the BIBD problem are subsequently considered to take into account the large number of possible scenarios that arise from the combination of these two different encodings, as well as the symmetry-breaking constraints for the BIBD problem (see Sect. 4.2). Moreover, each scenario is tackled with a number of metaheuristic techniques, including local search and genetic algorithms. As a further step, this paper also proposes mechanisms for hybridising these algorithms. In particular, we consider both an integrative model (Sect. 3) and a collaborative scheme (Sect. 4.3). The latter, in particular, defines a network (i.e. a set) of algorithms that intensify the search in certain parts of the search space; the communication strategy among these algorithms is defined by a certain spatial structure. Three different topologies are considered for this purpose. We also study different policies to control communication among algorithms, i.e. which information should be submitted and when/how it should be handled by the metaheuristics in the network. The resulting techniques are exhaustively analysed from an empirical point of view in Sect. 5. The next section provides an overview of the problem’s foundations as well as a brief look at related work. This paper proposes a (novel) formulation for the representation of BIBDs and a number of metaheuristics (based on this formulation) to handle the problem. This paper also describes a large number of metaheuristic approaches to deal with the generation of BIBDs. Some of these (i.e. the cooperative methods) constitute state-of-the-art metaheuristic methods to handle the problem. Moreover, we provide a general scheme from which other (possibly cooperative) metaheuristics can be generated. Finally, we also propose a methodology to address, in a general way, symmetrical combinatorial problems so that our methods can be easily adjusted to deal with other symmetrical combinatorial problems."
https://arxiv.org/html/2411.01922v1,Deep Memetic Models for Combinatorial Optimization Problems: Application to the Tool Switching Problem,"Memetic algorithms are techniques that orchestrate the interplay between population-based and trajectory-based algorithmic components. In particular, some memetic models can be regarded under this broad interpretation as a group of autonomous basic optimization algorithms that interact among them in a cooperative way in order to deal with a specific optimization problem, aiming to obtain better results than the algorithms that constitute it separately. Going one step beyond this traditional view of cooperative optimization algorithms, this work tackles deep meta-cooperation, namely the use of cooperative optimization algorithms in which some components can in turn be cooperative methods themselves, thus exhibiting a deep algorithmic architecture. The objective of this paper is to demonstrate that such models can be considered as an efficient alternative to other traditional forms of cooperative algorithms. To validate this claim, different structural parameters, such as the communication topology between the agents, or the parameter that influences the depth of the cooperative effort (the depth of meta-cooperation), have been analyzed. To do this, a comparison with the state-of-the-art cooperative methods to solve a specific combinatorial problem, the Tool Switching Problem, has been performed. Results show that deep models are effective to solve this problem, outperforming metaheuristics proposed in the literature.","The optimization of combinatorial optimization problems (COPs) has been addressed from different algorithmic approaches. Initially, exact/complete search methods showed good results to cope with problem instances of limited size, but they are also inefficient when the size of the problem scales up. Subsequently, other approaches called metaheuristics, such as bio-inspired optimization techniques, have shown a good performance to obtain high quality solutions at the expense of not proving optimality. One of the key factors of the success of metaheuristics methods to deal with complex COPs is the balance between exploitation and exploration of the search space. However, finding the best balance between these two facets of the search is in fact another optimization problem, and not an easy one to solve. An interesting proposal found in population-based optimization methods to tackle this issue is the use of structured populations, thus limiting interactions between individuals and allowing a better exploration and exploitation of the search space (Lim, 2014). On the other hand, hybridization basically means a synergistic union of different components, each one of them contributing different features to the search process and/or providing mechanisms for the exploitation of problem knowledge. From a broad perspective, hybridization can include the use of any problem-specific add-ons providing problem-knowledge (e.g., specialized decoders, ad-hoc variation operators, etc.), thus enhancing the search process. It is often the case that hybridization is used with a more specific connotation though, namely the combination of higher-level algorithmic components (frequently, techniques that could be used as stand-alone methods, such as other metaheuristics). Under this latter prism, hybridization can be viewed from two wide perspectives (Jourdan et al, 2009): integration and cooperation. Integration refers to the addition of one of the optimization techniques as a component of another optimization method whereas cooperation generally relates to setting up a mechanism to exchange information between methods that are applied one after another or in parallel. The interest for this approach to optimization dates back to the 1990s and, indeed, it can be considered itself a programming paradigm featuring two main elements (Crainic and Toulouse, 2007): (a) a set of autonomous programs, each implementing a particular solution method, and (b) a cooperative scheme that combines these autonomous elements in a simple and unified strategy for optimization. The cooperative optimization approach described above thus amounts to the application of various algorithmic components, each one exploring a specific search landscape through processes of intensification/diversification —inherent to the metaheuristics used— to obtain an effective mechanism with the ability of escaping from local minima by exchanging information about the search space being explored. These collaborative optimization models constitute a very appropriate framework for integrating different search techniques: each one may exploit problem knowledge in a complementary way, and have a different view of the search landscape. Therefore, if we combine their different exploration patterns, the search benefits from new ways to avoid local optima. In fact, this feature is more useful whenever the problem addressed raises a challenging optimization task to each one of the individual search algorithms since otherwise computational power might be diversified spending time in unproductive or duplicated explorations. Building on previous work on this kind of models (Amaya et al, 2011) – see also Section 2 – this paper tackles deep meta-cooperation, i.e., cooperative models in which at least one of its components is a cooperative model itself. Therefore, we bring together the notion of cooperative optimization sketched before with the idea of deep metaheuristics (Camacho et al, 2018) resulting in an unified model for building powerful algorithmic complexes for optimization. While admittedly interpretable from different angles, meta-cooperation naturally fits the broad memetic paradigm, since it constitutes a framework for arranging the interoperation among different methods, either based on populations or in local search, aiming to have synergistic effects by virtue of the adequately diversified exploitation/exploration capabilities of the techniques involved. In this line, this work highlights this idea by proposing in Section 3 a basic schema that can be easily instantiated into a number of different algorithms, and provides evidence that deep meta-cooperative algorithms are effective optimization methods to deal with combinatorial problems, capable of outperforming shallow cooperative methods. Being a heterogeneous hierarchical agent-based system, this model exhibits several advantages. Firstly, it eases escaping from local optima by the combination of different search patterns. Secondly, it can be parallelized in a trivial way, following a island-based approach. Thirdly, it naturally lends itself to incorporating problem-knowledge in a flexible way, via any of the algorithmic components involved in the system. Thus, the main contributions of this work are (i) the proposal and definition of a new model for deep meta-cooperation, (ii) its application to a hard combinatorial problem (the ToSP, see below), outperforming state-of-the-art methods for said problem, and (iii) a performance study involving some salient design factors. To validate the proposed model, the performance of a number of different algorithms (devised from the basic scheme, with varied depths of meta-cooperation) have been analyzed in Section 4, using a hard problem from the area of flexible manufacturing as test bench. The experimental analysis conducted considers issues such as the depth of the architecture and its spatial arrangement. We close the paper with a discussion of the main conclusions and an outline of future work in Section. 5."
https://arxiv.org/html/2411.00902v1,Differentiable architecture search with multi-dimensional attention for spiking neural networks,"Spiking Neural Networks (SNNs) have gained enormous popularity in the field of artificial intelligence due to their low power consumption. However, the majority of SNN methods directly inherit the structure of Artificial Neural Networks (ANN), usually leading to sub-optimal model performance in SNNs. To alleviate this problem, we integrate Neural Architecture Search (NAS) method and propose Multi-Attention Differentiable Architecture Search (MA-DARTS) to directly automate the search for the optimal network structure of SNNs. Initially, we defined a differentiable two-level search space and conducted experiments within micro architecture under a fixed layer. Then, we incorporated a multi-dimensional attention mechanism and implemented the MA-DARTS algorithm in this search space. Comprehensive experiments demonstrate our model achieves state-of-the-art performance on classification compared to other methods under the same parameters with 94.40% accuracy on CIFAR10 dataset and 76.52% accuracy on CIFAR100 dataset. Additionally, we monitored and assessed the number of spikes (NoS) in each cell during the whole experiment. Notably, the number of spikes of the whole model stabilized at approximately 110K in validation and 100k in training on datasets.","Spiking Neural Networks (SNNs) have been widely explored in the artificial intelligence field in recent years due to their sparsity, low power consumption, and their ability to mimic dynamic neuronal properties, earning them the title of ""third-generation neural networks"" Maass (1997). However, the existence of binary spikes makes training SNNs a challenging task. Existing SNN training algorithms can be roughly divided into two categories: those based on neurobiological characteristic training algorithms, and those adapted from traditional neural network training methods for spiking neural networks. Among the former, methods represented by Spike Timing-Dependent Plasticity(STDP) Caporale and Dan (2008) have achieved remarkable results only in shallow networks for lacking global information Iakymchuk et al. (2015). Therefore, most algorithms have been enhanced based on the work of traditional artificial neural networks(ANNs). However, directly inheriting network structures from ANN will inevitably lead to accuracy loss in deep spiking neural networks. Many researchers Lee et al. (2020), Sengupta et al. (2019), Hu et al. (2021),Han et al. (2020), Fang et al. (2021a) have developed improved residual networks considering aspects like residual block design, compensation mechanisms, and new neuron models, demonstrating decent performance in terms of accuracy, parameters and time steps. However, these methods, which are artificially designed sub-optimal network structures within a fixed framework, still lag behind traditional neural networks of the same structure in terms of performance. Neural Architecture Search (NAS) first proposed by Zoph and Le (2016) is an automated approach for designing effective neural networks, which often achieves more remarkable results than artificially designed networks in many scenarios Xie et al. (2021). Therefore, this method can also be applied to exploit the potential of the network structure of SNN. In this work, we introduce a significant neural architecture search method to automate the search for the optimal network structure of SNNs within a specific search space. Specifically, we initially established a search space consisting of two levels, including macro backbone architecture and micro candidate architecture. To effectively find the optimal network structure in this search space and consider the temporal information of SNN when processing event-driven data, we proposed a Multi-Attention Differentiable Architecture Search (MA-DARTS) algorithm, incorporating an attention mechanism Bahdanau et al. (2014) at a crucial stage based on the original algorithm Liu et al. (2018). Unlike traditional types of attention mechanisms, this attention mechanism not only includes channel attention Hu et al. (2018) and spatial attention but also temporal attention Yao et al. (2022, 2021). We conducted comprehensive experimental verification of the proposed method on two datasets, CIFAR10 and CIFAR100, evaluating and analyzing the network from accuracy, network parameters, and the number of spikes. The result demonstrates that MA-DARTS not only outperforms manually designed SNN network structures in terms of accuracy but also achieves more lightweight network parameters. Moreover, the addition of an attention mechanism can also bring accuracy gains to the model with a minimal number of parameters. The contributions can be summarized as follows: [•] 1. To simplify the search process, we first established a differentiable two-level search space and then searched for micro architecture within fixed layers. This method enables efficient search with limited computational resources. 2. We enhanced DARTS by incorporating multi-dimensional attention mechanisms within the defined search space and proposed MA-DARTS algorithm. Our algorithm effectively improved the model’s classification accuracy with the addition of a minimal number of parameters. 3. We conducted comprehensive experimental verification on CIFAR10 and CIFAR100. The results show that our method can identify network structures with superior accuracy and parameter metrics at the same network size compared to other methods. The rest of the paper is organized as follows. section 2 introduces related work. section 3 provides a comprehensive overview of our methodology in details. section 4 presents the experimental results compared to other methods and demonstrate a in-depth analysis of connections in micro architecture. Finally, the findings and the potential directions for future work are summarized in section 5."
https://arxiv.org/html/2411.02333v1,Discrete the solving model of time-variant standard Sylvester-conjugate matrix equations using Euler-forward formula: An analysis of the differences between sampling discretion errors and space compressive approximation errors in optimizing neural dynamics,"Time-variant standard Sylvester-conjugate matrix equations are presented as early time-variant versions of the complex conjugate matrix equations. Current solving methods include Con-CZND1 and Con-CZND2 models, both of which use ode45 for continuous model. Given practical computational considerations, discrete these models is also important. Based on Euler-forward formula discretion, Con-DZND1-2i model and Con-DZND2-2i model are proposed. Numerical experiments using step sizes of 0.1 and 0.001. The above experiments show that Con-DZND1-2i model and Con-DZND2-2i model exhibit different neural dynamics compared to their continuous counterparts, such as trajectory correction in Con-DZND2-2i model and the swallowing phenomenon in Con-DZND1-2i model, with convergence affected by step size. These experiments highlight the differences between optimizing sampling discretion errors and space compressive approximation errors in neural dynamics.","Standard Sylvester-conjugate matrix equations (SSCME) [1] are the earliest version of complex conjugate matrix equations (CCME) [2]. And SSCME is time-invariant. In recent years of studies, Wu et al. provided matrix algebraic formula methods [3] and iterative solving methods [2] based on approximation theory. The essence of the iterative methods is to gradually approach the theoretical solution using multi-step computations. And time-variant standard Sylvester-conjugate matrix equations (TVSSCME) [4] is the time-variant extension of SSCME. TVSSCME is supplemented by the differences between differential algebra and linear algebra operations [5, 6]. The difference between SSCME and TVSSCME solutions is shown in Fig. 1. Unless otherwise specified, let O𝑂Oitalic_O represent “null matrix"", and only consider the unique theoretical solution X∗⁢(τ)superscript𝑋∗𝜏X^{\ast}(\tau)italic_X start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_τ ), same as below. (a)X⁢(τ)⁢F−A⁢X⁢(τ)−C=O𝑋𝜏𝐹𝐴𝑋𝜏𝐶𝑂X(\tau)F-AX(\tau)-C=Oitalic_X ( italic_τ ) italic_F - italic_A italic_X ( italic_τ ) - italic_C = italic_O, where τ→+∞→𝜏\tau\to+\inftyitalic_τ → + ∞, X⁢(τ)→X∗⁢(τ).→𝑋𝜏superscript𝑋∗𝜏X(\tau)\to X^{\ast}(\tau).italic_X ( italic_τ ) → italic_X start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_τ ) .(b)X⁢(τ)⁢F⁢(τ)−A⁢(τ)⁢X⁢(τ)−C⁢(τ)=O𝑋𝜏𝐹𝜏𝐴𝜏𝑋𝜏𝐶𝜏𝑂X(\tau)F(\tau)-A(\tau)X(\tau)-C(\tau)=Oitalic_X ( italic_τ ) italic_F ( italic_τ ) - italic_A ( italic_τ ) italic_X ( italic_τ ) - italic_C ( italic_τ ) = italic_O, where τ→+∞→𝜏\tau\to+\inftyitalic_τ → + ∞, X⁢(τ)→X∗⁢(τ).→𝑋𝜏superscript𝑋∗𝜏X(\tau)\to X^{\ast}(\tau).italic_X ( italic_τ ) → italic_X start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_τ ) . Figure 1: Differences between SSCME(a) and TVSSCME(b). TVSSCME is currently primarily solved using zeroing neural dynamics (ZND) models Con-CZND1 [4] and Con-CZND2 [4]. Above two models structure can be seen in Fig. 2. Random InputRandom InputRandom InputRandom Input⋮⋮\vdots⋮n{⋮⋮\vdots⋮n{⋮⋮\vdots⋮n{⋮⋮\vdots⋮…⋮⋮\vdots⋮⋮⋮\vdots⋮OutputOutputOutputOutput⋮⋮\vdots⋮n{⋮⋮\vdots⋮n{ Hidden Complex Layer1 Input Real Layer Hidden Complex Layer2 Hidden Real Layer1 Output Real Layer Random InputRandom InputRandom InputRandom Input⋮⋮\vdots⋮n{⋮⋮\vdots⋮n{…⋮⋮\vdots⋮⋮⋮\vdots⋮OutputOutputOutputOutput⋮⋮\vdots⋮n{⋮⋮\vdots⋮n{ Input Real Layer Hidden Real Layer1 Output Real Layer Figure 2: Different between Con-CZND1 [4] model and Con-CZND2 [4] model. 2 Con-CZND1 model. 2 Con-CZND2 model. However, Con-CZND1 model essentially approximates using the complex field error, while Con-CZND2 model approximates using the real field error. In ode45 [7] solver, Con-CZND2 model does not perform as well as Con-CZND1 model. Discrete neural dynamics is validated in previous studies to reduce the error between theoretical and numerical solutions [8]. Zhang et al. continued to develop discretion in the real field, progressing from Euler-forward formula [9, 10] to an 11-point sampling discretion [11, 12, 13]. However, there is no exploration of neural models for solving TVSSCME using sampling discretion in the existing literature. According to the known studies, the two continuous solution models, Con-CZND1 and Con-CZND2, show significant differences due to the approximation effects of the internal ode45 [14] solver. Additionally, Con-CZND1 model exhibits space compressive approximation phenomenon. Therefore, it is essential to rigorously establish a discrete neural dynamics model for TVSSCME. The rest of this article is organized as follows: Section 2 provides the definition of TVSSCME and supplementary knowledge. Section 3 defines the Con-DZND1-2i discrete solving model over the complex field and the Con-DZND2-2i discrete solving model over the real field. Section 4 presents simulations that validate the effectiveness of each model and compares their strengths and weaknesses. Sections 5 and 6 summarizes this article and suggests future directions. Before proceeding to the next section, the main contributions of this article are as follows: (1) Con-DZND1-2i model, which directly defines complex field error, and Con-DZND2-2i model, which maps to real field error, are proposed for solving TVSSCME. (2) Based on Euler-forward formula, both discrete models, Con-DZND1-2i and Con-DZND2-2i, which use different step sizes, can ultimately approximate the theoretical solution. (3) Con-DZND1-2i model defines complex field error, while Con-DZND2-2i model maps to real field error. These models highlight a significant difference between optimizing space compressive approximation errors and optimizing sampling discretion errors in neural network optimization. Both aspects should be considered from different perspectives."
https://arxiv.org/html/2411.02001v1,Local Loss Optimization in the Infinite Width: Stable Parameterization of Predictive Coding Networks and Target Propagation,"Local learning, which trains a network through layer-wise local targets and losses, has been studied as an alternative to backpropagation (BP) in neural computation. However, its algorithms often become more complex or require additional hyperparameters because of the locality, making it challenging to identify desirable settings in which the algorithm progresses in a stable manner. To provide theoretical and quantitative insights, we introduce the maximal update parameterization (μ𝜇\muitalic_μP) in the infinite-width limit for two representative designs of local targets: predictive coding (PC) and target propagation (TP). We verified that μ𝜇\muitalic_μP enables hyperparameter transfer across models of different widths. Furthermore, our analysis revealed unique and intriguing properties of μ𝜇\muitalic_μP that are not present in conventional BP. By analyzing deep linear networks, we found that PC’s gradients interpolate between first-order and Gauss-Newton-like gradients, depending on the parameterization. We demonstrate that, in specific standard settings, PC in the infinite-width limit behaves more similarly to the first-order gradient. For TP, even with the standard scaling of the last layer, which differs from classical μ𝜇\muitalic_μP, its local loss optimization favors the feature learning regime over the kernel regime.","Deep learning has achieved remarkable performance by building upon the backpropagation (BP) algorithm and developing architectures specialized for it [Rumelhart et al., 1986, LeCun et al., 1998, 2015]. BP, however, is not always a suitable method for more general objectives, such as biologically plausible computation [Lillicrap et al., 2020, Bredenberg et al., 2024] or efficient distributed computation [Amid et al., 2022]. A representative alternative is local loss optimization, a type of credit assignment problem, in which loss functions are defined layer-wise, and targets are set locally. The basic formulation involves performing regression on target signals at each layer to reduce the global error across the entire network: Predictive Coding networks, usually referred to as PC, generate their targets through the internal dynamics of inference [Whittington and Bogacz, 2017, Song et al., 2020, Salvatori et al., 2023], while Target Propagation (TP) generates them using feedback networks [Bengio, 2014, Lee et al., 2015, Ernoult et al., 2022]. In many cases, the use of local losses requires additional hyperparameters (HPs) and their careful tuning, making the algorithm configuration significantly more complicated compared to that of BP. For example, PC requires not only the usual HPs, such as learning rate and initialization of weight parameters, but also those for the inference phase, such as the initialization of the state and the number of inference sequences. These HPs are primary considerations and have been reported as critical for ensuring stable training behavior [Pinchetti et al., 2024, Alonso et al., 2024, Rosenbaum, 2022]. A few analyses have succeeded in providing theoretical intuition for such local learning algorithms by introducing specific conditions or additional corrections that bridge them to classical optimization formulations [Song et al., 2020, Alonso et al., 2022, Meulemans et al., 2020]. However, such conditions are not always met in practice and may not be commonly shared across the entire family of methods. To develop local learning that is more easily manageable across a broader range of settings, it is promising to establish a theoretical foundation that enables the analysis of natural learning dynamics under fewer constraints. For standard BP, deep learning theory offers insights into the universal properties of learning [Bahri et al., 2020, Bartlett et al., 2021]. A key research focus in this area is understanding learning in the infinite-width limit, including studies on neural tangent kernel (NTK) and feature learning regimes [Jacot et al., 2018, Chizat et al., 2019, Mei et al., 2018, Bordelon and Pehlevan, 2022b]. In particular, Yang and Hu [2021] provided a unified perspective on the parameterizations that realize these learning regimes and proposed maximal update parameterization (μ𝜇\muitalic_μP) as a unique scaling of HPs, such as random initialization and learning rates, that achieves feature learning in the infinite-width limit. Building on this developing theoretical foundation, we expect to gain universal insight into local learning, which has not yet been systematically analyzed. In this work, we derive the μ𝜇\muitalic_μP for PC and TP and investigate hyperparameter transfer (the so-called μ𝜇\muitalic_μTransfer) across different widths. Our contributions are summarized as follows: • While it is known that PC inference trivially reduces to gradient computation of BP under the fixed prediction assumption (FPA), a technical and heuristic condition, there is generally no guarantee that PC will reduce to BP, making it highly non-trivial to identify its μ𝜇\muitalic_μP. We first consider PC with a single sequential inference and reveal the μ𝜇\muitalic_μP even without FPA (Theorem 11). We also empirically verify the μ𝜇\muitalic_μTransfer of learning rates, showing that the optimal learning rate does not depend on the order of width. • Second, for a more general context involving multiple inference sequences, we consider the convergence of the inference phase. We find that, for deep linear networks, we can explicitly obtain the local targets and losses at the fixed point of the inference, which depend on inference step sizes (Theorem 4.2). Interestingly, it takes a similar form to the Gauss-Newton (GN) gradient, but it can reduce to the conventional first-order gradient descent (GD) depending on the parameterization and step sizes. We find that the eventual gradient is closer to GD for sufficiently wide neural networks under standard experimental settings with μ𝜇\muitalic_μP. We also confirm that a larger inference step size, identified through this analysis, enhances μ𝜇\muitalic_μTransfer of HPs. • Finally, we derive μ𝜇\muitalic_μP for both TP and its variant difference target propagation (DTP) assuming linear feedback networks (Theorem 19). We reveal a distinct property that differs from BP and PC; the feedback network of (D)TP changes the preferable scale of the last layer compared to the usual μ𝜇\muitalic_μP and causes the absence of the kernel regime. In this sense, (D)TP favors feature learning more strongly than other learning methods. Thus, this study provides a solid and qualitative foundation for the further development of local learning schemes in large-scale neural networks in the future."
https://arxiv.org/html/2411.01628v1,Energy-Aware FPGA Implementation of Spiking Neural Network with LIF Neurons,"Tiny Machine Learning (TinyML) has become a growing field in on-device processing for Internet of Things (IoT) applications, capitalizing on AI algorithms that are optimized for their low complexity and energy efficiency. These algorithms are designed to minimize power and memory footprints, making them ideal for the constraints of IoT devices. Within this domain, Spiking Neural Networks (SNNs) stand out as a cutting-edge solution for TinyML, owning to their event-driven processing paradigm which offers an efficient method of handling dataflow. This paper presents a novel SNN architecture based on the 1s⁢tsuperscript1𝑠𝑡1^{st}1 start_POSTSUPERSCRIPT italic_s italic_t end_POSTSUPERSCRIPT Order Leaky Integrate-and-Fire (LIF) neuron model to efficiently deploy vision-based ML algorithms on TinyML systems. A hardware-friendly LIF design is also proposed, and implemented on a Xilinx Artix-7 FPGA. To evaluate the proposed model, a collision avoidance dataset is considered as a case study. The proposed SNN model is compared to the state-of-the-art works and Binarized Convolutional Neural Network (BCNN) as a baseline. The results show the proposed approach is 86% more energy efficient than the baseline.","Deep learning algorithms based on Artificial Neural Networks (ANN) are popular in Internet of Things (IoT) applications due to their AI capabilities. However, the size and complexity of these models require high-performance computing (HPC) clusters in data centers, which poses challenges for IoT devices that gather data on the edge (Navardi and Mohsenin, 2023; Humes et al., 2023). Edge computing, where data is processed on the device itself, can address these challenges but is limited by memory and energy constraints (Navardi and Mohsenin, 2023; Rashid et al., 2024; Mazumder and Mohsenin, 2023; Navardi et al., 2023). It led Tiny Machine Learning (TinyML) to emerge which considers efficient deployment of ANN models on tiny robots such as Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) (Manjunath et al., 2023). However, deploying ANN models on the edge is still limited by power and hardware constraints due to the highly intensive computational of such models. To overcome this, there is interest in bio-inspired approaches like Spiking Neural Networks (SNNs), which have lower energy consumption (Nguyen et al., 2022; Davidson and Furber, 2021; Véstias, 2019; Roy et al., 2017). Recent advancements in the field of neural networks have led to an era where the computational efficiency of SNNs is being harnessed in more innovative and practical ways (Pietrzak et al., 2023; Agatonovic-Kustrin and Beresford, 2000; Pfeiffer and Pfeil, 2018). SNNs, with their ability to mimic the intricacies of biological neural networks, represent a significant leap from traditional artificial neural networks. They offer a more nuanced approach to information processing, mimicking the dynamic, temporal characteristics of biological neuron activity. The integration of SNNs with Field-Programmable Gate Arrays (FPGAs) has opened new avenues in computing, merging the adaptability of neural networks with the energy efficiency and customization offered by FPGAs (Li et al., 2024; Omondi and Rajapakse, 2006; Panchapakesan et al., 2022; Kakani et al., 2023; Heidarpur et al., 2019; Pham et al., 2021; Mazumder et al., 2021). This synergy is crucial, especially in applications where power efficiency is important, such as in embedded systems or portable devices. While recent research has made strides in developing effective FPGA architectures for SNNs and creating more biologically plausible models (Farsa et al., 2019a, 2015; Lammie et al., 2018; Ali et al., 2022), there remains a gap in applying these advancements to complex, real-world scenarios. Much of the existing work (Iyer et al., 2021; Diehl and eCook, 2015; Zhou, 2023) focuses on pattern recognition tasks using simplified datasets like MNIST (Lecun et al., 1998), has not fully explored the potential of SNNs in more challenging and realistic environments. This research aims to bridge this gap by shifting the focus to a more demanding and realistic dataset: collision avoidance. The study explores the training of SNNs for collision avoidance and their implementation on FPGA platforms. By moving beyond the conventional realm of simple image classification tasks, this work aims to validate the practicality and robustness of SNNs in high-stakes scenarios, marking a significant stride toward their real-life applicability. This paper presents a novel approach to SNN implementation, leveraging the strengths of FPGAs to address the challenges of image recognition in dynamic environments. The choice to focus on FPGA implementations for SNNs was driven by several considerations such as FPGAs offer a unique blend of flexibility and performance, allowing for rapid prototyping and iterative design that is crucial for the evolving field of SNNs. This work contributes to the field by not only validating the FPGA-based hardware design using a complex dataset but also by offering a comprehensive comparison with prior standard works. This research underscores the progression of neuromorphic computing (Mead, 1990) towards practical, everyday applications, setting a new benchmark for future endeavors in the domain. In summary, the main contributions of this paper are: • Developing a vision-based TinyML framework optimized for tiny UAVs and UGVs. • Adapting a widely-recognized 1s⁢tsuperscript1𝑠𝑡1^{st}1 start_POSTSUPERSCRIPT italic_s italic_t end_POSTSUPERSCRIPT Order LIF model for on-device processing in TinyML applications, tested with a challenging dataset. • Architecting a robust hardware solution tailored for the efficient deployment of this SNN model on an FPGA platform."
https://arxiv.org/html/2411.01578v1,"Integrating Graph Neural Networks and Many-Body Expansion Theory
for Potential Energy Surfaces","Rational design of next-generation functional materials relied on quantitative predictions of their electronic structures beyond single building blocks. First-principles quantum mechanical (QM) modeling became infeasible as the size of a material grew beyond hundreds of atoms. In this study, we developed a new computational tool integrating fragment-based graph neural networks (FBGNN) into the fragment-based many-body expansion (MBE) theory, referred to as FBGNN-MBE, and demonstrated its capacity to reproduce full-dimensional potential energy surfaces (FD-PES) for hierarchic chemical systems with manageable accuracy, complexity, and interpretability. In particular, we divided the entire system into basic building blocks (fragments), evaluated their single-fragment energies using a first-principles QM model and attacked many-fragment interactions using the structure–property relationships trained by FBGNNs. Our development of FBGNN-MBE demonstrated the potential of a new framework integrating deep learning models into fragment-based QM methods, and marked a significant step towards computationally aided design of large functional materials.","Discovery of complex materials that exhibited exceptional quantum mechanical (QM) properties and function beyond single monomers and equilibrium structures, such as metal–organic frameworks (MOF) [yao2020metal], organic semiconductors (OSC) [doi:10.1021/acs.accounts.3c00750], and branched deoxyribonucleic acids (DNA) [doi:10.1021/acs.chemrev.0c00294], was crucial in emergent scientific and technological areas, such as carbon neutrality [doi:10.1021/accountsmr.2c00084], renewable energy [doi:10.1126/science.adq3799], and next-generation optoelectronics [https://doi.org/10.1002/advs.202003834]. Computational chemistry eliminated expensive trial-and-error experiments and explored the vast chemical space. In the present study, we aimed to accomplish a computational design for these complex materials based on their aggregate and dynamic QM properties, which required a rapid and rigorous evaluation of their full-dimensional potential energy surfaces (FD-PES) on the fly. This job cannot be done by first-principles QM models like second-order Møller–Plesset perturbation theory (MP2) [PhysRev.46.618] or density functional theory (DFT) [PhysRev.136.B864, PhysRev.140.A1133] due to the prohibitive costs for large systems because their computational complexity scaled as the fifth and third power of the number of basis functions [doi:https://doi.org/10.1002/9781119019572.ch14, C5CP00437C]. Motivated by this problem, many fragment-based “divide-and-conquer” methods were developed to accelerate typical QM approaches while maintaining the accuracy [doi:10.1021/acs.accounts.6b00356, 10.1063/1.5126216]. Among all these theories, many-body expansion (MBE) stood out due to its straightforward implementation and rapid convergence for many-body interactions [doi:10.1021/ct700223r, doi:10.1021/ar500119q, heindel2022many]. MBE partitioned a complex system into manageable fragments (bodies) and expanded the total electronic energy or other relevant properties into a series of one-body (1B) and many-body (n𝑛nitalic_nB) terms with progressively diminishing contributions. This hierarchical treatment not only streamlined a calculation with a reduced computational complexity but also enabled a deeper analysis of the electronic structure landscape and the intricate many-fragment interactions, both of which were critical properties for computational material discovery. The Herbert group and the Xantheas group made prominent and complementary contributions in recent methodology of MBE for both static and dynamic behaviors of condensed-phase systems. Herbert and coworkers developed the generalized many-body expansion (GMBE) framework to handle systems with ill-defined or overlapping fragments like fluoride-water complexes. They also introduced energy-screened MBE with enhanced efficiency and intact accuracy by selectively including only sizable many-body contributions in the total electronic energy [10.1063/1.4742816, doi:10.1021/ct300985h, doi:10.1021/jz401368u, 10.1063/1.4885846, doi:10.1021/ar500119q, 10.1063/1.4947087, doi:10.1021/acs.jctc.5b00955, 10.1063/1.4986110, doi:10.1021/acs.jctc.9b01095, 10.1063/5.0174293]. Xantheas and coworkers leveraged MBE for potential energy surfaces (PES) and demonstrated that MBE can provide a more quantitative understanding of molecular properties than simpler pairwise-additive models. They further incorporated MBE into molecular dynamics (MD) simulations to involve subtle QM phenomena for electrons and nuclei [heindel2020many, doi:10.1021/acs.jctc.0c01309, doi:10.1021/acs.jctc.1c00780, 10.1063/5.0095335, heindel2022many, D1CP00409C, 10.1063/5.0095739, doi:10.1021/acs.jctc.3c00575, D2CP03241D, doi:10.1021/acs.jpclett.2c03822, 10.1063/5.0094598, PhysRevC.107.044004]. Despite these advances, applying QM-based MBE to functional materials with more sophisticated structures and more intense interactions than water clusters remains a challenge due to the large numbers of n𝑛nitalic_n-fragment interactions for high n𝑛nitalic_n’s. The integration of neural networks (NNs) offered a revolutionary approach to accelerate QM methods like MBE [schutt2017quantum, keith2021combining, https://doi.org/10.1002/wcms.1645]. In particular, Parkhill and coworkers merged NN into MBE (NN-MBE) and demonstrated its strong predictive power for the FD-PES of methanol (\chCH3OH) clusters with mean absolute errors (MAEs) of 9.79 and 12.55 kcal/mol for two-body (2B) and three-body (3B) energies compared to MP2 but a reduced computational cost by six orders of magnitude [yao2017many]. However, intrinsic problems of traditional NNs in terms of the missing physical information [schutt2017quantum, doi:10.1126/sciadv.1603015], the limited transferability and interpretability [gilmer2017neural, C7SC04934J], and inability to handle graph-structured data [kipf2016semi, NIPS2017_5dd9db5e] compromised their capacity in QM modeling [behler2007generalized, doi:10.1126/science.aag2302]. Instead, the development of graph neural networks (GNNs) experienced exceptional success in chemical systems because their node–edge structures naturally aligned with three-dimensional atom–bond structures and encoded mechanical information about chemical bonds and intermolecular interactions [kipf2016semi, NIPS2017_5dd9db5e, chen2019graph, dai2021graph, dai2021graph-corr]. Outstanding examples included SchNet [schutt2018schnet], GeoMol [ganea2021geomol], FP-GNN (fingerprints-GNN) [cai2022fp], and dyMEAN (dynamic multi-channel equivariant graph network) [kong2023end] which incorporated complex geometric information in the graph representation, PhysNet [unke2019physnet], DimeNet/DimeNet++ (directional message passing NN) [gasteiger2020directional, gasteiger2020fast], E(n) EGNN (equivariant GNN) [satorras2021n], SEGNN (steerable E(3) equivariant GNN) [brandstetter2021geometric], and ViSNet (vector-scalar interactive GNN) [wang2024enhancing] which integrated directional message passing framework and physical principles, and ml-QM-GNN (QM-augmented GNN) [stuyver2022quantum], MD-GNN (mechanism-data-driven graph neural network) [chen2023md], MP-GNN (multiphysical GNN) [li2022multiphysical], and SS-GNN (simple-structured graph neural network) [zhang2023ss] which implemented quantitative mechanical and electronic properties. Most of these GNN models demonstrated enhanced performance in molecular representation learning but they treated all atoms on equal footing without considering the chemical hierarchy, which impacted their descriptive and predictive capacity for complex systems with many building blocks. State-of-the-art GNN models with subgraph of fragment-based frameworks, such as SubGNN (subgraph NN) [alsentzer2020subgraph], FragGraph [doi:10.1021/acs.jpca.1c06152], subGE (subgraph embedding) [chen2023subge], MXMNet (multiplex molecular GNN) [zhang2020molecular], and PAMNet (physics aware multiplex GNN) [zhang2023universal], all represented building blocks like molecules or monomers into subgraphs or local graphs, and captured interatomic, intermolecular and interfragment interactions using local and global message-passing architectures. Such an analogy between hierarchic graph structures and hierarchic chemical systems rendered these models outstanding methods for studying complex systems. In particular, MXMNet and PAMNet developed by Xie and coworkers significantly advanced the representation learning of hierarchic systems by integrating molecular mechanics and multiplex graph representations and proved successful in reproducing the molecular properties from the QM9 data set [QM9], the protein–ligand binding affinities from the PDBBind data set [doi:10.1021/jm048957q], and the three-dimensional (3D) structures of ribonucleic acids (RNA) [doi:10.1126/science.abe5650]. In the present study, we developed a novel computational model named FBGNN-MBE (fragment-based graph neural network driven many-body expansion) to address all problems mentioned above. Our ultimate goal was to accomplish a rapid, precise, transferable, and interpretable scheme to evaluate FD-PES for any functional materials with many building blocks and important dynamic properties. Our major contributions include: • We established FBGNN-MBE to integrate the divide-and-conquer strategy of the MBE formalism with the sophisticated modeling capacity of FBGNN. • We attacked the total ground state electronic energy using MBE and expected an extension to excited state energies and other properties. • We evaluated 1B energies using MP2 or DFT and generated 2B and 3B energies based on 3D atomistic geometries and structure–property relationships trained by MXMNet or PAMNet. • We provided a proof-of-concept for FBGNN-MBE using three benchmark systems with weak to moderate many-fragment interactions. • We arrived at chemical accuracy (<0.3absent0.3<0.3< 0.3 kcal/mol) for 2B and 3B energies across all systems and outperformed other MBE models using conventional GNNs. • We interpreted the outstanding performance of FBGNN-MBE through the natures and strengths of many-fragment interactions in benchmark systems. • We designed application systems to evaluate FBGNN-MBE in reproducing experimental measurable properties based on FD-PES. • We confirmed the potential of FBGNN-MBE as a revolutionary protocol for computational material discovery."
https://arxiv.org/html/2411.01008v1,AI-Guided Codesign Framework for Novel Material and Device Design applied to MTJ-based True Random Number Generators,"Novel devices and novel computing paradigms are key for energy-efficient, performant future computing systems. However, designing devices for new applications is often time-consuming and tedious. Here, we investigate the design and optimization of spin–orbit torque and spin transfer torque magnetic tunnel junction models as the probabilistic devices for true random number generation. We leverage reinforcement learning and evolutionary optimization to vary key device and material properties of the various device models for stochastic operation. Our AI-guided codesign methods generated different candidate devices capable of generating stochastic samples for a desired probability distribution, while also minimizing energy usage for the devices.","Figure 1: Our AI-Guided Framework for Device Discovery and Optimization for a given application. Overview of the device model, AI-guided discovery and optimization strategy, and RNG algorithm workflow. Given a target distribution, the optimization approach (b) uses a device model (a) to simulate a true random bit according to the RNG algorithm (c). The optimization algorithm designs novel device configurations (d) that must pass device checks to be viable. The viable devices are used to produce the target distribution for a given application (e). Designing devices for novel applications is oftentimes a time rigorous and resource-constrained process that requires utilizing computationally intensive simulations, device fabrication, and testing of the physical components in the application-specific environment. At the same time, customizing device characteristics to a particular application can allow for significant performance improvements. Automated codesign strategies are becoming increasingly popular with advancements in the artificial intelligence (AI) field that provide useful machine learning algorithms and frameworks [1, 2, 3, 4]. Such codesign provides new opportunities to automatically customize devices for application-specific needs to maximize performance—whether that involves a particular capability, energy usage, latency, throughput, or even combinations of metrics. The operation of emerging devices, such as magnetic tunnel junctions (MTJs) [5, 6, 7, 8], can be simulated using physics-based models that capture key behaviors based on materials and device properties. By pairing these models with AI-guided codesign, we are able to effectively optimize the device parameters for application requirements and constraints [9, 10, 11]. AI-guided methods are increasingly being adopted in electronic design automation (EDA) flows. Recently, reinforcement techniques have been used in EDA for multiple tasks including chip floor planning [12], architecture search [2], gate sizing of VLSI [13], circuit optimization [14] and analog circuit design [15]. Evolutionary algorithm (EA) approaches, on the other hand, have been used for decades to design analog circuits [16] and can be creative in the design of novel solutions to a variety of problems [17]. Both reinforcement learning (RL) and EA approaches are promising for optimization tasks, each offering unique pros and cons. In addition, recent work leverages generative AI (GAI)-based circuit characterization [18] and optimization techniques [19, 20]. In related work, physics-informed neural networks (PINNs) [21], originally designed for solving partial differential equations with informed loss functions, have been used to perform device design and optimization [22, 23, 24]. Codesign across devices, circuits, architectures, and applications for a full-stack solution is a challenge and an ongoing area of research. In previous work, we have shown initial results in leveraging RL for MTJ device codesign [10] and EA for probabilistic circuit optimization using different MTJ devices and tunnel diode device [9]. This new work presents an intelligent, automated codesign framework for emerging devices. In particular, we create a framework that is based on RL and EAs, which allows for multi-objective optimization of parameters of emerging devices for real-world applications. We showcase this framework by providing a comparison of RL and EA approaches for device design and parameter optimization and a demonstration of device parameters for energy-efficient random number generation for gamma distributions for both spin–orbit torque (SOT) and spin transfer torque (STT) MTJ devices. Though this framework is applied in the context of true random number generation using SOT and STT MTJ devices, it can be easily extended to other applications and other device types. Ultimately, our methods produce the best candidate devices and materials properties for optimizing both performance in function and energy efficiency. Generally, we see that performance is improved but energy efficiency is slightly increased, compared to the default parameters used to represent standard CoFeB MTJs. The results also show that for the SOT MTJs, a larger range of material parameters can provide good performance, and material parameters for stronger perpendicular magnetic anisotropy (PMA) are favored. In contrast, for STT MTJs there is a narrower range of parameters to achieve the performance, and weaker PMA is favored. This paper is structured as follows: in Section 2 we provide details on our application with a background on RNGs and the distribution sampling scheme we employ, Section 3 introduces the two MTJ device types we will be designing for our application, Section 4 discusses our AI-guided approach using RL and EA with Section 5 presenting our AI-guided approaches results for device discovery. Section 6 provides a discussion of the results and observations, and we finish with conclusions in Section 7. Additional supplementary information is provided in Section 9."
https://arxiv.org/html/2411.00625v1,Toward Automated Algorithm Design:A Survey and Practical Guide to Meta-Black-Box-Optimization,"In this survey, we introduce Meta-Black-Box-Optimization (MetaBBO) as an emerging avenue within the Evolutionary Computation (EC) community, which incorporates Meta-learning approaches to assist automated algorithm design. Despite the success of MetaBBO, the current literature provides insufficient summaries of its key aspects and lacks practical guidance for implementation. To bridge this gap, we offer a comprehensive review of recent advances in MetaBBO, providing an in-depth examination of its key developments. We begin with a unified definition of the MetaBBO paradigm, followed by a systematic taxonomy of various algorithm design tasks, including algorithm selection, algorithm configuration, solution manipulation, and algorithm generation. Further, we conceptually summarize different learning methodologies behind current MetaBBO works, including reinforcement learning, supervised learning, neuroevolution, and in-context learning with Large Language Models. A comprehensive evaluation of the latest representative MetaBBO methods is then carried out, alongside an experimental analysis of their optimization performance, computational efficiency, and generalization ability. Based on the evaluation results, we meticulously identify a set of core designs that enhance the generalization and learning effectiveness of MetaBBO. Finally, we outline the vision for the field by providing insight into the latest trends and potential future directions. Relevant literature will be continuously collected and updated at https://github.com/GMC-DRL/Awesome-MetaBBO.","Optimization techniques have been central to research for decades [1, 2], with methods applied across engineering [3], economics [4], and science [5]. The optimization problems can be classified into White-Box [6] and Black-Box [7] types. White-Box problems, with transparent structures, allow efficient optimization using gradient-based algorithms like SGD [8], Adam [9], and BFGS [10]. In contrast, Black-Box Optimization (BBO) only provides objective values for solutions, making the analysis and search of the problem space even more challenging. Evolutionary Computation (EC), including Evolutionary Algorithms (EAs) and Swarm Intelligence (SI), is widely recognized as an effective gradient-free approach for solving BBO problems [11]. Over the past decades, EC methods have been extensively applied to various optimization challenges [12, 13, 14, 15, 16], due to their simplicity and versatility. Though effective for solving BBO problems, traditional EC is constrained by the no-free-lunch theorem [17], which asserts that no optimization algorithm can universally outperform others across all problem types, leading to performance trade-offs depending on the problem’s characteristics. In response, various adaptive and self-adaptive EC variants [18, 19, 20, 21, 22, 23, 24, 25] have been developed. These variants leverage historical optimization data for hyper-parameter control or operator/algorithm selection during optimization, improving general performance. However, they face several limitations. 1) Limited generalization: these methods often focus on a specific set of problems, limiting their generalization due to customized designs. 2) Labor-intensive: designing adaptive mechanisms requires both deep knowledge of EC domain and the target optimization problem, making it a complex task. 3) Additional parameters: many adaptive mechanisms introduce extra hyper-parameters, which can significantly impact performance. 4) Sub-optimal performance: despite increased efforts, design biases and delays in reactive adjustments often lead to sub-optimal outcomes. Given this, a natural question arises: can we automatically design effective BBO algorithms while minimizing the dependence on expert input? A recently emerging research topic, known as Meta-Black-Box-Optimization (MetaBBO) [26], has shown possibility of leveraging the generalization strength of Meta-learning [27] to enhance the optimization performance of BBO algorithms in the minimal expertise cost. MetaBBO follows a bi-level paradigm: the meta level typically maintains a policy that takes the low-level optimization information as input and then automatically dictates desired algorithm design for the low-level BBO optimizer. The low-level BBO process evaluates the suggested algorithm design and returns a feedback signal to the meta-level policy regarding the performance gain. The meta-objective of MetaBBO is to meta-learn a policy that maximizes the performance of the low-level BBO process, over a problem distribution. Once the training completes, the learned meta-level policy can be directly applied to address unseen optimization problems, hence reducing the need for expert knowledge to adapt BBO algorithms. Numerous valuable ideas have been proposed and discussed in existing MetaBBO research. From the perspective of algorithm design tasks (meta tasks) that the meta-level policy can address, those MetaBBO works can be categorized into four branches: 1) Algorithm Selection, where for solving the given problem, a proper BBO algorithm is selected by the meta-level policy from a pre-collected optimizer/operator pool. 2) Algorithm Configuration, where the hyper-parameters and/or operators of a BBO algorithm are adjusted by the meta-level policy to adapt for the given problem. 3) Solution Manipulation, where the meta-level policy is trained to act as a BBO algorithm to manipulate and evolve solutions. 4) Algorithm Generation, where each algorithmic component and the overall workflow are generated by the meta-level policy as a novel BBO algorithm. From the perspective of learning paradigms adopted for training the meta-level policy, different learning methods such as reinforcement learning (MetaBBO-RL) [28, 29, 30, 31, 32, 33, 34], auto-regressive supervised learning (MetaBBO-SL) [35, 36, 37, 38, 39, 40], neuroevolution (MetaBBO-NE) [41, 42, 43], and Large Language Models (LLMs)-based in-context learning (MetaBBO-ICL) [44, 37, 45, 46, 47] have been investigated in existing works. From the perspective of low-level BBO process, MetaBBO has been instantiated to various optimization scenarios such as single-objective optimization [31, 32, 34], multi-objective optimization, multi-modal optimization [48], large scale global optimization [43, 42, 40], and multi-task optimization [49, 50]. Such an intricate combination of algorithm design tasks, learning paradigms, and low-level BBO scenarios makes it challenging for new practitioners to systematically learn, use, and develop MetaBBO methods. Unfortunately, there is still a lack of a comprehensive survey and practical guide to the advancements in MetaBBO. Figure 1: Roadmap of the content structure, beginning with a concept introduction, followed by a review of existing methods across different taxonomies, a evaluation of selected methods, and a summary of key design strategies and future vision. While some related surveys discussed the integration of learning systems into EC algorithm designs, they have several limitations: 1) Previous surveys [51, 52, 53] focus on one or two algorithm design tasks, such as algorithm configuration [53] and algorithm generation [51, 52]. These surveys therefore show short in providing comprehensive review and comparison analysis on all four design tasks. 2) Some surveys [54, 55, 56, 57] focus on a particular learning paradigm - RL [58]. However, in MetaBBO, various learning paradigms can be adopted, each with distinct characteristics. 3) In addition to reviewing relevant papers, existing surveys lack a practical guide that provides a comprehensive experimental evaluation of MetaBBO methods and a summary of key design strategies, falling short in offering in-depth evaluations or actionable insights for implementing MetaBBO methods. Figure 2: A conceptual overview of the bi-level learning framework of MetaBBO, illustrating the interactions between its core components to clarify the overall workflow. To address the gaps in previous surveys, this paper provides a more comprehensive coverage of the MetaBBO field. Fig. 1 offers a roadmap to help readers quickly navigate the overall content structure. We first provide a formal definition of MetaBBO in Section II. Subsequently, we identify four main algorithm design tasks in existing MetaBBO works and their working scenarios in Section III. In Section IV, we further elaborate four learning paradigms, with easy-to-follow technical details. Section V provides a proof-of-principle performance evaluation on nine representative MetaBBO methods. According to the evaluation results, Section VI provides in-depth discussion about the key design strategies in MetaBBO. Finally, we outline the vision for the MetaBBO field in Section VII. The contributions of this survey are generally summarized as follows: • The first comprehensive survey that sorts out existing literature on MetaBBO. We provide a clear categorization of existing MetaBBO works according to four distinct meta-level tasks, along with a detailed elaboration of four different learning paradigms behind. • A proof-of-principle evaluation is conducted to provide practical comparison between MetaBBO works, leading to an in-depth discussion over several key design strategies related to the learning effectiveness, training efficiency, and generalization. • In the end of this paper, we mark several interesting and promising future research directions of MetaBBO, focusing different aspects such as the generalization potential, the end-to-end workflow, and the integration of LLMs."
https://arxiv.org/html/2411.00140v1,ViT-LCA: A Neuromorphic Approach for Vision Transformers,"The recent success of Vision Transformers has generated significant interest in attention mechanisms and transformer architectures. Although existing methods have proposed spiking self-attention mechanisms compatible with spiking neural networks, they often face challenges in effective deployment on current neuromorphic platforms. This paper introduces a novel model that combines vision transformers with the Locally Competitive Algorithm (LCA) to facilitate efficient neuromorphic deployment. Our experiments show that ViT-LCA achieves higher accuracy on ImageNet-1K dataset while consuming significantly less energy than other spiking vision transformer counterparts. Furthermore, ViT-LCA’s neuromorphic-friendly design allows for more direct mapping onto current neuromorphic architectures.","Neuromorphic computing represents a paradigm shift in computing, characterized by its low-power processing capabilities and brain-inspired architectures [1, 2, 3, 4, 5, 6, 7]. This approach emulates biological neural networks through the use of Spiking Neural Networks (SNNs). One of the primary advantages of neuromorphic chips lies in their capacity for highly parallel and energy-efficient computations. By performing operations asynchronously and maintaining proximity between synapses and weight calculations, these systems significantly reduce data movement, thereby enhancing overall computational efficiency. These platforms integrate many-core systems capable of instantiating large populations of spiking neurons, enabling information processing that mimics the dynamics of biological neural systems. Additionally, by utilizing crossbar arrays and memristores [8, 9, 10] to store multi-bit quantities as conductance values, neuromorphic computing is particularly well-suited for efficiently evaluating matrix-vector-multiplications, which are fundamental to deep learning algorithms. A particularly interesting model in neuromorphic computing is the Locally Competitive Algorithm (LCA) [11, 12], which is a computational model and learning algorithm that iteratively updates neuron activity to achieve a sparse representation of input data. This computational model has been implemented on recent neuromorphic platforms [2, 13, 14]. The competitive mechanism inherent in LCA ensures that only a limited number of neurons become active at any given time, facilitating efficient coding of high-dimensional data. One proposal for leveraging the LCA in neuromorphic computing is the Exemplar LCA-Decoder [15]. Functioning as a single-layer encoder-decoder, this computational model iteratively updates neuron activity to identify a sparse representation of the input data (i.e, encoding) and then uses these neuron activities for classification tasks (i.e, decoding). Recently, the Transformer architecture [16] and its variants have demonstrated impressive performance across a range of tasks, including natural language processing [17, 18] and computer vision [19, 20, 21]. This success is largely due to their ability to effectively capture long-range dependencies, a capability primarily attributed to the self-attention mechanism. Given the enormous computational requirements of transformer architectures, deploying these models on devices with limited resources remains a significant challenge. As a result, integrating transformer architectures with neuromorphic computing represents a promising research avenue. In particular, the combination of transformer architectures and LCA-based learning could lead to more efficient and biologically inspired artificial intelligence systems. However, this area remains largely unexplored. This paper presents ViT-LCA, which leverages Vision Transformers (ViT) [19] to extract self-attention representations and incorporates these representations into an LCA-based SNN. This algorithm effectively addresses the challenges of deploying transformer models on energy-constrained neuromorphic platforms. The self-attention representations are extracted once and stored in non-volatile memory elements, enabling in-memory computation on neuromorphic systems that emphasize specialized operations and energy efficiency. Our approach consists of two stages. In the first stage, a transformer encoder generates self-attention representations from the input image. In the second stage, these representations are processed by a single-layer SNN that employs a LCA encoder-decoder architecture for classification tasks. In this study, we evaluate ViT-LCA on CIFAR-10 [22], CIFAR-100 [23] and ImageNet-1 [24] datasets and assess the effectiveness of integrating ViT’s self-attention representation with the efficiency of sparse coding through LCA for deployment on neuromorphic systems. By inputting self-attention representations (contextual embeddings) derived from ViT into a single-layer SNN model, we achieved high classification accuracy while ensuring low computational overhead and high energy efficiency."
https://arxiv.org/html/2411.00335v1,NCST: Neural-based Color Style Transfer for Video Retouching,"Video color style transfer aims to transform the color style of an original video by using a reference style image. Most existing methods employ neural networks, which come with challenges like opaque transfer processes and limited user control over the outcomes. Typically, users cannot fine-tune the resulting images or videos. To tackle this issue, we introduce a method that predicts specific parameters for color style transfer using two images. Initially, we train a neural network to learn the corresponding color adjustment parameters. When applying style transfer to a video, we fine-tune the network with key frames from the video and the chosen style image, generating precise transformation parameters. These are then applied to convert the color style of both images and videos. Our experimental results demonstrate that our algorithm surpasses current methods in color style transfer quality. Moreover, each parameter in our method has a specific, interpretable meaning, enabling users to understand the color style transfer process and allowing them to perform manual fine-tuning if desired.","Image color style transfer involves transferring the color style of style image onto content image, thereby altering the color of the content image while preserving its original structure as shown in Figure 1. Both image and video color style transfer have numerous real-world applications, including promotional material creation, post-production color grading of photos, and applying filters in videos and games to enhance thematic coherence and expressiveness. Consequently, this technology has attracted significant attention in recent years 11 ; 15 . Color style transfer, traditionally applied to individual images, is increasingly falling short of addressing the demands of video style transfer 10 . Techniques that rely on neural networks for image color style transfer may lead to disrupted visual continuity in videos. Moreover, these methods often result in inefficiencies due to inference with neural networks during the transfer 19 . Another limitation of neural network-reliant color style transfer is its opacity—the inability of users to access or adjust color parameters during conversion, which makes post-adjustment fine-tuning impossible. The NLUT 16 method improves the efficiencies of color style transfer by predicting a 3D lookup table with neural networks. However, it still conceals the color adjustment parameters involved in the process. To address these challenges, we propose using neural networks to predict the parameters for color style transfer. Our method predicts color transfer parameters—such as contrast and brightness—between a style image and a content image, meeting thereby diverse color style transfer demands. Additionally, these parameters can be manually adjusted by users, offering greater flexibility. The parameters we predict are inherently suitable for video color adjustment, and when applied, they mitigate the flickering issues typical of neural network-based video color style transfer. Furthermore, our method supports test-on-time training for specific video transfers, enhancing its effectiveness for tailored applications. These predicted color adjustment parameters can also be transformed into 3D LUTs for video color style transfer, achieving ultra-fast conversion speeds while maintaining high-quality results, similar to the capabilities of NLUT. In summary, our key contributions are as follows: • We propose a method that automatically predicts color style transfer parameters using a neural network for video style transfer, making the style transfer process transparent. • Our method can be seamlessly integrated with other methods to achieve personalized objectives such as enhancing efficiency. • Experiments show that our method has superior performance in color style transfer effects and achieves higher consistency in video color style transfer."
https://arxiv.org/html/2411.00288v1,Inducing Semi-Structured Sparsity by Masking for Efficient Model Inference in Convolutional Networks,"The crucial role of convolutional models, both as standalone vision models and backbones in foundation models, necessitates effective acceleration techniques. This paper proposes a novel method to learn semi-structured sparsity patterns for convolution kernels in the form of maskings enabling the utilization of readily available hardware accelerations. The approach accelerates convolutional models more than two-fold during inference without decreasing model performance. At the same time, the original model weights and structure remain unchanged keeping the model thus easily updatable. Beyond the immediate practical use, the effect of maskings on prediction is easily quantifiable. Therefore, guarantees on model predictions under maskings are derived showing stability bounds for learned maskings even after updating the original underlying model.111Code available at github.com/ddanhofer/Semi-Structured-Sparsity-CNNs","The increasing complexity of deep learning models [21], their deployment in applications [5], and the adoption of reflection incurring several inference passes per query, e.g., as in the O1 models from the GPT family [3], shifts the relative amounts of resources spent during the model lifetime from the training to the inference stage [7, 35]. It therefore becomes imperative to make models more efficient [46]. One way of achieving this is by spending a comparatively small, additional share of resources during training to learn a one-time modification of the model that lowers the model’s inference and thus lifetime cost [30, 40]. First and foremost, such a modification is effective if it decreases the model’s computational and time cost at a relatively low additional training overhead while not affecting the prediction performance of the model negatively [22]. Additionally, there are other desirable properties of such one-time modifications: From an application perspective the achievable gain in efficiency is only useful if it can be leveraged easily, a well-known challenge, e.g., with sparsifying models [8, 15]. Taking into consideration the increasing popularity of large, expensive to train, foundation models [16] or models employed in an online setting subject to continuous updates the proposed change should not affect the possibility to update the model, e.g., by changing the weights or architecture underlying the model. Ideally, if such a model is updated, the learned modification can even be reused under the constraint of the magnitude of change imposed by updating the model. Semi-structured sparse maskings satisfy the above properties by replacing the dense matrix operations usually required during inference by cheaper and faster operations on semi-structured sparse matrices [4]. While many works have demonstrated that sparse (pruned) submodels can solve the same task at almost no loss of performance [2, 26] the sparsity of the models does not necessarily have to adhere to a specific pattern making it difficult to turn theoretically obtained computational speedups by saving on data loading and computational operations into practical efficiency gains [14]. Regular patterns are more “machine-friendly” inducing the desired efficiency a priori but limiting the choices for the sparse patterns, which thus need to be chosen carefully with the goal of minimizing the loss of inference performance in mind. This paper proposes a novel method of learning regularly sparse masking patterns for convolutions, key building blocks for state-of-the art Computer Vision (CV) models [25] and foundation models building on CV models as their backbone [38]. The proposed method • shows how to effectively use readily available hardware accelerations for semi-structured sparse matrices in convolution kernels to accelerate inference, • outperforms available heuristics for semi-structured sparsity showing that semi-structured sparsity masks can be learned with a fraction of the original training resources while incurring a negligible performance loss in CV classification tasks, • provides the additional advantage of not changing the original set of trained weights keeping models updatable and rendering the method especially attractive for use in large models, e.g., foundation models and in online settings, • induces an easily quantifiable change to the model’s prediction behavior and thus lends itself to settings where hard guarantees on model predictions are of interest. In the following section the adoption of semi-structured sparsity and sparsity in convolutional models are addressed. Section 3 of the paper covers modeling semi-structured sparsity in general, in convolutional models, and the theoretical implications of such model alterations in inference. The results of empirically testing the method on widely used convolutional architectures are presented in Section 4 followed up by a discussion of the method presented and a conclusion."
https://arxiv.org/html/2411.00222v1,Protecting Feed-Forward Networks from Adversarial Attacks Using Predictive Coding,"An adversarial example is a modified input image designed to cause a Machine Learning (ML) model to make a mistake; these perturbations are often invisible or subtle to human observers and highlight vulnerabilities in a model’s ability to generalize from its training data. Several adversarial attacks can create such examples, each with a different perspective, effectiveness, and perceptibility of changes. Conversely, defending against such adversarial attacks improves the robustness of ML models in image processing and other domains of deep learning. Most defence mechanisms require either a level of model awareness, changes to the model, or access to a comprehensive set of adversarial examples during training, which is impractical. Another option is to use an auxiliary model in a preprocessing manner without changing the primary model. This study presents a practical and effective solution – using predictive coding networks (PCnets) as an auxiliary step for adversarial defence. By seamlessly integrating PCnets into feed-forward networks as a preprocessing step, we substantially bolster resilience to adversarial perturbations. Our experiments on MNIST and CIFAR10 demonstrate the remarkable effectiveness of PCnets in mitigating adversarial examples with about 82%percent8282\%82 % and 65%percent6565\%65 % improvements in robustness, respectively. The PCnet, trained on a small subset of the dataset, leverages its generative nature to effectively counter adversarial efforts, reverting perturbed images closer to their original forms. This innovative approach holds promise for enhancing the security and reliability of neural network classifiers in the face of the escalating threat of adversarial attacks.","An adversarial example is a modified input intended to cause a machine-learning model to make a mistake. The modifications are often imperceptible or very subtle to human observers. However, predictive coding can reverse such alterations due to its perturbation resiliency, providing more robustness against such attacks. This defensive strategy against adversarial attacks includes generative mechanisms that revert the perturbed images to their original form. Predictive coding offers a theoretical framework to support such a defence. To help understand this work, we will briefly discuss adversarial examples, including how to create and defend against them. We will also mention the attack methods we used in our experiments and popular corresponding defence strategies in subsections 1.1.1 and 1.1.2, respectively. We will introduce the predictive coding framework and its learning algorithm in subsection 1.2. We then explain the experiment setups in section 2, and show the results in section 3, which will be discussed in section 4. At the end, we will summarize our work and point to possible future venues in sections 5 and 6, respectively. 1.1 Adversarial Attacks and Defences Unlike humans, who robustly interpret visual stimuli, artificial neural networks can be deceived by adversarial attacks (ATs), particularly perturbation attacks [1]. These attacks subtly alter an image to trick a well-trained trained feed-forward network (FFnet) used for classification tasks [2, 3] (see figure 1). One standard method to create an adversarial example (AE) that causes the FFnet to misclassify the image as a specific target label is to find a perturbation that minimizes the loss function argminδ∈Δ⁢ℓ⁢(Fθ⁢(x+δ),yt),𝛿Δargminbold-ℓsubscript𝐹𝜃𝑥𝛿subscript𝑦𝑡\underset{\delta\in\Delta}{\mathrm{argmin}}\ \boldsymbol{\ell}(F_{\theta}(x+% \delta),y_{t}),start_UNDERACCENT italic_δ ∈ roman_Δ end_UNDERACCENT start_ARG roman_argmin end_ARG bold_ℓ ( italic_F start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x + italic_δ ) , italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , (1) where: • x𝑥xitalic_x represents the image, • δ𝛿\deltaitalic_δ is the perturbation needed to deceive the FFnet when applied to the image, and ‖δ‖∞<ϵsubscriptnorm𝛿italic-ϵ\|\delta\|_{\infty}<\epsilon∥ italic_δ ∥ start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT < italic_ϵ is enforced. • ΔΔ\Deltaroman_Δ represents allowable perturbations that are visually indistinguishable to humans. • ytsubscript𝑦𝑡y_{t}italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is the 1-hot vector (i.e., etsubscript𝑒𝑡e_{t}italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT) corresponding to the target label t𝑡titalic_t. • Fθ⁢(⋅)subscript𝐹𝜃⋅F_{\theta}(\cdot)italic_F start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( ⋅ ) is the FFnet model, (i.e., Fθ:x→y∈ℝk:subscript𝐹𝜃→𝑥𝑦superscriptℝ𝑘F_{\theta}:x\rightarrow y\in\mathbb{R}^{k}italic_F start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT : italic_x → italic_y ∈ blackboard_R start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT, where k𝑘kitalic_k is the number of classes), • θ𝜃\thetaitalic_θ represents all parameters defining the model. • ℓbold-ℓ\boldsymbol{\ell}bold_ℓ is the cross-entropy loss function. This optimization can be achieved iteratively [4]. Alternatively, instead of deceiving the model by a specific target label, the optimization can be solved for an untargeted attack by maximizing the loss argmaxδ∈Δ⁢(x)⁢ℓ⁢(Fθ⁢(x+δ),y),𝛿Δ𝑥argmaxbold-ℓsubscript𝐹𝜃𝑥𝛿𝑦\underset{\delta\in\Delta(x)}{\mathrm{argmax}}\ \boldsymbol{\ell}(F_{\theta}(x% +\delta),y),start_UNDERACCENT italic_δ ∈ roman_Δ ( italic_x ) end_UNDERACCENT start_ARG roman_argmax end_ARG bold_ℓ ( italic_F start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x + italic_δ ) , italic_y ) , (2) for the given pair (x,y)𝑥𝑦(x,y)( italic_x , italic_y ), which can be achieved in one step using methods like the fast gradient sign method (FGSM) [5], or other approaches [6, 7, 8, 9, 10, 11]. When testing a well-trained FFnet MNIST classifier (with an accuracy of approximately 98%percent9898\%98 %) against FGSM-generated AEs (with ϵ≃0.78%similar-to-or-equalsitalic-ϵpercent0.78\epsilon\simeq 0.78\%italic_ϵ ≃ 0.78 %), the adversarial success rate is about 41%percent4141\%41 %. To defend against ATs, augmenting the training dataset with AEs can improve the classifier’s resilience, achieving an accuracy of approximately 94%percent9494\%94 %. Alternatively, a min-max approach to directly counteract AEs can enhance robustness within specific perturbation limits [7, 12]. Figure 1: FFnet’s perception of the image changes as the noise perturbed the image. FFnet perceives the original image Pr⁡(y0=1|x)=0.99probabilitysubscript𝑦0conditional1𝑥0.99\Pr(y_{0}=1|x)=0.99roman_Pr ( start_ARG italic_y start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 1 | italic_x end_ARG ) = 0.99 while the perception changed to Pr⁡(y3=1|x+δ)=0.87probabilitysubscript𝑦3conditional1𝑥𝛿0.87\Pr(y_{3}=1|x+\delta)=0.87roman_Pr ( start_ARG italic_y start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT = 1 | italic_x + italic_δ end_ARG ) = 0.87 on perturbation. 1.1.1 Creating Adversarial Examples Adversarial examples (AEs) are crucial for assessing and improving the robustness of machine learning (ML) models, especially in deep learning. In image data, creating AEs involves making imperceptible changes to the original image to mislead the model into misclassifying the image. Various methods are available to generate such AEs, particularly for deceiving deep neural networks. Below, we briefly discuss the main gradient-based techniques relevant to our work while noting that there are other techniques to create AEs [8, 9, 10, 11]. • Fast Gradient Sign Method (FGSM) modifies the input image by computing the loss gradient for the input image and then making a small step in the opposite direction to increase the loss [5]. • Basic Iterative Method (BIM), an extension of FGSM, takes multiple small steps while adjusting the direction of the perturbation at each step [6]. • Projected Gradient Descent (PGD) modifies the input image in multiple iterations with a constraint on the perturbation’s size. PGD starts from a random point within a small ball (i.e., ϵitalic-ϵ\epsilonitalic_ϵ-ball) around the original image and performs a series of gradient descent steps to maximize the prediction error while ensuring the perturbation is smaller than the specified ϵitalic-ϵ\epsilonitalic_ϵ [7]. • Carlini & Wagner (C&W) attack optimizes the perturbation directly through a loss function that aims to deceive to a desired target label and keep the perturbation small. It often produces subtle perturbations that are highly effective at fooling neural networks [4]. These methods differ in complexity, the amount of required knowledge about the target model (white box vs. black box), the type of perturbations (targeted vs. non-targeted), and the strength and stealthiness of the attack. The choice of method often depends on the adversary’s access to the model parameters and its specific requirements, including the robustness of the target model and the desired invisibility of the modifications. 1.1.2 Defending Against Adversarial Attacks Although various adversarial attacks exist, some defence mechanisms attempt to protect ML models against such attacks. Here, we review defence strategies for each previously mentioned attack. • For FGSM and BIM/PGD: Adversarial training involves training the model using adversarial and clean examples. It has been particularly effective against gradient-based attacks like FGSM and BIM. Gradient masking attempts to hide or modify gradients so that they are less useful for generating adversarial examples. However, this method has often been criticized and can be circumvented [13]. • For C&W Attack: Some defences estimate the likelihood that input is adversarial using auxiliary models or statistical analyses [4]. Defensive distillation involves training a model to output softened probabilities of classes, making it harder for an attacker to find gradients that can effectively manipulate the model’s output [13]. While these methods offer some protection against specific types of adversarial attacks, it is essential to note that there is no one-size-fits-all solution, and sophisticated or adaptive attackers can circumvent many defences. However, some defence strategies come with a cost, and there is a trade-off between robustness and accuracy [12]. Continued research is crucial to improving the robustness of neural networks against these threats. 1.2 Predictive Coding Computational neuroscience seeks to understand behavioural and cognitive phenomena at the level of individual neurons or networks of neurons. One approach to solving difficult problems, such as adversarial attacks, which do not seem to be a problem for the brain, is to explore biologically plausible perception models. The model we will be using is predictive coding (PC)111Various cortical theories support the bidirectional model [14, 15, 16], as well as free-energy principles [17]., a neural model capable of implementing error backpropagation in a biologically plausible manner [18, 19, 20]. 1.2.1 Model Schema and The Learning Algorithms The concept of predictive coding suggests that the brain works to minimize prediction error [21]. This model aims to improve overall predictions, and all neurons work towards this common objective. In a predictive coding network (PCnet), each neuron, or PC unit, consists of a value (v𝑣vitalic_v) and an error node (ε𝜀\varepsilonitalic_ε). These PC units are organized into layers, similar to artificial neural networks (ANNs), forming PCnets that learn by adjusting parameters to refine predictions and reduce errors between layers. For example, in a PCnet, layer i𝑖iitalic_i contains vectors visubscript𝑣𝑖v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and εisubscript𝜀𝑖\varepsilon_{i}italic_ε start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, as illustrated in figure 2. Vector visubscript𝑣𝑖v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT predicts the values of the next layer, vi−1subscript𝑣𝑖1v_{i-1}italic_v start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT, using prediction weights Mi−1subscript𝑀𝑖1M_{i-1}italic_M start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT. The resulting error, εi−1subscript𝜀𝑖1\varepsilon_{i-1}italic_ε start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT, is then communicated back via correction weights Wi−1subscript𝑊𝑖1W_{i-1}italic_W start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT, allowing visubscript𝑣𝑖v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT to improve its predictions. Figure 2: A typical PCnet arranged in a feed-forward manner. Each box represents a population of neurons containing value and error nodes. The network dynamics (as in equations 3 - 7) are described by the activation function σ𝜎\sigmaitalic_σ, Hadamard product ⊙direct-product\odot⊙, outer product ⊗tensor-product\otimes⊗, decay coefficient ξ𝜉\xiitalic_ξ, and time constants τ𝜏\tauitalic_τ and γ𝛾\gammaitalic_γ, where τ<γ𝜏𝛾\tau<\gammaitalic_τ < italic_γ. τ⁢ε˙i𝜏subscript˙𝜀𝑖\displaystyle\tau\,\dot{\varepsilon}_{i}italic_τ over˙ start_ARG italic_ε end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT =vi−Mi⁢σ⁢(vi+1)−bi−ξ⁢εiabsentsubscript𝑣𝑖subscript𝑀𝑖𝜎subscript𝑣𝑖1subscript𝑏𝑖𝜉subscript𝜀𝑖\displaystyle=v_{i}-M_{i}\sigma(v_{i+1})-b_{i}-\xi\varepsilon_{i}= italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_M start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_σ ( italic_v start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT ) - italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_ξ italic_ε start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT (3) τ⁢v˙i𝜏subscript˙𝑣𝑖\displaystyle\tau\,\dot{v}_{i}italic_τ over˙ start_ARG italic_v end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT =−εi+Wi−1⁢εi−1⊙σ′⁢(vi)absentsubscript𝜀𝑖direct-productsubscript𝑊𝑖1subscript𝜀𝑖1superscript𝜎′subscript𝑣𝑖\displaystyle=-\varepsilon_{i}+W_{i-1}\varepsilon_{i-1}\odot\sigma^{\prime}(v_% {i})= - italic_ε start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_W start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT italic_ε start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ⊙ italic_σ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) (4) γ⁢M˙i𝛾subscript˙𝑀𝑖\displaystyle\gamma\,\dot{M}_{i}italic_γ over˙ start_ARG italic_M end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT =εi⊗σ⁢(vi+1)absenttensor-productsubscript𝜀𝑖𝜎subscript𝑣𝑖1\displaystyle=\varepsilon_{i}\otimes\sigma(v_{i+1})= italic_ε start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ⊗ italic_σ ( italic_v start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT ) (5) γ⁢W˙i𝛾subscript˙𝑊𝑖\displaystyle\gamma\,\dot{W}_{i}italic_γ over˙ start_ARG italic_W end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT =σ⁢(vi+1)⊗εiabsenttensor-product𝜎subscript𝑣𝑖1subscript𝜀𝑖\displaystyle=\sigma(v_{i+1})\otimes\varepsilon_{i}= italic_σ ( italic_v start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT ) ⊗ italic_ε start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT (6) γ⁢b˙i𝛾subscript˙𝑏𝑖\displaystyle\gamma\,\dot{b}_{i}italic_γ over˙ start_ARG italic_b end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT =εiabsentsubscript𝜀𝑖\displaystyle=\varepsilon_{i}= italic_ε start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT (7) where bisubscript𝑏𝑖b_{i}italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is the bias for error node εisubscript𝜀𝑖\varepsilon_{i}italic_ε start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. Training a PCnet involves clamping input and output-layer value nodes to sensory input and target values and running the network until it reaches equilibrium. The network’s state variables (visubscript𝑣𝑖v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, εisubscript𝜀𝑖\varepsilon_{i}italic_ε start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT) reach equilibrium faster than the parameters (Misubscript𝑀𝑖M_{i}italic_M start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, Wisubscript𝑊𝑖W_{i}italic_W start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, bisubscript𝑏𝑖b_{i}italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT) due to τ<γ𝜏𝛾\tau<\gammaitalic_τ < italic_γ. After training, the parameters M𝑀Mitalic_M, W𝑊Witalic_W, b𝑏bitalic_b are fixed, effectively setting γ𝛾\gammaitalic_γ to infinity. When a perfect prediction is achieved, the error signal (ε𝜀\varepsilonitalic_ε) is zero, stabilizing the value node without further corrections. This state minimizes the Hopfield-like energy function [18] given by equation, E=ξ2⁢∑i‖𝜺i‖2.𝐸𝜉2subscript𝑖superscriptnormsubscript𝜺𝑖2E=\tfrac{\xi}{2}\sum_{i}\|{\bf\it\varepsilon}_{i}\|^{2}.italic_E = divide start_ARG italic_ξ end_ARG start_ARG 2 end_ARG ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∥ bold_italic_ε start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . (8) After training, when initializing the network with a given input image, when the value nodes are unclamped, the network’s ability to reduce energy can lead to potential changes in the input image. The PCnet modifies images without impacting their correct classification, as illustrated in figure 3a. The left image displays the original version, while the right image shows the version altered by PCnet. Likewise, when PCnet introduces perturbations to the adversarial image, as seen in figure 3b, the FFnet can classify it correctly. (a) Perturbation made to an image. (b) Perturbation made to an adversarial image. Figure 3: PCnet perturbation is demonstrated using both the original and adversarial images. PCnet modifies the given input based on its trained dynamics. As shown in LABEL:sub@fig:imageBeforeAfterPC, the original image x𝑥xitalic_x is depicted on the left, while its perturbation PCnet⁢(x)PCnet𝑥\mathrm{PCnet}(x)roman_PCnet ( italic_x ) is shown on the right. Similarly, LABEL:sub@fig:advBeforeAfterPC presents the adversarial image z𝑧zitalic_z on the left, alongside its perturbation p𝑝pitalic_p on the right. PCnet’s approach to the credit assignment problem differs from backpropagation (backprop), which is the learning algorithm of ANNs [22]. Backpropagation seems unlikely in the brain for several reasons, such as its requirement for weight transposing and transferring between layers. In contrast, PCnet can effectively learn without these requirements using the dynamics of each parameter (equations (5), (6), and (7)). These dynamics are consistent with the Hebbian learning rule, which only requires local information and pre- and post-synaptic activities [23, 24, 25, 26, 27, 28, 29]. This learning algorithm facilitates flexibility and feasibility in using different architectures."
https://arxiv.org/html/2411.00156v1,Unlocking the Potential of Global Human Expertise,"Solving societal problems on a global scale requires the collection and processing of ideas and methods from diverse sets of international experts. As the number and diversity of human experts increase, so does the likelihood that elements in this collective knowledge can be combined and refined to discover novel and better solutions. However, it is difficult to identify, combine, and refine complementary information in an increasingly large and diverse knowledge base. This paper argues that artificial intelligence (AI) can play a crucial role in this process. An evolutionary AI framework, termed RHEA, fills this role by distilling knowledge from diverse models created by human experts into equivalent neural networks, which are then recombined and refined in a population-based search. The framework was implemented in a formal synthetic domain, demonstrating that it is transparent and systematic. It was then applied to the results of the XPRIZE Pandemic Response Challenge, in which over 100 teams of experts across 23 countries submitted models based on diverse methodologies to predict COVID-19 cases and suggest non-pharmaceutical intervention policies for 235 nations, states, and regions across the globe. Building upon this expert knowledge, by recombining and refining the 169 resulting policy suggestion models, RHEA discovered a broader and more effective set of policies than either AI or human experts alone, as evaluated based on real-world data. The results thus suggest that AI can play a crucial role in realizing the potential of human expertise in global problem-solving.","Integrating knowledge and perspectives from a diverse set of experts is essential for developing better solutions to societal challenges, such as policies to curb an ongoing pandemic, slow down and reverse climate change, and improve sustainability [33, 41, 57, 63, 64]. Increased diversity in human teams can lead to improved decision-making [25, 62, 83], but as the scale of the problem and size of the team increases, it becomes difficult to discover the best combinations and refinements of available ideas [37]. This paper argues that artificial intelligence (AI) can play a crucial role in this process, making it possible to realize the full potential of diverse human expertise. Though there are many AI systems that take advantage of human expertise to improve automated decision-making [4, 31, 66], an approach to the general problem must meet a set of unique requirements: It must be able to incorporate expertise from diverse sources with disparate forms; it must be multi-objective since conflicting policy goals will need to be balanced; and the origins of final solutions must be traceable so that credit can be distributed back to humans based on their contributions. An evolutionary AI framework termed RHEA (for Realizing Human Expertise through AI) is developed in this paper to satisfy these requirements. Evolutionary AI, or population-based search, is a biologically-inspired method that often leads to surprising discoveries and insights [5, 15, 39, 48, 67]; it is also a natural fit here since the development of ideas in human teams mirrors an evolutionary process [14, 17, 38, 32]. Implementing RHEA for a particular application requires the following steps (Fig. 1): 1. Define. Define the problem in a formal manner so that solutions from diverse experts can be compared and combined. 2. Gather. Solicit and gather solutions from a diverse set of experts. Solicitation can take the form of an open call or a direct appeal to known experts. 3. Distill. Use machine learning to convert (distill) the internal structure of each gathered solution into a canonical form such as a neural network. 4. Evolve. Recombine and refine the distilled solutions using a population-based search to realize the complementary potential of the ideas in the expert-developed solutions. RHEA is first illustrated through a formal synthetic example below, demonstrating how this process can result in improved decision-making. RHEA is then put to work in a large-scale international experiment on developing non-pharmaceutical interventions for the COVID-19 pandemic. The results show that broader and better policy strategies can be discovered in this manner, beyond those that would be available through AI or human experts alone. The results also highlight the value of soliciting diverse expertise, even if some of it does not have immediately obvious practical utility: AI may find ways to recombine it with other expertise to develop superior solutions. Figure 1: The RHEA (Realizing Human Expertise through AI) framework. The framework consists of four components: Defining the prediction and prescription tasks, gathering the human solutions, distilling them into a canonical form, and evolving the population of solutions further. a, The predictor maps context and actions to outcomes and thus constitutes a surrogate, or a “digital twin”, of the real world. For example, in the Pandemic Response Challenge experiment, the context consisted of data about the geographic region for which the predictions were made, e.g., historical data of COVID-19 cases and intervention policies; actions were future schedules of intervention policies for the region; and outcomes were predicted future cases of COVID-19 along with the stringency of the policy. b, Given a predictor, the prescriptor generates actions that yield optimized outcomes across contexts. c, Humans are solicited to contribute expertise by submitting prescriptors using whatever methodology they prefer, such as decision rules, epidemiological models, classical statistical techniques, and gradient-based methods. d, Each submitted prescriptor is distilled into a canonical neural network that replicates its behavior. e, This population of neural networks is evolved further, i.e., the distilled models are recombined and refined in a parallelized, iterative search process. They build synergies and extend the ideas in the original solutions, resulting in policies that perform better than the original ones. For example, in the Pandemic Response Challenge, the policies recommend interventions that lead to minimal cases with minimal stringency. To summarize, the main contributions of this paper are as follows: (1) Recognizing that bringing together diverse human expertise is a key challenge in solving many complex problems; (2) Identifying desiderata for an AI process that accomplishes this task; (3) Demonstrating that existing approaches do not satisfy these desiderata; (4) Formalizing a new framework, RHEA, to satisfy them; (5) Instantiating a first concrete implementation of RHEA using standard components; and (6) Evaluating this implementation in a global application: The XPRIZE Pandemic Response Challenge."
https://arxiv.org/html/2411.00110v1,Lagrangian neural networks for nonholonomic mechanics,"Lagrangian Neural Networks (LNNs) are a powerful tool for addressing physical systems, particularly those governed by conservation laws. LNNs can parametrize the Lagrangian of a system to predict trajectories with nearly conserved energy. These techniques have proven effective in unconstrained systems as well as those with holonomic constraints. In this work, we adapt LNN techniques to mechanical systems with nonholonomic constraints. We test our approach on some well-known examples with nonholonomic constraints, showing that incorporating these restrictions into the neural network’s learning improves not only trajectory estimation accuracy but also ensures adherence to constraints and exhibits better energy behavior compared to the unconstrained counterpart.","The laws of motion of a Lagrangian system are determined by the principle of stationary action, also known as Hamilton’s principle. This principle states that the action is minimal (or stationary) throughout a mechanical process. From this statement, the differential equations known as Euler-Lagrange equations are derived. If the Lagrangian function of a given mechanical system is known, then Euler-Lagrange equations establish the relationship between accelerations, velocities, and positions; that is, the system dynamics are obtained from Euler-Lagrange equations. Hence, the goal of Lagrangian mechanics is to write an analytic expression for the Lagrangian function in appropriate generalized coordinates and then develop the Euler-Lagrange equations symbolically into a system of second-order differential equations whose solutions give the system’s trajectory. In many cases, even when Euler-Lagrange equations are available, the solutions are not provided in analytical or explicit forms. Therefore, we can use numerical integrators to estimate the trajectories of a mechanical system. However, numerical integrators can sometimes produce poorly behaved trajectories concerning certain physical observables, such as energy. As an alternative, geometric integrators can be employed, since they are known to preserve energy (see, for instance, [2]). However, they may not be very accurate over long periods. Even worse is the case in which an analytical expression for the Lagrangian function is unknown or difficult to work with because we do not have a system of equations to solve. In recent years, there has been an increasing interest in using neural networks to address different issues of mechanical systems (see for example [7],[8],[11],[12],[14]). In this line, Lagrangian Neural Networks were introduced in [5] as an enhancement over other types of neural networks used in mechanical systems that do not preserve physical laws, providing a tool for scenarios where, for example, equations of motion are not available to get the actual trajectory. This method assumes that the Lagrangian of a mechanical system, a scalar function, can be parametrized using a neural network and be learned directly from the system’s data. That is, the goal of LNNs is to predict the Lagrangian function of a system based on data about its positions and velocities. This approach aims to represent the system’s equations of motion with a neural network while ensuring the preservation of some specific physical properties."
