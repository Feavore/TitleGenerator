URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.04824v1,Image-based adaptive domain decomposition for continuum damage models,"We present a novel image-based adaptive domain decomposition FEM framework to accelerate the solution of continuum damage mechanics problems. The key idea is to use image-processing techniques in order to identify the moving interface between the healthy subdomain and unhealthy subdomain as damage propagates, and then use an iterative Schur complement approach to efficiently solve the problem. The implementation of the algorithm consists of several modular components. Following the FEM solution of a load increment, the damage detection module is activated, a step that is based on several image-processing operations including colormap manipulation and morphological convolution-based operations. Then, the damage tracking module is invoked, to identify the crack growth direction using geometrical operations and ray casting algorithm. This information is then passed into the domain decomposition module, where the domain is divided into the healthy subdomain which contains only undamaged elements, and the unhealthy subdomain which comprises both damaged and undamaged elements. Continuity between the two regions is restored using penalty constraints. The computational savings of our method stem from the Schur complement, which allows for the iterative solution of the system of equations appertaining only to the unhealthy subdomain. We perform an exhaustive comparison between our approach and single domain computations through a series of benchmark examples, and we demonstrate the accuracy, efficiency, and numerical robustness of the proposed framework. We ensure the flexibility of our method by testing it on both local and non-local integral damage laws and structured and unstructured mesh idealizations, and we extend it to more challenging problems where different damage paths eventually merge. Since the key novelty of the method lies in using image processing tools to inform the domain decomposition module, our framework can be readily extended beyond damage mechanics and model several classes of non-linear problems such as plasticity, phase-field, and more. The code and data used in this work will be made publicly available upon publication of the article.","1.1 Thesis statement Many computational models, including damage and fracture models, are notorious for exhibiting elevated computational costs. In both literature and practice, several methods have been developed to counteract this amplified computational expense and promote the solution efficiency. However, many of these methods are either difficult to implement, or specific to one class of models. In this work, we propose an enhanced solution approach that relies on contour images, which are practically produced by almost every modern solver for visualization and post-processing purposes. In this study we focus on continuum damage modeling. The idea is to employ adaptive domain-decomposition and a Schur complement based solver in order to speed up the solution by applying the iterative non-linear solution only on the unhealthy subdomain, while the stiffness of the healthy subdomain remains constant. The novelty of this study relies on implementing a suite of image processing techniques to automatically and adaptively detect and track damage, and then advise the geometries of the subdomains. 1.2 Literature review The propagation of cracks and voids inside the microstructure of a material is highly detrimental for its macroscopic mechanical properties, and consequently it can pose a serious threat to the overall structural integrity. Understanding and mitigating fracture propagation both at the material and at the structural level is a critical prerequisite for establishing safe design protocols across many engineering fields [1, 2, 3, 4], and a plethora of theoretical approaches have been proposed over the last decades to describe fracture-associated phenomena [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]. In light of the elevated expense entailed by experimentally assessing fracture-induced failures, the main pathway by which these frameworks are evaluated is computational fracture modeling. Several mature frameworks exist on this front, such as the Finite Element Method (FEM) [17, 18], the eXtended Finite Element Method (XFEM) [19, 20], the Virtual Element Method (VEM) [21, 22], the Boundary Element Method (BEM) [23, 24], meshless approaches [25], and more [26, 27]. Undoubtedly, the robust development of these frameworks has significantly advanced the research frontiers and has enabled the understanding and exploration of complex fracture mechanisms. Despite the significant progress, a commonly shared bottleneck of most numerical frameworks is their simulation expense. Particularly for FEM, this is a byproduct of the need for a fine mesh discretization with finite elements, especially around the crack tip and along the fracture path. The fine mesh is required to capture accurately the crack initiation and the correct crack propagation trajectory. The computational cost is significantly amplified if one includes additional layers of complexity, such as the presence and interaction of multiple cracks, material anisotropy, multi-phase materials, large-scale domains spanning multiple length scales, or 3D considerations [28, 29, 30, 31, 32]. In particular, several studies report simulation times that can take several hours, days or even weeks to complete [33, 28, 30, 34]. This evidently underlines the importance of developing robust methods to accelerate the numerical solution of such problems, in order to enable their seamless utilization for industrial-level purposes. In order to counteract the notorious expense of fracture computational models researchers have explored a variety of advanced modeling approaches, and these are briefly discussed below. One major pathway to reduce this computational cost includes domain decomposition (DD) frameworks. The core idea of DD approaches is to split a large and complex domain into several smaller subdomains, solve each one separately, and then combine the individual solutions into a global one while satisfying necessary continuity constraints at the interfaces of the subdomains. Several notable DD frameworks for fracture modeling exist, such as the Finite Element Tearing and Interconnecting (FETI) approach [35] and its successors [36, 37, 38], space-time domain decomposition with mixed formulations [39], parallel computing using DD pre-conditioners, [40], and more [41]. Moving beyond domain decomposition, major efforts have also been made to enhance currently available numerical techniques by implementing advanced algorithmic variations in the conventional approaches. Without being nearly exhaustive, we mention modifications to the Newton-Raphson [42] and the arc-length approach [43], GPU parallelization and multi-thread optimization [31, 44], the development of staggered schemes [45, 46] and coupled methods [47, 48], as well as newly-emerged frameworks rooted on neural networks [33, 49]. Overall, as the size, scale and complexity of the fracture problems that are sought to be investigated increase, there is a growing need for developing new approaches in this already active field of research. More recently, researchers started exploring the potential of using image-processing techniques to aid, complement or even bypass the computational modeling of fracture-related problems. Arguably, a research path where image-processing tools are particularly efficient, is the detection of cracks and other discontinuities inside the domain. Several methodologies with varying levels of sophistication have been proposed on this front, and the interested reader is referred to [50, 51] for comprehensive reviews of image-based techniques focusing on crack detection. A closely related and currently growing field involves studies on image-based prediction of crack propagation, and we mention, for example [52], where the authors proposed a methodology to determine the crack kinematics for Mode I and Mode II problems based on image-crack patterns. Nevertheless, all the aforementioned approaches share a common conceptual basis: images are utilized to characterize the topological arrangement (static or evolving) of the crack network. A conceptually different approach of utilizing image-based techniques in fracture modeling regards the identification of constitutive laws. For example, [53] proposed an image-based process to identify a crack propagation law, and [54] developed an inverse approach using Digital Image Processing to identify the parameters of a material damage model. Images have found several other uses, see for example, [55], where the authors developed a convolution-based method to automate the irregular stone packing in the construction of masonry walls, and [56], where the authors trained a Generative Adversarial Network on image-pairs containing state variables (stresses, displacements, etc.) to emulate the entire FEM simulation. Altogether, it is evident that image-based algorithms are currently utilized in several ways as complementary methods to conventional computational modeling, and the development of more sophisticated machine learning algorithms and image-processing tools is expected to further accelerate the growth of this field. 1.3 Scope and Outline The contribution of this article is the development of an image-based adaptive domain decomposition framework that accelerates the numerical solution of damage mechanics problems. The major novelty of our work lies in using image-analysis techniques to monitor the propagation of damage and adaptively update the domain decomposition component of our framework. Conceptually this approach is clearly distinct from the relevant literature, and to the best of our knowledge this is the first time that image-processing methods are utilized in this fashion. To achieve our goal, we focus on continuum damage mechanics and we devise a framework which comprises the following four modules: a) FEM solver, b) damage detection, c) damage tracking and d) domain decomposition. The first module is invoked to perform the non-linear analysis of a load increment, at the end of which we produce an image of the damage contours. Then, we deploy a series of image-recognition algorithms whose purpose is to detect the presence of damage inside the domain. If damage is not detected we continue with the analysis of the next load increment. Otherwise, we proceed to the third module, and we perform an appropriate offset-scaling process to identify the FEM nodes that are enclosed by the area with the detected damage. Finally, we pass this information to the penalty-based domain decomposition module, and utilize it to separate the domain into a healthy subdomain (no damage) and an unhealthy subdomain (with damage). This allows us to arrive at the final partitioned system of equations, which we solve efficiently using the Schur complement method. In this study we focus on the application of the framework to damage mechanics problems, and we examine several benchmark numerical examples using both the local and the non-local integral damage models. However, we note that our method can be readily extended to other non-linear models such as plasticity or phase-field, or even be extended to process other types of images such as ultrasound, X-ray, and micro-CT. Compared to the conventional single domain analysis we record computational savings that scale with the model size, and we find that our method can overcome severe numerical instabilities that hinder the convergence of the conventional solver. The paper is organized as follows. Section 2 contains a brief presentation of the continuum damage mechanics models adopted in this study. Section 3 illustrates the overarching structure of the proposed framework, and Sections 4 - 7 present a detailed description of the utilized sub-routines. The results of the framework’s numerical implementation are presented in Section 8, followed by the summary and conclusions in Section 9."
https://arxiv.org/html/2411.04731v1,MISGUIDE: Security-Aware Attack Analytics for Smart Grid Load Frequency Control,"Incorporating advanced information and communication technologies into smart grids (SGs) offers substantial operational benefits while increasing vulnerability to cyber threats like false data injection (FDI) attacks. Current SG attack analysis tools predominantly employ formal methods or adversarial machine learning (ML) techniques with rule-based bad data detectors to analyze the attack space. However, these attack analytics either generate simplistic attack vectors detectable by the ML-based anomaly detection models (ADMs) or fail to identify critical attack vectors from complex controller dynamics in a feasible time. This paper introduces MISGUIDE, a novel defense-aware attack analytics designed to extract verifiable multi-time slot-based FDI attack vectors from complex SG load frequency control dynamics and ADMs, utilizing the Gurobi optimizer. MISGUIDE can identify optimal (maliciously triggering under/over frequency relays in minimal time) and stealthy attack vectors. Using real-world load data, we validate the MISGUIDE-identified attack vectors through real-time hardware-in-the-loop (OPALRT) simulations of the IEEE 39-bus system.","With the integration of advanced information and communication technologies, traditional power grids are evolving into smart grids (SGs) [1]. Although this integration brings numerous operational benefits, such as improved efficiency for demand response, it also enlarges the attack surface of power grids to cyber threats due to their high dependence on the vulnerable communication infrastructure [2]. According to the 2022 threat intelligence index report, 10.7% of all cyberattacks targeted the energy sector, making it the fourth most attacked industry [3]. One significant category of cyberattacks that has received extensive attention in recent research is the false data injection (FDI) attack [4, 5, 6, 7]. Several real-world incidents have been reported that caused damage to the SGs due to FDI attacks. For example, in 2015, an FDI attack compromised distribution grids in Ukraine, where hackers infiltrated the communication network, causing widespread power outages and affecting over 200,000 customers for several hours [5]. Other notable real-world attacks include Stuxnet [8] and Dragonfly [9], where attackers gained extensive knowledge of the targeted systems and complete access to real-time data in the control centers of critical SG infrastructures. Grid frequency is a crucial indicator of the stable operation of an SG. The primary controllers of the grid generators and a central load frequency controller (LFC) maintain the grid frequency (60 Hz in the US). Deviation from the nominal/standard frequency can result in undesirable effects on the SG, like degradation of load performance, equipment damage, or interference in the protection scheme, which may ultimately lead to grid instability. In response to disturbances, e.g., increased load demand in power grids, the primary frequency response utilizes the automatic decentralized control actions of generators’ active power output to regulate the grid frequency instantaneously. Although the primary controller ensures synchronization of the generator frequency, it cannot restore the system frequency to its nominal value, as the grid’s load demand continuously fluctuates. Hence, a centralized LFC involves re-dispatching the generators to maintain the grid frequency within the desired range as depicted in Figure 1. The SG uses several protection relays to handle underfrequency (UF) and overfrequency (OF) instances. These relays are essential in maintaining a balance between load and generation by disconnecting generators/loads at pre-defined locations as necessary [10]. For example, the OF relays trip generators during excessive frequency excursions to protect the system from frequency instability, thus protecting synchronous generators from causing overheating, mechanical stress, and voltage instability and contributing to broader system instability issues. While most protection relays operate based on local measurements, the occurrence of an FDI attack in the closed-loop control of the dispatching process can cause extreme frequency variations, ultimately leading to false relay operations (FRO), resulting in erroneous tripping of UF or OF relays and far-reaching consequences for the entire system. In an FDI attack, attackers can manipulate and send erroneous load and frequency measurements to the SG communication system as shown in Figure 1, deceiving the LFC into making the wrong reference setpoint of the generators. The figure illustrates the communication between a single bus or substation and the controller. Figure 1: Demonstration of generators dispatching process and possible point of attacks in SG. In response to these challenges, modern grid systems adopt different machine learning (ML)-based anomaly detection models (ADM) to detect abnormal events in safety-critical SG systems. Hence, to understand the actual robustness of the system, it is imperative to analyze the attack space of the system in the presence of the ADM. Traditional approaches, including formal analysis and ML techniques like adversarial ML and reinforcement learning (RL), have shown promise in attack vector identification. Adversarial ML and RL-based techniques are efficient and fast, yet they often fail to extract attacks that can evade ML-based ADMs. The primary limitation of the approaches is that they cannot guarantee optimal or verifiable attack vector identification, stemming from their non-adherence to established grid and ADM constraints, unlike formal methods. Formal method-based analyzers synthesize verifiable attack vectors; however, existing efforts often fail to extract attack vectors from complicated/interdependent ML-based CPS dynamics in a feasible time. One of our notable research works, SHChecker [11], identifies optimal FDI attacks to provide wrong treatment to the connected healthcare systems bypassing different ML models [12]. Another work, SHATTER, proficiently identifies attack vectors in ML-based smart home systems, focusing on time-series ADMs and considering multi-time slot attacks. However, such attack analysis frameworks can only identify attack vectors from independent controller dynamics. Jafari et al. identified FDI attacks by adopting a distinct approach and exploring the dynamics of complex power systems. Yet their research predominantly depends on a rules-based bad data detector (BDD). Throughout the write-up, we will comprehensively compare our proposed analyzer with this work, referred to as TIFS’23. While effective in some contexts, the strategy devised in TIFS’23 primarily identifies straightforward attack vectors subjected to be detected by the ML-based models. Hence, this highlights the need for more advanced and comprehensive attack analytics to effectively synthesize and verify complex attack vectors. The comparative analysis among the proposed and state-of-the-art approaches is highlighted in Table I. To address this research gap, we develop a novel attack analyzer naming Malicious-Activity Investigation for Smart Grid Utilizing Intrusion DEtector (MISGUIDE). In this work, we designed and formally modeled a density-based spatial clustering of applications with Noise (DBSCAN)-based ADM that learns the benign time-series pattern of load measurements (i.e., load measurements perceived in the control center) to identify anomalous events. The proposed framework can identify a stealthy attack vector to optimally (i.e., in minimal time) inject malicious measurements into the load measurements perceived by the control center to trip protective relays (UF/OF), evading the ADM. MISGUIDE is equipped to deal with the intricate dynamics of SG systems. The proposed analytics utilizes the Gurobi optimizer, overcoming the limitations of satisfiability modulo theories (SMT)-based solvers in handling complex mathematical equations and logical constraints. Hence, MISGUIDE not only synthesizes sophisticated attack vectors but also ensures their verifiability. By covering a broader range of attack scenarios and intricacies in SG systems, MISGUIDE significantly enhances the robustness of cybersecurity measures in these essential infrastructures. The load data for ADM training is obtained from state-of-the-art (SOTA) GEFCom2014 load forecasting dataset [13]. Our core contributions can be summarized as follows. • We formally model the SG dynamics, including LFC, with an ML-based ADM, using first-order predicate logic by extracting constraints from the dependent component models to analyze the system. • We develop an attack analytics to identify potential attack vectors in the communication packets between the generators and LFC by formally modeling FDI attacks with different attack attributes. The SOTA attack analytics (TIFS’23) investigated and identified FDI attacks based only on generator properties. In contrast, our proposed analyzer extends the analysis to identified attack vectors, considering diverse security properties, such as examining the ADM robustness, assessing attack space with different adversarial attributes, and analyzing system resiliency. • We conduct experiments with our proposed analytics on SOTA real-world load pattern datasets of the ISO New England’s IEEE 39-bus system to identify critical attack vectors and validate the identified attacks in a hardware-in-the-loop simulator (i.e., OPALRT). All implementation and evaluation results are reproducible with the source code on GitHub [14].111Github repository: https://github.com/misguidetdsc/misguide TABLE I: Summary of the Comparative Analysis of Formal Modeling, Adversarial ML, and RL Approaches for Attack Vector Identification from ML-based ADMs Criteria Formal Analysis Adversarial ML RL MISGUIDE Verifiable threat detection ✓✓\checkmark✓ X X ✓✓\checkmark✓ Solution identification guarantee ✓✓\checkmark✓ X X ✓✓\checkmark✓ Convergence ✓✓\checkmark✓ ✓✓\checkmark✓ X ✓✓\checkmark✓ Security-aware analysis X X X ✓✓\checkmark✓ Feasible solution from large and complex time-series models X X ✓✓\checkmark✓ ✓✓\checkmark✓ The rest of the paper is organized as follows: we provide an overview of the considered SG generators and LFC dynamics in Section II. We formally describe the problem domain and our considered attack model in Section III. In the following section (i.e., Section IV), we present the technical overview of the proposed framework. We provide case studies to give insights about our proposed framework’s working principle and capabilities in Section V. Then, we show the validation of the MISGUIDE with a real-time simulator. We evaluate MISGUIDE using SOATA datasets in Section VI. A comprehensive literature review is presented in Section VII. Section VIII summarizes and discusses our research findings and limitations, and Section IX concludes the write-up."
https://arxiv.org/html/2411.04459v1,GPT-Guided Monte Carlo Tree Search for Symbolic Regression in Financial Fraud Detection,"With the increasing number of financial services available online, the rate of financial fraud has also been increasing. The traffic and transaction rates on the internet have increased considerably, leading to a need for fast decision-making. Financial institutions also have stringent regulations that often require transparency and explainability of the decision-making process. However, most state-of-the-art algorithms currently used in the industry are highly parameterized black-box models that rely on complex computations to generate a score. These algorithms are inherently slow and lack the explainability and speed of traditional rule-based learners. This work introduces SR-MCTS (Symbolic Regression MCTS), which utilizes a foundational GPT model to guide the MCTS, significantly enhancing its convergence speed and the quality of the generated expressions which are further extracted to rules. Our experiments show that SR-MCTS can detect fraud more efficiently than widely used methods in the industry while providing substantial insights into the decision-making process.","Traditionally, financial fraud detection relied on rules constructed by subject matter experts to approve or deny transactions (rule_based, ). While these methods were fast and explainable, they struggled to adapt to evolving fraud patterns. Later, machine learning techniques like Logistic Regression (log_reg, ), Support Vector Machines (svm, ), Random Forest (rf, ), and XGBoost (xgb, ) offered better adaptability but made decision-making less interpretable. Tree-based learners employed algorithms like SHAP (shap, ) for insights, but these were computationally expensive and challenging to interpret as rules. Advanced Graph Neural Network models like Graph Attention Networks (gat, ), Care-GNN (care_gnn, ), and Semi-GNN (semi_gnn, ) improved decision-making but remained black-box and costly. In the recent past, Generative Pre-trained Transformers (GPT) (gpt, ) have shown path-breaking success in different domains of machine learning. Despite of its success, GPT suffers from issues like hallucinations (hallu_gpt, ), limiting their use in critical decision-making. For real-time applications, fraud detection methods must be fast, interpretable, and accurate. Rule-based methods excel in speed and interpretability. This work introduces SR-MCTS, an algorithm that enhances rule-based methods to match state-of-the-art accuracy. We fine-tune a large language model, Symbolic-GPT (sym_gpt, ), with financial data and use it to guide Monte Carlo Tree Search (MCTS) (mcts, ). SR-MCTS generates mathematical expressions combining dataset features, operators, and constants to create rule sets for fraud detection. It also minimizes the effect of hallucinations due to limited guidance to the MCTS from Symbolic-GPT. We evaluate SR-MCTS on our proprietary dataset, showing it outperforms widely used industry methods."
https://arxiv.org/html/2411.04946v1,SPGD: Steepest Perturbed Gradient Descent Optimization,"Optimization algorithms are pivotal in advancing various scientific and industrial fields but often encounter obstacles such as trapping in local minima, saddle points, and plateaus (flat regions), which makes the convergence to reasonable or near-optimal solutions particularly challenging.This paper presents the Steepest Perturbed Gradient Descent (SPGD), a novel algorithm that innovatively combines the principles of the gradient descent method with periodic uniform perturbation sampling to effectively circumvent these impediments and lead to better solutions whenever possible. SPGD is distinctively designed to generate a set of candidate solutions and select the one exhibiting the steepest loss difference relative to the current solution. It enhances the traditional gradient descent approach by integrating a strategic exploration mechanism that significantly increases the likelihood of escaping sub-optimal local minima and navigating complex optimization landscapes effectively. Our approach not only retains the directed efficiency of gradient descent but also leverages the exploratory benefits of stochastic perturbations, thus enabling a more comprehensive search for global optima across diverse problem spaces. We demonstrate the efficacy of SPGD in solving the 3D component packing problem, an NP-hard challenge. Preliminary results show a substantial improvement over four established methods, particularly on response surfaces with complex topographies and in multidimensional non-convex continuous optimization problems. Comparative analyses with established 2D benchmark functions highlight SPGD’s superior performance, showcasing its ability to navigate complex optimization landscapes. These results emphasize SPGD’s potential as a versatile tool for a wide range of optimization problems.","Mathematical optimization is a fundamental process in engineering, science, and economics. Its main objective is to find solutions that minimize a predefined objective, typically expressed in terms of a real-valued function, while adhering to given constraints. This pursuit of optimal solutions is crucial in solving complex problems, where achieving the best possible results necessitates a careful balance of numerous factors and variables. Among the many optimization techniques available, the gradient descent (GD) method stands out as a foundational and extensively used tool, and its origins can be traced back to Cauchy’s pioneering work [1]. However, despite its widespread use, the gradient descent method has certain limitations. One of its major drawbacks is its tendency to get trapped in sub-optimal states, including saddle points and local minima, which may offer minimal improvement in solution quality. Additionally, the method may encounter difficulties in making progress towards the desired outcome when faced with flat regions in the problem space. To address these challenges, extensive research efforts have been focused on enhancing the performance of the gradient descent method. As a result, numerous variants have been developed, each specifically designed to overcome the aforementioned pitfalls [2]. One notable variant is the Perturbed Gradient Descent (PGD), which has gained attention for its ability to navigate away from saddle points and potentially converge towards second-order optimal points [3]. In this paper, we present a strategic randomized perturbation algorithm combined with the gradient descent method, leveraging the strengths of both exploring the search space through randomized perturbation and converging to optimal points using gradient information. By introducing cyclical perturbations, our approach strategically balances the need for exploration with the efficiency of exploitation. Moreover, applying perturbations periodically rather than at every iteration significantly reduces computational costs, making the optimization process more efficient without sacrificing the thoroughness of the search. It promises a more reliable pathway to discovering superior solutions, thereby expanding the horizon of possibilities in optimization challenges. This enhanced method is designed not only to navigate more effectively through the complexities of practical optimization landscapes but also to refine the search for optimal solutions with greater precision. The remainder of this paper systematically explores the Steepest Perturbed Gradient Descent (SPGD) algorithm and its comparative advantages in the domain of optimization. Section 2 delves into a variety of related methodologies, focusing on variants of the gradient descent method and the integration of perturbation sampling techniques. These approaches establish a foundation for understanding the landscape of optimization strategies and highlight the necessity for innovations, such as SPGD. Section 3 is dedicated to a detailed exposition of the SPGD algorithm itself, including its theoretical underpinnings, algorithmic structure, and the rationale behind its design choices. Following this, Section 4 presents numerical results from a series of experiments designed to evaluate the performance of SPGD against various established optimization algorithms. These experiments are conducted on a selection of well-known optimization test functions, providing a rigorous comparison and demonstrating the practical implications of SPGD in addressing complex optimization challenges. Finally, Section 5 discusses the outcomes of these comparisons, emphasizing the superior performance of SPGD over the methods analyzed. The conclusions not only underscore the effectiveness of SPGD but also set the stage for future research directions and potential applications in broader optimization contexts."
https://arxiv.org/html/2411.03929v1,Inexact block LU preconditioners for incompressible fluids with flow rate conditions,"When studying the dynamics of incompressible fluids in bounded domains the only available data often provide average flow rate conditions on portions of the domain’s boundary. In engineering applications a common practice to complete these conditions is to prescribe a Dirichlet condition by assuming a-priori a spatial profile for the velocity field. However, this strongly influence the accuracy of the numerical solution. A more mathematically sound approach is to prescribe the flow rate conditions using Lagrange multipliers, resulting in an augmented weak formulation of the Navier-Stokes problem.In this paper we start from the SIMPLE preconditioner, introduced so far for the standard Navier-Stokes equations, and we derive two preconditioners for the monolithic solution of the augmented problem. This can be useful in complex applications where splitting the computation of the velocity/pressure and Lagrange multipliers numerical solutions can be very expensive. In particular, we investigate the numerical performance of the preconditioners in both idealized and real-life scenarios. Finally, we highlight the advantages of treating flow rate conditions with a Lagrange multipliers approach instead of prescribing a Dirichlet condition.","Incompressible Navier-Stokes equations are commonly used in engineering applications to study the dynamics of viscous fluids. Given a bounded domain Ω⊂ℝ3Ωsuperscriptℝ3\Omega\subset\mathbb{R}^{3}roman_Ω ⊂ blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT, classical boundary conditions impose three (one for each spatial dimension) point-wise data on the domain’s boundary ∂ΩΩ\partial\Omega∂ roman_Ω, typically prescribing the components of the velocity field (Dirichlet condition), of the Cauchy normal stress (Neumann condition), or a combination of the two (Robin condition). On the other hand, in some applications the only information available provides average conditions which are not enough to close the problem and thus need to be completed [10]. We will refer to them as defective boundary conditions. Some examples arise in blood-dynamics simulations, where clinical measures often provide information on either the flow rate or the mean pressure [40, 45], and flow in pipes simulations, where sensors measure the fluid flow rate [7, 37]. Moreover, in the geometric multiscale approach [38], where mathematical models with different spatial dimensions are coupled, the information provided by the lower-dimensional problem is not enough to close the higher-dimensional one [6, 32]. Regarding defective flow rate conditions, a widely employed strategy to close the problem, thanks to its practicality, is to prescribe a Dirichlet condition by assuming a-priori a spatial profile for the velocity field. In this case, the computational domain is often extended to reduce the impact of the chosen profile on the accuracy of the numerical solution. A more mathematically sound approach was proposed in [27], where the authors introduced a suitable variational formulation of the problem which includes the given flow rate data. However, this approach requires the definition of non-standard finite-dimensional subspaces which makes it problematic to implement in practice [46]. Hence, several alternative approaches have been proposed in literature [23] based on Lagrange multipliers [20, 45], control theory [21, 31], penalization technique [51] and the Nitsche method [29, 47]. The Lagrange multipliers approach was proposed in [20] and applied for a quasi-Newtonian Stokes problem [16], in a fluid-structure interaction framework [22] and in practical hemodynamic problems with patient-specific geometries [48, 11]. In this approach, flow rate boundary conditions are considered as a constraint for the solution and enforced using Lagrange multipliers, resulting in an augmented weak formulation of the Navier-Stokes problem. The problem is closed by assuming that, on the considered portion of the domain’s boundary, the Cauchy normal stress has zero tangential components and its normal component is constant in space. The resulting augmented problem can be solved by splitting the computation of the velocity/pressure fields and of the Lagrange multipliers in order to resort to available standard solvers for the solution of the Navier-Stokes step [45, 46]. However, iterative procedures based on this splitting can be very expensive in complex applications. In the recent work [28], the authors proposed monolithic block preconditioners based on inexact factorizations to deal with defective conditions arising from the coupling with lumped parameter models. In order to efficiently solve the augmented Finite Elements system, in this paper we consider a monolithic strategy and we propose a suitable block preconditioner for its efficient solution. Specifically, we start from the SIMPLE iterative solution strategy [35], studied as a preconditioner in [36, 33, 14, 15], and we extend it to the augmented flow rate defective case. To test the effectiveness of our proposal, we present several numerical results both in idealized and real-life scenarios. The outline of the paper is as follows: we provide a review of the augmented Navier-Stokes flow rate problem (Section 2) and of the SIMPLE preconditioner (Section 3); we propose an extension of the SIMPLE preconditioner (Section 4), discussing its formulation (Section 4.1) and introducing an efficient variant used in practice (Section 4.2); finally we provide some numerical results (Section 5), presenting the setting of our numerical experiments which involves a trivial extension of the SIMPLE preconditioner (Section 5.1), testing the performance of the proposed preconditioners for a varying number of Lagrange multipliers (Section 5.2) and in real-life hemodynamic scenarios (Section 5.3), and highlighting the advantages of treating defective flow rate condition with a Lagrange multipliers approach (Section 5.4)."
https://arxiv.org/html/2411.03671v1,Energy-based physics-informed neural network for frictionless contact problems under large deformation,"Numerical methods for contact mechanics are of great importance in engineering applications, enabling the prediction and analysis of complex surface interactions under various conditions. In this work, we propose an energy-based physics-informed neural network (PINN) framework for solving frictionless contact problems under large deformation. Inspired by microscopic Lennard-Jones potential, a surface contact energy is used to describe the contact phenomena. To ensure the robustness of the proposed PINN framework, relaxation, gradual loading and output scaling techniques are introduced. In the numerical examples, the well-known Hertz contact benchmark problem is conducted, demonstrating the effectiveness and robustness of the proposed PINN framework. Moreover, challenging contact problems with the consideration of geometrical and material nonlinearities are tested. It has been shown that the proposed PINN framework provides a reliable and powerful tool for nonlinear contact mechanics. More importantly, the proposed PINN framework exhibits competitive computational efficiency to the commercial FEM software when dealing with those complex contact problems. The codes used in this manuscript are available at https://github.com/JinshuaiBai/energy_PINN_Contact.(The code will be available after acceptance)","Contact and interaction between objects exist widely in nature and industrial production. It is of great importance to accurately simulate the deformation of bodies during contact. The contact problem is considered as a nonlinear problem in solid mechanics [1]. It is challenging due to its complexities, including the constantly changing boundary conditions, surface representation and contact pressure singularity [2]. Many numerical techniques have been developed, for example, the Lagrange multiplier method, the augmented Lagrangian formulation, barrier techniques and the third medium treatment [1, 3, 4, 5]. Additionally, great efforts have been made to improve the smoothness of the contact surface. Typical examples include the non-uniform rational B-splines (NURBS) based isogeometry analysis [6, 7, 8, 9, 10] and the mortar methods [11, 12]. In recent years, a novel kind of deep learning (DL) framework [13, 14], namely the physics-informed neural networks (PINNs), has merged as a powerful and promising tool for solving partial differential equations [15, 16, 17, 18], and therefore attracted great attention in the computational mechanics [19, 20, 21, 22]. Great numbers of PINN-based computational solid mechanics frameworks have been proposed [23, 24, 25, 26]. Among those frameworks, neural networks regulated by the energy form loss provide the most robust results with favourable efficiency. Those energy-based PINN frameworks are also known as the deep energy method (DEM) [25]. Great performances of those PINN frameworks have been witnessed in a variety of applications, including linear elasticity [27, 25], hyperelasticity [28], elastoplasticity [29, 30], fracture [31, 32, 33], large deformation problems [34], and topology optimisation [35, 36, 37, 38]. Besides, traditional PDE solving techniques have been introduced to the energy-based PINN frameworks, ending up with establishing novel methods for solid mechanics challenges. For example, complementary energy form and boundary integration form lead to novel deep complementary energy method [39] and boundary-informed neural network frameworks [40], respectively. Novel neural network structures are also implemented. Bai et al. [34] applied radial basis networks to deal with problems involving both material and geometrical nonlinearity. It has been demonstrated that such a framework can easily capture instability phenomena and is locking-free when modelling nearly incompressible materials. Wang et al. [41] implement the Kolmogorov Arnold network (KAN) with physics knowledge. By doing so, the KAN shows great performances for multi-scale and material discontinuity problems. Despite its great success in computational mechanics, only limited exploration regarding PINNs has been conducted on contact problems. Sahin et al. [42] proposed a PINN framework under the strong form formulation with the classical Karush–Kuhn–Tucker constraint to solve the forward and inverse contact problems. Efforts have also been paid to interfacial discontinuities induced by the use of multi-material [43]. However, in those existing and limited literature, either merely small deformation problems are discussed or the contact surfaces remain unchanged throughout the training, e.g., neither new contact area generated nor sliding between contact surfaces is considered. It has been demonstrated that the PINN framework is very effective for material nonlinearities and large deformation problems [28, 34]. Besides, the energy-based PINNs are computationally more efficient and robust than the vanilla PINNs [26]. Therefore, it is of great interest to implement the energy-based PINNs for solving contact problems. Moreover, the effectiveness and performances of the energy-based PINNs for contact problems with complex nonlinearities also worth investigating. To address this research gap, in this work, we propose an energy-based PINN framework for contact problems with both material and geometrical nonlinearities. The contact behaviour is described by a contact potential and together optimised by gradient descendant algorithms. Moreover, additional numerical treatments, including the relaxation, gradual loading and output scaling are also introduced to improve the robustness of the proposed framework. The Hertz contact benchmark problems under small deformation assumption are examined to show the performances of the proposed framework. More importantly, various contact examples considering both the material and the geometrical nonlinearities are tested, including the rubber ironing, the rubber ring contact instability and the compression of two rings. The good results demonstrate the effectiveness and great potential of the proposed PINN framework for complex nonlinear mechanics. Notably, the proposed framework is very straightforward to implement numerically and can easily incorporate experimental data. Additionally, the PINN framework demonstrates competitive computational efficiency compared to commercial FEM software when addressing complex contact problems. The remainder of the paper is organised as follows: In Section 2, the basic conceptions of energy-based PINN for solid mechanics are briefly introduced. In Section 3, the frictionless contact achieved by using the surface potential and its implementation based on the PINN framework are proposed. The Hertz contact benchmark problem is also examined and discussed in detail. In Section 4, numerical examples involving large deformation and material nonlinearities are presented, including the rubber ironing, the rubber ring contact instability and the compression of two rings. In Section 5, the conclusions of this work are summarised."
https://arxiv.org/html/2411.03318v1,Fusion-basedConstitutivemodel (FuCe): Towards model-data augmentation in constitutive modelling,"Constitutive modeling is crucial for engineering design and simulations to accurately describe material behavior. However, traditional phenomenological models often struggle to capture the complexities of real materials under varying stress conditions due to their fixed forms and limited parameters. While recent advances in deep learning have addressed some limitations of classical models, purely data-driven methods tend to require large datasets, lack interpretability, and struggle to generalize beyond their training data. To tackle these issues, we introduce ""Fusion-based Constitutive model (FuCe): Towards model-data augmentation in constitutive modelling"". This approach combines established phenomenological models with an ICNN architecture, designed to train on the limited and noisy force-displacement data typically available in practical applications. The hybrid model inherently adheres to necessary constitutive conditions. During inference, Monte Carlo dropout is employed to generate Bayesian predictions, providing mean values and confidence intervals that quantify uncertainty. We demonstrate the model’s effectiveness by learning two isotropic constitutive models and one anisotropic model with a single fiber direction, across six different stress states. The framework’s applicability is also showcased in finite element simulations across three geometries of varying complexities. Our results highlight the framework’s superior extrapolation capabilities, even when trained on limited and noisy data, delivering accurate and physically meaningful predictions across all numerical examples.","In simulations of physical systems, constitutive modeling is essential for characterizing the mechanical behavior of materials. The accuracy of these simulations heavily depends on choosing the right constitutive model [1]. While some models are grounded in physics, most are primarily phenomenological. These models work well for small deformations in materials with linear behavior. However, when dealing with large deformations in materials like rubbers, polymers, foams, and biological tissues –which exhibit nonlinear and complex behaviors, developing accurate empirical or phenomenological models becomes significantly more difficult. Even with careful formulation and calibration, these models are limited by their fixed forms and the small number of parameters, restricting their ability to accurately represent a wide range of deformation levels and stress states. Machine learning based data-driven approach [2] including methods like Gaussian process regression [3, 4] or neural networks (NNs) [5, 6] presents a reasonable alternatives to overcome the time consuming task of formulating classical constitutive models and to improve the limited expressibility associated with these phenomenological models. The concept of using NNs in constitutive modeling was first presented in early 1990s [7]. The fundamental concept of machine-learning-based material modeling involves choosing a highly expressive model approach, characterized by a large number of learnable parameters, and then tuning these parameters using the available data. However, in this early phase, black-box approaches were mostly used, i.e., networks that do not take into account any physical principles and therefore only performs satisfactorily in the training regime. However, these models offer poorly generalization beyond the training data regime. Moreover, data-driven constitutive modeling requires a large number of strain-stress pairs. This presents two significant challenges, especially when relying on experimental data. First, it is impractical to explore the entire high-dimensional strain-stress space using standard mechanical tests, such as uni-/bi-axial tensile or bending tests. Second, complete tensorial constitutive models are difficult to develop because stress tensors cannot be directly measured; force measurements are only boundary-averaged projections of stress tensors. While multiscale simulations can generate extensive and detailed stress-strain data [8, 9], the computational cost for complex material systems is currently prohibitive. Therefore, it is crucial to learn material behavior directly from the force-displacement data that is practically available through mechanical testing. Modern material model calibration primarily relies on full-field displacement data, such as that obtained via digital image correlation (DIC), combined with applied force data from load cells [10]. This calibration is typically performed using the finite element model updating (FEMU) [11] or virtual fields method (VFM) [12]. VFM directly solves for unknown material parameters by satisfying the weak form of momentum balance with measured displacement data, while FEMU iteratively optimizes model parameters until the simulated and measured displacement fields match. Recent efforts have demonstrated the estimation of stress fields using full-field displacement data in a model-free data-driven context [13, 14, 15]. In the model-based domain, physics-informed neural networks (PINNs) and their variants have shown promising results by first learning the forward solution to the mechanical boundary value problem and then estimating unknown parameters via gradient-based optimization [16, 17, 18, 19]. However, all these techniques, including FEMU, VFM, and PINNs, are limited to predefined constitutive models with a few unknown parameters. Such models are specific to particular materials and stress states and cannot be generalized beyond the calibration data. The limitations of the previously described frameworks can be mitigated by adopting a hybrid framework through data-model fusion [20]. These models integrate phenomenological and data-driven approaches. Various hybrid strategies exist for combining data-driven models with physics-based models to solve partial differential equations (PDEs). For instance, Chakraborty [21] introduced a transfer learning-based multi-fidelity physics-informed deep learning framework, which demonstrated efficiency but lacked interpretability. Our recent works [22, 23], also proposed an alternative framework that augments known physics with deep learning models for solving partial differential equations (PDEs) and stochastic differential equations (SDEs). These hybrid frameworks are found to be accurate, more interpretable and generalizes well to unseen environments. In this research work, our objective is to develop a hybrid framework for constitutive modeling utilising existing phenomenological models along with the deep learning frameworks, using only the global reaction force and full-field displacement data. This approach is crucial due to the difficulty in availing stress strain data, practical problem of data scarcity, the difficulty in formulating accurate phenomenological models, and the critical need for generalization when a material’s behavior changes under varying external conditions. The salient features of the proposed FuCe approach are as follows: • Model-data fusion: The proposed framework utilizes classical phenomenological constitutive models alongside deep learning models, which augment and correct the phenomenological models to allign with real material data. • Satisfies all necessary constitutive conditions: By employing an ICNN neural network with appropriate correction terms, the model satisfies all thermodynamic and physical feasibility conditions. • Utilizes practically feasible force-displacement noisy data: The framework uses practically available full-field displacement and global reaction force data for training purposes, accounting for noise in the data. • Physics-informed loss: The training process is guided by a physics-informed loss function, compensating for the lack of direct stress/strain energy data. • Approximate Bayesian inference: To quantify uncertainties in model predictions [24], a Bayesian approximation method called Monte Carlo Dropout [25] is used during the inference stage. Overall, the proposed framework is a novel hybrid model of its kind, that utilises deep learning models to learn the discrepancy between the known phenomenological model and the actual material behaviour utilizing the practically available full-field displacement and global reaction force data. The remainder of the paper is organized as follows. The background of constitutive models specifically hyperelasticity is formally described in Section 2. Detailed description on the problem statement is also presented at the end of Section 2. The proposed approach along with the algorithm is described in Section 3. Results and discussion of the numerical experiments that are carried out, are presented in Section 4. Finally, Section 5 presents the concluding remarks."
https://arxiv.org/html/2411.03900v1,"Retentive Neural Quantum States: Efficient Ansätzefor
Ab Initio Quantum Chemistry","Neural-network quantum states (NQS) has emerged as a powerful application of quantum-inspired deep learning for variational Monte Carlo methods, offering a competitive alternative to existing techniques for identifying ground states of quantum problems. A significant advancement toward improving the practical scalability of NQS has been the incorporation of autoregressive models, most recently transformers, as variational ansatze. Transformers learn sequence information with greater expressiveness than recurrent models, but at the cost of increased time complexity with respect to sequence length. We explore the use of the retentive network (RetNet), a recurrent alternative to transformers, as an ansatz for solving electronic ground state problems in ab initio quantum chemistry. Unlike transformers, RetNets overcome this time complexity bottleneck by processing data in parallel during training, and recurrently during inference. We give a simple computational cost estimate of the RetNet and directly compare it with similar estimates for transformers, establishing a clear threshold ratio of problem-to-model size past which the RetNet’s time complexity outperforms that of the transformer. Though this efficiency can comes at the expense of decreased expressiveness relative to the transformer, we overcome this gap through training strategies that leverage the autoregressive structure of the model—namely, variational neural annealing. Our findings support the RetNet as a means of improving the time complexity of NQS without sacrificing accuracy. We provide further evidence that the ablative improvements of neural annealing extend beyond the RetNet architecture, suggesting it would serve as an effective general training strategy for autoregressive NQS.","Variational Monte Carlo (VMC) with neural network quantum states (NQS) is a generative deep learning framework [4] that utilizes a classical neural network to efficiently encode the state vector of a quantum many-body system. This neural network ansatz learns a state vector approximating the ground state eigenvector of a given quantum Hamiltonian H𝐻Hitalic_H, via a VMC training algorithm analogous to policy gradient methods in reinforcement learning. More specifically, for state vectors formulated in the Hilbert space ℂ2nsuperscriptℂsuperscript2𝑛\mathbb{C}^{2^{n}}blackboard_C start_POSTSUPERSCRIPT 2 start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT, binary strings of length n𝑛nitalic_n are sampled from the Born probability distribution associated with the approximate state vector. These samples are used to efficiently estimate both the expectation value of H𝐻Hitalic_H and the associated network parameter gradients. Thus NQS bypasses the need to explicitly store the full ansatz state vector or Hamiltonian matrix, whose dimensions grow exponentially with respect to n𝑛nitalic_n. So long as H𝐻Hitalic_H is sufficiently row sparse with easily retrievable entries—a loose restriction for Hamiltonians relevant to many practical applications—the entire algorithm will execute in polynomial time with respect to n𝑛nitalic_n. Direct conceptual relationships have been drawn between NQS, as a high-level numerical black box solver, and both natural evolution strategies [34] and variational quantum eigensolvers (VQEs). In latter case, NQS may be viewed not only as an analogous method but also as a direct pipeline for de-quantization of existing VQEs [15], which rely on parameterized quantum circuits to prepare physical trial wavefunctions. As NQS is fundamentally problem-agnostic, a property it shares with VQEs, it is unsurprising that NQS has been successfully demonstrated on a variety of problems: Ising models [4], combinatorial optimization [9, 34], and high-dimensional linear algebra [15]. While some of the existing demonstrations of these use cases are proofs of concept, they all depict a significant greater potential for NQS to perform accurately and efficiently on practically-motivated problems. An exciting area of application of significant recent interest is to use NQS for ab initio quantum chemistry, specifically to solve electronic structure problems [6, 1, 36, 16, 33]. In this application, the second quantized electronic structure Hamiltonian corresponding to some molecule, which encodes the energy of the joint wavefunction describing all of that molecule’s electrons, is mapped to a qubit-based Hamiltonian that can be targeted by NQS. This line of research parallels similar recent efforts to address these problems using VQEs [5, 21, 30], and has positioned NQS as a potentially viable alternative to more established methods like coupled cluster [2] and tensor network methods like DMRG [24]. The first applications of NQS to electronic structure calculations utilized restricted Boltzmann machines—which model unnormalized wave functions that necessitate approximate Monte Carlo sampling methods—as ansatz networks [6]. Although there is promising research being conducted with unnormalized wave functions [16], much of NQS research in this area now relies on autoregressive wavefunction ansatze [1], which are normalized by construction and furthermore allow for exact sampling from their Born probability distribution. This paradigm has resulted in scalable quantum chemistry solvers, performing with accuracies comparable to CCSD, using the masked feedforward network MADE [36], and—more recently—transformers [33]. Transformers constitute a compelling alternative to MADE since they are more parameter-efficient and do not explicitly grow in size with respect to the number of qubits n𝑛nitalic_n, but they do possess one inherent computational drawback: the cost of performing a forward pass grows quadratically with the qubit number both during training and inference. When employed in large language model (LLM) applications, it is understood that this quadratic scaling does not contribute significantly to the compute requirements for training transformer-based models, as reflected in standard floating point operation (FLOP) count estimates [13]. For NQS, however, each optimization step necessitates multiple forward passes of the ansatz in order to generate Monte Carlo samples, so quadratic cost poses a nonneglible computational bottleneck at scale. In this work, we introduce a novel NQS ansatz based on retentive networks (RetNets) [28], a recurrent model designed as an alternative to transformers for LLM applications. Unlike transformers, RetNets process inputs in parallel during training, and recurrently during inference, allowing for a linear inference cost with respect to n𝑛nitalic_n. Since RetNets can alleviate the inference cost of transformers and have been shown to exhibit baseline performance comparable to transformers for natural language processing tasks, they serve as a promising candidate for NQS-based electronic structure calculations. Contributions. This paper aims to expose this potential as follows: section 3 provides the relevant background information and context for this work: the application of autoregressive NQS to ab initio quantum chemistry, the principles of variational neural annealing, the NQS-Transformer ansatz, and the RetNet architecture. We also extend existing transformer FLOP count estimates [13] to RetNets, demonstrating how the latter can provide substantial FLOP savings under reasonable model constraints. In section 4, we describe the primary contributions of this work: • We present the RetNet as a viable drop-in replacement to the transformer in NQS, illustrating how the structure and demands of NQS make it amenable to such a replacement. • We demonstrate how variational neural annealing [12] increases the robustness of NQS performance with respect to choice of training hyperparameters. Section 5 details experimental results that demonstrate the efficacy of RetNets as NQS ansatze on a slate of fundamental baseline molecules. We also experimentally demonstrate the beneficial effects of variational neural annealing on the training of NQS ansatze, by showing how annealing can improve the accuracy of NQS for these molecules, using model sizes that are smaller than what is previously reported in the literature. When shown alongside more incremental architectural changes intended to reduce the computational cost of NQS, these results indicate several promising avenues for improving the practical operation of NQS at scale. We give a conclusive summary of this work, alongside some discussion of potential future directions for it, in section 6."
https://arxiv.org/html/2411.03557v1,Shem: A Hardware-Aware Optimization Framework for Analog Computing Systems,"As the demand for efficient data processing escalates, reconfigurable analog hardware which implements novel analog compute paradigms, is promising for energy-efficient computing at the sensing and actuation boundaries. These analog computing platforms embed information in physical properties and then use the physics of materials, devices, and circuits to perform computation. These hardware platforms are more sensitive to nonidealities, such as noise and fabrication variations, than their digital counterparts and accrue high resource costs when programmable elements are introduced. Identifying resource-efficient analog system designs that mitigate these nonidealities is done manually today.While design optimization frameworks have been enormously successful in other fields, such as photonics, they typically either target linear dynamical systems that have closed-form solutions or target a specific differential equation system and then derive the solution through hand analysis. In both cases, time-domain simulation is no longer needed to predict hardware behavior. In contrast, described analog hardware platforms have nonlinear time-evolving dynamics that vary substantially from design to design, lack closed-form solutions, and require the optimizer to consider time explicitly. We present Shem, an optimization framework for analog systems. Shem leverages differentiation methods recently popularized to train neural ODEs to enable the optimization of analog systems that exhibit nonlinear dynamics, noise and mismatch, and discrete behavior. We evaluate Shem on oscillator-based pattern recognizer, CNN edge detector, and transmission-line security primitive design case studies and demonstrate it can improve designs. To our knowledge, the latter two design problems have not been optimized with automated methods before.","There has been an emergence of new workloads that place extreme power constraints on the hardware and require processing near the sensing and actuation interfaces (lequepeys2021overcoming, ; outeiral2021prospects, ; bayerstadler2021industry, ; irimia2012green, ). Analog computing systems are a promising class of hardware that can perform processing directly on analog signals domain and often at very low energy, enabling processing large amounts of analog data with little digitization (Decadal, ; murmann2020a2i, ). Analog computing systems encode information in physical signals (e.g., voltage) and then leverage the dynamics of materials, devices, and circuits to perform computation. Modern analog computing systems are reconfigurable and implement novel analog computational paradigms, such as oscillator-based computing and cellular nonlinear networks, which are inherently nonlinear and often use non-standard physical properties (ryynanen2001dual, ; gangopadhyay2014compressed, ; mehonic2020memristors, ; konatham2020real, ; sebastian2020memory, ). Ordinary differential equations capture the semantics of the analog compute paradigm and executing computations involves solving or simulating the differential equation system. Design Challenges. Identifying a resource-efficient, system-level design for this class of analog hardware that delivers acceptable fidelity remains a significant challenge and is primarily performed manually. First, analog hardware is sensitive to nonidealities that affect the fidelity of the computation, such as noise, fabrication-induced parameter variations, and environmental sensitivities. Second, the digital interface circuitry used to program the hardware and perform digital/analog conversion increases resource usage, especially with increasing precision (cowan2005vlsi, ; huang2017hybrid, ; tsividis2018analog-computer, ; guo2016hybrid-computer, ; achour2020Legno, ). Reducing the precision of these digital elements and the degree of programmability significantly reduces the complexity of the design. Enabling automated optimization of these analog systems would enable identification of useful design. 1.1. Existing Design Optimization Methods Design optimization tools find design parameterizations that minimize some cost function, which captures the end-to-end system-level property the designer desires. Gradient-based optimization methods have been extensively used in photonics (hughes2018adjoint, ; su2020nanophotonic, ; molesky2018inverse, ; li2022physics, ; li2023lightridge, ; molesky2018inverse, ). Because these optimizers use gradient information to drive the search intelligently and are built on heavily accelerated ML frameworks, designs of tens or even hundreds of thousands of design variables can be targeted (kang2024large, ; piggott2020inverse, ). Gradient-based optimizers have also been used to optimize analog circuits to a lesser degree; these methods optimize SPICE circuits to minimize specific circuit metrics (e.g., delay) and typically leverage optimization techniques that specifically work with circuit schematics (rohrer1967fully, ; director1969generalized, ; conn1998jiffytune, ; conn1999gradient, ; visweswariah2000noise, ; joshi2017analog, ; hu2020adjoint, ; li2023circuit, ). Limitations. The gradient-based methods devised in these prior works are insufficient for optimizing analog computing systems. To be able to use these methods, the gradient of the cost function must be taken. Previously developed optimizers primarily target linear dynamical systems that have closed-form solutions or target a specific differential equation system that has a hand-derived solution. In both cases, time-domain simulation is no longer needed to predict hardware behavior, and the gradient is relatively straightforward to compute. In contrast, this class of analog systems has nonlinear dynamics that rarely have analytic solutions, so gradient needs to be taken over the time domain simulation of the system. Analog systems also experience nonidealities, such as fabrication-induced errors and noise that introduce stochasticity into the system’s dynamics. This stochastic behavior is not inherently differentiable and interacts with the system’s nonlinearities, producing complex behaviors. In addition, digital logic that exists at the programming and measurement interfaces of the analog system is inherently discrete and, therefore, also not readily differentiable. These behaviors make the system even more challenging to differentiate. 1.2. Optimization of Analog Systems with Shem We present Shem, an optimization framework for analog systems that directly optimizes over time-domain differential equation models with nonlinear dynamics. Shem leverages the adjoint method, a differentiation method recently popularized to train neural ODEs, to directly differentiate over ordinary differential equation simulations and find the gradient of the cost function (kidger2022neural, ; chen2018neural, ; li2020scalable, ). Shem deploys a translation pass that makes noise, mismatch, and digital logic differentiable so the gradient may be taken. We build Shem on the JAX machine learning framework, which supports auto-differentiation and offers highly optimized backends for auto-parallelization and hardware acceleration, enabling scalable design optimization (jax2018github, ). Shem inherits the programming conveniences offered by JAX, enabling Shem to target a range of analog system designs and optimize complex cost functions that evaluate system-level properties such as end-to-end error. 1.2.1. Contributions • We introduce techniques that enable time-domain design optimization of analog systems and differentiation over noise, fabrication variations, and digital logic. • We present Shem, a framework built on JAX that supports auto-differentiation and optimization of nonlinear time-domain analog systems. • We evaluate Shem on oscillator-based pattern recognizer, cellular nonlinear network edge detector, and transmission-line security primitive design case studies and demonstrate it can improve designs. To our knowledge, the latter two design problems have not been optimized with automated methods before."
https://arxiv.org/html/2411.03530v1,Improving precision of A/B experiments using trigger intensity,"In industry, online randomized controlled experiment (a.k.a A/B experiment) is a standard approach to measure the impact of a causal change. These experiments have small treatment effect to reduce the potential blast radius. As a result, these experiments often lack statistical significance due to low signal-to-noise ratio. To improve the precision (or reduce standard error), we introduce the idea of trigger observations where the output of the treatment and the control model are different. We show that the evaluation with full information about trigger observations (full knowledge) improves the precision in comparison to a baseline method. However, detecting all such trigger observations is a costly affair, hence we propose a sampling based evaluation method (partial knowledge) to reduce the cost. The randomness of sampling introduces bias in the estimated outcome. We theoretically analyze this bias and show that the bias is inversely proportional to the number of observations used for sampling. We also compare the proposed evaluation methods using simulation and empirical data. In simulation, evaluation with full knowledge reduces the standard error as much as 85%. In empirical setup, evaluation with partial knowledge reduces the standard error by 36.48%.","Ranking and selection is a widely used solution for many industrial problems like product recommendation, search, and offer selection. In general, there is a control model already deployed in production. A new treatment model is proposed that contains some changes. In industry, online randomized controlled experiment, known as A/B experiment, (power_of_exp, ; bakshy2014designingdeployingonlinefield, ; xu_infra_to_culture_ab_test, ; kaufman2017democratizingonlinecontrolledexperiments, ; kohavi_practical_guide, ; diane_overlapping_exp, ; kohavi_trustworthy, ; xie_improving_sensitivity, ) is conducted to evaluate the performance of the treatment model. The main challenge in conducting A/B experiment in industrial setup is the low signal-to-noise ratio (xu_infra_to_culture_ab_test, ; Smith2019, ) as changes made in the treatment model are incremental in nature that impacts only a small number of observations. Hence, these experiments have higher standard error (lower precision) and lacks statistical significance. Most of the time, there are missing opportunities to roll out good treatments to production which would improve customer experience and overall revenue. In this paper, we propose evaluation methods concentrating on observations actually impacted by the experiment. Such observations are known as trigger observations. In a trigger observation, the output of the control and the treatment model are different. Therefore, it is reasonable to assume that the treatment effect is restricted only to these trigger observations. First evaluation method has full information (full knowledge) about trigger observations. We theoretically illustrate the proposed evaluation method significantly improves the precision of the evaluations results in comparison to a baseline method that disregards any information about trigger observations. This result is confirmed by the extensive simulation studies. However, detecting all trigger observations for an experiment is expensive as there can be billions of observations (amzn_traffic_stat_1, ), (amzn_traffic_stat_2, ) per day when conducting an A/B experiment. As an alternative, we propose a more practical sampling based approach (partial knowledge) where we sample a subset of observations and determine their trigger status. Obviously, this sampling based approach introduces bias in the evaluation outcome. But our theoretical analysis shows that this bias reduces linearly as the number of samples increases. Thus, we believe it is a promising approach. The theoretical findings are confirmed by simulated data. We also use empirical data collected from an A/B testing platform to validate our claims. In the empirical analysis, evaluation with partial knowledge decreases the standard error of the evaluation outcome by 36.48% without any detectable bias in the estimated treatment effect. This paper has the following major contributions: 1) To the best of our knowledge, this is the first work that introduces the idea of trigger observations for A/B experiment evaluation. 2) We propose two evaluation methods that utilize information about trigger observations. The performance of these two evaluation methods are analyzed theoretically. 3) Our theoretical analysis is further validated by the simulation and empirical data collected from a real A/B experiment platform."
https://arxiv.org/html/2411.03327v1,"Maximal Extractable Value in Decentralized Finance: Taxonomy, Detection, and Mitigation","Decentralized Finance (DeFi) leverages blockchain-enabled smart contracts to deliver automated and trustless financial services without the need for intermediaries. However, the public visibility of financial transactions on the blockchain can be exploited, as participants can reorder, insert, or remove transactions to extract value, often at the expense of others. This extracted value is known as the Maximal Extractable Value (MEV). MEV causes financial losses and consensus instability, disrupting the security, efficiency, and decentralization goals of the DeFi ecosystem. Therefore, it is crucial to analyze, detect, and mitigate MEV to safeguard DeFi. Our comprehensive survey offers a holistic view of the MEV landscape in the DeFi ecosystem. We present an in-depth understanding of MEV through a novel taxonomy of MEV transactions supported by real transaction examples. We perform a critical comparative analysis of various MEV detection approaches, evaluating their effectiveness in identifying different transaction types. Furthermore, we assess different categories of MEV mitigation strategies and discuss their limitations. We identify the challenges of current mitigation and detection approaches and discuss potential solutions. This survey provides valuable insights for researchers, developers, stakeholders, and policymakers, helping to curb and democratize MEV for a more secure and efficient DeFi ecosystem.","Decentralized Finance (DeFi) [1], powered by blockchain technology, offers financial services, such as investing, lending loans, and trading assets, to various stakeholders without the need for intermediary brokers. This creates a decentralized, secure, transparent, and traceable financial ecosystem. According to a Skyquest report, the DeFi market size is expected to reach 48.02 billion USD by 2031, up from 23.99 billion USD in 2023, a compound annual growth rate of 9.06%111https://www.skyquestt.com/report/decentralized-finance-market, accessed on 19 September 2024. Furthermore, as of September 2022, the total value locked in the DeFi ecosystem exceeded 82 billion USD222https://defillama.com/, accessed on 19 September 2024, with the Ethereum network accounting more than 45 billion USD (approximately 56%). Decentralized Exchanges (DEXes) are one of the prominent applications of DeFi, enabling users to directly swap tokens using smart contracts [2]. Most DEXes operate on the Ethereum blockchain to facilitate trustless and automated transactions. However, in the Ethereum network, transactions are stored in a public mempool before they are included in a block. This creates profitable opportunities for network participants, referred to as searchers, who can submit new transactions by observing pending financial transactions to gain additional revenue. These searchers maximize their profits by manipulating gas prices to influence the order of their transactions within the block. This additional value extracted from the blockchain network is termed Maximal Extractable Value (MEV). MEV searchers can be the block producers (i.e., miners in Proof of Work (PoW) and validators in Proof of Stake (PoS)), other network participants, or a bot. However, block producers have a unique advantage, as they can include transactions in blocks without paying high gas prices [3]. Furthermore, the transition from PoW to PoS in Ethereum has significantly reduced block producer rewards, further luring block producers to engage in MEV activities [4]. MEV could result in major financial losses, network congestion, increased gas prices, and blockchain inefficiency [3, 5]. Before Ethereum’s transition to PoS in September 2022, around 440,000 ETH in MEV was extracted. Since the transition, approximately 180,000 ETH has been extracted up until May 2023333https://milkroad.com/guide/mev/, accessed on 19 September 2024. However, not all MEV transactions are detrimental to the DeFi ecosystem. Some can destabilize consensus mechanisms and cause economic harm, while others may even stabilize financial markets [6]. Therefore, understanding, detecting, and mitigating MEV is critical to ensure the efficiency and stability of the DeFi ecosystem. Several surveys have explored various aspects of MEV, including transaction types and mitigation strategies [7, 8, 6, 5, 9]. In particular, [7] classifies MEV transactions, while [8, 6] emphasize on mitigation techniques. In contrast, [5, 9] cover both transaction types and mitigation strategies. However, none have thoroughly examined all transaction types, detection approaches, and mitigation strategies. This comprehensive survey addresses this gap by presenting a novel taxonomy of MEV transaction types supported by real-world examples from the Ethereum network. It also explores MEV detection approaches and mitigation strategies and examines various simulation and extraction methods. The main contributions of this survey are as follows. • We introduce a novel and comprehensive taxonomy of MEV transactions, supported by real Ethereum transaction examples for each identified MEV type. The taxonomy clearly distinguishes between value-diverting MEV transactions, which can lead to financial loss and network instability, and value-creating MEV transactions, which can stabilize markets or enhance efficiency. This distinction provides clarity on the dual nature of MEV activities and lays the foundation for discussions on mitigation and detection strategies. • We present a critical comparative analysis of the various MEV detection approaches proposed for different types of MEV transactions, highlighting their effectiveness in identifying these transaction types. • We provide an in-depth analysis of various MEV mitigation strategies, such as transaction ordering solutions, privacy-preserving public pools, and private pools. By identifying the specific limitations of these strategies, we offer a critical assessment of their real-world applicability and potential areas for improvement. • We explore existing MEV simulation frameworks and extraction methods, offering a comprehensive overview of how these tools model and replicate real-world MEV scenarios. • We critically examine the major challenges in MEV mitigation and detection, such as centralization, latency, layer-2 MEV, and multi-address MEV. Furthermore, we discuss potential solutions paving the way for a more secure and efficient DeFi system. The remainder of this article is organized as follows. Section 2 provides essential background on Ethereum, DEXes, and MEV, enabling readers to gain a clearer understanding of the key concepts. Section 3 reviews existing related surveys on MEV, highlighting gaps in the literature. In Section 4, we describe the methodology used to conduct this survey. Section 5 introduces and synthesizes the proposed taxonomy of MEV transactions. Sections 6 and 7 focus on MEV detection approaches and mitigation strategies, respectively. Section 8 explores MEV simulation and extraction methods. Section 9 addresses the challenges associated with MEV mitigation and detection, along with potential solutions. Finally, Section 10 concludes the survey."
https://arxiv.org/html/2411.02424v1,Set-based queries for multiscale shape-material modeling,"Multiscale structures are becoming increasingly prevalent in the field of mechanical design. The variety of fine-scale structures and their respective representations results in an interoperability challenge. To address this, a query-based API was recently proposed which allows different representations to be combined across the scales for multiscale structures modeling. The query-based approach is fully parallelizable and has a low memory footprint; however, this architecture requires repeated evaluation of the fine-scale structures locally for each individual query. While this overhead is manageable for simpler fine-scale structures such as parametric lattice structures, it is problematic for structures requiring non-trivial computations, such as Voronoi foam structures.In this paper, we develop a set-based query that retains the compatibility and usability of the point-based query while leveraging locality between multiple point-based queries to provide a significant speedup and further decrease the memory consumption for common applications, including visualization and slicing for manufacturing planning. We first define the general set-based query that consolidates multiple point-based queries at arbitrary locations. We then implement specialized preprocessing methods for different types of fine-scale structures which are otherwise inefficient with the point-based query. Finally, we apply the set-based query to downstream applications such as ray-casting and slicing, increasing their performance by an order of magnitude. The overall improvements result in the generation and rendering of complex fine-scale structures such as Voronoi foams at interactive frame rates on the CPU.","1.1 Motivation The application of multiscale structures to mechanical design is becoming increasingly accessible due to the rapid advancement of modern manufacturing techniques. A multitude of methods, algorithms, and tools have emerged to support the design and modeling of such structures [1, 2, 3, 4, 5]. Periodic lattice and foam-based structures have been applied to architect materials meeting specific material property criteria as needed by the end-user. Commercial tools, such as Autodesk Within and nTopology, have created lightweight infills with these structures. These approaches have specific strengths and weaknesses for addressing specific modeling needs; however, the proliferation of many such methods with specific uses has led to interoperability issues. A multiscale API based on shape and material queries [6] has been proposed as a possible solution for allowing the integration of different representations across the scales. The architecture allows for rapidly exchanging different fine-scale models, provides a common interface to external tools and downstream applications, and enables easy chaining and linking of multiple scales to create multi-scale structures. In this approach, each coarse- or fine-scale representation is fully encapsulated and only needs to provide methods for a small set of point-based queries for shape or material properties at a given location. Downstream applications, such as visualization or slicing for manufacturing planning, are built upon these shape and material queries, which are both fully parallelizable and require a low memory footprint. However, this point-based system architecture requires the repeated and redundant evaluation of local structure within each scale. This computational overhead is manageable when only a little computation is required to generate the fine-scale structure, such as parametrically defined unit cell lattice structures [7]. However, it is detrimental to fine-scale structures that require intensive computation, such as sample-based heterogeneous material modeling [8] and Voronoi foam structures [9, 10]. This repeated evaluation also fundamentally precludes acceleration techniques which could be efficient for groups of points, but are slower on individual queries. 1.2 Contributions and outline We propose a novel set-based query which groups and batch-processes point queries together based on their spatial positions, taking advantage of the spatial locality of the fine-scale structure generation algorithm to accelerate these queries as a whole. This grouping also allows further preprocessing computations which can accelerate the processing of each group but would otherwise be excessive for single-point queries. The proposed approach differs from simply caching information within a single query, as we not only allow the use of preprocessing which is more efficient on groups of points, but we also sort and re-order computation on the points in order to maximize efficiency. Our approach by itself does not cache or store information between queries, but it is fully compatible with such approaches; we show that in some use cases this can further improve performance. The concept of set-based queries applies to different fine-scale structures. However, the benefit and performance gain differs depending on the characteristics of the structure. We categorize existing approaches to model fine-scale structures into four quadrants based on the computational resource needed and the geometric similarity of the structure across different neighborhoods. Specifically, we implement two specialized sorting algorithms which are efficient for computationally intensive structures (e.g. Voronoi Foam [9], inverse homogenization [11]), and for highly-repetitive structures (e.g. parametric lattices [7], and cyclic parametric functions [12]). The rest of the paper is organized as follows: In Section 2, we review the existing fine-scale structure modeling algorithm and categorize them based on the metrics we developed. In Section 3, we formulate the general set-based query and discuss two different point grouping strategies for Voronoi foams and for repetitive structures. In addition, we introduce two different preprossessing algorithms that precompute the Voronoi edges for each neighborhood, and one preprocessing algorithm for repetitive structures. Such preprossessing would not be economic with the point-based query. In Section 4, we examine two downstream applications to our set-based query: ray-casting and slicing. These approaches have fundamentally different types of point sets. While slicing provides a very structured set of points that are fully defined, ray-casting provides a variable set of points due to variable step sizes and early ray termination. In Section 5, we test our approach against the naïve point-based query on commodity desktop hardware and show that it can improve performance for slicing by an order of magnitude. For ray-casting, our approach can improve performance by over an order of magnitude for the Voronoi foam representation. We also identify the inflection points at which the naïve point-based query may be faster for small queries. Broadly, our work helps to bridge the gap between high-performance, specialized implementations for specific fine-scale structure models and generalized, highly interoperable query-based models. The set-based query lies in the Goldilocks zone between the fully parallel computation of each query point individually, and full precomputation of the entire structure at once. The former approach, as we show, is slower. The latter approach is intractable for multiscale structures, as the level of detail grows exponentially with the number of finer scales, and the entire structure quickly becomes too large to fit in memory."
https://arxiv.org/html/2411.03173v1,A Stochastic Dynamic Network Model of the Space Environment,"This work proposes to model the space environment as a stochastic dynamic network where each node is a group of objects of a given class, or species, and their relationship is represented by stochastic links. A set of stochastic dynamic equations, governing the evolution of the network, are derived from the network structure and topology. It will be shown that the proposed system of stochastic dynamic equations well reproduces existing results on the evolution of the space environment. The analysis of the structure of the network and relationships among node can help to understand which species of objects and orbit regimes are more critical and affect the most the future evolution of the space environment. In analogy with ecological networks, we develop a theory of the carrying capacity of space based on the stability of equilibria of the network dynamics.Some examples are presented starting from the current population of resident objects and different launch traffic forecast models. It will be shown how the proposed network model can be used to study the effect of the adoption of different policies on the execution of collision avoidance and post mission disposal manoeuvres.","The space industry is one of today’s most growing sectors, and as a consequence, the number of resident space objects is continuously rising. According to ESA’s Space Environment Report 2023, at the reference epoch November 1st 2016, the estimated number of debris objects in orbit in the different size ranges is: 34,000 objects greater than 10 cm, 900,000 objects from 1 cm to 10 cm, and 128 million objects from 1 mm to 1 cm. A collision among space objects could generate a cloud of space debris that can trigger a cascade of collisions that could make the use of space dangerous for future missions [15] [27]. Large constellation is an important factor that affects the evolution of the space debris environment [45], and untracked debris will lead to potentially dangerous on-orbit collisions on a regular basis due to the large number of satellites within mega-constellation orbital shells [4]. The growth rate of collisional debris would exceed the natural decay rate in ≈\approx≈ 50 years [6], [23]. Moreover, when the number of objects increases, chain effects of collision may be triggered, which is known as Kessler Syndrome [15]. The Kessler Syndrome refers to a scenario in which earth orbits inevitably become so polluted with satellite-related orbital debris that a self-reinforcing collisional cascade, which destroys satellites in orbit and makes orbital space unusable, is inevitable [3]. It is therefore essential to develop the necessary techniques to model, understand and predict the current space environment and its future evolution [28]. An effective Space Traffic Management (STM) is pivotal to expand the population of space objects safely and to reduce the burden on space operators [29]. There are many existing efforts to model the long-term evolution of the space environment. Space agencies have developed their own models, such as the NASA LEGEND environment model [21] and ESA’s DELTA model [40]. Both of these examples have high-fidelity propagators that apply perturbations to all objects in the simulation. Their approaches to modelling collision rates calculate probabilities of collisions within control volumes, which is based on the widely used CUBE method [24]. This method is also used in MEDEE, developed by CNES [8], and SOLEM, developed by CNSA [42]. Another well established tool using a similar approach is SDM [35]. This approach to modelling environment evolution can be computationally expensive as it often requires propagating each individual object. Other environment models use some simplifying assumptions to reduce their computational requirement. It is common to divide the environment into discrete bins based on orbital parameters and define the evolution based on the statistics of each bin. One earlier example is IDES, which divides objects into bins based on some spatial and physical parameters [41]. INDEMN makes a further simplification of only considering densities of objects in discrete orbital shells in terms of altitude [25]. These approaches allow the generation of much faster estimates of the space environment evolution. MIT Orbital Capacity Tool (MOCAT) is operated by propagating all the Anthropogenic Space Objects (ASOs) forward in time, where MOCAT-MC [13] stands for the model based on Monte-Carlo method, and MOCAT-SSEM [10] stands for the model based on a source-sink model, and MOCAT-ML [33] stands for the model based on Machine Learning. When evaluating performing long term simulations through an environment model, the results are usually presented in terms of the number of objects, and the cumulated number of catastrophic collisions. Besides these basic information, several metrics have been proposed to quantify the health of space environment. For example, Environmental Consequences of Orbital Breakups (ECOB) focused on the evolution of the consequences (i.e. the effects) of a fragmentation. [19],[17]. The trend in number of fragments [16], the Criticality of Spacecraft Index (CSI) to measure the environmental impact of large bodies [36]. Undisposed Mass Per Year is a meaure as the amount of mass left in orbit due to the failure of PMD [12]. A maximum orbital capacity calculated by treating the model as a dynamical system and calculating the equilibrium point [7],[9]. Though these metrics provide valuable insights into the characteristic of the space environment from different scopes, there is not a single commonly-accepted metric to measure the health of the space environment. This paper proposes to model the space environment as a stochastic dynamic network, where each node is a group of objects, belonging to a given class, or species, and their relationship is represented by stochastic links. In the remainder of the paper, this model is called NESSY, or NEtwork model for Space SustainabilitY. This modeling approach is different from previous works on the modeling of the space environment in that it enables a direct and explicit analysis of the relationships among different species of objects, it allows identifying communities of objects with similar characteristics and their impact on the rest of the environment. It also differs from previous works on the use of network theory applied to the space environment [34] in that it derives the evolutionary equations from the network structure. The results in this paper build upon previous work by the authors [1, 2, 43] and introduces five new main contributions: i) an improvement of the completeness of the network model introduced in [1, 2, 43], with the ability to model more species of space objects and interactions; ii) a comparison of NESSY against another well-established and validated model; iii) a study of the effect of launch traffic, collision avoidance manoeuvres and post mission disposal policies on the evolution of the space environment; iv) a preliminary study of centrality and eigenvalues of the network dynamics and v) a theory of the carrying capacity of the space environment, starting from the stability of the equilibria of the network dynamics. The paper is structured as follows. In Section 2, we introduce the definition of our proposed network model and some validation tests against known evolutionary models. In Section 3, we present some example where the network model is used to predict the effect of launch traffic, collision avoidance manoeuvres and post mission disposal policy. In Section 4, we introduce some structural properties of the network like centrality and eigenvalues and in Section 5 we introduce a carrying capacity theory. Finally in Section 6 we conclude with some remarks and recommendations for future work."
https://arxiv.org/html/2411.02692v1,JPEC: A Novel Graph Neural Network for Competitor Retrieval in Financial Knowledge Graphs,"Knowledge graphs have gained popularity for their ability to organize and analyze complex data effectively. When combined with graph embedding techniques, such as graph neural networks (GNNs), knowledge graphs become a potent tool in providing valuable insights. This study explores the application of graph embedding in identifying competitors from a financial knowledge graph. Existing state-of-the-art(SOTA) models face challenges due to the unique attributes of our knowledge graph, including directed and undirected relationships, attributed nodes, and minimal annotated competitor connections. To address these challenges, we propose a novel graph embedding model, JPEC(JPMorgan Proximity Embedding for Competitor Detection), which utilizes graph neural network to learn from both first-order and second-order node proximity together with vital features for competitor retrieval. JPEC had outperformed most existing models in extensive experiments, showcasing its effectiveness in competitor retrieval.","Competitor retrieval is one of the most crucial use cases for financial organizations. Traditionally, it is mostly driven by multiple manual tasks involving collecting data and converting factors like revenue, products, pricing, marketing, and industry distributions. While manually gathered market data offer vital insights, they have limitations in terms of applicability and scalability. On the other hand, knowledge graphs can provide competitive clues by revealing meaningful connections, such supply-chain, between companies. Combined with graph embedding techniques, knowledge graphs can offer a structured and efficient approach for automatic and intelligent competitor retrieval. However, most SOTA graph embedding methods are sub-optimal for our task due to the complex structure of a real world knowledge graph(described in Section2). This paper introduces a novel graph neural network, JPEC, for competitor detection from a financial knowledge graph with various types of edges but limited labeled data."
https://arxiv.org/html/2411.02429v1,IdeaBench: Benchmarking Large Language Models for Research Idea Generation,"Large Language Models (LLMs) have transformed how people interact with artificial intelligence (AI) systems, achieving state-of-the-art results in various tasks, including scientific discovery and hypothesis generation. However, the lack of a comprehensive and systematic evaluation framework for generating research ideas using LLMs poses a significant obstacle to understanding and assessing their generative capabilities in scientific discovery. To address this gap, we propose IdeaBench, a benchmark system that includes a comprehensive dataset and an evaluation framework for standardizing the assessment of research idea generation using LLMs. Our dataset comprises titles and abstracts from a diverse range of influential papers, along with their referenced works. To emulate the human process of generating research ideas, we profile LLMs as domain-specific researchers and ground them in the same context considered by human researchers. This maximizes the utilization of the LLMs’ parametric knowledge to dynamically generate new research ideas. We also introduce an evaluation framework for assessing the quality of generated research ideas. Our evaluation framework is a two-stage process: first, using GPT-4o to rank ideas based on user-specified quality indicators such as novelty and feasibility, enabling scalable personalization; and second, calculating relative ranking based “Insight Score” to quantify the chosen quality indicator. The proposed benchmark system will be a valuable asset for the community to measure and compare different LLMs, ultimately advancing the automation of the scientific discovery process. Our code and dataset are available at: https://anonymous.4open.science/r/IdeaBench-2747/.","Recent years have witnessed the rapid development of Large Language Models (LLMs). LLMs like GPT-4 (OpenAI 2023) and LLama series (Touvron et al. 2023) introduced advanced capabilities that set them apart from previous generations of machine learning models. Among these capabilities, in-context learning allows LLMs to understand and respond to user prompts in a nuanced manner without requiring additional training for each specific task, enabling LLMs to generalize across a wide range of tasks, providing robust state-of-the-art performance even with limited data (Brown et al. 2020). As a result, LLMs have revolutionized the way humans interact with AI systems, making it possible to generate coherent text, translate languages, answer questions, and even compose creative content with unprecedented accuracy and fluency (Bubeck et al. 2023). The impact of these advancements extends beyond consumer applications, influencing various sophisticated domains such as education (Moore et al. 2023), healthcare (Yang et al. 2023a), and scientific research (Wysocki et al. 2024). Recently, the impressive performance of LLMs in everyday applications has sparked significant interest in academia, particularly for their potential use in scientific discovery or hypothesis generation (AI4Science and Quantum 2023). Several studies have explored leveraging LLMs to generate hypotheses or research ideas (Yang et al. 2023b; Wang et al. 2023b; Zhou et al. 2024; Baek et al. 2024; Qiu et al. 2023). However, despite numerous results, a unified and comprehensive framework for evaluating generated research ideas is still lacking, making it difficult for the community to clearly understand the performance spectrum of different techniques for generating research ideas. To address this limitation, we introduce a standardized evaluation framework designed to emulate how human researchers generate research ideas. This framework, termed IdeaBench, comprises three main components: dataset construction, research idea generation, and a novel metric to evaluate the quality of the generated research ideas. The intuition behind this framework is grounded in the typical research process of how researchers generate new scientific research ideas as described below: 1. Targeting a specific topic. 2. Reviewing related literature, focusing on recent findings and methodologies. 3. Identifying gaps in knowledge or methods within these recent findings. 4. Proposing research ideas to address these gaps. We first construct a benchmark dataset that includes meticulously filtered 2,374 target papers’ abstracts from biomedical research fields. These target papers serve as the ground-truth sources of research ideas. Additionally, the dataset contains the abstracts of the papers referenced by the target papers, providing the context necessary for LLMs to generate relevant research ideas. This comprehensive dataset aims to capture the complexity and specificity of scientific research, particularly in the biomedical domain, thus offering a solid foundation for evaluating LLMs’ capability in generating research ideas. Based on the benchmark dataset, we design a prompt template that leverages LLMs to generate research ideas. In addition to grounding the context for idea generation using reference papers from our dataset, we also profile the LLMs as domain-specific researchers in the prompt. This approach aims to dynamically maximize the utilization of the LLMs’ parametric knowledge, enabling the generation of more in-depth and insightful research ideas. It can also be used as a baseline for future comparisons. To accurately assess the quality of generated research ideas, we design an evaluation framework which incorporates two critical components: personalized quality ranking and relative quality scoring. This dual approach allows for a nuanced assessment that takes into account user-defined quality indicators such as novelty, feasibility, etc. Our design ensures a versatile and comprehensive evaluation framework, capable of adapting to different research contexts and providing meaningful insights into the quality of LLM-generated ideas. Our results show that recent high-capacity LLMs are capable of generating research ideas using IdeaBench dataset, and our metric is able to assess the quality of generated research ideas from different dimensions. We hope this work inspires academia to further unleash the potential of LLMs in supporting research ideation, ultimately accelerating scientific discovery in the future. To summarize, our contributions are as follows: • We construct IdeaBench dataset, which consists of 2,374 influential biomedical target papers along with their 29,408 reference papers, to evaluate LLMs’ capabilities in generating research ideas. • We propose an evaluation framework which offers a scalable and versatile metric called “Insight Score”, which can quantify novelty, feasibility, or any other quality indicators defined by human researchers. • We conduct extensive experiments to demonstrate several LLMs’ abilities in generating research ideas based on our dataset and evaluation metric."
https://arxiv.org/html/2411.02324v1,Non-parametric Inference for Diffusion Processes:A Computational Approach via Bayesian Inversionfor PDEs,"In this paper, we present a theoretical and computational workflow for the non-parametric Bayesian inference of drift and diffusion functions of autonomous diffusion processes. We base the inference on the partial differential equations arising from the infinitesimal generator of the underlying process. Following a problem formulation in the infinite-dimensional setting, we discuss optimization- and sampling-based solution methods. As preliminary results, we showcase the inference of a single-scale, as well as a multiscale process from trajectory data.","Various phenomena in science and engineering can be modelled via stochastic processes, or more precisely, diffusion processes. Applications include, to mention a few, problems in biology, climate science, energy technology, and finance (see, e.g., [29] and the references therein). More recently, diffusion processes have found their use in machine learning, such as in deep generative models [39]. A special use-case is the description of interacting many-particle systems, as they occur, for instance, in molecular dynamics simulations. Such systems often allow for an effective coarse-grain representation via stochastic dynamics. Diffusion processes need to be parameterized by suitable drift and diffusion functions, which are typically unknown from first principles. On the other hand, a wealth of data is often available in the form of stochastic trajectories. Indeed, computational procedures such as molecular dynamics simulations typically generate entire ensembles of trajectories [18]. Calibration of the parameter functions from this indirect data is therefore an important endeavor. As drift and diffusion have to be considered as functions defined on a system’s state space, they are formally infinite-dimensional objects. Thus, their inference from trajectory data, which is mostly available at discrete points in space and time, is severely ill-posed [27, 7]. A possible remedy is parametric inference [20, 21, 6], but this imposes possibly strong a-priori assumptions on the unknown quantities. In addition, the available data is commonly noise-polluted which, in combination with modelling assumptions, introduces uncertainty into the parameter estimates. A Bayesian framework provides an elegant approach to addressing both of these issues. By treating both the data and unknowns as random variables, we can naturally incorporate uncertainties into the inference process. The definition of a prior measure further provides a function space regularizer that incorporates a-priori knowledge about the parameter functions, i.e., the drift and diffusion. In this work, we present a workflow for non-parametric Bayesian inference for the parameter functions of diffusion processes. We base our exposition on the function space viewpoint presented in [32]. For the forward model, we exploit the link of a diffusion process with its generator via the Kolmogorov partial differential equations, which govern the evolution of the process’s observables and probability densities, respectively. We further utilize a collection of methods for the assessment of the posterior measure that is specifically tailored towards large-scale problems [13, 38]. The resulting computational pipeline is demonstrated through inference for a single-scale process from trajectory data. Lastly, we infer an effective coarse-grain representation of a multiscale process."
https://arxiv.org/html/2411.01928v1,Datasets for Advanced Bankruptcy Prediction: A survey and Taxonomy,"Bankruptcy prediction is an important research area that heavily relies on data science. It aims to help investors, managers, and regulators better understand the operational status of corporations and predict potential financial risks in advance. To improve prediction, researchers and practitioners have begun to utilize a variety of different types of data, ranging from traditional financial indicators to unstructured data, to aid in the construction and optimization of bankruptcy forecasting models. Over time, not only instrumentalized data improved, but also instrumentalized methodology for data structuring, cleaning, and analysis. With the aid of advanced analytical techniques that deploy machine learning and deep learning algorithms, bankruptcy assessment became more accurate over time. However, due to the sensitivity of financial data, the scarcity of valid public datasets remains a key bottleneck for the rapid modeling and evaluation of machine learning algorithms for targeted tasks. This study therefore introduces a taxonomy of datasets for bankruptcy research, and summarizes their characteristics. This paper also proposes a set of metrics to measure the quality and the informativeness of public datasets. The taxonomy, coupled with the informativeness measure, thus aims at providing valuable insights to better assist researchers and practitioners in developing potential applications for various aspects of credit assessment and decision making by pointing at appropriate datasets for their studies.","The prediction of corporate bankruptcy is crucial for the stability of the economic system and the sustainability of business management. In an ever-changing and increasingly competitive global business environment, timely identification of potential financial risks enables companies to implement effective strategic and management measures. Traditional accounting-based has played a pivotal role in bankruptcy prediction, providing a comprehensive assessment of an enterprise’s operational and financial status. There are many studies on bankruptcy prediction based on accounting-based data, and most of the studies focus on the model itself to explore how to use a better model to improve the prediction based on current data[1, 2, 3, 4]. However, publicly available accounting-based data is limited due to several reasons. First, the sensitivity of financial data often restricts full disclosure. Second, as only listed companies are obliged to disclose financial reports, it is difficult to obtain relevant data on small and medium-sized enterprises. Last, but not least, the lack of uniformity in financial report formats complicates the collection, cleaning, and structuring of large datasets, increasing the likelihood of noisy and redundant data. In response to these limitations, the research and professional communities are increasingly turning to machine learning and deep learning techniques, referred to in this paper as advanced bankruptcy prediction methods. We consider these methods as instruments to enhance and increase the quality and informativeness of the dataset for a better performance for bankruptcy prediction. Some studies have integrated macroeconomic data with accounting-based variables to capture some signals that cannot be detected by traditional indicators, enabling earlier warning of bankruptcy risk and providing more time and space for corporate decision-making[5, 6, 7, 8, 9, 10, 11, 12].Other research has identified new predictors in emerging areas, such as textual data and relational data, bringing new understanding and research directions to both industry and academia [13, 14, 15, 16]. By combining data from various fields—including financial, market, and social aspects—a more comprehensive understanding of the business ecosystem can be achieved, enriching the context for bankruptcy prediction. Given the multifaceted influences on business operations, utilizing multiple datasets helps address this complexity and build more robust predictive models. Currently, there have been many reviews on bankruptcy prediction modeling (see [17, 18, 19, 20, 21, 22, 23, 24]), and they summarize the modeling methods applied to bankruptcy prediction from various perspectives, ranging from traditional techniques to deep learning model applications. However, as the authors of this paper argue, the effectiveness of the model is highly dependent on the quality of the input data. The results of the comparison of the model effectiveness may become obsolete due to the quality differences of the input data. This paper therefore aims at providing a taxonomy by categorizing the datasets that can be found and instrumentalized by scientific publications. By offering the metrics to evaluate the effectiveness of various datasets for bankruptcy prediction, we are able to better understand and collect data that can help predict bankruptcy, so that we can understand the signs and trends and take effective measures ahead of time, thus reducing the risk of bankruptcy. Compared to other reviews, this paper is dedicated to a comprehensive and in-depth study of multiple datasets applied to corporate bankruptcy prediction, providing a more holistic perspective on the integration of datasets across disciplines. Our contribution has three important pillars, as follows: 1. To the best our knowledge, this paper is the first attempt to survey topical research papers and to define the taxonomy of bankruptcy prediction datasets that are instrumentalized for research. 2. The developed taxonomy provides criteria for classifying and comparing different datasets, allowing researchers to obtain a more focused view of relevant datasets, and to cater their research needs accordingly. 3. The proposed set of metrics evaluates the quality and the informativeness of the datasets, supporting therefore researchers in collecting the most appropriate datasets for bankruptcy prediction. The remainder of the paper is organized as follows. First, in Section 2 we describe the manually collected data for our research and we introduce our taxonomy. Section 3 narrows down our data sample universe to the publicly available datasets, and introduces our measure that we develop to address the informativeness of datasets. Section 4 concludes with a reflection on our findings."
https://arxiv.org/html/2411.01386v1,"A High-Resolution, US-scale Digital Similar of Interacting Livestock,
Wild Birds, and Human Ecosystems with Applications to Multi-host Epidemic Spread","One Health issues, such as the spread of highly pathogenic avian influenza (HPAI), present significant challenges at the intersection of human, animal, and environmental health. Recent H5N1 outbreaks underscore the need for comprehensive modeling that capture the complex interactions between various entities in these interconnected ecosystems, encompassing livestock, wild birds, and human populations. To support such efforts, we present a synthetic spatiotemporal gridded dataset for the contiguous United States, referred to as a digital similar. The methodology for constructing this digital similar involves fusing diverse datasets using statistical and optimization techniques. The livestock component includes farm-level representations of multiple livestock types—cattle, poultry, hogs, and sheep—including further categorization into subtypes, such as milk and beef cows, chicken, turkeys, ducks, etc. It also includes location-level data for livestock-product processing centers. Weekly abundance data for key wild bird species involved in avian flu transmission are included along with temporal networks of movements. Gridded distributions of the human population, along with demographic and occupational features, capture the placement of agricultural workers and the general population. The digital similar is verified and validated in multiple ways. This dataset aims to provide a comprehensive basis for modeling complex phenomena at the wild-domestic-human interfaces.","Importance and Challenges of High-Resolution Spatiotemporal Modeling. Recent years have seen the development of national-scale, realistic in silico representations that capture data on populations, socioeconomic activities, and built infrastructure to study complex phenomena such as epidemiology, emergency response, and food security at fine spatiotemporal resolutions [8, 17, 60, 20, 52, 30]. Here, we refer to such synthetic datasets as digital similars, as they have statistical similarity to real data, but differ from “digital twins”, which are intended as precise replicas of real-world systems. These realistic datasets are used for risk assessment and simulation modeling, as evidenced by studies conducted during the COVID-19 pandemic to analyze infectious disease dynamics [1, 22, 31, 33, 2, 16]. The objective of this work is to develop realistic representations of the wild-domestic-human ecosystems, motivated by the emerging threat of highly pathogenic avian influenza (HPAI). The growing threat of avian influenza to health, agriculture, and the environment. Highly pathogenic avian influenza (HPAI) is a major One Health issue [34, 32]. With increases in virulence of HPAIs [36], the risk of a potential pandemic increases, as they continue to affect new regions and more animal categories [38, 57]. The recent H5N1 virus clade, 2.3.4.4b, has gained a lot of attention globally due to the loss of wildlife, including seabirds and aquatic mammals across the globe [10, 35, 59, 53, 52]. In the US, spillover events have caused significant negative impacts to the livestock industry, including large-scale outbreaks in poultry and dairy cattle [9, 52, 11, 45]. There have also been several instances of zoonotic transmissions [15, 12, 13] posing a serious pandemic hazard. Therefore, there is an urgent need to understand the dynamics of HPAI spread for effective surveillance and mitigation of its impact on health and food security. Multi-pathway spread of HPAIs. The introduction and long-range spread of HPAI infections have been primarily attributed to migratory birds and the movement of livestock and livestock products [34, 48, 38, 52, 45, 11], though other pathways may facilitate local spread, including cross-species transmission within and between farms. It is also important to model the distribution of livestock populations into farms of various sizes, as concentrated livestock operations not only act as reservoirs, but can also lead to large outbreaks and rapid spread [28]. In this regard, the impact on the poultry population is notable, with multiple outbreaks across the US affecting millions of chickens [14, 5]. Representing the human population is necessary for studying (i) the role of agricultural workers in the spread, (ii) their susceptibility to zoonotic transmissions, and (iii) the introduction of the disease to the rest of the population. It is critical that modeling efforts account for these known key entities and the interactions among them that drive this spread at the appropriate spatial, temporal, and organization scales. Challenges. However, building digital similars presents various challenges due to data scarcity, as well as the need to fuse diverse datasets that may have been collected across different spatial resolutions. In the context of livestock, synthetic datasets have been developed to represent spatial population distributions and operations [8, 17, 7, 27], especially relevant to studies on highly pathogenic avian influenza (HPAI) spread [32, 52]. Yet, from the perspective of HPAIs, these works tend to focus on a limited set of entities and transmission pathways [52, 32]. Summary of our contributions. This work presents a high-resolution multi-layered spatiotemporal representation of the contiguous US, henceforth referred to as the digital similar (𝒟⁢𝒮𝒟𝒮\mathcal{DS}caligraphic_D caligraphic_S), that captures (i) the distribution of livestock populations and operations, (ii) associated food processing center locations, capacities, and functions, (iii) spatiotemporally-varying wild bird abundances for multiple species, and (iv) human populations with demographic features and attributes capturing agricultural employment, as illustrated in Figure 1. We leverage diverse datasets (detailed in Table 1), such as the Census of Agriculture, the Gridded Livestock of the World (GLW) dataset, eBird Status and Trends, and locations of livestock-related operations obtained from multiple sources. Our methods involve a combination of optimization techniques, statistical tools, and graph algorithms. We perform rigorous data quality checks with reference to parent data sets and verification & validation studies using independent data sets including known locations of large livestock farms and H5N1 incidence reports. The livestock layers consist of four main animal types: cattle, poultry, hogs (pigs), and sheep, with fine grid-level data (5555 arc minute resolution) on population size as well as farms. We also include subtypes for cattle (beef and milk) and poultry. The wild bird species include the species of high relevance to HPAI transmissions. To the best of our knowledge, this is the first work that models cattle populations at the national scale. Also, our work extends earlier works on hogs and chickens (e.g., [52, 8, 32]) capturing more livestock types and subtypes. This digital similar aims to provide a comprehensive platform for modeling and risk assessment of HPAI-like phenomena at high spatial and temporal resolutions across the contiguous US, thus informing disease surveillance and control efforts. Figure 1: Overview of the digital similar. A schematic of the system highlighting the various components including the various entities or agents that are represented, and the dashboard through which the data is exposed is provided at the top. Two of the four livestock layers are shown. We have zoomed in on major production regions for the respective livestock. Both population density and counts of farms are depicted. Also shown are livestock and dairy processing centers. For the human population, agricultural workers are highlighted. The spatiotemporal distribution of three wild birds is shown in the last layer."
https://arxiv.org/html/2411.01299v1,PMI-DT: Leveraging Digital Twins and Machine Learning for Predictive Modeling and Inspection in Manufacturing,"Over the years, Digital Twin (DT) has become popular in Advanced Manufacturing (AM) due to its ability to improve production efficiency and quality. By creating virtual replicas of physical assets, DTs help in real-time monitoring, develop predictive models, and improve operational performance. However, integrating data from physical systems into reliable predictive models, particularly in precision measurement and failure prevention, is often challenging and less explored. This study introduces a Predictive Maintenance and Inspection Digital Twin (PMI-DT) framework with a focus on precision measurement and predictive quality assurance using 3D-printed 1”-4 ACME bolt, CyberGage 360 vision inspection system, SolidWorks, and Microsoft Azure. During this approach, dimensional inspection data is combined with fatigue test results to create a model for detecting failures. Using Machine Learning (ML) —Random Forest and Decision Tree models—the proposed approaches were able to predict bolt failure with real-time data 100% accurately. Our preliminary result shows Max Position (30%) and Max Load (24%) are the main factors that contribute to that failure. We expect the PMI-DT framework will reduce inspection time and improve predictive maintenance, ultimately giving manufacturers a practical way to boost product quality and reliability using DT in AM.","Digital Twin (DT) technology is a virtual model, or ""twin,"" of a physical object that continuously updates with real-time data from its physical counterpart. This integration allows detailed monitoring, simulation, and predictive analysis, making DTs highly useful in manufacturing for managing product lifecycles, predictive maintenance, and optimizing processes through data drawn from sensors, machine learning models, and the Internet of Things (IoT) [1, 2]. The concept of DT was introduced by NASA’s space missions in the early 2000s. During these missions, DTs created replicas of spacecraft systems on earth. This approach allowed ground teams to simulate and solve in-flight issues in real time [3]. Since then, this idea has spread to manufacturing, driven by Industry 4.0’s focus on automation, data sharing, and interconnected smart systems [4]. DTs offer many advantages in manufacturing. For instance, DTs can leverage machine data for predictive maintenance to predict potential failures and make timely interventions to lessen downtime [5]. DTs can also be used in quality control to replicate production lines or parts, allowing for digital inspections and measurement-based QC even before physical tests[2]. DTs also allow users to simulate changes in production environments within the virtual model and evaluate their outcomes without affecting real-life setups [2]. By leveraging real-time simulations and feedback loops, DTs help prevalent analytical decision making that makes the best use of resources determines production cycles, furthers better product quality. All of this improved efficiency ultimately feeds into customer satisfaction, by reducing delays [6]. However, implementing DT technology in manufacturing brings specific challenges, particularly in predictive modeling and inspection, including: • Data management and integration: Manufacturing operations produce extensive sensor data, and incorporating this data accurately into DTs requires a robust setup that can handle high volumes, speeds, and types of data. Compatibility with older systems and diverse data formats also complicates integration [2, 7]. • Predictive model accuracy: DTs depend on precise, high-quality data from physical systems to accurately simulate performance. Incomplete or low-quality data can reduce predictive accuracy, limiting the effectiveness of maintenance forecasting. Effective models require comprehensive, well-organized datasets [1, 6]. • Real-time synchronization and computational load: Keeping physical and digital systems in real-time sync places demands on computational resources. Continuous updates can make it challenging to manage a large amount of data in real time and increase computational weights, reducing processing speed or inspection accuracy [3]. • Security and privacy: The continuous data flow between the physical and virtual environments can create cybersecurity risks in DT systems. A secure infrastructure is needed to protect manufacturing data and ensure ongoing monitoring [5]. • Integration with inspection processes: While DTs improve inspection with simulations, aligning physical and virtual inspections is challenging. To effectively model a series of diverse manufacturing tasks, close integration with physical inspection is inevitable to ensure quality standards are maintained [8]. 1.1 Motivation of the Study In the era of Industry 4.0, the growing complexity and increasing precision required for manufacturing made advanced methods such as DTs inevitable. This study underscores the importance of employing DTs in quality control and failure prediction to minimize risks while ensuring reliable operations, especially in critical environments. For instance, a notable manufacturing challenge involved a conical hole plug assembly that failed, resulting in a dangerous projectile in a manufacturing environment. The conical plug, shown in Figure 1(a), was designed to fit within a conical hole. Compressed by a 1-1/2”–6 Grade 8 bolt (Figure 1(b)), it formed a seal to manage elastomer extrusion under high pressure (up to 1400 bar/22,000 psi). Over time, repeated use caused the bolt threads to fatigue and deform, ultimately leading to failure. A predictive maintenance system based on DT technology could have foreseen this failure through early detection, and lifecycle analysis might help to prevent the incident. Figure 1: Conical hole plug assembly. (a) An external view of the process flow in the conical hole plug assembly. (b) A detailed view of the plug mechanism. Item 1 represents the conical plug, Item 2 is the conical hole, and Item 3 is the 1-1/2”–6 Grade 8 bolt. The red arrows depict internal pressure at 1400 bar (22,000 psi). During the elastomer extrusion process, extreme pressure on the conical hole plug exerted an upward force on the bolt threads. This repeated stress across multiple cycles deformed the threads, as shown in Figure 2. The deformation eventually led to bolt failure and the release of uncontrolled pressure. A robust thread inspection process combined with real-time monitoring could have predicted this failure and enabled preventive maintenance to ensure safety. Figure 2: Deformed vs acceptable threads of the Bolt. Considering the opportunity, this study aims to develop integrated DT approaches to improve quality control and prevent failures in manufacturing processes through the proposed Predictive Maintenance and Inspection DT (PMI-DT) framework. The objectives of this study are outlined below: 1. Create a DT using SolidWorks and Azure DTs to represent the CyberGage 360 and a 1”-4 ACME bolt, with model schema based on 3D mechanical models and critical dimensions. 2. Design a workflow and data stream to validate the CyberGage 360’s inspection process and integrate fatigue testing and dimensional inspection data into the Bolt DT, forming the basis of the PMI-DT framework. 3. Apply Machine Learning (ML) algorithms—Random Forest and Decision Tree models—to analyze inspection data and fatigue test results to detect patterns that indicate component failure."
https://arxiv.org/html/2411.01231v1,TDS Simulator: A MATLAB App to model temperature-programmed hydrogen desorption,"We present TDS Simulator, a new software tool aimed at modelling thermal desorption spectroscopy (TDS) experiments. TDS is a widely used technique for quantifying key characteristics of hydrogen-material interactions, such as diffusivity and trapping. However, interpreting the output of TDS experiments is non-trivial and requires appropriate post-processing tools. This work introduces the first software tool capable of simulating TDS curves for arbitrary choices of material parameters and hydrogen trap characteristics, using the primary hydrogen diffusion and trapping models (Oriani, McNabb-Foster). Moreover, TDS Simulator contains a specific functionality for loading experimental TDS data and conducting the inverse calibration of a selected transport model, providing automatic estimates of the density and binding energy of each hydrogen trap type in the material. In its first version, TDS Simulator is provided as a MATLAB App, which is made freely available to the community and provides a simple graphical user interface (GUI) to make use of TDS Simulator straightforward. As reported in the present manuscript, the outputs of TDS Simulator have been extensively validated against literature data. Demonstrations of automatic determination of trap characteristics from experimental data through the optimisation tool are also provided. The present work enables an efficient and straightforward characterisation of hydrogen-material characteristics relevant to multiple applications, from nuclear fusion to the development of hydrogen-compatible materials for the hydrogen economy. TDS Simulator can be downloaded from https://mechmat.web.ox.ac.uk/codes (To be uploaded immediately after the review process).","Hydrogen is a critical component of proposed decarbonization strategies due to its natural abundance, minimal projected environmental impact, and potential for decarbonizing traditionally hard-to-abate industries [1, 2, 3]. However, while it is clear that hydrogen offers significant promise as an energy carrier, a key impediment to the proliferation of a hydrogen-based economy is the propensity for hydrogen to degrade the mechanical properties of structural metals [4]. This hydrogen embrittlement phenomenon has been responsible for several recent high-profile component failures, such as the well-publicized fracture of 32 anchor rods on the new eastern span of the San Francisco-Oakland Bay Bridge [5]. While many factors can influence the susceptibility of a material to hydrogen embrittlement, variations in hydrogen-metal interactions (e.g., hydrogen uptake, diffusion, and trapping) have a particularly strong effect. For example, the extent of degradation in alloy toughness or ductility has been shown to strongly depend on hydrogen concentration [6, 4, 7]. Similarly, a strong correlation between the Stage II subcritical crack growth rate and hydrogen diffusivity has also been observed across a wide range of metallic materials [8]. As such, quantifying hydrogen-material interactions parameters can provide insights into embrittlement susceptibility; such insights are especially useful when developing new materials given that hydrogen diffusivity, uptake, and trapping behaviour are strongly influenced by alloy microstructure [9, 10, 11]. These dependencies have motivated the development of various experimental methods and techniques for assessing hydrogen-metal interactions [12, 13]. For example, total hydrogen content can be determined via inert gas fusion, vacuum fusion, silicone oil, and laser thermal desorption [14], while techniques such as the Barnacle cell method enable quantification of the diffusible hydrogen content [15]. However, the two most widely adopted methods for evaluating hydrogen interactions with metallic microstructure are hydrogen permeation and thermal desorption spectroscopy (TDS) [16, 17]. Briefly, permeation involves the generation and uptake of hydrogen on one side of a thin membrane (via gaseous hydrogen exposure or electrochemical hydrogen production) and then measuring the rate of hydrogen effusion on the other side via mass spectrometry, change in vacuum pressure, or oxidation current density [18]. The hydrogen flux versus time data are then evaluated using various theoretical models to determine the effective hydrogen diffusivity and diffusible hydrogen concentration for the employed environment/material combination. The benefits of electropermeation are the ease of implementation and straightforward analysis of generated data, but this method is prone to significant test-to-test variability [19, 20, 17]. Conversely, TDS involves controlled out-gassing of a hydrogen pre-charged specimen as a function of temperature, with the hydrogen flux monitored via (e.g.) a high-resolution mass spectrometer (Fig. 1). Analysis of the hydrogen flux versus temperature or time then enables the determination of hydrogen trapping characteristics (binding energies, site densities), effective diffusivity, and total hydrogen concentration [13]. The benefits of TDS experiments are the ability to calculate most primary hydrogen-metal interaction parameters, but such experiments generally require an ultra-high vacuum environment and the use of uniformly pre-charged specimens, although ambient pressure systems are now commercially available (albeit with reduced hydrogen sensing resolution). Figure 1: Thermal desorption spectroscopy (TDS) to unravel hydrogen-material interactions; (a) Schematic illustration of a type of TDS apparatus, and (b) a schematic of the different energy levels involved in the diffusion of hydrogen in metals. In addition to the need for specialized equipment, another challenge associated with TDS measurements is the interpretation of the desorption spectra [21, 22, 23, 24]. Several theoretical frameworks have been proposed to relate TDS output to trapping characteristics, with the three most common ones being (1) McNabb and Foster [25], (2) Oriani [26], and (3) Kissinger [27, 28]. McNabb and Foster’s model provides a generalised treatment that explicitly includes hydrogen trapping and detrapping kinetics in the treatment of hydrogen diffusion [29, 30]. Oriani sought to simplify McNabb and Foster’s framework by assuming a local equilibrium between trap sites and the lattice [26], which reduces the McNabb and Foster framework to a single equation [31, 32]. Lastly, Kissinger [27] assumes a detrapping-dominated paradigm where diffusion is infinitely fast, which also simplifies the McNabb and Foster framework and enables straightforward determination of trap binding energies [28]. Kissinger’s method, also referred to as the Choo-Lee approach, is frequently used due to its simplicity but the assumptions employed result in a very narrow regime of validity. Specifically, since it assumes infinitely fast diffusion, it is only suitable for a small range of heating rates, sufficiently thin samples, and high-diffusivity materials [33, 34, 35]. Moreover, removing an effect of diffusion inherently precludes capturing the effect of specimen thickness, trapping densities or initial hydrogen concentration. For this reason, both Oriani and McNabb-Foster models are often of interest for analysing TDS spectra, but such approaches require the use of numerical tools which are not readily available across the hydrogen community. The present work aims to fill this need by providing the first generalized framework for analyzing TDS data, including both McNabb and Foster and Oriani-based analyses for an arbitrary number of traps, and the first standalone software package for conducting virtual TDS experiments. In this way, the present work contributes to ongoing efforts in the community aimed at providing software tools to facilitate an improved understanding of hydrogen-material interactions [36, 37, 38]. In the remainder of this paper, we proceed to describe the characteristics of TDS Simulator, the new software tool that we have developed to automate the analysis of TDS data. TDS Simulator provides a GUI-based platform for creating synthetic TDS data and assessing experimental TDS spectra using the primary theories for hydrogen trapping and desorption. Critically, this MATLAB App enables the efficient determination of trapping parameters from experimental data using a deterministic variable inference algorithm. To establish appropriate context, we begin by providing a concise review of the relevant theories for modelling TDS data in Section 2. An overview of the software tool is then provided in Section 3. The robustness of the software package is demonstrated by validating against analytical and numerical data from the literature (Section 4), and its usage is exemplified by determining trapping characteristics from experimental TDS data (Section 5). Finally, the manuscript ends with concluding remarks in Section 6."
https://arxiv.org/html/2411.01121v1,Hedging and Pricing Structured Products Featuring Multiple Underlying Assets,"Hedging a portfolio containing autocallable notes presents unique challenges due to the complex risk profile of these financial instruments. In addition to hedging, pricing these notes, particularly when multiple underlying assets are involved, adds another layer of complexity. Pricing autocallable notes involves intricate considerations of various risk factors, including underlying assets, interest rates, and volatility. Traditional pricing methods, such as sample-based Monte Carlo simulations, are often time-consuming and impractical for long maturities, particularly when there are multiple underlying assets. In this paper, we explore autocallable structured notes with three underlying assets and proposes a machine learning-based pricing method that significantly improves efficiency, computing prices 250 times faster than traditional Monte Carlo simulation based method. Additionally, we introduce a Distributional Reinforcement Learning (RL) algorithm to hedge a portfolio containing an autocallable structured note. Our distributional RL based hedging strategy provides better P⁢n⁢L𝑃𝑛𝐿PnLitalic_P italic_n italic_L compared to traditional Delta-neutral and Delta-Gamma neutral hedging strategies. The V⁢a⁢R𝑉𝑎𝑅VaRitalic_V italic_a italic_R 5%percent55\%5 % (P⁢n⁢L𝑃𝑛𝐿PnLitalic_P italic_n italic_L value) of our RL agent based hedging is 33.9533.9533.9533.95, significantly outperforming both the Delta neutral strategy, which has a V⁢a⁢R𝑉𝑎𝑅VaRitalic_V italic_a italic_R 5%percent55\%5 % of −0.040.04-0.04- 0.04, and the Delta-Gamma neutral strategy, which has a V⁢a⁢R𝑉𝑎𝑅VaRitalic_V italic_a italic_R 5%percent55\%5 % of 13.0513.0513.0513.05. It also provides the hedging action with better left tail P⁢n⁢L𝑃𝑛𝐿PnLitalic_P italic_n italic_L, such as 95%percent9595\%95 % and 99%percent9999\%99 % value-at-risk (V⁢a⁢R𝑉𝑎𝑅VaRitalic_V italic_a italic_R) and conditional value-at-risk (C⁢V⁢a⁢R𝐶𝑉𝑎𝑅CVaRitalic_C italic_V italic_a italic_R), highlighting its potential for front-office hedging and risk management.","Autocallable notes are complex structured financial products that offer investors potential returns linked to the performance of an underlying asset, such as a stock or index. These notes have an embedded ”autocall” feature, meaning they can be automatically redeemed before maturity if the underlying asset meets certain predefined conditions on specified observation dates. The complexity of autocallable notes stems from their intricate components, such as barrier levels, coupon payments, and conditions for early call. This complexity poses significant challenges for both pricing and hedging, necessitating advanced financial models and a thorough understanding of market dynamics. The difficulty increases substantially when autocallable notes involve multiple underlying assets, such as a combination of different stocks or indices. The inclusion of potential coupon and call features complicates the price profile over time. Traditional methods for pricing, such as sample-based Monte Carlo pricers, are often very time-consuming, especially for long maturities involving multiple underlying assets, making them impractical in many cases. This inefficiency becomes especially problematic in applications such as Reinforcement Learning based hedging and XVA calculations. In Reinforcement Learning, where millions of scenarios must be processed for effective hedging, traditional pricers are too slow to be practical. Similarly, for XVA calculations, which require the pricing of instruments hundreds of thousands of times, using a faster pricer can significantly reduce computation time and enhance efficiency. Utilizing machine learning (ML) to approximate the original pricing model offers substantial efficiency gains (Kondratyev, 2018; Mcghee, 2018; Neufeld and Sester, 2022). In this direction, we propose a machine learning based approximation which enhances efficiency, expediting the pricing process and significantly reducing computational time compared to the traditional methods. For instance, in a case of autocallable notes with three underlying indexes, our ML approximator computes the pricing 250 times faster than the Monte Carlo based pricer. Moreover, the execution time of the ML approximator remains constant regardless of the original pricer’s complexity, resulting in significant efficiency enhancements. In addition to the pricing, hedging a portfolio containing autocallable notes with multiple underlying assets is also crucial due to the complex risk profile of these financial instruments. Effective hedging strategies are necessary to ensure that the portfolio is protected against adverse price movements and unexpected changes in asset correlations. Additionally, given the intricacies of pricing and managing such notes, sophisticated hedging techniques are required to optimize risk management and enhance portfolio performance against adverse movements in the portfolio Gamma or index prices. Employing RL techniques to hedge such portfolios allows for the dynamic adjustment of hedging positions (Kolm and Ritter, 2019), ensuring that the portfolio remains well-protected against market fluctuations while capitalizing on the benefits of multiple underlying assets. Other researchers have also demonstrated that RL is an attractive alternative to traditional hedging strategies based on the performance and Profit and Loss (P⁢n⁢L𝑃𝑛𝐿PnLitalic_P italic_n italic_L) distribution as compared to the baseline Delta neutral and Delta-Gamma neutral strategies (Cao et al., 2020; Kolm and Ritter, 2019; Cao et al., 2023). Recently, (Cui et al., 2023) proposed a method for pricing autocallable notes and employing a Delta neutral strategy for hedging. They emphasized that hedging autocallable notes is complex due to their intricate structure. Similarly, we investigate autocallable structured note hedging using RL which outperforms the traditional hedging strategies (Delta neutral and Delta-Gamma neutral). In particular, the structured products, which derive their value from multiple underlying assets, can offer diversified risk exposure and enhanced return potential. Employing RL techniques to hedge such portfolios allows for the dynamic adjustment of hedging positions, ensuring that the portfolio remains well-protected against market fluctuations while capitalizing on the benefits of multiple underlying assets. In our approach, we use Distributed Distributional DDPG (D4PG) algorithm with Quantile Regression (QR) to learn an optimal policy for hedging. This distributional RL enables a more nuanced understanding of uncertainty and risk in the learning process. Specifically, we use an American option as hedging instruments and the trained RL agent selects an action which quantifies the amount of hedging that needs to be performed. We compare the P⁢n⁢L𝑃𝑛𝐿PnLitalic_P italic_n italic_L distribution, Value at Risk (V⁢a⁢R𝑉𝑎𝑅VaRitalic_V italic_a italic_R), and Conditional Value at Risk (C⁢V⁢a⁢R𝐶𝑉𝑎𝑅CVaRitalic_C italic_V italic_a italic_R) of different hedging strategies and show that the RL algorithm not only reduces 95%⁢V⁢a⁢Rpercent95𝑉𝑎𝑅95\%VaR95 % italic_V italic_a italic_R, but also makes the P⁢n⁢L𝑃𝑛𝐿PnLitalic_P italic_n italic_L distribution more symmetric and retains positive returns. By employing our advanced pricing model, we have achieved very fast results, significantly reducing the time required for pricing and eventually hedging. Furthermore, our RL-based hedging strategy has demonstrated superior performance compared to traditional Gamma hedging, offering more effective risk mitigation. This approach not only simplifies the pricing and hedging processes but also enhances the overall efficiency and robustness of portfolio management in the face of dynamic market conditions. The prices for the underlying asset are generated using Geometric Brownian motion (GBM) model. Our specific contributions are the following: (1) We propose a machine learning-based option pricer for autocallable structured notes that computes prices 250 times faster than traditional Monte Carlo methods. (2) We use a distributional RL based method to hedge a portfolio containing one short autocallable note under multiple underlying assets. We conduct a thorough analysis and introduce a novel objective function which helps to learn a generalized policy that beats traditional hedging strategies. (3) We compare with traditional hedging strategies, including Delta neutral and Delta-Gamma neutral and show that RL hedging outperforms these traditional methods."
https://arxiv.org/html/2411.02125v1,Revisiting K-mer Profile for Effective and Scalable Genome Representation Learning,"Obtaining effective representations of DNA sequences is crucial for genome analysis. Metagenomic binning, for instance, relies on genome representations to cluster complex mixtures of DNA fragments from biological samples with the aim of determining their microbial compositions. In this paper, we revisit k𝑘kitalic_k-mer-based representations of genomes and provide a theoretical analysis of their use in representation learning. Based on the analysis, we propose a lightweight and scalable model for performing metagenomic binning at the genome read level, relying only on the k𝑘kitalic_k-mer compositions of the DNA fragments. We compare the model to recent genome foundation models and demonstrate that while the models are comparable in performance, the proposed model is significantly more effective in terms of scalability, a crucial aspect for performing metagenomic binning of real-world datasets.","Microbes influence all aspects of our environment, including human health and the natural environment surrounding us. However, understanding the full impact of the microbes through the complex microbial communities in which they exist, requires insight into the composition and diversity of these communities [19]. Metagenomics involves the study of microbial communities at the DNA level. However, sequencing a complex microbial sample using current DNA sequencing technologies [27] rarely produces full DNA sequences, but rather a mixture of DNA fragments (called reads) of the microbes present in the sample. In order to recover the full microbial genomes, a subsequent binning/clustering step is performed, where individual DNA fragments are clustered together according to their genomic origins. This process is also referred to as metagenomic binning [19, 9]. The fragments being clustered during the binning process consist of contiguous DNA sequences (contigs) obtained from the reads through a so-called assembly process [29] (contigs are generally longer and less error-prone than the reads). Metagenomic binning typically involves comparing and clustering DNA fragments using a distance metric in a suitable genome representation space. State-of-the-art methods for metagenomic binning typically rely on representations that include or build on top of the k𝑘kitalic_k-mer profiles of the contigs [17, 12, 18]. These representations have mostly been studied from an empirical perspective [6], although general theoretical analyses into their representational properties have also been considered [25]. With k=4𝑘4k=4italic_k = 4 (i.e., tetra-nucleotides) a 256256256256-dimensional vector is used to describe the genome, each entry in the vector encoding the frequency of a specific k𝑘kitalic_k-mer (e.g., ACTG, ATTT) in the genome sequence. The k𝑘kitalic_k-mer representation of a sequence thus has a fixed size and is independent of sequence length and downstream analysis tasks, providing a computationally efficient representation. Figure 1: Number of parameters of the different evaluated models (log10subscript10\log_{10}roman_log start_POSTSUBSCRIPT 10 end_POSTSUBSCRIPT-scale). A more recent and popular line of research focuses on using approaches inspired by modern Large Language Models (LLMs) to derive more powerful representations of genome fragments. The goal is to replicate the success of LLMs used in natural language processing for genomic data. These models, known as genome foundation models, have seen numerous versions proposed recently [32, 16, 31]. Similarly to popular LLMs, existing genome foundation models utilize next-token prediction or masked-prediction approaches within transformer-based architectures, where the tokens to be predicted are the nucleotides composing the genome fragments. These models, akin to LLMs, enable trainable and contextualized representations that can be defined either in a task-dependent or task-independent manner [32]. According to standardized benchmarks, the embeddings derived from these foundation models have the potential to offer substantial improvements over those based on k𝑘kitalic_k-mers [32]. However, these embeddings are also computationally far more intensive, reducing their scalability in light of the massive amounts of data generated by modern sequencing technologies. For instance, [23] conducted k𝑘kitalic_k-mer-based metagenomic binning [28, 9] on samples from wastewater treatment plants, encompassing approximately 1.9×10121.9superscript10121.9\times 10^{12}1.9 × 10 start_POSTSUPERSCRIPT 12 end_POSTSUPERSCRIPT base pairs and resulting in the recovery of over 3700 high to medium quality clusters/metagenome-assembled genomes (MAGs). In this paper, we demonstrate how k𝑘kitalic_k-mer-based embeddings of genome fragments provide a scalable and lightweight alternative to genome foundation models. We revisit the theoretical basis of k𝑘kitalic_k-mers and offer a theoretical characterization of the identifiability of DNA fragments based on their k𝑘kitalic_k-mer profiles. For non-identifiable fragments, we establish lower and upper bounds on their edit distance using the l1subscript𝑙1l_{1}italic_l start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT distance between their respective k𝑘kitalic_k-mer profiles. These findings offer theoretical justifications for k𝑘kitalic_k-mer-based genome representations and hold potential significance beyond the scope of this study. Building on these theoretical insights, we propose a simple and lightweight model for learning embeddings of genome fragments using their k𝑘kitalic_k-mer representations. We empirically assess the proposed embeddings on metagenomic binning tasks and compare their performance with large state-of-the-art genome foundation models. Our findings indicate that, while both sets of models produce comparable quality in terms of the MAGs recovered, the proposed models require significantly fewer computational resources. Figure 1 demonstrates this by showing the number of parameters in our k𝑘kitalic_k-mer embedding methods compared to those in the state-of-the-art genome foundation models. The k𝑘kitalic_k-mer-based embedding approaches involve models with several orders of magnitude fewer parameters. The main contributions of the paper can be summarized as follows: • We provide a theoretical analysis of the k𝑘kitalic_k-mer space, offering insights into why k𝑘kitalic_k-mers serve as powerful and informative features for genomic tasks. • We demonstrate that models based on k𝑘kitalic_k-mers remain viable alternatives to large-scale genome foundation models. • We show that scalable, lightweight models can provide competitive performance in the metagenomic binning task, highlighting their efficiency in handling complex datasets. The datasets and implementation of the proposed architectures can be found at the following address: https://github.com/abdcelikkanat/revisitingkmers."
https://arxiv.org/html/2411.02083v1,"Regress, Don’t Guess – A Regression-like Loss on Number Tokens for Language Models","While language models have exceptional capabilities at text generation, they lack a natural inductive bias for emitting numbers and thus struggle in tasks involving reasoning over quantities, especially arithmetics. This has particular relevance in scientific datasets where combinations of text and numerical data are abundant. One fundamental limitation is the nature of the CE loss, which assumes a nominal (categorical) scale and thus cannot convey proximity between generated number tokens. As a remedy, we here present two versions of a number token loss. The first is based on an Lpsubscript𝐿𝑝L_{p}italic_L start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT loss between the ground truth token value and the weighted sum of the predicted class probabilities. The second loss minimizes the Wasserstein-1 distance between the distribution of the predicted output probabilities and the ground truth distribution. These regression-like losses can easily be added to any language model and extend the CE objective during training. We compare the proposed schemes on a mathematics dataset against existing tokenization, encoding, and decoding schemes for improving number representation in language models. Our results reveal a significant improvement in numerical accuracy when equipping a standard T5 model with the proposed loss schemes.","Figure 1: Left: xVal [7] decodes numbers through a regression head carried alongside the regular token head, gated through the [NUM] token (figure reproduced with permission). Right: Instead, the Number Token Loss (NTL) circumvents the need for two heads and allows the computation of a regression loss directly on the token head. We propose two schemes to achieve this: ℒℒ\mathcal{L}caligraphic_LNTL-MSE (right) leverages a dot product of the values of the number tokens and their class probabilities. The ℒℒ\mathcal{L}caligraphic_LNTL-WAS (left) uses the Wasserstein-1 distance of the (sorted) number token labels and their class probabilities. As coined by Thawani et al. [14], numbers in natural texts are ubiquitous and important, yet systematically neglected by language models (LMs). Even worse, while Transformers [15] were invented for NLP, they have permeated various scientific domains (chemistry, biology, etc [2, 8, 1]), where tabular/numerical data is more prevalent than in NLP and often even fundamental for constructing task definitions: Molecules are labeled with drug efficacy, chemical reactions with yield, and synthesis procedures are natural text interspersed with quantities and times. Still, LMs notoriously struggle even with simple arithmetic tasks like three-digit multiplication [5] for multiple reasons: 1. Tokenization: Standard subword tokenization splits numbers into arbitrary tokens, disrupting their structure. Mitigation strategies include scientific notation [18] or digit-level tokenization [6], which may also preserve the decimal order of each digit [1]. 2. Embedding: Canonically, the model has to recover the structure of numbers from data because the embeddings of numerical tokens are learned like any other token. Countless flavors of numeracy-preserving word embeddings exist [13, 1, 7], often akin to positional encodings. 3. Training objective: The standard cross-entropy (CE) loss assumes a nominal scale, thus it fails to convey the proximity between numbers, effectively inducing a semi-supervised setting. For example, predicting a [3] instead of a [2] token will not generally induce lower loss than a [9]. This problem has been surprisingly neglected and is the focus of this work. Here, we aim to equip LMs with better inductive biases to handle combinations of textual and numerical data, such as math word problems or scientific datasets. In particular, we propose two versions of a regression loss on number tokens that respect numerical proximity (cf. Figure 1 right) and can be effectively combined with regular CE. The first version of this loss computes the Mean Squared Error (MSE) between the sum of the predicted class probabilities, weighted by their respective numerical token value, and the numerical token value of the label. The second version computes the Wasserstein distance between the distribution of the predicted number probabilities and the ground truth distribution, which is the one-hot encoding of the label. We integrate these improved training objectives with existing solutions for tokenization and embedding, in particular the Regression Transformer [1]. We evaluate all methods on a subset of the mathematical-question-answer dataset from DeepMind [12]. Prior art for joint language-number modeling suggested the use of verifiers [3, 10], calculators (typically: Python interpreters), or chain-of-thought (CoT) reasoning [19] to yield improved performance in Large Language Models (LLMs). We argue that all such strategies avoid the fundamental, underlying problem (i.e., number representation in LMs is poor) by reformulating the task, trying to correct answers a posteriori with calculators, or using significantly more compute (CoR). Therefore, we herein intentionally attempt to improve a classic, relatively small encoder-decoder LM with up to 220M parameters, namely T5 [11]."
https://arxiv.org/html/2411.01404v1,Hyperbox Mixture Regression for Process Performance Prediction in Antibody Production,"This paper addresses the challenges of predicting bioprocess performance, particularly in monoclonal antibody (mAb) production, where conventional statistical methods often fall short due to time-series data’s complexity and high dimensionality. We propose a novel Hyperbox Mixture Regression (HMR) model which employs hyperbox-based input space partitioning to enhance predictive accuracy while managing uncertainty inherent in bioprocess data. The HMR model is designed to dynamically generate hyperboxes for input samples in a single-pass process, thereby improving learning speed and reducing computational complexity. Our experimental study utilizes a dataset that contains 106 bioreactors. This study evaluates the model’s performance in predicting critical quality attributes in monoclonal antibody manufacturing over a 15-day cultivation period. The results demonstrate that the HMR model outperforms comparable approximators in accuracy and learning speed and maintains interpretability and robustness under uncertain conditions. These findings underscore the potential of HMR as a powerful tool for enhancing predictive analytics in bioprocessing applications.","Regression models have found widespread application in various fields, including robot controllers (Li et al., 2020), motion prediction (Zhong et al., 2022), and time series forecasting (Lemke and Gabrys, 2010; Ruta et al., 2011; Zhang et al., 2024). Predicting bioprocess performance presents a complex multivariate time-series challenge that conventional statistical methods often struggle to address (Gangadharan et al., 2021). While numerous types of research have focused on data pre-processing techniques—such as imputation, visualization, and feature selection—choosing a suitable predictive model remains a critical hurdle (Khuat et al., 2024). This paper aims to tackle these challenges by developing a machine-learning model specifically designed for bioprocess performance prediction. The growing complexity of time-series data has led to an increasing reliance on machine learning (ML) techniques to overcome the limitations of traditional statistical methods. These conventional methods often struggle with the inherent correlations in time-series observations, resulting in potential inaccuracies in predictions (Gangadharan et al., 2019). In contrast, ML methods have gained popularity for extracting essential information from time-series data, providing more robust and accurate insights (Lim et al., 2023). This trend underscores a significant shift towards leveraging ML as a powerful tool for addressing the challenges inherent in time-series analysis. Recent studies have demonstrated the effectiveness of ML algorithms in predicting critical quality attributes (CQAs) and process outcomes. For instance, Khuat et al. (2024) highlights the growing applications of ML in biopharmaceuticals, emphasizing its role in real-time monitoring and optimization of both upstream and downstream processes. By leveraging large datasets generated from production, ML models can identify patterns and relationships that are not easily discernible through conventional statistical methods. In addition, Waight et al. (2023) argued that identifying favorable biophysical properties is essential in the preclinical development of protein therapeutics, but predicting these properties remains challenging. They introduced an automated machine learning workflow that analyzes computationally derived features to build predictive models for key developability factors like hydrophobicity and poly-specificity in IgG molecules. More studies have been reviewed in Lim et al. (2023); Khuat et al. (2024). This approach addresses some of the challenges in preclinical development, where predicting favorable biophysical properties is crucial yet difficult. One of ML’s key advantages is its ability to handle bioprocess data’s high dimensionality and complexity. ML algorithms, such as random forests, support vector machines, and deep learning models, have been applied to predict CQAs and key performance indicators (KPIs) in monoclonal antibody (mAb) production. For instance, a recent study reports a deep learning-based 2D-convolutional neural network (2D-CNN) designed to predict various downstream processing attributes, including Protein A mAb elute concentration and aggregate percentages, from routinely collected process data (Alam et al., 2024). According to Alam et al. (2024), their model outperformed existing approaches, achieving a mean percentage deviation of less than 3%percent33\%3 % in experimental validation. In another study, Lai employed machine learning to predict therapeutic antibody aggregation rates and viscosity at high concentrations (150m⁢g/m⁢lsuperscript150𝑚𝑔𝑚𝑙150^{mg/ml}150 start_POSTSUPERSCRIPT italic_m italic_g / italic_m italic_l end_POSTSUPERSCRIPT), focusing on preclinical and clinical-stage antibodies. Lai et al. (2022) employed a k-nearest neighbors regression model and achieved a high correlation for predicting aggregation rates using features derived from molecular dynamics simulations. Moreover, Schmitt et al. (2023) employed an Artificial Neural Network (ANN) to predict mAb viscosity. High concentrations of mAb solutions can increase viscosity, affecting protein purification and administration. They utilized an ANN and combined experimental factors and simulated data to predict and model the viscosity of mAbs. Additionally, new research by Makowski et al. (2024) has shown the use of a transparent machine-learning model for predicting antibody (IgG1) variants with low viscosity based on the sequences of their variable (Fv) regions. This model not only identifies antibodies at risk for high viscosity with relatively high accuracy but also enables the design of mutations that reduce antibody viscosity, confirmed experimentally. According to Makowski et al., their model demonstrates high accuracy and exhibits excellent generalization (Makowski et al., 2024). These advancements underscore the growing role of ML and deep learning in enhancing the efficiency and quality of mAb production processes. In terms of predicting mAb stability, recent work has focused on chemical modifications, such as methionine oxidation, that can impair antibody potency. A study developed a highly predictive in silico model for methionine oxidation by extracting features from mAb sequences, structures, and dynamics, utilizing random forests to identify crucial predictive features (Sankar et al., 2018). This work emphasizes the potential for computational tools to complement experimental methods in therapeutic antibody discovery. However, despite the potential benefits, challenges remain in the widespread adoption of ML in bioprocessing. Issues such as limited samples, high-dimensional data, data quality, model interpretability, and robust validation protocols must be addressed to ensure reliable application in industrial settings (Gangadharan et al., 2021, 2019; Khuat et al., 2024). In addition, the model should be capable of making inferences under uncertain conditions and, ideally, provide explanations for its predicted outcomes (Lim et al., 2023). While neuro-fuzzy regression models (Jang, 1993; de Campos Souza, 2020) are effective in managing uncertainty, they face challenges in the high-dimensional data space (Pramod et al., 2019). Conversely, regression models such as Radial basis functions (RBF) and ANN, which excel in handling high-dimensional data spaces (Sung, 2004), struggle to address the inherent uncertainty in the problem. It motivates us to develop a neuro-fuzzy system that handles uncertainty within high-dimensional data spaces for bioprocess performance prediction. RBF is a regression and classification model that can be employed when the relationship between variables is unknown. Key advantages of RBF models are guaranteed learning algorithm through linear least squares optimization and efficiency in dealing with high dimensional data (Walczak and Massart, 1996). This model consists of an unsupervised clustering framework that helps partition the input feature space, then estimate the target signal using least squares optimization (Tagliaferri et al., 2001; Walczak and Massart, 1996). However, a significant challenge for RBF networks is determining the optimal number and distribution of nodes in the hidden layer (Walczak and Massart, 1996). Another class of powerful neuro-fuzzy machine learning models, which are of particular interest to us, are based on hyperbox fuzzy sets originally introduced by Simpson in the 1990s Simpson (1993) and then later improved, extended, and generalized by Gabrys Gabrys and Bargiela (2000); Gabrys (2002a, b, 2004) as well as a large number of other researchers Khuat et al. (2021). In its original paper, Simpson introduced the Fuzzy Min-Max (FMM) algorithm as an unsupervised clustering method for pattern clustering (Simpson, 1993). FMM is a neuro-fuzzy algorithm that integrates fuzzy inference systems and adaptive neural networks. Employing a fuzzy inference system facilitates the creation of a neuro-fuzzy system capable of handling uncertainty. Additionally, using an adaptive neural network structure allows one to use learning approaches to find optimal parameters (Cortés-Antonio et al., 2020). Furthermore, having the ability to extract fuzzy if-then rules from the network architecture means that it is no longer a black box model. As mentioned earlier, unsupervised clustering methods can be utilized for partitioning of an input space in neural network based regression (e.g. RBF) models and hence there have also been some examples of FMM-based regression models. Simpson and Jahns introduced an FMM-based framework for function approximation (Simpson and Jahns, 1993). In their approach, the authors utilized the FMM clustering method to partition the input feature space and used the hyperbox fuzzy sets representing clusters and the associated trapezoidal fuzzy membership functions as basis functions to estimate the target output. Similarly to the RBF networks, the output was a weighted combination of hyperbox fuzzy sets membership values (Simpson and Jahns, 1993). In another study, Tagliaferri developed an innovative FMM-based model for function approximation, enhancing the FMM clustering algorithm for better feature space partitioning (Tagliaferri et al., 2001). Tagliaferri asserted that batch learning algorithms, which partition the feature space using the entire dataset, help eliminate the dependence on the order of data presentation. According to Tagliaferri, this adjustment significantly enhances the model’s performance (Tagliaferri et al., 2001). Additionally, Brouwer proposed a novel automatic learning algorithm designed for the FMM-based function approximation model (Brouwer, 2005). Given that the loss function of the FMM is not differentiable and thus incompatible with gradient descent optimization, Brouwer implemented a helper neural network to approximate the loss function. This network allowed the application of the gradient descent algorithm to adjust the network parameters. Brouwer’s new learning framework demonstrated the capability to achieve superior optimal values compared to conventional training algorithms (Brouwer, 2005). All of these RBF-like models involve clustering-based input feature space partitioning, which helps manage high-dimensional input data and overcome a curse of dimensionality problem quite common in other neuro-fuzzy regression methods like ANFIS Jang (1993). Despite some initial interest, successees and many attractive features there have been far fewer hyperbox regression models than their classification/clustering counterparts Khuat et al. (2021). In this paper, we therefore introduce a novel neuro-fuzzy structure and learning procedure called Hyperbox Mixture Regression (HMR) that utilizes the hyperbox input space partitioning to overcome the curse of dimensionality problem while taking full advantage of its universal approximator capabilities and model transparency. The proposed method employs the hyperbox idea in constructing the basis functions in the first layer. It is worth noting that the new HMR structure is more straightforward than conventional neuro-fuzzy structures like ANFIS Jang (1993) and has a lower computational complexity to produce the output. A hyperbox is a convex n-dimensional box in the feature space that assigns a full membership value to patterns within it (Simpson, 1992; Khuat et al., 2021) and is defined by its maximum and minimum points (Simpson, 1992). The utilization of hyperboxes for the input space partitioning offers notable advantages. A key benefit is their ability to learn from an input sample in a single-pass process, leading to a significant boost in the learning speed of the system (Khuat and Gabrys, 2021; Gabrys and Bargiela, 2000; Gabrys, 2002a). Moreover, the learning framework of hyperboxes is free from limitations such as being trapped in a local minimum or divergence due to the existence of outliers. Additionally, employing hyperboxes enables the system to identify essential basis functions, reducing overall complexity, particularly in high dimensions. The ability of the HMR model to infer and explain under uncertain conditions using the generated hyperbox fuzzy sets is crucial in industrial fields such as mAb production, where measurement noise is frequently associated with process parameters. This paper will apply the proposed HMR model to predict the performance of mAb production processes based on the critical process parameters used as input features (Gangadharan et al., 2021). The empirical dataset encompasses information from 106 bioreactors, capturing biological parameters over 15 cell culture days. The model’s primary objective is to predict values of key performance indicators, such as Viable Cell Density (VCD) and mAb concentration, for antibody production processes within the subsequent two days. Consequently, the development involves the creation of two distinct predictive models: one for forecasting antibody production one day ahead and another for predicting the production two days ahead. The main contributions of this paper can be summarized as follows: 1. Introducing a new neuro-fuzzy model structure for bioprocess performance prediction to address the limited samples, high-dimensional data space, and transparency limitations under uncertain conditions. 2. Introducing a novel learning procedure that learns in a single input data pass process and generates essential basis functions for regression, effectively increasing the learning rate and reducing system complexity in high dimensional problems. 3. Employing a dynamically weighted combination of local linear regressors, associated with each hyperbox, in order to increase accuracy and decrease network complexity, especially in non-linear problems. 4. Introducing a normalization layer in the proposed structure to reduce the risk of numerical instability in the next layers. 5. Forecasting the key performance indicators of antibody production processes over the next two cell culture days. The rest of the paper is organized as follows. The next section describes in detail and illustrates the proposed model structure and training algorithm. In the third section, comprehensive experiments evaluate the proposed model in different scenarios. Finally, section four concludes the paper and highlights the key contributions."
https://arxiv.org/html/2411.00914v1,"AAD-LLM: Adaptive Anomaly Detection Using Large Language Models††thanks:This material is based upon work supported by the Engineering Research and Development Center - Information Technology Laboratory (ERDC-ITL) under Contract No. W912HZ23C0013. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the ERDC-ITL.","For data-constrained, complex and dynamic industrial environments, there is a critical need for transferable and multimodal methodologies to enhance anomaly detection and therefore, prevent costs associated with system failures. Typically, traditional PdM approaches are not transferable or multimodal. This work examines the use of Large Language Models (LLMs) for anomaly detection in complex and dynamic manufacturing systems. The research aims to improve the transferability of anomaly detection models by leveraging Large Language Models (LLMs) and seeks to validate the enhanced effectiveness of the proposed approach in data-sparse industrial applications. The research also seeks to enable more collaborative decision-making between the model and plant operators by allowing for the enriching of input series data with semantics. Additionally, the research aims to address the issue of concept drift in dynamic industrial settings by integrating an adaptability mechanism. The literature review examines the latest developments in LLM time series tasks alongside associated adaptive anomaly detection methods to establish a robust theoretical framework for the proposed architecture. This paper presents a novel model framework (AAD-LLM) that doesn’t require any training or finetuning on the dataset it is applied to and is multimodal. Results suggest that anomaly detection can be converted into a ”language” task to deliver effective, context-aware detection in data-constrained industrial applications. This work, therefore, contributes significantly to advancements in anomaly detection methodologies.","We rely on the operation of a variety of engineered systems, which degrade with use and may lead to failures of varying degrees of severity. These failures include unexpected stoppages, product waste, damage to equipment, and bodily harm which have consequences that range from annoying to disastrous. Maintenance practices are crucial to prevent such failures. Condition-based maintenance (CBM) involves performing maintenance actions based on the conditions of a system. Maintenance decisions based on the conditions of a system are often desirable because they allow for proactive and targeted actions. Predictive maintenance (PdM) leverages machine learning to enhance this decision-making process. PdM becomes more challenging under common real world conditions. Sensor data collected from machines is often non-stationary due to a combination of factors such as varying operational settings and individual machine deterioration [1]. This causes heterogeneous relationships between the sensor data and system health; thereby requiring that the normative profile used to identify degradation be updated regularly [2, 3]. We thus do not employ traditional PdM methods, instead employing an adaptive approach. The former cannot account for shifts in sensor data statistical characteristics, while the latter can, all while maintaining high fault detection accuracy. Unique production system structures and domain-specific constraints necessitate tailored approaches to effectively deploy PdM in diverse industrial settings. The use of expert knowledge into our target use-case facilitate a robust and domain-specific PdM implementation commensurate to these complexities. However, domain-specific knowledge (e.g. process parameter optimal ranges, equipment specifications, etc.) cannot usually be applied across domains and therefore would limit model application across industrial settings. Therefore, retraining or finetuning on the applied dataset with related domain-specific knowledge would typically be required. It is ideal for models to require infrequent training on a limited dataset yet yield robust generalization capabilities. This is because some critical assets are not allowed to run to failure; thus, event data needed to fine-tune or retrain some machine learning algorithms may be scarce [4]. Transferable models that excel in ”few-shot” and ”zero-shot” scenarios across related domains appear promising. Recent work suggests that pretrained Large Language Models (LLMs) offer noteworthy few/zero-shot capabilities and transferability [5, 6, 7]. Furthermore, the extension of LLMs beyond natural language to the time series domain showcases their broader potential [8, 9]. In particular, the application of pretrained LLMs to the problem of anomaly detection for PdM on time series data can improve the transferability of other approaches in data-constrained environments. To optimize PdM model effectiveness, it is crucial to develop models that are both adaptable and transferable. Adaptable models can adjust to changing conditions, ensuring continued relevance over time. Transferability enables these models to be applied across diverse systems and domains, increasing usability and practicality. By combining adaptability and transferability, PdM models become versatile tools that can evolve with operational environments and be leveraged on a variety of industrial datasets. Furthermore, a model that is optimized to detect anomalies across diverse input sources could enable more synergistic forecasting and foster more collaborative decision-making between the model and plant operators. The main contributions of this work are as follows: • We explore repurposing pretrained LLMs for the PdM use-case. More specifically, pretrained LLMs are explored for use in anomaly detection within manufacturing time series data. Thus, we aim to examine LLMs’ efficacy beyond conventional forecasting applications. • We present a novel anomaly detection framework (AAD-LLM) utilizing pretrained LLMs for improved transferability in data-constrained contexts. The improved transferability is shown to reduce the need to retrain between domains and systems. Additionally, the framework is shown to enable the enrichment of input time series data with semantics to deliver more collaborative decision-making between the model and plant operators. • We leverage an adaptability mechanism that enables the model to adjust to evolving conditions, consequently enhancing detection accuracy. The remaining sections of this paper are as follows. Section II discusses the background and foundational work for our proposed methodology. Section III examines the state-of-the-art in LLM time series tasks and adaptive anomaly detection methods. Section IV provides insight on the AAD-LLM architecture and methodology. Section V explains experimental results and implications of findings. Finally, Section VI concludes the paper and discusses limitations for future work."
https://arxiv.org/html/2411.00563v1,Simulate and Optimise: A two-layer mortgage simulator for designing novel mortgage assistance products,"We develop a novel two-layer approach for optimising mortgage relief products through a simulated multi-agent mortgage environment. While the approach is generic, here the environment is calibrated to the US mortgage market based on publicly available census data and regulatory guidelines. Through the simulation layer, we assess the resilience of households to exogenous income shocks, while the optimisation layer explores strategies to improve the robustness of households to these shocks by making novel mortgage assistance products available to households. Households in the simulation are adaptive, learning to make mortgage-related decisions (such as product enrolment or strategic foreclosures) that maximize their utility, balancing their available liquidity and equity. We show how this novel two-layer simulation approach can successfully design novel mortgage assistance products to improve household resilience to exogenous shocks, and balance the costs of providing such products through post-hoc analysis. Previously, such analysis could only be conducted through expensive pilot studies involving real participants, demonstrating the benefit of the approach for designing and evaluating financial products.","††footnotetext: Accepted at the 5th ACM International Conference on AI in Finance 2024 Designing financial products for assisting homeowners during mortgage related financial distress is a complex task due to the intricacies of the mortgage system. Traditionally, such development has relied on conducting human pilot studies to analyse the effectiveness and impact of such products in vivo. However, such pilot studies are typically expensive and as a result, are limited to small-scale analysis, restricting the exploratory capabilities. To address these issues, a way to conduct such analysis in vitro is needed. This paper introduces a computational approach to optimise and evaluate novel mortgage assistance products using a two-layer reinforcement learning (RL) and simulation structure. In this approach, the outer (product) layer is responsible for optimising or selecting new product configurations, while the inner (simulation) layer employs agent-based simulation to assess the impact of these product configurations in a simulated mortgage environment. With this approach, we can conduct extensive large-scale analysis across a range of products in vitro. The benefits of the simulation are two-fold: First, simulation enables the evaluation of a broader range of product configurations across different scenarios, at a significantly faster pace and substantially lower cost compared to traditional studies. Second, we can introduce and evaluate these products in a controlled setting, mitigating the risk of introducing potentially harmful financial products into the real market. The simulation serves as an exploratory platform before performing studies or introducing products in vivo. The specific contributions of the paper are: • A novel two-layer approach for optimising products through multi-agent simulation • An extension of a mortgage servicing ABM ((Garg et al., 2024)) with a more robust representation able to perform counterfactual analysis through product conditioned policy learning • A generic parameterised financial product configuration compatible with the conditional policy learning • Analysis into the financial resilience of households and costs of providing cover under different product configurations The remainder of the paper is organised as follows. Section 2 provides an overview of related work and background to the area. Section 3 introduces the novel two-layer approach. The impact of different mortgage assistance products is analysed in Section 4. Discussion and conclusions are presented in Section 5."
https://arxiv.org/html/2411.00554v1,Differentiable Physics-based System Identification for Robotic Manipulation of Elastoplastic Materials,"Robotic manipulation of volumetric elastoplastic deformable materials, from foods such as dough to construction materials like clay, is in its infancy, largely due to the difficulty of modelling and perception in a high-dimensional space. Simulating the dynamics of such materials is computationally expensive. It tends to suffer from inaccurately estimated physics parameters of the materials and the environment, impeding high-precision manipulation. Estimating such parameters from raw point clouds captured by optical cameras suffers further from heavy occlusions. To address this challenge, this work introduces a novel Differentiable Physics-based System Identification (DPSI) framework that enables a robot arm to infer the physics parameters of elastoplastic materials and the environment using simple manipulation motions and incomplete 3D point clouds, aligning the simulation with the real world. Extensive experiments show that with only a single real-world interaction, the estimated parameters, Young’s modulus, Poisson’s ratio, yield stress and friction coefficients, can accurately simulate visually and physically realistic deformation behaviours induced by unseen and long-horizon manipulation motions. Additionally, the DPSI framework inherently provides physically intuitive interpretations for the parameters in contrast to black-box approaches such as deep neural networks.","Figure 1: The proposed system identification framework enables a robot to interact with elastoplastic material via simple manipulation motions (orange box) and then identify the physics parameters of the real-world manipulation dynamics. The parameters are found using gradients computed, through differentiable simulation, from a differentiable point-cloud-based similarity function between the real and simulated observations of the manipulated material (blue and red boxes). These parameters then enable accurate simulations that allow the real-world grounding of motion planning, trajectory optimisation or policy learning techniques (cyan box). Despite the recognised importance of robotic manipulation of deformable materials, this topic remains underexplored, particularly when it comes to high-precision manipulation of volumetric elastoplastic materials. A primary challenge in this area arises from the materials’ infinite degrees of freedom (DoFs), leading to highly unpredictable deformation dynamics. The intrinsic complexity of these dynamics inhibits the direct application of conventional robotic motion planning methods, which typically require explicit physics models for all concerned objects Latombe (2012). Learning approaches, such as reinforcement learning (RL), often involve training an agent to learn to interpret its perception and take actions through inefficient trial & error in a realistic physics-based simulation Collins et al. (2021); Kroemer et al. (2021), which is both challenging and largely unavailable when it comes to deformable materials. In contrast to the well-studied rigid body dynamics in robotics, where motions can be predicted and controlled using well-defined equations of motion and deterministic models Featherstone (2014), deformable materials do not follow such straightforward patterns. Directly applying these methods is problematic because it is extremely difficult and often infeasible to accurately model and perceive real-world elastoplastic materials and measure the underlying physics parameters that govern their motions and deformations Arriola-Rios et al. (2020); Yin et al. (2021). As a result, achieving high-precision manipulation for such materials with motion planning or data-driven techniques is challenging due to the high computational cost and the lack of techniques to capture the dynamics accurately. To close this gap, this research proposes a Differentiable Physics-based System Identification (DPSI) framework for the robotic manipulation of volumetric elastoplastic materials. Our framework can efficiently estimate key physics parameters governing material deformation dynamics using minimal and simple manipulation motions. The estimated physics parameters enable accurate material simulation for long-horizon predictions of real-world elastoplastic deformation behaviours. The workflow of the proposed DPSI framework can be summarised as follows. As shown in Figure 1, a robot equipped with an in-hand 3D camera (Zivid) and three end-effectors is deployed to manipulate the elastoplastic object (e.g., play dough). Before manipulation, the robot takes multi-view point clouds of the object to minimise occlusions. These point clouds are used to create the initial particle system for the material point methods (MPM)-based simulation. The robot then performs a manipulation motion on the object and captures point clouds of the deformed state. Physics simulations of the same manipulation are run with the same initial state and motion, whose resultant particle states are compared to the real-world deformed state through variants of the Chamfer distance (CD) and the earth mover’s distance (EMD) loss. The parameters of the simulated physics models are updated after every simulation to minimise the loss. To facilitate fast optimisation, we use a differentiable simulator built on the TaiChi auto-differentiation mechanism, which allows automatic gradient computation from the losses and gradient-based optimisation for the physics parameters Hu et al. (2019, 2020). Our approach achieves unprecedented simulation-to-real alignment accuracy, characterised by the integration of the following novel features. High-fidelity physics: Unlike previous methods that either employ non-physics-based models (e.g., neural networks) or highly simplified material geometry representations (e.g., sparse keypoints), we use high-fidelity physics-based simulation powered by the MPM Jiang et al. (2016), which simulates materials as Lagrangian particles and keeps track of their positions and velocities. It achieves faster simulation by computing the motions, deformation gradients, and frictional contacts on a background Eulerian grid Stomakhin et al. (2013); Jiang et al. (2016); Gao et al. (2017); Hu et al. (2018). MPM-based simulations provide highly efficient and realistic simulation with high physical plausibility by closely following real-world physics laws such as Newton’s laws and elastic and plastic energy conservation models. Incomplete & noisy observations: Unlike existing works that rely on synthetic videos with complete sequences of perfect observations Murthy et al. (2020); Li et al. (2023); Kaneko (2024), our framework uses 3D point clouds to observe real-world object geometries. Capturing the full depth of an object during manipulation is impractical due to occlusions caused by the end-effector or environment. This means that only the point clouds before and after a manipulation motion are practical to obtain and informative enough to observe the full geometry of the deformed object in real-world experiments. In addition, real-world point clouds tend to suffer from inaccurately estimated camera matrices and sensory noises. Small data, short & simple motions: Using extensive and diverse manipulation motions to collect real-world deformation data is time-consuming and costly. Existing studies demand a significant number of real-world interactions or complete sequences of simulation videos to identify object deformations under various motions, yet still resulting in simulations with insufficient accuracy for real-world applications Lin et al. (2022); Shi et al. (2023); Li et al. (2023); Kaneko (2024). Our goal is to recover physics parameters that enable accurate predictions of long-horizon, unseen and complicated elastoplastic material manipulation dynamics, using minimal simple and short real-world interactions. Joint parameter estimation: We aim to jointly estimate the physics parameters provided by physics models. Besides Newton’s laws, we employ the fixed corotated elastic energy model Stomakhin et al. (2012), the von Mises plasticity model Jones (2009) and the dynamic friction model in our simulation. These lead to six key parameters: Young’s modulus E𝐸Eitalic_E, Poisson’s ratio ν𝜈\nuitalic_ν, yield stress 𝝈ysubscript𝝈𝑦\boldsymbol{\sigma}_{y}bold_italic_σ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT, material density ρ𝜌\rhoitalic_ρ, and the friction coefficients of the table ηtsubscript𝜂𝑡\eta_{t}italic_η start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and end-effectors ηmsubscript𝜂𝑚\eta_{m}italic_η start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT (assuming the three end-effectors share the same coefficient). The first four parameters primarily govern the deformation responses, while the last two handle frictional contacts. These parameters are heavily intertwined in governing the behaviours of the manipulated object and there is no intuitive solution to identify one of them without estimating the effects of other parameters. Therefore, we seek to identify these parameters simultaneously. Differentiable physics: Identifying the physics parameters in their discretised spaces via search or evolutionary algorithms is computationally slow due to the exponentially growing number of possible combinations as the discretisation becomes finer. While gradient-based optimisation methods offer faster convergence toward the minimum, it is infeasible with most physics simulations because many computation steps are not differentiable and these simulators do not support derivative computations. In this work, we explore the feasibility of optimising system parameters using gradients computed by differentiating loss functions through a physics simulator written by DiffTaiChi, a programming language tailored for GPU-accelerated parallel computation and automatic differentiation Hu et al. (2019, 2020). DiffTaichi generates derivative functions for simulation steps via source code transformation that retains arithmetic intensity and parallelism. It uses a memory-efficient tape to record the order of computation kernels for forward simulation and traverses their derivative functions in the backward order to generate gradients through the computation graph. We build DPSI upon DiffTaiChi and explore the feasibility of directly optimising several physics parameters jointly with gradients computed by differentiating point-cloud-based loss functions through the high-fidelity physics simulator. Substantial experiments demonstrate that our main contribution, DPSI, can achieve highly accurate simulation-to-reality alignment for elastoplastic materials manipulated by unseen, long horizon and complex motions using minimal simple and short interactions, and noisy and incomplete observations. Results show that when multiple solutions and parameter uncertainty exist, DPSI can provide physically intuitive parameter interpretations that can guide further system identification, model improvement, and motion adaptation. Statistics on the computation costs of DPSI indicates promising practical deployment of the DPSI framework. The rest of the article reviews related literature, presents formally our method and experiment results, and discusses limitations and future directions."
https://arxiv.org/html/2411.00241v2,"A Fast and Model Based Approach for Evaluating Task-Competence
of Antagonistic Continuum Arms","Soft robot arms have made significant progress towards completing human-scale tasks, but designing arms for tasks with specific load and workspace requirements remains difficult. A key challenge is the lack of model-based design tools, forcing advancement to occur through empirical iteration and observation. Existing models are focused on control and rely on parameter fits, which means they cannot provide general conclusions about the mapping between design and performance or the influence of factors outside the fitting data. As a first step toward model-based design tools, we introduce a novel method of analyzing whether a proposed arm design can complete desired tasks. Our method is informative, interpretable, and fast; it provides novel metrics for quantifying a proposed arm design’s ability to perform a task, it yields a graphical interpretation of performance through segment forces, and computing it is over 80x faster than optimization based methods. Our formulation focuses on antagonistic, pneumatically-driven soft arms. We demonstrate our approach through example analysis, and also through consideration of antagonistic vs non-antagonistic designs. Our method enables fast, direct and task-specific comparison of these two architectures, and provides a new visualization of the comparative mechanics. While only a first step, the proposed approach will support advancement of model-based design tools, leading to highly capable soft arms.","I INTRODUCTION Fluid-driven soft robot arms seek to capture the physical intelligence of muscular hydrostats, such as elephant trunks and octopus arms [1], in order to improve robot robustness and safety around humans [2]. These arms combine soft pneumatic actuators in parallel and in series to produce robotic arms that can bend in any direction at multiple points (see example in Fig. 1) [3, 4, 5, 6]. Recent works have taken major steps toward soft robotic arms that can complete human-scale tasks, exploring backbone-free antagonistic designs for stiffness control [7, 8, 9] and demonstrating contact rich tasks such as opening drawers [6], washing human subjects [10] and assisted eating [11]. However, arm demonstrations are consistently limited to small external loads, even with high actuator pressures [3, 8, 12, 6, 11]. The mechanical reason for limited force is not obvious, but it is evident even in recent, novel demonstrations: in Jiang et al., no tasks are demonstrated with additional weights [6], and in related work the arm’s maximum characterized load is 2.8N [6][12]. The arm in Guan et al. is shown, in a separate work, to deform its entire body length under 4N of tip loading [11, 13]. Investigations of the mapping between design and performance, e.g., load capacity across the workspace, are hampered by the lack of model-based design tools for soft robotic arm. Soft arm modeling efforts have focused on control, and use experimental, homogenized stiffness parameters that are specific to the design being tested [14, 15, 16]. Practically, an arm’s ability to complete a desired task has been determined by building and testing it. Figure 1: We propose a novel method to evaluate fluid-driven soft arm designs on their ability to meet a task shape at the task load (a). We mainly demonstrate our method with planar antagonistic arm designs (b). Each arm segment has two extending and two contracting actuators, and arms move via selective actuator pressurization. Figure 2: Overview of our mechanics model. A soft robot arm composed of two bellows and two muscles can be fully parameterized by its centerline twists go⁢isubscriptabsent𝑔𝑜𝑖\accentset{\scriptscriptstyle{\circleright}}{g}_{oi}start_OVERACCENT end_OVERACCENT start_ARG italic_g end_ARG start_POSTSUBSCRIPT italic_o italic_i end_POSTSUBSCRIPT and transformations between actuators go⁢αosubscriptsubscript𝑔𝑜𝛼𝑜{}_{o}g_{o\alpha}start_FLOATSUBSCRIPT italic_o end_FLOATSUBSCRIPT italic_g start_POSTSUBSCRIPT italic_o italic_α end_POSTSUBSCRIPT (a). When a tip load 𝐪tipsubscript𝐪tip\mathbf{q}_{\textrm{tip}}bold_q start_POSTSUBSCRIPT tip end_POSTSUBSCRIPT is applied, wrenches 𝐪isubscript𝐪𝑖\mathbf{q}_{i}bold_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT are induced along the arm’s backbone. To achieve static equilibrium, each actuator contributes a reaction force and moment to balance the load (b). We consider two types of actuators: contracting McKibben artificial muscles and extensile bellows actuators. The characterized force functions f⁢(ϵ,p)𝑓italic-ϵ𝑝f(\epsilon,p)italic_f ( italic_ϵ , italic_p ) of each actuator are also shown, with actuation regimes labeled according to [17] (c). Model-based design tools require model formulations that generalize across designs and methods of using those models to produce informative, interpretable results. Prior work [17] has developed generalizable models of soft arms, but significant gaps remain in developing approaches that utilize these models to provide insights about the mapping between arm design and task-specific performance. In this work, we develop a novel, model-based method for evaluating a proposed design’s ability to complete specific tasks. Our method is informative, interpretable, fast and provides a visualization of segment capabilities. We use it to concretely establish that antagonistic arms can complete a wider range of tasks than non-antagonistic arms, we provide novel insights for why, and computing it is 80x faster than existing methods. We first establish our underlying mechanics model in Section II, then introduce our method for analyzing task attainability in Section III, and finally apply our method to the comparison of arm designs in Section IV."
