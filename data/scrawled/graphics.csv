URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.05003v1,ReCapture: Generative Video Camera Controls forUser-Provided Videos using Masked Video Fine-Tuning,"Recently, breakthroughs in video modeling have allowed for controllable camera trajectories in generated videos. However, these methods cannot be directly applied to user-provided videos that are not generated by a video model. In this paper, we present ReCapture, a method for generating new videos with novel camera trajectories from a single user-provided video. Our method allows us to re-generate the reference video, with all its existing scene motion, from vastly different angles and with cinematic camera motion. Notably, using our method we can also plausibly hallucinate parts of the scene that were not observable in the reference video. Our method works by (1) generating a noisy anchor video with a new camera trajectory using multiview diffusion models or depth-based point cloud rendering and then (2) regenerating the anchor video into a clean and temporally consistent reangled video using our proposed masked video fine-tuning technique.","Recently, diffusion models have enabled significant advances in video generation and editing [35, 76, 31, 105, 103, 32, 94, 10, 109, 66, 107, 55, 99, 27], revolutionizing workflows in digital content creation. Camera control plays a vital role in practical applications of video generation and editing, allowing for greater customization and stronger user experience. Recent efforts have introduced camera control capabilities to video diffusion models [29, 2, 89, 38], yet, in this case the videos are entirely generated by the video model from a text-prompt and are neither captured in the real world, nor provided by a user. Effectively generating new videos with user-specified camera motion from an existing user-provided video that contains complex scene motion is still an open and challenging problem. The task is inherently ill-posed due to the limited amount of information in the reference video: one cannot know exactly how the scene looks like from all angles if there is not full knowledge of the scene’s 4D content. However, this does not preclude an approximate solution that is plausible and appreciated by users. Previous studies have shown promising results by generally assuming the availability of synchronized multi-viewpoint videos [64] and constructing 4D neural representations. Later works [87, 53, 45, 92, 80] enable 4D reconstruction using a single monocular video, but require accurate camera pose and depth estimation, and cannot capture content outside the original field of view. In this paper, we reformulate this problem as a video-to-video translation task. Camera Dolly [82] also develops a video-to-video pipeline, but requires 4D video data with different camera poses obtained via simulation, which limits it to in-domain scenes like driving or cubic objects. Given the challenge of obtaining paired videos in the wild with varying camera movements, it is hard to solve this problem with a video-to-video pipeline in an end-to-end manner and we separate it into two steps instead. Our approach leverages the prior knowledge of diffusion generative models, in both image and video domains, to effectively reangle the video as if filmed from the requested camera trajectory. For the first stage of our method, we want to generate an incomplete anchor video conditioned on the user-provided camera trajectory and reference video. We initially obtain a partial frame-by-frame depth estimation of the target video. We project each frame into 3D space using a depth estimator to obtain a sequence of point clouds. Then, we simulate the user-specified camera movement, which can include zoom, pan, and tilt, and render the point cloud sequence according to the new camera trajectory. This estimation is only partial since, as illustrated in Figure 4, these camera movements can introduce black areas outside the original video boundaries, cause some blurring due to the nature of point cloud projection and have poor temporal consistency since they are generated frame-by-frame. Another way to obtain the noisy anchor video, that uses recent advances in 3D reconstruction, is to use a multiview diffusion model [24] conditioned on camera pose and individual video frames. This method also results in an anchor video that has poor temporal consistency, along with blurring, artifacts and black areas outside the scene. Using this anchor video our method is able to generate a clean output with the desired camera trajectory. To achieve this we propose the novel technique of masked video fine-tuning. This technique consists of training a context-aware spatial LoRA and temporal motion LoRA on the known pixels from the generated anchor video, as well as from additional reference frame data. Specifically, the spatial LoRA is incorporated into the spatial layers of the video diffusion model and finetuned on augmented frames extracted from the source video. This enables the model to learn the subject’s appearance and the background context of the source video. The temporal LoRA enables the model to learn the scene motion with respect to the new camera trajectory, and is inserted into the temporal layers of the video diffusion model and finetuned using a masked loss on the anchor video. Unknown regions are masked, which excludes them from the loss computation, enabling the model to focus on meaningful and known regions and motion while ignoring the unknown areas. During inference, equipped with both video specific spatial and temporal LoRAs, the diffusion model can automatically fill the unknown regions of the anchor video with plausible content, leveraging the video diffusion model’s prior and the context provided by the spatial LoRA. It also significantly improves temporal consistency and removes anchor video jittering. This results in a coherent and meaningful video output, preserving the motion and layout of the original anchor video as learned through the temporal LoRA training. Finally, as a refining step, we can remove the temporal LoRA and retain only the context-aware spatial LoRA to apply SDEdit [56] to the generated video, thereby further reducing blurring and improving temporal consistency. In the end, we generate a video with new camera trajectories while preserving the original complex scene motion and the full content of the source video. Notably, this is accomplished without the need for paired video data. Ultimately, our method outperforms the generative approach Generative Camera Dolly [82], which requires paired videos as training data, and other 4D reconstruction methods [93, 48] on the Kubric dataset [82]. Furthermore, each component of our proposed method is validated through ablation studies on VBench [40]."
https://arxiv.org/html/2411.04928v1,DimensionX: Create Any 3D and 4D Scenes from a Single Image withControllable Video Diffusion,"In this paper, we introduce DimensionX, a framework designed to generate photorealistic 3D and 4D scenes from just a single image with video diffusion. Our approach begins with the insight that both the spatial structure of a 3D scene and the temporal evolution of a 4D scene can be effectively represented through sequences of video frames. While recent video diffusion models have shown remarkable success in producing vivid visuals, they face limitations in directly recovering 3D/4D scenes due to limited spatial and temporal controllability during generation. To overcome this, we propose ST-Director, which decouples spatial and temporal factors in video diffusion by learning dimension-aware LoRAs from dimension-variant data. This controllable video diffusion approach enables precise manipulation of spatial structure and temporal dynamics, allowing us to reconstruct both 3D and 4D representations from sequential frames with the combination of spatial and temporal dimensions. Additionally, to bridge the gap between generated videos and real-world scenes, we introduce a trajectory-aware mechanism for 3D generation and an identity-preserving denoising strategy for 4D generation. Extensive experiments on various real-world and synthetic datasets demonstrate that DimensionX achieves superior results in controllable video generation, as well as in 3D and 4D scene generation, compared with previous methods. Project Page: https://chenshuo20.github.io/DimensionX/","In the context of computer graphics and vision, understanding and generating 3D and 4D content are pivotal to create realistic visual experiences [5, 54]. By representing spatial (3D) and temporal (4D) dimensions, videos serve as a powerful medium for capturing dynamic real-world scenes. Despite substantial advancements in 3D and 4D reconstruction technologies [19, 52, 44, 47], there remains a critical shortage of large-scale 3D and 4D video datasets, limiting the potential for high-quality 3D and 4D scene generation from a single image. This scarcity poses a fundamental challenge in constructing photorealistic and interactive environments. Fortunately, recent advancements in video diffusion models have shown considerable promise in understanding and simulating real-world environments [4, 58]. Driven by advanced video diffusion models, recent works [43, 24, 55, 60] have made attempts to leverage the spatial and temporal priors embedded in video diffusion to generate 3D and 4D content from a single image. Despite these rapid developments, existing methods either concentrate on the object-level generation with video diffusion trained on static or dynamic mesh renderings [43, 24, 55] or employ time-intensive per-scene optimization for coarse scene-level generation [60] (e.g., Score Distillation Sampling [36]). This leaves the generation of coherent and realistic 3D/4D scenes an open challenge. In this paper, we present DimensionX, a novel approach to create high-fidelity 3D and 4D scenes from a single image with controllable video diffusion. While recent video diffusion models are capable of producing realistic results, it remains difficult to reconstruct 3D and 4D scenes directly from these generated videos, primarily due to their poor spatial and temporal controllability during the generation process. Our key insight is to decouple the temporal and spatial factors in video diffusion, allowing for precise control over each individually and in combination. To achieve the dimension-aware control, we establish a comprehensive framework to collect datasets that vary in spatial and temporal dimensions. With these datasets, we present ST-Director, which separates spatial and temporal priors in video diffusion through dimension-aware LoRAs. Additionally, by analyzing the denoising mechanics in video diffusion, we develop a training-free composition method that achieves hybrid-dimension control. With this control, DimensionX generates sequences of spatially and temporally variant frames, enabling the reconstruction of 3D appearances and 4D dynamic motions. To handle complex real-world scenes with our ST-Director, we design a trajectory-aware approach for 3D generation and an identity-preserving denoising mechanism for 4D generation. Extensive experiments demonstrate that our DimensionX outperforms previous methods in terms of visual quality and generalization for 3D and 4D scene generation, indicating that video diffusion models offer a promising direction for creating realistic, dynamic environments. In summary, our main contributions are: • We present DimensionX, a novel framework for generating photorealistic 3D and 4D scenes from only a single image using controllable video diffusion. • We propose ST-Director, which decouples the spatial and temporal priors in video diffusion models by learning (spatial and temporal) dimension-aware modules with our curated datasets. We further enhance the hybrid-dimension control with a training-free composition approach according to the essence of video diffusion denoising process. • To bridge the gap between video diffusion and real-world scenes, we design a trajectory-aware mechanism for 3D generation and an identity-preserving denoising approach for 4D generation, enabling more realistic and controllable scene synthesis. • Extensive experiments manifest that our DimensionX delivers superior performance in video, 3D, and 4D generation compared with baseline methods. Figure 2: Pipeline of DimensionX. Our framework is mainly divided into three parts. (a) Controllable Video Generation with ST-Director. We introduce ST-Director to decompose the spatial and temporal parameters in video diffusion models by learning dimension-aware LoRA on our collected dimension-variant datasets. (b) 3D Scene Generation with S-Director. Given one view, a high-quality 3D scene is recovered from the video frames generated by S-Director. (c) 4D Scene Generation with ST-Director. Given a single image, a temporal-variant video is produced by T-Director, from which a key frame is selected to generate a spatial-variant reference video. Guided by the reference video, per-frame spatial-variant videos are generated by S-Director, which are then combined into multi-view videos. Through the multi-loop refinement of T-Director, consistent multi-view videos are then passed to optimize the 4D scene."
https://arxiv.org/html/2411.03047v1,GarVerseLOD: High-Fidelity 3D Garment Reconstruction from a Single In-the-Wild Image using a Dataset with Levels of Details,"Neural implicit functions have brought impressive advances to the state-of-the-art of clothed human digitization from multiple or even single images. However, despite the progress, current arts still have difficulty generalizing to unseen images with complex cloth deformation and body poses. In this work, we present GarVerseLOD, a new dataset and framework that paves the way to achieving unprecedented robustness in high-fidelity 3D garment reconstruction from a single unconstrained image. Inspired by the recent success of large generative models, we believe that one key to addressing the generalization challenge lies in the quantity and quality of 3D garment data. Towards this end, GarVerseLOD collects 6,000 high-quality cloth models with fine-grained geometry details manually created by professional artists. In addition to the scale of training data, we observe that having disentangled granularities of geometry can play an important role in boosting the generalization capability and inference accuracy of the learned model. We hence craft GarVerseLOD as a hierarchical dataset with levels of details (LOD), spanning from detail-free stylized shape to pose-blended garment with pixel-aligned details. This allows us to make this highly under-constrained problem tractable by factorizing the inference into easier tasks, each narrowed down with smaller searching space. To ensure GarVerseLOD can generalize well to in-the-wild images, we propose a novel labeling paradigm based on conditional diffusion models to generate extensive paired images for each garment model with high photorealism. We evaluate our method on a massive amount of in-the-wild images. Experimental results demonstrate that GarVerseLOD can generate standalone garment pieces with significantly better quality than prior approaches while being robust against a large variation of pose, illumination, occlusion, and deformation. Code and dataset are available at garverselod.github.io.","High-quality 3D garment models are critical assets for a large variety of applications, ranging from entertainment to professional concerns, such as visual effects, physical simulation, and VR/AR telepresence. In the production-level pipeline, independent garment pieces are more desirable than a single clothed human model, as the former allows layered compositions with an internal body mesh to ensure the realism of physical motion and the flexibility of garment transfer. However, unlike clothed human reconstruction that can directly utilize the latest advances of neural implicit representation (Saito et al., 2019, 2020; Xiu et al., 2022), standalone garment modeling mostly relies on deforming parametric templates with open boundaries due to its strict requirement of correct topology. Nonetheless, reconstructing high-fidelity 3D garment from a single image remains a nuisance to current vision algorithms. While the high diversity of garment styles and the scarcity of the inputs render the problem highly ill-posed, the complex deformations resulted from the cloth dynamics make the inference even more challenging. There are two mainstream approaches for estimating the deformations of standalone garments from posed humans. Linear blend skinning (LBS)-based methods (Jiang et al., 2020; Corona et al., 2021) focus on predicting the deformations caused by human poses, where the learned skinning weights of the garment mesh are either bound to the skeleton or the surface vertices of a parametric model of unclothed humans (e.g., SMPL (Loper et al., 2015)). While this line of approaches can effectively represent posed-induced deformations, they struggle to model other intricate deformations caused by the environments or physical dynamics. Feature-line-based methods (Zhu et al., 2020, 2022) reconstruct garment meshes from SMPL surfaces and further fit them with garments’ manifold boundaries, making it versatile to model any type of deformations. However, the problem of boundary estimation from single images itself is challenging, due to the severe occlusions and 2D-to-3D ambiguities. Apart from the technical challenges, the other obstacle to learning-based garment reconstruction is the limited quantity and quality of 3D dataset. Due to the lack of local geometry details in existing garment datasets, current LBS-based methods are incapable of learning fine-grained geometries (e.g., wrinkles), resulting in coarse 3D garment quality. ReEF (Zhu et al., 2022) annotates the feature lines for only 400 garment models in the RenderPeople dataset (RenderPeople, 2018). The limited data scale hampers the prior approaches from generalizing to unseen images and often leads to poor reconstruction quality of feature lines (i.e., garment boundaries). In this work, we strive to address the above issues for standalone 3D garment reconstruction from the perspectives of both data and algorithm. We thereby introduce GarVerseLOD, a dedicated dataset and framework that achieves unprecedented robustness in reconstructing high-fidelity 3D garments from a single in-the-wild image (Fig. 1). To promote the quantity and quality of 3D garment data, GarVerseLOD collects 6,000 high-quality hand-crafted garment meshes with fine-grained details created by professional artists. It covers 5 most commonly seen categories – each category shares the same mesh topology, facilitating cross-instance interpolation and construction of blendshape models. While garment shapes differ globally in terms of style and topology, the local deformations are determined by a wide range of factors, including body poses, garment-environment interactions, self-collisions, etc. We, therefore, propose to craft GarVerseLOD as a hierarchical dataset with levels of details (LOD) to accommodate this key observation. In particular, as shown in Fig. 2, GarVerseLOD contains three basic levels of databases: 1) Garment Style Database with T-posed and detail-free coarse garment; 2) Local Detail Database enclosing pairs of T-posed models with and without fine-level local geometric details; and 3) Garment Deformation Database consisting of pairs of T-posed garment and its deformed counterpart (i.e., with global deformations). As the mesh topologies are identical within each category, we can easily extract the local details and global deformations from paired models in the corresponding database and combine all levels of geometries to obtain the Fine Garment Dataset. The disentangled granularities of geometry allows us to make this highly underconstrained problem tractable by factorizing the inference into smaller tasks, each can be tackled with narrowed solution space. Furthermore, we introduce a novel data labeling paradigm to generate extensive paired images for each garment model. Specifically, we leverage the latest advances in conditional diffusion model to transfer the textureless renderings to photorealistic images with diverse appearances. This further elevates the generalization capability of GarVerseLOD in handling unconstrained images. Algorithm-wise, we propose to connect the good ends of both LBS and feature-line based approaches. We first build a parametric model of the T-posed coarse shapes in the garment style database. After estimating the blendshape coefficients of the coarse garment, we progressively refine the result by adding pose-induced global deformations and fine-scale local deformations. Thanks to the LOD structure of GarVerseLOD, these three steps can be performed in a disentangled manner with eased complexity. While we employ linear blend skinning to estimate deformations caused by body poses, an implicit garment representation is learned to capture pixel-aligned fine surface from estimated 2D normal maps. We then fit the posed coarse garment with fine surfaces by aligning their open boundaries for the purpose of transferring the local details to the globally deformed mesh with correct topology. To combat with the occlusions, we present a novel geometry-aware boundary prediction strategy that equips the 2D features with 3D information from the estimated fine surface for better localization of 3D boundaries. Our experimental results show that GarVerseLOD can effectively reconstruct garments with diversified shapes and intricate deformations, demonstrating significantly better generalization ability over the prior arts. We summarize our contributions as follows: • We present the GarVerseLOD dataset, a large collection of high-fidelity 3D hand-crafted garments. It encloses 6,000 professionally hand-crafted garments, covers 5 categories, and, for the first time, contains 3 disentangled levels of details to ease the learning task. • We propose a novel data simulation pipeline to generate extensive paired images for supporting single-view reconstruction. • We devise a specially-tailored coarse-to-fine approach to fully utilize the LOD structure of the GarVerseLOD dataset. Experimental results show that our method excels in reconstructing high-quality garments from single images."
https://arxiv.org/html/2411.02607v1,Towards Context-Aware Adaptation in Extended Reality: A Design Space for XR Interfaces and an Adaptive Placement Strategy,"By converting the entire 3D space around the user into a screen, Extended Reality (XR) can ameliorate traditional displays’ space limitations and facilitate the consumption of multiple pieces of information at a time. However, if designed inappropriately, these XR interfaces can overwhelm the user and complicate information access. In this work, we explored the design dimensions that can be adapted to enable suitable presentation and interaction within an XR interface. To investigate a specific use case of context-aware adaptations within our proposed design space, we concentrated on the spatial layout of the XR content and investigated non-adaptive and adaptive placement strategies. In this paper, we (1) present a comprehensive design space for XR interfaces, (2) propose Environment-referenced, an adaptive placement strategy that uses a relevant intermediary from the environment within a Hybrid Frame of Reference (FoR) for each XR object, and (3) evaluate the effectiveness of this adaptive placement strategy and a non-adaptive Body-Fixed placement strategy in four contextual scenarios varying in terms of social setting and user mobility in the environment. The performance of these placement strategies from our within-subjects user study emphasized the importance of intermediaries’ relevance to the user’s focus. These findings underscore the importance of context-aware interfaces, indicating that the appropriate use of an adaptive content placement strategy in a context can significantly improve task efficiency, accuracy, and usability.","1 Related Work Previous work explored various innovative applications and design dimensions of XR to spatially place 2D & 3D objects and transition between them, visualize hierarchies, and provide persistent and portable presentation of the personal information [14, 20, 38, 16, 34]. Morrison et al. highlighted unique design elements within AR for enhancing accessibility for visually impaired children [45]. These studies underscore the broad design space of XR interfaces and the versatile and transformative applications that XR enables across various contexts. This work, investigates previous work and the design elements they utilized, providing a comprehensive XR design space. XR offers the potential to enable efficient information access, reduce cognitive load, and enhance user convenience compared to traditional methods such as mobile phones [55, 12, 43, 11, 40]. However, intrusive XR interfaces may result in challenges such as information overload and occlusion of important cues within the environment [31, 4], increase cognitive load and discomfort, and reduce the user’s situational awareness and performance [17, 25, 54]. Various approaches for intuitive and seamless integration of XR content into the environment have been extensively explored. For instance, to enhance efficiency and minimize intrusiveness, numerous designs adapt the XR content’s availability, transparency, placement, and Level of Detail (LoD) [15, 3, 46], as well as spatial layout and size [19, 9]. Lages & Bowman highlighted the significance of adapting the AR content placement strategy to avoid occlusions and accommodate activities like walking [33]. For adaptations to the XR content placement, the concept of the frame of reference, also referred to as fixation was introduced [20]. User-triggered adaptation through gaze, hand, and head-based inputs such as finger taps and handheld controllers are extensively explored for adjustments to the transparency, LoD, and spatial layout of XR content [19, 32, 50, 39, 48, 47]. In AR, for instance, many applications were designed to prioritize the real-world [11] by initially keeping the XR content hidden, in the peripheral, or at a lower LoD, and granting access to them through explicit interactions [49, 42, 46]. However, AR’s definition emphasizes the integration of the digital content into the real world [1], underscoring the significance of context awareness. While user-triggered adaptations offer control and predictability, they increase the user’s physical and mental workload of deciding when, what, and how to apply the adaptations [51]. Automatic XR adaptation can enhance efficiency and reduce workload compared to the user-triggered ones [11]. Numerous studies suggest rule-based approaches for XR adaptations. The significance of such rule-based adaptations in meeting the XR task requirements within various applications such as driving and conversation have been highlighted [4, 12]. To prevent occlusion issues, Ens et al. suggested a rule-based adaptive design to exclusively place the XR objects on empty surfaces [18]. Constraints, explicitly imposed by the users, were utilized as guidelines to group related XR objects together and prevent their occlusion within a rule-based view management [3]. Such rule-based adaptive approaches are highly tailored to specific use cases and applications.Even within the same application or use case, slight contextual deviations can cause a rule to fail, making it suitable only within unchanging contexts. This work proposes an adaptive placement strategy, applicable within changing contexts, to extract and utilize contextual information from the environment and user state to spatially place the XR content. Context refers to the external components that influence or relate to the user’s interactions with the interface [13]. In recent years, context-aware XR has become a focal point of research, promising the potential for “ubiquitous"" and “pervasive"" computing through AR [56, 26]. Contextual aspects such as user preferences, cognitive load, device profiles, task environment, semantic changes, and task-specific security parameters have been utilized for adaptations to the XR content’s appearance, LoD, frame of reference, and spatial layout [37, 9, 36]."
https://arxiv.org/html/2411.02424v1,Set-based queries for multiscale shape-material modeling,"Multiscale structures are becoming increasingly prevalent in the field of mechanical design. The variety of fine-scale structures and their respective representations results in an interoperability challenge. To address this, a query-based API was recently proposed which allows different representations to be combined across the scales for multiscale structures modeling. The query-based approach is fully parallelizable and has a low memory footprint; however, this architecture requires repeated evaluation of the fine-scale structures locally for each individual query. While this overhead is manageable for simpler fine-scale structures such as parametric lattice structures, it is problematic for structures requiring non-trivial computations, such as Voronoi foam structures.In this paper, we develop a set-based query that retains the compatibility and usability of the point-based query while leveraging locality between multiple point-based queries to provide a significant speedup and further decrease the memory consumption for common applications, including visualization and slicing for manufacturing planning. We first define the general set-based query that consolidates multiple point-based queries at arbitrary locations. We then implement specialized preprocessing methods for different types of fine-scale structures which are otherwise inefficient with the point-based query. Finally, we apply the set-based query to downstream applications such as ray-casting and slicing, increasing their performance by an order of magnitude. The overall improvements result in the generation and rendering of complex fine-scale structures such as Voronoi foams at interactive frame rates on the CPU.","1.1 Motivation The application of multiscale structures to mechanical design is becoming increasingly accessible due to the rapid advancement of modern manufacturing techniques. A multitude of methods, algorithms, and tools have emerged to support the design and modeling of such structures [1, 2, 3, 4, 5]. Periodic lattice and foam-based structures have been applied to architect materials meeting specific material property criteria as needed by the end-user. Commercial tools, such as Autodesk Within and nTopology, have created lightweight infills with these structures. These approaches have specific strengths and weaknesses for addressing specific modeling needs; however, the proliferation of many such methods with specific uses has led to interoperability issues. A multiscale API based on shape and material queries [6] has been proposed as a possible solution for allowing the integration of different representations across the scales. The architecture allows for rapidly exchanging different fine-scale models, provides a common interface to external tools and downstream applications, and enables easy chaining and linking of multiple scales to create multi-scale structures. In this approach, each coarse- or fine-scale representation is fully encapsulated and only needs to provide methods for a small set of point-based queries for shape or material properties at a given location. Downstream applications, such as visualization or slicing for manufacturing planning, are built upon these shape and material queries, which are both fully parallelizable and require a low memory footprint. However, this point-based system architecture requires the repeated and redundant evaluation of local structure within each scale. This computational overhead is manageable when only a little computation is required to generate the fine-scale structure, such as parametrically defined unit cell lattice structures [7]. However, it is detrimental to fine-scale structures that require intensive computation, such as sample-based heterogeneous material modeling [8] and Voronoi foam structures [9, 10]. This repeated evaluation also fundamentally precludes acceleration techniques which could be efficient for groups of points, but are slower on individual queries. 1.2 Contributions and outline We propose a novel set-based query which groups and batch-processes point queries together based on their spatial positions, taking advantage of the spatial locality of the fine-scale structure generation algorithm to accelerate these queries as a whole. This grouping also allows further preprocessing computations which can accelerate the processing of each group but would otherwise be excessive for single-point queries. The proposed approach differs from simply caching information within a single query, as we not only allow the use of preprocessing which is more efficient on groups of points, but we also sort and re-order computation on the points in order to maximize efficiency. Our approach by itself does not cache or store information between queries, but it is fully compatible with such approaches; we show that in some use cases this can further improve performance. The concept of set-based queries applies to different fine-scale structures. However, the benefit and performance gain differs depending on the characteristics of the structure. We categorize existing approaches to model fine-scale structures into four quadrants based on the computational resource needed and the geometric similarity of the structure across different neighborhoods. Specifically, we implement two specialized sorting algorithms which are efficient for computationally intensive structures (e.g. Voronoi Foam [9], inverse homogenization [11]), and for highly-repetitive structures (e.g. parametric lattices [7], and cyclic parametric functions [12]). The rest of the paper is organized as follows: In Section 2, we review the existing fine-scale structure modeling algorithm and categorize them based on the metrics we developed. In Section 3, we formulate the general set-based query and discuss two different point grouping strategies for Voronoi foams and for repetitive structures. In addition, we introduce two different preprossessing algorithms that precompute the Voronoi edges for each neighborhood, and one preprocessing algorithm for repetitive structures. Such preprossessing would not be economic with the point-based query. In Section 4, we examine two downstream applications to our set-based query: ray-casting and slicing. These approaches have fundamentally different types of point sets. While slicing provides a very structured set of points that are fully defined, ray-casting provides a variable set of points due to variable step sizes and early ray termination. In Section 5, we test our approach against the naïve point-based query on commodity desktop hardware and show that it can improve performance for slicing by an order of magnitude. For ray-casting, our approach can improve performance by over an order of magnitude for the Voronoi foam representation. We also identify the inflection points at which the naïve point-based query may be faster for small queries. Broadly, our work helps to bridge the gap between high-performance, specialized implementations for specific fine-scale structure models and generalized, highly interoperable query-based models. The set-based query lies in the Goldilocks zone between the fully parallel computation of each query point individually, and full precomputation of the entire structure at once. The former approach, as we show, is slower. The latter approach is intractable for multiscale structures, as the level of detail grows exponentially with the number of finer scales, and the entire structure quickly becomes too large to fit in memory."
https://arxiv.org/html/2411.02347v1,Physically Based Neural Bidirectional Reflectance Distribution Function,"We introduce the physically based neural bidirectional reflectance distribution function (PBNBRDF), a novel, continuous representation for material appearance based on neural fields. Our model accurately reconstructs real-world materials while uniquely enforcing physical properties for realistic BRDFs, specifically Helmholtz reciprocity via reparametrization and energy passivity via efficient analytical integration. We conduct a systematic analysis demonstrating the benefits of adhering to these physical laws on the visual quality of reconstructed materials. Additionally, we enhance the color accuracy of neural BRDFs by introducing chromaticity enforcement supervising the norms of RGB channels. Through both qualitative and quantitative experiments on multiple databases of measured real-world BRDFs, we show that adhering to these physical constraints enables neural fields to more faithfully and stably represent the original data and achieve higher rendering quality.","Representational learning has become the standard method for modeling complex spatial distributions and functions from measured data in computer graphics and vision. Implicit neural representations (INRs) utilize neural fields, such as multi-layer perceptrons (MLPs), to estimate functions that represent a signal continuously by training on discretely sampled data. These representations have applications ranging from 2D images (Sitzmann et al. 2020) to directional reflectance functions (Rainer et al. 2019; Sztrajman et al. 2021), 3D surfaces (Wang et al. 2021), and scene density and radiance (Mildenhall et al. 2020; Rainer et al. 2022). They have demonstrated superior expressiveness, interpolation ability, and fidelity to the underlying data compared to analytic parametric models. However, this expressiveness comes with a lack of constraints on the function learned by the network. This issue is often addressed through the use of priors (e.g., Pérez De Jesús, Linot, and Graham 2023), minimizing total variation (Yeh et al. 2022), or incorporating inductive biases relevant to the problem (e.g., Wang et al. 2021). In the domain of material appearance modeling, the most common function used for rendering is the bidirectional reflectance distribution function (BRDF) (Nicodemus et al. 1977b). Implicit neural representations for BRDF models (e.g., neural BRDFs (NBRDFs) (Sztrajman et al. 2021)) have proven efficient in modeling material appearance due to their expressiveness and compactness, coupled with high fidelity to real-world training data. However, BRDFs are derived from first principles and must obey strict physical constraints: Helmholtz reciprocity ensures that reflected and absorbed energy are independent of the direction of light travel, and energy passivity ensures that a point cannot reflect more light than it receives (see Sec. 3 for more details). Parametric BRDF models (Guarnera et al. 2016b) inherently respect these constraints, whereas NBRDFs, optimized to match measured data accurately, are not guaranteed to comply. This non-compliance may lead to significant artifacts in physical simulations of light transport, such as path tracing, where BRDFs are expected to satisfy strict physical constraints (e.g., Figs. 1 and 3). In this paper, we propose the physically based neural BRDF (PBNBRDF), which combines multiple novel methods to ensure physical plausibility and improve the perceptual fidelity of the underlying BRDFs. Through qualitative and quantitative evaluations, we demonstrate the effectiveness of our methods for creating physically sound and accurate neural BRDFs. We highlight the necessity of physically based neural fields to bridge the gap between well-established models like BRDFs and real-world data."
https://arxiv.org/html/2411.01488v1,ITS: Implicit Thin Shell for Polygonal Meshes,"In computer graphics, simplifying a polygonal mesh surface ℳℳ\mathcal{M}caligraphic_M into a geometric proxy that maintains close conformity to ℳℳ\mathcal{M}caligraphic_M is crucial, as it can significantly reduce computational demands in various applications. In this paper, we introduce the Implicit Thin Shell (ITS), a concept designed to implicitly represent the sandwich-walled space surrounding ℳℳ\mathcal{M}caligraphic_M, defined as {x∈ℝ3|ϵ1≤f⁢(x)≤ϵ2,ϵ1<0,ϵ2>0}conditional-setxsuperscriptℝ3formulae-sequencesubscriptitalic-ϵ1𝑓xsubscriptitalic-ϵ2formulae-sequencesubscriptitalic-ϵ10subscriptitalic-ϵ20\{\textbf{x}\in\mathbb{R}^{3}|\epsilon_{1}\leq f(\textbf{x})\leq\epsilon_{2},% \epsilon_{1}<0,\epsilon_{2}>0\}{ x ∈ blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT | italic_ϵ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ≤ italic_f ( x ) ≤ italic_ϵ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_ϵ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT < 0 , italic_ϵ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT > 0 }. Here, f𝑓fitalic_f is an approximation of the signed distance function (SDF) of ℳℳ\mathcal{M}caligraphic_M, and we aim to minimize the thickness ϵ2−ϵ1subscriptitalic-ϵ2subscriptitalic-ϵ1\epsilon_{2}-\epsilon_{1}italic_ϵ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_ϵ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. To achieve a balance between mathematical simplicity and expressive capability in f𝑓fitalic_f, we employ a tri-variate tensor-product B-spline to represent f𝑓fitalic_f. This representation is coupled with adaptive knot grids that adapt to the inherent shape variations of ℳℳ\mathcal{M}caligraphic_M, while restricting f𝑓fitalic_f’s basis functions to the first degree. In this manner, the analytical form of f𝑓fitalic_f can be rapidly determined by solving a sparse linear system. Moreover, the process of identifying the extreme values of f𝑓fitalic_f among the infinitely many points on ℳℳ\mathcal{M}caligraphic_M can be simplified to seeking extremes among a finite set of candidate points. By exhausting the candidate points, we find the extreme values ϵ1<0subscriptitalic-ϵ10\epsilon_{1}<0italic_ϵ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT < 0 and ϵ2>0subscriptitalic-ϵ20\epsilon_{2}>0italic_ϵ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT > 0 that minimize the thickness. The constructed ITS is guaranteed to wrap ℳℳ\mathcal{M}caligraphic_M rigorously, without any intersections between the bounding surfaces and ℳℳ\mathcal{M}caligraphic_M. ITS offers numerous potential applications thanks to its rigorousness, tightness, expressiveness, and computational efficiency. We demonstrate the efficacy of ITS in rapid inside-outside tests and in mesh simplification through the control of global error.","Polygonal mesh is a widespread form of 3D surface representation crucial in digital geometry processing. These meshes usually comprise a vast array of vertices, edges, and faces. Yet, direct manipulation or computational tasks on such meshes can be highly resource-demanding or sometimes rendered impractical due to issues with mesh quality. To overcome this challenge, a common approach involves employing a geometric proxy of reduced complexity, such as simplified meshes or boundary proxies, as noted in various studies [1, 2, 3, 4]. Thin shells, defined as the enclosed regions between two bounding surfaces—often described as sandwich-walled spaces [5]—provide notable benefits such as efficient memory usage [6] and the ability to design curves on the meshes [7]. As a result, thin shells become common popular proxies and analysis tools for a wide range of applications, including 3D printing [8], solid texturing [9], pattern engraving [10], surface and volume mappings [11], meshing [12], and cloth simulation [13], among others. The techniques for creating thin shells from a given polygonal mesh fall into two main categories: explicit and implicit methods. Explicit methods typically involve duplicating the base surface and then shifting it by a set distance. When it comes to choosing a direction for this displacement, there are several approaches, such as using normals [9, 14, 15] or the gradients of a generalized distance function [16, 17, 18, 19]. Alpha wrapping [20], a prominent explicit technique, differs by iteratively refining and carving a 3D Delaunay triangulation on an initially coarse enclosing surface of the input instead of merely offsetting triangles. Implicit methods conceptualize the bounding surface of thin shells as a level set of a distance field, defining the thin-shell space with an implicit function and two levels of distance [21, 22, 23, 24]. Wang et al. [25] proposed to represent the thin-shell of a polygonal surface for containment checks by using a collection of simple solids. Compared to explicit approaches, implicit methods provide more flexibility and uniformity, regardless of the quality of the input mesh. However, they often struggle to achieve a close and precise fit around the given mesh. Therefore, developing a tight, accurate implicit representation of thin shells continues to be a challenging area of research. This paper focuses on addressing the challenge of representing sandwich-walled spaces using an implicit function, denoted as f𝑓fitalic_f. While the signed distance function (SDF) can indeed be interpreted as a tri-variable function, its lack of an analytic form poses challenges in computing the shell. Let ℳℳ\mathcal{M}caligraphic_M be the input polygonal mesh. By approximating the SDF of ℳℳ\mathcal{M}caligraphic_M, we gain an analytic form that facilitates shell computation, and the sandwich-walled space can be expressed as {𝒗∈ℳ|f⁢(𝒗)∈[ϵ1,ϵ2]}conditional-set𝒗ℳ𝑓𝒗subscriptitalic-ϵ1subscriptitalic-ϵ2\{\boldsymbol{v}\in\mathcal{M}|f(\boldsymbol{v})\in[\epsilon_{1},\epsilon_{2}]\}{ bold_italic_v ∈ caligraphic_M | italic_f ( bold_italic_v ) ∈ [ italic_ϵ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_ϵ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ] }, where ,ϵ1<0,ϵ2>0,\epsilon_{1}<0,\epsilon_{2}>0, italic_ϵ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT < 0 , italic_ϵ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT > 0 delineate the values characterizing the inner and outer implicit bounding surfaces respectively, rigorously enveloping the base surface ℳℳ\mathcal{M}caligraphic_M. The desired implicit function must satisfy at least the following requirements: (1) Tightness: ϵ1subscriptitalic-ϵ1\epsilon_{1}italic_ϵ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT is as close to ϵ2subscriptitalic-ϵ2\epsilon_{2}italic_ϵ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT as possible, (2) Rigorousness: ϵ1≤f⁢(ℳ)≤ϵ2subscriptitalic-ϵ1𝑓ℳsubscriptitalic-ϵ2\epsilon_{1}\leq f(\mathcal{M})\leq\epsilon_{2}italic_ϵ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ≤ italic_f ( caligraphic_M ) ≤ italic_ϵ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, (3) Expressiveness: even if the geometry/topology of ℳℳ\mathcal{M}caligraphic_M is complicated, one can still find a suitable implicit function with a tight wrapping, (4) Rapid generation: the generation of the implicit function f𝑓fitalic_f can be accomplished in a short time period, and the generation can scale well to handle large-scale data, and (5) Rapid retrieval: it can report the function value quickly with minimal computational effort. In light of the requirements discussed, we present the Implicit Thin Shell (ITS) to represent the input’s ambient space, which holds significant utility in (1) meshless geometry processing, where the shell serves as the definition domain, and (2) expedited proximity search, with the shell acting as the proxy. ITS is designed to fulfill the outlined needs with a tri-variate tensor-product B-spline. First, B-spline functions strike a balance between geometric expressiveness and computational efficiency. This characteristic is particularly beneficial for function evaluation, as it involves computations with only a relevant subset of parameters, thus speeding up the process. For the sake of simplicity and efficiency, we restrict these B-spline functions to the first degree. Next, by adopting the established implicit B-spline methodology [26], we transform the signed distance function of a polygonal mesh surface into a tensor-product B-spline through a sparse linear system. Moreover, the task of identifying the extreme values of f𝑓fitalic_f with regard to the infinite points on ℳℳ\mathcal{M}caligraphic_M is refined to a search for extremes among a finite set of candidate points, enabling rigorous wrapping. To further optimize efficiency and minimize memory usage, we employ the sparse voxel octree (SVO) [27, 28] instead of the uniform octree, without compromising on approximation quality. We validate the utility of ITS in two key applications: conducting inside-outside tests and implementing mesh simplification with global precision control. In summary, our contributions are three-fold: • We introduce a novel representation of the sandwich-walled space of an input surface, in which the implicit function is expressed as a tri-variate tensor-product B-spline. • We reduce the problem of identifying the extreme values of f𝑓fitalic_f among a countless number of points to seeking extremes among a finite set of candidate points. As a result, our ITS can rigorously wrap the input surface. • We propose a set of acceleration strategies to enhance the implementation, achieving significant improvements in runtime performance for inside-outside tests. We also utilize ITS as a tool to adapt the well-known QEM for mesh simplification, enabling global error control."
https://arxiv.org/html/2411.02179v1,CleAR: Robust Context-Guided Generative Lighting Estimation for Mobile Augmented Reality,"High-quality environment lighting is the foundation of creating immersive user experiences in mobile augmented reality (AR) applications. However, achieving visually coherent environment lighting estimation for Mobile AR is challenging due to several key limitations associated with AR device sensing capabilities, including limitations in device camera FoV and pixel dynamic ranges. Recent advancements in generative AI, which can generate high-quality images from different types of prompts, including texts and images, present a potential solution for high-quality lighting estimation. Still, to effectively use generative image diffusion models, we must address their key limitations of generation hallucination and slow inference process. To do so, in this work, we design and implement a generative lighting estimation system called CleAR that can produce high-quality and diverse environment maps in the format of 360∘superscript360360^{\circ}360 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT images. Specifically, we design a two-step generation pipeline guided by AR environment context data to ensure the results follow physical environment visual context and color appearances. To improve the estimation robustness under different lighting conditions, we design a real-time refinement component to adjust lighting estimation results on AR devices. To train and test our generative models, we curate a large-scale environment lighting estimation dataset with diverse lighting conditions. Through quantitative evaluation and user study, we show that CleAR outperforms state-of-the-art lighting estimation methods on both estimation accuracy and robustness. In particular, CleAR achieves 56% - 51% (an average of 53%) accuracy improvement on virtual object renderings across objects with three distinctive types of materials over different reflective properties. Moreover, CleAR supports real-time refinement of lighting estimation results, ensuring robust and timely environment lighting updates for AR applications. Our end-to-end generative estimation takes as fast as 3.2 seconds, outperforming state-of-the-art methods by 110×\times×.","As new augmented reality (AR) hardware and software enter consumer markets, mobile AR technologies have positively impacted various industries, including e-commerce, education, and engineering (Chylinski et al., 2020; Rauschnabel et al., 2019). The growing public adoption of AR technologies demands new standards for content quality and application user experiences, particularly emphasizing the need for visual coherency between virtual and physical content to ensure high-quality user experiences. To create visual coherency, AR applications require an accurate and robust environment lighting estimation, which ensures that virtual objects blend naturally with the physical environment. We define the lighting estimation task for AR systems as estimating a complete environment map (a 360∘ HDR image) from partial observation of the environment (an LDR image with limited FoV) in real-time. This task is critical for supporting three key aspects of photorealistic rendering in real-time AR systems. (i) Reflective object rendering, which requires a complete environment map with visually coherent details. (ii) Highlights and shadows rendering, which requires HDR pixel information in environment maps. (iii) Temporally consistent visual coherency, which requires the system to robustly adapt to changing environmental lighting in real-time. To address this challenging task, traditional systems adopt autoregressive models (Zhao and Guo, 2021). These models extract low-frequency information based on camera input (shown in Fig. 1a). While these methods are able to capture coarse information about environmental lighting conditions, high-frequency details are missing. These details are important for creating a visually coherent AR system. To obtain the details, recent works (Somanath and Kurz, 2021; Yang et al., 2023; Wang et al., 2022) leverage the advancement of controllable generative models. These models have the potential to extract fine-grained environment details (shown in Fig. 1a), empowering AR systems with a better rendering effect. While generative-model-based methods seem promising, we identify two key challenges in using them on mobile AR systems. a Autoregressive Lighting Estimation b Generative Lighting Estimation Figure 1. Comparison between autoregressive and generative lighting estimation methods. (1a) An autoregressive lighting estimation system, Xihe (Zhao and Guo, 2021), estimates omnidirectional low-frequency lighting information from camera images with autoregressive models. The low-frequency lighting information misses important visual details, as visualized in the example environment map. (1b) Generative lighting estimation models can create high-frequency environment lighting estimation results from limited environment observations. The estimation process can be conditioned in several ways, such as partial environment observations and text prompts. First, robust lighting estimation demands accurate estimation under challenging lighting conditions. However, through a measurement study on existing datasets, we found that existing lighting estimation datasets have several distribution biases in key lighting properties, including light intensity and color temperatures. In further tests, we found that these data biases will affect the generalization and robustness of multiple recent lighting estimation models (Wang et al., 2022; Akimoto et al., 2022). Therefore, data diversity and fairness must be ensured when training and testing generative lighting estimation systems. Second, interactive mobile AR applications demand timely updates of lighting estimation results. However, generative models usually experience long inference latency. Without system latency optimization, naively integrating generative lighting estimation models into AR applications can easily violate the temporal constituency of immersive user experiences. In this paper, we address the above issues with CleAR, an edge-assisted novel generative lighting estimation system for mobile AR. To tackle AR device limitations in observation capability, we first design a two-step generative pipeline that estimates 360∘ HDR environment maps from partial LDR environment observations. Our key design insight is to separate the generative model training objective into two domains: LDR environment map completion and high-intensity pixel value estimation. This novel learning objective design addresses the practical challenges of the scarcity of high-quality lighting estimation training data, and allows us to leverage pre-trained large model to tackle each generation step effectively. On top of the two-stepped generative pipeline, CleAR uses controlling signals extracted from AR context data to ensure the estimated environment map aligns with the environment of physical AR devices. Specifically, CleAR uses environment semantic maps, which control the environment map visual details, and device ambient light sensor data, which informs the lighting intensity and color temperatures during generation. Specifically, this information is used as image and text inputs for ControlNet models. This design helps the generative pipeline to handle the significant information increases during environment map generation. Generative model inference latency is high, even on the edge servers. Therefore, we design an edge-device collaborative estimation system architecture with on-device refinement components to adjust edge estimation results to real-time lighting conditions. Specifically, we design a multi-output estimation strategy with an on-device adaptive output selection component. The estimation strategy is configured with the optimal number of generation outputs based on our performance measurement results. Additionally, CleAR uses a color appearance matching technique to efficiently and effectively adjust edge estimation results, even for challenging environment lighting conditions. To evaluate the lighting estimation quality of CleAR, we integrate CleAR into a mobile AR application using Unity, Python, and a recent AR data streaming framework ARFlow (Zhao and Guo, 2024). We compare CleAR’s virtual object rendering quality to ones using environment lighting acquired from three representative baselines: unwrapping a mirror ball (physical reference) (Debevec, 2006), ARKit (Apple, 2022) (commercial), and LitAR (Zhao et al., 2022) (academic). Our evaluation shows that CleAR can generate environment maps with better image details to support more visually coherent virtual object rendering. To quantitatively understand CleAR’s performance, we evaluate CleAR with standard testing dataset (Phongthawee et al., 2023) and compare with state-of-the-art lighting estimation models (Phongthawee et al., 2023; Wang et al., 2022; Akimoto et al., 2022; Gardner et al., 2017). Our evaluation shows that CleAR outperforms DiffusionLight (Phongthawee et al., 2023) by up to 56% using the three-sphere evaluation protocol (Wang et al., 2022). Also, notably, CleAR achieves 110X estimation latency reduction compared to DiffusionLight due to our efficient generative pipeline and generation control design. Furthermore, we verify the robustness of CleAR under different lighting conditions by testing CleAR on an augmented Laval dataset with diverse lighting conditions of light intensity and color temperatures. Qualitatively, our user study also confirms the effectiveness and robustness of CleAR, which shows at least 12% higher quality ratings. Related works on mobile AR lighting estimation systems seek to extract environment information from physical light probes (Prakash et al., 2019), user dynamics, and learning-based solutions (Zhao and Guo, 2021; Gardner et al., 2019; Wang et al., 2022; Yang et al., 2023). While physical light probes provide the most comprehensive environment observations, their use in practical applications is typically constrained due to the need for physical light probe presence. While environment lighting can also be extracted from dynamic environment observations, the estimation is usually incomplete as AR devices often do not have comprehensive environment observations. In contrast, leveraging learned models to estimate environment lighting from AR device camera images is a more feasible solution for AR applications. Over the past couple of decades, learning-based methods have evolved from discovering scene lighting cues from image details, such as highlights and shadows (Yu et al., 1999), to regress omnidirectional environment lighting representations (Zhao and Guo, 2020, 2021; Gardner et al., 2019). However, autoregressive models cannot effectively tackle the environment information generation in lighting estimation. For example, Xihe (Zhao and Guo, 2021) provides real-time low-frequency lighting estimation to AR applications but fails to support detailed environment reflections on object rendering. More recently, a new opportunity has arisen as generative models can support highly detailed image content for environment map estimation (Wang et al., 2022; Yang et al., 2023). In this work, we focus on novel AR system integrations with image-generative models to provide high-quality environment lighting estimation. We specifically focus on adapting generative lighting estimation models to achieve robust estimation under challenging environmental lighting conditions. We summarize our main contribution as the following: • We design and implement a novel AR context-guided generative lighting estimation system, CleAR. Our design leverages generative models to tackle the environmental observation limitation of AR devices. Specifically, our system uses a novel two-step generative lighting estimation pipeline to estimate an accurate environment map with visual details that match the AR user’s physical environment. To train the generative models, we craft a large-scale lighting estimation dataset with approximately 30K data items. • We present a measurement study on recent lighting estimation datasets to understand the complexity of environmental lighting conditions and explore the challenges of achieving robust lighting estimation. With the insights from this study, we design and conduct lighting estimation robustness testing experiments to evaluate several lighting estimation systems, including ours. • We develop two real-time estimation refinement techniques to improve the estimation quality of our system. Our first refinement component can automatically select the best environment map from generated results to match the current lighting conditions. Our second refinement component can match the color appearances between generated environment maps with real-time camera view to improve estimation robustness. • We implement CleAR as an end-to-end edge-assisted framework that can be integrated into third-party AR applications. We demonstrate the integration with an example object placement-based AR application. We will provide the dataset, system source code, and demo application links once the paper is accepted for publication to encourage follow-up research. • We conduct comprehensive experiments to evaluate the effectiveness of CleAR. Specifically, we tested CleAR with SoTA lighting estimation systems on standard testing datasets and our robustness testing dataset. We also conduct a quality assessment study with human perception feedback on the lighting estimation results. Overall, the CleAR rating score ranked the top with 12% higher than the second best method. Additionally, the overall standard deviation of the rating values for CleAR is also lower than the second best method (1.67) by 7%."
https://arxiv.org/html/2411.02090v1,Game Engines for Immersive Visualization:Using Unreal Engine Beyond Entertainment,"One core aspect of immersive visualization labs is to develop and provide powerful tools and applications that allow for efficient analysis and exploration of scientific data. As the requirements for such applications are often diverse and complex, the same applies to the development process. This has led to a myriad of different tools, frameworks, and approaches that grew and developed over time. The steady advance of commercial off-the-shelf game engines such as Unreal Engine has made them a valuable option for development in immersive visualization labs. In this work, we share our experience of migrating to Unreal Engine as a primary developing environment for immersive visualization applications. We share our considerations on requirements, present use cases developed in our lab to communicate advantages and challenges experienced, discuss implications on our research and development environments, and aim to provide guidance for others within our community facing similar challenges.","Developing immersive visualization applications imposes specific demands on the software used to create them. Choosing a software stack greatly impacts the application’s interaction, fidelity, and performance and affects the development process. Historically, there have been tendencies toward creating custom software to achieve immersive visualizations. These can broadly be divided into two categories: bespoke software specific to the applications and frameworks that can be used to build applications. While bespoke software gives much flexibility, it is often connected to a high (re-)implementation effort. On the other hand, custom frameworks develop centralized functionality into a common code base, which can then be (re-)used to create immersive visualization applications. Both, however, face a common challenge today: Maintenance efforts have increased drastically to meet the growing complexity and demands in various settings. Coincidentally, game engines — commercial-off-the-shelf (COTS) solutions — became more powerful, more open, and easier to use. This is particularly interesting to immersive visualization labs, as the development of such engines shares several key characteristics with immersive visualization applications. Both require low-latency computations, interactive rendering, ergonomic interactions, and visual fidelity. The rise of COTS virtual reality (VR) hardware in the early 2010s especially positively impacted the VR-readiness of COTS game engines. Besides the similar requirements for the end product, game engines have a large focus on tooling and developer efficiency. To enable low-friction game development that focuses on gameplay rather than technology, these engines are often highly optimized, bring ready-to-use techniques, and are easy to learn. Therefore, the obvious question is whether game engines could also be utilized to create immersive visualization applications. In this work, we present our considerations concerning the use of game engines as a key component of the workflow in immersive visualization labs. We reflect on historic decisions, development choices, and current workflows within our research lab at RWTH Aachen University, which led from custom software to the use of Unreal Engine (UE) (Epic Games, \APACyear\bibnodate\APACexlab\BCntND4). Based on these experiences, we suggest and motivate a list of requirements that we deem important when considering if a game engine is suitable for use in immersive visualization labs. We then present multiple use cases that highlight challenges within our lab and our solutions and discuss how the engine influenced the development in these particular cases. Finally, reiterating the introduced requirements, we evaluate and discuss Unreal Engine as a tool for immersive visualization applications and summarize our findings."
https://arxiv.org/html/2411.01947v1,HACD: Harnessing Attribute Semantics and Mesoscopic Structure for Community Detection,"Community detection plays a pivotal role in uncovering closely connected subgraphs, aiding various real-world applications such as recommendation systems and anomaly detection. With the surge of rich information available for entities in real-world networks, the community detection problem in attributed networks has attracted widespread attention. While previous research has effectively leveraged network topology and attribute information for attributed community detection, these methods overlook two critical issues: (i) the semantic similarity between node attributes within the community, and (ii) the inherent mesoscopic structure, which differs from the pairwise connections of the micro-structure. To address these limitations, we propose HACD, a novel attributed community detection model based on heterogeneous graph attention networks. HACD treats node attributes as another type of node, constructs attributed networks into heterogeneous graph structures and employs attribute-level attention mechanisms to capture semantic similarity. Furthermore, HACD introduces a community membership function to explore mesoscopic community structures, enhancing the robustness of detected communities. Extensive experiments demonstrate the effectiveness and efficiency of HACD, outperforming state-of-the-art methods in attributed community detection tasks. Our code is publicly available at https://github.com/Anniran1/HACD1-wsdm.","Community detection (wu2022clare, ) is a fundamental problem in network analysis, seeking to unveil closely connected subgraphs (i.e., communities) within complex networks. Previous research has adeptly utilized network topology to discern communities (kang2021adversarial, ; hou2022meta, ). However, nodes in real-world networks typically possess rich attribute information. For example, in citation networks (ye2023top, ), papers are associated with specific keyword domains. Such networks, known as attributed graphs (yang2013community, ), introduce additional complexity for community detection algorithms. To harness the potential of topology and attribute information for attributed community detection (ACD), existing methods, e.g., CommDGI (zhang2020commdgi, ) and ACDM (cheng2023significant, ), map these dual information sources to low-dimensional continuous vector spaces by using embedding techniques. While these methods have demonstrated promising results, we contend that current solutions may not be optimal because they overlook two critical issues: Figure 1. Most studies treat AI (artificial intelligence), CV (computer vision), and ML (machine learning) as independent attributes. However, AI and CV are subfields within the broader domain of ML, implying that they share underlying semantic similarities. • Semantic similarity. Semantic similarity refers to the degree of semantic resemblance or the extent of correlation between attributes. For instance, as illustrated in Figure 1, the semantic similarity of attributes can reveal latent relationships between nodes and enhance the attribute cohesiveness of detected communities(jiang2021query, ). However, existing methods usually disregard the semantic similarity between node attributes within communities, leading to the omission of crucial nodes in the detected communities. • Mesoscopic community structure. Inherent community structure, serving as a crucial mesoscopic description of network topology, imposes constraints on node representation at a higher structural level. If the mesoscopic community structure is considered to guide network embedding, the results would remain robust against minor local changes in the network structure, such as node noise and the addition or deletion of edges or nodes (liu2022robust, ). However, existing methods primarily focus on the pairwise connections of micro-structure between nodes (zhang2021spectral, ), rendering the results overly sensitive to minor changes in microscopic structure. To address these limitations, we propose a novel attributed community detection model based on a heterogeneous graph attention network (HAN), termed HACD. To tackle the first issue, we initially treat node attributes as another type of node, transforming real-world attributed networks into a heterogeneous graph structure. Subsequently, we propose an attribute-level attention mechanism (A2M), which utilizes weighted aggregation based on attention coefficients to identify key attributes within each community and employs an attention-based similarity metric to compute the distance between the semantic meanings of different attributes. By embedding with A2M, the representation learns the importance of different attributes and captures the semantic similarity between node attributes. This semantic similarity fully reflects the latent relationships between nodes, achieving attribute cohesion within communities. Furthermore, to address sensitivity issues and enhance robustness, we introduce a community membership function (CMF). By encoding initial community membership information and introducing a new modularity function to formulate CMF as a modularity optimization problem, we guide network embedding to explore mesoscopic community structures, ensuring the structural cohesiveness of detected communities. Our principal contributions can be summarized as follows: • We first identify two critical problems affecting attributed community detection: semantic similarity and mesoscopic community structure. • We propose a novel attributed community detection model, HACD. We construct the attribute network as a heterogeneous graph structure and introduce the heterogeneous graph neural network into attributed community detection tasks. We propose an attribute-level attention method to explore the semantic similarity between node attributes, as well as design a community membership function to obtain the mesoscopic community structure. • We conduct extensive experiments demonstrating the effectiveness and efficiency of HACD, showing superiority over state-of-the-art community detection methods in attributed graph datasets."
https://arxiv.org/html/2411.01512v1,InstantGeoAvatar:Effective Geometry and Appearance Modeling of Animatable Avatars from Monocular Video,"We present InstantGeoAvatar, a method for efficient and effective learning from monocular video of detailed 3D geometry and appearance of animatable implicit human avatars. Our key observation is that the optimization of a hash grid encoding to represent a signed distance function (SDF) of the human subject is fraught with instabilities and bad local minima. We thus propose a principled geometry-aware SDF regularization scheme that seamlessly fits into the volume rendering pipeline and adds negligible computational overhead. Our regularization scheme significantly outperforms previous approaches for training SDFs on hash grids. We obtain competitive results in geometry reconstruction and novel view synthesis in as little as five minutes of training time, a significant reduction from the several hours required by previous work. InstantGeoAvatar represents a significant leap forward towards achieving interactive reconstruction of virtual avatars.","Enabling the reconstruction and animation of 3D clothed avatars is a key step to unlock the potential of emerging technologies in fields such as augmented reality (AR), virtual reality (VR), 3D graphics and robotics. Interactivity and fast iteration over reconstructions and intermediate results can help speed up workflows for designers and practitioners. Different sensors are available for learning clothed avatars, including monocular RGB cameras, depth sensors and 4D scanners. RGB videos are the most widely available, yet provide the weakest supervisory signal, making this configuration elusive. Figure 1: InstantGeoAvatar. We introduce a system capable of reconstructing the geometry and appearance of animatable human avatars from monocular video in less than 10 minutes. In order to attain high quality geometry reconstructions, we propose a smoothing term directly on the learned signed distance field during optimization, requiring no extra computation or sampling and delivering noticeable qualitative improvements. Traditional depth-based methods fusing depth measurements over time [73, 74, 50, 8] produce compelling reconstructions but need complex and expensive setups to avoid sensor noise. Dense multi-view capture systems [1, 20, 34, 49, 64, 94, 96, 97, 41, 91, 30, 105, 9, 79] offer detailed 3D reconstructions by leveraging multi-view images and cues like silhouette or stereo. However, they can only be applied in controlled camera studios. Moreover, neither depth-based nor dense multi-view approaches can effectively work from RGB inputs alone or produce satisfactory results within short fitting times. Mesh-based approaches [2, 3, 6, 7, 10, 44, 12, 28, 71, 53, 4] struggle with garment deformations and are restricted to low-resolution topologies. Point cloud-based techniques [66, 68, 116, 60, 98, 59, 120, 80, 121, 117, 61] have shown promising outcomes, but are not yet capable of fast-training times with RGB supervision only. The advent of neural radiance fields (NeRFs) [70] enabled techniques for novel view synthesis and animation of human avatars from RGB image supervision only [26, 38, 109, 93]. Volume rendering-based approaches typically learn a canonical representation that is deformed with linear blend skinning [65] and an additional non-rigid deformation [77, 104, 115]. Despite producing good renderings of human avatars, these techniques lack awareness of the underlying geometry. In parallel, some works have adopted a signed distance function (SDF) [111] as basic primitive for learning clothed human avatars from 3D scans [85, 5, 107]. To remove the need for 3D supervision, some works have embedded SDFs within a volumetric rendering pipeline [29, 103, 77]. Significant steps have been taken to speed up training of NeRF-based approaches [26, 38] by leveraging an efficient hash-grid spatial encoding [72]. Subsequent work has tried to improve training on such unstable and noisy hash grids [56, 23] with some success. Up to date, however, fast geometry learning in general and effective use of hash grid-based representations in particular for clothed human avatars with RGB supervision only remains elusive. The challenges faced by NeRF- and SDF-based approaches trained with volume rendering can be succinctly reduced to effectively capturing realistic non-rigid deformations, dealing with noisy pose and camera estimates, slow training, and unstable training in the case of hash grid-based methods. In this paper, we specifically focus on the last two challenges, and aim at significantly advancing towards the realization of interactive use of human avatar modelling. We propose InstantGeoAvatar, a system capable of yielding good rendering and reconstruction quality in as little as 5 minutes of training, down from several hours as in prior work. Building on recent advances for fast training of NeRF based systems [72, 38] and efficient training of hash grid encodings [56, 23], we demonstrate that even in combination such prior improvements and techniques are insufficient for fast and effective learning of 3D clothed humans. Thus we propose a simple yet effective regularization scheme that imposes a local geometric consistency prior during optimization, effectively removing undesired artifacts and defects on the surface. The proposed approach, which effectively constrains surface curvature and torsion over continuous SDFs along ray directions, is easy to implement, fits neatly within the volume rendering pipeline, and delivers noticeable improvements over our base model without additional cost. Our experiments demonstrate the effectiveness of the proposed method for effective and fast learning of animatable 3D human avatars from monocular video. At the short-training regime, InstantGeoAvatar yields superior geometry reconstruction and rendering quality compared to previous work in less than 10 minutes (see Fig. 1). While SoTA methods can yield more accurate reconstructions after several hours upon convergence, InstantGeoAvatar still shows comparable and even superior results with out of distribution (OOD) poses. In addition, the presented ablation demonstrates that previous work on improving training of hash grid-based representations is insufficient for obtaining satisfying geometry reconstructions, highlighting the suitability of our proposal."
https://arxiv.org/html/2411.01212v1,Infinite-Resolution Integral Noise Warpingfor Diffusion Models,"Adapting pretrained image-based diffusion models to generate temporally consistent videos has become an impactful generative modeling research direction. Training-free noise-space manipulation has proven to be an effective technique, where the challenge is to preserve the Gaussian white noise distribution while adding in temporal consistency. Recently, Chang et al. (2024) formulated this problem using an integral noise representation with distribution-preserving guarantees, and proposed an upsampling-based algorithm to compute it. However, while their mathematical formulation is advantageous, the algorithm incurs a high computational cost. Through analyzing the limiting-case behavior of their algorithm as the upsampling resolution goes to infinity, we develop an alternative algorithm that, by gathering increments of multiple Brownian bridges, achieves their infinite-resolution accuracy while simultaneously reducing the computational cost by orders of magnitude. We prove and experimentally validate our theoretical claims, and demonstrate our method’s effectiveness in real-world applications. We further show that our method readily extends to the 3-dimensional space.","The success of diffusion models in image generation and editing (Rombach et al., 2022; Nichol et al., 2021; Ho et al., 2020; Zhang et al., 2023a) has spurred significant interest in lifting these capacities to the video domain (Singer et al., 2022; Durrett, 2019; Gupta et al., 2023; Blattmann et al., 2023; Ho et al., 2022; Guo et al., 2024). While training video diffusion models directly on spatiotemporal data is a natural idea, practical concerns such as limited availability of large-scale video data and high computational cost have motivated investigations into training-free alternatives. One such approach is to use pre-trained image models to directly generate video frames, and utilize techniques such as cross-frame attention, feature injection and hierarchical sampling to promote temporal consistency across frames (Ceylan et al., 2023; Zhang et al., 2023b; Khachatryan et al., 2023; Cong et al., 2023). Among these techniques, the controlled initialization of noise has been consistently shown to be an important one (Ceylan et al., 2023; Khachatryan et al., 2023). However, most existing approaches for noise manipulation either compromise the noise Gaussianity (and subsequently introduce a domain gap at inference time), or are restricted to simple manipulations such as filtering and blending which are insufficient for capturing complex temporal correlations. Recently, Chang et al. (2024) proposed a method that both preserves Gaussian white noise distribution and well captures temporal correlations via integral noise warping: each warped noise pixel integrates a continuous noise field over a polygonal deformed pixel region, which is computed by summing subpixels of an upsampled noise image. However, their method’s theoretical soundness and effectiveness are followed by its high-end computational cost in both memory and time, which not only incurs a significant overhead at inference time but also limits its useability in novel applications (Kwak et al., 2024). In this paper, we introduce a new noise-warping algorithm that dramatically cuts down the cost of Chang et al. (2024) while fully retaining its virtues. Our key insight for achieving this lies in that, when adopting an Eulerian perspective (as opposed to the original Lagrangian one), the limiting-case algorithm of Chang et al. (2024) for computing a warped noise pixel reduces to summing over increments from multiple Brownian bridges (Durrett, 2019, Section 8.4). In place of the costly upsampling procedure, sampling the increments of a Brownian bridge can be done efficiently in an autoregressive manner (2). We build upon this to devise the infinite-resolution integral noise warping algorithm (1) which directly resolves noise transport in the continuous space, when given an oracle that returns the overlapping area between a pixel square and a deformed pixel region (Section 2.3). We propose two concrete ways to compute this oracle, leading to a grid-based and a particle-based variant of our method. Similar to Chang et al. (2024), the grid-based variant (Algorithm 2) computes the area by explicitly constructing per-pixel deformed polygons, and is exactly equivalent to the existing approach (Chang et al., 2024) with an infinite upsampling resolution, while running 8.0×8.0\times8.0 × to 19.7×19.7\times19.7 × faster and using 9.22×9.22\times9.22 × less memory111Since the official code of Chang et al. (2024) is not available, performance is compared using our reimplementation in Taichi (Hu et al., 2019), which we find to be faster than as reported in the original paper.. Inspired by hybrid Eulerian-Lagrangian fluid simulation (Brackbill et al., 1988), our novel particle-based variant (Algorithm 3) computes area in a fuzzy manner, which not only offers a further 5.21×5.21\times5.21 × speed-up over our grid-based variant, but is also agnostic to non-injective maps. In real-world scenarios, the particle-based variant shows no compromise in generation quality compared to the grid-based one (see video results), while offering superior robustness, efficiency, simplicity, and extensibility to higher dimensions. In summary, we propose a new noise-warping method to facilitate video generation by lifting image diffusion models. Through analyzing the limiting case of the current state-of-the-art method (Chang et al., 2024) with an infinite upsampling resolution, we derive its continuous-space analogy, which fully retains its distribution-preserving and temporally-coherent properties, while achieving orders-of-magnitude speed-up, warping 1024×1024102410241024\times 10241024 × 1024 noise images in ∼0.045similar-toabsent0.045\sim 0.045∼ 0.045s (grid variant) and ∼0.0086similar-toabsent0.0086\sim 0.0086∼ 0.0086s (particle variant) using a laptop with a Nvidia RTX 3070 Ti GPU. Figure 1: When the image grid deforms, the Lagrangian view tracks a deformed pixel region, while the Eulerian view tracks the undeformed pixel square as it gets partitioned into multiple regions. On the right, we leverage the exchangeability of upsampled subpixels to convert the Lagrangian gathering procedure into scattering noise subpixels to overlapped deformed pixel regions."
https://arxiv.org/html/2411.01179v1,Hollowed Net for On-Device Personalization of Text-to-Image Diffusion Models,"Recent advancements in text-to-image diffusion models have enabled the personalization of these models to generate custom images from textual prompts. This paper presents an efficient LoRA-based personalization approach for on-device subject-driven generation, where pre-trained diffusion models are fine-tuned with user-specific data on resource-constrained devices. Our method, termed Hollowed Net, enhances memory efficiency during fine-tuning by modifying the architecture of a diffusion U-Net to temporarily remove a fraction of its deep layers, creating a hollowed structure. This approach directly addresses on-device memory constraints and substantially reduces GPU memory requirements for training, in contrast to previous methods that primarily focus on minimizing training steps and reducing the number of parameters to update. Additionally, the personalized Hollowed Net can be transferred back into the original U-Net, enabling inference without additional memory overhead. Quantitative and qualitative analyses demonstrate that our approach not only reduces training memory to levels as low as those required for inference but also maintains or improves personalization performance compared to existing methods.","Recent research on text-to-image (T2I) diffusion models [1, 2], which generate high-resolution images from text prompts, has increasingly focused on personalizing and customizing these generative models effectively [3, 4, 5, 6, 7]. A primary approach, termed subject-driven generation [5], involves fine-tuning pre-trained diffusion models with a few user-specific images to generate varied representations of a subject using simple text prompts. This allows users to create personalized images of specific subjects, such as family, friends, pets, or personal items, with preferred appearances, backgrounds, and styles. Such capabilities enable creative applications including art renditions, property modifications, and accessorization. From a practical standpoint, implementing subject-driven generation on-device offers significant benefits in efficiency and privacy. By operating independently of congested cloud servers or networks, users can generate personalized images anywhere at no additional cost and do not need to compromise their privacy as all data and personal information remain on the device. Despite extensive research aimed at efficiently personalizing diffusion models, limited attention has been paid to memory I/O, a critical bottleneck in on-device learning. Recent studies have mainly explored two strategies: (1) decreasing the number of training steps and (2) reducing the number of updating parameters. The first methods [8, 9, 10, 11, 12] utilize additional large pre-trained models to generate a set of personalized Low-Rank Adaptation (LoRA) parameters [13], text embeddings, or image prompts from a user-specific image. This strategy provides a better initial setup for personalizing the diffusion models, effectively reducing required training steps. Some models [10, 11, 12] even support zero-shot personalization, although they underline that further fine-tuning can enhance personalization quality and address failure cases. Nonetheless, these methods are not viable for environments with severely limited computational resources, as they necessitate additional inference using large pre-trained models (e.g., 2.7B parameters for BLIP-2 in BLIP-Diffusion [12] and 2.5B for apprentice models in SuTI [11]), which are substantially larger than standard diffusion models (e.g., 1B for Stable Diffusion v2 [2]), making their application challenging in on-device settings. The second approach [7, 14], often involving LoRA, aims to reduce the number of updating parameters by limiting updates to specific layers or decomposing weight matrices. However, even with fewer parameters to update, these parameters reside within large pre-trained models, and thus the backward pass through the large models is required to compute gradients. Given limited computational resources, where even simple inference tasks with diffusion models can strain GPU memory, performing backpropagation while keeping the entire diffusion model in GPU memory remains a significant limitation. A promising approach to address these challenges is side-tuning [15, 16, 17, 18], which fine-tunes a smaller auxiliary network rather than directly updating the parameters of a large pre-trained network. This method significantly reduces the heavy memory costs associated with computing backpropagation on the larger network. Particularly for Natural Language Processing (NLP) tasks, Ladder Side Tuning (LST) [18] has proven effective, reducing the memory costs required for fine-tuning large language models (LLMs) by 69 percent. However, applying LST directly to diffusion U-Nets presents significant challenges. Unlike transformer layers in LLMs, which maintain consistent input and output dimensions, diffusion U-Nets have varying spatial dimensions and channels, as well as skip-connections across different blocks. Additionally, the requirements for structural pruning and weight initialization to build side-tuning networks further complicate the rapid adaptability of LST to personalization tasks across different subjects and domains. To this end, we introduce a novel personalization technique called Hollowed Net, which is illustrated in Fig. 1. Based on our observation that deep layers in the middle of diffusion U-Nets play significantly less important roles than the rest of the layers, we propose to fine-tune LoRA parameters for the personalization using Hollowed Net, a layer-pruned U-Net featuring a central hollow, which is constructed by temporarily removing the middle deep layers from the pre-trained diffusion U-Net. By utilizing the symmetrical ""U-shape"" architecture of the diffusion U-Net, we avoid complicated processes of applying structural pruning and weight initialization to build a side network, and neither additional models nor extensive pre-training with large datasets are required. By fine-tuning LoRA parameters using Hollowed Net, we can significantly reduce the memory needed for storing model weights in GPU. Once the LoRA parameters are fine-tuned with Hollowed Net, they can be seamlessly transferred back to the original Diffusion U-Net for inference, without requiring any additional memory beyond the small set of transferred parameters. Our experiments demonstrate that Hollowed Net enables achieving performance that is comparable to or better than the direct fine-tuning with LoRA, while using 26 percent less GPU memory, which is only 11 percent increased GPU memory relative to an inference. To the best of our knowledge, Hollowed Net is the first technique that addresses subject-driven generation in terms of memory efficiency. Our method shows how T2I diffusion models can be fine-tuned under extremely limited computational resources with as low GPU memory as required for inference. Furthermore, it is important to note that our method does not preclude the use of previously described strategies for efficient personalization. Both enhanced parameter-efficient strategies and improved initializations with additional pre-trained models can be integrated with our approach to further increase efficiency according to given resource constraints. Our contributions can be summarized as follows: • We introduce Hollowed Net, a novel personalization technique for T2I diffusion models under limited computational resources. Our method significantly reduces the memory demands on GPU to levels as low as those required for inference, while maintaining a high-fidelity personalization capacity. This demonstrates its potential as a feasible on-device learning solution for resource-constrained devices. • Our method provides a scalable and controllable solution for on-device learning. As this method does not require any additional models or pre-training with large datasets, it is easily scalable to other architectures such as SDXL and Transformers. Moreover, we can simply adjust the fraction of hollowed layers to control the trade-offs between performance and memory requirements, depending on the target application and resources. • Unlike previous side-tuning methods, Hollowed Net does not need to be retained for inference. The LoRA parameters fine-tuned with Hollowed Net can be seamlessly transferred back to its original network, enabling inference with no additional memory cost. Figure 1: The LoRA personalization with Hollowed Net for resource-constrained environments. The input image is from the DreamBooth dataset [5]."
https://arxiv.org/html/2411.00356v1,All-frequency Full-body Human Image Relighting,"Relighting of human images enables post-photography editing of lighting effects in portraits. The current mainstream approach uses neural networks to approximate lighting effects without explicitly accounting for the principle of physical shading. As a result, it often has difficulty representing high-frequency shadows and shading. In this paper, we propose a two-stage relighting method that can reproduce physically-based shadows and shading from low to high frequencies. The key idea is to approximate an environment light source with a set of a fixed number of area light sources. The first stage employs supervised inverse rendering from a single image using neural networks and calculates physically-based shading. The second stage then calculates shadow for each area light and sums up to render the final image. We propose to make soft shadow mapping differentiable for the area-light approximation of environment lighting. We demonstrate that our method can plausibly reproduce all-frequency shadows and shading caused by environment illumination, which have been difficult to reproduce using existing methods.","Human image relighting can alter the lighting effects in a portrait by changing the lighting condition after the photo shoot. The fundamental procedure for human image relighting is to infer the intrinsic geometry and reflectance of the target person as well as the scene illumination from the input image via inverse rendering and then render an output image with a new lighting condition. Modern learning-based methods formulate these inverse and forward rendering stages as a unified differentiable pipeline within an analysis-by-synthesis framework. The current state-of-the-art techniques [28, 40, 37, 22, 25, 39] employ neural networks to approximate the forward rendering stage without explicitly considering the physical principles involved. In particular, the physical principle of shadows is often ignored; shadows appear when the target geometry occludes the incoming light. Explicitly modeling such light occlusion within a differentiable rendering pipeline has been proven challenging; recent approaches only support hard shadows caused by a single point/directional light [13, 36] or adopt a computationally expensive solution via non-differentiable ray tracing with a pre-inferred geometry [16]. Consequently, neural networks in the state-of-the-art techniques struggle to learn complicated shadow patterns and yield blurry shadows or flickering artifacts with dynamic lighting. In this paper, we step forward to reproduce physically plausible shadows for all-frequency relighting of human images. We simultaneously model the target geometry and environment illumination as a depth map and a fixed number of area lights within a differentiable framework to reproduce hard-to-soft shadows caused by multiple area lights. The ground-truth area lights for supervised learning are obtained via a novel optimization-based approach. We also infer the diffuse and specular reflectances of the target person for physically based shading. Such geometry and reflectance information is easier to learn with neural networks because it is simpler than the complicated shadow and reflection patterns. We demonstrate that our physically based formulation yields more plausible and stable relighting results even under dynamic lighting than the existing approximate solutions using neural networks (Figure 1). We will release our source codes, trained models, and synthetic dataset upon publication."
https://arxiv.org/html/2411.00131v1,"Two Dimensional Hidden Surface Removalwith Frame-to-frame
Coherence","We describe a hidden surface removal algorithm for two-dimensional layered scenes built from arbitrary primitives, particularly suited to interaction and animation in rich scenes (for example, in illustration). The method makes use of a set-based raster representation to implement a front-to-back rendering model which analyses and dramatically reduces the amount of rasterization and composition required to render a scene. The method is extended to add frame-to-frame coherence analysis and caching for interactive or animated scenes. A powerful system of primitive-combiners called filters is described, which preserves the efficiencies of the algorithm in highly complicated scenes. The set representation is extended to solve the problem of correlated mattes, leading to an efficient solution for high quality antialiasing. A prototype implementation has been prepared.","1 Related Work in Computer Graphics Our method, while it has some novelty, uses work from a wide range of previous work, both in two- and three-dimensional graphics. The simplest way to render a two-dimensional scene is to render each layer, retaining transparency information, and then to compose the layers one at a time using the methods described in [\citenamePorter and Duff 1984]. A good introduction can be found in [\citenameSmith 1995a] and [\citenameSmith 1995b]. Hidden surface removal in two dimensions is a special case of three-dimensional hidden surface removal where we already have a complete depth ordering on the objects to be drawn, but do not yet know whether we need draw all of each object, since objects may overlap partially or completely. Traditionally, two-dimensional systems have not considered the problem of correlated mattes (where multiple partially overlapping or intersecting objects contributing to a pixel cause wrong results). We extend our system to handle this in Section 7. Two three-dimensional systems which deal with this problem properly are Catmull’s Pixel Integrator [\citenameCatmull 1978] and Carpenter’s A-buffer [\citenameCarpenter 1984]. We reverse the rendering order (as is often done in 3D graphics, and in 2D in [\citenameFroumentin and Willis 1999]), and show how the number of pixels requiring special attention is thus reduced. Our system solves the hidden surface problem for two-dimensional layered scenes allowing for an arbitrary antialiasing filter—not just a box filter as is common—calculating pixel values for each layer only when needed (including solving the problem of correlated mattes exactly, and only, when needed). This improves on current systems which calculate the whole of each layer even when that layer will be partially or completely covered by an object further forward in the scene. We do not discuss the mathematical foundations of antialiasing theory, but for a description of the box filter’s insufficiency, see [\citenameSmith 1995c]. The success of our system relies upon the storage of fragments of rendered content (sprites) and sets of pixel locations (shapes) being efficient in the presence of plain fills, fancy fills and complicated antialiased pixels. These concepts are introduced in the context of the composition of primarily bitmap graphics in [\citenameSmith 1995d]. We recast those into the domain of a vector graphics editor, taking advantage of the preponderance of non-rectangular objects. It also requires that set-based operations on shapes are fast, and that composition of sprites is fast. We use data structures similar to Wallace’s cartoon cel work [\citenameWallace 1981] and Froumentin & Willis’ IRCS [\citenameFroumentin and Willis 1999]. Our system of primitive combiners called filters is rather like that developed by [\citenameBier et al. 1993]. Our method of drawing brush strokes (which we use as one example of a non-polygon primitive) comes from [\citenameWhitted 1983]. A discussion of some of the issues involved in writing an interactive editor for two-dimensional scenes such as ours is in [\citenameFekete and Beaudouin-Lafon 1996]."
https://arxiv.org/html/2411.00652v1,Towards High-fidelity Head Blending with Chroma Keyingfor Industrial Applications,"We introduce an industrial Head Blending pipeline for the task of seamlessly integrating an actor’s head onto a target body in digital content creation. The key challenge stems from discrepancies in head shape and hair structure, which lead to unnatural boundaries and blending artifacts. Existing methods treat foreground and background as a single task, resulting in suboptimal blending quality. To address this problem, we propose CHANGER, a novel pipeline that decouples background integration from foreground blending. By utilizing chroma keying for artifact-free background generation and introducing Head shape and long Hair augmentation (𝑯𝟐superscript𝑯2\bm{H^{2}}bold_italic_H start_POSTSUPERSCRIPT bold_2 end_POSTSUPERSCRIPT augmentation) to simulate a wide range of head shapes and hair styles, CHANGER improves generalization on innumerable various real-world cases. Furthermore, our Foreground Predictive Attention Transformer (FPAT) module enhances foreground blending by predicting and focusing on key head and body regions. Quantitative and qualitative evaluations on benchmark datasets demonstrate that our CHANGER outperforms state-of-the-art methods, delivering high-fidelity, industrial-grade results.","In the realm of modern digital content creation, Head Blending, the seamless integration of an actor’s head onto a body filmed in separate takes or contexts is a critical yet under-explored task. We focus on a such process, which is essential for various applications such as visual effects (VFX) post-production, digital human creation, and virtual avatar generation. In these scenarios, integrating an actor’s head with footage where the body and surrounding environment may differ significantly is often necessary. The main challenge of Head Blending arises from the discrepancies between the actor’s head and the target body, including differences in head shape and hair structure. These discrepancies often lead to unnatural boundaries or blending artifacts, which can be particularly problematic in professional applications where high fidelity and visual coherence are ultimate. The existing method, Head2Scene Blender (H2SB) [16], approaches this task by treating the foreground and background generation as a single process. H2SB shows unsatisfactory results (Figure 2(a), (b)), especially around the boundary regions. Although the generation region has two distinct background and foreground parts, H2SB considers the region at once, which results in an unclear border of a human and artifacts. Moreover, H2SB lacks in mimicking the cross-identity head blending and fails to cover large inpainting regions. To this end, we propose CHANGER, a novel pipeline for Consistent Head blending with predictive AtteNtion Guided foreground Estimation under chroma key setting for industRial applications. We decompose the problem into two distinct sub-tasks: background integration and foreground blending. This decomposition allows for a more focused treatment of each aspect of the task, ensuring higher fidelity in both the background and foreground. The background integration challenge is addressed by incorporating chroma keying [14], a widely used technique in content production where a uniformly colored background (e.g., a green screen) is replaced with the desired scene. This allows for flawless background generation, eliminating the artifacts that arise when the foreground and background are blended simultaneously. By decoupling the foreground blending from the background, we ensure that the visual integrity of the scene is preserved, even in complex environments. For the foreground blending, we tackle the problem of seamlessly integrating the actor’s head onto the body of the target, particularly in cases where significant differences exist in head shape and hair structure. To generate the high-fidelity foreground, we devise two contributions, one from a data-centric and the other from a model-centric perspective. First, we propose a novel data augmentation technique called Head shape and long Hair augmentation (𝑯𝟐superscript𝑯2\bm{H^{2}}bold_italic_H start_POSTSUPERSCRIPT bold_2 end_POSTSUPERSCRIPT augmentation), which simulates a wide range of head shapes and hair styles in the self-supervised training. This enables our model to better generalize to real-world variations and handle the significant visual discrepancies that often arise in professional content production. Second, we introduce the Foreground Predictive Attention Transformer (FPAT), a novel architecture designed for foreground blending. FPAT predicts the exact regions of the head and body that require attention and apply targeted attention to these areas during the blending process. By explicitly restricting the attention to these key regions, FPAT enhances the blending quality, particularly in areas where head shape and hair differences pose a challenge. To summarize, we propose the first comprehensive solution for the Head Blending task in industrial content production. Unlike the previous approach that treats this process as part of general head or face swapping, our method focuses explicitly on the seamless blending of an actor’s head with the target body, ensuring realistic and high-quality results. Our method, CHANGER, significantly outperforms state-of-the-art techniques, as demonstrated through both quantitative metrics and qualitative evaluations on benchmark datasets. Figure 3: Network overview of CHANGER. (a) We visualize how we conduct the input of the network (X𝑋Xitalic_X) at the train (blue) and the test (red). We apply H2superscript𝐻2H^{2}italic_H start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT augmentation during the training to improve the fidelity of the generated image by improving the diversity of the input. (b) We visualize the network of CHANGER. The head colorizer colorizes the gray head of X𝑋Xitalic_X, and the body blender inpaints the hidden body with a foreground mask-aware attention mechanism. Please refer to the detailed explanations of H2superscript𝐻2H^{2}italic_H start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT augmentation and FPAT in Section 3.2 and 3.3, respectively. In summary, our main contributions are as follows: • We propose CHANGER, a novel pipeline that utilizes chroma keying for the first time to decouple background integration from the head blending process, addressing the common artifacts seen in prior methods. • We introduce H2superscript𝐻2H^{2}italic_H start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT augmentation, a data-centric approach designed to handle significant variations in head shape and hair structure, enhancing the robustness of the Head Blending process. • We present the FPAT module, which uses predictive attention to focus on key regions of the head and body, resulting in high-fidelity blending with minimized artifacts. • CHANGER significantly outperforms existing methods on benchmark datasets, both quantitatively and qualitatively, showcasing its effectiveness in industrial content production scenarios."
https://arxiv.org/html/2411.00399v1,StyleTex: Style Image-Guided Texture Generation for 3D Models,"Style-guided texture generation aims to generate a texture that is harmonious with both the style of the reference image and the geometry of the input mesh, given a reference style image and a 3D mesh with its text description. Although diffusion-based 3D texture generation methods, such as distillation sampling, have numerous promising applications in stylized games and films, it requires addressing two challenges: 1) decouple style and content completely from the reference image for 3D models, and 2) align the generated texture with the color tone, style of the reference image, and the given text prompt. To this end, we introduce StyleTex, an innovative diffusion-model-based framework for creating stylized textures for 3D models. Our key insight is to decouple style information from the reference image while disregarding content in diffusion-based distillation sampling. Specifically, given a reference image, we first decompose its style feature from the image CLIP embedding by subtracting the embedding’s orthogonal projection in the direction of the content feature, which is represented by a text CLIP embedding. Our novel approach to disentangling the reference image’s style and content information allows us to generate distinct style and content features. We then inject the style feature into the cross-attention mechanism to incorporate it into the generation process, while utilizing the content feature as a negative prompt to further dissociate content information. Finally, we incorporate these strategies into StyleTex to obtain stylized textures. We utilize Interval Score Matching to address over-smoothness and over-saturation, in combination with a geometry-aware ControlNet that ensures consistent geometry throughout the generative process. The resulting textures generated by StyleTex retain the style of the reference image, while also aligning with the text prompts and intrinsic details of the given 3D mesh. Quantitative and qualitative experiments show that our method outperforms existing baseline methods by a significant margin.","We investigate an under-explored generation problem: style image-guided texture synthesis, which is crucial in computer vision and graphics, facilitating the creation of visually compelling and immersive digital environments in games and films. The generated texture needs to be harmonious with both the 3D shape and style of the reference image, which requires the texture to align with the geometry while conveying a consistent style from different views. Existing research mostly investigates the above two requirements separately. In 2D style-image generation methods, the style is conveyed by separating it from the reference image and incorporating it into the final output, which usually involves fine-tuning (Hu et al., 2022; Gal et al., 2023; Ruiz et al., 2023) the diffusion model to be a stylized image generator or adjusting the hidden layers of the diffusion model with the extracted style features (Jeong et al., 2024; Hertz et al., 2024; Wang et al., 2024; Voynov et al., 2023; He et al., 2024). In parallel, 3D texture can be generated by iteratively inpainting (Richardson et al., 2023; Chen et al., 2023c) or image synthesis with multi-view consistency (Cao et al., 2023; Liu et al., 2023b; Gao et al., 2024; Wu et al., 2024). More recently, distillation methods such as score distillation sampling (Metzer et al., 2023; Chen et al., 2023a; Youwang et al., 2023) have also proven their superior effectiveness in synthesizing 3D consistent textures. Compared to the direct generation of textures, distillation methods are capable of achieving better view and global style consistency while avoiding local seam problems. Despite the progress in these two distinct areas, incorporating the desired style into texture generation is not straightforward. One possible solution is to combine the distillation method with a diffusion distribution aligned with the reference image’s style. However, this leads to two challenges: 1) decoupling the style and content from the reference image entirely, and 2) preserving the color tone. Firstly, the ambiguity between style and content from different views complicates the decoupling process. In 2D domains, separating style and content within a single viewpoint may succeed in most situations. However, in 3D domains, failure to effectively decouple style from any single viewpoint can result in inaccurate style and unintended content leakage in the final texture. Thus, the generation of stylized textures in 3D domains requires a robust method for disentangling style and content. Secondly, distillation methods may result in over-saturation and over-smoothing within the generated textures, leading to color shifts and a lack of details, hindering the accurate reflection of the intended style. To overcome these challenges, we propose StyleTex, a diffusion-model-based pipeline to generate style textures under the guidance of a single image. Our key insight is to extract the style information from the reference image while disregarding the content information. Inspired by the multi-modal applications of the CLIP space, we propose to represent the content of the reference image as the CLIP embedding of its corresponding text prompt. A naive method to discard the content from the reference image in InstantStyle (Wang et al., 2024) is to drive the reference image embedding in the same CLIP space toward the opposite direction of the content embedding. However, the slight misalignment between the content embedding and the real content information of the image may cause undesirable image embedding alerting, which results in unclean content information remaining or color tone changing. To address this, we remove the content information from the reference image embedding by decomposing its CLIP embedding into two separate orthogonal features. One of these features aligns with the content embedding and encodes most of the content information of the reference image. We retain only the remaining feature, which predominantly relates to the style, to refine our diffusion model. To this end, we explicitly incorporate the style-relevant feature through the cross-attention mechanism, which also serves as a color tone guidance that can prevent unintentional color tone changing during the distillation process. Furthermore, we incorporate the content embedding as a negative prompt to further dissociate content information. We integrate the aforementioned strategies into StyleTex to generate stylized textures and utilize Interval Score Matching (ISM) (Liang et al., 2024) to further tackle the issue of over-smoothness. Moreover, we utilize a geometry-aware ControlNet to ensure geometric consistency throughout the generative process. In summary, our work makes the following major contributions: • A diffusion-model-based pipeline to generate style textures under the guidance of a single image, enabling the automatic creation of diverse stylized virtual environments. • A novel style decoupling and injection strategy that effectively guides stylization while addressing issues of content leakage and style deviation in texture generation."
https://arxiv.org/html/2411.00144v1,Self-Ensembling Gaussian Splatting for Few-shot Novel View Synthesis,"3D Gaussian Splatting (3DGS) has demonstrated remarkable effectiveness for novel view synthesis (NVS). However, the 3DGS model tends to overfit when trained with sparse posed views, limiting its generalization capacity for broader pose variations. In this paper, we alleviate the overfitting problem by introducing a self-ensembling Gaussian Splatting (SE-GS) approach. We present two Gaussian Splatting models named the 𝚺𝚺\mathbf{\Sigma}bold_Σ-model and the 𝚫𝚫\mathbf{\Delta}bold_Δ-model. The 𝚺𝚺\mathbf{\Sigma}bold_Σ-model serves as the primary model that generates novel-view images during inference. At the training stage, the 𝚺𝚺\mathbf{\Sigma}bold_Σ-model is guided away from specific local optima by an uncertainty-aware perturbing strategy. We dynamically perturb the 𝚫𝚫\mathbf{\Delta}bold_Δ-model based on the uncertainties of novel-view renderings across different training steps, resulting in diverse temporal models sampled from the Gaussian parameter space without additional training costs. The geometry of the 𝚺𝚺\mathbf{\Sigma}bold_Σ-model is regularized by penalizing discrepancies between the 𝚺𝚺\mathbf{\Sigma}bold_Σ-model and the temporal samples. Therefore, our SE-GS conducts an effective and efficient regularization across a large number of Gaussian Splatting models, resulting in a robust ensemble, the 𝚺𝚺\mathbf{\Sigma}bold_Σ-model. Experimental results on the LLFF, Mip-NeRF360, DTU, and MVImgNet datasets show that our approach improves NVS quality with few-shot training views, outperforming existing state-of-the-art methods. The code is released at:https://github.com/sailor-z/SE-GS.","Novel view synthesis (NVS) is a critical task [49] in computer vision and graphics, playing a pivotal role in applications such as virtual reality [8], augmented reality [50], and 3D content generation [15, 42]. The objective of NVS is to generate photo-realistic images from previously unseen viewpoints. Typically, NVS starts by constructing a 3D representation [26, 39] from a set of existing 2D observations. In recent years, 3D Gaussian Splatting (3DGS) [20, 6, 47] has emerged as a powerful representation, integrating the advantages of both explicit [33] and implicit [26] representations. This approach enables efficient novel view generation and yields promising synthesized results with densely sampled observations that cover a wide range of viewpoints. (a) Training (b) Testing Figure 2: Overfitting in 3D Gaussian Splatting with few-shot training views. 2(a) and 2(b) illustrate the performance of 3DGS on training and testing views, respectively. Each curve represents the PSNR values across different training iterations. However, 3DGS tends to overfit the available views when only a limited number of images are provided. As shown in Fig. 2, we evaluate the 3DGS model trained on sparse images with different numbers of iterations. The performance on the training data consistently improves as the number of iterations increases, while the testing results deteriorate after 2000 iterations. Moreover, the overfitting issue becomes more noticeable with fewer training views, such as when using only 3 views. To address this problem, we introduce a new 3DGS approach in this paper, which enhances the quality of novel view synthesis with sparse training views via self-ensembling. Self-ensembling [11, 21, 27] has been witnessed in the literature as an effective strategy to mitigate overfitting, particularly in scenarios where limited data is available [45]. This technique leverages multiple predictions from the model under different conditions such as dropout [37] and data augmentation [36], forming a consensus prediction via regularization. By aggregating outputs with diverse perturbations, self-ensembling creates a more stable target for training, reducing the noise in individual predictions. Despite its success, how to utilize self-ensembling in the 3DGS pipeline remains an open problem. Therefore, we explore this gap, introducing a new 3DGS training pipeline consisting of a 𝚺𝚺\mathbf{\Sigma}bold_Σ-model and a 𝚫𝚫\mathbf{\Delta}bold_Δ-model. Specifically, we present a 𝚫𝚫\mathbf{\Delta}bold_Δ-model that represents a temporal sample in the Gaussian parameter space. Notably, in the scenario of sparse views, the training data lacks sufficient information to identify an optimal sample in the parameter space. As a result, the training process tends to converge to a local optimum, leading to an overfitting problem. To tackle this issue, one could create more samples by training multiple 𝚫𝚫\mathbf{\Delta}bold_Δ-models, thereby increasing the likelihood of reaching a global optimum. Nevertheless, as we will demonstrate in Sec. 4, this approach incurs significant computational costs, limiting its ability to produce diverse samples. In contrast, we train a single 𝚫𝚫\mathbf{\Delta}bold_Δ-model following the typical 3DGS training strategy [20], while obtaining various samples via perturbing the 𝚫𝚫\mathbf{\Delta}bold_Δ-model. Rather than applying random perturbations, which often result in invalid samples, we perturb the 𝚫𝚫\mathbf{\Delta}bold_Δ-model based on uncertainties derived from the training data. We store images generated by rendering the 𝚫𝚫\mathbf{\Delta}bold_Δ-model from a set of pseudo views in buffers at different training iterations. These pseudo views are randomly sampled from the camera trajectories of the training views. We assess the reliability of the renderings by calculating the pixel-level uncertainties across the images within each buffer. Pixels with large uncertainty scores are identified, and the Gaussian parameters of the 𝚫𝚫\mathbf{\Delta}bold_Δ-model associated with these pixels are perturbed by random noise. Therefore, our perturbation strategy generates uncertainty-aware temporal samples without introducing significant training overhead. Building upon the 𝚫𝚫\mathbf{\Delta}bold_Δ-model, we construct an ensemble of the temporal samples by proposing a 𝚺𝚺\mathbf{\Sigma}bold_Σ-model. We train the 𝚺𝚺\mathbf{\Sigma}bold_Σ-model over the training views without the aforementioned perturbations. To achieve self-ensembling, we regularize the geometry of the 𝚺𝚺\mathbf{\Sigma}bold_Σ-model, minimizing the discrepancies between the 𝚺𝚺\mathbf{\Sigma}bold_Σ-model and the perturbed 𝚫𝚫\mathbf{\Delta}bold_Δ-model. The discrepancies are measured through a photometric loss between images synthesized from the pseudo views. During testing, we employ the 𝚺𝚺\mathbf{\Sigma}bold_Σ-model as the primary model for novel view synthesis. Since the regularization is performed based on diverse samples in the Gaussian parameter space, the self-ensembling process enhances the robustness of the 𝚺𝚺\mathbf{\Sigma}bold_Σ-model, thereby improving the generalization ability to novel views. We conduct experiments on several datasets consisting of LLFF [25], DTU [19], Mip-NeRF360 [3], and MVImgNet [46]. The evaluation is performed with varying numbers of training views. Our SE-GS achieves the best performance across all the datasets when limited training views are used, surpassing the state-of-the-art approaches. Additionally, we provide a comprehensive analysis in which the results demonstrate the effectiveness and efficiency of our method. To the best of our knowledge, we are the first to explore the potential of the ensembling mechanism in 3DGS for few-shot novel view synthesis. We present a new self-ensembling technique that effectively mitigates the overfitting of 3DGS without requiring additional ground-truth signals or incurring significant training overhead."
