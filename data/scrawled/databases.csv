URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.04525v1,GenJoin: Conditional Generative Plan-to-Plan Query Optimizer that Learns from Subplan Hints,"Query optimization has become a research area where classical algorithms are being challenged by machine learning algorithms. At the same time, recent trends in learned query optimizers have shown that it is prudent to take advantage of decades of database research and augment classical query optimizers by shrinking the plan search space through different types of hints (e.g. by specifying the join type, scan type or the order of joins) rather than completely replacing the classical query optimizer with machine learning models. It is especially relevant for cases when classical optimizers cannot fully enumerate all logical and physical plans and, as an alternative, need to rely on less robust approaches like genetic algorithms. However, even symbiotically learned query optimizers are hampered by the need for vast amounts of training data, slow plan generation during inference and unstable results across various workload conditions. In this paper, we present GenJoin - a novel learned query optimizer that considers the query optimization problem as a generative task and is capable of learning from a random set of subplan hints to produce query plans that outperform the classical optimizer. GenJoin is the first learned query optimizer that significantly and consistently outperforms PostgreSQL as well as state-of-the-art methods on two well-known real-world benchmarks across a variety of workloads using rigorous machine learning evaluations.","Query optimization remains an active area of research for learned query optimizers (LQOs). In recent years, increasingly sophisticated methods have been developed for both cardinality estimation (CE) (Reiner and Grossniklaus, 2023; Kipf et al., 2018; Yang et al., 2020; Wu et al., 2020; Hilprecht et al., 2019; Zhu et al., 2020; Wu et al., 2023; Liu et al., 2021; Hilprecht and Binnig, 2022; Zhao et al., 2022) and join order selection (JOS) (Krishnan et al., 2018; Marcus and Papaemmanouil, 2018a; Yang et al., 2022; Marcus et al., 2019; Heitz and Stockinger, 2019; Yu et al., 2022, 2020; Chen et al., 2023b; Zhu et al., 2023; Chen et al., 2023a; Marcus et al., 2022; Xu et al., 2023; Woltmann et al., 2023; Anneser et al., 2023). CE approaches typically use statistical and machine learning models to approximate multivariate distributions over database table attributes (Li et al., 2021). The resulting cardinality estimates serve as an input to the cost models of query optimizers. At the same time, JOS models are considered to be the ”brain” of query optimizers, whose outputs are logical and physical query plans (Ding et al., 2024). Figure 1. Illustration of how GenJoin is different from two main streams of learned query optimizers for evaluating the query A⋈B⋈C⋈D⋈𝐴𝐵⋈𝐶⋈𝐷A\Join B\Join C\Join Ditalic_A ⋈ italic_B ⋈ italic_C ⋈ italic_D: step-by-step plan generation and hint set methods. NL: nested loop join, HJ: hash join, MJ: merge join, SeqS: sequential scan, IdxS: index scan, K: number of hints, T: number of tables. Starting from Cascades (Graefe, 1995), JOS was mainly considered as dynamic programming (DP) (Cormen et al., 2009) task, naturally assuming a join choice as a step in top-down or bottom-up plan construction. With the rise of deep learning and reinforcement learning (RL) (Sutton and Barto, 2018) as a logical continuation of the DP ideas (Li, 2023), classical query optimizers started being challenged by LQOs. Such step-by-step plan-building models control the full target plan specifications like the types of joins used, which scans to apply and in what order to join the tables, via a set of explicit hints for the RDBMS111Using extensions such as pg_hint_plan: https://pg-hint-plan.readthedocs.io/en/latest/.. We call these approaches full plan-level hint222The complexity of the potential prediction space is O⁢(TT)𝑂superscript𝑇𝑇O(T^{T})italic_O ( italic_T start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ) (Wang and Chen, 1996) scaling with the number of tables T𝑇Titalic_T. methods (see second row of Figure 1). The trend of producing complete query plans as output was first questioned by Bao (Marcus et al., 2022). The idea is to use classical optimizers and empower them by just giving high-level hints like ’enable merge_join’ rather than building complete query plans. In such a way, the exact join order choice is made by the built-in classical optimizer, though its search space is constrained through the provided query-level hints333Each hint can be turned on or off independently, resulting in a potential prediction space complexity of O⁢(2K)𝑂superscript2𝐾O(2^{K})italic_O ( 2 start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT ) scaling with the number of hints K𝐾Kitalic_K. (see third row of Figure 1). Despite the progress made, LQOs have yet to yield significant improvements upon traditional approaches, and consistently outperforming them continues to be an elusive goal (Lehmann et al., 2024). We argue, that the following weaknesses persist in today’s LQOs: (1) Inefficient and limited reinforcement learning. Typical RL agents can take suboptimal steps and still end up with strong solutions due to the fact that either the game takes many steps to finish, and/or there is a possibility of stepping back. However, for query optimization and, in particular, bottom-up generation of join orders, every step is non-revertable and has a significant impact on further steps with potentially fatal consequences in case of a bad choice. (2) Hint-based methods are double-edged swords. When recommending only general hint sets, methods like Bao leave a high degree of freedom to the classical optimizers to choose the order of joins. However, any hint, such as ‘disable_hashjoin‘, impacts every single join in a query, limiting the query plan subspace like in k-d tree search (Panigrahy, 2008) at a coarse level of granularity and thus reducing the chance to find the optimal solution. (3) Requirement for vast amounts of (training) data. An important goal of query optimization is to learn the distributions and correlations of various attributes within and across tables in order to estimate the join cardinality. However, due to minute differences in query predicates, LQOs need to sample large amounts of query workloads to reach good generalization capabilities. (4) Finding optimal solutions with machine learning is very time-intensive. Certain machine learning algorithms do indeed find better query plans than traditional classical approaches. However, the time to find the solutions is often prohibitive due to the computational overhead for encoding queries or when using a model for inference which often greedily (though with pruning) explores the plan space. Previous approaches often ignored these performance aspects and only focused on execution time as their only metric. As a possible way to mitigate these LQOs’ hurdles, we developed GenJoin - a novel, generative plan-to-plan query optimizer that learns from subplan hints444The complexity of the potential prediction space is O⁢(T2)𝑂superscript𝑇2O(T^{2})italic_O ( italic_T start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) since all pairs of tables T𝑇Titalic_T can participate in a subplan hint. limited to join types such as nested loop join, merge join and hash join. GenJoin uses a conditional variational autoencoder as machine learning architecture and suggests the ”golden middle” between forcing exact query plans and giving a set of general hints (see bottom row of Figure 1). The output of the GenJoin model is a set of two-way-join hints (or subplan hints), e.g., use merge join on tables A𝐴Aitalic_A and B𝐵Bitalic_B, i.e. M⁢J⁢(A,B)𝑀𝐽𝐴𝐵MJ(A,B)italic_M italic_J ( italic_A , italic_B ), hash join on tables C𝐶Citalic_C and B𝐵Bitalic_B, i.e. H⁢J⁢(C,B)𝐻𝐽𝐶𝐵HJ(C,B)italic_H italic_J ( italic_C , italic_B ), or nested loop join on tables D𝐷Ditalic_D and A𝐴Aitalic_A, i.e. N⁢L⁢(D,A)𝑁𝐿𝐷𝐴NL(D,A)italic_N italic_L ( italic_D , italic_A ). GenJoin does not specify exactly where in the join tree this particular join should be performed (if performed at all) but leaves it up to the classical optimizer to decide. Also, this way, GenJoin gives the classical optimizer the freedom to choose all kinds of plans including bushy ones. More to say, GenJoin enables the classical optimizer to discover parts of the query plan space that it would not explore itself. For instance, the classical optimizer wanted to do H⁢J⁢(D,A)𝐻𝐽𝐷𝐴HJ(D,A)italic_H italic_J ( italic_D , italic_A ) initially, but GenJoin recommends doing N⁢L⁢(D,A)𝑁𝐿𝐷𝐴NL(D,A)italic_N italic_L ( italic_D , italic_A ) instead. This way, the classical optimizer might decide not to join (D,A)𝐷𝐴(D,A)( italic_D , italic_A ) at all, which will push it to search for alternatives that were initially discarded. Why does GenJoin recommend only the join type and neither the type of scans nor the join order? All the LQOs, including GenJoin, rely on internal RDBMS subquery cardinality estimations based on pre-calculated statistics. We never know if the RDBMS considers, e.g., a sequential scan over an index scan due to high predicate selectivity. Moreover, we also do not know what the RDBMS has already cached and LQOs typically ignore what is currently indexed and how. This applies that recommending just the join type gives the RDBMS the ability to solve those parts where it is more knowledgeable than we are - e.g., when performing a merge join, one table would require sorting, so it might be beneficial to perform an index scan. We choose not to recommend the full join order but only the join type to avoid overfitting and to enable the learned query optimizer to generalize better in a smaller search space. The major contributions of our paper are as follows: • We introduce GenJoin, a generative conditional query optimizer that learns from subplan hints using a conditional variational autoencoder-inspired architecture. GenJoin enhances query plans by pruning the search space of join types of the built-in optimizer to greatly boost their query execution performance. • We show, for the first time, that a learned query optimizer consistently outperforms PostgreSQL as well as state-of-the-art methods on the two well-known real-world benchmarks JOB and STACK across a variety of workloads using rigorous machine learning evaluations. • GenJoin is not only optimized for producing plans with low execution times but also minimizes the required inference time of the machine learning models during query execution. • Inside GenJoin, we introduce a new way of measuring the distance between query plans via unnormalized difference of execution times and the corresponding p-value of a T-test for the means of plan execution time samples, which can be both used for training purposes and for calculating the confidence intervals of the query optimizers’ differences."
https://arxiv.org/html/2411.04319v1,Towards Optimizing SQL Generationvia LLM Routing,"Text-to-SQL enables users to interact with databases through natural language, simplifying access to structured data. Although highly capable large language models (LLMs) achieve strong accuracy for complex queries, they incur unnecessary latency and dollar cost for simpler ones. In this paper, we introduce the first LLM routing approach for Text-to-SQL, which dynamically selects the most cost-effective LLM capable of generating accurate SQL for each query.We present two routing strategies (score- and classification-based) that achieve accuracy comparable to the most capable LLM while reducing costs. We design the routers for ease of training and efficient inference. In our experiments, we highlight a practical and explainable accuracy-cost trade-off on the BIRD dataset.","In recent years, Text-to-SQL has gained significant momentum with deployments within enterprise solutions to transform data accessibility [1, 13]. Text-to-SQL democratizes access to structured data, allowing non-experts to interact with databases directly without requiring data engineering expertise. For SQL analysts, it enhances their workflows by supporting query authoring, dataset exploration, and report generation. A major use case lies in iterative data exploration, where the complexity of user queries can range widely—from simple row retrievals to multi-way joins with aggregations. Current state-of-the-art Text-to-SQL approaches follow a multi-stage pipeline [5, 7, 9, 16]. The pipeline consists of two main phases: (i) retrieval of contextual information–such as schema elements, examples, and instructions relevant to the query–and (ii) SQL generation. Current enterprise solutions use highly capable LLMs for SQL generation to handle highly complex queries [10, 11]. While this is essential for complex queries, such highly capable LLMs introduce considerable latency and incur higher costs for simpler ones, such as inspecting a few rows from a table. This in turn, can negatively impact both user experience and the average cost per query. Leading Text-to-SQL benchmarks such as BIRD [8] rank submissions only based on accuracy while also indicating the model used on the leaderboard. Benchmarks typically assume a single-model approach, which mirrors the same inefficiencies found in enterprise deployment—namely, using the most capable models for all queries regardless of complexity. For instance, six of the top ten solutions on BIRD use GPT-4o or Gemini. To handle efficiently the varying levels of complexity, we investigate the implementation of LLM routers for Text-to-SQL. They route a query to the weakest, yet cheaper and faster model, capable of generating accurate SQL. We propose two routing approaches that achieve an accuracy close to that of the most capable LLM, always outperforming the second-best, and reducing costs by up to 1.4×1.4\times1.4 ×. This cost reduction is substantial for enterprise deployments of analytics or SQL assistants with a high volume of queries. These routers are designed to be easy to train and efficient at inference. Fig. 1 depicts our pipelines with an SQL generation router."
https://arxiv.org/html/2411.04304v1,Don’t go gaga with GIGO,"We revisit integrity checking in relational and deductive databases with an approach that tolerates erroneous, inconsistent data. In particular, we relax the fundamental prerequisite that, in order to apply any method for simplified integrity checking, all data must initially have integrity. As opposed to a long-standing belief, integrity in the old state before the update is not needed for a correct application of simplification methods. Rather, we show that correct simplifications preserve what was consistent across updates. We formally characterize this property, that we call inconsistency tolerance, and state its validity for some well-known methods for integrity checking.","Integrity checking has been a perennial topic in almost all database conferences, journals and research labs. The motivation behind this is that integrity checking is practically unfeasible for significant amounts of stored data without a dedicated approach to optimize the process. Progress has been made with extensions of basic approaches in several database areas. However, the fundamental ideas that are already present in the seminal paper [32] have not changed much. The basic principle is that, in most cases, a simplified form of the set of integrity constraints imposed on the database can be obtained from a given update (or just an update schema) and the current state of the database (or just the database schema). Thus, integrity, which is supposed to be an invariant of all possible database states, is checked upon each update request, which in turn is authorized only if the simplified check yields that integrity is not violated. Here, “simplified” essentially means “more efficiently evaluated at update time”. The “Garbage-In, Garbage-Out” (GIGO) problem may show up in a database whenever it contains information that is considered erroneous or unwanted, i.e., data that violate the integrity constraints. Then, answers to queries cannot be trusted, since they may be (partly or totally) composed of tuples coming from such erroneous data. This problem has typically been addressed in simplification methods to integrity checking in a very drastic way: in order to avoid “Garbage-Out” (possibly wrong answers), “Garbage-In” (possibly incorrect data in the database tables) needs to be completely prevented, even for the applicability of the simplification method itself. But this is most often unrealistic: the total absence of unwanted, incorrect or unexpected data is definitely an exception in virtually all real-world scenarios. Still, it is desirable to preserve the “good” data in the database while preventing further garbage from sneaking in and, thus, diminish trustability of query answers. More formally, in [32] and in virtually all publications on the same subject that came after it, a categorical premise for the correctness of the simplification approach has been that the constraints to be checked for a given update U𝑈Uitalic_U are satisfied in the “old” state, i.e., the database state given when U𝑈Uitalic_U is requested. Otherwise, correctness of simplification is not guaranteed. As opposed to the attention granted to integrity checking in academia, support for the declarative specification and efficient evaluation of semantic integrity in practical systems has always been relatively scant, apart from standard constructs such as constraints on column values and foreign keys in relational database tables. Various reasons have been identified for this lack of practical attention. Among them, the logically abstract presentation of many of the known simplification and the possible loss of performance due to integrity checking are often mentioned. Here, we focus on another issue of integrity checking which is responsible for a severe mismatch between theory and practice: hardly any database ever is in a perfectly consistent state with regard to its intended semantics. Clearly, this contradicts the fundamental premise that the database must always satisfy integrity. Thus, due to the intolerance of classical logic wrt inconsistency, integrity checking is very often not considered an issue of practical feasibility or even relevance. In this paper, we argue that inconsistency is far less harmful for database integrity than as suggested by commonly established results. We substantiate our claim by showing that, informally speaking, the consistent part of a possibly inconsistent database can be preserved across updates. More precisely, we show that, if the simplified form of an integrity theory is satisfied, then each instance of each constraint that has been satisfied in the old state continues to be satisfied in the new state, even if the database is not fully consistent. Such an approach is therefore tolerant wrt the presence of “Garbage-In” and yet helps preventing occurrences of “Garbage-Out”."
https://arxiv.org/html/2411.04920v1,GPTKB: Building Very Large Knowledge Bases from Language Models,"General-domain knowledge bases (KB), in particular the “big three” –Wikidata, Yago and DBpedia– are the backbone of many intelligent applications. While these three have seen steady development, comprehensive KB construction at large has seen few fresh attempts.In this work, we propose to build a large general-domain KB entirely from a large language model (LLM). We demonstrate the feasibility of large-scale KB construction from LLMs, while highlighting specific challenges arising around entity recognition, entity and property canonicalization, and taxonomy construction. As a prototype, we use GPT-4o-mini to construct GPTKB, which contains 105 million triples for more than 2.9 million entities, at a cost 100x less than previous KB construction projects.Our work is a landmark for two fields: For NLP, for the first time, it provides constructive insights into the knowledge (or beliefs) of LLMs. For the Semantic Web, it shows novel ways forward for the long-standing challenge of general-domain KB construction. GPTKB is accessible at https://gptkb.mpi-inf.mpg.de.","Figure 1: Overview of our approach for LM-based KB construction. General-world knowledge bases (KB) like Wikidata Vrandecic and Krötzsch (2014), Yago Suchanek et al. (2007) and DBpedia Auer et al. (2007) are important backbones for intelligent applications. While these projects exist for over a decade, innovation in the field of general-world KB construction is low, with neither fundamental paradigm shifts nor major novel projects emerging Weikum et al. (2021). Recently, large language models (LLMs) stirred up many fields of AI Bubeck et al. (2023), and also been proposed as sources for structured knowledge Petroni et al. (2019). More specifically, Cohen et al. (2023) showed how, in principle, one can build a KB from an LLM by simple factual prompts and iterative graph expansion, but have not attempted this at scale. The success of LLMs has also raised important intrinsic questions, in particular, what and how much these models know or believe Petroni et al. (2019); Jiang et al. (2020); Roberts et al. (2020); Veseli et al. (2023); Sun et al. (2024); Wu et al. (2024b). A large set of benchmarks and studies investigate this via example-based prompting, e.g., to determine, how many answers to common benchmarking question answering (QA) datasets are known by LLMs. However, all these works remain non-exhaustive, investigating samples from specific datasets or domains, without attempting to materialize all knowledge of an LLM. In this paper we propose to comprehensively materialize the knowledge/beliefs111The terminology here is contentious, see Section 6.1. of LLMs into a KB. In particular, we propose to use iterative graph expansion to obtain the complete set of (named-entity-centric) LLM knowledge, and to consolidate that knowledge with LLM-based entity disambiguation, class and relation canonicalization, and taxonomy induction (see Fig. 1). This proposal faces several challenges: 1. Termination/cost/runtime: State-of-the-art KBs contain millions of entries, and LLMs are known to hallucinate. It is therefore not clear if and when iterative graph exploration will terminate, and how to perform it under practical monetary and time constraints. 2. Flexible and relevant knowledge elicitation: LLMs possess a wide variety of knowledge, and the amount of knowlege per entity varies. We need a method that elicits as much general-world knowledge as possible, without encouraging hallucinations, and without falling into bottomless corners, e.g., open-ended phrases or translations. 3. Canonicalization and disambiguation: Hallmarks of existing KBs are disambiguated entities and coherent classes, relations and taxonomies. Iterative prompting risks surfacing expressions that are not globally coherent. Our approach builds on the following ideas: To overcome scaling issues, and obtain relevant knowledge, we utilize a commercial API that allows to massively send batch requests (GPT-4o-mini), and utilize named entity recognition (NER) and carefully crafted prompts to restrict the explored spaces, along with prompts that allow varied answer sizes. To obtain a coherent KB, we perform a set of canonicalization and disambiguation steps, entirely relying on the LLM itself. In summary, our salient contributions are: 1. To the best of our knowledge, we are the first to propose to construct comprehensive KBs entirely from LLMs. 2. We develop a KB construction pipeline that overcomes termination, quality and runtime issues. 3. We construct GPTKB, the first large-scale KB entirely built from an LLM, containing over 104M assertions for over 2.9M entities. Our work is a significant step for two communities: For the NLP community, for the first time, we provide a proof-of-concept methodology that enables constructive insights into what LLMs know (or believe). For the Semantic Web community, we provide fresh momentum for the long stale task of open-domain KB construction, and provide code, a concrete resource, GPTKB, both as a 3.8 GB download, and via an online browsing interface and SPARQL query interface at https://gptkb.mpi-inf.mpg.de."
https://arxiv.org/html/2411.04638v1,QCE’24 Tutorial: Quantum Annealing – Emerging Exploration for Database Optimization,"Quantum annealing is a meta-heuristic approach tailored to solve combinatorial optimization problems with quantum annealers. In this tutorial, we provide a fundamental and comprehensive introduction to quantum annealing and modern data management systems and show quantum annealing’s potential benefits and applications in the realm of database optimization. We demonstrate how to apply quantum annealing for selected database optimization problems, which are critical challenges in many data management platforms. The demonstrations include solving join order optimization problems in relational databases, optimizing sophisticated transaction scheduling, and allocating virtual machines within cloud-based architectures with respect to sustainability metrics. On the one hand, the demonstrations show how to apply quantum annealing on key problems of database management systems (join order selection, transaction scheduling), and on the other hand, they show how quantum annealing can be integrated as a part of larger and dynamic optimization pipelines (virtual machine allocation). The goal of our tutorial is to provide a centralized and condensed source regarding theories and applications of quantum annealing technology for database researchers, practitioners, and everyone who wants to understand how to potentially optimize data management with quantum computing in practice. Besides, we identify the advantages, limitations, and potentials of quantum computing for future database and data management research.","Quadratic Unconstrained Binary Optimization (QUBO) problems are a class of optimization problems formulated as quadratic functions with binary variables. Quantum annealers are designed to solve QUBO problems by mapping them directly onto the quantum hardware. Reasons for database (DB) experts to deal with quantum annealing (QA) are: Easy to learn: The first steps in using QA can be done by just studying QUBO problems and how to solve them on quantum annealers. To build upon initial research effectively, a deeper background in quantum computing (QC) is beneficial to understand which formulations of constraints are suitable and which are too complex to run on quantum annealers. For instance, in more intricate problems, the translation of linear and quadratic coefficients from a QUBO problem into qubit bias and coupling values for the quantum annealer does not scale well. Importance of quantum annealing for DB research: Analyzing survey papers about QC for DBs, the solutions of certain subareas of quantum accelerated DBs are dominated by approaches that can be run on a quantum annealer. Specifically, in the subareas of database query optimization and transaction management, 75% of the contributions described in [1] utilize quantum annealers. Approximating NP-hard problems: For some NP-hard problems like the subset sum and the maximum cut problems, the numbers of variables in the corresponding QUBO formulas scale linearly with the problem sizes [2]. The belief that quantum annealers will enjoy an increase in performance in the future may be a positive indicator of scalability and quantum benefits. However, careful benchmarking of quantum against classical optimization approaches exposes issues that need to be addressed, and finding concrete quantum speedups remains a challenge. Tutorial overview: After motivating QA for DB research, we provide a dive into the basics of QA in Section II. In Section III, we introduce our DB use cases for the QA demo. In the hands-on part, we show how to apply QA to the described DB use cases in Section IV. Finally, we discuss QA opportunities and future work for DB research in Section V. Related tutorials: Our previous tutorial in SIGMOD [3] has already addressed the benefits of quantum machine learning to data management tasks with a focus on query optimization. However, to the best of our knowledge, no tutorial contains an introduction to QA tailored to DB research. In order to provide a solid basis for future work about QA accelerated DBs, we will introduce the basics of QA and deliver a hands-on tutorial on applying QA to three important data management tasks—join order optimization, transaction scheduling, and virtual machine and task allocation. Contributions: To the best of our knowledge, this is the first tutorial to discuss QA approaches for DB research. This tutorial helps DB experts get into QA easily (by providing a dense introduction), newbies and practitioners discover the possibilities of QA by showing how to interact with quantum annealers with code examples, and researchers develop new approaches utilizing QA."
https://arxiv.org/html/2411.04579v1,Towards Robust Federated Analytics via Differentially Private Measurements of Statistical Heterogeneity,"Statistical heterogeneity is a measure of how skewed the samples of a dataset are. It is a common problem in the study of differential privacy that the usage of a statistically heterogeneous dataset results in a significant loss of accuracy. In federated scenarios, statistical heterogeneity is more likely to happen, and so the above problem is even more pressing. We explore the three most promising ways to measure statistical heterogeneity and give formulae for their accuracy, while simultaneously incorporating differential privacy. We find the optimum privacy parameters via an analytic mechanism, which incorporates root finding methods. We validate the main theorems and related hypotheses experimentally, and test the robustness of the analytic mechanism to different heterogeneity levels. The analytic mechanism in a distributed setting delivers superior accuracy to all combinations involving the classic mechanism and/or the centralized setting. All measures of statistical heterogeneity do not lose significant accuracy when a heterogeneous sample is used.","Processing sensitive data in distributed environments, where the data is distributed across multiple devices, is becoming more common. This is caused by the use of mobile devices increasing rapidly in the past decade or so, and companies such as Microsoft, Google, Apple and Dropbox advertising cloud storage options for ease of data access on all devices [1]. Too often, when analyzing data from multiple devices, an assumption is made that each local training dataset is independent and identically distributed (i.i.d.) [2]. However, in the real world most mobile devices differ greatly in terms of the apps stored, messages set, audio saved, etc. Therefore, we need to find a way to measure and compensate for the difference, or statistical heterogeneity, in this data. First, we give an overview of distributed environments, including the development of federated scenarios. Federated learning (FL) [3] is an alternative to traditional centralized machine learning techniques where a single central server coordinates a large number of participating devices or clients to provide results for its global model. In each round, the server provides a subset of the clients with its current global model, after which each selected client uses its own local training dataset to generate results which it sends back to the server. The server then aggregates these results, and uses the overall trends to improve its global model. Federated analytics (FA) [4] complements FL by having the ability to measure, analyse and improve FL models, while simultaneously removing the need for the transfer or release of results from any individual client. Real-world examples which use FA and FL include predicting the most likely word typed by a user given a sequence of the first few characters [5], and the Now Playing audio identification feature on Google’s Pixel phones [6]. Although both of these federated scenarios attempt to protect the privacy of clients, via the use of decentralized training datasets and secure aggregation, there are no direct measures to perturb the raw data itself. Therefore, for privacy purposes, FL and FA can be combined with differential privacy (DP), a mathematical definition that provides a strong, provable guarantee for the privacy of data. Another problem with Federated Machine Learning systems such as FL and FA is that the accuracy of their models depends on the results being gathered from each small sample of clients being representative of those of the overall population. However, in the real world this is not usually true: each individual typically has a particular style of messaging, a specific music taste and would use a select few favorite apps in their chosen way. Therefore, the data of one client would give vastly different results for the word prediction and audio identification tasks described above, compared to the mean of a population. Statistical heterogeneity (SH) is a term that can be used to describe a dataset if the observed trends from each sample differ from that of the overall population more than would be expected due to random error alone. Therefore, it is likely that the general trends inferred from a statistically heterogeneous dataset will be less accurate than those from a non-SH dataset, unless there is a way to manipulate the way in which clients are chosen to favor those that will bring greater accuracy. Furthermore, the combination of non-standard data with privacy is not as well studied as when an i.i.d. assumption can be put in place. Our objective is to allow a quantitative measure of the degree of SH to be made. In Section 2, we summarize the contributions that explore the ideas and themes of this work. We look at the progress that has been made so far to tackle SH in the FA setting. In Section 3, we give formal definitions of necessary concepts, such as DP and the measures of SH that we study. In Section 4, we build results for the mean squared error and a 95% confidence interval from first principles for the measures of dispersion, Q𝑄Qitalic_Q and I2superscript𝐼2I^{2}italic_I start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, using links between them to develop the more complex formulae in Sections 4.2 and 4.3. In Section 5, we present an experimental study to support these theoretical results."
https://arxiv.org/html/2411.04443v1,ACCIO: Table Understanding Enhancedvia Contrastive Learning with Aggregations,"The attention to table understanding using recent natural language models has been growing. However, most related works tend to focus on learning the structure of the table directly. Just as humans improve their understanding of sentences by comparing them, they can also enhance their understanding by comparing tables. With this idea, in this paper, we introduce ACCIO, tAble understanding enhanCed via Contrastive learnIng with aggregatiOns, a novel approach to enhancing table understanding by contrasting original tables with their pivot summaries through contrastive learning. ACCIO trains an encoder to bring these table pairs closer together. Through validation via column type annotation, ACCIO achieves competitive performance with a macro F1 score of 91.1 compared to state-of-the-art methods. This work represents the first attempt to utilize pairs of tables for table embedding, promising significant advancements in table comprehension. Our code is available at https://github.com/whnhch/ACCIO/.","Year Month Passengers 1949 January 112 1949 February 118 1949 March 132 1949 April 129 1949 May 121 (a) Month 1949 1950 1951 April 129 135 163 August 148 170 199 December 118 140 166 February 118 126 150 January 112 115 145 (b) Table 1: Passenger data and pivot table. (a) is the original tabular data containing year and month passenger attributes. (b) is a pivot table with a user’s query of “the average number of passengers by month and year” Leveraging the success of natural language processing techniques, the comprehension of tables has also significantly grown. Many works related to understanding tables have been helpful in applications, such as column type annotation, joining relation databases from data lakes, table to visualization, and table normalization. These efforts typically analyze tables based on the structure of the table, column relationships, or entity associations. However, to our knowledge, no prior research has been aimed at enhancing table understanding by comparing two tables. In the realm of sentence embeddings, success has been achieved by comparing sentences using techniques like SBERT Reimers and Gurevych (2019) or SimCSE Gao et al. (2021), leveraging natural language inference (NLI) datasets Bowman et al. (2015); Nie et al. (2020). These datasets typically contain triplets consisting of a premise, an entailment, and a contradiction. The entailment sentence can be logically inferred from the premise, while the contradiction sentence directly contradicts the premise. Previous studies Reimers and Gurevych (2019); Gao et al. (2021) have introduced methods such as closing the entailment sentence and premise and contrasting the premise with the contradiction sentence, ultimately leading to high-quality sentence embeddings. Therefore, in this paper, we exploit the notion that premise and entailment sentences should be closely related by leveraging pivot tables and the original table as the tables that should be conceptually close. Pivot tables are summaries of tables using aggregation by user-defined parameters such as index, column, value, and aggregation function. For instance, Table 1(a) represents the original tabular data consisting of year, month, and passengers. Users typically analyze or summarize tables by using pivot tables derived from such data. For example, when a user wants to determine “the average number of passengers by month and year,” they can obtain a pivot result like Table 1(b). We present a novel approach to table understanding, called ACCIO, tAble understanding enhanCed via Contrastive learnIng with aggregatiOns. By training an encoder with original data and its corresponding pivot table using contrastive learning, we aim to bring them closer together. It’s worth highlighting that this paper marks the first attempt to utilize a pair of tables for table embedding. We validate our training method through a downstream task known as column type annotation. This task is commonly used to evaluate the quality of table embeddings by predicting the types of given columns. The performances indicate that our approach achieves comparable performance in terms of macro F1 score 91.1 for column type annotation compared to state-of-the-art baselines."
https://arxiv.org/html/2411.03500v1,λ𝜆\lambdaitalic_λ-Tune: Harnessing Large Language Models for Automated Database System Tuning,"We introduce λ𝜆\lambdaitalic_λ-Tune, a framework that leverages Large Language Models (LLMs) for automated database system tuning. The design of λ𝜆\lambdaitalic_λ-Tune is motivated by the capabilities of the latest generation of LLMs. Different from prior work, leveraging LLMs to extract tuning hints for single parameters, λ𝜆\lambdaitalic_λ-Tune generates entire configuration scripts, based on a large input document, describing the tuning context. λ𝜆\lambdaitalic_λ-Tune generates alternative configurations, using a principled approach to identify the best configuration, out of a small set of candidates. In doing so, it minimizes reconfiguration overheads and ensures that evaluation costs are bounded as a function of the optimal run time. By treating prompt generation as a cost-based optimization problem, λ𝜆\lambdaitalic_λ-Tune conveys the most relevant context to the LLM while bounding the number of input tokens and, therefore, monetary fees for LLM invocations. We compare λ𝜆\lambdaitalic_λ-Tune to various baselines, using multiple benchmarks and PostgreSQL and MySQL as target systems for tuning, showing that λ𝜆\lambdaitalic_λ-Tune is significantly more robust than prior approaches.","The performance of database management system changes dramatically as a function of various tuning choices, including settings for system configuration parameters as well as physical design choices such as indexing, sorting, or partitioning. This has motivated a large body of research on automated database system tuning. Recent work exploits machine learning to find near-optimal configurations (Pavlo et al., 2017; Wang et al., 2021; Ding et al., 2019; Giannakouris and Trummer, 2022) but suffers from high training and exploration overheads. This has motivated a new line of research (Trummer, 2022; Lao et al., 2023), exploiting LLMs to heuristically prune the search space for tuning. Similar to human database administrators, such models leverage commonsense knowledge, extracted from text documents, to narrow the focus to tuning options that seem “reasonable”, given the tuning context. This paper presents λ𝜆\lambdaitalic_λ-Tune (LAnguage Models for Better Database Administration), a system that exploits capabilities offered by the latest generation of LLMs, including the likes of GPT-4 and Claude 3, to optimize various tuning choices for specific systems and OLAP workloads, including system parameter settings as well as physical design decisions. λ𝜆\lambdaitalic_λ-Tune. Prior approaches to LLM-enhanced database tuning (Lao et al., 2023; Trummer, 2022) parse text documents (e.g., the database manual) to extract value recommendations for specific parameters. They still need to perform an optimization stage in which hints about specific parameters are combined into complete configurations. This approach is in line with the limitations of early-stage language models such as BERT (Devlin et al., 2018) and GPT-2 (Radford et al., 2018). For those models, input and output sizes are limited to a few hundred tokens, restricting the scope of these models to settings for single parameters (rather than entire configurations). Modern LLMs such as GPT-4 support input and output sizes of hundreds of thousands of tokens. The design of λ𝜆\lambdaitalic_λ-Tune is motivated by these advances. It exploits increased input sizes by feeding to the language model a description of all information relevant for tuning, including the workload and target system. It also exploits the increased output size by generating entire configurations, rather than hints about single parameters. As shown in our experiments, modern LLMs such as GPT-4 are typically able to map information about the workload to efficient database configuration settings. Hence, unlike prior systems, λ𝜆\lambdaitalic_λ-Tune avoids expensive optimization steps, combining settings for single parameters. Instead, it delegates more responsibility to the language model itself. Prompt Generation. First, λ𝜆\lambdaitalic_λ-Tune automates the prompt generation step by crafting prompts tailored to the input workload (analytical SQL queries), hardware specifications, and the database system. Our approach incorporates a workload representation method that decomposes the input SQL queries into much smaller, mergeable components, called query snippets. As costs increase in the prompt size, minimizing monetary fees while conveying the most relevant information is challenging. We select the most informative subset of snippets to include in the prompt, given a bound on the number of prompt tokens (which are proportional to processing fees for providers like OpenAI). We formulate workload representation as a cost-based optimization problem that we solve by a transformation to integer linear programming. Using the resulting prompt, λ𝜆\lambdaitalic_λ-Tune issues multiple calls to the LLM with a certain degree of randomization to obtain multiple candidate configurations. By running the input queries with different configurations, λ𝜆\lambdaitalic_λ-Tune evaluates and identifies the most efficient configuration among them using the ideas discussed next. Configuration Selection. The LLM may return configurations of varying quality. In this context, a challenge is to avoid slowdowns due to particularly bad configurations, incurred, for instance, when evaluating configurations sequentially. To tackle this challenge, we introduce a configuration selection approach that incrementally evaluates the obtained configurations in multiple rounds. Each round comes with a timeout that limits the impact of bad configurations on tuning time. On the other hand, interrupting execution repeatedly may cause redundant work. λ𝜆\lambdaitalic_λ-Tune chooses timeouts according to a geometric progression scheme, limiting wasted work due to interruptions before the final round. At the same time, it avoids re-evaluating the same queries across multiple rounds and calculates configuration-specific timeouts, taking into account work accomplished in prior rounds. Reconfiguration overheads, e.g., index creations, may dominate query evaluation time if switching between configurations with a high frequency. Hence, λ𝜆\lambdaitalic_λ-Tune adapts query evaluation timeouts to ensure that reconfiguration overheads are proportional to query run time. Configuration Evaluation. Changing between different configurations can be costly, in particular if it involves index creations. This makes it challenging to keep switching overheads low during evaluations. λ𝜆\lambdaitalic_λ-Tune minimizes these overheads by utilizing a lazy index creation approach, that only creates the indexes before the execution of a query that might use them, according to the referenced column. At the same time, λ𝜆\lambdaitalic_λ-Tune optimally orders query execution according to their index creation costs using a dynamic-programming-based query scheduler, which minimizes query reconfiguration costs when switching between different configurations. Our algorithm is based on a custom cost model we built for our query scheduling needs. We prove that the principle of optimality applies to this cost model in Section 3. We evaluate λ𝜆\lambdaitalic_λ-Tune over Postgres and MySQL, using the Join Order Benchmark (JOB) and TPC-H as benchmarks. Our experimental evaluation illustrates λ𝜆\lambdaitalic_λ-Tune’s robustness, outperforming prior tools for automated database system tuning, including GPTuner (Lao et al., 2023), DB-Bert (Trummer, 2023) UDO (Wang et al., 2021), LlamaTune (Kanellis et al., 2022a), as well as ParamTree (Yang et al., 2023). In summary, our original scientific contributions are the following: • We present λ𝜆\lambdaitalic_λ-Tune, a framework that harnesses Large Language Models for automated, database system tuning for Online Analytical Processing (OLAP) workloads. • We introduce three powerful components that facilitate our LLM-assisted tuning approach, including prompt engineering, configuration selection, and configuration evaluation. • We present a thorough experimental evaluation that showcases that λ𝜆\lambdaitalic_λ-Tune is the most robust among its competitors, by consistently identifying the configuration that achieves the best performance. The rest of this paper is organized as follows. Section 2 presents an overview of λ𝜆\lambdaitalic_λ-Tune, its design, and main goals. Next, in Section 3, we describe the prompt generation component, which includes our workload compression method. In Section 4, we present our configuration selection approach. Next, Section 5 presents the configuration evaluation component. Section 6 presents our experimental evaluation of λ𝜆\lambdaitalic_λ-Tune compared to three baselines, as well as an ablation study that showcases the effectiveness of the λ𝜆\lambdaitalic_λ-Tune’s individual components. Finally, in Sections 7 and 8 we present the related work before concluding."
https://arxiv.org/html/2411.04003v1,Learning Aggregate Queries Defined byFirst-Order Logic with Counting111This is the extended version of the conference contribution[11].,"In the logical framework introduced by Grohe and Turán (TOCS 2004) for Boolean classification problems, the instances to classify are tuples from a logical structure, and Boolean classifiers are described by parametric models based on logical formulas. This is a specific scenario for supervised passive learning, where classifiers should be learned based on labelled examples. Existing results in this scenario focus on Boolean classification. This paper presents learnability results beyond Boolean classification. We focus on multiclass classification problems where the task is to assign input tuples to arbitrary integers. To represent such integer-valued classifiers, we use aggregate queries specified by an extension of first-order logic with counting terms called FOC1subscriptFOC1\textup{{FOC}}_{1}FOC start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT.Our main result shows the following: given a database of polylogarithmic degree, within quasi-linear time, we can build an index structure that makes it possible to learn FOC1subscriptFOC1\textup{{FOC}}_{1}FOC start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-definable integer-valued classifiers in time polylogarithmic in the size of the database and polynomial in the number of training examples.","We study the complexity of learning aggregate queries from examples. This is a classification problem of the following form. The elements that are to be classified come from a set X𝑋Xitalic_X, the instance space. For a given set V𝑉Vitalic_V, a V𝑉Vitalic_V-valued classifier on X𝑋Xitalic_X is a function c:X→V:𝑐→𝑋𝑉c\colon X\to Vitalic_c : italic_X → italic_V. We are given a training set S𝑆Sitalic_S of labelled examples (x,λ)∈X×V𝑥𝜆𝑋𝑉(x,\lambda)\in X\times V( italic_x , italic_λ ) ∈ italic_X × italic_V, i. e., λ𝜆\lambdaitalic_λ is the label assigned to the instance x𝑥xitalic_x. The goal is to find a classifier, called a hypothesis, that can be used to predict the label of elements from X𝑋Xitalic_X, including those not given in S𝑆Sitalic_S. The term Boolean classification problem refers to the case where |V|=2𝑉2\left\lvert V\right\rvert=2| italic_V | = 2 (often, V𝑉Vitalic_V is {1,0}10\{1,0\}{ 1 , 0 }). We use the term multiclass classification problem to refer to cases where V𝑉Vitalic_V may be arbitrarily large. In machine learning, these problems fall into the category of supervised learning tasks: we want to learn a function from given input-output pairs. In contrast to this, in unsupervised learning (e. g. clustering), the goal is to learn patterns from unlabelled data [49]. We focus on learning problems related to the framework introduced by Grohe and Turán [36]. There, the instance space X𝑋Xitalic_X is a set of tuples from a logical structure (that is sometimes called the background structure), and the classifiers are Boolean and are described using parametric models based on logical formulas. In this paper, we extend the framework to multiclass classification problems where the classifiers are integer-valued, i. e., V=ℤ𝑉ℤV=\mathbb{Z}italic_V = blackboard_Z. In the framework that we consider, the background structure is a relational database 𝒜𝒜\mathcal{A}caligraphic_A, and the instance space X𝑋Xitalic_X is the set Aksuperscript𝐴𝑘A^{k}italic_A start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT of all k𝑘kitalic_k-tuples of elements from the active domain A𝐴Aitalic_A of 𝒜𝒜\mathcal{A}caligraphic_A (also called the universe of 𝒜𝒜\mathcal{A}caligraphic_A). Here, k𝑘kitalic_k is a fixed positive integer. One fixes a parameter length ℓℓ\ellroman_ℓ (a fixed non-negative integer). A classifier is specified by a pair p=(t,w¯)𝑝𝑡¯𝑤p=(t,\bar{w})italic_p = ( italic_t , over¯ start_ARG italic_w end_ARG ), where w¯=(w1,…,wℓ)¯𝑤subscript𝑤1…subscript𝑤ℓ\bar{w}=(w_{1},\dots,w_{\ell})over¯ start_ARG italic_w end_ARG = ( italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_w start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT ) is an ℓℓ\ellroman_ℓ-tuple of elements in A𝐴Aitalic_A, and t𝑡titalic_t is a counting term in the first-order logic with counting FOC1subscriptFOC1\textup{{FOC}}_{1}FOC start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT [35] with free variables x1,…,xk,y1,…,yℓsubscript𝑥1…subscript𝑥𝑘subscript𝑦1…subscript𝑦ℓx_{1},\dots,x_{k},y_{1},\dots,y_{\ell}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_y start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT. This pair p𝑝pitalic_p represents the classifier cp:X→ℤ:subscript𝑐𝑝→𝑋ℤc_{p}\colon X\to\mathbb{Z}italic_c start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT : italic_X → blackboard_Z that assigns to each k𝑘kitalic_k-tuple a¯=(a1,…,ak)∈X¯𝑎subscript𝑎1…subscript𝑎𝑘𝑋\bar{a}=(a_{1},\dots,a_{k})\in Xover¯ start_ARG italic_a end_ARG = ( italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ∈ italic_X the integer i𝑖iitalic_i that is obtained by evaluating the counting term t𝑡titalic_t in the database 𝒜𝒜\mathcal{A}caligraphic_A while interpreting the variables x1,…,xksubscript𝑥1…subscript𝑥𝑘x_{1},\dots,x_{k}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT with the elements a1,…,aksubscript𝑎1…subscript𝑎𝑘a_{1},\dots,a_{k}italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT and the variables y1,…,yℓsubscript𝑦1…subscript𝑦ℓy_{1},\dots,y_{\ell}italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_y start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT with the “parameters” w1,…,wℓsubscript𝑤1…subscript𝑤ℓw_{1},\dots,w_{\ell}italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_w start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT. We will write ht,w¯𝒜subscriptsuperscriptℎ𝒜𝑡¯𝑤h^{\mathcal{A}}_{t,\bar{w}}italic_h start_POSTSUPERSCRIPT caligraphic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t , over¯ start_ARG italic_w end_ARG end_POSTSUBSCRIPT to denote this classifier cpsubscript𝑐𝑝c_{p}italic_c start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT. Given a training set S⊆Ak×ℤ𝑆superscript𝐴𝑘ℤS\subseteq A^{k}\times\mathbb{Z}italic_S ⊆ italic_A start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT × blackboard_Z, we want to find a pair p=(t,w¯)𝑝𝑡¯𝑤p=(t,\bar{w})italic_p = ( italic_t , over¯ start_ARG italic_w end_ARG ) such that the classifier ht,w¯𝒜subscriptsuperscriptℎ𝒜𝑡¯𝑤h^{\mathcal{A}}_{t,\bar{w}}italic_h start_POSTSUPERSCRIPT caligraphic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t , over¯ start_ARG italic_w end_ARG end_POSTSUBSCRIPT is consistent with S𝑆Sitalic_S, i. e., it satisfies ht,w¯𝒜⁢(a¯)=isubscriptsuperscriptℎ𝒜𝑡¯𝑤¯𝑎𝑖h^{\mathcal{A}}_{t,\bar{w}}(\bar{a})=iitalic_h start_POSTSUPERSCRIPT caligraphic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t , over¯ start_ARG italic_w end_ARG end_POSTSUBSCRIPT ( over¯ start_ARG italic_a end_ARG ) = italic_i for every (a¯,i)∈S¯𝑎𝑖𝑆(\bar{a},i)\in S( over¯ start_ARG italic_a end_ARG , italic_i ) ∈ italic_S. Example 1.1. Let 𝒜𝒜\mathcal{A}caligraphic_A be a relational database where the active domain A𝐴Aitalic_A contains authors and publications, the binary relation Author contains all pairs (a,p)𝑎𝑝(a,p)( italic_a , italic_p ) where a𝑎aitalic_a is an author of the publication p𝑝pitalic_p, and the binary relation Citation contains all pairs (p1,p2)subscript𝑝1subscript𝑝2(p_{1},p_{2})( italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) where the publication p1subscript𝑝1p_{1}italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT cites the publication p2subscript𝑝2p_{2}italic_p start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. Suppose we are given a training set S𝑆Sitalic_S that consists of a few pairs (a,i)𝑎𝑖(a,i)( italic_a , italic_i ) where a𝑎aitalic_a is an author and i𝑖iitalic_i is the total number of citations of the publications of a𝑎aitalic_a. A reasonable classifier for this setting would be a mapping c:A→ℤ:𝑐→𝐴ℤc\colon A\to\mathbb{Z}italic_c : italic_A → blackboard_Z that assigns to every author a𝑎aitalic_a present in the database the total number i𝑖iitalic_i of citations of their publications. In our setting, this can be represented as follows. We let k=1𝑘1k=1italic_k = 1 and ℓ=0ℓ0\ell=0roman_ℓ = 0. Since ℓ=0ℓ0\ell=0roman_ℓ = 0, the “parameter” w𝑤witalic_w is fixed to be the empty tuple ()()( ). Since k=1𝑘1k=1italic_k = 1, we use a counting term with a single free variable x𝑥xitalic_x (that will be assigned with authors present in the database). We choose the counting term t(x)≔#(z1,z2).(Author(x,z1)∧Citation(z2,z1)).t(x)\ \coloneqq\ \ \#{(z_{1},z_{2})}.{\bigl{(}\texttt{Author}(x,z_{1})\land% \texttt{Citation}(z_{2},z_{1})\bigr{)}}.italic_t ( italic_x ) ≔ # ( italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) . ( Author ( italic_x , italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ∧ Citation ( italic_z start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ) . Evaluating t⁢(x)𝑡𝑥t(x)italic_t ( italic_x ) for an author x𝑥xitalic_x yields the number of tuples (z1,z2)subscript𝑧1subscript𝑧2(z_{1},z_{2})( italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) such that x𝑥xitalic_x is an author of publication z1subscript𝑧1z_{1}italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, and z2subscript𝑧2z_{2}italic_z start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT is a publication that cites z1subscript𝑧1z_{1}italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. This is precisely the total number of citations of publications authored by x𝑥xitalic_x. Hence, ht,()𝒜subscriptsuperscriptℎ𝒜𝑡h^{\mathcal{A}}_{t,()}italic_h start_POSTSUPERSCRIPT caligraphic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t , ( ) end_POSTSUBSCRIPT is the desired classifier c𝑐citalic_c. Example 1.2. Suppose we have a database that maintains a list of all cakes colleagues brought to work. We model this as a relational database 𝒜𝒜\mathcal{A}caligraphic_A whose active domain A𝐴Aitalic_A contains persons, IDs of cakes, and types of cake. The binary relation Brought contains all pairs (p,c)𝑝𝑐(p,c)( italic_p , italic_c ) where p𝑝pitalic_p is a person that brought the cake with ID c𝑐citalic_c, and the binary relation Type contains all pairs (c,τ)𝑐𝜏(c,\tau)( italic_c , italic_τ ) where c𝑐citalic_c is the ID of a cake of type τ𝜏\tauitalic_τ (e. g., “chocolate cake”, “strawberry cake”, “carrot cake”, etc). Suppose we want to find a classifier that predicts the popularity of colleagues. For this, via a survey, we gather examples (p,i)∈A×ℤ𝑝𝑖𝐴ℤ(p,i)\in A\times\mathbb{Z}( italic_p , italic_i ) ∈ italic_A × blackboard_Z where p𝑝pitalic_p is a person and i𝑖iitalic_i is the popularity of the person, and we call the resulting set of labelled examples S𝑆Sitalic_S. We choose k=ℓ=1𝑘ℓ1k=\ell=1italic_k = roman_ℓ = 1, so we want to find a classifier that uses a single parameter. According to our own experience at work, it seems conceivable that the following classifier ht,w𝒜subscriptsuperscriptℎ𝒜𝑡𝑤h^{\mathcal{A}}_{t,w}italic_h start_POSTSUPERSCRIPT caligraphic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t , italic_w end_POSTSUBSCRIPT is consistent with S𝑆Sitalic_S: the parameter w𝑤witalic_w is “chocolate cake” and t𝑡titalic_t is the counting term t(x,y)≔#(z).(Brought(x,z)∧¬Type(z,y))+ 2⋅#(z).(Brought(x,z)∧Type(z,y)).t(x,y)\ \coloneqq\ \ \#{(z)}.{\bigl{(}\texttt{Brought}(x,z)\land\neg\texttt{% Type}(z,y)\bigr{)}}\ \ +\ \ 2\cdot\#{(z)}.{\bigl{(}\texttt{Brought}(x,z)\land% \texttt{Type}(z,y)\bigr{)}}.italic_t ( italic_x , italic_y ) ≔ # ( italic_z ) . ( Brought ( italic_x , italic_z ) ∧ ¬ Type ( italic_z , italic_y ) ) + 2 ⋅ # ( italic_z ) . ( Brought ( italic_x , italic_z ) ∧ Type ( italic_z , italic_y ) ) . Note that t𝑡titalic_t counts the number of cakes brought by person x𝑥xitalic_x, where cakes of type y𝑦yitalic_y are counted twice, and the variable y𝑦yitalic_y will always be assigned the value of the parameter w𝑤witalic_w. In many application scenarios, the same database is used multiple times with different training sets to learn different classifiers. Thus, we consider a setting in which we are first only given the database, without any training examples. In a precomputation step, we allow gathering information that will be helpful for solving future learning tasks. This precomputation step can be viewed as building an index structure that is designed in order to support solving multiple learning tasks. In the actual learning phase, we are repeatedly given training sets of labelled examples, and our task is to output a hypothesis that is consistent with the corresponding training set. For this learning phase, it would be desirable to have algorithms that run efficiently even if the database is too large to fit into the main memory. To achieve this, we are interested in algorithms that require only local access to the database, i. e., instead of having random access to the database, a learning algorithm should initially start with the elements given in the training set; subsequently, it may only retrieve the neighbours of elements it already holds in memory. By utilising the memory hierarchy, such local access can be achieved efficiently even in cases where random access is too prohibitive. In the context of learning (concerning Boolean classification problems), this local-access model has been introduced by Grohe and Ritzert [34]. Our contribution Our main result is an algorithm that builds the index structure in time linear in the size and polynomial in the degree of the database. Afterwards, upon input of concrete training sets, classifiers definable in FOC1subscriptFOC1\textup{{FOC}}_{1}FOC start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT can be learned in time polynomial in the degree of the database and polynomial in the number of examples given in the training set. Moreover, the classifiers returned by our algorithm can be evaluated in time polynomial in the degree of the database. Furthermore, our algorithms for finding a classifier and for evaluating this classifier do not require random access to the database but only rely on the local-access model. For databases of polylogarithmic degree (i. e., of degree up to (log⁡n)csuperscript𝑛𝑐(\log n)^{c}( roman_log italic_n ) start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT where c𝑐citalic_c is a constant and n𝑛nitalic_n is the size of the database), our main result implies that the index structure can be built in quasi-linear time (i. e., time n⋅(log⁡n)c⋅𝑛superscript𝑛𝑐n{\cdot}(\log n)^{c}italic_n ⋅ ( roman_log italic_n ) start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT); afterwards, FOC1subscriptFOC1\textup{{FOC}}_{1}FOC start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-definable integer-valued classifiers can be learned in time polylogarithmic (so, in particular, sublinear) in the size of the database and polynomial in the number of training examples. Previous results in the framework of Grohe and Turán for Boolean classification problems relied on the fact that it suffices to check a constant number of queries while limiting the search space for the parameters to a neighbourhood of a certain radius [34, 7, 10]. For our setting of multiclass classification with aggregate queries, however, this does not hold any more. Hence, a priori, it is not clear that sublinear-time learning algorithms are possible for the multiclass case at all. The main technical challenge towards our learnability result was to find an approach that keeps the number of queries to check small (i. e., polynomial in the degree of the database), while still being able to limit the search space for the parameters to a small neighbourhood around the given training tuples. Organisation We provide the necessary definitions concerning FOC1subscriptFOC1\textup{{FOC}}_{1}FOC start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT in Section 2, and we formally introduce the learning problem that we consider in Section 3. The precise statement of our main result is given in Theorem 3.1. Our proof makes heavy use of the locality properties of the logic FOC1subscriptFOC1\textup{{FOC}}_{1}FOC start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT shown in [35], including a decomposition of FOC1subscriptFOC1\textup{{FOC}}_{1}FOC start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-formulas into local formulas. These properties are used in Section 4 to provide our main technical tool for the proof of the main result. Section 5 concludes the paper with a summary and an outlook on future work. In the remainder of this introduction, we give an overview of related work. Related work The first-order logic with counting FOC was introduced in [43] and further studied in [35, 7]. This logic extends first-order logic (FO) by the ability to formulate counting terms that evaluate to integers, and by numerical predicates that allow to compare results of counting terms. It was shown in [43] that the model-checking problem for FOC is fixed-parameter tractable on classes of structures of bounded degree. From [35] it is known that the fixed-parameter tractability of FOC cannot be generalised to even very simple classes of structures of unbounded degree such as unranked trees (under a reasonable assumption in parameterised complexity). However, [35] identified a fragment called FOC1subscriptFOC1\textup{{FOC}}_{1}FOC start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT for which model-checking of formulas and evaluation of counting terms are fixed-parameter tractable on all nowhere dense classes of structures. The present paper uses counting terms of FOC1subscriptFOC1\textup{{FOC}}_{1}FOC start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT to represent integer-valued classifiers. The learning framework we consider has been introduced for Boolean classification problems in [36], which provides information-theoretic learnability results for classes of classifiers that can be specified using FO- and MSO-formulas on restricted classes of structures, such as the class of planar graphs or classes of graphs of bounded degree. Algorithmic aspects of the framework, including the running time of a learning algorithm, were first studied in [34]. The paper showed that Boolean classifiers definable in FO can be learned in sublinear time on structures of polylogarithmic degree. Analogous results have been obtained for MSO on strings [33] and on trees [30], which included a precomputation step to allow for efficient repeated learning. The paper [9] studied the parameterised complexity of the Boolean classification problem and showed that on arbitrary relational structures, learning hypotheses definable in FO is hard for the parameterised complexity class AW⁢[∗]AWdelimited-[]\textup{{AW}}[*]AW [ ∗ ] (i. e., subject to a plausible complexity-theoretic assumption, it is not fixed-parameter tractable). The paper also showed that the problem is fixed-parameter tractable if the structures come from a nowhere dense class. For Boolean classifiers definable in the extension FOCN of FO with counting quantifiers and numerical predicates, [7] obtained a sublinear-time learning algorithm for structures of bounded degree, i. e., classes of structures where the degree is bounded by a constant. Recently, [8] lifted this result to structures of tiny degree, i. e., classes of structures of degree up to (log⁡log⁡n)csuperscript𝑛𝑐(\log\log n)^{c}( roman_log roman_log italic_n ) start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT for some constant c𝑐citalic_c, where n𝑛nitalic_n is the size of the structure. The paper [10] considered a notion of weighted structures, which extend ordinary relational structures by assigning weights, i. e. elements from particular rings or abelian groups, to tuples present in the structure. It introduced the expressive logic FOWA, which extends FO by means of aggregating weights and formulating both formulas (that evaluate to “true” or “false”) and terms (that “aggregate” weights and evaluate to values in the associated ring or abelian group). For the fragment FOWA1subscriptFOWA1\textup{{FOWA}}_{1}FOWA start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT (that still extends FO), the paper showed that Boolean classifiers definable by FOWA1subscriptFOWA1\textup{{FOWA}}_{1}FOWA start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-formulas over weighted background structures of polylogarithmic degree can be learned in sublinear time after quasi-linear-time preprocessing. This lifts the results obtained in [34] for FO to the substantially more expressive logic FOWA1subscriptFOWA1\textup{{FOWA}}_{1}FOWA start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. As the logic FOC1subscriptFOC1\textup{{FOC}}_{1}FOC start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT can be embedded in FOWA1subscriptFOWA1\textup{{FOWA}}_{1}FOWA start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, it follows from [10] that Boolean classifiers definable by FOC1subscriptFOC1\textup{{FOC}}_{1}FOC start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-formulas over background structures of polylogarithmic degree can be learned in sublinear time after quasi-linear-time preprocessing. The main result of the present paper can be viewed as a generalisation of this to integer-valued classification problems. The algorithmic results obtained so far within the framework introduced in [36] all focus on Boolean classification problems. However, many application scenarios require multiclass classification (cf. [22, 15, 37]). In the database systems literature, multiclass classifiers typically are described by aggregate queries [52, 53, 54, 55, 46]. In this paper, aggregate queries are represented by the counting terms of FOC1subscriptFOC1\textup{{FOC}}_{1}FOC start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. Closely related to the framework we consider is the framework of inductive logic programming (ILP) [47, 48, 40, 20, 21]. Both frameworks deal with a passive supervised learning setting, where the learning algorithms are given labelled examples. These examples are labelled according to some target concept, and the algorithms should return a hypothesis that approximately matches this target concept. One of the main differences between both frameworks is that we represent the background knowledge by a relational database, whereas in ILP, it is represented in a background theory, i. e., a set of formulas. Related logical learning frameworks have also been studied in formal verification [28, 44, 24, 57, 19]. In the database literature, various approaches to learning queries from examples have been studied, both in passive (such as ours) and active learning settings. In passive learning settings, results often focus on conjunctive queries [38, 39, 6, 41, 5, 56] or consider queries outside the relational database model [51, 12], while we focus on FOC1subscriptFOC1\textup{{FOC}}_{1}FOC start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, an extension of full first-order logic. In the active learning setting introduced by Angluin [4], learning algorithms are allowed to actively query an oracle. Results in this setting [2, 50, 1, 12, 13, 16] again consider various types of queries. Another related subject in the database literature is the problem of learning schema mappings from examples [14, 29, 3, 17, 18]."
https://arxiv.org/html/2411.03351v1,Tabular Data Synthesis with Differential Privacy: A Survey,"Data sharing is a prerequisite for collaborative innovation, enabling organizations to leverage diverse datasets for deeper insights. In real-world applications like FinTech and Smart Manufacturing, transactional data, often in tabular form, are generated and analyzed for insight generation. However, such datasets typically contain sensitive personal/business information, raising privacy concerns and regulatory risks. Data synthesis tackles this by generating artificial datasets that preserve the statistical characteristics of real data, removing direct links to individuals. However, attackers can still infer sensitive information using background knowledge. Differential privacy offers a solution by providing provable and quantifiable privacy protection. Consequently, differentially private data synthesis has emerged as a promising approach to privacy-aware data sharing. This paper provides a comprehensive overview of existing differentially private tabular data synthesis methods, highlighting the unique challenges of each generation model for generating tabular data under differential privacy constraints. We classify the methods into statistical and deep learning-based approaches based on their generation models, discussing them in both centralized and distributed environments. We evaluate and compare those methods within each category, highlighting their strengths and weaknesses in terms of utility, privacy, and computational complexity. Additionally, we present and discuss various evaluation methods for assessing the quality of the synthesized data, identify research gaps in the field and directions for future research.","Data sharing is essential as it drives innovative collaboration and enables informed decision-making across various domains. Numerous public data-sharing platforms, including Kaggle (kag, 2024), Data.gov (dat, 2024), and the UCI repository (Dua and Graff, 2019), offer access to extensive datasets, with the primary goal of facilitating knowledge discovery and advancement. In most applications, such as FinTech and Smart Manufacturing, these datasets are represented in tabular form, given their structured nature and widespread applicability across different fields. However, it is important to note that these datasets often contain sensitive personal/business data, which can raise significant privacy concerns. In addition, due to evolving privacy regulations, exemplified by recent legislation like the AI Act (AIa, 2024), there is a heightened need for innovative methods for data sharing that protect individual privacy while enabling meaningful data analysis. Data synthesis has been attracting growing attention due to its unique ability to generate synthetic data based on statistical information without being linked to specific individuals or identities. However, it is important to note that while synthetic data offers privacy protection, several studies (Dinur and Nissim, 2003; Garfinkel et al., 2019) have shown that the attacker can still potentially infer sensitive information about users. For example, Jordon et al. (Jordon et al., 2022) show that “Synthetic data can leak information about the data it was derived from and is vulnerable to privacy attacks.” Moreover, Stadler et al. (Stadler et al., 2020) have demonstrated that generative models trained without privacy safeguards offer limited defence against inference attacks when compared to the alternative of directly sharing the original data. A cutting-edge solution involves integrating provable privacy measures, such as differential privacy (DP), into the synthetic data generation process. Differential privacy aims to ensure that the information derived from the released synthetic data remains nearly identical, whether or not specific individuals were part of the original datasets, thus effectively preventing the inference of personal information. Importantly, it does not rely on assumptions about the capabilities of potential attackers, providing robust privacy protection even in the presence of adversaries with significant background knowledge and resources (Yang et al., 2022b). The United States National Institute of Standards and Technology (NIST) has been instrumental in championing data sharing and privacy protection. In 2018, NIST organized the “Differential Privacy Synthetic Data Challenge” (NIS, 2018), a competition dedicated to advancing the field of differential privacy for generating synthetic data that retains the statistical characteristics of actual data while safeguarding individual privacy. The challenge highlighted the growing importance of balancing the need to share valuable insights with the necessity of protecting personal information, making differentially private data synthesis a promising research focus. In this paper, we present a comprehensive review of existing differential private tabular data synthesis methods. The generation of differentially private synthetic tabular data primarily falls into two key categories: statistical methods and deep learning-based methods. We delve into both approaches, analyzing their strengths and limitations under both centralized and distributed settings. Furthermore, we offer insights into the unique challenges and considerations that arise in each context. Table 1. Comparison with existing surveys Paper Year Consideration of tabular data synthesis with DP Centralized data synthesis Distributed data synthesis Discussion on Evaluation S-M DL-M Fidelity/Utility Privacy (Bowen and Liu, 2020) 2020 ✓ ✓ (Bourou et al., 2021) 2021 ✓ ✓ (Figueira and Vaz, 2022) 2022 ✓ ✓ (Ghatak and Sakurai, 2022) 2022 ✓ ✓ ✓ ✓ (Xing et al., 2022) 2022 ✓ ✓ ✓ ✓ (Lu et al., 2023) 2023 ✓ ✓ (Hassan et al., 2023) 2023 ✓ ✓ ✓ (Hu et al., 2024) 2024 ✓ ✓ ✓ (Bauer et al., 2024) 2024 ✓ ✓ Ours - ✓ ✓ ✓ ✓ ✓ ✓ Differences between this survey and others. Currently, several synthesis surveys have been published, as shown in Table 1. Bourou et al. (Bourou et al., 2021) reviewed several popular GAN-based models for tabular intrusion detection system data synthesis and experimentally evaluated their performance. However, this review exclusively focused on GAN-based models, without considering other methods. Figueira and Vaz et al. (Figueira and Vaz, 2022) slightly extended the scope but still concentrated on GAN-based models for data synthesis. Xing et al. (Xing et al., 2022) provided a broader review, covering data synthesis methods for non-imaging medical datasets, including both tabular and sequential data. Their discussion included statistical and deep learning-based methods, as well as evaluation metrics. Lu et al. (Lu et al., 2023) explored machine learning-based approaches for data synthesis and applications of synthetic data generation, addressing privacy and fairness concerns related to synthetic data. However, all the aforementioned papers discuss pure synthetic data generation methods, and none of them consider the methods with differential privacy protection. Bowen and Liu (Bowen and Liu, 2020) conducted an experimental study on differentially private data synthesis methods, focusing solely on statistical approaches. Hassan et al. (Hassan et al., 2023) explored the intersection of synthetic data and differential privacy, with a primary focus on deep generative models. Bauer et al. (Bauer et al., 2024) conducted a comprehensive study of various model types suitable for synthetic data generation, including methods that incorporate differential privacy. Ghatak and Sakurai (Ghatak and Sakurai, 2022) considered the methods with differential privacy protection, but exclusively discussed data synthesis methods that emerged victorious in the NIST 2018 challenge. Hu et al. (Hu et al., 2024) provided a review of differentially private data synthesis, including tabular data. However, their approach was more of a simple summary of existing methods rather than an in-depth analysis and discussion. Furthermore, all existing surveys focus on centralized data synthesis methods and do not address distributed data synthesis. In our paper, we target tabular data synthesis methods with differential privacy protection under both centralized and distributed settings. Additionally, we summarize and discuss various evaluation methods for the generated synthetic data, focusing on fidelity, utility, and privacy. Contributions of this survey. This survey provides a comprehensive review of differential private data synthesis methods, focusing on tabular data. We consider two application scenarios: centralized data synthesis, where the data curator holds all users’ datasets and aims to generate synthetic datasets for data analytics or sharing purposes, and distributed data synthesis, where data owners retain their data locally and collaborate with other parties for joint data synthesis. Our contributions are summarized as follows: • We provide a thorough and comprehensive overview of existing methods for differentially private tabular data synthesis, along with the evaluation techniques used to assess their performance and effectiveness. • Based on the synthetic data generation models, we categorize the primary approaches for data synthesis into two key research directions: statistical-based methods and deep learning-based methods, both applicable under two main scenarios: centralized and distributed data synthesis. • We provide an in-depth review and analysis of existing methods for generating synthetic data, highlighting strengths and weaknesses in capturing attribute dependencies, modeling the distribution of attributes, computational complexity, and the noise scales introduced during the model learning process, etc. • By analyzing the state-of-the-art in the field, we discuss the research gaps and identify several promising future research directions to address the emerging challenges an advance the domain of private tabular data synthesis. In this survey, we present the material in a tutorial manner, providing a clear introduction, comprehensive discussion, and valuable insights into the topics and methods. We aim to make the content accessible and informative for readers who are new to the subject as well as those looking to deepen their understanding. Roadmap. The rest of the paper is organized as follows: Section 2 provides background knowledge on tabular data synthesis and differential privacy. Section 3 and Section 4 discuss centralized data synthesis methods with differential privacy protection and distributed data synthesis methods with differential privacy protection, respectively. Section 5 introduces the synthetic data evaluation metrics. The research gaps and promising research directions are identified in Section 6, and the survey is concluded in Section 7."
https://arxiv.org/html/2411.03007v1,Data Quality Awareness: A Journey from Traditional Data Management to Data Science Systems,"Artificial intelligence (AI) has transformed various fields, significantly impacting our daily lives. A major factor in AI’s success is high-quality data. In this paper, we present a comprehensive review of the evolution of data quality (DQ) awareness from traditional data management systems to modern data-driven AI systems, which are integral to data science. We synthesize the existing literature, highlighting the quality challenges and techniques that have evolved from traditional data management to data science including big data and ML fields. As data science systems support a wide range of activities, our focus in this paper lies specifically in the analytics aspect driven by machine learning. We use the cause-effect connection between the quality challenges of ML and those of big data to allow a more thorough understanding of emerging DQ challenges and the related quality awareness techniques in data science systems. To the best of our knowledge, our paper is the first to provide a review of DQ awareness spanning traditional and emergent data science systems. We hope that readers will find this journey through the evolution of data quality awareness insightful and valuable.","The rapid emergence of data-centric technologies, particularly in the big data and ML field, has increased attention to the challenge of data quality, prompting a need for DQ awareness in emergent data science systems. Although recent research work still considers the importance of adapting existing data quality characteristics and frameworks from traditional data management practices, it is still somewhat unclear how to adapt them to improve DQ awareness in these systems. Data-driven AI systems, which are integral to data science, emphasize the integration of adaptive and scalable quality measures capable of dynamically responding to evolving data landscapes and user requirements. This is crucial for ensuring that data inconsistencies do not compromise the accuracy and reliability of AI outputs. Effective data quality management in these systems involves real-time integration of quality measures that adapt to changes in data and user needs. Our journey from traditional data management to data science, including big data and machine learning (ML), reflects an evolving landscape of data quality challenges. The singular focus of traditional data management systems on meeting the needs of immediate users often oversimplified the complexity of data quality issues. In a big data context, data quality issues are multiplying due to (i) large datasets that grow to the range of Tera- and Peta-Bytes, (ii) different types of data that existing data quality dimensions and evaluation methods cannot cope with, and (iii) real-time and time-evolving data, which may alter the data characteristics and consequently the insight derived from it. In (Saha and Srivastava, 2014), Saha and Srivastava present two main areas where data quality management challenges arise in the big data environment: (i) discovering data quality semantics and data repairing; and (ii) the trade-off between accuracy and efficiency using various computing models. This emphasizes the big data quality dimensions are more complex which leads to new techniques to assess data quality such as using big data platforms. The ability to extract value from big data depends on data analytics which includes business intelligence, data visualization, machine learning, and statistical analysis. In this paper, we focus on machine learning which is considered the core of the big data revolution (Cappiello et al., 2018; Polyzotis et al., 2017). This synergy between big data and machine learning allows data science to transform big data into insights, decisions, and predictions. However, the complex configurations of the datasets used in data science systems and the diverse backgrounds of ML practitioners introduce new data quality challenges. Despite the rich literature on data quality in both big data (Cappiello et al., 2018; Cai and Zhu, 2015) and machine learning (Priestley et al., 2023) fields, as well as on the challenges of ML in conjunction with big data (L’Heureux et al., 2017; Polyzotis et al., 2017), there is a gap in the literature that specifically explores the interplay between data quality issues and the unique challenges that arise from linking ML and big data within the scope of data science systems. Motivated by the need for data quality awareness in emergent data science systems, our paper aims to take readers on an enlightening journey through the evolution of data quality awareness from traditional to data science systems. We particularly focus on ML pipelines as a key component of data science systems. We emphasize the essential baggage from big data and traditional data management, including various quality awareness techniques, for effectively navigating the emerging data quality challenges in ML pipelines. By connecting these techniques with those of machine learning, we gain a deeper understanding of the challenges at hand. The rest of the paper provides an overview of fundamental data quality concepts in Section 2. Section 3 presents the main DQ techniques in traditional systems. Section 4 explores the DQ issues in big data systems, including new dimensions and techniques. Section 5 discusses quality awareness in ML pipelines. Finally, Section 6 summarizes the key findings and suggests future research directions to advance data quality awareness in modern data science systems."
https://arxiv.org/html/2411.02948v1,Grounding Natural Language to SQL Translation with Data-Based Self-Explanations,"Natural Language Interfaces for Databases empower non-technical users to interact with data using natural language (NL). Advanced approaches, utilizing either neural sequence-to-sequence or more recent sophisticated large-scale language models, typically implement NL to SQL (NL2SQL) translation in an end-to-end fashion. However, like humans, these end-to-end translation models may not always generate the best SQL output on their first try. In this paper, we propose Cyclesql, an iterative framework designed for end-to-end translation models to autonomously generate the best output through self-evaluation. The main idea of Cyclesql is to introduce data-grounded NL explanations of query results as self-provided feedback, and use the feedback to validate the correctness of the translation iteratively, hence improving the overall translation accuracy. Extensive experiments, including quantitative and qualitative evaluations, are conducted to study Cyclesql by applying it to seven existing translation models on five widely used benchmarks. The results show that 1) the feedback loop introduced in Cyclesql can consistently improve the performance of existing models, and in particular, by applying Cyclesql to Resdsql, obtains a translation accuracy of 82.0% (+2.6%) on the validation set, and 81.6% (+3.2%) on the test set of Spider benchmark; 2) the generated NL explanations can also provide insightful information for users, aiding in the comprehension of translation results and consequently enhancing the interpretability of NL2SQL translation111Our code is available at https://github.com/Kaimary/CycleSQL..","Natural language interfaces for databases (NLIDBs) [1, 2, 3] democratize data exploration by allowing users to interact with databases using natural language (NL), breaking down barriers to information retrieval and data analysis. Consequently, the development of NLIDBs has garnered significant attention from both the data management and natural language processing (NLP) communities since the late 1970s [4, 5, 6, 7]. In light of recent advancements in machine learning, the central focus of ongoing efforts to develop NLIDBs centers around elevating the accuracy of translating NL to SQL (NL2SQL) [8, 9]. This objective is primarily accomplished via direct NL2SQL translation in an end-to-end manner, either by adopting sequence-to-sequence (Seq2seq) models trained on annotated data [10, 11, 12, 13, 14, 15, 16, 17] or, more recently, by harnessing large-scale language models (LLMs) that push the boundaries of the field even further within the last two years [18, 19, 20, 21, 22]. 111122223333444455550.70.70.70.70.750.750.750.750.80.80.80.80.850.850.850.85Beam Size (Number of Chat Completion Choices)Translation AccuracyPicard (t5-3b)Resdsql (t5-large)Gpt-3.5-turboDailsql (chatgpt) Figure 1: Translation accuracy on Spider validation set with varied beam sizes (or chat completion choices). Accuracy for beam sizes (or chat completions) >>> 1 is evaluated by matching any beam result. Challenges. While significant advancements in enhancing overall accuracy, current end-to-end models face persistent challenges in producing desired quality output during their initial attempt, owing to the treatment of language translation as a “one-time deal”. Figure 1 shows the translation accuracy (defined as query execution result equivalence) on the Spider [7] benchmark, with varied beam outputs for two Seq2seq-based models222Most existing Seq2seq-based NL2SQL translation models utilize beam search decoding method to maintain a list of top-k best outcomes. (i.e., Picard [16], and Resdsql [17]) or diverse chat completion choices for two LLM-based models333LLMs often use a specific parameter to generate various responses. Refer to https://platform.openai.com/docs/api-reference/chat/create#chat-create-n. (i.e., Gpt-3.5-turbo and the state-of-the-art Dailsql model [22]). As indicated by the plateauing accuracy (below 80%) observed when the beam size or the number of chat completions is set to 1111, they may fail to generate best-quality translations in their initial attempts. However, expanding the search space through wider beams or more chat completions steadily improves accuracy, without necessitating modifications to the underlying model architectures. This example shows that end-to-end models may benefit from broader exploration options to enhance translation quality over successive attempts. aid flno origin destination 9 2 Los Angeles Tokyo 3 7 Los Angeles Sydney 3 13 Los Angeles Chicago 10 68 Chicago New York 9 76 Chicago Los Angeles 7 33 Los Angeles Honolulu 5 34 Los Angeles Honolulu 1 99 Los Angeles Washington D.C. 2 346 Los Angeles Dallas 6 387 Los Angeles Boston aid name distance 1 Boeing 747-400 8430 2 Boeing 737-800 3383 3 Airbus A340-300 7120 4 British Aerospace Jetstream 41 1502 5 Embraer ERJ-145 1530 6 SAAB 340 2128 7 Piper Archer III 520 8 Tupolev 154 4103 9 Lockheed L1011 6900 10 Boeing 757-300 4010 Figure 2: An example with simplified database (Flight (left) and Aircraft (right) tables) from the Spider benchmark. Inspired by the feedback mechanisms [23, 24] used in modern recommendation systems and iterative refinement methods [25, 26] introduced in LLMs, we present Cyclesql, an iterative framework built upon self-provided feedback to enhance translation accuracy of existing end-to-end models. Diverging from the traditional end-to-end translation paradigm, we introduce data-grounded NL explanations of query results as a form of internal feedback to create a self-contained feedback loop within the end-to-end process, facilitating iterative self-evaluation of translation correctness. \twemoji [height=1.0em]thinking faceSQL2NL back-translation for Self-Explanations? A natural way to generate self-provided feedback (explanations) in NL2SQL translation may involve its reverse process, namely SQL2NL. This entails establishing an NL-to-SQL-to-NL translation lifecycle, a concept analogous to back-translation [27, 28]. Several studies have explored this technique [29, 30, 31, 32] to refine initial SQL outputs, but we posit that solely using this “simple” back-translation to generate feedback may be limited, as it lacks additional contextual information beyond the NL and SQL components. Consider an intuitive example in Figure 2, which shows an NL query alongside a translation from an existing model. By simply using the SQL2NL technique, however, incorrect “positive” feedback is generated: the meaning of the explanation seems to align with the initial NL query, whereas the underlying SQL query is deemed incorrect (i.e., an incorrect aggregation function is used in the SELECT clause). Thus, such a back-translated explanation may not adequately serve as valid feedback for the NL2SQL process. Our Methodology. Cyclesql proposes a plug-and-play framework that facilitates seamless integration with existing end-to-end NL2SQL models. By integrating with a conventional NL2SQL process, Cyclesql 1 first rewrites the translated SQL query to retrieve provenance information from the underlying database for a specific query result. 2 Following this, the provenance is enriched by annotating the semantics of the translated SQL query to its different parts. Next, 3 a data-grounded NL explanation is interpreted from the enriched provenance to rationalize the query result by leveraging text generation techniques [33, 34, 35, 26]. Here, given that the NL explanation integrates both data-level (provenance) and operation-level (query) semantics, they possess significant potential to function as self-provided feedback for various underlying NL2SQL translation models. Finally, 4 the generated NL explanation is used to validate the correctness of the underlying translation iteratively, ultimately yielding a more reliable translation outcome. To assess the effectiveness of Cyclesql, we conduct a comprehensive experimental evaluation on the widely-used benchmark Spider, and its three variants, namely Spider-realistic [36], Spider-syn [37], and Spider-dk [38], as well as ScienceBenchmark [30], by applying Cyclesql to six contemporary end-to-end NL2SQL models. The results show that Cyclesql can consistently enhance the translation accuracy of all the models. Notably, by applying Cyclesql to Resdsql (with T5-3b scale), Cyclesql obtains a translation accuracy of 82.0% on the validation set and 81.6% on the test set of Spider benchmark, achieving best-reported result among the leading Seq2seq-based models on Spider leaderboard444https://yale-lily.github.io/spider. Moreover, a qualitative evaluation, including a case study and user study, is conducted to show that the explanations generated by Cyclesql can also greatly improve user interpretability in the black-box NL2SQL process. Contributions. We make the following three contributions: (1) Feedback Loop in NL2SQL. We propose, Cyclesql, a plug-and-play framework to establish a self-contained feedback loop within the NL2SQL process. Cyclesql employs data-grounded NL explanations as reliable feedback to iteratively validate the correctness of the translation, thereby enhancing the overall accuracy. (2) Rich Explanations for NL2SQL. The NL explanations generated by Cyclesql, incorporating not only the semantics from the query surface but also the semantics from the data instance, provide insightful information for users to understand the black-box NL2SQL translation process. (3) Quantitative and Qualitative Evaluations. We evaluate Cyclesql on five public benchmarks with seven NL2SQL models, demonstrating its substantial impact on performance improvements. Furthermore, a qualitative evaluation is conducted to gauge the utility of the generated NL explanations in enhancing user interpretability. Figure 3: Overview framework of Cyclesql"
https://arxiv.org/html/2411.02933v1,"P-MOSS: Learned
Scheduling For Indexes Over NUMA Servers Using Low-Level Hardware Statistics","Ever since the Dennard scaling broke down in the early 2000s and the frequency of the CPU stalled, vendors have started to increase the core count in each CPU chip at the expense of introducing heterogeneity, thus ushering the era of NUMA processors. Since then, the heterogeneity in the design space of hardware has only increased to the point that DBMS performance may vary significantly up to an order of magnitude in modern servers. An important factor that affects performance includes the location of the logical cores where the DBMS queries are scheduled, and the locations of the data that the queries access. This paper introduces P-MOSS, a learned spatial scheduling framework that schedules query execution to certain logical cores, and places data accordingly to certain integrated memory controllers (IMC), to integrate hardware consciousness into the system. In the spirit of hardware-software synergy, P-MOSS solely guides its scheduling decision based on low-level hardware statistics collected by performance monitoring counters with the aid of a Decision Transformer. Experimental evaluation is performed in the context of the B-tree and R-tree indexes. Performance results demonstrate that P-MOSS has up to 6×6\times6 × improvement over traditional schedules in terms of query throughput.","NUMA and Heterogeneity. NUMA servers are heterogeneous in nature leading to varying cache and memory access latencies across sockets. For example, the inter-core latency across sockets in Intel Skylake X (Intel, 2024c) can be 3×3\times3 × higher. Over the years, this heterogeneity has extended to within the socket itself. Earlier NUMA Servers (e.g., Intel Skylake X (Intel, 2024c)) that logically partition the cores within a socket into separate NUMA nodes introduce a 1.1×1.1\times1.1 × increase in random memory access latency for any inter-NUMA node communication. Intra-socket heterogeneity becomes more prominent in modern chiplet NUMA servers, e.g., AMD EPYC Milan (AMD, 2024a), where the inter-core latency between two cores in different chiplets can vary up to 4×4\times4 ×. Even for servers with a high core count that do not employ chiplet architecture or any logical partitioning, e.g., a 72-core NVIDIA GH200 Grace Hopper Superchip (NVIDIA, 2014), the inter-core latency between distant cores can vary up to 1.5×1.5\times1.5 ×.111All the numbers are generated on our testbed servers (See § 7). MMDBMS Indexes in the NUMA Landscape. The memory scalability enabled by the NUMA architecture has driven the revitalization of Main-Memory Database Management Systems (MMDBMSs) (Faerber et al., 2017) in the past decade. Indexes serve as one of the core components in an MMDBMS contributing more than 50% of the total database size (Zhang et al., 2016). An index facilitates faster lookups, range scans, and indexed joins, with DBMSs even supporting index-only query operators, e.g., index-only scans (PostgreSQL, 2024) and index-only query plans (Kissinger et al., 2013), leading to a wide range of index designs in the literature. Though overlooked in the literature, in the context of NUMA servers, the data partitioning and the query scheduling policies over a main-memory index can significantly dictate the index’s performance. Figure 1 shows the performance of a B+-Tree (Comer, 1979) in three different NUMA architectures under the 50% read-write YCSB (Cooper et al., 2010) workload. Each data point refers to a different query scheduling policy of the B+-Tree. It is evident that the same B+-Tree can exhibit different performance under different data partitioning policies. Even two policies with the same data placement but a different core scheduling strategy can yield very different performance (cf. markers in Figure 1). Moreover, no one policy performs optimally across all the NUMA architectures (cf. markers in Figure 1). Figure 1. The performance gap of a B+-Tree under different scheduling policies can differ by upto 5.83×\times× for different NUMA machines (§ 7.2). Spatial Scheduling for NUMA. To this end, we introduce the notion of spatial query scheduling. This is orthogonal to the traditional query scheduling that is temporal in nature, i.e., when to schedule a query. In contrast, spatial query scheduling explicitly decides which core in the underlying hardware gets to execute a query (compute selection), and which integrated memory controller (IMC) in the underlying hardware gets to store the data requested or generated by the scheduled query (data placement). Refer to Figure 2 for a complete overview of spatial scheduling. The core idea behind spatial scheduling is to keep the communication distance between two cores minimum to account for the intra-socket NUMA heterogeneity. The aim is to co-schedule queries that access common memory pages to nearby cores, while avoiding interference by scheduling interfering queries to distant cores. To account for the inter-socket NUMA heterogeneity, the goal is to strategically distribute data to appropriate IMCs and maximize local memory access, while ensuring that the full bandwidth of the memory channels and off-chip interconnects are utilized. Figure 2. Spatial query scheduling. P-MOSS. In this paper, we introduce P-MOSS, a learned Performance MOnitoring Unit (PMU)-driven Spatial Query Scheduling framework, that utilizes spatial query scheduling to improve the query execution performance of main memory indexes in NUMA servers. To accommodate larger indexes, P-MOSS logically partitions each index into multiple index slices based on the index key, with each slice corresponding to a specific key range. These index slices serve as the unit of spatial scheduling. P-MOSS addresses the following question: Given a main-memory index that is logically partitioned into multiple index slices on a NUMA server, how to find the best possible mapping for each index slice to a CPU core and a NUMA node, i.e., memory controller, such that the index performance, i.e., its throughput, is maximized. To the best of our knowledge, P-MOSS is the first to address the problem of spatial query scheduling. Prior work, e.g., (Porobic et al., 2012, 2014; Leis et al., 2014; Wagner et al., 2021; Psaroudakis et al., 2015, 2016; Mao et al., 2019; Sabek et al., 2022), on query scheduling draws upon the temporal aspect of query scheduling, partially ( (Porobic et al., 2012, 2014; Psaroudakis et al., 2015, 2016) only consider data placement in the context of relational tables) or completely ignoring its spatial aspect, and are orthogonal to the P-MOSS’s approach introduced in this paper. P-MOSS’s Philosophy. P-MOSS follows a novel Probe and Learn (PoLe) technique. It involves probing the hardware during query execution to observe the hardware state under a given scheduling policy, and then gaining insights from multiple such hardware-DBMS kernel interactions to learn a better scheduling policy in an automated manner. At the core of this technique is the observation that any scheduling policy to speed up query processing is only as good as the traces the queries leave during their lifetime within the processor pipeline, e.g., the number of executed instructions, cache accesses, cache misses, stalled clock cycles, etc. PoLe treats these hardware statistics as a first-class citizen for optimization purposes, and aims to learn the inherent dynamics between a scheduling policy and the associated hardware statistics in a data-driven manner via Machine Learning to realize a robust and novel scheduling policy. This paves the way to design a generalizable framework across different hardware architectures and DBMS kernels under different data distributions and query workloads. P-MOSS’s Driving Components. P-MOSS formulates the problem of spatial query scheduling as a Markov Decision Process (MDP) and adopts Offline Reinforcement Learning (§4.4) to learn the memory-page access and placement policies of an MMDBMS index. By adopting the offline RL scheme over the traditional RL, P-MOSS decouples the learning process from the DBMS kernel. This ensures that the DBMS does not under-perform during the learning cycle of the ML agent. Once the mapping policy of each index slice to the CPU cores and IMCs are learned, P-MOSS enforces the learned policy by localizing the query execution to the specific cores and distributing the memory pages over the mapped IMCs. The most noteworthy aspect of P-MOSS is that the learning process is solely guided by the hardware performance statistics sampled by the Performance Monitoring Unit (PMU) (Intel, 2024b; AMD, 2024b; ARM, 2024) of the processor (§4.4), without any sort of bookkeeping. These statistics are the hardware traces left behind by the executed queries at different parts of the hardware, e.g., CPU cores, CPU caches, memory controllers. Contributions. The main contribution of this paper is an architectural blueprint for a framework, P-MOSS, that is generalizable across diverse DBMS optimization tasks. P-MOSS requires no bookkeeping, and is adaptable across diverse hardware configurations, query workloads, and indexes. More specifically, the contributions of this paper are as follows: • We introduce P-MOSS, a learned query scheduling framework over a main-memory index that prioritizes the spatial aspect of query scheduling, i.e., query execution and data placement, at the logical core and IMC levels. • We develop DBMS kernel optimizations that are solely guided by the low-level hardware statistics collected by Performance Monitoring Units without any bookkeeping. • We formulate the problem of query scheduling as a Reinforcement Learning (RL) problem, and adopt Offline RL that facilitates learning without requiring any online interaction with the DBMS. • We perform an in-depth evaluation of P-MOSS for a diverse range of indexes (primary: B+-Tree, secondary: R-Tree), CPU vendors (Intel, AMD, ARM), NUMA hardware (1-4 NUMA Sockets, 2-8 NUMA Nodes), query workload, and report up to 6×6\times6 × throughput improvement over traditional scheduling."
https://arxiv.org/html/2411.02862v1,The Unreasonable Effectiveness of LLMsfor Query Optimization,"Recent work in database query optimization has used complex machine learning strategies, such as customized reinforcement learning schemes. Surprisingly, we show that LLM embeddings of query text contain useful semantic information for query optimization. Specifically, we show that a simple binary classifier deciding between alternative query plans, trained only on a small number of labeled embedded query vectors, can outperform existing heuristic systems. Although we only present some preliminary results, an LLM-powered query optimizer could provide significant benefits, both in terms of performance and simplicity.","Query optimization is the task of transforming complex SQL queries into efficient programs (Selinger et al. [1979]), referred to as query plans. Optimizers represent substantial engineering efforts (Giakoumakis and Galindo-Legaria [2008]), often spanning hundreds of thousands of lines of code (Graefe and McKenna [1993]). Most query optimizers today are driven by complex, manually-written heuristics. Despite significant advancements, query optimizers (QOs) are far from perfect, frequently making costly mistakes (Leis et al. [2015]). Recent work has shown that machine learning techniques can be used to steer query optimizers in the right direction, helping the optimizer determine which plan to select for query execution. Researchers have used supervised learning (e.g., Woltmann et al. [2023]), reinforcement learning (e.g., Marcus et al. [2021]), and hybrid approaches (e.g., Anneser et al. [2023]) to effectively steer optimizers. However, each approach performs sophisticated feature engineering on statistics kept internally by the database, and, as a result, requires complex and deep integration with the underlying query optimizer. This has a number of downsides that have hindered practical adoption (Zhu et al. [2024]). In this extended abstract, we present initial results for LLMSteer, a simpler approach to steering QOs. Instead of manually engineering complex features from plans or data statistics, we use a large language model (LLM) to embed raw SQL submitted by the database user. We then train a supervised learning model on a small labeled set of queries to predict the optimal direction in which to steer the QO. This places the entire “steering” component outside of the database, simplifying integration. Figure 1: LLMSteer system model. As database experts, we did not expect this simple approach to work. Common wisdom within the database community is that complex features — such as cardinality estimates (Kipf et al. [2018]) or operator models (Heinrich et al. [2022]) — are required for the task. Experimentally, we show that LLMs are capable of making these decisions without any such information. We found no simple explanation for LLMs’ apparent success; the LLM-based approach was insensitive to at least some syntax changes, and worked across two different workloads. In Section 2, we describe LLMSteer and its simple, yet powerful, design. In Section 3, we present results from initial experiments. We conclude in Section 4 with a discussion of future directions and questions left unanswered."
https://arxiv.org/html/2411.02597v1,Taming the Beast of User-Programmed Transactions on Blockchains: A Declarative Transaction Approach,"Blockchains are being positioned as the ”technology of trust” that can be used to mediate transactions between non-trusting parties without the need for a central authority. They support transaction types that are native to the blockchain platform or user-defined via user programs called smart contracts. Despite the significant flexibility in transaction programmability that smart contracts offer, they pose several usability, robustness and performance challenges.This paper proposes an alternative transaction framework that incorporates more primitives into the native set of transaction types (reducing the likelihood of requiring user-defined transaction programs often). The framework is based on the concept of declarative blockchain transactions whose strength lies in the fact that it addresses several of the limitations of smart contracts, simultaneously. A formal and implementation framework is presented and a subset of commonly occurring transaction behaviors are modeled and implemented as use cases, using an open-source blockchain database, BigchainDB as the implementation context. A performance study comparing the declarative transaction approach to equivalent smart contract transaction models reveals several advantages of the proposed approach.","Blockchains, as a technology for mediating and managing transactions between non-trusting parties, is becoming an increasingly popular concept. They are decentralized, fully replicated, append-only databases of transactions that are validated through a large, distributed consensus. These characteristics ensure that blockchain contents are tamper-proof and that no single authority controls a blockchain’s operation and contents, conferring a good degree of trust in them. Initially aimed at cryptocurrency, blockchain technology now extends to areas seeking data control and ownership decentralization, primarily for privacy and efficiency. This includes healthcare, (Agbo et al., 2019; McGhin et al., 2019), supply chain (Durach et al., 2021; Wu et al., 2019; Sund et al., 2020), decentralized finance (Defi) (Werner et al., 2021; Siyal et al., 2019), governance (Lumineau et al., 2021), web browsing, gaming, social media, and file sharing/storage (Al-Jaroodi and Mohamed, 2019). Blockchain transactions typically involve digital asset management aligned with business activities. The fundamental transaction type is asset 𝖳𝖱𝖠𝖭𝖲𝖥𝖤𝖱𝖳𝖱𝖠𝖭𝖲𝖥𝖤𝖱\mathsf{TRANSFER}sansserif_TRANSFER between accounts, a native function in most blockchains. To address the diverse needs of modern applications, blockchains have evolved to include user-designed transactions known as smart contracts (Szabo, 1997). These contracts execute business operations and adhere to specific conditions. Examples include auction bidding and regulated patient record management. Recent survey (num, 2022) indicates the existence of over 44 million smart contracts on the Ethereum blockchain alone. Problem: Smart contracts, despite their flexibility, face adoption barriers due to several issues: (i) They require significant effort in creation and verification, offer limited reusability across platforms, and constrain automatic optimization possibilities. (ii) Vulnerable to user errors and security breaches, they pose financial risks, exemplified by the DAO attack (Mehar et al., 2019) that resulted in a loss of approximately 3.6M ETH (about $6.8B). (iii) Many transactional behaviors in smart contracts, embedded in programming structures, remain hidden on the blockchain, hindering their utility in complex data analysis. (iv) Their execution involves higher latency and costs compared to native transactions. The lack of validation semantics for these user-programmed transactions complicates concurrency conflict management, leading most platforms, including Ethereum, to adopt sequential execution, which lowers throughput. Declarative smart contracts (Chen et al., 2022), domain-specific languages (Wöhrer and Zdun, 2020a), and smart contract templates (Hu et al., 2020) aim to ease creation and verification processes. However, they fall short in addressing performance, throughput, queryability, and other transactional model challenges in smart contracts. 1.1. Contributions: This paper investigates the feasibility and impact of lifting transactional behaviors typically found in smart contracts into the core blockchain layer as native transactions. Specifically, we propose: (1) a declarative and typed blockchain transaction model that includes the novel concept of nested blockchain transactions, as a foundation for modeling transactional behavior on blockchains. (2) concrete declarative blockchain transaction modeling of a sample transactional behavior represented in many smart contracts of the most popular blockchain application category - marketplaces. (3) an implementation framework for declarative blockchain transactions that builds on BigchainDB blockchain database’s architecture (big, [n. d.]), extending its transaction modeling and validation infrastructure. (4) a comparative performance and usability evaluation of the declarative transaction model vs. the smart contract model using Ethereum smart contracts as the baseline. The evaluation results demonstrate that the declarative transaction method significantly outperforms smart contracts, achieving improvements by a factor of 635 in latency and a minimum of 60 in throughput. The rest of the paper is organized as follows: Section 2 provides background information on blockchain native transactions, smart contracts, and BigchainDB. Section 3 introduces the formal blockchain transaction model and novel concepts of Non-nested and Nested transactions. Section 4 provides implementation details of the concepts presented in Section 3. Section 6 reviews the literature on the topic, while Section 5 reports on the comparative experiments conducted to evaluate our system and smart contract. Finally, we conclude the paper with a summary in Section 8."
https://arxiv.org/html/2411.02131v1,"Generating the Traces You Need: A Conditional Generative Model for Process Mining Data††thanks:We acknowledge the support of the PNRR project FAIR - Future AI Research (PE00000013), under the NRRP MUR program funded by the NextGenerationEU, the HORIZON 2020 HumanE-AI project (Grant 952026), and the PRIN project PINPOINT Prot. 2020FNEB27, CUP H23C22000280006 and H45E2100021000.","In recent years, trace generation has emerged as a significant challenge within the Process Mining community. Deep Learning (DL) models have demonstrated accuracy in reproducing the features of the selected processes. However, current DL generative models are limited in their ability to adapt the learned distributions to generate data samples based on specific conditions or attributes. This limitation is particularly significant because the ability to control the type of generated data can be beneficial in various contexts, enabling a focus on specific behaviours, exploration of infrequent patterns, or simulation of alternative “what-if” scenarios.In this work, we address this challenge by introducing a conditional model for process data generation based on a conditional variational autoencoder (CVAE). Conditional models offer control over the generation process by tuning input conditional variables, enabling more targeted and controlled data generation. Unlike other domains, CVAE for process mining faces specific challenges due to the multiperspective nature of the data and the need to adhere to control-flow rules while ensuring data variability. Specifically, we focus on generating process executions conditioned on control flow and temporal features of the trace, allowing us to produce traces for specific, identified sub-processes. The generated traces are then evaluated using common metrics for generative model assessment, along with additional metrics to evaluate the quality of the conditional generation.","Process mining (PM) [1] is a research field that focuses on the analysis, monitoring, and improvement of business processes based on event logs. Within this field, generative models have emerged in recent years as crucial tools for generating new event trace samples that replicate process behavior[2, 3, 4, 5, 6, 7, 8, 9]. These models support a range of applications, including anomaly detection [7, 10], predictive monitoring [2], what-if scenario analysis [11] and conformance checking [12]. An important yet underexplored aspect of trace generation is the ability to produce traces that follow different distributions from the training data, allowing exploration of various dimensions of interest within the process. These dimensions may include exploring what-if scenarios, expanding variants of interest (especially when significant for the process analysis but numerically low), or exploring resource contingency plans. According [13], generative models can be categorized into two main families: Data-Driven Process Simulation (DDPS) and Deep Learning (DL). DDPS constructs explicit process models from data, esnuring that complete information about the simulation is always available. These models are beneficial for providing insights into specific subprocesses and allow to modify almost every aspect of the simulation. However, DDPS often relies on oversimplified assumptions, leading to unrealistic simulations and data generation. Moreover, they struggle to capture long-term dependencies. DL models are statistical models that accurately capture the correlations between features in the generated samples. Despite their accuracy, DL models are “black box” systems, making it challenging to transparently expose the underlying process model. More importantly, existing DL-based generative models are rigid, limiting the generation of distributions different from the training data. This constraint significantly inhibits the exploration of specific scenarios or dimensions of interest. Hybrid models, which integrate the accuracy of DL techniques with the transparency of explicit process models, have been recently introduced [14, 15]. While they may have potential to generate specific data for dimensions of interest, an assessment of this capability is still missing. Conditional generative models [16] have been proven effective in various domains to mitigate the rigidity observed in DL generative models. These models generate outputs influenced by certain input variables. This offers a means to guide the generation process based on desiderata for the expected output. In this work, we introduce a conditional variational autoencoder (CVAE) model for generating traces based on LSTM neural networks. Compared to other domains, developing a CVAE for process mining presents two main challenges: • Process execution data have an inherently multi-perspective nature. Traces consist of sequence of temporal features with both categorical (events) and numerical (timestamps) characteristics. This complexity is further increased by the inclusion of payloads. Moreover, all these components are strongly interconnected. Therefore, due to their diverse nature, each component of the generated samples requires a dedicated module in the model architecture, which must still produce a coherent output. • While it is desirable for the generative model to produce data with some variability, event sequences must adhere to causal constraints and diverse contextual factors. Thus, the variability must remain consistent with process constraints to ensure meaningful results. A further contribution of our work is the proposal of a novel evaluation methodology for assessing process generation quality in a conditional context. Alongside common metrics for evaluating generative process models, such as the accuracy of the control flow and generated timestamps [17], we introduce a new analysis to measure the impact of the conditioning variable and measure the actual conditioning rate. Moreover, unlike many simulation scenarios that aim to closely replicate historical data, generative models in a conditional context should introduce variability in the generated output while remaining within process constraints. Thus, our evaluation framework includes metrics to assess the variability and conformance of the generated traces with the process rules. By integrating conditional models into process data generation, this work aims to enhance the flexibility and control of DL generative models in Process Mining. Evaluation on four different examples based on three real-world event logs shows promising results. Specifically, the generated traces are accurate, exhibit good variability, comply with process constraints, and demonstrate that the conditional generative model effectively controls the types of traces produced."
https://arxiv.org/html/2411.01807v1,Can Language Models Enable In-Context Database?,"Large language models (LLMs) are emerging as few-shot learners capable of handling a variety of tasks, including comprehension, planning, reasoning, question answering, arithmetic calculations, and more. At the core of these capabilities is LLMs’ proficiency in representing and understanding structural or semi-structural data, such as tables and graphs. Numerous studies have demonstrated that reasoning on tabular data or graphs is not only feasible for LLMs but also gives a promising research direction which treats these data as in-context data. The lightweight and human readable characteristics of in-context database can potentially make it an alternative for the traditional database in typical RAG (Retrieval Augmented Generation) settings. However, almost all current work focuses on static in-context data, which does not allow dynamic update. In this paper, to enable dynamic database update, delta encoding of database is proposed. We explore how data stored in traditional RDBMS can be encoded as in-context text and evaluate LLMs’ proficiency for CRUD (Create, Read, Update and Delete) operations on in-context databases. A benchmark named InConDB is presented and extensive experiments are conducted to show the performance of different language models in enabling in-context database by varying the database encoding method, prompting method, operation type and input data distribution, revealing both the proficiency and limitations.","The recent surge in enthusiasm for LLMs (Large Language Models) stems from their growing ability to handle and interpret textual data, as demonstrated by notable studies from (Vaswani et al., 2017; Devlin et al., 2018; Radford et al., 2018; Raffel et al., 2020; Brown et al., 2020; Touvron et al., 2023; Zhao et al., 2023; Ouyang et al., 2022). Originally designed to process sequential text, these models have now been adapted to performing a wide range of tasks across different modalities, such as voices, images and videos(Chen et al., 2022; Arnab et al., 2021; Zhao et al., 2023). Modern LLMs are evolving as few-shot (or zero-shot) learners (Brown et al., 2020) capable of handling a variety of tasks, ranging from question answering, semantic comprehension, planning, reasoning, arithmetic calculations and more (Achiam et al., 2023; Huang et al., 2022; Andreas, 2022; Adhikari et al., 2020; Ammanabrolu and Riedl, 2021; Tandon et al., 2019; Madaan et al., 2022; Creswell et al., 2022). Collectively, these developments have reinforced the belief that LLMs are critical milestones on the journey toward achieving artificial general intelligence (AGI) (Bubeck et al., 2023). Researchers believe LLMs’ capability rely on their representations and comprehension of various structures from outside world, which can be collected and expressed as structural data such as graphs and tables. Recently numerous studies have shown that LLMs are capable of doing in-context reasoning on graphs (Fatemi et al., 2023; Guo et al., 2023; Perozzi et al., 2024; Wang et al., 2024; Ye et al., 2023) and tables (Sui et al., 2023; Jiang et al., 2023; Gong et al., 2020; Liu et al., 2022; Sui et al., 2024; Zhang et al., 2024). Given the structural data and the description of a task, possibly with extra examples and instructions, LLMs need to reply with the correct result of the task on the structural data. By serializing the structural data using various encoding scheme and adopting different prompting technologies, these work try to find out the optimal protocol through which the input can maximize LLMs’ capability of reasoning on structural data. However, almost all of the exiting work does not take into account of the dynamic feature of the structural data, which may be updated frequently in the actual application scenarios. For instance, the nodes and edges may be added/removed to/from the graph, and new knowledge will be inserted into the knowledge graph while outdated knowledge will be removed. Similarly, tabular data may also be edited to reflect the updated situation. So the question arises naturally: can LLMs enable in-context update of structural data? In this case, not only a querying task is allowed as input to LLMs, but an updating task will also be allowed. If the update is acceptable and does not violate any constraint of the structural data, LLMs need to register the update, otherwise LLMs need to reply with an error message. In this setting, LLMs should not only be capable of comprehending the structures of the data, but also understanding the updates along the temporal dimension. Figure 1. Overview of our evaluation framework. First we compose several JSON files, each containing a bunch of CRUD operations for one database schema. Then we use a data sampler to generate samples of (instruction sequence, query) pair. The instruction sequence is used to imitate the daily database operations which constantly come in and change the status of the database. We can also consider the instruction sequence as a representation of the current status of the database. Then a query is used get the result on the current status of the database. We use real database such as MySQL to execute the instruction sequence followed by the query to get the ground truth query result. As another branch, we send the prompting-decorated encoding of the (instruction sequence, query) pair to the large language model, and ask the LLM to imitate a database to execute all the instructions and the query to get a result. Finally we use an accuracy calculator to get the accuracy score measuring the discrepancy between ground truth and LLM-generated query result. In this illustration, the LLM thinks the 4th command (delete operation) can be executed successfully, without noting it violates the foreign key constraint. As the context length of LLMs increases rapidly with some technologies being proposed to even scale the context to infinitely long (Munkhdalai et al., 2024), it is promising that LLMs’ context window can accommodate way larger dataset in the near future. Combined with LLMs’ in-context reasoning capability on dynamic structural data, we believe it is possible for LLMs to enable in-context database, which is a lightweight alternative of traditional database in which CRUD (Create, Read, Update and Delete) operations are handled by LLMs, rather than pre-programmed procedures as in traditional DBMS. In the prevalent settings of RAG (Retrival Augmented Generation), where the whole dataset is stored on external database and sampled as required by the specific task, in-context database provide an alternative solution. Also the adoption of in-context database and traditional database in RAG is not necessarily exclusive, they can complement with each other. For instance, external database can store the full dataset which has lower rate of updates whereas in-context database can act as a partial cache which is also more fresh. In a typical multi-agent (Guo et al., 2024; Liang et al., 2023) configurations, there can be an agent which is mainly responsible for data management, which act as a database, but can understand and execute queries more intelligently. In this work, we perform the first comprehensive evaluation about how the data stored in the traditional RDBMS database can be encoded as text and LLMs’ proficiency for CRUD operations on in-context database. A benchmark named InConDB is proposed which includes dataset and queries for RDBMS databases. We conduct extensive experiments to demonstrate the performance of language models in enabling in-context database depends on various factors such as database encoding method, prompt engineering, operation type and data distribution. Both the proficiency and limitations of current LLMs are revealed. Figure 1 illustrates the evaluation framework. Our contribution can be summarized as follows: • For the first time, we propose the concept of LLM based in-context database, which can be a lightweight alternative for the traditional database in a RAG setting. • To our knowledge, our research is the first to evaluate LLMs on dynamic structural data, which not only evaluate their capability of reasoning on structures but also on temporal updates. • We create a benchmark named InConDB to evaluate the performance of different LLM models in enabling in-context database, by varying different input factors such as database encoding method, prompt engineering, operation type and input data distribution. • The experiment results are not only valuable in evaluating language models’ performance as in-context database, but also valuable in evaluating large language model’s capability to understand the accumulating effects of historical events in a more general perspective. This paper is organized as following sections. Section 2 presents the framework of in-context database. Section 3 shows the experimental results.Section 4 introduces the closely related topics to our research. Section 5 concludes the paper."
https://arxiv.org/html/2411.01325v1,TrajRoute: Rethinking Routing with a Simple Trajectory-Based Approach — Forget the Maps and Traffic!,"The abundance of vehicle trajectory data offers a new opportunity to compute driving routes between origins and destinations. Current graph-based routing pipelines, while effective, involve substantial costs in constructing, maintaining, and updating road network graphs to reflect real-time conditions. In this study, we propose a new trajectory-based routing paradigm that bypasses current workflows by directly utilizing raw trajectory data to compute efficient routes. Our method, named TrajRoute, uniquely “follows” historical trajectories from a source to a destination, constructing paths that reflect actual driver behavior and implicit preferences. To supplement areas with sparse trajectory data, the road network is also incorporated into TrajRoute’s index, and tunable parameters are introduced to control the balance between road segments and trajectories, ensuring a unified and adaptable routing approach. We experimentally verify our approach by comparing it to an existing online routing service. Our results demonstrate that as the number of trajectories covering the road network increases, TrajRoute produces increasingly accurate travel time and route length estimates while gradually eliminating the need to downgrade to the road network. This highlights the potential of simpler, data-driven pipelines for routing, offering lower-maintenance alternatives to conventional systems.","The recent widespread adoption of location-acquisition technologies, such as GPS data from vehicles, has generated vast amounts of user trajectory data (Ding et al., 2015; Hu et al., 2018). For instance, companies such as Tesla and Google have access to comprehensive real-time trajectory datasets that their users continuously provide. This has offered a foundation for improving navigation services, where trajectory data are used to annotate the road network edges with more realistic travel time estimates (Dai et al., 2016). As more data becomes available, these edges are continuously updated to reflect real-time conditions. A typical pipeline used by routing services, shown in Figure 2(a), involves several cost-intensive preprocessing steps. One common step is map matching, where GPS data is aligned with the road network (Newson and Krumm, 2009). The network is then dynamically updated with this information, generating traffic snapshots that enable accurate path recommendations. In an effort to deliver personalized routing, a number of research studies have also focused on utilizing these trajectory data to extract more insights such as preferences, patterns, and popular paths which are also used to annotate the graph edges, allowing for more customized route recommendations (Ceikute and Jensen, 2015; Guo et al., 2018; Chen et al., 2011; Delling et al., 2015; Letchner et al., 2006; Patel et al., 2006; Yang et al., 2015; Chang et al., 2011). However, maintaining these graph-based pipelines is costly due to the need for continuous data collection, processing, and frequent updates to reflect real-time traffic estimates, rendering the entire process resource-intensive. Figure 1. Example of TrajRoute generating a route from Fisherman’s Wharf to the Mission District. \Description Example of TrajRoute generating a route from Fisherman’s Wharf to the Mission District. (a) Overview of a typical pipeline used by current routing services. The steps in the dashed box are repeated as newer data becomes available. (b) Our proposed, TrajRoute pipeline. Figure 2. Comparison of pipelines: (a) current routing services and (b) our proposed pipeline. \Description Comparison of pipelines: (a) current routing services and (b) our proposed pipeline. With the increasing availability of large-scale and up-to-date trajectory datasets, we have identified a new opportunity: the ability to extract routes directly from historical raw data. This approach eliminates the need for costly graph construction and ongoing maintenance, streamlining the process and leveraging the richness of real-world data. To this end, we propose a new approach for routing, namely TrajRoute, that directly leverages raw trajectory data for routing decisions. In this paper, we demonstrate that accurate route recommendations can be derived by following historical trajectories. TrajRoute’s approach is straightforward yet effective: given an origin, a destination, and a start time, the system utilizes historical trajectories to compute the optimal route in terms of travel time. If a complete historical trajectory between the origin and destination is unavailable, TrajRoute combines segments from multiple trajectories to construct a continuous path. Consider it analogous to hopping from one vehicle to another to reach the final destination. The following example, shown in Figure 1, illustrates how TrajRoute operates in practice. Example. A user starts at Fisherman’s Wharf in San Francisco, CA, and requests a route to their home near the Mission District using TrajRoute at 8:00 AM on a weekday. The system first identifies a commuter’s trajectory, marked in red, that was recorded around the same time and day in the past, which the user follows from Fisherman’s Wharf to the Civic Center (shown with dashed white lines). Based on historical data, the system calculates that the user will arrive at the Civic Center at 8:20 AM. At this point, the system selects a second trajectory, marked in green, that departed at 8:10 AM the same day from Civic Center. This green trajectory leads to the Mission District, with the remainder of the journey estimated to take about 7 minutes, bringing the user to their destination at around 8:27 AM. While there are other historical routes recorded at the same time frame, denoted in grey color, this combination, from parts of two other trajectories, offers the fastest route based on historical information, ensuring an optimized and realistic travel experience for the user, while eliminating the need for the system to maintain or artificially determine road speeds and turn costs. One key advantage of TrajRoute is that it eliminates the need for the costly preprocessing steps found in typical routing pipelines. As shown in Figure 2(a) modern routing services rely on extensive data preprocessing, which includes tasks like map matching and continuously updating road network annotations as new data becomes available. The steps within the dashed box are reprocessed regularly to account for current traffic conditions. In contrast, TrajRoute bypasses these steps by working directly with raw historical trajectory data. To retrieve relevant trajectories efficiently, TrajRoute uses a spatio-temporal grid-based index, allowing it to find trajectories that best match a user’s query. The approach performs optimally when there is consistently a segment of a trajectory near the current position and within the present time frame. For scenarios where a direct path to the destination is not entirely available from historical data, TrajRoute integrates adjacent road segments to bridge the gaps, ensuring that users will always receive a valid route as a query answer. It’s important to note that if no trajectories pass through these roads, conventional routing methods won’t have access to real-time travel times for those segments either. Figure 2(b), presents the proposed framework. We experimentally verify TrajRoute on a real trajectory dataset and compare the routes generated with those from Azure Maps111https://azure.microsoft.com/en-us/products/azure-maps in terms of route accuracy and travel time precision. The results confirm that TrajRoute consistently delivers reliable and precise routing recommendations. One of the primary advantages of TrajRoute is its ability to inherently integrate routing features that are implicitly encoded in the trajectories, such as variable traffic speeds, intersection-related delays, and individual driver preferences, which are typically challenging and expensive to incorporate in standard graph-based routing systems. When supplied with abundant up-to-date trajectory data, TrajRoute can effectively eliminate the need for the heavy-weight pipeline of map-matching, traffic inference, and graph edge updating. Additionally, it is simple in tuning and maintenance, requiring only three parameters: the grid size, which determines the granularity of trajectory indexing, and two additional parameters that adjust the reliance on historical trajectory data versus traditional road network segments. This setup ensures a flexible and effective routing solution. To summarize, we make the following contributions: • We introduce TrajRoute, a new simple routing paradigm that bypasses the reliance on graph-based approaches and instead utilizes raw historical trajectory data to compute optimal routes. To the best of our knowledge, this is the first study to fully eliminate the current preprocessing pipeline. • To address gaps in trajectory coverage, we integrate the road network into the same index as trajectories. We propose tunable parameters to control the balance between using historical trajectories and road network segments, ensuring flexible and complete route coverage while maintaining a unified approach. • We experimentally verify the effectiveness of TrajRoute using real-world trajectory data. Our results show that the extracted routes are intuitive and comparable to those from an established routing engine in terms of distance and estimated travel time, while requiring lower maintenance costs. The rest of the paper is organized as follows: Section 2 reviews related work regarding the utilization of trajectories in routing solutions. Section 3 formally defines the problem, and Section 4 describes our methodology. Section 5 discusses our experimental setup, dataset, and results. Section 6 concludes the paper."
https://arxiv.org/html/2411.01269v1,"Disaggregated Database Management Systems††thanks:This paper appeared in thePerformance Evaluation and Benchmarking- 14th TPC Technology Conference, TPCTC 2022, Sydney, NSW, Australia, September 5, 2022, Revised Selected Papers. Lecture Notes in Computer Science 13860, Springer 2023, ISBN 978-3-031-29575-1.","Modern applications demand high performance and cost efficient database management systems (DBMSs). Their workloads may be diverse, ranging from online transaction processing to analytics and decision support. The cloud infrastructure enables disaggregation of monolithic DBMSs into components that facilitate software-hardware co-design. This is realized using pools of hardware resources, i.e., CPUs, GPUs, memory, FPGA, NVM, etc., connected using high-speed networks. This disaggregation trend is being adopted by cloud DBMSs because hardware re-provisioning can be achieved by simply invoking software APIs. Disaggregated DBMSs separate processing from storage, enabling each to scale elastically and independently. They may disaggregate compute usage based on functionality, e.g., compute needed for writes from compute needed for queries and compute needed for compaction. They may also use disaggregated memory, e.g., for intermediate results in a shuffle or for remote caching. The DBMS monitors the characteristics of a workload and dynamically assembles its components that are most efficient and cost effective for the workload. This paper is a summary of a panel session that discussed the capability, challenges, and opportunities of these emerging DBMSs and disaggregated hardware systems.","Emerging data centers disaggregate hardware into pools of resources and connect them using fast networks such as high-speed Ethernet or Remote Direct Memory Access, RDMA [10]. The pool of resources may include CPUs, GPUs, memory, NVMe, disks, SSDs, FPGAs, and specialized hardware such as Amazon Trainium and Google TPU for machine learning among others. The software that implements a database management system may also be disaggregated into micro-services. Both trends facilitate a software-hardware co-design. Moreover, they enable composition of micro-services into new services. For example, an in-memory key-value store may be realized using a subset of micro-services that implement a relational database management system (DBMS) [6]. Assembly of the components must consider the communication latency and employ caches to prevent this latency from dominating performance. Disaggregated DBMSs hold the potential to transform today’s obsolete practices to enhance efficiency by providing sustainable solutions. Instead of asking a user to size a server, they may ask a user for their daily budget and desired performance objective. Now, it is the responsibility of an intelligent agent to assemble the hardware and software to meet the price and performance requirements. With a high (low) system load, the assembled system may scale-up (down). The system may use alternative forms of storage that provide different price/performance characteristics [7]. (a) A monolithic DBMS. (b) A disaggregated DBMS. Figure 1: Today’s monolithic DBMS and the envisioned disaggregated DBMS. Physical data design is a task performed by data administrators to enhance efficiency and meet the performance requirements of an application. They make decisions such as whether the data is stored in a column format or a row format. However, many startups do not have a budget to pay these experts, resulting in inefficiencies. A disaggregated DBMS will address this by monitoring the system workload and fine-tuning the physical design of the database, choice of hardware including a storage hierarchy, and microservices assembled to realize the most cost effective deployment. Today’s use of data centers by scientists, including those in the area of machine learning and AI, requires them to upload their data to a data center and run their computation in the cloud. A key question is what data to copy to the cloud? As an example, NIH’s-National Library of Medicine has 36.4 Petabytes of Genomic sequencing data on two commercial cloud platforms [15]. How are scientists to discover and use this data? Scientists want to know what is the minimum cost configuration for an experiment. They may have a fixed budget for running an experiment. Once the budget is exhausted, they may want to save their result files and have the service shut-down so there are no additional charges [15]. They may also want to take a snapshot of their mid-flight experiment that enables them to continue where it was stopped. A vision for a future system is one that reduces data movement and replication [15]. One way to realize this is to extend the disaggregation beyond a data center to include a scientist’s desktop. It provides for physical data independence, a concept pioneered by the database community, where the scientist is no longer burdened with the placement of data. It is the responsibility of the infrastructure to manage placement of data and computation seamlessly. The rest of this paper is organized as follows. Sections 2, 3 and 4 describe hardware, memory, and DBMS disaggregation in turn. Brief future research directions are presented in Section 5."
https://arxiv.org/html/2411.01246v1,"CAMP: A Cost Adaptive Multi-Queue Eviction Policy for Key-Value Stores††thanks:Sandy Irani and Jenny Lam are with the University of California, Irvine. Their research is supported in part by the NSF grant CCF-0916181. Shahram Ghandeharizadeh and Jason Yap are with the University of Southern California.111A shorter version of CAMP appeared in the Proceedings of the ACM/IFIP/USENIX
Middleware Conference, Bordeaux, France, December 2014. Seehttps://github.com/scdblab/CAMPfor an implementation.","Cost Adaptive Multi-queue eviction Policy (CAMP) is an algorithm for a general purpose key-value store (KVS) that manages key-value pairs computed by applications with different access patterns, key-value sizes, and varying costs for each key-value pair. CAMP is an approximation of the Greedy Dual Size (GDS) algorithm that can be implemented as efficiently as LRU. In particular, CAMP’s eviction policies are as effective as those of GDS but require only a small fraction of the updates to an internal data structure in order to make those decisions. Similar to an implementation of LRU using queues, it adapts to changing workload patterns based on the history of requests for different key-value pairs. It is superior to LRU because it considers both the size and cost of key-value pairs to maximize the utility of the available memory across competing applications. We compare CAMP with both LRU and an alternative that requires human intervention to partition memory into pools and assign grouping of key-value pairs to different pools. The results demonstrate CAMP is as fast as LRU while outperforming both LRU and the pooled alternative. We also present results from an implementation of CAMP using Twitter’s version of memcached.","Applications with a high read-to-write ratio augment their persistent infrastructure with an in-memory key-value store (KVS) to enhance performance. An example is memcached in use by popular Internet destinations such as Facebook, Twitter, and Wikipedia. Using a general purpose caching layer requires workloads to share infrastructure despite different access patterns, key-value sizes, and time required to compute a key-value pair [21]. An algorithm that considers only one factor may cause different application workloads to impact one another negatively, decreasing the overall effectiveness of the caching layer. As an example, consider two different applications of a social networking site: one shows the profile of members while a second determines the displayed advertisements. There may exist millions of key-value pairs corresponding to different member profiles, each computed using a simple database look-up that executed in a few milliseconds. The second application may consist of thousands of key-value pairs computed using a machine-learning algorithm that processed Terabytes of data and required hours of execution. This processing time is one definition of the cost of a key-value pair. With a limited memory size and a high frequency of access for member profile key-value pairs, a simple algorithm that manages memory using recency of references (LRU) may evict most of the key-value pairs of the second application, increasing the incurred cost. In general, reducing the incurred cost translates into a faster system that processes a larger number of requests per unit of time and may provide a better quality of service. The latter is due to availability of data (e.g., cache hit for a key-value computed using the machine learning algorithm) that enables the application to provide a user with more relevant content than content selected randomly. A possible approach is for a human expert to partition the available memory into disjoint pools with each pool managed using LRU. Next, the expert groups key-value pairs with similar costs together and assigns each group to a different pool [18]. With our example, the expert would construct two pools. One for the key-value pairs corresponding to members profiles and a second corresponding to advertisements. The primary limitation222Partitioning is known to reduce the utilization of resources by resulting in formation of hot spots and bottlenecks. One may address this limitation by over-provisioning resources. of this approach is that it requires a human familiar with the different classes of applications to identify the pools, construct grouping of key-value pairs, and assign each group to a pool. Over time, the service provider may either introduce a new application or discontinue an existing one. This means the human expert must again become involved to identify the pool for the key-value pairs of the new application and possibly rebalance memory across the pools once an application is discontinued. This paper introduces a novel caching method called Cost Adaptive Multi-queue eviction Policy (CAMP), that manages the available memory without partitioning it. CAMP is an approximation of the Greedy Dual Size (GDS) algorithm [4] that processes cache hits and misses more efficiently using queues. Hence, it is significantly faster than GDS and as fast as LRU. It is novel and different from LRU in that it constructs multiple LRU queues dynamically based on the size and cost of key-value pairs. The number of constructed LRU queues depends on the distribution of costs and sizes of the key-value pairs. CAMP manages these LRU queues without partitioning memory. Thus, there is no need for human involvement to construct groups of key-value pairs, dictate assignment of groups to the pools, or configure and adjust the memory pool characteristics. CAMP is robust enough to prevent an aged expensive key-value pair from occupying memory indefinitely. Such a key-value pair is evicted by CAMP as competing applications issue more requests. CAMP is parameterized by a variable that controls its precision. At the highest precision, CAMP’s eviction decisions are essentially equivalent to those made by GDS. Our empirical results show that CAMP does not suffer any degradation in the quality of its eviction decisions at lower precisions. Moreover, it is able to make those decisions much more efficiently than GDS. GDS requires an internal priority queue to determine a key-value pair to evict from the cache. The time to maintain its data structures consistent in a thread-safe manner is expensive because it requires synchronization primitives [12] with multiple threads performing caching decisions. Moreover, CAMP performs a significantly fewer updates of its internal data structures than GDS, reducing the number of times it executes the thread-safe software dramatically. The rest of this paper is organized as follows. Section 2 starts with a description of GDS to motivate CAMP and details its design decisions. Section 3 presents a simulation study of CAMP and compares it with LRU and the pooled approach that partitions resources, demonstrating its superiority. Section 4 describes an implementation of CAMP using a variant of Twemcache and compares this implementation with the original that uses LRU. Obtained results demonstrate that CAMP is as fast as LRU and provides superior performance as it considers, in addition to recency of requests, the size and the cost of the key-value pairs. Section 5 describes related work. Section 6 provides brief words of conclusions and future research directions."
https://arxiv.org/html/2411.01214v1,Multivariate Time Series Cleaning under Speed Constraints,"Errors are common in time series due to unreliable sensor measurements. Existing methods focus on univariate data but do not utilize the correlation between dimensions. Cleaning each dimension separately may lead to a less accurate result, as some errors can only be identified in the multivariate case. We also point out that the widely used minimum change principle is not always the best choice. Instead, we try to change the smallest number of data to avoid a significant change in the data distribution. In this paper, we propose MTCSC, the constraint-based method for cleaning multivariate time series. We formalize the repair problem, propose a linear-time method to employ online computing, and improve it by exploiting data trends. We also support adaptive speed constraint capturing. We analyze the properties of our proposals and compare them with SOTA methods in terms of effectiveness, efficiency versus error rates, data sizes, and applications such as classification. Experiments on real datasets show that MTCSC can have higher repair accuracy with less time consumption. Interestingly, it can be effective even when there are only weak or no correlations between the dimensions.","Outliers are common in time series and can have two different meanings (errors or anomalies) (Blázquez-García et al., 2022). We focus on errors that result from measurement inaccuracies, data collection or system malfunctions (Aggarwal, 2016). Even in areas such as stocks and flights, a surprisingly large amount of inconsistent data is observed (Li et al., 2012). It is important to identify and correct these erroneous values to ensure the accuracy and reliability of time series analyzes (Hyndman and Athanasopoulos, 2018). 1.1. Motivation Constraint-based methods for cleaning time series have been proposed in recent years. Existing studies (Song et al., 2015, 2021) consider the constraints of speeds and accelerations on value changes, namely speed/ acceleration constraints, respectively. They recognize the violations and modify these dirty points according to the minimum change principle (Bohannon et al., 2005) to the repaired results that correspond to the defined constraints. However, such a principle leads to maximum/minimum compatible values being the final results (referred to as border repair), which can significantly change the data distribution (Dasu and Loh, 2012). On the other hand, current constraint-based methods all focus on univariate time series, i.e. with only one dimension. A multivariate data point can meet the given constraint in each of the single dimensions, but still violate the speed constraint in the multivariate case (see data point at t8subscript𝑡8\mathit{t}_{8}italic_t start_POSTSUBSCRIPT 8 end_POSTSUBSCRIPT in Example 1.1). Furthermore, they cannot recognize the small errors that actually meet the speed constraints. The small errors are very important in some applications such as automatic driving (Zhang et al., 2016). Smoothing methods such as (Gardner, 2006) use the weighted average of previous values as repaired results and suffer from the problem of over-repair, where almost all data points are changed even though most of them are originally correct. The statistical-based method (Zhang et al., 2016) builds a probability distribution model of speed changes between neighboring data points over the entire data to repair small errors. (Wang et al., 2024) develops algorithms in streaming scenarios using a dynamic probability construction. Although small errors are successfully repaired, the above methods are still proposed for univariate time series and do not capture the correlations between data dimensions. The more recent learning-based methods (Tuli et al., 2022; Zhang et al., 2021) pay more attention to the anomalies (not errors), i.e. the observations that deviate significantly from the majority of the data. These methods always require a large number of clean data samples and are quite sensitive to the hyper-parameters. Figure 1. Example of observations and different repairs Example 1.1. Consider a continuous sequence of 12 GPS trajectories in Figure 1. There are considerable deviations in the observations (in black) from t5,t6,t8subscript𝑡5subscript𝑡6subscript𝑡8\mathit{t}_{5},\mathit{t}_{6},\mathit{t}_{8}italic_t start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT 8 end_POSTSUBSCRIPT, which are influenced by the passage through the building and are regarded as errors. The smoothing method (in orange) changes almost all points, even the originally correct ones. As a univariate method, SCREEN considers each dimension separately. The point at t8subscript𝑡8\mathit{t}_{8}italic_t start_POSTSUBSCRIPT 8 end_POSTSUBSCRIPT satisfies the speed constraint in every single dimension and remains unchanged, the point at t4subscript𝑡4\mathit{t}_{4}italic_t start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT is changed accordingly (in purple). Then we consider using speed constraint in the multivariate case. Following the minimum change principle, the border repair is applied at t5subscript𝑡5\mathit{t}_{5}italic_t start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT and t8subscript𝑡8\mathit{t}_{8}italic_t start_POSTSUBSCRIPT 8 end_POSTSUBSCRIPT, and t6subscript𝑡6\mathit{t}_{6}italic_t start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPT is therefore considered correct. If we take advantage of the data distribution (trend of succeeding points) and manage to change the least number of points, the repair result (our proposal MTCSC, in red) is closer to the ground truth (in green). 1.2. Solution To overcome the above problems of the existing methods, we apply the speed constraint over all dimensions together instead of considering each dimension separately. Moreover, we do not follow the minimum change principle (Bohannon et al., 2005) to minimize the total distance before and after the repair, since a certain portion of originally clean data is still changed. Instead, we alternatively manage to fix the minimum number of data points and maintain the data distribution, a.k.a. the minimum fix principle (Afrati and Kolaitis, 2009). For small errors that are difficult to detect by the speed constraint, we use the information about the data distribution in the sliding window and modify the points even if they satisfy the given constraints. Based on the above techniques, we propose MTCSC to clean the multivariate time series under speed constraints. However, there still exists some challenges. (1) It may be difficult to explain why it makes sense to consider the speed constraint across all dimensions together. For a GPS trajectory, it is obvious that the speed should be calculated across all two dimensions. But it is not convincing enough to calculate the speed across the dimensions without any correlations. Will this work in practice and why? (2) When considering the multivariate case, the search for a suitable repair candidate is not trivial due to the curse of dimensionality (Taylor, 2019). On the one hand, the higher the dimensions, the sparser the data points are, which makes it difficult to set a suitable speed constraint. On the other hand, the candidate space becomes quite large. In the univariate case, the repair candidate lies on a line, in the two-dimensional case on a circle, in the three-dimensional case on a cube, and so on. (3) Time series are not stationary (Ding et al., 2023) in some scenarios, but the success of our proposal depends on the correct specification of the speed constraint. How to dynamically determine the speed constraint when points arrive becomes another challenge. 1.3. Contribution The proposed MTCSC is a linear time, constant space cleaning method over multivariate time series. Our major contributions in this paper are summarized as: (1) We formalize the repair problem over multivariate time series under speed constraints by considering the whole time series or data points in a window, respectively, in Section 2. By transforming to the MIQP/MILP problem, existing solver can be employed. We further reduce the time complexity to polynomial time by using the idea of dynamic programming (MTCSC-G). (2) We design an online linear time cleaning method (MTCSC-L) in Section 3.1 to locally determine whether the first point (key point) in the current window to be repaired or not as data point arrives. It is notable that soundness w.r.t. speed constraint satisfaction is guaranteed in the devised algorithm. (3) We enhance the online method by capturing the data distribution in the local window via clustering (MTCSC-C) in Section 3.2 to further promote the accuracy. Considering the trend of the following data points in the given window, small errors that satisfy the speed constraint can also be repaired. (4) We present an adaptive method (MTCSC-A) that can capture the proper speed constraint in Section 4. Once the difference of the speed distribution in a certain period is larger than a per-defined threshold, which means the characteristic of the series has been changed, the speed constraint is then updated to capture the feature of the incoming data. (5) We analyze the experimental results on real datasets and discuss the superiority and limitations of our proposed methods in Section 5. Table 1 lists the notations frequently used in this paper. Table 1. Notations Symbol Description 𝒙𝒙\boldsymbol{\mathit{x}}bold_italic_x time series 𝒙isubscript𝒙𝑖\boldsymbol{\mathit{x}}_{i}bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, 𝒙⁢[i]𝒙delimited-[]𝑖\boldsymbol{\mathit{x}}[i]bold_italic_x [ italic_i ] i𝑖iitalic_i-th data point in 𝒙𝒙\boldsymbol{\mathit{x}}bold_italic_x tisubscript𝑡𝑖\mathit{t}_{i}italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT timestamp of i𝑖iitalic_i-th data point in 𝒙𝒙\boldsymbol{\mathit{x}}bold_italic_x s𝑠\mathit{s}italic_s speed constraint w𝑤\mathit{w}italic_w window size of speed constraint n𝑛nitalic_n length of time series 𝒙𝒙\boldsymbol{\mathit{x}}bold_italic_x D𝐷Ditalic_D dimension of time series 𝒙𝒙\boldsymbol{\mathit{x}}bold_italic_x 𝒙′superscript𝒙′\boldsymbol{\mathit{x}}^{\prime}bold_italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT repair of time series 𝒙𝒙\boldsymbol{\mathit{x}}bold_italic_x zisubscript𝑧𝑖\mathit{z}_{i}italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT 0 if 𝒙isubscript𝒙𝑖\boldsymbol{\mathit{x}}_{i}bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is not modified, otherwise, 1 M𝑀Mitalic_M a constant number which is sufficient large satisfy(𝒙i,𝒙j)subscript𝒙𝑖subscript𝒙𝑗(\boldsymbol{\mathit{x}}_{i},\boldsymbol{\mathit{x}}_{j})( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) 𝒙isubscript𝒙𝑖\boldsymbol{\mathit{x}}_{i}bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is compatible with 𝒙jsubscript𝒙𝑗\boldsymbol{\mathit{x}}_{j}bold_italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT w.r.t. s𝑠\mathit{s}italic_s"
https://arxiv.org/html/2411.00970v1,Incremental IVF Index Maintenance for Streaming Vector Search,"The prevalence of vector similarity search in modern machine learning applications and the continuously changing nature of data processed by these applications necessitate efficient and effective index maintenance techniques for vector search indexes. Designed primarily for static workloads, existing vector search indexes degrade in search quality and performance as the underlying data is updated unless costly index reconstruction is performed. To address this, we introduce Ada-IVF, an incremental indexing methodology for Inverted File (IVF) indexes. Ada-IVF consists of 1) an adaptive maintenance policy that decides which index partitions are problematic for performance and should be repartitioned and 2) a local re-clustering mechanism that determines how to repartition them. Compared with state-of-the-art dynamic IVF index maintenance strategies, Ada-IVF achieves an average of 2×2\times2 × and up to 5×5\times5 × higher update throughput across a range of benchmark workloads.","Modern machine learning applications increasingly rely on high-dimensional vector embeddings to transform complex data such as images, text, or entities in knowledge graphs to vector representations that retain semantically meaningful information (Gordo et al., 2016; Cai et al., 2018; Hossain et al., 2019; Minaee et al., 2021; Chami et al., 2022). Vector similarity search is a critical component in such applications as it enables more accurate and contextualized search (Grbovic and Cheng, 2018; Haldar et al., 2019; Hashemi et al., 2021; Qin et al., 2021) and recommendations (Okura et al., 2017; Liu et al., 2017; Wang et al., 2018; Pal et al., 2020; Liu et al., 2022) over multi-modal data. Partitioned indexes for vector similarity search have recently gained widespread adoption in these modern machine learning applications due to their performance and scalability (Zhang et al., [n.d.]; Jiang et al., 2023; Mohoney et al., 2023; Guo et al., 2020; Xu et al., 2023; Wei et al., 2020). A common type of partitioned index is the Inverted File (IVF) index, where a vector quantization algorithm (typically k-means) is used to partition a vector dataset, and the resulting clusters constitute the partitions of the index (Jégou et al., 2011a). While existing IVF index implementations are designed for static workloads, i.e., the partitioning is performed based on the initial state of the underlying vector dataset, real-world deployments require high-throughput search and updates over dynamic vector data where the underlying vector dataset is continuously modified through insertions and deletions (Xu et al., 2023; Singh et al., 2021; Baranchuk et al., 2023). In addition to being robust against modifications to the vector dataset, the indexes must be robust against changing query patterns over time, as observed in (Baranchuk et al., 2023). In this work, we study the effect of updates on the IVF index’s search and update throughput and propose an incremental maintenance methodology for IVF indexes. IVF indexes out-of-the-box do not have the notion of inserting new vectors or deleting existing vectors once constructed. Indeed, the most common method used by practitioners today is to rebuild the index from scratch to reflect any updates that have accumulated over time. However, depending on the scale of the vector dataset and the volume and frequency of updates, a full index rebuild can be prohibitively expensive. For example, it takes multiple days to rebuild an IVF index from scratch for billion-scale vector datasets (Xu et al., 2023; Chen et al., [n.d.]), making it necessary to revisit how updates can be reflected. Devising such an update mechanism consists of re-adjusting the partitioning of the high-dimensional space defined by the clusters and ensuring that the re-adjusted partitioning: (i) keeps the reconstruction error, i.e., the average distance between a vector and its nearest cluster centroid, at minimum, otherwise, queries would require scanning more partitions to reach a target recall and degrades search throughput; (ii) does not create an imbalance in the size distribution of the partitions, which can result in variable search latency across queries (Baranchuk et al., 2023; Xu et al., 2023; Jégou et al., 2010). In industrial vector search workloads, we observe that the partition access patterns of search and update operations over an IVF index are non-uniform and change over time. For instance, in a typical day of a KG entity search workload we studied (Mohoney et al., 2023; Ilyas et al., 2022), we find that only 15%percent1515\%15 % of partitions were accessed during search operations, and 80%percent8080\%80 % of the updates affected partitions that were not accessed by any search operation. Such skewed access patterns induced by real-world vector search workloads present an opportunity for efficient and effective maintenance of IVF indexes over dynamic datasets. Specifically, index maintenance overhead can be minimized by devising a local, incremental indexing strategy and focusing the maintenance process on frequently accessed partitions during search. To the best of our knowledge, no existing approaches in the literature utilize partition access patterns for IVF index maintenance. To this end, we propose Ada-IVF, an incremental maintenance mechanism for IVF indexes. Our approach is based on (i) the observation that reconstruction error and partition imbalance serve as indicators of an IVF index’s search quality and performance (Section 2.2), and (ii) workload patterns can be utilized for local, incremental maintenance of its underlying partitions (Section 2.3). Ada-IVF consists of a workload-adaptive policy that identifies which partitions should be reindexed based on real-time statistics in order to minimize reconstruction error and partition imbalance and a mechanism that performs local reindexing over the target subset of partitions. Our policy uses global and partition-local indicator functions that quantify changes in reconstruction error and partition imbalance. In addition, Ada-IVF tracks read frequencies for individual partitions to quantify a temperature of each partition, which is then used by the local indicator function. Using temperature allows Ada-IVF to prioritize the maintenance of commonly accessed partitions and to avoid unnecessary work on rarely accessed partitions. Through its incremental maintenance policy and local reindexing mechanism, Ada-IVF effectively mitigates IVF index performance degradation due to updates. Our experimental analysis confirms the efficacy of our approach. We conduct a comparison of Ada-IVF against the state-of-the-art LIRE (Xu et al., 2023) and other baseline IVF maintenance methodologies on synthetic data and public benchmarks. We show that Ada-IVF obtains similar or better search throughput when compared to baseline methods with an average of 2×2\times2 × improvement in throughput across all workloads and a maximum of 5×5\times5 × update throughput improvement on synthetic data. Furthermore, we verify Ada-IVF’s robustness across various workload patterns using traces of industrial workloads and public and synthetic benchmarks, thereby establishing its applicability in diverse real-world scenarios. In summary, our contributions include: • Global and local indicator functions to quantify the impact of updates on global index and individual partition levels. • A workload-adaptive policy that uses real-time statistics and these indicator functions to monitor reconstruction error and partition imbalance and decide when to perform reindexing. • A local reindexing mechanism that uses k-means to split and reindex candidates and their neighboring partitions. • A comprehensive empirical evaluation demonstrating the impact of updates on IVF indexes under a diverse array of real and synthetic workloads. In Section 2, we discuss the preliminaries behind our study, detailing existing index solutions and systems as well as observations on real workloads. Section 3 provides an overview of the design of Ada-IVF. Section 4 details our contributions to IVF index quality maintenance. Section 5 presents our experimental results on internal industrial workloads and public and synthetic benchmarks, with a suite of microbenchmarks to validate our contributions. Section 6 concludes with a discussion of our findings and related work."
https://arxiv.org/html/2411.00879v1,DEREC-SIMPRO: unlock Language Model benefitsto advance Synthesis in Data Clean Room,"Data collaboration via Data Clean Room offers value but raises privacy concerns, which can be addressed through synthetic data and multi-table synthesizers. Common multi-table synthesizers fail to perform when subjects occur repeatedly in both tables. This is an urgent yet unresolved problem, since having both tables with repeating subjects is common. To improve performance in this scenario, we present the DEREC 3-step pre-processing pipeline to generalize adaptability of multi-table synthesizers. We also introduce the SIMPRO 3-aspect evaluation metrics, which leverage conditional distribution and large-scale simultaneous hypothesis testing to provide comprehensive feedback on synthetic data fidelity at both column and table levels. Results show that using DEREC improves fidelity, and multi-table synthesizers outperform single-table counterparts in collaboration settings. Together, the DEREC-SIMPRO pipeline offers a robust solution for generalizing data collaboration, promoting a more efficient, data-driven society.","Data collaboration promotes innovation by leveraging the collective expertise of various stakeholders, e.g., two initiatives in Nepal have developed a platform to gather resources from different parties to improve local health and population data systems [54]. Increasingly, enterprises are turning to Data Clean Rooms, which connect and synthesize datasets from multiple sources [37] [36]. Multi-table synthesizers address both data synthesis and integration needs, sparking interest in incorporating Language Model backbones for data generation, particularly following the success of ChatGPT [32]. In managing a generation-based Data Clean Room that involves synthesis and evaluation processes, a key challenge for each is identified. During the synthesis process, existing multi-table synthesizers require a strict one-to-many table relationship, and any deviation from this can result in underperformance. However, real-life datasets often feature subjects that occur repeatedly (Section 3.1), which existing synthesizers are not equipped to handle due to their inability to manage many-to-many relationships. This limitation negatively impacts synthesizer performance (Refer to Section 6.2). In the evaluation step, current metrics are inefficient in assessing synthetic data quality in multi-table contexts, e.g., the Synthetic Data Metrics (”SDMetrics”) [11] tends to favor ’smoothed’ results without accurately reflecting shape similarity. The first challenge involves effectively establishing connections between two tables with many-to-many relationships, as the potential combinations for each column can be infinite unless additional information from the datasets helps differentiate specific observations. The second challenge focuses on defining an indicator that accurately measures the similarity between features of the original and synthetic datasets. In summary, the contributions of this work include: • Introducing a pipeline (Section 4.1) to transform many-to-many collaborative data into one-to-many scenarios (Figure 5) for data synthesis • Proposing the usage of conditional distribution (Section 4.2) and similarity metrics (Section 4.2.2 and 4.2.3) to measure individual column synthetic data fidelity • Developing an iterative algorithm to manage synthetic table fidelity in entirety (Section 4.2.1) using ’distribution of distribution similarity’ (Figure 12)"
https://arxiv.org/html/2411.00788v1,KeyInst: Keyword Instruction for Improving SQL Formulation in Text-to-SQL,"Text-to-SQL parsing involves the translation of natural language queries (NLQs) into their corresponding SQL commands. A principal challenge within this domain is the formulation of SQL queries that are not only syntactically correct but also semantically aligned with the natural language input. However, the intrinsic disparity between the NLQ and the SQL poses a significant challenge. In this research, we introduce Keyword Instruction (KeyInst), a novel method designed to enhance SQL formulation by Large Language Models (LLMs). KeyInst essentially provides guidance on pivotal SQL keywords likely to be part of the final query, thus facilitates a smoother SQL query formulation process. We explore two strategies for integrating KeyInst into Text-to-SQL parsing: a pipeline strategy and a single-pass strategy. The former first generates KeyInst for question, which are then used to prompt LLMs. The latter employs a fine-tuned model to concurrently generate KeyInst and SQL in one step. We developed StrucQL, a benchmark specifically designed for the evaluation of SQL formulation. Extensive experiments on StrucQL and other benchmarks demonstrate that KeyInst significantly improves upon the existing Text-to-SQL prompting techniques.","The task of Text-to-SQL parsing, which aims at translating natural language questions into executable SQL queries, has gained increasing attention in recent years, as it can help non-expert users quickly access information in the database without the need for technical background Deng et al. (2021); Yu et al. (2020); Rajkumar et al. (2022); Ni et al. (2023). Text-to-SQL parsing faces two main challenges: schema linking and SQL formulation. Schema linking involves identifying the pertinent tables and columns in a database schema in response to an NLQ. SQL formulation refers to generating SQL queries that are not only syntactically correct but also semantically aligned with the natural language input. This paper primarily focuses on the challenge of SQL formulation. Currently, most Text-to-SQL prompting methods induce Large Language Models (LLMs) to generate the target SQL directly using In-context Learning (ICL) Nan et al. (2023); Pourreza and Rafiei (2024a); Tan et al. (2024). However, the vast difference between natural language queries (NLQ) and SQL hinders precise query formulation. In previous works, the skeleton-aware decoder Li et al. (2023) was proposed to alleviate this challenge by initially generating an SQL skeleton followed by the full query. An SQL skeleton is a basic framework of an SQL query consisting of SQL operators, without specific details such as column names, table names, or conditions. Incorporating SQL skeleton in prompting has also proven to be effective Gao et al. (2023); Guo et al. (2023). In this work, we also use the SQL structure as a central element in SQL formulation, with a particular emphasis on identifying key SQL operators. For instance, in translating the NLQ ""List the customers’ first and last names from the 10 least expensive invoices"", accurately identifying ORDER BY and LIMIT is crucial for formulating the correct SQL query. Figure 1: Graphical illustration of KeyInst and its applications: A. An example of schema, question, KeyInst, and SQL, B. The pipeline approach of KeyInst application, C. The single-pass approach of KeyInst application. We introduce Keyword Instruction (KeyInst), a novel method designed to enhance SQL formulation by LLMs. KeyInst essentially provides guidance on pivotal SQL keywords likely to be part of the final query. Recognizing that SQL queries corresponding to different NLQs require distinct keywords, KeyInst adapts dynamically to each query. An example of KeyInst in action is depicted in Figure 1A, demonstrating how it analyzes a given NLQ and deduces the critical SQL keywords. This strategy effectively narrows the gap between NLQ and SQL, facilitating a smoother SQL query formulation process. While KeyInst significantly aids in SQL query formulation, further exploration is needed on its generation and integration into Text-to-SQL parsing. We present two approaches for KeyInst generation: a model fine-tuning method and an ICL-based method. The former fine-tunes a model to produce KeyInsts for specific queries, while the latter prompts LLMs to generate KeyInsts through ICL Brown et al. (2020). For the application of KeyInst in Text-to-SQL tasks, we also investigate two strategies. The first strategy prompts LLMs to produce SQL queries with KeyInst. The second strategy is a fine-tuning strategy that generates SQL queries directly following KeyInst generation, treating KeyInst creation as a preliminary reasoning step. To amalgamate KeyInst generation and application within Text-to-SQL, we introduce a two-fold strategy. The pipeline approach initially generates KeyInst using either the fine-tuned or ICL-based method, followed by prompting LLMs with the generated KeyInst, as illustrated in Figure 1B. Conversely, the single-pass approach employs a fine-tuned model to concurrently generate KeyInst and SQL in one step, as depicted in Figure 1C. Several benchmarks, such as Spider Yu et al. (2018) and Bird Li et al. (2024b), have been developed to assess Text-to-SQL systems. However, these benchmarks focus on overall parsing performance and lack mechanisms for isolating evaluations of semantic linking and SQL formulation. To specifically assess SQL formulation capabilities, a new benchmark called StrucQL (Structural Benchmark for Text-to-SQL) has been developed, derived from Spider. In StrucQL, questions and schemas are simplified: questions explicitly mention schema items, and irrelevant tables and columns are omitted from the schema. This simplification makes schema linking straightforward, shifting the primary challenge to SQL formulation. Consequently, StrucQL serves as an effective tool for evaluating SQL formulation proficiency in Text-to-SQL systems. KeyInst was assessed on StrucQL and other benchmarks, with outcomes indicating that keyword instructions are a valuable intermediary for Text-to-SQL parsing, whether applied independently or in conjunction with other techniques. The main contributions of this work are summarized as follows: • We propose KeyInst, a keyword instruction tailored for each Text-to-SQL task, to alleviate SQL formulation challenge. We offer two approaches for integrating KeyInst into Text-to-SQL parsing: a pipeline strategy and a single-pass strategy. • The StrucQL benchmark was developed to specifically assess the SQL formulation abilities of Text-to-SQL systems. By simplifying questions and schemas, StrucQL eliminates schema linking challenges, focusing evaluation on SQL formulation performance. • Comprehensive experiments across various benchmarks were conducted. The findings demonstrate that KeyInst significantly improves upon the existing state-of-the-art Text-to-SQL prompting techniques, showcasing its effectiveness and potential."
https://arxiv.org/html/2411.02309v1,Grid-Based Projection of Spatial Data into Knowledge Graphs,"The Spatial Knowledge Graphs (SKG) are experiencing growing adoption as a means to model real-world entities, proving especially invaluable in domains like crisis management and urban planning. Considering that RDF specifications offer limited support for effectively managing spatial information, it’s common practice to include text-based serializations of geometrical features, such as polygons and lines, as string literals in knowledge graphs. Consequently, Spatial Knowledge Graphs (SKGs) often rely on geo-enabled RDF Stores capable of parsing, interpreting, and indexing such serializations. In this paper, we leverage grid cells as the foundational element of SKGs and demonstrate how efficiently the spatial characteristics of real-world entities and their attributes can be encoded within knowledge graphs. Furthermore, we introduce a novel methodology for representing street networks in knowledge graphs, diverging from the conventional practice of individually capturing each street segment. Instead, our approach is based on tessellating the street network using grid cells and creating a simplified representation that could be utilized for various routing and navigation tasks, solely relying on RDF specifications.","Integrating geospatial data into knowledge graphs enriches the representation of real-world entities and fosters deeper insights into their spatial relationships. The resulting knowledge graphs, known as Spatial Knowledge Graph (SKG), are particularly useful in domains such as crisis management and urban planning, facilitating tasks such as proximity searches, resource planning, or spatial reasoning. Furthermore, SKGs enhance the understanding of relationships between entities, leveraging their geographic proximity or spatial interactions. The process of constructing SKGs from isolated spatial entities, such as landmarks, regions, points of interest, and observations, typically entails integrating their geometry serializations (i.e., as points or polygons) as well as their spatial relationships with other entities. While the Resource Description Framework (RDF) is currently used to capture, represent, and exchange the ever-growing volume of spatial and spatiotemporal data across diverse domains, RDF specifications offer limited support for managing spatial information effectively. Despite numerous efforts to represent and manage spatial data within RDF structures, as highlighted in [17], these endeavors often resort to encoding geometrical features in text-based formats like Well-Known Text (WKT) or Geography Markup Language (GML). Consequently, the reliance on third-party implementations for spatial operations (e.g., topological relations such as neighborhood, incidence, and overlapping) and data processing has resulted in a lack of widely adopted and interoperable SKG solutions. Beyond incorporating basic spatial features with geographic or spatial representations, typically depicted as points or polygons, integrating complex spatial networks like street networks into knowledge graphs is vital for specific applications, such as crisis management. However, despite the structural similarity between street network layouts and knowledge graph schema, encoding a projection of street networks that enables effective execution of network algorithms is not straightforward. In this paper, we adopt a grid-based methodology to construct SKGs using basic RDF specifications tailored to meet the diverse demands of spatial applications across various scenarios and use cases. To this end, we leverage grid cells as the foundational element of SKG and demonstrate how efficiently the spatial characteristics of real-world entities and their attributes can be encoded within knowledge graphs. By following the Semantic Sensor Network (SSN) [9] and SOSA (Sensor, Observation, Sample, and Actuator) [5] ontologies, this approach aligns with best practices for capturing measurements and observations at a fine-grained spatial and temporal dimensions. Another contribution of this paper is the introduction of a novel methodology for representing street networks in knowledge graphs, diverging from the conventional practice of individually capturing each street segment. Instead, our approach is based on tessellating the street network using grid cells and creating a simplified representation that could be utilized for various routing and navigation tasks."
https://arxiv.org/html/2411.02059v3,TableGPT2: A Large Multimodal Modelwith Tabular Data Integration,"The emergence of models like GPTs, Claude, LLaMA, and Qwen has reshaped AI applications, presenting vast new opportunities across industries. Yet, the integration of tabular data remains notably underdeveloped, despite its foundational role in numerous real-world domains.This gap is critical for three main reasons. First, database or data warehouse data integration is essential for advanced applications; second, the vast and largely untapped resource of tabular data offers immense potential for analysis; and third, the business intelligence domain specifically demands adaptable, precise solutions that many current LLMs may struggle to provide.In response, we introduce TableGPT2, a model rigorously pre-trained and fine-tuned with over 593.8K tables and 2.36M high quality query-table-output tuples, a scale of table-related data unprecedented in prior research. This extensive training enables TableGPT2 to excel in table-centric tasks while maintaining strong general language and coding abilities.One of TableGPT2’s key innovations is its novel table encoder, specifically designed to capture schema-level and cell-level information. This encoder strengthens the model’s ability to handle ambiguous queries, missing column names, and irregular tables commonly encountered in real-world applications. Similar to the VLMs, this pioneering approach integrates with the decoder to form a robust large multimodal model.We believe the results are compelling: over 23 benchmarking metrics, TableGPT2 achieves an average performance improvement of 35.20% in the 7B model and 49.32% in the 72B model over prior benchmark-neutral111 We focus exclusively on open-sourced benchmark-neutral LLMs, like Qwen [1], LLaMA [2], and DeepSeek [3], to ensure a fair, versatile comparison without overfitting to specific benchmarks. Specialized models like CHASE-SQL [4] that are designed for specific tasks are generally excluded from our evaluation. LLMs, with robust general-purpose capabilities intact.We release an open-source repository (Section 2) that includes both the model and a comprehensive agent workflow, along with a subset of RealTabBench. This release aims to foster further exploration and application in real-world data-driven and BI production environments.","The emergence of large language models (LLMs) has driven remarkable progress in artificial intelligence (AI), reshaping its applications across various domains. Models like ChatGPT [5] have enhanced the machine’s ability to comprehend and produce human-like language, opening new possibilities in diverse fields. 1.1 Motivation behind TableGPT2 A system perspective. No system works in a vacuum. The key motivation for proposing TableGPT2 is the need to address the limitations of current large language models (LLMs) in real-world, data-driven applications. No system operates in a vacuum, yet many LLMs are designed to function in an end-to-end fashion without external data integration. This approach is inherently flawed. For example, relying on an LLM to make stock investment recommendations without access to real-time market data is impractical. Similarly, an LLM making healthcare diagnoses without direct access to patient records or analyzing financial statements without the full dataset would lack the necessary depth and accuracy. Even when LLMs are integrated with external data sources, such as databases, their performance is often suboptimal. One approach involves using tools like natural-language-to-sql (NL2SQL) [6], which provide schema metadata, table descriptions, and sample values to bridge the gap. However, as mentioned earlier, this method falls short in complex scenarios. Additionally, thanks to innovations like longer context windows or novel architectures, some models attempt to handle large datasets within a single context. This, too, is inefficient, as digesting detailed information from each cell of a large database or Excel file exceeds the capacity and utility of the extended context window, which was primarily designed for textual data. Thus, there is a pressing need for new techniques that move beyond the vacuum operation paradigm. This is the motivation behind the development of TableGPT2—a model designed to integrate and process tabular data directly and efficiently, overcoming the inherent limitations of current LLMs, especially towards production-level deployment. A data perspective. A key factor in the success of large language models like GPT-4 [7] has been the availability of vast datasets sourced from the open web. GPT-4, for instance, was trained on hundreds of billions of tokens, drawn from a wide range of publicly available text data. This abundance of text allowed for effective scaling, enabling the model to capture complex linguistic patterns and perform well across various tasks. Similarly, vision-language models (VLMs) have thrived by leveraging large-scale datasets that pair images with descriptive text [8]. These models, such as CLIP [9] and DALL·E [10], succeed by aligning visual data with language, creating robust representations that allow them to excel in multimodal tasks. The richness and scale of both image and text data have been crucial to this success, demonstrating the power of large, diverse datasets. Despite the significant focus on text and visual data, tabular data is equally abundant and critical across industries. It’s estimated that over 70% of global data is in structured tabular form, whether stored in databases or spreadsheets. This vast resource points to the potential for developing a large-scale tabular model capable of harnessing the same scaling laws applied to text and images. By leveraging massive datasets of tabular data, along with schema metadata, we aim to explore whether these data formats can be modeled effectively, leading to functional and powerful models for business intelligence and other applications. This approach aligns directly with our novel encoder design, which focuses on modeling the structure and content of tabular data. The tabular data encoder within TableGPT2 enables it to capture schema-level and cell-level information, opening the door for large-scale, data-driven performance improvements, similar to those observed in text and vision-based models. A BI perspective. Business intelligence (BI), in particular, stands out as an area primed for innovation based upon LLMs. Traditional BI systems typically depend on fixed queries, static data structures, and inflexible interaction methods, which restrict their adaptability to dynamic business needs. LLMs offer a pathway to overcome these limitations by supporting more natural query interactions and accommodating a wide range of user intents. Despite their promise however, LLMs still face significant challenges in being effectively integrated into BI applications. Issues such as computational efficiency [11], the inability to fully comprehend tabular data [12], and a lack of alignment with complex BI schema and user demands remain key hurdles. These limitations have prevented LLMs from delivering on their full potential in BI contexts, where precision, performance, and ease of use are paramount. Diving further, closest to our work, popular approaches such as NL2SQL [13, 6, 14] and common table comprehension [15, 16, 17] have proven insufficient for modern BI systems due to several limitations. Table comprehension tasks often focus on small, clean datasets like those from Wikipedia, which fail to represent the complexity of real-world scenarios where tables are larger and more intricate. Similarly, NL2SQL models and benchmarks rely on well-structured queries and clean schema, but in actual BI environments, “dirty” schemas, missing values, and complex relational structures render these models ineffective. Furthermore, both approaches struggle to handle the diverse user intents, complex business logic, and multi-step reasoning required in real-world applications, highlighting their inability to scale to the demands of modern BI tasks. 1.2 Upon the Previous Version of TableGPT In the initial version of TableGPT [18], we introduced approaches such as structured Domain-Specific Languages (DSLs) and specialized table encoder to manage complex table-based queries. These techniques showed promise in structured data handling but also highlighted areas for further refinement. Our latest iteration, TableGPT2, advances these foundations with substantial improvements. We have scaled up data and training protocols, carefully redesigned each component, and introduced additional technical enhancements to improve robustness, broaden applicability, and optimize performance in diverse BI tasks. To provide context, TableGPT2 is a large-scale multimodal model available in two configurations—a 7-billion-parameter model and a 72-billion-parameter model—both based on the Qwen2.5 model family. Training encompassed over 86 billion tokens for continual pretraining (CPT), more than 437.5K table-language interleaved samples for encoder training and over 2.36M high quality query-table-output tuples being utilized for supervised fine-tuning. This scale of data is unprecedented in related research, ensuring that TableGPT2 meets the rigorous demands of modern applications involving structured or tabular data. 1.3 General Introduction of TableGPT2 TableGPT2’s language framework is built on the Qwen2.5 [1] architecture. It underwent continual pretraining (CPT), supervised fine-tuning (SFT), and an agent framework for production-level ability. These steps differ from traditional LLMs as our pretraining and fine-tuning place a stronger emphasis on coding, multi-turn reasoning, and tool usage. These characteristics ensure the model is not only adept at natural language processing but also highly capable of handling the intricate requirements of table-related tasks. In addition, we preliminarily explored the possibilities of multimodal alignment for tabular data. Specifically, TableGPT2 innovatively incorporates a separate modality module specifically for reading and interpreting tabular data. Similar to vision-language models (VLMs), TableGPT2 involve a tabular data reading module that generates specialized embeddings concatenated with the token embeddings from textual input. This additional module allows TableGPT2 to better capture the structure and semantics of tabular data, enabling more accurate tabular comprehension in complex BI scenarios. An overall model framework is depicted in Figure 1. Figure 1: Overall framework of TableGPT2. Continual pretraining. For the goals of TableGPT2, we first focused on enhancing the model’s coding and reasoning abilities through Continual Pretraining (CPT) [19]. Specifically, 80% of the CPT data was dedicated to well-commented code, aligning with approaches of DeepSeek-v2 [20], ensuring robust coding capabilities. To complement this, we also curated substantial reasoning data and general textbooks containing domain-specific knowledge (e.g., finance, manufacturing, biotechnology, market-tech) to maintain a balanced data ratio for reasoning enhancement. In terms of data processing, we employed a two-level filtering strategy. At the document level, we tagged the data using 54 distinct categories to ensure comprehensive coverage of different documentation types. Meanwhile, at the token level, we utilized the RHO-1 [21] technique to fine-tune the selection of high-quality tokens. Additionally, we introduced a novel approach to account for code length and context window settings [22], optimizing the model’s ability to handle various code segments effectively. The final CPT data comprised 86B tokens, after thorough filtering, and this robust pretraining ensures that TableGPT2 is equipped with the necessary coding and reasoning capabilities tailored for complex BI and other related tasks. Supervised fine-tuning. The purpose of Supervised Fine-Tuning (SFT) in TableGPT2 was to address the model’s limitations in adapting to BI-specific tasks and scenarios, where it lacked specialization. To improve its performance, we curated a dataset covering a wide range of critical and realistic scenarios, including multi-turn conversations, complex reasoning, tool usage, and highly business-specific queries. The dataset was constructed using a combination of manual annotations and an expert-driven automatic labeling pipeline, ensuring the quality and relevance of the data. In total, the SFT process involved 2.36M samples, with a token count in the range of several billion, further refining the model to meet the specific demands of BI and other environments that involve tables. More specifically, a key differentiator in TableGPT2’s SFT process is the balanced and diverse composition of these 2.36M instructional samples. Given that table-related tasks require both general model capabilities and specialized table-specific skills, we ensured a well-organized mix of data. This dataset includes table-specific tasks such as code generation (Python and SQL), table querying, data visualization, statistical testing, and predictive modeling. Additionally, it spans a wide variety of tasks like table comprehension, table generation, missing value imputation, and table-based question-answering, covering almost all stages of table usage. The input formats combined with random arrangements of table metadata like field descriptions, schema information, and value enumeration, producing over 20 different combinations of table-info inputs to ensure comprehensive coverage. To ensure the high quality of the data, we implemented a multi-step data filtering pipeline. First, a set of rule-based filters was applied, including checks for code executability and correctness using Python and SQL executors, which removed common errors such as key errors and type conversion issues. Regular expressions and additional rules were also used to discard anomalous outputs, such as outputs that improperly redefined the table or returned comments without executable code. The filtered data was then subject to scoring, using multiple models (e.g., GPT-4o [23]) and tailored prompts for more granular evaluation. Only samples that exceeded a set threshold across all scoring combinations were retained. Following this, sample calibration was performed through manual checks. If the sample accuracy fell below 95%, the data generation and filtering scripts were reviewed and optimized. Lastly, evaluation was conducted using a fixed validation set of approximately 94.9K cases (including existing and our constructed cases), ensuring the generated results were executable and accurate, with further manual verification to spot-check any inconsistencies and detect potential data issues, such as missing function calls or multi-turn dialogue capability gaps. Data augmentation for tabular data. To enhance TableGPT2’s performance, especially in handling complex BI-related tasks, we implemented several query augmentation techniques. By introducing fuzziness in how fields were referenced within queries, we minimized direct mappings between query terms and table names or fields. This approach helped reduce post-training key errors in the model. Additionally, we applied table data augmentation by anonymizing field names and category values, which improved the model’s robustness when dealing with noisy or incomplete datasets, making it more resilient to “dirty” table data often encountered in real-world applications. Furthermore, we reinforced the model’s versatility by incorporating single-turn and multi-turn QA tasks, using varied prompt formats and output structures to reduce TableGPT2’s sensitivity to specific prompt templates. To further diversify the training data, we applied post-processing enhancements during data generation. This involved randomly adding details like visualization instructions or summary explanations, ensuring that the generated data samples were more varied and capable of addressing a wide range of real-world scenarios. Semantic table encoder. The design of the semantic encoder in TableGPT2 is motivated by the limitations of conventional workflows in tasks like NL2SQL, which typically rely on predefined schemas and a few (or completely no) example cell values. While schemas may provide structural information, they often lack the depth and specificity required to capture the true semantics of a table. These methods often fall short when broader, more comprehensive information from the table’s content is required to perform accurate coding or analysis. In many real-world cases, the context needed for proper reasoning spans a much larger portion of the table, making it impractical to rely solely on individual cell values or small sections of data. Current LLMs also struggle in this regard. While context window sizes have increased, allowing for more information to be processed at once, the performance of these models declines significantly when faced with too many cell values or database-level data. The sheer volume of information exceeds the capacity of the model’s attention mechanisms, leading to a loss of context and accuracy in generating code or performing analysis. Moreover, tables possess unique properties such as bi-dimensional structure, redundancy, and sparsity, which creates a significant gap between understanding table data and ordinary text. To address this, we designed a tabular encoder that takes the entire table as input, producing a set of compact embeddings for each column. This architecture is tailored to the unique properties of tabular data, which differ fundamentally from text, images, or other data types. Table semantics arise from four key dimensions: cells, rows, columns, and the entire table structure, all of which exhibit permutation invariance. To accommodate this, we implemented a bi-dimensional attention mechanism without positional embeddings, alongside a hierarchical feature extraction process. This ensures that both row-wise and column-wise relationships are captured and effectively understood. Additionally, a column-wise contrastive learning approach is employed, encouraging the model to learn meaningful, structure-aware semantic representations of the table. Column embeddings are aligned with textual embeddings through a Q-former-style adapter [24], which is equipped with a set of learnable queries. We also introduce two special tokens, ""<tab>"" and ""</tab>"", to differentiate tabular features from the native text, allowing the model to process both modalities concurrently and without confusion. To further enhance the alignment between the textual information, column embeddings, and schema metadata, we applied joint instruction tuning. This process helps refine the model’s understanding of the tabular data, enabling it to more effectively integrate and interpret the various inputs. Agent framework. In our open-source repository, we provide a comprehensive agent workflow runtime framework designed to seamlessly integrate TableGPT2 with enterprise-level data analysis tools. This framework consists of three core components: runtime prompt engineering, a secure code sandbox, and an agent evaluation module, which together enhance the agent’s capabilities and reliability. The workflow supports complex data analysis tasks through modular steps—input normalization, agent execution with optional VLM support, and tool calls—that work in tandem to manage and monitor the agent’s performance. By incorporating retrieval-augmented generation (RAG) [25] for efficient context retrieval and a code sandbox for secure execution, this framework ensures TableGPT2 delivers accurate, contextually relevant insights in real-world problems. Evaluation. We conducted extensive evaluations across various widely-used tabular and general benchmarks, demonstrating TableGPT2’s exceptional capabilities in table comprehension, processing, and reasoning, with an average improvement of 35.20% for the 7B model and 49.32% for the 72B model while retaining robust general-purpose performance. Noted, for a fair assessment, we compare TableGPT2 only with open-sourced benchmark-neutral models such as Qwen [1] and DeepSeek [3], ensuring balanced, versatile performance across tasks without overfitting to any single benchmark. We also introduce and partially release a new benchmark, RealTabBench, which emphasizes unconventional tables, anonymized fields, and complex queries, aligning more closely with realistic scenarios."
https://arxiv.org/html/2411.01685v1,Mitigating Matching Biases Through Score Calibration,"Record matching, the task of identifying records that correspond to the same real-world entities across databases, is critical for data integration in domains like healthcare, finance, and e-commerce. While traditional record matching models focus on optimizing accuracy, fairness issues, such as demographic disparities in model performance, have attracted increasing attention. Biased outcomes in record matching can result in unequal error rates across demographic groups, raising ethical and legal concerns. Existing research primarily addresses fairness at specific decision thresholds, using bias metrics like Demographic Parity (DP), Equal Opportunity (EO), and Equalized Odds (EOD) differences. However, threshold-specific metrics may overlook cumulative biases across varying thresholds. In this paper, we adapt fairness metrics traditionally applied in regression models to evaluate cumulative bias across all thresholds in record matching. We propose a novel post-processing calibration method, leveraging optimal transport theory and Wasserstein barycenters, to balance matching scores across demographic groups. This approach treats any matching model as a black box, making it applicable to a wide range of models without access to their training data. Our experiments demonstrate the effectiveness of the calibration method in reducing demographic parity difference in matching scores. To address limitations in reducing EOD and EO differences, we introduce a conditional calibration method, which empirically achieves fairness across widely used benchmarks and state-of-the-art matching methods. This work provides a comprehensive framework for fairness-aware record matching, setting the foundation for more equitable data integration processes.","Record matching, also known as entity matching, record linkage, or entity linkage, is the task of determining whether two or more records from one or multiple databases correspond to the same real-world entities. This process is crucial in a variety of domains and facilitates the integration of fragmented data from multiple sources, enhancing data completeness and improving overall data quality. Typically, record matching is formulated as a binary classification problem, where a matcher evaluates pairs of records across one or more datasets and assigns each pair a label of either “match” or “non-match.” A persistent challenge for matchers lies in the occurrence of false positives and false negatives, which often arise due to data heterogeneity, data quality issues, or other underlying data problems. To address this, a common approach is to generate a matching score and then apply a matching threshold to find a balance between false positives and false negatives. This reframes record matching from a strict binary classification to a process of generating matching scores that can effectively distinguish matched pairs from those that should not match. Generally, a high score provides stronger support for a match, while a low score indicates that the records do not correspond. These scores are often normalized to values within [0,1], reflecting either a risk level or probability of matching, depending on the application. In recent years, fairness in machine learning has received considerable attention due to its significant impact on real-world applications (Zafar et al., 2017; Hardt et al., 2016; Dwork et al., 2012). Fairness is also an essential consideration in record matching, as biased outcomes may emerge, where matchers show higher accuracy for certain demographic groups over others. Several recent studies have investigated fairness issues in record matching (Shahbazi et al., 2024; Moslemi et al., 2024; Efthymiou et al., 2021; Nilforoushan et al., 2022; Shahbazi et al., 2023; Moslemi and Milani, 2024). However, some aspects of fairness in record matching remain unexplored, including fairness in methods that produce matching scores. In record matching, the potential bias introduced by false positives and false negatives has been identified as a significant concern (Shahbazi et al., 2023; Nilforoushan et al., 2022; Efthymiou et al., 2021; Louis et al., 2023; Makri et al., 2022). As mentioned earlier, most matchers generate a score and apply a threshold to assign labels. However, variations in thresholds can introduce different levels of bias, a factor that has received limited attention in previous record matching research. For example, studies such as (Efthymiou et al., 2021; Louis et al., 2023; Makri et al., 2022; Shahbazi et al., 2023) typically focus on specific thresholds and use traditional fairness definitions like Statistical or Demographic Parity (DP), Equal Opportunity (EO), and Equalized Odds (EOD) differences (Hardt et al., 2016; Dwork et al., 2012). These fairness metrics, however, are threshold-specific, which limits their broader applicability. (a) (b) Figure 1. Figure 1(a) highlights the significant variation in TPR across different thresholds. The ROC of HierMatch (Fu et al., 2021) (Figure 1(b)) shows that the AUC is nearly the same for both groups, with 93.33% for the minority group and 93.94% for the majority group. However, there is a noticeable difference in performance at specific thresholds. Example 1.1. Adjusting matching thresholds in record matching is essential for optimizing performance and can have a variety of practical implications across different domains. For instance, in financial institutions, lowering the threshold can increase the likelihood of identifying matches, though it also raises the risk of false positives. On the other hand, raising the threshold reduces false positives and is often preferred to avoid mistakenly merging accounts, a practice that helps protect privacy and mitigates financial risks. Similarly, in healthcare applications, higher thresholds prevent the merging of incorrect patient records, which could otherwise lead to treatment errors or misdiagnoses. Beyond its effects on false positives and false negatives, adjusting the matching threshold significantly impacts fairness. A method that appears fair at one threshold may exhibit substantial bias toward a particular group at another, as threshold adjustments can introduce or exacerbate disparities. For example, the ROC curves in Figure 1(b) show the true positive rate (TPR) and false positive rate (FPR) for minority and majority groups across different thresholds, using the HierMatch (Fu et al., 2021) method—a widely adopted approach on the Amazon-Google dataset (Mudgal et al., 2018), a popular benchmark in record matching. Additionally, Figure 1(a) illustrates how TPR varies between these groups across thresholds. These figures demonstrate that, while the method may perform similarly for both groups at certain thresholds (as seen in overlapping curves), biases become evident at other thresholds, indicated by the separation of the curves. The work by (Nilforoushan et al., 2022) introduced an AUC-based metric for exploring fairness in record matching, similar to those used in binary classification bias studies (Kallus and Zhou, 2019; Yang et al., 2023; Vogel et al., 2021). These AUC-based metrics measure bias for matching scores by comparing AUC values across different demographic groups. However, such metrics can be misleading (Kwegyir-Aggrey et al., 2023) because AUC is an aggregate measure that fails to capture threshold-specific biases. As shown in Figure 1(b), even when the AUC values for minority and majority groups are nearly identical, the ROC curves can differ significantly at certain thresholds, revealing hidden biases. To address the limitations of prior research on fairness in record matching, we extend traditional fairness metrics, such as DP, EOD, and EO differences, to account for biases in matching scores across all thresholds rather than focusing on a specific one. These metrics have been previously applied in the context of fairness in regression models (Komiyama et al., 2018; Bird et al., 2020; Caton and Haas, 2024; Binns, 2020; Zafar et al., 2017). We adapt them here to capture cumulative bias by summing the performance metric gap over the entire threshold range. Figure 1(a) illustrates the concept of DP difference for a score function. Using this expanded metric, we analyze potential biases in state-of-the-art matching methods that generate matching scores. Based on our analysis, we propose an algorithm to mitigate these matching biases. Bias mitigation generally follows one of three approaches: pre-processing, in-processing, and post-processing (Zemel et al., 2013; Pirhadi et al., 2024; Zhang et al., 2018; Hardt et al., 2016; Moslemi and Milani, 2024). We adopt a post-processing strategy by calibrating matching scores, treating any given matching method for score generation as a black box. This approach has the distinct advantage of being compatible with any matching method, unlike in-processing methods that require modification of the underlying model. Our score calibration technique does not require access to the model’s training data or any labeled data; instead, it operates on random unlabeled record pairs that represent the overall distribution of record pairs. This is particularly beneficial compared to pre-processing methods, as it enables the calibration of legacy models trained on inaccessible datasets. Our calibration method leverages optimal transport theory (Gordaliza et al., 2019; Janati et al., 2020), treating the distribution of matching scores per demographic group as a probability distribution. By using the Wasserstein barycenter (Chzhen et al., 2020; Miroshnikov et al., 2022; Avraham, 2023) as an averaging method for two or more distributions, we merge these distributions into a single, balanced distribution. We empirically demonstrate the effectiveness of this calibration method in reducing DP difference in matching scores, showing consistent performance across widely-used benchmarks and state-of-the-art matching methods. However, this method does not address other types of score biases, such as those required for EOD or EO differences in matching scores. To tackle this, we propose a second calibration algorithm, termed conditional calibration, and empirically demonstrate its effectiveness in reducing EOD and EO differences in matching score functions. This paper is structured as follows. Section 2 provides the necessary background, including an overview of record matching, the notation used in this work, and existing fairness metrics. Section 3 introduces our proposed fairness metric for matching scores and formulates the FairScore problem. In Sections 4 and 5, we present our two calibration algorithms, and in Section 6, we conduct an experimental analysis and discuss the results. Section 7 reviews the literature on state-of-the-art record matching methods, fairness considerations in record matching, and current bias mitigation techniques. Finally, Section 8 concludes our findings and outlines directions for future research."
https://arxiv.org/html/2411.01424v1,Effective Community Detection Over Streaming Bipartite Networks (Technical Report),"The streaming bipartite graph is extensively used to model the dynamic relationship between two types of entities in many real-world applications, such as movie recommendations, location-based services, and online shopping. Since it contains abundant information, discovering the dense subgraph with high structural cohesiveness (i.e., community detection) in the bipartite streaming graph is becoming a valuable problem. Inspired by this, in this paper, we study the structure of community on the butterfly motif in the bipartite graph. We propose a novel problem, named Community Detection over Streaming Bipartite Network (CD-SBN), which aims to retrieve qualified communities with user-specific query keywords and high structural cohesiveness at snapshot and continuous scenarios. In particular, we formulate the user relationship score in the weighted bipartite network via the butterfly pattern and define a novel (k,r,σ)𝑘𝑟𝜎(k,r,\sigma)( italic_k , italic_r , italic_σ )-bitruss as the community structure. To efficiently tackle the CD-SBN problem, we design effective pruning strategies to rule out false alarms of (k,r,σ)𝑘𝑟𝜎(k,r,\sigma)( italic_k , italic_r , italic_σ )-bitruss and propose a hierarchical synopsis to facilitate the CD-SBN processing. Due to the dynamic of streaming bipartite networks, we devise an efficient procedure for incremental graph maintenance. We develop an efficient algorithm to answer the snapshot and continuous CD-SBN query by traversing the synopsis and applying the pruning strategies. With extensive experiments, we demonstrate the efficiency and effectiveness of our proposed CD-SBN processing approach over real/synthetic streaming bipartite networks.","Nowadays, the management of bipartite graphs that involve two distinct types of nodes (e.g., customers and products, visitors and check-in locations, users and clicked websites) has been extensively studied in many real applications such as Online Recommendation from user-product transaction graphs (Fan et al., 2022; Weng et al., 2022; Kou et al., 2023), location-based services with visitor-location check-in graphs (Hang et al., 2018; Zhou et al., 2021a; Wang et al., 2017; Kim et al., 2022), and behavior analysis in user-Webpage click graphs (Sun et al., 2009; Neria et al., 2017). The community detection (CD) over such bipartite graphs has recently become increasingly important for bipartite graph analysis and mining (Barber, 2007; Bouguessa and Nouri, 2021; Yen and Larremore, 2020; Zhou and Amini, 2019), which uncovers hidden and structurally cohesive bipartite subgraphs (or groups). The CD problem over bipartite graphs has many useful real applications, such as online product recommendation and marketing, user behavior analysis, malicious user group identification, and so on. Although many prior works (Wang et al., 2023; Abidi et al., 2023a; Wang et al., 2021; Zhang et al., 2024) studied the CD problem in static bipartite graphs, real-world bipartite graphs are usually dynamically changing over time (e.g., updates of purchase transactions, check-ins, or website browsing). Therefore, in this paper, we will consider the Community Detection over Streaming Bipartite Network (CD-SBN), upon graph updates (e.g., edge insertions/deletions or edge weight changes). Below, we give a motivating example of our CD-SBN problem in the application of detecting malicious user groups (communities) over a user-website click bipartite graph. Example 0. (Anomaly Detection for the Cybersecurity) Figure 1(a) illustrates an example of a clickstream bipartite network Gt−1subscript𝐺𝑡1G_{t-1}italic_G start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT at timestamp (t−1)𝑡1(t-1)( italic_t - 1 ), which contains 3 user vertices, u1∼u3similar-tosubscript𝑢1subscript𝑢3u_{1}\sim u_{3}italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ∼ italic_u start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT, 2 website vertices, v1subscript𝑣1v_{1}italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and v2subscript𝑣2v_{2}italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, and 5 edges (ui,va)subscript𝑢𝑖subscript𝑣𝑎(u_{i},v_{a})( italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ) between two types of nodes, user uisubscript𝑢𝑖u_{i}italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and website vasubscript𝑣𝑎v_{a}italic_v start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT (for 1≤i≤31𝑖31\leq i\leq 31 ≤ italic_i ≤ 3 and 1≤a≤21𝑎21\leq a\leq 21 ≤ italic_a ≤ 2). Here, each website vertex vasubscript𝑣𝑎v_{a}italic_v start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT (for a=1,2𝑎12a=1,2italic_a = 1 , 2) has a set of keywords that represent the website’s features (e.g., {bank, finance} for a bank website v1subscript𝑣1v_{1}italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT). Moreover, each edge (ui,va)subscript𝑢𝑖subscript𝑣𝑎(u_{i},v_{a})( italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ) (for i=1∼3𝑖1similar-to3i=1\sim 3italic_i = 1 ∼ 3 and a=1,2𝑎12a=1,2italic_a = 1 , 2) is associated with an integer weight, indicating the frequency that user uisubscript𝑢𝑖u_{i}italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT accessed website vasubscript𝑣𝑎v_{a}italic_v start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT for the past five minutes. As shown in Figure 1(a), to detect malicious groups of users accessing sensitive/suspicious websites, the network security officer may want to specify some query keywords of websites (e.g., “bank” and “malware”), and identify some group (community), g1subscript𝑔1g_{1}italic_g start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, of users (e.g., u2subscript𝑢2u_{2}italic_u start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT and u3subscript𝑢3u_{3}italic_u start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT) who frequently visit some common websites with query keywords (e.g., bank website v1subscript𝑣1v_{1}italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and hacking website v2subscript𝑣2v_{2}italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT). The officer will warn those suspicious users in the resulting community g1subscript𝑔1g_{1}italic_g start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and/or take immediate actions (e.g., recording the evidence for the police). Figure 1(b) shows the streaming bipartite network Gtsubscript𝐺𝑡G_{t}italic_G start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT at timestamp t𝑡titalic_t, with a newly inserted edge (u1,v1)subscript𝑢1subscript𝑣1(u_{1},v_{1})( italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) and an expired edge (u3,v2)subscript𝑢3subscript𝑣2(u_{3},v_{2})( italic_u start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) (i.e., user u3subscript𝑢3u_{3}italic_u start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT has not visited website v2subscript𝑣2v_{2}italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT for the past five minutes). In this case, at timestamp t𝑡titalic_t, the community in Gtsubscript𝐺𝑡G_{t}italic_G start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is changed to g2subscript𝑔2g_{2}italic_g start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT (rather than g1subscript𝑔1g_{1}italic_g start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT at timestamp (t−1)𝑡1(t-1)( italic_t - 1 )), as circled by the orange dashed line. ■■\blacksquare■ In addition to the example above, the CD-SBN problem has many other real applications. For example, in the customer-product transaction bipartite graph, we can use the CD-SBN results to find communities of customers who recently have a purchasing behaviors for online advertising and marketing. Similarly, in the visitor-location check-in bipartite graph (e.g., from Yelp (Inc., 2024)), we can identify a group of visitors who frequently check in at some common points of interest (POIs) and provide them with group buying coupons/discounts (e.g., Groupon (Groupon, 2024)). In this paper, we formulate and tackle the CD-SBN problem, which obtains all bipartite communities with the user-specified query keywords (e.g., POIs’ features) and high structural cohesiveness (e.g., small distance between any two users and high structural score of the community). In particular, we formally define the community semantics in the context of bipartite graphs, that is, a keyword-aware and structurally dense bipartite subgraph (i.e., a so-called (k,r,σ)𝑘𝑟𝜎(k,r,\sigma)( italic_k , italic_r , italic_σ )-bitruss that contains query keywords, as will be described in Section 2.4). We consider both snapshot and continuous scenarios of our CD-SBN problem, which detect communities over a snapshot of streaming bipartite network Gtsubscript𝐺𝑡G_{t}italic_G start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT at timestamp t𝑡titalic_t, or continuously monitor the CD-SBN answer set for each registered community constraint upon streaming graph updates, respectively. Due to the large scale of bipartite graphs and rapid streaming graph changes, efficient processing of the snapshot and continuous CD-SBN is rather challenging. In order to tackle the challenges, in this paper, we propose effective pruning strategies with respect to the community constraints (e.g., query keywords, community radius, edge supports, and community scores) to significantly reduce the CD-SBN problem space. We design a hierarchical synopsis to effectively facilitate the candidate community search, and develop efficient snapshot and continuous algorithms to retrieve or incrementally maintain actual community answers (via our proposed synopsis and pruning methods), respectively. In this paper, we make the following major contributions. (1) We formally define the problem of the community detection over streaming bipartite network (CD-SBN) containing snapshot and continuous queries in Section 2. (2) We present a framework to efficiently process the two types of queries with incremental updates of the bipartite streaming graph in Section 3. (3) We design effective community-level pruning strategies to reduce the search space of the CD-SBN problem in Section 4. (4) We propose a novel hierarchical synopsis to facilitate CD-SBN query processing and devise an efficient procedure for incremental graph maintenance in Section 5. (5) We develop an efficient algorithm with effective synopsis-level pruning strategies to answer the snapshot CD-SBN query and another algorithm to maintain the result set of a continuous CD-SBN query with low computational cost in Section 6. (6) We demonstrate the efficiency and effectiveness of our CD-SBN processing approach through extensive experiments over synthetic/real-world graphs in Section 7. Section 8 reviews the related work on community search/ detection on unipartite graphs and static/streaming bipartite graphs. Finally, Section 9 concludes this paper."
https://arxiv.org/html/2411.01023v1,Capturing and Anticipating User Intents in Data Analytics via Knowledge Graphs,"In today’s data-driven world, the ability to extract meaningful information from data is becoming essential for businesses, organizations and researchers alike. For that purpose, a wide range of tools and systems exist addressing data-related tasks, from data integration, preprocessing and modeling, to the interpretation and evaluation of the results. As data continues to grow in volume, variety, and complexity, there is an increasing need for advanced but user-friendly tools, such as intelligent discovery assistants (IDAs) or automated machine learning (AutoML) systems, that facilitate the user’s interaction with the data. This enables non-expert users, such as citizen data scientists, to leverage powerful data analytics techniques effectively.The assistance offered by IDAs or AutoML tools should not be guided only by the analytical problem’s data but should also be tailored to each individual user. To this end, this work explores the usage of Knowledge Graphs as a basic framework for capturing in a human-centered manner complex analytics workflows, by storing information not only about the workflow’s components, datasets and algorithms but also about the users, their intents and their feedback, among others. The data stored in the generated Knowledge Graph can then be exploited to provide assistance (e.g., recommendations) to the users interacting with these systems. To accomplish this objective, two methods are explored in this work. Initially, the usage of query templates to extract relevant information from the Knowledge Graph is studied. However, upon identifying its main limitations, the usage of link prediction with knowledge graph embeddings is explored, which enhances flexibility and allows leveraging the entire structure and components of the graph. The experiments show that the proposed method is able to capture the graph’s structure and to produce sensible suggestions. To demonstrate the feasibility of the approach, a prototype is presented.","Machine Learning (ML) is a branch of Artificial Intelligence which focuses on building algorithms from data that can be used to perform different complex tasks (e.g., identify hidden patterns, make predictions, provide suggestions, etc.). This conversion of data into knowledge has numerous applications across multiple fields and industries, and its relevance is growing with the increasing availability and abundance of data. However, since transforming raw data into valuable information can be time-consuming and challenging, various efforts have been done to simplify some of the laborious steps of the ML workflow creation for experts, while enabling inexperienced users to construct valuable models without requiring significant programming or ML expertise. For instance, Intelligent Discovery Assistants (IDAs) are used to interactively assist users while creating Data Analytics (DA) workflows. To this end, they leverage expert derived rules, previously successful workflows, metadata about the input dataset or about the operators to propose workflows [1]. Another approach is AutoML [2], which is the automation of various ML stages by looking for the best workflow (or workflow component) by optimizing the analytical problem over a search space of algorithms and parameters. Using these tools, however, is not completely straightforward for non-technical users as various decisions still have to be made throughout the whole process (e.g., selecting the analytical purpose, identifying a suitable metric, etc.). These decisions can have a huge impact on the quality of the results, hence guiding users while generating ML workflows can improve the effectiveness of the analysis and make the whole process more efficient [3]. However, generating these recommendations is not a trivial task, as they should depend on the user’s characteristics (e.g., area of expertise, role in a company), user’s past experiences, datasets used, etc. To this end, this work proposes a method to provide recommendations for the users’ decisions by leveraging the information captured in a Knowledge Graph (KG). KGs are a form of knowledge representation that organize information in a structured and interconnected manner by representing a network of entities and the different relations between them. One of the standard ways of defining a KG is by expressing relationships as a set of three elements called a triple (h,r,t), consisting of the head entity (h), the relation (r), and the tail entity (t). This semantically rich representation preserves structural information while making it both machine-readable and easily understandable by humans. Figure 1 provides the context of how the anticipation system developed in this work can be used to assist the user in the initial phase of defining the input in different DA Assistants. For that, we assume that the assistant can interact with a KG, which contains annotations of the users’ past interactions with the system. To this end, after uploading a dataset (arrow 1), the system can provide recommendations (arrow 2) for the users’ input (e.g., intent, constraints, etc.) based on information about the dataset, the users and previous experiments stored in the KG. These are the recommendations that fall under the scope of this work and they can be reviewed and changed by the users (arrow 3) to finally define the input to the system (arrow 4). Next, considering the case of an IDA [4, 5], the system would suggest logical workflows (arrow 5), which the users can tune (arrow 6) before their execution (arrows 7 and 8). In the case of AutoML systems [6, 7, 8], steps 5-8 (highlighted with a gray box) are replaced by an automatic optimization of the process based on the specified input. In both cases, the users are presented with the results (arrow 9) and can provide their feedback (arrow 10). Figure 1: Overview of a DA Assistant interacting with a KG. In this work, the grayed steps have been automated via an AutoML tool. In this work, we instantiate the interaction shown in Figure 1 and design a KG for the DA domain. However, in addition to capturing the different components of analytical workflows, their characteristics and the relationships between them, the KG captures information about the users, their feedback over the workflows and the intent for which these workflows have been generated. Now, to transform the information stored in the KG into user-centered recommendations (see Figure 1, arrow 2), we first explore how the KG structure can be exploited by using predefined query templates. These templates allow the generation of simple but sensible recommendations, and are designed taking domain knowledge into account. However, this approach lacks flexibility and does not use the available information of the graph unless complex ad-hoc queries are posed for every request. To address this problem, we exploit the whole structure and components of the KG by creating embeddings for its different elements. With them, we build a recommender system for the user inputs based on link prediction, that enables leveraging all the information stored in the graph. To this end, the main contributions of this work are: 1. We develop a KG to capture and store the human-in-the-loop (i.e., users, intents, preferences, constraints) together with the analytical artifacts (e.g., datasets, algorithms, workflows) required for the automatic generation of analytical workflows. 2. We propose how to represent DA intents with a hierarchical structure with different levels of abstraction, going from the most general intents (e.g., describe, suggest, predict, etc.) to concrete algorithms and their implementations. 3. We explore the use of query templates to retrieve information from the KG to provide human-in-the-loop assistance when generating analytical workflows. 4. We propose a more flexible method, based on graph embeddings, that leverages the KG’s topology to anticipate and recommend the user input by using link prediction. 5. We evaluate our approach to show its feasibility and present a prototype to demonstrate its potential use in practice. The remainder of this paper is organized as follows. In Section 2, we discuss the Related Work. In Section 3, we detail the design of the KG and in Section 4 we describe how it has been populated. In Section 5, we discuss how the KG can be exploited to provide recommendations for the user input (i.e., intentions, constraints and preferences) by querying the KG and by using knowledge graph embeddings. For the latter, we provide an experimental evaluation in Section 6. Finally in Section 7, a prototype implementing both methods is presented, and in Section 8 we provide the conclusions and ideas for future work."
https://arxiv.org/html/2411.00744v1,CORAG: A Cost-Constrained Retrieval Optimization System for Retrieval-Augmented Generation,"Large Language Models (LLMs) have demonstrated remarkable generation capabilities but often struggle to access up-to-date information, which can lead to hallucinations. Retrieval-Augmented Generation (RAG) addresses this issue by incorporating knowledge from external databases, enabling more accurate and relevant responses. Due to the context window constraints of LLMs, it is impractical to input the entire external database context directly into the model. Instead, only the most relevant information, referred to as “chunks”, is selectively retrieved. However, current RAG research faces three key challenges. First, existing solutions often select each chunk independently, overlooking potential correlations among them. Second, in practice, the utility of chunks are “non-monotomic”, meaning that adding more chunks can decrease overall utility. Traditional methods emphasize maximizing the number of included chunks, which can inadvertently compromise performance. Third, each type of user query possesses unique characteristics that require tailored handling—an aspect that current approaches do not fully consider.To overcome these challenges, we propose a cost-constrained retrieval optimization system CORAG for retrieval-augmented generation. We employ a Monte Carlo Tree Search (MCTS)-based policy framework to find optimal chunk combinations sequentially, allowing for a comprehensive consideration of correlations among chunks. Additionally, rather than viewing budget exhaustion as a termination condition, we integrate budget constraints into the optimization of chunk combinations, effectively addressing the non-monotonicity of chunk utility. Furthermore, by designing a configuration agent, our system predicts optimal configurations for each query type, enhancing adaptability and efficiency. Experimental results indicate an improvement of up to 30% over baseline models, underscoring the framework’s effectiveness, scalability, and suitability for long-context applications.","Although LLMs have demonstrated exceptional capabilities in generation tasks, they often struggle with accessing up-to-date information, which can lead to hallucinations (Huang et al., 2023; Xu et al., 2024). To address these challenges, RAG has emerged as a crucial solution. By integrating external data sources into LLM, RAG can provide more accurate, relevant, and up-to-date information. Nowadays, RAG has been widely studied in the context of LLMs especially for tasks requiring update external knowledge such as question answering task (Asai et al., 2023; Sarthi et al., 2024; Microsoft, 2024), medical information retrieval (Singhal et al., 2022; Alkhalaf et al., 2024), and time series analysis (Ravuru et al., 2024; Jin et al., 2023; Ye et al., 2024). External data sources are often extremely large, making it impractical to input them directly into the LLM. To address this issue, data is typically split into disjoint chunks and stored in a vector database, and then users query the most useful chunks to construct prompts for LLMs. Therefore, designing efficient and accurate structures and algorithms to search for the most relevant chunks has become a prominent research topic and has been widely studied in both the database (Xue et al., 2024; Zhao et al., 2024c) and machine learning communities (Asai et al., 2023; Yu et al., 2024a; Wang et al., 2024b). Figure 1. Example of chunks combination order. However, there are three key challenges in the existing approaches. Challenge 1: Correlations between chunks Currently, two primary methods are used to identify the most relevant chunks. The first approach formulates the problem as a approximate k-nearest neighbor (AKNN) task (Zhang et al., 2024; Yin et al., 2024), where each chunk is assigned a score, and the approxiamte top-k𝑘kitalic_k chunks ranked by score are selected. The second approach clusters the chunks, returning all chunks within the most relevant clusters in response to a query (Microsoft, 2024; Sarthi et al., 2024). However, both methods overlook potential correlations between chunks: the first approach disregards correlations entirely, while the second approach accounts for them only superficially by treating all chunks within each cluster as equally relevant. As a result, when multiple chunks convey similar or overlapping information, these methods introduce substantial redundancy in the selected chunks. For example, as illustrated in Figure 1, when querying the height and history of the Eiffel Tower, if each chunk is treated independently, a greedy method would select chunks χ3subscript𝜒3\chi_{3}italic_χ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT and χ1subscript𝜒1\chi_{1}italic_χ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT since they have the top two scores. However, both chunks only provide historical information, which is insufficient to fully address the query. To better address the query, it is necessary to include a chunk with constructor’s name, such as χ4subscript𝜒4\chi_{4}italic_χ start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT. On the other hand, the clustering approach would return all of χ1,χ2,χ3subscript𝜒1subscript𝜒2subscript𝜒3\chi_{1},\chi_{2},\chi_{3}italic_χ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_χ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_χ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT, and χ4subscript𝜒4\chi_{4}italic_χ start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT, resulting in redundancy. An optimal solution would instead select χ3subscript𝜒3\chi_{3}italic_χ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT and χ4subscript𝜒4\chi_{4}italic_χ start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT, as they provide the required information without redundancy. Additionally, research (Lu et al., 2021; Jiang et al., 2024; Yu et al., 2024b) has shown that the order of chunks influences LLM performance, a factor that existing methods also overlook. Following the example of the Eiffel Tower, when chunks χ3subscript𝜒3\chi_{3}italic_χ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT and χ4subscript𝜒4\chi_{4}italic_χ start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT are selected, placing χ4subscript𝜒4\chi_{4}italic_χ start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT first yields a higher score compared with the reverse order will have a better performance. However, determining the optimal chunk combination order is a challenging task since both of them require a search space growing exponentially with the number of available chunks. In this paper, we further demonstrate that this problem is NP-hard (see Section 2.1). Challenge 2: Non-monotonicity of utility Current solutions operate on the assumption that including more chunks will always yield better final results. Specifically, in the AKNN-based approach, exactly k𝑘kitalic_k chunks are selected deterministically each time. In the clustering-based approach, a distance threshold between clusters and the query is set, and all clusters within this threshold are returned. Both of them return as many chunks as possible. However, in practice, the utility of chunks is not monotonic. More specifically, excessive chunks can dilute key information by adding marginally relevant content, creating noise that reduces clarity. Additionally, conflicting or nuanced differences across chunks may confuse the model, lowering response quality. For example, as illustrated in Figure 1, when χ3subscript𝜒3\chi_{3}italic_χ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT and χ4subscript𝜒4\chi_{4}italic_χ start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT are selected, adding the chunk χ1subscript𝜒1\chi_{1}italic_χ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT decreases utility, highlighting that utility scores are often non-monotonic in practice. Challenge 3: Diversity of queries: User queries come in different types, each requiring its own ranking strategy due to their unique characteristics (Zhao et al., 2024a). In current RAG systems, the utility scores of chunks often are determined by the assigned reranker model. So far, various reranker models exist, but we observe that their performance varies significantly across different query types, and no single fixed reranker model consistently outperforms the others across all query variations (see our experiments in Section 6.3.4 for more details). Current methods (Lyu et al., 2024; Zhao et al., 2024b) typically rely on static reranker models for ranking chunks, lacking the flexibility to adapt to varying query contexts. Problem Statement: Is there a RAG system that fully considers correlations between chunks and the non-monotonicity of utility while being adaptable to all types of queries? 1.1. Our Contributions In this paper, we answer this question in the affirmative, by proposing a novel MCTS based policy tree framework to optimize chunk retrieval in RAG systems. In summary, our contributions can be summarized as follows: • We propose the first RAG framework that considers the chunk combination order for the RAG task. Instead of considering each chunk independently or at the cluster level, we use MCTS to help search the optimal chunk combination order sequentially. The high-level idea is as follows: First, we initialize the root node. Then, in an iterative process, we expand the tree by selecting the highest utility node and computing its expended nodes’ utilities. After each expansion, we update the utilities throughout the entire policy tree. During this process, the decision at each iteration depends on the chunks already selected, allowing us to fully consider the correlations between chunks. Moreover, MCTS reduces the exponential search space to linear, and we apply parallel expansion techniques to further enhance computational efficiency. With such designs, we address Challenge 1. • In contrast to prior RAG frameworks that consider the exhaustion of the budget as one of termination conditions, we propose a novel formulation wherein budget constraints are integrated into the process of optimizing chunk combinations to fully consider the non-monotonicity of utility of chunks thereby addressing Challenge 2. Moreover, by prioritizing high-relevance, low-cost chunks and factoring in token length, we further reduce computational costs. • We propose a contrastive learning-based agent that dynamically adjusts MCTS configurations per query, adapting reranker models and configurations to the specific query domain. This approach tailors retrieval for dynamic, domain-specific queries with flexibility and robustness, addressing Challenge 3. • Additionally, we conducted comprehensive experiments, comparing our framework with several state-of-the-art methods. The results validate the effectiveness, efficiency, and scalability of our approach, also showing a performance improvement of 30% over the baseline."
https://arxiv.org/html/2411.00615v1,Apriori_Goal Algorithm for Constructing Association Rules for a Database with a Given Classification,"An efficient algorithm, Apriori_Goal, is proposed for constructing association rules for a relational database with a given classification. The algorithm’s features are related to the specifics of the database and the method of encoding its records. The algorithm proposes five criteria that characterize the quality of the rules being constructed. Different criteria are also proposed for filtering the sets used when constructing association rules. The proposed method of encoding records allows for an efficient implementation of the basic operation underlying the computation of rule characteristics.The algorithm works with a relational database, where the columns can be of different types, both continuous and discrete. Among the columns, a target discrete column is distinguished, which defines the classification of the records. This allows the original database to be divided into n𝑛nitalic_n subsets according to the number of categories of the target parameter.A classical example of such databases is medical databases, where the target parameter is the diagnosis established by doctors.A preprocessor, which is an important part of the algorithm, converts the properties of the objects represented by the columns of the original database into binary properties and encodes each record as a single integer. In addition to saving memory, the proposed format allows the complete preservation of information about the binary properties representing the original record. More importantly, the computationally intensive operations on records, required for calculating rule characteristics, are performed almost instantly in this format using a pair of logical operations on integers.","Association rules are one of the important tools used in knowledge discovery from databases. The advantage of rules is that the extracted knowledge is presented in the form of an implication: X⇒Y⇒𝑋𝑌X\Rightarrow Yitalic_X ⇒ italic_Y where X𝑋Xitalic_X is the antecedent of the rule, and Y𝑌Yitalic_Y is the consequent. This form is easily understood by experts for whom the knowledge is extracted. The rule is interpreted as: ”if X𝑋Xitalic_X, then Y𝑌Yitalic_Y,” or ”X𝑋Xitalic_X implies Y𝑌Yitalic_Y,” or ”the occurrence of X𝑋Xitalic_X is associated with the occurrence of Y𝑌Yitalic_Y.” Naturally, for the expert, a rule is valuable only when they know that the rule holds with a high degree of probability. Association rules extracted from databases became widespread due to the appearance of the Apriori algorithm, proposed in the works of Rakesh Agrawal and his co-authors [1, 2, 3], with the first of these works appearing as early as 1993. In the Apriori algorithm, a finite set of literals D={d1,d2,…,dm}𝐷subscript𝑑1subscript𝑑2…subscript𝑑𝑚D=\{d_{1},d_{2},\dots,d_{m}\}italic_D = { italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_d start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT } is considered, along with a database d⁢b={t1,t2,…,tn}𝑑𝑏subscript𝑡1subscript𝑡2…subscript𝑡𝑛db=\{t_{1},t_{2},\dots,t_{n}\}italic_d italic_b = { italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } consisting of records—transactions, each of which is a subset of literals: tk⊆Dsubscript𝑡𝑘𝐷t_{k}\subseteq Ditalic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ⊆ italic_D. In an association rule (1), extracted from the database, X𝑋Xitalic_X and Y𝑌Yitalic_Y are non-empty, non-overlapping subsets of literals: X⊆D,Y⊆D,X∩Y=∅formulae-sequence𝑋𝐷formulae-sequence𝑌𝐷𝑋𝑌X\subseteq D,\quad Y\subseteq D,\quad X\cap Y=\emptysetitalic_X ⊆ italic_D , italic_Y ⊆ italic_D , italic_X ∩ italic_Y = ∅ For each rule constructed by the Apriori algorithm, two characteristics of the rule are computed from the database, determining its value—frequency and confidence. Frequency is defined as the ratio of the number of transactions containing both the antecedent and consequent of the rule to the total number of transactions in the database. Confidence is defined as the ratio of the number of transactions containing both the antecedent and consequent to the number of transactions containing the antecedent. The Apriori algorithm efficiently constructs all association rules whose frequency and confidence exceed the minimally specified values. The basic idea of the Apriori algorithm is based on the anti-monotonicity property of frequency. If a set Z⊆D𝑍𝐷Z\subseteq Ditalic_Z ⊆ italic_D is not frequent, then any of its supersets—extensions of Z𝑍Zitalic_Z—are also not frequent. As a consequence, in the first step, the algorithm constructs all frequent subsets of D𝐷Ditalic_D, starting from the frequent singleton subsets, and then extends them step by step. In the second step, these subsets are used to construct rules that meet the specified confidence. Subsequent numerous variations of algorithms for constructing association rules, in one way or another, use the basic idea of the Apriori algorithm. In the work of F. Bodon [4], a sorting tree structure was proposed for representing transactions, demonstrating its efficiency in compressing the database. An efficient implementation of the algorithm was proposed in the works of Christian Borgelt [5, 6, 7]. In this implementation, the database is compressed into a transaction tree. In Borgelt’s review article [8], one can find descriptions of various algorithms for constructing frequent itemsets. In the fundamental book by Han, Kamber, and Pei [9], which is dedicated to the concepts and methods of knowledge discovery from databases, various algorithms and approaches to constructing association rules are discussed in detail within the broader context of the general Data Mining approach. The first work on constructing multi-level association rules was the work by Han and Fu [10]. In [11], the efficiency of constructing multi-level rules was analyzed. The number of works dedicated to association rules is increasing every year. I will mention just a few works related to the topic of this article. In [12], methods for constructing association rules are discussed when the original data can be of various types. The works [13, 14] consider the construction of rare association rules, which may be of greater interest to experts than frequent rules, which are often well-known to experts. Several works are devoted to constructing rules for specific applied domains. The works [15, 16] consider the construction of association rules for medical databases. In [17], an algorithm was proposed based on compressing database records, where a record is represented by a single integer while preserving all the information about the set of literals in the record. In [18], an important special case was considered where the database has a target column, simplifying the rule construction algorithm by allowing the antecedent to be built with a fixed consequent. This article presents the Apriori_Goal algorithm, which generalizes the ideas presented in [17, 18]."
https://arxiv.org/html/2411.00761v1,LCP: Enhancing Scientific Data Management withLossyCompression forParticles,"Many scientific applications opt for particles instead of meshes as their basic primitives to model complex systems composed of billions of discrete entities. Such applications span a diverse array of scientific domains, including molecular dynamics, cosmology, computational fluid dynamics, and geology. The scale of the particles in those scientific applications increases substantially thanks to the ever-increasing computational power in high-performance computing (HPC) platforms. However, the actual gains from such increases are often undercut by obstacles in data management systems related to data storage, transfer, and processing. Lossy compression has been widely recognized as a promising solution to enhance scientific data management systems regarding such challenges, although most existing compression solutions are tailored for Cartesian grids and thus have sub-optimal results on discrete particle data. In this paper, we introduce LCP, an innovative lossy compressor designed for particle datasets, offering superior compression quality and higher speed than existing compression solutions. Specifically, our contribution is threefold. (1) We propose LCP-S, an error-bound aware block-wise spatial compressor to efficiently reduce particle data size while satisfying the pre-defined error criteria. This approach is universally applicable to particle data across various domains, eliminating the need for reliance on specific application domain characteristics. (2) We develop LCP, a hybrid compression solution for multi-frame particle data, featuring dynamic method selection and parameter optimization. It aims to maximize compression effectiveness while preserving data quality as much as possible by utilizing both spatial and temporal domains. (3) We evaluate our solution alongside eight state-of-the-art alternatives on eight real-world particle datasets from seven distinct domains. The results demonstrate that our solution achieves up to 104% improvement in compression ratios and up to 593% increase in speed compared to the second-best option, under the same error criteria.","Scientific data management systems (HDF5, 2024; Gray et al., 2005; Cheng and Rusu, 2014; Kersten et al., 2011) are facing ever-increasing challenges from the rapid evolution of computing power versus the comparatively slow expansion of data infrastructure in HPC facilities. The fast-growing computing power enables scientific applications to run on a larger scale with higher precision, which as a consequence produces more data beyond the memory, storage, and I/O capacities of the data systems on supercomputers. For example, the EXAALT project which focuses on molecular dynamics (MD), generates trajectories containing over a trillion time steps using exascale machines by leveraging parallel-in-time approaches (EXAALT project, 2021). Storing all these frames in a scientific data management system (e.g., HDF5 (HDF5, 2024)) would require hundreds of terabytes of disk space, transferring them between facilities may take hours or days, and post-analysis of all frames on a single node is impractical due to insufficient memory size. Error-bounded lossy compression has been widely considered a promising solution for scientific applications facing data challenges (Zhao et al., 2020, 2021; Lindstrom, 2014; Liang et al., 2022a; Jiao et al., 2023; Tian et al., 2021). First, lossy compressors can reduce the data volume significantly (by a factor of 5∼1000similar-to510005\sim 10005 ∼ 1000 in most cases). By comparison, lossless compressors, including Zstd (Zstandard, 2021), Gorilla (Pelkonen et al., 2015), and Brotli (Alakuijala et al., 2018), can only reduce the data by a factor of 2 in most cases (Zhao et al., 2021). Second, error-bounded lossy compression can limit the compression error, ensuring that the quality of the decompressed data remains acceptable for post-analysis. (a) Structured mesh (b) Particle data Figure 1. Visualization of structured mesh versus particles Scientific data can generally be classified into particle-style (e.g., locations, connectivity) and mesh-style (e.g. regular multidimensional grid in space). Existing lossy compressors, including those designed for databases (e.g., ModelarDB (Jensen et al., 2018, 2021), SummaryStore (Agrawal and Vulimiri, 2017), SciDB (Cudre-Mauroux et al., 2009)) and those specifically designed for scientific data (e.g., SZ3 (Zhao et al., 2021), ZFP (Lindstrom, 2014), MGARD (Liang et al., 2022a)), are primarily tailored for structured mesh and suffer from low effectiveness on particle data (Zhao et al., 2022). However, various research fields such as material science, biology, cosmology, and computational fluid dynamics extensively utilize particle data. Scientific lossy compression techniques specific to the management of particle data remain under-explored. In this paper, our objective is to develop an efficient scientific lossy compressor for the management of particle data. This task presents several challenges: (1) Most spatial compression techniques suitable for meshes are not applicable to particle data. These techniques often depend on the correlation of adjacent data inherent in structured meshes representing physical fields. In contrast, particle-style data lacks this correlation as it represents particles that are arbitrarily positioned in space (as depicted in Figure 1), leaving little structure to exploit for high compression ratios. (2) Applying temporal compression to multi-frame particle data is often impractical. One reason is that frames may be saved at irregular or even random intervals, such that the temporal domain may not be correlated enough to improve compression. Second, temporal compression often requires loading consecutive frames into memory to identify global patterns or characteristics essential for compression. However, particle data frames can be exceptionally large allowing only a subset of frames to be loaded into memory. Without large numbers of consecutive frames, the effectiveness of temporal compression diminished. (3) Accessing frames by batch is required by applications (discussed in Section 2.1.3), which further restricts the approach. To support the retrieval of selected frames, compression is typically performed in small independent batches (each with a few frames, with no dependencies between batches). Since compression needs to be done multiple times, methods that have large amounts of metadata to store per compression introduce substantial overhead. Moreover, techniques requiring inter-batch dependencies should be excluded, otherwise decompressing a single frame will necessitate decompressing all of its preceding frames first, resulting in significant overhead on partial data retrieval. Taking into account all the aforementioned challenges, we introduce LCP, a novel scientific Lossy Compressor for the management of Particle data. The main contributions are outlined as follows: • We propose the spatial compressor LCP-S and the temporal compressor LCP-T for particles. LCP-S is equipped with error-bound-aware quantization and spatial-block-wise coding algorithms to reach high compression effectiveness and data fidelity while guaranteeing the arbitrary error-bound defined by users before compression. Moreover, LCP-S is universally applicable to any particle data, in contrast to existing methods that are constrained to domain-specific data (Zhao et al., 2022; Lundborg et al., 2014). • We propose LCP, our dynamic hybrid compression solution for multi-frame particle data. LCP is built on a hybrid design incorporating LCP-S and LCP-T, together with dynamic method selection and parameter optimization strategies, to maximize compression effectiveness by exploiting data characteristics in both spatial and temporal domains. Additionally, LCP is enhanced with the spatial-anchor-frame based batch compression technique to support the fast partial retrieval needs of applications. • We evaluate our solution LCP on eight particle datasets from seven distinct domains with eight state-of-the-art related works. Experiments demonstrate that LCP is the best lossy compressor for the management of particle data, achieving the highest compression ratios, speed, and fidelity among all the compressors. The remainder of the paper is structured as follows. In Section 2 delves into the research background. In Section 3, we provide an overview of related work. Section 4 formulates the research problem. Our developed particle compression framework is detailed in Section 5 to Section 7. Section 8 presents and discusses the evaluation results. Finally, we draw conclusions in Section 9."
https://arxiv.org/html/2411.00073v1,RSL-SQL: Robust Schema Linking in Text-to-SQL Generation,"Text-to-SQL generation aims to translate natural language questions into SQL statements. In large language models (LLMs) based Text-to-SQL, schema linking is a widely adopted strategy to streamline the input for LLMs by selecting only relevant schema elements, therefore reducing noise and computational overhead. However, schema linking faces risks that requires caution, including the potential omission of necessary elements and disruption of database structural integrity. To address these challenges, we propose a novel framework called RSL-SQL that combines bidirectional schema linking, contextual information augmentation, binary selection strategy, and multi-turn self-correction. Our approach improves the recall of schema linking through forward and backward pruning and hedges the risk by voting between full schema and contextual information augmented simplified schema. Experiments on the BIRD and Spider benchmarks demonstrate that our approach achieves state-of-the-art execution accuracy among open-source solutions, with 67.2% on BIRD and 87.9% on Spider using GPT-4o. Furthermore, our approach outperforms a series of GPT-4 based Text-to-SQL systems when adopting DeepSeek (much cheaper) with same intact prompts. Extensive analysis and ablation studies confirm the effectiveness of each component in our framework. The codes are available at https://github.com/Laqcce-cao/RSL-SQL.","The task of translating natural language questions into structured query language (SQL), known as Text-to-SQL or Text2SQL generation, is crucial for enabling non-expert users to interact with relational databases Wang et al. (2019); Qin et al. (2022). By bridging the gap between natural language and structured query languages, text2sql systems democratize data access and empower users to extract valuable insights without deep technical knowledge Katsogiannis-Meimarakis and Koutrika (2023). In very recent years, leveraging the powerful comprehension and generation capabilities of Large Language Models (LLMs) Achiam et al. (2023); OpenAI (2024); Anthropic (2024) for Text-to-SQL tasks has become a primary approach for boosting performance and prompt engineering emerges as the mainstream technical strategy. A typical prompt provided to the LLM for Text2SQL usually includes a description of the database, user queries, and few-shot demonstrations Gao et al. (2023); Pourreza and Rafiei (2024a), which allows the system to be applicable to various databases. Intuitively, assuming the LLM possesses sufficiently strong capabilities, the more precise and detailed the database description, the better the quality of the generated SQL queries. Features such as the database’s structure, annotations, sample data, and relational mappings has been validated in specific scenarios to benefit Text-to-SQL performance Li et al. (2024d); Lee et al. (2024). Fine-grained descriptions of databases pose challenges. It is common for databases, especially industrial-grade databases, to have hundreds or thousands of fields. Incorporating complete database features in the prompt leads to excessive input tokens, increased computational costs, and, critically, the introduction of substantial noise Talaei et al. (2024). Since user queries typically refer to a small proportion of database schema elements, a large amount of irrelevant schema information can confuse LLM and degrade performance. To mitigate this, schema linking Lei et al. (2020); Pourreza and Rafiei (2024a) techniques have been widely adopted to identify and include only relevant schema elements in the prompts. However, as shown in Figure 1, there are two types of information loss risks associated with schema linking: 1) The generated SQL will inevitably be erroneous if schema linking fails to identify all necessary tables and columns (assuming that the LLM does not generate any hallucinations, which means schema elements of the SQL generated by the LLM are entirely derived from input database schema); 2) Pruning the database schema might disrupt inherent structural relationships and impair the LLM’s understanding of the database’s raw structure, resulting in incorrect queries even though if schema linking identifies all required elements. Related research Maamari et al. (2024) shows that for high-performance LLMs, such as GPT-4 or GPT-4o, schema linking may even reduce the execution accuracy of generated SQL statements. To address these challenges, we propose RSL-SQL, a Robust Schema Linking based Text-to-SQL generation framework that mitigates the risks associated with schema linking while leveraging its benefits. In our framework, we first generates a preliminary SQL using complete database schema and achieves high recall rate through bidirectional schema linking. Next, we simplify the database schema and enhance it with rich contextual information to generate an another SQL. Subsequently, a binary selection strategy (selecting the better SQL generated from the complete or simplified database schema) is used to hedge against the potential risks of schema linking. Finally, we employ a multi-turn self-correction approach, integrating feedback from SQL execution results to iteratively refine and optimize bad SQL statements. In the experiments, we evaluate the proposed method RSL-SQL on BIRD and Spider datasets, comparing its performance against extensive Text-to-SQL methods. The experimental results show that when using GPT-4o as the backbone LLM, RSL-SQL achieves 67.21% execution accuracy and 69.39% valid efficiency score on the BIRD dataset (new SOTA among all open-source methods), and 87.2% execution accuracy on the Spider dataset (comparable to SOTA). Moreover, we demonstrate that, when using significantly more cost-effective DeepSeek, RSL-SQL outperforms many GPT-4-based methods on both BIRD and Spider datasets, despite that per-token cost of GPT-4 is 215 times higher than DeepSeek. The ablation study reveals that each component of our method contributes to the overall performance gains. Notably, our bidirectional schema linking technique achieves a high strict recall rate of 92% (a new SOTA) on BIRD dataset, while significantly reducing the average number of columns per query. Both contextual information augmentation and binary selection strategy are verified to improve accuracy steadily, thereby mitigating potential risks associated with schema linking. The main contributions of this paper can be summarized as follows: (1) We investigate potential risks associated with schema linking, and propose RSL-SQL, a novel Text-to-SQL generation framework with robust schema linking that achieves state-of-the-art (SOTA) performance on the BIRD dataset; (2) Extensive experimental results demonstrate the effectiveness and robustness of the proposed method. Our framework also exhibits good transferability, with its performance surpassing many GPT-4-based methods when using much cheaper DeepSeek, demonstrating excellent cost-effectiveness."
