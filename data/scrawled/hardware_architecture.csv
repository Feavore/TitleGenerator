URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.04410v1,The Survey of Chiplet-based Integrated Architecture: An EDA perspective,"Enhancing performance while reducing costs is the fundamental design philosophy of integrated circuits (ICs). With advancements in packaging technology, interposer-based chiplet architecture has emerged as a promising solution. Chiplet integration, often referred to as 2.5D IC, offers significant benefits, including cost-effectiveness, reusability, and improved performance. However, realizing these advantages heavily relies on effective electronic design automation (EDA) processes. EDA plays a crucial role in optimizing architecture design, partitioning, combination, physical design, reliability analysis, etc. Currently, optimizing the automation methodologies for chiplet architecture is a popular focus; therefore, we propose a survey to summarize current methods and discuss future directions. This paper will review the research literature on design automation methods for chiplet-based architectures, highlighting current challenges and exploring opportunities in 2.5D IC from an EDA perspective. We expect this survey will provide valuable insights for the future development of EDA tools chiplet-based integrated architectures.","The challenge of designing complex electronic systems that deliver high performance while keeping costs low has been a driving force in integrated circuit (IC) design. The concept of Moore’s law (moore1998cramming, ) has historically guided the industry, illustrating how technological advancements enable the integration of more transistors on a single die, thereby enhancing performance and reducing costs. However, chip scaling improvements resulting from the decreasing size of transistors are no longer effective today. Firstly, physical limitations make it increasingly difficult to reduce transistor sizes further. This results in a bottleneck in improving performance by placing more transistors in a unit area of the silicon wafer. Secondly, manufacturing costs continue to rise with the size reduction of transistors. Smaller transistors require more sophisticated and complex lithography systems, which significantly increase manufacturing costs for the chip industry. Moreover, the yield from silicon wafers also decreases, making it impossible to maintain cost benefits while improving overall performance. In response, many leading foundries, such as TSMC, Samsung, and Intel, are actively investigating alternative strategies to lower wafer costs and improve production yields (zhuang2022multi-package, ). One promising avenue is the adoption of advanced heterogeneous integration and multi-chiplet architectures. This design philosophy involves creating separate hardware modules with specific functions, which are then combined through an interposer to form a comprehensive system (ECTC2008interposer-source, ). Figure 1 illustrates the structure of a chiplet-based system fabricated using the advanced chip-on-wafer-on-substrate (CoWoS) technology (CoWos2017, ). The interposer-based 2.5D architecture has been used in products such as the Xilinx Virtex-7 2000T FPGA (eptc2013FPGA-Chiplet, ) and the AMD ZEN2 Processor (AMD2020ZEN, ). Figure 1. The architecture of chiplet-based 2.5D IC. In the design and implementation of 2.5D architectures, electronic design automation (EDA) plays a crucial role, starting from the front end with architectural design and performance simulation, to the back end with physical design and package design. Figure 2 shows the typical stages in the EDA flow for 2.5D architecture. At the design stage, EDA tools facilitate the simulation and exploration of various chiplet configurations, enabling designers to evaluate different architectural choices and their potential impacts on performance, power consumption, cost, etc. This early-stage analysis is essential for identifying potential bottlenecks and ensuring that the design meets the desired specifications. For physical design and package design, EDA assists in the precise arrangement and interconnection of chiplets on a silicon interposer, accounting for factors such as thermal management and communication latency. EDA tools also support reliability analysis and multi-physics modeling to ensure the final assembly is efficient and manufacturable. Figure 2. The EDA flow of the chiplet-based architecture. This paper reviews the critical role of EDA in the design flow of 2.5D IC and explores future directions to address current challenges. Section Section 2 highlights the benefits of 2.5D architecture, summarizing research in architectural design, functional simulation, and other issues encountered in the early design stages. In Section Section 3, we analyze strategies for partitioning chiplets and their interconnection mechanisms. Section Section 4 discusses the physical design workflow for 2.5D IC, emphasizing the importance of floorplan & placement, Routing, multiphysics modeling, and other topics in physics design. Next, Section Section 5 explores current challenges and potential advancements in EDA methodology for 2.5D integration. Finally, Section Section 6 concludes the paper."
https://arxiv.org/html/2411.04471v1,FQsun: A Configurable Wave Function-Based Quantum Emulator for Power-Efficient Quantum Simulations,"Quantum computing has emerged as a powerful tool for solving complex computational problems, but access to real quantum hardware remains limited due to high costs and increasing demand for efficient quantum simulations. Unfortunately, software simulators on CPUs/GPUs such as Qiskit, ProjectQ, and Qsun offer flexibility and support for a large number of qubits, they struggle with high power consumption and limited processing speed, especially as qubit counts scale. Accordingly, quantum emulators implemented on dedicated hardware, such as FPGAs and analog circuits, offer a promising path for addressing energy efficiency concerns. However, existing studies on hardware-based emulators still face challenges in terms of limited flexibility, lack of fidelity evaluation, and power consumption. To overcome these gaps, we propose FQsun, a quantum emulator that enhances performance by integrating four key innovations: efficient memory organization, a configurable Quantum Gate Unit (QGU), optimized scheduling, and multiple number precisions. Five FQsun versions with different number precisions, including 16-bit floating point, 32-bit floating point, 16-bit fixed point, 24-bit fixed point, and 32-bit fixed point, are implemented on the Xilinx ZCU102 FPGA, utilizing between 9,226 and 18,093 LUTs, 1,440 and 7,031 FFs, 344 and 464 BRAMs, and 14 and 88 DSPs and consuming a maximum power of 2.41W. Experimental results demonstrate high accuracy in normalized gate speed, fidelity, and mean square error, particularly with 32-bit fixed-point and floating-point versions, establishing FQsun’s capability as a precise quantum emulator. Benchmarking on quantum algorithms such as Quantum Fourier Transform, Parameter-Shift Rule, and Random Quantum Circuits reveals that FQsun achieves superior power-delay product, outperforming traditional software simulators on powerful CPUs by up to 9.87×1039.87superscript103\text{9.87}\times\text{10}^{\text{3}}9.87 × 10 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT times. Additionally, FQsun shows significantly improved normalized gate speed compared to hardware-based emulators, ranging from 1.31 times to 1.51 ×1010absentsuperscript1010\times\text{10}^{\text{10}}× 10 start_POSTSUPERSCRIPT 10 end_POSTSUPERSCRIPT times, highlighting its potential as a scalable and energy-efficient solution for advanced quantum simulations.","Quantum computing is captivating research field that has many promising applications in factorial problem [1], searching [2], optimization [3], or quantum machine learning [4]. Numerous researchers have developed real quantum devices successfully, such as IBM Quantum [5], Google Quantum AI [6], and QuEra [7], some even claim to have achieved “quantum supremacy”. Recently, quantum computers have rapidly transformed from the Noise-Intermediate Scale (NISQ) era to the Fault-Tolerant Quantum Computer (FTQC) era, in which the logical error rate from quantum calculations is acceptable. Therefore, it pursued the development of research in quantum computing. However, accessible quantum computers such as IBM quantum computers are limited, because of the high cost of access. To satisfy the growing demand for quantum computing simulations, several substantial researches have been conducted, as summarized in detail in Table I. Key criteria include physical hardware implementation, benchmarking tasks, the number of simulated qubits (#Qubits), precision, and evaluation metrics. Among existing quantum simulation software development kits (SDKs), the most well-known include Qiskit [8], ProjectQ [9], Cirq [10], and TensorFlow Quantum (TFQ) [11], which are developed in Python. These SDKs not only support a wide range of quantum simulation models with high #Qubits but also enable deployment on actual Quantum Processing Units (QPUs). Additional packages, such as QUBO [12], PennyLane [13], and cuQuantum [14], are also widely used due to their efficient performance on general-purpose processors including Central Processing Units (CPUs) and Graphics Processing Units (GPUs). Notably, cuQuantum is optimized specifically for NVIDIA GPUs. Another notable package is Qsun, a quantum simulation package proposed in [15], which introduces a Wave Function (WF) - based quantum simulation system to reduce the computational load. Overall, these quantum simulation packages exhibit flexible and increasingly fast performance on software platforms. However, as the demand for higher #Qubits grows, the reliance on 32- and 64-bit floating-point (FP) types and the general-purpose design of these software-based quantum simulators result in decreased processing speed and significant storage requirements. Consequently, these platforms tend to consume considerable energy without meeting speed requirements, an issue that is often overlooked in current research. In the future, if quantum simulations become widespread, software quantum simulators running on devices such as CPUs/GPUs, which consume between 150-350 W, will have a significant environmental impact. To solve the limitations of software, various quantum emulation systems have been developed for hardware platforms, particularly analog circuits and Field-Programmable Gate Arrays (FPGA). Specifically, analog-based emulators, introduced in [16], use CMOS analog circuits to represent real numbers naturally, which are suitable for simulating quantum states. Although analog circuits achieve higher energy efficiency, scalability, and simulation speed than software, they face limitations, when they only support algorithms like Grover’s Search Algorithm (GSA) and Quantum Fourier Transform (QFT) with lower fidelity due to noise and other unintended effects. In contrast, FPGA-based quantum emulators, introduced in [17, 18, 19, 20, 21, 22, 23, 24, 25, 26], offer a more practical and developable approach, while still achieving high speed and energy efficiency. However, these studies lack detailed fidelity evaluations, particularly when using 8888-bit and 16161616-bit fixed-point (FX) representations, which may not meet precision requirements. FPGA-based emulators are also limited by high storage and communication demands, restricting them to lower #Qubits. Moreover, these FPGA emulators have low flexibility, as they are optimized only for specific applications such as QFT, GSA, Quantum Haar Transform (QHT), and Quantum Support Vector Machine (QSVM). In general, existing related works continue to face specific challenges, including high energy consumption in quantum simulators, while hardware-based quantum emulators remain limited by low flexibility, reduced precision, and limited applicability due to implementation difficulties. Although they attempt to improve performance, most are constrained by the #Qubit they can simulate and lack detailed evaluations of parameters that reflect accuracy, such as fidelity or mean-square error (MSE). This severely impacts their quality and practicality. To address the aforementioned challenges, the FQsun quantum emulator is proposed to achieve optimal speed and energy efficiency. FQsun implements four innovative propositions: efficient memory organization, a configurable Quantum Gate Unit (QGU), optimal working scheduling, and multiple number precisions to maximize the flexibility, speed, and energy efficiency of the quantum emulator. FQsun is implemented on a Xilinx ZCU102 FPGA to demonstrate its effectiveness at the real-time SoC level. Through strict evaluation of execution time, accuracy (MSE, fidelity), power consumption, and power-delay product (PDP), FQsun will demonstrate its superiority over software quantum simulators running on powerful Intel CPUs and Nvidia GPUs, as well as existing hardware-based quantum emulators when performing various quantum simulation tasks. TABLE I: Surveyed results of related quantum simulators/emulators in recent five years Reference Devices Benchmarking tasks #Qubits Precision Comparison metrics Qiskit [8] CPUs, GPUs, QPUs Quantum Fourier Transform (QFT), Bernstein-Vazirani (BV), Random Clifford circuits, Hamiltonian simulation, Quantum Approximate Optimization Algorithm (QAOA) Depending on hardware resources 32-bit FP, 64-bit FP Execution time, Gate count, Gate depth, Fidelity ProjectQ [9] CPUs, GPUs, QPUs QFT, Shor’s Algorithm Depending on hardware resources 32-bit FP, 64-bit FP Execution time, Resource utilization, Circuit depth, Gate count, Fidelity Cirq [10] CPUs, GPUs, QPUs Random Quantum Circuits (RQC) Depending on hardware resources 32-bit FP Execution time, Fidelity TFQ [11] CPUs, GPUs, QPUs QAOA, Quantum Neural Network (QNN). Depending on hardware resources 32-bit FP, 64-bit FP Execution time, Fidelity, Gradient-based optimization QUBO [12] CPUs, GPUs QFT, Variational Quantum Eigensolver (VQE), Grover’s Search Algorithm (GSA), Adiabatic Time Evolution, Quantum Autoencoder, Quantum Classifier Depending on hardware resources 32-bit FP, 64-bit FP Execution time, Fidelity, Overlap PennyLane [13] CPUs, GPUs VQE, QAOA, Quantum Classification Depending on hardware resources 32-bit FP, 64-bit FP Execution time, Fidelity, Variational accuracy cuQuantum SDK [14] GPUs QFT, QAOA, Quantum Volume and Phase Estimation Depending on hardware resources 32-bit FP, 64-bit FP Execution time, Speedup factor, Memory bandwidth utilization Qsun [15] CPUs Quantum Linear Regression (QLR), QNN, Quantum Differentiable Programming (QDP) Depending on hardware resources 32-bit FP, 64-bit FP Execution time, Fidelity, MSE Analog-based emulator [16] UMC-180 nm CMOS Analog GSA, QFT 6 to 17 qubits 32-bit FP Execution time, Power consumption, Circuit compatibility ZCVU9P-based emulator [17] FPGA(Xilinx ZCVU9P) + CPU(Xeon™ E5-2686 v4) Quantum Support Vector Machine (QSVM), Quantum Kernel Estimation Up to 6 qubits 16-bit FX Execution time, Numerical accuracy, Test accuracy Alveo-based emulator [18] FPGA (Xilinx Alveo U250) Quantum Haar Transform (QHT), 3D-QHT 10 to 32 qubits (††\dagger†) 32-bit FP Execution time, Resource utilization, Circuit depth Arria-based emulator [19] FPGA (Arria 10AX115N4 F45E3SG) GSA Up to 32 qubits (††\dagger†) 16-bit FX Emulation time, Resource utilization, Frequency XCKU-based emulator [20] FPGA(Xilinx XCKU115) QFT Up to 16 qubits 16-bit FX Execution time Stratix-based emulator [21, 22] FPGA(Altera Stratix EP1S 80B956C6) QFT, GSA Up to 8 qubits 8-bit FX, 16-bit FX Logic cell usage, Emulation time, Frequency This work FQsun FPGA(Xilinx ZCU102) QFT, Parameter - Shift Rule (PSR) (renamed from QDP), RQC, QLR, QNN, etc. 3 to 17 qubits 16-bit FX, 16-bit FP, 24-bit FX, 32-bit FX, 32-bit FP Execution time, PDP, Fidelity, MSE, Power Hardware Utilization. ††{\dagger}† Qubit numbers marked as ”estimated” are based on projected performance given hardware constraints. The outline of this paper is organized as follows. Section II presents the background of the study. Next, details of the FQsun hardware architecture are presented in Section III. Then, benchmarking results, evaluation, and comparisons are elaborated upon in Section IV. Finally, Section V concludes the paper."
https://arxiv.org/html/2411.04270v1,Optimizing Multi-level Magic State Factoriesfor Fault-Tolerant Quantum Architectures,"We propose a novel technique for optimizing a modular fault-tolerant quantum computing architecture, taking into account any desired space–time trade-offs between the number of physical qubits and the fault-tolerant execution time of a quantum algorithm. We consider a concept architecture comprising a dedicated zone as a multi-level magic state factory and a core processor for efficient logical operations, forming a supply chain network for production and consumption of magic states. Using a heuristic algorithm, we solve the multi-objective optimization problem of minimizing space and time subject to a user-defined error budget for the success of the computation, taking the performance of various fault-tolerant protocols such as quantum memory, state preparation, magic state distillation, code growth, and logical operations into account. As an application, we show that physical quantum resource estimation reduces to a simple model involving a small number of key parameters, namely, the circuit volume, the error prefactors (μ𝜇\muitalic_μ) and error suppression rates (ΛΛ\Lambdaroman_Λ) of the fault-tolerant protocols, and an allowed slowdown factor (β𝛽\betaitalic_β). We show that, in the proposed architecture, 105superscript10510^{5}10 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT–108superscript10810^{8}10 start_POSTSUPERSCRIPT 8 end_POSTSUPERSCRIPT physical qubits are required for quantum algorithms with T𝑇Titalic_T-counts in the range 106superscript10610^{6}10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT–1015superscript101510^{15}10 start_POSTSUPERSCRIPT 15 end_POSTSUPERSCRIPT and logical qubit counts in the range 102superscript10210^{2}10 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT–104superscript10410^{4}10 start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT, when run on quantum computers with quantum memory ΛΛ\Lambdaroman_Λ in the range 3–10, for all slowdown factors β≥0.2𝛽0.2\beta\geq 0.2italic_β ≥ 0.2.","Fault-tolerant compilation of quantum algorithms is especially more complicated than that of classical computer programs because the final physical circuit depends on the specific noise characteristics of the quantum processing unit (QPU). It is commonly understood that the number of non-Clifford logical operations (i.e., the T𝑇Titalic_T-count of the algorithm) is a good indicator of the approximate cost of running the quantum algorithm. However, assembling the quantum program to run in hours, days, or even months of exact physical circuits on a quantum computer with millions of qubits is much more complicated. Recently, several physical resource estimation platforms have been developed [1, 2] to provide a finer analysis of fault-tolerant quantum computation (FTQC) at utility scale. However, these resource estimators rely on involved bookkeeping of physical qubit counts for various procedures, and therefore are not easily adaptable to alternative circuit decompositions (i.e., beyond Clifford+T𝑇Titalic_T, e.g., by including Toffoli gates), physical architectures (e.g., for using different state distillation and injection protocols), or the various and possibly very complicated descriptions of the noise of the hardware. This paper is intended to provide a systematic and simple approach to FTQC architecture design, analysis, and optimization. We present a framework for constructing flexible varieties of FTQC architectures in a modular fashion by allocating dedicated zones across a 2D physical qubit layout for the execution of specific fault-tolerant protocols. To this end, we view FTQC as the continual production and consumption of various types of expensive quantum resources, such as the production of lower-fidelity logical |T⟩ket𝑇\ket{T}| start_ARG italic_T end_ARG ⟩ or |C⁢C⁢Z⟩ket𝐶𝐶𝑍\ket{CCZ}| start_ARG italic_C italic_C italic_Z end_ARG ⟩ states in one zone and their consumption in another zone for obtaining higher-fidelity magic states via a distillation protocol [3]. We consider rotated surface codes as the typical choice of quantum error correction (QEC) codes, and rely on lattice surgery for the execution of entangling gates and seamless routing and teleportation of quantum information across a quantum bus [4, 5]. At utility scale, it is economical to dedicate small numbers of physical code patches as buffer registers connecting the FTQC architecture zones to each other. Newly produced resources in one zone are placed in a buffer, where they are fault-tolerantly maintained via continued stabilizer measurements and decoding (quantum memory) for consumption in a subsequent zone. One or many terminal core processors consume the final highest-fidelity resources to perform the logical gates of the quantum program. We refer the reader to Section II and Figs. 2 and 3 for further details. (a) Space-time costs for varying quantum circuits (b) Space-time costs for varying quantum hardware Figure 1: Space and time cost estimates of an efficient fault-tolerant quantum architecture. (a) Estimates for combinations of T𝑇Titalic_T-count T=106𝑇superscript106T=10^{6}italic_T = 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT to 1015superscript101510^{15}10 start_POSTSUPERSCRIPT 15 end_POSTSUPERSCRIPT and logical data qubits Q=102𝑄superscript102Q=10^{2}italic_Q = 10 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT to 104superscript10410^{4}10 start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT in a quantum circuit, based on an FTQC model with a quantum memory error prefactor of μmem=3.8×10−3subscript𝜇mem3.8superscript103\mu_{\text{mem}}=3.8\times 10^{-3}italic_μ start_POSTSUBSCRIPT mem end_POSTSUBSCRIPT = 3.8 × 10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT and an error suppression rate of Λmem=10subscriptΛmem10\Lambda_{\text{mem}}=10roman_Λ start_POSTSUBSCRIPT mem end_POSTSUBSCRIPT = 10, and a magic state preparation protocol with a linear form with μprep=1.44×10−4subscript𝜇prep1.44superscript104\mu_{\text{prep}}=1.44\times 10^{-4}italic_μ start_POSTSUBSCRIPT prep end_POSTSUBSCRIPT = 1.44 × 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT and Λprep=2.5×10−5subscriptΛprep2.5superscript105\Lambda_{\text{prep}}=2.5\times 10^{-5}roman_Λ start_POSTSUBSCRIPT prep end_POSTSUBSCRIPT = 2.5 × 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT. (b) Estimates for combinations of Λmem=3subscriptΛmem3\Lambda_{\text{mem}}=3roman_Λ start_POSTSUBSCRIPT mem end_POSTSUBSCRIPT = 3 to 10101010 and a magic state preparation protocol with Λprep=0subscriptΛprep0\Lambda_{\text{prep}}=0roman_Λ start_POSTSUBSCRIPT prep end_POSTSUBSCRIPT = 0 and μprep=eprep=10−2subscript𝜇prepsubscript𝑒prepsuperscript102\mu_{\text{prep}}=e_{\text{prep}}=10^{-2}italic_μ start_POSTSUBSCRIPT prep end_POSTSUBSCRIPT = italic_e start_POSTSUBSCRIPT prep end_POSTSUBSCRIPT = 10 start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT to 10−8superscript10810^{-8}10 start_POSTSUPERSCRIPT - 8 end_POSTSUPERSCRIPT for a circuit with T=1012𝑇superscript1012T=10^{12}italic_T = 10 start_POSTSUPERSCRIPT 12 end_POSTSUPERSCRIPT and Q=102𝑄superscript102Q=10^{2}italic_Q = 10 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. Each data point corresponds to a slowdown factor β𝛽\betaitalic_β, relative to a serial execution (β=1𝛽1\beta=1italic_β = 1) of the quantum circuit. A value of β<1𝛽1\beta<1italic_β < 1 reflects a scenario where the magic state factory (MSF) is expanded to meet the magic state consumption rate when parallelizing non-Clifford gates, resulting in faster execution. Conversely, β>1𝛽1\beta>1italic_β > 1 indicates a reduction in MSF size, leading to a longer runtime. While runtime is heavily influenced by the number of non-Clifford gates in the circuit and the MSF size, our estimates demonstrate that quantum computers require between 105superscript10510^{5}10 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT and 108superscript10810^{8}10 start_POSTSUPERSCRIPT 8 end_POSTSUPERSCRIPT physical qubits across all scenarios evaluated. The buffer registers provide several advantages. Practically speaking, they allow for scheduling operations within each zone independently and asynchronously from the other zones. Moreover, since fault-tolerant protocols typically fail with certain non-zero probability, keeping the buffers fully stacked with their associated resource states guarantees that the operations of any dependent zones do not halt since the probability of encountering a completely empty buffer is extremely small. More generally, the oversupply and undersupply of resource states in a buffer can be intentionally used to adjust the space–time trade-offs for FTQC. For example, a magic state factory (MSF) may require many magic state distillation units to keep its buffer fully stacked, while it is possible for a smaller number of distillation units to still be able to fault-tolerantly execute the quantum algorithm if some slowdown caused by occasionally empty buffer registers can be tolerated. The key idea in this work is to determine the size of each FTQC architecture zone by choosing the number of FTQC protocol units (e.g., 15:1 magic state distillation units, or the like) in that zone in such a way that the supply and demand of the interconnected zones are balanced. This balance, however, can be broken, for the assembly of the program using fewer physical qubits by incurring a longer execution time. However, reducing the (logical) size of the MSF increases the idle time in the core processor, which increases the overall accumulated error rates, potentially requiring larger code distances. Our study provides a detailed analysis of such trade-offs. We model the optimization of space and time as a bi-objective optimization problem in Section III. By solving this optimization problem, we observe that the space and time costs of any FTQC can be efficiently predicted from a small set of key attributes of the quantum program and the quantum computer executing it, and the potential space–time trade-offs for the architecture can be identified. For clarity we distinguish FTQC compilation and assembly as follows. We assume that the quantum program is given in an intermediate representation (IR) language, also called assembly, after compilation from a higher-level programming environment. Examples of common quantum IR languages are QASM [6] and QIR [7]. However, in this paper, we focus on translation at a lower level from assembly to machine instructions (i.e., QEC rounds and decoder instructions) performed by an assembler. Different IR programs may be transpiled into each other efficiently [5, 4]. Therefore, we assume Pauli-based lattice surgery as our IR of choice. For the assembler, the two relevant attributes of the FTQC program are denoted by the Greek small letters α𝛼\alphaitalic_α and β𝛽\betaitalic_β, representing the average size of lattice surgeries in the core processor relative to the number of logical qubits and a user-defined slowdown factor within the buffers, respectively. Our empirical studies showed that α𝛼\alphaitalic_α is not a significant factor for the assembly process, so we may omit it to obtain an even simpler model depending on only the slowdown factor β𝛽\betaitalic_β. The noise profile of the QPU is also important for the assembler. We observe that this information can be reduced to a small number of parameters for a predictive model of the logical error rates of each FTQC protocol, namely μ𝜇\muitalic_μ and ΛΛ\Lambdaroman_Λ, respectively representing the logical error prefactor and suppression rate of the protocol. Prior literature has used Λ=ΛmemΛsubscriptΛmem\Lambda=\Lambda_{\text{mem}}roman_Λ = roman_Λ start_POSTSUBSCRIPT mem end_POSTSUBSCRIPT to quantify the performance of the quantum hardware in a fault-tolerant quantum memory experiment on a single surface code patch [8]. ΛmemsubscriptΛmem\Lambda_{\text{mem}}roman_Λ start_POSTSUBSCRIPT mem end_POSTSUBSCRIPT describes the asymptotic multiple of improvement in the logical fidelity of the QEC code when increasing its distance by 2. More comprehensively, a predictive model for the logical error rate of every fault-tolerant protocol can be devised using additional model parameters. Figure 1 provides a summary of the resource estimates for quantum circuits with varying T𝑇Titalic_T-counts, logical qubit counts Q𝑄Qitalic_Q, and quantum computers with varying ΛmemsubscriptΛmem\Lambda_{\text{mem}}roman_Λ start_POSTSUBSCRIPT mem end_POSTSUBSCRIPT and first-stage preparation error rate eprepsubscript𝑒prepe_{\text{prep}}italic_e start_POSTSUBSCRIPT prep end_POSTSUBSCRIPT. Overall, the main contributions of our paper are the following: • A new modular architecture design procedure for FTQC with at least a multi-level MSF and a core processor; • An optimization framework that decides the size required for the MSF and the code distances of all logical qubits while minimizing space and time costs; and • A sensitivity analysis of space and time trade-offs for different quantum circuits and quantum computers. The paper is organized as follows. Section II introduces our proposed architecture. Section III introduces the error models driven from numerical and experimental hardware noise data. Section IV formalizes the bi-objective optimization problem to be solved and the optimization framework we used to solve the problem. Section V presents our numerical results for trading resource estimations on varying quantum circuits and quantum hardware noise profiles. Finally, Section VI offers concluding remarks."
https://arxiv.org/html/2411.03857v1,Efficient Message Passing Architecture for GCN Training on HBM-based FPGAs with Orthogonal Topology On-Chip Networks,"Graph Convolutional Networks (GCNs) are state-of-the-art deep learning models for representation learning on graphs. However, the efficient training of GCNs is hampered by constraints in memory capacity and bandwidth, compounded by the irregular data flow that results in communication bottlenecks. To address these challenges, we propose a message-passing architecture that leverages NUMA-based memory access properties and employs a parallel multicast routing algorithm based on a 4-D hypercube network within the accelerator for efficient message passing in graphs. Additionally, we have re-engineered the backpropagation algorithm specific to GCNs within our proposed accelerator. This redesign strategically mitigates the memory demands prevalent during the training phase and diminishes the computational overhead associated with the transposition of extensive matrices. Compared to the state-of-the-art HP-GNN architecture we achieved a performance improvement of 1.03×∼1.81×1.03\times\sim 1.81\times1.03 × ∼ 1.81 ×.","GCNs serve as a leading deep learning for graph and have found advanced applications in recommendation systems (Zhu et al., 2019), transportation networks (Derrow-Pinion et al., 2021), and drug discovery. Graph-based deep learning has become the mainstream model, fueling the field’s rapid growth. With the proliferation of applications and increased graphs, training graph neural networks have become inefficient and face several challenges. Firstly, unlike inference, training requires retaining the output from different layers during forward to calculate backpropagation. However, single-machine accelerators, such as GPU and FPGA, cannot provide sufficient out-chip storage and bandwidth for training as the graph increases. To mitigate this issue, GraphSage (Hamilton et al., 2017), GraphSAINT (Zeng et al., 2019), VRGCN (Chen et al., 2017), and FastGCN (Chen et al., 2018) have introduced sampling-based mini-batch training methods make training large graphs feasible under this computation paradigm. Secondly, GCN is sensitive to the order of execution. Previous work on inference accelerators (Geng et al., 2020; Zhou et al., 2021; Zhang et al., 2021; Liang et al., 2020; Li et al., 2021a) has explored this issue. However, in training, backpropagation is performed in the reverse order of forward, so optimizing the computing order of forward propagation only benefits some models. Additionally, during the training process, there is a significant need for matrix transposition, which adds complexity to optimizing the dataflow. It requires a holistic consideration of computation and storage (Guirado et al., 2021). Thirdly, high communication overhead caused by frequent node transmission. In single-machine setups, a centralized shared Unified Memory Architecture (UMA) structure is commonly used in the Symmetric Multi-Processing (SMP) model. However, during graph aggregation, on-chip computing units generate multiple random accesses in the shared memory, leading to tough competition and longer latency for local computing resources (Yang et al., 2022; Tian et al., 2022; Li et al., 2021b). Resulting in lower average utilization of computing resources (Abadal et al., 2021; Yuan et al., 2022; Wang et al., 2021). Addressing these challenges requires efficient communication schemes and memory management strategies in GNN training. Fourthly, High-bandwidth memory (HBM) plays a crucial role in facilitating ample bandwidth for neural network training on FPGAs. In particular, HBM has been shown to provide significant bandwidth (Wissolik et al., 2017; Holzinger et al., 2021), surpassing traditional DDR4 channels by a magnitude on FPGA platforms (Hu et al., 2021; Kang et al., 2022). However, when applying Graph Neural Network (GNN) computations, we encounter diverse memory access patterns, with the combination phase involving sequential address access and extended burst lengths, while the aggregation phase exhibits random memory access with shorter burst lengths. This poses a notable challenge in implementing a scalable GNN training architecture on an HBM-based FPGA system. We need to mitigating bandwidth reduction caused by multiple AXI ports accessing the same pseudo-channel during aggregation. Our main contributions are as follows: (1) We propose a dedicated Multi-Cores GNN training accelerator for high-performance message passing on HBM-based FPGAs with NUMA-based memory access. The on-chip networks of the accelerator adopt a strictly orthogonal hypercube topology, and design a highly concurrent routing mechanism for GNN applications. (2) We redesigned the dataflow of GNN backpropagation in the FPGA. It can decrease the off-chip memory requirements during training while reducing additional matrix transpose. (3) We evaluate the performance on Xilinx UltraScale+ VCU128. In comparison to the state-of-the-art GNN training architecture HP-GNN(Lin et al., 2022), we achieved a improvement of 1.03×\times× ∼similar-to\sim∼ 1.81×\times×."
https://arxiv.org/html/2411.03832v1,DART-PIM: DNA read mApping acceleRaTor Using Processing-In-Memory,"Genome analysis has revolutionized fields such as personalized medicine and forensics. Modern sequencing machines generate vast amounts of fragmented strings of genome data called reads. The alignment of these reads into a complete DNA sequence of an organism (the read mapping process) requires extensive data transfer between processing units and memory, leading to execution bottlenecks. Prior studies have primarily focused on accelerating specific stages of the read-mapping task. Conversely, this paper introduces a holistic framework called DART-PIM that accelerates the entire read-mapping process. DART-PIM facilitates digital processing-in-memory (PIM) for an end-to-end acceleration of the entire read-mapping process, from indexing using a unique data organization schema to filtering and read alignment with an optimized Wagner–Fischer algorithm. A comprehensive performance evaluation with real genomic data shows that DART-PIM achieves a 5.7×\boldsymbol{5.7\times}bold_5.7 bold_× and 𝟐𝟓𝟕×\boldsymbol{257\times}bold_257 bold_× improvement in throughput and a 𝟗𝟐×\boldsymbol{92\times}bold_92 bold_× and 𝟐𝟕×\boldsymbol{27\times}bold_27 bold_× energy efficiency enhancement compared to state-of-the-art GPU and PIM implementations, respectively.","Genome analysis serves as the cornerstone of numerous scientific and medical breakthroughs, playing a vital role in personalized medicine [33, 23, 11, 41] and advanced forensics [116, 18]. Genome sequencing machines [51, 52, 50, 86] produce vast volumes of short random fragmented strings of bases (A, C, G, T in DNA), commonly referred to as reads, which are extracted from the longer original DNA [4]. The process of reconstructing the original DNA sequence begins by approximating the location of each read in the overall genome and then using the reads at each location to deduce the most likely reference base. This procedure involves complex algorithms with high memory requirements that constitute a bottleneck to the entire genome analysis process. Sequence alignment is a popular approach that localizes read fragments based on their similarity to a corresponding reference genome of the target species. This approach is feasible thanks to the high degree (above 99%percent9999\%99 %) of resemblance between genomes of different specimens within the same species. This computational process, called read mapping [4, 21], involves offline indexing of the reference genome (utilizing short representative fragments known as minimizers [93]), followed by three consecutive online steps (illustrated in Figure 1): (1) Seeding of potential locations (PLs) in the reference genome based on read and reference minimizer similarity; (2) pre-alignment filtering to eliminate false PLs; and (3) read alignment that determines the most probable PL according to a similarity metric. This latter stage is the most computationally intensive task, invoking string matching algorithms [40, 105, 80] for each read and PL within the complete genome. Figure 1: Genome sequence alignment process. The genomic samples are input into a sequencing machine that fragments them into small segments. The sequencing machine generates short strings known as reads, which are then processed during the read-mapping procedure. Read mapping involves an offline indexing stage followed by the online seeding, pre-alignment filtering, and read alignment stages. State-of-the-art read alignment techniques predominantly employ a dynamic programming (DP)-based approach, such as the Needleman-Wunsch or Smith-Waterman algorithms [68, 81, 97]. The complexity of these algorithms follows quadratic scaling in execution time and memory usage, thereby generating a significant latency and energy bottleneck [6, 4, 48, 79, 104]. This read-mapping bottleneck has been exacerbated by the recent enhancements of read generation rates on the one hand and insufficient corresponding growth in computational power on the other [4, 42]. To bridge the gap between read volumes and compute capabilities, prior work focused mainly on accelerating each read-mapping stage separately. This effort often prioritizes optimizing read alignment due to its significant computational demands, which can consume over 70% of the read-mapping execution time [19, 36, 79, 26, 60, 61, 20]. Read mapping, however, involves not only the processing of huge datasets, but also the frequent movements of huge volumes of data between the processor(s) and memory units, which increase both execution time and energy consumption [77, 4]. Since the data being transferred throughout the read-mapping process is roughly 100×100\times100 × larger than the original input reads, the acceleration of computational tasks across the process is still bottlenecked by the data transfer between the processes. Numerous studies have proposed alternative approaches to efficiently process large amounts of reads [19, 36, 79, 26, 60, 61, 20]. For instance, the state-of-the-art read mapper SeGraM [20] leverages near-memory computing [91] within a 3D-stacked memory architecture to mitigate costly chip-to-chip data transfers. Consequently, SeGraM’s read alignment process alone achieves a 144×144\times144 × speedup over existing CPU-executed software read mappers, alongside a 4.4×4.4\times4.4 × power consumption reduction. Nevertheless, when handling the entire read-mapping process, SeGraM’s performance demonstrates only a 2.5×2.5\times2.5 × speedup for short reads over the existing CPU-based read mappers. A promising solution to overcoming the data transfer bottleneck involves conducting read-mapping computations directly within the memory. This approach utilizes digital processing-in-memory (PIM) techniques to enable high parallelism, eliminating the need for frequent data transfer between the memory and the processor [78]. This paper presents DART-PIM, a DNA read mApping acceleRaTor using Processing-In-Memory, which is a comprehensive framework for accelerating the entire read-mapping process. The uniqueness of DART-PIM lies in its ability to execute all read-mapping stages for a given read and PL inside a single memory crossbar array. The majority of computations are performed using the memory cells, thus circumventing the data transfer bottleneck. A small number of complementary operations are executed in adjacent RISC-V cores. A key element for efficient computation in DART-PIM is careful organization of the reads and the reference genome within memory instances to minimize data transfer along all steps of read mapping. Within these instances, the data allocation is optimized for maximal utilization of the inherent massive parallelism offered by PIM. Consequently, DART-PIM greatly enhances the performance and energy efficiency of the entire end-to-end read-mapping process. This paper makes the following main contributions: 1. We propose a novel end-to-end accelerator architecture for the entire read-mapping process. 2. We present a data organization technique for accelerated indexing and seeding of a reference genome. 3. We develop a high-performance, memory efficient, in-memory pre-alignment filtering mechanism based on the linear Wagner-Fischer algorithm [107]. 4. We improved read alignment in-memory performance by enhancing the Wagner–Fischer algorithm with an affine-gap penalty and traceback capability. 5. We build system-level simulators that use real genome datasets to ensure precise evaluation of DART-PIM’s accuracy and performance, and compare it to state-of-the-art implementations (NVIDIA Parbricks [85] and SeGraM [20]), demonstrating faster execution time (5.7×5.7\times5.7 × and 257×257\times257 × improvement, respectively) and improved energy efficiency (92×92\times92 × and 27×27\times27 ×, respectively)."
https://arxiv.org/html/2411.03697v1,TATAA: Programmable Mixed-Precision Transformer Acceleration with a Transformable Arithmetic Architecture,"Modern transformer-based deep neural networks present unique technical challenges for effective acceleration in real-world applications. Apart from the vast amount of linear operations needed due to their sizes, modern transformer models are increasingly reliance on precise non-linear computations that make traditional low-bitwidth quantization methods and fixed-dataflow matrix accelerators ineffective for end-to-end acceleration. To address this need to accelerate both linear and non-linear operations in a unified and programmable framework, this paper introduces TATAA. TATAA employs 8-bit integer (int8) arithmetic for quantized linear layer operations through post-training quantization, while it relies on bfloat16 floating-point arithmetic to approximate non-linear layers of a transformer model. TATAA hardware features a transformable arithmetic architecture that supports both formats during runtime with minimal overhead, enabling it to switch between a systolic array mode for int8 matrix multiplications and a SIMD mode for vectorized bfloat16 operations. An end-to-end compiler is presented to enable flexible mapping from emerging transformer models to the proposed hardware. Experimental results indicate that our mixed-precision design incurs only 0.14 %times0.14percent0.14\text{\,}\mathrm{\char 37\relax}start_ARG 0.14 end_ARG start_ARG times end_ARG start_ARG % end_ARG to 1.16 %times1.16percent1.16\text{\,}\mathrm{\char 37\relax}start_ARG 1.16 end_ARG start_ARG times end_ARG start_ARG % end_ARG accuracy drop when compared with the pre-trained single-precision transformer models across a range of vision, language, and generative text applications. Our prototype implementation on the Alveo U280 FPGA currently achieves 2935.22935.22935.22935.2 GOPS throughput on linear layers and a maximum of 189.5189.5189.5189.5 GFLOPS for non-linear operations, outperforming related works by up to 1.45×1.45\times1.45 × in end-to-end throughput and 2.29×2.29\times2.29 × in DSP efficiency, while achieving 2.19×2.19\times2.19 × higher power efficiency than modern NVIDIA RTX4090 GPU.","Since its introduction in 2017, the Transformer model [1] and its variations have rapidly risen to the forefront of modern deep learning architectures. Unlike previous-generation convolutional neural networks (CNNs) that were based predominantly on linear operations, modern transformer models are increasingly reliance on high-precision non-linear operations in their designs. For instance, the self-attention mechanism of a transformer model is typically based on the SoftMax function, which has been demonstrated to require high precision computation in order to achieve a model’s accuracy [2]. Normalization layers such as LayerNorm [3] or root mean square normalization (RMSNorm) [4], require complex nonlinear operations on data that cannot easily be fused into preceding linear layers. Finally, sophisticated activation functions such as GELU [5], SiLU [6] and SwiGLU [7] are often used in transformer models which require precise computation, unlike in CNNs. Figure 1: Illustration of a typical Transformer block, and how TATAA maps different operations in Transformers including linear MatMul and a variety of non-linear functions into transformable architecture, based on a general end-to-end framework design. To address the need to approximate these non-linear functions with high precision and high performance, specialized hardware modules have previously been extensively explored [8, 9, 10, 11]. Yet, customized hardware must be designed for every non-linear function that is being employed in a model, which is impractical when new non-linear operations for transformers are still actively being developed in this rapidly-evolving field [12, 13]. Other researchers have focused on quantizing such non-linear functions into low-bitwidth fixed point formats in order to reduce the computation complexity [14, 15, 16, 17]. Due to the outliers in transformers [18, 19], retraining is generally required for such quantization to maintain good accuracy. However, the large size of modern transformer models, data availability and privacy concerns, have all made such retraining method impractical in most real-world scenarios. Besides, existing accelerators either rely on individual and specific non-linear function units [20, 21, 22], or attempt to handle non-linear functions with general arithmetic logic units [17]. Both strategies often lead to increased hardware overhead, reduced hardware efficiency, and it complicates the workload balance between linear and non-linear layers. Instead, to improve future compatibility, a general-purpose transformer accelerator that can be reprogrammed to support new non-linear functions in floating-point arithmetic with low hardware overhead is highly desirable. In this paper, we present TATAA, a novel end-to-end framework for flexible and quantized Transformer Acceleration using a Transformable Arithmetic Architecture that supports both floating-point and integer operations (Figure 1). In TATAA, static post-training quantization (PTQ) is performed on the linear layers of a transformer model to facilitate 8888-bit integer (int8) operations. On the other hand, non-linear layers are performed using high-precision bfloat16 operations. In contrast to certain previous efforts that reduced data bitwidths in nonlinear layers, we alleviate the need for retraining by maintaining high bitwidth data formats in these layers, all the while ensuring high efficiency during execution. To support both int8 and bfloat16 operations efficiently, TATAA hardware architecture consists of dual-mode processing units (DMPUs) that feature configurable arrays of integer processing elements (PE). The proposed architecture can be transformed between a systolic array for int8 matrix multiplications and a SIMD-like vectorized bfloat16 computing unit. In particular, the proposed TATAA architecture employs a single type of processing unit, which is reused for all run-time operations, leveraging the bit field patterns of bfloat16. This design choice minimizes hardware overhead and maximizes flexibility compared to previous studies. By minimizing the overhead for run-time reconfiguration, the proposed transformable architecture ensures the high hardware processing density necessary to deliver the highest performance on FPGAs with limited resources. Finally, a compilation framework is developed that maps the user-provided transformer models to the custom instruction set architecture (ISA) of the TATAA processor cores to facilitate all operations in both linear and non-linear layers. To the best of our knowledge, TATAA is the first FPGA-based acceleration framework for transformer inference that integrates floating-point non-linear functions into integer-based linear processing units. It is programmable and is ready to support emerging transformer models with potentially new non-linear functions. Our experimental results indicate that when simulating model performance with a hybrid data format for transformer inference, TATAA achieves only a minimal accuracy reduction, with 0.14 %times0.14percent0.14\text{\,}\mathrm{\char 37\relax}start_ARG 0.14 end_ARG start_ARG times end_ARG start_ARG % end_ARG to 1.16 %times1.16percent1.16\text{\,}\mathrm{\char 37\relax}start_ARG 1.16 end_ARG start_ARG times end_ARG start_ARG % end_ARG decrease across all evaluated models compared to the original pre-trained model in single-precision floating point (fp32). Additionally, the FPGA accelerator reaches a peak throughput of 2935.22935.22935.22935.2 giga-operations-per-second (GOPS) for int8 linear operations and a peak throughput of 169.8169.8169.8169.8 giga-floating-point-operations-per-second (GFLOPS) when the processor is configured for bfloat16 non-linear operations at a clock frequency of 225 MHztimes225megahertz225\text{\,}\mathrm{MHz}start_ARG 225 end_ARG start_ARG times end_ARG start_ARG roman_MHz end_ARG. Compared to related studies, TATAA achieves up to 1.45×1.45\times1.45 × higher throughput and 2.29×2.29\times2.29 × higher throughput efficiency on DSP blocks. With the transformable architecture for non-linear functions, our implementation achieves 4.25×4.25\times4.25 × lower latency for these complex bfloat16 operations compared with other works, while supporting flexible and general compilation for emerging functions. Our end-to-end compilation framework also presents optimal mapping from non-linear functions to hardware ISA by appropriate approximation schemes and efficient dataflow control. Moreover, compared to state-of-the-art GPUs, our TATAA architecture outperforms a maximum 2.19×2.19\times2.19 × higher power efficiency over a variety of transformer models. This prototype underscores the potential of the TATAA approach for expediting transformer models and sets the stage for future optimization at the microarchitectural level, while our extensive compilation flow opens up a significant optimization space for non-linear functions and to quickly adapt to future transformer-based model as they are being developed."
https://arxiv.org/html/2411.03471v1,MetRex: A Benchmark for Verilog Code Metric Reasoning Using LLMs,"Large Language Models (LLMs) have been applied to various hardware design tasks, including Verilog code generation, EDA tool scripting, and RTL bug fixing. Despite this extensive exploration, LLMs are yet to be used for the task of post-synthesis metric reasoning and estimation of HDL designs. In this paper, we assess the ability of LLMs to reason about post-synthesis metrics of Verilog designs. We introduce MetRex, a large-scale dataset comprising 25,868 Verilog HDL designs and their corresponding post-synthesis metrics, namely area, delay, and static power. MetRex incorporates a Chain of Thought (CoT) template to enhance LLMs’ reasoning about these metrics. Extensive experiments show that Supervised Fine-Tuning (SFT) boosts the LLM’s reasoning capabilities on average by 37.0%, 25.3%, and 25.7% on the area, delay, and static power, respectively. While SFT improves performance on our benchmark, it remains far from achieving optimal results, especially on complex problems. Comparing to state-of-the-art regression models, our approach delivers accurate post-synthesis predictions for 17.4% more designs (within a 5% error margin), in addition to offering a 1.7x speedup by eliminating the need for pre-processing. This work lays the groundwork for advancing LLM-based Verilog code metric reasoning.","Recent advancements in Large Language Models (LLMs) have demonstrated remarkable potential to transform the field of hardware design, across a wide range of tasks such as Verilog code generation (pearce2020dave, ; thakur2023benchmarking, ; Liu2023verilog, ; lu2024rtllm, ; thakur2023verigen, ), EDA tools scripting (wu2024chateda, ), designing AI accelerators (fu2023gpt4aigchip, ), and fixing RTL syntax errors (tsai2023rtlfixer, ). However, an area yet to be explored is the application of LLMs for reasoning and estimation of post-synthesis metrics of HDL designs. Given HDL code as input, LLMs could potentially infer gate-level details and estimate key metrics, such as area, delay, and static power. While current LLMs can generate raw Verilog code, they lack awareness of post-synthesis metrics and struggle to reason about them effectively. Prior works utilized LLMs to tweak Verilog code to meet area, delay, and power requirements using prompting methods (thorat2023advanced, ) or search methods like Monte-Carlo tree search (yao2024rtlrewriter, ). However, these approaches mainly focus on refining Verilog code, and they do not fundamentally enhance the LLM’s understanding of how different design choices impact post-synthesis metrics. Thus, there is a need for approaches that empower LLMs with deeper insights into the underlying relationships between HDL code and post-synthesis metrics. In light of this, we introduce MetRex, an LLM-based framework for high level metric estimation of HDL designs. MetRex encompasses a large-scale dataset of 25,8682586825,86825 , 868 HDL designs, each annotated with post-synthesis metrics on area, delay, and static power. To enhance the LLM’s capability to understand and reason about these metrics, we propose a Chain of Thought (CoT) template that details the logical steps necessary for computing these metrics. To the best of our knowledge, MetRex is the first framework that addresses the task of LLM-based code analysis for metric estimation of HDL designs. Our contributions are summarized as follows: • We introduce a new dataset, MetRex, for benchmarking Large Language Models (LLMs) for the task of reasoning about post-synthesis metrics of HDL designs. The dataset comprises 25,868 Verilog designs, each annotated with area, delay, and static power metrics. • We developed an automated flow using a Verilog compiler, a synthesis tool, and an LLM agent to detect and resolve syntax and synthesis errors, ensuring a dataset of clean, synthesizable designs. • We introduce a Chain of Thought (CoT) prompting technique that improves the LLM’s reasoning and understanding of post-synthesis metrics by 5.1%, 5.4%, and 8.9% on the area, delay, and static power metrics, respectively, compared to direct prompting methods. • We employ the MetRex dataset in extensive Supervised Fine-Tuning (SFT) experiments, demonstrating that SFT can significantly improve the LLM performance in reasoning and estimating post-synthesis metrics on average by 37.0%, 25.3%, and 25.7% on the area, delay, and static power, compared to few-shot prompting techniques. • We compare the LLM estimation accuracy to regression-based models (wenj2023masterrtl, ), highlighting their potential for this task in offering insightful and direct analysis of HDL code without the need for intermediary formats. LLMs improve the rate of obtaining accurate estimates within a 5% error margin by 17.4% while offering 1.7x faster analysis by eliminating the need for feature extraction and pre-processing. This paper is organized as follows. Section 2 discusses related work. Section 3 presents a general problem formulation of the metric reasoning task with LLMs. Section 4 discusses the MetRex dataset. Section 5 presents experimental results. Section 6 discusses current limitations and future directions. Finally, Section 7 concludes the paper."
https://arxiv.org/html/2411.03398v1,DP-HLS: A High-Level Synthesis Framework for Accelerating Dynamic Programming Algorithms in Bioinformatics,"Dynamic programming (DP) based algorithms are essential yet compute-intensive parts of numerous bioinformatics pipelines, which typically involve populating a 2-D scoring matrix based on a recursive formula, optionally followed by a traceback step to get the optimal alignment path. DP algorithms are used in a wide spectrum of bioinformatics tasks, including read assembly, homology search, gene annotation, basecalling, and phylogenetic inference. So far, specialized hardware like ASICs and FPGAs have provided massive speedup for these algorithms. However, these solutions usually represent a single design point in the DP algorithmic space and typically require months of manual effort to implement using low-level hardware description languages (HDLs). This paper introduces DP-HLS, a novel framework based on High-Level Synthesis (HLS) that simplifies and accelerates the development of a broad set of bioinformatically relevant DP algorithms in hardware. DP-HLS features an easy-to-use template with integrated HLS directives, enabling efficient hardware solutions without requiring hardware design knowledge. In our experience, DP-HLS significantly reduced the development time of new kernels (months to days) and produced designs with comparable resource utilization to open-source hand-coded HDL-based implementations and performance within 7.7–16.8% margin. DP-HLS is compatible with AWS® EC2 F1 FPGA instances. To demonstrate the versatility of the DP-HLS framework, we implemented 15 diverse DP kernels, achieving 1.3–32×\times× improved throughput over state-of-the-art GPU and CPU baselines and providing the first open-source FPGA implementation for several of them. The DP-HLS codebase is available freely under the MIT license at https://github.com/TurakhiaLab/DP-HLS and its detailed wiki at https://turakhia.ucsd.edu/DP-HLS/.","Genomic data is one of the fastest-growing data types globally, far outpacing Moore’s law in terms of data generation [1]. To meet the rising computational demands of analyzing and interpreting this data, several efforts have focused on accelerating bioinformatics applications on hardwares like GPUs, FPGAs, and ASICs [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]. While a lot of these accelerators are custom solutions to target a narrow application in bioinformatics, they also share notable similarities. For example, many of these solutions accelerate an algorithm based on dynamic programming (DP) [26]. This is unsurprising, as DP provides an efficient framework for comparing biological sequences—such as DNA, RNA, proteins, or even electrical signals from sequencing instruments—which is fundamental to many bioinformatics tasks, such as local pairwise alignments [27, 28], multiple sequence alignment [29, 30], homology searches [31, 32], whole-genome alignments [33], basecalling [34], and variant calling [35]. DP algorithms are computationally intensive, and therefore, they often dominate the runtime of these applications [36]. Recognizing their importance to bioinformatics, NVIDIA® recently introduced a specialized instruction, DPX, specifically to accelerate DP algorithms on GPUs [37]. Another key characteristic, particularly in FPGA and ASIC solutions [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], is the use of a hardware primitive—linear systolic arrays—that has been recognized since the 1980s for its efficiency in accelerating DP algorithms [38]. Most of these solutions focus on a specific or narrow set of 2-D DP algorithms and are typically designed at the Register Transfer Level (RTL) using low-level Hardware Description Languages (HDLs) like Verilog or VHDL, which makes them difficult to design and modify. A similar observation was made by a recent work, GenDP [39], which proposed a linear systolic array with software-programmable processing elements to accelerate a broad range of DP algorithms. However, software-programmable solutions introduce significant overhead on circuit-programmable FPGAs, which are the primary target of our work since they have already found commercial applications in bioinformatics [40, 41, 42]. Fig. 1. Common variations in 2-D DP algorithms in Bioinformatics. Kernels are indexed using #’s based on Table 1. In this paper, we present DP-HLS, a novel framework based on High-Level Synthesis (HLS) for accelerating broad DP kernels on FPGAs. A key feature of the framework is the separation of back-end optimizations from the front-end interface, enabling users to define new kernels in C++ without needing expertise in HLS or digital design. DP-HLS makes the following key contributions: (1) We developed DP-HLS, an HLS framework that, by separating front-end and back-end, introduces a layer of abstraction in the HLS flow to significantly enhance the productivity in deploying DP kernels on FPGAs. Specifically, the front-end provides a high degree of flexibility to specify new DP kernels in C++ without needing to add or modify HLS directives, while the back-end uses fixed HLS directives to efficiently translate these specifications into optimized RTL implementations with systolic arrays. Our experiments confirm that the generated RTL code exhibits the expected linear systolic array behavior. (2) Using the DP-HLS framework, we implemented 15 bioinformatically relevant DP kernels on FPGAs, covering a wide range of applications from basecalling to protein sequence alignment. All kernels are functionally verified and deployed on AWS® EC2 F1 instances for broad accessibility. For most kernels, no open-source FPGA implementations are available. (3) For the three DP kernels with available hand-optimized RTL implementations, DP-HLS achieved throughput within 7.7-16.8% and comparable resource utilization. However, DP-HLS significantly boosts design productivity, from months to days, compared to manual RTL design. (4) For several DP kernels implemented in state-of-the-art CPU and GPU libraries, DP-HLS delivered a 1.3–32×\times× improvement in throughput over CPU- and GPU-optimized AWS® EC2 instances of the same cost. (5) We demonstrate, through an example, that recently proposed tiling heuristics [11] are compatible with DP-HLS and can be used for performing both short and long sequence alignments on the FPGA. (6) We have made the entire framework publicly available, including all 15 DP kernels as case studies at https://github.com/TurakhiaLab/DP-HLS. Additionally, we provide a comprehensive wiki to assist new users at https://turakhia.ucsd.edu/DP-HLS/."
https://arxiv.org/html/2411.04027v1,Prototyping O-RAN Enabled UAV Experimentation for the AERPAW Testbed,"The Open Radio Access Network (O-RAN) architecture is reshaping the telecommunications landscape by enhancing network flexibility, openness, and intelligence. This paper establishes the requirements, evaluates the design tradeoffs, and introduces a scalable architecture and prototype of an open-source O-RAN experimentation platform within the Aerial Experimentation and Research Platform for Advanced Wireless (AERPAW), an at scale testbed that integrates unmanned aerial vehicles (UAVs) with advanced wireless network technologies, offering experimentation in both outdoor testbed and emulation via a custom digital twin (DT). Through a series of aerial experiments, we evaluate FlexRIC, an open-source RAN Intelligent Controller, within the AERPAW hardware-software platform for network data monitoring, providing valuable insights into the proposed integration and revealing opportunities for leveraging O-RAN to create custom service based optimizations for cellular connected UAVs. We discuss the challenges and potential use cases of this integration and demonstrate the use of a generative artificial intelligence model for generating realistic data based on collected real-world data to support AERPAW’s DT.","The advent of the Open Radio Access Network (O-RAN) architecture marked a significant advancement in telecommunications, enhancing flexibility, openness, and intelligence across diverse network configurations. O-RAN’s modular design, featuring open interfaces, supports real-time resource management and artificial intelligence (AI) driven optimizations, fostering interoperability and innovation. This allows operators to tailor networks to evolving demands, reduce operational costs, and enhance service delivery. Key components of the O-RAN architecture include the near-real time RAN Intelligent Controller (near-RT RIC) and the non-RT RIC. These components, O-RAN’s disaggregated RAN, and the open interfaces enhance network flexibility, reduce costs via competitive vendor markets, and improve network intelligence [1]. There are several efforts in the United States and the rest of the world to promote open RANs and O-RAN. One of the primary goals is vendor diversity, which spurs domestic deployment, allows customized network scaling with more control over cellular network technology and parameters, and supports technology adoption and evolution toward 6G in a given region or country. The Platforms for Advanced Wireless Research (PAWR) program in the United States plays a pivotal role in integrating O-RAN technologies into its diverse testbeds, providing critical infrastructure for exploring the practical applications of O-RAN in real-world settings. The PAWR program supports five Wireless Community Testbeds (WCTs) designed to provide a controlled environment where researchers and developers can experiment with and evaluate new wireless technologies, protocols, and applications. The Cloud Enhanced Open Software Defined Mobile Wireless Testbed for City-Scale Deployment (COSMOS) focuses on millimeter wave technology, providing an urban environment to explore advanced communications and collect data. The Platform for Open Wireless Data-Driven Experimental Research (POWDER) centers around sub-6 GHz deployments, offering a flexible, programmable environment for wireless experimentation and data collection. The Aerial Experimentation and Research Platform for Advanced Wireless (AERPAW) integrates wireless communications with aerial platforms, exploring protocols for aerial networks and gathering relevant data. The Agriculture and Rural Communities (ARA) testbed focuses on providing connectivity solutions for rural and underserved areas, supporting a range of wireless technologies for exploring innovative approaches for broader network coverage. Colosseum is a large-scale wireless network emulator that offers a highly configurable environment for testing and evaluating advanced wireless technologies. These platforms feature software-defined radio (SDR) hardware and software along with specialized radio equipment. In the realm of wireless research, the O-RAN architecture is reshaping how WCTs are used for experimentation and data collection as exemplified by O-RAN use cases in POWDER such as RAN slicing [2]. At this time COSMOS, POWDER, and Colosseum offer O-RAN experiments while AERPAW and ARA consider offering them in the future [3]. The existing O-RAN capabilities available through WCTs are either for an emulated radio frequency (RF) channel or with no or limited mobility [4]. AERPAW offers distinctive advanced wireless research features, facilitating controlled three-dimensional (3D) mobility in an at-scale outdoor testbed and in a digital twin (DT). While [4] provides insights into some of the essential requirements for O-RAN experimentation within AERPAW, it does not actualize the integration of O-RAN into the testbed. Unmanned aerial vehicles (UAVs) require adaptive networks, which O-RAN facilitates, for reliable connectivity. This paper outlines the design requirements and evaluates the available deployment choices before proposing an architecture and prototype for constructing a reproducible open-source O-RAN experimentation platform for advanced wireless UAV research. This platform furnishes the necessary interfaces and performance metrics to facilitate extensive research opportunities involving cellular network-connected UAVs. Our proposed platform lays the groundwork and the first practical integration for pioneering O-RAN experiments in AERPAW, enabling research on AI-driven UAV communication, network, and trajectory optimization [5]. The contributions of this work are: • We describe the tradeoffs and design choices for a testbed that combines O-RAN components with AERPAW’s capabilities, creating an open-source, experimental research platform for UAV communications (Section II). • We offer experimental 5G key performance indicators collected between a UAV and ground nodes and introduce a generative AI (GAI) method to generate new data samples based on collected data for AERPAW’s DT, enabling further research and analysis (Section III). • We identify the key challenges in integrating O-RAN research capabilities into AERPAW and propose practical solutions and future research and development (R&D) opportunities for O-RAN enabled UAV communications (Section IV)."
https://arxiv.org/html/2411.03375v1,Kernel Approximation using Analog In-Memory Computing,"Kernel functions are vital ingredients of several machine learning algorithms, but often incur significant memory and computational costs. We introduce an approach to kernel approximation in machine learning algorithms suitable for mixed-signal Analog In-Memory Computing (AIMC) architectures. Analog In-Memory Kernel Approximation addresses the performance bottlenecks of conventional kernel-based methods by executing most operations in approximate kernel methods directly in memory. The IBM HERMES Project Chip, a state-of-the-art phase-change memory based AIMC chip, is utilized for the hardware demonstration of kernel approximation. Experimental results show that our method maintains high accuracy, with less than a 1% drop in kernel-based ridge classification benchmarks and within 1% accuracy on the Long Range Arena benchmark for kernelized attention in Transformer neural networks. Compared to traditional digital accelerators, our approach is estimated to deliver superior energy efficiency and lower power consumption. These findings highlight the potential of heterogeneous AIMC architectures to enhance the efficiency and scalability of machine learning applications.","Kernel functions[1, 2] of the form k⁢(x,y)𝑘𝑥𝑦k(x,y)italic_k ( italic_x , italic_y ), where x𝑥xitalic_x and y𝑦yitalic_y are d𝑑ditalic_d-dimensional input vectors, are specialized mathematical constructions that are equivalent to the computation of scalar measures such as similarities or inner products between data points in some designated higher, possibly infinite, dimensional space, but can be evaluated exclusively within the original space. Because kernel functions allow to capture non-linear relationships in the input data while operating in the original space, they can be found in many learning algorithms with a wide range of applications, including classification[3], regression[4], and dimensionality reduction[5]. However, algorithms based on kernel functions are often hampered by the inability to intertwine the evaluation of the kernel function with other linear operations. This often incurs high computational cost during inference, as some algorithms require the evaluation of the kernel function k⁢(xi,y)𝑘subscript𝑥𝑖𝑦k(x_{i},y)italic_k ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_y ) between the new sample y𝑦yitalic_y and every data point xisubscript𝑥𝑖x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT in the training data. Furthermore, the space cost also grows quadratically with the number of training samples as some algorithms require to store the Gram matrix 𝒢𝒢\mathcal{G}caligraphic_G, where 𝒢i,j=k⁢(xi,xj)subscript𝒢𝑖𝑗𝑘subscript𝑥𝑖subscript𝑥𝑗\mathcal{G}_{i,j}=k(x_{i},x_{j})caligraphic_G start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT = italic_k ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) for all the pairs xi,xjsubscript𝑥𝑖subscript𝑥𝑗x_{i},x_{j}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT in the training set. To address this scalability issue, researchers have devised a family of techniques known as kernel approximation methods, which provide an efficient means of approximating kernel computations while maintaining competitive predictive performance and strong mathematical guarantees on their representational power. These techniques seek to strike a balance between computational efficiency and accuracy, making it feasible to use kernel functions in large-scale problems. The essential idea behind kernel approximation is to explicitly map the input vectors x,y∈ℝd𝑥𝑦superscriptℝ𝑑x,y\in\mathbb{R}^{d}italic_x , italic_y ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT to some D𝐷Ditalic_D-dimensional space, where D>d𝐷𝑑D>ditalic_D > italic_d, using a mapping function z:ℝd↦ℝD:𝑧maps-tosuperscriptℝ𝑑superscriptℝ𝐷z:\mathbb{R}^{d}\mapsto\mathbb{R}^{D}italic_z : blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ↦ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT so that k⁢(x,y)≈⟨z⁢(x),z⁢(y)⟩.𝑘𝑥𝑦𝑧𝑥𝑧𝑦k(x,y)\approx\langle z(x),z(y)\rangle.italic_k ( italic_x , italic_y ) ≈ ⟨ italic_z ( italic_x ) , italic_z ( italic_y ) ⟩ . (1) A series of algorithms following this idea have been proposed over the past years[6], among which a prominent role is played by algorithms based on random feature sampling such as random Fourier features[7] and its enhanced variants[8, 9, 10, 11, 12, 13, 14]. Kernel approximation based on random features finds a wide range of applications, such as kernel Support Vector Machines (SVMs) [15, 16, 17, 18], dimensionality reduction[19], neural networks[20, 21, 22, 23], and kernel regression[24, 25]. This technique can be used to approximate a particular family of kernels, called shift-invariant kernels, and allows to approximate k𝑘kitalic_k using explicit mappings obtained by sampling weights from probability functions that are specific to that kernel[7]. Although the speed-up offered for large-scale problems is already significant, these approximation algorithms based on random features leverage expensive mappings from the original space (ℝdsuperscriptℝ𝑑\mathbb{R}^{d}blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT) to the feature space (ℝDsuperscriptℝ𝐷\mathbb{R}^{D}blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT) for every input sample. When implemented on conventional von Neumann architectures, this mapping operation is costly in terms of energy consumption and latency as the weights need to be constantly fetched from memory. This limitation is commonly referred to as the von Neumann bottleneck. One way to overcome this bottleneck is analog in-memory computing (AIMC) [26, 27, 28], a technique in which certain operations can be directly performed in-memory using analog computation. For example, SRAM-based AIMC [29] is an instance of AIMC using volatile memory that is fast and has good endurance, but has poor scalability in terms of capacity due to limitations in the SRAM cell size. Another variant of AIMC uses Non-Volatile Memory (NVM) devices such as Flash [30], MRAM [31], phase change memory (PCM) [32] or ReRAM [33, 34]. The advantage of using NVM devices is their high density. However, intrinsic device noise, inter-device variability, and limited endurance impact the computational accuracy in applications. One application that does not require very high endurance and high-precision computation is the inference of neural networks, which has been widely studied in prior works [32, 35, 34]. Here, the weights of the layers of the neural network are programmed on the NVM devices arranged in a crossbar array. When applying voltage pulses to the rows of the crossbar array, current flows through the NVM devices and accumulates along the column. The resulting current, which represents a dot product between the input voltages and the NVM devices in the column, is converted back to the digital domain using analog to digital converters (ADCs). Because the crossbar array has multiple columns, multiple dot products are computed, which is the equivalent to a matrix-vector multiplication. By exploiting Kirchhoff’s current law and Ohm’s law, all the multiply-accumulate operations that take place inside the crossbar are executed in parallel. Therefore, one considers the matrix-vector multiplication (MVM) operation as one primitive operation which can be executed in one time step. The duration of this step depends on various factors such as the encoding scheme of the 8-bit inputs to voltage pulses and the ADC conversion time. For more details, see Sebastian et al. 36. By combining such analog computing with light-weight digital processing units on a heterogeneous architecture, AIMC can provide 10×10\times10 × to 140×140\times140 × higher energy efficiency [37] at competitive throughput compared to modern GPUs [38]. Leveraging this idea, we propose in-memory kernel approximation, which allows to accelerate kernel approximation methods based on random features on such a heterogeneous architecture. This is – compared to prior efforts that accelerated the kernel approximation using GPUs [39, 40] – achieved by implementing the compute-intensive mapping from the original to the feature space using AIMC. By doing in-memory kernel approximation of non-linear kernels, a vast majority of operations associated with the linear projection to a higher dimensional space are accelerated using analog computing. This is followed by executing some element-wise non-linear operations in digital units as part of kernel approximation which is in sharp contrast with previous methods that needed to perform the entire kernel approximation in digital units. We experimentally validate the concept of in-memory kernel approximation using a state-of-the-art 14-nm CMOS PCM-based mixed-signal AIMC chip referred to as the IBM HERMES Project Chip [32]. This chip comprises 64 cores, each of which has a 256×256256256256\times 256256 × 256 crossbar, 256 digital to analog converters (DACs), 256 ADCs and local digital post-processing circuitry (see Methods). Because there exists one DAC/ADC per row/column, fully-parallel MVMs can be performed in a single computational time step, leading to a peak throughput of 63.1 Tera Operations Per Second (TOPS) and energy efficiency of 9.76 TOPS per Watt. Furthermore, the peak power consumption of 6.5W of the IBM HERMES Project Chip is significantly lower than that of a GPU. For example, the NVIDIA A100 GPU has a peak power consumption of 400W of which 70W are static. While analog computing suffers from noise and hence, at present, only allows for low precision computations, several contemporary applications in Machine Learning can be made robust to low precision computations [32]. We first study a representative pool of kernel approximation techniques and measure the approximation error between the 32-bit floating point (FP-32) software kernel approximation and the in-memory kernel approximation. Furthermore, we measure the impact of the additional approximation error introduced by the chip on the downstream classification accuracy on a set of well-known machine learning benchmarks, using a simple linear classifier (kernel ridge classification model) on top of the extracted features. Finally, we turn to a more recent application of kernel approximation in linear-complexity Transformer neural networks. We show that in-memory kernel approximation of the Softmax kernel used in kernelized attention yields no degradation in downstream performance compared to the FP-32 software equivalent, while offloading between half and one third of the Floating Point Operations (FLOPs) involved in the linear attention computation to AIMC. This is different from previous approaches to accelerate attention using AIMC. One previous approach [41] uses AIMC to remove tokens that would yield low attention scores. Although this can lead to the removal of up to 90% of the tokens, only 20% of the FLOPs get offloaded to AIMC for a sequence length of 4,096. For longer context lengths this percentage further diminishes as the attention computation (which is done in digital) still has quadratic complexity with respect to the sequence length. Other approaches [42, 43] require frequent re-programming of the crossbar arrays during inference. Our approach requires only a one-time programming of the memory devices, and also makes the attention mechanism faster and more memory-efficient as a result of the linear complexity of both time and space with respect to the sequence length. Assuming peak throughput and peak power consumption, we estimate that in comparison to an NVIDIA A100 GPU (INT8 precision), accelerating the projection operation in kernel-approximation is up to 6.3×\times× less energy consuming."
https://arxiv.org/html/2411.03149v1,"Hardware for converting floating-pointto the
microscaling (MX) format","This paper proposes hardware converters for the microscaling format (MX-format), a reduced representation of floating-point numbers. We present an algorithm and a memory-free hardware model for converting 32 single-precision floating-point numbers to MX-format. The proposed model supports six different types of MX-format: E5M2, E4M3, E3M2, E2M3, E2M1, and INT8. The conversion process consists of three steps: calculating the maximum absolute value among 32 inputs, generating a shared scale, and producing 32 outputs in the selected MX-format type. The hardware converters were implemented in FPGA, and experimental results demonstrate.","The Microscaling (MX) number representation was proposed in 2023 [1, 2] through a collaboration between AMD, Arm, Intel, Meta, Microsoft, NVIDIA, and Qualcomm. This format aims to minimize hardware costs and storage requirements via reduced bit-width, while maintaining the efficiency and accuracy needed for machine learning training and inference tasks. MX-formats provide a unified representation for half-precision (FP16), single-precision (FP32), BFloat16, and others formats by using truncated binary formats with a sign bit, up to 5 bits for the exponent, and up to 6 bits for the mantissa. A key feature of the MX-format is its use of a shared scale factor that represents the exponent of the floating-point format. MX-format is the modified floating-point representation with w𝑤witalic_w-bit shared scale X𝑋Xitalic_X (represented in exponent format) and privet n𝑛nitalic_n element Pisubscript𝑃𝑖P_{i}italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, where i=1,2,…,n𝑖12…𝑛i=1,2,\dots,nitalic_i = 1 , 2 , … , italic_n. Fig. 1 shows the mapping of 32-bits floating point numbers (FP32) V1,V2,…,V32subscript𝑉1subscript𝑉2…subscript𝑉32V_{1},V_{2},\dots,V_{32}italic_V start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_V start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_V start_POSTSUBSCRIPT 32 end_POSTSUBSCRIPT to MX-format X,P1,P2,…,P32𝑋subscript𝑃1subscript𝑃2…subscript𝑃32X,P_{1},P_{2},\dots,P_{32}italic_X , italic_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_P start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_P start_POSTSUBSCRIPT 32 end_POSTSUBSCRIPT. Figure 1: The ratio between FP32 and MX-format: S1,S2,…,S32subscript𝑆1subscript𝑆2…subscript𝑆32S_{1},S_{2},\dots,S_{32}italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_S start_POSTSUBSCRIPT 32 end_POSTSUBSCRIPT - sign bits, E⁢V1,E⁢V2,…,E⁢V32𝐸subscript𝑉1𝐸subscript𝑉2…𝐸subscript𝑉32EV_{1},EV_{2},\dots,EV_{32}italic_E italic_V start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_E italic_V start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_E italic_V start_POSTSUBSCRIPT 32 end_POSTSUBSCRIPT - exponent and M⁢V1,M⁢V2,…,M⁢V32𝑀subscript𝑉1𝑀subscript𝑉2…𝑀subscript𝑉32MV_{1},MV_{2},\dots,MV_{32}italic_M italic_V start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_M italic_V start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_M italic_V start_POSTSUBSCRIPT 32 end_POSTSUBSCRIPT - mantissa of V1,V2,…,V32subscript𝑉1subscript𝑉2…subscript𝑉32V_{1},V_{2},\dots,V_{32}italic_V start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_V start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_V start_POSTSUBSCRIPT 32 end_POSTSUBSCRIPT, X𝑋Xitalic_X - shared scale, E⁢K1,E⁢K2,…,E⁢K32𝐸subscript𝐾1𝐸subscript𝐾2…𝐸subscript𝐾32EK_{1},EK_{2},\dots,EK_{32}italic_E italic_K start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_E italic_K start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_E italic_K start_POSTSUBSCRIPT 32 end_POSTSUBSCRIPT - exponent and M⁢R1,M⁢R2,…,M⁢R32𝑀subscript𝑅1𝑀subscript𝑅2…𝑀subscript𝑅32MR_{1},MR_{2},\dots,MR_{32}italic_M italic_R start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_M italic_R start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_M italic_R start_POSTSUBSCRIPT 32 end_POSTSUBSCRIPT - mantissa of P1,P2,…,P32subscript𝑃1subscript𝑃2…subscript𝑃32P_{1},P_{2},\dots,P_{32}italic_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_P start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_P start_POSTSUBSCRIPT 32 end_POSTSUBSCRIPT, where K𝐾Kitalic_K and R𝑅Ritalic_R - nimber of bits of exponent and mantissa, respectively, of MX-format MX-format is characterized by the w𝑤witalic_w-bit shared scale X𝑋Xitalic_X and private elements P1,P2,…,Pnsubscript𝑃1subscript𝑃2…subscript𝑃𝑛P_{1},P_{2},\dots,P_{n}italic_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_P start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_P start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT. This paper considers six types of MX-format. The bit-with of the shared scale depends on the type of MX. The general form of a type is represented as EKMR, where ”K” and ”R” are the number of bits of exponent (EK on Fig. 1) and mantissa (MR on Fig. 1) respectively. The sign bit (S on Fig. 1) is mentioned by default. This paper considers the following types of MX-format: E5M2, E4M3, E3M2, E2M3, E2M1, and INT8, which are the ones currently considered for possible standardization. The bit range of the shared scale X𝑋Xitalic_X for FP32 in the mentioned types of MX-format is 8, i.e. w=8𝑤8w=8italic_w = 8. For example, for n=2𝑛2n=2italic_n = 2 and E5M2, V1,V2subscript𝑉1subscript𝑉2V_{1},V_{2}italic_V start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_V start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT in FP32 correspond to two private elements P1,P2subscript𝑃1subscript𝑃2P_{1},P_{2}italic_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_P start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT that consist of 8-bit, i.e. 1 sign bit, 5-bit exponent, and 2-bit mantissa each. The bit-width of the shared scale, sign bit, exponent, and mantissa for the considered formats are shown in Table I. TABLE I: Bit-width of the shared scale (X𝑋Xitalic_X), sign bit S𝑆Sitalic_S, exponent E⁢K𝐸𝐾EKitalic_E italic_K, and mantissa M⁢R𝑀𝑅MRitalic_M italic_R for E5M2, E4M3, E3M2, E2M3, E2M1, and INT8 X S EK MR E5M2 5 2 E4M3 4 3 E3M2 8 1 3 2 E2M3 2 3 E2M1 2 1 INT8 1 6 According to the original mechanism [1, 2], the converting consists of two steps: 1. calculating X𝑋Xitalic_X; 2. generating quantized outputs P1,P2,…,Pnsubscript𝑃1subscript𝑃2…subscript𝑃𝑛P_{1},P_{2},\dots,P_{n}italic_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_P start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_P start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT. The shared scale X𝑋Xitalic_X can represent NaN, but not infinity. X𝑋Xitalic_X value is treated separately from the private elements. Even when X𝑋Xitalic_X is NaN, the private elements can be anything (normal number, infinity, and NaN). However, in backward transformation, the resulting values will all be NaN if X𝑋Xitalic_X is NaN, because multiplication of NaN to a value equals NaN. This paper proposes an algorithm and a hardware architecture for converting FP32 to MX formats. The proposed models were implemented on FPGA, with experimental results obtained after synthesis and place-and-route. In [3], a converter is described that processes input into blocks of MX values using ’round to nearest even’ for FP32 and BFloat16 and focuses on a pipelined (memory-based) design. In contrast, the approach in this paper uses combinational logic (i.e., without memory) and applies standard IEEE 754 rounding. The rest of the paper is organized as follows. Section II will consider the converting algorithm from FP32 to MX-format and quantization of the considered MX-formats with the examples. Section III will represent the structure of the hardware converter. This will be followed by the results of the synthesis in section IV. Finally, the conclusions are presented in section V."
https://arxiv.org/html/2411.02854v1,SpiDR: A Reconfigurable Digital Compute-in-Memory Spiking Neural Network Accelerator for Event-based Perception,"Spiking Neural Networks (SNNs), with their inherent recurrence, offer an efficient method for processing the asynchronous temporal data generated by Dynamic Vision Sensors (DVS), making them well-suited for event-based vision applications. However, existing SNN accelerators suffer from limitations in adaptability to diverse neuron models, bit precisions and network sizes, inefficient membrane potential (Vmem) handling, and limited sparse optimizations. In response to these challenges, we propose a scalable and reconfigurable digital compute-in-memory (CIM) SNN accelerator SpiDR with a set of key features: 1) It uses in-memory computations and reconfigurable operating modes to minimize data movement associated with weight and Vmem data structures while efficiently adapting to different workloads. 2) It supports multiple weight/Vmem bit precision values, enabling a trade-off between accuracy and energy efficiency and enhancing adaptability to diverse application demands. 3) A zero-skipping mechanism for sparse inputs significantly reduces energy usage by leveraging the inherent sparsity of spikes without introducing high overheads for low sparsity. 4) Finally, the asynchronous handshaking mechanism maintains the computational efficiency of the pipeline for variable execution times of different computation units. We fabricated SpiDR in 65 nm Taiwan Semiconductor Manufacturing Company (TSMC) low-power (LP) technology. It demonstrates competitive performance (scaled to the same technology node) to other digital SNN accelerators proposed in the recent literature and supports advanced reconfigurability. It achieves up to 5 TOPS/W energy efficiency at 95% input sparsity with 4-bit weights and 7-bit Vmem precision.","In the evolving landscape of machine vision, dynamic vision sensors (DVS) [1, 2] have emerged as a unique visual sensing paradigm. Their event-based operation, where pixels asynchronously output data only upon detecting changes in intensity, offers inherent advantages over traditional frame-based cameras. This approach reduces data redundancy and enables high temporal resolution, which makes DVS particularly suitable for dynamic and fast-changing environments [3]. Among the numerous applications benefiting from DVS technology, object detection, tracking, optical flow estimation, etc. are critical for real-time motion analysis, robotics, and autonomous navigation systems [3]. Fig. 1 provides an overview of a DVS-based system for real-time motion analysis in a robotic agent. Figure 1: A conceptual diagram showcasing the use of DVS camera and a specialized accelerator for real-time motion analysis tasks (such as optical-flow estimation) in a robotic agent. Nevertheless, processing Dynamic Vision Sensor (DVS) data introduces unique challenges and opportunities, primarily due to its event-driven and sparse nature. Traditional Analog Neural Networks (ANNs)111ANN refers to standard neural networks that operate on multi-bit input and activation values (unlike spike-based digital inputs), including CNNs, MLPs, RNNs, LSTMs, etc., while powerful for a wide range of applications, face difficulties fully utilizing the asynchronous event streams and precise temporal information provided by DVS data. In contrast, Spiking Neural Networks (SNNs), inspired by biological neural dynamics, inherently align with the sparse, event-driven nature of DVS data and offer a more efficient processing paradigm [4, 5]. However, traditional general-purpose computing architectures, such as CPUs and GPUs, struggle to accommodate the sparse and asynchronous nature of SNN computations [6]. Designed around synchronous data flows, they incur overheads when converting event-based data into frame-like representations. In addition, their regular memory access patterns do not align with the unpredictable dynamic computations of SNNs, leading to an underutilization of processing units during sparse computations. Therefore, there is increased interest in developing specialized accelerators tailored to the unique demands of event-based processing to maximize performance and efficiency [7, 8, 9, 10, 11, 12, 13]. Figure 2: Lack of reconfigurability in existing SNN architectures. Despite these efforts, current SNN accelerators face several limitations in terms of scalability and flexibility that hinder their widespread adoption. A key limitation among these is the lack of versatile support for diverse network architectures [9, 10, 14, 15], variable bit precision [9, 10, 14] and different neuron models [8, 9, 10, 11, 15] (Fig. 2). Limitations in architectural support restrict the types of networks that can be efficiently implemented on the hardware. Furthermore, bit precision constraints hinder the exploration of accuracy and energy consumption trade-offs, and the inability to accommodate different neuron models limits the computational capabilities of SNNs on these accelerators. Additionally, many existing designs do not discuss the storage and data movement overhead of the membrane potential (Vmem) data structure. As a result, even when low precision is used for weights, high precision is used for Vmems [8, 9, 13], which can add high processing and memory overhead, particularly for layers with large output dimensions. Figure 3: An example spiking convolution layer. Figure 4: Overhead of using AER for input spikes for varying input sparsity. (a) Gesture recognition (b) Optical flow estimation Figure 5: Variation in input sparsity across different networks and layers of the same network. Finally, to achieve high efficiency, most SNN accelerators rely on sparse optimizations, such as address event representation (AER) for input spikes [9, 10]. However, AER is primarily beneficial for data structures with a high sparsity or small size (fewer data points). This leads to inconsistencies since the sparsity levels of input spikes in SNNs can vary significantly across different network architectures or even within layers of the same network. Fig. 3 illustrates the various data structures in a general spiking convolution layer. Fig. 4 then highlights the overhead of using AER at different input sparsity levels for such a layer, demonstrating that AER is effective only when sparsity exceeds 94.7% in this example. Fig. 5 further reinforces our point by showcasing the variation in sparsity across layers in two SNNs designed for gesture recognition and optical flow estimation tasks. The details of these networks are presented in Section III. Notably, the sparsity of inputs for the second layer of the optical flow estimation network can be as low as 60% and never goes beyond 75% whereas, the sparsity of the next layer inputs ranges from 75% to 99%. This analysis underscores the need for SNN accelerators that can efficiently handle varying sparsity levels rather than relying solely on optimizations that are effective in specific cases but increase the overheads in other cases. Figure 6: Proposed SNN core architecture This work introduces a scalable and reconfigurable SNN inference core SpiDR, that addresses the limitations discussed above through several key strategies outlined below: • It uses in-memory computation, reconfigurable operating modes, and timestep pipelining to efficiently support various workload sizes while minimizing weight and Vmem data movement. • It seamlessly incorporates three weight/Vmem bit precisions (4/7, 6/11, 8/15) into the macro design, enhancing parallelism with full resource utilization, allowing a balanced trade-off between computational accuracy and energy consumption. • It employs a zero-skipping strategy to leverage a wide range of input sparsity, reducing energy and latency with minimal overhead. • It uses an asynchronous handshaking mechanism to maintain the efficiency of the computation pipeline."
https://arxiv.org/html/2411.01628v1,Energy-Aware FPGA Implementation of Spiking Neural Network with LIF Neurons,"Tiny Machine Learning (TinyML) has become a growing field in on-device processing for Internet of Things (IoT) applications, capitalizing on AI algorithms that are optimized for their low complexity and energy efficiency. These algorithms are designed to minimize power and memory footprints, making them ideal for the constraints of IoT devices. Within this domain, Spiking Neural Networks (SNNs) stand out as a cutting-edge solution for TinyML, owning to their event-driven processing paradigm which offers an efficient method of handling dataflow. This paper presents a novel SNN architecture based on the 1s⁢tsuperscript1𝑠𝑡1^{st}1 start_POSTSUPERSCRIPT italic_s italic_t end_POSTSUPERSCRIPT Order Leaky Integrate-and-Fire (LIF) neuron model to efficiently deploy vision-based ML algorithms on TinyML systems. A hardware-friendly LIF design is also proposed, and implemented on a Xilinx Artix-7 FPGA. To evaluate the proposed model, a collision avoidance dataset is considered as a case study. The proposed SNN model is compared to the state-of-the-art works and Binarized Convolutional Neural Network (BCNN) as a baseline. The results show the proposed approach is 86% more energy efficient than the baseline.","Deep learning algorithms based on Artificial Neural Networks (ANN) are popular in Internet of Things (IoT) applications due to their AI capabilities. However, the size and complexity of these models require high-performance computing (HPC) clusters in data centers, which poses challenges for IoT devices that gather data on the edge (Navardi and Mohsenin, 2023; Humes et al., 2023). Edge computing, where data is processed on the device itself, can address these challenges but is limited by memory and energy constraints (Navardi and Mohsenin, 2023; Rashid et al., 2024; Mazumder and Mohsenin, 2023; Navardi et al., 2023). It led Tiny Machine Learning (TinyML) to emerge which considers efficient deployment of ANN models on tiny robots such as Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) (Manjunath et al., 2023). However, deploying ANN models on the edge is still limited by power and hardware constraints due to the highly intensive computational of such models. To overcome this, there is interest in bio-inspired approaches like Spiking Neural Networks (SNNs), which have lower energy consumption (Nguyen et al., 2022; Davidson and Furber, 2021; Véstias, 2019; Roy et al., 2017). Recent advancements in the field of neural networks have led to an era where the computational efficiency of SNNs is being harnessed in more innovative and practical ways (Pietrzak et al., 2023; Agatonovic-Kustrin and Beresford, 2000; Pfeiffer and Pfeil, 2018). SNNs, with their ability to mimic the intricacies of biological neural networks, represent a significant leap from traditional artificial neural networks. They offer a more nuanced approach to information processing, mimicking the dynamic, temporal characteristics of biological neuron activity. The integration of SNNs with Field-Programmable Gate Arrays (FPGAs) has opened new avenues in computing, merging the adaptability of neural networks with the energy efficiency and customization offered by FPGAs (Li et al., 2024; Omondi and Rajapakse, 2006; Panchapakesan et al., 2022; Kakani et al., 2023; Heidarpur et al., 2019; Pham et al., 2021; Mazumder et al., 2021). This synergy is crucial, especially in applications where power efficiency is important, such as in embedded systems or portable devices. While recent research has made strides in developing effective FPGA architectures for SNNs and creating more biologically plausible models (Farsa et al., 2019a, 2015; Lammie et al., 2018; Ali et al., 2022), there remains a gap in applying these advancements to complex, real-world scenarios. Much of the existing work (Iyer et al., 2021; Diehl and eCook, 2015; Zhou, 2023) focuses on pattern recognition tasks using simplified datasets like MNIST (Lecun et al., 1998), has not fully explored the potential of SNNs in more challenging and realistic environments. This research aims to bridge this gap by shifting the focus to a more demanding and realistic dataset: collision avoidance. The study explores the training of SNNs for collision avoidance and their implementation on FPGA platforms. By moving beyond the conventional realm of simple image classification tasks, this work aims to validate the practicality and robustness of SNNs in high-stakes scenarios, marking a significant stride toward their real-life applicability. This paper presents a novel approach to SNN implementation, leveraging the strengths of FPGAs to address the challenges of image recognition in dynamic environments. The choice to focus on FPGA implementations for SNNs was driven by several considerations such as FPGAs offer a unique blend of flexibility and performance, allowing for rapid prototyping and iterative design that is crucial for the evolving field of SNNs. This work contributes to the field by not only validating the FPGA-based hardware design using a complex dataset but also by offering a comprehensive comparison with prior standard works. This research underscores the progression of neuromorphic computing (Mead, 1990) towards practical, everyday applications, setting a new benchmark for future endeavors in the domain. In summary, the main contributions of this paper are: • Developing a vision-based TinyML framework optimized for tiny UAVs and UGVs. • Adapting a widely-recognized 1s⁢tsuperscript1𝑠𝑡1^{st}1 start_POSTSUPERSCRIPT italic_s italic_t end_POSTSUPERSCRIPT Order LIF model for on-device processing in TinyML applications, tested with a challenging dataset. • Architecting a robust hardware solution tailored for the efficient deployment of this SNN model on an FPGA platform."
https://arxiv.org/html/2411.01417v1,BF-IMNA: A Bit Fluid In-Memory Neural Architecture for Neural Network Acceleration,"Mixed-precision quantization works defining per-layer, per-network, per-channel, or per-parameter precisions for Neural Networks (NNs) are gaining traction for their efficient realization on the hardware leading to higher throughput and lower energy. In-Memory Computing (IMC) accelerator architectures like PUMA [1], PipeLayer [41], and ISAAC [35], are offered as alternatives to traditional architectures relying on a data-centric computational paradigm, diminishing the memory wall problem, and scoring high throughput and energy efficiency. These accelerators can support static fixed-precision but are not flexible to support mixed-precision NNs.In this paper, we present BF-IMNA, a bit fluid IMC accelerator for end-to-end Convolutional NN (CNN) inference that is capable of static and dynamic mixed-precision without any hardware reconfiguration overhead at run-time. At the heart of BF-IMNA are Associative Processors (APs), which are bit-serial word-parallel Single Instruction, Multiple Data (SIMD)-like engines. We report the performance of end-to-end inference of ImageNet on AlexNet, VGG16, and ResNet50 on BF-IMNA for different technologies (eNVM and NVM), mixed-precision configurations, and supply voltages. To demonstrate bit fluidity, we implement HAWQ-V3’s [53] per-layer mixed-precision configurations for ResNet18 on BF-IMNA using different latency budgets, and results reveal a trade-off between accuracy and Energy-Delay Product (EDP): On one hand, mixed-precision with a high latency constraint achieves the closest accuracy to fixed-precision INT8 and reports a high (worse) EDP compared to fixed-precision INT4. On the other hand, with a low latency constraint, BF-IMNA reports the closest EDP to fixed-precision INT4, with a higher degradation in accuracy compared to fixed-precision INT8. We also show that BF-IMNA with fixed-precision configuration still delivers performance that is comparable to current state-of-the-art accelerators: BF-IMNA achieves 20%percent2020\%20 % higher energy efficiency compared to PipeLayer and 2%percent22\%2 % higher throughput compared to ISAAC.","Deep Learning (DL) algorithms, especially Convolutional Neural Networks (CNNs), have gained widespread popularity, playing a pivotal role in applications like computer vision, natural language processing, and recommendation systems [25, 10, 7, 39]. Deploying and scaling CNNs on resource-constrained edge devices amid Moore’s law stagnation and the rise of IoT applications requires maximizing performance while minimizing computational overhead [22, 42, 13]. To achieve this ”Pareto” optimality, literature explores a mix-and-match plethora of techniques including hardware customization and algorithmic optimizations. Hardware customization encompasses developing accelerator platforms that maximize efficiency and throughput. Examples of State-of-The-Art (SOTA) specialized von Neumann architectures include the Tensor Processing Unit (TPU) which relies on systolic array architectures [17], Eyeriss which relies on two-dimensional spatial arrays of processing elements [4], and the architecture in [37] which has dedicated dual-range Multiply-Accumulate (MAC) blocks. Beyond von Neumann, PUMA [1], ISAAC [35], PipeLayer [41], and [43, 52] leverage In-Memory Computing (IMC)-based paradigms for CNN acceleration, relying on analog/digital computing. IMC is a data-centric computational paradigm that abolishes the memory wall problem suffered by Von-Neumann by allowing computation to be performed inside memory [28]. Mixed-precision quantization has recently emerged as an algorithmic optimization that enables a fast, low-memory footprint deployment of CNNs without compromising performance. With mixed-precision quantization, the model’s weights/activations are quantized to a certain precision chosen from a set of possible bitwidths, in a way that the overall mixed-precision configuration yields high accuracy. The granularity of mixed-precision can be fine-grained (at the weight/activation level; each weight/activation can be assigned a precision) or coarse-grained (at the layer level; all weights/activations in one layer are assigned the same precision, but precisions vary between layers). Mixed-precision CNN frameworks[34] are categorized based on their optimization techniques: 1-) gradient-based works like Bayesian Bits [44], 2-) Reinforcement Learning-based works like AutoQ [27], 3-) heuristic-based works like HAWQ-V3 [53], and 4-) meta-heuristic-based works like APQ [47]. Several works concluded by calling for hardware-aware mixed-precision frameworks; capable of accommodating run-time changing requirements of the target platforms and call for ”hardware platforms capable of supporting dynamic mixed-precision, where the precision can be changed at run-time” [34]. All of the aforementioned SOTA CNN accelerators were designed with fixed-precision (all weights/activations have the same precision across all models). Even if the same design rules/considerations of these accelerators were followed to accommodate for mixed-precision, those accelerators will end up supporting one mixed-precision configuration which is chosen statically at design time. This is not practical as different mixed-precision works have shown that the optimal configuration changes with the model [53, 27, 46]. Moreover, such accelerators would not be able to accommodate changing requirements at run-time which would require a change in the mixed-precision configuration [2]. While the list of hardware accelerators that support fixed-precision CNN inference is long, the literature currently lacks end-to-end hardware accelerators tailored for mixed-precision CNN inference, especially dynamic mixed-precision. SOTA mixed-precision CNN accelerators include Stripes [18] and BitFusion [36]. While Stripes is a bit-serial CNN accelerator that enables mixed-precision, it is not an end-to-end accelerator design but rather focuses on accelerating only convolution. BitFusion is a more recent, spatial, end-to-end CNN accelerator capable of bit-level fusion and outperforms Stripes [36]. In this paper, we propose BF-IMNA: A Bit Fluid In-Memory Neural Architecture for end-to-end full-fledged fixed and dynamic CNN acceleration. Bit fluidity is a term we henceforth use to describe the ability to enable mixed-precision inference. Unlike BitFusion, BF-IMNA relies on the IMC-based Associative Processor (AP) as its building block. The AP shown in Fig. 1 relies on a Content Addressable Memory (CAM) to carry out massively parallel logical/arithmetic operations in a bit-serial word parallel manner [9, 20]. The bit-serial operation inherently supports mixed-precision without the need of dynamic reconfigurability. CAMs are IMC architectures utilized in several applications that require parallel searching like IP routing, text analytics, data mining, and data analytics [19, 54, 8]. APs perform arithmetic and logical operations by a series of search/write stages following the operation’s Look-Up Table (LUT) applied in a bit-serial word-parallel manner (i.e. between pairs of columns of the CAM). An extension of the AP is a 2D AP proposed in [51] which provides the capability of performing logical/arithmetic operations between pairs of rows in addition to being able to perform those between pairs of columns. The 1D and 2D APs are the only building blocks of BF-IMNA: we use 1D APs as Memory APs (MAPs); the on-chip memory storage, and we use 2D APs as Computation APs (CAPs), the on-chip computational units capable of performing any CNN operation. The architecture of BF-IMNA is shown in Fig. 3. Our contributions are below. 1- We formulate the complexities of performing CNN-related operations such addition, multiplication, ReLU activation, average pooling, and max pooling on the 2D AP, taking into consideration the data movements. 2- We map the end-to-end inference operations into AP blocks, and we present, based on different hardware configurations, two BF-IMNA designs: one that offers maximum parallelism and another with limited resources. 3- We develop an in-house simulator for BF-IMNA that estimates the performance metrics (energy, area, latency) of end-to-end inference of ImageNet on AlexNet, ResNet50, and VGG16. Then we carry out a design space exploration for different CAM cell technologies (SRAM/ReRAM), different per-layer mixed-precision configurations, and different supply voltages. 4- We demonstrate BF-IMNA’s bit fluidity by implementing HAWQ-V3’s [53] per-layer mixed-precision configurations for different latency budgets. The different mixed-precisions reveal a trade-off between accuracy and Energy-Delay Product (EDP). 5- We compare BF-IMNA using fixed-precision configurations to SOTA accelerators. Our results demonstrate that BF-IMNA achieves comparable results even at a high fixed-precision (16 bits) where APs start to lose their advantage: 1.02×1.02\times1.02 × higher throughput with 3.66×3.66\times3.66 × lower energy efficiency and 1.19×1.19\times1.19 × higher energy efficiency with 2.95×2.95\times2.95 × lower throughput compared to ISAAC and PipeLayer. The rest of the paper is organized as follows: Section II provides background, and section III elaborates on BF-IMNA. Sections IV and V present the implementation details, results, and discussion. Section VI concludes the paper. Figure 1: General architecture of an AP."
https://arxiv.org/html/2411.02322v1,LayerDAG: A Layerwise Autoregressive Diffusion Model for Directed Acyclic Graph Generation,"Directed acyclic graphs (DAGs) serve as crucial data representations in domains such as hardware synthesis and compiler/program optimization for computing systems. DAG generative models facilitate the creation of synthetic DAGs, which can be used for benchmarking computing systems while preserving intellectual property. However, generating realistic DAGs is challenging due to their inherent directional and logical dependencies. This paper introduces LayerDAG, an autoregressive diffusion model, to address these challenges. LayerDAG decouples the strong node dependencies into manageable units that can be processed sequentially. By interpreting the partial order of nodes as a sequence of bipartite graphs, LayerDAG leverages autoregressive generation to model directional dependencies and employs diffusion models to capture logical dependencies within each bipartite graph. Comparative analyses demonstrate that LayerDAG outperforms existing DAG generative models in both expressiveness and generalization, particularly for generating large-scale DAGs with up to 400 nodes—a critical scenario for system benchmarking. Extensive experiments on both synthetic and real-world flow graphs from various computing platforms show that LayerDAG generates valid DAGs with superior statistical properties and benchmarking performance. The synthetic DAGs generated by LayerDAG enhance the training of ML-based surrogate models, resulting in improved accuracy in predicting performance metrics of real-world DAGs across diverse computing platforms. Our implementation is available at https://github.com/Graph-COM/LayerDAG.","A Directed Acyclic Graph (DAG) is a data structure that represents the order of elements (Bang-Jensen & Gutin, 2008). Unlike linear sequences/chains, which follow a single, straight path from the first to the last element, DAGs incorporate branching and merging, allowing for the modeling of complex dependencies and hierarchies. This flexibility makes DAGs ideal for representing diverse problem domains such as workload behavior during system execution Sridharan et al. (2023), operator dependence for program analysis (Phothilimthana et al., 2023b; Luo et al., 2021; Cortez et al., 2017), dataflows in circuits (Dong et al., 2023), task dependencies in project management (Skiena, 2008; Borenstein, 2000), and cause-effect relationships (Pearl, 1995; Tennant et al., 2020). Additionally, DAGs have recently been used to create challenging benchmarks for evaluating the reasoning capabilities of large language models (Zhu et al., 2024). Figure 1: (a) A real-world DAG (the computation flow for a transformer layer (Vaswani et al., 2017)) encompasses complex logical and directional dependencies. Examples of logical dependencies include 1) dimension matching in matrix multiplications and 2) exactly two matrices pointed to a ×\times× operation. One example of directional dependencies here is softmax(qk)v being computed after qk. (b) Each DAG has a unique layerwise partition, an ordered partition of nodes/edges into a sequence of bipartite graphs. In LayerDAG, each bipartite graph G(l+1)superscript𝐺𝑙1G^{(l+1)}italic_G start_POSTSUPERSCRIPT ( italic_l + 1 ) end_POSTSUPERSCRIPT is generated by a diffusion model conditioned on G(≤l)superscript𝐺absent𝑙G^{(\leq l)}italic_G start_POSTSUPERSCRIPT ( ≤ italic_l ) end_POSTSUPERSCRIPT. LayerDAG generates in order the number of new nodes, their attributes, and the new edges. Despite the benefits of DAGs mentioned above, it is non-trivial to get largescale datasets of real DAGs to enable analysis and optimization. Taking workload execution as an example, DAGs capturing execution behavior can help engineers gain valuable insights into performance metrics such as latency and resource consumption exhibited by candidate systems (Luo et al., 2021; Cortez et al., 2017) to optimize the software and hardware accordingly. However, collecting a workload execution DAG (Sridharan et al., 2023) for even a single Large Language Model training job, potentially involving trillions of operations, is extremely prohibitive given the size of modern AI platforms. For example, Meta’s Llama3 was trained on a 24K GPU platform (Dubey et al., 2024). Moreover, very few companies in the world even have access to platforms of this scale. Even if such a large workload execution DAG could be collected, practical constraints such as the storage requirements of the DAGs, and potential to leak private information about the AI model architecture or the training platform configuration further limit DAG sharing. Furthermore, a representative and smaller graph is more effective for most performance optimization tasks. This work focuses on developing generative models for DAGs, which allows various potential applications. First, being able to be data-driven and generate representative small set of DAGs can be much more compute and memory efficient than a real DAG given repetitive patterns. Second, this could enable data sharing without compromising intellectual property (Gao et al., 2024; Lin et al., 2020), thereby promoting software and hardware co-design among different companies (Sridharan et al., 2023). Third, a conditional DAG generative model can efficiently search the space of valid DAGs for optimization purposes in scenarios such as circuit design (Takagi, 1999; Dong et al., 2023), compiler optimization (Aho et al., 2006; Phothilimthana et al., 2023b), and neural architecture search (NAS) (Zhang et al., 2019; Li et al., 2023a; An et al., 2023). Serving as an abstraction for flows and node dependencies, DAGs pose significant challenges in developing powerful and efficient generative models due to their intrinsic strong directional and logical dependencies, such as control flows, logic gates, and dimension requirements of matrix operations (as illustrated in Fig. 1 (a)). These complexities are further magnified in large-scale DAGs, presenting a unique combination of challenges regarding both scale and logical rules. In contrast, although social networks may be large in scale, they do not exhibit strong dependencies between nodes and edges. At the other end of the spectrum, molecular graphs, whose generation also has received much interest recently, must adhere to chemical rules but are generally smaller in size. This work proposes the use of autoregressive diffusion models to generate DAGs, aiming to decouple the strong node dependencies in DAGs into manageable units and handle them sequentially. Our model, named LayerDAG, is based on a novel perspective of DAGs: as illustrated in Fig. 1 (b), the partial order of nodes dictated by the DAG structure can be decoupled as a sequence of tokens, each corresponding to a bipartite graph. This perspective enables the natural modeling of directional dependencies in DAGs through autoregression, and the rest is to address the logical dependencies within a set of nodes incomparable (i.e. no ordering relations) according to the partial order in each autoregressive step, where we leverage diffusion models known for effectively capturing multidimensional dependencies (Rombach et al., 2022; Vignac et al., 2023a). Specifically, the diffusion model is conditioned on previously generated bipartite graphs to generate the next bipartite graph consisting of node attributes and edges. Our model advances existing DAG generative models in multiple aspects (Zhang et al., 2019; Li et al., 2023a; An et al., 2023). Methodologically, although autoregressive models have been adopted by D-VAE (Zhang et al., 2019) and GraphPNAS (Li et al., 2023a) for DAG generation, they treat either a single node or a node set of constant size as a token. This tokenization method imposes an order between nodes that should be incomparable in the partial order, violating the inductive bias inherent in the DAG structure. We argue that the violation may hurt the generalization capability of generative models. Moreover, D-VAE suffers from a potentially slow generation process by generating one node per iteration. Although GraphPNAS generates multiple nodes per iteration, it uses a mixture of Bernoulli distributions to model intra-set edge dependencies, which is less expressive than diffusion models (Wang et al., 2023; Pearce et al., 2023). DiffusionNAG (An et al., 2023) employs diffusion models to generate only node attributes given a DAG structure. Other diffusion models have been developed for undirected graphs (Niu et al., 2020; Jo et al., 2022; Vignac et al., 2023a), but they ignore the directional information in DAGs, while our work demonstrates the necessity of the autoregressive component in modeling directional dependencies in DAGs. From the application perspective, all the existing works (Zhang et al., 2019; Li et al., 2023a; An et al., 2023) focus on generating small DAGs (with ##\##nodes ≤24absent24\leq 24≤ 24) for NAS, while our model is capable of generating much larger flow graphs (up to ∼similar-to\sim∼ 400 nodes) for system/hardware benchmarking. Overall, our work is the first to use autoregressive diffusion models for DAG generation, aiming to take the advantages of both autoregressive models and diffusion models to model the strong dependencies commonly in DAG data. We conduct extensive experiments to verify the effectiveness of our model. To assess the model’s ability to learn strong directional and logical rules, we construct a challenging synthetic DAG dataset with injected dependencies for evaluation. Additionally, we employ three real-world datasets— computational graphs on Tensor Processing Units (TPU), flow graphs on Field Programmable Gate Arrays (FPGA), and neural architectures deployed on edge devices — for computing system benchmarking applications. Each dataset contains thousands of DAGs, with individual DAGs comprising up to hundreds of nodes. We compare the validity and statistical properties of the synthetic DAGs generated by our model with baselines. To measure benchmarking performance, we use the synthetic labeled DAGs to train surrogate machine learning (ML) models to predict TPU runtime, FPGA resource usage, and the inference latency of neural architectures for the three application scenarios. These trained surrogate models are then applied to the real-world DAGs for testing. Note that ML-based surrogate models are widely used today to measure the performance of systems and programs without the need for time-consuming simulations (Adams et al., 2019; Chen et al., 2018; Mendis et al., 2019; Sỳkora et al., 2022; Steiner et al., 2021; Baghdadi et al., 2021; Zheng et al., 2020; Li et al., 2020; Ahn et al., 2020; Dubach et al., 2007; Jia et al., 2020; Kaufman et al., 2021; Cao et al., 2023; Phothilimthana et al., 2023b). We compare the predictions given by the surrogate models with the ground-truth behavior of the DAGs on the corresponding systems. Results show that the surrogate models trained on our generated synthetic DAGs consistently outperform the ones derived with baseline generative models. We also evaluate the extrapolation and interpolation properties of LayerDAG by generating DAGs with labels out of the regime used for training, and LayerDAG demonstrates a superior generalization capability."
https://arxiv.org/html/2411.02282v2,A Comprehensive Simulation Framework for CXL Disaggregated Memory,"Compute eXpress Link (CXL) has emerged as a key enabler of memory disaggregation for future heterogeneous computing systems to expand memory on-demand and improve resource utilization. However, CXL is still in its infancy stage and lacks commodity products on the market, thus necessitating a reliable system-level simulation tool for research and development. In this paper, we propose CXL-DMSim111Open sourced at https://github.com/ferry-hhh/CXL-DMSim., an open-source full-system simulator to simulate CXL disaggregated memory systems with high fidelity at a gem5-comparable simulation speed. CXL-DMSim incorporates a flexible CXL memory expander model along with its associated device driver, and CXL protocol support with CXL.io and CXL.mem. It can operate in both app-managed mode and kernel-managed mode, with the latter using a dedicated NUMA-compatible mechanism. The simulator has been rigorously verified against a real hardware testbed with both FPGA-based and ASIC-based CXL memory prototypes, which demonstrates the qualification of CXL-DMSim in simulating the characteristics of various CXL memory devices at an average simulation error of 4.1%. The experimental results using LMbench and STREAM benchmarks suggest that the CXL-FPGA memory exhibits a ∼similar-to\sim∼2.88×\times× higher latency than local DDR while the CXL-ASIC latency is ∼similar-to\sim∼2.18×\times×; CXL-FPGA achieves 45-69% of local DDR memory bandwidth, whereas the number for CXL-ASIC is 82-83%. We observed that the performance of CXL memory is 3×\times× more sensitive to Rd/Wr patterns than local DDR, with the max. bandwidth at 74%:26% rather than 50%:50% due to the current compromised CXL+DDR controller design. The study also reveals that CXL memory can significantly enhance the performance of memory-intensive applications, improved by 23×\times× at most with limited local memory for Viper and approximately 16% in bandwidth-sensitive scenarios such as MERCI. Moreover, the simulator’s observability and expandability are showcased with detailed case-studies, highlighting its great potential for research on future CXL-interconnected hybrid memory pool.","With the prevalence of massive data-driven applications such as AI/ML and big data analytics, the demand for larger memory is ever-increasing in today’s heterogeneous parallel computing systems. Over the past two decades, the CPU performance has been boosted dramatically thanks to Moore’s law and multi/many-core scaling. However, the memory capacity and bandwidth per core have been decreasing, which apparently poses a bottleneck for system performance [1]. In modern datacenters, the deployment unit is a monolithic server which contains closely-coupled computing and memory resources. This monolithic architecture for many years is always CPU-biased, leading to memory over-provision across the whole system. It has been observed that more than 50% of the aggregated memory is unused most of the time in production clusters at Google and Facebook [2]. Considering the rising DRAM chip prices in recent years, the under-utilization of memory resources becomes prohibitively expensive, which greatly boosts the TCO of datacenters [3]. With the advent of memory disaggregation technologies, new solutions can be explored to tackle the memory wall and memory under-utilization challenges [4]. Memory disaggregation technologies decouple memory resources from CPUs, providing a feasible option for memory pooling. Conventionally, Remote Direct Memory Access (RDMA) technology is exploited to realize memory disaggregation [2, 5, 6, 7, 8]. But RDMA is based on networking IO semantics which requires specialized NICs and software intervention, leading to a latency multiple orders of magnitude longer than that of local memory access. In recent years, several low-latency and high-bandwidth memory-coherent interconnect protocols arise in industry, which have shown advantages over RDMA for memory disaggregation [9, 10, 11, 12, 13, 14, 15]. Among them, the CXL protocol [16] is very promising and embraced by an increasingly number of semiconductor vendors worldwide. With CXL, memory expansion becomes more flexible over the interconnect fabric while enabling coherent memory access via load/store instructions. Furthermore, the CXL protocol is independent on the underlying memory technology, which can be DRAM, Flash, or even emerging non-volatile memories such as MRAM and RRAM. This facilitates the construction of a unified heterogeneous memory pool for future energy- and cost-efficient computing systems. Despite its attractive features disclosed in the protocol specifications, CXL is still in its infancy stage and lacks commodity products on the market. As a result, the prior research work on CXL-based memory disaggregation is conducted based on four main methods: software-based emulation [17], software-based simulation, [18, 19, 20], hardware-based emulation [21, 22, 23], and hardware prototyping [9, 24, 25]. However, the software-based emulation such as QEMU fails to model the physical characteristics and internal micro-architecture of real CXL memory devices. The simulation endeavors of gem5-CXL and CXLMemSim are both nascent, with gem5-CXL failing to accurately model the CXL protocol behavior and provide clear access interfaces, while CXLMemSim lacks full-system simulation capabilities and cycle-accurate fidelity, resulting in limited functionality and poor usability. The hardware-based emulation such as remote NUMA lacks CXL protocol support and there is a big difference in the memory access path and performance. As for the fourth method, there are currently no market-ready prototypes of CXL-based memory-disaggregated systems; CXL commodity products are also expensive to produce and purchase. Given the above limitations, there is a clear need for an accurate, cost-effective, and flexible tool for research on CXL-based disaggregated memory systems. In this paper, we present CXL-DMSim, a full-system CXL Disaggregated Memory Simulator based on gem5 for cycle-accurate simulation, architectural exploration, and evaluation of CXL-interconnected memory systems. CXL-DMSim is as easily configurable as the original gem5 simulator and fits to a variety of CXL devices. It has been rigorously verified and calibrated by a real-world CXL1.1 testbed with both an in-house ASIC CXL memory expander and an FPGA-based CXL device prototype. To the best of our knowledge, CXL-DMSim is the first usable full-system disaggregated memory simulator. The main contributions of this paper are listed below. • A flexible device model of CXL memory expander (Type 3 device) which currently supports both DRAM and Flash as underlying storage media. • Supports for CXL.io and CXL.mem sub-protocols, which are used to enumerate, configure, and access our CXL memory device on CXL-DMSim. • A driver for the device to operate in an application-managed mode and a NUMA-aware memory management mechanism to operate in a kernel-managed mode. • An extensive evaluation that validates CXL-DMSim including performance tests, usability&fidelity tests, real-world app. tests, and observation&expandability tests; this verifies the system’s feasibility and offers guidance for appropriate usage of CXL disaggregated memory."
https://arxiv.org/html/2411.00843v1,The Graph’s Apprentice: Teaching an LLM Low-Level Knowledge for Circuit QualityEstimation,"Logic synthesis is a crucial phase in the circuit design process, responsible for transforming hardware description language (HDL) designs into optimized netlists. However, traditional logic synthesis methods are computationally intensive, restricting their iterative use in refining chip designs. Recent advancements in large language models (LLMs), particularly those fine-tuned on programming languages, present a promising alternative. In this paper, we introduce VeriDistill, the first end-to-end machine learning model that directly processes raw Verilog code to predict circuit quality-of-result metrics. Our model employs a novel knowledge distillation method, transferring low-level circuit insights via graphs into the predictor based on LLM. Experiments show VeriDistill outperforms state-of-the-art baselines on large-scale Verilog datasets and demonstrates robust performance when evaluated on out-of-distribution datasets.","Rapid technological advancements in computing power has taken an increasingly important role in the past decades in driving scientific research in biology (Schatz, 2012), chemistry (Akimov & Prezhdo, 2015), physics (Dongarra & Keyes, 2024) and especially artificial intelligence, where it has been estimated that at least half of all performance gains in the past ten years have stemmed from hardware improvements alone (Hernandez & Brown, 2020; Dorner, 2021; Karpathy, ; Erdil & Besiroglu, 2022; Ho et al., 2024). This ever-rising demand for compute power means that efficient and effective electronic chip design has become increasingly critical. Modern electronic chip design is a complex, multi-stage endeavor that begins with a chip architect specifying the digital circuit’s functionality in a Hardware Description Language (HDL), such as Verilog (Thomas & Moorby, 2008) or VHDL (Coelho, 2012). This HDL code is then subjected to a series of transformations and optimizations, ultimately yielding a physical circuit design that can be manufactured (LaMeres, 2023). In a previous era where circuits were small and limited in functionality, this logic synthesis process was quick and the chip architect could quickly receive feedback and iterate on its HDL code. However, with the increasing complexity of industrial designs, which now can comprise hundreds of millions of logic gates (Amarú et al., 2017), even a single synthesis run has become massively expensive. This has driven the need for alternate ways of providing feedback on HDL code without running the actual logic synthesis process. A natural way to tackle this problem is to train a machine learning model that can take the HDL code as input, and output estimates of circuit quality such as wire length or delay that could have been computed had the logic synthesis process been run. A few works have approached this topic, by extracting graphical information about the code and using hand-designed statistics of those graphs as features (Zhou et al., 2019; Sengupta et al., 2022; Fang et al., 2023). Although these works had encouraging results, their performance has been limited by the relatively shallow understanding of the semantics of the code that these statistics can provide. Recently, Large Language Models fine-tuned on code, such as Code-T5 (Wang et al., 2021), Codex (Chen et al., 2021), CodeGen Nijkamp et al. (2023), CodeLlama (Roziere et al., 2023) and DeepSeek-Coder (Guo et al., 2024), have emerged that have proven remarkably successful on a wide range of tasks (Zheng et al., 2023), most notably as code assistants such as Github Copilot111https://github.com/features/copilot. Although most models are generalists trained on general-purpose programming languages such as C++ and Python, a few models, such as CodeGen-Verilog (Thakur et al., 2023), VeriGen (Thakur et al., 2024), RTLCoder (Liu et al., 2023c) and CodeV (Zhao et al., 2024), have been specifically trained on Verilog, the most popular HDL language. The analysis of these models, however, has been so far limited to investigating their ability to generate realistic code, and an investigation of the predictive power of those internal representations has been lacking. In this work, we demonstrate for the first time that the hidden states computed by these novel Verilog large language models contain rich insights which can be used to predict quality-of-result metrics with higher accuracy than previous machine learning models. Namely, we feed Verilog code to the state-of-the-art CodeV model, and train an inexpensive decoder neural network that uses the LLM’s hidden states as features to predict area and delay. In addition, and critically, we regularize this decoder to encourage its embeddings to resemble those of a graph neural network model trained on Look-Up Table (LUT) graph, an intermediate representation used during the logic synthesis process. The resulting decoder is shown to strongly outperform state-of-the-art baselines, and incidentally shows that those novel Verilog language models extract in their hidden states surprisingly complex insights about the circuits represented by raw code. Our work makes the following main contributions: 1. We develop the first truly end-to-end machine learning model in the literature, named VeriDistill, which can take raw Verilog code, without any preprocessing, and produce accurate estimates of circuit area/delay metrics. 2. Moreover, we apply during training a novel knowledge distillation method which allows to transfer low-level insights about the circuit, in the form of LUT graphs, back into the machine learning predictor model. 3. We demonstrate through experiments that the combination of those two elements outperforms previous state-of-the-art baselines in a large-scale Verilog dataset and enhances the model’s ability to transfer to out-of-distribution data. 4. Finally, we also demonstrate that both using LLM representations and the knowledge distillation are essential, in that removing any one of these components brings the performance back below the previous baselines. The remainder of this paper is structured as follows. Section 2 provides an overview of the relevant literature and background information. In Section 3, we present a detailed description of our proposed methodology, including its key components and underlying assumptions. The efficacy of our approach is then demonstrated through a series of experiments, which are reported in Section 4. Finally, Section 5 summarizes our main findings, discusses their implications, and outlines potential avenues for future research."
https://arxiv.org/html/2411.00439v1,Pandora’s Box in Your SSD: The Untold Dangers of NVMe,"Modern operating systems manage and abstract hardware resources, to ensure efficient execution of user workloads. The operating system must securely interface with often untrusted user code while relying on hardware that is assumed to be trustworthy. In this paper, we challenge this trust by introducing the eNVMe platform, a malicious NVMe storage device. The eNVMe platform features a novel, Linux-based, open-source NVMe firmware. It embeds hacking tools and it is compatible with a variety of PCI-enabled hardware. Using this platform, we uncover several attack vectors in Linux and Windows, highlighting the risks posed by malicious NVMe devices. We discuss available mitigation techniques and ponder about open-source firmware and open-hardware as a viable way forward for storage. While prior research has examined compromised existing hardware, our eNVMe platform provides a novel and unique tool for security researchers, enabling deeper exploration of vulnerabilities in operating system storage subsystems.","Non-Volatile Memory express (NVMe) is becoming the de-facto standard for fast Solid-State Drives (SSDs). Most computers sold nowadays come with one or more NVMe SSDs, which are thus ubiquitous. The NVMe standard is based on PCI Express (Peripheral Component Interconnect Express, or PCIe in short) a high-speed serial computer expansion bus. NVMe SSDs come equipped with multi-core embedded controllers and a number of Direct Memory Access (DMA) engines to handle the large number of Input-Output (IO) requests per second (IOPS) and are capable of bandwidths up to tens of gigabytes per second. Today’s storage controllers are no longer dumb devices and pack substantial compute power, often with specialized co-processors, crypto-engines, inline compression, and more. The capabilities of NVMe storage are growing by the day through efforts such as computational storage, backed by the NVMe standard itself [1], and the Storage Networking Industry Association (SNIA) technical works on computational storage [2]. The combination of this processing power and the strategic position in the hardware architecture of a computer, make NVMe SSDs the perfect vector for large-scale attacks and a cyber-warfare super-weapon. To the best of our knowledge, storage-based attacks remain a largely unexplored threat vector. In this work, we try to raise awareness of the serious threats that an evil NVMe device can pose to the global computing infrastructure. To this end, we designed a low-cost, Linux based, NVMe capable research platform. We implanted this platform in several hardware machines running Linux and Windows Operating Systems (OS) and used it to take full control of our targets. We started with simple DMA attacks when the victim’s hardware configuration allowed it. When this was not possible, we used the device’s storage capabilities to develop new attack vectors. We open this work with a cautionary tale depicting possible scenarios of weaponized NVMe SSDs. We hope it motivates the necessity of a research platform to study these scenarios; After the story, we provide the necessary background and introduce our research platform. We then demonstrate a number of attacks that make the initial story-line less fictional than it may appear at first glance. Thereafter, we discuss mitigation techniques and their limits. Finally, we conclude with a call to action for a transparent storage system initiative. Our main contributions in this paper are the following: • A low-cost and fully open-source platform to explore the security implications of an evil NVMe device; • A demonstration that many systems today are vulnerable to storage-specific attacks; • A number of reproducible attacks; • Storage-specific attacks demonstrating that, even if a target computer is properly configured and protected against traditional DMA attacks, it is still vulnerable from its internal storage. Available on GitHub: https://github.com/rick-heig/eNVMe 1.1 A cautionary tale Let’s imagine government X has control over company ΦΦ\Phiroman_Φ. ΦΦ\Phiroman_Φ is a well-respected NVMe controller chip maker. ΦΦ\Phiroman_Φ’s controllers are used by big storage brands all around the world. These storage brands have been selling NVMe drives with ΦΦ\Phiroman_Φ’s controllers for several years and have a large market share: they can be found in most laptops, workstations, and data centers. It turns out that government X tasked ΦΦ\Phiroman_Φ to add some extra capabilities and dormant code inside its controllers. A war breaks out and government X made enemies all around the world. In face of this global conflict, government X decides to unleash its weapon, an army of NVMe SSDs! X does not have direct remote access to the NVMe SSDs, however, let’s say it decides to launch a promotional ad campaign on the Internet for a trendy online marketplace. The ad campaign uses cookie technology for tracking. The tracking cookie contains a unique binary key. People from all around the world see the ads while browsing popular sites or when using their search engine. The cookie gets stored on the NVMe SSD. It turns out the unique key value in the cookie is actually the nuclear button for the NVMe SSDs to turn evil, apocalypse ensues… Although this is an imaginary tale, there have been multiple reports of firmware poisoning [3, 4, 5], intentionally malicious hardware [6, 7, 8, 9], and governmental implication in the development of hardware cyber-warfare and cyber-security tools [10, 11, 12, 13]. Therefore, this might actually be unfolding right now, behind our backs, and dormant malicious NVMe SSDs may already be installed in a number of PCs. 1.1.1 The apocalypse Death Upon being triggered by the activation unique key, all NVMe SSDs with a ΦΦ\Phiroman_Φ controller destroy themselves. They will never start again, all their data is erased, the chip destroyed itself, millions of computers are non-operational. This is, however, quite brutal and easily detectable. While a possible attack, other more subtle and powerful approaches may be devised. Conquest Upon activation, the SSDs take control of their host operating systems, they now have total control of millions of machines, all conquered, silently. Famine The ΦΦ\Phiroman_Φ-equipped SSDs have been scanning their own contents in the background for years, they know people and companies have been storing their private access keys, cryptocurrency wallets, seed phrases, etc. on the SSD. After conquest, all wallets and keys are sent back to a given IP address controlled by X. Government X is now the de-facto cryptocurrency king! All wallets and funds are moved in one go, the victims are now starving and X possesses a new gigantic war fund. War The SSDs can spy on all files stored on it, but also in most cases all files on the computer, not only the files inside the SSD. The SSD has been mining for interesting, secret, documents, intelligence of any kind, it can silently transfer them to one of the attacker servers. Besides spying, the SSDs can also alter any content, inject malware, and use the host machine for its own evil schemes. 1.2 Motivation Although the imagined scenarios sound right out of a science-fiction thriller, they can be very close to reality. Our goal with this work is to raise awareness about how vulnerable computers are from storage devices, and provide a platform to study NVMe-based attacks. Most of the state-of-the-art research in NVMe SSD security is focused on the software and driver side and very little research is available on the device itself being the attack vector. The National Institute of Standards and Technology (NIST) released security guidelines for storage infrastructure [14] that formalizes the threats, risks, and attack surface while discussing security guidelines for the storage infrastructure. The document lists the following threats: Credential theft or compromise, Cracking encryption, Infection of malware and ransomware, Backdoors and unpatched vulnerabilities, Privilege escalation, Human error and deliberate misconfiguration, Physical theft of storage media, Insecure images, software and firmware. To the best of our knowledge, there is no security-oriented publicly available NVMe hardware research platform for testing these kinds of attacks from the disk itself rather than from within the storage infrastructure. In this work we fill this gap by providing a fully open-source platform for the development of NVMe compliant hardware focused on security and penetration testing."
