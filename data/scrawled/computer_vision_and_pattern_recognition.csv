URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.05007v1,SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models,"Diffusion models have been proven highly effective at generating high-quality images. However, as these models grow larger, they require significantly more memory and suffer from higher latency, posing substantial challenges for deployment. In this work, we aim to accelerate diffusion models by quantizing their weights and activations to 4 bits. At such an aggressive level, both weights and activations are highly sensitive, where conventional post-training quantization methods for large language models like smoothing become insufficient. To overcome this limitation, we propose SVDQuant, a new 4-bit quantization paradigm. Different from smoothing which redistributes outliers between weights and activations, our approach absorbs these outliers using a low-rank branch. We first consolidate the outliers by shifting them from activations to weights, then employ a high-precision low-rank branch to take in the weight outliers with Singular Value Decomposition (SVD). This process eases the quantization on both sides. However, naïvely running the low-rank branch independently incurs significant overhead due to extra data movement of activations, negating the quantization speedup. To address this, we co-design an inference engine Nunchaku that fuses the kernels of the low-rank branch into those of the low-bit branch to cut off redundant memory access. It can also seamlessly support off-the-shelf low-rank adapters (LoRAs) without the need for re-quantization. Extensive experiments on SDXL, PixArt-ΣΣ\Sigmaroman_Σ, and FLUX.1 validate the effectiveness of SVDQuant in preserving image quality. We reduce the memory usage for the 12B FLUX.1 models by 3.5×, achieving 3.0× speedup over the 4-bit weight-only quantized baseline on the 16GB laptop 4090 GPU, paving the way for more interactive applications on PCs. Our quantization library***Quantization library: github.com/mit-han-lab/deepcompressor and inference engine†††Inference Engine: github.com/mit-han-lab/nunchaku are open-sourced.","Diffusion models have shown remarkable capabilities in generating high-quality images (Ho et al., 2020), with recent advances further enhancing user control over the generation process. Trained on vast data, these models can create stunning images from simple text prompts, unlocking diverse image editing and synthesis applications (Meng et al., 2022b; Ruiz et al., 2023; Zhang et al., 2023). Figure 2: Computation vs. parameters for LLMs and diffusion models. LLMs’ computation is measured with 512 context and 256 output tokens, and diffusion models’ computation is for a single step. Dashed lines show trends. To pursue higher image quality and more precise text-to-image alignment, researchers are increasingly scaling up diffusion models. As shown in Figure 2, Stable Diffusion (SD) (Rombach et al., 2022) 1.4 only has 800M parameters, while SDXL (Podell et al., 2024) scales this up to 2.6B parameters. AuraFlow v0.1 (fal.ai, 2024) extends this further to 6B parameters, with the latest model, FLUX.1 (Black-Forest-Labs, 2024), pushing the boundary to 12B parameters. Compared to large language models (LLMs), diffusion models are significantly more computationally intensive. Their computational costs‡‡‡Computational cost is measured by number of Multiply-Accumulate operations (MACs). 1 MAC=2 FLOPs. increase more rapidly with model size, posing a prohibitive memory and latency barrier for real-world model deployment, particularly for interactive use cases that demand low latency. As Moore’s law slows down, hardware vendors are turning to low-precision inference to sustain performance improvements. For instance, NVIDIA’s Blackwell Tensor Cores introduce a new 4-bit floating point (FP4) precision, doubling the performance compared to FP8 (NVIDIA, 2024). Therefore, using 4-bit inference to accelerate diffusion models is appealing. In the realm of LLMs, researchers have leveraged quantization to compress model sizes and boost inference speed (Dettmers et al., 2022; Xiao et al., 2023). However, unlike LLMs–where latency is primarily constrained by loading model weights, especially with small batch sizes–diffusion models are heavily computationally bound, even with a single batch. As a result, weight-only quantization cannot accelerate diffusion models. To achieve speedup, both weights and activations must be quantized to the same bit width; otherwise, the lower-precision side will be upcast during computation, negating potential performance enhancements. In this work, we focus on quantizing both the weights and activations of diffusion models to 4 bits. This challenging and aggressive scheme is often prone to severe quality degradation. Existing methods like smoothing (Xiao et al., 2023; Lin et al., 2024a), which attempt to transfer the outliers between the weights and activations, are less effective since both sides are highly vulnerable to outliers. To address this issue, we propose a new general-purpose quantization paradigm, SVDQuant. Our core idea is to introduce a low-cost branch to absorb outliers on both sides. To achieve this, as illustrated in Figure 3, we first aggregate the outliers by migrating them from activation 𝑿𝑿{\bm{X}}bold_italic_X to weight 𝑾𝑾{\bm{W}}bold_italic_W via smoothing. Then we apply Singular Value Decomposition (SVD) to the updated weight, 𝑾^^𝑾\hat{{\bm{W}}}over^ start_ARG bold_italic_W end_ARG, decomposing it into a low-rank branch 𝑳1⁢𝑳2subscript𝑳1subscript𝑳2{\bm{L}}_{1}{\bm{L}}_{2}bold_italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT and a residual 𝑾^−𝑳1⁢𝑳2^𝑾subscript𝑳1subscript𝑳2\hat{{\bm{W}}}-{\bm{L}}_{1}{\bm{L}}_{2}over^ start_ARG bold_italic_W end_ARG - bold_italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. The low-rank branch operates at 16 bits, allowing us to quantize only the residual to 4 bits, which has significantly reduced outliers and magnitude. However, naively running the low-rank branch separately incurs substantial memory access overhead, offsetting the speedup of 4-bit inference. To overcome this, we co-design a specialized inference engine Nunchaku, which fuses the low-rank branch computation into the 4-bit quantization and computation kernels. This design enables us to achieve measured inference speedup even with additional branches. SVDQuant can quantize various text-to-image diffusion architectures, including both UNet (Ho et al., 2020; Ronneberger et al., 2015) and DiT (Peebles & Xie, 2023) backbones, into 4 bits, while maintaining visual quality. It supports both INT4 and FP4 data types, and integrates seamlessly with pre-trained low-rank adapters (LoRA) (Hsu et al., 2022) without requiring re-quantization. To our knowledge, we are the first to successfully apply 4-bit post-training quantization to both the weights and activations of diffusion models, and achieve measured speedup on NVIDIA GPUs. On the latest 12B FLUX.1, we largely preserve the image quality and reduce the memory footprint of the original BF16 model by 3.5× and deliver a 3.0× speedup over the NF4 weight-only quantized baseline, measured on a 16GB laptop-level RTX4090 GPU. See Figure 1 for visual examples. Figure 3: Overview of SVDQuant. (a) Originally, both the activation 𝑿𝑿{\bm{X}}bold_italic_X and weight 𝑾𝑾{\bm{W}}bold_italic_W contain outliers, making 4-bit quantization challenging. (b) We migrate the outliers from the activation to weight, resulting in the updated activation 𝑿^^𝑿\hat{{\bm{X}}}over^ start_ARG bold_italic_X end_ARG and weight 𝑾^^𝑾\hat{{\bm{W}}}over^ start_ARG bold_italic_W end_ARG. While 𝑿^^𝑿\hat{{\bm{X}}}over^ start_ARG bold_italic_X end_ARG becomes easier to quantize, 𝑾^^𝑾\hat{{\bm{W}}}over^ start_ARG bold_italic_W end_ARG now becomes more difficult. (c) SVDQuant further decomposes 𝑾^^𝑾\hat{{\bm{W}}}over^ start_ARG bold_italic_W end_ARG into a low-rank component 𝑳1⁢𝑳2subscript𝑳1subscript𝑳2{\bm{L}}_{1}{\bm{L}}_{2}bold_italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT and a residual 𝑾^−𝑳1⁢𝑳2^𝑾subscript𝑳1subscript𝑳2\hat{{\bm{W}}}-{\bm{L}}_{1}{\bm{L}}_{2}over^ start_ARG bold_italic_W end_ARG - bold_italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT with SVD. Thus, the quantization difficulty is alleviated by the low-rank branch, which runs at 16-bit precision."
https://arxiv.org/html/2411.05006v1,\themodel: Simple Progression is All You Needfor High-Quality 3D Scene Editing,"This paper proposes \themodel – a simple yet effective framework for high-quality 3D scene editing guided by diffusion distillation in a novel progressive manner. Inspired by the crucial observation that multi-view inconsistency in scene editing is rooted in the diffusion model’s large feasible output space (FOS), our framework controls the size of FOS and reduces inconsistency by decomposing the overall editing task into several subtasks, which are then executed progressively on the scene. Within this framework, we design a difficulty-aware subtask decomposition scheduler and an adaptive 3D Gaussian splatting (3DGS) training strategy, ensuring high quality and efficiency in performing each subtask. Extensive evaluation shows that our \themodelachieves state-of-the-art results in various scenes and challenging editing tasks, all through a simple framework without any expensive or sophisticated add-ons like distillation losses, components, or training procedures. Notably, \themodelalso provides a new way to control, preview, and select the “aggressivity” of editing operation during the editing process.","The emergence and advancement of modern scene representation models, exemplified by neural radiance fields (NeRFs) [1] and 3D Gaussian splatting (3DGS) [2], have significantly reduced the difficulty associated with high-quality reconstruction and rendering of large-scale scenes. In addition to reconstructing known scenes, there is growing interest in editing existing scenes to create new ones. Among the various editing operations, the instruction-guided scene editing (IGSE) stands out as one of the most free-form tasks, supporting editing based on simple text descriptions. Due to the lack of 3D supervision data to train editing models in 3D, current state-of-the-art methods tackle IGSE using 2D diffusion distillation, which involves distilling editing signals from a pre-trained 2D diffusion model [3, 4]. These methods leverage the 2D diffusion model to edit rendered images of scenes from multiple viewpoints, and then reconstruct the edited scene from these edited images using specific distillation losses. However, a substantial challenge faced by such distillation-based approaches in achieving high-quality scene editing lies in ensuring that the scene representation converges on the edited multi-view images. Failure to achieve so results in gloomy colors, blurred textures, and noisy geometries (e.g., the failure cases from [5]). We argue that this challenge is rooted in the diffusion model’s large feasible output space (FOS) for the same instruction – since a text instruction can be interpreted in different yet plausible ways. For example, “make the person wear a hat” could be implemented with a hat of any style, shape, size, position, etc. Therefore, large FOS is the underlying cause of multi-view inconsistency in 2D editing results, making the scene representation – originally designed for reconstructing from consistent images – hard to converge. Previous work, often unaware of this fundamental issue, deals with multi-view inconsistency by introducing inconsistency-robust distillation losses [6, 7] to tolerant inconsistency, or proposing additional components and training procedures [8, 9] to select consistent images from the FOS. While adding costs and complexities, these methods frequently fail to converge to a high-quality scene when the FOS is considerably large, especially for operations that change the scene’s geometry. In overcoming this challenge posed by the large FOS, our key insight is to control the FOS size through editing task decomposition, as illustrated in Fig. 1. Building on this insight, we propose \themodel, a simple, novel framework to achieve high-quality IGSE, by decomposing the original, large-FOS task into multiple subtasks with significantly smaller FOS, and then progressively performing high-quality editing for each of these tasks. With each subtask’s FOS effectively controlled, they can be solved under a simple solution without the need for additional distillation losses, components, or complex training procedures. Progressively solving all these subtasks naturally leads to a high-quality edited scene that meets the requirements of the original task. To perform subtask decomposition, we introduce an intuitive formulation of “subtasks” with text encoding interpolation. Based on this formulation, we propose a subtask scheduler to determine the subtask decomposition and guide the editing process. This decomposition consists of a sequence of subtasks, where each subtask is applied to the edited scene from the previous one. We adaptively assign subtasks according to the estimated FOS size, so that each subtask has comparable FOS sizes and difficulty levels and can thus be solved relatively easily with high quality and efficiency. Guided by the subtask scheduler, we progressively iterate on the subtasks to apply editing. Though their FOS size and difficulty are controlled, it still remains non-trivial to make the scene representation converge in precise geometry. Failing to achieve this will accumulate errors across subtasks, leading to unreasonable geometry in the final results. To this end, we choose 3D Gaussian splatting (3DGS) [2] as our scene representation for its high training efficiency. We design a novel adaptive Gaussian creation strategy in training to maintain and refine the geometric structure in each subtask, by controlling the size of the splitting and duplication operations. This strategy allows the geometry to be adjusted toward the goal of each subtask, while preventing and removing floc, floating noise, and multi-face structures. With these key designs, our \themodelachieves high-quality instruction-guided scene editing in various scenes and editing tasks with precise geometry and detailed textures, as shown in Fig. 1. Notably, \themodeldoes not rely on complicated or expensive add-ons, such as specialized distillation losses, additional 3D attention or convolution components, or extended training procedures on the diffusion model. Moreover, as each subtask represents a partial completion of the overall task, our method enables users to control, preview, and select the intermediate stages of editing, which we refer to as “aggressivity” of editing operation during the editing process. This can be simply achieved by taking the edited scene from a subtask either during or after the editing process. Thus, in contrast to previous methods such as classifier-free guidance [10] and SDEdit [11], our \themodelprovides a novel way to monitor and manage the editing process. Users can preview different versions of editing with the intermediate outcomes, adjust the subtasks on the fly accordingly to achieve improved final results, and finally select the most satisfactory editing result from all the intermediate ones. Our contributions are three-fold. (1) We offer a novel insight into subtask decomposition and progressive editing, tailored to address the core challenge of large feasible output space in 3D scene editing. (2) We propose a simple yet effective framework, \themodel, that generates high-quality edited scenes by progressively solving each subtask, without requiring any complicated or expensive add-ons to the diffusion model, while also supporting control, training-time preview, and selection of editing task aggressivity. (3) We consistently achieve high-quality editing results in various scenes and challenging tasks, establishing state-of-the-art performance."
https://arxiv.org/html/2411.05005v1,Diff-2-in-1: Bridging Generation and Dense Perception with Diffusion Models,"Beyond high-fidelity image synthesis, diffusion models have recently exhibited promising results in dense visual perception tasks. However, most existing work treats diffusion models as a standalone component for perception tasks, employing them either solely for off-the-shelf data augmentation or as mere feature extractors. In contrast to these isolated and thus sub-optimal efforts, we introduce a unified, versatile, diffusion-based framework, Diff-2-in-1, that can simultaneously handle both multi-modal data generation and dense visual perception, through a unique exploitation of the diffusion-denoising process. Within this framework, we further enhance discriminative visual perception via multi-modal generation, by utilizing the denoising network to create multi-modal data that mirror the distribution of the original training set. Importantly, Diff-2-in-1 optimizes the utilization of the created diverse and faithful data by leveraging a novel self-improving learning mechanism. Comprehensive experimental evaluations validate the effectiveness of our framework, showcasing consistent performance improvements across various discriminative backbones and high-quality multi-modal data generation characterized by both realism and usefulness.","Diffusion models have emerged as powerful generative modeling tools for various high-fidelity image synthesis tasks (Song et al., 2021; Ho et al., 2020; Rombach et al., 2022; Zhang et al., 2023b). Beyond their primary synthesis capabilities, diffusion models are increasingly recognized for their expressive representation abilities. This has spurred interest in leveraging them for dense pixel-level visual perception tasks, such as semantic segmentation (Baranchuk et al., 2022; Wu et al., 2023; Xu et al., 2023a) and depth estimation (Saxena et al., 2023b; Zhao et al., 2023). Nonetheless, most existing approaches treat diffusion models as a standalone component for perception tasks, either employing them for off-the-shelf data augmentation (Burg et al., 2023), or utilizing the diffusion network as feature extraction backbone (Xu et al., 2023a; Zhao et al., 2023; Ji et al., 2023; Saxena et al., 2023a). These efforts overlook the unique diffusion-denoising process inherent in diffusion models, thus limiting their potential for discriminative dense visual perception tasks. Inspired by foundational studies that explore the interplay between generative and discriminative learning (Rubinstein & Hastie, 1997; Ng & Jordan, 2001; Raina et al., 2003; Ulusoy & Bishop, 2005), we argue that the diffusion-denoising process plays a critical role in unleashing the capability of diffusion models for the discriminative visual perception tasks. The diffusion process corrupts the visual input with noise, enabling the generation of abundant new data with diversity. Subsequently, the denoising process removes the noise from noisy images to create high-fidelity data, thus obtaining informative features for discriminative tasks at the same time. As a result, the diffusion-denoising process naturally connects the generative process with discriminative learning. Interestingly, this synergy further motivates us to propose a novel unified diffusion modeling framework that integrates both discriminative and generative learning within a single, coherent paradigm. From the generative perspective, we focus on synthesizing photo-realistic multi-modal paired data (i.e., RGB images and their associated pixel-level visual attributes) that accurately capture various types of visual information. Simultaneously, the unified diffusion model can achieve promising results in different visual prediction tasks from the discriminative standpoint. As an example illustrated in Figure 1, when considering RGB and depth interactions, if the model receives an RGB image as input, its function is to predict an accurate depth map. Meanwhile, the model is equipped to produce photo-realistic and coherent RGB-depth pairs sampled from noise. Despite its conceptual simplicity, fully operationalizing the unified framework – acquiring enhanced performance for both multi-modal generation and dense perception such as by effectively leveraging generated samples for discriminative tasks – presents non-trivial challenges. In particular, the generation process inevitably produces data of relatively inferior quality compared to real data. Additionally, generated samples may exhibit considerable data distribution gaps from the target domain. Figure 1: A single, unified diffusion-based model for both generative and discriminative learning. If the model receives an RGB image as input, its function is to predict an accurate visual attribute map. Simultaneously, the model is equipped to produce photo-realistic and coherent multi-modal data sampled from Gaussian noise. We use depth as an example here for illustration, and the framework is also applicable to other visual attributes such as segmentation, surface normal, etc. To address these challenges, we introduce Diff-2-in-1, a diffusion framework bridging multi-modal generation and discriminative dense visual perception within one unified diffusion model. The core design within our Diff-2-in-1 is a self-improving learning mechanism, featuring two sets of parameters for our unified diffusion model during the training process. Specifically, the creation parameters are tailored to generate additional multi-modal data for discriminative learning, while the exploitation parameters are employed for utilizing both the original and synthetic data to learn the discriminative dense visual perception task. Meanwhile, the creation parameters continuously undergo self-improvement based on the weights of the exploitation parameters via exponential moving average (EMA). With our novel design of two sets of parameters interplaying with each other, the discriminative learning process can benefit from the synthetic samples generated by the model itself, while the quality of the generated data is iteratively refined at the same time. We validate the effectiveness of Diff-2-in-1 through extensive and multi-faceted experimental evaluations. We start with the evaluation of the discriminative perspective, demonstrating its superiority over state-of-the-art discriminative baselines across various tasks in both single-task and multi-task settings. We additionally show that Diff-2-in-1 is generally applicable to different backbones and consistently boosts performance. Next, we ablate the experimental settings such as different training data sizes, to gain a comprehensive understanding of our method. Finally, we demonstrate the realism and usefulness of the multi-modal data generated by our Diff-2-in-1. Our contributions include: (1) We propose Diff-2-in-1, a unified framework that seamlessly integrates multi-modal generation and discriminative dense visual perception based on diffusion models. (2) We introduce a novel self-improving mechanism that progressively enhances multi-modal generation in a self-directed manner, thereby effectively boosting the discriminative visual perception performance via generative learning. (3) Our method demonstrates consistent performance improvements across various discriminative backbones and high-quality multi-modal data generation under both realism and usefulness."
https://arxiv.org/html/2411.05003v1,ReCapture: Generative Video Camera Controls forUser-Provided Videos using Masked Video Fine-Tuning,"Recently, breakthroughs in video modeling have allowed for controllable camera trajectories in generated videos. However, these methods cannot be directly applied to user-provided videos that are not generated by a video model. In this paper, we present ReCapture, a method for generating new videos with novel camera trajectories from a single user-provided video. Our method allows us to re-generate the reference video, with all its existing scene motion, from vastly different angles and with cinematic camera motion. Notably, using our method we can also plausibly hallucinate parts of the scene that were not observable in the reference video. Our method works by (1) generating a noisy anchor video with a new camera trajectory using multiview diffusion models or depth-based point cloud rendering and then (2) regenerating the anchor video into a clean and temporally consistent reangled video using our proposed masked video fine-tuning technique.","Recently, diffusion models have enabled significant advances in video generation and editing [35, 76, 31, 105, 103, 32, 94, 10, 109, 66, 107, 55, 99, 27], revolutionizing workflows in digital content creation. Camera control plays a vital role in practical applications of video generation and editing, allowing for greater customization and stronger user experience. Recent efforts have introduced camera control capabilities to video diffusion models [29, 2, 89, 38], yet, in this case the videos are entirely generated by the video model from a text-prompt and are neither captured in the real world, nor provided by a user. Effectively generating new videos with user-specified camera motion from an existing user-provided video that contains complex scene motion is still an open and challenging problem. The task is inherently ill-posed due to the limited amount of information in the reference video: one cannot know exactly how the scene looks like from all angles if there is not full knowledge of the scene’s 4D content. However, this does not preclude an approximate solution that is plausible and appreciated by users. Previous studies have shown promising results by generally assuming the availability of synchronized multi-viewpoint videos [64] and constructing 4D neural representations. Later works [87, 53, 45, 92, 80] enable 4D reconstruction using a single monocular video, but require accurate camera pose and depth estimation, and cannot capture content outside the original field of view. In this paper, we reformulate this problem as a video-to-video translation task. Camera Dolly [82] also develops a video-to-video pipeline, but requires 4D video data with different camera poses obtained via simulation, which limits it to in-domain scenes like driving or cubic objects. Given the challenge of obtaining paired videos in the wild with varying camera movements, it is hard to solve this problem with a video-to-video pipeline in an end-to-end manner and we separate it into two steps instead. Our approach leverages the prior knowledge of diffusion generative models, in both image and video domains, to effectively reangle the video as if filmed from the requested camera trajectory. For the first stage of our method, we want to generate an incomplete anchor video conditioned on the user-provided camera trajectory and reference video. We initially obtain a partial frame-by-frame depth estimation of the target video. We project each frame into 3D space using a depth estimator to obtain a sequence of point clouds. Then, we simulate the user-specified camera movement, which can include zoom, pan, and tilt, and render the point cloud sequence according to the new camera trajectory. This estimation is only partial since, as illustrated in Figure 4, these camera movements can introduce black areas outside the original video boundaries, cause some blurring due to the nature of point cloud projection and have poor temporal consistency since they are generated frame-by-frame. Another way to obtain the noisy anchor video, that uses recent advances in 3D reconstruction, is to use a multiview diffusion model [24] conditioned on camera pose and individual video frames. This method also results in an anchor video that has poor temporal consistency, along with blurring, artifacts and black areas outside the scene. Using this anchor video our method is able to generate a clean output with the desired camera trajectory. To achieve this we propose the novel technique of masked video fine-tuning. This technique consists of training a context-aware spatial LoRA and temporal motion LoRA on the known pixels from the generated anchor video, as well as from additional reference frame data. Specifically, the spatial LoRA is incorporated into the spatial layers of the video diffusion model and finetuned on augmented frames extracted from the source video. This enables the model to learn the subject’s appearance and the background context of the source video. The temporal LoRA enables the model to learn the scene motion with respect to the new camera trajectory, and is inserted into the temporal layers of the video diffusion model and finetuned using a masked loss on the anchor video. Unknown regions are masked, which excludes them from the loss computation, enabling the model to focus on meaningful and known regions and motion while ignoring the unknown areas. During inference, equipped with both video specific spatial and temporal LoRAs, the diffusion model can automatically fill the unknown regions of the anchor video with plausible content, leveraging the video diffusion model’s prior and the context provided by the spatial LoRA. It also significantly improves temporal consistency and removes anchor video jittering. This results in a coherent and meaningful video output, preserving the motion and layout of the original anchor video as learned through the temporal LoRA training. Finally, as a refining step, we can remove the temporal LoRA and retain only the context-aware spatial LoRA to apply SDEdit [56] to the generated video, thereby further reducing blurring and improving temporal consistency. In the end, we generate a video with new camera trajectories while preserving the original complex scene motion and the full content of the source video. Notably, this is accomplished without the need for paired video data. Ultimately, our method outperforms the generative approach Generative Camera Dolly [82], which requires paired videos as training data, and other 4D reconstruction methods [93, 48] on the Kubric dataset [82]. Furthermore, each component of our proposed method is validated through ablation studies on VBench [40]."
https://arxiv.org/html/2411.04998v1,HourVideo: 1-Hour Video-Language Understanding,"We present HourVideo, a benchmark dataset for hour-long video-language understanding. Our dataset consists of a novel task suite comprising summarization, perception (recall, tracking), visual reasoning (spatial, temporal, predictive, causal, counterfactual), and navigation (room-to-room, object retrieval) tasks. HourVideo includes 500 manually curated egocentric videos from the Ego4D dataset, spanning durations of 20 to 120 minutes, and features 12,976 high-quality, five-way multiple-choice questions. Benchmarking results reveal that multimodal models, including GPT-4 and LLaVA-NeXT, achieve marginal improvements over random chance. In stark contrast, human experts significantly outperform the state-of-the-art long-context multimodal model, Gemini Pro 1.5 (85.0% vs. 37.3%), highlighting a substantial gap in multimodal capabilities. Our benchmark, evaluation toolkit, prompts, and documentation are available at hourvideo.stanford.edu.","Humans demonstrate a remarkable ability to process visual stimuli over long time horizons, enabling them to perceive, plan and act in the real world. Consider the routine task of cooking a meal. This activity involves a continuous and adaptive visual process: identifying and using ingredients and tools, monitoring state changes of various dishes, and adjusting cooking duration/techniques based on visual cues such as color and texture. Such sustained visual processing is crucial to achieving the desired culinary outcomes. Naturally, endowing autonomous agents with this capability has been a long-standing goal in the field of Artificial Intelligence. In recent years, large multimodal models [alayrac2022flamingo, openai2023gpt4v, reid2024gemini] have emerged as a promising approach toward achieving this goal. Typically, these models are evaluated using multiple datasets that test capabilities such as object recognition [Russakovsky2014ImageNetLS, Lin2014MicrosoftCC], image comprehension [antol2015vqa, singh2019text_vqa, hudson2019gqa], and action recognition [kay2017kinetics]. However, these benchmarks are often restricted to single images or short video clips, usually lasting from a few seconds to no more than three minutes [kay2017kinetics, zhou2018towards, yu2019activitynet, mangalam2024egoschema]. While these benchmarks have spurred significant advancements, a deeper exploration into long-form video-language understanding is essential to develop multimodal systems that can form the basis for future autonomous agents and assistants. A significant challenge in evaluating long-form video-language understanding capabilities is designing tasks that genuinely necessitate long-term comprehension, i.e., tasks that require long-range dependencies. Merely posing questions that can be answered by watching a brief segment of a lengthy video effectively reduces the task to a combination of temporal localization and short-clip understanding. Furthermore, while intriguing narrative inquiries can certainly be formulated for long-form videos such as television shows and films, it is imperative to ensure that the questions are not trivially answerable due to the vast prior knowledge encoded in modern large language models. In this work, we introduce HourVideo—a benchmark dataset designed for long-form video-language understanding. To design tasks that require long-term comprehension, we first propose a novel task suite (Tab. LABEL:table_main:task_suite), comprising summarization, perception (recall, tracking), visual reasoning (spatial, temporal, predictive, causal, counterfactual), and navigation (room-to-room, object retrieval) tasks. For each task, we manually create question prototypes designed to ensure that correctly answering them requires identification and synthesis of information across multiple temporal segments within the long-form videos. Guided by our task suite, we curated 500 egocentric videos from the Ego4D dataset [grauman2022ego4d]—covering 77 unique everyday activities and ranging from 20 to 120 minutes—to generate questions based on our prototypes. The combination of our comprehensive task suite and everyday mundane egocentric videos provides a robust framework to rigorously evaluate multimodal models’ capabilities in understanding long-form videos. Finally, we developed a question-answer generation pipeline utilizing the expertise of trained human annotators (800+ hours of effort) and large language models (LLMs), resulting in a collection of 12,976 high-quality, five-way multiple-choice questions. We comprehensively evaluate state-of-the-art multimodal models on HourVideo (Tab. LABEL:table_main:baseline_results, Fig. LABEL:fig_main:spider_diagram), including GPT-4V [openai2023gpt4v], Gemini 1.5 Pro [reid2024gemini], and LLaVA-NeXT [zhang2024llavanextvideo] in a zero-shot setting. Our findings reveal that GPT-4V and LLaVA-NeXT achieve only marginal improvements over a random predictor (20%), obtaining accuracies of 25.7% and 22.3%, respectively. Gemini 1.5 Pro, designed specifically for long-context multimodal understanding, obtains an accuracy of 37.3%, which, while better, is still substantially lower than the average performance of human experts at 85.0%. These results suggest that while the multimodal community has made meaningful progress, a significant gap remains to be bridged before these systems can match human-level long-form video understanding capabilities. Progress in long-form video understanding could enable new applications including AR assistants, embodied agents, and interactive video platforms. We hope that HourVideo will serve as a benchmark to facilitate research in this direction and enable the development of multimodal models that can understand endless streams of visual data. Figure 1: Example MCQs from HourVideo for different tasks. The correct answers are underlined."
https://arxiv.org/html/2411.04997v1,LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation,"CLIP is one of the most important multimodal foundational models today, aligning visual and textual signals into a shared feature space using a simple contrastive learning loss on large-scale image-text pairs. What powers CLIP’s capabilities? The rich supervision signals provided by natural language — the carrier of human knowledge — shape a powerful cross-modal representation space. As a result, CLIP supports a variety of tasks, including zero-shot classification, detection, segmentation, and cross-modal retrieval, significantly influencing the entire multimodal domain. However, with the rapid advancements in large language models (LLMs) like GPT-4 and LLaMA, the boundaries of language comprehension and generation are continually being pushed. This raises an intriguing question: can the capabilities of LLMs be harnessed to further improve multimodal representation learning? The potential benefits of incorporating LLMs into CLIP are clear. LLMs’ strong textual understanding can fundamentally improve CLIP’s ability to handle image captions, drastically enhancing its ability to process long and complex texts — a well-known limitation of vanilla CLIP. Moreover, LLMs are trained on a vast corpus of text, possessing open-world knowledge. This allows them to expand on caption information during training, increasing the efficiency of the learning process. However, realizing this potential is challenging. Despite LLMs’ powerful internal comprehension, their autoregressive nature hides this capability within the model, leading to output features with poor discriminability. Our experiments show that directly integrating LLMs into CLIP results in catastrophic performance drops. In this paper, we propose LLM2CLIP, a novel approach that embraces the power of LLMs to unlock CLIP’s potential. By fine-tuning the LLM in the caption space with contrastive learning, we extract its textual capabilities into the output embeddings, significantly improving the output layer’s textual discriminability. We then design an efficient training process where the fine-tuned LLM acts as a powerful teacher for CLIP’s visual encoder. Thanks to the LLM’s presence, we can now incorporate longer and more complex captions without being restricted by vanilla CLIP text encoder’s context window and ability limitations. Our experiments demonstrate that this approach brings substantial improvements in cross-modal tasks. Our method directly boosted the performance of the previously SOTA EVA02 model by 16.5% on both long-text and short-text retrieval tasks, transforming a CLIP model trained solely on English data into a state-of-the-art cross-lingual model. Moreover, when integrated into multimodal training with models like Llava 1.5, it consistently outperformed CLIP across nearly all benchmarks, demonstrating comprehensive performance improvements.","CLIP (Radford et al., 2021) is one of the most important multimodal foundational models today. It aligns vision and language signals into a shared feature space by employing a simple contrastive learning loss on large-scale image-text pairs. As a retriever, CLIP supports a wide range of tasks, including zero-shot classification (Qian & Hu, 2024), detection (Lin & Gong, 2023), segmentation (Zhou et al., 2023), and image-text retrieval (Lülf et al., 2024; Koukounas et al., 2024). As a feature extractor, it has become dominant in virtually all cross-modal representation tasks, such as image understanding, video understanding, and text-to-image/video generation. For instance, works like LLaVA (Liu et al., 2023) and Qwen-VL (Bai et al., 2023) leverage CLIP as a feature extractor to obtain visual features for text models, while models like Stable Diffusion (Rombach et al., 2021) and DALL·E 2 (Ramesh et al., 2022) use CLIP’s text encoder to extract textual features for visual models. What makes CLIP so powerful, particularly as a vision encoder? The core of its strength lies in its unprecedented ability to align visual pretraining with natural language — the carrier of human knowledge. Unlike earlier vision encoders such as VGG and ResNet, which relied on the limited ImageNet dataset and simple image categories with just a few words, CLIP is trained on web-scale data using rich descriptive text. This alignment with language is what sets CLIP apart and unlocks its vast potential. However, since CLIP’s introduction, large language models (LLMs) have advanced significantly. Models like GPT-4 (Achiam et al., 2023) and Llama (Dubey et al., 2024) now demonstrate remarkable language capabilities, yet these advancements have not translated to corresponding improvements in visual representation learning. This prompts the question: can the capabilities of LLMs be harnessed to further improve multimodal representation learning? The potential benefits of incorporating LLMs into CLIP are clear. LLMs’ strong textual understanding can fundamentally improve CLIP’s ability to handle image captions, drastically enhancing its ability to process long and complex texts — a well-known limitation of vanilla CLIP. Moreover, LLMs are trained on a vast corpus of text, possessing open-world knowledge. This allows them to expand on caption information during training, increasing the efficiency of the learning process. In this work, we aim to leverage large language models (LLMs) to enable CLIP to learn more powerful, fine-grained, and rich visual representations. Currently, CLIP is often criticized for its bag-of-words-like perception and the limitations of its text encoder, which suffers from a constrained model size, limited context length, and is trained predominantly on image captioning data, lacking exposure to diverse world corpora. A natural approach would be to replace CLIP’s text encoder with an LLM that embeds rich human knowledge. However, this presents significant challenges. In the cross-modal contrastive learning framework employed by CLIP, the text encoder functions as a set of knowledge anchors in the shared latent space, guiding the alignment of the vision encoder with human knowledge of the physical world. The structure, richness, and discriminability of these knowledge anchors are critical to the visual model’s effectiveness. In contrast, LLMs are primarily designed to predict the next word rather than generate explicit representations of the knowledge they contain. Their textual comprehension abilities and open-world knowledge are latent within the model, rather than present in the output embeddings, making them difficult to utilize in the same explicit manner as CLIP’s text encoder. As a result, using LLMs as a text encoder may not produce linearly separable features, which are crucial for effective feature alignment. Table 1: Comparison of top-1 Caption Retrieval Accuracy (CRA) for various language models in MS COCO validation set. Language Model CRA CLIP-L/14 66.6 EVA02-L/14 69.8 Llama3-8B 18.4 Llama3.2-1B 18.3 Llama3-8B-CC 73.0 Llama3.2-1B-CC 72.8 Figure 2: Real examples of top-1 results from the caption-to-caption retrieval experiment. Before fine-tuning, Llama3’s results were often completely unrelated. To validate our hypothesis, we designed a caption-to-caption retrieval experiment, as shown in Table 1 and Figure 2. Each image in the MS-COCO dataset has five human-annotated captions. We selected the first two captions as positive samples and performed retrieval across the entire validation set. Using the caption retrieval accuracy (CRA), we evaluated the text model’s ability to differentiate between captions, helping us determine which language model is better suited for CLIP. We found that Llama-3 8B achieved only 18.4% top-1 accuracy, while the standard CLIP-ViT-L reached 66.0% top-1 accuracy. As illustrated in Figure 2, the top-1 caption retrieved by original Llama-3 can be entirely unrelated to the query caption, clearly obstructing effective CLIP learning. Therefore, directly using an LLM to guide CLIP’s visual encoder training is highly constrained. We believe that enhancing the discriminative power of LLM output tokens through fine-tuning is vital for the success of our proposed approach, allowing the latent capabilities of LLMs to surface. Encouragingly, we found that this can be achieved very efficiently. Specifically, we designed a caption contractive (CC) fine-tuning strategy, applying lightweight fine-tuning to the output tokens of Llama-3 8B using LoRA on the CC3M (Sharma et al., 2018) image captioning dataset. The primary goal of this training task was to adjust the output space, improving the model’s ability to distinguish between different captions. We utilized a supervised SimCSE (Gao et al., 2021; BehnamGhader et al., 2024) contrastive learning loss, where the original captions and re-annotated captions generated by ShareCaptioner (Chen et al., 2023) were treated as positive pairs, pulling them closer. In contrast, all other captions formed a negative sample set that the model learned to push away. Remarkably, after this CC fine-tuning, the caption retrieval accuracy, as shown in Table 1, rose from 18.4% to 73%, which is a 7% improvement over the original CLIP-ViT-L text encoder. This successful fine-tuning process enables us to more effectively harness the open-world capabilities of LLMs for CLIP training. In a nutshell, we present LLM2CLIP, a novel approach for enhancing visual representation learning through the integration of large language models (LLMs) as shown in Figure 1. This method takes a straightforward yet audacious step by replacing the original CLIP text encoder and augmenting the CLIP visual encoder with the vast knowledge embedded in LLMs. We have identified key obstacles associated with this innovative idea and proposed a cost-effective fine-tuning strategy to overcome them. Our experiments demonstrate that leveraging LLMs as teachers for CLIP training yields substantial improvements, with LLM2CLIP significantly outperforming state-of-the-art pre-trained CLIP models. Our method increased the performance of the previously SOTA EVA02 model by 16.5% on both long-text and short-text retrieval tasks, transforming a CLIP model trained solely on English data into a state-of-the-art cross-lingual model. Furthermore, when incorporated into multimodal model training, such as with Llava 1.5, it consistently achieved comprehensive improvements over EVA02 across nearly all benchmarks. Additionally, the efficient training method proposed by LLM2CLIP ensures that the training cost is nearly identical to fine-tuning the original CLIP. We have also demonstrated that using more powerful language models and larger training datasets can further boost LLM2CLIP’s performance, showcasing the immense potential of our approach. These promising outcomes affirm that we have successfully transformed CLIP into a more general-purpose foundational model. The enhanced LLM2CLIP model possesses richer knowledge and exhibits a remarkable capacity for distinguishing fine-grained and complex long-text semantics. This advancement not only broadens the range of supported downstream tasks but also propels progress across the entire vision domain."
https://arxiv.org/html/2411.04995v1,LoFi: Scalable Local Image Reconstruction withImplicit Neural Representation,"Neural fields or implicit neural representations (INRs) have attracted significant attention in machine learning and signal processing due to their efficient continuous representation of images and 3D volumes. In this work, we build on INRs and introduce a coordinate-based local processing framework for solving imaging inverse problems, termed LoFi (Local Field). Unlike conventional methods for image reconstruction, LoFi processes local information at each coordinate separately by multi-layer perceptrons (MLPs), recovering the object at that specific coordinate. Similar to INRs, LoFi can recover images at any continuous coordinate, enabling image reconstruction at multiple resolutions. With comparable or better performance than standard CNNs for image reconstruction, LoFi achieves excellent generalization to out-of-distribution data and memory usage almost independent of image resolution. Remarkably, training on 1024×1024102410241024\times 10241024 × 1024 images requires just 3GB of memory—over 20 times less than the memory typically needed by standard CNNs. Additionally, LoFi’s local design allows it to train on extremely small datasets with less than 10 samples, without overfitting or the need for regularization or early stopping. Finally, we use LoFi as a denoising prior in a plug-and-play framework for solving general inverse problems to benefit from its continuous image representation and strong generalization. Although trained on low-resolution images, LoFi can be used as a low-dimensional prior to solve inverse problems at any resolution. We validate our framework across a variety of imaging modalities, from low-dose computed tomography to radio interferometric imaging.","Imaging inverse problems are ubiquitous in domains like medicine (Wang et al., 2008), material science (Holler et al., 2017), remote sensing (Blahut, 2004) and cosmology (Kaiser and Squires, 1993). Deep learning has been, for a long time, the method of choice for solving inverse problems. In particular, convolutional neural networks (CNNs) like U-Net (Ronneberger et al., 2015) have shown remarkable performance across diverse tasks including computed tomography (CT) (Jin et al., 2017), magnetic resonance imaging (MRI) (Hyun et al., 2018), photoacoustic imaging (Davoudi et al., 2019) and inverse scattering (Wei and Chen, 2018). The success of the U-Net is primarily attributed to its multiscale architecture with a large receptive field that extracts features from the input image at different scales (Liu et al., 2022). Despite their strong performance on low-dimensional 2D imaging problems, deep learning architectures can become computationally expensive for high-dimensional images (Zhang et al., 2017a; Liang et al., 2021; Wang et al., 2021). This inefficiency arises because current deep neural networks reconstruct the entire image simultaneously, necessitating substantial memory for back-propagation during training. Additionally, interpreting such complex architectures (Zhang et al., 2021; Liang et al., 2021; Fabian et al., 2022) and analyzing their reconstructions can be challenging. Simplified neural architectures can enhance our understanding of reconstruction mechanisms, enabling the design of robust models with strong generalization (Aggarwal et al., 2018). Recently, implicit neural representations (INRs) (Sitzmann et al., 2020; Atzmon and Lipman, 2020; Chabra et al., 2020) have emerged as a promising tool for representing continuous signals, images, and 3D volumes. Unlike most existing deep learning models that treat signals as discrete arrays, INRs map coordinates to signal values using a deep neural network, typically a multi-layer perceptron (MLP), resulting in a continuous signal representation. INRs have been widely applied to various tasks, including solving partial differential equations (PDEs) (Sitzmann et al., 2020; Vlašić et al., 2022; Khorashadizadeh et al., 2023a), 3D shape modeling (Chen and Zhang, 2019; Peng et al., 2020; Jiang et al., 2020), and image super-resolution (Chen et al., 2021b; Khorashadizadeh et al., 2023a). One of the most well-known applications of INRs is 3D scene representation using neural radiance fields (NeRF) (Mildenhall et al., 2021). INRs have several advantages over standard deep learning models. Rather than representing signals at a single resolution, INRs can conveniently interpolate signals in a continuous space. This capability is particularly interesting because we can conveniently adjust the required memory making INRs well-suited for high-dimensional 3D reconstructions (Dupont et al., 2021, 2022). In this paper, we build on INRs and introduce a scalable coordinate-based local reconstruction pipeline for solving imaging inverse problems. In many tasks such as image denoising, super-resolution, and low-dose computed tomography (LDCT), the image intensity at a specific pixel is primarily influenced by the observed image in the pixel’s neighborhood. Our proposed model, termed LoFi (Local Field), recovers the image intensity at each pixel separately, using local information extracted from the input image around that pixel. This information is processed by a neural network (composed of MLP blocks) to determine the image intensity at the target pixel, enabling image reconstruction at any resolution or arbitrary continuous coordinate. LoFi’s coordinate-based design brings several advantages. Similar to INRs, we can train LoFi on mini-batches of both objects and pixels, resulting in resolution-agnostic memory usage. As a result, LoFi can efficiently process high-resolution images with a tiny memory footprint. Notably, it requires just 3GB of memory to train on 1024×1024 images. Furthermore, LoFi’s local design provides a strong inductive bias for image reconstruction, as demonstrated in our experiments, where LoFi performs effectively on tiny datasets with very few samples, without overfitting or needing regularization and early stopping. In addition, Deep learning models for image reconstruction like multiscale CNNs (Ronneberger et al., 2015; Zhang et al., 2021) and vision transformers (ViTs) (Liang et al., 2021; Wang et al., 2022; Fabian et al., 2022) often use sophisticated architectures that complicate downstream interpretations. Through our proposed coordinate-conditioned patch geometry, LoFi can learn the position of the relevant features in the input image for each pixel, providing insights for downstream image analysis and interpretation. Figure 1: LoFi; the neural network NNθsubscriptNN𝜃\text{NN}_{\theta}NN start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT, typically composed of MLP modules, processes the local information extracted from the observed image around the given pixel (x,y)𝑥𝑦(x,y)( italic_x , italic_y ). LoFi’s inductive bias for image reconstruction brings strong generalization on OOD data, it requires less training data and uses small memory when training on high-resolution images."
https://arxiv.org/html/2411.04989v1,SG-I2V: Self-Guided Trajectory Controlin Image-to-Video Generation,"Methods for image-to-video generation have achieved impressive, photo-realistic quality. However, adjusting specific elements in generated videos, such as object motion or camera movement, is often a tedious process of trial and error, e.g., involving re-generating videos with different random seeds. Recent techniques address this issue by fine-tuning a pre-trained model to follow conditioning signals, such as bounding boxes or point trajectories. Yet, this fine-tuning procedure can be computationally expensive, and it requires datasets with annotated object motion, which can be difficult to procure. In this work, we introduce SG-I2V, a framework for controllable image-to-video generation that is self-guided—offering zero-shot control by relying solely on the knowledge present in a pre-trained image-to-video diffusion model without the need for fine-tuning or external knowledge. Our zero-shot method outperforms unsupervised baselines while being competitive with supervised models in terms of visual quality and motion fidelity. Additional details and video results are available on our project page: https://kmcode1.github.io/Projects/SG-I2V.","Recent advances in video diffusion models demonstrate significant improvements in visual and motion quality (Ho et al., 2022b; Blattmann et al., 2023a; b; He et al., 2022). These models typically take a text prompt (Ho et al., 2022b; Blattmann et al., 2023a; Ho et al., 2022a) or image (Chen et al., 2023; 2024a; Guo et al., 2024; Xing et al., 2024) as input and generate video frames of a photorealistic, animated scene. Current methods can generate videos that are largely consistent with an input text description or image; however, fine-grained adjustment of specific video elements (e.g., object motion or camera movement) is conventionally a tedious process that requires re-running the model with different text prompts or random seeds (Wu et al., 2024a; Qiu et al., 2024). Approaches for controllable video generation aim to eliminate this process of trial-and-error through direct manipulation of generated video elements, such as object motion (Wu et al., 2024b; Yin et al., 2023; Wang et al., 2024a), pose (Hu, 2024; Xu et al., 2024b), and camera movement (Wang et al., 2024c; Li et al., 2024; He et al., 2024a; Hu et al., 2024). One line of work fine-tunes pre-trained video generators to incorporate control signals such as bounding boxes or point trajectories (Wu et al., 2024b; Wang et al., 2024c). One of the primary challenges with these supervised methods is the expensive training cost, and thus, previous methods usually incorporate trajectory control by fine-tuning at a lower resolution than the original model Wu et al. (2024b); Yin et al. (2023). More recently, several methods for zero-shot, controllable text-to-video generation have been developed (Ma et al., 2023; Qiu et al., 2024; Jain et al., 2024). They control object trajectories by modulating the cross-attention maps between features within a bounding box and an object-related text token. Still, it is not always possible to associate a desired edit with the input text prompt (consider, e.g., motion of object parts). Moreover, these methods cannot be directly applied to animate existing images, as they are only conditioned on text. In this work, we propose SG-I2V, a new method for controllable image-to-video generation. Our approach is self-guided, in that it offers zero-shot control by relying solely on knowledge present in a pre-trained video diffusion model. Concretely, given an input image, a user specifies a set of bounding boxes and associated trajectories. Then, our framework alters the generation process to control the motion of target scene elements. It is essential to manipulate the structure of the generated video to achieve precise control over element positions, which is mainly decided by early denoising steps (Balaji et al., 2022; Wang & Vastola, 2023). In image diffusion models, it is known that feature maps extracted from the output of upsampling blocks are semantically aligned, i.e., pixels belonging to the same object share similar feature vectors on the feature map and thus can be used to control the spatial layout of generated images (Tang et al., 2023; Shi et al., 2024; Namekata et al., 2024; Tumanyan et al., 2023). However, our analysis reveals that feature maps extracted from the upsampling blocks of video diffusion models are only weakly aligned across frames (see Fig. 2). This misalignment poses challenges, as directly manipulating these feature maps fails to give useful guidance signals for layout control. Instead, we find that feature maps extracted from the self-attention layers can be semantically aligned by replacing the key and value tokens for each frame with those of the first frame (see bottom row of Fig. 2). After that, we can control the motion of generated videos by optimizing the latent (the input to the denoising network) with a loss that encourages similarity between the aligned features within each bounding box along the input trajectory. Finally, we apply a post-processing step to enhance output quality by ensuring that our optimization does not disrupt the distribution of high-frequency noise expected by the diffusion model. In summary, our work makes the following contributions: • We conduct a first-of-its-kind analysis of semantic feature alignment in a pre-trained image-to-video diffusion model and identify important differences from image diffusion models. • Building on this analysis, we propose SG-I2V, a zero-shot, self-guided approach for controllable image-to-video generation. Our method can control object motion and camera dynamics for arbitrary input images and any number of objects or regions of a scene. • We conduct extensive experiments to show superior performance over zero-shot baselines while being competitive with supervised baselines in visual and motion quality. Figure 2: Semantic correspondences in video diffusion models. We analyze feature maps in the image-to-video diffusion model SVD (Blattmann et al., 2023a) for three generated video sequences (row 1). We use PCA to visualize the features at diffusion timestep 30 (out of 50) at the output of an upsampling block (row 2), a self-attention layer (row 3), and the same self-attention layer after our alignment procedure (row 4). Although output feature maps of upsampling blocks in image diffusion models are known to encode semantic information (Tang et al., 2023), we only observe weak semantic correspondences across frames in SVD. Thus, we focus on the self-attention layer and modify it to produce feature maps that are semantically aligned across frames."
https://arxiv.org/html/2411.04984v1,Planar Reflection-Aware Neural Radiance Fields,"Neural Radiance Fields (NeRF) have demonstrated exceptional capabilities in reconstructing complex scenes with high fidelity. However, NeRF’s view dependency can only handle low-frequency reflections. It falls short when dealing with complex planar reflections, often interpreting them as erroneous scene geometries and leading to duplicated and inaccurate scene representations. To address this challenge, we introduce a planar reflection-aware NeRF that jointly models planar reflectors, such as windows, and explicitly casts reflected rays to capture the source of the high-frequency reflections. We query a single radiance field to render the primary color and the source of the reflection. We propose a sparse edge regularization to help utilize the true source of reflections for rendering planar reflections rather than creating a duplicate along the primary ray at the same depth. As a result, we obtain accurate scene geometry. Rendering along the primary ray results in a clean, reflection-free view, while explicitly rendering along the reflected ray allows us to reconstruct highly detailed reflections. Our extensive quantitative and qualitative evaluations of real-world datasets demonstrate our method’s performance in accurately handling reflections.","Planar reflections are a ubiquitous and visually significant phenomenon in our everyday environments. Windows, in particular, are common planar reflectors, making their proper handling a critical aspect of realistic scene reconstruction. In this work, we focus on accurately modeling these planar reflections during scene reconstruction. Neural Radiance Fields (NeRF) (Mildenhall et al., 2020) have made groundbreaking strides in rendering complex scenes with remarkable fidelity. Recent advancements have further improved NeRF’s capabilities, such as anti-aliasing (Barron et al., 2021, 2022), handling transient elements (Martin-Brualla et al., 2021; Liu et al., 2023a), and addressing inconsistent camera exposure or illumination (Rematas et al., 2022; Mildenhall et al., 2020). However, a notable limitation of NeRF lies in its handling of reflections. As illustrated in Figure 2(a), NeRF traces only a single ray and relies on view-dependency or false geometry to account for reflections. For low-frequency reflections like highlights, this is achieved by inputting 3D point positions and viewing directions into a small MLP, which predicts view-dependent emitted radiance. However, this method falls short when dealing with complex, high-frequency reflections. NeRF tends to create false geometry along the primary ray, incorrectly mirroring the actual object’s position along the reflected ray to represent planar reflections (Figure 2a). While this offers a plausible explanation for reflections, it fundamentally misinterprets them. NeRF cannot distinguish a reflected object from a duplicated one, as both appear visually similar. This misrepresentation prevents us from creating correct novel views if we see the space behind the reflector, not through the reflector. It also leads to incorrect scene geometry, adversely affecting downstream applications, such as mesh extraction. Several methods have been proposed to address reflections in NeRF (Ma et al., 2024; Verbin et al., 2024; Liu et al., 2023b; Guo et al., 2022; Yin et al., 2023; Verbin et al., 2022). NeRFReN (Guo et al., 2022) proposes to model the transmitted and reflected parts of a forward-facing scene with separate neural radiance fields. MS-NeRF (Yin et al., 2023) represents the scene using multiple feature fields decoded by small MLPs for rendering and blending. These methods decompose the scene into several independent components in a self-supervised manner, sometimes leading to sub-optimal decomposition where false geometries persist in the primary component. Ref-NeRF (Verbin et al., 2022) addresses specular reflections by explicitly parameterizing outgoing radiance as a function of the reflection of the view direction about the local normal vector. It shows promising results in representing specular reflections but lacks the ability to separate reflections. In this paper, we introduce a planar reflection-aware NeRF that jointly models planar surfaces and explicitly casts reflected rays to capture the source of high-frequency reflections (Figure 2 b). Our core idea is to maintain a single geometry and appearance instance. This principle dictates that if a real object is present along the reflected ray, our method should not create a false duplicate along the primary ray at the same depth to explain the reflection. Instead, it should always use the reflected ray to explain the reflection, ensuring the primary view is clean and accurate. To render novel views, we query a single radiance field to render the primary color and the source of the reflection. Then, we learn to attenuate the source of the reflection and compose it with the primary color to obtain the final color. We introduce regularization to help utilize the true source of reflections for rendering planar reflections rather than creating a false duplicate. As a result, we obtain accurate scene geometry. Rendering along the primary ray results in a clean, reflection-free view, while explicitly rendering along the reflected ray allows us to reconstruct highly detailed reflections. We validate our method’s reconstruction performance on our real-world 360-degree dataset and demonstrate significant enhancements in NeRF’s performance in handling reflections. Our contributions are summarized as follows: • We introduce a method for handling planar reflections by jointly modeling planar surfaces and casting reflected rays to accurately capture the source of high-frequency reflections. • We develop a regularization loss to discourage the creation of false geometries. • We provide a real-world 360-degree dataset that contains pronounced reflections, facilitating the quantitative evaluation of reflection-free reconstruction."
https://arxiv.org/html/2411.04967v1,AsCAN: Asymmetric Convolution-Attention Networks for Efficient Recognition and Generation,"Neural network architecture design requires making many crucial decisions. The common desiderata is that similar decisions, with little modifications, can be reused in a variety of tasks and applications. To satisfy that, architectures must provide promising latency and performance trade-offs, support a variety of tasks, scale efficiently with respect to the amounts of data and compute, leverage available data from other tasks, and efficiently support various hardware. To this end, we introduce AsCAN—a hybrid architecture, combining both convolutional and transformer blocks. We revisit the key design principles of hybrid architectures and propose a simple and effective asymmetric architecture, where the distribution of convolutional and transformer blocks is asymmetric, containing more convolutional blocks in the earlier stages, followed by more transformer blocks in later stages. AsCAN supports a variety of tasks: recognition, segmentation, class-conditional image generation, and features a superior trade-off between performance and latency. We then scale the same architecture to solve a large-scale text-to-image task and show state-of-the-art performance compared to the most recent public and commercial models. Notably, even without any computation optimization for transformer blocks, our models still yield faster inference speed than existing works featuring efficient attention mechanisms, highlighting the advantages and the value of our approach.","Convolutional neural networks (CNNs) and transformers have been deployed in a wide spectrum of real-world applications, addressing various computer vision tasks, e.g., image recognition [1, 2] and image generation [3, 4, 5, 6, 7, 8]. CNNs encode many desirable properties like translation equivariance facilitated through the convolutional operators [9]. However, they lack the input-adaptive weighting and the global receptive field capabilities offered by transformers [10, 11]. By recognizing the potential benefits of combining these complementary strengths, research endeavors explore hybrid architectures that integrate both convolutional and attention mechanisms [12, 13, 14, 15]. Recently, such architectures witness huge success when scaled up to train large-scale text-to-image (T2I) diffusion models [16, 17, 18, 19], enabling a vast range of visual applications, such as content editing [20, 21, 22, 23, 24, 25, 26] and video generation [27, 28, 29, 30]. One prominent research area for hybrid models involves the creation of building blocks that can effectively combine convolution and attention operators [31, 32]. While these efforts seek to use the strengths of both operators, their faster attention alternatives only approximate the global attention, leading to compromised model performance as lacking a global receptive field. Thus, they necessitate incorporating additional layers to compensate for the capacity reduction due to the attention approximation. On the other hand, minimal effort is directed toward optimizing the entire hybrid architecture. This raises the question: Is the current macro design of hybrid architecture optimal? In this work, we propose a simple yet effective hybrid architecture, wherein the number of convolution and transformer blocks is asymmetric in different stages. Specifically, we adopt more convolutional blocks in the early stages, where the feature maps have relatively large spatial sizes, and more transformer blocks at the later stages. This design is verified across different tasks. For example, in Fig. 3, we demonstrate superior advantages of our model for the latency-performance trade-off on ImageNet-1K [33] classification task. Particularly, our model achieves even faster speed than many works featuring efficient attention operations. Additionally, we scale up our architecture to train the large-scale T2I diffusion model for high-fidelity generation (Fig. 1, Fig. 4, and Tab. 3). Furthermore, considering the high training cost for the large-scale T2I diffusion models, we introduce a multi-stage training pipeline to improve the training efficiency. Overall, our contributions can be summarized as follows: • We revisit the macro design principles of hybrid convolutional-transformer architectures and propose one with asymmetrically distributed convolutional and transformer blocks. • For the image classification task, we perform extensive latency analysis on the ImageNet-1K dataset and show our models achieve superior throughput-performance trade-offs than existing works (see Fig. 3). Notably, we show that the model runtime can be significantly accelerated even without any acceleration optimization on attention operations. Additionally, we show our pre-trained models on ImageNet-1K can be applied to downstream tasks such as semantic segmentation. • For the class-conditional generation on ImageNet-1K (256×256256256256\times 256256 × 256), our asymmetric UNet achieves similar performance as state-of-the-art models with half the compute resources (see Tab. 4). • For the text-to-image generation task, we demonstrate that our network can be scaled up for the large-scale T2I generation with a better performance-latency trade-off than existing public models (as in Tab. 3). Additionally, we improve the training efficiency through a multi-stage training pipeline, where we first train the model on a small dataset, i.e., ImageNet-1K, for T2I generation, then fine-tune the model on a large-scale dataset."
https://arxiv.org/html/2411.04963v1,"VAIR: Visuo-Acoustic Implicit Representations for Low-Cost, Multi-Modal Transparent Surface Reconstruction in Indoor Scenes","Mobile robots operating indoors must be prepared to navigate challenging scenes that contain transparent surfaces. This paper proposes a novel method for the fusion of acoustic and visual sensing modalities through implicit neural representations to enable dense reconstruction of transparent surfaces in indoor scenes. We propose a novel model that leverages generative latent optimization to learn an implicit representation of indoor scenes consisting of transparent surfaces. We demonstrate that we can query the implicit representation to enable volumetric rendering in image space or 3D geometry reconstruction (point clouds or mesh) with transparent surface prediction. We evaluate our method’s effectiveness qualitatively and quantitatively on a new dataset collected using a custom, low-cost sensing platform featuring RGB-D cameras and ultrasonic sensors. Our method exhibits significant improvement over state-of-the-art for transparent surface reconstruction. Website and Dataset: https://umfieldrobotics.github.io/VAIR_site/","Mobile robots navigating indoor spaces encounter various challenges for modern perception sensors, including highly reflective surfaces, poor lighting, and transparent objects. Popular sensors like RGB-D cameras and LiDAR cannot faithfully reconstruct transparent surfaces such as glass panes [1, 2]. When using 3D maps built using these sensors, robots will lack crucial geometric information about indoor spaces. This can lead to robots colliding with glass doors, floor-to-ceiling glass panes, and mirrored surfaces [3]. Most prior work on robot perception for transparent objects and surfaces has focused on the manipulation of small transparent objects such as drinking glasses [2, 4]. Although these methods work well in controlled table-top environments with fixed lighting, they are not well-equipped to handle transparent surface perception at the larger scale of indoor scenes. Prior approaches use techniques that work well when specularities and distortions are present on small glass objects [5]. However, these phenomena are rare and unreliable indicators of indoor glass structures, which are often planar. In this work, we explore the fusion of low-cost acoustic sensing modalities (i.e., ultrasonic sensors) with RGB-D imagery to enable reconstruction of transparent surfaces in indoor scenes. Figure 1: We augment the mapping and reconstruction capabilities of mobile robots with low-cost acoustic sensors that can measure sparse returns from glass surfaces. Our framework, VAIR, fully reconstructs glass surfaces, producing useful 3D geometry for robotic systems. VAIR’s transparent surface prediction is shown in green. While many time-of-flight sensors that use electromagnetic radiation (e.g., structured light, LiDAR, etc.) fail to capture the geometry of transparent surfaces, we note that acoustic sensors are able to detect transparent surfaces reliably. However, acoustic sensors suffer from noise, interference, and, most importantly, measurement sparsity. To address this problem, we leverage the power of generative models to fuse the sparse readings of low-cost acoustic sensors with RGB-D information to reconstruct transparent surfaces in indoor scenes. Our proposed pipeline, illustrated in Fig. 1, effectively fuses visual (RGB-D, semantic) and acoustic data to produce an implicit representation that we call Visuo-Acoustic Implicit Representation (VAIR). Concretely, we present the following contributions: • We develop a novel scene-conditioned generative model that produces an implicit representation of transparent surfaces in indoor scenes by fusing RGB-D point clouds, semantic segmentation predictions, and sparse acoustic sensor measurements. • We develop a novel method for semantic-guided planar acoustic projection that initializes and guides our network training. • We present a multi-modal dataset consisting of 3 real, indoor scenes with glass surfaces. Our dataset contains RGB-D images, acoustic sensor readings, camera poses, and ground truth geometry for glass surfaces. We demonstrate that VAIR exhibits a significant improvement over state-of-the-art in geometry prediction tasks, yielding more accurate 3D reconstructions of indoor scenes that feature transparent surfaces."
https://arxiv.org/html/2411.04956v1,Uncovering Hidden Subspaces in Video Diffusion Models Using Re-Identification,"Latent Video Diffusion Models can easily deceive casual observers and domain experts alike thanks to the produced image quality and temporal consistency. Beyond entertainment, this creates opportunities around safe data sharing of fully synthetic datasets, which are crucial in healthcare, as well as other domains relying on sensitive personal information. However, privacy concerns with this approach have not fully been addressed yet, and models trained on synthetic data for specific downstream tasks still perform worse than those trained on real data. This discrepancy may be partly due to the sampling space being a subspace of the training videos, effectively reducing the training data size for downstream models. Additionally, the reduced temporal consistency when generating long videos could be a contributing factor.In this paper, we first show that training privacy preserving models in latent space is computationally more efficient and generalize better. Furthermore, to investigate downstream degradation factors, we propose to use a re-identification model, previously employed as a privacy preservation filter. We demonstrate that it is sufficient to train this model on the latent space of the video generator. Subsequently, we use these models to evaluate the subspace covered by synthetic video datasets and thus introduce a new way to measure the faithfulness of generative machine learning models. We focus on a specific application in healthcare – echocardiography – to illustrate the effectiveness of our novel methods. Our findings indicate that only up to 30.8% of the training videos are learned in latent video diffusion models, which could explain the lack of performance when training downstream tasks on synthetic data.","Figure 1: We propose leveraging Variational Autoencoders (VAEs), which were trained to enable the training of video diffusion models, to enhance the computational efficiency of models used for privacy filtering. Additionally, we demonstrate how the trained filter model can be applied to various other tasks, such as evaluating temporal consistency and model recall. Latent Video Diffusion Models became popular due to their realistic depiction of synthetic scenes, mostly framed as text-to-video tasks [4]. However, video synthesis also has important applications in domains beyond entertainment, such as healthcare. Analysis of diagnostic counterfactuals [34], faithful representation of anatomy [30], automated diagnosis [26] or data enrichment and domain generalization [22, 20] are only a few examples. Images and videos are the most prevalent data structure for point-of-care and clinical diagnosis of disease with medical ultrasound imaging. The most common applications are prenatal screening and echocardiography. Echocardiography is of crucial importance to prevent avoidable deaths caused by cardiac disease. Ejection fraction is one of the main diagnostic measurement in echocardiography due to its correlation to ventricular function assessment [31]. It describes the ratio of change in cardiac volume during different phases of the cardiac cycle [27]. For example, it serves as an important marker for chemotherapy dosing in pediatric ultrasound [24] and pacemaker placement [37]. Consequently, interest in the automatic assessment of ejection fraction from videos has grown [27, 35, 6]. Similar to other medical fields, these models suffer from challenges related to difficult data acquisition processes, as well as robustness and domain generalization problems [27]. One potential solution is the use of generative models to learn and generate videos from these domains, with the added potential to publish the data with privacy guarantees [11, 32]. This option got traction with recent advances in the quality of video generation, which predominantly operate in a latent space [2, 4, 5, 17, 32, 21]. To publicly release these synthetic videos, we need privacy measures to ensure that the synthetically generated videos are not memorized from the training data, which could lead to privacy concerns. While current approaches have begun to apply these techniques to ensure privacy [7, 32], there has been no evidence that we can directly ensure privacy in the latent space of diffusion models. Contribution: Our contributions can be divided into two parts. First, we demonstrate that training privacy models in latent space is computationally more efficient and results in better generalization. This is illustrated in Fig. 1. Secondly, we are the first to use a model trained for privacy to evaluate the learned subspace of the training dataset and the temporal consistency of the generated videos. In summary, we achieve the following: 1. We are the first to evaluate the feasibility of privacy-preserving models on synthetic videos, providing evidence that privatizing the latent model directly is sufficient to ensure the privacy of the entire video model. 2. We show that training identification models in the latent space is computationally more effective than training them in image space. This approach also enhances the robustness of the learned representation of the privacy model, improving its generalization potential to new datasets. 3. We establish a privacy filter that is simple to interpret and computationally efficient. We find that using the prediction head directly works better than taking the correlation coefficient. 4. We illustrate how trained privacy models can be used to evaluate several other quality measurements, including the temporal consistency of diffusion-generated videos and the completeness of the generated subspace. 5. We reveal that current generative models tend to only learn a distinct subspace of the training set, which might be the main reason for reduced performance of downstream models trained on synthetic data."
https://arxiv.org/html/2411.04954v1,CAD-MLLM: Unifying Multimodality-Conditioned CAD Generation With MLLM,"This paper aims to design a unified Computer-Aided Design (CAD) generation system that can easily generate CAD models based on the user’s inputs in the form of textual description, images, point clouds, or even a combination of them. Towards this goal, we introduce the CAD-MLLM, the first system capable of generating parametric CAD models conditioned on the multimodal input. Specifically, within the CAD-MLLM framework, we leverage the command sequences of CAD models and then employ advanced large language models (LLMs) to align the feature space across these diverse multi-modalities data and CAD models’ vectorized representations. To facilitate the model training, we design a comprehensive data construction and annotation pipeline that equips each CAD model with corresponding multimodal data. Our resulting dataset, named Omni-CAD, is the first multimodal CAD dataset that contains textual description, multi-view images, points, and command sequence for each CAD model. It contains approximately 450K instances and their CAD construction sequences. To thoroughly evaluate the quality of our generated CAD models, we go beyond current evaluation metrics that focus on reconstruction quality by introducing additional metrics that assess topology quality and surface enclosure extent. Extensive experimental results demonstrate that CAD-MLLM significantly outperforms existing conditional generative methods and remains highly robust to noises and missing points. The project page and more visualizations can be found at: https://cad-mllm.github.io/","Computer-Aided Design (CAD) is the use of computers to aid the creation, modification, and optimization of objects. It plays a pivotal role in industrial design and manufacturing and has been widely used for architectural design, shipbuilding, automobile, and aerospace industries, etc. Classical CAD workflow usually involves the design of 2D sketches (e.g., circles, lines, splines) and 3D operations (e.g., extrusion, loft, fillet) of these 2D elements, which is a sequence of actions with fixed types but various parameters, which can be explicitly and precisely represented by text. Then, the final CAD models are shaved as boundary representations (B-Rep), which facilitates the control of the design history and modification of the models. However, current CAD software requires experts to design and modify the model, while the CAD needs to be frequently updated by communicating with the users. It is desirable to develop a toolbox with which the expert, or even the non-expert, can easily design the CAD models by using simple instructions and illustrations to make the ideas in their mind easily come true. With the advancement of generative models, recent approaches have explored CAD generation, of which, DeepCAD [1] is a very representative one. DeepCAD leverages an autoencoder to generate CAD models from command sequence representation, but it operates exclusively within a latent space and is not initially designed for conditional generation, which could not meet the users’ needs for interactive design, where the users’ input can be images, textual descriptions, or point clouds. To tackle this issue, Img2CAD[2] and GenCAD [3] have been proposed to generate a CAD model based on the input images. Text2CAD [4], Query2CAD [5] have been proposed to generate a CAD model based on the text. Point2cyl [6], TransCAD [7] have been proposed to generate a CAD model based on the point cloud. However, all these methods propose different methods for conditions of different modalities. It is desirable to design a unified framework to tackle the CAD generation task with different input conditions or even multiple conditions. Dataset Publication CAD Model Representation‡‡\ddagger‡ CAD Model Size Input Condition ABC [8] CVPR 2019 B-rep 1,000,000+ Uncondition CC3D-Ops [9] 3DV 2022 B-rep 37,000+ Uncondition CADParser [10] IJCAI 2023 Command Sequence 40,000+ Uncondition DeepCAD [1] ICCV 2021 Command Sequence 179,133 Uncondition Fusion360 [11] TOG 2021 Command Sequence 8,625 Uncondition SketchGraphs [12]††\dagger† Arxiv, 2020.7 Command Sequence 15,000,000+ Uncondition Free2CAD [13] SIGGRAGH 2022 Command Sequence 210,000+ User Drawing Img2CAD [2] Arxiv, 2024.7 Command Sequence 4,574 Single Image OpenECAD [14] Arxiv, 2024.6 Command Sequence 200,000+ Single Image ABC-mono [15] Arxiv, 2024.10 Command Sequence 208,853 Single Image Query2CAD [5] Arxiv, 2024.5 Python Macro 57 Text Text2CAD [4] NeurIPS 2024 Command Sequence 158,000+ Text Omni-CAD(Ours) Command Sequence 453,220 Multi-view Images/Text/Point TABLE I: Comparison of previous datasets and our proposed dataset. Our proposed Omni-CAD dataset is the only dataset available that simultaneously supports multi-view images, text, and point cloud conditioned data for CAD modeling. Notably, our dataset includes a large-scale collection of CAD models, second only to the ABC [8] and SketchGraphs [12] datasets. ††\dagger†: SketchGraphs [12] focuses on the 2D CAD sketches instead of the 3D CAD models. ‡‡\ddagger‡: Command Sequence Representation can convert to the B-rep representation. On the other hand, multimodal large language models (MLLMs) have demonstrated their capability in content generation across different modalities [16, 17]. However, the use of MLLMs for CAD generation remains unexplored. While MLLMs support direct input from various modalities, meeting the requirements for conditional generation, there are two main challenges when applying MLLMs to conditional CAD generation: (1) the lack of an efficient representation that MLLMs can interpret and manipulate the CAD models, and (2) the unavailability of a large scale multimodal CAD dataset to align CAD models to the text, image, and point cloud modalities. To be specific, CAD models require a high level of specificity in terms of dimensions, connectivities, and functional requirements, while current LLMs are primarily trained in natural language. Thus, it is desirable to find a suitable CAD representation for LLM generation. Also, from the perspective of user-system interaction, how to bridge CAD models with text, image, and point cloud, these three modalities into a unified framework remains a significant challenge. Each of these modalities represents information in vastly different formats. The text describes concepts and attributes, images capture visual details, point clouds represent spatial data, and CAD models require precise geometric and structural definitions. From the dataset side, the current CAD datasets with command sequence, including the Fusion360 [11] and DeepCAD [1], are on a relatively small scale (8,625 and 179,133, respectively). A detailed comparison of datasets is provided in Tab. I. More importantly, current CAD datasets do not contain paired multimodal CAD data. To support the training of MLLMs, it is desirable to have an even larger scale dataset with CADs paired with different modalities to support the conditional CAD generation with MLLMs. Figure 1: A simple example about the construction process of a CAD model with command sequence representation. Starting with a sketch operation on a chosen 2D plane, the extrusion operation then “drags” this 2D sketch into a 3D solid volume. Further editing requires another extruded 3D solid volume. Subsequently, the union will “merge” these two 3D solids into a single integrated solid. Other boolean operators from Constructive Solid Geometry (CSG) support the construction of more complex geometries. As a result, this CAD model can be represented with these command sequences. To address these challenges, we present CAD-MLLM to unleash the potential of MLLMs for CAD generation conditioned on multimodality inputs. Given that the primitive boundary representation of CAD models is non-sequential and unsuitable for an auto-regressive pipeline, motivated by the DeepCAD [1], we instead utilize the command sequences, vectorizing them into a condensed sequential data flow that is more efficient for MLLMs to learn from. Combined with multimodality data, our model is capable of constructing complete CAD models by conditioning on text, images, point clouds, and any combination of them. When multiple modalities are input as a combination, our multimodal model demonstrates its strength by supplementing missing or suboptimal information from one data modality with input from another. To support our methodology, we propose a data annotation pipeline combined with a data augmentation method to generate a new multimodality-conditioned CAD dataset named Omni-CAD. Omni-CAD includes text descriptions, multi-view images, point clouds, and their corresponding constructive modeling command sequences. Omni-CAD reaches 453,220 models after data augmentation. To evaluate the quality of the generated CAD models, although some previous CAD reconstruction works [18, 19, 20, 21] have proposed some well-established metrics for performance evaluation, such as utilizing sampled point clouds and patch topology to assess reconstruction fitting quality and patch structure fidelity, these metrics overlook an important nature of the CAD model: the overall topology quality of the CAD model in its final mesh representation. As a remedy, we propose three topology metrics, Segment Error (SegE), Dangling Edge Length (DangEL), and Self-Intersection Ratio (SIR), to evaluate the topological quality of the final generated model. Additionally, since CAD models use boundary representation to form closed surfaces, we introduce Flux Enclosure Error (FluxEE) to quantify how well the generated model encloses space. These metrics are broadly applicable to general models in mesh representation as well. We conduct extensive evaluations on the proposed benchmark, and our experiments demonstrate that our method achieves state-of-the-art performance compared to other CAD generation methods and shows high robustness under various data flaws at the inference stage. Our contributions can be summarized as follows: • We propose a unified multimodality-conditioned CAD generation method by leveraging the pretrained MLLM, and the condition can be text, images, point clouds, and any combination of these modalities. • We present a data annotation pipeline and create a large-scale dataset, named Omni-CAD, the first multimodality CAD dataset includes constructive modeling command sequences and the corresponding textual descriptions, multi-view images, and point cloud data. • We introduce four new evaluation metrics, namely, SegE, DangEL, SIR, and FluxEE, to evaluate the topological quality and enclosure of the generated CAD models, respectively. • Extensive experiments show that our method demonstrates state-of-the-art performance over the baseline methods and high robustness under data flaws during inference."
https://arxiv.org/html/2411.04952v1,M3DocRAG: Multi-modal Retrieval is What You Needfor Multi-page Multi-document Understanding,"Document visual question answering (DocVQA) pipelines that answer questions from documents have broad applications. Existing methods focus on handling single-page documents with multi-modal language models (MLMs), or rely on text-based retrieval-augmented generation (RAG) that uses text extraction tools such as optical character recognition (OCR). However, there are difficulties in applying these methods in real-world scenarios: (a) questions often require information across different pages or documents, where MLMs cannot handle many long documents; (b) documents often have important information in visual elements such as figures, but text extraction tools ignore them. We introduce M3DocRAG, a novel multi-modal RAG framework that flexibly accommodates various document contexts (closed-domain and open-domain), question hops (single-hop and multi-hop), and evidence modalities (text, chart, figure, etc.). M3DocRAG finds relevant documents and answers questions using a multi-modal retriever and an MLM, so that it can efficiently handle single or many documents while preserving visual information. Since previous DocVQA datasets ask questions in the context of a specific document, we also present M3DocVQA, a new benchmark for evaluating open-domain DocVQA over 3,000+ PDF documents with 40,000+ pages. In three benchmarks (M3DocVQA/MMLongBench-Doc/MP-DocVQA), empirical results show that M3DocRAG with ColPali and Qwen2-VL 7B achieves superior performance than many strong baselines, including state-of-the-art performance in MP-DocVQA. We provide comprehensive analyses of different indexing, MLMs, and retrieval models. Lastly, we qualitatively show that M3DocRAG can successfully handle various scenarios, such as when relevant information exists across multiple pages and when answer evidence only exists in images.","Document visual question answering (DocVQA) [42, 57, 40, 14, 31] is a multi-modal task that answers textual questions by interpreting information contained within document images. Existing methods on DocVQA either focus on visual question answering (VQA) on a single-page document (Fig. 1 (a)) or extract text from documents (e.g., via optical character recognition (OCR) [53, 43] or PDF text extraction [49, 18]) and use retrieval-augmented generation (RAG) [35], where a retrieval model finds relevant paragraphs and a language model answers questions given the paragraphs (Fig. 1 (b)). However, there are difficulties in applying these methods in real-world document understanding scenarios: (a) questions often require information across different pages or documents, where existing VQA methods cannot handle many long documents; (b) some documents feature complex visual formats such as tables, charts, and mixed layouts, but text extraction methods such as OCR ignore these nuances, leading to incomplete or inaccurate document interpretations. Accurately and efficiently answering questions across numerous, lengthy documents with intricate layouts would greatly benefit many domains such as finance, healthcare, and law, where document AI assistants can streamline the daily processing of large volumes of documents, improving productivity and enabling faster, more informed decision-making. Figure 2: Comparison of existing DocVQA datasets (left; e.g., DocVQA [42]) and our M3DocVQA dataset (right). In contrast to previous DocVQA datasets that have questions that are specific to a single provided PDF (e.g., “What was the gross profit in the year 2009?”), M3DocVQA has information-seeking questions that benchmark open-domain question answering capabilities across more than 3,000 PDF documents (i.e., 40,000+ pages). To overcome these limitations of existing DocVQA approaches, we introduce M3DocRAG (Multi-modal Multi-page Multi-Document Retrieval-Augmented Generation; Sec. 2), a novel multi-modal RAG framework that flexibly accommodates various document contexts (closed-domain and open-domain), question hops (single-hop and multi-hop), and evidence modalities (text, chart, figure, etc.). As illustrated in Fig. 1 (c), the M3DocRAG framework retrieves relevant document pages using a multi-modal retrieval model, such as ColPali [17], and generates answers to questions from the retrieved pages using a multi-modal language model (MLM), such as Qwen2-VL [59]. M3DocRAG operates in three stages: In (1) document embedding (Sec. 2.1), we convert all document pages into RGB images and extract visual embeddings (e.g., via ColPali) from the page images. In (2) page retrieval (Sec. 2.2), we retrieve the top-K pages of high similarity with text queries (e.g., MaxSim operator for ColPali). For the open-domain setting, we create approximate page indices, such as inverted file index (IVF) [52, 66], for faster search. In (3) question answering (Sec. 2.3), we conduct visual question answering with MLM to obtain the final answer. Please also see Fig. 3 for the detailed illustration of the framework. M3DocRAG can flexibly handle DocVQA in both closed domain (i.e., a single document) and open-domain (i.e., a large corpus of documents) settings. Figure 3: Our M3DocRAG framework (Sec. 2) consists of three stages: (1) document embedding (Sec. 2.1), (2) page retrieval (Sec. 2.2), and (3) question answering (Sec. 2.3). In (1) document embedding, we extract visual embedding (with ColPali) to represent each page from all PDF documents. In (2) page retrieval, we retrieve the top-K pages of high relevance (MaxSim scores) with text queries. In an open-domain setting, we create approximate page indices for faster search. In (3) question answering, we conduct visual question answering with multi-modal LM (e.g. Qwen2-VL) to obtain the final answer. While M3DocRAG framework supports DocVQA in an open-domain setting, the existing DocVQA datasets are not adequate for this setting, since their questions are in the context of a specific document, such as “What was the gross profit in the year 2009?” [42, 57, 14, 40], as illustrated in Fig. 2 (left). Hence, we also introduce M3DocVQA (Multi-modal Multi-page Multi-Document Visual Question Answering), an open-domain dataset that significantly raises the challenge of DocVQA to answering questions from a large document corpus (Sec. 3). By extending the MultimodalQA dataset’s [54] closed-domain context to an open-domain setting, M3DocVQA introduces 2,441 multi-hop questions spanning 3,368 PDF documents, which collectively contain over 41,005 pages of diverse multi-modal content, including text, images, and tables. This dataset presents real-world challenges by requiring models to navigate complex reasoning paths across pages and within various types of document elements, better reflecting the intricacies of document understanding. To demonstrate the effectiveness of M3DocRAG, we compare M3DocRAG with state-of-the-art baselines in three benchmarks: M3DocVQA, MMLongBench-Doc [40], and MP-DocVQA [57], which cover both open-domain (Sec. 5.1) and closed-domain (Sec. 5.2) DocVQA settings. Experiment results show that M3DocRAG with ColPali and Qwen2-VL 8B achieves superior performance than many strong baselines, including the state-of-the-art performance in MP-DocVQA. We also provide a comprehensive analysis (Sec. 5.3) about different indexing, MLMs, and retrieval components. Finally, we show qualitative examples (Sec. 5.4) where M3DocRAG can successfully handle various scenarios, such as when the relevant information exists across multiple pages and when answer evidence only exists in images. Overall, M3DocRAG is an effective, efficient, and flexible framework for answering questions from multi-modal documents in various settings."
https://arxiv.org/html/2411.04942v1,A Reinforcement Learning-Based Automatic Video Editing Method Using Pre-trained Vision-Language Model,"In this era of videos, automatic video editing techniques attract more and more attention from industry and academia since they can reduce workloads and lower the requirements for human editors. Existing automatic editing systems are mainly scene- or event-specific, e.g., soccer game broadcasting, yet the automatic systems for general editing, e.g., movie or vlog editing which covers various scenes and events, were rarely studied before, and converting the event-driven editing method to a general scene is nontrivial. In this paper, we propose a two-stage scheme for general editing. Firstly, unlike previous works that extract scene-specific features, we leverage the pre-trained Vision-Language Model (VLM) to extract the editing-relevant representations as editing context. Moreover, to close the gap between the professional-looking videos and the automatic productions generated with simple guidelines, we propose a Reinforcement Learning (RL)-based editing framework to formulate the editing problem and train the virtual editor to make better sequential editing decisions. Finally, we evaluate the proposed method on a more general editing task with a real movie dataset. Experimental results demonstrate the effectiveness and benefits of the proposed context representation and the learning ability of our RL-based editing framework.","With the development of multimedia technology, video has become a prime medium for conveying information (Zhang et al., 2022), thus creating high-quality videos becomes more and more crucial. Typically, video editing involves the shot selection problem and requires the editors to select and order the shots from a vast footage gallery. However, editing video or live multi-camera directing requires a multitude of skills and domain expertise. Even for professional editors/directors, editing and directing video streams are still challenging and labor-intensive processes (Saini and Ooi, 2018). A few automatic editing systems (Shrestha et al., 2010; Saini et al., 2012; Wu et al., 2015; Bano and Cavallaro, 2016) and directing systems (Ariki et al., 2006; Wang et al., 2008; Kaiser et al., 2012) have been proposed for event-driven scenes. The editing process is illustrated in Fig.1(a), where the virtual editor selects a view based on the scene events at each time point. In previous works, different view selection strategies have been developed, such as the Finite State Machine (FSM) model (Wang et al., 2007), trellis graph model (Arev et al., 2014), integer programming model (Pan et al., 2021), Markov Decision Problem (MDP) model(Hu et al., 2021), etc. However, there are still some drawbacks to these systems. On the one hand, these event-driven methods are scene-specific, and the features and selection algorithms are designed based on the demands of the scenes. This fact limits them to adapt to a more general setting ( Fig.1(b)) that focuses on retrieving the shots of various scenes sequentially from a candidate list based on the given shot context. On the other hand, although these systems are developed to mimic human editors using various scenario-related hand-crafted features and empirical rules (Germeys and d’Ydewalle, 2007; Gottlieb, 2013), the quality of their productions heavily depends on empirical parameter tuning. There is still an aesthetic gap between automatic productions and professional-looking videos due to the difficulties in transforming empirical practices into computable formulas. To solve the scene diversity problem for the general editing in Fig.1(b), this paper first defines a proxy editing task and explores a general editing representation. Furthermore, we posit that the view selection (Fig.1 (a)) can be considered as a particular variant of shot selection (Fig.1(b)) with a strict timeline constraint. Thereby, we propose a general learning-based editing framework to mitigate the aesthetic gap problem for both two settings. We propose a new editing paradigm to accommodate diverse scenes for general editing, e.g., movie editing, as shown in Fig.1(b). Specifically, the editing task is to predict the multi-dimensional attributes of subsequent shots, since shot attributes are common in different scenes and determine the types of shots. Thereby, the editor is able to retrieve or create the shots from the raw footage based on the predicted attributes. Furthermore, the gap between professional productions and automated productions also exists in the automatic cinematography area, which focuses on automatically placing and moving the cameras to capture well-composed photos or videos (Heck et al., 2007; Chen et al., 2016). To mitigate this problem, some recent works (Gschwindt et al., 2019; Huang et al., 2019; Jiang et al., 2020) start studying the methods that learn a camera controller from expert behaviors or human productions. Inspired by these works, this paper will explore a learning-based strategy to learn the editing pattern styles from professional demonstrations. Figure 1. Definitions of different editing tasks. (a) is event-driven editing; (b) is attribute prediction-based editing. Learning the editing pattern from demonstration videos of the general scene is nontrivial. Like the editing process of existing event-driven editing systems (Saini et al., 2012; Arev et al., 2014; Wu et al., 2015), the learning-based editing undergoes two stages, i.e., the representation extraction of context shots and the training of the virtual editor. As the representation for general editing is rarely studied before, it is still challenging to distill the context information for editing general scenes. Recently, pre-trained Vision-Language Model (VLM) (Radford et al., 2021) has successfully assisted various vision tasks (Minderer et al., 2022; Xu et al., 2022) due to its great generalization and adaption abilities. Motivated by this, we propose to leverage VLM’s extraordinary zero-shot recognition capability to extract the attribute distributions of context shots as the context representations. Afterward, a transformer-based neural network is developed to encode the temporal relationship of context representations for the prediction task. Relevant experiments are conducted in Sec.4. The methodology of training a virtual editor also plays a crucial role. Previous methods have attempted to train a Long Shot-Term Memory (LSTM) model (Chen et al., 2018; Wu and Jhala, 2018) as a virtual director with manual annotations in a supervised manner, while the virtual director/editor trained in a conventional supervised way cannot produce optimal sequential editing results. To this end, we propose an RL-based editing framework to optimize the virtual editor for making better sequential editing decisions. The details are discussed in Sec.3. To investigate the style/pattern learning capability and the generalization of the proposed framework, we apply it to the task of learning personal preferences in an event-driven scene, i.e., the automatic multi-camera lecture broadcasting (Liu et al., 2001; Rui et al., 2004; Wang et al., 2007) scene, in Sec.4. Finally, we design a set of metrics to quantitatively evaluate the effectiveness of the proposed representations and the editing framework on the AVE dataset (Argaw et al., 2022). To the best of our knowledge, this is the first work to explore the knowledge of VLM for general editing purposes. It is also the first time to study the RL technique for automatic editing/directing tasks. In summary, we have made the following contributions: • We attempt to leverage the knowledge of pre-trained VLM to represent the context shots for general editing purposes. • We propose an RL-based editing framework to mitigate the video quality gap and optimize the sequential decisions. • We develop a set of metrics and conduct experiments to validate the benefits of the proposed components. We further show an application of our framework to a real-world broadcasting scene to verify its learning capability."
https://arxiv.org/html/2411.04933v1,SaSR-Net: Source-Aware Semantic Representation Network for Enhancing Audio-Visual Question Answering,"Audio-Visual Question Answering (AVQA) is a challenging task that involves answering questions based on both auditory and visual information in videos. A significant challenge is interpreting complex multi-modal scenes, which include both visual objects and sound sources, and connecting them to the given question. In this paper, we introduce the Source-aware Semantic Representation Network (SaSR-Net), a novel model designed for AVQA. SaSR-Net utilizes source-wise learnable tokens to efficiently capture and align audio-visual elements with the corresponding question. It streamlines the fusion of audio and visual information using spatial and temporal attention mechanisms to identify answers in multi-modal scenes. Extensive experiments on the Music-AVQA and AVQA-Yang datasets show that SaSR-Net outperforms state-of-the-art AVQA methods.","Figure 1: Leveraging semantic representation for AVQA involves: (1) Extracting features of various instrument types based on semantic tokens, (2) Identifying the location of the relevant sounding instruments, and (3) Establishing connections between the extracted semantic features, identified instrument locations, and the crucial parts of the question, guiding the model to answer the question accurately. Recent contributions to the field of audio-visual question answering (AVQA) include the creation of diverse datasets and sophisticated models Yun et al. (2021); Yang et al. (2022); Li et al. (2022, 2023); Jiang and Yin (2023). For example, the Pano-AVQA dataset Yun et al. (2021) contains 360-degree videos paired with corresponding QA sets, while the AVQA-Yang dataset Yang et al. (2022) is designed for answering audio-visual questions in real-world scenarios. The MUSIC-AVQA dataset Li et al. (2022) further broadened the research scope by focusing on spatio-temporal understanding in audio-visual scenes. This dataset uses a dual attention mechanism, identifying sound-producing areas visually first and then applying attention for spatio-temporal reasoning. More recently, PSTP-Net Li et al. (2023) was introduced, which progressively identifies key regions relevant to audio-visual questions using refined attention mechanisms. Existing AVQA methods typically employ general audio and visual encoders to extract features from videos. However, this strategy often fails to link certain sound-producing objects in the video with the responses. Consider questions like What is the instrument on the left of the cello? which necessitates specific type and location awareness, as shown in Fig. 1. Current models often find it difficult to associate the cello mentioned in the question with its actual representation in the video scene. To address these challenges, we propose the Source-aware Semantic Representation Network (SaSR-Net). This model enhances the understanding and integration of individual sound sources and visual objects in AVQA by two strategies: (1) Source-wise Learnable Tokens: Embedded within the Source-aware Semantic Representation Block, these tokens capture essential semantic features from both audio and visual data. This facilitates precise alignment and enhances semantic richness, enabling the model to accurately associate auditory and visual elements based on the query context. (2) Attention Mechanisms: The model utilizes spatial and temporal attention mechanisms to identify and synchronize relevant visual and audio regions with the query. This not only enhances the accuracy of localization but also strengthens cross-modal associations, crucial for forming a coherent understanding of the scene. The efficacy of SaSR-Net is demonstrated by its performance on the Music-AVQA Li et al. (2022) and AVQA-Yang Yang et al. (2022) datasets, where it surpasses state-of-the-art AVQA approaches. The results highlight the effectiveness of the model’s source-aware and semantically driven approach in managing complex audio-visual data. Our key contributions are as follows: 1. We introduce SaSR-Net, a novel framework that enriches the understanding of sound and visual information, leveraging Source-wise Learnable Tokens to extract semantic-aware audio and visual representations for AVQA. 2. SaSR-Net integrates multi-modal spatial and temporal attention mechanisms to adaptively leverage visual and audio information in videos for accurate scene understanding. 3. Our comprehensive experiments and ablation studies demonstrate the effectiveness of our proposed method."
https://arxiv.org/html/2411.04928v1,DimensionX: Create Any 3D and 4D Scenes from a Single Image withControllable Video Diffusion,"In this paper, we introduce DimensionX, a framework designed to generate photorealistic 3D and 4D scenes from just a single image with video diffusion. Our approach begins with the insight that both the spatial structure of a 3D scene and the temporal evolution of a 4D scene can be effectively represented through sequences of video frames. While recent video diffusion models have shown remarkable success in producing vivid visuals, they face limitations in directly recovering 3D/4D scenes due to limited spatial and temporal controllability during generation. To overcome this, we propose ST-Director, which decouples spatial and temporal factors in video diffusion by learning dimension-aware LoRAs from dimension-variant data. This controllable video diffusion approach enables precise manipulation of spatial structure and temporal dynamics, allowing us to reconstruct both 3D and 4D representations from sequential frames with the combination of spatial and temporal dimensions. Additionally, to bridge the gap between generated videos and real-world scenes, we introduce a trajectory-aware mechanism for 3D generation and an identity-preserving denoising strategy for 4D generation. Extensive experiments on various real-world and synthetic datasets demonstrate that DimensionX achieves superior results in controllable video generation, as well as in 3D and 4D scene generation, compared with previous methods. Project Page: https://chenshuo20.github.io/DimensionX/","In the context of computer graphics and vision, understanding and generating 3D and 4D content are pivotal to create realistic visual experiences [5, 54]. By representing spatial (3D) and temporal (4D) dimensions, videos serve as a powerful medium for capturing dynamic real-world scenes. Despite substantial advancements in 3D and 4D reconstruction technologies [19, 52, 44, 47], there remains a critical shortage of large-scale 3D and 4D video datasets, limiting the potential for high-quality 3D and 4D scene generation from a single image. This scarcity poses a fundamental challenge in constructing photorealistic and interactive environments. Fortunately, recent advancements in video diffusion models have shown considerable promise in understanding and simulating real-world environments [4, 58]. Driven by advanced video diffusion models, recent works [43, 24, 55, 60] have made attempts to leverage the spatial and temporal priors embedded in video diffusion to generate 3D and 4D content from a single image. Despite these rapid developments, existing methods either concentrate on the object-level generation with video diffusion trained on static or dynamic mesh renderings [43, 24, 55] or employ time-intensive per-scene optimization for coarse scene-level generation [60] (e.g., Score Distillation Sampling [36]). This leaves the generation of coherent and realistic 3D/4D scenes an open challenge. In this paper, we present DimensionX, a novel approach to create high-fidelity 3D and 4D scenes from a single image with controllable video diffusion. While recent video diffusion models are capable of producing realistic results, it remains difficult to reconstruct 3D and 4D scenes directly from these generated videos, primarily due to their poor spatial and temporal controllability during the generation process. Our key insight is to decouple the temporal and spatial factors in video diffusion, allowing for precise control over each individually and in combination. To achieve the dimension-aware control, we establish a comprehensive framework to collect datasets that vary in spatial and temporal dimensions. With these datasets, we present ST-Director, which separates spatial and temporal priors in video diffusion through dimension-aware LoRAs. Additionally, by analyzing the denoising mechanics in video diffusion, we develop a training-free composition method that achieves hybrid-dimension control. With this control, DimensionX generates sequences of spatially and temporally variant frames, enabling the reconstruction of 3D appearances and 4D dynamic motions. To handle complex real-world scenes with our ST-Director, we design a trajectory-aware approach for 3D generation and an identity-preserving denoising mechanism for 4D generation. Extensive experiments demonstrate that our DimensionX outperforms previous methods in terms of visual quality and generalization for 3D and 4D scene generation, indicating that video diffusion models offer a promising direction for creating realistic, dynamic environments. In summary, our main contributions are: • We present DimensionX, a novel framework for generating photorealistic 3D and 4D scenes from only a single image using controllable video diffusion. • We propose ST-Director, which decouples the spatial and temporal priors in video diffusion models by learning (spatial and temporal) dimension-aware modules with our curated datasets. We further enhance the hybrid-dimension control with a training-free composition approach according to the essence of video diffusion denoising process. • To bridge the gap between video diffusion and real-world scenes, we design a trajectory-aware mechanism for 3D generation and an identity-preserving denoising approach for 4D generation, enabling more realistic and controllable scene synthesis. • Extensive experiments manifest that our DimensionX delivers superior performance in video, 3D, and 4D generation compared with baseline methods. Figure 2: Pipeline of DimensionX. Our framework is mainly divided into three parts. (a) Controllable Video Generation with ST-Director. We introduce ST-Director to decompose the spatial and temporal parameters in video diffusion models by learning dimension-aware LoRA on our collected dimension-variant datasets. (b) 3D Scene Generation with S-Director. Given one view, a high-quality 3D scene is recovered from the video frames generated by S-Director. (c) 4D Scene Generation with ST-Director. Given a single image, a temporal-variant video is produced by T-Director, from which a key frame is selected to generate a spatial-variant reference video. Guided by the reference video, per-frame spatial-variant videos are generated by S-Director, which are then combined into multi-view videos. Through the multi-loop refinement of T-Director, consistent multi-view videos are then passed to optimize the 4D scene."
https://arxiv.org/html/2411.04925v1,StoryAgent: Customized Storytelling Video Generation via Multi-Agent Collaboration,"The advent of AI-Generated Content (AIGC) has spurred research into automated video generation to streamline conventional processes. However, automating storytelling video production, particularly for customized narratives, remains challenging due to the complexity of maintaining subject consistency across shots. While existing approaches like Mora and AesopAgent integrate multiple agents for Story-to-Video (S2V) generation, they fall short in preserving protagonist consistency and supporting Customized Storytelling Video Generation (CSVG). To address these limitations, we propose StoryAgent, a multi-agent framework designed for CSVG. StoryAgent decomposes CSVG into distinct subtasks assigned to specialized agents, mirroring the professional production process. Notably, our framework includes agents for story design, storyboard generation, video creation, agent coordination, and result evaluation. Leveraging the strengths of different models, StoryAgent enhances control over the generation process, significantly improving character consistency. Specifically, we introduce a customized Image-to-Video (I2V) method, LoRA-BE, to enhance intra-shot temporal consistency, while a novel storyboard generation pipeline is proposed to maintain subject consistency across shots. Extensive experiments demonstrate the effectiveness of our approach in synthesizing highly consistent storytelling videos, outperforming state-of-the-art methods. Our contributions include the introduction of StoryAgent, a versatile framework for video generation tasks, and novel techniques for preserving protagonist consistency.","Storytelling videos, typically multi-shot sequences depicting a consistent subject such as a human, animal, or cartoon character, are extensively used in advertising, education, and entertainment. Producing these videos traditionally is both time-consuming and expensive, requiring significant technical expertise. However, with advancements in AI-Generated Content (AIGC), automated video generation is becoming an increasingly researched area, offering the potential to streamline and enhance traditional video production processes. Techniques such as Text-to-Video (T2V) generation models [He et al., 2022, Ho et al., 2022, Singer et al., 2022, Zhou et al., 2022, Blattmann et al., 2023a, Chen et al., 2023a] and Image-to-Video (I2V) methods [Zhang et al., 2023a, Dai et al., 2023, Wang et al., 2024a, Zhang et al., 2023b] enable users to generate corresponding video outputs simply by inputting text or images. While significant advancements have been made in video generation research, automating storytelling video production remains challenging. Current models struggle to preserve subject consistency throughout the complex process of storytelling video generation. Recent agent-driven systems, such as Mora [Yuan et al., 2024] and AesopAgent [Wang et al., 2024b], have been proposed to address Story-to-Video (S2V) generation by integrating multiple specialized agents, such as T2I and I2V generation agents. However, these methods fall short in allowing users to generate storytelling videos featuring their designated subjects, i.e., Customized Storytelling Video Generation (CSVG). The protagonists generated from story descriptions often exhibit inconsistency across multiple shots. Another line of research focusing on customized text-to-video generation like DreamVideo [Wei et al., 2023] and Magic-Me [Ma et al., 2024] can also be employed to synthesize storytelling videos. They first fine-tune the models using the data about the given reference protagonists, then generate the videos from the story descriptions. Despite these efforts, maintaining fidelity to the reference subjects remains a significant challenge. As shown in Figure 1, the results of TI-AnimateDiff, DreamVideo, and Magic-Me fail to preserve the appearance of the reference subject in the video. In these methods, the learned concept embeddings cannot fully capture and express the subject in different scenes. Considering the limitations of existing storytelling video generation models, we explore the potential of multi-agent collaboration to synthesize customized storytelling videos. In this paper, we introduce a multi-agent framework called StoryAgent, which consists of multiple agents with distinct roles that work together to perform CSVG. Our framework decomposes CSVG into several subtasks, with each agent responsible for a specific role: 1) Story designer, writing detailed storylines and descriptions for each scene.2) Storyboard generator, generating storyboards based on the story descriptions and the reference subject. 3) Video creator, creating videos from the storyboard. 4) Agent manager, coordinating the agents to ensure orderly workflow. 5) Observer, reviewing the results and providing feedback to the corresponding agent to improve outcomes. By leveraging the generative capabilities of different models, StoryAgent enhances control over the generation process, resulting in significantly improved character consistency. The core functionality of the agents in our framework can be flexibly replaced, enabling the framework to complete a wide range of video-generation tasks. This paper primarily focuses on the accomplishment of CSVG. However, simply equipping the storyboard generator with existing T2I models, such as SDXL [Podell et al., 2023] as used by Mora and AesopAgent, often fails to preserve inter-shot consistency, i.e., maintaining the same appearance of customized protagonists across different storyboard images. Similarly, directly employing existing I2V methods such as SVD [Blattmann et al., 2023b] and Gen-2 [Esser et al., 2023] leads to issues with intra-shot consistency, failing to keep the character’s fidelity within a single shot. Inspired by the image customization method AnyDoor [Chen et al., 2023b], we develop a new pipeline comprising three main steps—generation, removal, and redrawing—as the core functionality of the storyboard generator agent to produce highly consistent storyboards. To further improve intra-shot consistency, we propose a customized I2V method. This involves integrating a background-agnostic data augmentation module and a Low-Rank Adaptation with Block-wise Embeddings (LoRA-BE) into an existing I2V model [Xing et al., 2023] to enhance the preservation of protagonist consistency. Extensive experiments on both customized and public datasets demonstrate the superiority of our method in generating highly consistent customized storytelling videos compared to state-of-the-art customized video generation approaches. Readers can view the dynamic demo videos available at this anonymous link: https://github.com/storyagent123/Comparison-of-storytelling-video-results/blob/main/demo/readme.md111The codes will be released upon the acceptance of the paper The main contributions of this work are as follows: 1) We propose StoryAgent, a multi-agent framework for storytelling video production. This framework stands out for its structured yet flexible systems of agents, allowing users to perform a wide range of video generation tasks. These features also enable StoryAgent to be a prime instrument for pushing forward the boundaries of CSVG. 2) We introduce a customized Image-to-Video (I2V) method, LoRA-BE (Low-Rank Adaptation with Block-wise Embeddings), to enhance intra-shot temporal consistency, thereby improving the overall visual quality of storytelling videos. 3) In the experimental section, we present an evaluation protocol on public datasets for CSVG and also collect new subjects from the internet for testing. Extensive experiments have been carried out to prove the benefit of the proposed method."
https://arxiv.org/html/2411.04924v1,MVSplat360: Feed-Forward 360 Scene Synthesisfrom Sparse Views,"We introduce MVSplat360, a feed-forward approach for 360° novel view synthesis (NVS) of diverse real-world scenes, using only sparse observations. This setting is inherently ill-posed due to minimal overlap among input views and insufficient visual information provided, making it challenging for conventional methods to achieve high-quality results. Our MVSplat360 addresses this by effectively combining geometry-aware 3D reconstruction with temporally consistent video generation. Specifically, it refactors a feed-forward 3D Gaussian Splatting (3DGS) model to render features directly into the latent space of a pre-trained Stable Video Diffusion (SVD) model, where these features then act as pose and visual cues to guide the denoising process and produce photorealistic 3D-consistent views. Our model is end-to-end trainable and supports rendering arbitrary views with as few as 5 sparse input views. To evaluate MVSplat360’s performance, we introduce a new benchmark using the challenging DL3DV-10K dataset, where MVSplat360 achieves superior visual quality compared to state-of-the-art methods on wide-sweeping or even 360° NVS tasks. Experiments on the existing benchmark RealEstate10K also confirm the effectiveness of our model. The video results are available on our project page: donydchen.github.io/mvsplat360.","The rapid advancement in 3D reconstruction and NVS has been facilitated by the emergence of differentiable rendering [29, 31, 41, 40, 21]. These methods, while fundamental and impressive, are primarily tailored for per-scene optimization, requiring hundreds or even thousands of images to comprehensively capture every aspect of the scene. Consequently, the optimization process for each scene can be time-consuming, and collecting thousands of images is impractical for casual users. In contrast, we consider the problem of novel view synthesis in diverse real-world scenes using a limited number of source views through a feed-forward network. In particular, this work investigates the feasibility of rendering wide-sweeping or even 360° novel views using extremely sparse observations, like fewer than 5555 images. This task is inherently challenging due to the complexity of scenes, where the limited views do not contain sufficient information to recover the whole 3D scene. Consequently, there is a necessity to ensemble visible information under minimal overlap accurately and generate missing details reasonably. This represents a new problem setting in sparse-view feed-forward NVS. Existing feed-forward methods typically focus on two distinct scenarios: 360° NVS with extremely sparse observations, but only at object-level [20, 52, 66, 59, 27, 72, 63, 56, 62, 50, 46, 47, 18], or generating reasonable results for scene-level synthesis, but only for nearby viewpoints [53, 7, 19, 43, 9, 13, 6, 60, 10, 70, 42, 61]. In contrast, we argue that the time is ripe to unify these previously distinct research directions. Our goal should be to develop systems capable of synthesizing wide-sweeping or even 360° novel views of large, real-world scenes with complex geometry and significant occlusion. Specifically, this work explores synthesising 360° novel views from fewer than 5555 input images. We show that in this challenging setting, existing feed-forward scene synthesis approaches [9, 13, 6, 60, 10, 57] struggle to succeed. This failure arises from two main factors: i) the limited overlap among input views causes many contents to appear in only a few views or even a single one, posing significant challenges for 3D reconstruction; ii) the extremely sparse observations lack sufficient information to capture the comprehensive details of the whole scene, resulting in regions unobserved from novel viewpoints. Figure 1: Examples of our MVSplat360. Given sparse and wide-baseline observations of diverse in-the-wild scenes, MVSplat360 can directly render 360° novel views (inward or outward facing) or other natural camera trajectory views in a feed-forward manner, without any per-scene optimization. In this paper, we propose a simple yet effective framework to address these limitations and introduce the first benchmark for feed-forward 360° scene synthesis from sparse input views. Our key idea is to leverage prior knowledge from a large-scale pre-trained latent diffusion model (LDM) [35] to “imagine” plausible unobserved and disoccluded regions in novel views, which are inherently highly ambiguous. Unlike existing 360° object-level NVS approaches [27, 63, 49, 56, 25, 72, 50], large-scale real-world scenes comprise multiple 3D assets with complex arrangements, heavy occlusions, and varying rendering trajectories, which makes it particularly challenging to condition solely on camera poses, as also verified by concurrent work ViewCrafter [68]. To develop a performant framework for scene-level synthesis, we opt to treat the LDM as a refinement module, while relying on a 3D reconstruction model to process the complex geometric information. Broadly, we build upon the feed-forward 3DGS [21] model, MVSplat [10], to obtain coarse novel views by matching and fusing multi-view information with the cross-view transformer and cost volume. Although these results are imperfect, exhibiting visual artifacts and missing regions (see Fig. 1), they represent the reasonable geometric structure of the scene, as they are rendered from 3D representation. Furthermore, we choose Stable Video Diffusion (SVD) [3] over other image-based LDM as the refinement module, since its strong temporal consistency capabilities align better with the view-consistent requirement of the NVS task, as also observed by concurrent work 3DGS-Enhancer [28]. Conditioning SVD with the 3DGS rendered outputs, our MVSplat360 produces visually appealing novel views that are multi-view consistent and geometrically accurate (see Fig. 1). Importantly, the original MVSplat outputs only RGB images, which is not the optimal condition for the generator, and is difficult to optimize jointly with the SVD denoising module. To tackle this, we propose a simple Gaussian feature rendering with multi-channels, supervised with an introduced latent space alignment loss. Despite a seemingly minor change, the additional feature condition for SVD leads to a significant impact: It bypasses the SVD’s frozen image encoder, allowing the gradients from SVD to backpropagate to enhance the geometry backbone and lead to improved visual quality, especially on the new challenging DL3DV-10K dataset. While related work Reconfusion [58], CAT3D [15] and latentSplat [57] also combine the 3D representation with 2D generators, the former two focus more on per-scene optimisation, while the latter only shows 360° NVS at the object level. We conduct a series of experiments, mainly on two datasets. First, we establish a new benchmark on DL3DV-10K dataset [23], creating a new training and testing split for feed-forward wide-sweeping and 360° NVS. In this challenging setting, our MVSplat360 achieves photorealistic 360° NVS from sparse observations and demonstrates significantly better visual quality, where the previous scene-level feed-forward methods [9, 6, 60, 10] fail to achieve plausible results. Second, we deploy MVSplat360 on the existing RealEstate10K [74] benchmark. Following latentSplat [57], we estimate both interpolation and extrapolation NVS, and report state-of-the-art performance. Our main contributions can be summarized as follows. 1) We introduce a crucial and pressing problem for novel view synthesis, i.e., how to do wide-sweeping or even 360° NVS from sparse and widely-displaced observations of diverse in-the-wild scenes (not objects) in a feed-forward manner (no any per-scene optimization). 2) We propose an effective solution that nicely integrates the latest feed-forward 3DGS and the pre-trained Stable Video Diffusion (SVD) model with meticulous integration designs, where the former is for reconstructing coarse geometry and the latter is for refining the noisy and incomplete coarse reconstruction. 3) Extensive results on the challenging DL3DV-10K and RealEstate10K datasets demonstrate the superior performance of our MVSplat360."
https://arxiv.org/html/2411.04923v1,VideoGLaMM: A Large Multimodal Model for Pixel-LevelVisual Grounding in Videos,"Fine-grained alignment between videos and text is challenging due to complex spatial and temporal dynamics in videos. Existing video-based Large Multimodal Models (LMMs) handle basic conversations but struggle with precise pixel-level grounding in videos. To address this, we introduce VideoGLaMM, a LMM designed for fine-grained pixel-level grounding in videos based on user-provided textual inputs. Our design seamlessly connects three key components: a Large Language Model, a dual vision encoder that emphasizes both spatial and temporal details, and a spatio-temporal decoder for accurate mask generation. This connection is facilitated via tunable V→→\rightarrow→L and L→→\rightarrow→V adapters that enable close Vision-Language (VL) alignment. The architecture is trained to synchronize both spatial and temporal elements of video content with textual instructions. To enable fine-grained grounding, we curate a multimodal dataset featuring detailed visually-grounded conversations using a semiautomatic annotation pipeline, resulting in a diverse set of 38k video-QA triplets along with 83k objects and 671k masks. We evaluate VideoGLaMM on three challenging tasks: Grounded Conversation Generation, Visual Grounding, and Referring Video Segmentation. Experimental results show that our model consistently outperforms existing approaches across all three tasks.","Figure 1: Grounded Conversation with VideoGLaMM. Our proposed multimodal video conversational model provides text responses grounded at the pixel level in the input video. The generated masks are spatio-temporally consistent across frames. The fine-grained grounded outputs from VideoGLaMM describe different levels of granularity, e.g., person, objects (bike), stuff (road), and explain object and scene attributes. Existing Video-LMMs do not offer pixel-level grounded conversational capability. The rise of Large Language Models (LLMs) has significantly advanced progress in language-based tasks [7, 10, 13, 34, 45]. Their success in solving language-based complex reasoning tasks has led to their adoption in visual domains, resulting in Large Multimodal Models (LMMs). To align textual and visual modalities, previous works [55, 27, 11, 20, 19] train a projection layer or a cross-attention block that maps visual features into the latent space of an LLM. This straightforward adaptation has enabled advanced spatial understanding, allowing detailed conversations about image content. Recently, these models have been extended to video, aligning textual instructions with the spatio-temporal inputs, leading to the development of Video-LMMs. Existing Video-LMMs [23, 21, 30, 50, 29, 31, 9], similar to image-based LMMs, tune single or multiple projection layers to align videos with the language modality using the conventional visual instruction tuning paradigm. Although this simple alignment aids in understanding the global content of videos, it poses challenges in capturing localized object-specific context. Consequently, existing works [31, 21, 50, 23] have demonstrated capabilities in video comprehension and dialogue, they lack the crucial feature of fine-grained visual grounding, which aims to associate the LMM’s response to specific objects within the video input. The ability of an LMM to generate visually grounded responses ensures that the model understands fine-grained spatial and temporal details in a video and can relate them with the generated text. To bridge this gap, we introduce VideoGLaMM, a large video multimodal model capable of pixel-level spatio-temporal grounding. The model responds to natural language queries from the user and intertwines spatio-temporal object masks in its generated textual responses to provide a detailed understanding of video content. VideoGLaMM seamlessly connects three key components: a Large Language Model (LLM); dual vision encoders; and a spatio-temporal pixel decoder. The dual vision encoders extract spatial and temporal features separately, which are jointly passed to the LLM to output responses rich in both spatial and temporal cues. Our spatio-temporal pixel decoder outputs the fine-grained object masks corresponding to the specific objects in the LLM output to visually ground its responses. These components are integrated via tunable Vision-to-Language (V→L) and Language-to-Vision (L→V) adapters that enable close vision-language alignment, unlike existing works that perform alignment with a single adapter. As there currently exists no instruction-tuning dataset with fine-grained masks associated with video conversations, we present a benchmark instruction tuning dataset curated through a semi-automatic pipeline (Sec. 4). The dataset consists of 38k grounded video-QA triplet pairs with 83k objects and 671k fine-grained masks. The proposed benchmark dataset enables spatio-temporal modeling and significantly augments the capacity of the model to understand videos comprehensively, leading to state-of-the-art performance in grounded conversation generation, temporal grounding, and referring video segmentation tasks under zero-shot settings. In summary, our contributions are as follows: • We introduce VideoGLaMM, a video large multimodal model, capable of pixel-level spatio-temporal grounding, featuring an end-to-end alignment mechanism. • To achieve fine-grained spatio-temporal alignment, we introduce a benchmark instruction tuning dataset consisting of 38k grounded video-QA triplet pairs and 83k objects and roughly 671k fine-grained spatio-temporal masks. • We assess the performance of VideoGLaMM across diverse tasks spanning grounded conversation generation, visual grounding, and referring video segmentation, where it achieves state-of-the-art performance."
https://arxiv.org/html/2411.04892v1,In the Era of Prompt Learning with Vision-Language Models,"Large-scale foundation models like CLIP have shown strong zero-shot generalization but struggle with domain shifts, limiting their adaptability. In our work, we introduce StyLIP, a novel domain-agnostic prompt learning strategy for Domain Generalization (DG). StyLIP disentangles visual style and content in CLIP’s vision encoder by using style projectors to learn domain-specific prompt tokens and combining them with content features. Trained contrastively, this approach enables seamless adaptation across domains, outperforming state-of-the-art methods on multiple DG benchmarks. Additionally, we propose AD-CLIP for unsupervised domain adaptation (DA), leveraging CLIP’s frozen vision backbone to learn domain-invariant prompts through image style and content features. By aligning domains in embedding space with entropy minimization, AD-CLIP effectively handles domain shifts, even when only target domain samples are available. Lastly, we outline future work on class discovery using prompt learning for semantic segmentation in remote sensing, focusing on identifying novel or rare classes in unstructured environments. This paves the way for more adaptive and generalizable models in complex, real-world scenarios.","1. Vision-Language Models Vision-Language Models have revolutionized the fields of computer vision and natural language processing by leveraging shared embeddings across these modalities. Models such as CLIP (Radford et al., 2021) align images with corresponding text descriptions in a common space, allowing for flexible downstream tasks such as zero-shot learning, image classification, and retrieval. The success of VLMs lies in their ability to generalize across diverse tasks and domains without fine-tuning for each specific case. The integration of prompt learning into VLMs is a natural evolution that enables fine-tuned adaptation to specific domains and tasks, offering opportunities for efficient transfer learning. Instead of retraining the model, task-specific prompts guide the model’s focus, ensuring alignment with the desired domain or objective."
https://arxiv.org/html/2411.04873v1,Boosting Latent Diffusion with Perceptual Objectives,"Latent diffusion models (LDMs) power state-of-the-art high-resolution generative image models. LDMs learn the data distribution in the latent space of an autoencoder (AE) and produce images by mapping the generated latents into RGB image space using the AE decoder. While this approach allows for efficient model training and sampling, it induces a disconnect between the training of the diffusion model and the decoder, resulting in a loss of detail in the generated images. To remediate this disconnect, we propose to leverage the internal features of the decoder to define a latent perceptual loss (LPL). This loss encourages the models to create sharper and more realistic images. Our loss can be seamlessly integrated with common autoencoders used in latent diffusion models, and can be applied to different generative modeling paradigms such as DDPM with epsilon and velocity prediction, as well as flow matching. Extensive experiments with models trained on three datasets at 256 and 512 resolution show improved quantitative – with boosts between 6% and 20% in FID – and qualitative results when using our perceptual loss.","Latent diffusion models (LDMs) (Rombach et al., 2022) have enabled considerable advances in image generation, and elevated the problem of generative image modeling to a level where it has become available as a technology to the public. A critical part to this success is to define the generative model in the latent space of an autoencoder (AE), which reduces the resolution of the representation over which the model is defined, thereby making it possible to scale diffusion methods to larger datasets, resolutions, and architectures than original pixel-based diffusion models (Dhariwal and Nichol, 2021; Sohl-Dickstein et al., 2015). To train an LDM, all images are first projected into a latent space with the encoder of a pre-trained autoencoder, and then, the diffusion model is optimized directly in the latent space. Note that when learning the diffusion model the AE decoder is not used – the diffusion model does not receive any training feedback that would ensure that all latent values reachable by the diffusion process decode to a high quality image. This training procedure leads to a disconnect between the diffusion model and the AE decoder, prompting the LDM to produce low quality images that oftentimes lack high frequency image components. Moreover, we note that the latent spaces of pre-trained LDM’s autoencoders tend to be highly irregular, in the sense that small changes in the latent space can lead to large changes in the generated images, further exacerbating the autoencoder-diffusion disconnect problem. In this work, we propose to alleviate this autoencoder-diffusion disconnect and propose to include the AE decoder in the training objective of LDM. In particular, we introduce latent perceptual loss (LPL) that acts on the decoder’s intermediate features to enrich the training signal of LDM. This is similar to the use of perceptual losses for image-to-image translation tasks (Johnson et al., 2016; Zhang et al., 2018), but we apply this idea in the context of generative modeling and use the feature space of the pre-trained AE decoder rather than that of an external pre-trained discriminative network. Our latent perceptual loss results in sharper and more realistic images, and leads to better structural consistency than the baseline – see Figure 1. We validate LPL on three datasets of different sizes – the commonly used datasets ImageNet-1k (1M data points) and CC12M (12M data points), and additionally a private dataset S320M (320M data points) – as well as three generative models formulation – DDPM (Ho et al., 2020) with velocity and epsilon prediction, and conditional flow matching model (Lipman et al., 2023). In our experiments, we report standard image generative model metrics – such as FID (Heusel et al., 2017), CLIPScore (Hessel et al., 2021), as well as Precision and Recall (Sajjadi et al., 2018; Kynkäänniemi et al., 2019). Our experiments show that the use of LPL leads to consistent performance boosts between 6% and 20% in terms of FID. Our qualitative analysis further highlights the benefits of LPL, showing images that are sharp and contain high-frequency image details. In summary, our contributions are: • We propose the latent perceptual loss (LPL), a perceptual loss variant leveraging the intermediate feature representation of the autoencoder’s decoder. • We present extensive experimental results on the ImageNet-1k, CC12M, and S320M datasets, demonstrating the benefits of LPL in boosting the model’s quality by 6% to 20% in terms of FID. • We show that LPL is effective for a variety of generative model formulations including DDPM and conditional flow matching approaches."
https://arxiv.org/html/2411.04865v1,ZAHA: Introducing the Level of Facade Generalization and the Large-Scale Point Cloud Facade Semantic Segmentation Benchmark Dataset,"Facade semantic segmentation is a long-standing challenge in photogrammetry and computer vision. Although the last decades have witnessed the influx of facade segmentation methods, there is a lack of comprehensive facade classes and data covering the architectural variability. In ZAHA111Project page: https://github.com/OloOcki/zaha, we introduce Level of Facade Generalization (LoFG), novel hierarchical facade classes designed based on international urban modeling standards, ensuring compatibility with real-world challenging classes and uniform methods’ comparison. Realizing the LoFG, we present to date the largest semantic 3D facade segmentation dataset, providing 601 million annotated points at five and 15 classes of LoFG2 and LoFG3, respectively. Moreover, we analyze the performance of baseline semantic segmentation methods on our introduced LoFG classes and data, complementing it with a discussion on the unresolved challenges for facade segmentation. We firmly believe that ZAHA shall facilitate further development of 3D facade semantic segmentation methods, enabling robust segmentation indispensable in creating urban digital twins.","Figure 1: ZAHA: The dataset comprising 66 facades of various architectural style yielding over 601 million facade-level annotated terrestrial point clouds with distinct 15 facade-relevant classes. Facade semantic segmentation is a fundamental issue in photogrammetry and computer vision [51]. The issue has became compounded thanks to such innovative architects as Zaha Hadid, challenging the standard assumptions of the wall’s planarity, utilizing new materials, and mingling old with new architecture. Throughout the years, various methods have been created for image-based facade segmentation, predominantly facilitated by datasets of annotated facade images [38, 32, 26]. Although the segmentation performance on the ortho-rectified images can reach even 90% [32], the 2D-image nature hinders the immediate understanding of 3D scenes, limiting the 3D facade segmentation capabilities. The striking example are intruded and extruded facade elements, which have been under-explored, owing to inability to capture their structure and depth with 2D ortho-rectified images. These flaws impact other research fields frequently relying on a 3D facade semantic segmentation, such as 3D semantic building reconstruction at a high level of detail [66], finding its applications in simulating flood risk [1] or testing automated driving functions [45], among others [64, 61]. Recent developments have demonstrated that mobile laser scanning (MLS) devices can fill this 3D data gap: Once they are mounted on a mobile platform, they deliver dense, street-level point clouds, enabling capturing a 3D urban environment and, thus, 3D facade geometry (Fig. 1) [68]. This trait has sparked significant growth in urban point cloud benchmark datasets [63, 23] and the development of many semantic segmentation methods [30, 68, 52]. Yet, one of the crucial impediments in developing facade segmentation methods is the facade classes heterogeneity; leading to misinterpretations of facade element characteristics and hampering the development of a large-scale, high-variability data. This overlooked issue also impedes applying semantically segmented facades directly to the 3D semantic building reconstruction tasks since they mismatch taxonomically and geometrically the well-established modeling classes defined in the international standards [66, 17, 68]. Another key issue pertains to the limited annotated facade samples for methods’ training and validation. Only two relatively small datasets comprise facade-level classes: TUM-FAÇADE [63] and ArCH [35], which however either focus on a single university dataset or on only specific cultural heritage buildings, respectively. To tackle the challenges mentioned above, in this paper, we present the following contributions: • Proposing Level of Facade Generalization (LoFG): Novel hierarchical facade classes designed based on international facade-related standards; • Realizing LoFG concept on the introduced, to date, the largest point cloud facade semantic segmentation dataset, boasting 601 million points (Fig. 1); • Identifying outstanding challenges of the 3D facade semantic segmentation based on the extensive experimental and research analysis."
https://arxiv.org/html/2411.04859v1,A multi-purpose automatic editing system based on lecture semantics for remote education,"Remote teaching has become popular recently due to its convenience and safety, especially under extreme circumstances like a pandemic. However, online students usually have a poor experience since the information acquired from the views provided by the broadcast platforms is limited. One potential solution is to show more camera views simultaneously, but it is technically challenging and distracting for the viewers. Therefore, an automatic multi-camera directing/editing system, which aims at selecting the most concerned view at each time instance to guide the attention of online students, is in urgent demand. However, existing systems mostly make simple assumptions and focus on tracking the position of the speaker instead of the real lecture semantics, and therefore have limited capacities to deliver optimal information flow. To this end, this paper proposes an automatic multi-purpose editing system based on the lecture semantics, which can both direct the multiple video streams for real-time broadcasting and edit the optimal video offline for review purposes. Our system directs the views by semantically analyzing the class events while following the professional directing rules, mimicking a human director to capture the regions of interest from the viewpoint of the onsite students. We conduct both qualitative and quantitative analyses to verify the effectiveness of the proposed system and its components.","Mixed-mode or hybrid teaching with both onsite and online students has become a popular teaching practice during the pandemic situation, and it also provides a way to spread knowledge and promote education fairness around the world. Nowadays, students who cannot attend the onsite lectures for various reasons can still participate through video conferencing platforms online or watch the recordings offline. Nonetheless, the information and experiences received by these students are inferior to those received by the onsite students. One reason is that the information conveyed through the views provided by the platform is usually very limited, as shown in Fig.LABEL:fig:zoomview. The students cannot acquire the entire events from different views freely, and staring at the same view for a long time may cause mental stress[1]. On the other hand, if the platforms provide the students with many different views, it is both technically challenging (issues with synchronization, bandwidth, etc.) and the multiple video sources are difficult to browse through, so the remote students still have to manually select the view of interest during the class. Recently, a few automatic lectures recording systems [2, 3, 4, 5] have been proposed. However, these systems focus on automatically adjusting the camera viewpoint to capture the content of interests during the classes, instead of editing multiple video streams together. What the remote student can watch from these systems is either a shot displaying the speaker and his/her surroundings, or a set of raw video streams that require the students to switch manually. It may distract the students and cause information lapses. Therefore, in this paper, we propose an automatic multi-view editing system for lecture videos, which can process more diverse views and automatically edit/switch views based on class semantics. A few similar systems have been proposed by Rui et al. [6, 7] and Wang et al. [8]. Their systems adopted the Finite State Machine (FSM) as the editing model where each state corresponds to a camera view, and use the tracked positions and gestures of the speakers as the primary cues to trigger the state transitions. These systems typically contain four cameras and assume that the speakers can directly interact with the projector screen and the blackboard, so the positions of the hand or body are the regions of interest. Whereas, for the large classroom or reporting hall, as shown in Fig.2 which is an extended scene from previous works and used as an experimental scene by our system, these low-level cues are not always effective in representing the focus since students shift their attention according to the events in class instead of where the teacher is. For example, the attention would focus on the slide view instead of the speaker when the speaker flips the slides through a computer. Moreover, the editing rules are hard-coded in the FSM framework employed, which makes the resultant videos too predictable [9] and limits the capacity of the system to embed new events and new rules. As a result, it will be difficult for the users to adjust the styles of the generated videos according to their preferences. Figure 2: The illustration of our multi-view teaching environment. There are seven video streams, including close-up shots, medium shots, and long shots. Different shots can be used to convey different information.. In this work, we propose a semantics-based automatic editing system with a computational framework. Unlike previous studies [10, 11, 6, 7] that assume the speaker’s positions are the attention regions for the remote students, our system firstly analyzes the semantics of video contents to assess the focus scores of different shots. We observe that the student’s attention is dominated by some special events in class. For example, The students would focus on the content of the blackboard when the teacher is writing something, rather than the teacher himself. As the purposes of different shots vary, different semantics analysis methods are proposed to assess the shots based on their functions, e.g., a writing event recognizer is proposed to assess the blackboard close-up shot, and more details are introduced in Sec.III-B. In addition, we also take general cinematographic rules [12, 13] into consideration to improve the viewing experience. Unlike the previous systems that hard-code editing rules, our system converts the declarative cinematographic rules into computational expressions, as discussed in Sec.III-C. Besides focus assessment, the editing framework also plays an important role. To improve flexibility and optimality, we propose a multi-purpose optimization-based editing framework. Unlike previous studies that select the shots based on a set of predefined rules, this framework integrates the editing rules, e.g, shot duration, as soft constraints, which allows the system breaks the cycle of rigidity if necessary. For example, if the teacher has been writing something for a relatively long time, the system should stay in the close-up view. In contrast, the rigid shot duration constraint in the rule-based system will motivate a switch to a new view. In addition, our system is multi-purpose and allows users to choose the online mode (live broadcast purpose), offline mode (editing purpose), or a balance between them (look-ahead broadcast purpose) by adjusting a look-ahead duration. More details will be discussed in III-D. To summarize, our contributions mainly include the following aspects: 1. Firstly, we propose several practical class semantics analysis methods to assess the attention of shots. To the best of our knowledge, this study is the first attempt to explore video semantics to guide the editing of lecture videos. To evaluate the semantics analysis methods and the proposed editing system, we build a dataset by collecting synchronized multi-view videos from real classes and annotating the writing event. We will make this dataset public to promote the research in this direction. 2. We further develop a multi-purpose optimization-based editing framework, in which the general editing rules are treated as soft constraints to achieve an optimal solution, and the users can choose different modes by simply adjusting the look-ahead duration. 3. Qualitative and quantitative analyses have been conducted on the collected dataset to demonstrate the effectiveness of the proposed system and its components. Moreover, in order to compare the real user experience of different systems, we also conduct a user study to assess our system."
https://arxiv.org/html/2411.04826v1,D3epth: Self-Supervised Depth Estimation with Dynamic Maskin Dynamic Scenes,"Depth estimation is a crucial technology in robotics. Recently, self-supervised depth estimation methods have demonstrated great potential as they can efficiently leverage large amounts of unlabelled real-world data. However, most existing methods are designed under the assumption of static scenes, which hinders their adaptability in dynamic environments. To address this issue, we present D3epth, a novel method for self-supervised depth estimation in dynamic scenes. It tackles the challenge of dynamic objects from two key perspectives. First, within the self-supervised framework, we design a reprojection constraint to identify regions likely to contain dynamic objects, allowing the construction of a dynamic mask that mitigates their impact at the loss level. Second, for multi-frame depth estimation, we introduce a cost volume auto-masking strategy that leverages adjacent frames to identify regions associated with dynamic objects and generate corresponding masks. This provides guidance for subsequent processes. Furthermore, we propose a spectral entropy uncertainty module that incorporates spectral entropy to guide uncertainty estimation during depth fusion, effectively addressing issues arising from cost volume computation in dynamic environments. Extensive experiments on KITTI and Cityscapes datasets demonstrate that the proposed method consistently outperforms existing self-supervised monocular depth estimation baselines. Code is available at https://github.com/Csyunling/D3epth.","I INTRODUCTION Depth information is crucial for various real-world applications, such as autonomous vehicles [1], robot navigation [2], and human-robot interaction [3]. Self-supervised depth estimation [4, 5, 6] has emerged as a promising method due to its ability to predict pixel-level depth maps from images without requiring annotated data. It infers depth from a single target image and supervises learning by constructing a photometric loss through the reprojection of adjacent images back to the target image [7]. Compared to single-frame methods, multi-frame self-supervised depth estimation [8, 9, 10] has garnered more attention for practical applications such as autonomous navigation, as it leverages richer information by inferring depth maps using multiple images during inference. However, both single-frame and multi-frame self-supervised methods are typically built on the assumption of static scenes, which presents challenges when dynamic objects are encountered in real-world environments. This stems from two primary factors: (i) The common framework of self-supervised depth estimation [7] relies on the reprojection loss to capture geometric relationships between consecutive frames under the assumption of photometric consistency. However, this assumption breaks down in the presence of dynamic objects, as shown by the red-circled section in Fig. 1, where a person riding a bike leads to errors during the view synthesis phase, significantly degrading the accuracy of the generated depth maps. To mitigate the impact of dynamic objects, Monodepth2 [11] proposes the Minimum Reprojection Loss. However, this approach is not exhaustive and fails to address the challenging dynamic conditions illustrated in the lower part of Fig. 2. Figure 1: Overall framework of the proposed D3epth. We propose a Dynamic Mask (DM) within the self-supervised framework by masking regions that are likely to be dynamic objects, identified where both reprojection losses exhibit high values. Additionally, we tackle the issue of dynamic objects from the perspective of depth estimation in DepthNet, focusing primarily on multi-frame depth estimation. (ii) In multi-frame depth estimation, the construction of cost volumes does not account for dynamic objects and occlusions, introducing additional errors and making multi-frame depth estimation more susceptible to challenges posed by dynamic scenes. To alleviate this, some studies [8, 10, 9] use teacher-student distillation, where a single-frame depth network guides the multi-frame depth estimation network to correct feature matching errors caused by cost volumes. Other methods involve semantic segmentation to identify dynamic objects and adjust their matching [9, 12] or use optical flow estimation [13] to alleviate the impact of dynamic objects. Unfortunately, these techniques often introduce complex algorithms with increasing inference costs. Figure 2: Comparison of our D3epth with Monodepth2. The upper part of the figure illustrates the typical occlusion scenario addressed by Monodepth2. Both our method and Monodepth2 can handle this situation. However, the lower part of the figure depicts a case where Monodepth2 fails to resolve, as it relies on the minimum of two reprojection losses. In contrast, our method effectively addresses this issue by performing an additional calculation of a Dynamic Mask (DM) to correct the loss. In this paper, we propose D3epth (Self-Supervised Depth Estimation with Dynamic Mask in Dynamic Scenes), a novel method for addressing the challenging problem of dynamic scenes. Specifically, we approach the issue from the general self-supervised framework by utilizing the reprojection loss to identify regions likely affected by dynamic objects, thereby establishing a dynamic mask to reduce the impact of dynamic objects at the loss level, without adding additional inference overhead. As illustrated in Fig. 1, for a set of three consecutive frames, we compute two sets of reprojection losses. If both sets exhibit high losses in certain regions (highlighted by the red-circled area where a person is riding a bike), it suggests that these regions are affected by dynamic objects, enabling us to mask them out. Our method builds upon the Minimum Reprojection Loss proposed in Monodepth2 [11] and offers a more robust solution for handling occlusions and dynamic objects. Monodepth2 addresses occlusions, as illustrated in the upper part of Fig. 2, where one of the adjacent frames is occluded. In this case, taking the minimum of the reprojection losses from two frames helps correct the issue. However, Monodepth2 fails to handle the scenario depicted in the lower part of Fig. 2, where a region in the current frame is occluded by a dynamic object, while the corresponding regions in both the previous and subsequent frames remain unobstructed. Our method effectively addresses this case by identifying dynamic regions and correcting the reprojection loss, whereas Monodepth2 struggles with occlusions caused by dynamic objects due to its reliance on the minimum of two high-loss areas. From the perspective of depth estimation in DepthNet, we focus on multi-frame depth estimation. This approach constructs a cost volume through feature matching to regress depth values, which makes it more susceptible to issues caused by dynamic objects compared to single-frame depth estimation. To address this, we introduce the Cost Volume Auto-Masking strategy, which generates masks for potential dynamic object regions based on adjacent frame images. These masks guide subsequent computations. Furthermore, we integrate spectral entropy after the cost volume to capture richer information for uncertainty estimation and depth fusion, effectively addressing the dynamic challenges introduced by cost volumes. In summary, the contributions of this paper are as follows: • We propose D3epth for self-supervised depth estimation in dynamic scenes. It utilizes the Dynamic Mask to address the problem of dynamic objects by masking regions with high reprojection losses, enhancing results without extra computational cost during inference. • A Cost Volume Auto-Masking strategy is introduced to identify dynamic regions before cost volume construction, forming the basis for subsequent processes. Additionally, a Spectral Entropy Uncertainty module is proposed to use these identified regions to guide uncertainty estimation and improve depth fusion. • Our D3epth achieves state-of-the-art results on the Cityscapes and KITTI datasets. Figure 3: Overall framework of DepthNet in our D3epth. It consists of three main modules: single-frame depth estimation (MonoDepth), multi-frame depth estimation (MultiDepth), and Spectral Entropy Uncertainty (SEU) module. Cost Volume Auto-Masking is applied before computing the cost volume to filter out regions affected by dynamic objects and to guide subsequent processing. And the SEU module leverages the guidance from Cost Volume Auto-Masking and incorporates spectral entropy to enrich the information. This combined approach enhances uncertainty estimation and improves the fusion of MonoDepth and MultiDepth."
https://arxiv.org/html/2411.04821v1,End-to-end Inception-Unet based Generative Adversarial Networks for Snow and Rain Removals,"The superior performance introduced by deep learning approaches in removing atmospheric particles such as snow and rain from a single image; favors their usage over classical ones. However, deep learning-based approaches still suffer from challenges related to the particle appearance characteristics such as size, type, and transparency. Furthermore, due to the unique characteristics of rain and snow particles, single network based deep learning approaches struggle in handling both degradation scenarios simultaneously. In this paper, a global framework that consists of two Generative Adversarial Networks (GANs) is proposed where each handles the removal of each particle individually. The architectures of both desnowing and deraining GANs introduce the integration of a feature extraction phase with the classical U-net generator network which in turn enhances the removal performance in the presence of severe variations in size and appearance. Furthermore, a realistic dataset 111 Inception-UNet-GAN-for-Snow-and-Rain-Removals that contains pairs of snowy images next to their groundtruth images estimated using a low-rank approximation approach; is presented. The experiments show that the proposed desnowing and deraining approaches achieve significant improvements in comparison to the state-of-the-art approaches when tested on both synthetic and realistic datasets.","The Performance of several computer vision applications such as object detection, object tracking, and semantic segmentation is highly dependent on the perceptual as well as the visual quality of the processed images. Therefore, bad weather conditions such as rain, fog, and snow have a negative impact on most image/video processing algorithms due to the image degradations caused by such conditions. Numerous learning-based approaches show better restoration performance in the presence of different weather degradations such as rain (Yang et al.,, 2020) and snow (Liu et al.,, 2018), (Chen et al.,, 2020) when compared to hand-crafted video based ones (Tian et al.,, 2018; Santhaseelan and Asari,, 2015). Due to the difficulty of obtaining pairs of degraded images and their corresponding clean images, the majority of learning-based approaches including GANs are trained, tested and evaluated on datasets that consist of pairs of clean images and their synthetic degraded images. However, synthetic images fail to provide the level of authenticity that realistic images have. Hence, training networks on such synthetic datasets ends up in techniques that perform well on images degraded by synthetic rain masks, while fail drastically when they are applied on realistic images (Li et al.,, 2021). Moreover, the availability of realistic test images allows more effective and accurate evaluation of existing deraining and desnowing approaches. For example, a recent study (Yang et al.,, 2020) that reviewed most of the state-of-the-art deraining approaches shows that only two techniques (out of 27 reviewed) are trained and quantitatively evaluated using realistic datasets. To address this challenge, Wang et al. () introduced a large-scale realistic rain/rain-free dataset for training and evaluating learning-based deraining techniques. Despite the availability of such an efficient dataset, a significant number of deraining approaches (Zhu et al.,, 2020; Chen et al.,, 2014; Ahn et al.,, 2021; Jiang et al., 2020a, ; Huang et al.,, 2021) have not considered this dataset (or other realistic datasets) for training better models or evaluating them quantitatively. On the other hand, the existing desnowing approaches lack the availability of a testing dataset that consists of realistic snow/snow-free pair images that contain a large variety of snowflakes in terms of size and appearance. Most recently, different GANs based approaches have been proposed for single image desnowing and deraining where distinct architectures, descriptors, and loss functions are introduced (Cai et al.,, 2021; Ding et al.,, 2021; Zhang et al.,, 2019). Generally, a single GANs framework consists of a generator network and a discriminator network, which mainly judges the image generated by the generator whether it is real or fake. However, the generator is also guided by several loss functions that provide meaningful feedbacks about the accuracy of its generated images. Multiscale pixel-level loss, structural similarity index (SSIM) loss, refined perceptual loss, and multiscale perceptual loss are good examples of such loss functions that are classically added to the objective function of a GANs framework. However, such loss functions are region blind, where snow/rain-free regions have the same impact as regions with snow/rain on minimizing the generation loss. In other words, the generator is not well guided to focus on the regions that are mainly occupied by snow/rain particles. Therefore, we manage in this paper to tackle this challenge by introducing a spatially guided loss that forces the generator to pay more attention to the snow/rain regions. This is mainly achieved by defining a loss that measures the similarity between the generated and the ground-truth snow/rain maps rather than their pixel-wise images. Due to the unique aspects of snowflakes compared to rain drops in terms of size, shape, and level of transparency, most learning-based approaches that are designed to remove snowflakes cannot simultaneously remove the rain streaks and vice versa. Therefore, developing one model that represents all image degradations caused by both rain and snow is a non-trivial task. Hence, we introduce a deraining/desnowing framework that consists of two degradation removers. The input images are fed into the corresponding network to remove the occurred degradation. In this paper, the proposed desnowing and deraining networks are designed based on the GANs architecture. However, each network has its unique architecture, generator and discriminator networks, and loss function that make them fundamentally different from other existing deraining and desnowing GANs. More specifically, the contributions of the paper are as follows: • A novel architecture that introduces the usage of a feature extraction phase to the classical U-net generator network; is proposed. Such a modification enhances the image generation performance in handling a variety of sizes and appearances of snow and rain particles. • A novel GAN architecture that allows the collaboration between two generators to derain and refine complex rainy images is introduced. • Two novel loss functions that respect the unique characteristics of both snowflakes and rain particles; are designed to spatially guide the GAN generators to focus its removal performance toward corrupted regions. • A realistic test dataset for evaluating desnowing learning approaches is introduced where it is generated based on video-based low-rank approximation and human supervision."
https://arxiv.org/html/2411.04810v1,GANESH: Generalizable NeRF for Lensless Imaging,"Lensless imaging offers a significant opportunity to develop ultra-compact cameras by removing the conventional bulky lens system. However, without a focusing element, the sensor’s output is no longer a direct image but a complex multiplexed scene representation. Traditional methods have attempted to address this challenge by employing learnable inversions and refinement models, but these methods are primarily designed for 2D reconstruction and do not generalize well to 3D reconstruction. We introduce GANESH, a novel framework designed to enable simultaneous refinement and novel view synthesis from multi-view lensless images. Unlike existing methods that require scene-specific training, our approach supports on-the-fly inference without retraining on each scene. Moreover, our framework allows us to tune our model to specific scenes, enhancing the rendering and refinement quality. To facilitate research in this area, we also present the first multi-view lensless dataset, LenslessScenes. Extensive experiments demonstrate that our method outperforms current approaches in reconstruction accuracy and refinement quality. Code and video results are available at https://rakesh-123-cryp.github.io/Rakesh.github.io/","In recent years, mask-based lensless imaging systems have received significant interest due to their potential to offer compact, lightweight, and cost-efficient alternatives to traditional cameras[18]. Rather than utilizing standard optical lenses, these systems rely on amplitude[2] or phase masks[6, 1] placed in close proximity to the sensor. This design not only minimizes the physical dimensions and mass of the imaging device, i.e., a smaller form factor, but also enables the use of non-traditional sensor geometries, such as spherical, cylindrical, or even flexible configurations[28]. In the absence of a conventional focusing element, the measurements captured by the sensor are not straightforward images of the scene. Instead, they consist of intricate, multiplexed data that encode the light information from the scene in a highly compressed and non-intuitive form. This image formation model necessitates advanced computational techniques to decode and reconstruct the original scene, as the sensor no longer produces a one-to-one representation of the visual information but rather a superimposition of light intensities across the entire field of view. Figure 1: Reconstructing 3D scenes from multi-view lensless captures presents significant challenges. To tackle this, we propose GANESH, a novel framework that refines lensless captures while simultaneously rendering novel views. Existing 2D approaches address this task in a sequential, two-step process, resulting in suboptimal 3D reconstruction quality. In contrast, GANESH integrates these two stages into a unified framework, enabling joint optimization for superior novel view synthesis. Numerous studies have investigated the problem of scene reconstruction from single-image lensless captures [18, 3, 12, 25]. For instance, FlatNet [18] employs a two-step process to recover the scene. Initially, a trainable inversion module is utilized to reconstruct most of the scene’s details; however, the resulting output still contains significant noise, which is subsequently addressed by a refinement network. While there are several works in this field of 2D scene reconstruction, there has been little to no work in reconstructing a 3D scene from multi-view lensless captures. This advancement is particularly significant for applications such as endoscopic surgery, where the compact size of lensless cameras offers a substantial advantage. Achieving 3D reconstruction from multi-view lensless images could greatly benefit medical fields and AR/VR applications[17, 13] Recently, NeRFs have amassed a lot of attention for their ability to reconstruct 3D scenes from real-world multi-view captures. Most NeRF-based methods rely on RGB images to model the underlying 3D scene. There have been several works in this space which prompt NeRFs with different image modalities like Thermal Images[33], Event Data[14], Multi-spectral captures[35], Single-Photon Data[15]. For instance, in Thermal NeRF[33], they present an approach for reconstructing novel views exclusively from thermal imagery, particularly beneficial in visually degraded robotics scenarios. Ev-NeRF[14] learns to reconstruct multi-view images from a raw stream of event data captured by a neuromorphic camera, which helps in better reconstruction, especially in high dynamic range scenes. Despite these promising developments, a significant limitation of current NeRF models is their reliance on scene-specific training. One might consider using established refinement techniques such as FlatNet [18] and then feeding the outputs to rendering networks like NeRF or Gaussian Splatting [16]. Though this is a viable option, it comes with its downside. NeRF and Gaussian Splatting operate on the principle of using images solely for supervisory purposes rather than as direct inputs. Such models cannot be trained on paired multi-view lensless and RGB images, resulting in poor novel view synthesis quality. Secondly, while Gaussian Splatting offers improved computational efficiency compared to NeRF, it may be susceptible to overfitting the noisy outputs produced by FlatNet, potentially resulting in suboptimal reconstruction quality. Finally, each model must be re-trained from scratch whenever presented with a new set of images, limiting their scalability and practical application across diverse scenarios. Generalizable Radiance Fields methods have recently gained traction due to their ability to perform on-the-fly inference on new scenes without specific training. Many of these approaches [30, 29, 36] utilize a set of source views and enforce epipolar constraints across them to generate novel target views. The current state-of-the-art method, GNT [29], employs a transformer-based architecture to aggregate epipolar information from multiple views effectively. It then accumulates these point features along each ray to compute the final pixel color, enabling accurate and efficient rendering of new views. However, as previously noted, most Radiance Fields methods have predominantly focused on using RGB images as input, with limited exploration of alternative modalities such as lensless captures. Given the diverse applications of lensless imaging, incorporating this modality into radiance fields presents significant opportunities for expanding their utility across various fields. In this paper, we introduce a novel methodology that enables us to reconstruct scenes from multi-view lensless captures in a generalizable setting. Unlike traditional methods that require scene-specific training for each new dataset, our technique can generalize across various multi-view lensless inputs to render novel views. Our proposed method, GANESH, can effectively reconstruct 3D scenes from lensless data. Our model is trained on extensive data of synthetically generated multi-view lensless images. Despite being trained exclusively on synthetic data, the model can refine and render novel views when applied to real multi-view lensless captures. Experimental results illustrate the model’s effective generalization to both synthetic and real-world scenes. Additionally, our method allows for scene-specific tuning with minimal finetuning steps, enhancing reconstruction quality. We also present LenslessScenes, a dataset of real-world multi-view lensless captures comprising six distinct scenes. These scenes, acquired in a controlled laboratory setting, are accompanied by ground truth data for precise quantitative evaluation. The key contributions of our work are as follows: • We present a novel framework that simultaneously achieves refinement and rendering of lensless captures. • Our approach is generalizable, i.e., it can render views on-the-fly without any need for scene-specific training. • We present LenslessScenes, the first dataset of multi-view lensless captures. • Our experimental results demonstrate that the proposed method outperforms existing techniques that separately handle refinement and novel view synthesis. Figure 2: Overview of GANESH: 1) Given multi-view lensless images of a scene, we first Wiener deconvolve the lensless captures to obtain coarse images. 2) These are then passed onto a deep convolutional network to extract features for every input view. 3) Using the source view features, we estimate the target refined rendered view via an epipolar-based rendering pipeline. 4) By supervising this pipeline end-to-end on paired synthetic data, our model learns to inherently refine the coarse estimated images and simultaneously render novel views eliminating the need for a separate refiner. Our method can directly generalize to any new scene during inference."
https://arxiv.org/html/2411.04746v1,Taming Rectified Flow for Inversion and Editing,"Rectified-flow-based diffusion transformers, such as FLUX and OpenSora, have demonstrated exceptional performance in the field of image and video generation. Despite their robust generative capabilities, these models often suffer from inaccurate inversion, which could further limit their effectiveness in downstream tasks such as image and video editing. To address this issue, we propose RF-Solver, a novel training-free sampler that enhances inversion precision by reducing errors in the process of solving rectified flow ODEs. Specifically, we derive the exact formulation of the rectified flow ODE and perform a high-order Taylor expansion to estimate its nonlinear components, significantly decreasing the approximation error at each timestep. Building upon RF-Solver, we further design RF-Edit, which comprises specialized sub-modules for image and video editing. By sharing self-attention layer features during the editing process, RF-Edit effectively preserves the structural information of the source image or video while achieving high-quality editing results. Our approach is compatible with any pre-trained rectified-flow-based models for image and video tasks, requiring no additional training or optimization. Extensive experiments on text-to-image generation, image & video inversion, and image & video editing demonstrate the robust performance and adaptability of our methods. Code is available at this URL.","Recently, rectified-flow-based generation methods [31] have demonstrated remarkable performance in generating high-quality images and videos. Compared with traditional approaches such as Stable Diffusion [19, 42], their impressive capabilities stem primarily from two aspects. First, rectified flow utilizes a straightforward Ordinary Differential Equation (ODE), which constructs a continuous straight-line motion system to produce the desired data distribution. This simplified training and inference paradigm enables the model to more effectively learn the underlying distribution of real image data. Second, most of the latest rectified flow methods employ the Diffusion Transformer (DiT) [38, 59] architecture as the backbone, which has shown superior performance compared to traditional U-Net [43] architectures. As a result, image generation models like FLUX [1] and video generation models like OpenSora [2], both based on rectified flow and DiT, have respectively become one of the state-of-the-art (SOTA) models in Text-to-Image (T2I) and Text-to-Video (T2V) generation. Apart from fundamental T2I and T2V tasks, other downstream tasks such as reconstruction [46, 35, 16] and editing [17] have been attracting growing interest. These tasks typically depend on performing inversion on the source image/video. The inversion process yields the corresponding representation in the noise space, which is followed by denoising with various conditions provided by users. Given the robust generative capabilities of rectified-flow-based models, they are expected to exhibit superior performance on these downstream tasks. However, their performance remains unsatisfactory compared with traditional methods (based on DDPM and UNets), and research in this area still lags behind. Delving into this problem, we identify that the primary challenge lies in the significant errors during the inversion process of the rectified flow, which fail to accurately reconstruct the original images or videos (Task 1 and Task 2 in Fig. 1). This limitation further constrains its performance on other downstream tasks such as editing. Although [45] attempted to address this challenge through dynamic optimal control, its application is restricted to simple image editing scenarios such as stylization and facial editing, which typically involve simple content and uniform backgrounds. In contrast, both the image contents and editing requirements (such as addition, replacement, and global editing) are considerably more complex in the real world. Moreover, video editing requires highly consistent temporal modeling, presenting even greater challenges for editing algorithms. To the best of our knowledge, there are still no existing methods that effectively tackle these issues. Instead of focusing on designing a specific inversion method, we aim to address the above problem from a more general and fundamental perspective: the sampler. This is because the essence of the inversion and generation process for rectified flow is to employ a sampler that estimates the solution of rectified flow ODE. Consequently, the primary source of inaccurate inversion lies in the approximation error in the solution, which accumulates at each timestep. Intuitively, if the ODE is solved more accurately, the accuracy of inversion can be enhanced subsequently. Based on this insight, we propose the RF-Solver. Specifically, we note that the ODE formulation for rectified flow can be solved directly using the variation of constants method, yielding an exact formulation of the solutions. For the nonlinear component of this solution (i.e., the integral of the neural network), we utilize Taylor expansion for estimation. By employing higher-order Taylor expansion, the ODE can be solved with reduced error, thereby improving the performance of rectified flow models. RF-Solver is a generic sampler that can be seamlessly integrated into any rectified flow model without the need for training or optimization. Experimental results demonstrate that RF-Solver not only significantly enhances the accuracy of inversion and reconstruction, but also improves performance on fundamental tasks such as T2I generation. Building upon this, we propose RF-Edit to apply RF-Solver in editing tasks. Real-world image and video editing require the model to make precise modifications to a source image/video while maintaining its overall structure unchanged. This makes editing a more challenging task compared to reconstruction. In this scenario, it is inadequate to solely rely on the inverted noises as prior knowledge for editing, which could lead to edited results being excessively influenced by the target prompt, resulting in a completely different output compared with source image/video. Addressing this problem, we store the V𝑉Vitalic_V (value) feature in the self-attention layers at several timesteps during inversion. In the process of denoising, these features are used to replace the corresponding features. Practically, we design two specific sub-modules for RF-Edit, respectively leveraging FLUX [1] and OpenSora [2] as the backbone for image and video editing. With the effective design of RF-Edit, it demonstrates superior performance in both image and video domains, outperforming various SOTA methods. Our core contributions are summarized as follows: • We propose RF-Solver, a training-free sampler that significantly reduces errors in the inversion and reconstruction processes of the rectified-flow model. • We present RF-Edit, which leverages RF-Solver for image and video editing, effectively preserving the structural integrity of the source image/video while achieving high-quality results. • Extensive experiments on images and videos demonstrate the efficacy of our methods, showcasing superior performance in both inversion and high-quality editing compared to various existing baselines."
https://arxiv.org/html/2411.04724v1,Controlling Human Shape and Pose in Text-to-Image Diffusion Models via Domain Adaptation,"We present a methodology for conditional control of human shape and pose in pretrained text-to-image diffusion models using a 3D human parametric model (SMPL). Fine-tuning these diffusion models to adhere to new conditions requires large datasets and high-quality annotations, which can be more cost-effectively acquired through synthetic data generation rather than real-world data. However, the domain gap and low scene diversity of synthetic data can compromise the pretrained model’s visual fidelity. We propose a domain-adaptation technique that maintains image quality by isolating synthetically trained conditional information in the classifier-free guidance vector and composing it with another control network to adapt the generated images to the input domain. To achieve SMPL control, we fine-tune a ControlNet-based architecture on the synthetic SURREAL dataset of rendered humans and apply our domain adaptation at generation time. Experiments demonstrate that our model achieves greater shape and pose diversity than the 2d pose-based ControlNet, while maintaining the visual fidelity and improving stability, proving its usefulness for downstream tasks such as human animation. https://ivpg.github.io/humanLDM","The advent of advanced generative diffusion models, particularly Latent Diffusion Models [27], has revolutionized the field of image generation. Models for text-to-image synthesis such as Stable Diffusion [27], DALLE-3 [2] or Gemini [39] have made high quality image synthesis from complex prompts accessible to a broad audience of users worldwide. Text alone, however, is not sufficient in many cases to describe exact scene layouts, subjects or style. Recent techniques for added spatial control have enhanced the capabilities of these models by incorporating various forms of spatial guidance. ControlNet and T2I-Adapter [43, 23], for instance, augment LDMs with localized, task-specific conditions like edges and human poses, allowing for precise control over the generated images. Our work focuses on precise control over humans in generated scenes (Fig. 1). While 2D pose conditions [43, 23] allow specifying constraints on human poses, body shapes are not controllable, and specified poses might suffer from 3D ambiguities, complicating precise human-centric illustration and animation. To address this, we aim to control body pose and shape using the commonly used 3D human parametric model, SMPL [19]. Our approach modifies the ControlNet [43] architecture to use SMPL parameter embeddings in the cross-attention blocks that typically attend to text condition embeddings. This modification enables the model to control global image content based on SMPL guidance, seamlessly integrating human geometry information into the generation process (Fig. 1a). However, training such control networks for large LDMs like Stable Diffusion (SD) [27] typically requires an annotated training dataset with a large and diverse set of high-quality images to retain the output fidelity of the original network. Most existing real-world datasets for 3D annotations, including SMPL parameters, do not meet these criteria, particularly regarding scale and diversity in scenes and body shapes. Synthetic datasets such as SURREAL [41] provide images with diverse body shapes and scenes at scale, and offer further advantages such as high-quality annotations and lower regulatory barriers. However, the visual fidelity of synthetic datasets is typically degraded. For instance, SURREAL [41] suffers from non-photorealistic shading (Fig. 1b), inadequate blending between subjects and background, as well as low image resolution, leading to a visual domain shift in SD networks fine-tuned on it. Our goal is to extract the human shape and pose information while discarding the visual aesthetics from such datasets. While traditional domain adaptation techniques [14, 1] aim to translate synthetic data to real-world appearance , adapting synthetic datasets to the full visual range of outputs of large text-to-image LDMs representing diverse and complex scenes like SD is challenging. We introduce a technique for domain adaptation in the latent space of LDMs to extract the control condition of a synthentically-trained LDM control network and apply it in the visual domain of the original model using classifier-free guidance. For human body shape and pose control, we obtain a SMPL-conditioned vector from our synthetically trained ControlNet, which is then composed with outputs of another control network for visual domain guidance, effectively adapting the isolated SMPL-condition to the visual appearance and fidelity of the original SD model. Our results demonstrate significantly improved adherence to shape and pose compared to current control approaches [43, 23], while simultaneously retaining the visual fidelity (in terms of KID and Inception Score) of SD. In summary our contributions are as follows: • We introduce a classifier-free guidance-based technique for domain adaptation in LDMs to adapt the visual domain of control models trained on synthetic data to the original high fidelity domain . • We propose a SMPL-based control model for shape and pose control trained on the synthetic SURREAL [41] dataset. • We demonstrate the efficacy of our approach in terms of visual fidelity, SMPL accuracy, and comparisons against state-of-the-art methods and ablated configurations."
https://arxiv.org/html/2411.04715v1,NeuroFly: A framework for whole-brain single neuron reconstruction,"Neurons, with their elongated, tree-like dendritic and axonal structures, enable efficient signal integration and long-range communication across brain regions. By reconstructing individual neurons’ morphology, we can gain valuable insights into brain connectivity, revealing the structure basis of cognition, movement, and perception. Despite the accumulation of extensive 3D microscopic imaging data, progress has been considerably hindered by the absence of automated tools to streamline this process. Here we introduce NeuroFly, a validated framework for large-scale automatic single neuron reconstruction. This framework breaks down the process into three distinct stages: segmentation, connection, and proofreading. In the segmentation stage, we perform automatic segmentation followed by skeletonization to generate over-segmented neuronal fragments without branches. During the connection stage, we use a 3D image-based path following approach to extend each fragment and connect it with other fragments of the same neuron. Finally, human annotators are required only to proofread the few unresolved positions. The first two stages of our process are clearly defined computer vision problems, and we have trained robust baseline models to solve them. We validated NeuroFly’s efficiency using in-house datasets that include a variety of challenging scenarios, such as dense arborizations, weak axons, images with contamination. We will release the datasets along with a suite of visualization and annotation tools for better reproducibility. Our goal is to foster collaboration among researchers to address the neuron reconstruction challenge, ultimately accelerating advancements in neuroscience research. The dataset and code are available at https://github.com/beanli161514/neurofly.","Figure 1: A mouse neuron reconstructed from fluorescence microscope images at a resolution of 1⁢μ⁢m/voxel1𝜇𝑚voxel1~{}\mu m/\text{voxel}1 italic_μ italic_m / voxel. (a) Image of a whole mouse brain and a reconstructed neuron. The color of the neuron segments indicates the structural distance, defined as the number of branch points encountered along the shortest path to the soma. The image size is 14000×10000×14000⁢μ⁢m3140001000014000𝜇superscript𝑚314000\times 10000\times 14000~{}\mu m^{3}14000 × 10000 × 14000 italic_μ italic_m start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT. (b) Arborization of the axon at the surface of the brain cortex, with an image size of 512×512×512⁢μ⁢m3512512512𝜇superscript𝑚3512\times 512\times 512~{}\mu m^{3}512 × 512 × 512 italic_μ italic_m start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT. (c) Soma and dendrites of the neuron, with an image size of 384×384×384⁢μ⁢m3384384384𝜇superscript𝑚3384\times 384\times 384~{}\mu m^{3}384 × 384 × 384 italic_μ italic_m start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT. Structural connectivity between neurons underpins communication across brain regions, forming the foundation for brain functions such as sentience and intelligence. Mesoscale imaging techniques [31, 34, 11] provide a powerful tool for studying the connections between different neuronal types across various brain regions [32]. Reconstructing neuron morphology from mesoscale brain images, particularly axonal projection, is crucial for building mesoscale connectome. Fig.1 (a) shows a mouse brain imaged at mesoscopic resolution. The neuron’s axon begins at the soma as shown in Fig.1 (c), extends across thousands of voxels and spans to several brain regions. Manual annotation of such a mouse neuron without any automatic algorithm takes about 20 hours according to Gou et al. [12]. Constructing a comprehensive mesoscale connectome for a species requires the reconstruction of tens of thousands of long-projection neurons, a task that is nearly impossible without advanced automated approaches. Extracting neuron structures from the terabyte-scale images poses significant demands on the development of automated algorithms. The DIADEM challenge [10, 4] was first launched in 2011 to benchmark and advance neuron reconstruction algorithms, later the BigNeuron project [23] introduced standardized data protocols and evaluation methods to this field. As a result of these effort, many neuron reconstruction methods [5, 6, 30, 17, 21] were published in the following years. However, recent benchmark [20] reveals that these neuron reconstruction algorithms struggle with complex scenarios not represented in existing datasets. It is obvious that a comprehensive, high-quality, and large-scale dataset is equally important as algorithms for advancing the construction of mesoscopic connectomes. Yet, annotating gold-standard datasets for developing and evaluating algorithms is as labor-intensive as labeling single neurons at the whole-brain scale. Therefore, a efficient data labeling approach is urgently needed. In recent years, a variety of deep-learning based methods have been proposed for neuron reconstruction. Liu et al. [18] trained a 3D segmentation network to improve the performance of traditional neuron reconstruction algorithms. Chen et al. [5, 6] combined spherical patches extration with a 2D CNN to predict the types and tracing directions of 3D critical points, significantly reducing computational cost. To avoid the high cost in obtaining manual annotations, Zhao et al. [33] proposed a progressive learning scheme for neuron segmentation which learns from the coarse labels produced by traditional neuron tracing algorithms. However, whole-brain single neuron reconstruction was not solved end-to-end due to its complexity and low error-tolerance. To make deep-learng based methods more feasible in this field, a series of well-defined tasks with associated datasets is necessary. The contributions of our paper are summarized as follows: 1. We introduce an efficient whole-brain neuron reconstruction framework, named NeuroFly, formulating whole-brain single neuron reconstruction as a streamlined process involving segmentation, connection, and proofreading. The segmentation and connection tasks are explicitly defined and supported by corresponding datasets. 2. We gather and label a variety of data of different species imaged using different techniques, constructing a diverse dataset covering a wide range of scenarios and varying scales up to whole mouse brain. The dataset is easily extendable benefiting from its succinct data protocol and the efficiency of NeuroFly framework. 3. We propose a 3D image-based path following method as part of the neuron reconstruction pipeline, addressing the gaps between neuron segments that were not correctly captured in the segmentation stage. This method serves as the baseline for the connection stage in NeuroFly framework."
https://arxiv.org/html/2411.04714v1,Revisiting Disparity from Dual-Pixel Images:Physics-Informed Lightweight Depth Estimation,"In this study, we propose a high-performance disparity (depth) estimation method using dual-pixel (DP) images with few parameters. Conventional end-to-end deep-learning methods have many parameters but do not fully exploit disparity constraints, which limits their performance. Therefore, we propose a lightweight disparity estimation method based on a completion-based network that explicitly constrains disparity and learns the physical and systemic disparity properties of DP. By modeling the DP-specific disparity error parametrically and using it for sampling during training, the network acquires the unique properties of DP and enhances robustness. This learning also allows us to use a common RGB-D dataset for training without a DP dataset, which is labor-intensive to acquire. Furthermore, we propose a non-learning-based refinement framework that efficiently handles inherent disparity expansion errors by appropriately refining the confidence map of the network output. As a result, the proposed method achieved state-of-the-art results while reducing the overall system size to 1/5151/51 / 5 of that of the conventional method, even without using the DP dataset for training, thereby demonstrating its effectiveness. The code and dataset are available on our project site.","Dual-pixel (DP) sensors [33, 12, 36, 51, 21] have traditionally been used for camera autofocusing [19, 10]. This sensor splits each pixel into two photodiodes, providing two views of the same scene, as shown in Fig. 2 (b). The DP view has a disparity that correlates with the amount of defocus blur. Therefore, moving the lens position to minimize the disparity allows the scene to come into focus quickly. Similar to stereo cameras [4], the disparity can also be converted into depth, and the ease with which a depth map can be acquired without active-light illumination has led to its use in applications such as depth-of-field extension [2, 47, 1, 50] and segmentation [44]. Because of their affinity for these applications, DP sensors are utilized in mobile cameras, such as Google Pixel, which require lightweight systems. Many techniques have been proposed for DP sensor disparity estimation; however, most of them use end-to-end estimation with deep learning, as shown at the top left of Fig. 1 (a). Although this method is simple and direct, it does not fully exploit the disparity constraint, which is highly reliable for disparity near textures. This unnecessarily increases the network size and limits performance; specifically, it is prone to disparity artifacts correlated with RGB textures, as shown in Fig. 9 (c). As a naïve solution, traditional template matching (TM) [7, 30] can explicitly calculate disparity only in regions of high reliability. A lightweight network optimized for completion can complement the disparity, thereby minimizing the network size and suppressing artifacts. However, the prominent anisotropic blurring property of DP outside the depth of field causes TM errors, which are further propagated by completion, resulting in degraded results. There is also the problem of disparity errors near the edges owing to the combination of lightweight completion networks, and the expansion of disparity in principle owing to TM, which causes the side effects of intermediate value disparity even when simple filter-based refinement processes are applied. Therefore, we propose a physics-informed lightweight disparity estimation method that explicitly exploits the disparity property of DP, as shown at the bottom of Fig. 1 (a). Specifically, we propose an approach that improves accuracy by modeling the TM errors arising from the physical and systemic properties of DP and applying appropriate sampling during training. The physics DP simulator simulates varying optical conditions and fits parametric functions by using symbolic regression to efficiently model the TM errors. Furthermore, we propose a framework that effectively compensates for disparity expansion errors that occur in principle [15, 16] by carefully refining the network’s output confidence map based on a filtering process. The proposed non-learning-based disparity refinement framework is generally applicable and can be applied to the estimation results of other models. In addition, a DP dataset is required to train the network; however, no dataset of DP images containing the actual disparity value exists to date [31]. Therefore, conventional methods use depth as a reference and learn by devising a loss function to absorb the differences in the domain. However, the network performance is limited by differences in the domain, and obtaining a large DP dataset with the ground truth of depth requires time and workforce, which are also limited in number and scale. By converting the depth of widely available monocular RGBD datasets to DP disparity, we enabled large-scale training in the same domain without needing large amounts of DP data. The performance of the proposed method exceeds that of the state-of-the-art method by less than 1/5151/51 / 5 of the parameters, demonstrating its superiority (Fig. 1(b)). In summary, our contributions of this study are as follows: • A practical method for estimating disparity (depth) from DP images in a lightweight and high-performance manner by modeling and appropriately constraining physics and systemic disparity properties. (Sec. 4.1, 4.2) • A non-learning-based refinement framework that efficiently handles disparity expansion errors by appropriately refining the confidence map that network output (Sec. 4.3). • An approach for learning disparity completion from common monocular RGBD data (Sec. 4.2)."
https://arxiv.org/html/2411.04713v1,Multi-Reward as Condition for Instruction-based Image Editing,"High-quality training triplets (instruction, original image, edited image) are essential for instruction-based image editing. Predominant training datasets (e.g., InsPix2Pix) are created using text-to-image generative models (e.g., Stable Diffusion, DALL-E) which are not trained for image editing. Accordingly, these datasets suffer from inaccurate instruction following, poor detail preserving, and generation artifacts. In this paper, we propose to address the training data quality issue with multi-perspective reward data instead of refining the ground-truth image quality. 1) we first design a quantitative metric system based on best-in-class LVLM (Large Vision Language Model), i.e., GPT-4o in our case, to evaluate the generation quality from 3 perspectives, namely, instruction following, detail preserving, and generation quality. For each perspective, we collected quantitative score in 0∼5similar-to050\sim 50 ∼ 5 and text descriptive feedback on the specific failure points in ground-truth edited images, resulting in a high-quality editing reward dataset, i.e., RewardEdit20K. 2) We further proposed a novel training framework to seamlessly integrate the metric output, regarded as multi-reward, into editing models to learn from the imperfect training triplets. During training, the reward scores and text descriptions are encoded as embeddings and fed into both the latent space and the U-Net of the editing models as auxiliary conditions. During inference, we set these additional conditions to the highest score with no text description for failure points, to aim at the best generation outcome. 3) We also build a challenging evaluation benchmark with real-world images/photos and diverse editing instructions, named as Real-Edit. Experiments indicate that our multi-reward conditioned model outperforms its no-reward counterpart on two popular editing pipelines, i.e., InsPix2Pix and SmartEdit. The code and dataset will be released.","Text instruction-based image editing provides a natural way for general users to express their requests and customize their assets easily. Predominant state-of-the-art methods for instruction-based image editing [5, 32, 33, 12] follow a data-driven pipeline to finetune pre-trained diffusion models [27] with editing data triplets, i.e., (instruction, original image, edited image). Creating a high-quality dataset of the above triplets is thus essential for successful model training. Figure 1: Existing image editing datasets and our method. Best viewed with zoom-in. Predominant state-of-the-art methods for instruction-based image editing [5, 32, 33, 12, 11] follow a data-driven pipeline to create the editing triplets, from which they build a dataset to fine-tune a pre-trained diffusion model [27]. The most widely used InsPix2Pix [5] dataset is created with a pre-trained text-to-image Stable Diffusion (SD) model [27], Prompt-to-Prompt [10] and a fine-tuned GPT-3 [6]. The dataset can easily scale up to 300k triplets but the quality is unsatisfactory from three perspectives, i.e., instruction following, detail preserving, and generation quality. 1) Instruction following means that the model needs to closely and accurately follow the editing request, which we regard as the most important factor in instruction-based image editing. Since the SD model was originally trained for image generation tasks, it might fail to apply the correct editing action to the edited image. As shown in Fig. 1 (a), the text instruction is “make the glasses green” but the glasses in the ground-truth edited image are not green, which does not follow the major editing instruction. 2) Detail Preserving indicates how the model preserves identity, background or any other details that are not meant to be changed in the editing instruction. InsPix2Pix adopts prompt-to-prompt to generate edited images which could contain undesired modifications on the edited images. For example, the instruction of the first case in Fig. 1 (a) is to edit the color of the glasses, but the color of the clothes and background is also changed in the ground-truth edited image, which could lead to wrong supervision. 3) Generation Quality represents the relative quality of edited images compared to the input images, i.e., to determine whether the editing action introduces quality degradation like artifacts to the real-world input images. It is common for SD models to generate artifacts, especially for images with human or small objects. In the third case of Fig. 1 (a), the generated “giant squid” in the ground-truth image has serious artifacts (viewed with zoom-in). MagicBrush [32] leverages a more powerful text-to-image model (i.e., DALL-E 2 [25]) and human workers to improve the training data quality on a relatively small scale. The background preserving is significantly improved due to mask-based editing. However, for the edited regions inside the mask, the edited image may contain undesired modification or generation artifacts due to occlusion or small objects (see example 5,6 in Fig. 1). HQEdit [13] adopts GPT-4V [3] and DALL-E 3 [1] to improve the instruction and generation quality. However, the edited images are usually significantly modified on the regions that are not included in the editing instruction, leading to poor detail preserving on background or identity (see example 7,8 in Fig. 1). Hive [33] follows the same procedure of InsPix2Pix to create training data triplets, thus having a similar quality as InsPix2Pix. A relatively small-scale human feedback dataset is collected to improve the overall quality of the editing model, but it does not have detailed feedback information for the three perspectives of editing (i.e., following, preserving, and quality). In a nutshell, the majority of training samples in existing datasets remain noisy which could lead to inaccurate supervision. In this paper, we propose to rectify the inaccurate supervision from a different perspective, i.e., introducing multi-perspective reward as an auxiliary input condition. 1) Instead of directly refining the quality of ground-truth edited images, we evaluate the training data triplets from three perspectives (i.e., instruction following, detail preserving, generation quality) with GPT-4o [2] to generate scores on a of 0∼5similar-to050\sim 50 ∼ 5 and text description for unsatisfactory points. With proper prompt engineering, the generated reward/feedback is mostly aligned with humans. We collect 20k multi-perspective reward data in total for training, namely RewardEdit-20K. Examples of the scores and text description reward are included in Fig. 1 (b). 2) To integrate reward information into the existing instruction-based image editing framework, we first encode the reward score and reward text description separately as embeddings, and then concatenate them to obtain the reward condition. This reward condition is then integrated into the latent noise through an attention mechanism. To further enhance the guidance provided by the reward information, we also feed the reward condition into the U-Net [28] of the SD model. 3) To evaluate the editing models on real-world photos and diverse instructions covering major 7 categories (defined in Sec. 5), we create an evaluation set with 80 high-quality Unsplash [4] photos and 560 challenging instructions, which are initially generated by GPT-4o and verified by human annotators. We evaluate the model output from the three perspectives with GPT-4o in terms of yes/no accuracy and score from 0∼5similar-to050\sim 50 ∼ 5. We also conduct a human evaluation with 0∼5similar-to050\sim 50 ∼ 5 score from three perspectives to further verify the results. Experiments show that the proposed method can be combined with InsPix2Pix and SmartEdit with significant performance improvement. We summarize the contributions as follows: ♠ The RewardEdit-20K dataset with multi-perspective reward data to address the limitations of existing image editing datasets. ♥ A novel framework to effectively integrate multi-perspective reward information as an additional condition to guide image editing. ♠ A real-world image editing evaluation benchmark Real-Edit and introduced a GPT-4o-based image editing evaluation method. ♣ Extensive experiments showing that the proposed method can be combined with existing editing models with a significant performance boost on all three perspectives, achieving state-of-the-art performance for both GPT-4o and human evaluation."
https://arxiv.org/html/2411.04712v1,SEE-DPO: Self Entropy Enhanced Direct Preference Optimization,"Direct Preference Optimization (DPO) has been successfully used to align large language models (LLMs) according to human preferences, and more recently it has also been applied to improving the quality of text-to-image diffusion models. However, DPO-based methods such as SPO, Diffusion-DPO, and D3PO are highly susceptible to overfitting and reward hacking, especially when the generative model is optimized to fit out-of-distribution during prolonged training. To overcome these challenges and stabilize the training of diffusion models, we introduce a self-entropy regularization mechanism in reinforcement learning from human feedback. This enhancement improves DPO training by encouraging broader exploration and greater robustness. Our regularization technique effectively mitigates reward hacking, leading to improved stability and enhanced image quality across the latent space. Extensive experiments demonstrate that integrating human feedback with self-entropy regularization can significantly boost image diversity and specificity, achieving state-of-the-art results on key image generation metrics.","Recent advancements in text-to-image generation models have achieved remarkable success [27, 25, 28, 29], enabling the creation of high-quality images that are both visually striking and semantically coherent. By leveraging large-scale image datasets, these models have demonstrated an unprecedented ability to generate images that align closely with the intended descriptions. This progress has garnered significant attention, highlighting the vast potential for practical applications in fields such as art, design, and content generation, while also raising important questions regarding the broader implications of such technology. [28, 29, 10] Text-to-image generation models are typically pretrained on vast datasets, which often contain low-quality contents, leading to the introduction of various deficiencies in the trained models [27]. To mitigate these issues and improve the quality of generated outputs, additional post-training steps are usually required. Common approaches for post-training include Supervised Fine-Tuning (SFT) [6] and Reinforcement Learning with Human Feedback (RLHF) [9]. RLHF has gained popularity in recent years due to its ability to improve generative model’s ability using self generated data [3]. RLHF has been highly successful in aligning LLMs [26, 7, 24], and more recently it has demonstrated great potential for enhancing the generation quality of diffusion models [39, 34, 4, 11, 22]. Standard RLHF algorithms rely on a reward model to evaluate output images and a learning algorithm, such as Proximal Policy Optimization (PPO) [30] or REINFORCE [41], to adjust the generative model based on these rewards. Reward models are often trained on human preference data [18, 36]. More recently, it was shown that Direct Preference Optimization (DPO) can be used to train large language model directly on preference data without reward modeling. The algorithm became popular because it yields competitive performance and is more stable to train than PPO, and has since been applied to diffusion models as well [34, 39, 22]. Methods like D3PO [39], SPO [22] and Diffusion-DPO [34] adapt the DPO objective to train diffusion models. Some more recent studies showed that for large language models, online iterative DPO, plus reward model learning, is superior to DPO [40, 37], and similar techniques can be applied to diffusion models as well. Online DPO algorithms consist of two key steps: sample generation and model training. At each timestep, the current model undergoes fine-tuning on the available dataset using DPO. Following this, new samples are generated from the updated model. These samples are subsequently evaluated and ranked by an existing reward model. The newly scored data is then appended to the existing dataset, ensuring continuous improvement and adaptation of the model.This is the approach we will employ in this paper. However, for large language models, when we use online iterative DPO for multiple iterations, the generation model deteriorates significantly due to significant shifts from the original distribution which leads to artificially high rewards and mode collapse. This phenomenon, referred to as reward hacking [38], is a challenging problem which we have also observed in diffusion models. [Fig: 1] A commonly used method to mitigate reward hacking is to include an KL regularization term in the objective function that enforces the trained language model to be close to the pretrained model (reference distribution) [9]. We observe that this regularization, while alleviating the reward hacking problem, is insufficient to prevent it for diffusion models. We identify a key reason of this to be that pretrained diffusion model’s generation is not diverse enough, and thus after RLHF, the trained model will have further reduced diversity. To address this problem, we propose to incorporate an additional self-entropy regularization to improve diversity. Mathematically, we demonstrate that this approach flattens the reference distribution in the KL-divergence term. Therefore the regularization condition encourages the model to be more exploratory, effectively reducing overfitting and mitigating reward hacking. We also show that the strength of this regularization requires minimal hyperparameter tuning, maintaining strong performance across a broad range of values. Moreover, our method achieves state-of-the-art performance across various metrics. Finally, we qualitatively show that our approach generates more diverse and aesthetically superior images compared to existing methods, which tend to produce high-quality results only for a limited subset of the input latent space for a given prompt (that is, lacks diversity). Our main contributions are as follows: 1. We introduce a novel self-entropy regularization term into the standard KL-regularized formulation of RLHF objective function. It is shown that this additional term can effectively improve RLHF results for diffusion models. 2. Our method is effective in mitigating reward hacking in DPO-based algorithms for diffusion models. When combined with the prior state-of-the-art method D3PO, Diffusion-DPO and SPO we are able to obtain new state-of-the-art results on various image quality metrics. 3. We carry out empirical studies to demonstrate that this regularization technique encourages broader exploration of the solution space, reducing overfitting and preventing reward hacking. Moreover, we achieve enhanced stability and image quality across the latent space of the diffusion model, leading to more consistent outputs."
https://arxiv.org/html/2411.04711v1,Progressive Multi-Level Alignments for Semi-Supervised Domain Adaptation SAR Target Recognition Using Simulated Data,"Recently, an intriguing research trend for automatic target recognition (ATR) from synthetic aperture radar (SAR) imagery has arisen: using simulated data to train ATR models is a feasible solution to the issue of inadequate measured data. To close the domain gap that exists between the real and simulated data, the unsupervised domain adaptation (UDA) techniques are frequently exploited to construct ATR models. However, for UDA, the target domain lacks labeled data to direct the model training, posing a great challenge to ATR performance. To address the above problem, a semi-supervised domain adaptation (SSDA) framework has been proposed adopting progressive multi-level alignments for simulated data-aided SAR ATR. First, a progressive wavelet transform data augmentation (PWTDA) is presented by analyzing the discrepancies of wavelet decomposition sub-bands of two domain images, obtaining the domain-level alignment. Specifically, the domain gap is narrowed by mixing the wavelet transform high-frequency sub-band components. Second, we develop an asymptotic instance-prototype alignment (AIPA) strategy to push the source domain instances close to the corresponding target prototypes, aiming to achieve category-level alignment. Moreover, the consistency alignment is implemented by excavating the strong-weak augmentation consistency of both individual samples and the multi-sample relationship, enhancing the generalization capability of the model. Extensive experiments on the Synthetic and Measured Paired Labeled Experiment (SAMPLE) dataset, indicate that our approach obtains recognition accuracies of 99.63% and 98.91% in two common experimental settings with only one labeled sample per class of the target domain, outperforming the most advanced SSDA techniques.","Compared with optical imaging systems, synthetic aperture radar (SAR) can capture images with high resolutions in any weather condition. As a result, SAR has been applied extensively in both the military and private sectors. And, it has demonstrated great potential in the fields of terrain survey, environmental monitoring, and battlefield reconnaissance. Automatic target recognition (ATR) occupies a crucial position in SAR image processing and application, being significant for the fields of military intelligence. The emergence of deep learning (DL) has brought remarkable progress in SAR ATR [1, 2, 3], since DL can effectively extract high-level semantic features in contrast to traditional hand-craft features. There are several DL-based SAR ATR methods [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], all of which achieve outstanding recognition performance. However, a successful DL model needs a vast amount of labeled samples for training. Nevertheless, since SAR targets are usually noncooperative, it is challenging to gather sufficient labeled data. Furthermore, the acquisition of experimental target data by real flight experiments is usually characterized by high economic cost and time-consuming, which further increases the difficulty of obtaining labeled data [14, 15]. Therefore, how to tackle the problem of limited training samples faced by SAR ATR is one of the main challenges in current research. 1.1 Background In recent years, a compelling research trend for SAR ATR has emerged, that is, the utilization of simulated SAR data as a substitute for training DL models is a solution to the problem of insufficient measured data. However, despite the theoretical attractiveness of this solution, modeling inaccuracies in simulation generally lead to a significant domain gap between the simulated and measured data [16, 17]. The gap may lead to severe performance degradation in real-world applications, making it difficult to apply DL-based ATR models trained on simulated data to measured data. Consequently, there is an urgent need for the solution to bridge the domain gap between the simulated and measured data. To address this problem, a number of researchers have looked into techniques based on unsupervised domain adaptation (UDA) [17, 18, 19, 20, 21]. In the absence of labeled data on the target domain, UDA aims to efficiently align distributions between the simulation and measurement domains. By doing this, a more accurate and dependable ATR model may be trained merely using the simulated data. For example, Shi et al. [17] proposed a UDA method for simulated data-aided SAR ATR, by carrying out alignments across domain-level and category-level granularities. In UDA, no data are labeled within the target domain, posing a great challenge to ATR models. In contrast, semi-supervised domain adaptation (SSDA) can excavate a minority of labeled samples in the target domain to guide cross-domain alignment, being superior than UDA [22]. For instance, natural optical image classification under SSDA learning fashion can gain great accuracies with only one labeled sample per class in the target domain [23, 22]. It is noteworthy that the current SSDA methods are built on real optical datasets, whereas the properties of SAR images differ greatly from those of optical images. Thus, domain adaptation strategies specific to SAR images should be considered [17, 18, 24]. In particular, the domain shift is unique between the real and simulated SAR images. This is because the simulated SAR images are usually generated by physics-based electromagnetic (EM) modeling. The EM modeling is based on a fine CAD model of the observed target, followed by an image formation algorithm with the same radar parameters as the measured data. Hence, despite the domain shift existing between the real and simulated SAR data, there is abundant priori information in the former closely related to the latter. And, these two domains differ from general SAR image domains in terms of domain discrepancies. Therefore, it is essential to consider an SSDA method suitable for the simulated data-aided SAR ATR. 1.2 Solution and Main Contributions To address the above challenges, we propose an SSDA approach based on progressive multi-level alignments. In the proposed framework, a novel data augmentation technique termed as the progressive wavelet transform data augmentation (PWTDA), is developed to carry out the domain-level alignment, based on deeply analyzing the image characteristic discrepancies between the simulation and measurement domains. In order to implement cross-domain category-level alignment, we design the asymptotic instance-prototype alignment (AIPA) strategy, by gradually facilitating the source domain samples close to the corresponding category prototypes of the target domain. Furthermore, the consistency alignment is introduced as another categorical alignment only in the target domain to increase the robustness and generalization of the entire model, according to the consistency regularization principle of semi-supervised learning. The proposed approach can effectively boost the accuracy of simulation data-assisted SAR ATR in the case of incredibly rare measured samples with labels. The main contributions of the proposed SSDA approach for simulated data-assisted SAR ATR can be summarized as follows. 1) We present the PWTDA to achieve domain-level alignment, by blending the wavelet transform high-frequency sub-band components of images from the simulation and measurement domains. Specifically, the PWTDA is implemented in a progressive manner enhancing the diversity of the augmented samples and narrowing the domain gap gradually, which is conducted via adding reliable pseudo-labeled samples of the target domain batch-by-batch. 2) The AIPA is developed to force the samples of the simulation domain to approximate the associated category prototypes of the measurement domain, being beneficial for the category-level alignment. It is worth mentioned that the AIPA is also in a gradual manner, due to the fact that the class prototypes of the measurement domain are updated progressively. On the basis of the AIPA, we developed an asymptotic prototype alignment loss. 1.3 Organization The following are the additional portions of this paper. The associated work is briefly described in Section 2. We describe the specifics of our suggested approach in Section 3. The experimental results and the related analysis are displayed in Section 4. The paper is discussed in Section 5. Finally, the paper is summarized in Section 6."
https://arxiv.org/html/2411.04709v1,TIP-I2V: A Million-Scale Real Text and Image Prompt Datasetfor Image-to-Video Generation,"Video generation models are revolutionizing content creation, with image-to-video models drawing increasing attention due to their enhanced controllability, visual consistency, and practical applications. However, despite their popularity, these models rely on user-provided text and image prompts, and there is currently no dedicated dataset for studying these prompts. In this paper, we introduce TIP-I2V, the first large-scale dataset of over 1.701.701.701.70 million unique user-provided Text and Image Prompts specifically for Image-to-Video generation. Additionally, we provide the corresponding generated videos from five state-of-the-art image-to-video models. We begin by outlining the time-consuming and costly process of curating this large-scale dataset. Next, we compare TIP-I2V to two popular prompt datasets, VidProM (text-to-video) and DiffusionDB (text-to-image), highlighting differences in both basic and semantic information. This dataset enables advancements in image-to-video research. For instance, to develop better models, researchers can use the prompts in TIP-I2V to analyze user preferences and evaluate the multi-dimensional performance of their trained models; and to enhance model safety, they may focus on addressing the misinformation issue caused by image-to-video models. The new research inspired by TIP-I2V and the differences with existing datasets emphasize the importance of a specialized image-to-video prompt dataset. The project is publicly available at https://tip-i2v.github.io.","Image-to-video diffusion models transform static images into dynamic videos, with wide-ranging applications in animation, content creation, and visual storytelling [39, 67, 21, 72, 71]. As video generation becomes more commercialized, image-to-video methods are increasingly preferred over text-to-video for several reasons: (1) they offer users more control, allowing for precise direction of objects to perform specific actions; (2) they provide greater consistency, enabling narratives focused on a central subject; and (3) they are more practical, particularly on social media where image-driven videos tend to gain higher engagement. Despite their popularity and importance, there currently lacks a dataset from the user’s perspective – one that features user-provided text and image prompts alongside the corresponding generated videos. Such a dataset could help improve the alignment of image-to-video models with real-world user needs, while also enhancing safety. Therefore, this paper conducts the first study of its kind in the image-to-video community. Specifically, we focus on curating the first image-to-video prompt-gallery dataset, analyzing the differences between the proposed dataset and similar ones, and exploring the new research directions inspired by us. The first Text and Image Prompt dataset for Image-to-Video generation (TIP-I2V). As shown in Fig. 1, our TIP-I2V dataset includes over 1.701.701.701.70 million unique user-provided text and image prompts for image-to-video diffusion models, along with the corresponding generated videos, sourced from Pika Discord channels [5]. It is important to note that: (1) We intend to include videos generated by other state-of-the-art image-to-video diffusion models, including 𝚂𝚝𝚊𝚋𝚕𝚎𝚂𝚝𝚊𝚋𝚕𝚎\mathtt{Stable}typewriter_Stable 𝚅𝚒𝚍𝚎𝚘𝚅𝚒𝚍𝚎𝚘\mathtt{Video}typewriter_Video 𝙳𝚒𝚏𝚏𝚞𝚜𝚒𝚘𝚗𝙳𝚒𝚏𝚏𝚞𝚜𝚒𝚘𝚗\mathtt{Diffusion}typewriter_Diffusion [8], 𝙾𝚙𝚎𝚗⁢-⁢𝚂𝚘𝚛𝚊𝙾𝚙𝚎𝚗-𝚂𝚘𝚛𝚊\mathtt{Open\text{-}Sora}typewriter_Open - typewriter_Sora [73], 𝙸𝟸𝚅𝙶𝚎𝚗⁢-⁢𝚇𝙻𝙸𝟸𝚅𝙶𝚎𝚗-𝚇𝙻\mathtt{I2VGen\text{-}XL}typewriter_I2VGen - typewriter_XL [71], and 𝙲𝚘𝚐𝚅𝚒𝚍𝚎𝚘𝚇⁢-⁢𝟻⁢𝙱𝙲𝚘𝚐𝚅𝚒𝚍𝚎𝚘𝚇-5𝙱\mathtt{CogVideoX\text{-}5B}typewriter_CogVideoX - typewriter_5 typewriter_B [69]; however, due to limited computing resources, we only use 100,000100000100,000100 , 000 randomly selected prompts to generate videos for each image-to-video model. Researchers are free to extend our TIP-I2V by generating more videos with these methods (or other state-of-the-arts, such as 𝙾𝚙𝚎𝚗⁢-⁢𝚂𝚘𝚛𝚊⁢-⁢𝙿𝚕𝚊𝚗𝙾𝚙𝚎𝚗-𝚂𝚘𝚛𝚊-𝙿𝚕𝚊𝚗\mathtt{Open\text{-}Sora\text{-}Plan}typewriter_Open - typewriter_Sora - typewriter_Plan [31]) and our prompts; (2) we acknowledge that the currently generated videos are not perfect, and in the future, researchers are encouraged to use newly released image-to-video models (such as 𝚂𝚘𝚛𝚊𝚂𝚘𝚛𝚊\mathtt{Sora}typewriter_Sora [4] and 𝙼𝚘𝚟𝚒𝚎𝙼𝚘𝚟𝚒𝚎\mathtt{Movie}typewriter_Movie 𝙶𝚎𝚗𝙶𝚎𝚗\mathtt{Gen}typewriter_Gen [3]) and our prompts to further extend our TIP-I2V. Besides prompts and generated videos, our TIP-I2V also includes Universally Unique Identifier (UUIDs), timestamps, embeddings, subjects, and not-safe-for-work (NSFW) scores for these data points. Differences between TIP-I2V and other similar datasets in basic and semantic information. We notice that there are two popular prompt-gallery datasets in the visual generation community, i.e., VidProM [58] for text-to-video and DiffusionDB [64] for text-to-image. Our TIP-I2V mainly differs from them in: (1) Basic information: Both VidProM and DiffusionDB begin the generation with a text, while our TIP-I2V starts with a text and an image. (2) Semantics: Each text in our TIP-I2V focuses on how to bring static elements in the corresponding image to life through motion. For instance, some text prompts are “the hair and body movement, dancing”, “the flowers falling, the woman move”, and “make the statue broke down”. In contrast, the prompts in VidProM and DiffusionDB are more descriptive, i.e., they directly specify the content to be generated without referencing a specific object, such as “a dragon flying over a medieval city”, “a serene beach at sunset”, and “an astronaut walking on Mars”. The differences in basic and semantic information highlight a need for a specialized image-to-video prompt dataset. Exciting new research areas inspired by TIP-I2V. Our TIP-I2V helps researchers develop better and safer image-to-video diffusion models. For better models: (1) Enhancing user experience. Before TIP-I2V, researchers do not know which subjects users prefer to transform into videos and what directions they expect. However, with our dataset, researchers can cater to more users in the real world with limited training sources. This avoids the problem researchers previously faced, where blindly expanding the training set led to wasted resources and low user satisfaction. (2) Improving evaluation practicality. Existing image-to-video benchmarks, such as 𝚅𝙱𝚎𝚗𝚌𝚑𝚅𝙱𝚎𝚗𝚌𝚑\mathtt{VBench}typewriter_VBench-𝙸𝟸𝚅𝙸𝟸𝚅\mathtt{I2V}typewriter_I2V [25], 𝙸𝟸𝚅𝙸𝟸𝚅\mathtt{I2V}typewriter_I2V-𝙱𝚎𝚗𝚌𝚑𝙱𝚎𝚗𝚌𝚑\mathtt{Bench}typewriter_Bench [49], and 𝙰𝙸𝙶𝙲𝙱𝚎𝚗𝚌𝚑𝙰𝙸𝙶𝙲𝙱𝚎𝚗𝚌𝚑\mathtt{AIGCBench}typewriter_AIGCBench [18], suffer from a limited number of topics and prompts designed by experts, which may not accurately reflect real-world user needs. With the help of TIP-I2V, researchers can build a more comprehensive and practical benchmark for evaluating image-to-video models. For safer models: A major safety concern of image-to-video generation is misinformation, i.e., they can make objects or humans in images to perform actions they never did. For instance, given an image of 𝙴𝚕𝚘𝚗𝙴𝚕𝚘𝚗\mathtt{Elon}typewriter_Elon 𝙼𝚞𝚜𝚔𝙼𝚞𝚜𝚔\mathtt{Musk}typewriter_Musk and 𝙳𝚘𝚗𝚊𝚕𝚍𝙳𝚘𝚗𝚊𝚕𝚍\mathtt{Donald}typewriter_Donald 𝚃𝚛𝚞𝚖𝚙𝚃𝚛𝚞𝚖𝚙\mathtt{Trump}typewriter_Trump together, image-to-video models could generate a video of them fighting, misleading the public. To address this issue, our TIP-I2V allows researchers to train a model to (1) distinguish between generated videos from images and real videos, and (2) trace the source image from any given frame in a generated video. Beyond these areas, we also encourage researchers to explore additional directions inspired by the TIP-I2V. Figure 2: A data point in our TIP-I2V includes UUID, timestamp, text and image prompt, subject, NSFW status of text and image, text and image embedding, and the corresponding generated videos. In conclusion, our key contributions are as follows: 1. We present TIP-I2V, the first dataset of text and image prompts specifically for image-to-video generation. This dataset contains over 1.701.701.701.70 million prompts from real users, along with corresponding videos generated by five state-of-the-art image-to-video diffusion models. 2. We compare TIP-I2V with two popular prompt datasets, i.e., VidProM (text-to-video) and DiffusionDB (text-to-image), highlighting their differences in both basic and semantic information. This emphasizes the need for a specialized image-to-video prompt dataset. 3. We demonstrate how TIP-I2V contributes to building better and safer image-to-video diffusion models. Specifically, it can help enhance user experience, improve the practicality of evaluation, distinguish generated videos from real ones, and trace the source image."
https://arxiv.org/html/2411.04707v1,From CNN to ConvRNN: Adapting Visualization Techniques for Time-Series Anomaly Detection,"Nowadays, neural networks are commonly used to solve various problems. Unfortunately, despite their effectiveness, they are often perceived as black boxes capable of providing answers without explaining their decisions, which raises numerous ethical and legal concerns. Fortunately, the field of explainability helps users understand these results. This aspect of machine learning allows users to grasp the decision-making process of a model and verify the relevance of its outcomes. In this article, we focus on the learning process carried out by a “time distributed“ convRNN, which performs anomaly detection from video data.","Deep neural networks play a key role today in solving complex problems, particularly in real-time anomaly detection from videos. However, their opaque nature makes them “black boxes” that are difficult to interpret, which raises not only technical but also ethical and legal challenges. This opacity is particularly problematic when it comes to justifying decisions made by these systems, especially in sensitive domains like security, where anomalies such as fights, gunshots, or car accidents need to be detected. In this context, the European Union implemented the General Data Protection Regulation (GDPR) in May 2019, which imposes strict rules regarding the use of algorithms in decision-making. Article 22-1 of the GDPR states that a decision cannot be based solely on automated processing if it has significant effects on a person. This makes it even more crucial to develop techniques that explain and help understand the decisions made by anomaly detection models, particularly to ensure their legal and ethical compliance. In this study, we implemented a convRNN model to detect anomalies in videos. We focused our analysis on critical security actions such as fights or gunshots. Regarding explainability, it is much easier to interpret the features learned by our convolutional network, as they can be visualized, unlike those generated by our RNN. In projects employing convolutional networks for image processing, it is common to visualize the features extracted by the model to assess the relevance of its learning. However, video data processing complicates this approach. Given that a video essentially consists of a sequence of images, it is legitimate to question whether the same visualization techniques can be applied to models incorporating “time distributed“ convolutions. This question is particularly important in the context of anomaly detection, where understanding the areas on which the model focuses its attention is essential. To address this issue, we begin by reviewing the main visualization techniques available for neural networks, particularly in the context of image and video analysis. Next, we detail the procedure we adopted to apply these techniques to our “time distributed“ convRNN model. Finally, we present our results and discuss their implications before concluding."
https://arxiv.org/html/2411.04706v1,ESC-MISR: Enhancing Spatial Correlations for Multi-Image Super-Resolution in Remote Sensing,"Multi-Image Super-Resolution (MISR) is a crucial yet challenging research task in the remote sensing community. In this paper, we address the challenging task of Multi-Image Super-Resolution in Remote Sensing (MISR-RS), aiming to generate a High-Resolution (HR) image from multiple Low-Resolution (LR) images obtained by satellites. Recently, the weak temporal correlations among LR images have attracted increasing attention in the MISR-RS task. However, existing MISR methods treat the LR images as sequences with strong temporal correlations, overlooking spatial correlations and imposing temporal dependencies. To address this problem, we propose a novel end-to-end framework named Enhancing Spatial Correlations in MISR (ESC-MISR), which fully exploits the spatial-temporal relations of multiple images for HR image reconstruction. Specifically, we first introduce a novel fusion module named Multi-Image Spatial Transformer (MIST), which emphasizes parts with clearer global spatial features and enhances the spatial correlations between LR images. Besides, we perform a random shuffle strategy for the sequential inputs of LR images to attenuate temporal dependencies and capture weak temporal correlations in the training stage. Compared with the state-of-the-art methods, our ESC-MISR achieves 0.70dB and 0.76dB cPSNR improvements on the two bands of the PROBA-V dataset respectively, demonstrating the superiority of our method.","Super-resolution algorithms serve the reconstruction of a High-Resolution (HR) image from one or a set of Low-Resolution (LR) images. High-Resolution (HR) remote sensing images, derived from advanced remote sensing techniques, have plenty of applications including land cover change analysis, urban planning, and meteorological forecasting. Nevertheless, the acquisition of HR images often necessitates costly satellites or alternative data sources because of hardware constraints. To address this challenge, super-resolution technologies continue to develop and have received widespread attention in the realm of remote sensing. Regarding Super-Resolution in Remote Sensing (SR-RS), research predominantly focuses on Single-Image Super-Resolution in Remote Sensing (SISR-RS) and Multi-Image Super-Resolution in Remote Sensing (MISR-RS). SISR concentrates on reconstructing an HR image from a single LR image. However, remote sensing images exhibit a weaker correlation than general images and necessitate leveraging effective information from multiple scenes. Consequently, MISR has emerged as a vital technology in remote sensing for generating HR images from multiple LR images. Through extensive research on MISR-RS [22, 9, 31, 5], we note that existing models for MISR-RS are highly sensitive to temporal sequences and neglect spatial correlations among LR images. These methods consider LR images as sequences strongly correlated over time. However, factors like long intervals, cloud occlusions, and lighting variations result in weak temporal correlations among LR images in each scene, and LR images contain complementary spatial information. It is essential to weigh up the spatial-temporal characteristics in the process of generating HR images by employing multiple LR images. Although RAMS [25] and PIU [27] attempt to overcome temporal dependencies, they do not fully eliminate the temporal impact of frame orders. TR-MISR [1] focuses on all image patches and adapts to multi-image scenarios with weak temporal correlations. However, it does not emphasize the spatial correlations between multiple images. To address these limitations, we propose a novel end-to-end framework Enhancing Spatial Correlations in MISR (ESC-MISR), tapping complementary spatial features and mitigating temporal dependencies. Concretely, we design a new fusion module Multi-Image Spatial Transformer (MIST), to enhance spatial correlations between multiple LR images. Our MIST consists of several Multi-Image Spatial Attention Blocks (MISAB), aiming to enhance the spatial correlations between multiple images. The swin transformer, as a typical self-attention mechanism, performs self-attention in each window and obtains spatial information of the entire image by moving the window. However, this cannot effectively perceive the global information of each image. Instead of using the window-based approach, we introduce message tokens [24] in our MISAB to obtain global spatial information directly. The MISAB first calculates the cross-attention of all patches containing multiple pixels within each image. During this period, there is no information exchange between multiple images to weaken temporal correlations. Moreover, after independently obtaining global cross-attention information of each image, we fuse the spatial features of multiple images to mine their complementary spatial information. For encoding networks, the vision transformer [32, 18, 19] has already demonstrated strong feature encoding capabilities in super-resolution. Nonetheless, most researchers still prioritize Convolutional Neural Networks (CNNs) for feature encoding in MISR-RS. The CNNs-Meet-Transformers (CMT) has proven its stronger encoding ability than CNNs and pure transformer networks such as ViT [10] and swin transformer [20]. Thus, we explore the CMT [13] to encode LR images better in our ESC-MISR. Additionally, we employ the Fast Fourier Convolution (FFC) [4] in the decoder to increase the receptive field of the images. Simultaneously, we employ a random shuffle strategy for LR images of each scene in the training stage, which makes the model insensitive to weak temporal information and generates stable SR images for different orders. The experimental results clearly demonstrate the effectiveness of our proposed method in improving the MISR-RS performance. Specifically, ESC-MISR is 0.70dB-1.87dB higher than the state-of-the-art methods on the NIR band, and there is a significant improvement of 0.76dB-1.63dB on the RED band. The contributions of this paper are mainly four-fold: • We propose a new framework Enhancing Spatial Correlations in MISR (ESC-MISR) to excavate spatial-temporal relations of multiple Low-Resolution (LR) images, strengthening spatial features while weakening temporal correlations among LR images. • To enhance spatial correlations between LR images, we design a novel fusion module, Multi-Image Spatial Transformer (MIST), which fully utilizes global spatial features for High-Resolution (HR) image reconstruction. • To attenuate temporal dependencies, a random shuffle strategy is implemented for the sequence of multiple LR images to assist our ESC-MISR in fitting weak temporal correlations in the training stage. • Compared with the state-of-the-art MISR-RS methods, ESC-MISR obtains 0.65dB and 0.71dB cPSNR improvements on the two bands of the PROBA-V dataset respectively, demonstrating the superiority of our method."
https://arxiv.org/html/2411.04697v1,Dynamic Brightness Adaptation for Robust Multi-modal Image Fusion,"Infrared and visible image fusion aim to integrate modality strengths for visually enhanced, informative images. Visible imaging in real-world scenarios is susceptible to dynamic environmental brightness fluctuations, leading to texture degradation. Existing fusion methods lack robustness against such brightness perturbations, significantly compromising the visual fidelity of the fused imagery. To address this challenge, we propose the Brightness Adaptive multimodal dynamic fusion framework (BA-Fusion), which achieves robust image fusion despite dynamic brightness fluctuations. Specifically, we introduce a Brightness Adaptive Gate (BAG) module, which is designed to dynamically select features from brightness-related channels for normalization, while preserving brightness-independent structural information within the source images. Furthermore, we propose a brightness consistency loss function to optimize the BAG module. The entire framework is tuned via alternating training strategies. Extensive experiments validate that our method surpasses state-of-the-art methods in preserving multi-modal image information and visual fidelity, while exhibiting remarkable robustness across varying brightness levels. Our code is available: https://github.com/SunYM2020/BA-Fusion.","Intelligent unmanned systems are susceptible to a decrease in perception ability due to the interference of real dynamic environments Birk (2021). Configuring multimodal sensors can effectively enhance their perception ability in complex environments Sun et al. (2022b). Infrared and visible cameras, as a typical set of multimodal sensors, have been widely applied in casualty searching, surveillance missions, e⁢t⁢c.𝑒𝑡𝑐etc.italic_e italic_t italic_c . However, due to the limitations of hardware devices and imaging mechanisms, visible or infrared cameras can usually only capture partial information of the scene and cannot fully represent the entire scene. Therefore, multimodal image fusion Ma et al. (2019a); Zhang et al. (2021); Xu et al. (2022) can aggregate the significant contrast information of the infrared modality and the texture detail information of the visible modality, generating fusion images with sufficient information and good visual effects. In the past decades, how to design advanced fusion methods has attracted a lot of research attention. Figure 1: Visual comparisons on fused results and pixel histograms under dynamic brightness conditions. Our method keeps robust performance under varying levels of brightness. Existing infrared and visible fusion methods can be divided into two categories: traditional methods represented by image decomposition Li et al. (2020) and sparse representation Zhang et al. (2018), and methods based on deep learning. Among them, deep learning-based methods can be further categorized into methods based on pre-trained autoencoder Li and Wu (2018); Zhao et al. (2020); Li et al. (2021), methods based on GAN Ma et al. (2019b, 2020), methods based on CNN Tang et al. (2022b); Sun et al. (2022a); Cao et al. (2023), methods based on diffusion models Zhao et al. (2023), and methods based on Transformer Wang et al. (2022); Tang et al. (2022c). However, most of these methods directly combined the texture details and object contrasts of different modalities using a fixed correlation pattern, ignoring the dynamic changes in reality. This makes it difficult for the models to achieve dynamically robust adaptive fusion effects when facing fluctuations in environmental brightness. As shown in Fig. 1, the visible image is affected by brightness interference, resulting in overexposed or dark images. Existing methods lack the dynamic adaptive capability to handle brightness changes, inevitably leading to fluctuations in the fusion image quality with varying environmental brightness, thereby reducing the visual fidelity of the fusion image. In practical applications, infrared and visible image fusion models should possess robust adaptive capabilities to handle variations in environmental brightness to avoid fusion results being compromised due to brightness fluctuations. To fill this gap, we propose an adaptive dynamic fusion framework that can adapt to changes in brightness and achieve robust fusion of multimodal images under dynamic variations in environmental brightness. In Fig. 1, our method shows a more consistent histogram distribution under different brightness conditions, demonstrating the proposed method effectively balances the impact of brightness fluctuations on the model’s learning of texture details and contrast information of different modalities, thus achieving the most robust fusion effect. Specifically, we propose a dynamic image fusion framework with brightness adaptive gating, termed BA-Fusion, which consists of two parts: Brightness Adaptive Gate (BAG) and multimodal fusion backbone network. The multimodal backbone network is composed of an encoder and a decoder, which are used to extract features from the infrared and visible modalities and generate the fusion image, respectively. The BAG module guides the model to dynamically select the most relevant feature channels with respect to brightness variations in a data-driven manner. It performs brightness normalization on these channels to eliminate the impact of brightness, while the brightness-independent channel features continue to be reused to preserve structural detail information. To train the BAG module, we designed a brightness consistency loss function, which serves as a constraint by ensuring the frequency domain brightness representation of fusion results under different brightness perturbations is consistent with that of normal fusion results. The BAG module gradually establishes the connection between brightness variations and feature channels through alternating training strategies. In this way, the proposed BA-Fusion has the capability of brightness-adaptive robust multimodal fusion. The main contributions of this paper are summarized as follows: • We propose a brightness adaptive dynamic image fusion framework, which effectively mitigates the instability issue in fusion effect caused by environmental brightness fluctuations, enabling robust fusion of infrared and visible images under dynamic brightness conditions. • We introduce a brightness adaptive gate module that establishes the correspondence between input image brightness and channel feature representation under the constraint of the brightness consistency loss function. • The proposed model dynamically balances the advantages of visible and infrared modalities in terms of texture details and contrast. Extensive experiments on multiple infrared-visible datasets clearly demonstrate our superiority from both quantitative and qualitative perspectives."
https://arxiv.org/html/2411.04692v1,"Personalized Federated Learning for Cross-view Geo-localization††thanks:This paper was supported by the the EU’s
H2020 TRUSTEE (No. 101070214).","In this paper we propose a methodology combining Federated Learning (FL) with Cross-view Image Geo-localization (CVGL) techniques. We address the challenges of data privacy and heterogeneity in autonomous vehicle environments by proposing a personalized Federated Learning scenario that allows selective sharing of model parameters. Our method implements a coarse-to-fine approach, where clients share only the coarse feature extractors while keeping fine-grained features specific to local environments. We evaluate our approach against traditional centralized and single-client training schemes using the KITTI dataset combined with satellite imagery. Results demonstrate that our federated CVGL method achieves performance close to centralized training while maintaining data privacy. The proposed partial model sharing strategy shows comparable or slightly better performance than classical FL, offering significant reduced communication overhead without sacrificing accuracy. Our work contributes to more robust and privacy-preserving localization systems for autonomous vehicles operating in diverse environments.","Cross-view Geo-localization can provide a robust solution to the limitations of traditional GPS-based localization for vehicles. While GPS is generally reliable, it can fail in certain scenarios such as dense urban environments or when network connectivity is lost [1]. In these cases, relying solely on GPS can lead to inaccuracies and inconsistencies in vehicle localization. This can be accomplished by matching points of interest between the different views, the ground view and the satellite image. The arrival of Deep Learning (DL) techniques in feature extraction and their predominance over handcrafted techniques facilitated a new era characterized by improved accuracy and efficiency in processing and interpreting complex visual data, enabling more reliable Cross-view Geo-localization [2], [3]. Data-driven models require large volumes of raw data for training something which can pose challenges advantages in terms of privacy and security [4]. On the other hand, Federated Learning frameworks offer a promising solution by enabling models to be trained in a decentralized manner without the need to exchange local raw data, thus providing advantages in terms of data privacy and resource allocation [5]. However, models participating in an FL training scheme often face challenges due to heterogeneous datasets, as autonomous vehicles typically operate in diverse environments [6]. To address this issue of local dataset heterogeneity, we propose a more personalized federated learning scenario. This approach implements a coarse-to-fine methodology, enforcing each client to selectively share only parts of its local model parameters. By doing so, we can better accommodate the unique characteristics of each vehicle’s operating environment while still benefiting from collaborative learning. Furthermore, the aggregations of the components of local models responsible for extracting coarse, general features, which are more universal across different environments, can effectively improve overall model robustness and produce patterns that benefit all participants. Meanwhile, fine-grained feature extractors remain specific to each vehicle, capturing unique characteristics of local environments (e.g., urban vs. rural areas). This prevents potential performance degradation that could occur from aggregating highly specialized feature. In addition, by selectively sharing only the coarse feature extractors, our approach not only enhances model performance but also improves computational efficiency by reducing the amount of data transmitted during the federated learning process. The main contributions of this paper can be summarized as follows: • We propose the first methodology that combines federated learning with Cross-view image Geo-localization techniques. Our approach leverages deep neural networks to extract robust features from both satellite and ground-view images in a distributed setting, resulting in more accurate models that are resilient to view changes and discriminative for feature correspondences. • We introduce a personalized federated learning scenario to address dataset heterogeneity from autonomous vehicles in diverse environments. Our approach allows clients to selectively share parts of their local model parameters using a coarse-to-fine methodology. We aggregate components responsible for extracting general features, enhancing model robustness, while keeping fine-grained features specific to each vehicle to capture unique local characteristics. This prevents performance degradation from aggregating specialized features and improves computational efficiency by reducing data transmission. • We conduct a thorough evaluation of the proposed federated learning Cross-view Geo-localization scenarios, comparing them against traditional centralized and single-client training schemes. This evaluation demonstrates the effectiveness of our approach. Extensive numerical results on a real-world dataset demonstrate the superiority of the proposed Federated Cross-view method and the benefits of cooperation to extract more robust features from diverse environments. The remaining paper is structured as follows: Section II offers a brief overview of Cross-view Geo-localization and reviews related works on Federated Learning in the automotive domain. Section III details our methodology, including the theoretical formulation of the Cross View Geo Localization problem, the algorithm used, and the Federated Learning training schema. Section IV describes the experimental setup and presents the results. Finally, Section V concludes the paper and summarizes our findings."
https://arxiv.org/html/2411.04682v1,DNN-based 3D Cloud Retrieval for Variable Solar Illumination and Multiview Spaceborne Imaging,"Climate studies often rely on remotely sensed images to retrieve two-dimensional maps of cloud properties. To advance volumetric analysis, we focus on recovering the three-dimensional (3D) heterogeneous extinction coefficient field of shallow clouds using multiview remote sensing data. Climate research requires large-scale worldwide statistics. To enable scalable data processing, previous deep neural networks (DNNs) can infer at spaceborne remote sensing downlink rates. However, prior methods are limited to a fixed solar illumination direction. In this work, we introduce the first scalable DNN-based system for 3D cloud retrieval that accommodates varying camera poses and solar directions. By integrating multiview cloud intensity images with camera poses and solar direction data, we achieve greater flexibility in recovery. Training of the DNN is performed by a novel two-stage scheme to address the high number of degrees of freedom in this problem. Our approach shows substantial improvements over previous state-of-the-art, particularly in handling variations in the sun’s zenith angle.","Clouds play a key role in the climate system [trenberth_2009]. They form multi-scale, complex dynamical systems, with a multitude of feedbacks. However, clouds are challenging to resolve or represent in climate models. A finer understanding of clouds is needed. We focus on retrieving the three-dimensional (3D) heterogeneous extinction coefficient field of shallow clouds using multiview remotely sensed data. To sense clouds volumetrically in 3D, adequate observations are required as a prerequisite. This need led to the CloudCT space mission [schilling2019cloudct, tzabari2021cloudct]. Cloud CT includes a coordinated formation of ten nano-satellites simultaneously imaging a cloud field from multiple directions as illustrated in Fig. 1. In addition to observational improvements, advanced analysis is required. Figure 1: A) A satellite formation in orbit abserves a cloud. The uncontrolled scene is illuminated by the sun in direction 𝝎sun=[ωzenith,ωazimuth]superscript𝝎sunsuperscript𝜔zenithsuperscript𝜔azimuth{\boldsymbol{\omega}}^{\rm sun}=[\omega^{\rm zenith},\omega^{\rm azimuth}]bold_italic_ω start_POSTSUPERSCRIPT roman_sun end_POSTSUPERSCRIPT = [ italic_ω start_POSTSUPERSCRIPT roman_zenith end_POSTSUPERSCRIPT , italic_ω start_POSTSUPERSCRIPT roman_azimuth end_POSTSUPERSCRIPT ]. Radiation that reaches a point 𝐗𝐗{\bf X}bold_X in the atmospheric domain is multiply scattered until sensed by camera sensor at 𝐗csubscript𝐗𝑐{\bf X}_{c}bold_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT. B) Cloud single-channel intensity images are shown from the same satellite position. The images change significantly when illuminated from a different direction. Recent work [aides2020distributed, holodovsky2016situ, levis2015airborne, levis2017multiple, ronen20214d] showed the potential of cloud scattering tomography, which is a special form of computed tomography (CT). In contrast to X-ray CT, cloud imaging is passive, relying on sun radiation as a source. Image readouts relate nonlinearly to volumetric cloud structure by 3D radiative transfer (RT): sunlight reaches the sensor by scattering, mainly from cloud droplets and atmospheric particles. To invert the 3D RT, Refs. [aides2020distributed, holodovsky2016situ, levis2015airborne, levis2017multiple, ronen20214d] use iterative-based optimization. However, these methods are slow and unscalable for extracting rich worldwide 3D cloud retrievals. To address recovery speed, deep neural network (DNN) methods were suggested [main_vipct, probct, nemf]. DNNs shift the computational load to a training stage, making it feasible to analyse large data during inference, at rates comparable to downlink rates typical of spaceborne remote sensing. DNN-based cloud recovery requires training data of cloud volumes and corresponding multiview images. Cloud imaging is performed in an uncontrolled environment (see Fig. 1). Hence, data should include images with an arbitrary incoming sun direction and variable camera poses. The large number of degrees of freedom poses a challenge in acquiring such a large and diverse dataset. Furthermore, even if such a dataset exists, designing and training a DNN system for such complex data is not trivial. In this work, we address these challenges, presenting Projection Integration for Variable OrienTation in Computed Tomography (PIVOT-CT): The first scalable system for 3D cloud retrieval with flexibility in both camera poses and sun direction. We accomplished this by (i) extending the VIP-CT [main_vipct] model by incorporating into its decoder the sun direction; and (ii) designing a novel two-stage training scheme. This scheme mitigates the challenge of the many degrees of freedom in our setup."
https://arxiv.org/html/2411.04656v1,ICH-SCNet: Intracerebral Hemorrhage Segmentation and Prognosis Classification Network Using CLIP-guided SAM mechanism,"Intracerebral hemorrhage (ICH) is the most fatal subtype of stroke and is characterized by a high incidence of disability. Accurate segmentation of the ICH region and prognosis prediction are critically important for developing and refining treatment plans for post-ICH patients. However, existing approaches address these two tasks independently and predominantly focus on imaging data alone, thereby neglecting the intrinsic correlation between the tasks and modalities. This paper introduces a multi-task network, ICH-SCNet, designed for both ICH segmentation and prognosis classification. Specifically, we integrate a SAM-CLIP cross-modal interaction mechanism that combines medical text and segmentation auxiliary information with neuroimaging data to enhance cross-modal feature recognition. Additionally, we develop an effective feature fusion module and a multi-task loss function to improve performance further. Extensive experiments on an ICH dataset reveal that our approach surpasses other state-of-the-art methods. It excels in the overall performance of classification tasks and outperforms competing models in all segmentation task metrics.","Intracerebral hemorrhage (ICH) represents a significant public health challenge, affecting over 2 million individuals annually and accounting for approximately 10-15% of all stroke cases [14]. To date, completely effective therapy for all stages of ICH treatment remains elusive [3]. Despite advances in medical imaging, particularly computed tomography (CT), and ongoing research on ICH, the rates of mortality and long-term disability in post-ICH patients have not markedly declined [12]. Accurate segmentation of ICH lesions and timely prognosis prediction are indispensable for formulating effective treatment strategies. However, these tasks frequently rely on manual annotations and assessments by neurosurgeons, which are labor-intensive and may be influenced by subjective factors and individual experience [4]. Moreover, most current ICH-related models are neither multifunctional nor fully exploiting non-image modalities [13, 1, 10]. Such models often overlook the potential for information exchange between different modalities and tasks, which could otherwise enhance overall performance. Figure 1: (a) Overview of the framework of ICH-SCNet, including depictions of (b) the SAM-CLIP Cross-Modal Interaction module and (c) the MTFF module. In a medical context, segmentation tasks typically require pixel-level prediction within images, while classification tasks involve the overall categorization of entire images. In this case, segmentation can benefit from the image-level semantic information provided by classification tasks, which assists in accurately delineating lesion areas. Conversely, segmentation tasks offer detailed pixel-level information that aids classification tasks in better understanding the nuances and boundaries within images. Recognizing this interdependence can significantly improve both the precision of segmentation and the reliability of prognosis, ultimately enhancing patient outcomes in clinical practice. Given this symbiotic relationship, some medical networks have embarked on multi-task endeavors [25, 16]. However, efforts in multi-task learning for brain regions, specifically for ICH, remain nascent and are sometimes absent. Thus, there exists a practical need for a multi-task network that can concurrently address ICH segmentation and prognosis classification, recognizing and leveraging the inherent interdependence of these tasks. Another significant challenge is the incorporation of diverse and comprehensive medical information into models. Relying solely on imaging data can be inadequate given the complexity and multifaceted nature of medical tasks, which demand a rich variety of information. Particularly in biomedical tasks, especially those involving the brain, it is common to incorporate non-image modal data to guide the model by inputting additional domain knowledge, diagnostic information, and patient demographics. Recent methodologies [13, 1, 10] have attempted to merge image features with additional medical data through simple concatenation, resulting in limited cross-modal feature interaction. Moreover, evolutionary strategies have endeavored to integrate tabular data [9], domain-specific knowledge [18], and textual reports [22] to achieve a deeper understanding of both image and non-imaging data. However, these approaches still exhibit potential for further refinement, particularly in establishing the internal dependencies of cross-modal features. Recognizing the strengths of both the Segment Anything Model (SAM) [6] for integrating segmentation auxiliary information, and the Contrastive Language-Image Pre-training (CLIP) [15] for cross-modal fusion of texts and images, we propose that the collaboration of these two models can bridge the current gap in merging distinct modalities. Based on these observations, we innovatively present a SAM-CLIP cross-modal interaction mechanism that incorporates additional medical information to guide segmentation and cross-modal feature extraction. To the best of our knowledge, our ICH-SCNet represents one of the first attempts to develop multi-task models that utilize cross-modal information to concurrently address segmentation and prognosis classification tasks, particularly in the context of ICH."
https://arxiv.org/html/2411.04642v1,TAP-VL: Text Layout Aware Pretraining for Enriched Vision-Language Models,"Vision-Language (VL) models have garnered considerable research interest; however, they still face challenges in effectively handling text within images. To address this limitation, researchers have developed two approaches. The first method involves utilizing external Optical Character Recognition (OCR) tools to extract textual information from images and prepend it to the textual inputs. The second strategy is OCR-free and focuses on employing extremely high-resolution images to improve text recognition capabilities. In this paper, we focus on enhancing the first strategy by introducing a novel method, named TAP-VL, which treats OCR information as a distinct modality and seamlessly integrates it into any VL model. TAP-VL employs a lightweight transformer-based OCR module to receive OCR with layout information, compressing it into a short fixed-length sequence which serves as an input for the LLM. To this end, we conduct model-agnostic pretraining of the OCR module on unlabeled documents, followed by its integration into any VL architecture through short fine-tuning. Extensive experiments demonstrate consistent performance improvements when applying TAP-VL to top-performing VL models, across scene-text and document-based benchmarks.","Large Vision-Language (VL) models have emerged as a key research area in the field of artificial intelligence, leading to significant progress in multimodal reasoning [4, 20, 23, 24, 29, 33, 34, 38, 8, 18, 25, 35, 12]. Such architectures bridge the gap between visual and textual data by integrating a vision encoder and a Large Language Model (LLM) via a translation module. This module projects the visual encodings into the text embeddings space. As VL models can generate content based on both visual and textual information, they play a pivotal role in a diverse set of applications and tasks, including image captioning (CAPS) [15] and visual question answering (VQA) [5]. While open-source VL models have shown impressive performance across various tasks, many still face challenges when dealing with with OCR-oriented tasks such as TextVQA [50], TextCaps [49], and DocVQA [43]. There are two main strategies to address this challenge: (1) integrating an external OCR system to extract OCR tokens and use them as additional input, and (2) employing very high-resolution images combined with extensive pre-training to enhance text recognition. Each approach has its own advantages and limitations, and both are active areas of research. In this paper, we focus on the first approach. Figure 1: Layout-aware OCR Adapter. (a) Previous methods extract OCR data and input it into the LLM as plain text. (b) TAP-VL introduces a plug-and-play OCR adapter that leverages layout information and can be seamlessly integrated with any vision-language LLM (VLLM). The prevailing paradigm for incorporating OCR into VL systems involves prepending raw OCR-extracted words into the LLM (left side of Fig. 1). While this strategy enhances performance on OCR-oriented benchmarks, it exhibits critical shortcomings. Firstly, it relies solely on OCR tokens, neglecting crucial spatial layout information proven highly beneficial in OCR-oriented tasks [6, 10, 24, 27]. Moreover, when applied to domains with text-rich images, inserting lengthy OCR sequences into the LLM results in significant computational overhead due to the quadratic complexity of the attention mechanism. Figure 2: TAP-VL Approach. (a) Our model-agnostic layout-aware pretraining framework for creating condensed rich OCR embeddings conditioned on text. (b) TAP-VL fully integrated, enhancing any VL model on OCR-oriented tasks. In this study, we address these limitations and introduce TAP-VL, a technique for seamlessly integrating OCR information into any VL model through short fine-tuning (right side of Fig. 1). By incorporating 2D positional data in addition to the OCR-extracted word tokens, the model can interpret relationships between different textual elements and understand hierarchical structures that are essential for accurate information extraction and holistic document understanding. Conceptually, our method treats OCR as a distinct modality and thus employs an OCR module, similar to the use of a dedicated vision module for encoding visual input. Following this, we introduce a transformer-based lightweight OCR-Q, to generate meaningful representations conditioned on user queries. The OCR encoder captures vital spatial layout information, while the OCR-Q condenses lengthy OCR details into a fixed-size sequence length representation. This condensed representation serves as input for the LLM alongside visual and textual data (right side of Fig. 2). TAP-VL employs these condensed representations to integrate OCR with spatial information into the VL model. Initially, we introduce a standalone, model-agnostic layout-aware pretraining, as depicted on the left side of Fig. 2. This phase operates independently of the VL model, enhancing efficiency and enabling a focused exploration of OCR understanding without introducing a distribution shift to the VL model. Aimed at distilling and extracting the most relevant OCR information, we propose a designated layout-aware pretraining that leverages the abundant unlabeled document data with rich layouts and text [10, 11]. Specifically, we pretrain the OCR-Q in a three-objectives scheme, drawing inspiration from previous works [10, 6, 34]. In more detail, our approach consists of the following layout-aware tasks: (1) OCR-Grounded Mask Denoising, which predicts masked spans based on the noisy OCR input; (2) OCR-Mask Contrastive Learning, which aims to align OCR and word representations within the same document while distinguishing between representations from different documents, and (3) OCR-Mask Matching,which aligns noisy OCR text with missing spans. Combining such objectives propels the model to acquire a deep layout and OCR understanding while providing a compact representation. Following this, we integrate the same pretrained model into various leading VL models via a short multi-task fine-tuning procedure. Specifically, we examine prominent VL models such as InstructBLIP [20], LLaVA [38] and Qwen-VL [8]. Our extensive experimentation demonstrates the efficacy of TAP-VL across document understanding and scene-text VL benchmarks, resulting in substantial enhancements compared to diverse baseline methods across all assessed benchmarks, including a zero-shot scenario. In addition, we propose TAP-VLLightsubscriptTAP-VLLight\text{TAP-VL}_{\text{Light}}TAP-VL start_POSTSUBSCRIPT Light end_POSTSUBSCRIPT, a light-weight version of TAP-VL that solely utilizes our compressed OCR representations without providing the LLM with the raw OCR tokens. This approach is specifically efficient in tasks related to document understanding with dense-text images. Notably, we demonstrate that applying TAP-VLLightsubscriptTAP-VLLight\text{TAP-VL}_{\text{Light}}TAP-VL start_POSTSUBSCRIPT Light end_POSTSUBSCRIPT not only significantly reduces the computational costs compared to the relevant baselines (reduces the FLOPs by up to a factor of seven), but also leads to substantial performance improvements. Notably, we showcase TAP-VLLightsubscriptTAP-VLLight\text{TAP-VL}_{\text{Light}}TAP-VL start_POSTSUBSCRIPT Light end_POSTSUBSCRIPT’s ability to extrapolate to multi-page scenarios without any specific multi-page training. In the most challenging case of multi-page document understanding, TAP-VLLightsubscriptTAP-VLLight\text{TAP-VL}_{\text{Light}}TAP-VL start_POSTSUBSCRIPT Light end_POSTSUBSCRIPT achieves performance improvements of up to 4.8%, while substantially reducing the computational costs. In summary, our contributions include: • Introducing TAP-VL, a novel approach for seamlessly integrating OCR information into any pretrained VL model, enabling effective reasoning over both textual and spatial information. • Proposing a unique layout-aware model-agnostic pretraining strategy, utilizing unlabeled document data to acquire rich, condensed OCR features. • Demonstrating the effectiveness of our method in enhancing performance across various state-of-the-art VL architectures, showcasing its ability to elevate performance across multiple benchmarks in scene-text and document understanding tasks, including challenging zero-shot multi-page setting. • We present TAP-VLLightsubscriptTAP-VLLight\text{TAP-VL}_{\text{Light}}TAP-VL start_POSTSUBSCRIPT Light end_POSTSUBSCRIPT, a lightweight version of TAP-VL, capable of handling multi-page documents without any specific training. TAP-VLLightsubscriptTAP-VLLight\text{TAP-VL}_{\text{Light}}TAP-VL start_POSTSUBSCRIPT Light end_POSTSUBSCRIPT decreases FLOPs by up to four times while still achieving superior performance compared to approaches relying on the uncompressed OCR sequence."
https://arxiv.org/html/2411.04632v1,Improved Multi-Task Brain Tumour Segmentation with Synthetic Data Augmentation,"This paper presents the winning solution of task 1 and the third-placed solution of task 3 of the BraTS challenge. The use of automated tools in clinical practice has increased due to the development of more and more sophisticated and reliable algorithms. However, achieving clinical standards and developing tools for real-life scenarios is a major challenge. To this end, BraTS has organised tasks to find the most advanced solutions for specific purposes. In this paper, we propose the use of synthetic data to train state-of-the-art frameworks in order to improve the segmentation of adult gliomas in a post-treatment scenario, and the segmentation of meningioma for radiotherapy planning. Our results suggest that the use of synthetic data leads to more robust algorithms, although the synthetic data generation pipeline is not directly suited to the meningioma task. The code for these tasks is available at https://github.com/ShadowTwin41/BraTS_2023_2024_solutions.","Gliomas are a type of brain tumour originating from glial cells [1, 3]. This type of tumour is among the deadliest types of cancer and constitutes the most prevalent malignant primary brain tumours in adults [17]. Gliomas’ aggressive nature and resistance to therapy make them a major problem in oncology [1]. As such, promptly and correctly identifying them is crucial for an effective treatment and post-treatment. Therefore, the development of algorithms to automatically detect and segment the gliomas, as the Brain Tumour Segmentation (BraTS) challenges propose, would help patients worldwide. Magnetic resonance imaging (MRI) is a type of medical imaging modality that remains the gold standard imaging method for post-treatment across the spectrum of gliomas. The patients’ MRI exams provide crucial information on tumour size, location, and morphological changes over time. Post-treatment imaging of gliomas is fundamental and significantly influences clinical decision-making and outcomes on patients [17]. Meningioma is another type of brain tumour, comprising 40.8% of all central nervous system (CNS) tumours and 55.4% of all non-malignant CNS tumours, making it the most common primary intracranial tumour. The vast majority, 99.1%, of meningiomas, are non-malignant and can be followed with MRI exams if asymptomatic [11, 9, 10]. Although segmentation of pre-operative meningioma is essential for patients’ treatment, it is a complex and very time-consuming task. As such, just like the gliomas’ task, the BraTS challenge meningioma task aims to create a community benchmark for automated segmentation of these tumours which will save time and improve patients’ radiotherapy planning. 1.1 State of the art In 2024, the first task will switch from the segmentation of adult gliomas to post-treatment of adult gliomas. Even though these tasks have specific characteristics and challenges, the tools used for the first can be applied and fine-tuned for the new task. Since the first edition of the BraTS challenge, multiple different approaches have been developed. Deep convolutional networks have performed the best since 2014 [15, 6]. The nnUNet [5] has dominated the competition since 2020. The recent winners have also integrated the nnUNet pipeline into their solution, as the U-Net offers a large capacity for segmentation and the robustness of the nnUNet. In the 2020 edition, [6] introduced the nnUNet. Some specific changes were made to the basic nnUNet pipeline: Use of larger batch size (from 2 to 5), use of more aggressive data augmentation, use of batch normalisation, use of batch dice, and threshold-based post-processing. In 2021, [12] extended the nnUNet pipeline by increasing the size of the encoder, replacing batch normalisation with group normalisation and preserving batch size 2. In 2022 [18] the ensemble of three architectures was used: DeepScan [13], DeepSeg [19], and the nnUNet developed by the winner of 2020 edition. In 2023, the winning solution [3] uses an ensemble of three networks using two methods of data augmentation for training. The networks (nnUNet[5], Swin[4] and nnUNet of the 2021 winner [12]) were trained with the pipeline provided by the nnUNet. Data augmentation with GANs and registration was used to increase the amount of data available for training. The segmentation of meningiomas in radiotherapy (Task 3) is similar to the segmentation of gliomas in adults but differs from the type of tumour. Meningiomas are usually benign extra-axial tumours with different radiological and anatomical appearances and a tendency to multiplicity. The dataset used last year is very different from the one used this year, as last year there were four modalities (T2, T2/FLAIR, T1, and T1Gd), whereas this year only one modality (pre-radiation therapy planning brain MRI T1Gd) is present in the dataset. The segmentation has also changed, as this year only the gross tumour volume (GTV) was segmented. Although the datasets have differences, the tools used may be similar. In 2023, the winning team used a similar tool to nnUNet, the Auto3dseg [14], which was developed by Nvidia and is available in the MONAI library. Once again, a self-configuring U-Net-based solution shown very strong performance. Our approach this year (2024) aims to compare the default 3D full resolution nnUNet with the MedNeXt [16], as the latter provided better results on the adult glioma segmentation task [7], but there is no study on the effectiveness of this network on the adult glioma post-treatment and meningioma radiotherapy tasks. We also applied the same data augmentation with GANs used by [3]."
https://arxiv.org/html/2411.04630v1,Brain Tumour Removing and Missing Modality Generation using 3D Wavelet Diffusion Model,"This paper presents the second-placed solution for task 8 and the participation solution for task 7 of BraTS 2024. The adoption of automated brain analysis algorithms to support clinical practice is increasing. However, many of these algorithms struggle with the presence of brain lesions or the absence of certain MRI modalities. The alterations in the brain’s morphology leads to high variability and thus poor performance of predictive models that were trained only on healthy brains. The lack of information that is usually provided by some of the missing MRI modalities also reduces the reliability of the prediction models trained with all modalities. In order to improve the performance of these models, we propose the use of conditional 3D wavelet diffusion models. The wavelet transform enabled full-resolution image training and prediction on a GPU with 48 GB VRAM, without patching or downsampling, preserving all information for prediction. For the inpainting task of BraTS 2024, the use of a large and variable number of healthy masks and the stability and efficiency of the 3D wavelet diffusion model resulted in 0.007, 22.61 and 0.842 in the validation set and 0.07 , 22.8 and 0.91 in the testing set (MSE, PSNR and SSIM respectively). The code for these tasks is available at https://github.com/ShadowTwin41/BraTS_2023_2024_solutions.","Generative models are becoming increasingly explored as these models can significantly optimise productivity and efficiency, shorten development time and improve quality of deep learning solutions. Generative models, more specifically Generative Adversarial Networks (GANs) and Denoising Diffusion Probabilistic Model (DDPM), are the most advanced technologies capable of generating synthetic data indistinguishable from real data. In some cases, DDPMs have outperformed GANs, e.g., generating synthetic natural images [9]. DDPMs are able to successfully handle tasks such as reconstruction, denoising, image translation, classification among others. They can even be used for tasks in low data regime [1]. However, these models suffer from some limitations. Due to the use of the Markov chain, they are slow to train and generate new samples. They also require a lot of computational resources, which limits their use. Several efforts have been made to improve the results [24], reduce the computational effort [16], increase the training speed and reduce the inference time [26]. 1.1 Inpainting Inpainting is a reconstruction task that has been intensively researched due to the possibility of correcting noisy regions, removing unwanted objects or even replacing a region with an object. Inpainting in natural images has already been extensively researched [22, 17], but some problems arise when applying it to medical data. The dataset used to train the DDPM may be insufficient to avoid overfitting, leading to poor generations where the generated data may contain multiple artefacts that do not correspond to reality. It could also produce always the same result and therefore not be able to adapt successfully to new cases. In addition, medical data is usually larger than natural images, especially magnetic resonance imaging (MRI) and computed tomography (CT) scans, which have an extra dimensionality, that increases the complexity of training and the amount of resources required. Since 2023, the Brain Tumour Segmentation (BraTS) Challenge has set a task that challenges the community to present their best algorithm for the inpaiting task. The BraTS datasets contain brain tumours that are properly fully segmented, which is useful for training segmentation models. However, there are a number of algorithms that are not suitable for MRI scans of brains with tumours e.g. brain parcellation algorithms. An algorithm capable of removing these lesions would therefore allow these algorithms to be used without modification. As mentioned by the organisers, this would allow a better understanding of the relationship between different brain tumour regions and abnormal brain tissue. Brain modelling would also benefit from these findings [12, 13]. In the 2023 edition, [10] developed a conditional 2D DDPM in which only the slices with the region to be painted over are processed. The model is conditioned by the masked MRI slices and the binary mask in each step of the denoising phase. However, the use of a full 2D DDPM leaded to stripe artifacts. To solve this problem, [11] evaluated 6 distinct methods for healthy brain tissue inpainting, namely DDPM 2D slice-wise [11], DDPM 2D seq-pos [11], DDPM Pseudo3D [28], DDPM 3D mem-eff [6], LDM 3D [19] and 3D WDM [16]. The best model (DDPM Pseudo3D) achieved the best results in the test set, i.e. 0.0103, 20.9258, 0.8527 for mean absolute error (MSE), peak-signal-noise-ratio (PSNR) and similarity index measure (SSIM) respectively. DDPM Pseudo3D is characterised by the use of 2D convolutional layers followed by 1D convolutions in the z-axis, which allows for better continuity between layers and results in an efficient pseudo-3D network. All networks can be trained on a GPU with 48 GB VRAM, with the exception of DDPM 3D mem-eff, which requires more than 78 GB and does not achieve the same results as DDPM Pseudo3D. 3D WDM is one of the networks capable of processing the image at full resolution with the lowest sampling time and memory consumption. However, this network produced very poor results (0.1060, 10.5693, 0.6074 for MSE, PSNR and SSIM respectively) probably due to the chosen pipeline for training and inpainting as well as the short training time. Zhu et al. [29] suggests the use of three distinct networks, namely pGAN [8], ResViT [7], and Palette [25]. pGAN and ResViT are conditional GANs and Palette is a conditional diffusion model. Although these networks were adapted and were able to achieve good results, they were not able to achieve better results than the baseline (3D Pix2Pix [18]). 1.2 Missing MRI MRI scans are one of the main imaging technique for detection and segmentation of brain tumours. Generally, four modalities are acquired in order to produce the best clinical decision, namely T1-weighted with and without contrast enhancement, T2-weighted images, and FLAIR. Several automatic tools require the input of all modalities to work properly. However, obtaining MRI data is very time-consuming and expensive. Patients who suffer from claustrophobia, have a mental disorder, or move too much during the procedure are challenging to scan, often resulting in data with artifacts. For some patients with metal implants, it may even be impossible. Often, in real clinical practice is difficult to obtain all MRI modalities, which results in missing MRI sequences. Therefore, since 2023 the BraTS challenge also contains a task for Brain MRI Synthesis for Tumour Segmentation (BraSyn). The objective of this task is to generate the missing modality having the other three as reference. For this, researchers are invited to develop automatic image-to-image translation tools. With such a tool, it is possible to reduce acquisition time, or generate the missing modality to use in other algorithms to improve the detection and segmentation. Pix2Pix [18] is one of the most frequently used approaches to fulfil this task. [27] used an edge-aware GAN (Ea-GANs) which integrated edge information to reduce the slice discontinuity and increase sharpness, which are problematic in several medical tasks that use conditional GAN models. Ea-GANs, which is based on the Pix2Pix model, uses T1 scans to generate T2 and FLAIR scans. In 2023 BraTS edition, [5] explored the use of Pix2Pix optimised with several distinct loss functions. Loss functions based on pixel-to-pixel similarities (e.g., mean absolute error and similar), adversarial loss, structural SSIM, frequency domain consistency, and latent feature (VGG-based perceptual) consistency. The submitted solution was the combination of all losses, which resulted in the best solution among the participating teams. We propose the use of conditional 3D WDM [16] for tasks 7 BraTS Synthesis (Global) - Missing MRI [21] and 8 Synthesis (Local) - Inpainting [20]. Our experiments have shown that using several different healthy regions for training, and a distinct scheduler and pipeline for sampling resulted in a strong and efficient solution."
https://arxiv.org/html/2411.04620v1,Multi-temporal crack segmentation in concrete structure using deep learning approaches,"Cracks are among the earliest indicators of deterioration in concrete structures. Early automatic detection of these cracks can significantly extend the lifespan of critical infrastructures, such as bridges, buildings, and tunnels, while simultaneously reducing maintenance costs and facilitating efficient structural health monitoring. This study investigates whether leveraging multi-temporal data for crack segmentation can enhance segmentation quality. Therefore, we compare a Swin UNETR trained on multi-temporal data with a U-Net trained on mono-temporal data to assess the effect of temporal information compared with conventional single-epoch approaches. To this end, a multi-temporal dataset comprising 1356 images, each with 32 sequential crack propagation images, was created. After training the models, experiments were conducted to analyze their generalization ability, temporal consistency, and segmentation quality. The multi-temporal approach consistently outperformed its mono-temporal counterpart, achieving an IoU of 82.72%percent82.7282.72\%82.72 % and a F1-score of 90.54%percent90.5490.54\%90.54 %, representing a significant improvement over the mono-temporal model’s IoU of 76.69%percent76.6976.69\%76.69 % and F1-score of 86.18%percent86.1886.18\%86.18 %, despite requiring only half of the trainable parameters. The multi-temporal model also displayed a more consistent segmentation quality, with reduced noise and fewer errors. These results suggest that temporal information significantly enhances the performance of segmentation models, offering a promising solution for improved crack detection and the long-term monitoring of concrete structures, even with limited sequential data.","Infrastructures such as highways, roads, dams, harbors, and buildings are essential for the sustainability and efficiency of economic activities, ensuring public safety, and facilitating social interactions (Zhou et al.,, 2023). Many of these infrastructures are constructed from materials such as concrete, asphalt, or stone and face various challenges, including fatigue stress, cyclic loading, and the cumulative effects of time, along with increasing human and environmental pressure (Mohan and Poobal,, 2018; König et al.,, 2022; Kheradmandi and Mehranfar,, 2022). These factors can compromise structural integrity (Hamishebahar et al.,, 2022) and diminish the asset value of these constructions (Kheradmandi and Mehranfar,, 2022). Consequently, there is a pressing need for effective Structural Health Monitoring (SHM) to maintain and prolong the lifespan of critical structures (Hamishebahar et al.,, 2022). This helps reduce maintenance and repair costs (Kheradmandi and Mehranfar,, 2022; König et al.,, 2022) and prevents damage to humans and the environment (Mohan and Poobal,, 2018). Numerous structures worldwide are in poor condition and require continuous monitoring and damage detection systems. For instance, German seaports feature approximately 3000⁢km3000km3000\,\text{km}3000 km quay walls and 2500⁢km2500km2500\,\text{km}2500 km facilities along the federal waterways. Throughout 140⁢km140km140\,\text{km}140 km of these quay walls, approximately 279279279279 million tons of goods are handled, underscoring the significant role that these structures play in the German economy. However, the current state of quay walls and facilities is a concern. According to the Federal Waterways Engineering and Research Institute (BAW), 70%percent7070\%70 % of quay walls and facilities on waterways are in adequate condition. This situation foreshadows the considerable maintenance efforts required to ensure continued transportation of essential goods on German waterways (Alamouri et al.,, 2024). Looking across the Atlantic, a 2022 report highlighted that the United States has 2000200020002000 dams and over 46 0004600046\,00046 000 bridges with structural deficits (König et al.,, 2022), and 11%percent1111\%11 % of roads are classified as being in poor or mediocre conditions (Zhou et al.,, 2023). Similarly, in the United Kingdom, approximately 10%percent1010\%10 % of roads are disrepaired (König et al.,, 2022). These examples illustrate the significant global need for infrastructure assessment and maintenance. One of the earliest and most common indicators of structural degradation and reduced structural integrity is the presence of cracks on the surfaces of structures (König et al.,, 2022; Hamishebahar et al.,, 2022). Cracks begin at the microscopic level and continuously reduce the local stiffness of materials and create material discontinuities (Mohan and Poobal,, 2018). If left untreated, these cracks grow in size, and the cost and effort required to repair them increase accordingly. To mitigate consequential safety issues, it is crucial for SHM to precisely assess the state of a structure and identify indicators of future damage such as cracks König et al., (2022). Traditionally, manual visual assessment by an inspector has been the standard method for identifying cracks. However, new machine learning (ML) techniques, particularly deep learning (DL), have begun to change this landscape. Manual assessments are costly, labor-intensive, time-consuming, and require highly trained experts. Despite this training, human factors can lead to subjective results (Hamishebahar et al.,, 2022; Mohan and Poobal,, 2018; Maboudi et al.,, 2021). For instance, inspectors may experience fatigue during assessments, exhibit inconsistencies in their evaluations, or receive inadequate training (König et al.,, 2022). Moreover, the manual inspection can pose hazards to inspectors owing to unsafe structures (Kheradmandi and Mehranfar,, 2022). In some instances, the locations requiring inspection may be inaccessible to humans (König et al.,, 2022), or the inspection process could result in downtime for the structure (König et al.,, 2022; De Arriba López et al.,, 2024), leading to interruptions in traffic on roads, bridges, or tunnels (Kheradmandi and Mehranfar,, 2022). Given these disadvantages, automatic crack detection methods are required. Computer vision (CV) techniques, in conjunction with ML/DL, have proven to be effective for this purpose. In this subfield, image-based approaches have emerged as the most cost-effective methods because of the widespread availability of cameras (Hamishebahar et al.,, 2022). When mounted on UAVs, these systems can reach otherwise inaccessible locations without interrupting structure use (Mohan and Poobal,, 2018). To the best of the authors’ knowledge, as stated by König et al. (2022), there is a lack of research regarding the use of multi-temporal data for CV tasks related to crack detection. In this context, we focus on the semantic segmentation of cracks using multi-temporal data, which is compared to approaches that utilize mono-temporal data. In that regard, a multi-temporal dataset and a corresponding deserialized mono-temporal dataset were created. Moreover, we aim to address the research gap in multi-temporal crack propagation data for semantic segmentation. Additionally, the development of a multi-temporal dataset can provide a foundation for future research involving multi-temporal data. The contributions of this study can be summarized as follows: • Development of mono- and multi-temporal models for crack segmentation. • Assessment of mono- and multi-temporal models based on performance metrics."
https://arxiv.org/html/2411.04607v1,Cross- and Intra-image Prototypical Learning for Multi-label Disease Diagnosis and Interpretation,"Recent advances in prototypical learning have shown remarkable potential to provide useful decision interpretations associating activation maps and predictions with class-specific training prototypes. Such prototypical learning has been well-studied for various single-label diseases, but for quite relevant and more challenging multi-label diagnosis, where multiple diseases are often concurrent within an image, existing prototypical learning models struggle to obtain meaningful activation maps and effective class prototypes due to the entanglement of the multiple diseases. In this paper, we present a novel Cross- and Intra-image Prototypical Learning (CIPL) framework, for accurate multi-label disease diagnosis and interpretation from medical images. CIPL takes advantage of common cross-image semantics to disentangle the multiple diseases when learning the prototypes, allowing a comprehensive understanding of complicated pathological lesions. Furthermore, we propose a new two-level alignment-based regularisation strategy that effectively leverages consistent intra-image information to enhance interpretation robustness and predictive performance. Extensive experiments show that our CIPL attains the state-of-the-art (SOTA) classification accuracy in two public multi-label benchmarks of disease diagnosis: thoracic radiography and fundus images. Quantitative interpretability results show that CIPL also has superiority in weakly-supervised thoracic disease localisation over other leading saliency- and prototype-based explanation methods.","The last decade has witnessed the success of deep learning [1, 2], e.g., deep neural networks (DNNs), in supporting disease diagnosis and treatment in healthcare, including disease classification [3, 4, 5, 6], lesion detection [7, 8], and organ segmentation [9, 10, 11]. Typically, DNNs require intricate network architectures, high-dimensional feature representations, and a massive number of trainable network parameters, posing significant challenges in interpreting the internal mechanisms of the model and gaining insights into how it makes predictions. As a result, DNNs are often considered as black-box models [12]. However, in safety-critical healthcare applications with profound implications for human lives, there is a growing demand to utilise interpretable deep-learning disease diagnosis algorithms that can be effortlessly comprehended and trusted by human experts [12, 13]. Most existing disease diagnosis algorithms attempt to improve model interpretability or transparency with saliency maps that are typically based on post-hoc explanations [14] (e.g., CAM and its variants [15], visual attentions [16], counterfactual example [17], and occlusion sensitivity [18]). However, these methods have many drawbacks, such as: 1) saliency maps only give a spatial importance of image regions related to the model’s predictions but they do not explain how those image regions are used for the predictions, which are not enough to interpret the model’s inner working [12, 13]; 2) saliency maps could be essentially the same for different classes, as evidenced in [12]; and 3) post-hoc explanations are often unreliable or even misleading since they are not involved in the model’s training to penalise explanation errors, unless additional regularisation training objectives [19, 20] are applied to constrain the saliency maps. Figure 1: In multi-label learning, (a) conventional prototypical learning strategy directly learns class prototypes from entangled multiple diseases present in training samples, by considering only individual-image information; (b) our cross-image prototypical learning strategy leverages common semantics of paired images to learn class prototypes from disentangled multiple diseases; (c) our intra-image prototypical learning strategy exploits consistent cues between paired augmented views of an image for regularising both interpretations and predictions. These drawbacks are mitigated with model interpretability by prototypical learning [21, 22], as demonstrated in Fig. 1(a), which leverages self-explaining models [23, 24, 25]. The basic notion behind this strategy is that the model is trained to automatically learn class-specific prototypes (image-level [21], region-level [23], or pixel-level [25, 26]) from training samples. Then, the prediction interpretation is performed by comparing a testing sample with the learned prototypes, which aiding in interpreting the model’s inner workings. Such a self-explaining strategy relies on train-test sample associations for decision making, which resembles how humans reason according to cognitive psychological studies [27, 28] revealing that humans use past cases as models when learning to solve problems. Because of this intuitive nature, recent studies successfully leveraged it for single-label classification problems, i.e., diagnosis and interpretation of the breast cancer [29], diabetic retinopathy [30], and Alzheimer’s disease [31], where class-specific prototypes are clearly learned from the single-label training samples of the corresponding class. However, the application of the prototype-based interpretation strategy to multi-label classification problems faces critical challenges, mostly due to the complexity of multi-label samples, which inherently intertwine representative disease features from multiple classes. Such complexity presents significant obstacles to the effective learning of the class-specific prototypes, resulting in prototypes that are highly entangled with multi-class disease features. In this paper, we address the problem of multi-label disease diagnosis and interpretation via Cross- and Intra-image Prototypical Learning (CIPL). The overall motivation of our CIPL method is illustrated in Fig. 1. Compared with the conventional prototypical learning strategy in Fig. 1(a) that merely learns class prototypes from multi-label training images containing entangled disease features of multiple classes, we propose to leverage the common cross-image semantics of paired images to disentangle the multi-class disease features with a co-attention mechanism, as shown in Fig. 1(b). In this way, our CIPL method enables the class prototypes to gain a comprehensive understanding of pathological lesions for each disease class. Moreover, to obtain robust interpretation and improved classification, our CIPL further exploits consistent and meaningful intra-image cues between different views of an image, as demonstrated in Fig. 1(c). This is achieved through a two-level alignment-based paradigm, which regularises the model interpretations and predictions in a self-supervised fashion. Generally, the cross-image and and intra-image strategies can complementarily contribute to the effective learning of class prototypes from multi-label training samples. In summary, our main contributions are listed as follows: 1. We present the CIPL method to address the multi-label disease diagnosis and interpretation with prototypical learning. CIPL leverages rich cross-image semantics and consistent intra-image cues to complementarily facilitate the prototype learning from multi-label images. 2. We propose a novel multi-disease disentangling strategy by discovering common cross-image semantics between multi-label samples, enabling the class prototypes to comprehensively understand the pathological lesions. 3. We introduce a new regularisation paradigm based on two-level alignments which exploit consistent intra-image information to promote both interpretation robustness and predictive performance. Extensive experiments on two public multi-label benchmarks reveal that our CIPL achieves the state-of-the-art (SOTA) disease diagnosis and interpretation results."
https://arxiv.org/html/2411.04598v1,Social EgoMesh Estimation,"Accurately estimating the 3D pose of the camera wearer in egocentric video sequences is crucial to modeling human behavior in virtual and augmented reality applications. The task presents unique challenges due to the limited visibility of the user’s body caused by the front-facing camera mounted on their head. Recent research has explored the utilization of the scene and ego-motion, but it has overlooked humans’ interactive nature. We propose a novel framework for Social Egocentric Estimation of body MEshes (SEE-ME). Our approach is the first to estimate the wearer’s mesh using only a latent probabilistic diffusion model, which we condition on the scene and, for the first time, on the social wearer-interactee interactions. Our in-depth study sheds light on when social interaction matters most for ego-mesh estimation; it quantifies the impact of interpersonal distance and gaze direction. Overall, SEE-ME surpasses the current best technique, reducing the pose estimation error (MPJPE) by 53%. The code is available at SEEME.","(a) (b) Figure 1: (Left) Frame from the input egocentric video stream. We experience the immersive subjective perspective of the front-facing camera wearer, but the wearer is behind the wearable and, therefore, invisible. Still, we recognize the points of interest of the wearer, parts of the scene where the action happens, and, most importantly, the interactee engaged in communication with the wearer. (Right) Third-view reconstruction of the ego mesh of the camera wearer by our proposed SEE-ME. Our vantage point reveals the surrounding environment, featuring a sofa and a person from an overhead perspective, leading us to infer the wearer’s likely standing position. Specifically, vicinity and gaze interactions are important cues for our reconstruction as we experimentally quantify. Estimating the 3D motion of a person from an egocentric video sequence is a critical task with diverse applications in virtual reality (VR) and augmented reality (AR). This research is motivated by the necessity to realistically depict the entire body to enable total immersion in these environments [7]. The user typically wears a camera in such videos, capturing their surroundings from a first-person perspective. Large field of view (FOV) cameras can capture some body parts (hands, feet), which standard FOV cameras cannot, as illustrated from the first-person perspective in Figure 1. The task, dubbed egopose estimation, is influenced by the type of camera used to record the sequences. Top-down head-mounted cameras have the best view of the wearer [47] but are intrusive due to the required displacement from the face. Front-facing cameras solve the ergonomic issue, but the downside is that the wearer is almost always invisible except when using very large FOV. In [16], the authors introduce a two-stage framework for fish-eye head-mounted cameras, where a traditional SLAM algorithm [43] provides the camera rotation and translation. Simultaneously, a neural network exploits visible body parts to predict the egopose. Recently, EgoEgo [25] proposed an updated pipeline, relaxing the assumption on the large FOV. However, they still rely on (deep learning-based) SLAM [44] to estimate the head’s rotation and translation, fed to a generative model [12] to synthesize the pose for the rest of the body. These approaches exploit the video to extract the head pose, ignoring the surrounding environment and the other actors in the scene that can provide essential cues for the egopose estimation. For this reason, we propose a new social egocentric mesh estimation task and contribute with a novel probabilistic framework for Social Egocentric Estimation of body MEshes (SEE-ME). Humans have a social nature and are often involved in social activities in real and virtual worlds, accomplishing tasks that usually require cooperation and coordination. You2Me [33] explicitly captures the interplay between two persons from a chest-mounted camera in a controlled environment where two people interact by conversing, playing hand games, or doing sports. However, their approach does not extract high-level scene information and predicts deterministic skeleton-like representation that may not be suitable for AR/VR applications. In contrast with our work, [33] employs a person-centric coordinate system and does not estimate global orientation or translation, while our approach enables end-to-end learning of both of them. [33] also relies on OpenPose [4] to extract the interactee, which faces challenges in accurately estimating occluded keypoints [26]. We exploit EgoHMR [58] to estimate the interactee’s mesh, demonstrating robustness against occlusions, using scenes as input. In contrast, our approach leverages the scene and interactions to understand the invisible camera wearer’s pose, with no input body cues required. The motivation behind our research stems from the wearer’s perspective, which highlights both their surroundings and an overlooked aspect: the presence of another actor within the scene (see Figure 1). Our unified approach links the generation of the egomesh to both the 3D scene depiction and the wearer’s social interaction with the interactee. We do not rely on SLAM for the head’s orientation or translation but predict them with our model. Our approach pioneers conditional Latent Diffusion in egocentric human mesh estimation, which provides a notable speedup in generation times. We build the latent space encoding human poses using a Variational Autoencoder (VAE) [20]. Subsequently, we perform conditional diffusion on this latent space. The conditioning strategies aim to guide the process by modeling the 3D point cloud of the scene in which the wearer moves and the estimated mesh of the interactee recovered from the egocentric video feed. Our model depends on the 3D scene and other actors’ poses. Therefore, provided the wearer’s pose, we can predict the interactee’s one even if it is not paired with an egocentric video stream. To assess the performance of our approach, we evaluate it on EgoBody [59]: the solely available dataset that features multiple individuals, an egocentric perspective, and an environment. We reach state-of-the-art results, establishing the efficacy of environmental and social components. In our research, we thoroughly examine the influence of social interactions on estimating the wearer’s egomesh. We establish when this conditioning strategy has the highest impact. We use proxies for social interaction that can be extracted from our setup. We demonstrate how proximity and eye contact between the wearer and the interactee bring the most from our conditioning. Moreover, we study the effect of future knowledge about the interactee’s motion, as we humans use the experience to predict and react to the future movements of the people around us. Our framework, exposed to this information, provides a significant performance boost. To corroborate the strength of our approach, we evaluate our model on the dataset GIMO [61], which is egocentric and includes the environment but is not multi-person. To summarize our contributions: • We propose the task of social egomesh estimation. Emphasizing the role of other actors in 3D scenes to account for the lack of information about the camera wearer. • We introduce SEE-ME, a unified framework based on latent diffusion that achieves state-of-the-art performance in predicting the wearer’s pose and is also capable of predicting the interactee’s pose. • We perform an in-depth ablation study to highlight the scenarios in which modeling social interactions brings the most benefits."
https://arxiv.org/html/2411.04596v1,The Impact of Semi-Supervised Learning on Line Segment Detection,"In this paper we present a method for line segment detection in images, based on a semi-supervised framework. Leveraging the use of a consistency loss based on differently augmented and perturbed unlabeled images with a small amount of labeled data, we show comparable results to fully supervised methods. This opens up application scenarios where annotation is difficult or expensive, and for domain specific adaptation of models. We are specifically interested in real-time and online applications, and investigate small and efficient learning backbones. Our method is to our knowledge the first to target line detection using modern state-of-the-art methodologies for semi-supervised learning. We test the method on both standard benchmarks and domain specific scenarios for forestry applications, showing the tractability of the proposed method. Code: https://github.com/jo6815en/semi-lines/","Ground Truth Fully Supervised Fully Supervised Proposed (1/16 data) (All data) (1/16 labeled data) Figure 1: From left to right: Ground truth detection, supervised model on 1/16 of data, supervised model on all data, proposed semi-supervised approach with 1/16 labeled data. The semi-supervised method improves significantly on the supervised method. Line segments are prevalent in images depicting scenes of both in- and outdoor human-made environments as well as in nature. Line structures are used by humans for efficient and robust interpretation of the world [8, 4], and can also play a crucial role in automatic scene comprehension, encapsulating scene structure efficiently [42]. Applications and down stream tasks that are made more tractable encompass various computer vision tasks, including 3D reconstruction, Structure-from-Motion (SfM) [17, 26, 27], Simultaneous Localization and Mapping (SLAM) [12], visual localization [10], tracking, and vanishing point estimation [32]. Lines can also be efficient intermediate representations for subsequent semantic processing. Compared to point based features, line features are more stable over time (this is true both on a small and large time scale) and more invariant to changes in scene environment such as e.g. lighting, season and weather conditions as shown in [30]. Although lines are perhaps mostly associated with man-made structures, there are many examples of linear structures in nature. In this paper we use, as a domain specific scenario, forestry images. Detecting lines in tree imagery is challenging, and many applications related to them are of particular importance. From an environmental stand-point trees can contribute with both carbon-dioxide binding and new sustainable materials in diverse industries. For many automation use cases involving forestry we would like to monitor, measure and interpret forestry scenes, which translates to both semantic computer vision tasks such as classification and scene understanding and geometric computer vision tasks such as camera pose estimation and tracking. DeepLSD LETR M-LSD Figure 2: Many trained models fail to generalize to new image domains. Top row shows the output from left to right: DeepLSD [28], LETR [36], M-LSD [14], trained on the Wireframe dataset. Bottom row shows the output from these models on an out of domain image. Many newly formulated learned line segment detectors have shown both promising and impressive results [28, 36, 14]. However, many such large models fail to transition and generalize to new image domains without retraining, see Fig. 2. To tackle the expensive need for annotations and in contrast to large general and slower models, we propose a method for semi-supervised line segment detection. Our method allows for domain specific and light-weight processing. At the same time, our use of semi-supervision enables better generalization and generates a method that better adapts to new scenes. We build on the ideas of semi-supervised learning for semantic segmentation from [31] and [39]. Combining these ideas with a compact line segment model [14] we propose the first framework for semi-supervised line segment detection. This results in a fast, efficient, and accurate system that is trained on a smaller portion of the whole dataset and performs as well or even better than the standard supervised method. As there are no other semi-supervised frameworks for line segment detection, we perform comparison between our models and the supervised version of our model. Exploring the fundamental possibility of gaining information and boosting the performance with a lower amount of annotated data. We do however include the performance of pre-trained DeepLSD [28] and LETR [36] models for reference. We show that our method learns consistency information from unlabeled images together with task specific information from annotated images. For our forestry use case, only having annotations of half the dataset but using our method results in 28% higher score in sAP10 compared to using a fully supervised method with all the data annotated. Our main contributions are: 1. We present the first framework for semi-supervised learning for line segment detection. 2. We show that semi-supervised methods work well for line segment detection, improving significantly on the state-of-the-art for new environments, where there is little annotation. 3. We show that small models, suitable for real-time applications, work well and can heavily benefit from semi-supervised methods. Additionally we provide two new datasets for line segment detection containing forest scenes with ground truth lines. We will publicly release and publish all code and data."
https://arxiv.org/html/2411.04586v1,On the Inherent Robustness of One-Stage Object Detection against Out-of-Distribution Data,"Robustness is a fundamental aspect for developing safe and trustworthy models, particularly when they are deployed in the open world. In this work we analyze the inherent capability of one-stage object detectors to robustly operate in the presence of out-of-distribution (OoD) data. Specifically, we propose a novel detection algorithm for detecting unknown objects in image data, which leverages the features extracted by the model from each sample. Differently from other recent approaches in the literature, our proposal does not require retraining the object detector, thereby allowing for the use of pretrained models. Our proposed OoD detector exploits the application of supervised dimensionality reduction techniques to mitigate the effects of the curse of dimensionality on the features extracted by the model. Furthermore, it utilizes high-resolution feature maps to identify potential unknown objects in an unsupervised fashion. Our experiments analyze the Pareto trade-off between the performance detecting known and unknown objects resulting from different algorithmic configurations and inference confidence thresholds. We also compare the performance of our proposed algorithm to that of logits-based post-hoc OoD methods, as well as possible fusion strategies. Finally, we discuss on the competitiveness of all tested methods against state-of-the-art OoD approaches for object detection models over the recently published Unknown Object Detection benchmark. The obtained results verify that the performance of avant-garde post-hoc OoD detectors can be further improved when combined with our proposed algorithm.","The rapid advancement and widespread adoption of Artificial Intelligence (AI) systems for real-world applications have underscored the urgent need for these models to be safe and trustworthy [1]. What trustworthiness means for AI is a highly debated concern in the recent years, attracting significant interest from the research community. A major breakthrough in shedding light on trustworthy AI was the publication of the “Ethical Guidelines for Trustworthy AI” [2] in 2019 by the European Union, a regulatory actor of relevance in this matter. The text outlines seven key requirements for trustworthy AI and identify three essential pillars for fulfilling these requirements. The technical robustness and safety of an AI system is recognized as one of the seven requirements and a fundamental pillar of trustworthiness [3]. More recently, another important actor in the regulatory landscape, the National Institute of Standards and Technology (NIST) has defined in [4] the technical robustness as a system’s capacity to sustain its performance across diverse conditions. It entails not only consistent functioning under anticipated scenarios, but also the ability to minimize potential risks to individuals when operating in environments subject to unexpected events. For this purpose, ensuring the safety of Machine Learning (ML) models involves developing robust systems capable of handling unknown semantics, effectively distinguishing between known and unknown data instances. When it comes to object detection tasks from image data, these models must be able to navigate open-world environments by discerning between background (irrelevant information), known objects (relevant information within the training distribution), and unknown objects (relevant information outside the training distribution). In this context, substantial efforts have been directed towards the field of Open World Object Detection (OWOD) [5]-[8]. The goal pursued in this research area is to develop models capable of detecting unknown objects and incrementally learning new categories over time. Predominantly, research in this area has concentrated on methodologies that require retraining two-stage object detection models, such as Faster R-CNN. Different retraining strategies are designed to endow the learned model with the capability to discern a broader variety of objects (including those not present in the training data), such as continual learning approaches or loss functions related to the presence of objects (objectness). Conversely, pretrained object detection models require less computational power when compared to retrained models because they are optimized on a fixed dataset of known classes. However, this efficiency comes at the cost of increased bias towards detecting only familiar objects, making them less suitable for open-world scenarios where new, unseen objects may appear. This manuscript contributes to understanding the robustness of pretrained object detection models in detecting unknown objects in image data. We depart from our hypothesis that, without retraining, single-stage object detection models can inherently detect unknown objects. To explore this, we introduce a simple OoD detection algorithm based on neural activations (feature maps) of pretrained models. Additionally, we present an unsupervised learning method that leverages feature maps to enhance recall for unknown objects. Our extensive experimental setup evaluates the algorithm across different configurations and parameter choices, assessing its effectiveness in identifying known and unknown objects compared to logits-based OoD methods and retrained object detection models. We further explore potential improvements through a fusion of tested methods, inspired by findings in [9]. Finally, we compare our approach with state-of-the-art OWOD methods, showing that the proposed algorithm achieves superior detection scores in the benchmark without requiring model retraining. The remainder of the paper is organized as follows: Section 2 revisits OoD detection techniques for object detection, along with a brief summary of the most influential works in OWOD and a detailed statement of the contribution brought by this work to the related literature. Section 3 presents and describes our algorithm for detecting unknown objects111Throughout this work “unknown object” and “OoD object” are used interchangeably. in images, beginning with an overview of its compounding algorithmic steps, and followed by a detailed description of the method. This section also introduces two techniques aimed at enhancing the overall performance of the algorithm. The experimental setup is outlined in Section 4, whereas the results are analyzed and discussed in Section 5. Finally, Section 6 concludes the manuscript with a summary of the main findings and an outlook towards potential research paths to be addressed in the future."
https://arxiv.org/html/2411.04584v1,PASSION for Dermatology: Bridging the Diversity Gap with Pigmented Skin Images from Sub-Saharan Africa,"Africa faces a huge shortage of dermatologists, with less than one per million people. This is in stark contrast to the high demand for dermatologic care, with 80% of the paediatric population suffering from largely untreated skin conditions. The integration of AI into healthcare sparks significant hope for treatment accessibility, especially through the development of AI-supported teledermatology. Current AI models are predominantly trained on white-skinned patients and do not generalize well enough to pigmented patients. The PASSION project aims to address this issue by collecting images of skin diseases in Sub-Saharan countries with the aim of open-sourcing this data. This dataset is the first of its kind, consisting of 1,653 patients for a total of 4,901 images. The images are representative of telemedicine settings and encompass the most common paediatric conditions: eczema, fungals, scabies, and impetigo. We also provide a baseline machine learning model trained on the dataset and a detailed performance analysis for the subpopulations represented in the dataset. The project website can be found at https://passionderm.github.io/.","Access to adequate health care remains a global challenge, largely due to a shortage of health workers [18]. This is particularly acute in Africa, which is the continent with the lowest number of health workers per patient, with a shortage of up to 2.4 million doctors and nurses [18]. In addition, Africa accounts for 25% of the global demand for health workers, while having only 1.3% of the workforce [18]. This phenomenon spans across all medical specialities. In the case of dermatology, many African countries have less than one dermatologist per million patients [17], making skincare inaccessible to the majority of people. However, the prevalence of skin conditions can be as high as 87% in the paediatric population [14], a situation that naturally extends to the adult population. The consequences of skin diseases are far-reaching and can be physical, psychological and socio-economic, especially in the case of chronic diseases [32]. Teledermatology is a solution that has already been attempted in sub-Saharan Africa [29] to address the lack of access to healthcare. However, it does not increase the number of specialists able to provide primary care. Artificial Intelligence (AI) holds great promise for scalable diagnosis support, and facilitating access to proper medication and treatment. Although the field of medical imaging has made significant advances in recent years [20], a major shortcoming is that publicly available dermatological datasets are significantly biased towards lighter skin tones [4, 2]. This bias persists even in private databases, which primarily include local patients, while images of individuals from diverse geographical origins are conspicuously absent [31, 2]. Such a pronounced skew in pigmentation levels does not accurately reflect the diversity of the global population, particularly those without access to traditional dermatologic care. Moreover, common ailments in Africa —namely eczematous dermatitis, fungal infections, bacterial infections, and scabies [21, 14]— differ significantly in prevalence from those in Europe [22] and North America [15], which are the usual sources of patient data for public datasets. Efforts were made to address this disparity, but existing datasets are often small [4] or fail to capture the spectrum of prevalent skin conditions in Africa, particularly Neglected Tropical Diseases (NTDs) [4]. This paper aims to fill a critical gap in dermatological research by collecting a comprehensive and inclusive dataset from regions where access to dermatologists is difficult. To achieve this goal, a prospective collection of dermatological cases in sub-Saharan Africa was initiated in 2020. Several countries have collaborated in a global effort to create this dataset, which is intended to serve not only as a valuable resource for training machine learning algorithms and evaluating existing models in these underrepresented populations, but also as a resource for training medical personnel and helping to enable AI-driven teledermatology systems. The dataset consists of patient images, with phototypes \Romannum4, \Romannum5 and \Romannum6 on the Fitzpatrick skin type (FST) scale, acquired in conditions similar to teledermatology consultation, and representative of the most common skin conditions in sub-Saharan Africa. These conditions cover up to 80% of the conditions seen in the paediatric population [25]. During the data collection process, substantial effort was devoted to including the paediatric population, as this is the demographic group most affected by skin disease. This results in a dataset that represents the diversity observed in these countries and effectively covers the dermatological conditions of children. Additionally, we provide a baseline machine learning model with a corresponding in-depth performance analysis for subpopulations, showcasing evaluation possibilities with this dataset. By releasing the PASSION dataset, we aim to contribute to the advancement of dermatological research and the equity of healthcare solutions."
https://arxiv.org/html/2411.04571v1,DomainGallery: Few-shot Domain-driven Image Generation by Attribute-centric Finetuning,"The recent progress in text-to-image models pretrained on large-scale datasets has enabled us to generate various images as long as we provide a text prompt describing what we want. Nevertheless, the availability of these models is still limited when we expect to generate images that fall into a specific domain either hard to describe or just unseen to the models. In this work, we propose DomainGallery, a few-shot domain-driven image generation method which aims at finetuning pretrained Stable Diffusion on few-shot target datasets in an attribute-centric manner. Specifically, DomainGallery features prior attribute erasure, attribute disentanglement, regularization and enhancement. These techniques are tailored to few-shot domain-driven generation in order to solve key issues that previous works have failed to settle. Extensive experiments are given to validate the superior performance of DomainGallery on a variety of domain-driven generation scenarios. Codes are available at https://github.com/Ldhlwh/DomainGallery.","Walking down the street, you see an artist painting portrait sketches for people. You are fascinated by a couple of masterpieces set by the side, showing his/her unique painting style which you find it difficult to describe by words. Deeply interested, you are in the mood for seeing more sketches, and it would be perfect to see him/her painting other things like dogs, especially your favorite ones at home. As a fundamental topic in computer vision, image generation has been attracting enormous research efforts. However, through the years from VAEs [23], GANs [13] to diffusion models [16], generative models are becoming more and more data-hungry in order to properly model the distribution of images, as the most recent Stable Diffusions [35] have been trained on billions of text-image pairs [39]. Thus unfortunately, it is usually infeasible to directly train a generative model given only few-shot (around ten, or fewer) images of a specific target domain. To tackle such challenging scenarios, one paradigm of solutions is model transfer, which first trains a model on a relevant source domain and then transfers it to the target domain by finetuning on the few-shot target dataset. Nevertheless, as Zhao et al. [56] have pointed out, the performance of model transfer methods will be significantly influenced by the relevance between source/target domains. Therefore, the applicability of these methods will be limited if we either fail to find a proper source dataset, or just do not have enough resources to train a generative model from scratch. With the recent progress in pretrained text-to-image (T2I) models [32, 33, 28, 38, 52, 35, 31], it seems that anything can be generated simply by putting a text prompt into an off-the-shelf pretrained T2I model. However, T2I models are still far from once for all solutions to image generation. Sometimes it is difficult or even impossible to precisely describe certain styles (e.g. sketches by an artist) and contents (e.g. new concepts or personalized subjects), or what we want is simply unseen (thus unknown) to the model. Fortunately, T2I models can serve as universal source models to be finetuned on specific target datasets. Recent works finetuning T2I models have mostly focused on either finetuning with relatively abundant images (tens, hundreds, or more) [11], or few-shot subject-driven generation whose datasets consist of a single person or object [12, 37]. On the contrary, few-shot domain-driven generation analogous to the conventional model transfer has rarely been explored. In this work, we analyze and perform few-shot domain-driven image generation from the view of attributes, as a domain is defined by common attributes shared among images (see Sec. 3). We seek to master four generation cases as illustrated in Fig. 1: Intra-category: The generated images contain both the domain attributes and the categorical attribute of the given target dataset, as in conventional model transfer; Cross-category: While containing non-categorical domain attributes, images of other categories can be generated through text control, as a feature of T2I models; Extra attribute: Either intra- or cross-category, we can attach additional attributes to the images; Personalization: We hope to combine domain-driven and subject-driven generation for better personalization. In order to achieve these goals, we propose DomainGallery, adopting DreamBooth-like [37] finetuning paradigm where the non-categorical domain attributes are learned and bound to an identifier word, so that the generation can be done via a normal T2I pipeline. DomainGallery features four attribute-centric finetuning techniques which respectively settle four challenges: (1) Prior attribute erasure: The prior attributes of the identifier word may possibly show up even if we have bound new domain attributes to it. Therefore, we pre-erase these prior attributes to avoid unexpected elements in images. (2) Attribute disentanglement: The domain/categorical attributes corresponding to the identifier/category word may be leaked into each other, causing missing domain attributes and/or unexpected categorical attributes when we change the category word in cross-category generation. Therefore, we explicitly encourage domain-category disentanglement to prevent such leakage. (3) Attribute regularization: The model is prone to overfitting when finetuned on few-shot datasets. Therefore, we regularize the finetuning process (with a strategy to construct paired source/target latent codes and a regularization loss) to reduce overfitting caused by excessive presence of domain attributes and possible biases of dataset distributions. (4) Attribute enhancement: Sometimes the strengths of the domain attributes learned on a specific dataset category are insufficient for cross-category generation. Therefore, we adjust the intensity of the domain attributes when generating cross-category images for better fidelity. These techniques spreading over pre-finetuning (1)(2), finetuning (2)(3) and inference (4), are tailored to few-shot domain-driven generation, aiming at solving key issues that previous works have failed to settle. Later in Sec. 5, we conduct thorough experiments on several few-shot datasets. These experiments manifest the superior and satisfying performance of DomainGallery on all of the four generation scenarios, which can serve as a state-of-the-art method of few-shot domain-driven image generation."
https://arxiv.org/html/2411.04533v1,Neural Fingerprints for Adversarial Attack Detection,"Deep learning models for image classification have become standard tools in recent years. However, a well known vulnerability of these models is their susceptibility to adversarial examples. Adversarial examples are generated by slightly altering an image of a certain class in a way that is imperceptible to humans but causes the model to classify it wrongly as another class. Many algorithms have been proposed to address this problem, falling generally into one of two categories: (i) building robust classifiers (ii) directly detecting attacked images. Despite the very good performance of the proposed detectors, we argue that in a white-box setting, where the attacker knows the configuration and weights of the network and the detector, the attacker can overcome the detector by running many examples on a local copy, and sending only examples that were not detected to the actual model. This problem of addressing complete knowledge of the attacker is common in security applications where even a very good model is not sufficient to ensure safety. In this paper we propose to overcome this inherent limitation of any static defence with randomization. To do so, one must generate a very large family of detectors with consistent performance, and select one or more of them randomly for each input. For the individual detectors, we suggest the method of neural fingerprints. In the training phase, for each class we repeatedly sample a tiny random subset of neurons from certain layers of the network, and if their average is sufficiently different between clean and attacked images of the focal class they are considered a fingerprint and added to the detector bank. During test time, we sample fingerprints from the bank associated with the label predicted by the model, and detect attacks using a likelihood ratio test. We evaluate our detectors on ImageNet with different attack methods and model architectures, and show near-perfect detection with low rates of false detection. https://github.com/HaimFisher/fingerprints-armor","Deep learning models have become ubiquitous in recent years across a wide range of applications, from image classification and natural language processing to speech recognition and transportation. However, these models have been shown to be vulnerable to adversarial attacks - small, imperceptible perturbations to inputs that cause models to make incorrect predictions (Szegedy et al., 2014; Goodfellow et al., 2014). These attacks pose serious concerns regarding the security and reliability of deep learning systems, especially in critical domains (Kurakin et al., 2016). A growing body of research has focused on developing adversarial attacks that can reliably fool models as well as defences to mitigate these threats (Madry et al., 2017; Song et al., 2017; Gowal et al., 2020). The main defence approaches can be divided into the broad categories of robust classification and adversarial attack detection. As the name suggests, robust models aim to mitigate the threat by being robust to such inputs. This is often achieved by introducing training schemes that include adversarial examples, or by alterations of the inputs aiming to negate the adversarial perturbation. Alternatively, in adversarial attack detection, the main model is used as-is, but a parallel model performs the binary classification of the input as clean or attacked. This is done for instance by adding and training an additional output head from one of the layers of the network, or via statistical models that consider network activations or the final output layer. A short introduction to the main adversarial attack and protection methods is given in Section (2). When considering the truly white-box threat model – that is, assuming that the attacker has complete knowledge of the system – even near-perfect detection will not suffice. Knowing the structure and parameters used both for the main classifier and for the detector model, the attacker need only run many attack attempts on an offline copy of the system, and present the actual system only with inputs that were already verified to be successful adversarial attacks. If the detector model is differentiable, as is the case when adding an extra binary output head, the attacker can feasibly bypass the defence even more directly, by adding the desired (negative) response of the detector model to the objective when computing the perturbation for the adversarial attack. Imagine however if we could have multiple detector networks, each providing a consistent and acceptable detection level. During inference, we randomly choose one of these detectors to apply to the input. This randomized strategy prevents users from crafting an input that could compromise both the network and the detector, as they won’t know which detector will be selected. For this method to be effective, we need: (i) a large pool of detectors to choose from, and (ii) detectors that are not highly correlated, so that attacking one does not affect many others. Additionally, the entire process must be computationally efficient to ensure practicality. In this paper, we introduce the concept of Neural Fingerprints. A neural fingerprint consists of a subset of neurons with a known distributions given a specific class. We demonstrate that grouping just a few dozen neurons into a fingerprint can achieve considerable detection rates, and by using many fingerprints together we can achieve near-perfect detection with a negligible false alarm rate. Additionally, we present an efficient method to prepare a large bank of fingerprints that share very few neurons, allowing for the selection of an uncorrelated random subset of fingerprints at test time. Our method is validated on the ImageNet dataset, where we systematically created adversarial attacks across classes. This extensive experimentation surpasses that of most studies in adversarial detection methods, suggesting the practical effectiveness and scalability of our method in real-world scenarios. The intuition behind the use of neural fingerprints for adversarial attack detection relies on several facts to hypothesize that such detectors would exist in many cases. First, from the lottery ticket hypothesis (Frankle and Carbin, 2018), we know that most neurons are not actively driving the classification result. Moreover, the various adversarial attacks try to make as little change as possible, and hence will mostly change the value of the activations that do affect the classification. From this we conclude that most neurons in a random set of neurons will not be significantly impacted by an adversarial attack. Finally, due to the way that networks are trained (e.g., small gradient steps, dropout) many neurons that are not currently important for the classification do carry some information about the class that was gathered during training. In total, we hypothesise that information about the identity of the true class is distributed among a large population of neurons, most of which are not highly influential in the classification output and thus they will not be targeted by adversarial attacks. We attempt to extract and exploit this information in our detectors. The main contribution of this paper is twofold. First, to the best of our knowledge this paper is the first to address the insufficiency of deterministic adversarial attack detectors in the truly white-box setting when the attacker is assumed to have full information of the methods used. Second, we propose and demonstrate the Neural Fingerprint approach for the creation of large detector banks, and application of randomized attack detection. The rest of the paper is organized as follows: In the next section we briefly review the main approaches used for adversarial attacks, robust classification and detection of attacks. In Section (3) we present the proposed method of Neural Fingerprints for adversarial attack detection. In Section (4) we review the related work and highlight the similarity and differences from this work. Next, in Section (5) we present an evaluation of the proposed method on the ImageNet dataset, followed by a short summary and conclusion in Section (6)."
https://arxiv.org/html/2411.04519v1,ℓ0subscriptℓ0\ell_{0}roman_ℓ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT-Regularized Sparse Coding-based Interpretable Network for Multi-Modal Image Fusion,"Multi-modal image fusion (MMIF) enhances the information content of the fused image by combining the unique as well as common features obtained from different modality sensor images, improving visualization, object detection, and many more tasks. In this work, we introduce an interpretable network for the MMIF task, named FNet, based on an ℓ0subscriptℓ0\ell_{0}roman_ℓ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT-regularized multi-modal convolutional sparse coding (MCSC) model. Specifically, for solving the ℓ0subscriptℓ0\ell_{0}roman_ℓ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT-regularized CSC problem, we develop an algorithm unrolling-based ℓ0subscriptℓ0\ell_{0}roman_ℓ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT-regularized sparse coding (LZSC) block. Given different modality source images, FNet first separates the unique and common features from them using the LZSC block and then these features are combined to generate the final fused image. Additionally, we propose an ℓ0subscriptℓ0\ell_{0}roman_ℓ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT-regularized MCSC model for the inverse fusion process. Based on this model, we introduce an interpretable inverse fusion network named IFNet, which is utilized during FNet’s training. Extensive experiments show that FNet achieves high-quality fusion results across five different MMIF tasks. Furthermore, we show that FNet enhances downstream object detection in visible-thermal image pairs. We have also visualized the intermediate results of FNet, which demonstrates the good interpretability of our network. .","Multi-modal image fusion (MMIF) is an active area of research for many years. In MMIF, the aim is to integrate information captured using different modality sensor images into a single fused image, offering enhanced information content compared to individual single-modality sensor images. This improves visualization and interpretation, making the fused image more appropriate for downstream tasks like object detection, segmentation, disease diagnosis, and more [1, 2]. In MMIF, different modality sensors capture images of the same scene, collecting some common image features shared across all the modalities. Additionally, each modality acquires its own unique image features. Some recent MMIF methods [3, 4, 5] have been designed to first separate the unique and common features in the source images and then generate the fused image by integrating these features. To separate the unique and common features, [3, 5] solved a multi-modal dictionary learning model using the alternating direction method of multipliers (ADMM) [6]. However, this approach is time-consuming and not very effective for large-scale data. To overcome the limitation of the optimization-based method, Deng et al. [4] designed an interpretable deep neural network (DNN), named CUNet, based on an ℓ1subscriptℓ1\ell_{1}roman_ℓ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-regularized multi-modal convolutional sparse coding (MCSC) model. CUNet uses learned convolutional sparse coding (LCSC) blocks to estimate the unique and common features. Though CUNet appears effective for separating the unique and common features, there still remains an issue with the LCSC block, which is designed by unrolling a convolutional extension of the iterative shrinkage thresholding algorithm (ISTA) [7] to estimate the ℓ1subscriptℓ1\ell_{1}roman_ℓ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-regularized sparse features. ISTA uses a soft thresholding function, which nullifies the features with absolute values smaller than a threshold value and thus promotes sparsity. However, the soft thresholding function also reduces the magnitude of the features with higher absolute values and overpenalizes them. This makes the magnitude of the ISTA-based sparse estimation lower than the ℓ0subscriptℓ0\ell_{0}roman_ℓ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT-regularized optimal sparse estimation [8]. Constraining the unique and common features to be ℓ0subscriptℓ0\ell_{0}roman_ℓ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT-regularized can overcome this problem [9]. Moreover, for the MMIF task, where the ground truth fused image is not available, considering an inverse fusion process proves effective [10, 11]. In the inverse fusion process, the fused image is separated back into its source images, and as the quality of these separated images is dependent on the quality of the fused image, constraining them to be similar to the original source images helps enforce the generation of a higher-quality fused image. This work introduces a novel ℓ0subscriptℓ0\ell_{0}roman_ℓ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT-regularized MCSC model to address the MMIF task. In this model, we represent each modality source image as a combination of ℓ0subscriptℓ0\ell_{0}roman_ℓ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT-regularized unique and common sparse features. Based on this model, a DNN named FNet is developed to solve the MMIF task. Due to such a model-based design, FNet has the benefits of both model-based and DNN. Model-based methods promote prior domain knowledge and interpretability of the underlying physical process, whereas DL methods are very efficient in learning from large-scale data [12]. Since no existing work has designed a DNN to solve the ℓ0subscriptℓ0\ell_{0}roman_ℓ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT-regularized CSC problem, we propose a novel learnable ℓ0subscriptℓ0\ell_{0}roman_ℓ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT-regularized sparse coding (LZSC) block. To design the LZSC block, we first develop an iterative algorithm for solving the ℓ0subscriptℓ0\ell_{0}roman_ℓ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT-regularized CSC problem and then unroll this algorithm into our learnable LZSC block. In FNet, first, the unique and common features are separated from different modality source images using our proposed LZSC blocks. Then, these features are integrated to get the final fused image. In our work, constraining the unique and common features to be ℓ0subscriptℓ0\ell_{0}roman_ℓ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT-regularized solves the fundamental formulation of sparse coding and estimates accurate sparse features. Moreover, we propose a novel MCSC model to represent the inverse fusion process. In this model, we represent the fused image as a combination of ℓ0subscriptℓ0\ell_{0}roman_ℓ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT-regularized sparse features corresponding to the different modality source images. Based on this model, we design an interpretable inverse fusion network named IFNet. Given the fused image, IFNet first estimates the features corresponding to the source images using our LZSC blocks and then obtains the source images using the convolution operation. We utilize IFNet in the training of FNet. Constraining the decomposed source images obtained by IFNet to be similar to the original source images improves the scene representation quality in the fused image. Our primary contributions can be outlined below: 1. We develop a novel algorithm unrolling-based learnable LZSC block to solve the ℓ0subscriptℓ0\ell_{0}roman_ℓ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT-regularized CSC problem. This is the first learnable block to solve the ℓ0subscriptℓ0\ell_{0}roman_ℓ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT-regularized CSC problem to the best of our awareness. 2. We introduce an MCSC model to address the MMIF task. In this model, we represent each modality source image as a combination of ℓ0subscriptℓ0\ell_{0}roman_ℓ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT-regularized unique and common sparse features. Based on this model and our LZSC block, we propose an interpretable fusion network named FNet. 3. We introduce an MCSC model for the inverse fusion process. In this model, we represent the fused image as a combination of ℓ0subscriptℓ0\ell_{0}roman_ℓ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT-regularized sparse features corresponding to the different modality source images. Based on this model and our LZSC block, we propose an interpretable inverse fusion network named IFNet, which is utilized in the training of FNet. 4. FNet achieves leading results on five MMIF tasks: visible and infrared (VIS-IR), visible and near-infrared (VIS-NIR), computed tomography and magnetic resonance imaging (CT-MRI), positron emission tomography and MRI (PET-MRI), and single-photon emission computed tomography and MRI (SPECT-MRI) image fusion. Moreover, FNet also facilitates downstream object detection in visible-thermal image pairs. The remaining paper is structured as follows. In Section II, we review the sparse coding methods and prior works on the MMIF task and the inverse fusion process. Section III describes our proposed LZSC block, FNet, IFNet, and the training process in detail. We conduct extensive experiments in Section IV to validate our proposed method. Finally, Section V concludes the paper."
https://arxiv.org/html/2411.04509v1,FedDP: Privacy-preserving method based on federated learning for histopathology image segmentation,"Hematoxylin and Eosin (H&E) staining of whole slide images (WSIs) is considered the gold standard for pathologists and medical practitioners for tumor diagnosis, surgical planning, and post-operative assessment. With the rapid advancement of deep learning technologies, the development of numerous models based on convolutional neural networks and transformer-based models has been applied to the precise segmentation of WSIs. However, due to privacy regulations and the need to protect patient confidentiality, centralized storage and processing of image data are impractical. Training a centralized model directly is challenging to implement in medical settings due to these privacy concerns.This paper addresses the dispersed nature and privacy sensitivity of medical image data by employing a federated learning framework, allowing medical institutions to collaboratively learn while protecting patient privacy. Additionally, to address the issue of original data reconstruction through gradient inversion during the federated learning training process, differential privacy introduces noise into the model updates, preventing attackers from inferring the contributions of individual samples, thereby protecting the privacy of the training data.Experimental results show that the proposed method, FedDP, minimally impacts model accuracy while effectively safeguarding the privacy of cancer pathology image data, with only a slight decrease in Dice, Jaccard, and Acc indices by 0.55%, 0.63%, and 0.42%, respectively. This approach facilitates cross-institutional collaboration and knowledge sharing while protecting sensitive data privacy, providing a viable solution for further research and application in the medical field.","Histopathological imaging has long been the gold standard for diagnosing cancer [1]. H&E-stained whole slide images (WSIs), which are widely utilized for surgical evaluation, have become an essential point of reference. Accurate segmentation of WSIs enables in-depth analysis of the tumor microenvironment, offering detailed insights [2]. However, the massive datasets required for pathological image processing involve not only significant computational costs but also raise concerns about patient privacy. In this context, traditional centralized learning faces substantial challenges. Conventional machine learning models typically require training on centralized servers, meaning all pathological image data must be consolidated in one location for processing. However, due to privacy regulations and the need to protect patient confidentiality, the centralized storage and processing of pathological image data have become increasingly impractical [3]. The core concept of federated learning is to shift the model training process from centralized servers to local devices, allowing each medical device to collaboratively learn while protecting patient privacy. Federated learning enables data from different institutions to remain local, eliminating the need to share original data and only sharing model updates instead. This distributed learning approach offers a novel solution to the privacy and data security challenges faced in the field of pathological image processing [4]. Under the federated learning framework, each medical device can train models locally, learning from the unique features of its local data [5]. The local model updates are then sent to a central server, where they are aggregated to form a global model. This updated global model is subsequently distributed back to the local devices, continuing iteratively. Throughout this process, the original data remains local, with only the model parameters being shared. This not only helps protect patient privacy but also effectively addresses the diversity and heterogeneity of pathological image data. The collaborative nature of federated learning is also evident during the model update process, as each local device shares model parameters with others, allowing them to complement each other and enhance the overall performance of the model. Moreover, the security of federated learning is ensured. Model updates are transmitted using encryption technologies, safeguarding the security of model parameters during transmission. This is crucial for preventing malicious attacks or theft, especially in the field of pathological image processing, where patient-sensitive information is involved. Federated learning holds tremendous potential in the field of pathological imaging. It can be applied not only in medical image diagnostics but also extended to medical research. In the diagnostic process of pathological image segmentation, federated learning can enhance model performance, adapt to the data characteristics of different institutions and devices, and better serve patients’ diagnostic and treatment needs. In medical research, federated learning facilitates collaboration among multiple institutions, enabling data sharing and mutual benefits [6, 7, 8, 9, 10, 11, 12, 13]. However, in federated learning, protecting data privacy remains a critical issue [14, 15]. Although federated learning is typically designed to safeguard user data privacy, in some instances, methods such as gradient inversion attacks [10] can pose risks. These attacks involve analyzing a model’s outputs and corresponding gradient information to infer the model’s input data, potentially leading to the reconstruction of original data and resulting in privacy breaches [16, 17]. Overall, the introduction of federated learning into the field of pathological image processing offers a new perspective on addressing the privacy and data security challenges associated with centralized learning. Its collaborative nature helps enhance the generalizability of models, accommodating the diversity and heterogeneity of pathological image data. Moreover, by decentralizing the handling of data, it preserves the privacy of pathological image data, bringing new hope to the development of the medical field. However, federated learning still faces significant challenges in pathological image processing. This paper addresses the high computational costs and privacy leakage issues by incorporating methods such as differential privacy. Thus, in response to the issues of data silos and privacy breaches in cancer pathological imaging, this paper proposes a privacy-preserving method based on federated learning (FedDP) for segmenting cancer pathology images."
https://arxiv.org/html/2411.04501v1,Pose2Trajectory: Using Transformers on Body Pose to Predict Tennis Player’s Trajectory,"Tracking the trajectory of tennis players can help camera operators in production. Predicting future movement enables cameras to automatically track and predict a player’s future trajectory without human intervention. Predicting future human movement in the context of complex physical tasks is also intellectually satisfying. Swift advancements in sports analytics and the wide availability of videos for tennis have inspired us to propose a novel method called Pose2Trajectory, which predicts a tennis player’s future trajectory as a sequence derived from their body joints’ data and ball position. Demonstrating impressive accuracy, our approach capitalizes on body joint information to provide a comprehensive understanding of the human body’s geometry and motion, thereby enhancing the prediction of the player’s trajectory. We use encoder-decoder Transformer architecture trained on the joints and trajectory information of the players with ball positions. The predicted sequence can provide information to help close-up cameras to keep tracking the tennis player, following centroid coordinates. We generate a high-quality dataset from multiple videos to assist tennis player movement prediction using object detection and human pose estimation methods. It contains bounding boxes and joint information for tennis players and ball positions in singles tennis games. Our method shows promising results in predicting the tennis player’s movement trajectory with different sequence prediction lengths using the joints and trajectory information with the ball position.","Artificial Intelligence (AI) has become indispensable in many domains, including autonomous systems, surveillance systems, and sports analysis al2022generating . The power of AI technology has profoundly impacted and transformed sectors such as transportation, surveillance, logistics, manufacturing, and healthcare siciliano2008springer . In surveillance systems, developers leverage AI to enhance performance with facial recognition, action recognition, and prediction technologies. Sports analysis involves using data points to evaluate and improve athletic performance by generating insights that can be used to optimize player training and game strategies, including recommending beneficial and optimal ball trajectory and speed. Sports analysis has become increasingly popular due to the availability of wearable devices, videos, and other advanced technologies that make data collection and analysis more accessible. In sports, AI can potentially revolutionize how athletes and coaches approach performance analysis. A trained AI can efficiently process vast amounts of data in real time. This data-driven approach can help with injury prevention and enable coaches and trainers to make informed decisions, optimize training regimes, identify areas for improvement, and provide invaluable insights into player and team performance. Figure 1: The figure shows the prediction of the future movement trajectory of a tennis player fifteen (250 ms), thirty (500 ms), and sixty (1s) frames ahead (until reaching the ball). The trajectory is predicted as the second player’s centroid around which we show a 224×224224224224\times 224224 × 224 bounding box in future frames. The player appears lighter in future frames. The original image has been taken from videos on the TennisTV YouTube channel. Tennis is a popular sport that first originated in England in the \nth19 century. In recent years, tennis has seen numerous innovations that help improve referees’ decision-making and enhance player performance, including the Hawk-Eye line calling system owens2003hawk and telemetry sensors bergeron2006voluntary . Processing and analyzing large amounts of data in tennis utilizing AI algorithms can help recognize and predict player actions and movements, forecast future location, speed, and spin of shots, and recognize racquet movements (swing). All this can help trainers evaluate player performance, focus on strengths and weaknesses, and potentially help coaches make more informed decisions during tennis matches. Machine Learning algorithms can also help enhance the production of professional tennis game videos by predicting the players’ future movement trajectories. The process of recording a tennis match is complex, requiring multiple cameras, skilled camera operators, and advanced broadcasting equipment. Typically, six to eight cameras are strategically positioned around the court, capturing the action from different angles sheets2011kinematics . The equipment may include long-range cameras, which provide a comprehensive view of the court and player movements, and close-up cameras, which offer more detailed shots of the players following the ball. All cameras are controlled by skilled operators who follow the action and adjust the angles and zoom levels to capture the best shots. By using the predicted future movement trajectories of the tennis players, these cameras have the potential to automatically track the movement of the players, reducing the need for human intervention and enhancing the accuracy and speed of the recording process. Predicting the future trajectory of the player in sports is a challenging task. Researchers have attempted to solve this problem in football and basketball lindstrom2020predicting hauri2021multi . In tennis, people use the players’ trajectory information to evaluate performance, style, strategy, and the nature of the movement of the players pingali2001visualization giles2020machine . Enhancing the accuracy of future trajectory prediction for tennis players can enable cameras to autonomously estimate a player’s upcoming movements without human intervention. To achieve this, it is crucial to supply predicted data for the player’s trajectory at least half a second in advance, taking into consideration the physical constraints of the close-up camera. Typically, cameras used in tennis matches can process multiple requests per second, with a maximum capacity of ten requests per second. However, factors such as processing speed, image resolution, compression settings, and network conditions can impact the camera’s response time. Consequently, it is essential to provide predictions at least half a second ahead, or ideally, a full second, to accommodate the camera’s response time limitations. While many research studies on predicting future trajectories for single or multiple individuals rely on the centroid of the bounding box to estimate their movement path, this approach may not be adequate for accurately predicting a tennis player’s trajectory. Tennis players exhibit a wide range of movements to reach the ball, which can introduce noise to the centroid-based analysis. To improve prediction accuracy, it is essential to consider specific body parts, such as the arms’ joints (determining which one holds the racket), legs’ joints, and head, as well as their orientation. This prompts the question: how crucial are the various body parts of a tennis player in making predictions? Examining these elements can offer a more holistic understanding of a player’s geometry and motion, ultimately leading to more precise trajectory predictions. Kalman filter and machine learning methods can be potentially used in sports to track players. However, these algorithms are not explicitly built or reliable for predicting future trajectories over long periods. In our novel approach, we predict the future movement trajectory as a sequence based on the players’ body joints, trajectory, and ball position information. In Figure 1, we show the suitability of our method by predicting 15, 30, and 60 frames ahead. We use the encoder-decoder Transformer model in this approach. The encoder part is fed all the joint positions and the centroid points of the players and ball positions in several frames as a sequence, representing the player’s past movement. The decoder part predicts the future movement trajectory of the players as a sequence of centroid points. We use a high-quality dataset we created for this work. In Section 2, we present relevant previous work, including the prediction of future movements in sports. Section 3 describes our prediction system, including tennis player and ball detection, body joint detection, and the use of the Transformer model to predict future movement trajectory. In Section 4, we demonstrate that the body joint information is essential to predict the future trajectory of the players by evaluating alternate models trained on the trajectory information. Finally, Sections 5 and 6 provide information about our dataset and our conclusions."
https://arxiv.org/html/2411.04493v1,Synergy-Guided Regional Supervision of Pseudo Labels for Semi-Supervised Medical Image Segmentation,"Semi-supervised learning has received considerable attention for its potential to leverage abundant unlabeled data to enhance model robustness. Pseudo labeling is a widely used strategy in semi supervised learning. However, existing methods often suffer from noise contamination, which can undermine model performance. To tackle this challenge, we introduce a novel Synergy-Guided Regional Supervision of Pseudo Labels (SGRS-Net) framework. Built upon the mean teacher network, we employ a Mix Augmentation module to enhance the unlabeled data. By evaluating the synergy before and after augmentation, we strategically partition the pseudo labels into distinct regions. Additionally, we introduce a Region Loss Evaluation module to assess the loss across each delineated area. Extensive experiments conducted on the LA dataset have demonstrated superior performance over state-of-the-art techniques, underscoring the efficiency and practicality of our framework. The code is available in the Supplementary Material.","With the rapid development of computer vision and deep learning, automatic segmentation of medical images has become a focal point of research. Supervised learning segmentation methods, such as U-Net (Ronneberger, Fischer, and Brox 2015) and UNeXt (Valanarasu and Patel 2022) have achieved significant success. However, traditional medical image segmentation methods often rely on a large amount of precisely annotated training data, which is expensive and time-consuming to obtain. This dependency limits the scalability and applicability of these methods. In recent years, semi-supervised learning has attracted widespread attention for its potential to utilize fewer labeled data in conjunction with abundant unlabeled data, thereby enhancing model generalization. Numerous semi-supervised medical image analysis methods have been introduced, including pseudo labels (Chaitanya et al. 2023), deep co-training (Zheng et al. 2022), deep adversarial learning (Lei et al. 2023), the mean teacher and its extensions (Tarvainen and Valpola 2017)(Li et al. 2020a), and contrastive learning (Wang et al. 2022), among others. These methods effectively leverage both labeled and unlabeled data to develop robust models. Pseudo-label learning is widely adopted. This method begins by training the model on labeled data, then generating pseudo labels for unlabeled data using predicted probability maps. These pseudo labels are combined with labeled data for further model training, enhancing accuracy and generalization (Lee et al. 2013). Building on this approach, Lu et al. (Lu et al. 2023) introduced two auxiliary decoders into a network to generate pseudo labels from these auxiliary decoders. Similarly, Li et al. (Li et al. 2020b) explored generating pseudo labels through the cyclic optimization of neural networks, incorporating self-supervised tasks. Additionally, Luo et al. (Luo et al. 2022a) developed the Cross Teaching framework, where the prediction of one network serves as the pseudo label to directly guide another network in an end-to-end manner. While these methods effectively enhance model performance, the inherent inaccuracy of pseudo labels compared to ground truth remains a challenge. While some pseudo labels are comparable to ground truth, others may be compromised by noise. As iterative training progresses, this noise can adversely affect models robustness (Song et al. 2023). Although applying noise-robust loss functions can mitigate the effects of noise to some extent (Olmin and Lindsten 2022), optimizing against unnecessary noise suppression for high-quality pseudo labels may reduce model accuracy due to over-smoothing (Tian et al. 2023). To effectively utilize pseudo labels, we introduce the Synergy-Guided Regional Supervision (SGRS-Net). During training, a Pseudo Label Generation (PLG) Module is employed within the mean-teacher framework and transform the predictions of the teacher network into pseudo labels. Additionally, we introduce a Mix Augumentation (MA) Module which utilize annotated data to augment unlabeled data. Following this, our Synergy Evaluation module partitions pseudo labels into different regions and then apply distinct loss functions to evaluate the corresponding loss for each region. This approach is designed to maximize the use of pseudo labels while minimizing the impact of potential noise. In summary, our contributions are listed as follows: • We introduce a novel SGRS-Net framework for semi-supervised medical image segmentation. This framework employs a PLG module to generate pseudo labels and a MA module to enhance the diversity of the unlabeled dataset. • Based on PLG and MA modules, we propose a Synergy Evaluation module and a Regional Loss Evaluation module to mitigate the impact of noise while fully leveraging the supervisory signal from pseudo labels. • SGRS-Net was evaluated on three public available datasets, showing promising results, especially when using only 5%percent\%% of labeled data. This highlights the model’s effectiveness in scenarios with limited labeled data."
https://arxiv.org/html/2411.04480v1,CFPNet: Improving Lightweight ToF Depth Completion via Cross-zone Feature Propagation,"Depth completion using lightweight time-of-flight (ToF) depth sensors is attractive due to their low cost. However, lightweight ToF sensors usually have a limited field of view (FOV) compared with cameras. Thus, only pixels in the zone area of the image can be associated with depth signals. Previous methods fail to propagate depth features from the zone area to the outside-zone area effectively, thus suffering from degraded depth completion performance outside the zone. To this end, this paper proposes the CFPNet to achieve cross-zone feature propagation from the zone area to the outside-zone area with two novel modules. The first is a direct-attention-based propagation module (DAPM), which enforces direct cross-zone feature acquisition. The second is a large-kernel-based propagation module (LKPM), which realizes cross-zone feature propagation by utilizing convolution layers with kernel sizes up to 31. CFPNet achieves state-of-the-art (SOTA) depth completion performance by combining these two modules properly, as verified by extensive experimental results on the ZJU-L5 dataset. The code will be made public.","Depth completion from sparse depth measurements is an essential component of many tasks, including SLAM [23], novel view synthesis [40], and robot navigation [27]. Previous depth completion methods usually simulate sparse depth inputs randomly from depth maps acquired by RGB-D cameras, e.g., RealSense. Nevertheless, it is impractical to obtain such sparse depth as inputs for real applications. Recently, ToF sensors have been applied to depth super-resolution [13] and depth completion [21] due to their low power consumption and cost-effectiveness compared with RGB-D cameras. In this paper, we focus on the depth completion task, particularly implemented on a popular type of lightweight ToF sensor (e.g., ST VL53L5CX [2], denoted as L5), though our method is not limited to it. Despite its low resolution (e.g., 8×8888\times 88 × 8), the low power consumption (e.g., 200mW) and low cost (e.g., $6currency-dollar6\$6$ 6) of L5 allows it to be deployed in more applications. Note that we refer to this depth estimation task assisted by a lightweight ToF sensor as depth completion due to their similarity in input depth signals [35]. Figure 1: L5 sensing principle and performance comparison. (a) L5 would return zones of resolution 8×8888\times 88 × 8, and each zone provides depth distribution information. (b) We overlay aligned zone areas on the paired RGB image and display the error maps of DELTAR and our CFPNet. The largest rectangle is the zone area, and some zones are missing due to too few received photons or inconsistency in measurement. Notice that CFPNet obtains smaller errors in outside-zone areas (the yellow rectangle). To get a preliminary overview of depth completion with a lightweight ToF sensor, we illustrate the sensing principle of L5 in Fig. 1 (a) as an example. Unlike conventional ToF sensors, L5 is lightweight and has extremely low resolution (e.g., 8×8888\times 88 × 8). Each zone in L5 produces a depth distribution about the corresponding 3D scene by counting the number of photons returned in discretized time intervals. DELTAR fits this distribution with a Gaussian distribution and transmits the mean and variance to save bandwidth and energy [21]. In Fig. 1 (b), we overlay the zones on the image. The zone area is the largest red rectangle, and each small red rectangle corresponds to one ToF measurement. Note that some zones are missing due to too few received photons or inconsistency in measurement. Though depth inputs collected from lightweight ToF sensors (e.g., L5) are sparse and noisy, researchers have made depth completion from them plausible. DELTAR is a RGB-guided lightweight-ToF-based depth completion model [21]. At the core of DELTAR, an effective D-to-image module conducts feature fusion between depth and RGB features patchwisely in the zone area. Then, a self-attention layer [39] is leveraged to propagate the fused features to outside-zone areas. However, since this self-attention [39] queries values based on similarities, pixels in outside-zone areas may gather more information from outside-zone pixels which do not contain ToF information than from in-zone pixels. Thus, it is unlikely to propagate depth features effectively from zone areas to outside-zone areas. To tackle the restricted cross-zone feature propagation issue, we introduce a direct-attention-based module and a large-kernel-based module to establish steady and long-range dependencies between the two areas. Direct-Attention-based Propagation Module. DAPM is based on the attention mechanism [39], which enables feature propagation regardless of pixel distances in the image. Specifically, we directly perform cross-attention from in-zone pixels to outside-zone pixels. Thus, outside-zone pixels can query information from in-zone pixels dynamically. More importantly, our DAPM avoids feature acquisition from outside-zone areas where ToF information does not exist during feature propagation. Large-Kernel-based Propagation Module. LKPM incorporates a convolution layer of a large kernel size (e.g., 31×31313131\times 3131 × 31) [9]. Large-kernel CNNs have been shown to have larger effective receptive fields [26] compared with small-kernel CNNs. Consequently, we use convolution layers of large kernels to establish long-range dependencies between pixels from zone areas and outside-zone areas. Moreover, the interaction between input signals in convolution depends on location rather than similarities as in attention [5]. Thus, with the large-kernel design, LKPM is not likely to fall into the situation that feature acquisition only comes from outside-zone areas, mitigating potential limitations. Owing to the proposed DAPM and LKPM, our CFPNet can reduce the errors in outside-zone areas as in Fig. 1 (b). As a result, compared with the previous method [21], we reduce the mean absolute relative error (REL) from 0.127 to 0.103 on the ZJU-L5 dataset[21]. Notably, we decrease REL and RMSE by 46.5% and 30.8%, respectively, when the ToF is of resolution 2×2222\times 22 × 2. In summary, our contributions are as follows: 1. We notice that outside-zone areas suffer from a great performance drop compared with in-zone areas. Furthermore, we propose CFPNet to alleviate this degradation with more effective cross-zone feature propagation. 2. Our CFPNet contains two feature propagation modules, namely DAPM and LKPM. DAPM allows direct feature propagation with the help of cross-attention, and LKPM propagates features using convolution layers of large kernel sizes. 3. CFPNet achieves remarkable performance gain on the ZJU-L5 dataset over previous methods. Codes will be released for peer research."
https://arxiv.org/html/2411.04475v1,Deep Learning Models for UAV-Assisted Bridge Inspection: A YOLO Benchmark Analysis,"Visual inspections of bridges are critical to ensure their safety and identify potential failures early. This inspection process can be rapidly and accurately automated by using unmanned aerial vehicles (UAVs) integrated with deep learning models. However, choosing an appropriate model that is lightweight enough to integrate into the UAV and fulfills the strict requirements for inference time and accuracy is challenging. Therefore, our work contributes to the advancement of this model selection process by conducting a benchmark of 23 models belonging to the four newest YOLO variants (YOLOv5, YOLOv6, YOLOv7, YOLOv8) on COCO-Bridge-2021+, a dataset for bridge details detection. Through comprehensive benchmarking, we identify YOLOv8n, YOLOv7tiny, YOLOv6m, and YOLOv6m6 as the models offering an optimal balance between accuracy and processing speed, with mAP@50 scores of 0.803, 0.837, 0.853, and 0.872, and inference times of 5.3ms, 7.5ms, 14.06ms, and 39.33ms, respectively. Our findings accelerate the model selection process for UAVs, enabling more efficient and reliable bridge inspections.","Visual inspection, a method using one’s naked eye to assess the condition of structures, has played an essential role in maintaining and ensuring the quality of urban public works [1]. This method involves detecting structural issues such as corrosion, cracks, and other problems to ensure the works’ safety and durability. For instance, visual inspection is routinely used to inspect the condition of high-rise buildings’ envelopes. For bridges constantly subjected to harsh weather conditions and traffic loads, visual inspection is an excellent method for inspectors to detect early signs of deterioration, corrosion, and damage. As a result, experts can suggest timely solutions to maintain and repair these structures. Technology devices, like cameras, digital image analyzers, and other devices, have contributed to improving visual inspection. These tools allow experts to remotely examine components for corrosion and damage, which reduces cost and safety risks for inspectors in visual inspection [2]. Among them, UAVs are commonly used to inspect structures that are difficult to access or have complex geometries. However, the monitoring process by UAVs encounters particular challenges. First, during the visual inspection process, a large amount of data requires specific expertise to analyze and examine, which is labor intensive [3]. Secondly, inspectors might face low-quality images due to the lack of proper light conditions and wind turbulent disturbance, leading to false detection [4]. Therefore, it is necessary to automate the process with AI-supported to increase accuracy while reducing the inspection time. The mentioned automatic process can be achieved using computer vision models. Computer vision (CV) is a branch of artificial intelligence that enables computers and systems to extract meaningful insights from digital images, videos, and other visual data sources. Subsequently, they can provide recommendations based on the information they gather. In visual inspection, automation can be done by combining a computer vision model with a camera-attached UAV. This could be a game-changer in the field of visual construction inspection as UAVs have adaptability, easy installation, low maintenance costs, versatility, and relatively small operating cost [5], while computer vision models are robust, easy to train and deploy. In this process, UAVs take pictures of structures. After that, the computer vision model processes the images and gives predictions about what components appear in the images and their possible problems. Then, this information would be sent to the inspectors, which would assist them in assessing the structure’s condition and accordingly decide whether the structures need to be repaired or renovated. Currently, deep learning models are also the main topic of this paper. Although the combination of computer vision models and camera-attached UAV(s) is the future of automated visual inspections, specific problems need to be examined. Despite the rapid advancement of UAVs in recent years, their computational power and storage space are still limited. Meanwhile, in visual inspection, the number of images that need to be processed could grow enormously, and data collection is often repeated since external factors, such as wind-induced vibrations in captured footage or low illumination leading to dark images, can significantly affect the quality of data [6]. The size of images is also a significant problem since processing, storing, or transmitting such a large amount of data is time-consuming and storage-intensive. In the traditional centralized CV-UAV method, these problems are not properly resolved because CV models are placed on a local computer or in the cloud, and UAVs have an additional task to transmit these images to where CV models are placed and wait to receive the results. Having to transmit images between UAVs and CV models is a bottleneck of the process, as it is slow and vulnerable to poor connections and hackers’ attacks. Edge AI is a rising technology that can deal with that. It overcomes this problem by embedding CV models into UAVs, so images are processed inside the UAVs, and only the results are sent to inspectors. The results are often text and damaged-predicted images, just a tiny part of the captured images. By doing so, the amount of data needed to be transmitted is reduced by a lot, thus speeding up the process and minimizing the risk caused by poor connections and hackers. We found that the task of discovering the potential of Edge AI in the field of visual inspection, especially of bridges, is essential [7]. The principal reason is that bridges are sensitive infrastructure transportation components, so they must be routinely inspected. It is necessary to develop an automatic bridge inspection method combining Edge AI and visual inspections to quickly and correctly assess the condition of bridges while requiring low effort. This method would help the bridge preservation process go smoothly and hence ensure the safety of traffic participants as well as social connectivity. However, incorporating CV models into UAVs is a challenge. This is because some modern CV models achieve high accuracy but require enormous computational and storage capacity, which UAVs lack. On the other hand, some older CV models fit UAVs’ computational and storage capacity, but their accuracy is inadequate. Through this work, we aim to find suitable CV models for bridge inspection with UAVs and provide a benchmark for future works. Therefore, in this paper, we focus on experimenting with CV models, comparing their performance through accuracy and inference time on COCO-Bridge 2021+, a dataset for bridge inspection. In detail, we benchmark four YOLO variants, including YOLOv5 [8], YOLOv6 [9], YOLOv7 [10], and YOLOv8 [11]. The experimental results show that YOLOv8n, YOLOv7tiny, YOLOv6m, and YOLOv6m6 are the models that achieve the best trade-off between accuracy and inference time among 23 total models of these four YOLO variants."
https://arxiv.org/html/2411.04469v1,FreeCap: Hybrid Calibration-Free Motion Capture in Open Environments,"We propose a novel hybrid calibration-free method FreeCap to accurately capture global multi-person motions in open environments. Our system combines a single LiDAR with expandable moving cameras, allowing for flexible and precise motion estimation in a unified world coordinate. In particular, We introduce a local-to-global pose-aware cross-sensor human-matching module that predicts the alignment among each sensor, even in the absence of calibration. Additionally, our coarse-to-fine sensor-expandable pose optimizer further optimizes the 3D human key points and the alignments, it is also capable of incorporating additional cameras to enhance accuracy. Extensive experiments on Human-M3 and FreeMotion datasets demonstrate that our method significantly outperforms state-of-the-art single-modal methods, offering an expandable and efficient solution for multi-person motion capture across various applications.","Vision-based human motion capture refers to the process of accurately predicting and reconstructing human meshes in open environments by utilizing visual data from LiDAR or camera. The approach eliminates the need for wearable sensors, enabling the capture of natural and unrestricted human motion. It is particularly advantageous in scenarios such as daily activities and sports, where the use of wearable devices may be impractical or restrictive. Camera-based methods (Belagiannis and Zisserman 2016; Miezal, Taetz, and Bleser 2017; Rajasegaran et al. 2022; Shin et al. 2024) can effectively capture the accurate human local pose by image texture information but lack depth information, and depth camera-based methods (Sridhar et al. 2015; Wei, Zhang, and Chai 2012; Yu et al. 2017, 2019; Zheng et al. 2018) suffer from lighting conditions and the limited sensor range. LiDAR is widely used in robotics and autonomous driving (Zhu et al. 2021, 2020; Cong et al. 2022; Xu et al. 2023) for perceiving depth information in large-scale scenes. Previous LiDAR-based methods (Li et al. 2022; Fan et al. 2023b; Ren et al. 2024) can estimate the global human pose in large-scale scenes by accurate depth information. However, the sparsity of long-distance point clouds and the lack of texture information can lead to inaccuracies in predictions. To leverage both image texture and LiDAR depth information for more accurate global human motion predictions in open environments. The previous method(Cong et al. 2023; Fan et al. 2023a) utilizes calibrated multiple cameras and LiDARs for involving the fusion of raw data, which can lead to difficulties in unifying data distribution and reduced generalization ability. In contrast, as shown in Fig. LABEL:fig:teaser, our system consists of the single LiDAR and any number of moving cameras. The LiDAR captures extensive global depth information, while the moving cameras provide detailed texture information in local areas. This more flexible and expandable setup enables accurate multi-person motion estimation within a unified world coordinate system, making it suitable for diverse applications. In this paper, we propose a novel calibration-free multi-model method FreeCap. We can not directly get the cross-sensor human matching in large-scale multi-person scenes without the calibration and the data-driven method can not intentionally learn the calibration when the camera is moving. To address these challenges, we introduce an efficient and high-quality local-to-global matching module, Pose-aware Cross-sensor Matching. Specifically, we use RTMPose (Jiang et al. 2023) and WHAM (Shin et al. 2024) to estimate the 2D key points and body pose in image and use LiveHPS (Ren et al. 2024) to estimate the 3D key points and body pose in point cloud. The body poses are used for local matching and the global key points are used for matching refinement. Next, we design a coarse-to-fine sensor-expandable pose optimizer to optimize the coarse align matrix calculated based on the matched 2D key points and 3D key points, which can unify the calibration-unknown multi-modal data and guide the network to further optimize the 3D human key points. Furthermore, considering the narrow of the camera range, our network can accept expandable camera data, the more cameras can assist LiDAR to predict more accurate results. Extensive experiments conducted on the multi-person large-scale dataset Human-M3(Fan et al. 2023a) and the multi-view sensor dataset FreeMotion(Ren et al. 2024), demonstrate that our method achieves significant improvements in human pose compared to other single-modal SOTA methods. Our main contributions can be summarized as follows: ∙∙\bullet∙ We present the first calibration-free and sensor-expandable system designed for capturing multi-person motions in open environments. ∙∙\bullet∙ We propose a local-to-global cross-sensor human-matching approach for predicting alignment matrix. ∙∙\bullet∙ We design an effective multi-modal coarse-to-fine fusion method by optimizing human key points. ∙∙\bullet∙ Our method achieves SOTA on the large-scale dataset Human-M3 and multi-sensor dataset FreeMotion."
https://arxiv.org/html/2411.04457v1,Efficient single image non-uniformity correction algorithm,"This paper introduces a new way to correct the non-uniformity (NU) in uncooled infrared-type images. The main defect of these uncooled images is the lack of a column (resp. line) time-dependent cross-calibration, resulting in a strong column (resp. line) and time dependent noise. This problem can be considered as a 1D flicker of the columns inside each frame. Thus, classic movie deflickering algorithms can be adapted, to equalize the columns (resp. the lines). The proposed method therefore applies to the series formed by the columns of an infrared image a movie deflickering algorithm. The obtained single image method works on static images, and therefore requires no registration, no camera motion compensation, and no closed aperture sensor equalization. Thus, the method has only one camera dependent parameter, and is landscape independent. This simple method will be compared to a state of the art total variation single image correction on raw real and simulated images. The method is real time, requiring only two operations per pixel. It involves no test-pattern calibration and produces no “ghost artifacts”.","Infrared (IR) imaging has proved to be a very efficient tool in a wide range of industry, medical, and military applications. IR cameras are used to measure temperatures, IR signatures, detection, etc. However, the performance of the imaging system is strongly affected by a random spatial response of each pixel sensor. Under the same illumination the readout of each sensor is different. This is due to mismatches in the fabrication process, among other issues [1]. Furthermore for uncooled cameras the problem is even worse because the sensor response non-uniformity is not stationary and slowly drifts in time. For this kind of camera a periodic update of the non-uniformity-correction (NUC) is required. A good non-uniformity-correction is a key success factor for any post processing such as pattern recognition, image registration, etc. To get the rid of the non-uniformity, two main kinds of methods have been developed: • Calibration based techniques consist in an equalization of the response to an uniform black body source radiation. They are not convenient for real time applications, since they force to interrupt the image flow. (This calibration is usually automatic, a shutter closing in front of the lens periodically). • Scene based techniques, involving motion compensation or temporal accumulation. Such methods are complex and require certain observation conditions. The perturbation model is zt⁢(X)=f(X,t)⁢(u⁢0t⁢(X))+ηt⁢(X)subscript𝑧𝑡𝑋subscript𝑓𝑋𝑡𝑢subscript0𝑡𝑋subscript𝜂𝑡𝑋z_{t}(X)=f_{(X,t)}(u0_{t}(X))+\eta_{t}(X)italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_X ) = italic_f start_POSTSUBSCRIPT ( italic_X , italic_t ) end_POSTSUBSCRIPT ( italic_u 0 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_X ) ) + italic_η start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_X ) where (X𝑋Xitalic_X is the position and t𝑡titalic_t is the time for the following) zt⁢(X)subscript𝑧𝑡𝑋z_{t}(X)italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_X ) is the observed value, u⁢0t⁢(X)𝑢subscript0𝑡𝑋u0_{t}(X)italic_u 0 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_X ) is the ideal landscape, f(X,t)subscript𝑓𝑋𝑡f_{(X,t)}italic_f start_POSTSUBSCRIPT ( italic_X , italic_t ) end_POSTSUBSCRIPT is the (unknown) transfer function of the sensor, and ηt⁢(X)subscript𝜂𝑡𝑋\eta_{t}(X)italic_η start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_X ) is a random sensor Poisson noise. A non-uniformity correction algorithm aims at discovering f(X,t)subscript𝑓𝑋𝑡f_{(X,t)}italic_f start_POSTSUBSCRIPT ( italic_X , italic_t ) end_POSTSUBSCRIPT or u⁢0t⁢(X)𝑢subscript0𝑡𝑋u0_{t}(X)italic_u 0 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_X ) for each X𝑋Xitalic_X and t𝑡titalic_t. In this paper we propose a single frame based algorithm and show that motion compensation or accumulation algorithms are not necessary to achieve a good image quality. However, the proposed method can be viewed as a first step fostering the success of more sophisticated motion based correction algorithms. These are slow while the proposed algorithm is real time, and the obtained quality after a single frame correction might be sufficient for many uses. The paper is organized as follows. Section 2 presents related works. The new algorithm is described in section 3. Experiments on simulated, real, cooled and uncooled images are described in section 4. Section 5 contains a discussion. Possible improvements are envisaged in section 6."
https://arxiv.org/html/2411.04420v1,BendVLM: Test-Time Debiasing of Vision-Language Embeddings,"Vision-language model (VLM) embeddings have been shown to encode biases present in their training data, such as societal biases that prescribe negative characteristics to members of various racial and gender identities. VLMs are being quickly adopted for a variety of tasks ranging from few-shot classification to text-guided image generation, making debiasing VLM embeddings crucial. Debiasing approaches that fine-tune the VLM often suffer from catastrophic forgetting. On the other hand, fine-tuning-free methods typically utilize a “one-size-fits-all"" approach that assumes that correlation with the spurious attribute can be explained using a single linear direction across all possible inputs. In this work, we propose Bend-VLM, a nonlinear, fine-tuning-free approach for VLM embedding debiasing that tailors the debiasing operation to each unique input. This allows for a more flexible debiasing approach. Additionally, we do not require knowledge of the set of inputs a priori to inference time, making our method more appropriate for online, open-set tasks such as retrieval and text guided image generation.111code: https://github.com/waltergerych/bend_vlm","Background. Pretrained foundation Vision-language models (VLMs) such as CLIP [33], BLIP [22], and LLaVA [25] have seen wide adoption for tasks like image retrieval [21], zero and few-shot classification [33, 4], text-guided image generation [32], and facial recognition [58]. But VL models also encode societal biases [5, 27, 43, 49, 53]. As more and more systems rely on CLIP, the encoded representational harm [12, 3, 15, 52] can lead to allocative harm [34, 46, 14, 51, 16, 29], such as Black individuals being three times more likely to be misclassified into a nonhuman category by computer vision systems [1]. State of the art. Debiasing VLMs is an active area of research [6, 10, 20, 19, 50, 28]. One common approach is finetuning the embedding models to remove spurious correlations [59, 2, 42]. However, finetuning often decreases accuracy and generalizability of foundation models [31]—a significant drawback as these models are commonly used for zero-shot tasks. Most existing finetuning-free methods learn debiasing transformations of the initial text embeddings, but typically use one-size-fits-all linear debiasing functions that apply the same fixed transformation to every input [6, 10, 50]. While recent work has explored nonlinear VLMs [11], their method assumes access to the set of classes at test-time, requiring the debiasing training pipeline to be rerun if a query for a new class is made. This is a major limitation in practice because many tasks VLMs are used for are often naturally open-set, where the classes to be evaluated for at test-time are unknown prior to inference. Problem Definition. We study online, open-set debiasing for VLM embeddings. In this setup, we only have access to a VLM, along with a single-modal image dataset. This image dataset is only for the purpose of ""training"", and is not the dataset that the downstream task will work on. We assume that this dataset, which we call the reference dataset, has labels for the protected attribute(s) of interest. During test-time, we receive online input queries one at a time. These queries are also open-set, meaning that the classes or concepts they refer to are not known to us beforehand. For instance, the query may be ""a photo of a nurse"", but we do not have knowledge that nurse is a potential class of interest before receiving the query. Our goal is to debias the query embedding from the VLM in such as way that it does not more strongly associate the query embedding with any protected attribute value over another. For instance, the embedding for ""a photo of a nurse"" should not be more associated with images of women than with men. Challenges. Online, open-set VLM debiasing is a challenging task. First, we must overcome catastrophic forgetting—a solution that debiases the embeddings, but degrades performance. Second, the interaction between protected attributes and query classes may be nonlinear and instance-dependent. For example, the transformation required to remove the gender bias from the embedding of ""nurse"" is likely not the same as the one to untangle gender bias associated with the embedding of ""handyman"". Third, queries from open-set classes means that our approach must be flexible enough to remove the association of protected attributes from classes unknown prior to inference time. Lastly, online settings demand computational efficiency and thus rule out refitting the debiasing component for each now class or query. Proposed approach. We propose Bias Elimination with Nonlinear Debiasing of Vision Language Models (Bend-VLM), a test-time VLM debiasing method that leaves the VLM’s weights unchanged, being efficient enough for online streaming queries. By using the easy-to-get pre-debiasing reference dataset with protected attributes, Bend-VLM allows for unsupervised test-time debiasing. On a high level, Bend-VLM consists of two main parts: First, given an online query, we generate augmented queries that introduce protected attribute information. For example, given ""a photo of a nurse"" we generate ""a photo of a {{\{{ATTRIBUTE}}\}} nurse"", filling in {{\{{ATTRIBUTE}}\}} with male / female / nonbinary for gender debiasing, for instance. We get these augmented queries from a small language model, and use them to find the directions in the embedding space for that specific query that are most associated with the protected attribute. Given these directions, we project the embedding such that it is orthogonal to the protected attribute dimension, resulting in the first-stage debiased representation. For the second step, we make use of the reference image dataset. We find the images in this dataset that are most associated with the query, and then subset them by protected attribute value. We find an updated, debiased query representation by solving a constrained optimization equation with the goal of finding an embedding with minimal distance to the first-stage debiased representation while being equally similar to the example images for each attribute value. For instance, we find an embedding that is equally similar to the nearest images for each gender. The resulting embedding will have little to no excess association with any of the debiased protected attribute values over any other. The output can then be passed to the downstream task. Contributions. • We introduce Bend-VLM, a novel test-time VLM debiasing approach that does not require finetuning. • We propose a technique for finding local attribute subspaces specific to each query on-the-fly. • We introduce a novel method for equalization by using a reference image dataset. • We experimentally evaluate for classification, retrieval, and image captioning settings, showing Bend-VLM consistently outperforms the compared approaches."
https://arxiv.org/html/2411.04406v1,Image Understanding Makes for A Good Tokenizer for Image Generation,"Modern image generation (IG) models have been shown to capture rich semantics valuable for image understanding (IU) tasks. However, the potential of IU models to improve IG performance remains uncharted. We address this issue using a token-based IG framework, which relies on effective tokenizers to project images into token sequences. Currently, pixel reconstruction (e.g., VQGAN) dominates the training objective for image tokenizers. In contrast, our approach adopts the feature reconstruction objective, where tokenizers are trained by distilling knowledge from pretrained IU encoders. Comprehensive comparisons indicate that tokenizers with strong IU capabilities achieve superior IG performance across a variety of metrics, datasets, tasks, and proposal networks. Notably, VQ-KDCLIP achieves 4.104.104.104.10 FID on ImageNet-1k (IN-1k). Visualization suggests that the superiority of VQ-KD can be partly attributed to the rich semantics within the VQ-KD codebook. We further introduce a straightforward pipeline to directly transform IU encoders into tokenizers, demonstrating exceptional effectiveness for IG tasks. These discoveries may energize further exploration into image tokenizer research and inspire the community to reassess the relationship between IU and IG. The code is released at https://github.com/magic-research/vector_quantization.","Image understanding (IU) and image generation (IG) have been the core pursuits of computer vision research for a long time. Thanks to the progress in generative models [35, 37, 15, 36, 48] and network architectures [43, 9], IG has witnessed remarkable advancements in recent years. These advancements spurred extensive research on leveraging powerful IG models for IU tasks (Fig. 1). Studies have shown that IG models can benefit IU tasks in various ways, including data augmentation through synthetic data generation [51, 46, 45], improved representation learning [28, 16, 52], and utilizing intermediate features from IG models for solving perception tasks [53, 26]. However, the reciprocal question remains largely uncharted: how might IU models aid IG tasks? The primary focus of this paper lies in the AutoRegressive (AR) IG framework, which is gaining considerable attention for its excellence in generating high-quality images and videos [48, 50, 20]. This framework operates in a two-stage process. The first stage learns a tokenizer to map images into sequences of discrete tokens. Subsequently, the second stage trains a proposal network to model the token sequences. As underlined by prior research [49, 4], the quality of the tokenizers significantly influences overall IG performance. Meanwhile, tokenizers and IU encoders adhere to a similar structure as they both aim to map images into latent representations, either discrete or continuous. As a result, the token-based IG framework provides an optimal environment for investigating the relationship between IU and IG. Through comprehensive studies, we demonstrate that existing IU models from representation learning can be useful in generative models, even if they are not specifically designed for the IG task. Our study involves training three components within the AR framework: tokenizer, decoder, and proposal network. Traditionally, pixel reconstruction has been the dominant objective for training tokenizers, such as VQGAN [10] and FSQ [25]. To the best of our knowledge, we are the first to systematically demonstrate that feature reconstruction (VQ-KD [27]) achieves better IG performance. 111VQ-KD was initially proposed for image pretraining. This approach distills knowledge from pretrained IU encoders to tokenizers. Therefore, the training strategy of the IU encoder is crucial for the performance of the tokenizer. In this regard, we investigate four representative IU encoders: ViT [9], CLIP [29], DINO [3], and MAE [13]. Following VQGAN [10], we train decoders to restore pixel values from discrete tokens, and proposal networks (AR or NAR) that can model the distribution of image tokens. The models are then evaluated using various metrics, including codebook usage, Fréchet Inception Distance (FID) [14], Inception Score (IS) [34], perplexity (PPL), etc. Initially, we compare the above tokenizers on IN-1k for class-conditional IG. VQ-KD achieves 4.104.104.104.10 FIDAR, outperforming VQGAN (15.7815.7815.7815.78 FIDAR) by a large margin. FSQ experiments confirm that the superiority of VQ-KD is not solely attributable to the specific quantization operation or high codebook usage. More generally, VQ-KD consistently outperforms across different proposal networks, datasets, and tasks. We analyze VQ-KD from multiple perspectives. By visualizing the codebook, we discover that codes from VQ-KD carry more semantics than VQGAN, which makes them easier to model and subsequently improve the IG quality. Building upon this insight, we propose a straightforward pipeline to efficiently transform IU encoders into tokenizers, outperforming VQ-KD on the MS-COCO dataset. We also find that tokenizers with weaker IU capabilities require larger proposal networks for effective AR modeling and show less robustness to variations in the training images. Finally, we conduct qualitative analysis to present the visual results. In sum, the key insights from our study include the following: 1) This research is the first to demonstrate that IU models can substantially enhance IG through VQ-KD; 2) Tokenizers with strong IU capabilities consistently outperform conventional VQGAN-based methods across various metrics, datasets, tasks, and network architectures; 3) The VQ-KD codebook encapsulates more semantics than VQGAN, contributing to the superiority of VQ-KD in IG. We believe these findings can benefit future research on image tokenizers and provoke further discussion on the relationship between IU and IG. Figure 1: Extensive studies have tried to adopt IG models for IU. However, few attempts have been made to use IU models in IG."
https://arxiv.org/html/2411.04399v1,ProGraph: Temporally-alignable Probability Guided Graph Topological Modeling for 3D Human Reconstruction,"Current 3D human motion reconstruction methods from monocular videos rely on features within the current reconstruction window, leading to distortion and deformations in the human structure under local occlusions or blurriness in video frames. To estimate realistic 3D human mesh sequences based on incomplete features, we propose Temporally-alignable Probability Guided Graph Topological Modeling for 3D Human Reconstruction (ProGraph). For missing parts recovery, we exploit the explicit topological-aware probability distribution across the entire motion sequence. To restore the complete human, Graph Topological Modeling (GTM) learns the underlying topological structure, focusing on the relationships inherent in the individual parts. Next, to generate blurred motion parts, Temporal-alignable Probability Distribution (TPDist) utilizes the GTM to predict features based on distribution. This interactive mechanism facilitates motion consistency, allowing the restoration of human parts. Furthermore, Hierarchical Human Loss (HHLoss) constrains the probability distribution errors of inter-frame features during topological structure variation. Our Method achieves superior results than other SOTA methods in addressing occlusions and blurriness on 3DPW.","3D human pose and shape reconstruction models find applications in various fields such as medical rehabilitation, game development, sports analysis, and clothing design (Lupión et al. 2024; Du et al. 2024; Yan et al. 2024). Scenes captured by monocular cameras often exhibit occlusion and depth blur due to motion. One of the significant technical challenges that arise in this context is accurately estimating the 3D human body mesh sequence based on incomplete human features in single-view scenarios. 3D human body video reconstruction aims to estimate the real 3D human body mesh, and its technical means can be roughly classified into regression-based reconstruction methods and probability distribution-based reconstruction methods. Regression methods focus primarily on frame prediction and lack cross-frame feature dependency modeling without exploiting long-term dependencies. To reconstruct a realistic 3D human body mesh under occlusion blurred segments, methods based on direct learning of spatial position alignment of 3D human body geometrical structures for direct regression are employed (Cho, Youwang, and Oh 2022a; Goel et al. 2023). These methods can perceive the topological structure of the human body in the current frame. However, these methods make it difficult to predict the motion features of the same sequence over long distances between different frames. When the body moves, occluding, etc., features can be lost, resulting in inaccurate and distorted reconstruction (Sengupta, Budvytis, and Cipolla 2022). The other line of work Hierarchical autoregressively learns the rotation probability distribution of each joint according to the topological structure of the human kinematic tree. This is done by utilizing the distribution to improve the accuracy of joint reconstruction in occlusion-ambiguous scenarios (Sengupta, Budvytis, and Cipolla 2021). However, methods based on probability distribution also have limitations. Since the number of joints in the whole body is relatively sparse, it is difficult to represent the features of the joints in the whole body only through rotation probability distribution, which often leads to distortion and muscle stretching reconstruction results. Therefore, the introduction of topological relationships between vertices of the human body mesh is of vital importance in the probability distribution-based approach. To align human body topology with 3D spatial features, we introduce a framework named ProGraph. This approach involves creating a probability distribution through human body mesh vertex topology-guided prediction. The framework aims to align features with the 3D geometry of the human body by constructing probability distributions based on feature spaces of the same topology in various frames within a sequence. Specifically, it focuses on capturing potential long-distance motion feature information and addressing missing features to guide the generation of human body topology. ProGraph is organized into three modules. Graph Topological Modelling (GTM), uses the spatial dimension-based Graph Network module to explicitly learn the topological connections among the vertices of the body mesh and to transform the implicit topological connections within the human body into an explicit representation of a graph structure. This explicit connection ensures that in case of local node loss, it is possible to predict the missing nodes based on the graph structure. The first module can learn the topological associations implicit in the human body from the current frame, but in order to maintain the stability of reconstruction results, the cross-frame feature dependencies implicit in the feature space are also needed, which can be expressed as a probability distribution over the 3D geometry of the human body. Therefore, we developed Temporally-alignable Probability Distribution (TPDist), which uses a time-guided latent diffusion module to align the probability distributions of the implied human 3D geometric structures between the adjacent frames in a time-guided manner. This approach is able to predict the probability distribution of the missing features with the aid of cross-frame dependency loss, in order to guide the regression of human topology and construct the coherence of the human mesh sequence. Meanwhile, to hierarchically calculate the error between the predicted results and the true values of different body parts, we proposed Hierarchical Human Loss (HHLoss), a progressive part-aware spatio-temporal loss to guide perception through the body. Finally, we use a non-parametric model reconstruction method to accurately restore the original human muscle movements. Our contributions mainly lie in: 1. To the best knowledge, we make the first attempt to combine the temporally-alignable probability and graph Topology in 3D human pose and shape estimation. The proposed Temporally-alignable Probability Guided Graph Topological Modeling (ProGraph) merges the knowledge from probability distribution and human prior structures, improving our method’s accuracy. 2. In ProGraph, we carefully design Hierarchical Human Loss (HHLoss), for accurately calculating the progressive probability error of different body parts. 3. The experimental results reveal the effectiveness of ProGraph in 3D human mesh reconstruction, particularly its ability to excel in challenging scenarios involving occlusions and motion blur. Notably, our approach outperforms the state-of-the-art (SOTA) in video-based reconstruction on the 3DPW (von Marcard et al. 2018) dataset."
https://arxiv.org/html/2411.04357v1,MegaPortrait: Revisiting Diffusion Control for High-fidelity Portrait Generation,"We propose MegaPortrait, an innovative system tackling the challenge of personalized portrait image creation in computer vision. Through three meticulously designed modules – Identity Net, Shading Net, and Harmonization Net – we achieve a delicate balance between identity preservation and artistic adaptation. Leveraging a customized model fine-tuned with source images, Identity Net generates learned identity, while Shading Net re-renders portrait images using extracted representations. Harmonization Net seamlessly fuses pasted faces and the reference image’s body, ensuring coherent final results. Our approach, utilizing off-the-shelf Controlnets, surpasses state-of-the-art AI portrait products in both identity preservation and image fidelity, establishing MegaPortrait as a significant advancement in the field. Without additional so-called novelties, MegaPortrait introduces a simple but effective system design in tackling a well-established problem and we compare our results with the state-of-the-art AI portrait methods as well as products to demonstrate the superiority of our proposed system in identity preserving and image fidelity.","The development of AI-driven portrait generation within the framework of diffusion models encounters multifaceted challenges necessitating innovative solutions. The central issue revolves around striking a delicate balance: preserving subject identity while enabling effective generalization across diverse artistic styles. This persistent challenge has historically impeded progress in the field. For generative portrait models to succeed, they must seamlessly adapt to various cues and preferences, ensuring that the resulting images faithfully capture the unique characteristics of each subject while embodying the intended style or mood. Addressing this dilemma directly, our proposed method, “MegaPortrait”, offers a solution by decoupling shading or color information from the geometry through our split-and-merge pipeline. This approach allows us to generate coherent results aligned with the style reference image, overcoming the longstanding obstacle of balancing subject identity and artistic expression. To shed insight on this challenging problem, identity and style trade-off, instead of training a diffusion network from scratch to directly model the mapping relationship, we try to build a portrait generation system by adopting different controlnets [22] and carefully picking the input conditions. Our system can be recognized as a free-lunch application of controlnet and provides new insights on designing next-level generative applications. Current AI portrait designs basically focus on training better identity encoder such as [20, 21, 19], but fail to maximize the conventional generative power in pretrained diffusion models [13]. The capabilities in such design are limited by the design choices of encoders. Inspired my portrait relighting [11], we try to decouple the shading and structure first and fuse them together. Specifically, the comprehensive MegaPortrait pipeline consists of three carefully designed modules: Identity Net, Shading Net, and Harmonization Net, which are collectively designed to produce high-fidelity and visually compelling personalized portrait images. In the initial stage, the identity network dynamically generates a pair of images, each embodying a different identity coloring trade-off. From this pair of images, we extract the illumination map from the image through a low-pass filter wise, exhibiting a more reference-like quality, while deriving a high-frequency map (HF-Map) from the image with a more source-like appearance. This dual extraction process ensures a meticulous rendering of subject identity and shadow characteristics. The subsequent Shading Net module uses these extracted light maps and HF-Map as control conditions to perform the synthesis process and generate geometrically correct and stylized portrait images. The images were carefully crafted to balance shadow and identity, then seamlessly pasted back onto the reference image. The final harmonization step is expertly handled by Harmonization Net, which integrates the composite portrait with the reference image, ensuring the final result is cohesive and visually harmonious. This complex orchestration of modules in the MegaPortrait pipeline embodies our commitment to achieving a delicate balance between preserving personal identity and adapting to different artistic styles. To summarize our contributions: 1) We propose a principled pipeline, MegaPortrait, which effectively addresses the identity-style dilemma in AI-driven portrait generation. By adopting different controlnets and carefully selecting input conditions, our system represents a free-lunch application of controlnet, providing new insights for designing next-level generative applications. 2) To shed light on the challenging identity and style trade-off problem, we introduce a novel approach of decoupling shading or color information from geometry through our split-and-merge pipeline. This technique, inspired by human relighting practices, is the first of its kind applied in realistic AI portrait generation, offering a unique perspective on overcoming the intricacies of the mapping relationship. 3) Our comprehensive MegaPortrait pipeline, consisting of Identity Net, Shading Net, and Harmonization Net, produces high-fidelity and visually compelling personalized portrait images. By surpassing research methods and top products such as Remini, both quantitatively and qualitatively, our results demonstrate the effectiveness of our approach in maximizing generative power within pretrained diffusion models. Figure 3: The cross extensive results of MegaPortrait with various source IDs and reference styles. The reference images are blue-boxed and the source images are red-boxed."
https://arxiv.org/html/2411.04351v1,LidaRefer: Outdoor 3D Visual Grounding for Autonomous Driving with Transformers,"3D visual grounding (VG) aims to locate relevant objects or regions within 3D scenes based on natural language descriptions. Although recent methods for indoor 3D VG have successfully leveraged transformer-based architectures to capture global contextual information and enable fine-grained cross-modal fusion, they are unsuitable for outdoor environments due to differences in the distribution of point clouds between indoor and outdoor settings. Specifically, first, extensive LiDAR point clouds demand unacceptable computational and memory resources within transformers due to the high-dimensional visual features. Second, dominant background points and empty spaces in sparse LiDAR point clouds complicate cross-modal fusion owing to their irrelevant visual information. To address these challenges, we propose LidaRefer, a transformer-based 3D VG framework designed for large-scale outdoor scenes. Moreover, during training, we introduce a simple and effective localization method, which supervises the decoder’s queries to localize not only a target object but also ambiguous objects that might be confused as the target due to the exhibition of similar attributes in a scene or the incorrect understanding of a language description. This supervision enhances the model’s ability to distinguish ambiguous objects from a target by learning the differences in their spatial relationships and attributes. LidaRefer achieves state-of-the-art performance on Talk2Car-3D, a 3D VG dataset for autonomous driving, with significant improvements under various evaluation settings.","3D visual grounding (VG) aims to locate objects or regions within 3D visual scenes guided by natural language descriptions. This task is crucial for enhancing interactions between humans and agents in various applications, such as autonomous driving, robotics, and VR/AR [4, 39, 5, 34, 31, 36]. In autonomous driving, precisely recognizing the locations of specific objects enables vehicles to make correct decisions based on human instructions. For example, in Fig. 1(a), 3D VG allows a vehicle to pinpoint a ""gray car"", guiding it to a designated parking spot. Currently, most 3D VG research [16, 14, 33, 28, 46, 44, 18] has focused on indoor scenarios with various datasets [2, 6, 49, 1, 47] obtained through RGB-D cameras. Outdoor 3D VG is more challenging due to the extensive and sparse distribution of point clouds captured by LiDAR sensors and the frequent coexistence of objects in the same category as a target compared to indoor settings. To the best of our knowledge, MSSG [9] is a pioneering LiDAR-based outdoor 3D VG model for autonomous driving systems. However, it merely focuses on the correlation between a sentence-level text feature and distinct visual features for target identification, neglecting the meaning of individual words in a description and the contextual information among visual objects. This can lead to two issues: 1) When a description mentions multiple objects and attributes, a sentence-level text feature may not retain the specific meanings of individual words, failing to align adequately with corresponding visual features during cross-modal fusion. In Fig. 1(a), MSSG may incorrectly align visual features with the mentioned objects (""car"" and ""truck"") and their colors (""gray"" and ""yellow""). 2) Furthermore, it also may struggle to identify the target ""gray car"", from two similar objects (red and blue circles) due to a lack of consideration for understanding the spatial relationship with ""yellow truck"" (green circle). Figure 1: (a) Our transformer-based outdoor 3D VG framework utilizes 3D point clouds and RGB images as visual inputs, along with a language description as text input. (b) Some non-target objects can lead to ambiguity when they share attributes with the target or are mentioned in the description. The differences in their spatial relationships or attributes can distinguish these objects from the target. Recent indoor 3D VG models [20, 41] employ a transformer-based cross-modality encoder to effectively capture global contextual information within each modality and achieve the fine-grained alignment between word-level text features and their corresponding visual counterparts. Despite their effectiveness, directly ap-plying this approach to outdoor 3D VG demands unacceptable computational and memory resources for processing high-dimensional visual features within transformers [37]. Even if feasible, the sparsity and dominance of background points in outdoor scenes can complicate cross-modal interactions due to their irrelevant visual information. To this end, we propose LidaRefer, a transformer-based 3D VG framework, for autonomous driving in large-scale outdoor scenes. LidaRefer consists of five modular networks, as depicted in Fig. 1(a). Initially, visual and text data are encoded into deep features using separate modality encoders. Subsequently, inspired by recent 3D outdoor detectors [51, 11, 32], useful foreground features where objects are likely to occur are selected from high-dimensional visual features to efficiently retain a small number of relevant visual features. The selected visual and word features are fed into a transformer-based cross-modality encoder to capture contextual information and perform a fine-grained cross-modal alignment. Then, a target identification network determines a target query based on target confidence scores derived from cross-modal features. Finally, the target query is refined through interactions with the cross-modal features within the decoder to predict a 3D bounding box for the target object. In VG scenarios, some non-target objects that exhibit similar attributes to the target or are mentioned in a description may be confused for the target, leading to ambiguity in target identification, as shown in Fig. 1(b). To deal with this, we introduce a simple and effective supervision method, ambiguous object localization, by considering the differences between the target and each type of ambiguous object. This method localizes both the target and some non-target objects that exhibit the highest confidence scores in target identification during training, thus enhancing an ability to distinguish ambiguous objects from the target by learning the differences derived from their spatial relationships or attributes within a scene. Our contributions can be summarized as follows: • We propose LidaRefer, a transformer-based 3D VG framework, to effectively understand global contextual information and facilitate cross-modal fusion in large-scale outdoor scenes for autonomous driving. • We introduce a simple and effective localization meth-od to distinguish ambiguous objects from the target by recognizing the differences in spatial relationships and attributes. • We demonstrate LidaRefer’s superiority by achieving state-of-the-art results on Talk2Car-3D dataset and analyzing the effects under various evaluation settings."
https://arxiv.org/html/2411.04348v1,UEVAVD: A Dataset for Developing UAV’s Eye ViewActive Object Detection,"Occlusion is a longstanding difficulty that challenges the UAV-based object detection. Many works address this problem by adapting the detection model. However, few of them exploit that the UAV could fundamentally improve detection performance by changing its viewpoint. Active Object Detection (AOD) offers an effective way to achieve this purpose. Through Deep Reinforcement Learning (DRL), AOD endows the UAV with the ability of autonomous path planning to search for the observation that is more conducive to target identification. Unfortunately, there exists no available dataset for developing the UAV AOD method. To fill this gap, we released a UAV’s eye view active vision dataset named UEVAVD and hope it can facilitate research on the UAV AOD problem. Additionally, we improve the existing DRL-based AOD method by incorporating the inductive bias when learning the state representation. First, due to the partial observability, we use the gated recurrent unit to extract state representations from the observation sequence instead of the single-view observation. Second, we pre-decompose the scene with the Segment Anything Model (SAM) and filter out the irrelevant information with the derived masks. With these practices, the agent could learn an active viewing policy with better generalization capability. The effectiveness of our innovations is validated by the experiments on the UEVAVD dataset. Our dataset will soon be available at https://github.com/Leo000ooo/UEVAVD__\__dataset.","I INTRODUCTION In recent years, Unmanned Aerial Vehicles (UAVs) have made a big splash in many applications like traffic monitoring [1], industrial facilities inspection [2], and post-disaster search and rescue [3] owing to its high flexibility and strong maneuverability. Behind these applications, target detection is an indispensable key technology [4], which aims to locate and identify targets from aerial images, providing important prior information for subsequent actions. Since the renaissance of deep learning, the Deep Neural Network (DNN) based methods, such as Faster-RCNN [5], YOLO [6], SSD [7], and their variants have gradually become the mainstream in the field of UAV target detection [8]. However, in the air-to-ground scenario, detecting a target would inevitably encounter challenge of the occlusion among terrestrial objects, which greatly diminishes the detection performance [9]. To address this difficulty, most existing methods make adaptive improvements to the detection model. For example, literature [10] uses the Soft-NMS method to suppress redundant prediction frames during post-processing to alleviate the occlusion problem. However, the detector’s ability of anti-occlusion is still far from satisfactory [11]. Fundamentally, the quality of the input data limits the upper bound of detection performance. If the UAV could autonomously change the viewing angle to get the observation more conducive to identifying the target, the detection performance would greatly improve. To achieve this, Active Object Detection (AOD) is a practical way. Figure 1: A demonstration of the AOD decision process by the UAV platform. The UAV’s policy network gives moving instructions based on observations. It can autonomously decide whether to move and how to move to acquire an ideal observation while minimizing the movement cost. AOD is a subfield that utilizes active vision in target detection, and its objective is to improve the detection result by endowing the mobile sensing platform the ability to actively adjust the viewing angle to better identify the target whose location is given at first [12]. As shown in Fig. 1, the UAV cannot determine the target identity under some viewpoints due to occlusion. At this time, the UAV has to decide whether to move and how to move to acquire an ideal observation while minimizing the movement cost. Deep Reinforcement Learning (DRL) has now become a mainstream framework for solving the AOD problems [13]. Ammirato et al. first used the REINFORCE algorithm for AOD tasks and released the Active Vision Dataset (AVD) collected in indoor environments for developing and benchmarking active vision algorithms [14]. Han et al. solved the AOD problem by a Dualing Deep Q-learning Network (DualingDQN) with prioritized experience replay for the first time [15]. Fang et al. used self-supervised representation learning to improve the sample efficiency of the DRL method [12]. Based on literature [16], Liu et al. incorporated the target crop into the state representation and designed a novel reward function to help the robot approach the target object more smoothly [17]. However, existing AOD researches mainly focu on indoor robotic applications, neglecting the outdoor air-to-ground circumstances. For the indoor environment, there are several datasets concerning active vision published, such as the AVD [14], T-LESS [18], and R3ED [19] dataset, all of which collect the multi-view target images indoor under various occlusion conditions. However, there is no similar counterpart in the scenario of the UAV’s eye view AOD. Although the existing datasets (e.g., VisDrone-DET dataset [20]) for UAV-based target detection commonly cover abundant environment settings and targets, they cannot be used for studying the air-to-ground AOD problem because of lacking densely collected multi-view images over the targets. To fill this gap, we release a UAV’s Eye View Active Vision Dataset (UEVAVD) and hope it can facilitate research on the AOD problem in the air-to-ground scenario. The dataset is collected in the simulated environment constructed by Unreal Engine (UE), focusing on five types of vehicular targets. UAV observes each target under different environment settings (different occlusions, different terrains), and its predefined sampling positions are densely and regularly arranged over the target. Various combinations of the samples can be seen as the continuous observation result by the UAV while flying along different routes, laying a foundation for future research on the UAV AOD problem. Besides, given that a good state representation can dramatically strengthen the effectiveness, robustness, and generalization capability of the agent’s policy [21], we improve the existing AOD method by incorporating inductive biases into the state representation learning process. Specifically, for the AOD task, two aspects of prior knowledge can be exploited. First, due to the partial observability, the state representation of the agent needs to be extracted from the historical observation sequence instead of relying on the single-view observation. Therefore, we utilize a combination of the convolutional neural network and the Gated Recurrent Unit (GRU) to extract state representation from the observation sequence. Second, what should be stressed are the characteristics of the target itself in terms of its appearance, pose, and the positional relationship between the target and its surroundings, whereas features such as the color and texture of the terrestrial objects can be discarded. Therefore, we pre-decompose the scene using the powerful Segment Anything Model (SAM) and filter out the irrelevant information with the masks derived. With these practices, the backbone of the decision network could learn better state representation, thus improving the performance of the agent’s policy. The main contributions of this paper are summarized as follows. 1. We release a new dataset, UEVAVD, aiming to promote the research on the UAV’s eye view AOD problem. On this basis, we could find out how to better exploit the UAV’s autonomy and maneuverability to overcome difficulties like occlusion in UAV-based object detection. 2. We improve the existing DRL-based AOD method by incorporating the inductive bias while learning the state representation. With the practice of scene pre-decomposition and memory based state estimation, the policy learned by the agent could get stronger generalization capability and perform better in the testing environment."
https://arxiv.org/html/2411.04335v1,GazeGen: Gaze-Driven User Interaction for Visual Content Generation,"We present GazeGen, a user interaction system that generates visual content (images and videos) for locations indicated by the user’s eye gaze. GazeGen allows intuitive manipulation of visual content by targeting regions of interest with gaze. Using advanced techniques in object detection and generative AI, GazeGen performs gaze-controlled image adding/deleting, repositioning, and surface material changes of image objects, and converts static images into videos. Central to GazeGen is the DFT Gaze (Distilled and Fine-Tuned Gaze) agent, an ultra-lightweight model with only 281K parameters, performing accurate real-time gaze predictions tailored to individual users’ eyes on small edge devices. GazeGen is the first system to combine visual content generation with real-time gaze estimation, made possible exclusively by DFT Gaze. This real-time gaze estimation enables various visual content generation tasks, all controlled by the user’s gaze. The input for DFT Gaze is the user’s eye images, while the inputs for visual content generation are the user’s view and the predicted gaze point from DFT Gaze. To achieve efficient gaze predictions, we derive the small model from a large model (10x larger) via novel knowledge distillation and personal adaptation techniques. We integrate knowledge distillation with a masked autoencoder, developing a compact yet powerful gaze estimation model. This model is further fine-tuned with Adapters, enabling highly accurate and personalized gaze predictions with minimal user input. DFT Gaze ensures low-latency and precise gaze tracking, supporting a wide range of gaze-driven tasks in AR environments. Leveraging these precise gaze predictions, GazeGen facilitates visual content generation through diffusion processes, allowing users to intuitively manipulate visual content by targeting regions with their gaze. Additionally, it enables real-time object detection by focusing on specific regions indicated by the user’s gaze, improving responsiveness. We validate the performance of DFT Gaze on AEA and OpenEDS2020 benchmarks, demonstrating low angular gaze error and low latency on the edge device (Raspberry Pi 4). Furthermore, we describe applications of GazeGen, illustrating its versatility and effectiveness in various usage scenarios.","Recent advancements in visual content editing interfaces have highlighted the need for systems that are both intuitive and accessible. Traditional methods often rely on physical manipulation, which can be limiting, especially for individuals with physical disabilities. To address this, we introduce GazeGen, a system leveraging eye gaze for hands-free interaction, enhancing user engagement and accessibility beyond conventional augmented reality (AR) environments. By utilizing natural human behavior—where gaze directs attention and guides actions—GazeGen provides a straightforward interface for managing and interacting with digital content. This approach capitalizes on instinctual behaviors, such as looking and seeing, to simplify complex operations, making GazeGen more user-friendly and widely accessible. Consider a designer adjusting visual elements in a digital design platform. Traditionally, this task requires manual adjustments, which can be cumbersome and time-consuming. With GazeGen, the designer simply looks at the elements they want to adjust. The system interprets these gaze points as commands, enabling immediate and precise edits. Real-time eye interaction is crucial as it allows for seamless and intuitive control, and since everyone has different eye shapes and movements (He et al. 2019; Krafka et al. 2016; Zhang et al. 2018; Yu, Liu, and Odobez 2019; Park et al. 2019; Lindén, Sjöstrand, and Proutière 2019; Liu et al. 2018, 2021a; Chen and Shi 2020; Liu et al. 2024), personalization is essential for accuracy. This capability not only accelerates the creative process but also makes it more inclusive, allowing anyone to express their creativity regardless of physical capabilities. At the core of GazeGen is the DFT Gaze (Distilled and Fine-Tuned Gaze) agent, an ultra-lightweight gaze estimation model designed for real-time, accurate predictions tailored to individual users. DFT Gaze captures gaze points in real time for both object retrieval and visual content manipulation. Integrating gaze estimation technology into visual content generation applications presents unique challenges, which GazeGen addresses through effective personalization for accurate gaze prediction and a lightweight design. The DFT Gaze agent is designed to be adaptable and efficient, requiring minimal computational resources for real-time interactions. It learns from general gaze patterns and supports easy personalization with just a few user-specific samples. With only 281K parameters, DFT Gaze is very compact, achieving performance comparable to larger models while operating 2x faster on edge devices (e.g., the Raspberry Pi 4). The lightweight and real-time capabilities of DFT Gaze enable direct manipulation of objects through eye gaze. This allows users to interact with digital content naturally and intuitively, enabling hands-free interactions in AR environments. We demonstrate the broad applications of GazeGen in Fig. 2. With advanced object detection and generative AI methods, GazeGen extends the functionality of eye gaze from simple tracking to dynamic visual content manipulation. Users can perform complex tasks such as adding, deleting, repositioning elements, and even transforming static images into videos, all through their gaze. This capability makes visual content creation accessible to everyone, regardless of physical limitations, and enhances the creative process with a seamless, unobtrusive interface. To support these advanced functionalities, we begin by developing a compact gaze estimation model through knowledge distillation. This process preserves the teacher model’s knowledge while significantly reducing computational complexity by reconstructing the teacher’s features using self-supervised learning. To achieve accurate gaze estimation, we integrate Adapters into this model, allowing it to learn diverse gaze patterns and personalize predictions for individual users. With this robust gaze estimation foundation, GazeGen extends its capabilities to real-time object detection by leveraging gaze points to focus on specific regions of the image, retrieving object categories and bounding boxes. For visual content generation, GazeGen uses gaze as a natural command for dynamic image editing and video creation, enabling intuitive operations such as addition, deletion, repositioning, and material transfer. This comprehensive approach allows users to seamlessly manipulate visual content through their gaze, setting a new standard for accessibility and efficiency in the field. GazeGen offers a new standard in gaze-driven visual content generation with the following key contributions: 1. Use of Eye Gaze for Visual Content Manipulation: We are the first to propose using eye gaze for comprehensive visual content generation and editing, such as adding, deleting, repositioning elements, material transfer, and generating videos. Additionally, GazeGen can detect and interact with objects based on where the user is looking, offering a hands-free and intuitive interface for content manipulation. 2. Compact and Efficient Gaze Model: We developed the DFT Gaze agent, a highly compact gaze estimation model with only 281K parameters, created through knowledge distillation coupled with a masked autoencoder. Our model leverages self-supervised learning techniques to reconstruct input images and teacher network features, effectively capturing the teacher’s knowledge. Despite its compact size, the student model shows minimal performance drop compared to the teacher model and achieves 2x faster performance on the edge device. 3. Enhanced User Experience: GazeGen leverages natural human behaviors, providing a seamless and intuitive interface for visual content manipulation. By personalizing gaze estimation with minimal samples, our system adapts to individual users, ensuring high accuracy and ease of use. 4. Broad Application Scope: We demonstrate the wide applicability of GazeGen in various scenarios. Fig. 2 illustrates the diverse potential applications of our system 111Text can be converted through voice., showcasing its versatility and effectiveness."
https://arxiv.org/html/2411.04332v1,HandCraft: Anatomically Correct Restoration of Malformed Handsin Diffusion Generated Images,"Generative text-to-image models, such as Stable Diffusion, have demonstrated a remarkable ability to generate diverse, high-quality images. However, they are surprisingly inept when it comes to rendering human hands, which are often anatomically incorrect or reside in the “uncanny valley”. In this paper, we propose a method HandCraft for restoring such malformed hands. This is achieved by automatically constructing masks and depth images for hands as conditioning signals using a parametric model, allowing a diffusion-based image editor to fix the hand’s anatomy and adjust its pose while seamlessly integrating the changes into the original image, preserving pose, color, and style. Our plug-and-play hand restoration solution is compatible with existing pretrained diffusion models, and the restoration process facilitates adoption by eschewing any fine-tuning or training requirements for the diffusion models. We also contribute MalHand datasets that contain generated images with a wide variety of malformed hands in several styles for hand detector training and hand restoration benchmarking, and demonstrate through qualitative and quantitative evaluation that HandCraft not only restores anatomical correctness but also maintains the integrity of the overall image.","Text-to-image diffusion models, such as Stable Diffusion [25], have gained wide popularity due to their remarkable capability to generate diverse, high-quality images across a wide range of styles [27, 23]. However, they struggle to accurately render human hands, often producing anatomically incorrect or highly unusual forms [22]. These errors can include hands with supernumerary or missing digits, atypical relative finger lengths, and other distortions. Fig. 15 illustrates two cases of such malformed hands, with a missing finger in the top row and abnormal relative finger lengths in the bottom row. These examples highlight the discrepancy between the generated depictions and human anatomy. Due to humans’ high sensitivity to deviations from the expected human form, generating malformed hands often leads to an “uncanny valley” [21] effect, which affects the realism of these images. This in turn hinders the use of these models as artistic tools. We note here that we do not use the term “malformed” in the pejorative sense, since we recognize that a wide variety of hand shapes are naturally present in the human population or may arise from misadventure. That is, the model is inadvertently forming the hands atypically, rather than intentionally depicting the difference that exists in the human population. Diffusion models’ propensity for generating malformed hands has been widely recognized [19, 22, 3]. There has been growing interest for techniques to repair these malformed hands, reflected by a large number of tutorials across various languages for this purpose [2, 7, 15, 1]. However, the restoration methods proposed in these tutorials often necessitate human intervention. For instance, repeatedly inpainting the manually-annotated affected areas until a satisfactory outcome is achieved [12]. The requirement for human involvement makes the correction process laborious. Prompt engineering has also emerged as a popular strategy to mitigate the issue of malformed hands in images generated by diffusion models [17, 26]. By meticulously designing and refining text prompts, users attempt to guide the model towards generating more anatomically accurate hands [5]. Despite these efforts, even well-crafted prompts often fail to prevent the occurrence of malformed hands [4]. We introduce an end-to-end framework designed to repair malformed hands in generated images while minimizing the need for human intervention. To achieve this, we propose an approach for generating a hand shape as a conditioning image to guide ControlNet [29], a diffusion-based image editing method, in correcting malformed hands. Our method is capable of responsively adjusting the size and angle of the hand shape, ensuring that the restored hand seamlessly integrates with the original human figure, while preserving the surrounding regions of the image unaltered. Experimental results demonstrate the robustness of our approach. Furthermore, our restoration process is designed to be plug-and-play, requiring no further fine-tuning or training of the ControlNet, and is therefore easy to integrate into various diffusion models. Our contributions are 1. HandCraft, a framework for detecting and restoring malformed hands generated by diffusion models while minimizing alterations to other image regions; 2. a simple yet robust control image generation method to construct a mask and an aligned depth image for the hand region as condition signals, enabling a diffusion-based image editor to restore malformed hands; and 3. the MalHand datasets, comprising portraits with malformed hands across diverse styles, that can be used to train a malformed hand detector and thoroughly evaluate baseline models. HandCraft achieves state-of-the-art performance on both the MalHand-realistic and MalHand-artistic datasets."
https://arxiv.org/html/2411.04269v1,Increasing the scalability of graph convolution for FPGA-implemented event-based vision,"Event cameras are becoming increasingly popular as an alternative to traditional frame-based vision sensors, especially in mobile robotics. Taking full advantage of their high temporal resolution, high dynamic range, low power consumption and sparsity of event data, which only reflects changes in the observed scene, requires both an efficient algorithm and a specialised hardware platform. A recent trend involves using Graph Convolutional Neural Networks (GCNNs) implemented on a heterogeneous SoC FPGA. In this paper we focus on optimising hardware modules for graph convolution to allow flexible selection of the FPGA resource (BlockRAM, DSP and LUT) for their implementation. We propose a ”two-step convolution” approach that utilises additional BRAM buffers in order to reduce up to 94% of LUT usage for multiplications. This method significantly improves the scalability of GCNNs, enabling the deployment of models with more layers, larger graphs sizes and their application for more dynamic scenarios.","Despite the rapid development of computer vision over the past 60 years many challenges remain – especially in dynamic environments characteristic for mobile robotics. As an answer to the requirements of vision systems in such applications, the scientific literature increasingly mentions a state-of-the-art neuromorphic sensor inspired by the structure of the human eye – the event camera or Dynamic Vision Sensor (DVS) [1]. Unlike traditional cameras which record brightness levels for all pixels at fixed time intervals, DVS records only changes in the observed scene – independently and asynchronously for each pixel. Each time the change in the logarithm of the brightness of a given pixel reaches a certain threshold, the camera generates an ’event’, which is described by four values {x,y,t,p}𝑥𝑦𝑡𝑝\{x,y,t,p\}{ italic_x , italic_y , italic_t , italic_p }, where x,y𝑥𝑦x,yitalic_x , italic_y are the coordinates of the pixel, t𝑡titalic_t is the timestamp (with a resolution of microseconds), and p𝑝pitalic_p is the polarity determining the positive or negative change. This approach has an interesting consequences. Recording only changes in the scene leads to a significant reduction in redundant information and lower average energy consumption. Moreover, the high temporal resolution reduces motion blur. Independent detection of the logarithm of brightness change for each pixel increases dynamic range, enabling effective operation in extreme and rapidly changing lighting conditions. However, processing event data, which can be described as a sparse point cloud in space-time, in a way that takes full advantage of this sensor is challenging. To ensure adequate performance and efficiency of event-based vision systems, it is necessary to use appropriate algorithms and hardware platforms. One potential solution under consideration is the implementation of Graph Convolutional Neural Networks (GCNNs) on heterogeneous FPGAs [2]. In this work, we analyse the resource utilisation of graph convolution modules and propose a method to significantly reduce LUT and DSP usage by limiting the number of redundant computations through the use of additional BRAM buffers. By proposing various module variants that prioritise certain resources (LUT, DSP or BRAM) while minimising the use of others, we enable the implementation of larger GCNNs. The remainder of this paper is organised as follows. In Section II, we briefly describe the existing work related to the discussed topic. In Section III we present the methods for using various FPGA resources to implement graph convolution. In Section IV we describe the conducted experiments, which allowed us to draw the conclusions described in Section V."
https://arxiv.org/html/2411.04255v1,Pose-Transformation and Radial Distance Clustering for Unsupervised Person Re-identification,"Person re-identification (re-ID) aims to tackle the problem of matching identities across non-overlapping cameras. Supervised approaches require identity information that may be difficult to obtain and are inherently biased towards the dataset they are trained on, making them unscalable across domains. To overcome these challenges, we propose an unsupervised approach to the person re-ID setup. Having zero knowledge of true labels, our proposed method enhances the discriminating ability of the learned features via a novel two-stage training strategy. The first stage involves training a deep network on an expertly designed pose-transformed dataset obtained by generating multiple perturbations for each original image in the pose space. Next, the network learns to map similar features closer in the feature space using the proposed discriminative clustering algorithm. We introduce a novel radial distance loss, that attends to the fundamental aspects of feature learning - compact clusters with low intra-cluster and high inter-cluster variation. Extensive experiments on several large-scale re-ID datasets demonstrate the superiority of our method compared to state-of-the-art approaches.","Person re-identification deals with matching the identity of a query image from a dataset consisting of person images taken across a disjoint set of cameras. There has been considerable research on this problem using supervised [Zhang et al.(2020)Zhang, Lan, Zeng, Jin, and Chen, Zhuang et al.(2020)Zhuang, Wei, Xie, Zhang, Zhang, Wu, Ai, and Tian, Tay et al.(2019)Tay, Roy, and Yap, Chen et al.(2019)Chen, Ding, Xie, Yuan, Chen, Yang, Ren, and Wang, Zheng et al.(2019)Zheng, Yang, Yu, Zheng, Yang, and Kautz, Ge et al.(2018)Ge, Li, Zhao, Yin, Yi, Wang, and Li], one-shot [Wu et al.(2018)Wu, Lin, Dong, Yan, Ouyang, and Yang, Ye et al.(2018)Ye, Lan, and Yuen], transfer learning [Deng et al.(2018)Deng, Zheng, Ye, Kang, Yang, and Jiao, Fan et al.(2018)Fan, Zheng, Yan, and Yang, Peng et al.(2016)Peng, Xiang, Wang, Pontil, Gong, Huang, and Tian], unsupervised domain adaptation [Jin et al.(2020)Jin, Lan, Zeng, Chen, and Zhang, Ge et al.(2020a)Ge, Chen, and Li, Zhai et al.(2020b)Zhai, Ye, Lu, Jia, Ji, and Tian, Zhong et al.(2019)Zhong, Zheng, Luo, Li, and Yang, Zhai et al.(2020a)Zhai, Lu, Ye, Shan, Chen, Ji, and Tian, Li et al.(2019)Li, Lin, Lin, and Wang, Huang et al.(2019)Huang, Wu, Xu, and Zhong], and unsupervised learning [Liao et al.(2015)Liao, Hu, Xiangyu Zhu, and Li, Lin et al.(2019)Lin, Dong, Zheng, Yan, and Yang, Ding et al.(2019a)Ding, Khan, Tang, Zhang, and Porikli, Wu et al.(2020)Wu, Zhu, and Gong, Lin et al.(2020)Lin, Xie, Wu, Yan, and Tian, Wang and Zhang(2020)] frameworks. Supervised approaches require a labeled dataset having both identity and camera information, making them inherently biased towards the dataset they are trained on and are therefore not generalizable. The unsupervised counterpart does not assume the availability of any such labeled information making it more deployment friendly in real-world scenarios. Unsupervised domain adaptation (UDA) methods [Jin et al.(2020)Jin, Lan, Zeng, Chen, and Zhang, Ge et al.(2020a)Ge, Chen, and Li, Zhai et al.(2020b)Zhai, Ye, Lu, Jia, Ji, and Tian, Zhong et al.(2019)Zhong, Zheng, Luo, Li, and Yang, Zhai et al.(2020a)Zhai, Lu, Ye, Shan, Chen, Ji, and Tian, Li et al.(2019)Li, Lin, Lin, and Wang, Huang et al.(2019)Huang, Wu, Xu, and Zhong] tackle the problem of domain generalizability by training on some available labeled source dataset and then adapting to the target unlabeled dataset. Though such methods have shown to yield state-of-the-art results, they raise two important concerns: 1) labeled dataset is expensive to obtain, restricting the scalability to real-world environments, and 2) there is no way to ascertain the domain gap between the source and target datasets. Thus, one cannot ascertain the ideal dataset that will yield the most superior domain transfer on target. Purely unsupervised methods [Liao et al.(2015)Liao, Hu, Xiangyu Zhu, and Li, Lin et al.(2019)Lin, Dong, Zheng, Yan, and Yang, Ding et al.(2019a)Ding, Khan, Tang, Zhang, and Porikli, Wu et al.(2020)Wu, Zhu, and Gong, Lin et al.(2020)Lin, Xie, Wu, Yan, and Tian, Wang and Zhang(2020)] differ from UDA techniques as they delve one step further into the complexity of the problem by being completely unaware of any identity information, either in source or target dataset. In this work, we propose a novel discriminative clustering approach for the purely unsupervised re-ID setup. Learning without labels is challenging for any problem in general. For the re-ID task, this involves learning discriminative features from the unlabeled dataset to identify query images in the gallery set. Initially, we create pseudo labels by assigning each image its own unique label. However, a simple re-ID training using pseudo labels as the supervisory signal fails to learn any useful features. We thus seek to enhance the network’s discriminative ability using a two-stage training procedure. We first take each image and generate similar looking, pose-transformed images in the identity space. These are then assigned labels corresponding to their original images. This helps project the image identity into different poses, thereby yielding a set of more realistic augmentations, thus giving the model an insight into different camera angles inherently present in the dataset. The base CNN is then trained on these samples using the aforementioned pseudo labels as supervision. This is done in conjunction with a metric learning loss to learn a latent space where data points belonging to different identities are farther apart than those belonging to the same identity. The second stage builds on the first stage by using the proposed discriminative clustering algorithm. Plugging out the pose-transformed samples from the dataset, this step only lets the network access the original unlabeled dataset. This not only eases the burden of training with a large dataset but also helps in learning true distribution features. Our novel loss function aims to explicitly move points in different clusters farther apart and the points inside a cluster within a certain distance, thereby increasing the inter-cluster variation and decreasing the intra-cluster variation. This helps the network achieve powerful discriminability. Our contributions are summarized as follows: • We design a pose-transformed dataset that does not require any identity information, is non-parametric, and can be adopted to any person re-identification method. • We introduce a novel radial distance loss that explicitly attends to inter and intra cluster variations by maintaining a minimum radial distance between clusters while pushing dissimilar samples away. We also show that it helps discerning similar looking identities. • We achieve state-of-the-art results for the unsupervised person re-identification task on the Market-1501 [Zheng et al.(2015)Zheng, Shen, Tian, Wang, Wang, and Tian], DukeMTMC-reID [Zheng et al.(2017)Zheng, Zheng, and Yang], and MSMT17 [Wei et al.(2018)Wei, Zhang, Gao, and Tian] datasets."
https://arxiv.org/html/2411.04249v1,PocoLoco: A Point Cloud Diffusion Model of Human Shape in Loose Clothing,"Modeling a human avatar that can plausibly deform to articulations is an active area of research. We present PocoLoco – the first template-free, point-based, pose-conditioned generative model for 3D humans in loose clothing. We motivate our work by noting that most methods require a parametric model of the human body to ground pose-dependent deformations. Consequently, they are restricted to modeling clothing that is topologically similar to the naked body and do not extend well to loose clothing. The few methods that attempt to model loose clothing typically require either canonicalization or a UV-parameterization and need to address the challenging problem of explicitly estimating correspondences for the deforming clothes. In this work, we formulate avatar clothing deformation as a conditional point-cloud generation task within the denoising diffusion framework. Crucially, our framework operates directly on unordered point clouds, eliminating the need for a parametric model or a clothing template. This also enables a variety of practical applications, such as point-cloud completion and pose-based editing – important features for virtual human animation. As current datasets for human avatars in loose clothing are far too small for training diffusion models, we release a dataset of two subjects performing various poses in loose clothing with a total of 75K point clouds. By contributing towards tackling the challenging task of effectively modeling loose clothing and expanding the available data for training these models, we aim to set the stage for further innovation in digital humans. The source code is available at https://github.com/sidsunny/pocoloco.","The tremendous recent advances in generative AI models are also driving a recent interest in creating articulated 3D avatars of clothed humans. The ultimate goal is to create a personalized 3D model of a clothed body that can be controlled by the skeletal body pose, with the clothing deforming naturally as the body moves. Such digital avatars should ideally be learned from just a given set of 3D meshes of a person performing various movements. This will greatly simplify the creation of avatars for immersive experiences in applications like telepresence, virtual reality, content creation, and character animation, to name a few. Creating human avatars is an artistic process and often requires several hours of work to produce plausible character animation. In recent years, there have been tremendous improvements in automatic human avatar creation methods that generate avatars from inputs ranging from multi-view images [16, 45, 29] to highly-detailed 3D scans [21, 19]. The majority of works base the animatable avatar on the parametric body model SMPL [18] that provides coarse pose-dependent deformation and then either displace the vertices of the body template mesh [1, 20, 21, 19] or use it to learn a neural implicit field that captures the clothing shape conditioned on the body parameters [34, 16]. However, approaches based on using SMPL as templates are typically limited to rather simple tight clothing topology as they need to learn an accurate surface correspondence from the template mesh to the 3D training scans [34]. This proves to be prohibitively simple and ineffective for loose clothing. Consequently, even those methods that explicitly address modeling humans in loose clothing, but use SMPL as an underlying template, suffer from artifacts like tearing of the surface of loose clothing [21], or crease-formation between legs due to the UV parameterization [34]. In this work, we introduce PocoLoco, the first template-free method based on a conditional diffusion model for learning articulated 3D avatars in loose clothing. Our model operates directly on unordered point clouds without assuming point correspondences, thus avoiding the requirement of using parametric models or a clothing template (Fig. 1). We use diffusion to learn the uncertainty associated with clothing deformation due to external forces. This is especially important when articulating the human body given a single pose which provides no information on the current action sequence and consequently the resulting deformations. Concretely, given an input pose of the character, our goal is to produce a corresponding point cloud of the character that agrees with the pose with the clothing deforming naturally. We formulate this conditional generative process within the generative framework of denoising diffusion models [35, 36, 11]. However, extending diffusion models to 3D human point clouds is not a straightforward extension of 2D diffusion. In Section 3.1, we discuss our various design choices behind the proposed transformer-based architecture for pose-conditioned human shape synthesis. We highlight the importance of an appropriate scheduling policy for our setting, observing that in contrast to prior work [46] a quartic beta-schedule offers the best trade-off between utilization of the network capacity and high-quality shape synthesis with 10K points. Notably, our model learns a person-specific distribution of cloth deformations without any explicit cloth deformation model from a dataset of point clouds that capture a few minutes of a person performing various movements. Once learned, the avatar can be driven by arbitrary skeletal poses, akin to Linear Blend Skinning (LBS) but without having to learn it. Moreover, our generative approach allows us to model clothing deformations in a stochastic manner, thus permitting us to generate multiple diverse hypotheses for the same input pose. As existing datasets for human avatars in loose clothing are too small for training diffusion models, we release a dataset of two subjects in loose clothing with a total of 75K frames performing a variety of articulated motions. We record this data in a multi-view camera setting and obtain high-quality 3D shape reconstructions following neural surface reconstruction proposed in [40]. Using our recorded data, we show that compared to popular methods for human modeling like NeuralActor [16] we achieve a significant improvement in the representation of loose clothing. Even when compared to methods that focus on posing a pre-scanned high-quality template of the human subject, like DDC [7], our method performs competitively. The main contributions of this work are three-fold: • We present PocoLoco, which to the best of our knowledge, is the first 1) template-free and 2) conditional generative model for articulated 3D avatars in loose clothing. Our model operates directly on unordered point clouds without assuming point correspondences. • Going beyond 3D shape generation, our diffusion-based formulation also enables capturing the inherent stochasticity in the deformations of loose clothes, enabling several practical applications. • To evaluate and benchmark our method, we capture a dataset of high-quality 3D reconstructions of 2222 subjects with 75K frames performing a variety of actions. By expanding the available data for training large-scale generative models of humans in loose clothing, we are setting the stage for even more innovative breakthroughs in the future. Methods Scanned Registration/ Unseen Generative Template SMPL Poses Ability DDC [7] ✓ ✗ ✓ ✗ NeuralActor [16] ✗ ✓ ✓ ✗ SCANimate [34] ✗ ✓ ✗ ✗ POP [21] ✗ ✓ ✗ ✗ SkiRT [19] ✗ ✓ ✓ ✗ PocoLoco ✗ ✗ ✓ ✓ Table 1: Characteristics of SOTAs on human modeling in loose clothing. PocoLoco does not require an underlying template, SMPL body shape, or LBS. Furthermore, it is the first generative method to model the ambiguity associated with loose clothing deformation. Green denotes a positive (and red a negative) trait."
https://arxiv.org/html/2411.04227v1,Bare Demo of IEEEtran.clsfor IEEE Conferences,The abstract goes here.,"This demo file is intended to serve as a “starter file” for IEEE conference papers produced under LaTeX using IEEEtran.cls version 1.8b and later. I wish you the best of success. mds August 26, 2015 I-A Subsection Heading Here Subsection text here. I-A1 Subsubsection Heading Here Subsubsection text here."
https://arxiv.org/html/2411.04224v1,WiFlexFormer: Efficient WiFi-Based Person-Centric Sensing,"We propose WiFlexFormer, a highly efficient Transformer-based architecture designed for WiFi Channel State Information (CSI)-based person-centric sensing. We benchmark WiFlexFormer against state-of-the-art vision and specialized architectures for processing radio frequency data and demonstrate that it achieves comparable Human Activity Recognition (HAR) performance while offering a significantly lower parameter count and faster inference times. With an inference time of just 10 ms on an Nvidia Jetson Orin Nano, WiFlexFormer is optimized for real-time inference. Additionally, its low parameter count contributes to improved cross-domain generalization, where it often outperforms larger models. Our comprehensive evaluation shows that WiFlexFormer is a potential solution for efficient, scalable WiFi-based sensing applications. The PyTorch implementation of WiFlexFormer is publicly available at: https://github.com/StrohmayerJ/WiFlexFormer.","WiFi has emerged as a promising modality in person-centric sensing due to its advantages over optical approaches, including cost-effectiveness, unobtrusiveness, visual privacy protection, and the ability to perform long-range sensing through walls [15, 4]. Together, these characteristics enable efficient, contactless monitoring of human activities in confined indoor environments without the need for per-room sensor deployment, representing a significant economic advantage [18]. Channel State Information (CSI) serves as the foundation for modern WiFi-based person-centric sensing. CSI is a metric obtained in the Orthogonal Frequency-Division Multiplexing (OFDM) scheme, which subdivides a WiFi channel into multiple sub-channels with different carrier frequencies (subcarriers) [7]. This subdivision allows for fast, parallel transmission of data, while CSI provides detailed information about how each subcarrier is affected by the environment, enabling the correction of environment-induced noise at the receiver on a per-subcarrier basis, and through correlating the distinctive patterns of amplitude attenuation and phase shifts in CSI caused by specific human movements, applications such as Human Activity Recognition (HAR) [12]. While existing approaches to CSI-based HAR often rely on generic CNN-based vision architectures, they are not optimal due to their focus on local dependencies and shift-invariance [25]. To effectively leverage the unique properties of CSI, specialized architectures have been developed [2, 11, 25]. However, they tend to suffer from other problems such as overly complex designs and reliance on computationally expensive features, resulting in high inference times and limited practicality for real-time applications. Contributions. To address these challenges, we make the following contributions: (I) We propose WiFlexFormer, a Transformer-based architecture that achieves similar HAR performance with significantly lower parameter count and inference time, making it highly efficient and well-suited for WiFi-based real-time person-centric sensing applications. (II) We conduct comprehensive evaluations of WiFlexFormer on publicly available WiFi datasets, assessing its HAR performance using amplitude and Doppler Frequency Shift (DFS) features and comparing it against existing state-of-the-art architectures. (III) We investigate the effectiveness of various subcarrier sub-sampling strategies to further optimize inference speed while maintaining model performance."
https://arxiv.org/html/2411.04168v1,DiMSUM:DiffusionMamba - AScalable andUnified Spatial-FrequencyMethod for Image Generation,"We introduce a novel state-space architecture for diffusion models, effectively harnessing spatial and frequency information to enhance the inductive bias towards local features in input images for image generation tasks. While state-space networks, including Mamba, a revolutionary advancement in recurrent neural networks, typically scan input sequences from left to right, they face difficulties in designing effective scanning strategies, especially in the processing of image data. Our method demonstrates that integrating wavelet transformation into Mamba enhances the local structure awareness of visual inputs and better captures long-range relations of frequencies by disentangling them into wavelet subbands, representing both low- and high-frequency components. These wavelet-based outputs are then processed and seamlessly fused with the original Mamba outputs through a cross-attention fusion layer, combining both spatial and frequency information to optimize the order awareness of state-space models which is essential for the details and overall quality of image generation. Besides, we introduce a globally-shared transformer to supercharge the performance of Mamba, harnessing its exceptional power to capture global relationships. Through extensive experiments on standard benchmarks, our method demonstrates superior results compared to DiT and DIFFUSSM, achieving faster training convergence and delivering high-quality outputs. The codes and pretrained models are released at https://github.com/VinAIResearch/DiMSUM.git.","Diffusion models [58, 22] are a trending generative model technique that has gained significant attention in machine learning and computer vision. The core idea behind diffusion models is to learn how to reverse the diffusion process by gradually transforming a simple initial distribution, like Gaussian noise, into a complex data distribution. The flexibility, robust performance, and high-quality outputs make them powerful tools for advancing the state-of-the-art in generative modeling, and large diffusion-based generators have revolutionized the field of image [51, 3], video [23, 21], and 3D synthesis [50, 62, 61]. While diffusion models initially rely on UNet architectures, recent methods have shifted gear to build upon transformer backbones. A line of works [48, 14, 11] have shown that transformer-based diffusion models are scalable and consistently offer higher generation quality than the UNet-based counterparts. Even the most common open-source text-to-image tool, Stable Diffusion, has switched to use transformers in their upcoming release [53]. Hence, transformers are becoming the new backbone standard for diffusion models. The power of this structure lies in the attention mechanism for capturing richer in-context relations. However, transformers have the drawback of a costly quadratic complexity, which might hinder their feasibility for high-dimensional data. While transformers are taking over state-of-the-art diffusion backbones, a novel technique called state-space models (SSMs) has suddenly arrived, promising a better alternative. SSMs [18, 16, 15] have revolutionized the NLP field, favoring linear time complexity and excelling at long-context modeling. This type of network bears similarities to the recurrent process of RNNs while being capable of fully operating in parallel like convolutional networks. SSM promises to surpass transformers in most tasks, prioritizing compute efficiency, such as long-sequence modeling. Mamba [15] is a special type of SSM that offers greater quality by introducing time conditioning and context dependency to the hidden states. In the context of computer vision, within a very short period, this architecture has been used to address a variety of problems, including image perception [74, 28, 33], image restoration [20, 71], and image generation [41, 74, 65, 25]. In diffusion-based image generation, Diffusion State Space models (DIFFUSSM) [65] already surpass their transformer-based counterparts. Though showing many advantages, Mamba still has a critical weakness when processing 2D imagery data. Like vision transformers, images are divided into patches and then mapped into tokens. Mamba processes these tokens following a specific scanning order, thus introducing an inductive bias about 2D images into the model. Specifically, this order greatly impacts the interplay between image tokens, thereby affecting model performance. This characteristic is unfavorable, particularly when transformers have no such order-dependency issue. Many vision-based Mamba studies have focused on solving this problem on proposed advanced scanning mechanisms like bi-directional [74], cross-scanning [41, 33], or 8-directions zigzag [25]. Despite improving performance, these scanning techniques still fail to capture global and long-range relations and do not fully release Mamba’s potential. In this paper, we enhance Mamba-based diffusion models, specifically focusing on image generation. Previous models failed to address the scanning order issue due to their exclusive reliance on spatial processing, overlooking crucial long-range relations that manifest in the frequency spectrum. We suggest a novel approach integrating frequency scanning with the conventional spatial scanning mechanism. Although initial work in Mamba has explored this combination for a limited task of image deraining [71], it lacked a comprehensive analysis of the effective integration of these features. Motivated by the above observation, this paper introduces DiMSUM, a novel architecture that harnesses Mamba’s power to unlock diffusion models’ generation capabilities. Our approach enhances sensitivity to local structures and long-range dependencies by integrating wavelet transforms and spatial information. Using a query-swapped cross-attention technique, we dynamically synergize spatial and frequency information, accelerating convergence and improving image synthesis quality. Consequently, this boosts image quality and enhances the efficiency and scalability of the training. Additionally, we incorporate globally shared transformer blocks to address global context integration, a limitation of traditional Mamba models. The block can also be viewed as a token-mixing layer that enriches global relations among image tokens, addressing the weak inductive bias of the manually defined scanning orders in the original Mamba. Hence, DiMSUM can maintain high performance even with larger, more complex datasets. Extensive experiments show that DiMSUM achieves state-of-the-art FID scores and recall, setting a new benchmark in generative image modeling. In summary, our contributions lie three-fold: (1) A novel Mamba architecture for diffusion models that leverages both spatial and frequency features to enhance the awareness of local structures within input images, leading to better image generation. (2) We interleave globally shared transformer blocks per a certain number of Mamba blocks. The transformer with a strong capacity for capturing global relationships significantly boosts generation results when integrating with Mamba. The transformer can also be seen as an order-invariant mixing layer that complements Mamba’s loose assumptions about the order of 2D data. (3) Superior results across image generation benchmarks like ImageNet, CelebA-HQ, and LSUN Church. Additionally, our method maintains comparable GFLOPs and parameters with existing diffusion architectures while offering faster training convergence."
https://arxiv.org/html/2411.04151v1,UnityGraph: Unified Learning of Spatio-temporal features for Multi-person Motion Prediction,"Multi-person motion prediction is a complex and emerging field with significant real-world applications. Current state-of-the-art methods typically adopt dual-path networks to separately modeling spatial features and temporal features. However, the uncertain compatibility of the two networks brings a challenge for spatio-temporal features fusion and violate the spatio-temporal coherence and coupling of human motions by nature. To address this issue, we propose a novel graph structure, UnityGraph, which treats spatio-temporal features as a whole, enhancing model coherence and coupling. Specifically, UnityGraph is a hypervariate graph based network. The flexibility of the hypergraph allows us to consider the observed motions as graph nodes. We then leverage hyperedges to bridge these nodes for exploring spatio-temporal features. This perspective considers spatio-temporal dynamics unitedly and reformulates multi-person motion prediction into a problem on a single graph. Leveraging the dynamic message passing based on this hypergraph, our model dynamically learns from both types of relations to generate targeted messages that reflect the relevance among nodes. Extensive experiments on several datasets demonstrates that our method achieves state-of-the-art performance, confirming its effectiveness and innovative design.","Multi-person motion prediction aims to predict future human motion sequences for multiple individuals based on historical sequences. Single-person motion prediction methods [33, 69, 10] focus solely on modeling an individual’s spatial joints relation or trajectory features in the temporal domain, neglecting the interactions between different individuals, as shown in Fig. 1 (a). In contrast, multi-person motion prediction carries more practical significance, as it is more common for multiple people to be present in one scene in the real world. At the same time, multi-person motion prediction is also more challenging because of the complicated interactions between individuals and plays a significant role in many real-world applications, such as autonomous driving [68, 40], robotics [70, 55], surveillance systems [71]. Most current researches [41, 47] involving multi-person scenes focus on trajectory prediction, which model the agent as a single 2D point on the ground plane and exploit the temporal features and the spatial interactions among different person to make prediction, as shown in Fig. 1 (b). However, these methods are insufficient for 3D tasks such as motion prediction, which requires detailed body pose information. Adeli et al. [49] first combine scene context to model interactions between humans and objects in 3D task. Subsequently, more approaches [50, 52, 11] are being developed to capture spatio-temporal features more effectively. However, previous methods usually employ dual-path networks to learn temporal and spatial features separately, as shown in Fig. 1 (c). Given the variations and diverse action types in human motion data, this decoupled modeling strategy inherently disrupts the real world unified spatio-temporal interdependencies, making it challenging to accurately capture the cross-dependencies of spatio-temporal relationships and potentially limiting the accuracy and realism of the predictions [13, 44]. To address the above issue, we propose a novel graph structure named UnityGraph based on hypervariate graph to learn human motion features with the spatio-temporal consistent perspective, as shown in Fig. 1 (d). The core idea of the hypervariate graph is to construct a space-time fully-connected structure. Specifically, given the observation of scenes involving N𝑁Nitalic_N persons over a duration of T𝑇Titalic_T time steps. The hypergraph can be viewed as a graph with N×T𝑁𝑇N\times Titalic_N × italic_T nodes that are connected by hyperedges, as shown in Fig. 3 (a). The higher-order connectivity [8, 43] of hyperedge can connect multiple nodes at the same time is utilised to express complex interactions between multiple people over the spatio-temporal domain. Such a special design formulates spatio-temporal features of individuals as node-to-node dependencies in the hypergraph. Different from previous methods that learn spatio-temporal features separately with a dual-path structure, our approach views spatio-temporal correlations as a whole. It abandons the uncertain compatibility of spatial and temporal modeling, constructs adaptive spatio-temporal dependencies. We set three types of hyperedges to learn spatio-temporal features from node-to-node dependencies: i) Short-term hyperedges connect neighboring nodes of the same individual to model short-term dynamics. ii) Long-term hyperedges connect all nodes of the same individual throughout the time series, capturing the long-term dynamics. iii) Spatial hyperedges link different nodes at the same frame to capture interaction features between different individuals. Notably, our strategy of designing hyperedges achieves more accurate predictions than treating the hypergraph as a fully connected graph. This approach ensures spatio-temporal coupling while avoiding the unacceptably large computational effort associated with full connectivity, as discussed in our extensive ablation experiments. Following the construction of the hypergraph, multi-person motion prediction is reformulated as a prediction task on a single graph. To capture spatio-temporal features from this graph, we introduce dynamic message passing that enhances the spatio-temporal feature transfer between nodes and hyperedges. In the initial node-to-hyperedge phase, nodes transmits information to its associated hyperedges, which update their state by aggregating this information through weighted sums. Subsequently, hyperedges send their aggregated information back to the connected nodes during hyperedge-to-node phase. This bidirectional flow enables nodes to receive and integrate relevant global information, facilitating timely updates to their states. Figure 1: Comparison between our method with single-person prediction methods [44, 37], human trajectory prediction methods [41, 47] and traditional multi-person motion prediction methods [50, 52, 11]. (a)Single-person prediction methods focus on modeling joint relations, neglecting interactions within the group. (b)Human trajectory prediction methods lack the representation of 3D pose. (c)Traditional multi-person motion prediction methods employ multiple sub-networks to capture spatial and temporal features separately. These methods inevitably diminish spatio-temporal coupling and consistency. (d)Our method unifies the learning of spatio-temporal features within a single network for multi-person motion prediction. For clarity, edges that connect nodes across different frames are omitted. Figure 2: Illustration of our motivation. In the past scene, person 2 is walking together with person 3, while person1 is walking towards person 2. Most current methods consider the interaction during this phase. (The red dash lines denote the interaction between different individuals.) In future scene 1 and 2, a sudden situation occurs-person1 meet person 2 and stops to talk. If we do not continually consider the existing interaction in the future, person 3 keeps walking and forgets about the person he was walking with, as shown in future scene 1. In contrast, if we think about interaction in the future, there would be a different result: person 3 should also stop and wait his partner, person 2, as shown in future scene 2. Our method is dedicated to making prediction that comply with scene 2. After message passing, we decode the learned features to predict future motions. Current methods [52, 11, 30] often adopt non-autoregressive decoding and make prediction sequence in one time. The single-person methods benefit from this decoding strategy because each predicted frame is independent and avoid error accumulation. However, the multi-person scenes force us to reconsider this “independent strategy” because of the evolve environment and others’ future motions. Let’s discuss a special situation, as shown in Fig. 2. The past scene shows person 2 is walking together with person 3, while person 1 is walking towards person2. Future scene 1 shows the results when we don’t consider the motions and intentions of others during prediction. We can see person1 and person 2 stop to talk, and person 3 ignore the sudden situation and keep walking. There would be a different result if we still consider the others’ motions during the prediction, as shown in future scene 2. Person 2 also stops and waits for his partner, person 2, which is comply with real-world behavior. Relying only on historical information is insufficient for accurate predictions in an evolving environment, especially since human motion is variable. To improve the capability of handling sudden situation in the future, we include future interactions to our perspective and take account other people’s actions during the same window of time. Specifically, we introduce a predictor to continuously update interaction information and make future motion step by step, as shown in Fig. 3 (b). To generate accurate interaction, we design an inference loss function combine with position loss to supervise our train. Finally, we perform our experiments on multiple datasets, including 3DPW [53], CMU-Mocap [58], MuPoTS-3D [59], and Mix1&Mix2 [49]. The quantitative results demonstrate that our method achieves state-of-the-art performance across most datasets. In summary, our contributions are as follows: • We introduce UnityGraph, a novel graph structure for human motion prediction. UnityGraph formulates human motions as graph nodes, enabling the unified extraction of spatio-temporal information from nodes within a single graph. This perspective aligns with the inherent consistency and coupling of spatio-temporal features observed in the real world, thereby facilitating more accurate and natural predictions. • We develop a interactive decoding that considers both past and future information. Specifically, our method focuses not only on others’ historical motions but also on the future. We propose a predictor to update relations and make future motion step by step during decoding. Meanwhile, we design a novel inference loss function combined with position loss to supervise the model effectively."
https://arxiv.org/html/2411.04919v1,Stem-OB:Generalizable Visual Imitation Learning with Stem-Like Convergent Observation through Diffusion Inversion,"Visual imitation learning methods demonstrate strong performance, yet they lack generalization when faced with visual input perturbations like variations in lighting and textures. This limitation hampers their practical application in real-world settings. To address this, we propose Stem-OB that leverages the inversion process of pretrained image diffusion models to suppress low-level visual differences while maintaining high-level scene structures. This image inversion process is akin to transforming the observation into a shared representation, from which other observations also stem. Stem-OB offers a simple yet effective plug-and-play solution that stands in contrast to data augmentation approaches. It demonstrates robustness to various unspecified appearance changes without the need for additional training. We provide theoretical insights and empirical results that validate the efficacy of our approach in simulated and real settings. Stem-OB shows an exceptionally significant improvement in real-world robotic tasks, where challenging light and appearance changes are present, with an average increase of 22.2% in success rates compared to the best baseline. See our website for more info.","Visual Imitation Learning (IL), where an agent learns to mimic the behavior of the demonstrator by learning a direct mapping from visual observations to low-level actions, has gained popularity in recent real-world robot tasks (Chi et al., 2023; Zhao et al., 2023; Wang et al., 2023; Chi et al., 2024; Ze et al., 2024). Despite the versatility demonstrated by visual IL, learned policies are often brittle and fail to generalize to unseen environments, even minor perturbations such as altering lighting conditions or changing the texture of the object may lead to failure of the learned policy (Xie et al., 2023; Yuan et al., 2024b). The underlying reason is that the high-dimensional visual observation space is redundant with virtually infinite variations in appearance that are irrelevant to the task and hard to generalize. As human beings, we can easily manipulate objects that have different appearances. For example, we can pick up a cup of coffee regardless of its color, texture, or the lighting condition of the room. This is partially because our visual system is capable of abstracting the high-level semantics of the scene, such as the silhouette of the object, the structure and arrangement of different objects, etc in a hierarchical manner (Hochstein & Ahissar, 2002), effectively merging scenes with perceptual differences to similar “meta” observations. Augmentation techniques such as Spectrum Random Masking (SRM) (Huang et al., 2022) and Mixup (Zhang et al., 2018) remove details from observations to encourage the model to focus on structural features; however, they lack the ability to distinguish between low-level and high-level features. It is preferable if we can sweep the photometrical differences while maintaining the high-level structure for the scene. Achieving this requires a semantic understanding of the observations, and naively perturbing the data with Gaussian noise can lead to irreversible information loss. Pretrained large image diffusion models, such as Stable Diffusion (Rombach et al., 2022; Esser et al., 2024), embed essential world knowledge for visual understanding. Apart from synthesizing new images from random noise, these models are capable to perform a reverse procedure called inversion (Song et al., 2022), which converts an image back to the space of random noises. A recent study (Yue et al., 2024) indicates that this inversion process selectively eliminates information from the image. Rather than uniformly removing information from different semantic hierarchies, it will push those images with similar structures closer in the early stages of the inversion process. Inversion is like the reprogramming of a differentiated cell back to a stem cell, which bears the totipotency to differentiate into any cell type. This characteristic aligns perfectly with our will of enhancing the robustness and generalizability of visual IL algorithms to visual variations. To distill such property into a visual IL policy, we propose an imitation learning pipeline which applies diffusion inversion to the visual observations. We name our method Stem-OB to highlight the similarity between the inversed observation and the stem cell in biology, as illustrated in Figure 1. To be specific, our method is as simple as inverting the image for reasonable steps before sending them to the downstream visual IL algorithms, effectively serving as a preprocessing step that converges low-level appearance differences into similar high-level structures. From this perspective, our approach fundamentally distinguishes from generative augmentation methods, which aim to enrich the training dataset with more unseen objects and appearances (Yu et al., 2023; Mandlekar et al., 2023). Moreover, Stem-OB is indifferent to many unspecified appearance changes, in contrast to augmentation-based methods that must concentrate on a few selected types of generalization, thereby introducing inevitable inductive biases. We provide theoretical analysis and a user study to support our claim that Stem-OB can effectively merge scenes with perceptual differences to similar “stem observations”. Empirical study demonstrates the effectiveness of our approach in a variety of simulated and real-world tasks and a range of different perturbations. Stem-OB proves to be particularly effective in real-world tasks where appearance and lighting changes hamper the other baselines, establishing an overall improvement in the success rate of 22.2%. What’s better, no inference time inversion is required for Stem-OB to take effect, making the deployment of our method virtually free of computational cost."
https://arxiv.org/html/2411.04844v1,Differentiable Gaussian Representation for Incomplete CT Reconstruction,"Incomplete Computed Tomography (CT) benefits patients by reducing radiation exposure. However, reconstructing high-fidelity images from limited views or angles remains challenging due to the ill-posed nature of the problem. Deep Learning Reconstruction (DLR) methods have shown promise in enhancing image quality, but the paradox between training data diversity and high generalization ability remains unsolved. In this paper, we propose a novel Gaussian Representation for Incomplete CT Reconstruction (GRCT) without the usage of any neural networks or full-dose CT data. Specifically, we model the 3D volume as a set of learnable Gaussians, which are optimized directly from the incomplete sinogram. Our method can be applied to multiple views and angles without changing the architecture. Additionally, we propose a differentiable Fast CT Reconstruction method for efficient clinical usage. Extensive experiments on multiple datasets and settings demonstrate significant improvements in reconstruction quality metrics and high efficiency. We plan to release our code as open-source.","Computed Tomography (CT) has evolved into an essential tool in medical imaging since its inception in the 1970s, playing a pivotal role in disease diagnosis, surgical guidance, and treatment response monitoring (Zhu et al. 2018; Brenner et al. 2001; Szczykutowicz et al. 2022). However, the radiation dose associated with CT imaging presents a substantial risk to patients. In response to patient concerns about radiation exposure, Incomplete CT has emerged as a promising solution (Ma et al. 2023; Lahiri et al. 2023). This technique uses fewer views or angles to capture X-ray projections (Figure 1), thereby reducing the radiation dose to which patients are exposed (Wu et al. 2021; Fahrig et al. 2006). Not only does this method mitigate the risks associated with radiation exposure, but it also enhances the overall patient experience during medical imaging procedures. Figure 1: I, Illustration of CT scanning process and two paradigms of Incomplete CT: Sparse-View CT and Limited-Angle CT. II, deep-learning reconstruction (DLR) methods, which learn to reconstruct the volume under the supervision of Full-Dose data. III, Our proposed Gaussian Representation for CT Reconstruction (GRCT), directly optimizing a set of learnable Gaussians under the supervision of only incomplete sinograms, i.e., without Full-Dose data. The essence of incomplete CT is to reconstruct the 3D volume of the patient using fewer views or angles of CT projection data (formally known as sinogram). As illustrated in Figure 1, Sparse-View CT limits the number of X-ray beams, while Limited-Angle CT constrains the angle of X-ray beams, both of which are ill-posed problems. Currently, deep learning reconstruction (DLR) methods dominate this task, typically learning to reconstruct the 3D volume under the supervision of full-dose volume data. These methods vary widely, from pure image-domain denoising methods (Jin et al. 2017; Chen et al. 2017), to dual-domain methods that leverage information from both sinogram and image domains (Hu et al. 2020), and cutting-edge diffusion-based methods (Chung et al. 2022, 2023; Xu et al. 2024). However, despite the promising advancements in incomplete CT reconstruction, the clinical application of DLR methods remains limited, and patients rarely benefit from the reduced radiation dose in real-time. We summarize the challenges of incomplete CT reconstruction as follows: Firstly, the requirement for full-dose CT data for training neural networks is a significant limitation. Regardless of the dataset size, it is impossible to cover all possible scenarios in clinical practice. The diversity in scanning data, including different radiation views or angles, patient ages, body parts, and imaging protocols, makes it challenging to collect a comprehensive dataset (Moen et al. 2021). Secondly, the versatility of neural networks in handling multiple views and angles within a single architecture is limited. Current methods are designed for specific views and angles, such as 60-view CT, 120-view CT, 90∘ CT, etc., and are not easily adaptable to other views or angles. When the views or angles change, the neural networks need to be retrained, which is time-consuming and labor-intensive (Yang et al. 2022). Thirdly, the computational complexity during the deployment stage remains high, making it less efficient for real-time applications (Koetzier et al. 2023). Advanced diffusion-based methods (Chung et al. 2022, 2023; Xu et al. 2024) can achieve a PSNR of over 45 dB, but their computational cost is also high, rendering them unsuitable for real-time applications. Furthermore, the lack of patient-specific adaptation in current methods hinders their ability to provide tailored reconstruction for each patient. As anticipated by (Szczykutowicz et al. 2022), future CT reconstruction methods should be capable of learning in real-time and adaptively updating parameters for specific patients. Numerous challenges drive us to rethink the representation of the CT reconstruction problem. Do we need neural networks to learn the mapping from the sinogram domain to the image domain? Can we directly learn the 3D volume from the sinogram domain without full-dose CT data? Can we design a method that can be applied to multiple views and angles without changing the architecture? Can we design a method that is highly efficient for training and deployment? Can we design a method that can provide tailored reconstruction for each patient? To this end, our ideas began to surface gradually. We made a bold attempt: to abandon the classic neural network architecture and instead use a set of Gaussians to represent CT scenes. This motivation is inspired by the 3D Gaussian Splatting (3DGS) (Kerbl et al. 2023) method in neural graphics, which has shown great potential for reconstructing 3D scenes with a set of Gaussians. However, we are not satisfied with the current 3DGS on incomplete CT, as it is not designed for the medical imaging domain and lacks a direct reconstruction process for the entire volume. Instead, we designed a novel Gaussian Representation as well as a Fast CT Reconstruction method. Our representation is differentiable, and our reconstruction process is highly parallelized, allowing for end-to-end training. Our method can be applied to multiple views and angles without changing the architecture. We extensively evaluate our method under multiple settings of incomplete CT reconstruction to demonstrate that it can achieve significant improvements in both reconstruction quality metrics and efficiency. The main contributions of this paper are as follows: • We propose a novel Gaussian Representation for Incomplete CT Reconstruction (GRCT). This method directly learns to reconstruct the 3D volume from Gaussian functions within the current patient’s sinogram domain, regardless of the number of views or angles acquired. • We provide a Fast CT Reconstruction method that combines the contributions of the Gaussians in a highly parallel manner into a discretized volume, where the whole process is differentiable and end-to-end. • We conduct extensive experiments in multiple settings to demonstrate the effectiveness of our method in improving reconstruction quality metrics and efficiency."
https://arxiv.org/html/2411.04796v1,MPVO: Motion-Prior based Visual Odometry for PointGoal Navigation,"Visual odometry (VO) is essential for enabling accurate point-goal navigation of embodied agents in indoor environments where GPS and compass sensors are unreliable and inaccurate. However, traditional VO methods face challenges in wide-baseline scenarios, where fast robot motions and low frames per second (FPS) during inference hinder their performance, leading to drift and catastrophic failures in point-goal navigation. Recent deep-learned VO methods show robust performance but suffer from sample inefficiency during training; hence, they require huge datasets and compute resources. So, we propose a robust and sample-efficient VO pipeline based on motion priors available while an agent is navigating an environment. It consists of a training-free action-prior based geometric VO module that estimates a coarse relative pose which is further consumed as a motion prior by a deep-learned VO model, which finally produces a fine relative pose to be used by the navigation policy. This strategy helps our pipeline achieve up to 2x sample efficiency during training and demonstrates superior accuracy and robustness in point-goal navigation tasks compared to state-of-the-art VO method(s). Realistic indoor environments of the Gibson dataset is used in the AI-Habitat simulator to evaluate the proposed approach using navigation metrics (like success/SPL) and pose metrics (like RPE/ATE). We hope this method further opens a direction of work where motion priors from various sources can be utilized to improve VO estimates and achieve better results in embodied navigation tasks.","Autonomous visual navigation in novel indoor environments is a fundamental skill for robots to perform further intelligent downstream tasks like finding and retrieving an object, rearranging various stuff, etc. This has been the focus of computer vision and robotics researchers for a long time. To bring together the community’s efforts and standardize the evaluation framework and metrics, Anderson et al. [1] proposed the task of PointGoal navigation. In PointNav, (illustrated in Fig. 1), the agent or robot is initialized in a previously unseen environment and tasked to reach a goal location specified with respect to its initial location, i.e. go to (△⁢x△𝑥\triangle x△ italic_x, △⁢y△𝑦\triangle y△ italic_y). The action space of the agent is discrete and mainly consists of 4 types of action : move_forward, turn_right, turn_left and stop (to end the episode). A pointnav episode is considered successful if the agent stops within a pre-determined distance of the goal location (say for e.g. 0.36 metres) and within the maximum number of time-steps allowed in an episode (say for e.g. 500 time-steps). Apart from success, the agent is evaluated via navigation metrics like SPL [1] and SoftSPL [8]. This point-goal navigation task can either be approached via map-based methods (where the agent simultaneously maps an unexplored area, localizes within it and then plans a path towards the goal) [7, 15] or via recent map-less end-to-end reinforcement learning based methods [8, 31, 19]. Under the assumption of an ideal scenario, i.e. perfect localization using noiseless GPS+Compass, noiseless egocentric RGB-D sensors and absence of actuation noise; this PointNav task (v1) is fully solved by both map-based and map-less approaches. But the real-world is not ideal, so PointNav v2 came into existence where the agent needs to localize itself (absence of GPS+Compass sensors), has noisy RGB-D observations and noisy actuations. Under this noisy setting, both map-based and map-less approaches need to focus first on accurate localization of the agent, before navigating. Recent map-less learning based approaches try to solve this by breaking down the task into two parts - learning visual odometry (VO) (for localization) and learning navigation policy (for actions) separately. During inference, this VO model can be used as a drop-in replacement for the GPS+Compass sensor with navigation policies trained using ground-truth pose in simulation. Figure 1: Point-Nav Task: The agent must navigate from its initial location (blue square) to a goal location (red square) specified as goal coordinates w.r.t. its initial location, using only its noisy RGB-D observations and noisy actuation. The agent’s and the oracle’s path is shown as blue and green lines respectively on the top-down map. Now, VO has been studied in Computer/Robot Vision literature for a long-time and many matured solutions exist but this PointNav task definition makes this problem harder to solve. Due to the agent’s discrete action space and large motion per action (default: 0.25m forward and 30 deg turns), VO needs to be estimated in a wide camera baseline setting, i.e. the two views are wide-apart and the overlapping region is less. Most of the VO methods, both traditional and learned variants assume availability of frame-pairs with a large overlap or in other words, a narrow baseline setting. But for practical robot navigation, wide-baseline VO is necessary because situations might arise where the robot motion is fast and/or observation processing or transmission FPS is low. Recent learned VO methods used with RL-based navigation policies in map-less approaches [19, 31] have tried to solve this problem and has achieved robust performance but suffer from sample-inefficiency, embodiment specificity and dataset specificity. It requires huge datasets and compute resources to train such a model and it can’t be zero-shot transferred to any other embodiment or dataset. So, we propose a robust and sample-efficient novel VO pipeline based on motion priors available while an agent is navigating an environment. We found out from our experiments that simple motion priors like action prior from agent’s controller or planner, or coarse pose prior from another geometric pose estimator, etc can help the VO model learn faster using fewer samples than that required by state-of-the-art (SoA) learned methods. This finding not only reduces compute resources needed to train one model but also helpful in scenarios when the target domain data is scarce and costly to collect and curate. Instead of solving for model generalization, we tweaked these models to be more sample-efficient. We also propose a training-free action-prior based geometric pose estimator or VO module for embodied agents which shows superior performance compared to frame-to-frame VO baselines created using state-of-the-art geometric modules, in standalone evaluation. With this novel geometric VO module, we show that motion priors can be effectively utilized to improve relative pose estimates from sparse feature matching methods. We use this module to estimate a coarse pose which serves as a better motion prior than action prior to train our neural VO model."
https://arxiv.org/html/2411.04782v1,An Effective Pipeline for Whole-Slide ImageGlomerulus Segmentation,"Whole-slide images (WSI) glomerulus segmentation is essential for accurately diagnosing kidney diseases. In this work, we propose a practical pipeline for glomerulus segmentation that effectively enhances both patch-level and WSI-level segmentation tasks. Our approach leverages stitching on overlapping patches, increasing the detection coverage, especially when glomeruli are located near patch image borders. In addition, we conduct comprehensive evaluations from different segmentation models across two large and diverse datasets with over 30K glomerulus annotations. Experimental results demonstrate that models using our pipeline outperform the previous state-of-the-art method, achieving superior results across both datasets and setting a new benchmark for glomerulus segmentation in WSIs. The code and pre-trained models are available at https://github.com/huuquan1994/wsi_glomerulus_seg.","Fig. 1: The overview of our proposed pipeline for WSI glomerulus segmentation. The glomerulus is the main filtering unit of the kidney that helps to retain essential proteins and cells in the blood while filtering out waste and excess substances. Glomerular disease is the leading cause of chronic kidney disease (CKD), which is reported to affect over 10% of the global population, impacting more than 800 million people [1]. Analyzing and early detection of glomerular disease is essential for diagnosing and treating CKD. Following the tremendous success of deep learning in the field of computer vision, numerous computer-aided diagnostics systems for glomerulus detection on whole-slide images (WSI) have been proposed [2, 3, 4, 5, 6]. Despite the success of the above studies, their results are either reported on the well-known HuBMAP dataset [7] or on private datasets. Recently, there are two large and diverse datasets for glomeruli segmentation on kidney tissue that have been released publicly called the mice glomeruli dataset [8] and the kidney pathology image segmentation challenge dataset (KPIs) [6]. To our knowledge, very few studies have been reported, with only one study available for each dataset [6, 5]. Thus, more performance analysis on these new and diverse datasets is essential to enrich this field of research. In this work, we propose an effective glomeruli segmentation pipeline and comprehensively evaluate its performance on those two datasets. Different from [5], which focused on patch-level segmentation, we further extend our evaluation to WSI-level segmentation of the mice glomeruli dataset. Additionally, while the work in [6] used large input patches (i.e., up to 4K) for their end-to-end WSI inference system, we demonstrate that our proposal outperforms theirs using much smaller patches (e.g., 768×768768768768\times 768768 × 768). To our knowledge, this study is the first to analyze the performance of various segmentation models for both patch-level and WSI-level segmentation tasks on these datasets. It is worth noting that with our pipeline, we won 1st place in both tracks of the kidney pathology image segmentation challenge 2024111https://sites.google.com/view/kpis2024/. Our contributions are summarized as follows: • We propose a glomeruli segmentation pipeline for effectively improving both patch-level and WSI-level segmentation tasks. • We conduct comprehensive performance evaluations from different segmentation models on the two large and diverse glomeruli datasets. • Models using our pipeline achieve state-of-the-art results on both datasets. The code and pre-trained models are made publicly available to encourage reproducibility and further research."
https://arxiv.org/html/2411.04732v1,Convolutional Differentiable Logic Gate Networks,"With the increasing inference cost of machine learning models, there is a growing interest in models with fast and efficient inference. Recently, an approach for learning logic gate networks directly via a differentiable relaxation was proposed. Logic gate networks are faster than conventional neural network approaches because their inference only requires logic gate operators such as NAND, OR, and XOR, which are the underlying building blocks of current hardware and can be efficiently executed. We build on this idea, extending it by deep logic gate tree convolutions, logical OR pooling, and residual initializations. This allows scaling logic gate networks up by over one order of magnitude and utilizing the paradigm of convolution. On CIFAR-10, we achieve an accuracy of 86.29% using only 61 million logic gates, which improves over the SOTA while being 29×\times× smaller.","Deep learning has led to a variety of new applications, opportunities, and use-cases in machine vision. However, this advancement has come with considerable computational and energy costs for inference [desislavov2021compute]. Therefore, an array of methods has been developed for efficient deep learning inference [qin2020binary, gholami2021survey, hoefler2021sparsity, liu2022unreasonable, molchanov2017pruning, petersen2022difflogic]. These include binary weight neural networks (BNNs) [qin2020binary], a set of methods for quantizing neural network weights down to binary representations (and sometimes also binary activations); quantized low-precision neural networks [gholami2021survey], a superset of BNNs and sparse neural networks [hoefler2021sparsity, liu2022unreasonable, molchanov2017pruning], a set of approaches for pruning neural networks and increasing sparsity. These methods have been successfully utilized for efficient vision model inference. The state-of-the-art (SOTA) method for small architectures, deep differentiable logic gate networks (LGNs) [petersen2022difflogic], approaches efficient machine learning inference from a different direction: learning an LGN (i.e., a network of logic gates such as NAND and XOR) directly via a differentiable relaxation. Differentiable LGNs directly learn the combination of logic gates that have to be executed by the hardware. This differs from other approaches (like BNNs) that require translating an abstraction (like matrix multiplication-based neural networks) into executable logic for inference, an inductive bias that comes with a considerable computational burden. By optimizing the logic directly on the lowest possible level instead of optimizing an abstraction, differentiable LGNs lead to very efficient inference on logic gate-based hardware (e.g., CPU, GPU, FPGA, ASIC). Recently, differentiable LGNs achieved SOTA inference speeds on MNIST [lecun2010mnist, petersen2022difflogic]. However, a crucial limitation was the random choice of connections, preventing LGNs from learning spatial relations, as they arise in images, which limited performance to an accuracy of only 62%percent6262\%62 % on CIFAR-10 [krizhevsky2009cifar10, petersen2022difflogic]. To address this limitation, we propose to extend differentiable LGNs to convolutions. Specifically, we propose deep logic gate tree convolutions, i.e., kernels comprised of logic gate trees applied in a convolutional fashion. Using trees of logic gates, instead of individual gates, increases the expressivity of the architecture while minimizing memory accesses, improving accuracy and accelerating training as well as inference. Further, we adapt pooling operations by representing them with logical or gates (relaxed via the maximum t-conorm), improving the effectiveness of convolutions in LGNs. Additionally, we propose “residual initializations”, a novel initialization scheme for differentiable LGNs that enables scaling them up to deeper networks by providing differentiable residual connections. These advances lead to an accuracy of 86.29%percent86.2986.29\%86.29 % on CIFAR-10 using only 61616161 million logic gates, leading to cost reductions by ≥29×\geq 29\times≥ 29 × compared to SOTAs as displayed in Figure 1."
https://arxiv.org/html/2411.04723v1,Exploring the Stability Gap in Continual Learning:The Role of the Classification Head,"Continual learning (CL) has emerged as a critical area in machine learning, enabling neural networks to learn from evolving data distributions while mitigating catastrophic forgetting. However, recent research has identified the stability gap – a phenomenon where models initially lose performance on previously learned tasks before partially recovering during training. Such learning dynamics are contradictory to the intuitive understanding of stability in continual learning where one would expect the performance to degrade gradually instead of rapidly decreasing and then partially recovering later. To better understand and alleviate the stability gap, we investigate it at different levels of the neural network architecture, particularly focusing on the role of the classification head. We introduce the nearest-mean classifier (NMC) as a tool to attribute the influence of the backbone and the classification head on the stability gap. Our experiments demonstrate that NMC not only improves final performance, but also significantly enhances training stability across various continual learning benchmarks, including CIFAR100, ImageNet100, CUB-200, and FGVC Aircrafts. Moreover, we find that NMC also reduces task-recency bias. Our analysis provides new insights into the stability gap and suggests that the primary contributor to this phenomenon is the linear head, rather than the insufficient representation learning.","Neural networks have been widely adopted across various domains, including computer vision, natural language processing, speech and audio processing, and control tasks [31, 10, 5]. However, most neural network applications are limited to offline settings with static data distributions, primarily due to the challenge of catastrophic forgetting [27]—a phenomenon where a model loses previously acquired knowledge when exposed to new data or distribution shifts. Adapting to and learning from evolving data distributions is crucial for many tasks, such as autonomous driving [35], robotics [20], edge devices [28], and reinforcement learning [45]. Consequently, the field of continual learning (CL) [9, 40, 26] has emerged to enable neural networks to learn from dynamic data streams while mitigating catastrophic forgetting. Figure 1: Stability gap phenomenon throughout learning the first two tasks from the CIFAR100 5-task split. When evaluated with a standard linear head and Nearest-Mean Classifier (NMC), the NMC performance on the first task with more stable through the learning phase and achieves a better final accuracy, even though both networks use the same representations. Continual learning methods developed over the years mitigate forgetting and facilitate good performance on continual learning benchmarks through several techniques such as regularization [16, 22], replay [32, 6, 36, 3], parameter isolation [33, 21, 41, 44] and their hybrid combinations [30, 14, 46, 1]. Despite recent progress in the field [26], usually measured by the final performance of the continual learner on all tasks, training dynamics remain relatively underexplored. Recent work of Lange et al. [19] sheds more light on how the network behaves during continual training and highlights a curious phenomenon of the stability gap. The authors show that CL methods evaluated throughout the learning on the new task initially forget the previously learned knowledge and only recover a part of the performance on the previously learned tasks in the later stages of training. The stability gap brings into question a traditional understanding of stability in continual learning. In light of the stability gap, it appears that instead of mitigating actual forgetting, continual learning methods allow for initial forgetting to facilitate knowledge recovery later. Further investigation of this phenomenon is required to understand continual learning dynamics better. Initial work on the stability gap focused on image classification and class-, task- and domain-incremental continual learning settings [19]. Later works demonstrated that the stability gap also occurs during continual pre-training of Large Language Models [8], and even incremental training with full replay [13, 15]. The fact that the stability gap occurs even without a task or domain shift suggests that it might be an inherent property of the optimization algorithms used to train neural networks. Nevertheless, continual learning is the domain most affected by its presence, as its algorithms are particularly exposed to significant data shifts and are often required to function in an online manner. Therefore, recently a few works started to explore mitigating the stability gap in continual learning [4, 11], so far within a limited scope. To mitigate the impact of the stability gap across various applications of continual learning, we still need a better understanding of this phenomenon. Previous works [29] discovered that higher layers of neural networks are responsible for most of the forgetting that occurs in the continually trained neural network. That raises the question whether we can locate the source of the stability gap. Therefore, we pose the following research question: Which part of the neural network is the primary source of the stability gap? To this end, we disentangle the problem into backbone- and classifier-level stability gap by employing nearest-mean classifier (NMC) - a powerful continual learning tool [30, 25, 7] that, to the best of our knowledge, has been overlooked in previous works addressing the stability gap. NMC classifies the samples based on their proximity to the class prototypes in the representation space and therefore offers a disentangled classification framework that we can compare with a classic linear head learned with cross-entropy. We show that an oracle NMC (which uses prototypes constructed from all training samples seen so far) can mitigate most of the instability compared to using a linear head. We show that even in a more realistic scenario with a limited number of exemplars, an NMC classifier on top of representations trained with cross-entropy not only outperforms a linear head, but also exhibits way higher stability through the training, mitigating a large part of the stability gap when compared to the standard approach (see Fig. 1). We provide a thorough evaluation of NMC in comparison with the standard replay approach on well-established continual learning benchmarks split CIFAR100, ImageNet100 and fine-grained classification datasets CUB-200 and FGVC Aircrafts. We show that in all cases NMC facilitates more stable training and improves the final performance, regardless of the size of the memory. In addition, we demonstrate that the NMC classifier also alleviates a large part of task-recency bias[46, 1] - networks’ preference towards predicting the classes from the latest task. The main contributions of our paper can be summarised as: • We are the first work to investigate the phenomenon of stability gap in the context of different parts of the network. • We compare the performance and stability of linear head and NMC across a wide range of well-established continual learning baselines. • To strengthen our findings, we provide a thorough analysis of the phenomena described above examining the impact of memory buffer size, initialization with the pre-trained network, and task-recency bias exhibited by the final model. Our findings indicate that the linear head is responsible for the main portion of the stability gap phenomenon, and that the network representations are sufficient for way more stability when used with non-parametric NMC classifier. We hope that our work will shed new light on the stability gap in continual learning and prompt further investigation of this phenomenon."
https://arxiv.org/html/2411.04717v1,"Subspace-Constrained
Quadratic Matrix Factorization: Algorithm and Applications","Matrix Factorization has emerged as a widely adopted framework for modeling data exhibiting low-rank structures. To address challenges in manifold learning, this paper presents a subspace-constrained quadratic matrix factorization model. The model is designed to jointly learn key low-dimensional structures, including the tangent space, the normal subspace, and the quadratic form that links the tangent space to a low-dimensional representation. We solve the proposed factorization model using an alternating minimization method, involving an in-depth investigation of nonlinear regression and projection subproblems. Theoretical properties of the quadratic projection problem and convergence characteristics of the alternating strategy are also investigated. To validate our approach, we conduct numerical experiments on synthetic and real-world datasets. Results demonstrate that our model outperforms existing methods, highlighting its robustness and efficacy in capturing core low-dimensional structures.","Matrix factorization has achieved remarkable success across various domains, including clustering [1], recommendation systems [2], graph learning [3], and factor analysis [4]. The classical linear matrix factorization models [7] can be expressed as X≈A⁢B𝑋𝐴𝐵X\approx ABitalic_X ≈ italic_A italic_B, where X=[x1,x2,…,xn]𝑋subscript𝑥1subscript𝑥2…subscript𝑥𝑛X=[x_{1},x_{2},...,x_{n}]italic_X = [ italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ] stacks the samples by columns, the columns of A𝐴Aitalic_A represent the basis, and each column of B𝐵Bitalic_B represents the coordinates under the basis A𝐴Aitalic_A. However, this model has its limitations, as it can only discern the linear structures within X𝑋Xitalic_X. In cases where the data showcases nonlinear, low-dimensional properties, the linear representation model will fall short of capturing more intrinsic details [36]. In specific, we assume xi=f⁢(τi)+ϵi,i=1,…,n,formulae-sequencesubscript𝑥𝑖𝑓subscript𝜏𝑖subscriptitalic-ϵ𝑖𝑖1…𝑛x_{i}=f(\tau_{i})+\epsilon_{i},\ i=1,...,n,italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_f ( italic_τ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) + italic_ϵ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_i = 1 , … , italic_n , (1) where f⁢(⋅):ℝd→ℝD:𝑓⋅→superscriptℝ𝑑superscriptℝ𝐷f(\cdot):{\mathbb{R}}^{d}\rightarrow{\mathbb{R}}^{D}italic_f ( ⋅ ) : roman_ℝ start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT → roman_ℝ start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT is a nonlinear transformation from the low-dimensional representation to the ambient high-dimensional space and ϵisubscriptitalic-ϵ𝑖\epsilon_{i}italic_ϵ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is the noise which obeys some distribution. To reduce the dimension of searching space for f⁢(⋅)𝑓⋅f(\cdot)italic_f ( ⋅ ), we restrict f⁢(⋅)𝑓⋅f(\cdot)italic_f ( ⋅ ) to the subspace-constrained quadratic function class. The subspace-constrained quadratic function class can help us extract latent structural insights, including the origin, tangent space, normal space, and even the second fundamental form, through an optimization model. The rationale behind this model is that, regardless of the complexity of the distribution of {xi}subscript𝑥𝑖\{x_{i}\}{ italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }, we can at least select a subset of {xi}subscript𝑥𝑖\{x_{i}\}{ italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } in the local neighborhood that exhibits quadratic characteristics. In other words, we learn f⁢(⋅)𝑓⋅f(\cdot)italic_f ( ⋅ ) from the subspace-constrained quadratic function class of ℱ={f⁢(τ)=c+U⁢τ+V⁢𝒜⁢(τ,τ),UT⁢U=Id,VT⁢V=Is,UT⁢V=𝟎},ℱformulae-sequence𝑓𝜏𝑐𝑈𝜏𝑉𝒜𝜏𝜏formulae-sequencesuperscript𝑈𝑇𝑈subscript𝐼𝑑formulae-sequencesuperscript𝑉𝑇𝑉subscript𝐼𝑠superscript𝑈𝑇𝑉0{\cal F}=\{f(\tau)=c+U\tau+V{\cal A}(\tau,\tau),U^{T}U=I_{d},V^{T}V=I_{s},U^{T% }V=\bf 0\},caligraphic_F = { italic_f ( italic_τ ) = italic_c + italic_U italic_τ + italic_V caligraphic_A ( italic_τ , italic_τ ) , italic_U start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_U = italic_I start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_V start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_V = italic_I start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , italic_U start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_V = bold_0 } , where the columns of U∈ℝD×d𝑈superscriptℝ𝐷𝑑U\in{\mathbb{R}}^{D\times d}italic_U ∈ roman_ℝ start_POSTSUPERSCRIPT italic_D × italic_d end_POSTSUPERSCRIPT consist of the basis on the tangent space, and the columns of V∈ℝD×s𝑉superscriptℝ𝐷𝑠V\in{\mathbb{R}}^{D\times s}italic_V ∈ roman_ℝ start_POSTSUPERSCRIPT italic_D × italic_s end_POSTSUPERSCRIPT span a s𝑠sitalic_s-dimensional subspace in the normal space. Here, the tensor 𝒜∈ℝs×ℝd×ℝd𝒜superscriptℝ𝑠superscriptℝ𝑑superscriptℝ𝑑{\cal A}\in{\mathbb{R}}^{s}\times{\mathbb{R}}^{d}\times{\mathbb{R}}^{d}caligraphic_A ∈ roman_ℝ start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT × roman_ℝ start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT × roman_ℝ start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT can be viewed as a bilinear operator 𝒜⁢(⋅,⋅):ℝd×ℝd→ℝs:𝒜⋅⋅→superscriptℝ𝑑superscriptℝ𝑑superscriptℝ𝑠{\cal A}(\cdot,\cdot):{\mathbb{R}}^{d}\times{\mathbb{R}}^{d}\rightarrow{% \mathbb{R}}^{s}caligraphic_A ( ⋅ , ⋅ ) : roman_ℝ start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT × roman_ℝ start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT → roman_ℝ start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT. We set s𝑠sitalic_s to be less than (d2+d)/2superscript𝑑2𝑑2(d^{2}+d)/2( italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_d ) / 2. This choice is motivated by the fact that the quadratic form of τ∈ℝd𝜏superscriptℝ𝑑\tau\in{\mathbb{R}}^{d}italic_τ ∈ roman_ℝ start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT produces results that are linear combinations of (d2+d)/2superscript𝑑2𝑑2(d^{2}+d)/2( italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_d ) / 2 cross terms in the form of {τi⁢τj,i,j=1:d}conditional-setsubscript𝜏𝑖subscript𝜏𝑗𝑖𝑗1𝑑\{\tau_{i}\tau_{j},i,j=1:d\}{ italic_τ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_τ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_i , italic_j = 1 : italic_d }. Meanwhile, since the dimension of the normal space is D−d𝐷𝑑D-ditalic_D - italic_d, it also necessitates that s≤D−d𝑠𝐷𝑑s\leq D-ditalic_s ≤ italic_D - italic_d. To the best of our knowledge, few studies have delved into estimating the subspace information and the second-order information within the distribution of the data. However, most of these studies, such as [24] and [20], primarily focus on theoretically estimating the tangent space or the second fundamental form from statistical perspectives, leaving computational challenges unresolved. We propose a novel approach to simultaneously learn the tangent space, a subspace of the normal space, the second fundamental form, and the low-dimensional representation τisubscript𝜏𝑖{\tau_{i}}italic_τ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT via the optimization problem: minf∈ℱ,{τi∈ℝd}⁢∑i=1n‖xi−f⁢(τi)‖22.subscript𝑓ℱsubscript𝜏𝑖superscriptℝ𝑑superscriptsubscript𝑖1𝑛superscriptsubscriptnormsubscript𝑥𝑖𝑓subscript𝜏𝑖22\min_{f\in{\cal F},\{\tau_{i}\in{\mathbb{R}}^{d}\}}\sum_{i=1}^{n}\|x_{i}-f(% \tau_{i})\|_{2}^{2}.roman_min start_POSTSUBSCRIPT italic_f ∈ caligraphic_F , { italic_τ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ roman_ℝ start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT } end_POSTSUBSCRIPT ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ∥ italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_f ( italic_τ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . (2) This model can be further reformulated as a special matrix factorization problem. Recalling the definition of ℱℱ\cal Fcaligraphic_F and the orthogonal relationship between U𝑈Uitalic_U and V𝑉Vitalic_V, we can rewrite (2) as: minc,𝒜,{τi}UT⁢U=Id⁡minV∈𝔽D,s⁢(U)⁢∑i=1n‖xi−(c+U⁢τi+V⁢𝒜⁢(τi,τi))‖22,subscriptFRACOP𝑐𝒜subscript𝜏𝑖superscript𝑈𝑇𝑈subscript𝐼𝑑subscript𝑉subscript𝔽𝐷𝑠𝑈superscriptsubscript𝑖1𝑛superscriptsubscriptnormsubscript𝑥𝑖𝑐𝑈subscript𝜏𝑖𝑉𝒜subscript𝜏𝑖subscript𝜏𝑖22\min_{c,{\cal A},\{\tau_{i}\}\atop U^{T}U=I_{d}}\min_{V\in{\mathbb{F}}_{D,s}(U% )}\sum_{i=1}^{n}\|x_{i}-(c+U\tau_{i}+V{\cal A}(\tau_{i},\tau_{i}))\|_{2}^{2},roman_min start_POSTSUBSCRIPT FRACOP start_ARG italic_c , caligraphic_A , { italic_τ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } end_ARG start_ARG italic_U start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_U = italic_I start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_ARG end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT italic_V ∈ roman_𝔽 start_POSTSUBSCRIPT italic_D , italic_s end_POSTSUBSCRIPT ( italic_U ) end_POSTSUBSCRIPT ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ∥ italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - ( italic_c + italic_U italic_τ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_V caligraphic_A ( italic_τ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_τ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , (3) where"
https://arxiv.org/html/2411.04679v1,"CaPo: Cooperative Plan Optimization for Efficient Embodied Multi-Agent
Cooperation","In this work, we address the cooperation problem among large language model (LLM) based embodied agents, where agents must cooperate to achieve a common goal. Previous methods often execute actions extemporaneously and incoherently, without long-term strategic and cooperative planning, leading to redundant steps, failures, and even serious repercussions in complex tasks like search-and-rescue missions where discussion and cooperative plan are crucial. To solve this issue, we propose Cooperative Plan Optimization (CaPo) to enhance the cooperation efficiency of LLM-based embodied agents. Inspired by human cooperation schemes, CaPo improves cooperation efficiency with two phases: 1) meta-plan generation, and 2) progress-adaptive meta-plan and execution. In the first phase, all agents analyze the task, discuss, and cooperatively create a meta-plan that decomposes the task into subtasks with detailed steps, ensuring a long-term strategic and coherent plan for efficient coordination. In the second phase, agents execute tasks according to the meta-plan and dynamically adjust it based on their latest progress (e.g., discovering a target object) through multi-turn discussions. This progress-based adaptation eliminates redundant actions, improving the overall cooperation efficiency of agents. Experimental results on the ThreeDworld Multi-Agent Transport and Communicative Watch-And-Help tasks demonstrate CaPo’s much higher task completion rate and efficiency compared with state-of-the-arts.","Large Language Models (LLMs) have demonstrated remarkable capabilities in understanding and generating human language, complex reasoning, and planning, achieving impressive performance (OpenAI, 2024; Touvron et al., 2023). These advancements empower LLM-based embodied agents to autonomously make plans (Li et al., 2023a; Padmakumar et al., 2022; Zhu et al., 2023; Wang et al., 2023; Wu et al., 2023b; Huang et al., 2022b) and perform reasoning (Du et al., 2023; Hao et al., 2023; Zhou et al., 2024; Huang et al., 2022a) by using human language to assist people in daily activities, such as housework and daily chores. The next milestone for agents is to cooperate with others to achieve joint tasks. This is crucial not only for efficiently performing simple tasks but also for tackling complex ones that cannot be completed in isolation due to their inherent complexity or the dynamic nature of the environment (Zhang et al., 2023b; Guo et al., 2024; Mandi et al., 2023; Zhang et al., 2023a). Figure 1: Procedure example of task accomplishment of CoELA (Zhang et al., 2023b) and our CaPo. In CoELA, after each action execution, Alice and Bob communicate to decide next action which is a greedy single-step plan and suboptimal. For example, they do not use wood basket which can contain sever objects, and both extemporaneously move a single item to the target bed without a long-term strategic and collaborative plan. Differently, in CaPo, Alice and Bob first discuss to make a long-term meta-plan for strategical cooperation in which Alice is arranged to move several target items into a wood basket, and Bob moves the remaining target items and also searches the unknown objects. Then during execution phase, both follow the meta-plan to accomplish task, and dynamically adapt the meta-plan the latest task progress, ensuring its effectiveness and efficiency in coordinating agents. Notably, the cooperation among LLM-based embodied agents is rarely investigated despite being highly desired. Conventional works often focus on adopting reinforcement learning (RL) (Jiang & Lu, 2018; Liu et al., 2021; Wang et al., 2021) to explore the dynamics of cooperative behavior among non-LLM-based agents. In spite of their promising performance in certain scenarios, RL-based cooperation methods exhibit limited adaptability across different tasks (Dittadi et al., 2021; Cobbe et al., 2019), since they are often not trained on large-scale data and lack sufficient generalization ability. To solve this issue, in this work, we are particularly interested in the problem of “how to develop an effective collaboration framework for LLM-based agents”, since LLMs have revealed strong reasoning, planning, and communication ability across different tasks and thus are regarded as good agents’ brains. Among the limited related works, CoELA (Zhang et al., 2023b) proposes an LLM-based multi-agent cooperation framework in which after each action execution, agents communicate to devise a single-step plan for the next action. Despite its significant advancements, CoELA’s short-term, single-step planning, which lacks consideration for long-term strategic collaboration, often results in extemporaneous and incoherent actions among agents, leading to several potential issues. Firstly, without a long-term coherent collaboration plan, it leads to numerous redundant action steps and increased costs, since agents’ movement is not easy and is indeed expensive in the physical world. For instance, as shown in Fig. 1, for the object transport task, agent Alice and Bob do not use the wood basket which can contain several objects, and extemporaneously move their nearest target objects one by one, leading to inferior efficiency. Moreover, complex tasks are difficult to accomplish without thorough discussion and long-term collaboration, especially in (embodied) environments where each agent has only partial observations. Finally, without a long-term cooperative plan, agents’ extemporaneous actions can result in mistakes with severe consequences. For instance, in search-and-rescue missions, poor coordination can have dire outcomes, such as endangering human lives due to the complex nature of these operations. Contributions. To address the above issues, we propose a novel and effective Cooperative Plan Optimization (CaPo) framework that uses LLMs’ strong reasoning and planning ability to enhance the cooperation efficiency of LLM-based embodied agents. Inspired by human cooperation schemes (Tuomela, 1998; Thürmer et al., 2017), CaPo engages agents in multi-turn discussions to create and update a long-term strategic and coherent meta-plan, providing step-by-step guidance to coordinate agents and efficiently complete tasks. Specifically, to accomplish a task, CaPo consists of two phases: 1) meta-plan generation, providing long-term strategical and coherent guidance for coordinating agents, and 2) progress-adaptive meta-plan and execution, dynamically adapting the meta-plan to agents’ latest progress. In the first phase, agents analyze the task and discuss with other agents for collecting relevant information. Next, one agent is responsible for making a meta-plan which decomposes the task into subtasks with detailed accomplishment steps like agent allocations, and then collects the feedback from other agents for further meta-plan refinement. The steps of meta-plan generation and refinement will continue until all agents reach a consensus or the communication cost is exhausted. This approach ensures the thorough discussion and analysis of all agents, helping to make a long-term strategical and coherent meta-plan for efficiently coordinating all agents. For example, as illustrated in Fig. 1, in the object transport task, agents Alice and Bob are strategically assigned to different subtasks. In the second phase, as shown in Fig. 1, agents follow the meta-plan from the first phase, and focus on their assigned subtasks. As progress is made, agents may complete subtasks or make important observations, such as Alice in Fig. 1 discovering the object “iPhone” which is Bob’s target. Accordingly, agents dynamically adapt meta-plan to the latest task progress through multi-turn discussions, allowing Alice to handle the object “iPhone” and complete the task efficiently. This progress-adaptive approach ensures that the meta-plan remains effective in coordinating all agents, thereby enhancing cooperation efficiency. Finally, experimental results demonstrate that CaPo significantly improves task completion rates and efficiency compared to state-of-the-art (SoTA) methods on the widely used ThreeDworld Multi-Agent Transport task (Zhang et al., 2023b) (object transport task) and the Communicative Watch-And-Help task (Zhang et al., 2023b) (household chore task). For instance, on the ThreeDworld Multi-Agent Transport task, CaPo surpasses the SoTA CoELA by 16.7% and 4.7% in completion rate with GPT-3.5 and GPT-4 based agents, respectively"
https://arxiv.org/html/2411.04595v1,TexLiverNet: Leveraging Medical Knowledge and Spatial-Frequency Perception for Enhanced Liver Tumor Segmentation,"Integrating textual data with imaging in liver tumor segmentation is essential for enhancing diagnostic accuracy. However, current multi-modal medical datasets offer only general text annotations, lacking lesion-specific details critical for extracting nuanced features, especially for fine-grained segmentation of tumor boundaries and small lesions. To address these limitations, we developed datasets with lesion-specific text annotations for liver tumors and introduced the TexLiverNet model. TexLiverNet employs an agent-based cross-attention module that integrates text features efficiently with visual features, significantly reducing computational costs. Additionally, enhanced spatial and adaptive frequency domain perception is proposed to precisely delineate lesion boundaries, reduce background interference, and recover fine details in small lesions. Comprehensive evaluations on public and private datasets demonstrate that TexLiverNet achieves superior performance compared to current state-of-the-art methods. Code is available at https://github.com/sky-visionX/TexLiverNet.","Deep learning is widely applied to accurately map liver anatomy, supporting treatment planning for procedures like tumor resection and minimally invasive surgery. U-Net [1] and its variants [2, 3] are especially popular for their efficient encoder-decoder structure, optimized for lesion feature extraction from medical images. However, most liver tumor segmentation methods [4, 5] rely on single-image modality potentially limiting the depth of tumor characterization. Recently, the availability of annotated text data has advanced text-guided medical image segmentation, which integrates complex medical text with image data. Attention-based methods have shown particular promise. For instance, TGANet [6] translates textual descriptions into attention-weighted text features, enhancing polyp segmentation when combined with image data. LanGuideSeg [7] extracts text features using CXR-BERT, and realizes deep fusion of text and image information at the decoder stage through the cross-attention mechanism, effectively segmenting lung lesions. The Universal Model [8] utilizes contrastive learning to integrate simple text-image information for multi-organ tasks. Transformer-based models have further enhanced multi-modal integration. For example, LViT [9] combines lesion location text input with a U-shaped Visual Transformer (ViT) to accurately segment pneumonia lesions. However, these methods often lack detailed morphological descriptions of lesions and involve high computational costs for text-image fusion. Moreover, existing multi-modal approaches encounter limitations in detecting liver tumors, particularly in identifying small lesions and resolving blurred tumor boundaries. Traditional image processing and multi-scale analysis have shown utility; for instance, SBCNet [5] employs Sobel operators to refine tumor boundary detection, while DA-Tran [10] applies 3D transformers for multi-scale segmentation on CT images, enhancing small lesion detection. However, these methods can inadvertently magnify non-target regions, leading to interference from adjacent organs. Fig. 1: The architecture of the proposed TexLiverNet. We argue that integrating domain-specific medical knowledge can effectively guide network learning. Hence, we developed the Visual-Text Liver Tumor Segmentation (VTLiTS) dataset, which provides both imaging data and more medically informed textual descriptions. We propose a novel model, TexLiverNet, leveraging spatial-frequency domain-enhanced imaging features alongside these text annotations. By efficiently fusing both modalities, TexLiverNet addresses challenges posed by small targets and unclear lesion boundaries, leading to more precise liver tumor segmentation. Contributions are summarized as follows: 1) We proposed a text-image agent cross-attention module to enhance efficient interaction between the two modalities. The intermediate agent mechanism is adopted to reduce the computation complexity of the attention mechanism. 2) We improve texture detail preservation by integrating spatial and adaptive frequency domain perception capabilities, restoring deep information lost in images and clarifying lesion boundaries. 3) Experimental comparison with the state-of-the-art methods on two datasets demonstrate the effectiveness of TexLiverNet."
https://arxiv.org/html/2411.04594v1,"Verification of Neural Networks against Convolutional Perturbations
via Parameterised Kernels","We develop a method for the efficient verification of neural networks against convolutional perturbations such as blurring or sharpening. To define input perturbations we use well-known camera shake, box blur and sharpen kernels. We demonstrate that these kernels can be linearly parameterised in a way that allows for a variation of the perturbation strength while preserving desired kernel properties. To facilitate their use in neural network verification, we develop an efficient way of convolving a given input with these parameterised kernels. The result of this convolution can be used to encode the perturbation in a verification setting by prepending a linear layer to a given network. This leads to tight bounds and a high effectiveness in the resulting verification step. We add further precision by employing input splitting as a branch and bound strategy. We demonstrate that we are able to verify robustness on a number of standard benchmarks where the baseline is unable to provide any safety certificates. To the best of our knowledge, this is the first solution for verifying robustness against specific convolutional perturbations such as camera shake.","As neural networks are increasingly deployed in a range of safety-critical domains such as autonomous vehicles, aviation, or robotics, concerns about their reliability are rising. Networks have been shown to be vulnerable to Adversarial Attacks, perturbations that are often imperceptible but change the output of the network on a given instance (Szegedy et al. 2014; Madry et al. 2017). Such adversarial examples have been shown to also exist in the physical world and pose a threat to algorithms deployed in practical applications (Eykholt et al. 2018; Tu et al. 2020). Neural Network Verification has been put forward as a way to address these issues by formally proving that for a given input, a network is robust to a set of specified perturbations, often referred to as local robustness (Katz et al. 2017; Gehr et al. 2018; Singh et al. 2018a). Algorithms are usually divided into complete and incomplete approaches. Given enough time, complete methods are guaranteed to provide a definitive answer to the verification problem. Meanwhile, incomplete methods may not be able to answer the verification problem, returning an undecided result. Complete approaches often employ an exact encoding of the network at hand. They rely on techniques such as Mixed Integer Linear Programming (MILP) (Tjeng, Xiao, and Tedrake 2019; Anderson et al. 2020; Bunel et al. 2020) or Satisfiability Modulo Theories (SMT) (Pulina and Tacchella 2012; Katz et al. 2017). Incomplete verifiers on the other hand employ Semidefinite Programming (Raghunathan, Steinhardt, and Liang 2018; Dathathri et al. 2020; Fazlyab, Morari, and Pappas 2020) or bound propagation (Wang et al. 2018a, b; Singh et al. 2019b; Xu et al. 2021; Wang et al. 2021). They usually overapproximate the true behaviour of the neural network and can be made complete by combining them with a Branch and Bound (BaB) strategy. Stronger verifiers either employ tighter relaxations such as SDP-based ones or linear constraints that reason over multiple neurons simultaneously (Singh et al. 2019a; Müller et al. 2022; Ferrari et al. 2022; Zhang et al. 2022) State-of-the-art (SoA) verifiers achieve low runtimes through exploiting GPU-enabled parallelism (Brix et al. 2023b, a). Early works usually verified robustness against norm-based perturbations, often referred to as white noise, which covers a limited number of scenarios and rarely appears in the real world (Pulina and Tacchella 2010, 2012; Singh et al. 2018a; Katz et al. 2019). A number of other perturbations were later proposed. Photometric perturbations such as brightness, contrast, hue or saturation changes as well as more expressive bias field perturbations can be encoded by prepending suitable layers to a neural network (Kouvaros and Lomuscio 2018; Henriksen et al. 2021; Mohapatra et al. 2020). Verifiers can equally be extended to handle more complex geometric perturbations such as rotations, translations, shearing or scaling, although the efficient verification against such perturbations requires further modifications and extensions (Singh et al. 2019b; Balunovic et al. 2019; Mohapatra et al. 2020; Kouvaros and Lomuscio 2018). Other works focus on the efficient verification of robustness to occlusions (Mohapatra et al. 2020; Guo et al. 2023) or semantically rich perturbations in the latent space of generative models (Mirman et al. 2021; Hanspal and Lomuscio 2023). More relevant to this work are previous investigations of camera shake effects. Guo et al. (2020) examine the performance of networks in the presence of motion blur and find that it is highly problematic, significantly degrading the performance of the models. The phenomenon can be modeled by applying a convolution operation using suitable kernels to a given input image (Sun et al. 2015; Mei et al. 2019). Using different kernels, convolution operations can similarly be used to implement other image transformations which include box blur (Shapiro and Stockman 2001, pp.153–154) and sharpen (Arvo 1991, pp.50–56). Since many semantically interesting and realistic perturbations can be modelled using convolution, being able to verify robustness to perturbations in a kernel space is highly valuable. Some attempts at verifying the robustness of models to such perturbations have been made before. Paterson et al. (2021) encode contrast, haze and blur perturbations but only perform verification for haze while resorting to empirical testing for contrast and blur. One previous work presents a general method which, if successful, certifies robustness of a network to all possible perturbations represented by a kernel of the given size (Mziou-Sallami and Adjed 2022). However, this generality comes at a cost. It leads to loose bounds and a high dimensionality of the perturbation which makes verification difficult, even more so for large networks. The universality also implies that counterexamples which are misclassified by the network may be difficult to interpret. We propose a new method which aims at the tight verification of networks against convolutional perturbations with a semantic meaning. Our key contributions are the following: • To enable an efficient symbolic encoding of the perturbations, we show how an arbitrary constant input can efficiently be convolved with a linearly parameterised kernel using standard convolution operations from a machine learning library. • We present parameterised kernels for motion blur perturbations with various blurring angles as well as box blur and sharpen. • Using standard benchmarks from past editions of the Verification of Neural Networks Competition (Brix et al. 2023b, a) as well as self-trained models, we show experimentally that verification is significantly easier with our method due to the tighter bounds and the low dimensionality of the perturbation. Our ablation study demonstrates that the existing method is unable to verify any properties on the networks we use. At the same time, our method certifies a majority of the properties for small kernel sizes and perturbation strengths while still being able to certify robustness in a number of cases for large kernel sizes and strengths."
https://arxiv.org/html/2411.04456v1,Properties ofB⁢V−G𝐵𝑉𝐺BV-Gitalic_B italic_V - italic_Gstructures+textures decomposition models. Application to road detection in satellite images,"In this paper we present some theoretical results about a structures-textures image decomposition model which was proposed by the second author. We prove a theorem which gives the behavior of this model in different cases. Finally, as a consequence of the theorem we derive an algorithm for the detection of long and thin objects applied to a road networks detection application in aerial or satellite images.","A few years ago, the second author proposed several image decomposition models which are aimed to split an image into three components. The first component should describe the objects contained in the image, the second one is given by the textured components and the third one is an additive noise. The structures are modelized as functions belonging to the B⁢V−limit-from𝐵𝑉BV-italic_B italic_V -space (the space of bounded variations functions) and the textures as oscillating functions belonging to a space, called G𝐺Gitalic_G by the second author, which is close to the dual of the B⁢V−limit-from𝐵𝑉BV-italic_B italic_V -space. Many papers deal with numerical implementation [1, 5, 4], extensions to other cases (like color images [12]) and some applications [13] but few of them study the theoretical behavior of this model. In this paper, we propose to explore this behavior through different results. The main theorem we present depicts the optimal decompositions obtained by an adapted tuning of parameters and the properties in the different functional spaces of the image to be decomposed. The remainder of the paper is organized as follows. In Section II, we remind the Rudin-Osher-Fatemi algorithm which is the origin of the work of the second author about image decomposition model. In Section III, we present the B⁢V−G𝐵𝑉𝐺BV-Gitalic_B italic_V - italic_G model and give some notations which will be used in the rest of the paper. Section IV is the main section of the paper and presents some theoretical results detailing the behavior of the model. In Section V, we present an application of the main theorem proved in Section IV to the enhancement of long and thin structures which can be used for example for road networks detection in aerial or satellite images. We will finish by concluding and giving some perspectives to this work for future research."
https://arxiv.org/html/2411.04404v1,Enhancing Bronchoscopy Depth Estimation through Synthetic-to-Real Domain Adaptation,"Monocular depth estimation has shown promise in general imaging tasks, aiding in localization and 3D reconstruction. While effective in various domains, its application to bronchoscopic images is hindered by the lack of labeled data, challenging the use of supervised learning methods. In this work, we propose a transfer learning framework that leverages synthetic data with depth labels for training and adapts domain knowledge for accurate depth estimation in real bronchoscope data. Our network demonstrates improved depth prediction on real footage using domain adaptation compared to training solely on synthetic data, validating our approach.","I INTRODUCTION Depth information plays a crucial role in localization and visual odometry [1], as it provides essential 3D information about the environment. RGB-D cameras, which simultaneously capture color and depth, are widely used in general environments for depth acquisition. However, the compact size of bronchoscopes restricts the direct implementation of RGB-D cameras for depth measurement. Despite the progress in monocular depth estimation for general images [2], applying these techniques to bronchoscopic images remains challenging because of the scarcity of labeled data for training of learning-based approaches. To overcome these challenges, researchers have explored specific depth estimation methods suitable for bronchoscopic images [3, 4]. These methods predominantly utilize cycle adversarial architectures [5] for unsupervised training, leveraging unpaired bronchoscopic frames and depth information from virtual bronchoscopy. However, the training process of GAN suffer from mode collapse and instability [6]. This work is motivated by the fact that although ground truth depth labels are inaccessible in real clinical settings, they are easily available in virtual bronchoscopy. In this work, we develop a domain adversarial network that is trained on labeled synthetic data, and transfer to real bronchoscopic data with high accuracy. Experiments highlight the effectiveness of transfer learning in leveraging synthetic data for depth estimation in bronchoscopy, demonstrating its potential for clinical applications. Figure 1: Diagram of the Synthetic-to-Real Domain Adaptation pipeline."
https://arxiv.org/html/2411.04291v1,Unfair Alignment: Examining Safety Alignment Across Vision Encoder Layers in Vision-Language Models,"Content warning: This paper contains unsafe model-generated content.Vision-language models (VLMs) have improved significantly in multi-modal tasks, but their more complex architecture makes their safety alignment more challenging than the alignment of large language models (LLMs). In this paper, we reveal an unfair distribution of safety across the layers of VLM’s vision encoder, with earlier and middle layers being disproportionately vulnerable to malicious inputs compared to the more robust final layers. This “cross-layer” vulnerability stems from the model’s inability to generalize its safety training from the default architectural settings used during training to unseen or out-of-distribution scenarios, leaving certain layers exposed. We conduct a comprehensive analysis by projecting activations from various intermediate layers and demonstrate that these layers are more likely to generate harmful outputs when exposed to malicious inputs. Our experiments with LLaVA-1.5 and Llama 3.2 show discrepancies in attack success rates and toxicity scores across layers, indicating that current safety alignment strategies focused on a single default layer are insufficient.","With the recent success of LLMs in advancing natural language understanding and generation, researchers have extended these models to incorporate additional modalities, such as vision, leading to the development of VLMs. Given their popularity, an essential question arises: “Are these large models truly safe for everyone or could they be jailbroken to produce harmful outputs?”. Extensive work has been done in this area, with researchers using techniques like supervised fine-tuning (SFT) (Zong et al., 2024; Liu et al., 2024), reinforcement learning from human feedback (RLHF) (Bai et al., 2022), and unlearning (Chakraborty et al., 2024) to safety align these models, adapting techniques used for safety alignment of LLMs. Despite efforts to enhance model safety, recent research shows LLMs and VLMs remain vulnerable to various attacks, including adversarial perturbations (Zou et al., 2023), trojan attacks (Zhao et al., 2023a), and recently shown multi-modal attacks (Shayegani et al., 2024; Gong et al., 2023) even though the LLM backbones of these VLMs are safety aligned. This is due to the limited generalization of these large models to out-of-distribution (OOD) inputs not included in safety training datasets. Consequently, when faced with harmful multi-modal prompts, which appear OOD, these limitations result in safety mechanism failures, leading to jailbreaks. This underscores the need for an in-depth investigation into the vulnerabilities and risks of VLMs. To understand the inner workings of VLMs, we propose studying how intermediate layer activations in the vision encoder affect overall safety alignment. By focusing on these activations, we seek to better understand how they influence the VLM’s ability to handle harmful multi-modal prompts. Our work is motivated by observing that current safety alignment techniques default to using activations from the last or second-to-last layers of the vision encoder. This approach may unintentionally compromise safety training when activations from other intermediate layers are projected instead of the default layer that was used for safety training. We hypothesize that exposing the language model backbone to activations from other intermediate layers of the vision encoder that were not part of the training process, they are treated as OOD, which can then cause the model to violate the safety alignment. To validate our hypothesis, we present a methodology where the VLM is presented with harmful cross-modal input prompts (safe images, harmful text). Instead of using default alignment layers, we systematically project activations from various intermediate layers of the vision encoder to assess their impact on model’s safety alignment. Aligning with our hypothesis and prior research, we find that activations from specific intermediate layers can lead the VLM to generate harmful outputs. To assess the effectiveness of our approach, we evaluate two widely used VLMs, LLaVA-1.5 (Liu et al., 2023a) and Llama 3.2 (Dubey et al., 2024), using a set of prompts combining harmful text from the AdvBench dataset and safe images curated online. Additionally, we employ recent evaluation tools like Llama Guard (Inan et al., 2023), and Perspective API (Lees et al., 2022) to calculate metrics like Average Success Rate (ASR), Toxicity Scores (TS). Experimental results show a mismatch in alignment across intermediate layers of vision encoder, with late layers being more safety-aligned than early and middle layers. To our knowledge, we are the first to conduct a layer-wise analysis of the vision encoder in VLMs, revealing how intermediate activations affect the model’s overall safety alignment. Figure 1: (A) The standard approach uses the hidden state from the last layer of the vision encoder. We explore using hidden states from various layers, revealing that safety alignment varies across different layers and is not fairly distributed. (B) With the same input (image and prompt), selecting different vision encoder layers significantly impacts the safety of the output. (C) Safety training is applied with the model’s default settings and architecture. However, limited generalization causes vulnerabilities, leaving certain areas of the embedding space uncovered when architectural changes are made."
https://arxiv.org/html/2411.04263v1,Object Recognition in Human Computer Interaction:- A Comparative Analysis,"Human-computer interaction (HCI) has been a widely researched area for many years, with continuous advancements in technology leading to the development of new techniques that change the way we interact with computers. With the recent advent of powerful computers, we recognize human actions and interact accordingly, thus revolutionizing the way we interact with computers. The purpose of this paper is to provide a comparative analysis of various algorithms used for recognizing user faces and gestures in the context of computer vision and HCI. This study aims to explore and evaluate the performance of different algorithms in terms of accuracy, robustness, and efficiency. This study aims to provide a comprehensive analysis of algorithms for face and gesture recognition in the context of computer vision and HCI, with the goal of improving the design and development of interactive systems that are more intuitive, efficient, and user-friendly.","The field of human-computer interaction (HCI) has undergone significant transformation in recent years with advancements in computing power, computer vision, and machine learning techniques. The demand for more interactive and natural user interfaces has led to the development of systems that can recognize human actions and gestures [1], [2], [3]. These smart-systems enable users to interact with computers more effectively and intuitively, making HCI a topic of extensive research in academia and industry. One critical aspect of HCI is the recognition of users’ faces and gestures. Recognizing faces and gestures accurately and in real-time is an essential component of many applications, including gaming, security, and human-robot interaction. Several approaches have been proposed in the literature for face and gesture recognition, including traditional computer vision-based methods [4], [5], and more recent deep learning-based techniques [6]. Face recognition verifies identity via facial features, with practical applications in security, biometrics, and HCI. It has been an area of extensive research for decades, with a wide range of practical applications, including security and surveillance systems, biometric identification, and access control. Similar to face recognition, gesture recognition is an important area of research in human-computer interaction, allowing machines to recognize and understand human actions and gestures. It has numerous practical applications, including gaming, virtual reality, and human-robot interaction. In recent years, there has been significant progress in the development of face and gesture recognition techniques, driven largely by advances in machine learning and computer vision algorithms. These techniques include traditional computer vision-based methods and more recent deep learning-based approaches. In this paper, we will look at them in detail."
https://arxiv.org/html/2411.04155v1,MINDSETS: Multi-omics Integration with Neuroimaging for Dementia Subtyping and Effective Temporal Study,"In the complex realm of cognitive disorders, Alzheimer’s disease (AD) and vascular dementia (VaD) are the two most prevalent dementia types, presenting entangled symptoms yet requiring distinct treatment approaches. The crux of effective treatment in slowing neurodegeneration lies in early, accurate diagnosis, as this significantly assists doctors in determining the appropriate course of action. However, current diagnostic practices often delay VaD diagnosis, impeding timely intervention and adversely affecting patient prognosis. This paper presents an innovative multi-omics approach to accurately differentiate AD from VaD, achieving a diagnostic accuracy of 89.25%. The proposed method segments the longitudinal MRI scans and extracts advanced radiomics features. Subsequently, it synergistically integrates the radiomics features with an ensemble of clinical, cognitive, and genetic data to provide state-of-the-art diagnostic accuracy, setting a new benchmark in classification accuracy on a large public dataset. The paper’s primary contribution is proposing a comprehensive methodology utilizing multi-omics data to provide a nuanced understanding of dementia subtypes. Additionally, the paper introduces an interpretable model to enhance clinical decision-making coupled with a novel model architecture for evaluating treatment efficacy. These advancements lay the groundwork for future work not only aimed at improving differential diagnosis but also mitigating and preventing the progression of dementia.","Dementia is a complex brain disorder, affecting 55 million individuals globally with ten million new cases annually, and represents a critical public health challenge [1]. This umbrella term encompasses various cognitive disorders, including Alzheimer’s disease (AD) and vascular dementia (VaD), which are the most prevalent forms. AD, responsible for 60-80% of dementia cases, is the sixth leading cause of death in the USA, with its mortality rates rising in contrast to those of heart disease and cancer [2, 3]. This trend underscores the urgent need for enhanced research, healthcare strategies, and societal support to address the escalating burden of AD. VaD, constituting 10-15% of dementia cases, shares symptoms with AD but requires distinct treatment due to its cerebrovascular pathology [4]. It results from cerebral ischemic or hemorrhagic events, leading to cognitive decline that varies with the affected brain regions. Early detection and management of vascular risk factors are critical for preventing VaD progression and improving patient outcomes. The differential diagnosis between AD and VaD is vital given their unique etiologies and treatment approaches. AD primarily impairs memory and cognitive functions related to the hippocampus and entorhinal cortex, affecting language, navigation, and recognition skills [5]. In contrast, VaD affects executive functions early, resulting in difficulties with planning, organization, complex tasks, and mood disturbances [6, 7]. VaD patients have a more severe speech impairment and score up to 7.7% less on phonetic fluency tasks than AD patients, [7]. The Hachinski Ischemic Score (HIS) is vital in distinguishing between degenerative and vascular dementia origins [8]. Both AD and VaD have a profound impact on individuals’ lives, extending far beyond the physiological symptoms. They affect patients’ social, psychological, and economic well-being, but the nature and progression of cognitive decline differ between the two. Understanding the multifaceted nature of this impact emphasizes the critical importance of early diagnosis. Our research aims to bridge the gap in diagnostic capabilities, offering new insights and methodologies to enhance early detection and treatment strategies for AD and VaD. Contributions. This paper introduces a method for differentiating AD and VaD using multi-omics data. The main contributions are: • Propose a unified and robust multi-omics method to distinguish between AD and VaD from MRI radiomics, genetic data, and clinical assessments to reveal their pathological differences while achieving state-of-the-art classification performance on a large public dataset. • Introduce an explainable model to facilitate its adoption in clinical practice and provide a personalized interpretable diagnosis. • Develop a model architecture to monitor longitudinal changes in MCI diagnosis confidence and probability to assess treatment efficacy. These contributions underscore our paper’s potential to transform the diagnostic landscape for AD and VaD, offering a clinically applicable solution to bridge current gaps in the diagnosis and management of dementia."
https://arxiv.org/html/2411.04153v1,"Urban Flood Mapping Using Satellite Synthetic Aperture Radar Data: A Review of Characteristics, Approaches and Datasets","Understanding the extent of urban flooding is crucial for assessing building damage, casualties and economic losses. Synthetic Aperture Radar (SAR) technology offers significant advantages for mapping flooded urban areas due to its ability to collect data regardless weather and solar illumination conditions. However, the wide range of existing methods makes it difficult to choose the best approach for a specific situation and to identify future research directions. Therefore, this study provides a comprehensive review of current research on urban flood mapping using SAR data, summarizing key characteristics of floodwater in SAR images and outlining various approaches from scientific articles. Additionally, we provide a brief overview of the advantages and disadvantages of each method category, along with guidance on selecting the most suitable approach for different scenarios. This study focuses on the challenges and advancements in SAR-based urban flood mapping. It specifically addresses the limitations of spatial and temporal resolution in SAR data and discusses the essential pre-processing steps. Moreover, the article explores the potential benefits of Polarimetric SAR (PolSAR) techniques and uncertainty analysis for future research. Furthermore, it highlights a lack of open-access SAR datasets for urban flood mapping, hindering development in advanced deep learning-based methods. Besides, we evaluated the Technology Readiness Levels (TRLs) of urban flood mapping techniques to identify challenges and future research areas. Finally, the study explores the practical applications of SAR-based urban flood mapping in both the private and public sectors and provides a comprehensive overview of the benefits and potential impact of these methods.","Floods are among the most destructive natural disasters worldwide, whose frequency and magnitude are increasing due to growing heavy precipitation resulting from global climate change [1]. Generally, floods can occur in different land cover/land use areas, such as open areas, agricultural areas/vegetated areas and urban areas. Among those areas, urban flooding is a growing concern due to rapid urbanization and increasing flood risk. More than half of the world’s population now lives in cities [2], and many of these urban areas are expanding into flood-prone zones. More specifically, human settlements, ranging from villages to megacities, expanded globally by 84.5% between 1985 and 2015, and settlement moved into high hazard flood zones outpacing growth in flood-safe areas [3]. Thus, urban floods can lead to high death tolls, enormous financial losses and hinder socially sustainable development. For example, [4] analyzed over ten thousand cloud-free optical images from 2000 to 2018 and found that a growing number of people are at risk of flooding. This trend is alarming, with economic losses estimated at $651 billion USD during this period. The devatating floods in Kenya and Libya in 2023 further underscore the urgent need for action. In Kenya, 46 people lost their lives, and 58,000 were displaced [5]. The situation in Libya was even more tragic, with 5,300 deaths and thousand missing [6]. Additionally, the UK’s Suffolk region faced flooding in October 2023 due to Storm Babet, affecting 1,260 properties [7]. Urban floods can also have severe consequences beyond property damage and loss of life. The COVID-19 pandemic highlighted the increased risk of disease transmission during and after floods [8]. Clearly, developing effective methods for urban flood mapping is essential for supporting flood risk managers, emergency responders, and local governments in their efforts to monitor, forecast, and manage flood events. Synthetic Aperture Radar (SAR) is a powerful tool for the extraction of flooded urban from space and it has several advantages in comparison with optical data. Indeed, SAR systems use longer wavelengths of the electromagnetic spectrum, allowing cloud penetration and weather independent image acquisition. Furthermore, SAR systems are active sensors, transmitting and receiving their own electromagnetic impulses, which allows operations independent of daylight. Currently, thanks to the growing number of SAR satellite constellations (e.g., Sentinel-1, COSMO-SkyMed, TerraSAR-X, ALOS-2, RCM, SAOCOM and GF3), a massive amount of diverse SAR data is now available to map water bodies at global scale. Moreover, the rapid development of commercial SAR satellites constellations (e.g., Capella, HiSea, ICEYE, Synspective,Umbra-X) provides a possibility for monitoring of ground features and their changes over time at increasing temporal resolution, making near-real time flood mapping envisageable in the near future. Although SAR data offers significant advantages for detecting urban floods, interpreting the radar signal can be complex due to various factors. Densely built-up areas often have challenges like mutual shadowing between buildings, multiple reflections, building orientation, and reflections from flat roofs. These factors, along with scattering from other elements like windows, can make it difficult to accurately map floods in urban areas [9]. For example, SAR backscatter intensity values vary depending on the difference between floodwater depth and the surrounding buildings’ heights [10, 11]: SAR backscatter can significantly increase during urban floods due to the stronger double-bounce interaction between the specular reflecting water surface and the buildings’ façades if buildings are partially submerged; When buildings are fully submerged by floodwater, the radar backscatter from the area can decrease significantly due to the smooth water surface reflecting the signal directly back to the satellite [12]. The radar backscatter from urban floods can vary in different ways. To address this, interferometric SAR (InSAR) technology, which analyzes both the signal strength and phase, has been shown to be a valuable tool for flood mapping [13, 14, 15, 16, 17, 18, 12]. This is bacause the presence of floodwater in urban areas often causes a rapid decrease in InSAR coherence. It should be noted that the InSAR coherence is calculated using a moving window of a fixed size, leading to comparatively lower spatial resolution when compared to SAR intensity. Additionally, changes in coherence in flooded urban areas often occur around buildings due to the way radar signals bounce off structures. In some cases, the coherence might not change significantly due to flooding, especially in areas with large, flat buildings and narrow streets. This could lead to missed detections of urban floods, as noted in previous studies [12]. Therefore, [19, 17] argue that it is important to consider the phase information in addition to coherence in dense urban areas because phase is more sensitive to subtle changes caused by floodwater. It is important to note that the SAR characteristics discussed so far primarily focus on buildings surrounded by water. This is different from the broader concept of urban flooding, which can also include flooded streets, open spaces, and other areas within cities. The term ’urban’ is complex and can have different meanings depending on the context. While it generally refers to densely populated areas with infrastructure, the specific definition can vary widely. For example, it could refer to a small village, a town, or a magacity [20, 21]. In principle, urban regions should contain all of the surroundings, including all of the built-ups, vegetation, parks, airports, small lakes and roadways. In the remote sensing field, we have several different terms/products for the definition of urban areas, such as the impervious surfaces/areas in global land cover map (FROMFROM-GLC10) [22, 23] including roads, driveways, sidewalks, parking lots, rooftops, etc [24], global urban footprint (GUF) / world settlement footprint (WSF) covering all built-ups from megacities to small villages [25, 26, 27], the global human settlement layer (GHSL) proposed by the European Joint Research Center (JRC) containing all the built-ups classification from Sentinel-2 images [28], and the urban/built-up layers in several global land cover/land use maps [29]. To the best of our knowledge, the definitions of urban floods are not always the same among different studies: some methods are described that urban floods are able to be delineated with relatively low accuracies in flooded urban areas, as their methods are developed mainly for flooded open areas/bare soils [30, 31]. These approaches can only identify flooded public areas in urban settings that are not surrounded by buildings; some urban flood mapping approaches are proposed focusing on the built-up regions [12, 15] and some others investigated floodwater mapping in residential regions [32]. To be clear, the general concept of urban flooding usually encompasses two scenarios: buildings surrounded by floodwater, and flooding that only affects main streets and public areas without encircling any buildings. In the second scenario, these areas can be considered similar to rural flooding, as there are no significant obstacles like buildings. Methods for detecting flooded open areas are summarized in several studies such as [33, 34]. However, due to the side-looking nature of the SAR sensor and complex environments, detecting flooded buildings remains a challenging task (details explained in SectionIII). Therefore, our study concentrates on urban flood mapping studies focusing on flooded built-ups where the buildings are surrounded by floodwater based on the characteristics in SAR data. In recent years, significant research has focused on developing algorithms to extract urban flood information from SAR data. This is driven by advancements in computer vision, SAR technology, and the growing need for real-time flood monitoring. Several studies have explored the potential of SAR systems for detecting urban floods. These reports summarize the knowledge about the relationships between the sensors’ parameters (i.e., wavelength, polarization, incidence angle, interferometry) and environmental conditions (e.g. dielectric constant, topographic effects, rain/snow, etc.) [35, 33, 34, 36, 9, 37]. However, no specific review has taken place on SAR-based urban flood mapping. This article aims to provide a comprehensive overview of SAR-based urban flood mapping, addressing the following key areas: • Sensor Characteristics: Understanding how sensor parameters (i.e., wavelength, polarization, incidence angle, interferometry) and land cover types affect the SAR signal for urban flood detection. • Classification Algorithms: Evaluating the latest methods for extracting urban flood information from SAR data, including their advantages and limitations. • Future trends: Exploring emerging technologies and challenges that will shape the future of SAR-based urban flood mapping. • Case Studies: Examining sepecific examples of urban flood events capture by Sentinel-1 to illustrate the application of SAR-based methods. • Practical Implications: Highlighting the importance of urban flood mapping for both private and public sectors. The article is composed of seven distinct sections. Section II provides an overview of the literature that serves as the foundation for all the analyses discussed in this work. Section III presents a concise summary of the properties of SAR for urban floods. It outlines the connection between SAR features such as wavelength, polarisation, and incidence angle, and the environmental factors like water level and urban form that are relevant to urban floods. Section IV categorises current approaches into four distinct categories, each possessing its own advantages and limitations. Section V examines the previously described findings and demonstrates upcoming trends, along with the necessity to enhance urban flood mapping. Additionally, this section allows users to access a comprehensive list of urban flood events, along with a detailed description of the dataset utilised. Section VI delineates the practical uses of SAR-based techniques for mapping urban floods, encompassing flood risk management and the endeavour to achieve sustainable development goals. Section VII presents conclusions."
https://arxiv.org/html/2411.04125v1,Community Forensics: Using Thousands of Generators to TrainFake Image Detectors,"One of the key challenges of detecting AI-generated images is spotting images that have been created by previously unseen generative models. We argue that the limited diversity of the training data is a major obstacle to addressing this problem, and we propose a new dataset that is significantly larger and more diverse than prior work. As part of creating this dataset, we systematically download thousands of text-to-image latent diffusion models and sample images from them. We also collect images from dozens of popular open source and commercial models. The resulting dataset contains 2.7M images that have been sampled from 4803 different models. These images collectively capture a wide range of scene content, generator architectures, and image processing settings. Using this dataset, we study the generalization abilities of fake image detectors. Our experiments suggest that detection performance improves as the number of models in the training set increases, even when these models have similar architectures. We also find that detection performance improves as the diversity of the models increases, and that our trained detectors generalize better than those trained on other datasets.111https://jespark.net/projects/2024/community_forensics","Figure 1: Performance vs. model diversity. We use images sampled from different numbers of open source latent diffusion models in the Community Forensics dataset to train fake image detectors (shown in Fig. 2(a)). As the number of models increases, so does the detector’s performance, even though these models have similar designs and same total number of images. This improvement is largest for test images from out-of-distribution generative model classes, such as pixel-based diffusion models or GANs. For each data point, we sample 10 random model subsets with 100K training images each and report the mean and standard error values. Our ability to automatically generate realistic images is quickly outpacing our ability to detect them, potentially leading to a state of affairs in which neither humans nor machines can reliably tell real from fake. While the field of image forensics has been developing methods to address this problem, existing fake image detectors still struggle with generalization. These methods often excel at detecting images from generators that were present in their training sets, but fail when given images sampled from unseen models [105, 126, 91, 103]. A core challenge is dealing with the large amounts of variation between models. Each generator has a potentially unique combination of the architecture, loss function, and training distribution. Even seemingly minor differences in low-level image processing details, such as the ways that training images are resized or compressed, can strongly influence detection accuracy [126]. As a result of these model-specific idiosyncrasies, a generator’s images may evade detection, even when images from architecturally similar models exist in the training set. This issue has been exacerbated by the thousands of open source models that are now available online, many of which extend pretrained base models in complex ways. (a) Systematically collected diffusion models (4763 models) (b) Manually chosen open source models (19 models) (c) Commercial models (11 models) Figure 2: The Community Forensics dataset. Our dataset contains images sampled from three types of generative models. (a) We systematically download open-source latent diffusion models from a model-sharing community [124, 37]. (b) We select popular open source generators with a variety of architectures and training procedures. (c) We sample from both closed and open state-of-the-art commercial models. We present example images and their corresponding model names. We hypothesize that the lack of diversity in training datasets is a major source of these shortcomings. Although today’s datasets often contain millions of fake images, they come from a relatively small number of generators (e.g., fewer than 20 models in any given previous work that we are aware of). As a result, this data fails to capture many sources of variation that one might encounter in the wild. These limitations also make it challenging to accurately benchmark performance, since it is easy for cues that work well on one set of generators to fail on others. To address these problems, we propose Community Forensics, a dataset that is significantly more diverse and comprehensive than those in prior works (Fig. 2). Our dataset contains images generated by: (a) thousands of systematically downloaded open-source latent diffusion models, (b) hand-selected open source models with various architectures, and (c) state-of-the-art commercial models. We use this dataset to conduct a study of generalization in image forensics. To acquire large numbers of models, we sample images from thousands of text-to-image diffusion models hosted on a popular model-sharing website, Hugging Face [37]. We exploit the fact that these models use a common programming library [124] and thus can be sampled in a standardized way. A large fraction of them are extensions of Stable Diffusion [102], but collectively capture a variety of common model variations, such as in the architecture, image processing, and image content. We also sample images from many other open source models, including GANs [40], autoregressive models [42], and consistency models [117, 81]. To help study how image content affects classification performance, we provide a corresponding set of real images that are designed to resemble the generated images. For example, we condition the text-to-image models using text obtained by captioning our real images. Our dataset contains 4803 distinct models, approximately 250×250{}\times250 × more than the previous forensics datasets that sample images from generative models [126, 91, 35, 18, 9, 133], and covers a variety of recent model designs (Fig. 2). We use this dataset to study generalization in the generated image detection problem. Our experiments support the hypothesis that increasing the diversity of generative models used in training is important for generalization. Through experiments, we find: • Classifiers trained on our dataset obtain strong performance, both on our newly proposed evaluations and on multiple previously-proposed benchmarks. • Adding more generative models improves performance. Fig. 1 demonstrates the performance of fake image detection when trained on samples from varying numbers of diffusion models. Notably, the performance improves as more models are added, even across different architectures. • Including diverse generative model architectures significantly improves results, since classifiers do not fully generalize between generator architectures. Likewise, the performance gain from including large numbers of images from any particular architecture is relatively marginal. • Standard classifiers perform well. In contrast to observations from recent work, we find that end-to-end training of classifiers based on CNNs or ViTs generalizes well, with qualitatively similar to that of other recognition problems."
https://arxiv.org/html/2411.04097v1,RaVL: Discovering and Mitigating Spurious Correlations in Fine-Tuned Vision-Language Models,"Fine-tuned vision-language models (VLMs) often capture spurious correlations between image features and textual attributes, resulting in degraded zero-shot performance at test time. Existing approaches for addressing spurious correlations (i) primarily operate at the global image-level rather than intervening directly on fine-grained image features and (ii) are predominantly designed for unimodal settings. In this work, we present RaVL, which takes a fine-grained perspective on VLM robustness by discovering and mitigating spurious correlations using local image features rather than operating at the global image level. Given a fine-tuned VLM, RaVL first discovers spurious correlations by leveraging a region-level clustering approach to identify precise image features contributing to zero-shot classification errors. Then, RaVL mitigates the identified spurious correlation with a novel region-aware loss function that enables the VLM to focus on relevant regions and ignore spurious relationships during fine-tuning. We evaluate RaVL on 654 VLMs with various model architectures, data domains, and learned spurious correlations. Our results show that RaVL accurately discovers (191% improvement over the closest baseline) and mitigates (8.2% improvement on worst-group image classification accuracy) spurious correlations. Qualitative evaluations on general-domain and medical-domain VLMs confirm our findings.111Code: https://github.com/Stanford-AIMI/RaVL","Contrastive vision-language models (VLMs) (e.g., CLIP [36] and ALIGN [24]) are a powerful class of models that jointly learn relationships between images and text. VLMs are generally pretrained on web-scale datasets with millions of image-text pairs and have been shown to exhibit impressive capabilities on a wide range of downstream tasks. In particular, VLMs have the ability to perform tasks in a zero-shot manner without utilizing explicit task-specific training data; this is accomplished by modeling downstream tasks (e.g., image classification, text-to-image retrieval) as image-text matching tasks [36]. However, pretrained VLMs can exhibit poor zero-shot performance when compared to state-of-the-art task-specific models, particularly on challenging or out-of-domain downstream tasks [36, 7, 17, 19]. As a result, pretrained VLMs are often fine-tuned on domain-specific vision-language datasets in order to improve zero-shot performance on tasks of interest. For instance, recent works have fine-tuned the CLIP VLM [36] on vision-language datasets consisting of (i) chest X-rays and paired physician reports [45], (ii) pathology data and paired text [17, 19], and (iii) product images and paired captions from online fashion retailers [7]. Domain-specific vision-language datasets used to fine-tune VLMs may be small in size, preventing VLMs from gaining the robustness benefits that come with training on diverse, web-scale data [6, 14]. As a result, fine-tuned VLMs may capture spurious correlations between image features and textual attributes [56]. For instance, consider a VLM fine-tuned on an animal image-text dataset where the presence of butterflies is closely correlated with the presence of flowers (Figure 1). Consequently, the VLM may learn to incorrectly associate the image features corresponding to flower with the textual attribute butterfly. At test time, the VLM is likely to exhibit degraded zero-shot classification performance on (i) images of butterflies without flowers and (ii) images of other animals with flowers. Improving robustness of fine-tuned VLMs to spurious correlations is challenging for the following two reasons. First, existing automated approaches primarily discover and mitigate spurious correlations at the global image level rather than intervening directly on fine-grained image features. Such approaches discover spurious correlations by identifying coherent groups of misclassified images in an automated fashion [13, 43, 22, 42]; then, the identified spurious correlation can be mitigated during training using data augmentation or robust optimization [43, 39, 22, 56]. However, recent works have suggested that such global image-level strategies (i) discover spurious correlations that align poorly with human-interpretable attributes [25] and (ii) may not effectively enable models to ignore spurious correlations during training [15, 18]. Second, existing approaches for discovering and mitigating spurious correlations are predominantly designed to improve robustness of unimodal image classification models [39, 43] or pretrained VLMs [60, 49]. These settings differ substantially from the fine-tuned VLM setting, which presents several unique challenges such as the absence of class and subgroup labels in the training set and the inclusion of free-form text. Figure 1: Region-aware Vision-Language learning (RaVL). RaVL takes a fine-grained perspective on VLM robustness by discovering and mitigating spurious correlations using local image features. In this work, we address these challenges by introducing Region-aware Vision-Language learning (RaVL), an approach for improving the robustness of fine-tuned VLMs to spurious correlations. RaVL takes a fine-grained perspective on VLM robustness by discovering and mitigating spurious correlations using local image features, rather than operating at the global image level. Our contributions are: • First, given a fine-tuned VLM, RaVL discovers learned spurious correlations between image features and textual attributes. Using a labeled classification dataset, we decompose images into candidate regions, utilize the VLM embedding space to group visually-similar regions into feature clusters, and quantitatively evaluate the effects of each feature on zero-shot classification errors. • Second, given a ranked list of image features that the VLM has learned to spuriously correlate with one or more textual attributes, RaVL mitigates the identified spurious correlations. Our key insight is that region-level information can be leveraged during VLM fine-tuning in order to improve model robustness. To this end, we introduce a novel region-aware loss function that encourages the VLM to focus on relevant regions and ignore spurious relationships during fine-tuning. In order to evaluate RaVL, we introduce a large-scale evaluation framework for controlled, fine-grained evaluations of VLM robustness on synthetic and real-world data. Our framework consists of 654 fine-tuned VLMs paired with annotations for the ground-truth spurious correlations learned by each VLM. Across these evaluation settings, (i) RaVL accurately discovers spurious correlations, achieving a 191% improvement over the closest baseline, and (ii) RaVL effectively mitigates spurious correlations, achieving up to an 8.2% improvement on worst-group image classification accuracy. Qualitative evaluations on general-domain and medical-domain VLMs confirm the utility of RaVL. This paper is organized as follows. In Section 2, we introduce our problem setting. Then, in Section 3, we present Stage 1 of RaVL, including our proposed methodology for discovering spurious correlations, our large-scale evaluation framework, and experimental results. In Section 4, we introduce Stage 2 of RaVL, including our proposed methodology for mitigating spurious correlations as well as experimental results. Finally, we conclude in Section 5. Related Work: Our work builds on several recent research directions for discovering and mitigating spurious correlations. We provide an analysis of related works in Appendix Section A."
https://arxiv.org/html/2411.04079v1,Textual Decomposition Then Sub-motion-space Scattering for Open-Vocabulary Motion Generation,"Text-to-motion generation is a crucial task in computer vision, which generates the target 3D motion by the given text. The existing annotated datasets are limited in scale, resulting in most existing methods overfitting to the small datasets and unable to generalize to the motions of the open domain. Some methods attempt to solve the open-vocabulary motion generation problem by aligning to the CLIP space or using the Pretrain-then-Finetuning paradigm. However, the current annotated dataset’s limited scale only allows them to achieve mapping from sub-text-space to sub-motion-space, instead of mapping between full-text-space and full-motion-space (full mapping), which is the key to attaining open-vocabulary motion generation. To this end, this paper proposes to leverage the atomic motion (simple body part motions over a short time period) as an intermediate representation, and leverage two orderly coupled steps, i.e., Textual Decomposition and Sub-motion-space Scattering, to address the full mapping problem. For Textual Decomposition, we design a fine-grained description conversion algorithm, and combine it with the generalization ability of a large language model to convert any given motion text into atomic texts. Sub-motion-space Scattering learns the compositional process from atomic motions to the target motions, to make the learned sub-motion-space scattered to form the full-motion-space. For a given motion of the open domain, it transforms the extrapolation into interpolation and thereby significantly improves generalization. Our network, DSO-Net, combines textual decomposition and sub-motion-space scattering to solve the open-vocabulary motion generation. Extensive experiments demonstrate that our DSO-Net achieves significant improvements over the state-of-the-art methods on open-vocabulary motion generation. Code is available at https://vankouf.github.io/DSONet/.","Figure 1: Compared with current text-to-motion paradigms (Simple mapping, CLIP-based alignment, and Pretrain-then-Finetuning), our method proposes the textual decomposition to decompose the raw motion text into atomic texts and sub-motion-space scattering to learn the composition process from atomic motions to target motions, which significantly improves the ability of open-vocabulary motion generation. Text-to-motion (T2M) generation, aiming at generating the 3D target motion described by the given text, is an important task in computer vision and has garnered significant attention in recent research. It plays a crucial role in various applications, such as robotics, animations, and film production. Benefiting from advancements in GPT-style (e.g., LlamaGen (Sun et al., 2024)) and diffusion-style generative paradigm in text-to-image and text-to-video domains, some studies (Zhang et al., 2023a; Jiang et al., 2023; Tevet et al., 2022b; Shafir et al., 2023) have started using these technologies to address the T2M generation task. During the training process, paired text-motion data are utilized to align the text space with the motion space. However, the open-vocabulary text-to-motion generation remains a challenging problem, requiring good motion generation quality for unseen open-vocaulary text at inference. Due to the limited scale of recent high-quality annotated datasets (e.g., KIT-ML (Plappert et al., 2016) and HumanML3D (Guo et al., 2022)), as illustrated in Fig. 1 top-left, the Simple Mapping paradigm only learns a mapping between a limited sub-text-space and a sub-motion-space, rather than the mapping from full-text-space to full-motion-space. Consequently, generalization to unseen open-vocabulary text is almost impossible. To enhance the model’s generalization capabilities, two main strategies have been explored. As shown in Fig. 1 top-right, the first paradigm is CLIP-based Alignment (e.g., MotionCLIP (Tevet et al., 2022a) and OOHMG (Lin et al., 2023)). This kind of approach aims to align the motion space with both the CLIP text space (Radford et al., 2021) and the image space. The core process involves fitting the motion skeleton onto the mesh of the human body SMPL (Loper et al., 2023) model and performing multi-view rendering to obtain pose images, thereby achieving alignment between motion and image spaces. The second paradigm is Pretrain-then-Finetuning (e.g., OMG (Liang et al., 2024a)), as illustrated in Fig. 1 bottom-left. Inspired by the success of the Stable Diffusion (Rombach et al., 2022) model in the text-to-image field, this paradigm follows a pretrain-then-finetuning process, along with scaling up the model, to enable generalization to open-vocabulary text. Although these two types of methods have achieved some progress in open-vocabulary text-to-motion generation, they suffer from inherent flaws: (1) The CLIP-based alignment paradigm aligns static poses with the image space, which results in the loss of temporal information during the learning process. Consequently, this approach generates unrealistic motion. Furthermore, this method overlooks the feature space differences between the CLIP and T2M task datasets, potentially leading to misalignment, unreliability, and inaccuracy in motion control. (2) Although the Pretrain-then-Finetuning paradigm utilizes an adequate motion prior and expands the motion space by pretraining on large-scale motion data, the annotated paired data in the finetuning stage is severely limited. The significant imbalance between labeled and unlabeled data results in the fine-tuning stage only learning the mapping from text to a condensed subspace of the full motion-space. Consequently, the model has to perform extrapolation and has difficulties in generating motions that are outside the subspace distribution, as shown in Fig. 1. Consequently, we conclude that the problem of insufficient generalization ability in current methods arises from the limited amount of high-quality labeled data and the inadequate utilization of large-scale unsupervised motion data for pretraining. Existing methods can only establish overfitted mappings within a limited subspace. To achieve open-vocabulary motion generation, it is essential to establish a mapping from the full-text-space to the full-motion-space. We observe that when understanding a motion, human beings tend to partition it into the combination of several simple body part motions (such as ”spine bend forward”, ”left hand up”) over a short time period, which we define as atomic motions. All these atomic motions are combined spatially and temporally to form the raw motion. This observation motivates us to decompose a raw motion into atomic motions and leverage atomic motions as an intermediate representation. Since raw motion texts often contain abstract and high-level semantic meanings, directly using raw texts to guide motion generation can hinder the model’s ability to understand open-vocabulary motion texts. In contrast, atomic motion texts provide a low-level and concrete description of different limb movements, which are shared across different domains. Leveraging the atomic motions as an intermediate representation, we propose to address the full-mapping problem through two orderly coupled steps: (1) Textual Decomposition. To enhance generalization ability, we first design a textual decomposition process that converts a raw motion text into several atomic motion texts, subsequently generating motions from these atomic texts. To prepare training data, we develop a fine-grained description conversion algorithm to establish atomic texts and motion pairs. Specifically, we partition the input motion into several time periods, and describe the movements of each joint and spatial relationships from the aspects of velocity (e.g., fast), magnitude (e.g., significant), and low-level behaviors (e.g., bending) for each period. The fine-grained descriptions and the raw text are then input into a large language model (LLM) to summarize the atomic motion texts. Each raw motion text is decomposed into atomic motion texts of six body parts: the spine, left/right-upper/lower limbs, and trajectory. By this process, we guarantee the converted atomic motion texts are consistent with the actual motion behavior. During inference, for any given text, we employ the LLM to split the whole motion into several periods and describe each period using atomic motion texts. In this way, we establish a mapping from the full-text-space to the full-atomic-text-space. (2) Sub-motion-space Scattering. After obtaining the full-text-space to the full-atomic-text-space mapping in the first step, we aim to further achieve alignment from the full-atomic-text-space to the full-motion-space, through the Sub-motion-space Scattering step, thereby establishing the mapping from the full-text-space to the full-motion-space. Given the limited labeled data, it is difficult for the Pretrain-then-Finetuning paradigm to learn the full-motion-space, because its trivial alignment process only learns a mapping from text to a condensed subspace (which we refer to as sub-motion-space), requiring extrapolation for out-of-domain motions. In contrast, our approach scatters the sub-motion-space to form the full-motion-space, as shown in the bottom right of Fig. 1, transforming extrapolation into interpolation and significantly improving generalization. The sub-motion-space scattering is achieved by learning the combinational process of atomic motions to generate target motions, with a text-motion alignment (TMA) module to extract features for atomic motion texts, and a compositional feature fusion (CFF) module to fuse atomic text features into motion features and learn the the combinational process from atomic motions to target motions. As shown in Fig. 1, Interpolating an out-domain motion is essentially a combination of several nearest clusters of scattered sub-motion-space, which is highly consistent with the process of CFF we design. Therefore, the CFF ensures for scattering sub-motion-space we learned. Overall, we adopt the discrete generative mask modeling and follow the pretrain-then-finetuning pipeline for open-vocabulary motion generation. First, we pretrain a residual VQ-VAE (Martinez et al., 2014) network using a pre-processed large-scale unlabeled motion dataset, to enable the network to have prior knowledge of large-scale motion. For the fine-tuning stages, we first leverage our textual decomposition module to convert the raw motion text into atomic texts. Then, we utilize both raw text and the atomic texts with our proposed TMA and CFF modules to train a text-to-model generative model. Our network, abbreviated as DSO-Net, combines textual decomposition and sub-motion-space scattering to solve the open-vocabulary motion generation. We conduct extensive experiments comparing our approach with previous state-of-the-art approaches on various open-vocabulary datasets and achieve a significant improvement quantitatively and qualitatively. In summary, our main contributions include: (1) we propose to leverage atomic motions as an intermediate representation, and design textual decomposition and sub-motion-space scattering framework to solve open-vocabulary motion generation. (2) For textual decomposition, we design a rule-based fine-grained description conversion algorithm and combine it with the large language model to obtain the atomic motion texts for a given motion. (3) For sub-motion-space scattering, we propose to leverage a text-motion alignment (TMA) module and a compositional feature fusion (CFF) module to learn the generative combination of atomic motions, thereby significantly improving the model’s generalization ability."
https://arxiv.org/html/2411.04077v1,H-POPE: Hierarchical Polling-based Probing Evaluation of Hallucinations in Large Vision-Language Models,"By leveraging both texts and images, large vision language models (LVLMs) have shown significant progress in various multi-modal tasks. Nevertheless, these models often suffer from hallucinations, e.g., they exhibit inconsistencies between the visual input and the textual output. To address this, we propose H-POPE, a coarse-to-fine-grained benchmark that systematically assesses hallucination in object existence and attributes. Our evaluation shows that models are prone to hallucinations on object existence, and even more so on fine-grained attributes. We further investigate whether these models rely on visual input to formulate the output texts.","Recent advances in multi-modal models have enabled a wide array of impressive capabilities. For example, such models can generate a well-formulated description of a given image and thus reflect an understanding of both the textual and the visual domains. Despite their promising progress, existing models often suffer from a phenomenon called hallucination, which not only degrades the model’s performance but also raises questions about safety and reliability. To understand and assess hallucinations in LVLMs, several benchmarks have been proposed. In specific, Caption Hallucination Assessment with Image Relevance (CHAIR) [9] demonstrates that when asked to provide an accurate description of a given image, responses from these models frequently include objects that are not actually present [9]. Moreover, the Polling-based Object Probing Evaluation (POPE) [5] shows that these models tend to confirm the presence of an object that does not exist, if it frequently co-occurs with other objects in the image. While these aforementioned benchmarks effectively highlight the general issue of hallucination, they often focus on assessing hallucinations on object presence. A natural extension is to further assess hallucination on granular details such as attributes. Therefore, the main objective of our project is to conduct a more fine-grained evaluation on not only the existence of objects but also the attributes associated with them. To achieve this goal, we leverage the discriminative approach outlined in POPE [5]. For each image, we start with coarse-grained questions about the existence of objects and continue with fine-grained questions about the existence of attributes. The resulting benchmark is called Hierarchical POPE (H-POPE). The original POPE includes three sampling strategies for negative objects. Some of them were specially designed to formulate challenging questions based on occurrence statistics in the dataset. We utilise these settings in our study, while also aiming to devise an additional sampling strategy for attributes. To this end, we propose Image-based Adversarial setting, which shifts the focus to the local context of the given image and selects attributes that appear in the image but do not describe the object in question. Our evaluation shows that models tend to hallucinate worst on questions sampled in this setting, which suggests that existing LVLMs struggle to match attributes correctly to objects that possess them. Finally, it is noticeable that the current work centers their evaluation metrics around the models’ textual output. We investigate relevance maps to see if the models make different use of visual information when giving hallucinated answers versus correct answers. In summary, our main contributions include: • Introducing H-POPE, a benchmark to assess hallucination on objects and attributes in LVLMs, including a new adversarial strategy targeted at evaluating whether LVLMs can match attributes to the correct objects in the image. • Evaluating three current LVLMs, namely mPLUG-Owl [12], InstructBLIP [3] and LLaVa [7] on H-POPE. • Qualitative examples of the visual cues used by LVLMs, which are derived from LVLM-Interpret [10]."
https://arxiv.org/html/2411.04059v1,Pseudo-labeling with Keyword Refining for Few-Supervised Video Captioning,"Video captioning generate a sentence that describes the video content. Existing methods always require a number of captions (e.g., 10 or 20) per video to train the model, which is quite costly. In this work, we explore the possibility of using only one or very few ground-truth sentences, and introduce a new task named few-supervised video captioning. Specifically, we propose a few-supervised video captioning framework that consists of lexically constrained pseudo-labeling module and keyword-refined captioning module. Unlike the random sampling in natural language processing that may cause invalid modifications (i.e., edit words), the former module guides the model to edit words using some actions (e.g., copy, replace, insert, and delete) by a pretrained token-level classifier, and then fine-tunes candidate sentences by a pretrained language model. Meanwhile, the former employs the repetition penalized sampling to encourage the model to yield concise pseudo-labeled sentences with less repetition, and selects the most relevant sentences upon a pretrained video-text model. Moreover, to keep semantic consistency between pseudo-labeled sentences and video content, we develop the transformer-based keyword refiner with the video-keyword gated fusion strategy to emphasize more on relevant words. Extensive experiments on several benchmarks demonstrate the advantages of the proposed approach in both few-supervised and fully-supervised scenarios. The code implementation is available at https://github.com/mlvccn/PKG_VidCap.","Video captioning has established itself as a fundamental task in computer vision by describing video content using natural language sentences [29][22]. It is more complicated than image captioning [3] due to the existence of rich temporal relations and spatio-temporal dynamics in video. Generally, the generated captions are expected to accurately describe video content and adhere to the rules of grammar. It has found wide applications in many practical scenarios, such as video retrieval, the blind assistance, and human-computer interaction. However, existing methods are mostly fully-supervised methods, which require a large number of ground-truth descriptions (e.g., at least 20 or more for each video) to guide the model training, which is very costly due to expensive human labeling. This raises a problem that how to guarantee the captioning quality when only single or very few (e.g., less than three) ground-truth descriptions (a.k.a., sentences) are available. Little effort has been made to consider this setting, which is named as few-supervised video captioning in this work. Note that it is different from few-shot setting (only a few samples per class) or semi-supervised setting (using both labeled and unlabeled data) in machine learning. Existing fully-supervised methods always adopt the encoder-decoder framework, e.g., encoder uses ResNet (Residual Network) [13], Faster R-CNN [31] (Regional Convolutional Neural Network), and C3D (Convolutional 3D network) [35] to extract appearance, object, and motion features respectively, while decoder employs LSTM (Long-Short Term Memory) to decode video features into a descriptive sentence. For example, Gao et al. [10] employ hierarchical attention mechanism as well as the knowledge and parameter sharing in language generation module to obtain hierarchical representations with rich semantics. Recently, transformer has received much interests in video captioning, e.g., Ye et al. [48] extract entity-level features by transformer to capture the most possibly referred object in caption. However, all these methods require an abundant of complete video-text pairs during training, which consumes very expensive labeling costs. Unlike them or weakly supervised learning which employs video-level annotation, there is only single or very few ground-truth sentence for each video in few-supervised setting. By contrast, few-supervised video captioning aims to approach or achieve the comparable performance of fully-supervised scenario. Figure 1: Motivation illustration (3 vs 1 human). Video is from MSVD [5]. As depicted in Fig. 1, full supervision needs three human annotations while few-supervision only needs one human annotation, largely saving the labeling costs. To bridge the semantic gap between full supervision and few supervision, there are two primary challenges, i.e., 1) how to expand sentences by pseudo labeling, and 2) how to guarantee the high-quality pseudo-labeled sentences for model training. Naturally, we may resort to data augmentation for expanding sentences by adopting the Natural Language Processing (NLP) techniques, such as synonym replacement and replacement with grammar rules. But they neglect sentence context which may cause the ambiguous problem, because a word has multiple synonyms which represent distinct meanings in a sentence. For example, “A dog is giving a high five to man” is edited as “A dog is giving a tall five to man”, which changes the original meaning of “high five”. Besides, one can use a deep learning language model [15] to randomly replace a word, or translate between two languages, which considers the context but hard to keep still the semantics of some key words (i.e., nouns and verbs). To address this issue, we employ a lexical constrained sentence generation technique that makes sentence include specified words or phrases. In particular, we adopt a two-step strategy [14] with lexical constraints, by first employing pretrained word classifier to guide the model to edit words using some action (e.g., copy, replace, insert, and delete) and then fine-tuning generated sentences by pretrained language model (e.g., XLNet [47]), leading to candidate pseudo-labeled sentences. Moreover, we use the repetitive penalized sampling to impose the penalty when repeated words appear, which reduces repetitions in sentence. Having obtained candidate sentences, how to effectively employ them for model training poses a major challenge. The above data augmentation neglects visual cues, which may cause inconsistency between pseudo-labeled sentences and video content, leading to noisy pseudo labels. To overcome this drawback, we introduce a pretrained video-text model, i.e., X-CLIP [28], for matching the most relevant candidate sentences with video, i.e., pseudo-labeled sentences. These pseudo-labeled sentences and human-labeled sentence are both fed into the captioning model as augmented supervision knowledge to guide the model training. To enhance the captioning ability of model, we design a video-keyword gated fusion scheme in the captioning model, which employs the attention mechanism in transformer to adjust the weights of key words in pseudo-labeled sentences. In particular, it increases the weights of the key words more relevant to video while decreasing the weights of those less relevant or irrelevant to video. As expected, the irrelevant words are overlooked and the relevant ones are more emphasized in pseudo-labeled sentence. In addition, we design a semantic loss of key words between pseudo-labeled and human-labeled sentences to ensure their semantic consistency, which is beneficial for improving the generalization ability of the model yielding both more fluent and more accurate sentences for a given video. Therefore, we propose a Pseudo-labeling with Keyword-refiner and Gated fusion (PKG) approach for few-supervised video captioning. It consists of two primary components, i.e., lexically constrained pseudo-labeling module and keyword-refined captioning module. The former is designed to generate pseudo-labeled sentences to compensate for the lack of sufficient supervision knowledge during model training, while the latter aims to match those most relevant candidate sentences with video by modeling the global context of visual cues and language semantics. To investigate the performance of our method, a large number of experiments and ablation studies were carried out on three publicly available data sets including MSVD [5], MSR-VTT [45], and VATEX [41]. The main contributions of this paper are summarized below: 1. We introduce a new task named few-supervised video captioning, which uses only one human-labeled (ground-truth) sentence to train the model. 2. We propose a pseudo labeling strategy with lexical constraint to augment supervision knowledge, which strengthens the guidance of model training. 3. We design a keyword-refined captioning module with video-text gated fusion for generating high-quality sentences that matches video content by modeling the global context. 4. Empirical studies on several benchmarks demonstrate our approach using only one human-labeled sentence achieves more promising performance than those using more human-labeled sentences. In addition, our gated fusion captioning model surpasses State-Of-The-Art (SOTA) supervised methods when all ground-truth labels are used."
https://arxiv.org/html/2411.04008v1,Aligning Characteristic Descriptors with Images for Human-Expert-like Explainability,"In mission-critical domains such as law enforcement and medical diagnosis, the ability to explain and interpret the outputs of deep learning models is crucial for ensuring user trust and supporting informed decision-making. Despite advancements in explainability, existing methods often fall short in providing explanations that mirror the depth and clarity of those given by human experts. Such expert-level explanations are essential for the dependable application of deep learning models in law enforcement and medical contexts. Additionally, we recognize that most explanations in real-world scenarios are communicated primarily through natural language. Addressing these needs, we propose a novel approach that utilizes characteristic descriptors to explain model decisions by identifying their presence in images, thereby generating expert-like explanations. Our method incorporates a concept bottleneck layer within the model architecture, which calculates the similarity between image and descriptor encodings to deliver inherent and faithful explanations. Through experiments in face recognition and chest X-ray diagnosis, we demonstrate that our approach offers a significant contrast over existing techniques, which are often limited to the use of saliency maps. We believe our approach represents a significant step toward making deep learning systems more accountable, transparent, and trustworthy in the critical domains of face recognition and medical diagnosis.","Deep learning models have revolutionized various applications, enhancing accuracy, efficiency, and scalability. However, their lack of transparency poses significant risks in security applications like face recognition, where accountability is crucial to identify biases or failures [38]. In legal contexts, identity search decisions must be justified, similar to how facial forensic examiners testify in court [32]. Investigations have exposed biases in commercial face recognition systems, exemplified by a wrongful arrest due to a flawed match [2] [3]. In computer-aided diagnosis, the black-box nature of these models makes it difficult for physicians to trust and explain decisions, raising concerns about biases and spurious correlations [24]. Incorporating explainability or interpretability mechanisms can address these challenges by clarifying decision-making processes, aiding in debugging, and revealing model biases [5] Although there are several existing methods relating to explainable face recognition [38, 25, 18, 22] they focus on indistinct visual explanations which are less interpretable than textual explanations [32]. Visual explanations can lack the nuanced detail and context often provided in textual explanations and may be subject to misinterpretation [32]. While prior works dealt with explaining the chest x-ray diagnosis made by deep learning models, none have explored using fine-grained, and atomic characteristic descriptors for providing radiologist-like explanations. The prior work can be mainly categorized into the explainable x-ray report generation [34] [19], or saliency maps for the x-ray images to depict the important regions for the diagnosis [31]. The ability of deep learning systems to provide human-expert-like explanations can help in regulatory compliance especially in legal proceedings, improved communication, and increased user trust and acceptance [32]. Contrary to the prior work, our approach employs precisely defined characteristic descriptors which are textual concepts to emulate the explanatory abilities of human experts in justifying facial recognition and chest x-ray diagnosis decisions, while also ensuring the faithfulness of the explanations through self-explainable architecture. An example of the explanations provided by our framework is given in figure 1. Figure 1: Illustration of our proposed explainable system for face recognition and chest x-ray diagnosis. Our framework is designed to function in both supervised and unsupervised environments and we demonstrate the efficacy of our framework in both contexts. For the unsupervised case, we utilize face recognition, and for the supervised case, we apply it to chest x-ray diagnosis. The characteristic descriptors or concepts (used interchangeably) for face recognition are derived from the facial features standard published by FISWG [4] for morphological analysis in face comparison for face recognition. For x-ray diagnosis, we extracted the descriptors from the radiology reports of the MIMIC-CXR [20] dataset. Our key contribution is proposing a generic explainable framework capable of functioning in both supervised and unsupervised contexts, which can be used to provide expert-like explanations to any classification decisions made by deep learning models. We show the results of our method which provides a coherent, faithful, user-friendly, and expert-like textual explanation for the tasks of face recognition, and x-ray diagnosis."
https://arxiv.org/html/2411.03993v1,Local vs distributed representations: What is the right basis for interpretability?,"Much of the research on the interpretability of deep neural networks has focused on studying the visual features that maximally activate individual neurons. However, recent work has cast doubts on the usefulness of such local representations for understanding the behavior of deep neural networks because individual neurons tend to respond to multiple unrelated visual patterns, a phenomenon referred to as “superposition”. A promising alternative to disentangle these complex patterns is learning sparsely distributed vector representations from entire network layers, as the resulting basis vectors seemingly encode single identifiable visual patterns consistently. Thus, one would expect the resulting code to align better with human-perceivable visual patterns, but supporting evidence remains, at best, anecdotal. To fill this gap, we conducted three large-scale psychophysics experiments collected from a pool of 560 participants. Our findings provide (i) strong evidence that features obtained from sparse distributed representations are easier to interpret by human observers and (ii) that this effect is more pronounced in the deepest layers of a neural network. Complementary analyses also reveal that (iii) features derived from sparse distributed representations contribute more to the model´s decision.Overall, our results highlight that distributed representations constitute a superior basis for interpretability, underscoring a need for the field to move beyond the interpretation of local neural codes in favor of sparsely distributed ones.","One of the goals of explainable AI (XAI) in computer vision is to identify the visual features and characterize the representations used by deep neural networks (DNNs) to categorize images (Ribeiro et al., 2016; Sundararajan et al., 2017; Smilkov et al., 2017; Petsiuk et al., 2018; Selvaraju et al., 2017; Linsley et al., 2019; Fel et al., 2021; 2023c; 2023a; Novello et al., 2022; Zhou et al., 2016; Bau et al., 2017; Cammarata et al., 2020b; Kim et al., 2018; Ghorbani et al., 2019). In general, identifying these features requires uncovering the visual patterns that drive the activation of units within a network. Figure 1: (a) ∙∙\bullet∙ Local (neuron) versus ∙∙\bullet∙ Distributed (sparsely distributed vector) visual representations. The activation of individual neurons may be driven by multiple unrelated visual elements (depicted in the images at the bottom) whereas distributed representations, obtained via dictionary learning methods, break down complex patterns into simpler ones corresponding to single visual features. (b) In practice, dictionary learning methods “disentangle” local activations to yield a new vector basis whose activation is driven by single features. The hope for interpretability is that those features align better with the set of features that humans can interpret S={f1,f2,…,fn}𝑆subscript𝑓1subscript𝑓2…subscript𝑓𝑛S=\{f_{1},f_{2},...,f_{n}\}italic_S = { italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_f start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }. This goal is shared with the study of biological vision, where there is extensive research over the last several decades focused on identifying the “preferred stimulus” of individual neurons in the visual cortex (Hubel & Wiesel, 1959; Lettvin et al., 1959; Tsunoda et al., 2001; Wang et al., 1996; Pasupathy & Connor, 2001; Quiroga, 2005). This approach to visual neuroscience reflected the dominant theory at the time, known as the “grandmother (cell)” theory , which postulates that information in the visual system is stored locally – at the level of single neurons – and that the visual system contains specific neurons that respond to particular objects or people (including one’s own grandmother). Early XAI advances inspired by neuroscience research similarly focused on understanding local representations (Zhou et al., 2016; Bau et al., 2017). This led to the development of more sophisticated optimization methods to synthesize maximally activating images for individual neurons (Erhan et al., 2009; Zeiler & Fergus, 2014a; Yosinski et al., 2015; Olah et al., 2017; 2020; Nguyen et al., 2016a; b; Cammarata et al., 2020b). This parallelism between XAI and neuroscience extends beyond vision as recent work has found neurons in multi-modal systems that respond to very high-level concepts beyond simple image appearance, including hand-drawing and text (Goh et al., 2021). Interestingly, the authors identified a “Halle Berry” neuron in CLIP, reminiscent of the neuroscience finding reported two decades ago in the human brain (Quiroga, 2005). At the same time, a paradigm shift is taking place in neuroscience, where the study of neural populations is quickly superseding the study of single neurons (for a review see Ebitz & Hayden (2021)) because the neural code is believed to be sparse and distributed rather than local (Haxby et al., 2001; Quiroga et al., 2008; 2013). Interestingly, a similar shift is emerging in XAI, because local representations are known to suffer from the “superposition” problem (Arora et al., 2018; Cheung et al., 2019; Olah et al., 2020; Elhage et al., 2022; Fel et al., 2023b): the number of features captured by DNNs might be larger than the number of neurons. Therefore, the neurons’ activations might be driven by multiple unrelated features. To address this challenge, the XAI community has started using dictionary learning methods (Fel et al., 2023c; b; Bricken et al., 2023; Templeton et al., 2024) to project the activations of DNNs onto a new basis of vectors, each activated by a single distinct feature. From an interpretability perspective, representations driven by single features are more desirable because they are expected to be easier to understand by a human, i.e., a unit responding to a single visual pattern –compared to multiple patterns– is inherently easier to interpret. An implicit hypothesis is that applying dictionary learning methods to network activations helps break down complex visual patterns into simpler ones corresponding to single features (Fig. 1.a), which, in turn, might be easier for humans to interpret (Fig. 1.b). Scaling up standard interpretability evaluations to study representations poses significant challenges (Colin et al., 2022). A practical alternative is to assess the ambiguity—or perplexity—of the visual features derived from these representations (Borowski et al., 2021) (see 3.2.1 for a more thorough elaboration on this point). Despite the growing consensus that distributed representations constitute a stronger basis for interpretability compared to local ones (single neurons), empirical evidence remains scarce. This paper aims to fill this gap. Specifically, we contend that a representation can be considered superior if the features derived from it are more intelligible, i.e., easier for humans to make sense of while being demonstrably used by the model in its decision-making process. By means of computational and psychophysics experiments, we set out to find which of the local vs. distributed representations better meets these two conditions. In sum, the main contributions of this paper are as follows: • We conduct three large-scale psychophysics experiments for a total of 15,720 responses from a pool of 560 participants, to evaluate the visual ambiguity of the features derived from local vs distributed representations (see Fig 1). In the process, we identify a potential semantic bias in the experimental protocols commonly used in the field (Borowski et al., 2021; Zimmermann et al., 2023), and provide an approach to at least partially mitigate it. • Our findings provide strong evidence that (i) features derived from distributed representations are significantly easier for humans to interpret than features derived from local representations, and (ii) this effect is even more pronounced in the deepest layers of a neural network. • Additionally, we observe that (iii) models rely significantly more on features derived from distributed representations compared to those derived from local representations. Overall, our results suggest that distributed representations provide a substantially better foundation for the interpretability of models than local representations."
https://arxiv.org/html/2411.03982v1,ReEdit: Multimodal Exemplar-Based Image Editing with Diffusion Models,"Modern Text-to-Image (T2I) Diffusion models have revolutionized image editing by enabling the generation of high-quality photorealistic images. While the de facto method for performing edits with T2I models is through text instructions, this approach non-trivial due to the complex many-to-many mapping between natural language and images. In this work, we address exemplar-based image editing – the task of transferring an edit from an exemplar pair to a content image(s). We propose ReEdit, a modular and efficient end-to-end framework that captures edits in both text and image modalities while ensuring the fidelity of the edited image. We validate the effectiveness of ReEdit through extensive comparisons with state-of-the-art baselines and sensitivity analyses of key design choices. Our results demonstrate that ReEdit consistently outperforms contemporary approaches both qualitatively and quantitatively. Additionally, ReEdit boasts high practical applicability, as it does not require any task-specific optimization and is four times faster than the next best baseline.","Image editing [21, 1, 30, 11, 35] is a rapidly growing research area, with a wide range of practical applicability in domains like multimedia, cinema, advertising, etc. Recent advancements in text-based diffusion models [17, 43, 35, 37] have accelerated the progress in the field of image editing, yet diffusion models remain limited in their practical viability to real world applications. For example, if a practitioner is making detailed edits—such as transforming a scene from daytime to nighttime—and wants to apply the same adjustments to multiple images, they would face a considerable challenge, since crafting each image individually can be time consuming. In such cases, simple textual prompts might not be sufficient to achieve the desired consistency and efficiency. Notably, an ideal editing application should be fast, have the ability to understand the exact user intent and produce high fidelity outputs. Most existing work in this domain leverages textual descriptions to perform image editing [4, 49, 14, 22, 33, 19, 23, 36], however, text is inherently limited in its ability to adequately describe edits. These challenges motivate us to focus on a relatively unexplored field of exemplar based image editing. This formulation is motivated by ‘visual prompting’ proposed in [3]. Existing works in this area typically optimize a text embedding during inference to capture each edit [34, 20] which is time taking. Other methods like [15, 48] utilize sophisticated models trained specifically for the task of editing like InstructPix2Pix [4] (IP2P), which requires a large labelled training dataset. These datasets can be extremely difficult to obtain due to the nature of the problem. Further, recent approaches like VISII [34] can only capture a limited type of edits (performs well only for global style transfer type edits) as a result of the way its text embedding is optimized. Unlike existing approaches, we propose an efficient end-to-end optimization-free framework for exemplar based image editing - ReEdit. The proposed framework consists of three primarily components - first we capture the edit from the exemplar in the image embedding space using pretrained adapter modules [41], second, we capture the edit in natural language by incorporating multimodal VLMs like [28] capable of detailed reasoning, and last we ensure that the content and structure of the test image is maintained and only the relevant parts are edited by conditioning the image generator on the features and self attention maps [49] of the test image. Overall, none of the components of our approach are explicitly trained for image editing, do not require inference time optimization, and easily generalize to a wider variety of edit types while being independent of the base diffusion model and prove to be extremely efficient. To summarize, the contributions of our work are listed below: 1. We propose an inference-time approach for exemplar-based image editing that does not require finetuning or optimizing any part of the pipeline. Compared to the most optimal baseline, the runtime of our method is ∼similar-to\sim∼4x faster. 2. We collate a dataset of 1500 exemplar pairs (x,xedit𝑥subscript𝑥editx,x_{\text{edit}}italic_x , italic_x start_POSTSUBSCRIPT edit end_POSTSUBSCRIPT), and corresponding test images with ground truth (y,yedit)𝑦subscript𝑦edit(y,y_{\text{edit}})( italic_y , italic_y start_POSTSUBSCRIPT edit end_POSTSUBSCRIPT ), covering a wide range of edits. Due to a lack of standardized datasets, our dataset paves towards a standardized evaluation of exemplar-based image editing approaches. 3. Our rigorous qualitative and quantitative analysis shows that our method performs well on a variety of edits while preserving the structure of the original image. These observations are corroborated by significant improvements in quantitative scores over baselines. We plan on open sourcing the dataset and code."
https://arxiv.org/html/2411.03976v1,HRDecoder: High-Resolution Decoder Network for Fundus Image Lesion Segmentation,"High resolution is crucial for precise segmentation in fundus images, yet handling high-resolution inputs incurs considerable GPU memory costs, with diminishing performance gains as overhead increases. To address this issue while tackling the challenge of segmenting tiny objects, recent studies have explored local-global fusion methods. These methods preserve fine details using local regions and capture long-range context information from downscaled global images. However, the necessity of multiple forward passes inevitably incurs significant computational overhead, adversely affecting inference speed. In this paper, we propose HRDecoder, a simple High-Resolution Decoder network for fundus lesion segmentation. It integrates a high-resolution representation learning module to capture fine-grained local features and a high-resolution fusion module to fuse multi-scale predictions. Our method effectively improves the overall segmentation accuracy of fundus lesions while consuming reasonable memory and computational overhead, and maintaining satisfying inference speed. Experimental results on the IDRID and DDR datasets demonstrate the effectiveness of our method. Code is available at https://github.com/CVIU-CSU/HRDecoder.","(a) Performance-cost Trade-off on DDR [10] (b) An example from IDRiD [19] (c) Improvement on IDRiD [19] Figure 1: (a) Our method reaches SOTA performance and is memory efficient, the numbers represent input resolutions. (b) Example of tiny lesions in fundus images. (c) Performance gains on each category. Fundus image lesion segmentation poses a significant challenge in medical image analysis, which is crucial for the early detection and monitoring of various retinal diseases. The pixel-wise classification of tiny lesions, as shown in Fig. 1b, demands considerably higher resolution compared to other segmentation tasks. Simply increasing the input resolution does boost segmentation performance of tiny lesions, while this is accompanied by rising memory usage, increasing computational overhead, and slower inference speed. These issues significantly hinder practical application and further performance improvement of models. Numerous efforts have been made to address the segmentation of tiny objects efficiently. Some studies explore FPN-like [12] or UNet-like [20] multi-scale features fusion methods to compensate for fine-grained details and enhance performance on small objects [29, 15, 22, 6, 5, 7, 27]. However, these multi-scale feature fusion methods are susceptible to distortion from low-resolution features and result in suboptimal performance. There are also other methods that emphasize local features [13, 14, 25, 2, 8]. M2MRF [13] enhances performance on tiny lesions by designing specialized local feature fusion modules, while it excessively emphasizes local features and exhibits slow convergence on larger lesions. HRDA [8] and GLNet [2] design a dual-branch network composed of a High-Resolution(HR) branch to learn texture details from randomly cropped images and a Low-Resolution(LR) branch to extract contextual information through scaling operations, consequently achieving impressive results. However, they need multiple forward passes, which significantly increase computational overhead and sharply decrease inference speed, thus restricting further applications. Instead, we propose HRDecoder, a simple framework combining the idea of local-global high-resolution crops and multi-scale fusion at the decoder stage, to efficiently and effectively segment tiny lesions. HRDecoder consists of an HR representation learning module to mine detailed features and an HR fusion module for integrating multi-scale predictions, significantly enhancing performance on small lesions (see Fig. 1c). By simply using scaling and cropping and a light-weight decoder, our method does not introduce extra parameters, and can significantly alleviate issues of high memory usage, computational overhead, and slow inference speed, as shown in Fig. 1a. We summarize the contributions as follows: (1) We propose HRDecoder, a simple framework to address the challenge of segmenting tiny lesions in fundus images. (2) Our method not only improves segmentation performance but also effectively mitigates high memory consumption, high computational overhead and slow inference speed. (3) Our framework dose not introduce any extra trainable parameters and can be easily applied to existing methods. Figure 2: Overview of HRDecoder. (a) Training and testing pipeline. (b) HR representation learning module aims to learn local detailed features from simulated HR feature maps. (c) HR fusion module aggregates multi-scale predictions."
https://arxiv.org/html/2411.03960v1,Face Reconstruction from Face Embeddingsusing Adapter to a Face Foundation Model,"Face recognition systems extract embedding vectors from face images and use these embeddings to verify or identify individuals. Face reconstruction attack (also known as template inversion) refers to reconstructing face images from face embeddings and using the reconstructed face image to enter a face recognition system. In this paper, we propose to use a face foundation model to reconstruct face images from the embeddings of a blackbox face recognition model. The foundation model is trained with 42M images to generate face images from the facial embeddings of a fixed face recognition model. We propose to use an adapter to translate target embeddings into the embedding space of the foundation model. The generated images are evaluated on different face recognition models and different datasets, demonstrating the effectiveness of our method to translate embeddings of different face recognition models. We also evaluate the transferability of reconstructed face images when attacking different face recognition models. Our experimental results show that our reconstructed face images outperform previous reconstruction attacks against face recognition models. Project Page","Face recognition systems tend toward ubiquity and are extensively used in various applications that require automatic authentication, ranging from unlocking smartphones to border control. In these systems, a deep neural network is used to extract an embedding vector (also known as a template), which is stored in the database of the system during enrollment. Later during the recognition stage, the extracted embeddings are compared with ones stored in the database of the system. Hence, the extracted embedding vectors play a pivotal role in the automatic recognition of subjects. With the growing application of face recognition systems, the security of these systems has become a critical topic and attracted considerable attention from the research community [14, 2, 17, 15, 27, 6, 11]. Among different types of attacks against face recognition systems, face reconstruction attacks (also known as template inversion) threaten the security and privacy of subjects. In a face reconstruction attack, the adversary gains access to face embeddings stored in the database of a face recognition system and tries to reconstruct the face image of enrolled users. The reconstructed face images can provide privacy-sensitive (such as age, gender, ethnicity, etc) information about the user, and in addition, can be used to enter the face recognition system. Existing face reconstruction attacks in the literature can be categorized into two groups: learning-based methods and optimization methods. In learning-based methods, such as [5, 26, 38], a network is trained, on a dataset of pairs of embeddings and images, to reconstruct face images from the embeddings. In optimization-based methods, such as [44, 10], usually a face generator model (such as StyleGAN) is used and through an iterative optimization a latent input is found to generate the reconstructed face images. In each case, the attack requires large computation time for the training (in learning-based methods) or the inference stage (in optimization-based methods). Recently, generative models have also absorbed much attention, and there have been significant advancements in the generation capabilities of these models. In a similar vein, different researchers tried to develop large foundation models, which are trained on massive data and can be later used for different applications. Recently, the first face foundation model, called Arc2Face [33], was proposed in which the CLIP [34] and Stable Diffusion [35] models were fine-tuned on 42 million face images from WebFace260M dataset [49]. The resulting model is able to generate different face images from embeddings of a fixed face recognition model while preserving identity information. Therefore, the Arc2Face model can be used as a foundation model in many problems which require an identity-conditioned generator. For example, the authors showed that the proposed method achieves state-of-the-art performance in training face recognition using synthetic data. However, an important question is that “given the considerable capabilities of foundation models, can face foundation models threaten face recognition systems?” In this paper, we focus on face reconstruction attacks and propose a new method to reconstruct face images from different face recognition models using a foundation model. We propose an adapter module that can map the face embeddings to the input space of the foundation model. Then, the foundation model can be used to generate face images from the given embeddings. The adapter module enables the use of a foundation model for embeddings of different face recognition models and prevents the need for training a foundation model for a new face recognition model. Therefore, an adversary can easily leverage the capabilities in the foundation model to perform a successful reconstruction attack in a blackbox scenario. We perform extensive experiments using several face recognition models on different face recognition datasets. Furthermore, we explore the transferability of reconstructed face images to enter a different face recognition system. The experimental results demonstrate the effectiveness of our reconstructed face images in attacking face recognition systems and superiority compared to previous face reconstruction attacks. Fig. 1 illustrates sample reconstructed face images by our attack. In summary, the contributions of this paper are as follows: • We propose a simple yet effective adapter module to map the face embeddings to the input of the face foundation model. The adapter module enables the use of a foundation model for embeddings of different face recognition models and prevents the need for training a foundation model for a new face recognition model. • We propose a new face reconstruction attack against face recognition systems using a foundation model. While the foundation model can generate face images from embeddings of a fixed model, we use this model to reconstruct face images from embeddings of any blackbox model. To our knowledge, this is the first attack against face recognition systems based on foundation models. • We demonstrate the effectiveness of our reconstructed face images with extensive experiments for different face recognition models on different datasets. We also evaluate the transferability of reconstructed face images for different face recognition models. In the remainder of the paper, we first review related work in the literature in Sec. 2. Next, we propose our face reconstruction method based on a foundation model in Sec. 3. In Sec. 4, we present our experiments and compare our method with previous attacks in the literature. In Sec. 5, we further discuss different aspects of our attack. Finally, the paper is concluded in Sec. 6."
https://arxiv.org/html/2411.03959v1,Energy Score-based Pseudo-Label Filtering and Adaptive Loss for Imbalanced Semi-supervised SAR target recognition,"Automatic target recognition (ATR) is an important use case for synthetic aperture radar (SAR) image interpretation. Recent years have seen significant advancements in SAR ATR technology based on semi-supervised learning. However, existing semi-supervised SAR ATR algorithms show low recognition accuracy in the case of class imbalance. This work offers a non-balanced semi-supervised SAR target recognition approach using dynamic energy scores and adaptive loss. First, an energy score-based method is developed to dynamically select unlabeled samples near to the training distribution as pseudo-labels during training, assuring pseudo-label reliability in long-tailed distribution circumstances. Secondly, loss functions suitable for class imbalances are proposed, including adaptive margin perception loss and adaptive hard triplet loss, the former offsets inter-class confusion of classifiers, alleviating the imbalance issue inherent in pseudo-label generation. The latter effectively tackles the model’s preference for the majority class by focusing on complex difficult samples during training. Experimental results on extremely imbalanced SAR datasets demonstrate that the proposed method performs well under the dual constraints of scarce labels and data imbalance, effectively overcoming the model bias caused by data imbalance and achieving high-precision target recognition.","1 INTRODUCTION Synthetic Aperture Radar (SAR) is a high-resolution imaging radar that is frequently employed in homeland security since it can operate day and night[1]. Automatic target recognition (ATR) technology can interpret complicated SAR target samples into valuable intelligence information[2, 3], making it useful in a variety of military applications such as precise guidance and battlefield monitoring. In recent years, the rapid growth of deep learning technology has brought about tremendous improvements in SAR ATR tasks[4]. Zhou et al. introduced morphological operations to improve the quality of SAR target data. They designed a large-margin softmax batch-normalization CNN network structure to enhance the separability of samples after clutter removal[5]. Zhang et al. fused the semantic features extracted by convolutional neural networks with traditional scattering center features, achieving a recognition accuracy of 99.59% on the MSTAR dataset[6]. Li et al. proposed a multiscale convolutional network and fully utilized the scattering centers of SAR targets to learn robust target features[7]. Deep learning algorithms offer higher prediction accuracy but larger model parameter sizes than conventional machine learning algorithms, and they require a significant amount of labeled data to support model training[8, 9]. However, manually labeling SAR images is costly, time-consuming, and error-prone. In recent years, various semi-supervised learning (SSL) systems have arisen, including Mean-Teacher[10], MixMatch[11], ReMixMatch[12], and FixMatch[13]. These models have gained great success in the field of optical image identification, as well as offered strategies to address the label scarcity problem in SAR-ATR. Wang et al. introduced Mixup[14] technique to combine labeled and unlabeled SAR data, which effectively utilizes unlabeled data and enhances SAR ATR performance[15]. Yue et al. developed a semi-supervised SAR ATR framework that leverages labeled data for active learning and employs unlabeled data to impose constraints, yielding a notable improvement in recognition performance[16]. Zhang et al. proposed a pseudo-label selection mechanism based on epoch and uncertainty sensitivity, which resulted in remarkable performance gains in scenarios with scarce labels[17]. Zhang et al. designed two azimuth-aware discriminative representation losses that suppress intra-class variations among samples with large azimuth-angle differences, while simultaneously enlarging inter-class differences of samples with the same azimuth angle[18]. In general, the success of deep learning approaches is implicitly determined by the data scale’s completeness. However, due to the unpredictable nature of wartime conditions and the high value of some prized military targets, certain unusual targets have characteristics such as excellent camouflage and mobility[19]. This complicates the acquisition of their image data. Consequently, the lack of samples in some categories causes many SAR image datasets to exhibit an unbalanced data distribution[20], as shown in Figure 1, where some categories have abundant samples, referred to as the head categories, and data for other categories is scarce, termed as the tail categories. This objective contradiction creates a severe bottleneck for the SAR ATR technology. Under these circumstances, models are prone to severe overfitting difficulties, and their generalization capabilities deteriorate significantly[21]. Figure 1: Long tail distribution diagram. Existing strategies for dealing with imbalanced datasets in classification problems are broadly classified into two categories: data-level approaches and algorithm-level approaches. Data-level techniques preprocess the dataset without changing the network structure and are divided into five categories: oversampling[22], under-sampling[23], mixed sampling[24], feature selection[25], and deep generative models[26]. Algorithm-level techniques do not preprocess the imbalanced dataset. Instead, they optimize the base classifier or adjust the network structure to improve the model’s recognition rate. One common algorithm-level method assigns different costs to samples of different classes[27, 28, 29]. For example, Li et al. proposed a cost-sensitive multi-decision tree method that alleviates the imbalance problem by imposing different penalties on different classes[28]. Focal Loss reduces the weighting factor of accurately classified samples in the loss function, thereby enhancing the model’s focus on the minority class samples[29]. Ensemble learning has also been introduced to address data imbalance issues. Bhowmick et al. proposed a hybrid ensemble model based on the Boosting algorithm, which oversamples minority classes according to the recall rate and uses a voting algorithm for final classification[30]. Transfer learning-based methods address class imbalance by modeling the majority and minority class samples separately and transferring the information learned from the majority class to the minority class[31]. Metric learning aims to autonomously learn representative distance metric functions based on specific tasks to explore the similarities or differences between different class targets[32]. Current semi-supervised target recognition algorithms perform badly when data classes are imbalanced. This is because SSL implementations frequently use a high threshold for producing pseudo-labels to ensure correctness, which causes two issues. First, using high confidence exacerbates the imbalance problem by significantly decreasing the recall rate of pseudo-labels for data from minority classes[33]. Second, samples far from the training data distribution might still have high prediction scores. This can lead to the incorrect classification of tail-class samples as high-confidence head-class samples[34], lowering the accuracy of recognition. Apart from the possible class imbalance found in SAR datasets, there is a high probability that the pseudo-labels produced will display a bias in favor of head classes[35], particularly in the first phases of training the model. To address the aforementioned difficulties, this work investigates a non-balanced semi-supervised SAR ATR approach using dynamic energy scores and adaptive losses. First, we reject commonly used confidence-based pseudo-label generation methods and advocate the use of an Energy Score-based in-distribution Pseudo-label Selection (ESIDPS) mechanism that ensures the reliability of pseudo-labels in long-tailed data. Second, adaptive loss functions appropriate for imbalanced data are developed. On the one hand, to address the natural imbalance problem of pseudo-labels, we employ the Adaptive Margin Loss (AML) function rather than the generally used cross-entropy loss function in unsupervised losses. In addition, to address the model’s preference for majority class samples, we propose an Adaptive Hard Triplet Loss (AHTL) function, which allows the model to focus on complicated hard samples and learn more discriminative feature representations. Our main contributions are four-fold: 1) We introduce the concept of ”within-distribution” and integrate it with the energy scores from the out-of-distribution detection task to enable quantitative selection. Throughout the training iteration process, the data distribution of pseudo-labels is constantly updated and expanded, boosting the reliability of pseudo-labels in scenarios of data imbalance. 2) We used an adaptive margin loss function to replace the traditional cross-entropy term in the unsupervised loss function. This was done to strengthen the model’s awareness of the marginal differences between head and tail classes, thereby avoiding excessive bias towards majority classes and reducing the issue of biased pseudo-labels. 3) A self-adaptive hard triplet loss function was designed specifically for SSL. This loss function not only directs the model to focus more on complex and challenging samples during training, enhancing the network’s feature learning capability and overcoming model bias but also adapts the weights of hard and easy samples dynamically. This adaptation prevents the model from being influenced by noisy samples and outliers, thereby improving the stability and robustness of the model. 4) Extensive experiments were conducted on two sets of imbalanced SAR target datasets, MSTAR and FUSAR-ship. The experimental results demonstrate that the proposed method effectively enhances the recognition rate of SAR targets in class-imbalanced scenarios. The remaining sections of this paper are organized as follows. In Section II, the works related to this methodology are introduced. The proposed method is described in Section III. Section IV presents the experimental findings. Discussions of our method are included in Section V. In Section VI, this paper is finally concluded."
https://arxiv.org/html/2411.03926v1,Act in Collusion: A Persistent Distributed Multi-Target Backdoor in Federated Learning,"Federated learning, a novel paradigm designed to protect data privacy, is vulnerable to backdoor attacks due to its distributed nature. Current research often designs attacks based on a single attacker with a single backdoor, overlooking more realistic and complex threats in federated learning. We propose a more practical threat model for federated learning: the distributed multi-target backdoor. In this model, multiple attackers control different clients, embedding various triggers and targeting different classes, collaboratively implanting backdoors into the global model via central aggregation. Empirical validation shows that existing methods struggle to maintain the effectiveness of multiple backdoors in the global model. Our key insight is that similar backdoor triggers cause parameter conflicts and injecting new backdoors disrupts gradient directions, significantly weakening some backdoors performance. To solve this, we propose a Distributed Multi-Target Backdoor Attack (DMBA), ensuring efficiency and persistence of backdoors from different malicious clients. To avoid parameter conflicts, we design a multi-channel dispersed frequency trigger strategy to maximize trigger differences. To mitigate gradient interference, we introduce backdoor replay in local training to neutralize conflicting gradients. Extensive validation shows that 30 rounds after the attack, Attack Success Rates of three different backdoors from various clients remain above 93%. The code will be made publicly available after the review period.","Federated Learning (FL) [19, 31, 56] enables decentralized devices to collaboratively train a global model without data sharing, enhancing privacy and reducing transmission costs. This approach uses a central server to aggregate updates from client nodes, each with its own dataset, and is promising in sectors like finance, healthcare, and autonomous driving [28, 41, 6]. However, FL’s distributed nature introduces significant security challenges [53, 60, 45, 4], notably backdoor attacks [13, 38, 36] that stealthily misclassify specific triggers without impacting the main task performance. Secure aggregation [42] facilitates backdoor attacks by limiting server access to client updates, emphasizing the need for focused research in federated learning. Existing backdoor attacks are classified into single-target [53, 13, 38, 36, 7, 2, 29] and multi-targets [23, 21, 54, 3, 55, 16] based on the number of target labels. Single-target attacks have one misclassification task, misclassifying inputs that meet specific triggers into a predetermined class during inference. Multi-target attacks involve multiple labels, activating different backdoors for various triggers to perform targeted misclassifications. However, these models don’t fit the complex attack scenarios in FL, where multiple attackers may control different clients, each with unique triggers and target labels, and launch attacks at different times. Single-target models assume a single backdoor type, while multi-target models rely on centralized computing, both unsuitable for FL’s decentralized nature. Consequently, there is a gap in research regarding complex FL attack scenarios and the effectiveness of backdoor attacks in such settings. Figure 1: ASRs for multiple backdoors in complex attack scenarios. Additionally, we empirically validated the limitations of existing backdoor methods, which struggle in complex scenarios to maintain high Attack Success Rates (ASRs) for multiple backdoors, as demonstrated in Fig. 1 (details in Appendix A). The underperformance stems from two main issues: high similarity among backdoor triggers causing severe model parameter conflicts and the injection of new backdoor samples producing conflicting gradients with previous samples [2], which diminish the impact of past malicious updates. To address complex attack scenarios in federated learning, we introduce a new backdoor attack, DMBA, noted for its efficiency and durability. We begin by analyzing the characteristics of attack scenarios in FL and propose a new threat model. Next, we develop a multi-target trigger strategy to activate backdoors efficiently and covertly, avoiding parameter conflicts. Lastly, we introduce a backdoor replay component for local training to mitigate gradient interference during model aggregation. Extensive validation confirms DMBA maintains high ASR, with averages over 93% across tasks and above 83% after 30 rounds. Our contributions are summarized as follows: • We introduce DMBA, a distributed multi-target backdoor attack in federated learning, characterized by its practical threat model and new objective function tailored for complex scenarios. DMBA outperforms three baseline attacks in terms of persistence and stealth across various datasets and models. We also evaluated the impact of the backdoor replay component and several important factors on DMBA performance through ablation studies and a series of hyperparametric sensitivity analyses. Finally, we evaluated DMBA under two state-of-the-art defenses, demonstrating its strong robustness. • We develop a trigger strategy based on multi-channel dispersed frequency block perturbations. This strategy addresses the performance decline caused by high similarity among backdoor triggers, which leads to parameter conflicts in the global model. By transforming pixel matrices of different channels into the frequency domain and perturbing different frequency blocks, this approach enhances trigger distinguishability while maintaining stealth. • We propose a backdoor replay component, inspired by the idea of experience replay, to guide the backdoor training process in malicious clients. This component allows learning from a small amount of previous backdoor samples alongside new ones, neutralizing conflicting gradients and extending the poisoning duration, thereby mitigating catastrophic forgetting in backdoors."
https://arxiv.org/html/2411.03924v1,Self-supervised Representation Learning for Cell Event Recognition through Time Arrow Prediction,"The spatio-temporal nature of live-cell microscopy data poses challenges in the analysis of cell states which is fundamental in bioimaging. Deep-learning based segmentation or tracking methods rely on large amount of high quality annotations to work effectively. In this work, we explore an alternative solution: using feature maps obtained from self-supervised representation learning (SSRL) on time arrow prediction (TAP) for the downstream supervised task of cell event recognition. We demonstrate through extensive experiments and analysis that this approach can achieve better performance with limited annotation compared to models trained from end to end using fully supervised approach. Our analysis also provides insight into applications of the SSRL using TAP in live-cell microscopy.","In live-cell microscopy where the data is often spatio-temporal by nature, one interesting problem is recognising cell divisions and deaths. Most existing methods using machine learning focus on cell segmentation and tracking (Greenwald et al. (2022), Stringer et al. (2020), Weigert et al. (2020), Schmidt et al. (2018)). This often requires large amount of annotated data which can be costly to obtain. A new paradigm which appeared recently is to leverage self-supervised representation learning (SSRL) (Ericsson et al., 2022) which is capable of building feature maps from pretext tasks trained on unlabelled data (e.g. frames from movies of live cells) and apply the obtained dense features in downstream tasks such as cell event recognition or cell tracking. By incorporating SSRL, we are able to boost the performance of the model on downstream tasks using the same amount of annotated data compared to models trained from end to end using fully supervised learning. The benefits of SSRL become more evident with spatio-temporal data compared with static cell images, where human annotations become even more time consuming due to the extra effort needed to annotate each frame over time. In this paper, we built on the work from Gallusser et al. (2023) and apply SSRL to the problem of cell event recognition where for a given image pair following the time direction, we predict whether there are cell divisions and deaths happening. Compared to Gallusser et al. (2023), our work provides a comprehensive evaluation of cell event recognition using features maps learned from SSRL and offers insight into the application of SSRL for cell event recognition. Our contributions 1 We demonstrate through extensive experiments that using time arrow prediction (TAP) (Pickup et al., 2014) is a very effective way to obtain dense features that is extremely useful for the downstream task of cell event recognition. When we apply the feature map trained from TAP with fine-tuning on our annotated dataset, we achieve better performance compared to a fully supervised approach from end to end. 2 We provide analysis on the mistaken predictions, carry out comparisons on the performance from various labelling criteria and calibrate the predictions using temperature scaling (Guo et al., 2017). We hope these analyses can provide insights to the application of SSRL using TAP in live-cell microscopy. In Section 2, we give a formal description of our learning problem and methods. In Section 3, we explain the details of our data pre-processing method, followed by results on evaluation of TAP features, analysis of mistaken predictions, comparisons on labelling criteria and results on model calibration. We review related work in Section 4 and summarise limitations and future work in Section 5."
https://arxiv.org/html/2411.03862v1,ROBIN:Robust andInvisible Watermarks for Diffusion Models with Adversarial Optimization,"Watermarking generative content serves as a vital tool for authentication, ownership protection, and mitigation of potential misuse. Existing watermarking methods face the challenge of balancing robustness and concealment. They empirically inject a watermark that is both invisible and robust and passively achieve concealment by limiting the strength of the watermark, thus reducing the robustness. In this paper, we propose to explicitly introduce a watermark hiding process to actively achieve concealment, thus allowing the embedding of stronger watermarks. To be specific, we implant a robust watermark in an intermediate diffusion state and then guide the model to hide the watermark in the final generated image. We employ an adversarial optimization algorithm to produce the optimal hiding prompt guiding signal for each watermark. The prompt embedding is optimized to minimize artifacts in the generated image, while the watermark is optimized to achieve maximum strength. The watermark can be verified by reversing the generation process. Experiments on various diffusion models demonstrate the watermark remains verifiable even under significant image tampering and shows superior invisibility compared to other state-of-the-art robust watermarking methods.","Diffusion models (DMs) are revolutionizing content creation and generating stunningly realistic imagery across diverse domains [17, 33, 60]. The advent of text-to-image diffusion models [31, 30, 58], coupled with personalized generation techniques [53, 7, 32, 15, 41, 59], enables the creation of highly specific content by virtually anyone. However, it has raised concerns about authenticity and ownership, including the risk of plagiarism [34, 22] and the potential misuse of images of public figures [39, 5]. Consequently, governments and businesses are increasingly advocating for robust mechanisms to verify the origins of generative content [19, 45]. Watermarking offers a proactive approach to authenticate the source of generated content. This technique embeds imperceptible secret messages within the generated content. These messages serve as unique identifiers, confirming the image’s origin while remaining invisible to the human eye. They also need to be robust enough to withstand potential distortions encountered during online sharing. Existing watermarking techniques face a significant challenge in striking a balance between concealment and robustness. Traditional post-processing methods [46, 9] employ an empirical approach to identify an invisible and robust watermark and embed it within the generated image. They passively achieve concealment by limiting the watermark strength, consequently compromising robustness. Conversely, stronger watermarks, while enhancing robustness, can introduce visible artifacts into the generated image. Recent advancements in in-processing watermarking for diffusion models expect the generative model to learn this balance and directly produce watermarked content. However, these methods often require expensive model retraining [55, 48, 13] or can lead to unintended semantic alterations within the generated images [44]. Our ROBIN scheme introduces an explicit watermark hiding process to actively achieve concealment. This approach reduces the invisibility limitation of the watermark itself and thus enables the embedding of more robust watermarks. Specifically, we implant a robust watermark within an intermediate diffusion state, and then directionally guide the model to gradually conceal the implanted watermark, thus achieving invisibility in the final generated image. In this way, robust watermarks can be secretly implanted in the generated content without model retraining. We focus on the text-to-image diffusion models, which support an additional prompt signal to guide the generation process. We employ an adversarial optimization algorithm to design an optimal prompt guidance signal specifically tailored for each watermark. The prompt embedding is optimized to minimize artifacts in the generated image, and the watermark is optimized to achieve maximum strength. The optimized watermark and prompt signal are universally applicable to all images. During the generation process, the watermark is implanted within an intermediate state following the semantic formation stage. Subsequently, the optimized prompt guidance signal is introduced throughout the remaining diffusion steps. After image generation, following previous works [44, 49], we reverse the diffusion process to the watermark embedding point to verify the existence of the watermark. This innovative approach offers a promising way to overcome the trade-off between watermark strength and stealth by explicitly introducing an additional watermark hiding process. In summary, our key contributions are as follows: • We propose a novel watermarking method for diffusion models that embed a robust watermark and subsequently employ an active hiding process to achieve imperceptibility. • We develop an adversarial optimization algorithm to generate a prompt signal for watermark hiding and a strong watermark that can be hidden and strategically select the watermarking point within the diffusion trajectory. • Evaluations on both latent and image diffusion models demonstrate that our scheme exhibits superior robustness against various image manipulations while preserving semantic content."
https://arxiv.org/html/2411.03861v1,FedRISE: Rating Induced Sign Election of Gradients forByzantine Tolerant Federated Aggregation,"One of the most common defense strategies against model poisoning in federated learning is to employ a robust aggregator mechanism that makes the training more resilient. Many of the existing Byzantine robust aggregators provide theoretical guarantees and are empirically effective against certain categories of attacks. However, we observe that certain high-strength attacks can subvert the aggregator and collapse the training. In addition, most aggregators require identifying tolerant settings to converge. Impact of attacks becomes more pronounced when the number of Byzantines is near-majority, and becomes harder to evade if the attacker is omniscient with access to data, honest updates and aggregation methods. Motivated by these observations, we develop a robust aggregator called FedRISE for cross-silo FL that is consistent and less susceptible to poisoning updates by an omniscient attacker. The proposed method explicitly determines the optimal direction of each gradient through a sign-voting strategy that uses variance-reduced sparse gradients. We argue that vote weighting based on the cosine similarity of raw gradients is misleading, and we introduce a sign-based gradient valuation function that ignores the gradient magnitude. We compare our method against 8 robust aggregators under 6 poisoning attacks on 3 datasets and architectures. Our results show that existing robust aggregators collapse for at least some attacks under severe settings, while FedRISE demonstrates better robustness because of a stringent gradient inclusion formulation. Code is available at https://github.com/anonymous/","Federated learning (FL) [18] has emerged as a vital tool for leveraging decentralized data, especially in situations with strict data storage and sharing regulations, such as medical institutions, financial organizations, and other entities managing data. The main goal of regulations like GDPR is to protect people’s privacy and give them control over how their personal data is stored and shared, helping to guard against security risks for both individuals and countries. Because of this, the data exists in isolated silos inaccessible to the external world, making collaboration challenging. Despite these inherent challenges, there are potential benefits for organizations in improving the quality of models while safeguarding data privacy and security. In healthcare, sensitive patient information is often stored separately by various institutions distributed across different geographical locations and subject to varying legislative frameworks. Yet collaboration is essential for improving accuracy across diverse sub-populations and a broader range of diseases and conditions. In aerial imagery analytics, federated learning allows collaboration among operators of remote sensing spacecrafts. This enables aggregating insights in geographically dispersed geospatial data while adhering to jurisdictional data governance protocols. This is crucial to improve precision across heterogeneous terrains and operational contexts. FL solves the challenges involved in collaborative learning by sharing statistical updates/models learned on data instead of sharing actual data with other clients or servers, thus mitigating systemic privacy risks by enabling a strong layer of abstraction[10]. When members of consortium111consortium refers to a set of organizations/entities participating in a cross-silo federated learning procedure. collaboratively develop ML systems, they become a potential target for malicious actors. These actors may include third parties seeking to disrupt the system through sophisticated man-in-the-middle style attacks, intercepting and altering transmissions. Otherwise, competing entities within the consortium may also seek to undermine or sabotage the alliance to gain a competitive advantage in the market. We refer to these agents/clients that defect intentionally or unintentionally as Byzantines. Model poisoning is an extensively studied threat model in FL, where a Byzantine sends in corrupt or tailored updates to derail the entire training process. Certain entities wishing to sabotage the collaboration for competitive gains might initiate certain model performance attacks [9]. Robust aggregators (RAggr) are a fundamental defense strategy to make training resilient to Byzantine attacks. Such preemptive defense measures are necessary to ensure secure and mutually beneficial collaboration. Issues with Existing Approaches. Though many RAggrs have been proposed for FL, fundamental issues exist when they are adapted for practical use. (1) Middle-seeking [6, 35, 19] and outlier-suppression [1] methods by their design consider only a small subset of clients while suppressing/discarding the rest, using central tendency or mutual deviation of clients. These approaches do not leverage the richness of heterogeneity, a primary motive for collaboration, and become highly sub-optimal in the presence of heterogeneity even without Byzantines. In cross-silo FL, having few clients with diverse distribution, if the server consistently excludes certain clients due to heterogeneity, the gradients will not represent the entire distribution and will diverge far away from true gradients. Furthermore, once the data distribution is known, attacks can be crafted to hide among the variance of honest clients [27], especially under the assumption of an omniscient attacker. This is mainly because of the underlying assumption that larger deviations are due to maliciousness when, in fact, they could be due to large heterogeneity. (2) As an alternative to excluding clients, variance-reduction approaches like trimmed mean [35], centered-clipping [12], and bucketing [11, 23, 2] try to reduce the variance bounds of gradients, thereby curtailing the extreme malicious updates. They include gradients from clients in the tail distributions, thus enabling access to richer gradients. Clipping only considers radius (L2-norm), making it oblivious to the angular/sign variance of update vectors (see Fig. 1); thus, they need to be used in conjunction with other strategies. Bucketing helps in controlling the variance before clipping [11] and provides isolation among clients with angular variations [23]. Although the intensity of malicious gradients is reduced, the direction of the update is still perturbed, which can easily overpower the direction when aggregated with weak honest updates, leading to poisoning. Especially in the cross-silo FL, with few number of clients; each bucket may end up with a malicious vector corrupting the outputs of aggregation. (3) Finally, these methods often rely on certain hyperparameters (e.g., making initial judgments on the number of Byzantines). To determine the hyperparameters, some methods require knowledge of variance among updates or to be empirically searched for a given data distribution and setup. Practically, it is infeasible to undertake multiple trials with the established consortium. Moreover, the Byzantines can very well skew these estimates from the beginning. Figure 1: Similarity Scores between Benign & Poisoned Gradients: To identify Byzantine, a reliable Gradient Valuation Function is needed that can distinguish between honest and attack gradients. But the usual measures like L2-norm and cosine similarity yield high similarity between those two, as shown for Fang and ALIE attacks. *Signum-similarity (i.e. Cosine on signum of gradients) gives better separability compared to the other metrics. Displayed gradients are values at evenly spaced coordinates taken from ResNet18 trained on FL split CIFAR10. In our work, we address training-time model poisoning attacks by an omniscient attacker that significantly degrades performance under a cross-silo setting, resulting in a denial of service. We specifically focus on developing an aggregation protocol [31, 36, 25] that is robust to high-strength attacks, even with close to the majority of Byzantines in the consortium. The only assumption we impose is that the number of Byzantines will be less than 50%percent5050\%50 %, which is a common assumption across most of the RAggrs. Our main contributions are: • We propose FedRISE robust aggregation function to determine signs of individual gradients using variance-reduced sparse gradients. For sign election, we use weighted voting based on gradient quality (rating). The rating is calculated for each aggregation round independent of the previous round. • We evaluate existing popular RAggrs against established state-of-the-art attacks and show that all methods are susceptible to severe failure under harsher attack settings. Our experiments show that FedRISE is more resilient in handling attacks with varying objectives. • FedRISE uses only two hyperparameters for aggregation (sparsification γ𝛾\gammaitalic_γ and server momentum βr⁢asubscript𝛽𝑟𝑎\beta_{ra}italic_β start_POSTSUBSCRIPT italic_r italic_a end_POSTSUBSCRIPT) that are minimally dependent on client counts, training settings, and data distribution. Figure 2: The steps involved in FedRISE are outlined here; please refer Sec. 4.1 for more explainations. FedRISE determines the ideal sign (ξ)𝜉(\xi)( italic_ξ ) needed to ensure convergence at individual gradient locations based on variance-reduced sparse gradients (𝐯)𝐯(\mathbf{v})( bold_v ). During sign election, each client is given weightage (ϱ)italic-ϱ(\varrho)( italic_ϱ ) in voting based on the quality of their gradients with respect to other clients measured using Gradient Valuation Function shown in Eq. 9. Only individual gradients aligning with the computed sign ξ𝜉\xiitalic_ξ are averaged location-wise, while others are ignored. Thus, FedRISE accommodates useful gradients from heterogeneous clients while filtering out only those that are poisoned or contradict the optimization. Results on heterogenous datasets are shown in Fig. 3"
https://arxiv.org/html/2411.03835v1,An Edge Computing-Based Solution for Real-Time Leaf Disease Classification using Thermal Imaging,"Deep learning (DL) technologies can transform agriculture by improving crop health monitoring and management, thus improving food safety. In this paper, we explore the potential of edge computing for real-time classification of leaf diseases using thermal imaging. We present a thermal image dataset for plant disease classification and evaluate deep learning models, including InceptionV3, MobileNetV1, MobileNetV2, and VGG-16, on resource-constrained devices like the Raspberry Pi 4B. Using pruning and quantization-aware training, these models achieve inference times up to 1.48x faster on Edge TPU Max for VGG16, and up to 2.13x faster with precision reduction on Intel NCS2 for MobileNetV1, compared to high-end GPUs like the RTX 3090, while maintaining state-of-the-art accuracy.","Diseases in plants caused by living entities can be classified as biotic and abiotic [1]. The primary sources of these biotic diseases are fungi, bacteria, and viruses. Conversely, abiotic diseases arise from non-living environmental factors, including hail, spring frosts, various weather conditions, exposure to chemicals, and more. These abiotic diseases, being non-infectious and non-transmissible, are generally less harmful and often preventable. Plant diseases pose a significant threat to food security, requiring prompt detection. However, the complexity of these diseases often challenges even experienced agronomists and plant biologists, leading to potential misdiagnosis and improper treatment [2]. In São Paulo, Brazil, crops are particularly vulnerable to climate variability, with extreme weather events like droughts, heat, and frost further complicating agricultural productivity. To address these challenges, efficient agricultural techniques such as crop rotation, cover cropping, and water management are essential for safeguarding and enhancing productivity [3],[4]. In addition, the significant reduction in crop yield is due to the inability to detect diseases early on, which invariably leads to decreased agricultural output. Consequently, the early recognition and examination of crop diseases are essential for maintaining crop quality. With recent advancements in computing speed and power, the use of large datasets has enhanced the efficiency of these systems [5]. Therefore, the development of an automated process for leaf disease detection can also help with the reduction of agrochemicals and pesticide. Most research on leaf disease classification has focused on visible images [6, 7],[8, 9], which are sensitive to light conditions and may underperform without daylight. In contrast, thermal imaging, with its potential for overcoming these limitations, has been widely used in studies on environmental stress, crop yield, and seed vigor [10]. Moreover, the use of traditional cloud infrastructures approaches for image classification poses a latency challenge between an edge device and the cloud [11], which is not feasible for real-time image classification. In this context, edge computing is a paradigm that facilitates computation near data sources, allowing processing at the edge layer instead of in the cloud. This approach is particularly advantageous when handling numerous distributed data sources where cloud computing may not be feasible [12]. Therefore, in this study we leverage the use of an embedded device such as the Raspberry Pi 4B as an edge device and perform aggressive model compression by means of pruning-quantization aware training (PQAT) in addition of hardware acceleration devices such as the Edge TPU and Intel NCS2 to perform real-time image classification of leaf diseases using thermal camera. Our main contributions can be summarized as follows: • Introduction of a new dataset: We introduce a thermal image dataset with leaf diseases across 16 types of plants. • Evaluation of model optimization techniques: We evaluate pruning and quantization-aware training on MobileNetV1, MobileNetV2, VGG16, and InceptionV3, optimizing these models for performance on resource-constrained devices, such as the Raspberry Pi 4B. • Real-time classification hardware-based solution: We develop a hardware-based solution that enables real-time image classification using hardware acceleration devices such as the Edge TPU (Coral USB) and the Visual Processing Unit (VPU), specifically the Intel NCS2."
https://arxiv.org/html/2411.03829v1,Generalize or Detect? Towards Robust Semantic Segmentation Under Multiple Distribution Shifts,"In open-world scenarios, where both novel classes and domains may exist, an ideal segmentation model should detect anomaly classes for safety and generalize to new domains. However, existing methods often struggle to distinguish between domain-level and semantic-level distribution shifts, leading to poor out-of-distribution (OOD) detection or domain generalization performance. In this work, we aim to equip the model to generalize effectively to covariate-shift regions while precisely identifying semantic-shift regions. To achieve this, we design a novel generative augmentation method to produce coherent images that incorporate both anomaly (or novel) objects and various covariate shifts at both image and object levels. Furthermore, we introduce a training strategy that recalibrates uncertainty specifically for semantic shifts and enhances the feature extractor to align features associated with domain shifts. We validate the effectiveness of our method across benchmarks featuring both semantic and domain shifts. Our method achieves state-of-the-art performance across all benchmarks for both OOD detection and domain generalization. Code is available at https://github.com/gaozhitong/MultiShiftSeg.","Semantic segmentation, a fundamental task in computer vision, has become indispensable in various real-world applications, such as autonomous driving [35]. Recent progress in deep learning-based semantic segmentation has exhibited promising results under the assumption of consistent distributions between the training and testing data. However, these models often falter when faced with distributional shifts. Consequently, research on semantic segmentation under distributional shifts has garnered significant attention in recent years. Some studies approach this challenge from a generalization perspective, aiming to train networks to adapt to data with covariate distribution shifts, such as novel domains [9, 45]. Another line of research focuses on training models to discern (or detect) test data exhibiting semantic distributional shifts, such as anomalies or unfamiliar objects, to ensure reliable predictions [5, 4]. In real-world situation, both types of distribution shifts often occur jointly. This leaves us with the question: Can a model jointly handle both kinds of distribution shift? To address this question, we assess the ability of current domain generalization techniques [9, 45] to detect unknown objects and that of out-of-distribution detection techniques [31, 42, 46] to generalize to unknown domains. Interestingly, we find that models trained using domain generalization techniques, such as domain randomization or whitening transformation, often fail to identify unknown objects, and sometimes even perform worse than the baseline without domain generalization. Furthermore, we observe that models trained using out-of-distribution detection techniques struggle to generalize to unknown domains, exhibiting overly high uncertainty towards objects experiencing domain shifts compared to baseline methods without OOD training. While one intuitive approach is to combine existing anomaly segmentation and domain generalization techniques during training, we note that current domain generalization strategies primarily address image-level shifts, whereas anomaly segmentation focuses on object-level semantic differences. Consequently, the resulting models tend to generalize well to image-level variations, such as changes in weather but struggle with object-level shifts. They often misinterpret any object-level distribution shift as a semantic anomaly, assigning high uncertainty scores to known objects that exhibit covariate changes, such as color variations in cars or changes in pedestrian attire, as demonstrated in Fig. 1.These experiments underscore the challenge of differentiating and jointly handling different types of distribution shifts. In this work, we jointly study both semantic and covariate distribution shifts. That is, we aim to equip the model to generalize effectively to covariate-shift regions while precisely identifying semantic-shift regions. To achieve this, we design a novel generative augmentation method to produce coherent images that incorporate both anomaly (or novel) objects and various covariate shifts at both image and object levels. Furthermore, we introduce a training strategy that re-calibrates uncertainty specifically for semantic shifts and enhances the feature extractor to align features associated with domain shifts 111In this work, we use ’domain shift’ and ’covariate shift’ interchangeably.. Specifically, we first introduce a novel data augmentation technique that employs a semantic-to-image generation model to create data that encompasses both covariate and semantic shifts at various levels, allowing the model to learn the essential differences between the shift types. Additionally, we introduce a learnable, semantic-exclusive uncertainty function trained using a relative contrastive loss. We adopt a two-stage training paradigm designed to balance the integration of these enhancements while minimizing their potential interference. A noise-aware training strategy further complements this approach, employing online, pixel-wise selection to mitigate noise in the generated images. Altogether, our approach not only boosts the model’s generalization across domain shifts but also ensures a high level of uncertainty in response to semantic shifts. Figure 1: We study semantic segmentation with both semantic-shift and covariate-shift regions. (a) Training for Out-of-distribution (OOD) detection alone [31] yields high uncertainty for both types of shifts, whereas training for domain generalization (DG) alone [9] tends to produce low uncertainty for both. Our method effectively differentiates between the two, generating high uncertainty only for semantic-shift regions. (b) We achieve strong performance in both OOD detection and domain-generalized semantic segmentation. (c) This is achieved by coherently augmenting original images (first row) with both covariate and semantic shifts (second row). We validate the effectiveness of our method across benchmarks featuring both semantic and domain shifts, including RoadAnomaly [30], SMIYC [5], ACDC-POC [12] and MUAD [15] benchmarks. Our results demonstrate that our method achieves state-of-the-art performance across all benchmarks, employing different segmentation backbones for both OOD detection and known class segmentation. In summary, our contributions are: (1) We study semantic segmentation under both semantic and domain shifts, revealing limitations in methods focused on a single shift; (2) We introduce a coherent-generative augmentation method that augments training data with both shifts; (3) We propose a two-stage, noise-aware training pipeline to optimally leverage augmented data, learning a semantic-exclusive uncertainty function while aligning features for domain shifts."
https://arxiv.org/html/2411.03823v1,Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM Data Contamination,"The rapid progression of multimodal large language models (MLLMs) has demonstrated superior performance on various multimodal benchmarks. However, the issue of data contamination during training creates challenges in performance evaluation and comparison. While numerous methods exist for detecting dataset contamination in large language models (LLMs), they are less effective for MLLMs due to their various modalities and multiple training phases. In this study, we introduce a multimodal data contamination detection framework, MM-Detect, designed for MLLMs. Our experimental results indicate that MM-Detect is sensitive to varying degrees of contamination and can highlight significant performance improvements due to leakage of the training set of multimodal benchmarks. Furthermore, We also explore the possibility of contamination originating from the pre-training phase of LLMs used by MLLMs and the fine-tuning phase of MLLMs, offering new insights into the stages at which contamination may be introduced.","The development of MLLMs has exceeded expectations (Liu et al., 2023a; Lin et al., 2023), showcasing extraordinary performance on various multimodal benchmarks (Lu et al., 2022; Liu et al., 2023b; Song et al., 2024), even surpassing human performance. However, due to the partial obscurity associated with MLLMs training (OpenAI, 2023; Reid et al., 2024), it remains challenging to definitively ascertain the impact of training data on model performance, despite some works showing the employment of the training set of certain datasets (Liu et al., 2023a; Chen et al., 2023; Bai et al., 2023b). The issue of data contamination, occurring when training or test data of benchmarks is exposed during the model training phase (Xu et al., 2024), could potentially instigate inequitable performance comparisons among models. This not only creates a dilemma for users in model selection but also poses a significant hurdle to further advancements in this domain. While numerous works in the field of LLMs have proposed methods for detecting data contamination (Yeom et al., 2018; Deng et al., 2024; Dong et al., 2024), MLLMs, due to their various modalities and multiple training phases (Liu et al., 2023a; Li et al., 2023), face limitations when applying these methods. Therefore, there is a pressing need for a more suitable multimodal contamination detection framework specifically tailored for MLLMs. In this study, we carry out a systematic analysis of multimodal data contamination. Initially, we define Multimodal Data Contamination, as it pertains to the modality of data sources exposed to the MLLMs, into two categories: Unimodal Contamination and Cross-modal Contamination. Subsequently, we unveil a detection framework for multimodal data contamination, MM-Detect, which incorporates two methods, Option Order Sensitivity Test and Slot Guessing for Perturbation Caption, designed to handle two common types of Visual Question Answering (VQA) tasks: multiple-choice and caption-based questions, respectively. Then, applying MM-Detect on eleven widely-used MLLMs across five prevalent VQA datasets, we observe that both open-source and proprietary MLLMs do exhibit contamination, with the degree of contamination varying across different models. To corroborate the validity and sensitivity of our approach, we deliberately induce contamination in MLLMs, thus simulating feasible real-world scenarios. Experimental results indicate that our method proves to be quite effective and sensitive in identifying varying degrees of contamination. Interestingly, our findings reveal that not only does leakage in the multimodal benchmark test set play a role, but the training set can also contribute significantly to enhancing the model’s performance. To further delve into the stage where contamination is introduced, we employ a heuristic method. This method seeks to distinguish whether the contamination originates from the pre-training phase of LLMs or the multimodal training phase. Our findings suggest that the contamination observed in some MLLMs may not necessarily stem from the multimodal training phase. Instead, it could potentially be traced back to the pre-training stage of their respective LLMs. To the best of our knowledge, our work is the first effort to systematically analyze multimodal data contamination. In conclusion, our research makes several important contributions: • We formulate the definition for multimodal contamination detection and present the MM-Detect framework, comprising two innovative methods specifically designed for effective contamination detection in MLLMs. • We demonstrate that leakage from multimodal benchmark data can significantly boost the model’s performance on test sets, with this enhancement intensifying as the degree of contamination increases. • By employing a heuristic method, we pioneer the exploration into the stage at which contamination is introduced, revealing that it may stem not solely from the multimodal data but could also from the LLMs."
https://arxiv.org/html/2411.03819v1,SA3DIP: Segment Any 3D Instance withPotential 3D Priors,"The proliferation of 2D foundation models has sparked research into adapting them for open-world 3D instance segmentation. Recent methods introduce a paradigm that leverages superpoints as geometric primitives and incorporates 2D multi-view masks from Segment Anything model (SAM) as merging guidance, achieving outstanding zero-shot instance segmentation results. However, the limited use of 3D priors restricts the segmentation performance. Previous methods calculate the 3D superpoints solely based on estimated normal from spatial coordinates, resulting in under-segmentation for instances with similar geometry. Besides, the heavy reliance on SAM and hand-crafted algorithms in 2D space suffers from over-segmentation due to SAM’s inherent part-level segmentation tendency. To address these issues, we propose SA3DIP, a novel method for Segmenting Any 3D Instances via exploiting potential 3D Priors. Specifically, on one hand, we generate complementary 3D primitives based on both geometric and textural priors, which reduces the initial errors that accumulate in subsequent procedures. On the other hand, we introduce supplemental constraints from the 3D space by using a 3D detector to guide a further merging process. Furthermore, we notice a considerable portion of low-quality ground truth annotations in ScanNetV2 benchmark, which affect the fair evaluations. Thus, we present ScanNetV2-INS with complete ground truth labels and supplement additional instances for 3D class-agnostic instance segmentation. Experimental evaluations on various 2D-3D datasets demonstrate the effectiveness and robustness of our approach. Our code and proposed ScanNetV2-INS dataset are available HERE.","3D instance segmentation is a fundamental task pivotal to 3D understanding across various domains such as autonomous driving, robotics navigation, and virtual reality applications. State-of-the-art methods [1, 2] are predominantly supervised and rely heavily on precise 3D annotations for training, thus constraining their applications in open-world scenes. Compared to scarce 3D labeled data, the acquisition and annotation of 2D images are more convenient. Recently, 2D foundation models [3, 4, 5, 6] trained on large-scale annotated 2D data show impressive performance and strong generalization capabilities in zero-shot scenarios. Recent efforts have sought to leverage Segment Anything Model (SAM) by lifting its class-agnostic 2D segmentation results to 3D tasks [7, 8, 9, 10]. Specifically, some methods [8, 7] propose a pipeline that decomposes the 3D scene into geometric primitives and leverages 2D multi-view masks from SAM to calculate pairwise similarity scores as merging guidance. Further well-designed algorithms or Graph Neural Networks (GNNs) are included to ensure multi-view consistency. Figure 1: Comparison of our SA3DIP with other methods. Methods like SAI3D (bottom) fail to distinguish instances with similar normals when computing superpoints, which accumulate to the final segmentation. Moreover, the part-level 2D segmentation transfers to 3D space, resulting in over-segmented 3D instances. We present a novel pipeline for segmenting any 3D instances, which overcomes the limitations by exploiting additional 3D priors, specifically by incorporating both geometric and textural prior on superpoints computing, and supplementing 3D space constraint provided 3D prior by utilizing a 3D detector. However, the geometric rudimentary pre-segmentation initialization impedes their ability to group superpoints on points with highly similar normals, such as boards on walls and books on tabletops. As shown in Fig. 1 bottom left, the blackboard and the wall are wrongly allocated within the same superpoint using previous methods. Owing to the coarse-to-fine pattern of the pipeline, errors at this stage propagate to subsequent stages, which the sophisticated merging algorithms fail to rectify. Furthermore, current approaches heavily rely on 2D foundation models and design algorithms or GNNs within 2D space, neglecting the inherent 3D priors of the data. Part-level segmentation in the generated 2D masks by SAM transfers to 3D space and leads to over-segmented 3D instances. As illustrated in Fig. 1 bottom right, the sofa and chairs are segmented at the part level in 2D space, causing over-segmentation in the final results. These limitations primarily stem from the under-exploitation of 3D priors: (1) Complete point cloud data encompasses not only spatial coordinates but also color channels; (2) Constraints provided by 3D space prior to the merging process cannot be neglected. In this paper, we present SA3DIP (Segment Any 3D Instance with potential 3D Priors), a novel method for segmenting high-quality 3D instances. Specifically, we observe that distinct instances with similar normals often exhibit different colors. Therefore, we incorporate both geometric and textural priors to generate finer-grained complementary primitives. As shown in Fig. 1 top right, our method identifies the boundary between the blackboard and the wall clearly. In this way, the initial errors are minimized, which reduces error accumulation in the subsequent process. Moreover, we exploit the 3D prior at the merging stage to provide constraints on the over-segmented 3D instances, which is implemented by incorporating a 3D detector. This additional 3D prior enables rectification on the over-segmented 3D instances, while preserving the capability in handling fine-grained objects. Therefore, the sofa and chairs maintain their integrity in 3D space by our approach, which is illustrated in Fig. 1 top right. Additionally, we notice that the widely-used benchmark, ScanNet [11], contains a considerable portion of low-quality ground truth annotations for instance segmentation, which leads to biases in assessing model performance. Thus, we propose ScanNetV2-INS, a point-level enhanced version tailored for 3D class-agnostic instance segmentation. The revised dataset contains fewer incomplete labels and fewer missing instances, which better showcases real-world scenarios. Our contributions are three-fold: (1) We present SA3DIP, a novel pipeline for segmenting any 3D instances by exploiting potential 3D priors, which includes incorporating both geometric and color priors on computing 3D superpoints, and introducing constraints provided by 3D prior at the merging stage; (2) We propose a point-level enhanced version of ScanNetV2, specifically for 3D class-agnostic instance segmentation by rectifying incomplete annotations and incorporating more instances; (3) Extensive experiments are conducted on ScanNetV2, ScanNetV2-INS and ScanNet++ [12] datasets, and the competitive results demonstrate the effectiveness and robustness of our method."
https://arxiv.org/html/2411.03807v2,GS2Pose: Two-stage 6D Object Pose Estimation Guided by Gaussian Splatting,"This paper proposes a new method for accurate and robust 6D pose estimation of novel objects, named GS2Pose. By introducing 3D Gaussian splatting, GS2Pose can utilize the reconstruction results without requiring a high-quality CAD model, which means it only requires segmented RGBD images as input. Specifically, GS2Pose employs a two-stage structure consisting of coarse estimation followed by refined estimation. In the coarse stage, a lightweight U-Net network with a polarization attention mechanism, called Pose-Net, is designed. By using the 3DGS model for supervised training, Pose-Net can generate NOCS images to compute a coarse pose. In the refinement stage, GS2Pose formulates a pose regression algorithm following the idea of reprojection or Bundle Adjustment (BA), referred to as GS-Refiner. By leveraging Lie algebra to extend 3DGS, GS-Refiner obtains a pose-differentiable rendering pipeline that refines the coarse pose by comparing the input images with the rendered images. GS-Refiner also selectively updates parameters in the 3DGS model to achieve environmental adaptation, thereby enhancing the algorithm’s robustness and flexibility to illuminative variation, occlusion, and other challenging disruptive factors. GS2Pose was evaluated through experiments conducted on the LineMod dataset, where it was compared with similar algorithms, yielding highly competitive results. The code for GS2Pose will soon be released on GitHub.","Accurate 6D object pose estimation is a fundamental problem in the field of computer vision, with broad application prospects in technologies such as robot navigation[1, 2] and virtual reality[3, 4]. However, classical pose estimation algorithms[5, 6, 7] lack robustness against environmental interference, such as non-uniform lighting, varying degrees of occlusion, and dynamic blur. Moreover, the lightweight of the algorithm is also demanding in the field of embodied intelligence[8, 9, 10]. With the widespread application of deep learning methods, the robustness of related algorithms[11, 12, 13, 14, 15] against interference has continually improved. Early works[16, 17, 18, 19] have achieved high-precision instance-level pose estimation. However, these models can only handle a specific object after training session and cannot generalize to others. Additionally, they require datasets with precise ground truth pose, which is difficult to obtain in practical applications. With the emergence of novel pose representation methods[20] such as NOCS, breakthroughs have been made in category-level pose estimation methods[21, 22, 23, 24, 25, 26], achieving notable intra-class generalization. Trained models can perform high-precision pose estimation on objects with similar geometric and color features. However, these methods typically require a substantial amount of CAD models of the same category during training phase, results in huge time expenditure. Additionally, since the 6D pose of the target object is bound to the objects coordinate system under the CAD model, which can lead to issues, such as parameter ambiguity in the estimation results during the inference phase. In recent years, with the development of large models[27, 28, 29], some research[30, 31, 32] have introduced the concept of pre-training on large datasets into the field of 6D pose estimation. These methods construct large datasets by collecting numerous CAD models of common objects from different categories, enabling effective generalization to unseen objects. They require only the CAD model of the target object during inference, allowing for the artificial setting of strict coordinate relationships without the need for additional training on the target object. However, these models also have drawbacks, such as the inability to generalize to uncommon objects, high computational resource consumption, and their accuracy being heavily dependent on the quality of CAD modeling. To address the aforementioned shortcomings of these algorithms, we propose a novel pose estimation method that does not require artificial designed CAD models. This method is designed for application scenarios where high-quality CAD models of the target object are unavailable, and only untextured scanned models or SFM point cloud models can be obtained. To achieve lightweight training, accurate reference relationships, and robustness to interference, GS2POSE consists of a two-stage pose estimation approach comprising coarse estimation followed by pose refinement. The detailed process of GS2Pose is illustrated in in Fig. 1 The 3DGS point cloud model of the object (hereafter referred to as the 3DGS model) is obtained using existing 3DGS reconstruction techniques, with the object coordinate system manually specified. Inspired by the GS-SLAM model[33], we introduce the commonly used reprojection-based pose optimization iterative approach from the SLAM domain[34, 35, 36], also known as Bundle Adjustment (BA). By representing object poses using Lie algebra and integrating it with the differentiable 3DGS rendering pipeline, we developed an approach that utilizes reprojection and backpropagation. It enables an iterative optimization algorithm that can regress both the object pose and the camera pose, referred to as GS-Refiner. Since this iterative optimization algorithm requires a reasonable initial pose as a starting point for optimization, we also need to design an algorithm that can provide a rough pose estimate using only the segmented object image. Inspired by the NeRF-Pose model[37], we designed a rough pose estimation network called Pose-Unet.RGB images and their corresponding NOCS images are obtained from the camera perspective using 3DGS. These images are subsequently input into a pre-trained coarse pose estimation network (Pose-Unet) for fine-tuning, resulting in a coarse pose estimation for any novel rendering view of the object. . On the other hand, GS-Refiner leverages the parameter interpretability of the 3DGS model to selectively optimize and refine parameters, such as higher-order spherical harmonic color parameters, transparency, and ellipsoid orientation through backpropagation. This allows the model surface colors to adaptively adjust to environmental factors encountered during actual capture, such as lighting, occlusion, and motion blur. The primary contributions of the paper can be summarized as follows: i) By incorporating 3DGS reconstruction technology, lightweight 6D pose estimation of previously unseen objects is achieved in the absence of CAD models. ii) By employing Lie algebra to modify the differentiable rendering pipeline of 3DGS, a re-projection iterative algorithm called GS-Refiner was developed, enabling the correction of both object poses and camera poses. iii) By selectively regressing the parameters of 3DGS, a 6D pose estimation algorithm was developed with robust resistance to complex lighting, motion blur, and occlusions. iv) Through experiments on datasets such as LineMod, GS2POSE model demonstrated substantial advantages over comparable algorithms, particularly in accuracy, inference speed, and computational resource efficiency."
https://arxiv.org/html/2411.03795v1,VQA2:Visual Question Answering for Video Quality Assessment,"The advent and proliferation of large multi-modal models (LMMs) have introduced new paradigms to video-related computer vision fields, including training and inference methods based on visual question answering (VQA). Video Quality Assessment (VQA), a classic field in low-level visual quality evaluation, originally focused on quantitative video quality scoring. However, driven by advances in LMMs, it is now evolving towards more comprehensive visual quality understanding tasks. Recent works in the image domain have proved that visual question answering can significantly improve low-level visual evaluation. Nevertheless, related work is almost nonexistent in the video domain, leaving substantial room for improvement. To address this gap, we introduce the VQA2 Instruction Dataset—the first visual question answering instruction dataset entirely focuses on video quality assessment—and, based on it, we propose the VQA2 series models. The VQA2 Instruction Dataset consists of 3333 stages and covers various video types, containing 157,735157735157,735157 , 735 instruction question-answer pairs, including manually annotated and synthetic data. We conduct extensive experiments on video quality scoring and understanding tasks. Results demonstrate that the VQA2 series models achieve state-of-the-art (SOTA) performance in quality scoring tasks, and their performance in visual quality question answering surpasses the renowned GPT-4o. Additionally, our final model, the VQA2-Assistant, performs well across both scoring and question-answering tasks, validating its versatility.","With the advent and rise of large language models (LLMs) and large multi-modal models (LMMs) [1, 14, 63, 33], the field of computer vision related to videos has entered a new era. Visual Question Answering (VQA) [2], serving as a crucial tool for modality alignment, is widely employed in both the training and testing stages. The paradigm of instruction tuning [33] using multi-modal instruction datasets, which encompass vast amounts of high-quality data, has significantly enhanced the performance of video LMMs in high-level visual tasks such as video understanding [67, 30] and video temporal analysis[25]. However, unlike high-level tasks that are closely related to video semantics information, low-level visual tasks that are more pertinent to video quality attributes (flicker, blur, stalling, etc) have not become the primary focus of research in these large models. Low-level Video Quality Assessment (VQA) aims to enhance models’ ability to perceive and evaluate low-level visual signals in videos, improve the accuracy of quantitative video quality scoring, and provide more precise and nuanced quality understanding and analysis. This task holds broad application potential: quantitative scoring predictions can be widely utilized in streaming video transmission processes [23], serving as effective feedback. Additionally, understanding and analyzing local visual quality can be applied in the image/video generation domain as effective guidance for refining local generation details [27]. Unfortunately, the majority of video quality assessment models focus solely on quantitative video quality scoring [36], entirely lacking the capability to understand and analyze quality details, which results in significant deficiencies in model versatility. Moreover, models equipped with low-level visual quality understanding and analysis capabilities almost exclusively apply to the image domain [66], lacking effective perception abilities for video-specific temporal and motion quality attributes. To address the gap above, we construct the VQA2 instruction dataset—the first large-scale instruction dataset dedicated to video quality assessment based on visual question answering and develop the VQA2 series models based on this dataset. Our dataset construction and training methodology are designed to simultaneously enhance the model’s capabilities in precise quantitative quality scoring and nuance quality understanding while striving to maintain its versatility. This approach ensures that the final model possesses both robust quantitative scoring and quality understanding capabilities. The dataset construction is divided into three stages: • Stage-1: Pre-training data subset centered on distortion recognition We design two types of distortion recognition—spatial distortion recognition and motion distortion recognition—utilizing annotated data in various datasets. The objective is to establish the model’s foundational perception capabilities of different quality attributes. • Stage-2: Instruction tuning subset centered on video quality scoring We utilize the mean opinion score (MOS) from various existing datasets and transform quantitative scoring into the quality level, Which is more compatible with the training of generative language models [40]. • Stage-3: Instruction tuning subset for video quality understanding In Stage-3, we utilize high-quality and diversified data that has been expanded through GPT-4 following human expert annotation to finetune the trained models in the first two stages. Our human annotations are centered on quality attributes, providing comprehensive depictions of video quality. Our core contributions are as follows: 1) We construct the first visual-question-answering-based instruction-tuning dataset for video quality assessment - the VQA2 Instruction Dataset. Our dataset encompasses 3333 stages and includes over 150,000150000150,000150 , 000 instruction pairs, covering a diverse range of video types such as user-generated content (UGC) videos, streaming videos, and artificial intelligence-generated content (AIGC) videos. This ensures both data adequacy and diversity. 2) We design a complete training strategy and construct the VQA2 series models, including the VQA2-Scorers and the VQA2-Assistant. The VQA2-Scorers achieves state-of-the-art (SOTA) performance in multiple video quality scoring tasks. Meanwhile, the VQA2-Assistant excels in video quality understanding tasks, outperforming the proprietary GPT-4o on relevant benchmark tests. Additionally, it maintains robust performance in quality scoring tasks, thereby demonstrating the model’s functional versatility."
https://arxiv.org/html/2411.03794v1,Harmformer: Harmonic Networks Meet Transformers for Continuous Roto-Translation Equivariance,"Convolutional Neural Networks exhibit inherent equivariance to image translation, leading to efficient parameter and data usage, faster learning, and improved robustness. The concept of translation equivariant networks has been successfully extended to rotation transformation using group convolution for discrete rotation groups and harmonic functions for the continuous rotation group encompassing 360∘superscript360360^{\circ}360 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT. We explore the compatibility of the Self-Attention mechanism with full rotation equivariance, in contrast to previous studies that focused on discrete rotation. We introduce the Harmformer, a harmonic transformer with a convolutional stem that achieves equivariance for both translation and continuous rotation. Accompanied by an end-to-end equivariance proof, the Harmformer not only outperforms previous equivariant transformers, but also demonstrates inherent stability under any continuous rotation, even without seeing rotated samples during training.","A key strength that positions Convolutional Neural Networks (CNNs) [1] as a superior architecture for computer vision tasks is the weight sharing across the spatial domain. This design ensures that CNN feature maps retain their values as the input is translated, only being shifted according to the input. Formally known as translation equivariance, this property provides CNNs with inherent robustness and efficiency in managing translations. Equivariance can be extended to other groups of transformations, such as rotation, scaling, or mirroring. The advantage of equivariant models is that they ensure a tractable response of the model to the transformation of the input. As a result, the model can eliminate the effects of the transformations and produce predictions that are invariant to them. For instance, to achieve translation invariance in conventional CNNs, the feature maps are commonly aggregated by global average pooling before the classification layer. Group Equivariant Convolutional Neural Networks (G-CNNs) [2] show that CNNs can be modified to become equivariant to any discrete transformation group, such as rotation by a discrete set of angles. An extension to continuous rotation and translation group was introduced by Worrall et al. [3]. The authors proposed Harmonic Networks (H-Nets), which restrict the convolution filters to a family of harmonic functions ideal for expressing full rotation equivariance. Both approaches improve the generalization and efficiency of training for the chosen group, similarly to how CNNs benefit from translation equivariance. For example, rotation equivariant networks are well suited for object detection in aerial imagery because such images lack natural orientation and equivariant networks inherently accommodate all rotations. Beyond aerial imagery [4, 5], equivariant CNNs are effective in many other applications, such as microscopy [6, 7], histology [8], and remote sensing [9]. With the adoption of transformer architectures in computer vision, the Self-Attention mechanism has also been integrated into equivariant networks [10, 11, 12]. Equivariant transformers are gaining importance especially in domains such as graph-based structures (e.g. molecules) [13, 14, 15, 16], vector fields [17], manifolds [18], and generic geometric data [19, 20]. In the 2D domain, Romero and Cordonnier [21] proposed a transformer equivariant to discrete rotation and translation groups by using the principle of G-CNNs in the positional encoding of the Self-Attention (SA). The formulation was further improved by Xu et al. [22]. In both cases, the computational complexity of the equivariant SA increases quadratically with the number of angles in the considered rotation group, which limits the model angular resolution. Equivariance to continuous rotation presents a versatile solution. In this paper, we introduce Harmformer, the first vision transformer capable of achieving continuous 2D roto-translation equivariance. The name is derived from circular harmonics [23] which provide the equivariance property preserved throughout the architecture. To ensure computational efficiency, our network starts with an equivariant convolutional stem based on Harmonic networks [3], where we redesign the key components, such as activations, normalization layers, and introduce equivariant residual connections. The stem output is divided into equivariant patches, which are then passed to the transformer. Alongside a novel self-attention SA mechanism, we introduce layer normalization and linear layers to guarantee end-to-end equivariance. The equivariance property allows Harmformer to remove the effect of roto-translation just before classification, preserving all relevant information at earlier stages (see Fig. 1). Through experimental validation, we show that Harmformer surpasses all previous discrete equivariant transformers [21, 22] on established benchmarks [24, 25, 26]. It also outperforms earlier invariant models [27, 28, 29] on classification tasks where the model is trained solely on non-rotated data."
https://arxiv.org/html/2411.03745v1,Homotopy Continuation Made Easy: Regression-based Online Simulation of Starting Problem-Solution Pairs,"While automatically generated polynomial elimination templates have sparked great progress in the field of 3D computer vision, there remain many problems for which the degree of the constraints or the number of unknowns leads to intractability. In recent years, homotopy continuation has been introduced as a plausible alternative. However, the method currently depends on expensive parallel tracking of all possible solutions in the complex domain, or a classification network for starting problem-solution pairs trained over a limited set of real-world examples. Our innovation consists of employing a regression network trained in simulation to directly predict a solution from input correspondences, followed by an online simulator that invents a consistent problem-solution pair. Subsequently, homotopy continuation is applied to track that single solution back to the original problem. We apply this elegant combination to generalized camera resectioning, and also introduce a new solution to the challenging generalized relative pose and scale problem. As demonstrated, the proposed method successfully compensates the raw error committed by the regressor alone, and leads to state-of-the-art efficiency and success rates while running on CPU resources, only.","The solution of geometric camera calibration problems is a crucial step in many Structure-from-Motion (SfM) [20, 53, 54, 51], Visual Odometry (VO) [45], and visual Simultaneous Localization And Mapping (SLAM) [42] frameworks. Solvers developed over the years address camera resectioning [28, 22, 64, 30], two-view relative pose [43, 57, 29], pose estimation from lines [15, 40], partially calibrated cameras [63], generalized cameras [56, 30, 59, 40], rolling shutter cameras [50, 10], and many other scenarios. The problems often appear in the form of a polynomial equation system, and special techniques from the field of algebraic geometry have been used to solve them. Once an efficient solver is found, it is typically embedded into a random sampling and consensus scheme [18, 47] in order to gain robustness in the presence of outliers. Hence, there is a requirement for such solvers to be efficient and embeddable into iterative schemes. The dominant approaches to efficiently solve systems of polynomial equations are given by the Gröbner basis method and polynomial resultants [9, 58]. Based on the Gröbner basis theory, Stewénius et al. [57] and Kukelova et al. [32] demonstrate that the calculations required to obtain a minimal set of ideal generators can be effectuated efficiently at the hand of a fixed elimination template. Kukelova et al. [32] in particular propose a solver generator to automatically discover such elimination templates. Later on, sparse resultant-based solvers [16] have been demonstrated as a powerful alternative sometimes leading to smaller elimination templates. While polynomial elimination techniques have lead to significant progress in the solution of geometric vision problems, the methods are limited as the complexity of the template search grows uncontrollably with the degree and dimensionality of the problem at hand. Furthermore, owing to their failure to respect inequality constraints and rule out practically infeasible solutions, high degrees or dimensionalities naturally lead to an elevated number of algebraically possible solutions and—assuming that it can be found at all—large elimination templates [44, 64, 63, 13, 14, 62]. This in turn causes problems in terms of numerical stability [62]. In recent years, a different solution strategy has therefore gained popularity: Homotopy Continuation (HC) [60, 4, 12]. The method proceeds by starting from a known problem-solution pair, and then tracks its roots to the target problem at hand during a step-wise interpolation of polynomial coefficients. The method has been used to successfully solve challenging tri-focal relative pose problems [17]. While promising, HC remains expensive as it generally requires the parallel continuation of many roots in the complex domain in order to identify all real solutions to a target problem, followed up by disambiguation. Recently, Hruby et al. [24] propose an efficient, learning-based extension to the pure HC paradigm, which serves as our primary motivation. They propose the introduction of a classifier to pick a good problem-solution pair trained over a large set of known pairs obtained from SfM. This enables a highly efficient application of HC, as only a single root of interest needs to be continued. While still presenting a margin for improvement in terms of success rate, the method is demonstrated to have unprecedented computational efficiency on the challenging three-view-four-point problem. The method proposed in this work is similar to the work of Hruby et al. [24] in that it leverages a learning-based approach to produce a starting problem-solution pair that enables single-root-tracking. However, we note a couple of important differences: • We do not train a classifier on a fixed set of problem-solution pairs taken from a real-world dataset. Instead, we propose to employ a solution regression network trained over an arbitrarily large set of simulated input correspondences for the considered geometric problem. The network is hence not limited by a finite set of candidate problems, and aims at being a general solution approximator for any instance of the polynomial problem. • The bridge to HC is formed by appending an online simulator that uses the input correspondences and the predicted solution to generate a complete, consistent problem-solution pair. Given only moderate regression accuracy, the produced problem-solution pair is sufficiently close to the original problem for successful single-root tracking via HC. • We argue that the prediction of a single solution (or the unambiguous classification of a single problem-solution pair) from a minimal set may not always be possible as for complicated geometric problems, there may indeed be multiple geometrically feasible solutions. We therefore add one additional correspondence, which leads to a performance gain during both the regression and the continuation stage. Figure 1: Overview of the proposed geometric problem solution scheme. Given input correspondences, a regression network is utilized to approximate a solution. A subsequent online simulator generates a new set of correspondences that is consistent with the regression output. The obtained problem-solution pair is finally used to bootstrap homotopy continuation. The final solution is found efficiently by tracking a single root. The newly proposed simulate-and-solve paradigm is summarized in Figure 1. We demonstrate that this strategy can lead to high success-rate and efficient CPU-based solvers for two problems: generalized camera resectioning, and generalized relative pose and scale. Owing to its challenging nature, the latter problem has thus far only seen an optimization-based solution [31]. The solvers are compared against a purely learning-based alternative, a regressor followed by simple local refinement, as well as the exhaustive application of homotopy continuation over all roots. The proposed method is efficient, generalizable, and highly successful, and should thus be considered as an interesting alternative for the ongoing development of complicated geometric problem solvers."
https://arxiv.org/html/2411.03729v1,Relation Learning and Aggregate-attention for Multi-person Motion Prediction,"Multi-person motion prediction is an emerging and intricate task with broad real-world applications. Unlike single person motion prediction, it considers not just the skeleton structures or human trajectories but also the interactions between others. Previous methods use various networks to achieve impressive predictions but often overlook that the joints relations within an individual (intra-relation) and interactions among groups (inter-relation) are distinct types of representations. These methods often lack explicit representation of inter&intra-relations, and inevitably introduce undesired dependencies. To address this issue, we introduce a new collaborative framework for multi-person motion prediction that explicitly modeling these relations: a GCN-based network for intra-relations and a novel reasoning network for inter-relations. Moreover, we propose a novel plug-and-play aggregation module called the Interaction Aggregation Module (IAM), which employs an aggregate-attention mechanism to seamlessly integrate these relations. Experiments indicate that the module can also be applied to other dual-path models. Extensive experiments on the 3DPW, 3DPW-RC, CMU-Mocap, MuPoTS-3D, as well as synthesized datasets Mix1 & Mix2 (9∼similar-to\sim∼15 persons), demonstrate that our method achieves state-of-the-art performance.","Human motion prediction (HMP) aims to predict future human motion sequences based on the observed sequences. The HMP plays a major role in many real-world applications, such as autonomous driving [61, 36], robotics [63, 48], surveillance systems [64]. Due to the remarkable development of deep learning, the HMP has made unprecedented progress in recent years [39, 29, 62, 6]. However, single-person motion prediction methods focus solely on intra-relations, which contain the relative positions and movement patterns among body joints (e.g., hip, ankle, wrist) of the same individual. Humans are intuitively social agents, and they continuously interact with other people and their motion may also be influenced by the motions of others. Therefore, multi-person motion prediction task carries more practical significance compared to single-person motion prediction. This task is more challenging because of the sophisticated interactions across different individuals. Figure 1: Compared to previous methods [43, 42, 45, 55] with relation learning, we propose a new collaborative learning framework that explicitly explore joints relations, including intra-relations and inter-relations. The red and blue dashed lines indicate the inter-relation and intra-relation, respectively. Figure 2: Visualization of Pearson correlation coefficient (PCC) between different individuals. We conducted two Transformer-based architecture experiments on the CMU-Mocap dataset: (i) The explicit relation modeling adopts cross-attention to learn inter-relations between different individuals’ joints, and self-attention to learn intra-relation of each individual’s joints. (ii) The global modeling utilizes self-attention to learn all relations of all inputting skeleton joints. For each scene, the upper image shows the true scene in the sequence. The lower image shows the visualization of PCC between the 15 joints of person 1 and the other two persons’ joints. The red color indicates higher correlation (larger PCC) between two joints, while the blue indicates lower (smaller PCC). The previous works on multi-person motion prediction generally tend to exploring interactive behaviors by global relation modeling in which all skeleton joints are treated as a whole to establish the relations between them via self-attention [43, 42, 45, 55]. As shown in Fig. 1(a), these methods ignore the distinct representations of inter-relation and intra-relation. Noted that we define inter-relation as the relation of across different individuals’ joints, as the blue dash line shown. Corresponding, we define intra-relation as the relations of an individual’s joints, as the red dash line shown. By inputting all skeleton joints as a whole into the network, these methods may inevitably introduce undesired relations which weakens the learning effects of interaction information and simplifies the constraints of joints. To reveal the advantages of explicit representation relations, we conducted some Transformer-based architecture experiments on the CMU-Mocap dataset: (i) The explicit relation modeling adopts cross-attention to learn inter-relations between different individuals’ joints, and self-attention to learn intra-relation of each individual’s joints. (ii) The global modeling utilizes self-attention to learn all relations of all inputting skeleton joints. Both baselines are based on Transformer architecture. Inspired by methods [3, 4], we employ the Pearson correlation coefficient (PCC) to model the correlation between different people in the observed scene. The calculation can be expressed as follow: 𝐏𝐏𝐂⁢(𝐏𝟏,𝐏𝟐)=𝐏𝐏𝐂subscript𝐏1subscript𝐏2absent\displaystyle\mathbf{PPC\left(P_{1},P_{2}\right)}=bold_PPC ( bold_P start_POSTSUBSCRIPT bold_1 end_POSTSUBSCRIPT , bold_P start_POSTSUBSCRIPT bold_2 end_POSTSUBSCRIPT ) = ∑t=1T(𝐏𝟏⁢(t,j)−P¯1⁢(j))∑t=1T(𝐏𝟏⁢(t,j)−P¯1⁢(j))2superscriptsubscript𝑡1𝑇subscript𝐏1𝑡𝑗subscript¯𝑃1𝑗superscriptsubscript𝑡1𝑇superscriptsubscript𝐏1𝑡𝑗subscript¯𝑃1𝑗2\displaystyle\frac{\sum_{t=1}^{T}\left(\mathbf{P_{1}}\left(t,j\right)-% \overline{P}_{1}\left(j\right)\right)}{\sqrt{\sum_{t=1}^{T}\left(\mathbf{P_{1}% }\left(t,j\right)-\overline{P}_{1}\left(j\right)\right)^{2}}}divide start_ARG ∑ start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ( bold_P start_POSTSUBSCRIPT bold_1 end_POSTSUBSCRIPT ( italic_t , italic_j ) - over¯ start_ARG italic_P end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_j ) ) end_ARG start_ARG square-root start_ARG ∑ start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ( bold_P start_POSTSUBSCRIPT bold_1 end_POSTSUBSCRIPT ( italic_t , italic_j ) - over¯ start_ARG italic_P end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_j ) ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG end_ARG (1) ×∑t=1T(𝐏𝟐⁢(t,j)−P¯2⁢(j))∑t=1T(𝐏𝟐⁢(t,j)−P¯2⁢(j))2absentsuperscriptsubscript𝑡1𝑇subscript𝐏2𝑡𝑗subscript¯𝑃2𝑗superscriptsubscript𝑡1𝑇superscriptsubscript𝐏2𝑡𝑗subscript¯𝑃2𝑗2\displaystyle\times\frac{\sum_{t=1}^{T}\left(\mathbf{P_{2}}\left(t,j\right)-% \overline{P}_{2}\left(j\right)\right)}{\sqrt{\sum_{t=1}^{T}\left(\mathbf{P_{2}% }\left(t,j\right)-\overline{P}_{2}\left(j\right)\right)^{2}}}× divide start_ARG ∑ start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ( bold_P start_POSTSUBSCRIPT bold_2 end_POSTSUBSCRIPT ( italic_t , italic_j ) - over¯ start_ARG italic_P end_ARG start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_j ) ) end_ARG start_ARG square-root start_ARG ∑ start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ( bold_P start_POSTSUBSCRIPT bold_2 end_POSTSUBSCRIPT ( italic_t , italic_j ) - over¯ start_ARG italic_P end_ARG start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_j ) ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG end_ARG Where 𝐏𝐧subscript𝐏𝐧\mathbf{P_{n}}bold_P start_POSTSUBSCRIPT bold_n end_POSTSUBSCRIPT denote the motion representation of n𝑛nitalic_n-th person. t𝑡titalic_t denotes the t𝑡titalic_t-th motion sequence and j𝑗jitalic_j denotes the j𝑗jitalic_j-th skeleton joint. P¯nsubscript¯𝑃𝑛\overline{P}_{n}over¯ start_ARG italic_P end_ARG start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT denotes the mean value of person n. We visualize PCC between different individuals’ joints from two scenes, as shown in Fig. 2. In the first scene, person 1 is walking with person 2. Meanwhile, person 3 is phoning by moving his right arm (in red dashed circle). The red solid circled portions of Fig. 2(a) and Fig. 2(b) indicate the correlation between person 3’s right hand and person 1’s right leg. (The closer the color is to red, the greater the correlation.) Surprisingly, using the global relation modeling approach, person 3’s right hand is correlated with person 1’s leg joints, which is not consistent with the fact, as shown in Fig. 2(b). In the second scene, person 2 is conversing with a seated person 3 while person 1 is passing by them. In the observed sequence, there is little correlation between the trajectory of person 1 and the lower body of the stationary person 3 (in red dashed circle). From Fig. 2(c), we can see that the response values between the lower body (hip, knee, ankle) of person 1 and the lower body of person 3 are lower (within the red circle) than those of other body parts. The phenomenons demonstrate that the explicit modeling is more effective represent relations and avoid the undesired one. Moreover, global modeling methods make it difficult to assess which parts of the relationships are crucial for prediction accuracy, compromising the model’s interpretability. Therefore, we advocate explicitly modeling relations, as shown in Fig. 1(b). Specifically, we propose a new explicitly modeling framework that contains Graph Convolutional Networks (GCN) for intra-relations and the cross-attention attention mechanism for inter-relations. GCN-based methods have surpassed attention mechanism in single-person motion prediction tasks, as demonstrated by references [39, 40, 33], effectively isolating these influences from own joints. In contrast, attention-based approaches excel at handling complex dependencies [16] and prove more effective in multi-person scenarios [43, 44, 45]. In particular, the cross-attention mechanism allows us to exclusively focus on the influence of others (inter-relations) while extracting interaction features, effectively isolating these influences from own joints. Figure 3: The illustration of different fusion strategies: (a) TRiPOD [42] concats features from two different branch, then feeds them to an RNN decoder. (b) MRT [43] feeds distinct features to a Transformer decoder to explore their dependency automatically. (c) Our method leverages inter&intra-relations by a designed fusion module (IAM). Furthermore, the fusion of intra-relation and inter-relation is critical for high-quality prediction results. Since the simple addition or concatenation operation only mixes the features according to the dimensions [45, 42], it includes too much irrelevant information to fully extract more helpful information. To tackle the fusion issues, comparing to the previous approaches as shown in Fig. 3, we introduce a novel plug-and-play aggregation module called the Interaction Aggregation Module (IAM), which utilizes a variant attention mechanism that allows one type of relation to directly influence another, ensuring that the fused feature fully takes both types of relationships into account. Moreover, we conduct experiments on several baselines with replacing the our IAM and all these methods achieve improved results. This approach not only improves predictions but also showcases IAM’s robust plug-and-play capabilities, as confirmed by our experimental results. We perform our experiments on multiple datasets, including 3DPW [46], 3DPW-RC [45], CMU-Mocap [51], MuPoTS-3D [52], and synthesized datasets Mix1&Mix2 [43] (9∼similar-to\sim∼15 persons). The quantitative results demonstrate that our method achieves state-of-the-art performance. In summary, our contributions are as follows: • We propose a collaborative learning framework for multi-person motion prediction to explicitly modeling both the intra-relations within individuals and the inter-relations between them. Explicit modeling clearly illustrate the role of these two relations, enhancing the model’s robustness and interpretability. • We propose a novel plug-and-play aggregation module called the Interaction Aggregation Module (IAM), which introduces an aggregate-attention mechanism to seamlessly integrate intra&inter-relations. Experiments show that this module effectively aggregates these relations and is adaptable to other models requiring integrated information. • We conduct experiments on several common datasets and our method achieves state-of-the-art performance on all datasets."
https://arxiv.org/html/2411.03728v1,Efficient Fourier Filtering Network with Contrastive Learning for UAV-based Unaligned Bi-modal Salient Object Detection,"Unmanned aerial vehicle (UAV)-based bi-modal salient object detection (BSOD) aims to segment salient objects in a scene utilizing complementary cues in unaligned RGB and thermal image pairs. However, the high computational expense of existing UAV-based BSOD models limits their applicability to real-world UAV devices. To address this problem, we propose an efficient Fourier filter network with contrastive learning that achieves both real-time and accurate performance. Specifically, we first design a semantic contrastive alignment loss to align the two modalities at the semantic level, which facilitates mutual refinement in a parameter-free way. Second, inspired by the fast Fourier transform that obtains global relevance in linear complexity, we propose synchronized alignment fusion, which aligns and fuses bi-modal features in the channel and spatial dimensions by a hierarchical filtering mechanism. Our proposed model, AlignSal, reduces the number of parameters by 70.0%, decreases the floating point operations by 49.4%, and increases the inference speed by 152.5% compared to the cutting-edge BSOD model (i.e., MROS). Extensive experiments on the UAV RGB-T 2400 and three weakly aligned datasets demonstrate that AlignSal achieves both real-time inference speed and better performance and generalizability compared to sixteen state-of-the-art BSOD models across most evaluation metrics. In addition, our ablation studies further verify AlignSal’s potential in boosting the performance of existing aligned BSOD models on UAV-based unaligned data. The code is available at: https://github.com/JoshuaLPF/AlignSal.","Figure 1: Examples from the UAV RGB-T 2400 dataset. RGB images typically have higher resolution and a wider field of view compared to thermal images. Unmanned aerial vehicle (UAV)-based object detection methods have been broadly applied in various fields, such as emergency rescue [1, 2], plant maintenance [3], and electrical overhaul [4]. Despite the impressive achievement of existing detection models [1, 5, 6, 7], methods that rely solely on RGB [1, 5] or thermal imaging [6, 7] face challenges in scenes with low illumination, occlusion, and background clutter. The two modalities have their own characteristics. While the RGB modality provides rich color and texture information about a scene, the thermal modality utilizes temperature differences to identify objects independently of lighting conditions. Therefore, recent works [8, 9] have attempted to combine the strengths of RGB and thermal modalities to improve UAV-based object detection in complex scenes. In practice, the resolution and scale of images captured by the RGB and thermal cameras mounted on UAVs may differ, resulting in unaligned bi-modal images, as shown in Fig. 1. These differences lead to varying proportions and spatial locations of the same object between the two modalities. However, most existing bi-modal object detection datasets [10, 11, 12] are manually aligned, avoiding many real-world challenges due to the misalignment between the two modalities. Consequently, models designed based on these aligned datasets [13, 14, 15, 16, 17, 18, 12, 19, 20, 21, 22, 23] usually underperform when applied to unaligned data under realistic conditions. To address these issues, Song et al. [9] constructed the first unaligned bi-modal salient object detection (BSOD) dataset, UAV RGB-T 2400, based on the UAV view and proposed a paradigm model named MROS. MROS achieves remarkable performance by aligning the two modalities, then performing modal fusion, followed by decoding and generating predictions. However, two primary challenges remain. First, UAVs are often used for real-time scene detection, but the complexity of MROS - with 90.4M parameters and 45.3G floating point operations - places a significant burden on the UAV’s processor and memory. In particular, modal alignment and fusion modules have high computational demands, limiting the overall inference speed. Second, since UAVs operate at high altitudes, the scale of the objects captured is small, which increases spatial location offsets of small-scale objects between the two modalities. The alignment strategies in MROS, based on convolutional attention operations [24], struggle to handle large spatial offsets effectively due to their limited receptive fields. In this paper, we propose a two-step solution to address the aforementioned limitations of the current BSOD methods. First, we aim to develop parameter-free learning strategies that reduce model complexity while achieving essential modal alignment. Second, we simultaneously perform weak bi-modal alignment and fusion, further minimizing testing complexity to improve inference speed. By integrating bi-modal features into a shared feature space, we aim to establish a multi-dimensional, multi-filtering mechanism based on the mutual relationships between the two modalities on a global scale to address varying offset scales and leverage complementary bi-modal information effectively. To achieve this, in this work, we propose AlignSal, an efficient Fourier filtering network with contrastive learning for accurate and real-time UAV-based unaligned BSOD. Specifically, AlignSal is composed of two major novelties that contribute to its superior performance. First, we propose the semantic contrastive alignment loss (SCAL) to align RGB and thermal modalities at the semantic level. Inspired by contrastive learning [25, 26, 27, 28, 29], SCAL forces similar local features in RGB and thermal modalities to stay close in the embedding space while pushing the dissimilar features apart. This knowledge exchange process allows both modalities to refine each other, achieving modal alignment while improving their individual representations. Notably, SCAL does not increase computational loads during inference. Second, we propose the synchronized alignment fusion (SAF) module, which leverages the fast Fourier transform to align features in the channel and spatial dimensions and facilitate bi-modal fusion. By modeling channel dependencies, SAF employs a multi-group global filtering mechanism that hierarchically captures spatial offsets and bi-modal salient cues at multiple scales, further aligning and integrating them into the fused feature. The low complexity of SAF ensures real-time performance during inference. With the proposed SCAL and SAF, AlignSal effectively aligns RGB and thermal modalities by leveraging their correlations and complementary strengths, producing accurate saliency maps from the UAV perspective. In summary, our contributions are as follows: • We propose an efficient model, AlignSal, for UAV-based unaligned BSOD. With detailed ablation studies, we validate the effectiveness of the proposed key components, namely SCAL and SAF. We further demonstrate SCAL’s ability to enhance the performance of existing aligned BSOD models on the unaligned bi-modal data. • With extensive experiments conducted on the UAV RGB-T 2400 and three weakly aligned datasets, we show that our proposed AlignSal achieves better performance and generalizability compared to sixteen existing state-of-the-art models, while achieving real-time inference speed. • Compared to the current top-performing model (i.e. MROS), AlignSal achieves better and more robust performance in different scenarios across most evaluation metrics, while improving the inference speed by 152.5% (30.8 frames per second111Tested on an NVIDIA RTX4060 laptop GPU.) and using only 30.0% of MROS’ parameters (27.1M) and 50.6% of its floating point operations (22.9G)"
https://arxiv.org/html/2411.03725v1,PX2Tooth: Reconstructing the 3D Point Cloud Teeth from a Single Panoramic X-ray,"Reconstructing the 3D anatomical structures of the oral cavity, which originally reside in the cone-beam CT (CBCT), from a single 2D Panoramic X-ray(PX) remains a critical yet challenging task, as it can effectively reduce radiation risks and treatment costs during the diagnostic in digital dentistry. However, current methods are either error-prone or only trained/evaluated on small-scale datasets (less than 50 cases), resulting in compromised trustworthiness. In this paper, we propose PX2Tooth, a novel approach to reconstruct 3D teeth using a single PX image with a two-stage framework. First, we design the PXSegNet to segment the permanent teeth from the PX images, providing clear positional, morphological, and categorical information for each tooth. Subsequently, we design a novel tooth generation network (TGNet) that learns to transform random point clouds into 3D teeth. TGNet integrates the segmented patch information and introduces a Prior Fusion Module (PFM) to enhance the generation quality, especially in the root apex region. Moreover, we construct a dataset comprising 499 pairs of CBCT and Panoramic X-rays. Extensive experiments demonstrate that PX2Tooth can achieve an Intersection over Union (IoU) of 0.793, significantly surpassing previous methods, underscoring the great potential of artificial intelligence in digital dentistry.","Cone-Beam Computed Tomography (CBCT) [17] provides detailed volumetric three-dimensional information of anatomical structures, playing a critical role in dental treatment such as implanting and orthodontics [24]. Despite its advantages, CBCT involves a certain amount of radiation doses and costs, which impedes its utility in the real world. In contrast, Panoramic X-ray (PX) images, characterized by the low radiation exposure and cost-effective capture [25], allow patients to undergo subsequent treatment without CBCT in many clinical scenarios, reducing both radiation exposure and economic burden [23]. Furthermore, PX imaging significantly enhances the efficiency of dental practices, both in preoperative and postoperative phases [18], marking a substantial leap toward digitalization in dentistry [19]. Figure 1: Generation quality illustration on a single tooth. PX2Tooth (ours) generates more accurate meshes representing tooth shapes, with significant improvements in the root tip area and the smoothness of the mesh surface. This shift improves access to diagnostic and treatment options for a wider patient base. Nevertheless, while PX images provide 2D information about the oral cavity, they fall short in providing adequate 3D information regarding tooth volumes and spatial positioning, representing considerable difficulties yet holding substantial potential for advancing dental care [16]. Recent research has launched attempts to reconstruct the 3D anatomical structures, such as teeth, in CBCT from the 2D PX images. These approaches fall into two categories. The first line of work reconstructed 3D teeth by augmenting PX images with additional labeling data, such as tooth landmarks or photographs of tooth crowns [25]. For instance, [7] developed a model that utilized tooth landmarks identified in PX images to predict tooth shapes. Similarly, [11] reconstructed an individual tooth by employing a shape prior and a reflectivity model derived from the corresponding crown photograph. These methods necessitate substantial time and effort from professional dentists for annotations, resulting in a slow, costly, and resource-intensive process. Another line of methods reconstructs 3D teeth only with a single PX image. For instance, existing works first segment the 2D teeth and subsequently employ generative networks such as GANs to reconstruct the 3D tooth structures [10, 14, 6] Recent work also introduces a framework for 3D tooth reconstruction from PX images using a neural implicit function [12]. Nevertheless, there remain significant challenges for 3D tooth reconstruction from 2D PX images. First, the accuracy achieved by existing methods is still unsatisfactory to meet the requirements for clinical dental applications. Specifically, the detailed features in the root and the apex region of the tooth are inaccurate or inadequate. Moreover, these methods have only been validated on small datasets (comprising only 23-37 cases), while a large-scale dataset for more convincing evaluation is yet under development. To address the aforementioned challenges, we introduce a novel method PX2Tooth which generates 3D point cloud teeth from single 2D PX images, eliminating the need for extra labeling while ensuring high precision. PX2Tooth works in a two-stage manner. First, we design a PXSegNet model to segment PX images into 32 permanent tooth categories, ensuring accurate morphological and categorical information for each tooth. The position and shape information of individual teeth are subsequently employed during the reconstruction process. Afterwards, we design a 3D tooth generation network TGNet, which transforms any 3D random point clouds to the desired teeth. The segmentation outputs from PXSegNet are treated as prior information, which is further integrated to guide the generation of TGNet with a Prior Fusion Module (PFM). Such prior information can help improve the generation quality, especially for the tooth root and tip areas. We also employ tailored segmentation, as well as reconstruction loss, functions to train PXSegNet and TGNet. We evaluate the performance of our method with CBCT samples and corresponding generated PX images following existing works [10, 6]. To ensure reliable assessment, we construct a dataset with 499 CBCT cases, which is one order of magnitude larger than existing datasets. Our experimental results show that PX2Tooth achieved a reconstruction Intersection over Union (IoU) of 0.793, which significantly surpasses existing state-of-the-art methods, as illustrated in Fig 1. Extensive analysis and ablation studies further reveal the effectiveness of our method, suggesting the great potential of AI in future digital dentistry."
https://arxiv.org/html/2411.03724v1,Estimation of Psychosocial Work Environment Exposures Through Video Object Detection,"This paper examines the use of computer vision algorithms to estimate aspects of the psychosocial work environment using CCTV footage. We present a proof of concept for a methodology that detects and tracks people in video footage and estimates interactions between customers and employees by estimating their poses and calculating the duration of their encounters. We propose a pipeline that combines existing object detection and tracking algorithms (YOLOv8 and DeepSORT) with pose estimation algorithms (BlazePose) to estimate the number of customers and employees in the footage as well as the duration of their encounters. We use a simple rule-based approach to classify the interactions as positive, neutral or negative based on three different criteria: distance, duration and pose. The proposed methodology is tested on a small dataset of CCTV footage. While the data is quite limited in particular with respect to the quality of the footage, we have chosen this case as it represents a typical setting where the method could be applied. The results show that the object detection and tracking part of the pipeline has a reasonable performance on the dataset with a high degree of recall and reasonable accuracy. At this stage, the pose estimation is still limited to fully detect the type of interactions due to difficulties in tracking employees in the footage. We conclude that the method is a promising alternative to self-reported measures of the psychosocial work environment and could be used in future studies to obtain external observations of the work environment.","The use of Closed Circuit Television (CCTV) in public and private settings has increased substantially over the last 20 years. Its use is often associated with the prevention of crimes and aggressive behaviour, apprehension of criminals and raising the general perception of safety in public spaces. However, there are a number of other widespread uses of CCTV as well such as the estimation of traffic flows, automatic number plate recognition and the monitoring of wildlife and weather conditions. Surveillance of people in shops and supermarkets are routinely used in order to prevent crimes and apprehend criminals that expose personnel for threats and assaults (Norris et al., 2004). A meta-analysis of 76 studies on the use of CCTV in crime prevention showed that the use of CCTV in public places especially car parks reduced theft and other crimes substantially.(Piza et al., 2019) In a randomized controlled trial in the retail sector, Hayes & Downs (Hayes and Downs, 2011) showed that the use of surveillance in a retail setting reduced loss due to theft in shops with CCTV installed compared to the control group without CCTV. There are in other words some evidence for the usefulness of CCTV as a preventive measure against crimes in general as well as theft in particular in the retail sector. The use of surveillance is also justified by the usefulness of the data for investigating crimes that have been committed. In a review from the UK, Ashby (Ashby, 2017) showed that useful CCTV footage were available in 29% of the cases of reported crimes on the British railway network. No such review has been carried out in the retail sector but we can assume that the footage would be even more useful here as the availability of the footage is higher. The use of CCTV in the retail sector is therefore justified both by the potential for preventing crimes and theft as well as the potential for investigating crimes that have been committed. As theft can sometimes lead to violence and threats against employees, the use of CCTV can also be seen as a preventive measure against workplace violence and threats, creating a safer atmosphere for employees. This has been most explicitly researched in the transport sector where CCTV as well as body cams has been used to prevent violence as well as other types of aggressive behaviour towards personnel.(Nobili et al., 2023) Fortunately, threats and violence are rare events in the retail sector. Arbejdstilsynet (The Danish Working Environment Authority) bi-annually carries out surveillance on the work environment in Denmark through the survey NOA-L (Arbejdstilsynet, 2021). In 2021, 6.3 % of the employees in the retail sector reported that they had been exposed to threats of violence in the work environment, a majority of these events were reported to be from external sources i.e. customers. A much smaller proportion of employees reported that they had been exposed to actual violent events at their workplace. Because the rate of crime has been declining in most developed countries over a long period (Farrell and Brown, 2016), the consequence of the decline is that despite CCTV being very widespread most of the actual recorded footage is never used for anything at all. The question is whether this is a good or a bad thing? On the one hand, the use of CCTV footage is problematic even in the case of crime prevention because surveillance raises a number of privacy related questions. (Macnish, 2012) Michelman (Michelman, 2009) argues that widespread surveillance of citizens in general can produce two kinds of harms: the first is the loss of privacy which is a basic right in modern democratic societies. Being able to move around freely without being caught on camera is a basic right that is threatened by the widespread use of CCTV. The loss of privacy is a harm in itself but it can also lead to other harms such as discrimination. Discrimination can occur when certain groups of people are singled out for surveillance because they are perceived to be more likely to commit crimes. This is a problem in particular when the surveillance is biased and the groups of people that are singled out are not actually more likely to commit crimes than other groups. The second harm that (Michelman, 2009) identifies is called ’chilling effects’ where people are discouraged from engaging in certain activities because they fear that they are being watched. Criminals being deterred from committing crimes is a beneficial outcome of such ’chilling effects’ of CCTV surveillance. However, if the surveillance is perceived to be biased or unfair it might lead to certain groups of people refraining from going to the shop because they fear being singled out as potential criminals e.g. in the case where a theft is committed in the shop and the police is called to investigate. For this reason, Macnish (Macnish, 2012) argues that a semi-automated (computational) approach to the processing of CCTV footage (e.g. object detection) is preferable to one that is solely based on human operators as it reduces the risk of human bias and discrimination. In addition, such an approach is more efficient as the human operator can only process a limited amount of footage at a time. When processing CCTV footage manually the limited ’processing capacity’ of a human operator risks leading to the use of profiling and the influence of personal prejudices increases the risk of surveillance leading to stigmatisation and harassment of certain groups of people. Despite efforts to reduce negative effects of CCTV surveillance, there are still a number of ethical issues related to this and other forms of surveillance both in public and private settings which suggests that the use of CCTV should be limited as much as possible. In other words, the ’non-use’ of CCTV footage is in itself a good thing because it reduces the risk of negative effects of surveillance. On the other hand, in the case where CCTV footage is being recorded and stored anyway, it could be argued that the ’non-use’ of the footage also carries problematic ethical issues (Jones et al., 2017), such as the historical examples where companies or institutions decided to ignore information that was available to them because they did not want to act on it. We are reminded of this when it comes to e.g. harmful effects of fossil fuels, tobacco use and asbestos where companies selling these products knew about the harmful effects but chose to ignore them in order to maximize profits (Shearer, 2015). In the case of CCTV footage, the footage is being recorded and stored for the purpose of preventing in particular theft, i.e. primarily for the benefit of the company. However, the data could also be used for other purposes: it could for instance be used to advice customers on not to visit the shop at certain times of the day because the shop is too crowded which could be beneficial for the customers but also for the employees. Choosing not to use the data for these purposes could be seen as a missed opportunity to improve the work environment for the employees and the shopping experience for the customers. In this paper, we argue for the use of CCTV footage to estimate aspects of the psychosocial work environment and present a proof of concept demonstrating how this can be achieved using a combination of existing object detection and computer vision algorithms. The psychosocial work environment is an important factor for employees health and well-being (Stansfeld and Candy, 2006). The global burden of work related accidents and diseases is substantial and is estimated to be 4% of the global GDP and 2.7% in Denmark (Takala et al., 2014). Although work accidents and occupational diseases related to e.g. exposures to harmful substances and physical work environment factors are more common, the psychosocial work environment is becoming an increasingly important factor for employees health and well-being as the labour markets of highly developed countries such as Denmark are becoming more and more service oriented. For this reason, reducing adverse working conditions is important for society due to the costs associated with adverse work environment, e.g. prolonged sick leave and early retirement. At the same time a safe and healthy work environment is associated with increased productivity and reduced turnover which are important goals for companies. From the perspective of the individual, psychosocial work environment factors are important for a number of reasons not least because being outside the labour market has profound negative effects on the quality of life and longevity. Being able to estimate aspects of the psychosocial work environment using CCTV footage thus constitutes a way of using information that is already being collected for other purposes in a way that is beneficial for society and for the individual employees without incurring additional ethical problems that are not already present in the current limited use of CCTV footage for crime prevention. The contribution of this paper is thus to provide a proof of concept that demonstrates how the use of object detection algorithms can be extended to estimate important aspects of the psychosocial work environment using CCTV footage. The contribution of this paper can thus be summarised as follows: • Our paper proposes an implementation of object detection algorithms as a way of estimating quantitative job demands in the psychosocial work environment using CCTV footage. • We propose a set of metrics that can serve as proxy measures for important aspects of the psychosocial work environment. • The methodology is tested on a dataset of CCTV footage establishing the feasibility of our approach. Figure 1. Example footage of an interaction between employee and customer in CCTV footage We have structured the paper as follows: Section 2: reviews related work on computer vision algorithms for detecting and tracking people in video footage, as well as recognising human activities using such footage. We also review how psychosocial work environment factors have been measured in previous studies and how the use of CCTV footage could provide a way of estimating some of these factors. Section 3: introduces the concepts and notation used in the paper. Section 4: describes the overall system architecture and methodology used to analyse the CCTV footage. The approach used combines several existing computer vision algorithms to detect and track people in the footage as well as to estimate the interactions between customers and employees. Section 5: presents the results of our experiments based on CCTV footage demonstrating the feasibility of our proposed method. Section 6: discusses the results and points to future work that needs to be carried out in order to validate the method further. We discuss the limitations of the method and how they can be addressed in future work."
https://arxiv.org/html/2411.03717v1,These Maps Are Made by Propagation:Adapting Deep Stereo Networks to Road Scenarios with Decisive Disparity Diffusion,"Stereo matching has emerged as a cost-effective solution for road surface 3D reconstruction, garnering significant attention towards improving both computational efficiency and accuracy. This article introduces decisive disparity diffusion (D3Stereo), marking the first exploration of dense deep feature matching that adapts pre-trained deep convolutional neural networks (DCNNs) to previously unseen road scenarios. A pyramid of cost volumes is initially created using various levels of learned representations. Subsequently, a novel recursive bilateral filtering algorithm is employed to aggregate these costs. A key innovation of D3Stereo lies in its alternating decisive disparity diffusion strategy, wherein intra-scale diffusion is employed to complete sparse disparity images, while inter-scale inheritance provides valuable prior information for higher resolutions. Extensive experiments conducted on our created UDTIRI-Stereo and Stereo-Road datasets underscore the effectiveness of D3Stereo strategy in adapting pre-trained DCNNs and its superior performance compared to all other explicit programming-based algorithms designed specifically for road surface 3D reconstruction. Additional experiments conducted on the Middlebury dataset with backbone DCNNs pre-trained on the ImageNet database further validate the versatility of D3Stereo strategy in tackling general stereo matching problems.","Ensuring safe and comfortable driving requires the timely assessment of road conditions and the prompt repair of road defects [1]. With an increasing emphasis on maintaining high-quality road conditions [2], the demand for automated 3D road data acquisition systems has grown more intense than ever [3, 4]. The study presented in [5] employs a laser scanner to collect high-precision 3D road data. Nevertheless, the high equipment costs and the long-term maintenance expenses have limited the widespread adoption of such laser scanner-based systems [6]. Therefore, stereo vision, a process similar to human binocular vision that provides depth perception using dual cameras, has emerged as a practical and cost-effective alternative for accurate 3D road data acquisition [7, 8]. Existing stereo matching approaches are either explicit programming-based or data-driven. The former ones rely on hand-crafted feature extraction and estimate disparities through local block matching or global energy minimization [9]. Nonetheless, hand-crafted feature extraction faces challenges in handling varying lighting conditions and noises. With recent advances in deep learning, researchers have resorted to deep convolutional neural networks (DCNNs) for stereo matching [10, 11]. These data-driven approaches can learn abstract features directly from input stereo images, making them increasingly favored in this research domain. Unfortunately, the limited availability of well-annotated road disparity data restrains the transfer learning of these DCNNs [12]. Therefore, explicitly programming-based stereo matching approaches [7, 13, 14] remain the mainstream in the field of road surface 3D reconstruction. Building upon the local coherence constraint [15], seed-and-grow stereo matching algorithms [16, 17, 7] have been widely utilized for quasi-dense disparity estimation. Given that road disparities change gradually across continuous regions, our previously published road surface 3D reconstruction algorithm search range propagation (SRP) [7] initializes disparity seeds using a winner-take-all (WTA) strategy at the bottom row of the image and estimates disparities iteratively with the search range propagated from three neighboring seeds. Another significant contribution of [7] lies in the perspective transformation (PT), designed to convert the target view of road image into a reference view. This transformation helps decrease computations by reducing the disparity search range and improving stereo matching accuracy by increasing the similarity of the compared blocks. While the combination of SRP and PT yields a remarkable 3D geometry reconstruction accuracy of approximately 3 mm, it is noteworthy that the disparity estimation accuracy remains constrained by the reliability of the initial seeds generated using the simple WTA strategy. The unidirectional disparity propagation process further leads to disparity estimation errors on discontinuities, such as road defects. Additionally, both seed-and-grow stereo matching and perspective transformation require a set of sparse yet reliable initial correspondences, and the density and reliability of these correspondences directly affect the efficiency and accuracy of the seed-growing process. Drawing inspiration from recent advances in plug-and-play sparse correspondence matching [18, 19] approaches, we propose a feasible solution to address these limitations. For example, the deep feature matching (DFM) method [18] utilizes a backbone DCNN pre-trained on the ImageNet database [20] to extract feature pyramids for both views, and subsequently refines the coarse correspondences initialized at the deepest feature layer to former layers following a linear hierarchical manner. These methods have demonstrated the effectiveness of using deep features provided by pre-trained backbones to solve the correspondence matching task. Therefore, our primary motivation is to develop a dense deep feature matching strategy by improving the seed-and-grow stereo matching with the hierarchical refinement strategy in DFM. Leveraging accurate sparse correspondences as disparity seeds, such a dense deep feature matching strategy exhibits compatibility with perspective transformation, thus leading to improvements in both stereo matching accuracy and efficiency compared with the combination of SRP and PT. However, directly incorporating a hierarchical refinement strategy into seed-and-grow stereo matching still has the following limitations: • The dense matching process in the stereo matching task requires additional matching noise elimination operations in challenging areas with weak/repetitive textures. • Additional efforts in eliminating inaccurate sparse correspondences are required to mitigate error accumulation and propagation in the seed-growing process. • The linear hierarchical refinement strategy in DFM is designed to enhance the spatial details of the coarse initial correspondences, while having limited effectiveness in enhancing their density. To address these limitations, we propose a plug-and-play stereo matching strategy for road surface 3D reconstruction, referred to as Decisive Disparity Diffusion Stereo (D3Stereo), serving as the first exploration of dense deep feature matching. D3Stereo is compatible with any hand-crafted feature extraction approaches, stereo matching networks pre-trained on other public datasets, and even backbone DCNNs pre-trained for image classification. We first propose the recursive bilateral filtering (RBF) algorithm, a more efficient alternative to traditional bilateral filtering (BF) [13] for matching cost aggregation. By recursively applying a small filtering kernel, our BRF achieves a significantly expanded receptive field while maintaining the same computational cost as BF, thereby gathering more context information for cost aggregation. The proposed method leverages the powerful semantic feature extraction ability of a pre-trained DCNN backbone in a hierarchical manner. It consists of two algorithms that diffuse decisive disparities at both intra and inter scales, respectively. With a cost volume pyramid built with different layers of feature maps, we first find coarse decisive disparities at the deepest layer. Then, the coarse decisive disparities are adversarially diffused to their neighboring pixels in the same layer to yield a dense disparity map, within which reliable decisive disparities are inherited into the former layer by checking the matching cost local minima consistency between consecutive layers. Our adversarial disparity diffusion process and novel disparity inheritance strategy help in eliminating the inaccurate correspondences initialized at the last layer. Afterwards, the derived refinement results activate the decisive disparity intra and inter scale diffusion in the former layer. This process is repeated until a dense disparity map is obtained at the finest resolution layer. In general, the combined usage of diffusing decisive disparities at both intra and inter scales fully exploits the semantic information at different scales of feature maps and thus obtaining improved disparity seeds in terms of both accuracy and distribution uniformity compared with the hierarchical refinement strategy in DFM [18]. Additionally, we create a synthetic road dataset called the UDTIRI-Stereo dataset using the CARLA simulator [21] for disparity estimation evaluation. Although collecting datasets using simulators has emerged as a prevalent alternative for real-world datasets [22, 23], these simulators model the road surface as a ground plane, thereby significantly reducing the complexity of the stereo matching task. In order to narrow the domain gap between the idealized road surface in CARLA and the real-world road surface, we originally augment the road surface mesh model in CARLA with 1) 2D Perlin noise and 2) digital twins of pothole models. By applying linear interpolation between initial random noises, 2D Perlin noise is utilized to generate the natural undulations of the real-world road surface. Moreover, digital twins of pothole models yielded in real-world [6] are randomly transplanted onto the road surface, thus further introducing disparity discontinuities into the UDTIRI-Stereo dataset."
https://arxiv.org/html/2411.03714v1,Explaining Human Activity Recognition with SHAP: Validating Insights with Perturbation and Quantitative Measures,"In Human Activity Recognition (HAR), understanding the intricacy of body movements within high-risk applications is essential. This study uses SHapley Additive exPlanations (SHAP) to explain the decision-making process of Graph Convolution Networks (GCNs) when classifying activities with skeleton data. We employ SHAP to explain two real-world datasets: one for cerebral palsy (CP) classification and the widely used NTU RGB+D 60 action recognition dataset. To test the explanation, we introduce a novel perturbation approach that modifies the model’s edge importance matrix, allowing us to evaluate the impact of specific body key points on prediction outcomes. To assess the fidelity of our explanations, we employ informed perturbation, targeting body key points identified as important by SHAP and comparing them against random perturbation as a control condition. This perturbation enables a judgment on whether the body key points are truly influential or non-influential based on the SHAP values. Results on both datasets show that body key points identified as important through SHAP have the largest influence on the accuracy, specificity, and sensitivity metrics. Our findings highlight that SHAP can provide granular insights into the input feature contribution to the prediction outcome of GCNs in HAR tasks. This demonstrates the potential for more interpretable and trustworthy models in high-stakes applications like healthcare or rehabilitation.","In recent years, there has been a notable expansion in the field of developing Graph Convolution Networks (GCNs) for Human Activity Recognition (HAR). Nevertheless, the principal objective of researchers has been to enhance the utilized GCN architectures, particularly on benchmark datasets such as NTU RGB+D 60/120 [shahroudyNTURGBLarge2016, liuNTURGB1202020], and Kinetics [kayKineticsHumanAction2017]. Because of this, exploring the underlying mechanisms of the developed GCN models has been largely neglected. At the same time, these models’ increasing complexity and size have led to a lack of interpretability, effectively turning them into black boxes. Consequently, there is a conspicuous research gap between examining the accuracy and interpretability of GCNs in the HAR domain, limiting the applicability of these methods in high-risk fields such as medical and clinical decision-making [bharatiReviewExplainableArtificial2024, chaddadSurveyExplainableAI2023]. To address this issue, methods within the field of Explainable Artificial Intelligence (XAI) could be employed to shed light on the internal mechanisms of GCNs, thereby fostering trust and providing informed decision-making foundations for the users of such systems. This is particularly crucial within the medical domain and other high-risk areas, where understanding the reasoning behind AI-generated outcomes is vital for users to make informed decisions [bharatiReviewExplainableArtificial2024, aliExplainableArtificialIntelligence2023, nazirSurveyExplainableArtificial2023, tjoaSurveyExplainableArtificial2021]. Therefore, meaningful explanations provided by - or alongside the predictions of - GCN models must be transparent and comprehensible for the users to accept and safely use such systems within their respective environment [chaddadSurveyExplainableAI2023, saeedExplainableAIXAI2023]. Most XAI methods for GCNs rely on gradient-based explanation concepts to interpret their models [dasGradientWeightedClassActivation2022, Song2022ConstructingSA, pellanoMovementsMetricsEvaluating2024]. These methods are currently preferred due to their direct correlation with the skeleton body key points, allowing the identification of the specific body parts important for the model’s prediction. However, while such approaches effectively highlight body key points, they fail to capture the nuanced contributions of other essential input features - such as position, velocity, acceleration, and segment orientation - vital for a deeper understanding. SHapley Additive exPlanations (SHAP) [lundbergUnifiedApproachInterpreting2017] can overcome this fundamental shortcoming of gradient-based methods as it provides a direct relationship between the different input features and model predictions, offering a more comprehensive understanding of how each input feature contributes to the final prediction of the GCNs. This level of detailed insight is not possible with gradient-based methods, which typically provide only a prevailing sense of the general importance of the features by probing the overall gradient at the last network layer. Conversely, SHAP assigns an exact contribution value to each feature for a given prediction, facilitating a more granular and interpretable understanding of how each feature influences the model’s output. Although SHAP exhibits substantial potential compared to gradient-based methods, it has yet to be implemented in the context of GCNs within HAR. This gap presents an opportunity to enhance the accuracy and granularity of model interpretation. The comprehensive explanations provided by SHAP have the potential to make it an effective method for interpreting HAR models. While XAI plays a pivotal role in making machine learning (ML) more understandable, trustworthy, and accessible, commonly used methods often rely heavily on visual, qualitative validation methods, which is also the case for the few endeavors within the HAR domain. In many cases, researchers tend to cherry-pick results that support their stated claim, emphasizing the critical need for comprehensive and standardized evaluation frameworks for XAI methodologies [leavittFalsifiableInterpretabilityResearch2020, millerExplanationArtificialIntelligence2019]. This reliance highlights the need for quantitative metrics to objectively evaluate explanation methods beyond what visual and qualitative assessments alone can deliver [nautaAnecdotalEvidenceQuantitative2023]. Since single performance metrics such as a high accuracy score do not provide insights into how and why an ML model makes its predictions, the interpretability of a complex ML system cannot be reduced to one simple metric, necessitating a multifaceted perspective instead [nautaAnecdotalEvidenceQuantitative2023, doshi-velezRigorousScienceInterpretable2017, molnarInterpretableMachineLearning2022]. A prominent method for evaluating XAI methods is the perturbation-based approach [fisherAllModelsAre2019, ivanovsPerturbationbasedMethodsExplaining2021]. In essence, by observing how sensitive a model’s predictions are to changes in particular inputs, one can gain insight into which features are most important or influential for decision-making. This method allows for an investigation of how modifications to input features impact the model’s predictions, thus enabling an examination of the model’s explanation. This technique is powerful due to its intuitive nature and broad applicability across various ML models, rendering it a straightforward and effective technique for XAI testing. Despite its potential, this approach has not yet been explored within the HAR domain, and there is a conspicuous absence of perturbation methods that provide relevant insights. Within HAR, where understanding the model’s reasoning process can be crucial for interpreting predictions related to physical motion, the lack of tailored perturbation methods suggests an opportunity for further research and development. Extending perturbation techniques to HAR at the model level, where the model is treated as a dynamic entity rather than a static, unchangeable component, could yield more meaningful insights from explanation methods such as SHAP. Such an approach can enhance the transparency and reliability of the model’s decision-making processes. 1.1 Contributions In this paper, we present several novel contributions addressing the gaps mentioned earlier within current HAR and XAI research: • First work applying Shapley value-based explanation to a GCN model for human skeleton data on the primary input features, encapsulated within a new algorithmic framework named ShapGCN. • Introduction of a novel perturbation approach tailored to GCNs within the domain of HAR to test the explanation. • Apply quantitative metrics to validate the explanations with the perturbation on two real-world HAR datasets. These contributions collectively advance the understanding of HAR by bridging the gap between model performance and interpretability, offering valuable insights for researchers and practitioners in the respective HAR domain."
https://arxiv.org/html/2411.03706v1,3DGS-CD: 3D Gaussian Splatting-based Change Detection for Physical Object Rearrangement,"We present 3DGS-CD, the first 3D Gaussian Splatting (3DGS)-based method for detecting physical object rearrangements in 3D scenes. Our approach estimates 3D object-level changes by comparing two sets of unaligned images taken at different times. Leveraging 3DGS’s novel view rendering and EfficientSAM’s zero-shot segmentation capabilities, we detect 2D object-level changes, which are then associated and fused across views to estimate 3D changes. Our method can detect changes in cluttered environments using sparse post-change images within as little as 18s, using as few as a single new image. It does not rely on depth input, user instructions, object classes, or object models – An object is recognized simply if it has been re-arranged. Our approach is evaluated on both public and self-collected real-world datasets, achieving up to 14% higher accuracy and three orders of magnitude faster performance compared to the state-of-the-art radiance-field-based change detection method. This significant performance boost enables a broad range of downstream applications, where we highlight three key use cases: object reconstruction, robot workspace reset, and 3DGS model update. Our code and data will be made available at https://github.com/520xyxyzq/3DGS-CD.","I INTRODUCTION 3D change detection involves identifying changed objects or regions in the environment from two sets of local observations taken at different times or from new observations of a previously modeled scene. It is a critical task in robotics as it can accommodate not only short-term, fully observed scene dynamics but also long-term scene changes where the transition process is typically unobserved. Such long-term changes are particularly important for robot operations, as environmental changes often occur without being fully noticed or monitored. Despite the success of 3D change detection with depth input using scene representations such as TSDF [1], 3D point cloud [2] and neural descriptor fields [3], detecting changes from multi-view RGB images remains a challenging problem. Traditional methods have relied on hand-crafted techniques like voxelization [4], multi-view stereo [5], and image warping [6] to identify changes in unaligned images and lift to 3D. These approaches are particularly sensitive to occlusions and lighting variations, especially when there are large viewpoint differences between the two sets of images. The emergence of radiance field models, such as neural radiance fields (NeRF) [7] and 3DGS [8], presents new opportunities to address these challenges. These models provide high-fidelity representations of scene geometry and appearance, with novel-view rendering capabilities that enable the generation of photo-realistic images and dense depths at arbitrary viewpoints. This allows for direct comparison of pre- and post-change images from the same viewpoint. NeRF-based solutions have been explored to a limited extent [9, 10]. However, these methods are constrained by the computational cost of NeRF’s ray-casting-based rendering. In contrast, 3DGS offers a more efficient alternative, achieving real-time rendering with comparable or even superior quality. In our work, we leverage 3DGS as scene representation to identify 3D scene changes including object removal, insertion and movement from multi-view images. We exploit the zero-shot segmentation capability of EfficientSAM [11] to compare pre- and post-change images at the same viewpoints, associating and fusing the 2D object changes to obtain accurate 3D object segments and pose changes. Our method has the following key advantages: (1) It can handle sparse post-change image inputs, requiring as few as a single new image to detect 3D changes. (2) It requires no depth sensors or monocular depth estimators. (3) It does not rely on pre-defined object classes, models or object detectors – An object is recognized simply if it has been moved, removed or inserted. (4) It requires no user instructions such as user-provided click or language prompts. Our method is evaluated on both public and self-collected real-world datasets, achieving up to 14% higher accuracy and three magnitudes faster performance compared to the state-of-the-art NeRF-based method. This significant performance improvement enables a wide range of real-world applications, including: (1) object removal as a prompt for object reconstruction (Sec. VI-A); (2) robot workspace reset (Sec. VI-B); (3) 3DGS model update (Sec. VI-C). Figure 1: Our method detects 3D object-level changes from pre- and potentially sparse post-change images of a 3D scene. We first train a 3DGS model on pre-change images (Sec.IV-A), localize the post-change cameras with respect to this model (Sec.IV-B), and render RGB-D images at post-change views for 2D change detection using EfficientSAM [11] (Sec.IV-C). The detected 2D object segments are associated across the post-change views (Sec. IV-D) to initialize 3D object templates. These templates are used to classify object change types and query EfficientSAM on pre-change views to obtain additional 2D object segments, which are fused to obtain 3D object segments (Sec. IV-E). For moved objects, we leverage image-template feature correspondences to estimate their 6D pose changes (Sec. IV-F) and refine the estimates by a render-and-compare approach (Sec. IV-G)."
https://arxiv.org/html/2411.03702v1,Graph-Based Multi-Modal Sensor Fusion for Autonomous Driving,"The growing demand for robust scene understanding in mobile robotics and autonomous driving has highlighted the importance of integrating multiple sensing modalities. By combining data from diverse sensors like cameras and LIDARs, fusion techniques can overcome the limitations of individual sensors, enabling a more complete and accurate perception of the environment. We introduce a novel approach to multi-modal sensor fusion, focusing on developing a graph-based state representation that supports critical decision-making processes in autonomous driving. We present a Sensor-Agnostic Graph-Aware Kalman Filter (Sani et al., 2024), the first online state estimation technique designed to fuse multi-modal graphs derived from noisy multi-sensor data. The estimated graph-based state representations serve as a foundation for advanced applications like Multi-Object Tracking (MOT), offering a comprehensive framework for enhancing the situational awareness and safety of autonomous systems. We validate the effectiveness of our proposed framework through extensive experiments conducted on both synthetic and real-world driving datasets (nuScenes). Our results showcase an improvement in MOTA and a reduction in estimated position errors (MOTP) and identity switches (IDS) for tracked objects using the SAGA-KF. Furthermore, we highlight the capability of such a framework to develop methods that can leverage heterogeneous information (like semantic objects and geometric structures) from various sensing modalities, enabling a more holistic approach to scene understanding and enhancing the safety and effectiveness of autonomous systems.","In applications of mobile robotics and autonomous driving, the integration of various sensing modalities via multi-modal sensor fusion has become paramount for achieving comprehensive scene understanding that enables effective decision-making. Similar to any other multi-sensor fusion technique, our objective is to leverage the complementarity of the different types of sensors to enable an improved, more holistic view of the environment. For a mobile robot or an autonomous driving (AD) agent, the typical example is that of using a camera and a LIDAR. We present a sensor fusion approach that utilizes cameras and LIDARs mounted on an AD vehicle and aims to build holistic scene representations that facilitate downstream decision-making. Moreover, our proposal also relies on the observation that an AD agent needs both semantic and geometric information about its environment (scene) for decision-making. For instance, in Figure 1, say the light-blue car (A) in the left lane has a right-of-way at the lane merge ahead. However, the apparent violation of its right-of-way by other agents (B) should be accounted for by its path planner, which in turn would need semantic information, such as the types of vehicle and their behavior in its environment, in addition to the geometric information of the road, lanes and position and velocity of other agents and also the topological information representing the correlation between different entities in the scene. A resulting plan (e.g., lowering speed or stopping) that ensures safety needs a holistic understanding of a dynamic environment that can be achieved by effectively processing the multi-modal sensory data to develop appropriate representations that aid decision-making. Figure 1. How should car ‘A’ take into account the temporary geometry of the scene formed by cones in order to ensure safety and to avoid any collision with ‘B’? In this extended abstract, we present a Sensor-Agnostic Graph-Aware Kalman Filter (Sani et al., 2024) and propose to develop a graph-based dynamic scene representation that permits us to capture heterogeneous information from multiple sensing modalities. Using the AD use case, we plan to develop methods for processing dynamic scene graphs that capture semantic (type of vehicle, traffic light, etc.) and geometric (road planes, lane boundary curve, etc.) information as nodes and their pairwise relationships as edges. Our methods will enable inferences drawn using this dynamic scene graph representation with two key applications in AD: Multi-Object Tracking (MOT) and Simultaneous Localization and Mapping (SLAM). Figure 2. The observed scene graph is constructed using SOTA detectors and is fed into the SAGA-KF framework to predict, associate and estimate the state of the scene graph."
https://arxiv.org/html/2411.03696v1,OccLoff: Learning Optimized Feature Fusion for 3D Occupancy Prediction,"3D semantic occupancy prediction is crucial for finely representing the surrounding environment, which is essential for ensuring the safety in autonomous driving. Existing fusion-based occupancy methods typically involve performing a 2D-to-3D view transformation on image features, followed by computationally intensive 3D operations to fuse these with LiDAR features, leading to high computational costs and reduced accuracy. Moreover, current research on occupancy prediction predominantly focuses on designing specific network architectures, often tailored to particular models, with limited attention given to the more fundamental aspect of semantic feature learning. This gap hinders the development of more transferable methods that could enhance the performance of various occupancy models. To address these challenges, we propose OccLoff, a framework that Learns to Optimize Feature Fusion for 3D occupancy prediction. Specifically, we introduce a sparse fusion encoder with entropy masks that directly fuses 3D and 2D features, improving model accuracy while reducing computational overhead. Additionally, we propose a transferable proxy-based loss function and an adaptive hard sample weighting algorithm, which enhance the performance of several state-of-the-art methods. Extensive evaluations on the nuScenes and SemanticKITTI benchmarks demonstrate the superiority of our framework, and ablation studies confirm the effectiveness of each proposed module.","Fine-grained perception of 3D scenes is crucial for ensuring the safety of autonomous vehicles. Unlike traditional 3D object detection methods [5, 10, 33, 23] and Bird’s Eye View (BEV)-based perception approaches [20, 49, 35, 23], which often oversimplify the shapes of objects, 3D semantic occupancy prediction [43, 40, 44, 48, 52] assigns semantic labels to every voxel in the 3D space, enabling precise perception of various irregular objects. Given that fine-grained perception better reflects the complexities of real-world driving scenarios, occupancy-based perception has emerged as a highly promising research direction. Recent studies [43, 29, 31, 42] have demonstrated that leveraging the complementary characteristics of multiple sensor types, such as camera and LiDAR, for multi-modal feature fusion can substantially improve the performance of occupancy prediction. Specifically, cameras provide rich semantic information, whereas LiDARs offer precise spatial localization of objects. Despite these complementary strengths, the inherent heterogeneity between these two modalities presents a significant challenge for achieving efficient fusion. Drawing inspiration from recent advancements in camera-based perception [35, 4, 27, 37], most existing multi-modal approaches [43, 31, 42, 29] perform a 2D-to-3D view transformation on image features, followed by computationally intensive 3D operations—such as 3D convolutions [43, 29, 42] or KNN search [31]—to fuse these features with 3D LiDAR features. Although this approach is conceptually straightforward, it is both computationally costly and constrained by the robustness of the view transformation process [20, 44]. On the other hand, existing research on occupancy-based perception primarily focuses on the detailed design of network architectures, with little attention given to the more fundamental issue of learning better feature representations for occupancy prediction. Although adjustments to network details can improve prediction performance, we argue that many techniques, which are closely tied to specific network structures, often suffer from limited transferability. Furthermore, most of the module designs in current occupancy-based approaches are either directly borrowed or minimally adapted from prior BEV perception research [21, 25, 13, 12, 20, 35], primarily involving the extension of 2D operations to 3D. These studies have not sufficiently explored how the inherent fine-grained nature of occupancy perception distinguishes it from BEV perception, and as a result, they have yet to fully unlocked the potential of occupancy-based perception. Figure 1: A comparison of the impact of two different image feature processing paradigms on multi-modal fusion. The lifting paradigm requires additional modules to lift image features into 3D space before fusing them through 3D operations, which leads to high computational costs and can introduce additional noise (e.g., errors from depth estimation). In contrast, the querying from 3D to 2D approach performs feature fusion in a single step, making it more robust (see Sec. 3.1 for details). To address these challenges, we propose OccLoff, an occupancy prediction framework designed to facilitate more efficient multi-modal feature fusion. Rather than relying on the commonly employed lifting paradigm [35], we adopt an alternative strategy frequently used in camera-based perception—querying from 3D to 2D [19, 44, 20, 46]—to enable direct fusion of camera and LiDAR data. Specifically, we introduce an efficient sparse fusion encoder based on entropy masks, which leverages the prior knowledge derived from the geometric structure of point clouds to assist in feature fusion for challenging regions. Additionally, we enhance the fused data by applying sparse convolutions to multi-modal features in these difficult areas. Following the multi-modal fusion process, we incorporate multi-frame temporal information to improve robustness, and leverage CONet [43] to extract fine-grained features while minimizing computational overhead. Additionally, inspired by research in fine-grained image analysis [47, 8, 6] and metric learning [30], we introduce an occupancy proxy loss to learn more distinctive occupancy features, which significantly mitigates the class imbalance problem in occupancy semantics. Furthermore, we design an Adaptive Hard Sample Weighting (AHSW) mechanism to help the model learn more effectively from complex scenes. Our approach achieves outstanding results on the nuScenes [43, 40, 3] and SemanticKITTI [1] benchmarks, notably improving the perception of small objects. Moreover, experiments demonstrate that our proposed learning methods are transferable and can consistently enhance the performance of other state-of-the-art occupancy prediction models. In summary, our contributions are threefold: (1) We propose an efficient and powerful sparse fusion encoder that better fuses LiDAR and camera features. (2) We introduce two transferable learning-based methods, the occupancy proxy loss and adaptive hard sample weighting. (3) Our framework demonstrates superior performance on the nuScenes and SemanticKITTI benchmarks."
https://arxiv.org/html/2411.03695v2,AMNCutter: Affinity-Attention-Guided Multi-View Normalized Cutter for Unsupervised Surgical Instrument Segmentation,"Surgical instrument segmentation (SIS) is pivotal for robotic-assisted minimally invasive surgery, assisting surgeons by identifying surgical instruments in endoscopic video frames. Recent unsupervised surgical instrument segmentation (USIS) methods primarily rely on pseudo-labels derived from low-level features such as color and optical flow, but these methods show limited effectiveness and generalizability in complex and unseen endoscopic scenarios. In this work, we propose a label-free unsupervised model featuring a novel module named Multi-View Normalized Cutter (m-NCutter). Different from previous USIS works, our model is trained using a graph-cutting loss function that leverages patch affinities for supervision, eliminating the need for pseudo-labels. The framework adaptively determines which affinities from which levels should be prioritized. Therefore, the low- and high-level features and their affinities are effectively integrated to train a label-free unsupervised model, showing superior effectiveness and generalization ability. We conduct comprehensive experiments across multiple SIS datasets to validate our approach’s state-of-the-art (SOTA) performance, robustness, and exceptional potential as a pre-trained model. Our code is released at https://github.com/MingyuShengSMY/AMNCutter.","Minimally invasive surgery (MIS) has greatly improved patient experiences, including reduced pain, lower risk of infection, and shorter hospitalization period, owing to the smaller incisions compared to common open surgery [35, 34]. Despite its advantages, surgeons face new challenges due to the complex in vivo scenes and low-quality endoscopic frames, which are adversely affected by misting, noise, and a narrow field of view [48]. To address these challenges and support surgeons, robotic assistance has been applied to a range of tasks, such as surgical workflow segmentation and surgical instrument segmentation [63, 51, 64, 42, 65]. Surgical instrument segmentation (SIS) aids surgeons by displaying distinguishable object masks for every surgical video frame. The SIS technique has evolved from traditional machine learning methods, such as Support Vector Machines, to advanced deep learning approaches, including CNNs and Transformers [6, 45, 26, 55, 32]. However, the supervised SIS methods require extensive annotated data, which is labor-intensive, time-consuming, and highly expertise-demanding. This leads to restrictive data diversity, which hinders model performance on unseen datasets and real-world applications, resulting in weak robustness. In contrast, unsupervised learning allows the model to be trained without manual annotations and to serve as a pre-trained model for other downstream tasks [61, 20, 12, 11, 14, 30, 31]. Therefore, developing unsupervised SIS methods is a significant and valuable open problem in the SIS field. In recent years, unsupervised surgical instrument segmentation (USIS) has been preliminarily explored in several studies [46, 29, 50], with the pseudo-label technique playing a crucial role in transforming the unsupervised task into a pseudo-supervised one. In SOTA USIS studies, pseudo-labels are primarily derived from low-level image information (e.g., color and optical flow). These methods face several limitations, such as reduced robustness, limited effectiveness in complex surgical scenes, and an inability to handle multi-class segmentation. This is because pseudo-labels handcrafted from low-level features like color and optical flow sometimes struggle to differentiate between surgical instruments and patients’ tissues, especially as the complexity of endoscopic video frames increases. For example, a tissue wall with intense light reflection may appear similar in color to metal surgical instruments, and incised tissues that move with the instruments may make optical flow inadequate for distinguishing them [60]. For label-free methods in the field of general image segmentation, feature extraction followed by pixel- or patch-wise clustering is a prevalent strategy [36, 59, 56, 18]. However, this approach is non-end-to-end, low-parameterized, and overly focused on the deepest features. It leads to relatively weak learning ability due to separate optimization and overlooks meaningful low-level object information (e.g., shape, color, and texture) presented in shallower feature maps [33], which drives our work. In addition, the limited exploration of label-free approaches in the USIS domain further motivates this study. Inspired by the recent graph-cutting-based segmentation methods [36, 59, 9, 53], we propose a fully label-free USIS method named Affinity-Attention-Guided Multi-View Normalized Cutter (AMNCutter). Diverging from the non-end-to-end methods that rely solely on the deepest feature map extracted from an image encoder, our approach fully parameterizes the graph-cutting process and incorporates multi-level patch affinities through a novel module termed Multi-View Normalized Cutter (m-NCutter). This multi-level/view module captures the semantic information and affinities from different levels, such as color sameness, shape likeness, and function proximity. Additionally, we introduce a novel attention block named Multi-View Self-Attention, which adaptively assigns attention scores to affinities, guiding the model in prioritizing certain affinities at specific levels. For example, the model can dynamically determine whether the color affinity (low-level) or function similarity (high-level) is more crucial for segmentation. In contrast to existing pseudo-label-based USIS works [46, 29, 50] and label-free segmentation studies like [36, 59, 56, 18, 53], our method does not rely on pre-crafted pseudo-labels and operates within an end-to-end framework, demonstrating superior performance, robustness, and real-time capability, which are validated through comprehensive experiments across diverse datasets. In addition, its potential for use as a pre-trained model in downstream tasks has been confirmed through transfer learning experiments. Our primary contributions are summarized as follows: • We propose an end-to-end USIS method called AMNCutter, requiring neither ground truth nor pseudo-labels for supervision. • We devise a novel Multi-View Normalized Cutter (m-NCutter) optimized by a graph-cutting loss Normalized Cut Loss (NCut Loss). This architecture allows the model to be trained without any labels, perform prediction in an end-to-end manner, and fully consider the features and their affinities across various levels. • We conduct extensive experiments across various datasets to validate our method’s outstanding performance, robustness, and potential for use as a pre-trained model in supervised segmentation tasks. In Sec. 2, we review related SIS studies. Sec. 3 details our approach. The experimental results and their analysis are presented in Sec. 4, followed by conclusions in Sec. 5. Figure 1: Method Overview. a) Pre-trained Backbone introduced in Sec. 3.1; b) m-NCutter, our novel module presented in Sec. 3.2; c) NCut Loss detailed in Sec. 3.4."
https://arxiv.org/html/2411.03688v1,Where Do We Stand with Implicit Neural Representations? A Technical and Performance Survey,"Implicit Neural Representations (INRs) have emerged as a paradigm in knowledge representation, offering exceptional flexibility and performance across a diverse range of applications. INRs leverage multilayer perceptrons (MLPs) to model data as continuous implicit functions, providing critical advantages such as resolution independence, memory efficiency, and generalisation beyond discretised data structures. Their ability to solve complex inverse problems makes them particularly effective for tasks including audio reconstruction, image representation, 3D object reconstruction, and high-dimensional data synthesis. This survey provides a comprehensive review of state-of-the-art INR methods, introducing a clear taxonomy that categorises them into four key areas: activation functions, position encoding, combined strategies, and network structure optimisation. We rigorously analyse their critical properties—such as full differentiability, smoothness, compactness, and adaptability to varying resolutions—while also examining their strengths and limitations in addressing locality biases and capturing fine details. Our experimental comparison offers new insights into the trade-offs between different approaches, showcasing the capabilities and challenges of the latest INR techniques across various tasks. In addition to identifying areas where current methods excel, we highlight key limitations and potential avenues for improvement, such as developing more expressive activation functions, enhancing positional encoding mechanisms, and improving scalability for complex, high-dimensional data. This survey serves as a roadmap for researchers, offering practical guidance for future exploration in the field of INRs. We aim to foster new methodologies by outlining promising research directions for INRs and applications.","Knowledge representation (Brachman, 2004) is a foundation in computational fields, playing a critical role in enabling systems to efficiently model, interpret, and manipulate information across various domains. Deep neural networks have demonstrated a powerful capacity for learning robust knowledge representation from data, and have become the predominant tools for addressing complex tasks in areas like computer vision (LeCun et al., 2015). The significance of effective knowledge representation extends beyond traditional methods, as it directly influences the performance and scalability of systems when handling diverse types of information such as images, video, and audio, whether in 1D, 2D, or 3D formats. Conventional approaches to encoding input signals typically rely on explicit discretisation, where the input space is segmented into distinct elements, such as point clouds (Achlioptas et al., 2018; Fan et al., 2017), voxel grids (Gadelha et al., 2017; Liao et al., 2018; Stutz & Geiger, 2018; Jimenez Rezende et al., 2016), and meshes (Kanazawa et al., 2018; Ranjan et al., 2018; Wang et al., 2018). While these methods can achieve adequate results, they present significant challenges when dealing with high-dimensional data (Mescheder et al., 2019). The computational cost of discretisation rises sharply with increasing dimensionality, making it inefficient, particularly for complex or irregular spaces. Moreover, traditional discretisation methods tend to require substantial memory, posing limitations for large-scale applications. Implicit Neural Representations (INRs) offer a promising alternative by using continuous functions to represent data, addressing many of the limitations of explicit methods, such as memory inefficiency and the high computational cost associated with discretisation. Figure 1: The four categories of state-of-the-art (SOTA) implicit neural representation (INR) methods. The yellow blocks highlight the specific components each method enhances. Specifically: (a) focuses on improving activation functions, (b) enhances position encoding, (c) integrates both (a) and (b) to simultaneously improve activation functions and position encoding, and (d) advances the overall network structure. The rise and continued development of INR have recently emerged as a new way to learn representation efficiently. Implicit representations differ from explicit (or discrete) representations by encoding information as a continuous generator function, which maps input coordinates to corresponding values within the defined input space, rather than directly storing feature or signal values. Consequently, there has been significant interest in utilising these networks as implicit functions, with notable success (Mildenhall et al., 2021; Mescheder et al., 2019; Xie et al., 2022). Specifically, Multi-Layer Perceptrons (MLPs) are trained to parameterise signals by taking input coordinates by a mapping technique that projects the network input into a higher-dimensional space. It predicts the associated data values. In this framework, the MLP functions as an Implicit Neural Representation, encoding the signal’s information within its weights. For example, when applied to image data, pixel coordinates are fed into the MLP, which generates the corresponding RGB values, effectively learning a continuous, high-resolution representation of the image. However, the classic use of the ReLU activation function often resulted in suboptimal performance across many applications. To address this issue, reparametrised learning techniques (Rahaman et al., 2019) have been first used to adjust the weights and mitigate bias, further enhancing the network’s performance. (Fathony et al., 2020) presented a new architecture where the output of each layer is multiplied by a Gabor wavelet. Further researchers have introduced various activation functions. These include periodic sinusoidal functions (Sitzmann et al., 2020), time-frequency localised Gabor wavelets (Saragadam et al., 2023), Gaussian functions (Ramasinghe & Lucey, 2022), and the FINER network (Liu et al., 2024). Additionally, Trident (Shen et al., 2023) is a network that integrates both positional encoding and a carefully chosen activation function. Implicit neural functions have been further adjusted in various tasks, including image generation (Reddy et al., 2021), super-resolution (Wu et al., 2021; Chen et al., 2021), 3D object reconstruction (Chabra et al., 2020; Mescheder et al., 2019; Mildenhall et al., 2021), and modelling of complex signals (Xu et al., 2022). The use of multi-layer perceptrons (MLPs) for image and shape parameterisation provides distinct advantages. First, MLPs are resolution-independent as they operate within a continuous domain, enabling them to generate values for coordinates beyond pixel- or voxel-based grids. Thus, it improves performance in vision tasks. Second, their memory requirements are not constrained by signal resolution, allowing more memory-efficient representations compared to traditional grid or voxel methods (Huang et al., 2021; Park et al., 2019). The memory demand scales according to the complexity of the signal rather than the resolution. Additionally, MLPs address the limitations of locality biases often found in convolutional neural networks (CNNs), which can hinder generalisation (Chen & Zhang, 2019). Finally, MLP-based models are fully differentiable, offering adaptability across various applications (Zhang et al., 2024; Liu et al., 2020). Their weights can be optimised using gradient-based techniques, providing the flexibility needed for diverse tasks (Zhang et al., 2023; Xie et al., 2022; Tancik et al., 2020; Cheng et al., 2023). Figure 2: A comprehensive comparison of the INR methods, each represented by a method card. The cards outline key properties, including frequency compactness, spatial compactness, adaptability of the methods, and implementation details with the number of hyperparameters. The categories mentioned correspond to those in Figure 1. Despite the significant advancements in Implicit Neural Representations (INRs), there remains a notable gap in the literature that this work aims to address. The objective of this survey is to provide a comprehensive examination of diverse INR methods, offering both an in-depth analysis of their foundational principles and a thorough exploration of their wide-ranging applications. While progress has been made, existing reviews, such as the one by (Molaei et al., 2023) that focuses on medical applications, do not include experimental comparisons across different approaches, nor do they cover the full spectrum of use cases. Moreover, these works primarily address task-specific applications, rather than delving into the underlying technical principles that define INRs. As many of the tasks are built on the same core INR techniques, there is a need for a broader comparison that highlights differences in performance across various methodologies. This survey aims to bridge that gap by delivering an extensive review of INR methodologies and applications, while also offering a performance comparison to reveal their strengths and limitations. By doing so, we provide a more complete understanding of the field and its potential for future advancements. To the best of our knowledge, this is the first survey paper to comprehensively explore both the fundamental and advanced functions of INRs through practically experimental comparisons across various applications, where we considered the SOTA methods listed in Figure 2. Our work serves as a systematic guide and roadmap for researchers, offering new perspectives on the capabilities of INR models. Additionally, we aim to inspire the broader research community to further investigate the potential of INRs across various domains. We believe this paper will help future exploration and innovation, encouraging deeper engagement with INR methodologies. Contributions. This survey introduces a clear taxonomy of existing state-of-the-art (SOTA) INR techniques, organising them into four key categories that represent critical advancements in the field (see Figure 1). First, methods in the activation function category (a) enhance the expressiveness and adaptability of INRs by improving activation functions, resulting in more flexible and capable representations. Notable examples include Siren (Sitzmann et al., 2020), Wire (Saragadam et al., 2023), Gauss (Ramasinghe & Lucey, 2022), Hosc (Serrano et al., 2024), Sinc (Saratchandran et al., 2024), and Finer (Liu et al., 2024), each offering distinct benefits in signal modelling through specialised activations. Second, the position encoding category (b), represented by techniques such as Fourier Features (Mescheder et al., 2019), focuses on refining how positional information is encoded into the model, enhancing the ability to capture fine-grained details in complex signals. Third, methods that combine activation functions and position encoding (c), like Trident (Shen et al., 2023), address both aspects simultaneously, providing a more robust and flexible approach to representation learning. Finally, the network structure category (d), featuring techniques such as Incode (Kazerouni et al., 2024), Mfn (Fathony et al., 2020), and Fr (Shi et al., 2024), focuses on optimising the overall network architecture, incorporating additional components like incoding blocks and filters to enhance learning and generalisation. These four categories collectively define the landscape of current INR research. Our work not only introduces this taxonomy but also further explores these foundational approaches through experimental analysis. Moreover, our survey offers valuable insights into why current methods are effective and highlights the key factors influencing performance trade-offs. In contrast to existing studies, we provide a comprehensive performance comparison across various inverse problem tasks."
https://arxiv.org/html/2411.03670v1,A Large-Scale AI Benchmark for 3D Multi-Organ Segmentation,"How can we test AI performance? This question seems trivial, but it isn’t. Standard benchmarks often have problems such as in-distribution and small-size test sets, oversimplified metrics, unfair comparisons, and short-term outcome pressure. As a consequence, good performance on standard benchmarks does not guarantee success in real-world scenarios. To address these problems, we present Touchstone, a large-scale collaborative segmentation benchmark of 9 types of abdominal organs. This benchmark is based on 5,195 training CT scans from 76 hospitals around the world and 5,903 testing CT scans from 11 additional hospitals. This diverse test set enhances the statistical significance of benchmark results and rigorously evaluates AI algorithms across out-of-distribution scenarios. We invited 14 inventors of 19 AI algorithms to train their algorithms, while our team, as a third party, independently evaluated these algorithms. In addition, we also evaluated pre-existing AI frameworks—which, differing from algorithms, are more flexible and can support different algorithms—including MONAI from NVIDIA, nnU-Net from DKFZ, and numerous other open-source frameworks. We are committed to expanding this benchmark to encourage more innovation of AI algorithms for the medical domain.","1 Introduction The development of AI algorithms has led to enormous progress in medical segmentation, but few algorithms are reliable enough for clinical use [ardila2019end, isensee2021nnu, cao2023large]. Most AI algorithms fall short of expert radiologists, who are much more reliable and consistent when dealing with medical images from multiple hospitals, varied in different scanners, clinical protocols, patient demographics, or disease prevalences [svanera2024fighting, lin2024shortcut, huang2023eval, zhou2022interpreting]. Therefore, the question remains: How can we test medical AI in the diverse scenarios that are encountered by radiologists? Establishing a trustworthy AI benchmark is important but exceptionally challenging, and seldom achieved in the medical domain. Tougher tests, like out-of-distribution evaluation on large, varied datasets, are needed. Standard benchmarks have underlying problems that cause confusion in algorithm comparisons and delay progress. First, in-distribution test sets. In the medical domain, CT scans in the test set often share sources, scanners, and populations with the training set. As a result, AI algorithms may perform well on the test set but generalize poorly to out-of-distribution (OOD) scenarios [geirhos2020shortcut, banerjee2023shortcuts, bassi2024improving, lin2024shortcut, huang2023eval]. For example, Xia et al. [xia2022felix] found that AI algorithms trained on data from Johns Hopkins Hospital (Baltimore, USA) lose accuracy in pancreatic tumor detection when evaluated on CT scans from Heidelberg Medical School (Heidelberg, Germany). Second, small-size test sets. Annotating medical data is expensive and time-consuming, but training AI requires substantial annotated data [park2020annotated, qu2023annotating]. Therefore, most annotated data is used for training, leaving very little assigned for testing. Recent CT datasets such as TotalSegmentator [wasserthal2022totalsegmentator], WORD [luo2021word], and MSD [antonelli2021medical], offered fewer than 100 CT scans for testing. Even a single success or failure can skew results, reducing the statistical power and potentially misleading conclusions. Third, over-simplified metrics. Most standard benchmarks only compare average performance, failing to identify each AI algorithm’s strengths and weaknesses in different scenarios. For instance, one algorithm might excel at segmenting small, circular structures (like the gall bladder) while another performs better on long, tubular ones (such as the aorta). Average performance across many classes can hide these nuances. Fourth, unfair comparisons. Almost every paper reports that the newly ‘proposed AI’ outperforms existing ‘alternative AIs.’ The improvement becomes more significant if alternative AIs are reproduced and evaluated on an unknown training/test split. There are biases in comparison due to asymmetric efforts made in optimizing the proposed and alternative AIs. Many independent studies have reported these comparison biases over the years [isensee2021nnu, isensee2024nnu] but remain unresolved. There is a need to have more widely adopted benchmarks (e.g., challenges) where all AI algorithms are trained by their inventors and evaluated by third parties. Fifth, short-term outcome pressure. Standard benchmarks are often in short-term and non-recurring, requiring a final solution within several months. For example, RSNA 2024 Abdominal Trauma Detection [rsna-2023-abdominal-trauma-detection] only opened for three months for data access and AI development & evaluation. The short-term outcome pressure can discourage new classes of AI algorithms that need considerable time and computational resources for a thorough investigation, as their vanilla versions (e.g., Mamba [gu2023mamba, yu2024mambaout] in early 2024 and Transformers [dosovitskiy2020image] in early 2021) might not outperform all the alternatives judged. The benchmark must have long-term commitment and allowance. To address this AI mismeasurement issue, we present the Touchstone benchmark, an effort towards the objective of creating a fair, large-scale, and widely-adopted medical AI benchmark. Its scale is large, featuring a training set of 5,195 publicly available CT scans from 76 hospitals and a test set of 5,903 CT scans from additional 11 hospitals. Test sets were unknown to the participants of the benchmark. All 11,098 scans are annotated per voxel for 9 anatomical structures. The training set annotations were created by collaboration between AI specialists and radiologists followed by manual revision [qu2023annotating], 5,160 out of 5,903 test scans are proprietary and manually annotated, and the remaining test datasets are publicly available, annotated by AI-radiologist collaboration. As of May 2024, 14 global teams from eight countries have contributed to our benchmark. These teams are known for inventing novel AI algorithms for medical segmentation. In summary, the Touchstone benchmark explores an evaluation philosophy defined by the following five contributions: 1. Evaluating on out-of-distribution data: The JHH test set (Sec. 2.1) presents 5,160 CT scans from an hospital never seen during training, introducing a new scale of external validation for abdominal CT benchmarks. The test data distribution varies in contrast enhancement (pre, venous, arterial, post-phases), disease condition (30% containing abdominal tumors at varied stages), demographics (age, gender, race), image quality (e.g., slice thickness of 0.5–1.5 mm), and scanner types. We have collected metadata information for 72% of the test set (N𝑁Nitalic_N=5,160) and reported AI performance in each sub-group. 2. Providing a large test set: Our test set (N𝑁Nitalic_N=5,903) is much larger than the test sets of all current public CT benchmarks combined. It can enhance the statistical significance of the benchmark results: a 1% average accuracy increment across 5,000 CT scans is more indicative of a genuine algorithmic improvement than a 1% variation across 50 CT scans. 3. Analyzing pros/cons from multiple perspectives: We evaluated segmentation performance of 9 anatomical structures, comparing the average results and analyzing them by metadata groups. We also reported per-class algorithm rankings and visualized worst-case performance. Moreover, we assessed inference time and computational cost, key factors for the clinical deployment of AI algorithms. 4. Inviting inventors to train their own algorithms: Each AI algorithm is configured by its own inventors, who know it best and have the most interest in its success. In our benchmark, each inventor trained their AI algorithm on 5,195 annotated CT scans in AbdomenAtlas [qu2023annotating], and we, as a third party, independently evaluated these algorithms on 5,903 CT scans that are unknown and inaccessible to the AI inventors. This setting protects the integrity of our results (i.e., precluding the use of test data for hyperparameter tuning). 5. Evaluating new algorithms with long-term commitment: Our Touchstone benchmark not only invited established AI algorithms that are already published in major conferences/journals, but also invited newly developed algorithms appearing in recent pre-prints. We have a long-term commitment to this benchmark by organizing recurring challenges for at least five years, curating larger datasets, and improving label quality and task diversity. The first edition was featured as an invitation-only challenge at ISBI-2024. Related benchmarks/challenges & our innovations. In a general sense, we define a benchmark as an algorithmic comparison. Accordingly, the most common type of benchmark are the standard comparisons found in thousands of research papers [oktay2018attention, zhou2018unet++, zhou2019unet++, chen2021transunet, he2021dints, hatamizadeh2022unetformer, liu2024universal, wu2024medsegdiff] where authors present new algorithms and compare baselines. As previously explained, this type of benchmark incurs the risk of unfairness, due to possible asymmetric efforts made in optimizing the proposed and alternative algorithms. However, open challenges are a different type of benchmark, where developers train their own algorithms and submit them for third-party evaluation, mitigating the risk of unfair comparisons. For this reason, Table 1 contrasts our Touchstone benchmark to a non-exhaustive list of the most influential abdominal CT segmentation challenges. Notably, our training dataset is considerably larger and comes from more hospitals than any CT dataset ever used in a challenge. Furthermore, the only challenge training datasets on a scale similar to AbdomenAtlas 1.0 have partial labels and/or unlabeled portions [antonelli2021medical, ma2022fast]. Our dataset is 17.3×\times× larger than the second-largest fully-annotated CT dataset [heller2023kits21] in Table 1. Boosting our results’ statistical significance, our evaluation dataset is 8.6×\times× larger than any CT segmentation challenge test dataset. Moreover, Touchstone is the only benchmark in Table 1 to, simultaneously, explicitly analyze the performance of AI algorithms controlled by age, sex, race, and other metadata information. Lastly, this work is the starting point of a long-term benchmark, which we commit to maintain and improve over the years. Considering the importance of long-term commitment, we must acclaim KiTS, an abdominal segmentation challenge that had 3 editions since 2019 [heller2019kits19, heller2020international, heller2021state, heller2023kits21] and FLARE, a challenge being consistently held yearly since 2021 [ma2021abdomenct, ma2022fast, ma2024unleashing]. Table 1: Related benchmarks & our innovations. We compare Touchstone with influential CT segmentation benchmarks in light of the five contributions presented in the introduction. contribution promoting superior OOD performance with a large and diverse training dataset (#1) boosting results’ significance & large-scale OOD test (#1, #2) multi-faceted evaluation (#3) encouraging innovative AI (#4, #5) benchmark # CT scans train # hospitals train # countries train # CT scans test AI consistency analysis targeted invitation MSD-CT [antonelli2021medical] 947† 1 1 465 IID none no FLARE’22 [ma2022fast] 2,050† 22 5+ 200 IID, 600 OOD sex, age no FLARE’23 [ma2023unleashing] 4,000† 30 n/a n/a n/a no KiTS21 [heller2023kits21] 300 50+ 1 100 OOD sex, race no AMOS22-CT [ji2022amos] 200 3 1 78 IID, 122 OOD none no LiTS [bilic2019liver] 130 7 5 70 IID none no BTCV [landman2015miccai] 30 1 1 20 IID none no CHAOS-CT [valindria2018multi] 20 1 1 20 IID none no Touchstone (ours) 5,195 76 8 5,903 OOD sex, age, race yes • †Partially labeled: annotations for each organ do not cover the entire dataset, and/or may contain unlabeled samples. Figure 1: Summary of JHH and TotalSegmentator metadata. The diversity of data distribution includes more than just the number of centers; it also includes age, sex, manufacturer, diagnosis, and many other factors. JHH is the only dataset that provides race information, allowing us to compare the results; the race information is unknown in TotalSegmentator and most publicly available datasets. Therefore, the inclusion of JHH is value-added because it enabled the analysis on race."
https://arxiv.org/html/2411.03638v1,Adaptive Stereo Depth Estimation with Multi-Spectral Images Across All Lighting Conditions,"Depth estimation under adverse conditions remains a significant challenge. Recently, multi-spectral depth estimation, which integrates both visible light and thermal images, has shown promise in addressing this issue. However, existing algorithms struggle with precise pixel-level feature matching, limiting their ability to fully exploit geometric constraints across different spectra. To address this, we propose a novel framework incorporating stereo depth estimation to enforce accurate geometric constraints. In particular, we treat the visible light and thermal images as a stereo pair and utilize a Cross-modal Feature Matching (CFM) Module to construct a cost volume for pixel-level matching. To mitigate the effects of poor lighting on stereo matching, we introduce Degradation Masking, which leverages robust monocular thermal depth estimation in degraded regions. Our method achieves state-of-the-art (SOTA) performance on the Multi-Spectral Stereo (MS2) dataset, with qualitative evaluations demonstrating high-quality depth maps under varying lighting conditions.","I INTRODUCTION Depth estimation has received considerable attention in recent years due to its widespread applications in areas such as autonomous driving [1, 2], robotics [3], and 3D reconstruction [4]. Significant advancements have been made in both monocular and stereo depth estimation through deep learning-based approaches. However, these depth estimation algorithms predominantly rely on the visible domain. Consequently, their performance often suffers significant degradation due to the decline in image quality, particularly under poor illumination conditions such as nighttime or rainy weather [5], which prevented these algorithms from being widely applied in real-world scenarios. To address this challenge, recent research has increasingly investigated alternative vision modalities such as near-infrared images [6, 7], and long-wave infrared (also known as thermal) images [5, 8, 9] to achieve reliable and robust depth estimation in adverse conditions. Among these alternative modalities, thermal images have gained more popularity due to their low acquisition cost, robustness in adverse conditions, and consistent performance regardless of lighting variations. Some researchers have attempt to achieve depth estimation with only thermal images [5, 8]. However, thermal images typically exhibit lower texture information, resolution, and higher noise levels compared to visible light images, resulting in less accurate depth estimation in well-lit areas. Given that thermal and visible light depth estimation each have distinct strengths and weaknesses, researchers have focused on integrating thermal images with visible light images to leverage complementary information from both modalities. However, the significant differences between thermal and visible images present challenges in exploiting correlations across modalities. The substantial appearance differences, pacrtiularly the lack of texture in thermal images, complicate keypoint matching. Additionally, low illumination can obscure objects in visible light images, leading to mismatches with thermal images. Consequently, previous multi-spectral methods [9, 10] generally avoid direct pixel-level matching between images, limiting their ability to leverage geometric constraints. As a result, their performance is highly dependent on the training dataset and exhibits poor generalization. Figure 1: Depth from images of different modalities. (a) and (b) show the visible light and thermal images, respectively; (c) is the LiDAR ground truth corresponding to the thermal image; (d) and (e) present the depth maps obtained from monocular methods using the visible light and thermal modalities, respectively; (f) illustrates the depth map estimated by our multi-spectral method. To this end, we propose a novel framework that integrates thermal and visible light images for robust and accurate depth estimation under varying lighting conditions, as shown in Fig. 1. Specifically, we first treat the input visible light and thermal images as a stereo pair and train the Cross-modal Feature Matching (CFM) Module to generate aligned feature vectors for each pixel. This alignment allows us to project visible light features onto thermal features across candidate depths, constructing a cost volume that provides accurate pixel-level matching in well-lit regions. Then we estimate depth probability distributions for both modalities, and introduce a degradation mechanism based on those distributions, which reverts to monocular thermal image depth estimation for regions with adverse conditions. Finally, we employ a Depth Module to generate the final depth map. Our experimental evaluations demonstrate that our method surpasses existing state-of-the-art depth estimation methods, marking a significant advancement in the field. Our contributions are summarized as follows: • We propose a novel multi-spectral depth estimation method that leverages pixel-level matching through stereo depth estimation. By incorporating geometric constraints between cameras, our method achieves more accurate depth estimation. To the best of our knowledge, this is the first application of stereo depth estimation architecture in this domain. • For low-light regions where the visible light image becomes unreliable, we introduce a novel degradation mechanism that effectively degrades to monocular thermal depth estimation. • We demonstrate that the proposed method achieves substantial improvements over current state-of-the-art techniques on the MS2 benchmark dataset [5]."
https://arxiv.org/html/2411.03628v1,StreamingBench: Assessing the Gap for MLLMs to Achieve Streaming Video Understanding,"The rapid development of Multimodal Large Language Models (MLLMs) has expanded their capabilities from image comprehension to video understanding. However, most of these MLLMs focus primarily on offline video comprehension, necessitating extensive processing of all video frames before any queries can be made. This presents a significant gap compared to the human ability to watch, listen, think, and respond to streaming inputs in real time, highlighting the limitations of current MLLMs. In this paper, we introduce StreamingBench, the first comprehensive benchmark designed to evaluate the streaming video understanding capabilities of MLLMs. StreamingBench assesses three core aspects of streaming video understanding: (1) real-time visual understanding, (2) omni-source understanding, and (3) contextual understanding. The benchmark consists of 18 tasks, featuring 900 videos and 4,500 human-curated QA pairs. Each video features five questions presented at different time points to simulate a continuous streaming scenario. We conduct experiments on StreamingBench with 13 open-source and proprietary MLLMs and find that even the most advanced proprietary MLLMs like Gemini 1.5 Pro and GPT-4o perform significantly below human-level streaming video understanding capabilities. We hope our work can facilitate further advancements for MLLMs, empowering them to approach human-level video comprehension and interaction in more realistic scenarios.111Our dataset and code are available at: https://github.com/THUNLP-MT/StreamingBench","The rapid evolution of Multimodal Large Language Models (MLLMs) has significantly reshaped the field of Artificial Intelligence (Yang et al., 2023; Reid et al., 2024; Liu et al., 2024c; a). Current advanced MLLMs (Reid et al., 2024; Wang et al., 2024a; Yao et al., 2024) have already demonstrated exceptional performance in video understanding tasks, excelling on existing video benchmarks (Fu et al., 2024; Wang et al., 2024b; Zhou et al., 2024; Ataallah et al., 2024). Moreover, several pioneering studies (Chen et al., 2024a; Zhang et al., 2024a; Wu et al., 2024) have focused on improving the ability of MLLMs to comprehend real-time online video streams, pushing the boundaries of their applicability and efficiency in dynamic environments. In the industry, streaming video understanding has also attracted significant attention, with OpenAI’s GPT-4o (OpenAI, 2024) as a prominent example that demonstrates human-like perception and understanding of streaming inputs. Despite the recognition of the importance of streaming video understanding for MLLMs, most existing video understanding benchmarks (Fu et al., 2024; Wang et al., 2024b; Zhou et al., 2024) are primarily designed for offline evaluation. In such setups, all video frames are pre-loaded into the MLLMs before any queries are made, assuming the model has complete access to the entire video content. In contrast, streaming video understanding tasks differ in three key aspects: (1) queries can arise at any point during the video stream, rather than just at the end; (2) synchronized visual and audio inputs must be considered as in real-world streaming scenarios; (3) the influence of context must be taken into account, such as redundant information in long video streams and the history of streaming interactions. These differences in design principles between offline and streaming tasks make it quite challenging to adapt offline benchmarks for streaming evaluation. To the best of our knowledge, the only current benchmark related to streaming video understanding is VStream-QA (Zhang et al., 2024a). The main attribute of VStream-QA is that each question-answer pair is assigned a timestamp indicating its position in the video and is only related to the content preceding that point. However, VStream-QA includes only 32 videos from Ego4d (Grauman et al., 2022) and MovieNet (Huang et al., 2020), with a limited variety of video types and a narrow range of scenarios. In addition, it only covers five types of tasks, focuses solely on the visual modality, and the questions for each video are independent of each other. These limitations prevent VStream-QA from fully assessing streaming video understanding abilities for MLLMs when confronted with complex, multimodal streaming inputs in real-world scenarios. Figure 1: Illustrative comparison between StreamingBench and previous offline video benchmarks. In offline video benchmarks, questions are designed based on the entire video being visible. In contrast, StreamingBench presents questions at specific moments, with three main task categories specifically designed to evaluate fundamental capabilities in streaming video understanding. To address the limitations of existing video benchmarks, we introduce StreamingBench, the first comprehensive benchmark for assessing the streaming video understanding capabilities of MLLMs. StreamingBench consists of 900 videos and 4,500 questions, spanning eight diverse video categories that reflect a wide range of real-world scenarios. Each video features five questions that are manually curated to ensure a high level of relevance to the streaming video scenarios. These questions are categorized into 18 tasks, and based on the characteristics of streaming video tasks, they can be grouped into three main categories as illustrated in Figure 1: • Real-Time Visual Understanding, which focuses on the ability of MLLMs to comprehend visual content in real-time, recognizing and interpreting objects, actions, and changes as they happen within the video stream. For example, in Figure 1, the answer to the question “What words are currently shown?” may vary depending on the specific moment in time the question is asked, highlighting the dynamic nature of streaming video tasks. • Omni-Source Understanding, which refers to the ability integrate visual and audio information in real-time video streams. MLLMs must handle both sources simultaneously to provide a comprehensive understanding of the scene and answer questions that depend on their synchronization, such as “What is happening in the video when [sound] is made?”. • Contextual Understanding, which evaluates the capability of MLLMs to comprehend the broader context within a video stream, including detecting anomalies, filtering misleading information, maintaining continuity across sequential interactions, and responding proactively based on predefined conditions. For instance, as shown in the last query of Figure 1, a follow-up question is asked based on the content of the previous query interaction, with a reference to“the event referred to in the previous query”. We conduct experiments on StreamingBench with state-of-the-art MLLMs, including three proprietary models GPT-4o (OpenAI, 2024), Gemini 1.5 Pro (Reid et al., 2024) and Claude 3.5 Sonnet (Anthropic, 2024), and 10 advanced open-source MLLMs like LLaVA-OneVision (Li et al., 2024a), Qwen2-VL (Wang et al., 2024a) and MiniCPM-V 2.6 (Yao et al., 2024). Since these models currently cannot accept streaming video input222The GPT-4o API currently does not support video inputs., we convert each streaming task into an offline one for evaluation. For each question, the model processes the video segment from the start to the point when the question is asked, treating it as the complete input, and provides a response based on that segment. The results show that even the best-performing model, Gemini 1.5 Pro, achieves only an average accuracy of 67.07%, which is 24.59% lower than human performance. This indicates that there is a significant gap between MLLMs and human performance in understanding video streams. To further investigate this gap, we conduct a series of analytical experiments, revealing that current models perform poorly in terms of real-time processing. This may be attributed to the fact that most existing MLLMs are primarily trained on offline videos. Additionally, we find that these models generally lack the ability to understand and interact with streaming contexts. Specifically, redundant information in the context of streaming videos significantly affects model performance, and current models struggle with proactive output in streaming scenarios and fail to effectively respond to continuous queries. We hope these findings will provide valuable insights for improving future MLLMs and contribute to the development of the next generation of multimodal systems. Table 1: Comparison between StreamingBench and other video benchmarks. Timestamp denotes whether to assign timestamps to questions. Temporal Clues denote whether the questions are related to different temporal clues within videos (Section 4.4)). SQA and PO denote sequential question answering and proactive output, respectively (Section 3.1.3). Benchmark #Videos #QA Pairs Timestamp Temporal Clues Modality Streaming Interaction Annotation Prior Concurrent Subsequent Vision Audio SQA PO Auto Manual Offline (Short) MSRVTT-QA (Xu et al., 2017) 2,990 72,821 ✗ ✓ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ TGIF-QA (Jang et al., 2017) 9,575 8,506 ✗ ✓ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ MV-Bench (Li et al., 2024b) 3,641 4,000 ✗ ✓ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ How2QA (Li et al., 2020) 1,166 2,852 ✗ ✓ ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✓ ActivityNet-QA (Yu et al., 2019) 800 8,000 ✗ ✓ ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✓ Offline (Long) InfiniBench (Ataallah et al., 2024) 1219 108,200 ✗ ✓ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ MLVU (Zhou et al., 2024) 1,334 2,593 ✗ ✓ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✓ LVBench (Wang et al., 2024b) 500 1,549 ✗ ✓ ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✓ Video-MME (Fu et al., 2024) 900 2,700 ✗ ✓ ✗ ✗ ✓ ✓ ✗ ✗ ✗ ✓ Online VStream-QA (Zhang et al., 2024a) 32 3,500 ✓ ✓ ✓ ✗ ✓ ✗ ✗ ✗ ✓ ✗ StreamingBench(Ours) 900 4,500 ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓"
https://arxiv.org/html/2411.03576v1,Hybrid Attention for Robust RGB-T Pedestrian Detection in Real-World Conditions,"Multispectral pedestrian detection has gained significant attention in recent years, particularly in autonomous driving applications. To address the challenges posed by adversarial illumination conditions, the combination of thermal and visible images has demonstrated its advantages. However, existing fusion methods rely on the critical assumption that the RGB-Thermal (RGB-T) image pairs are fully overlapping. These assumptions often do not hold in real-world applications, where only partial overlap between images can occur due to sensors configuration. Moreover, sensor failure can cause loss of information in one modality. In this paper, we propose a novel module called the Hybrid Attention (HA) mechanism as our main contribution to mitigate performance degradation caused by partial overlap and sensor failure, i.e. when at least part of the scene is acquired by only one sensor. We propose an improved RGB-T fusion algorithm, robust against partial overlap and sensor failure encountered during inference in real-world applications. We also leverage a mobile-friendly backbone to cope with resource constraints in embedded systems. We conducted experiments by simulating various partial overlap and sensor failure scenarios to evaluate the performance of our proposed method. The results demonstrate that our approach outperforms state-of-the-art methods, showcasing its superiority in handling real-world challenges.","Pedestrian detection is one of the important domains within computer vision for robotics, playing a significant role in applications such as self-driving vehicles, surveillance automation, and mobile robot navigation [1]. RGB cameras are commonly preferred sensors for such applications. However, they tend to suffer from overexposure in daylight, low illumination in night scenarios, and high-contrast lighting. To address these shortcomings, a number of sensors and fusion solutions were investigated. In particular, thermal cameras seem to provide several advantages in terms of costs, algorithms, and data [2]. Among them, RGB images provide texture and color information, while thermal images focus on the infrared heat emitted by the objects and are therefore invariant to lighting conditions [3]. RGB and thermal images are thefore complementary with each other by nature. This led the community to collect multispectral datasets such as KAIST [1], CVC [4] or FLIR [5], providing thermal data in addition to RGB data. KAIST dataset provides fully-overlapping RGB-T image pairs, i.e. both images are acquired at the same time and cover the same field of view. However, acquiring such image pairs requires specialised sensor setup over conventional stereo setup which is widely used in real-world applications. In stereo setups, partial overlap will occur inherently due to a different camera Field of View (FoV) and pixel-level misalignment will occur due to parallax [6]. Figure 1: Constraints on (a) stereo setup (extrinsic parameters) and (b) sensor resolution (intrinsic parameters) resulting in only partial overlap between RGB and thermal images. Information discrepancy between one image and the other can cause features to be out of their corresponding positions, resulting in decreased algorithm performance and less accurate predictions during the inference process [7]. Even in the KAIST dataset that has fully overlapping image pairs, the authors attempted to reduce the pixel-level misalignment problem. This was achieved by further improving the original data labels to “sanitised” cross-modal annotations [8] and “paired” modality-specific annotations [7] (in this paper, different modalities correspond to different spectral images: RGB or thermal). Recent methods, such as multi-label learning [6], aimed to learn more discriminative features while using semi-unpaired augmentation to generate unpaired inputs between two modalities where considering a single bounding-box label is irrelevant. Even with different learning approaches, the robustness of an algorithm is questioned when one of the modalities is unavailable or partially available; for example, when a malfunction in one of the camera sensor arrays leads to a partial or even complete loss of one modality. This situation is investigated in very few existing literature [6], and the performance drop in such scenarios is quite high even for best performing models on fully overlapping images. To improve performance and enhance algorithm robustness, our research examines the issue of partial overlap caused by various factors such as stereo configurations, sensor malfunctions, and others. This can result in partial or complete invisibility of regions in one modality during inference. In this paper, we adopt the term blackout to refer to areas in the union of pictures where data are absent from one of the modalities. Figure 1 shows sample cases of blackout that can arise from sensor setups, Figure 1-a showing sides blackout from different camera extrinsics in stereo setup, Figure 1-b depicting surrounding blackout arising due to difference in camera intrinsics such as sensor resolution or focal length. To achieve robustness in such scenarios, we present a hybrid attention module, which reduces performance degradation irrespective of network architecture. We also consider a much lighter backbone compared to previous works for coping with hardware resource constraints in embedded systems. Our contributions are as follows: • We introduce the Hybrid-Attention (HA) module, which combines self-attention and cross-attention, to mitigate performance degradation arising from modality-specific blackouts; • We propose an improved RGB-T fusion algorithm, named Hybrid Attention-based Multi-Label Pedestrian Detector (HA-MLPD), robust against partial overlap and sensor failure encountered in real-world scenarios, while being resource-friendly; • We provide experimental evidence that the proposed method prevents a performance drop and makes the fusion algorithm more robust and reliable irrespective of the network backbone architecture. The remainder of the paper is structured as follows: Section II provides an overview of the related literature. In Section III, the HA-MLPD algorithm is introduced, with a detailed explanation of its implementation. Our approach is tested on the KAIST dataset [1] under various simulated blackout conditions, as discussed in Section IV. Finally, Section V concludes with a discussion on future research directions."
https://arxiv.org/html/2411.03561v1,Estimating Ego-Body Pose from Doubly Sparse Egocentric Video Data,"We study the problem of estimating the body movements of a camera wearer from egocentric videos. Current methods for ego-body pose estimation rely on temporally dense sensor data, such as IMU measurements from spatially sparse body parts like the head and hands. However, we propose that even temporally sparse observations, such as hand poses captured intermittently from egocentric videos during natural or periodic hand movements, can effectively constrain overall body motion. Naively applying diffusion models to generate full-body pose from head pose and sparse hand pose leads to suboptimal results. To overcome this, we develop a two-stage approach that decomposes the problem into temporal completion and spatial completion. First, our method employs masked autoencoders to impute hand trajectories by leveraging the spatiotemporal correlations between the head pose sequence and intermittent hand poses, providing uncertainty estimates. Subsequently, we employ conditional diffusion models to generate plausible full-body motions based on these temporally dense trajectories of the head and hands, guided by the uncertainty estimates from the imputation. The effectiveness of our method was rigorously tested and validated through comprehensive experiments conducted on various HMD setup with AMASS and Ego-Exo4D datasets.","((a)) ((b)) ((c)) ((d)) Figure 1: Overview of DSPoser. Our goal is to estimate ego-body pose without dependency on hand controllers in an HMD environment. (a) Given the egocentric video and head tracking signals as input, (b) our approach first predicts the hand pose in the frames where hands are visible (dark blue). It then estimates the hand poses in frames with invisible hands (light blue) using imputation, and (c) estimates uncertainty associated with the hand poses where the hands are invisible, (d) The predicted and imputed hand pose is then used with head pose to predict the 3D full body pose. The evolution of augmented reality (AR) devices such as the Apple Vision Pro, Meta Quest 3, Microsoft HoloLens 2, and etc. has dramatically reshaped interactive technologies. These head-mounted displays (HMDs) feature inertial measurement units (IMUs) and video capture capabilities, offering a unique egocentric perspective. However, their limited visibility of the user’s body parts poses a significant challenge for accurate egocentric body pose estimation—a key element for immersive AR experiences. Previous approaches have tackled this problem by spatially reconstructing the entire body from spatially sparse data. For instance, EgoEgo [16] first estimates head poses using SLAM on the egocentric video, then generates body poses from these estimated head positions. Other methods, such as AvatarPoser [13] and BoDiffusion [3], primarily depend on temporally dense tracking signal from spatially sparse body parts, notably the head and hands. This dependency on specific hardware such as head-mounted displays and hand controllers constrains their versatility and diminishes their applicability in broader AR/VR scenarios where hand controllers might not be used, like sports training or analysis applications where the user needs to move freely without holding any devices, or augmented reality experiences in outdoor environments where carrying controllers is impractical. We observe that even temporally sparse observations, such as hand poses captured intermittently from egocentric videos during natural or periodic hand movements, can effectively constrain overall body motion. While it is possible to utilize other visible body parts such as feet or elbows, we opted to rely on hand poses. This decision is based on the availability of hand pose detectors [24, 14] and the fact that hands are visible in approximately 20%percent\%% of video frames, as demonstrated in Table 8. Unlike previous work that concentrated only on spatial completion, our method incorporates temporal completion by leveraging the intermittent appearance of hands in egocentric videos. This dual completion approach not only enhances the robustness of body pose estimation under varying conditions but also reduces reliance on specific sensor hardware, making it more adaptable to various AR environments. In our setup, we use temporally sparse 3D hand poses from detections in egocentric videos combined with dense head tracking signals to reconstruct the full body. Initially, we temporally complete sparse hand information using a Masked Autoencoder (MAE) [11], which estimates hand pose trajectories by capturing the spatiotemporal correlations between intermittent hand poses and head tracking signals. We develop a probabilistic extension of the MAE to provide uncertainty estimates of the predicted hand pose sequence. Subsequently, using a conditional diffusion model, we spatially reconstruct the full body based on the head tracking signal data and imputed hand trajectories along with their predictive uncertainties. We call our approach DSPoser (Doubly Sparse Poser) because it can effectively utilize data that is doubly sparse (sparse both temporally and spatially), as shown in Figure 1. This flexible framework is designed to seamlessly adapt to diverse AR/VR setups and devices, ranging from spatially sparse scenarios (e.g., using only head tracking signal or combining it with hand controllers) to doubly sparse scenarios (utilizing head signal data alongside hand detection from egocentric video). The key advantage lies in the assumption that the HMD’s tracking signal is consistently available, enabling our approach to function across a wide range of environments and hardware configurations. Extensive experiments have proved our model’s versatility and accurate pose estimation capabilities in various settings. Furthermore, our ablation studies highlight the significance of incorporating uncertainty estimates, as this crucial information enhances the overall quality of pose estimation, resulting in more reliable outputs. By addressing both temporal and spatial completion through our double completion approach, we have developed a robust and adaptable solution that reduces dependency on specific sensor hardware, making it well-suited for immersive AR experiences in diverse scenarios, such as sports training, outdoor environments, and beyond. In summary, our research presents three key contributions: • A robust and versatile framework for egocentric body pose estimation tailored for HMDs. The framework adapts to various AR/VR settings and can leverage tracking signals available in most modern HMD devices without controllers. • We decomposed the problem into temporal completion and spatial completion. Our approach captures the uncertainty from hand trajectory imputation to guide the diffusion model for accurate full-body motion generation. • Extensive evaluations demonstrating the effectiveness of our framework on diverse datasets, outperforming existing methods and underscoring its potential for enhancing user interaction and immersion in AR experiences."
https://arxiv.org/html/2411.03555v1,Object and Contact Point Tracking in Demonstrations Using 3D Gaussian Splatting,"This paper introduces a method to enhance Interactive Imitation Learning (IIL) by extracting touch interaction points and tracking object movement from video demonstrations. The approach extends current IIL systems by providing robots with detailed knowledge of both where and how to interact with objects, particularly complex articulated ones like doors and drawers. By leveraging cutting-edge techniques such as 3D Gaussian Splatting and FoundationPose for tracking, this method allows robots to better understand and manipulate objects in dynamic environments. The research lays the foundation for more effective task learning and execution in autonomous robotic systems.","Autonomous robotic systems have advanced significantly [8, 9], but challenges remain in manipulating novel objects and articulated structures like doors or cabinets. Robots must accurately identify, grasp, and move objects within dynamic environments. Reinforcement Learning (RL) [10] and Imitation Learning (IL) [11] are widely used approaches, with RL often requiring extensive training and careful reward design, while IL benefits from expert demonstrations but struggles with generalization to unseen scenarios [12, 13]. Interactive Imitation Learning (IIL) improves IL by incorporating real-time human feedback [14, 15], allowing robots to adjust during execution, but still faces difficulties in tasks requiring nuanced object interaction [16]. To address this, we propose a method for extracting touch interaction points, or contact points, and tracking object movement from video demonstrations, providing robots with detailed information for manipulating articulated objects. By combining video-based learning with 3D Gaussian Splatting [1] for 3D scene reconstruction, we create a task-relevant representation of the environment, enabling improved robot performance in complex tasks. This method enhances the IIL framework by offering precise interaction data, laying the groundwork for more autonomous robotic manipulation."
https://arxiv.org/html/2411.03554v1,Benchmarking Vision Language Model Unlearning via Fictitious Facial Identity Dataset,"Machine unlearning has emerged as an effective strategy for forgetting specific information in the training data. However, with the increasing integration of visual data, privacy concerns in Vision Language Models (VLMs) remain underexplored. To address this, we introduce Facial Identity Unlearning Benchmark (FIUBench), a novel VLM unlearning benchmark designed to robustly evaluate the effectiveness of unlearning algorithms under the Right to be Forgotten setting. Specifically, we formulate the VLM unlearning task via constructing the Fictitious Facial Identity VQA dataset and apply a two-stage evaluation pipeline that is designed to precisely control the sources of information and their exposure levels. In terms of evaluation, since VLM supports various forms of ways to ask questions with the same semantic meaning, we also provide robust evaluation metrics including membership inference attacks and carefully designed adversarial privacy attacks to evaluate the performance of algorithms. Through the evaluation of four baseline VLM unlearning algorithms within FIUBench, we find that all methods remain limited in their unlearning performance, with significant trade-offs between model utility and forget quality. Furthermore, our findings also highlight the importance of privacy attacks for robust evaluations. We hope FIUBench will drive progress in developing more effective VLM unlearning algorithms 111FIUBench data is hosted at https://huggingface.co/datasets/gray311/FIUBench. New unlearn method can be evaluated by using the code at https://github.com/SaFoLab-WISC/FIUBench..","Vision language models (VLMs) are increasingly utilized in various real-world applications (openai team, 2023; Liu et al., 2024a; Ma et al., 2023; Wang et al., 2024; Li et al., 2023a). Training VLMs typically require extensive data and computational resources. However, the massive amounts of data, collected from various sources including web scraping, may inadvertently contain personal images and information. Directly incorporating such data in training poses serious privacy issues (Gong et al., 2023; Ma et al., 2024; Tömekçe et al., 2024; Samson et al., 2024). For instance, private information such as home addresses or phone numbers could be exposed if VLMs are applied to facial identities captured by a passerby in public or recorded by closed-circuit television. Fortunately, due to the regulation of the Right to be forgotten (Regulation, 2016; OAG, 2021; Voigt & Von dem Bussche, 2017; Zhang et al., 2023), individuals have the right to request that model owners remove their personal data to protect their privacy. To realize the excluding of private data under the Right to be forgotten, one promising method is machine unlearning (Cao & Yang, 2015; Bourtoule et al., 2021; Baumhauer et al., 2022; Nguyen et al., 2022a), which modifies models to facilitate the forgetting of risky data. While retraining model is the most straightforward and effective way for unlearning, it is unrealistic in the VLM era due to the enormous volume of training data involved, leading to significant costs whenever the forgetting requests are updated. Therefore, in this paper, we primarily focus on unlearning methods that do not require retraining. Numerous studies (Yao et al., 2024b; Maini et al., 2024; Liu et al., 2024b; Chen & Yang, 2023; Eldan & Russinovich, 2023) have investigated unlearning techniques to remove specific textual data from large language models (LLMs). However, the application of unlearning to VLMs remains underexplored. With the increasing integration of visual data, it is crucial to determine whether VLMs can effectively forget privacy knowledge through machine unlearning under the Right to be forgotten setting. While early studies (Cheng & Amiri, 2023a; Li et al., 2024b) have explored unlearning in VLMs, none of them have considered practical privacy concerns under Right to be forgotten. For instance, MultiDelete (Cheng & Amiri, 2023a) focuses solely on image-text pairing classification tasks, which are far from real-world use cases of VLMs with generative purposes. Although Li et al. (2024b) introduces a benchmark named MMUBench for unlearning evaluation, the unlearning target concepts derived from MIKE (Li et al., 2024a), such as “Van Gogh” and “Facebook”, are privacy unrelated, lacking a strong incentive for removal. Therefore, a standardized benchmark is still needed to assess VLM unlearning performance in the context of the Right to be Forgotten. Moreover, without a standardized benchmark, it also prevents from development of an efficient VLM unlearning algorithm since there is no effective way to show effectiveness. However, several challenges remain when constructing VLM unlearning benchmarks. Considering the limitations of previous works and the unique requirements of the Right to be Forgotten, we identify the following key challenges: (i) What should be unlearned from VLMs, given the integration of both image and text data? (ii) How can we identify unlearning targets when privacy-sensitive information, subject to the Right to be Forgotten, is rare in the training dataset and unknown to us? (iii) How can we ensure a robust evaluation of VLM unlearning? Figure 1: Overview of the pipeline from construction to evaluation for FIUBench. To address these challenges, here we introduce the Facial Identity Unlearning Benchmark (FIUBench) as illustrated in Figure 1, specifically designed for robustly evaluating VLM unlearning under Right to be Forgotten. We list our contributions in this new benchmark as follows: (I) Formalizing the VLM unlearning tasks: Unlike unlearning in LLMs, which primarily focuses on forgetting sensitive text information, unlearning in VLMs extends to both images and text. Since users have already seen the input images, removing visual attributes is meaningless and potentially undermines the basic visual capabilities of VLMs. Instead, the focus should be on unlearning sensitive information linked to images rather than the visual attributes themselves. For example, a VLM after unlearning should retain the ability to describe basic facial features, but private information like names, addresses, and personal medical information should be forgotten. Therefore, in our paper, we formalize VLM unlearning as the task of unlearning private image and text-paired information. (II) Two-stage evaluation pipeline with Fictitious Facial Identity VQA dataset: To study privacy under the Right to be Forgotten scenario, where individual private information is rare within the pretraining dataset, it is crucial to ensure that unlearning targets exist in the VLMs without being overly exposed. Inspired by TOFU (Maini et al., 2024), we perform a two-stage evaluation pipeline with learning followed by unlearning on the Fictitious Facial Identity VQA dataset. This approach allows us to precisely control the source of information and the exposure levels of the dataset’s knowledge prior to unlearning, effectively simulating the Right to be Forgotten scenario with rarely occurring personal information. To construct the facial identity unlearning dataset, we selected 400 synthetic faces from SFHQ (Beniaguev, 2022), each associated with fictitious private backgrounds, including personal backgrounds, health records, and criminal histories. For each facial identity, we generated 20 related VQA pairs using GPT-4o, focused on their private knowledge. All these facial identities and their corresponding VQA pairs form the Fictitious Face Identity VQA Dataset. (III) Robust evaluation with privacy attacks: To ensure that VLM unlearning is achieved without significantly compromising the model’s functionality or the integrity of retained knowledge, both forget quality and model utility are commonly used to evaluate unlearning performance. However, since VLM supports various forms of ways to ask questions with the same semantic meaning, more robust evaluations are still needed. Therefore, our FIUBench further incorporates membership inference attacks and adversarial privacy extraction to robustly evaluate unlearning performance, testing whether the private information is unlearned even under attacks. Empirically evaluating four baseline unlearning methods (Yao et al., 2023; 2024a; Maini et al., 2024) on FIUBench, the results indicate that all approaches are still limited in reaching effective VLM unlearning performance, in terms of both model utility and forget quality. Additionally, while Preference Optimization can prevent the model from answering private questions with only a slight reduction in model utility, our robust evaluation using membership inference attacks reveals that it cannot truly forget private knowledge. This underscores the importance of incorporating privacy attacks into unlearning performance assessments. We hope our benchmark provides valuable insights and raises awareness of the challenges in VLM unlearning under the Right to be Forgotten scenario."
https://arxiv.org/html/2411.03531v1,Personalized Video Summarization by Multimodal Video Understanding,"Video summarization techniques have been proven to improve the overall user experience when it comes to accessing and comprehending video content. If the user’s preference is known, video summarization can identify significant information or relevant content from an input video, aiding them in obtaining the necessary information or determining their interest in watching the original video. Adapting video summarization to various types of video and user preferences requires significant training data and expensive human labeling. To facilitate such research, we proposed a new benchmark for video summarization that captures various user preferences. Also, we present a pipeline called Video Summarization with Language (VSL) for user-preferred video summarization that is based on pre-trained visual language models (VLMs) to avoid the need to train a video summarization system on a large training dataset. The pipeline takes both video and closed captioning as input and performs semantic analysis at the scene level by converting video frames into text. Subsequently, the user’s genre preference was used as the basis for selecting the pertinent textual scenes. The experimental results demonstrate that our proposed pipeline outperforms current state-of-the-art unsupervised video summarization models. We show that our method is more adaptable across different datasets compared to supervised query-based video summarization models. In the end, the runtime analysis demonstrates that our pipeline is more suitable for practical use when scaling up the number of user preferences and videos.","The availability of online content, such as news articles, live broadcasts, and video blogs, has led to a demand for video summarization in different practical scenarios. The growing interest in multimodal learning has focused on the development of personalized video summaries using natural language queries (Lei et al., 2021). Unlike conventional video summarization methods (Apostolidis et al., 2022) that solely rely on video content to capture repetitive scenes as highlights, query-guided video summarization (Lin et al., 2023) incorporates information from natural language queries to produce concise video summaries. This approach provides users with condensed information, which is particularly useful for lengthy videos such as live streams and product reviews, where redundant content is often present. Figure 1. The overall structure of VSL. VSL utilizes a captioning model to transform long videos into text and summarize the text to represent the video. Existing methods for query-guided video summarization (Lei et al., 2021) have utilized text information but can only generate summaries based on descriptive sentences, such as ”A shark is swimming under water,” rather than more abstract concepts like genres, such as ”Sci-Fi,” ”Romance,” or ”Comedy.” However, genre tags can be easily obtained from recommendation systems and can represent user preferences when recommending videos. Unfortunately, current query-guided models struggle to understand genre-specific tags due to limitations in the training data, which was only trained with paired natural language sentences and lacks a deeper understanding of videos. Recently, there have been studies exploring user-preferred video summarization (Lei et al., 2021; Moon et al., 2023; Lin et al., 2023), which aim to generate summaries based on specific user applications. However, genre-specific user preferences have not been thoroughly explored. In this work, our goal is to generate video summaries that are conditioned on user preferences, resulting in user-centric video summarization. In order to facilitate research on long-video summarization with user-preferred information, we have collected a large-scale video dataset called UserPrefSum. This dataset consists of over 1K movie videos from Condensed Movies (Bain et al., 2020), covering a diverse range of genres (21 classes in total). For genre-based video summarization, users can provide one or multiple preferred movie genres as a query. We start by detecting scenes in the videos and then use the zero-shot ability of CLIP(Radford et al., 2021) to automatically label the genre of each scene. The final video summarization contains multiple scenes that are related to the given genre. This approach allows for a more realistic and user-centered evaluation of video summarization methods. This is motivated by the fact that video summarization can be quite subjective. Instead, we are offering a more unbiased approach to assess such situations by representing user preference by genres. This is based on our hypothesis on the Content-Based Recommendation System (Pazzani and Billsus, 2007), the underlying principle of this type of recommendation system being that if a user enjoyed a certain movie or show, they may also enjoy something similar. Motivated by the observations mentioned above, we propose a Video Summarization with Language (VSL) approach, as depicted in Figure 1. VSL is based on the Socratic model (Zeng et al., 2022) and consists of four components: multimodal scene detection, video captioning, multimodal summarization, and video selection. The multimodal scene detection component utilizes both video and closed captioning as inputs to identify clear scene cuts that preserve the integrity of both the video and dialogue in the input movie. Video captioning converts the input movie from the video domains to a text-based captioning domain for semantic analysis. The multimodal summarization component generates summaries for both video captioning and closed captioning, capturing the most important information at the scene level. The video selection component analyzes the summarization results from the multimodal summarization component, taking into account the input genre, in order to select appropriate scenes and create the final summary video that aligns with the specific user preferences. Experimental results demonstrate that VSL outperforms current state-of-the-art methods in both general video summarization (TVSum(Song et al., 2015)) and user-specific video summarization (UserPrefSum). We also test summarization in user-generated videos SumMe(Gygli et al., 2014). Additionally, we conduct runtime analysis on varying numbers of videos and user preferences to showcase the practical applicability of the VSL model. To summarize, our contributions include: • A new benchmark is proposed to facilitate the research of user-preferred video summarization. • A language-based architecture VSL makes use of pre-trained large language models for video captioning and multimodal summarization. This allows the architecture to analyze natural language at a semantic level, which takes into account multimodal input and genre recommendation, an aspect overlooked by previous studies. • A runtime analysis has been conducted to demonstrate the scalability of our model. Unlike the previous state-of-the-art method that requires feeding user preference queries to each video individually, our VSL can generate summarization in parallel. This real-time capability is crucial for practical applications."
https://arxiv.org/html/2411.03511v1,Beyond Complete Shapes: A Quantitative Evaluation of 3D Shape Matching Algorithms,"Finding correspondences between 3D shapes is an important and long-standing problem in computer vision, graphics and beyond. While approaches based on machine learning dominate modern 3D shape matching, almost all existing (learning-based) methods require that at least one of the involved shapes is complete. In contrast, the most challenging and arguably most practically relevant setting of matching partially observed shapes, is currently underexplored. One important factor is that existing datasets contain only a small number of shapes (typically below 100), which are unable to serve data-hungry machine learning approaches, particularly in the unsupervised regime. In addition, the type of partiality present in existing datasets is often artificial and far from realistic. To address these limitations and to encourage research on these relevant settings, we provide a generic and flexible framework for the procedural generation of challenging partial shape matching scenarios. Our framework allows for a virtually infinite generation of partial shape matching instances from a finite set of shapes with complete geometry. Further, we manually create cross-dataset correspondences between seven existing (complete geometry) shape matching datasets, leading to a total of 2543 shapes. Based on this, we propose several challenging partial benchmark settings, for which we evaluate respective state-of-the-art methods as baselines. Our entire benchmark can be viewed on: https://becos-authors.github.io/BeCoS/.","Finding correspondences between deformable shapes is a fundamental problem in computer vision and graphics, with various applications such as texture and deformation transfer [19, 42], or statistical shape analysis [30, 10, 27]. The problem has been widely studied in the literature [42, 25, 1, 54, 15], and can vary significantly in complexity. For example, current methods still struggle when estimating severe non-isometric deformations for shapes of different categories [22, 44, 25, 7, 16], or when they are only partially observed [48, 24]. In real-world scenarios, it is particularly common to encounter shapes with missing parts due to occlusions, limited sensor coverage, or acquisition errors during 3D scanning. Furthermore, a semantic or geometric similarity might exist only between parts of the shapes but not between their whole geometry. The setting in which at least part of one shape does not exist on the other is called partial shape matching [46, 3, 48, 23, 24]. Despite its practical relevance, partial shape matching has received limited attention in machine learning, partly because of increased difficulty, but largely due to the lack of suitable and realistic large-scale datasets, a requirement for data-hungry learning algorithms. The full-to-full matching setting is not only simpler but also benefits from wider data availability, which has supported the development of data-driven methods that already display convincing performance even without supervision [38, 20, 15]. In contrast, existing datasets for partial shape correspondence often suffer from a small number of shapes, limited realism of generated partiality (e.g., cuts or holes) [17], and a lack of diversity in shape categories [11, 3]. This limited data availability hampers the progress of data-driven methods for partial shape matching; recent approaches have thus required pre-training with large full-geometry datasets [15], or rely on ground-truth correspondences [3] which can be challenging to obtain in practice. To address the absence of large partial datasets and foster research on partial shape correspondence, we introduce a procedural data generation framework to generate realistic and diverse partial shape pairs. Our framework is designed to unify existing full geometry correspondence datasets to generate virtually infinite partial shape pairs with known correspondences. By simulating realistic partiality patterns using ray casting, we bridge the gap between existing synthetic datasets and real-world partial shape correspondence problems. Furthermore, we propose BeCoS, a carefully curated benchmark based on existing datasets comprising diverse shape categories, and containing more realistic partial instances to assess the performance of existing and foster the development of novel methods. Through extensive experiments, we demonstrate the limitations of current state-of-the-art methods for partial shape correspondence and highlight the need for further research in this area. Contributions. We summarise our main contributions as follows: • A flexible and scalable framework that enables researchers to generate virtually unlimited training examples for partial shape correspondence tasks based on existing (full or partial geometry) data. The framework supports the selection and propagation of cross-category matchings and provides a versatile solution for creating custom training data tailored to specific application requirements. • A challenging partial non-rigid shape dataset constituting a carefully curated instantiation of our framework. This dataset serves as a standardized benchmark for shape matching methods, promoting the development of more robust and accurate algorithms. • An analysis of the performance of current state-of-the-art methods on the new benchmark, providing valuable insights into their performance, limitations, and key challenges. Results suggest that partial shape correspondence is far from being solved."
https://arxiv.org/html/2411.03505v1,SynthSet: Generative Diffusion Model for Semantic Segmentation in Precision Agriculture,"This paper introduces a methodology for generating synthetic annotated data to address data scarcity in semantic segmentation tasks within the precision agriculture domain. Utilizing Denoising Diffusion Probabilistic Models (DDPMs) and Generative Adversarial Networks (GANs), we propose a dual diffusion model architecture for synthesizing realistic annotated agricultural data, without any human intervention. We employ super-resolution to enhance the phenotypic characteristics of the synthesized images and their coherence with the corresponding generated masks. We showcase the utility of the proposed method for wheat head segmentation. The high quality of synthesized data underscores the effectiveness of the proposed methodology in generating image-mask pairs. Furthermore, models trained on our generated data exhibit promising performance when tested on an external, diverse dataset of real wheat fields. The results show the efficacy of the proposed methodology for addressing data scarcity for semantic segmentation tasks. Moreover, the proposed approach can be readily adapted for various segmentation tasks in precision agriculture and beyond.","In recent years, deep learning has played a pivotal role in precision agriculture [19], particularly for tasks such as object detection [13] and image segmentation [16], owing to its ability to extract valuable information with unprecedented accuracy and efficiency. Though deep learning is a powerful tool, in many domains, data-related challenges hinder the development of robust models, particularly in tasks where pixel-level data annotation is required [22]. Manual annotation is time-consuming, costly, and subject to inconsistencies and errors. Therefore, the challenge of obtaining large-scale annotated datasets for supervised learning in tasks requiring pixel-level annotations, such as semantic segmentation, remains a bottleneck for developing generalizable models. To address this, methods for synthetic data generation have gained traction, providing an alternative for developing extensive labeled datasets with minimal human intervention [22]. The field of computer vision also has witnessed remarkable advancements, particularly in the area of generative models. Among these, Denoising Diffusion Probabilistic Models (DDPMs) have shown promising performance in generating high-quality images, outperforming methods [3] such as Generative Adversarial Networks (GANs) [8] and Variational Autoencoders [15]. DDPMs can produce diverse images and are less prone to issues such as mode collapse during training. DDPMs are suitable for various tasks, including in-painting [17], super-resolution [26], and text-conditioned [27] image generation. To address the data annotation bottleneck, we propose a novel DDPM-based architecture for generating pixel-accurate annotated images. In this architecture, we leverage both diffusion models and GANs to generate realistic paired images and their binary segmentation masks (image-mask pairs). We design a dual diffusion model setup for image and mask synthesis, connected through skip connections and cross-attention mechanisms to enhance the coherence between the generated images and their corresponding generated masks. To improve the quality of the synthetic data, we apply super-resolution techniques to enhance the resolution of generated image-mask pairs. As a use case, we evaluate the proposed methodology in the agricultural domain, using it for a downstream wheat head semantic segmentation task. Specifically, we externally evaluate the performance of a segmentation model that we train only on our generated data. We then use the final domain adaptation technique described in [22] to improve the performance of the trained model. This work presents a deep model architecture to synthesize annotated agricultural data; however, our method is applicable in any domain requiring semantic masks. We demonstrate its efficacy by generating high-quality image-mask pairs, advancing synthetic data generation techniques, and mitigating data scarcity in precision agriculture. Additionally, our results show enhanced realism of synthetic data and promising results for improving performance in downstream semantic segmentation tasks."
https://arxiv.org/html/2411.03480v1,Rainfall regression from C-band Synthetic Aperture Radar using Multi-Task Generative Adversarial Networks,"This paper introduces a data-driven approach to estimate precipitation rates from Synthetic Aperture Radar (SAR) at a spatial resolution of 200 meters per pixel. It addresses previous challenges related to the collocation of SAR and weather radar data, specifically the misalignment in collocations and the scarcity of rainfall examples under strong wind. To tackle these challenges, the paper proposes a multi-objective formulation, introducing patch-level components and an adversarial component. It exploits the full NEXRAD archive to look for potential co-locations with Sentinel-1 data. With additional enhancements to the training procedure and the incorporation of additional inputs, the resulting model demonstrates improved accuracy in rainfall estimates and the ability to extend its performance to scenarios up to 15 m/s.","1.1 Context Accurate rainfall estimation holds significant importance across various domains, including meteorology, hydrology, and disaster management. Rainfall can be estimated using in situ gauges, which provide continuous but localized measurements, or through remote sensing techniques. The latter commonly involves ground-based weather radar or satellite observations. While ground-based radars offer high temporal and spatial resolution, they are location-bound and impacted by topography. On the contrary, satellite observations offer global coverage with lower temporal and spatial resolution. Existing products like the Integrated Multi-satellitE Retrievals for Gpm (IMERG) (Huffman et al., 2020) and Climate prediction center MORPHing method (CMORPH) (Joyce et al., 2004) provide data at an approximate resolution of 10 km per pixel. Both methods combine passive low-orbit microwaves satellites with geostationary infrared sensors. Common microwaves satellites include Tropical Rainfall Measuring Mission (TRMM) (Simpson et al., 1996), the Global Precipitation Measurement (GPM) (Skofronick-Jackson et al., 2018), or the Defense Meteorological Satellite Program (DMSP) (Weng et al., 1994). The low orbit satellites have a resolution of a few kilometers per pixel, but are not continuously over the zones of interest, in contrast to the geostationary satellites. Examples of geostationary instruments used for this application are the Advanced Baseline Imager of the Geostationary Operational Environmental Satellites (GOES/ABI) (Schmit et al., 2017), the Advanced Himawari Imager (AHI) (Da, 2015), or the Spinning Enhanced Visible and Infrared Imager of the Meteosat Second Generation (MSG/SEVIRI) (Schmetz et al., 2002). The Rain Rate Quantitative Precipitation Estimate (RRQPE) is available from Himawari and GOES at 2km/px (Ba and Gruber, 2001). Another example of earth-observation satellites is the Sentinel-1 constellation, part of the Copernicus program, and board Synthetic Aperture Radar (SAR) instruments (Geudtner et al., 2014). The Sentinel-1 constellation consists of several satellites in a low-polar orbit: Sentinel-1A (launched in 2014), Sentinel-1B (launched in 2016 before experiencing failure at the end of 2021), and Sentinel-1C (expected to be launched in 2024). Sentinel-1D is expected to follow at a later date. They provide observation of the ocean both for coastal areas (producing continuous slices 250 km in width, refered as “Interferometric Wideswath”, or IW) and the open ocean (producing regularly spaced observations around 20 by 20 kilometers) at a resolution of several meters per pixel, depending on the observation mode. SAR are able to detect heavy rain on the land by determining soil moisture (Bazzi et al., 2019). Over the oceans, SAR has been recognized for its ability to image the signature of hydrometeors. Their effects are numerous, both in the air column and on the ocean surface. 1.2 Previous studies Recent studies have demonstrated the effect of atmospheric factors like the melting layer (Alpers et al., 2021). In addition, the splash of hydrometeors generates ring waves that also increases the sea surface roughness (Black et al., 1985). Adversarial effects are also reported, such as a long-lasting dampening of the waves produced by heavy rainfall (Atlas and Black, 1994) which decrease the sea surface roughness (SSR). Most importantly, rain conditions also correlate with changes in the air circulation, particularly under convective cells (Alpers et al., 2016). These downdrafts, upon reaching the surface, lead to downbursts, which are localized strong winds that can reach to devastating speeds. They are also referred to as Straight Line Winds (SLW), as opposed to tornadic winds, and are know to increase with climate change (Prein, 2023). The combined possibilities to estimate heavy rain patches, together with the wind maps derived from SAR at high-resolution will help quantify Straight Line Winds (SLW) and heavy rains, which is a unique contribution from SAR. Disentangling the intricate interactions between SSR and meteorological phenomena, particularly wind, poses challenges (Guo et al., 2022) and is a subject of ongoing research (Subrahmanyam et al., 2023). Recent efforts have aimed to estimate rainfall from SAR observations. Colin et al. (2024) segment precipitation into three levels (roughly equivalent to 1, 3 and 10 mm/h) a fully-convolutional deep learning model trained using 53 IW collocated with NEXRAD weather radar. However, it suffers from poor performances at wind speeds higher than 8 m/s, especially overestimation of the rain under downslope winds. Guo et al. (2023) categorize SAR patches of 5 km by 5 km from 125 IW into four classes (based on three thresholds at 2.5, 8 and 16 mm/h) based on GPM collocations but does not study the impact of the wind speed on the rain detection. Both studies opt to estimate rain intensity grades, which illustrates the difficulty in retrieving a continuous precipitation rate. Zhao et al. (2023) develop a continuous function to derive precipitation rates from 7 dual-polarization IW of cyclones, validated against measurements from TRMM, which has a resolution of 0.25° x 0.25° (approximately 28 km x 28 km at the equator) and 3h. This precipitation rate estimation requires ancillary information of the surface currents, winds and waves to isolate the patterns caused by the precipitation, whose mis-estimation would hinder the rainfall estimation. In particular, the wind speed information relies on the cross-polarization radiometric signal and is therefore particularly sensitive to the estimation of the Noise Equivalent Sigma Zero (NESZ) whose azimuthal contribution is not taken into account on most of the dataset. These studies highlight the difficulty of obtaining large quantity of collocated rain information, especially at high resolution. However, the prospect of achieving a SAR-based rainfall product may be of interest in complementing satellite observations used in data assimilation frameworks, where observations are used to re-calibrate physic models and are commonly used in hydrology (R. et al., 2012). Rainfall information is notably of interest for improving early flood warnings (Torcasio et al., 2021; Giannaros et al., 2022), and the high resolution of SAR observations could provide valuable information on the morphology and intensity of rain cells. The objective of this paper is to improve the quantitative estimation of precipitation rates, particularly in challenging conditions involving strong winds and complex interactions with sea surface roughness. 1.3 Challenges and strategies Collocating two satellite-based sensors globally involves a time lag and deals with the low spatial resolution of space-based rain data. This issue is mitigated with ground-based observations from weather radar, though it is to the detriment of the variety of locations as only coastal regions are available for studying co-observations with SSR. Examples of weather radar instruments are the Next-Generation Radar (NEXRAD) (Heiss et al., 1990) in the United States of America or the Operational Program on the Exchange of Weather Radar information (OPERA) in Europe (Saltikoff et al., 2019). This paper aims to address data scarcity by systematically collocating NEXRAD and Sentinel-1 observations. Particular attention is given to the co-occurrence of strong wind and precipitation due to both of their extrema being infrequent (Cho et al., 2004; Morgan et al., 2011), yet tremendously important for risk management, for understanding the meteorological situation, and because of their concurrent effect on SAR measurements (Zhang and Alpers, 2018). In earth observation, sensor collocation can lead to misalignment issues (Faiza et al., 2012). These misalignments often arise from discrepancies between proxy measurements and actual groundtruths. When matching NEXRAD data with ground gauges, (Dalezios, 1990) noticed a horizontal drift ranging from 2 to 5 km due to hydrometeor advections. Similarly, while collocating lightning flashes with precipitation rates (Xu et al., 2013) observed misalignment, on a 5 km/px grid, between areas of active lightning and of heavy rainfall. For SAR data, the adversarial effects of precipitation on backscattering make it difficult to correct such misalignment, which can span from several hundred meters to a few kilometers. Unlike Colin et al. (2024), the present paper avoids manual realignment, which is labor-intensive and reduces the size of the data set. Instead, the paper optimizes a deep learning model with a multi-objective loss function to minimize pixel-level regression errors while preserving patch-level statistics. Multi-objective learning (Ruder, 2017) serves as a form of regularization (Girard et al., 2019) and ensures that the model meets predetermined properties (Fablet et al., 2020) of a known distribution. Among the additional terms of the loss, we introduce an adversary loss (Schonfeld et al., 2020) to further constrain the output distribution. In the subsequent sections, we describe the dataset of collocated Sentinel-1 and NEXRAD co-observations. We then elaborate on the training methodology and finally present the model’s evaluation and its performance analysis concerning various meteorological characteristics."
https://arxiv.org/html/2411.03475v1,Self Supervised Networks for Learning Latent Space Representations of Human Body Scans and Motions,"This paper introduces self-supervised neural network models to tackle several fundamental problems in the field of 3D human body analysis and processing. First, we propose VariShaPE (Varifold Shape Parameter Estimator), a novel architecture for the retrieval of latent space representations of body shapes and poses. This network offers a fast and robust method to estimate the embedding of arbitrary unregistered meshes into the latent space. Second, we complement the estimation of latent codes with MoGeN (Motion Geometry Network) a framework that learns the geometry on the latent space itself. This is achieved by lifting the body pose parameter space into a higher dimensional Euclidean space in which body motion mini-sequences from a training set of 4D data can be approximated by simple linear interpolation. Using the SMPL latent space representation we illustrate how the combination of these network models, once trained, can be used to perform a variety of tasks with very limited computational cost. This includes operations such as motion interpolation, extrapolation and transfer as well as random shape and pose generation.","1.1 Motivation The examination of human body shapes holds significant importance in various fields such as computer vision, graphics, and virtual reality. The key deliverables in this areas include: 1. Fast algorithms for clustering and classification of (large scale) data of unregistered human body scans; 2. Efficient computational frameworks for accurate motion transfer, motion interpolation and motion extrapolation; 3. Generative modelling via empirical distributions learned from (large scale) unregistered human body scan data bases. Numerous studies [2, 16, 29, 19, 24] have focused on addressing these challenging computational tasks by parameterizing the space of human body shapes using low-dimensional latent space models. However, several difficulties persist with these latent space parameterizations. One key challenge consists in the non-uniqueness of the raw shape data, i.e., the same human body shape can be represented by infinitely many different mesh representation. In the continous (smooth) setting this non-uniqueness is encoded by defining two shapes to be the same if they only differ by a reparametrization. The analogue in the discrete setting is to consider two human body scans (represented as simplicial complexes) to be the same if they only differ by a remeshing operation. This non-uniqueness makes the task of retrieving latent code representations from unregistered data significantly more challenging. In the quest for faithful latent space representation of the space of human body shapes it is thus paramount to develop mesh invariant latent space representations from raw body scans with minimal mesh preprocessing and computational demands. A second key challenge consists of the fact that the linear geometry of a given latent space representation will not accurately describe the non-linear nature of human body movements, which brings up the challenging task of accurately capturing the geometry of human motions within latent spaces and thereby obtaining a faithful computational framework based on these latent space representations. 1.2 Contributions Towards these ends, we propose two self-supervised deep learning models which can be used in conjunction to address these demands: 1. First, we introduce VariShaPE (Varifold Shape Parameter Estimator), a self-supervised neural network model for real-time mesh invariant latent space encoding of human body scans; 2. Second, we derive MoGeN (Motion Geometry Network), a framework for learning the geometry of human body motion latent spaces from 4D-data. We note that, as demonstrated in Section 4, these two models can be used in conjunction to obtain efficient (real time) and accurate algorithms for operations such as motion transfer, generative modelling and 4D-interpolation. Next we describe the benefits of the two methods, as compared to state-of-the-art algorithms, in more details. Benefits of VariShaPE and Comparison to State-of-the-art Methods: Finding low-dimensional parametric representation for the statistical analysis of human shapes is an active research area [2, 16, 29], that dates back to the groundbreaking work on 3D Morphable Models for faces by Blanz et al. [4]. In the particular case of modelling for whole body scans, common approaches include skeleton based models in combination with physically motivated transformation models. Among those, one of the most widely used approaches, is the SMPL latent space [19], which is learned from thousands of 3D body scans; see also [26, 24] for recent extensions that further enhance this model by adding eg. articulated hands and face expressions. Our first contribution, VariShaPE, deals with the challenging task of estimating such latent space representations given raw data, i.e., to retrieve the best latent parameter to fit a given unregistered scan of a human body. Standard methods for this retrieval task are usually optimization based and often require manual intervention in order to lead to satisfactory results [19]; this includes spectral based methods such as the functional map framework [25], minimizing the Hausdorff or Chamfer distance [9, 13], or methods based on Riemannian geometry and elastic shape analysis [15]. In contrast to these methods our model employs a self-supervised network approach making use of concepts of geometric measure theory [18], which allows us to incorporate shape invariances directly in the network architecture and thereby significantly boost the performance of the corresponding algorithms. A further key benefit of the proposed VariShaPE model is that it can be trained on (applied to, resp.) raw scans without any additional mesh preprocessing. This is precisely due to the aforementioned geometric measure theory inspired architecture, which is specifically designed to ensure complete mesh invariance, i.e., our model will produce (approximately) the same latent code representations of scans that represent the same human body invariant to the mesh structure or scanning noise which appears in real data. An additional benefit of our framework is the fast training time as compared to other network-based approaches in human shape analysis. This again stems from our use of geometrically motivated constructions in the network, which allows us to encode the invariances of the data directly in the corresponding architecture. Thereby we are able to significantly reduce the amount of trainable parameters and thus require an order of magnitudes lower amount of training time as compared to previous approaches such as LIMP [8], ARAPReg [17], or 3D-Coded [13]. Finally we present results that show our latent code retrieval framework is competitive with the state of the art 3D-Coded [13] method for shape registration while requiring orders of magnitude less computational time, c.f Table 1 in Section 3.1. Additionally we provide experiments to highlight our models superior robustness to mesh resampling, c.f Table 2 in Section 3.1. Benefits of MoGeN and Comparison to State-of-the-art Methods: Once data (eg. human body scans) is embedded in a linear (Euclidean) latent space, one directly obtains a corresponding framework for statistical analysis: operations such as interpolation and extrapolation can be simply defined by performing these operations in the linear latent space. In the context of human body scans, this raises the issue of whether the linear geometry of the latent space accurately describes the complex movements and deformations appearing in such data, see also [21, 20, 30] for a discussion on this matter in the context of latent space interpolation for general machine learning applications. Indeed, the performance of such a naive approach is rather limited in the context of large movements of human bodies. To address these issues multiple (physically motivated) deformation energy losses have been introduced in the training phase, thereby altering the corresponding latent space: this includes geodesic distances [8], the As-Rigid-As-Possible (ARAP) energy [17, 22], or volumetric constraints [3]. See also [31, 10] for an approach using manifold regularization of learned pose spaces. These geometric quantities, however, significantly increase the total training costs of those approaches. More importantly, they are still based on user-defined assumption to model the movement of human shapes. Our work differs in two different aspects from these approaches: first we keep the latent space unchanged, but instead equip it with a different non-linear geometry. Secondly, we do not make any assumptions on the physics behind the deformations of human body motions, but instead learn them in a purely data-driven approach using 4D training data. In our experiments we argue that this data driven approach leads to a significantly improved performance in terms of interpolation and extrapolation accuracy. A further key benefit of this methodology is that it allows us to capture more complex motions of human bodies, including those which reach the ”boundary” of feasible human motions. Future work and Choice of Latent Space Model: In summary, we have proposed in this paper new self-supervised models for learning not only latent space representations of human body shape and pose but also the underlying geometry of the latent space itself. The latter one is achieved through the introduction of an additional lifted space in which observed body pose motions get unfolded to straight paths. We have further shown how these two frameworks can be used to perform common tasks such as motion interpolation, extrapolation and transfer, and to estimate generative models in the space of human body pose and shape. In our experimental section, we demonstrate the capabilities of our models using data from the DFAUST dataset and by employing the SMPL model (A Skinned Multi-Person Linear body model) [19] as the corresponding latent space representation. We want to emphasize, that our framework is not tied in any way to this particular choice, but could be directly applied to any other existing latent space representation for human bodies, such as SMPL-X [26], STAR [24], BLISS [23] or the elastic shape analysis basis representation used in [15, 28]. While we aim to extend our framework in future work to these latent spaces, we decided to focus in the experiments of this present paper on the SMPL representation, due to its convenient implementation in PyTorch and its wide use in the human body shape community. In one of our ablation studies, cf. Appendix A, we demonstrate a similar performance using the elastic shape analysis basis setting as an alternative latent space representation; in future work we plan to explore the use of some of the recent SMPL extensions, such as the above mentioned STAR [24]."
https://arxiv.org/html/2411.03405v1,Fine-Grained Spatial and Verbal Losses for 3D Visual Grounding,"3D visual grounding consists of identifying the instance in a 3D scene which is referred to by an accompanying language description. While several architectures have been proposed within the commonly employed grounding-by-selection framework, the utilized losses are comparatively under-explored. In particular, most methods rely on a basic supervised cross-entropy loss on the predicted distribution over candidate instances, which fails to model both spatial relations between instances and the internal fine-grained word-level structure of the verbal referral. Sparse attempts to additionally supervise verbal embeddings globally by learning the class of the referred instance from the description or employing verbo-visual contrast to better separate instance embeddings do not fundamentally lift the aforementioned limitations. Responding to these shortcomings, we introduce two novel losses for 3D visual grounding: a visual-level offset loss on regressed vector offsets from each instance to the ground-truth referred instance and a language-related span loss on predictions for the word-level span of the referred instance in the description. In addition, we equip the verbo-visual fusion module of our new 3D visual grounding architecture AsphaltNet with a top-down bidirectional attentive fusion block, which enables the supervisory signals from our two losses to propagate to the respective converse branches of the network and thus aid the latter to learn context-aware instance embeddings and grounding-aware verbal embeddings. AsphaltNet proposes novel auxiliary losses to aid 3D visual grounding with competitive results compared to the state-of-the-art on the ReferIt3D benchmark.","With the ever-increasing popularity of the interaction- and 3D-heavy applications of augmented/virtual reality and human-support-oriented robotics in indoor home environments, bridging the gap between language understanding and 3D visual perception is becoming essential. This need has motivated a large body of recent works on the task of 3D visual grounding [43, 42, 18, 16, 32], surveys papers [21], which consists in identifying and localizing the object in the input 3D scene which is referred by a language description. 3D visual grounding works are classified into two main categories: (i) referral-based 3D object identification methods and (ii) referral-based 3D object localization methods. The former variant requires as visual inputs both the raw 3D point cloud and the ground-truth 3D bounding boxes or instance masks of all objects in the input scene which belong to the class of the referred object [2]. On the contrary, the latter variant consists in the 3D localization of the referred object directly from the raw 3D point cloud [5]. While the two variants might appear fundamentally different at first sight, referral-based localization reduces to referral-based identification after candidate object boxes or masks have been generated. 3D grounding is typically performed in both cases in a grounding-by-selection fashion, using a verbo-visual fusion module which aims to classify the referred object based on the language description. The de-facto choice in the literature [43, 36, 42, 18, 5] for supervising the above verbo-visual fusion module is via a supervised cross-entropy loss on the module’s softmax classification outputs over the set of candidate objects. The limitations of this loss are that (i) it treats the candidate 3D objects as isolated entities that independently correlate with the language description, even though these objects co-exist in the 3D scene and participate in mutual spatial arrangements and relative configurations, and (ii) it views the language description as a monolithic entity, neglecting its internal structure. Some works [17, 43, 2] extend supervision via an additional, language-driven cross-entropy loss on a class distribution of the referred object predicted from a global language embedding. While this language-driven loss can increase the awareness of language features about the semantics of the referred object, it does not lift limitation (ii) above, as it still operates on a global language embedding, which obscures the attribution of gradients to individual word embeddings that are more closely associated with the referred object. Another line of works [18, 36] proposes contrastive losses between verbal and visual embeddings to inject a better structure in the joint verbo-visual embedding space. However, such cross-modal contrast improves separation between distinct instance embeddings only indirectly by pulling them to / pushing them away from sentence [36] or word [18] embeddings and it still treats each instance individually and not jointly with others, leaving limitation (i) above unaddressed. In this work, we attempt to smooth the loss manifold for 3D visual grounding by proposing two novel losses to overcome the two aforementioned limitations of the basic supervised grounding-by-selection setup. First, we propose to make 3D instances aware of spatial visual grounding context by letting each of them regress a 3D offset vector pointing to the ground-truth referred instance. In this way, we encourage the instance embeddings to attend to each other in a way that is informed about the spatial location to which the semantics of the referral is grounded. The specific implementation we follow for the verbo-visual fusion module involves a basic attention-based building block. As we aim at injecting the aforementioned grounding awareness into embeddings from an early-on stage of the verbo-visual fusion module, we add offset vector regression heads after each such attention block and supervise each of them with a distinct offset regression loss. For the attention-based block, we build on the generic design proposed in [36] but reverse the bottom-up masked attention into a top-down counterpart. In this transformer architecture, earlier attention blocks are allowed to attend to a larger spatial context of instances in order to be able to already predict potentially large offsets if necessary, while later blocks are confined to a narrower attention mask, as they have already attended to more distant parts of the scene in earlier stages. Second, we propose to exploit the input language description at a finer level of granularity than the previously considered global sentence level by supervising the predicted word-level span of the ground-truth referred object in the description, which is produced by the final verbal embeddings of the verbo-visual fusion module. These spans are normally available in standard 3D visual grounding datasets [2], but they have been largely overlooked by virtually all existing works when designing their models. By applying our proposed span loss, we can precisely attribute the gradient of each of the terms of the loss back to the embedding of the corresponding word, treating words inside the span differently from words outside it. This makes our word embeddings aware of the verbally described characteristics of the referred object. To allow this verbal awareness to be combined with visual awareness and thus produce word embeddings that are discriminative for the referred instance, we additionally enable cross-attention in the direction from the visual branch to the verbal branch in the attention-based building blocks of our verbo-visual fusion module, in a bidirectional attention formulation similar to that in [18]. Put together, our two novel losses along with top-down bidirectional attentive fusion form our complete AsphaltNet architecture for 3D visual grounding. AsphaltNet is evaluated on the Nr3D and Sr3D benchmarks and compared to the state-of-the-art approaches."
https://arxiv.org/html/2411.03403v1,Enhancing Maritime Situational Awareness by End-to-End Onboard Raw Data Analysis,"Satellite-based onboard data processing is crucial for time-sensitive applications requiring timely and efficient rapid response. Advances in edge artificial intelligence are shifting computational power from ground-based centers to on-orbit platforms, transforming the ""sensing-communication-decision-feedback"" cycle and reducing latency from acquisition to delivery. The current research presents a framework addressing the strict bandwidth, energy, and latency constraints of small satellites, focusing on maritime monitoring. The study contributes three main innovations. Firstly, it investigates the application of deep learning techniques for direct ship detection and classification from raw satellite imagery. By simplifying the onboard processing chain, our approach facilitates direct analyses without requiring computationally intensive steps such as calibration and ortho-rectification. Secondly, to address the scarcity of raw satellite data, we introduce two novel datasets, VDS2Raw and VDV2Raw, which are derived from raw data from Sentinel-2 and Vegetation and Environment Monitoring New Micro Satellite (VENµS) missions, respectively, and enriched with Automatic Identification System (AIS) records. Thirdly, we characterize the tasks’ optimal single and multiple spectral band combinations through statistical and feature-based analyses validated on both datasets. In sum, we demonstrate the feasibility of the proposed method through a proof-of-concept on CubeSat-like hardware, confirming the models’ potential for operational satellite-based maritime monitoring.","Considering the enormous impact on maritime applications, the efficient and timely recognition of vessels using Earth Observation (EO) satellite imagery is imperative for scenarios requiring rapid response. Applications such as traffic and environmental monitoring, emergency search and rescue operations, and detecting illegal fishing or smuggling require rapid, low-latency responses to be effective. The traditional data processing chains based on the classical bent-pipe approaches [1, 2] face significant challenges in addressing these needs due to several inherent limitations, such as the increased latency for data download, ground station availability and high-level product calculation, thereby incurring substantial delays from image acquisition to information delivery [3]. Additionally, the need to transmit satellite data to Earth strains the available communication bandwidth [1, 2]. AI can enable real-time data processing, reducing the need for large data transfers to Earth and accelerating response times to various events [4]. A growing body of research is focused on leveraging Artificial Intelligence (AI) onboard satellites to retrieve actionable information for latency-sensitive applications quickly. This includes natural disaster response [5, 6, 3] and the detection of anomalies or targets in localized areas [7, 8]. The increasing interest in this area is further demonstrated by the rising number of companies and research institutes worldwide developing advanced edge AI avionics subsystems for CubeSats [9]. Recent missions, such as ΦΦ\Phiroman_ΦSat-2 [10], Intuition-1 by KP Labs, CogniSAT-6U by Ubotica [11], and Kanyini by SmartSat CRC [12], showcase the growing commitment to AI-enabled satellite technology. These missions utilize AI-enabled processing units, demonstrating the significant advantages of AI in enhancing real-time maritime surveillance and other critical applications. This trend underscores the pivotal role of onboard AI as the key to revolutionizing maritime monitoring and response, providing faster, more accurate, and more efficient operations in the ever-challenging maritime domain. Past satellite missions, such as ΦΦ\Phiroman_Φ-Sat-1 [2] and HYPSO-1 [13], relied on extensive pre-processing workflows, including geometric and radiometric corrections, to prepare data for onboard Machine Learning (ML) applications. ΦΦ\Phiroman_Φ-Sat-1, for example, implemented a Convolutional Neural Network (CNN) on an Intel® Movidius™ Myriad™ 2 Vision Processing Unit (VPU), processing selected hyperspectral bands after creating the hyperspectral data cube and performing band-to-band spatial registration. Similarly, HYPSO-1 employed a hyperspectral payload to monitor ocean color, requiring onboard image processing that involved linear radiometric and geometric corrections. Although current research is advancing towards novel technologies, particularly in Deep Learning (DL), few attention has been dedicated to the pre-processing stage, which continues to mimic traditional ground-based workflows. The reliance on such demanding schemes necessitates specialized hardware on orbit which increases payload complexity and resource utilzation. Therefore, to optimize onboard resource usage while providing real-time actionable information, it is essential to bypass all unnecessary pre-processing steps, realising models that can analyse raw imagery. Tackling this issue poses several challenges, including hardware and energy limitations, resource efficiency of ML models, but also the scarcity of available raw data datasets. The very first direct attempt to process raw data end-to-end for onboard machine learning was made during the “OPS-SAT Case” competition [14, 15], hosted on the European Space Agency (ESA) Kelvins platform, which focused on few-shot learning for satellite applications. This effort represented a significant shift from previous missions, directly tackling the challenges of onboard raw data processing. Building on this, Meoni et al. [16] developed a methodology for creating raw datasets using Sentinel-2 (S-2) imagery, providing the first raw dataset of thermal hotspots. Continuing this trend, Del Prete et al. [8] introduced VDS2Raw, a dataset specifically designed for vessel detection using raw S-2 data, offering a comparative analysis of various DL techniques for the task. ΦΦ\Phiroman_ΦSat-2 [10] offers the possibility of processing Level-1A (L1A) data on board thanks to an advanced data processing framework for generating multispectral and panchromatic imagery at three distinct levels 111The L1A product provides Top Of Atmosphere (TOA) radiance without geo-referencing or band alignment, while Level 1B enhances this with precise geo-referencing and band alignment. Level 1C delivers TOA reflectance with fine geo-referencing and band alignment, though it lacks orthorectification.. Emphasizing on this innovative apporach of raw data exploitation, the Orbital AI challenge [17] was the first to explore onboard AI applications with raw satellite data simulated. This manuscript builds upon the aforementioned works and makes several novel key contributions: 1. End-to-End Workflow: This work presents the first fully integrated end-to-end workflow for onboard vessel identification (detection and classification), streamlining the process from data acquisition to information delivery, thereby reducing latency and improving the efficiency of maritime surveillance operations. 2. Dataset Creation: Two new datasets of raw, uncalibrated multispectral data for vessel detection & classification have been created, incorporating additional Automatic Identification System (AIS) information and a novel annotation format that includes both bounding boxes and AIS records. These datasets are built from two different EO space missions with different characteristics 3. Spectral Band Analysis: Through the application of DL techniques on the developed datasets, the study identifies the most useful raw spectral bands across the spectrum for vessel detection and classification, providing insights into the performance across different geographic areas, sensors, and resolutions. 4. Onboard Implementation: A proof-of-concept for onboard implementation is demonstrated using an AI edge device with flight heritage, such as CogniSAT-6 [11, 18]. This deployment, tested under varying sensor operating conditions, confirms the feasibility of real-time and efficient maritime surveillance. The remainder of the paper is organized as follows. Section 2 details the data creation and curation strategy for the two raw multispectral datasets designed for vessel detection and classification, enriched with AIS information. Section 3 discusses the proposed methodology, focusing on the cascaded application of coregistration and DL-based detection techniques. Section 4 presents the results obtained from the developed datasets, highlighting the generalization and applicability of the proposed approach to several multispectral bands. Section 5 is dedicated to the onboard implementation, showcasing a proof-of-concept demonstration by representative hardware. Finally, Section 6 provides a discussion of the results and concludes the paper."
https://arxiv.org/html/2411.03359v1,Self-Calibrated Tuning of Vision-Language Models for Out-of-Distribution Detection,"Out-of-distribution (OOD) detection is crucial for deploying reliable machine learning models in open-world applications. Recent advances in CLIP-based OOD detection have shown promising results via regularizing prompt tuning with OOD features extracted from ID data. However, the irrelevant context mined from ID data can be spurious due to the inaccurate foreground-background decomposition, thus limiting the OOD detection performance. In this work, we propose a novel framework, namely, Self-Calibrated Tuning (SCT), to mitigate this problem for effective OOD detection with only the given few-shot ID data. Specifically, SCT introduces modulating factors respectively on the two components of the original learning objective. It adaptively directs the optimization process between the two tasks during training on data with different prediction uncertainty to calibrate the influence of OOD regularization, which is compatible with many prompt tuning based OOD detection methods. Extensive experiments and analyses have been conducted to characterize and demonstrate the effectiveness of the proposed SCT. The code is publicly available at: https://github.com/tmlr-group/SCT.","The deep neural networks (DNNs) are demonstrated to be overconfident on the OOD data out of the pre-defined label space (Hendrycks and Gimpel, 2017), which can induce severe problems in those safety-critical applications like autonomous driving or medical intelligence. Various explorations (Liang et al., 2018; Djurisic et al., 2022; Du et al., 2022; Zhu et al., 2023b) thus have been conducted in designing scoring functions or fine-tuning methods with auxiliary outliers to improve the OOD distinguishability. Specially, with the emergence of the powerful pretrained vision-language models (VLMs) (Radford et al., 2021), a series of prompt tuning based methods (Miyai et al., 2024b; Tao et al., 2023; Bai et al., 2023; Ming et al., 2022c) show impressive performance in current OOD detection benchmarks, with the regularization given only few-shot in-distribution (ID) data. Generally, these regularizations (Wang et al., 2023; Miyai et al., 2024b) are built upon the ID-irrelevant local context as the surrogate OOD source, which is extracted by VLMs (refer to Figure 1) based on its alignment with ID-class text features. Although this saves the costly collection of auxiliary outliers from the open world, the quality of the ID-irrelevant local context also becomes the bottleneck, which can be greatly affected by the foreground-background decomposition with VLMs. Specifically, as revealed in previous studies (Oh et al., 2023; Tu et al., 2024; Wang et al., 2024), the prevalent VLMs struggle with poor calibration, which means that the decomposition performance on downstream data might not be well guaranteed. Thus, it naturally motivates the following question: Can we flexibly leverage the imperfect OOD features extracted by the VLM itself, to facilitate the few-shot prompt tuning for effective OOD detection? As illustrated in Figure 1, a significant portion of extracted local context from ID data are not valid OOD features due to the inevitable imperfect decomposition. Consequently, OOD regularization based on such unreliable OOD features may potentially constrain the improvement of OOD detection. To investigate this problem, we conduct a proof-of-concept experiment on CLIP with ImageNet as the ID dataset and prompt-tune the model with different groups of data divided by their overall prediction uncertainty. Specifically, we find that the performance of prompt tuning based methods significantly deteriorates as the uncertainty of the given ID data rises, as presented in Figure 2, which motivates us to leverage such clues to overcome the current issue. Intuitively, as the model prediction on ID samples is less certain, the OOD features extracted from these data are less reliable. Performing OOD regularization on such unreliable surrogate OOD features can degrade the OOD detection performance of CLIP. Therefore, a potential idea is to adaptively adjust the importance of OOD features extracted from ID data according to their prediction uncertainty during model training. Figure 1: Imperfect foreground background decomposition. The top row shows the original images from ImageNet-1k and the bottom row shows the ID-irrelevant context extracted from the original images (shown as the colored patches of images on the second row), using CLIP fine-tuned with CoOp on 16-shot data. Due to the imperfect decomposition of fine-tuned vision-language models, large portions of the extracted local features from ID data belong to ID-related regions, thus harming the performance of OOD detection. More illustrations are presented in the Appendix A.3.3. Based on the previous observation, we propose a new learning framework, i.e., Self-Calibrated Tuning (SCT), to alleviate the problem induced by spurious OOD features. At the high level, we aim to dynamically adjust the weight of OOD regularization from different training samples based on their prediction uncertainty to calibrate their influence on model training. In detail, we introduce modulating factors based on the sample uncertainty estimation respectively on the two parts of the original learning objective of prompt tuning for OOD detection (refer to Eq (4)). Under this new learning framework, the model’s attention is directed towards the classification task to better generalize to the downstream ID dataset when training with low-confidence data. OOD features extracted from high-confidence ID data are attached more importance to achieve more effective OOD regularization. The redirection effect of these two modulating factors facilitates VLMs learning from imperfect OOD features to ultimately improve the OOD detection of prompt tuning. Our main contributions can be summarized as follows, • Conceptually, we investigate the problem of imperfect OOD features extracted in prompt tuning based OOD detection methods and observe that ID data with different prediction uncertainty exhibits a distinctive influence on the OOD regularization. (in Section 3.2) • Technically, we propose a novel learning framework, namely Self-Calibrated Tuning (SCT), for facilitating prompt tuning for effective OOD detection given only few-shot ID samples, which conducts adaptive redirection of model optimization process between the two tasks to calibrate the influence of OOD features mined from different ID data. (in Sections 3.3) • Empirically, extensive experiments from different perspectives are conducted to verify the effectiveness of SCT in improving OOD detection performance. To be specific, SCT improves the false positive rate (FPR95) by 3% compared to the previous best method on the large-scale ImageNet-1k (Deng et al., 2009) benchmark. Furthermore, we perform various ablations and further discussions to provide a thorough understanding. (in Section 4)"
https://arxiv.org/html/2411.03340v1,Unlocking the Archives: Large Language Models Achieve State-of-the-Art Performance on the Transcription of Handwritten Historical Documents,"This study demonstrates that Large Language Models (LLMs) can transcribe historical handwritten documents with significantly higher accuracy than specialized Handwritten Text Recognition (HTR) software, while being faster and more cost-effective. We introduce an open-source software tool called Transcription Pearl that leverages these capabilities to automatically transcribe and correct batches of handwritten documents using commercially available multimodal LLMs from OpenAI, Anthropic, and Google. In tests on a diverse corpus of 18th/19th century English language handwritten documents, LLMs achieved Character Error Rates (CER) of 5.7 to 7% and Word Error Rates (WER) of 8.9 to 15.9%, improvements of 14% and 32% respectively over specialized state-of-the-art HTR software like Transkribus. Most significantly, when LLMs were then used to correct those transcriptions as well as texts generated by conventional HTR software, they achieved near-human levels of accuracy, that is CERs as low as 1.8% and WERs of 3.5%. The LLMs also completed these tasks 50 times faster and at approximately 1/50th the cost of proprietary HTR programs. These results demonstrate that when LLMs are incorporated into software tools like Transcription Pearl, they provide an accessible, fast, and highly accurate method for mass transcription of historical handwritten documents, significantly streamlining the digitization process.","Introduction As historians and archivists digitize ever larger collections of handwritten records, accurate transcription remains a barrier to their systematic analysis, publication, and accessibility. It is also the most time-consuming and costly aspect of the digitization process. The field of Handwritten Text Recognition (HTR) attempts to automate the task through machine learning but it is a complex and technologically advanced subfield of computer vision and artificial intelligence research, an area in which few historians are comfortable operating. In recent years, programs like as Transkribus, run by the READ-COOP cooperative which includes more than 150 major universities and archives, has made HTR far more accessible through graphical, “drag and drop” interfaces (Kahle et al 2017; Nockels et al 2022). Transkribus largely automates the most complex image pre-processing and segmentation elements of the workflow and allows users to use pretrained neural network models to generate a rough transcription of handwritten documents. These models are highly accurate when they have been trained to recognize a specific individual’s handwriting, achieving word-level accuracy between 85 and 95% (Al Kendi, 2024). That said, they are also moderately expensive and time-consuming to use at roughly $0.27 USD per page with variable turnaround times. Although actual processing normally takes 15-20 minutes, depending on customer volumes transcription requests can be placed in a queue for a day or more. A more important issue is that because handwriting is unique, the most impressive results do not normally reflect most users’ experiences with HTR. Historians are often discouraged when they find that most HTR models have significant difficulty generalizing “out of the box” to new hands, document formats, or styles that they did not encounter in training. By “out-of-the-box”, we mean a model that can be employed “as is” without the user having to train the model on a specific handwriting style. On such tasks, they typically achieve word level accuracy of only 50-75% which make the results largely unusable without major editing. Teaching or fine-tuning models to recognize a specific, individual handwriting style requires users to generate perfect transcriptions of at least 75 handwritten pages (about 15,000 words)—referred to as “ground-truth” documents—which can then be used as training data. As a result, unless historians are working with hundreds or thousands of pages of documents written in a single hand, this fine-tuning process is rarely worth the effort. This means that for most historians, HTR remains a tantalizing but impractical solution to the transcription problem. In this paper, we introduce Transcription Pearl, an easy-to-use software tool that automatically transcribes batches of handwritten documents “out-of-the-box” using widely available multi-modal Large Language Models (LLMs) like GPT-4o, Claude Sonnet-3.5, and Gemini 1.5-Pro, quickly, cost effectively, and to a high degree of accuracy. Depending on how capitalization, punctuation, and historical spelling errors are evaluated, it achieves accuracy levels of between 84 and 93% on transcription tasks without the need for image pre-processing or fine-tuning. We also demonstrate that unlike conventional HTR software, LLMs can be used to correct transcriptions generated by other LLMs as well as Transkribus to levels of accuracy that would normally require extensive fine-tuning, that is above 96%. At this level of accuracy these transcriptions not only approach human levels of accuracy but are “good enough” for most day-to-day use cases such as full text keyword and semantic search as well as improving accessibility and readability. We thus show that the application of LLMs to HTR potentially provides an accessible, affordable, fast, and accurate pathway for both mass transcription projects and individual historians working on their own corpus of records."
https://arxiv.org/html/2411.04112v1,Fed-EC: Bandwidth-Efficient Clustering-Based Federated Learning For Autonomous Visual Robot Navigation,"Centralized learning requires data to be aggregated at a central server, which poses significant challenges in terms of data privacy and bandwidth consumption. Federated learning presents a compelling alternative, however, vanilla federated learning methods deployed in robotics aim to learn a single global model across robots that works ideally for all. But in practice one model may not be well suited for robots deployed in various environments. This paper proposes Federated-EmbedCluster (Fed-EC), a clustering-based federated learning framework that is deployed with vision based autonomous robot navigation in diverse outdoor environments. The framework addresses the key federated learning challenge of deteriorating model performance of a single global model due to the presence of non-IID data across real-world robots. Extensive real-world experiments validate that Fed-EC reduces the communication size by 23x for each robot while matching the performance of centralized learning for goal-oriented navigation and outperforms local learning. Fed-EC can transfer previously learnt models to new robots that join the cluster.","Poor availability of high-speed internet is limiting outdoor robots from realizing their full potential. In today’s world, robots are seamlessly deployed in diverse conditions all over the world, from bustling urban landscapes to rugged terrains in the wild. Many of these robots are using visually guided autonomy architectures powered by machine learning and self-supervision. Recent works [1], [2], [3], [4], [5] have shown that with access to large amounts of data, robots can achieve state-of-the-art navigation performance and can be deployed in various scenarios with minimum human intervention needed. These and other such methods are driving tremendous progress in self-driving cars [6], [7], robots navigation in indoor [8], [9] and outdoor environments [10], [11], [12]. However, in practice, traditional learning approaches require access to all of the data in one place, uploaded to a central server for model training requiring high speed internet. Furthermore, robots operating in the world experience diverse and varied environments requiring continuous upload of large amounts of data to the central server. While effective in controlled environments with high bandwidths, uploading big chunks of data can be a challenge for robots in environments where high-speed internet is not available, is intermittent, outright denied or even leads to significant battery power consumption. Federated learning (FL) [13] reduces the bandwidth requirement while enabling these robots to collectively enhance their learning by sharing model updates. With recent advancements in the capabilities of edge devices, federated learning takes advantage of edge computation to train models locally and shares model parameters instead of raw data with the server to learn a shared global model. FL also allows robots to send updates at intervals, rather than continuously streaming data reducing bandwidth usage. Further, through federated learning, there is hope that robots can gather insights from their respective environments, while also contributing to a global pool of knowledge to learn adaptable models for varied environments on the go. Traditionally, FL learns a single model that tries to minimize the average loss across robots. However, local data on deployed robots is highly non-IID due to different usage and operating locations. During FL, the divergence of the local datasets due to their non-IID nature leads to slower convergence and worsening learning performance when the models are aggregated. In such cases, a singular global model suffers and may perform worse than local models for some robots. With non-IID data, it is improbable that there exists a single global model that fits the needs for all robots. The global model can be biased and unfair. Current robotic systems that use federated learning frameworks do so in simulation [14] or in structured indoor environments [15] and do not account for heterogeneity that arises in the real-world deployment of robots. One way to avoid biased global models is to learn personalized models by clustering robots with similar local data distributions and training one aggregate model for each cluster. As a result, robots collaborate with only robots with similar experiences avoiding biases and negative performance. Previous clustered FL methods compare local model weights or gradients that rely on indirect information of the data distribution. [16] and [17] cluster the clients and learn individual cluster models but incur a high communication cost in doing so. In this paper, we highlight the first clustering-based system, Federated-EmbedCluster(Fed-EC) for self-supervised visually guided autonomous navigation which overcomes the need for high bandwidth speeds. Fed-EC is deployed on two different visual navigation models to showcase its modularity. To overcome the negative affect of non-IID data on model performance, Fed-EC groups the aggregation of local models by looking at similarity between the local datasets. Within each cluster group, the data is similar and mimics an IID set up ensuring that model aggregation does not degrade performance. Unlike previous methods where multiple rounds are needed [16] or multiple models are communicated [17], in each communication round the mean embedding vector which does not incur any additional communication cost is shared along with the local model. Fed-EC does not know the cluster identities beforehand and hence simultaneously identifies clusters within participating robots and learns individual cluster models in the federated setting. Figure 1: Workflow of Fed-EC. (a) Participating robots learn and communicates a mean embedding vector and local model weight to the server. The server clusters the robots using the mean embedding vector and aggregates local models in each cluster to learn a model which is shared with the robots based on their cluster identity. (b) The robots navigate to the a given GPS goal using the learnt model which takes as input RGB and depth images from the front facing camera. (c) If a new robot is deployed it computes a mean embedding and shares it with the server. The server assigns a cluster to the robot and sends the respective cluster model to the robot to use. In this paper, we consider robots in the wild that are constantly deployed with limited hardware on board, limited communication bandwidth, and battery power. The main contributions of our papers are as follows: • We propose a clustering-based personalized FL strategy Fed-EC, to overcome the problems generated by the heterogeneous nature of robotic operations. • We implement and test the framework of Federated learning in the robotics settings, in particular on real robots using two different navigation models to navigate to a given GPS point. • We validate through real-world robot experiments in diverse outdoor terrains that Fed-EC can perform as well as the centralized framework while reducing communication size and is better than just local training. We also show that learning a personalized FL model for each cluster is better than learning a singular global FL model over all robots. • We also show the transferability properties of our system to new robots that join the network."
https://arxiv.org/html/2411.04055v1,Multi-branch Spatio-Temporal Graph Neural Network For Efficient Ice Layer Thickness Prediction,"Understanding spatio-temporal patterns in polar ice layers is essential for tracking changes in ice sheet balance and assessing ice dynamics. While convolutional neural networks are widely used in learning ice layer patterns from raw echogram images captured by airborne snow radar sensors, noise in the echogram images prevents researchers from getting high-quality results. Instead, we focus on geometric deep learning using graph neural networks, aiming to build a spatio-temporal graph neural network that learns from thickness information of the top ice layers and predicts for deeper layers. In this paper, we developed a novel multi-branch spatio-temporal graph neural network that used the GraphSAGE framework for spatio features learning and a temporal convolution operation to capture temporal changes, enabling different branches of the network to be more specialized and focusing on a single learning task. We found that our proposed multi-branch network can consistently outperform the current fused spatio-temporal graph neural network in both accuracy and efficiency.","As global temperatures continue to rise, research has shown that the accelerated mass loss of polar ice sheets is increasingly contributing to climate change[2, 8, 11, 21]. Polar ice sheets comprise several internal ice layers formed in different years. A better understanding of the status of internal ice layers can provide valuable information on snowfall melting and accumulation and enable a comprehensive understanding of the global climate system and future climate change. Traditional methods to study the internal ice layer are through onsite ice core[9]. However, the limited and discrete coverage makes it impossible to study the continuous change of the ice layer. Additionally, onside ice cores are expensive to obtain and will cause destructive damage to the ice sheet. In recent years, airborne snow radar sensors have proven to be a more effective way to study ice layers. Internal layers with different depths are captured continuously as radargrams by measuring the reflected signal strength[1], shown in Figure1(a). With the development of deep learning techniques, various convolution-based neural networks[10, 13, 14, 15] have been proposed to extract ice layer boundaries from radargrams. However, noise in the radargrams is shown to be a major obstacle to achieve high-quality results. Instead of the convolution-based neural networks, Zalatan et al.[17, 18, 19] focus on learning the relationship between ice layers formed in different years and graph neural network. They utilized a fused spatio-temporal graph neural network, AGCN-LSTM, and aimed to make predictions on the thickness of deeper ice layers based on the thickness information of shallow layers. In their network, graph convolutional network(GCN) for spatio features is fused into the long short-term memory(LSTM) structure for temporal changes. Although their proposed methods have decent performance in predicting ice thickness, training the fused spatio-temporal graph neural network usually takes a few hours due to high computational costs. In this paper, we aim to build upon the work of Zalatan et al.[17, 18, 19] by improving the network performance in both accuracy and efficiency. Our major contributions are: 1) We developed a novel multi-branch spatio-temporal graph neural network that uses the GraphSAGE framework for spatio feature learning and uses temporal convolution operation to learn temporal changes, making different parts of the network more specialized to one single learning task. 2) We conducted extensive experiments on comparison with fused spatio-temporal graph neural networks, and results show that our proposed network can consistently outperform other methods in efficiency and accuracy."
https://arxiv.org/html/2411.04004v1,Synomaly Noise and Multi-Stage Diffusion: A Novel Approach for Unsupervised Anomaly Detection in Ultrasound Imaging,"Ultrasound (US) imaging is widely used in routine clinical practice due to its advantages of being radiation-free, cost-effective, and portable. However, the low reproducibility and quality of US images, combined with the scarcity of expert-level annotation, make the training of fully supervised segmentation models challenging. To address these issues, we propose a novel unsupervised anomaly detection framework based on a diffusion model that incorporates a synthetic anomaly (Synomaly) noise function and a multi-stage diffusion process. Synomaly noise introduces synthetic anomalies into healthy images during training, allowing the model to effectively learn anomaly removal. The multi-stage diffusion process is introduced to progressively denoise images, preserving fine details while improving the quality of anomaly-free reconstructions. The generated high-fidelity counterfactual healthy images can further enhance the interpretability of the segmentation models, as well as provide a reliable baseline for evaluating the extent of anomalies and supporting clinical decision-making. Notably, the unsupervised anomaly detection model is trained purely on healthy images, eliminating the need for anomalous training samples and pixel-level annotations. We validate the proposed approach on carotid US, brain MRI, and liver CT datasets. The experimental results demonstrate that the proposed framework outperforms existing state-of-the-art unsupervised anomaly detection methods, achieving performance comparable to fully supervised segmentation models in the US dataset. Additionally, ablation studies underline the importance of hyperparameter selection for Synomaly noise and the effectiveness of the multi-stage diffusion process in enhancing model performance. Code: https://github.com/yuan-12138/Synomaly.","Ultrasound (US) plays a crucial role in daily medical practice. It is commonly used for the screening and early assessment of internal organs and vasculature. Compared to computed tomography (CT) and magnetic resonance imaging (MRI), US is more frequently utilized for routine examinations. CT, although efficient in providing high-quality images, involves radiation exposure, which makes it less suitable for regular check-ups. MRI, while providing detailed soft tissue contrast, is often cost- and time-intensive for routine use. Consequently, physicians typically reserve CT and MRI for cases where a patient has already exhibited specific symptoms or conditions. As a result, CT and MRI are more commonly associated with the imaging of diseased cases. In contrast, US is radiation-free, cost-effective, and portable, making it an ideal modality for regular monitoring. Therefore, the US images collected are often from non-diseased individuals, making it as an ideal modality to learn the physiology of healthy tissues. However, the inherent challenges of low reproducibility and poor image quality in US imaging make it difficult to gather large amounts of diseased data for training of deep neural networks. This challenge is particularly evident in conditions like stroke, a leading cause of mortality worldwide, with ischaemic stroke accounting for about 65.3% of all cases [16]. This type of stroke is caused by the blockage of a blood vessel supplying the brain, and carotid atherosclerosis is a common underlying cause [22]. US is frequently used for the detection and monitoring of carotid plaques. One important indicator to assess the risk of stroke for atherosclerosis patients is total plaque volume [1]. One pre-requisite for such measurement is an accurate segmentation of the carotid plaque in US images. However, unlike healthy anatomy, plaques vary significantly in shape and location, making their detection more challenging. This variability means that high-quality datasets are crucial for training segmentation models to reliably identify plaques. Without such datasets, models may struggle to generalize and correctly segment plaques in diverse cases. Because of ethical and privacy issues, it is not an easy practice to gather large amount of diseased US data. To tackle such challenge, unsupervised anomaly detection strategy appears to be an effective solution. Similar to how humans learn about diseased tissues, unsupervised anomaly detection method tends to distinguish the anomaly tissue from the normal anatomy by learning from healthy structures [17]. Generative models are commonly utilized to generate synthetic counterfactual healthy samples based on diseased images. Since the model is purely trained on healthy data, the need for pixel-level labels is eliminated. Considering the high noise ratio and low contrast of US compared to CT and MRI, the labelling process for US sometimes introduces unavoidable ambiguities, thus complicating the fully-supervised training process [15]. However, such drawback does not concern the unsupervised anomaly detection method. Furthermore, it also provides the possibility to detect unseen or rare anomaly cases, which are not well represented in the dataset for fully-supervised models. Nonetheless, unsupervised anomaly detection is less common in US due to the presence of speckle noise, shadows, and acoustic artifacts, which make it challenging to differentiate anomalies from healthy tissues. Such complexity of US images necessitates the development of more advanced generative models. Diffusion models have proven effective in modeling complex data distributions, resulting in higher image sample quality compared to generative adversarial networks (GANs) and variational autoencoders (VAEs) [12, 10]. These advantages are particularly valuable for unsupervised anomaly segmentation, especially in cases of small datasets, leading to growing interest in applying diffusion models to this application domain [27]. Most recent works have also reported initial attempts of applying diffusion models for unsupervised anomaly detection in US [34, 32], specifically exploring their effectiveness for detecting anomalies in fetal brain US. The performance of diffusion model is closely linked to the choice of corrupting noise, especially for medical image anomaly detection [26]. Superior performance over naive pixel-wise Gaussian noise has been reported when applying Simplex noise [42] and Coarse noise [25] for anomaly detection of brain MRI. In order to develop a robust unsupervised anomaly detection model for US images, in this work, we propose Synomaly noise, which stands for synthetic anomaly noise based on statistical disease morphology prior. Different from the aforementioned noise functions, Synomaly noise is specifically designed for unsupervised medical anomaly detection by corrupting healthy images with synthetic anomalies during training. This design of noise can result in a better erasure of anomalies in the images. To further suppress the loss of detailed information during noising process, a multi-stage diffusion process is proposed for inference. The detailed contributions are summarized as follows: 1. We introduce the Synomaly noise function to enhance the effectiveness of anomaly detection. This noise is applied to healthy training images to simulate the appearance of real anomaly tissues after the noising process. By exposing the model to these synthetic anomalies during training, the effectiveness of generating healthy images from diseased images is significantly improved, tailored to individual diseases. 2. We propose a multi-stage diffusion process in the inference phase designed to preserve healthy tissue details while precisely isolating and removing anomalous disease tissue. Instead of executing the diffusion process with a large number of noising and denoising steps in one single stage, the generation is divided into successive diffusion stages, each with fewer noising and denoising steps. Through such gradual and controlled transformation, the model can preserve finer details in the reconstructed images, thus increasing the accuracy in anomaly detection tasks. 3. This is the first work to apply a diffusion model for unsupervised anomaly detection in carotid US images. We demonstrate the effectiveness of the proposed Synomaly noise and multi-stage diffusion process on a US carotid plaque dataset. To further validate the performance of the proposed approach, we conducted experiments on public brain MRI and liver CT datasets, achieving superior performance over state-of-the-art methods in detecting brain tumors and liver diseases, respectively. The results of the three most common medical image modalities (US, CT, and MRI) demonstrate the potential of the proposed method in wide application. To facilitate the reproducibility of this work, the code will be released on this website upon acceptance."
https://arxiv.org/html/2411.03990v1,ET-SEED:EfficientTrajectory-LevelSE(3)EquivariantDiffusion Policy,"Imitation learning, e.g., diffusion policy, has been proven effective in various robotic manipulation tasks. However, extensive demonstrations are required for policy robustness and generalization. To reduce the demonstration reliance, we leverage spatial symmetry and propose ET-SEED, an efficient trajectory-level S⁢E⁢(3)𝑆𝐸3SE(3)\ italic_S italic_E ( 3 )equivariant diffusion model for generating action sequences in complex robot manipulation tasks. Further, previous equivariant diffusion models require the per-step equivariance in the Markov process, making it difficult to learn policy under such strong constraints. We theoretically extend equivariant Markov kernels and simplify the condition of equivariant diffusion process, thereby significantly improving training efficiency for trajectory-level S⁢E⁢(3)𝑆𝐸3SE(3)\ italic_S italic_E ( 3 )equivariant diffusion policy in an end-to-end manner. We evaluate ET-SEED on representative robotic manipulation tasks, involving rigid body, articulated and deformable object. Experiments demonstrate superior data efficiency and manipulation proficiency of our proposed method, as well as its ability to generalize to unseen configurations with only a few demonstrations. Website: https://et-seed.github.io/","Imitation learning has achieved promising results for acquiring robot manipulation skills [1, 2, 3]. Though, one of the main challenges of imitation learning is that it requires extensive demonstrations to learn a robust manipulation policy [4, 5, 6]. Especially once the spatial pose of the object to be manipulated runs out of the demonstration distribution, the policy performance will easily decrease. Although some works seek to tackle these issues through data augmentation [7] or contrastive learning [8], they usually require task-specific knowledge or extra training, and without theoretical guarantee of spatial generalization ability. Another promising idea is to leverage symmetry. Symmetry is ubiquitous in the physical world, and many manipulation tasks exhibit a specific type of symmetry known as SE(3) Equivariance. S⁢E⁢(3)𝑆𝐸3SE(3)\ italic_S italic_E ( 3 )is a group consisting of 3D rigid transformations. For example, as shown in fig. 1(b), a real robot arm is required to write characters “ICLR” on a paper or fold a garment, when the pose of the paper or the garment changes, the manipulation trajectories of the end-effector should transform equivalently. Employing such symmetries into policy learning can not only improve the data efficiency but also increase the spatial generalization ability. Recent works on 3D manipulation have explored using SE(3) equivariance in the imitation learning process. Most of these works focus on equivariant pose estimation of the target object or end-effector [9, 10, 11]. Trajectory-level imitation learning has achieved state-of-the-art performances on diverse manipulation tasks [3, 12]. By generating a whole manipulation trajectory, this kind of method is capable to tackle more complex manipulation task beyond pick-and-place. For trajectory-level equivariance, Equivariant Diffusion Policy [13] and Equibot [14] propose equivariant diffusion process for robotic manipulation tasks. However, previous trajectory-level diffusion models for robotic manipulation have two key limitations. First, to maintain equivariance throughout the diffusion process, these models assume that every transition step must preserve equivariance. As we will show in section 3.1, training neural networks with equivariance is more challenging than neural networks with invariance, requiring additional computational resources and leading to slower convergence. This design constrains the model’s efficiency, making it hard for tackling complex long-horizon manipulation tasks. Second, these models define the diffusion process in Euclidean space, which is not a natural definition, and limits the expressiveness. Since the focus is on equivariant diffusion processes within the S⁢E⁢(3)𝑆𝐸3SE(3)\ italic_S italic_E ( 3 )group, it is more natural to define both the diffused variables and the noise as elements of the S⁢E⁢(3)𝑆𝐸3SE(3)\ italic_S italic_E ( 3 )group, which will lead to better convergence and multimodal distributions representation [15]. In this work, we propose ET-SEED, a new trajectory-level S⁢E⁢(3)𝑆𝐸3SE(3)\ italic_S italic_E ( 3 )equivariant diffusion model for manipulation tasks. ET-SEED improves the sample efficiency and decreases the training difficulty by restricting the equivariant operations during the diffusion denoising process. We extend the equivaraint Markov kernels theory and prove that during the full denoising process, at least only one equivariant transition is required. Then, we integrate the diffusion process on S⁢E⁢(3)𝑆𝐸3SE(3)\ italic_S italic_E ( 3 )manifold [16] and S⁢E⁢(3)𝑆𝐸3SE(3)\ italic_S italic_E ( 3 )transformers [17] to design a new trajectory-level equivariant diffusion model on S⁢E⁢(3)𝑆𝐸3SE(3)\ italic_S italic_E ( 3 )space. In experiment, we evaluate our method on several common and representative manipulation tasks, as shown in fig. 1(c), including rigid body manipulation (rotate triangle, open bottle cap), articulated object manipulation (open door), long-horizon tasks (robot calligraphy), and deformable object manipulation (fold and fling garment). Experiments show our method outperforms SOTA methods in terms of data efficiency, manipulation proficiency and spatial generalization ability (fig. 1(a)). Further, in real-world experiments, with only 20 demonstration trajectories, our method is able to generalize to unseen scenarios. In summary, our contributions are mainly as followed: • We propose ET-SEED, an efficient trajectory-level S⁢E⁢(3)𝑆𝐸3SE(3)\ italic_S italic_E ( 3 )equivariant diffusion policy defined on S⁢E⁢(3)𝑆𝐸3SE(3)\ italic_S italic_E ( 3 )manifold, which achieves a proficient and generalizable manipulation policy with only a few demonstrations. • We extend the theory of equivariant diffusion processes and derive a novel S⁢E⁢(3)𝑆𝐸3SE(3)\ italic_S italic_E ( 3 )equivariant diffusion process, for simplified modeling and inference. • We extensively evaluate our method on standard robot manipulation tasks in both simulation and real-world settings, demonstrating its data efficiency, manipulation proficiency, and spatial generalization ability, significantly outperforming baseline methods."
https://arxiv.org/html/2411.03855v1,MambaPEFT: Exploring Parameter-Efficient Fine-Tuning for Mamba,"An ecosystem of Transformer-based models has been established by building large models with extensive data. Parameter-efficient fine-tuning (PEFT) is a crucial technology for deploying these models to downstream tasks with minimal cost while achieving effective performance. Recently, Mamba, a State Space Model (SSM)-based model, has attracted attention as a potential alternative to Transformers. While many large-scale Mamba-based models have been proposed, efficiently adapting pre-trained Mamba-based models to downstream tasks remains unexplored. In this paper, we conduct an exploratory analysis of PEFT methods for Mamba. We investigate the effectiveness of existing PEFT methods for Transformers when applied to Mamba. We also modify these methods to better align with the Mamba architecture. Additionally, we propose new Mamba-specific PEFT methods that leverage the distinctive structure of Mamba. Our experiments indicate that PEFT performs more effectively for Mamba than Transformers. Lastly, we demonstrate how to effectively combine multiple PEFT methods and provide a framework that outperforms previous works. To ensure reproducibility, we will release the code after publication.","Modern large-scale models, also known as Foundation Models, are heavily based on Transformers (Vaswani, 2017). Transformer-based pre-trained models span diverse domains such as language, vision, and multi-modal applications. Despite their widespread use, Transformers have a notable drawback: their computational inefficiency with long sequences. The computational complexity of the attention module scales with the square of the sequence length. To address this fundamental drawback, Gu & Dao (2023) proposed Mamba, a linear-time sequence model that leverages the strengths of State Space Models (SSMs). While Transformers are constructed from attention modules, Mamba is based on the SSM architecture, allowing it to handle long sequences more efficiently. Additionally, Mamba has been shown to perform better than Transformers with the same number of parameters on major tasks such as natural language processing (NLP) (Gu & Dao, 2023) and computer vision (CV) (Zhu et al., 2024). This fact makes Mamba stand out from other post-Transformer architectures with sub-square time complexity (Peng et al., 2023; Sun et al., 2023). Mamba-based models have been proposed across a wide range of domains (Behrouz & Hashemi, 2024; Liang et al., 2024; Zhang et al., 2024b; Li et al., 2024). We believe that Mamba has the potential to go beyond the Transformer ecosystem. Parameter-efficient fine-tuning (PEFT) is essential for adapting such large-scale models to downstream tasks. Fine-tuning all parameters of these models results in high computational costs. PEFT enables additional training for large-scale Transformers with limited computing resources and data. Early examples of PEFT include the fine-tuning of pre-trained language models for NLP tasks (Houlsby et al., 2019; Hu et al., 2021). Subsequently, it has been extensively adopted across a wide range of applications (Yeh et al., 2024; Wang et al., 2023; 2024a). While many PEFT methods have been extensively studied for Transformers (Lialin et al., 2023; Han et al., 2024), research on PEFT methods for Mamba remains limited. In this paper, we provide an exploratory and comprehensive investigation of PEFT for Mamba. First, we adapt representative PEFT methods used in Transformers to the Mamba architecture and conduct extensive experiments. We also propose new PEFT methods specific to the Mamba architecture. Figure 1: An overview of our proposed MambaPEFT. We investigate, improve, and propose 20 variations of seven PEFT methods for Mamba and search for the best combination. In the experiments, we benchmarked Mamba using PEFT methods, including seven main methods and a total of 20 derived variations (see Figure 1). Our benchmarks indicate that PEFT is more crucial for Mamba than for Transformers, with several methods outperforming the PEFT methods used for Transformers. Additionally, we demonstrate that these PEFT methods can be combined to surpass the performance of individual methods. We propose an efficient search technique to identify optimal PEFT combinations and hyperparameters. Unlike existing works that focus on specific high-performance methods, we explore a wide variety of PEFT methods. This exploration reveals suitable PEFT combinations and shows that merely combining high-performing methods is not sufficient. The main contributions are as follows. First, to the best of our knowledge, we perform the first extensive and comprehensive benchmarking of PEFT methods for Mamba, including proposed Mamba-specific methods that are distinct from all PEFT methods for Transformers. Second, we propose a framework for achieving higher performance by combining multiple PEFT methods, which are obtained through our efficient search technique. Third, our results indicate that PEFT is more effective for Mamba than for Transformers, and several Mamba-specific phenomena are discovered through the experiments."
https://arxiv.org/html/2411.03752v1,Deferred Poisoning: Making the Model More Vulnerable via Hessian Singularization,"Recent studies have shown that deep learning models are very vulnerable to poisoning attacks. Many defense methods have been proposed to address this issue. However, traditional poisoning attacks are not as threatening as commonly believed. This is because they often cause differences in how the model performs on the training set compared to the validation set. Such inconsistency can alert defenders that their data has been poisoned, allowing them to take the necessary defensive actions. In this paper, we introduce a more threatening type of poisoning attack called the Deferred Poisoning Attack. This new attack allows the model to function normally during the training and validation phases but makes it very sensitive to evasion attacks or even natural noise. We achieve this by ensuring the poisoned model’s loss function has a similar value as a normally trained model at each input sample but with a large local curvature. A similar model loss ensures that there is no obvious inconsistency between the training and validation accuracy, demonstrating high stealthiness. On the other hand, the large curvature implies that a small perturbation may cause a significant increase in model loss, leading to substantial performance degradation, which reflects a worse robustness. We fulfill this purpose by making the model have singular Hessian information at the optimal point via our proposed Singularization Regularization term. We have conducted both theoretical and empirical analyses of the proposed method and validated its effectiveness through experiments on image classification tasks. Furthermore, we have confirmed the hazards of this form of poisoning attack under more general scenarios using natural noise, offering a new perspective for research in the field of security.","Deep learning models have achieved remarkable achievements in fields involving Computer Vision [1] to Natural Language Processing [2] even Artificial Intelligence Generated Content [3]. Their success is largely attributed to the proliferation of large-scale datasets, such as ImageNet [4], COCO [5], and CelebA [6]. Nevertheless, recent studies have highlighted the potential hazards posed by poisoning attacks which can undermine the integrity of deep learning models by introducing malicious data into training datasets [7, 8, 9]. Figure 1: The scenario considered by DPA. Most prevalent poisoning attacks [10, 11, 12, 13] share a common limitation: their malicious intentions are highly apparent. The victim may notice a significant disparity in the performance of the model between the training and validation datasets. In response, strategies such as adversarial training [14], appropriate data preprocessing methods [15], or the elimination of anomalous [16] data can be employed as defensive measures against these attacks. In this paper, we reveal a more threatening method of poisoning attack namely Deferred Poisoning Attack (DPA). As the term ""Deferred"" implies, this attack does not disrupt the training process, allowing the model to maintain normal performance over the validation set. Instead, the ""toxicity"" of the attack manifests by undermining the model’s robustness at the deployment stage. Fig. 1 illustrates the scenario of our attack. When a recognition system (e.g. the one deployed on an autonomous driving platform) is trained using clean datasets, it functions effectively on normal targets and demonstrates relative robustness against natural noise or adversarial perturbation generated by evasion attacks such as FGSM [17], PGD [18], etc. In contrast, when the training dataset is tainted by our proposed method, the recognition system built upon the corrupted datasets operates as usual on clean inputs but exhibits high sensitivity to adversarial perturbations or even natural noise. The threat posed by DPA is evidently more severe than the typical poisoning attacks. The developers of the recognition system are unable to detect anomalies through the detection of irregularities in the training process. Once deployed in vehicles, the system would be easily misled by natural factors such as fog, rain, and varying lighting conditions. Additionally, adversaries may deceive the model at a low cost, using subtle adversarial perturbations. We fulfill the purpose of DPA by forcing the model trained over the contaminated dataset to converge to a similar point as the one trained over the clean dataset, making the poisoned model perform normally on the validation dataset. On the other hand, we enlarge the local curvature of the poisoned model around each sample in the training dataset to amplify the sensitivity of the poisoned model. Fig. 2 illustrates the above motivation. A large local curvature (the red curve) results in a significant increase in model loss with a small perturbation of a given sample. In contrast, a small local curvature (the blue curve) enables the model loss to remain stable even with a large perturbation. Formally, a large local curvature implies that the Hessian matrix is ill-conditioned with a large conditional number [19]. Along this line, our DFA generates poisoned samples to induce the model trained on this contaminated dataset to become singularization (a large conditional number.) with respect to the input samples. In summary, our principal contributions are as follows: • The proposed DPA, to the best of our knowledge, has not been previously addressed in the literature, thus revealing a new threat within the field of artificial intelligence. • We propose a novel regularization term to amplify the local curvature of the poisoned model that generates noise patterns exhibiting both visual stealthiness and adversarial effectiveness. • Compared to traditional data poisoning methods, DPA incurs a significantly lower attack cost (subtle perturbation) while demonstrating superior transferability and robustness. • We validate the generality of the deferred poisoning attack across a broader range of scenarios. Figure 2: The illustration of the motivation of DPA."
https://arxiv.org/html/2411.03730v1,NeurIPS 2023 Competition: Privacy Preserving Federated Learning Document VQA,"The Privacy Preserving Federated Learning Document VQA (PFL-DocVQA) competition challenged the community to develop provably private and communication-efficient solutions in a federated setting for a real-life use case: invoice processing. The competition introduced a dataset of real invoice documents, along with associated questions and answers requiring information extraction and reasoning over the document images. Thereby, it brings together researchers and expertise from the document analysis, privacy, and federated learning communities. Participants fine-tuned a pre-trained, state-of-the-art Document Visual Question Answering model provided by the organizers for this new domain, mimicking a typical federated invoice processing setup. The base model is a multi-modal generative language model, and sensitive information could be exposed through either the visual or textual input modality. Participants proposed elegant solutions to reduce communication costs while maintaining a minimum utility threshold in track 1 and to protect all information from each document provider using differential privacy in track 2. The competition served as a new testbed for developing and testing private federated learning methods, simultaneously raising awareness about privacy within the document image analysis and recognition community. Ultimately, the competition analysis provides best practices and recommendations for successfully running privacy-focused federated learning challenges in the future.","Automatic document image processing has become a highly active research field in recent years (Appalaraju et al., 2024; Lee et al., 2023; Tito et al., 2023), with invoices being one of the most frequently processed document types (Šimsa et al., 2023). In a typical real-life invoicing scenario, business suppliers produce invoices for their services and send them to their customers. These documents contain sensitive information, such as consumer/purchaser identity, transaction details, purpose, date, phone numbers, amount paid, account information for payment, etc. The customers (document users) need to extract this information and take the corresponding actions (i.e. reject, or make a payment against the invoice). In automated pipelines, these documents would be sent to AI technology providers, typically offered in the form of cloud services111Automatic document processing services offered by large corporations (AWS Intelligent Document Processing, Google Cloud Document AI, Microsoft Azure Form Recognizer, etc) or specialized providers., which automatically extract all required information from the documents, and return it to the document users. A generic approach to extract information from invoices is DocVQA (Mathew et al., 2020). The extraction is done by asking questions in a natural language form to get specific information as answers, using a deep learning model. However, training an accurate DocVQA model requires a considerable amount of data, that is rarely held by a single entity. One solution is to train this model collaboratively by aggregating and centralizing data from a set of clients that face the same problem. But, documents often cannot be freely exchanged due to the sensitive information they contain. Federated Learning (FL) is a learning paradigm that purports to solve this problem (McMahan et al., 2017b). Rather than exchanging privately-held data, participating entities (known as clients) train models on their data in a decentralized fashion, exchanging only the local model updates with a central server. However, even though FL is more private than the centralized approach, a significant amount of information can still be inferred from the updates shared during training, or from the parameters of the resulting trained model, whether by an adversarial server, client, or downstream user (Sikandar et al., 2023). Differential Privacy (DP) (Dwork et al., 2016) is considered the gold standard in terms of privacy preservation and can be used to provide provable privacy guarantees. DP formally quantifies the maximum information leakage from the inclusion of any one individual record in a dataset. Deep learning models can be trained under DP by clipping parameter updates and adding noise to them (Rajkumar and Agarwal, 2012; Song et al., 2013; Abadi et al., 2016). However, this introduces a trade-off between privacy and utility. Stronger privacy guarantees require introducing more noise, which proportionately degrades model accuracy. Another drawback of FL is the high communication cost (Kairouz et al., 2021). At each federated round, the global model is transmitted by the server to selected clients (downstream step) to be trained on their local data, and then the update of this model is sent by these selected entities back to the server (upstream step). For models with millions or even billions of parameters, this requires significant bandwidth, multiplied by the number of federated rounds required to reach model convergence. In this paper, we present an analysis of the NeurIPS 2023 competition on privacy preserving FL DocVQA that we designed to expose the above challenges and invite the community to design novel creative solutions for this real-life use case. It brought together researchers and expertise from the document analysis, privacy, and FL communities. Additionally, it added a realistic use case for privacy and FL researchers as well as expanding the scope of document analysis to DP solutions."
https://arxiv.org/html/2411.03618v1,Cross Feature Fusion of Fundus Image and Generated Lesion Map for Referable Diabetic Retinopathy Classification,"Diabetic Retinopathy (DR) is a primary cause of blindness, necessitating early detection and diagnosis. This paper focuses on referable DR classification to enhance the applicability of the proposed method in clinical practice. We develop an advanced cross-learning DR classification method leveraging transfer learning and cross-attention mechanisms. The proposed method employs the Swin U-Net architecture to segment lesion maps from DR fundus images. The Swin U-Net segmentation model, enriched with DR lesion insights, is transferred to generate a lesion map. Both the fundus image and its segmented lesion map are used as complementary inputs for the classification model. A cross-attention mechanism is deployed to improve the model’s ability to capture fine-grained details from the input pairs. Our experiments, utilizing two public datasets, FGADR and EyePACS, demonstrate a superior accuracy of 94.6%, surpassing current state-of-the-art methods by 4.4%. To this end, we aim for the proposed method to be seamlessly integrated into clinical workflows, enhancing accuracy and efficiency in identifying referable DR.","(a) (b) Figure 1: Example of DR signs: (a) showing microaneurysms, hemorrhages, and exudates, clearly illustrating how each lesion appears in DR images and (b) showing the examples of lesion map. Diabetic Retinopathy(DR) is a serious and prevalent complication of diabetes mellitus, posing a significant public health concern due to its potential to cause irreversible blindness if left untreated[2]. DR develops as chronic hyperglycemia damages the small blood vessels within the retina. When these vessels are compromised, they can leak fluid or blood, leading to visual impairment. Without rapid treatment, DR can advance from mild, non-proliferative stages to severe proliferative stages, characterized by the growth of abnormal new blood vessels on the retinal surface, potentially causing significant vision loss or blindness[3, 12]. As depicted in Fig. 1, Microaneurysms, the earliest clinical signs of DR, manifest as small red dots caused by weakened capillary walls. Hemorrhages occur when these fragile vessels rupture, resulting in small blot or flame-shaped red spots. Exudates, which are lipid residues, appear as yellow or white spots on the retina, often forming a circinate pattern. Proliferative DR is distinguished by neovascularization, where new and abnormal blood vessels form on the retinal surface, posing a high risk of severe vision impairment if they bleed into the vitreous humor. Despite significant advancements in applying Convolutional Neural Network(CNN) for DR detection, achieving high accuracy in referable DR classification remains a challenging task. The subtle and heterogeneous characteristics of retinal lesions and the diversity of data collection environments can degrade the performance of deep learning networks, so integrating lesion-specific insights into the classification process can address these challenges more effectively[5]. In medical applications, where diagnostic accuracy is paramount, inaccuracies can lead to delayed treatments or unnecessary interventions, impacting patient outcomes. While CNN has shown promise, its limitations necessitate the development of more robust methods. Therefore, this study proposes a novel approach that combines transfer learning and cross-attention mechanisms to improve the precision and reliability of DR classification. By incorporating lesion-specific insights into the classification process, our method overcomes the limitations of existing models and provides a more effective tool for early DR detection. In this paper, we propose a cross-learning framework to enhance the classification of referable DR. Firstly, we introduce a DR segmentation method that uses the Swin U-Net architecture for segmenting lesion maps from retinal fundus images. Secondly, we generate lesion maps using the model trained in the first stage. Then, using the generated lesion maps, i.e. pseudo-lesion maps and the original images, we perform cross-learning to train the classification model. The integration of original images with pseudo-lesion maps via cross-fusion represents a novel approach in the medical imaging field. Segmentation annotations for medical images are costly to obtain and typically limited to small datasets. Our method effectively handles cases where datasets are imbalanced, with abundant image-level labels but limited pixel-level segmentation labels, enhancing generalizability across datasets. Our experimental results show an outstanding classification accuracy of 94.6%, surpassing existing state-of-the-art methods. The cross-attention mechanism allows the model to focus on relevant regions of the image, enhancing its ability to distinguish different DR stages. The remainder of this paper is structured as follows: In Section 2, we review related work on DR classification using deep learning and discuss recent technological advancements in this domain. Section 3 provides a detailed explanation of the model architecture and the proposed cross-learning framework. Section 4 presents data preprocessing steps, our experimental setup, and results, and offers an in-depth analysis of these findings, including both quantitative and qualitative assessments. Finally, Section 5 concludes the paper with a summary of our experiments and suggestions for future research directions."
https://arxiv.org/html/2411.03615v1,"ADMIRE: a locally adaptive single-image, non-uniformity correction and denoising algorithm: application to uncooled IR camera","We propose a new way to correct for the non-uniformity (NU) and the noise in uncooled infrared-type images. This method works on static images, needs no registration, no camera motion and no model for the non uniformity. The proposed method uses an hybrid scheme including an automatic locally-adaptive contrast adjustment and a state-of-the-art image denoising method. It permits to correct for a fully non-linear NU and the noise efficiently using only one image. We compared it with total variation on real raw and simulated NU infrared images. The strength of this approach lies in its simplicity, low computational cost. It needs no test-pattern or calibration and produces no “ghost-artefact”.","Infrared (IR) imaging has proved to be a very efficient tool in a wide range of industry, medical, and military applications. Infrared cameras are used to measure temperatures, signatures, to perform detection, etc. However, the performance of the imaging system is strongly affected by the random spatial response of each pixel sensor. Under the same illumination the readout of each sensor is different. It leads to a structured noise resulting in a row or line pattern in the images (depending on the readout system). This “noise” is called fixed pattern noise and produces “non uniformity” in the observed images. Theses differences between sensor readout are due to mismatches in the fabrication process, among other issues [1] and are stronger at longer wavelength such as in infrared imaging [2]. The readout of a pixel sensor is a non linear [1] function of the incoming luminance. Figure 1: On the left : an image (RAW) taken with an infrared camera. The non uniformity is so strong that it is hard to distinguish between the noise and the underlying landscape. On such an image performing an identification, matching pattern, etc. is almost hopeless. On the right the same image corrected with the proposed method. The non uniformity is a serious practical limitation to both civilian and military applications - as it severely degrades image quality [3] (see Fig. 1). For uncooled infrared cameras the problem is even worse because the detector response evolves quickly with time. Therefore the correction cannot be done once and for all by the manufacturer. It also means that we need to estimate, for each pixel, a function with little or no model at all and using little or one image to aim a good correction. Indeed the use of numerous images to achieve the correction leads to artefacts –those are called “ghosts artefacts” and are challenging to remove[4, 5, 6, 7, 8, 9]– because of the sensor drift[2, 10, 11]. In other words, for each pixel, the correction at times t1subscript𝑡1t_{1}italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and t2≠t1subscript𝑡2subscript𝑡1t_{2}\neq t_{1}italic_t start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ≠ italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT are different. A correction is so much needed, that in many uncooled infrared cameras a flap closes every 30 seconds to perform a partial calibration [12, 13]. This interrupts the image flow, which is calamitous for many applications. Thus, for uncooled infrared cameras a periodic update of the non uniformity correction is required. A good non uniformity algorithmic correction is a key factor in ensuring the best image quality and the robustness of the downstream applications such as pattern recognition, image registration, etc. In this paper we introduce locally adaptive version of our precedent work [14]. A review of existing techniques is proposed in section 3. A single image, fully automatic, non uniformity correction algorithm is detailed in section 4 and generalized in section 5. It shows that motion compensation or accumulation algorithms are not necessary to achieve a good image quality. The proposed method can compensate for a fully non linear non uniformity, without any parametric model on the non uniformity side. It does not require motion, or motion compensation, does not need a test pattern or calibration and does not produce any “ghost artifact”. A state of the art denoising algorithm is modified to suit our context.The proposed method is illustrated in section 6 on simulated non linear non uniformity in section Figs. 6-7, compared with a total variation based method (this method is described in section 6.1) in section 6.2 and evaluated on real raw images from thermal infrared cameras in section 6.3."
https://arxiv.org/html/2411.03610v1,LCP-Fusion: A Neural Implicit SLAM with Enhanced Local Constraints and Computable Prior,"Recently the dense Simultaneous Localization and Mapping (SLAM) based on neural implicit representation has shown impressive progress in hole filling and high-fidelity mapping. Nevertheless, existing methods either heavily rely on known scene bounds or suffer inconsistent reconstruction due to drift in potential loop-closure regions, or both, which can be attributed to the inflexible representation and lack of local constraints. In this paper, we present LCP-Fusion, a neural implicit SLAM system with enhanced local constraints and computable prior, which takes the sparse voxel octree structure containing feature grids and SDF priors as hybrid scene representation, enabling the scalability and robustness during mapping and tracking. To enhance the local constraints, we propose a novel sliding window selection strategy based on visual overlap to address the loop-closure, and a practical warping loss to constrain relative poses. Moreover, we estimate SDF priors as coarse initialization for implicit features, which brings additional explicit constraints and robustness, especially when a light but efficient adaptive early ending is adopted. Experiments demonstrate that our method achieve better localization accuracy and reconstruction consistency than existing RGB-D implicit SLAM, especially in challenging real scenes (ScanNet) as well as self-captured scenes with unknown scene bounds. The code is available at https://github.com/laliwang/LCP-Fusion.","I INTRODUCTION Dense visual Simultaneous Localization and Mapping (SLAM) plays a vital role during perception, navigation and manipulation in unknown environments. In recent decades, traditional SLAM methods [1, 2, 3] have made significant progress in localization accuracy and real-time applications. However, due to the use of explicit scene representations like occupancy-grids [4, 5, 6], point cloud [7, 8, 9], Signed Distance Function [10, 11, 12], and surfels [13], which directly store and update limited scene information at fixed resolution without context, they struggle to balance memory consumption and mapping resolution, while being unable to reconstruct complete and consistent surfaces in noisy or unobserved areas. Therefore, recent research has focused on implicit representation using neural networks [14] or radiance fields [15] to encode any points in scenes as continuous function, which can be used to extract isosurfaces at arbitrary resolution or to synthesize realistic unseen views. Utilizing the representation coherence and ability to render unseen views, numerous neural implicit SLAM systems [16, 17, 18] have emerged to perform high-fidelity mapping and camera tracking in various scenes. However, most of them require known scene bounds due to inflexible scene representation [19], resulting in performance degradation or failure in unknown scenarios. Focusing on applications in unknown scenes, one of the mainstream solutions is to allocate implicit feature grids dynamically in surface areas utilizing flexible sparse voxel octree (SVO) [19]. Since SVO-based methods [20] only represent scenes using high-dimensional features in sparse voxel grids, they tend to be sensitive to odometry drift caused by their insufficient local constraints in loop closure regions. This may contribute to inconsistent reconstruction much further, which is shown in Fig. 1. Additionally, with an explicit SDF octree prior, hybrid methods [21] are proposed for precise mapping, but instead use a traditional visual odometry [22] as tracking module. Thus, for the unified dense SLAM system utilizing a neural implicit representation for both tracking and mapping, it is worth investigating in alleviating reconstruction inconsistency caused by localization drift in unknown scenes with potential loop closure. Figure 1: Example from baseline [19] of inconsistent surfaces due to drift in potential loop-closure regions composed of frames 119, 3449 and 5549 (top). Our method can reconstruct unknown scenes with less drift utilizing enhanced local constraints and easily computable SDF priors (bottom). To this end, we introduce LCP-Fusion (a neural implicit SLAM system with enhanced Local Constraints and Computable Prior), which can alleviate drift in potential loop-closure without other external modules. Our key ideas are as follows. First, to handle unknown scene boundaries, we utilize the SVO to dynamically allocate hybrid voxel grids containing coarse SDF priors and residual implicit features, which yields scene geometry and color through sparse volume rendering. Second, through pixel reprojection between frames, we propose a novel sliding window selection strategy based on visual overlap, which not only strengthens local constraints but also alleviates catastrophic forgetting. In addition to only evaluating individual frame, a practical warping loss constraining relative poses is introduced to further improve localization accuracy. Third, to reduce the redundancy of iterations in joint optimization, we adopt an adaptive early ending without significant performance degradation owing to our proposed hybrid representation. We perform extensive evaluations on a series of RGB-D sequences to demonstrate the localization improvement of our method, as well as the applications in real scenes with unknown bounds. In summary, our contributions are: • We present LCP-Fusion, a neural implicit SLAM system based on hybrid scene representation, which allocates hybrid voxels with implicit features and estimated SDF priors dynamically in scenes without known bounds. • We introduce a novel sliding window selection strategy based on visual overlap and a warping loss constraining relative poses for the enhanced local constraints. • Extensive evaluations on various datasets demonstrate our competitive performance in localization accuracy and reconstruction consistency, as well as robustness to fewer iterations and independence on scene boundaries."
https://arxiv.org/html/2411.03569v1,Towards Personalized Federated Learning via Comprehensive Knowledge Distillation,"Federated learning is a distributed machine learning paradigm designed to protect data privacy. However, data heterogeneity across various clients results in catastrophic forgetting, where the model rapidly forgets previous knowledge while acquiring new knowledge. To address this challenge, personalized federated learning has emerged to customize a personalized model for each client. However, the inherent limitation of this mechanism is its excessive focus on personalization, potentially hindering the generalization of those models. In this paper, we present a novel personalized federated learning method that uses global and historical models as teachers and the local model as the student to facilitate comprehensive knowledge distillation. The historical model represents the local model from the last round of client training, containing historical personalized knowledge, while the global model represents the aggregated model from the last round of server aggregation, containing global generalized knowledge. By applying knowledge distillation, we effectively transfer global generalized knowledge and historical personalized knowledge to the local model, thus mitigating catastrophic forgetting and enhancing the general performance of personalized models. Extensive experimental results demonstrate the significant advantages of our method.","The rapid evolution of distributed intelligent systems has brought data privacy to the forefront. Federated Learning (FL), a distributed machine learning paradigm, enables collaborative model training through parameter sharing rather than raw data exchange, effectively reducing the risk of exposing sensitive data [1]. By leveraging FL, we can efficiently utilize data from distributed clients to collectively train high-performance models. FL has demonstrated its significant value in a variety of fields, including medical health [2], financial analytics [3], and social network [4]. In traditional FL, multiple clients collaborate to train a global model to achieve an optimal universal solution for all clients. However, the non-independent and identically distributed nature of the data distribution [5], known as data heterogeneity, often causes a decline in the performance of distributed clients and can even lead to catastrophic forgetting. Fig. 1 depicts this phenomenon. It is evident that the data distribution varies in each communication round. During each training round, distributed clients update their local model with the global model. Unfortunately, the post-update local model exhibits a significant performance decline compared to the pre-update local model, indicating the phenomenon of forgetting previously learned knowledge. Personalized Federated Learning (PFL) is an innovative approach to tackle data heterogeneity in FL [6]. It involves the collaborative training of a global model by all clients, after which each client develops a personalized model through personalized strategies, reflecting the distinctive characteristics of its local data. However, while existing PFL methods excel in model personalization, they often neglect model generalization. For instance, pFedSD [7] employs knowledge distillation to transfer knowledge from the historical model to the local model for achieving model personalization. Nevertheless, this method may hinder the local model’s generalization, as the historical model represents the previous local model and incorporates personalized knowledge. The primary limitation of existing PFL methods lies in their design principles, which overemphasize personalized learning and may lead to model overfitting on individual clients, thereby reducing their adaptability to varied client [8]. (a) Data distribution (b) Catastrophic forgetting Figure 1: Data heterogeneity in FL leads to catastrophic forgetting. In order to alleviate catastrophic forgetting and achieve a balance between generalization and personalization, we propose Personalized Federated Learning via Comprehensive Knowledge Distillation (FedCKD). Our method integrates multi-teacher knowledge distillation [9] into FL for comprehensive knowledge transfer. The global model represents the aggregated model from the last round of server aggregation, containing global generalized knowledge, while the historical model represents the local model from the last round of client training, containing historical personalized knowledge. By employing the multi-teacher knowledge distillation, we utilize global and historical models as teachers and the local model as the student. This method effectively transfers both global generalized knowledge and historical personalized knowledge to the local model. Global generalized knowledge enhances model performance, whereas historical personalized knowledge addresses the issue of catastrophic forgetting. In summary, our primary contributions are as follows: (1) We propose a novel PFL method called FedCKD. Through multi-teacher knowledge distillation, our method effectively transfers global generalized knowledge and historical personalized knowledge to the local model, thereby addressing catastrophic forgetting and enhancing model performance. (2) We introduce the annealing mechanism in knowledge distillation that dynamically adjusts the weight factor in the loss function, facilitating a smooth transition of training from knowledge transfer to local learning. This mechanism enhances the model’s personalization ability and improves the training process’s stability. (3) We validate the superior performance of our method through an extensive series of experiments, surpassing existing state-of-the-art methods."
https://arxiv.org/html/2411.03568v1,The American Sign Language Knowledge Graph:Infusing ASL Models with Linguistic Knowledge,"Language models for American Sign Language (ASL) could make language technologies substantially more accessible to those who sign. To train models on tasks such as isolated sign recognition (ISR) and ASL-to-English translation, datasets provide annotated video examples of ASL signs. To facilitate the generalizability and explainability of these models, we introduce the American Sign Language Knowledge Graph (ASLKG), compiled from twelve sources of expert linguistic knowledge. We use the ASLKG to train neuro-symbolic models for 3 ASL understanding tasks, achieving accuracies of 91% on ISR, 14% for predicting the semantic features of unseen signs, and 36% for classifying the topic of Youtube-ASL videos.","Computational models of signing aim to improve access to language technologies by automatically understanding and producing a sign language (Bragg et al., 2019). Recognizing that sign language models could benefit signing communities broadly, including tens of millions of deaf and hard-of-hearing people, there have been repeated calls for new technological resources to assist in sign language modeling Diab and Yifru (2022); Yin et al. (2021). These calls strongly emphasize the fair and equitable treatment of those who sign, including respect for signing communities’ autonomy, diversity, and right to privacy (Burke et al., 2024); improving the transparency of how models use signing data (Bragg et al., 2021); and mitigating audist biases in research (Desai et al., 2024; Börstell, 2023). Figure 1: The ASLKG relates the form (e.g., 2/V handshape) and meaning (e.g., related to sight) of signs in the ASL lexicon. We use this knowledge to neuro-symbolically recognize signs (e.g., read) and infer their meaning. In meeting these sociotechnical goals, sign language models must address the scarcity and inconsistent curation quality of signing data, including coverage of the lexicon, consistency of sign labels (if they exist at all), and documented representation among signers (Moryossef and Goldberg, 2021; Muller et al., 2022). Even using the largest and most externally valid datasets of American Sign Language (ASL) for isolated sign recognition (in: video; out: one of 2–3k sign identifiers), end-to-end, data-hungry neural models have struggled to surpass 70%percent7070\%70 % top-1 accuracy (Athitsos et al., 2008; Li et al., 2020; Desai et al., 2023; Kezar et al., 2023b). Meanwhile, end-to-end models can translate ASL weather reports into English (Forster et al., 2014) with a BLEU score of approximately 29292929 (Guan et al., 2024; Chen et al., 2023); however, this metric does not clearly indicate where, how, and why models make mistakes. To facilitate technological progress on these fronts, we introduce the American Sign Language Knowledge Graph (ASLKG), a collection of over 71717171k linguistic facts related to 5802580258025802 ASL signs (§3). The ASLKG is built from 8888 knowledge bases pertaining to ASL linguistics, and supplemented with 4444 more for English lexical semantics. To validate the quality of the ASLKG, we use it to train neuro-symbolic models on three downstream tasks (§4): (a) recognizing isolated signs, (b) predicting the semantic features of out-of-vocabulary signs, and (c) classifying the topic of continuous ASL videos. By grounding a video to phonological features in the ASLKG and reasoning about what those features might mean (e.g., signs, semantic features), we achieve 91%percent9191\%91 % accuracy at recognizing isolated signs, 14%percent1414\%14 % accuracy at predicting unseen signs’ semantic features, and 36%percent3636\%36 % accuracy at classifying the topic of Youtube-ASL videos (§5). The ASLKG is released under the CC BY-NC-SA 4.0 License at this link."
https://arxiv.org/html/2411.03551v1,Enhancing Weakly Supervised Semantic Segmentation for Fibrosis via Controllable Image Generation,"Fibrotic Lung Disease (FLD) is a severe condition marked by lung stiffening and scarring, leading to respiratory decline. High-resolution computed tomography (HRCT) is critical for diagnosing and monitoring FLD; however, fibrosis appears as irregular, diffuse patterns with unclear boundaries, leading to high inter-observer variability and time-intensive manual annotation. To tackle this challenge, we propose DiffSeg, a novel weakly supervised semantic segmentation (WSSS) method that uses image-level annotations to generate pixel-level fibrosis segmentation, reducing the need for fine-grained manual labeling. Additionally, our DiffSeg incorporates a diffusion-based generative model to synthesize HRCT images with different levels of fibrosis from healthy slices, enabling the generation of the fibrosis-injected slices and their paired fibrosis location. Experiments indicate that our method significantly improves the accuracy of pseudo masks generated by existing WSSS methods, greatly reducing the complexity of manual labeling and enhancing the consistency of the generated masks.","Fig. 1: Framework of DiffSeg. (a) Pre-training of a diffusion-based autoencoder and classifier. (b) Slice-injected fibrosis generation workflow with trained models. (c) Pseudo mask generation and refinement process. Fibrotic Lung Disease (FLD) represents a group of severe conditions characterized by stiffening and scarring of the lungs, leading to progressive loss of respiratory function. Despite its significant health impact, FLD remains under-recognized; in 2021, it accounted for 1% of all deaths in the United Kingdom [1], comparable to breast cancer mortality rates. Among FLD patients, a subset suffers from progressive pulmonary fibrosis (PPF), a rapidly advancing form with poor prognosis and a median survival time of just 2 to 5 years [2]. Early prediction of PPF is crucial to enable timely intervention. High-resolution computed tomography (HRCT) is essential for diagnosing and monitoring FLD, with fibrosis extent on HRCT correlating strongly with mortality and serving as a prognostic marker [3]. However, fibrosis appears as irregular, overlapping patterns (e.g., honeycombing, reticulation and ground-glass opacity) that lack clear boundaries, leading to significant inter-observer variability [4]. In recent years, computer-aided methods like DTA and INTACT [5, 6] have emerged to automate segmentation and improve image analysis efficiency. Nevertheless, these methods rely heavily on extensive pixel-level annotations of high-dimensional CT volumes, and their performance is constrained by the availability of manual labels. Weakly Supervised Semantic Segmentation (WSSS) methods offer a promising alternative by using less detailed annotations and have shown strong performance in natural image segmentation, with potential applications emerging in medical image domain[7, 8]. Recent image-level WSSS methods mainly rely on class activation maps, which tend to capture only the most discriminative regions and produce low-resolution maps, limiting segmentation quality [9]. In this work, we introduce WSSS to the challenging task of fibrosis segmentation through a novel generative framework named Diffusion-Based Segmentation Model (DiffSeg). DiffSeg achieves pixel-level fibrosis annotation using only image-level labels (presence of fibrosis) provided by clinicians. It leverages a controllable latent space within a generative model to generate a fibrosis-injected HRCT slice from a fibrosis-free one, guided by a classifier pre-trained on image-level labels. This control ensures the synthesized and original slices are identical except for fibrosis patterns, enabling precise localization. The localized region is then refined into a pseudo mask to train final segmentation model. By combining controllable generative model and weak supervision, our approach enables WSSS for fine-grained, medical segmentation tasks. This not only significantly reduces the manual labeling burden but also enhances the consistency of labels for boundary-ambiguous tasks."
https://arxiv.org/html/2411.03464v1,TopoTxR: A topology-guided deep convolutional network for breast parenchyma learning on DCE-MRIs,"Characterization of breast parenchyma in dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) is a challenging task owing to the complexity of underlying tissue structures. Existing quantitative approaches, like radiomics and deep learning models, lack explicit quantification of intricate and subtle parenchymal structures, including fibroglandular tissue. To address this, we propose a novel topological approach that explicitly extracts multi-scale topological structures to better approximate breast parenchymal structures, and then incorporates these structures into a deep-learning-based prediction model via an attention mechanism. Our topology-informed deep learning model, TopoTxR, leverages topology to provide enhanced insights into tissues critical for disease pathophysiology and treatment response. We empirically validate TopoTxR using the VICTRE phantom breast dataset, showing that the topological structures extracted by our model effectively approximate the breast parenchymal structures. We further demonstrate TopoTxR’s efficacy in predicting response to neoadjuvant chemotherapy. Our qualitative and quantitative analyses suggest differential topological behavior of breast tissue in treatment-naïve imaging, in patients who respond favorably to therapy as achieving pathological complete response (pCR) versus those who do not. In a comparative analysis with several baselines on the publicly available I-SPY 1 dataset (N=161, including 47 patients with pCR and 114 without) and the Rutgers proprietary dataset (N=120, with 69 patients achieving pCR and 51 not), TopoTxR demonstrates a notable improvement, achieving a 2.6% increase in accuracy and a 4.6% enhancement in AUC compared to the state-of-the-art method.","Breast cancer imaging faces a critical challenge in accurately modeling complex breast parenchyma structures, which change dynamically due to factors like angiogenesis, radiation therapy, and chemotherapy. Utilizing 3D breast imaging such as MRI to capture and model these changes can significantly impact diagnosis, prognosis, and treatment planning. Traditional cancer imaging studies have primarily focused on tumor texture and shape, overlooking valuable information in the tumor microenvironment. Evidence suggests that diagnostic and prognostic insights lie in the peritumoral stroma and parenchyma, where phenotypic diversity arises from factors like immune infiltration, vascularity, and tissue composition. Parameters like fibroglandular tissue pattern and parenchymal enhancement also influence breast cancer risk and treatment responses. To tailor diagnosis and treatment strategies to improve patient care, there is a critical need for innovative quantitative methods to comprehensively understand breast cancer biology by exploring the tumor microenvironment and surrounding parenchyma, which can be routinely observed in imaging scans such as breast MRI. Various approaches for breast image analysis have been proposed. Radiomic approaches learn diagnostic and prognostic signatures from breast tumor and surrounding peritumoral regions using radiomics features [61, 70]. These handcrafted features, inspired by human knowledge, attempt to capture different measurements such as tumor/peritumor texture [8], vessel geometry descriptors [6], and other similar characteristics. However, these approaches have two fundamental limitations. First, they usually lack an explicit modeling of the complex structural pattern of peritumoral stroma and parenchyma. Second, these handcrafted features lack sufficient flexibility to model the heterogeneous breast parenchyma and thus cannot provide the desired level of predictive power in practice, despite abundant interpretability. On the other hand, data-driven approaches, such as deep neural networks [77] and convolutional neural networks (CNNs) [9, 29, 68, 67, 66, 83, 86], have shown great promise in various domains, as they learn feature representations in an end-to-end manner. While direct application of CNNs to MRIs seems promising [38, 42, 43, 35, 46, 50, 15, 57], CNN methods take the whole MRI as a direct input; a large portion of the input volume may be biologically irrelevant and even noisy enough to bias the prediction model. Additionally, a 3D CNN possesses millions of parameters and requires a substantial amount of training data. Unfortunately, obtaining such amount of data is often impractical for controlled clinical trials like the I-SPY 1 trial [51]. Furthermore, CNNs suffer from the limitation of feature interpretability as they lack direct association with the underlying breast tissue structures. (a) (b) (c) Fig. 1: (a): 3D rendering of a phantom breast with highlighted glandular tissues (white) and topological structures (blue); (b): glandular tissues; (c): topological structures. Fig. 2: (a) A example MRI image, and different radiomics features such as (b) 3D shape of a tumor, (c) intratumoral texture (Haralick entropy), and (d) whole breast texture (Haralick energy). In (e), we show topological structures from TopoTxR, capturing the geometry of fibroglandular tissues. In this paper, we propose TopoTxR, a novel method that overcomes the aforementioned disadvantages. Our method extracts the breast parenchyma structures using the mathematical language of topology. It then effectively incorporates these rich topological structures into deep convolutional neural networks, thereby significantly improving predictive power. By explicitly modeling parenchyma, our approach ensures that predictions are firmly based on biological structures, and thus significantly enhances the performance of the data-intensive CNN model, even with a limited training set. Our method is based on the theory of persistent homology [20], which extracts 1D (loops) and 2D (bubbles) topological structures with guaranteed robustness [14]. These structures correspond to curvilinear tissue structures (e.g., ducts, vessels, etc.) and voids enclosed by tissues and glands in their proximity. As shown in Fig. 2, compared to previous radiomics features, the topological structures provide a much richer structural context for the modeling of tumor microenvironment. Since these structures are extracted in an unsupervised manner, the quality of their interpretation becomes a key consideration. Using a phantom breast imaging dataset (VICTRE [5]), we validate both quantitatively and qualitatively that these topological structures are reasonable approximations of the breast tissue structures. As illustrated in Fig. 1, the extracted topological structures delineate the glandular tissues of a phantom breast image. To fully exploit the information carried by these topological structures, we propose a topology-guided deep learning model for breast images. The key idea is to direct the model’s attention to voxels adjacent to these topological structures/tissue structures. As the model is focused on a much smaller set of voxels with high biological relevance, it can be efficiently trained with limited MRI data. Meanwhile, the learning outcomes have the potential to connect the biological causes of various breast pathologies with the manifestations observed in the topology of tissue structures. A method closely related to ours, developed by [17], employs a form of weaker topological information known as Betti curves to enhance prediction accuracy. However, the information carried by Betti curves is much more limited when compared to the explicit topological structures used in our approach, leading to suboptimal performance. While our approach is task-agnostic, we specifically focus on predicting the response to neoadjuvant chemotherapy (NAC) in breast cancer treatment as a practical application. Correct prediction of pathological complete response (pCR) prior to NAC administration can prevent ineffective treatments, reducing unnecessary patient suffering and healthcare costs. Empirically, we have evaluated our method on the I-SPY 1 dataset [51] and a proprietary dataset. In these evaluations, TopoTxR outperforms various baselines, including radiomics approaches, CNNs trained without topological priors, and other state-of-the-art approaches. This highlights the effectiveness of our topology-centric approach in achieving superior predictive performance. In summary, we present a novel topological method to characterize parenchyma in breast DCE-MRI, aiming to predict pCR. Our method bridges the two extremes of engineered imaging features and completely data-driven CNNs. Our key contributions are: 1. Utilization of persistent homology theory to extract topological structures that closely approximate breast fibroglandular tissue. 2. Comprehensive evaluation using a phantom breast imaging dataset to validate the accuracy of the extracted topological structures in approximating breast tissues. 3. Introduction of a topology-guided spatial attention mechanism designed to direct the focus of 3D CNNs, thereby enhancing their predictive capabilities. This work builds upon our previous conference paper [73] with four key enhancements: (1) A new topology-guided spatial attention (TGSA) module that explicitly directs the attention of the 3D CNN to biologically relevant sets of voxels with a mask loss. This new module effectively eliminates the need for prior persistent homology computing during the inference stage, making it more practical for real-world applications. (2) An exhaustive empirical evaluation on the VICTRE phantom dataset to validate the accuracy of the topological approximation of breast tissues. (3) Model attention visualization that confirms the proposed model’s attention is concentrated on a smaller biologically relevant set of voxels when predicting treatment responses using Gradient-weighted Class Activation Mapping (Grad-CAM) [62]. (4) A new proprietary dataset along with comprehensive evaluations to demonstrate the proposed model’s generalization and versatility across different clinical settings. We have made the source code publicly available via this GitHub repository to facilitate the reproducibility of our research. 1.1 Related Work Quantitative imaging features have been used in conjunction with machine learning classifiers for the prediction of pCR [10, 44]. Radiomics approaches, involving analysis of quantitative attributes of tumor texture and shape, have shown promise in the assessment of treatment response. In particular, such features capture the appearance of the tumors and, more recently, peritumor regions [7, 23]. Such approaches are often limited by their predefined nature, lack of generalizability, reliance on accurate lesion segmentation, and inability to explain phenotypic differences beyond the peritumoral margin. CNNs have been previously applied to breast DCE-MRI for pCR prediction [39, 58, 24, 56]. Owing to the sub-optimal performance of image-only models, image-based CNN approaches have been fused with non-imaging clinical variables to bolster prediction [18]. Moreover, there is extensive literature highlighting the use of CNNs for cancer diagnosis in mammography [32, 1, 67]. However, mammograms are only 2D projections of 3D tissue structures. This results in a loss of interpretability of the extracted structures. 2D mammography is limited by the loss of interpretability of extracted structures and 3D mammography using tomography is limited by the inability of the mammogram to provide information regarding background parenchymal enhancement or tumoral/peritumoral enhancement kinetics. Moreover, MRI has been shown to be superior to mammography in determining the extent of breast cancer. In contrast, mammography and ultrasound fail to accurately evaluate tumor size in 8% of cases post-neoadjuvant chemotherapy, as reported in a retrospective review by [3, 30, 41]. The purpose of post-neoadjuvant chemotherapy examination is to evaluate the extent of residual disease. However, mammography, ultrasound, and physical exams accurately detect only 13-25% of pCR cases, as indicated in a study by [25, 72, 48]. MRI and ultrasound each remains superior to mammography with respect to residual tumor detection. Notably, MRI is significantly more effective than mammography in identifying cases of multifocal or multicentric disease, as reported by [41]. Topological information, in particular, persistent homology [20], provides a robust way to quantify the topological information in an image. This information, encoded as persistence diagrams or persistence barcodes, has found diverse applications in various image analysis tasks, such as cardiac image analysis [78], brain network analysis [34, 80], and neuron image segmentation [27]. In recent years, it has been combined with deep neural networks to enforce topological constraints in image segmentation tasks [13, 27, 63, 65]. Abundant work has been done to learn topology from persistence diagrams, for instance, through vectorization [2], kernel machines [11, 33, 59], and deep neural networks [26]. Additionally, topology has been formulated as graphs, which are then effectively integrated with Graph Neural Networks (GNN) for applications such as pathology image classification [75] and retinal artery/vein classification [49]. An even weaker topological information called Betti curve is extracted for learning with breast images [17]. However, the actual geometric realization of the topological structures, e.g., cycles and bubbles, has not yet been fully explored. These topological structures capture the geometric details of breast tissues, such as fibroglandular tissues, and can be mapped back to the original breast volumes to provide biologically relevant information for further CNN analysis. For the first time, we propose a deep learning method that leverages the geometry of topological structures as an explicit attention mechanism in this paper."
https://arxiv.org/html/2411.03456v1,"BOston Neonatal Brain Injury Data for Hypoxic
Ischemic Encephalopathy (BONBID-HIE): II. 2-year Neurocognitive Outcome and NICU Outcome","Hypoxic Ischemic Encephalopathy (HIE) affects approximately 1-5/1000 newborns globally and leads to adverse neurocognitive outcomes in 30% to 50% of cases by two years of age. Despite therapeutic advances with Therapeutic Hypothermia (TH), prognosis remains challenging, highlighting the need for improved biomarkers. This paper introduces the second release of the Boston Neonatal Brain Injury Dataset for Hypoxic-Ischemic Encephalopathy (BONBID-HIE), an open-source, comprehensive MRI and clinical dataset featuring 237 patients, including NICU outcomes and 2-year neurocognitive outcomes from Massachusetts General Hospital and Boston Children’s Hospital.","Background & Summary HIE is a clinical syndrome due to a lack of blood flow and oxygen to the brain. It affects around 1-5/1000 newborns globally (Graham et al., 2008; Lee et al., 2013). Despite advancements in Therapeutic Hypothermia (TH), the prognosis for many infants remains challenging, with 35%–50% suffering adverse neurocognitive outcomes by 2 years of age (Shankaran et al., 2005; Azzopardi et al., 2009; Gluckman et al., 2005). Therefore, 63 of the 130 ongoing HIE-related trials worldwide are testing whether new therapies (Laptook et al., 2017; Shankaran et al., 2017; Liu et al., 2006; Potter et al., 2018; Nuñez-Ramiro et al., 2019; Liang et al., 2019; Cotten et al., 2014) can supplement TH and further reduce adverse outcomes. However, therapeutic innovation is slow and inconclusive, for 1) before therapy, patients at high risk of developing adverse outcomes cannot be identified; 2) after therapy, outcomes cannot be measured until age 2 years (Laptook et al., 2021). Both issues point to a lack of a neonatal biomarker that can predict adverse 2-year outcomes. To facilitate the development of biomarkers in HIE study, we present BOston Neonatal Brain Injury Dataset for Hypoxic Ischemic Encephalopathy (BONBID-HIE), an open-source, comprehensive, and representative MRI and clinical dataset for HIE. This paper introduces the second part of the BONBID-HIE data. This release contains raw and derived diffusion parameter maps, as well as NICU outcome and 2-year outcome with 237 patients. Our data was from Massachusetts General Hospital (MGH) and Boston Children’s Hospital (BCH). It includes MRIs from different scanners (Siemens 3T and GE 1.5T), different MRI protocols, and from patients of different races/ethnicities and ages (0-14 days postnatal age). Part I of our data release (Bao et al., 2023) focuses on lesion detection, while Part II (this paper) is focus on clinical, treatment, and neurologic outcome data for further developing prognostic biomarkers."
https://arxiv.org/html/2411.03445v1,Solving Trojan Detection Competitions with Linear Weight Classification,"Neural networks can conceal malicious Trojan backdoors that allow a trigger to covertly change the model behavior. Detecting signs of these backdoors, particularly without access to any triggered data, is the subject of ongoing research and open challenges. In one common formulation of the problem, we are given a set of clean and poisoned models and need to predict whether a given test model is clean or poisoned. In this paper, we introduce a detector that works remarkably well across many of the existing datasets and domains. It is obtained by training a binary classifier on a large number of models’ weights after performing a few different pre-processing steps including feature selection and standardization, reference model weights subtraction, and model alignment prior to detection. We evaluate this algorithm on a diverse set of Trojan detection benchmarks and domains and examine the cases where the approach is most and least effective.","Trojan backdoors are hidden modifications in neural network models that allow an attacker to alter the model’s behavior in response to a specific trigger, posing significant risks to AI systems. The vulnerability of neural networks to Trojan backdoors is well documented. Techniques for inserting triggers vary from simple data poisoning (Gu et al., 2019) to clean label attacks (Turner et al., 2018; Saha et al., 2019; Liu et al., 2018b) to weight manipulation (Liu et al., 2018b; Garg et al., 2020). There have been several recent surveys covering backdoor attacks (Liu et al., 2020; Li et al., 2021; Wang et al., 2022). A variety of techniques for detecting Trojan behavior have emerged in recent years. These include detecting anomalous samples during neural network training or inference (Chou et al., 2020; Gao et al., 2020; Chen et al., 2018), attempting to recover the Trojan trigger via trigger inversion (Wang et al., 2019; Guo et al., 2019; Wang et al., 2020; Sun et al., 2020; Shen et al., 2021; Huster & Ekwedike, 2021), functional analysis (Sikka et al., 2020; Xu et al., 2019; Edraki et al., 2020; Erichson et al., 2020), activation analysis (Tang et al., 2019), and weight analysis (Fields et al., 2021; Clemens, 2021). In computer vision, techniques like activation clustering (Chen et al., 2018) detect abnormal neuron activations that correspond to backdoor triggers, while Neural Cleanse (Wang et al., 2019) reverse-engineers potential triggers by identifying small input modifications that flip model predictions. Fine-pruning (Liu et al., 2018a) is used to prune rarely activated neurons associated with triggers, effectively neutralizing the backdoor. ABS scanning (Shen et al., 2021), detects neurons that respond abnormally to synthetic perturbations, uncovering hidden backdoors. Spectral signature analysis (Tran et al., 2018) is also employed to identify outliers in neuron activations caused by backdoor inputs . These approaches aim to uncover hidden visual patterns or anomalies that activate backdoors in image-based models. In natural language processing (NLP), backdoors typically appear as specific words or phrases that trigger malicious behavior. Detection techniques include input perturbation, where small modifications to text inputs help reveal triggers, and anomaly detection in embeddings, which identifies outliers in word embeddings or hidden states that correlate with backdoor behavior. Early stopping, perplexity and BERT Embedding distance were proposed in (Wallace et al., 2020) to mitigate and identify poison examples in the training dataset. Traditional defense strategies, relying on model fine-tuning and gradient calculations, are insufficient for Large Language Models due to their computational demands, so the proposed Chain-of-Scrutiny (CoS) method (Li et al., 2024) detects backdoor attacks by generating and scrutinizing detailed reasoning steps to identify inconsistencies with the final answer. In this paper, we introduce a simple, scalable, and powerful method for detecting Trojan backdoors across different domains including computer vision and NLP using linear weight classification. We focus on a common formulation of the problem where a set of clean and poisoned deep neural network models is provided, and the task is to predict whether a given test model is clean or poisoned. The detector is obtained by training a linear classifier on a large number of models’ weights after performing a few different pre-processing steps. We start first by applying tensor and weight selection strategies resembling the first step in a forward-stagewise regression approach (Hastie et al., 2009). Normalization was particularly effective when combined with reference model subtraction. We also explored permutation-invariant representations of tensors, and found that sorting was highly effective in addressing the arbitrary permutations of hidden units in trained neural networks. Our method falls under the category of weight analysis detection, which does not require any prior knowledge of the trigger or model outputs and is applicable across multiple domains. We evaluate our approach on several benchmarks including datasets from the Trojan Detection Challenge (TDC22)(Mazeika, 2022) and the IARPA/NIST TrojAI program(Karra et al., 2020). The Trojan Detection Challenge(TDC22), a NeurIPS 2022 competition, tasks participants with detecting and analyzing Trojan attacks on deep neural networks designed to evade detection. The IARPA/NIST TrojAI program is a long-running initiative that has developed over 16 challenges using this formulation, addressing the issue of adversaries inserting Trojan behaviors into AI models by compromising the training pipeline. The program focuses on identifying such Trojans, which can be activated by specific triggers in an AI’s input, causing the model to produce incorrect responses. We also trained both clean and poisoned models from scratch using the Fashion MNIST dataset for our experiments. This dataset was especially valuable in demonstrating the importance of sorting tensors before training the logistic regression detector for neural networks initialized with random weights. The structure of the paper is as follows: Section 2 discusses weight analysis methods for detecting backdoor models, followed by Section 3, which introduces our proposed methodology. Section 4 explains the experimental setup, including the evaluation metrics and datasets. The experimental results are presented in Section 5 followed by conclusions."
https://arxiv.org/html/2411.03348v2,Undermining Image and Text Classification Algorithms Using Adversarial Attacks111The study was conducted during the Next Generation Stem Internship Program 2023 at Oak Ridge National Laboratory,"Machine learning models are prone to adversarial attacks, where inputs can be manipulated in order to cause misclassifications. While previous research has focused on techniques like Generative Adversarial Networks (GANs), there’s limited exploration of GANs and Synthetic Minority Oversampling Technique (SMOTE) in text and image classification models to perform adversarial attacks. Our study addresses this gap by training various machine learning models and using GANs and SMOTE to generate additional data points aimed at attacking text classification models. Furthermore, we extend our investigation to face recognition models, training a Convolutional Neural Network(CNN) and subjecting it to adversarial attacks with fast gradient sign perturbations on key features identified by GradCAM, a technique used to highlight key image characteristics of CNNs use in classification. Our experiments reveal a significant vulnerability in classification models. Specifically, we observe a 20% decrease in accuracy for the top-performing text classification models post-attack, along with a 30% decrease in facial recognition accuracy. This highlights the susceptibility of these models to manipulation of input data. Adversarial attacks not only compromise the security but also undermine the reliability of machine learning systems. By showcasing the impact of adversarial attacks on both text classification and face recognition models, our study underscores the urgent need for develop robust defenses against such vulnerabilities.","Machine learning algorithms have experienced an exponential surge in popularity due to their efficiency in making classifications and predictions. The algorithms have been incorporated into systems that are supporting real world applications, such as object recognition in self driving cars and cancer prediction in medical diagnoses. However, adversarial attacks can make these algorithms insecure and prone to incorrect predictions. An adversarial attack is an input provided to machine learning classifiers for the purpose of causing a misclassification. Past research shows the implications of adversarial attacks in image and text classifiers, demonstrating how adding specific perturbations to inputs result in a substantial decrease in model performance. In this study, we seek to analyze the types of inputs that fool classification models by utilizing Fast Gradient Sign Method (FGSM) perturbation vectors on the result of GradCAM highlighted features, GANs, and SMOTE to generate adversarial attacks. As a result, this study demonstrates the vulnerabilities of machine learning models to adversarial attacks using GANs and SMOTE. This paper presents a novel adversarial attack strategy that combines GANs and SMOTE to target text classifiers and a novel attack on image classifiers with FGSM and GradCAM. Our experiments work to validate the influence of these adversarial attacks against machine learning models deployed in real-world scenarios. The structure of this manuscript is as follows: Section II provides a review of the existing literature and contributions in the domains of GANs and adversarial attacks. Section III articulates the methodological framework employed in the current investigation. Section IV presents the experimental setup, alongside the resulting data and analysis. Finally, Section V offers a summary of the findings, encapsulates the study’s contributions, and outlines potential future research inquiries."
https://arxiv.org/html/2411.03341v1,Interpretable Embeddings for Segmentation-Free Single-Cell Analysis in Multiplex Imaging,"Multiplex Imaging (MI) enables the simultaneous visualization of multiple biological markers in separate imaging channels at subcellular resolution, providing valuable insights into cell-type heterogeneity and spatial organization. However, current computational pipelines rely on cell segmentation algorithms, which require laborious fine-tuning and can introduce downstream errors due to inaccurate single-cell representations. We propose a segmentation-free deep learning approach that leverages grouped convolutions to learn interpretable embedded features from each imaging channel, enabling robust cell-type identification without manual feature selection. Validated on an Imaging Mass Cytometry dataset of 1.8 million cells from neuroblastoma patients, our method enables the accurate identification of known cell types, showcasing its scalability and suitability for high-dimensional MI data.","Multiplex Imaging (MI) has gained significant attention in recent years with the introduction of techniques like CODEX [1], MIBI [2], and Imaging Mass Cytometry (IMC) [3], which enable the visualization of dozens of protein markers, each displayed in its own individual image channel. However, MI presents new challenges due to its high dimensionality. In MI, single-cell characterization is driven by expression profiles of specific surface proteins, rather than by morphology or localization patterns. Quantifying and classifying individual cells from image data has a long history, with numerous methods developed for subcellular protein localization [4] and morphological phenotype classification [5, 6]. Despite the introduction of IMC almost a decade ago, methods for analyzing MI images still depend heavily on cell segmentation, using manually crafted features such as mean intensity per image channel over segmentation masks as a proxy for protein expression [7, 8, 9, 10]. These manually crafted features remain common, because they allow experts to easily interpret protein expression using their domain knowledge for specific cell types. However, these techniques require accurate segmentation, which is challenging with IMC images due to low spatial resolution, ambiguous cell boundaries, and high cell density. Common errors include merging multiple cells, splitting individual cells, and inaccurate border detection, leading to signal contamination from neighboring cells, especially in dense regions, thereby compromising downstream analyses. To mitigate some of these problems, considerable efforts have been made to optimize segmentation algorithms and reduce error rates, but this does not fully solve the issue. To address these challenges, we propose a segmentation-free deep learning model with the following contributions: Segmentation-Free Analysis: We bypass segmentation, using cell-centered patches to learn representations directly from full image content, avoiding manual feature extraction. Interpretability-Focused Design: Our proposed architecture enables experts to directly interpret the model’s outputs by tracking the influence of individual image channels on the embedding space, offering crucial insights otherwise hidden by the complexity of MI data. Validation on Real-World Neuroblastoma IMC Dataset: We validate our method’s applicability for cell phenotyping using an IMC dataset of 1.8 million cells collected from the bone marrow of neuroblastoma patients. Fig. 1: A: Core unit of the proposed architecture. White blocks represent convolutional operations, formatted as: input features, kernel size×kernel sizekernel sizekernel size\text{kernel size}\times\text{kernel size}kernel size × kernel size, and output features. Here, D𝐷Ditalic_D is the number of input features, G𝐺Gitalic_G denotes the number of groups, and E𝐸Eitalic_E indicates the expansion factor of the block. B: Example data flow through the model, starting from a 32×32×C3232𝐶32\times 32\times C32 × 32 × italic_C pixel input patch, where C𝐶Citalic_C is the number of image channels. White blocks represent convolutional layers with a 3×3333\times 33 × 3 kernel size and number of groups (G𝐺Gitalic_G). The interpretability stage (green) is used to obtain channel-specific activation, enabling expert interpretation. The final representation is generated by processing the previously disentangled channels together, allowing for crosstalk between them. C: Markers used in the IMC panel."
https://arxiv.org/html/2411.03314v1,MME-Finance: A Multimodal Finance Benchmark for Expert-level Understanding and Reasoning,"In recent years, multimodal benchmarks for general domains have guided the rapid development of multimodal models on general tasks. However, the financial field has its peculiarities. It features unique graphical images (e.g., candlestick charts, technical indicator charts) and possesses a wealth of specialized financial knowledge (e.g., futures, turnover rate). Therefore, benchmarks from general fields often fail to measure the performance of multimodal models in the financial domain, and thus cannot effectively guide the rapid development of large financial models. To promote the development of large financial multimodal models, we propose MME-Finance, an bilingual open-ended and practical usage-oriented Visual Question Answering (VQA) benchmark. The characteristics of our benchmark are finance and expertise, which include constructing charts that reflect the actual usage needs of users (e.g., computer screenshots and mobile photography), creating questions according to the preferences in financial domain inquiries, and annotating questions by experts with 10+ years of experience in the financial industry. Additionally, we have developed a custom-designed financial evaluation system in which visual information is first introduced in the multi-modal evaluation process. Extensive experimental evaluations of 19 mainstream MLLMs are conducted to test their perception, reasoning, and cognition capabilities. The results indicate that models performing well on general benchmarks cannot do well on MME-Finance; for instance, the top-performing open-source and closed-source models obtain 65.69 (Qwen2VL-72B) and 63.18 (GPT-4o), respectively. Their performance is particularly poor in categories most relevant to finance, such as candlestick charts and technical indicator charts. In addition, we propose a Chinese version, which helps compare performance of MLLMs under a Chinese context. Therefore, we hope to open-source our benchmark to foster the development of multimodal models in the financial domain. The code and data will be released at https://hithink-research.github.io/MME-Finance.","Multimodal Large Language Models (MLLMs) [45], which equip the Large Language Models (LLMs) [34, 32, 12, 39] with the capability of visual understanding, have experienced a revolutionary advancement recently. Works including Flamingo [3], LLaVA [27], CogVLM [41], Gemini [37], and GPT-4o [31] have demonstrated intriguing capability to solve complex multimodal recognition and reasoning tasks. A reasonable and objective benchmark is of enormous significance in the success of MLLMs, which not only helps a better comparison of the performances of MLLMs but also provides valuable guidance for model optimization and real-world applications. Early works of multimodal benchmarks, such as COCO Caption [8], GQA [19], and Flickr30k [47], have served as foundational resources for evaluating MLLMs. However, these benchmarks are task-specific, limiting the scope for fine-grained analysis of MLLMs’ capabilities. More recent efforts, including MME series [13, 14, 51], MMBench [28], and MM-Vet [48], have shifted focus towards general multimodal tasks. These benchmarks comprehensively evaluate diverse capabilities of MLLMs, such as perception and reasoning, through a broader range of tasks. Alongside these general-purpose benchmarks, domain-specific benchmarks are rapidly emerging. For instance, in the medical field, benchmarks like GMAI-MMBench [7] and Asclepius [42] have been developed, while in the autonomous driving domain, NuScenes-QA [33] and DriveLM-DATA [36] are advancing research. These benchmarks significantly accelerated the progress of MLLMs within respective industries. In the financial field, understanding charts presents more unique challenges. (1) Jargon: Financial charts are filled with technical terms such as “bullish”, “bearish”, “support levels”, and “resistance levels”, which may be hard to grasp. (2) Complexity: Financial charts often contain a vast amount of data and information, such as the open, close, high, and low prices on a candlestick chart, along with various technical indicators and oscillators. (3) Diversity of Chart Types: There are multiple types of charts in the financial domain, such as line charts, bar charts, and candlestick charts, each with its specific use cases and interpretation methods. (4) Data Density: Financial charts may include a large number of data points, making it more difficult to identify trends and patterns. Therefore, it is challenging to comprehensively and professionally evaluate the financial capability of MLLMs. Benchmarks like FINANCEBENCH [20] and CFBenchmark [21] are focusing on the evaluation of LLMs. To the best of our knowledge, there is no multimodal benchmark in the financial area, and a significant dearth of Chinese multimodal benchmarks. Hence, a bilingual financial multimodal benchmark is urgent for promoting the development of MLLMs. To break this gap, we propose MME-Finance, a bilingual financial multimodal benchmark for MLLMs. We conduct extensive research on real-world financial application scenarios and select 6 common types of financial charts, including candlestick charts, technical indicator charts, statistical charts, tables, documents, and mixed charts. Based on these images and the actual usage of users in financial scenarios, we design a hierarchical series of open-ended Question Answering (QA) tasks, ranging from general visual perception like Optical Character Recognition (OCR) tasks to complex cognitive tasks such as providing investment advice. To ensure the quality of MME-Finance, we carefully design the annotation pipeline and invite experts with 10+ years of experience in the financial industry to conduct detailed verification of the answers. LLMs and MLLMs are employed for automated evaluation in MME-Finance. Considering the challenges of evaluating financial open-ended questions, we meticulously design the evaluation process and first introduce visual information to boost the evaluation performance. The effectiveness of our evaluation method has been validated through human consistency experiments. Extensive experiments indicate existing MLLMs remain inadequate in meeting the requirements of financial tasks, where the best open-source and closed-source models have scored unsatisfactorily, with only 65.69% (Qwen2VL-72B) and 63.18% (GPT-4o), respectively. Particularly, there are three points worthy of our attention: The first point is that models encounter difficulty in some tasks, especially spatial awareness and estimated numerical calculation. The second point is that the performance related to stock charts is not good (e.g., candlestick charts and technical indicator charts), and the last is that MLLMs generally perform poorly in questions about mobile photography, which however is a relatively high-frequency use case in financial QA. We summarize our major contributions as follows: • We propose MME-Finance, a novel bilingual multimodal benchmark specifically designed to evaluate the capabilities of MLLMs in the financial domain. It comprises 1,171 English and 1,103 Chinese questions, covering diverse financial image types and various multimodal capabilities, and providing a comprehensive evaluation of MLLMs’ performance in the financial domain. • We introduce an evaluation approach of open-ended questions in the financial domain. By designing appropriate prompts for corresponding tasks and exploring evaluation methods firstly combined with image information, we propose a novel evaluation strategy that has a high consistency with humans. The strategy can serve as a reference for evaluating MLLMs for other works. • We conduct extensive evaluation on 19 MLLMs based on MME-Finance, revealing critical insights about the strengths and shortcomings of the current MLLMs in financial applications. The insights gained from this study provide a foundation for future research, guiding the development of more robust MLLMs capable of meeting the demands of complex financial tasks."
https://arxiv.org/html/2411.03313v2,Classification Done Right for Vision-Language Pre-Training,"We introduce SuperClass, a super simple classification method for vision-language pre-training on image-text data. Unlike its contrastive counterpart CLIP clip who contrast with a text encoder, SuperClass directly utilizes tokenized raw text as supervised classification labels, without the need for additional text filtering or selection. Due to the absence of the text encoding as contrastive target, SuperClass does not require a text encoder and does not need to maintain a large batch size as CLIP clip does. SuperClass demonstrated superior performance on various downstream tasks, including classic computer vision benchmarks and vision language downstream tasks. We further explored the scaling behavior of SuperClass on model size, training length, or data size, and reported encouraging results and comparisons to CLIP .","Pretraining methodologies alexnet ; clip ; mehta2024catlip ; rombach2022high that directly harness web-scale image-text dataset have transformed the field of computer vision in recent years. Among them, contrastive language image pretraining (CLIP) clip has gained escalading popularity and become predominant due to the following reasons. First, it serves as the industry-standard pre-trained model that facilitates zero-shot visual recognition luddecke2022image ; minderer2022simple and finetuning on downstream tasks fang2023eva ; dong2022clip . Second, proper scaling behaviors cherti2023reproducible are observed such that CLIP can consistently benefit from larger models and bigger data to some extent. Third, it offers strong cross-modal abilities as it is inherently designed to understand and connect information across text and images. Therefore, CLIP-style models are the default choices for most modern Visual Language Models liu2023llava ; bai2023qwen ; flamingo_paper_22 , which connect a vision backbone with a deep language model touvron2023llama ; chiang2023vicuna . Despite its success, CLIP necessitates very large batch sizes for training—typically over 64,000—to achieve optimal performance, along with substantial computational resources for text encoding. This high computational demand limits accessibility for researchers with limited resources and engineering expertise. In our work, we aim to address the heavy computational burden by replacing contrastive methodology with a simpler classification approach, eliminates the need for large contrastive batch sizes, and text encoders. In this work, we revisit the classification method for pretraining on large-scale text-image pairs. Some previous works mori1999image ; joulin2016learning ; visual_ngram ; huang2023tag2text ; mehta2024catlip attempt to tackle this by employing bag-of-words classification in a weak supervised learning manner. However, most of these studies have been conducted on a small scale, and there is no evidence demonstrating their scalability in terms of data and model size. In contrast, our method demonstrates the performance of SuperClass on a scale comparable to CLIP clip , achieving favorable model performance with 13 billion seen samples on 1 billion unique text-image pairs. Some other concurrent efforts joulin2016learning have also attempted to replace contrastive learning with classification. However, they rely heavily on preprocessing the text modality, using bag-of-words and other hand-crafted rules to convert text into semi-labels. Some common practices include filtering, word segmentation, lemmatization, and the removal of numbers and stopwords to create a unique vocabulary of clean words. We found the preprocessing often eliminates long-tailed words or stopwords that contain valuable information for representation learning (see Sec. 4.4). In contrast, SuperClass simply utilizes raw word tokens as supervision signals without requiring any hand-crafted preprocessing: no filtering or removal of stopwords. Hence SuperClass preserves all information from the original text descriptions as supervision signal. Figure 1: (left) CLIP uses two separate Transformer encoders to extract vector representations from image-text pairs. The text encoder operates on a subword-level tokenizer. (right) The proposed bag of subwords classification both only use the single Transformer encoder. We proposed a Super-simple-Classification approach (SuperClass) that simply trains to classify raw text tokens and scales as good as CLIP. As shown in Figure 1, similar to CLIP, SuperClass directly operate on text tokens with any manual text filtering. Our comprehensive empirical study shows that even without the need for a text encoder, classification methods can achieve performance comparable to the contrastive approach in terms of both model capabilities and data scalability. We demonstrate that SuperClass is a competitive alternative to its contrastive counterpart on both image classification and vision & language tasks. Pretrained on the same Datacomp-1B gadre2023datacomp datasets with an equal number of seen samples, SuperClass dominantly outperforms its contrastive counterparts across various of vision only and vision & language scenarios. We further explore the scaling behavior of SuperClass concerning model size and number of seen samples. Experiments suggest that classification-based methods can exhibit competitive or even superior scaling behavior compared to their contrastive counterparts. We hope our method, experiments and analysis can encourage future potentials of classification-based methods as the foundational vision-language pretraining methods."
https://arxiv.org/html/2411.03312v1,Inference Optimal VLMs Need Only OneVisual Token but Larger Models,"Vision Language Models (VLMs) have demonstrated strong capabilities across various visual understanding and reasoning tasks. However, their real-world deployment is often constrained by high latency during inference due to substantial compute required to process the large number of input tokens (predominantly from the image) by the LLM. To reduce inference costs, one can either downsize the LLM or reduce the number of input image-tokens, the latter of which has been the focus of many recent works around token compression. However, it is unclear what the optimal trade-off is, as both the factors directly affect the VLM performance. We first characterize this optimal trade-off between the number of visual tokens and LLM parameters by establishing scaling laws that capture variations in performance with these two factors. Our results reveal a surprising trend: for visual reasoning tasks, the inference-optimal behavior in VLMs, i.e., minimum downstream error at any given fixed inference compute, is achieved when using the largest LLM that fits within the inference budget while minimizing visual token count — often to a single token. While the token reduction literature has mainly focused on maintaining base model performance by modestly reducing the token count (e.g., 5 – 10×5\text{ -- }10\times5 – 10 ×), our results indicate that the compute-optimal inference regime requires operating under even higher token compression ratios. Based on these insights, we take some initial steps towards building approaches tailored for high token compression settings. Code is available at https://github.com/locuslab/llava-token-compression.","Recent advancements in Large Language Models (LLMs) have enabled Vision Language Models (VLMs) to perceive, reason, and respond through both text and image inputs (Liu et al., 2023; Alayrac et al., 2022; Dai et al., 2023). Many VLMs are built on top of pretrained vision encoders, like CLIP, and pass the patch-based tokens from the visual encoder into the pretrained LLM backbone at a one-to-one ratio for visual context. This results in the LLM processing hundreds of tokens per image, overshadowing those from the user prompt and accounting for most of inference time compute. Consequently, deploying VLMs in real-world applications, particularly on consumer-side edge devices such as monitoring systems, driving assistants, etc., is often limited by the significant inference cost and resulting latency. To reduce the inference cost of VLMs, many recent works have focused on decreasing the number of visual tokens, via a small learnable module, prior to passing image tokens into the LLM while minimizing performance degradation (Li et al., 2024c; Shang et al., 2024). For example, (Li et al., 2024c) learn a cross attention module over the CLIP output tokens to compress the number of tokens. Alternatively, inference FLOPs, proportional to the number of parameters and number of tokens processed, can be reduced by using a smaller LLM. Since both the LLM size and number of visual input tokens directly affect the VLM’s performance, it becomes unclear what the optimal trade-off between the two is. For example, one could process all visual input tokens using a 4B LLM or use an 8B LLM on a reduced set of half the original visual tokens, as both result in similar inference costs — currently, the ideal choice is unknown. This raises an important question: given a fixed inference budget, what is the optimal trade-off between LLM size and the number of visual tokens processed for downstream performance? In this work, we try to answer this question by building the first inference-time compute-optimal scaling laws for VLMs, modeling performance as a function of both key factors affecting inference cost: LLM size and the number of visual tokens processed. We observe that the downstream error varies 5x faster with LLM parameters than with the number of input visual tokens. In fact, our scaling laws reveal a striking observation: for visual reasoning tasks, the compute-optimal inference regime entails using the largest feasible LLM with a very small number of visual input tokens — often just one — when the input query can be cached. We show that for any given fixed inference cost, trading off the visual tokens for a larger LLM size leads to a reduction in downstream error for visual reasoning tasks. However, for certain use cases like Optical Character Recognition (OCR) or document understanding tasks, the optimal approach is quite the opposite, requiring as many visual tokens as possible, as token compression proves ineffective for capturing the dense and diverse information present in such tasks. Most existing work on token compression has focused on reducing visual tokens by a modest factor (e.g., from 576 to 144 tokens or 64 tokens). In contrast, our results underscore the critical importance of pursuing much higher compression rates (e.g., reducing tokens to 1 or 4) for visual reasoning tasks where such compression is not only feasible but also compute-optimal. Building upon these insights, we take initial steps toward developing token compression algorithms specifically tailored for high compression regimes. We propose a query-based token compression approach, recognizing that in extreme compression scenarios, it is essential to selectively curate tokens based on the user’s query to preserve the most relevant tokens. In summary, our work identifies the compute-optimal inference regime for VLMs, emphasizing the importance of high token compression for visual reasoning tasks. We hope these findings will serve as a motivation and foundation for shifting token reduction techniques towards more effective and higher compression ratios. Our work is organized as follows. We first introduce some preliminaries around inference costs and visual token compression for VLMs in Section 2. Then we talk about our compute optimal scaling laws in Section 3, the results of which motivate our compression algorithm designed for high token-reduction regimes are covered in Section 4. (a) Scaling laws for VLMs at Q=0𝑄0Q=0italic_Q = 0 (cached text). (b) Scaling laws for VLMs at Q=50𝑄50Q=50italic_Q = 50 (variable text). Figure 1: Inference optimal scaling laws for VLMs: The number of visual tokens (V𝑉Vitalic_V) passed to the LLM (after token compression, § 2.2), along with the LLM parameter count (N𝑁Nitalic_N), directly determine the inference cost of VLMs (𝒪⁢(N⁢(Q+V))𝒪𝑁𝑄𝑉\mathcal{O}(N(Q+V))caligraphic_O ( italic_N ( italic_Q + italic_V ) )), where Q𝑄Qitalic_Q is the text input tokens. Since a VLM’s downstream performance is directly affected by both these factors, it is unclear what the optimal trade-off is for a fixed inference compute. In this work, we try to answer this question with our scaling laws. Left (a): We plot the fitted scaling curves, assuming cached text input tokens (Q=0𝑄0Q=0italic_Q = 0). We observe a surprising trend: for visual reasoning tasks, the compute optimal behavior (dotted black curve) requires using a single visual token with the largest possible language model that can fit under the inference budget. Right (b): Inference optimal behavior under Q=50𝑄50Q=50italic_Q = 50 requires slightly higher number of visual tokens as the LLM already incurs a fixed cost due to the text tokens."
https://arxiv.org/html/2411.03286v2,DiT4Edit: Diffusion Transformer for Image Editing,"Despite recent advances in UNet-based image editing, methods for shape-aware object editing in high-resolution images are still lacking. Compared to UNet, Diffusion Transformers (DiT) demonstrate superior capabilities to effectively capture the long-range dependencies among patches, leading to higher-quality image generation. In this paper, we propose DiT4Edit†††Project page: https://github.com/fkyyyy/DiT4Edit, the first Diffusion Transformer-based image editing framework. Specifically, DiT4Edit uses the DPM-Solver inversion algorithm to obtain the inverted latents, reducing the number of steps compared to the DDIM inversion algorithm commonly used in UNet-based frameworks. Additionally, we design unified attention control and patches merging, tailored for transformer computation streams. This integration allows our framework to generate higher-quality edited images faster. Our design leverages the advantages of DiT, enabling it to surpass UNet structures in image editing, especially in high-resolution and arbitrary-size images. Extensive experiments demonstrate the strong performance of DiT4Edit across various editing scenarios, highlighting the potential of Diffusion Transformers in supporting image editing.","Recent advances in diffusion models have witnessed impressive progress in text-driven visual generation. The development of these text-to-image (T2I) models, e.g., Stable Diffusion (SD) (Rombach et al. 2022), DALL⋅⋅\cdot⋅E 3 (Betker et al. 2023), and PixArt (Chen et al. 2023), has led to significant impacts on numerous downstream applications (Ma et al. 2024a) (Ma et al. 2024b) (Wang et al. 2024), with image editing as one of the most challenging tasks. Given a synthetic or real input image, image editing algorithms aim to add, remove, or replace entire objects or object attributes according to the user’s intent. A primary challenge in text-driven image editing is maintaining the consistency between the source and target images. Earlier approaches (Choi et al. 2021) (Kawar et al. 2023a) (Zhang et al. 2023) often relied on fine-tuning diffusion models to address this issue. However, these methods typically require considerable time and computation resources, which limits their practical applicability. Recent approaches often utilize DDIM (Song, Meng, and Ermon 2020) inversion to obtain latent maps and then control the attention mechanism in diffusion models for real image editing (Mokady et al. 2023) (Cao et al. 2023). However, the consistency of the edited images depends heavily on the invertibility of the DDIM inversion process. Although some efforts have focused on optimizing this inversion (Ju et al. 2024) (Dong et al. 2023) for better results, the editing framework still relies on too many timesteps (e.g., 50 steps). In addition, current research on image editing tasks mainly uses the UNet-based diffusion model structure (Rombach et al. 2022), making the final editing results heavily bounded by the generative capacity of UNet. Although the attention mechanism in UNet is also derived from the transformer, DiT (Peebles and Xie 2023) based on pure transformers offers a global attention calculation between patches, allowing them to capture broader and more detailed features compared to the UNet with convolution blocks, leading to higher-quality images. In addition, evidence from DiT demonstrates that transformer-based diffusion models offer better scalability and outperform UNet-based models in large-scale experiments. To address these challenges, we explore image editing tasks using the diffusion transformer architecture and provide a valuable empirical baseline for future research. First, we aim to leverage solvers that require fewer inversion steps to reduce our inference time while maintaining the image quality of the results. Specifically, we employ an inversion algorithm based on a high-order DPM-Solver (Lu et al. 2023) to obtain better latent maps with fewer timesteps. We then implement a unified attention control scheme for text-guided image editing while preserving background details. Third, to mitigate the increased computational complexity of transformers compared to UNet, we use patches merging to accelerate computation. By integrating these key components, we introduce DiT4Edit, the first diffusion transformer-based editing framework to our knowledge. Experiments demonstrate that our framework achieves superior editing results with fewer inference steps and offers distinct advantages over traditional UNet-based methods. Figure 1: Visual results of DiT4Edit. Our method is the first DiT-based image editing framework, which is capable of handling images of various sizes: from small (512×512512512512\times 512512 × 512) to large (1024×1024102410241024\times 10241024 × 1024), and even arbitrary dimensions (up to 1024×2048102420481024\times 20481024 × 2048). In summary, our contributions are as follows: • Based on the advantages of transformer-based diffusion models in image editing, we introduce DiT4Edit, the first tuning-free image editing framework using Diffusion Transformers (DiT). • To adapt to the computing mechanism of transformer-based denoising, we first propose a unified attention control mechanism to achieve image editing. Then, we introduce the DPM-Solver inversion and patches merging strategy to reduce inference time. • Extensive qualitative and quantitative results demonstrate the superior performance of DiT4Edit in object editing, style editing, and shape-aware editing for various image sizes, including 512×512512512512\times 512512 × 512, 1024×1024102410241024\times 10241024 × 1024, 1024×2048102420481024\times 20481024 × 2048."
https://arxiv.org/html/2411.03260v1,ShadowMamba: State-Space Model with Boundary-Region Selective Scan for Shadow Removal,"Image shadow removal is a typical low-level vision problem, where the presence of shadows leads to abrupt changes in brightness in certain regions, affecting the accuracy of upstream tasks. Current shadow removal methods still face challenges such as residual boundary artifacts, and capturing feature information at shadow boundaries is crucial for removing shadows and eliminating residual boundary artifacts. Recently, Mamba has achieved remarkable success in computer vision by globally modeling long-sequence information with linear complexity. However, when applied to image shadow removal, the original Mamba scanning method overlooks the semantic continuity of shadow boundaries as well as the continuity of semantics within the same region. Based on the unique characteristics of shadow images, this paper proposes a novel selective scanning method called boundary-region selective scanning. This method scans boundary regions, shadow regions, and non-shadow regions independently, bringing pixels of the same region type closer together in the long sequence, especially focusing on the local information at the boundaries, which is crucial for shadow removal. This method combines with global scanning and channel scanning to jointly accomplish the shadow removal. We name our model ShadowMamba, the first Mamba-based model for shadow removal. Extensive experimental results show that our method outperforms current state-of-the-art models across most metrics on multiple datasets. The code for ShadowMamba is available at (Code will be released upon acceptance).","Shadows are cast when objects block light, making it inevitable that they are captured in images during acquisition. The presence of shadows not only causes the image to lose certain information but also affects the accuracy of downstream tasks, such as object detection [1, 2], instance segmentation [3, 4], and image classification [5, 6]. Therefore, image shadow removal is a crucial task. Shadow removal is a typical low-level vision problem, but it differs from other low-level vision tasks like image super-resolution [7, 8], low-light image enhancement [9, 10], and image deraining [11] or dehazing [12], which involve processing the entire image, shadow removal focuses only on the damaged regions within the image. As a result, it requires separate modeling of the shadow regions, making it a challenging task. Traditional shadow removal methods are primarily divided into illumination transfer methods [13, 14] and shadow region relighting methods [15, 16]. These approaches rely on physical modeling of the shadow itself and are effective for single shadow types, but they often perform poorly in complex background scenarios. In recent years, deep learning methods have gradually replaced traditional approaches, leading to the emergence of numerous shadow removal methods [17, 18, 19, 20, 21] based on CNN and transformer architectures. These methods model shadow images based on brightness information [22, 23], shadow boundary information [24, 25], or regional information [18, 20, 21], utilizing deep networks for shadow removal. Among these, shadow boundary information is an effective feature for shadow removal. Due to the highly complex transition of intensity and color at shadow boundaries, removing shadow pixels at the boundary is more challenging than removing pixels within the shadowed area. Therefore, helping the model understand shadow boundary information and local features at the boundary is crucial for shadow removal. Most current approaches [26, 17, 27] post-process the shadow boundaries. While this can lead to some performance improvements, balancing shadow removal with detail preservation at shadow boundary pixels is challenging when relying solely on smoothing constraints, thereby limiting its effectiveness. Some methods [25, 24] directly model the shadow boundary, with one representative supervised approach being BA-ShadowNet [24]. It uses a shadow boundary mask to crop the boundary pixels from the original image and applies supervised constraints directly to these pixels, fusing the boundary information with the information from the shadow removal branch. Although this method effectively improves the removal of boundary artifacts, it processes boundary pixels separately by cropping them out, which completely discards the semantic relationships between boundary pixels and shadow or non-shadow regions, affecting the model’s understanding of brightness variation between regions. Moreover, the CNN architecture used in this method cannot model all boundary pixels globally, resulting in suboptimal results. Based on this idea, we envision developing a model that can effectively model all boundary pixels while preserving the relationships among boundary regions, shadow regions, and non-shadow regions, thereby improving the performance of boundary-based shadow removal methods. Recently, an improved structured state-space sequence model named Mamba [28] has been introduced. It is capable of modeling long-sequence relationships with linear complexity and introduces a novel form of attention through the selective scanning mechanism, outperforming transformer models in many tasks [29, 30]. Originally, Mamba was designed for sequence modeling of one-dimensional (1D) data, employing 1D causal convolutions to process the input. In the field of computer vision, it is crucial to fully consider the relationships between pixels in the up, down, left, and right directions, ensuring that the scanning sequence is spatially continuous and enhancing semantic continuity. Therefore, many methods improve the processing performance of two-dimensional (2D) image data by modifying the scanning paths of the selective scanning mechanism. Vim [31] and Vmamba [29] are among the first to apply Mamba to the visual field, designing bidirectional scanning and horizontal-vertical cross-scanning mechanisms. Many Mamba-based visual models have been proposed based on this, such as Plain-Mamba [32], Mamba-ND [33], and LocalMamba [34]. These models have significantly improve Mamba’s performance in image tasks by adjusting the scanning order and increasing semantic relevance. Based on the previous assumption, if the scanning order in Mamba’s selective scanning mechanism can follow the shadow boundaries as much as possible, or bring the pixels at the shadow boundaries closer in the long sequence, while retaining the information and connections from both shadow and non-shadow regions, it could enhance the performance of shadow removal. Most of the existing Mamba-based methods perform sequential scanning of the entire image. Although LocalMamba [34] captures local information using windows, it still scans all windows sequentially. For shadow removal, while sequential scanning can also be used for shadow removal, it does not take into account the specific characteristics of shadow images, thus failing to achieve promising performance. Sequential scanning ignores shadow boundary information, causing the semantic relationships of pixels at shadow boundaries to be disrupted. Furthermore, sequential scanning causes pixels within the same region to be separated by pixels from another region, affecting the correlation between pixels within the same region and resulting in longer distances between them in the long sequence, which hinders the ability to effectively capture their semantic relationships. Based on this, this paper designs a new boundary-region selective scanning mechanism by leveraging the characteristics of shadow images. It is inspired by LocalMamba’s local scanning mode [34] and introduces an additional mask input to divide the image into several windows. According to the rules designed by this mechanism, the windows are classified into shadow regions, boundary regions, and non-shadow regions, and then rearranged into a new sequence. The pixel information in the three regions is scanned sequentially using horizontal and vertical cross-scanning. This significantly enhances the semantic relevance among pixels within the same type of region, allowing for better capture of detailed information at the shadow boundaries, thereby improving shadow removal performance. Additionally, global scanning is also applied in our proposed method, in which another branch is employed for global scanning to capture the global information of the image and the relationships between the original pixels. Fig.1 provides a detailed explanation of the differences between the boundary-region scanning mechanism and the local scanning mechanism. (a) Local scan [34] (b) Boundary-region scan (Ours) Figure 1: The difference between the proposed boundary-region scanning and local scanning. Both the purple window and the orange window belong to the boundary region of the shadow, and they have a strong semantic correlation, so their distance in the long sequence should be close. Whether scanning horizontally or vertically, our proposed boundary-region scanning reduces their distance in the long sequence. The main contributions of this paper are as follows: • We introduce the state-space sequence model Mamba to image shadow removal for the first time and design a U-Net-based model called ShadowMamba. It consists of several Dual-branch Selective Scanning Blocks (DSSB), with the two branches respectively capturing the global information of shadow images and the local information of boundaries and regions. • We design a Boundary-Region Selective Scanning Module (BRSSM) that uses the shadow mask to classify the divided windows into shadow regions, boundary regions, and non-shadow regions. It scans these three types of windows sequentially to ensure the semantic continuity of pixels of the same type, which helps capture boundary information useful for the shadow removal. • ShadowMamba outperforms state-of-the-art methods on most metrics of the SRD, ISTD, and ISTD+ datasets, particularly demonstrating remarkable effectiveness in handling soft shadows. This novel architecture has the potential to challenge the dominance of transformer architectures in the field of shadow removal, offering researchers a fresh perspective."
https://arxiv.org/html/2411.03239v1,Decoupling Fine Detail and Global Geometry forCompressed Depth Map Super-Resolution,"Recovering high-quality depth maps from compressed sources has gained significant attention due to the limitations of consumer-grade depth cameras and the bandwidth restrictions during data transmission. However, current methods still suffer from two challenges. First, bit-depth compression produces a uniform depth representation in regions with subtle variations, hindering the recovery of detailed information. Second, densely distributed random noise reduces the accuracy of estimating the global geometric structure of the scene. To address these challenges, we propose a novel framework, termed geometry-decoupled network (GDNet), for compressed depth map super-resolution that decouples the high-quality depth map reconstruction process by handling global and detailed geometric features separately. To be specific, we propose the fine geometry detail encoder (FGDE), which is designed to aggregate fine geometry details in high-resolution low-level image features while simultaneously enriching them with complementary information from low-resolution context-level image features. In addition, we develop the global geometry encoder (GGE) that aims at suppressing noise and extracting global geometric information effectively via constructing compact feature representation in a low-rank space. We conduct experiments on multiple benchmark datasets, demonstrating that our GDNet significantly outperforms current methods in terms of geometric consistency and detail recovery. In the ECCV 2024 AIM Compressed Depth Upsampling Challenge [2], our solution won the 1st place award. Our codes will be available.","Depth perception technologies play an increasingly critical role in advanced applications such as autonomous driving [19], augmented reality [17], and robotics [4], where systems depend on accurate depth maps to deliver detailed 3D information for precise navigation [20], object recognition [8], and interaction [22]. High-quality depth data is essential for ensuring safety [37], enhancing reliability [10], and improving overall system performance [46]. However, in practical scenarios, both low-cost consumer-grade depth cameras and bandwidth-limited transmission can significantly degrade the quality of depth maps [2]. Consumer-grade cameras often produce depth maps with reduced bit-depths to strike a balance between cost and performance, resulting in less accurate data [35]. Moreover, under bandwidth constraints, maintaining system efficiency requires compression through downsampling and bit-depth reduction, which further degrades the quality of depth maps [2]. The introduction of random noise during both acquisition and transmission exacerbates these issues, making it even more difficult to recover high-quality depth maps from such compressed sources [32]. Figure 1 (a) shows the detailed degradation process during depth data acquisition and transmission. Two major issues are revealed during acquisition and transmission: (1) bit-depth compression leads to uniform depth representation in areas with subtle variations, making it difficult to recover fine geometry details accurately; (2) densely distributed noise in the compressed depth map negatively impacts the reasoning of global geometric information in the scene. However, existing guided depth map super-resolution (GDSR) primarily address challenges associated with resolution downsampling in depth maps, neglecting the additional complexities introduced by bit-depth compression and random noise [30, 45, 44, 52, 24, 5]. To further investigate this issue, we present the paired RGB image and compressed depth map in Figure 1 (b). We observe that the RGB image lacks depth information, while the compressed depth map exhibits poor performance in capturing fine geometry details. Conversely, the compressed depth map effectively provides global depth cues, whereas the RGB image contains rich fine geometry details. In this paper, we introduce geometry-decoupled network (GDNet), a new framework designed for compressed depth map super-resolution. The primary objective of the proposed GDNet is to decouple the high-quality depth map reconstruction process into detailed geometric feature learning and global geometric feature extraction. Specifically, we propose the fine geometry detail encoder (FGDE), crafted to maintain fine geometry details in high-resolution low-level image features while concurrently enhancing them with supplementary information from low-resolution context-level image features. In addition, we develop the global geometry encoder (GGE) that constructs the compressed feature representation in a low-rank space, effectively minimizing noise and facilitating the extraction of global geometric information. To verify the effectiveness of the proposed method, we synthesis a new dataset termed Compressed-NYU, where the samples suffer from synchronous downsampling, bit-depth compression and random noise. Through comprehensive experiments on multiple benchmarks, we demonstrate that GDNet significantly surpasses existing methods in terms of recovery quality, noise suppression, and fine geometry detail restoration. In summary, the main contributions of this paper are outlined as follows: • We present a new framework termed geometry-decoupled network (GDNet), which develops a decoupling strategy to independently learn global and detailed geometric features for compressed depth map super-resolution. • We propose the fine geometry detail encoder (FGDE) designed to preserve fine geometry details in high-resolution low-level image features while enriching them with complementary information from low-resolution context-level image features. • We develop the global geometry encoder (GGE) that facilitates compact feature representation in a low-rank space, enhancing noise suppression and effectively extracting global geometric information. • Our model achieves state-of-the-art performance and obtains superior visual results on benchmark datasets. Figure 2: The overall framework of the proposed GDNet. Our GDNet leverages RGB images to capture fine geometric details while utilizing compressed depth maps to provide global depth information. By employing the above decoupling strategy, the proposed GDNet is able to reconstruct high-quality depth maps with improved accuracy. Specifically, GDNet comprises three main components: a fine geometry detail encoder, responsible for detailed geometric feature extraction; a global geometry encoder, aiming at capturing global geometric features; a depth decoder to produce high-quality depth map."
https://arxiv.org/html/2411.03228v1,Topograph: An efficient Graph-Based Framework for Strictly Topology Preserving Image Segmentation,"Topological correctness plays a critical role in many image segmentation tasks, yet most networks are trained using pixel-wise loss functions, such as Dice, neglecting topological accuracy. Existing topology-aware methods often lack robust topological guarantees, are limited to specific use cases, or impose high computational costs. In this work, we propose a novel, graph-based framework for topologically accurate image segmentation that is both computationally efficient and generally applicable. Our method constructs a component graph that fully encodes the topological information of both the prediction and ground truth, allowing us to efficiently identify topologically critical regions and aggregate a loss based on local neighborhood information. Furthermore, we introduce a strict topological metric capturing the homotopy equivalence between the union and intersection of prediction-label pairs. We formally prove the topological guarantees of our approach and empirically validate its effectiveness on binary and multi-class datasets. Our loss demonstrates state-of-the-art performance with up to fivefold faster loss computation compared to persistent homology methods.111Code is available at https://anonymous.4open.science/r/Topograph","In segmentation and structural analysis tasks, maintaining topological integrity is often more critical than simply improving pixel-wise accuracy. For example, in medical imaging, the topological integrity of segmented structures, such as blood vessels or neural pathways, can be crucial for accurate diagnosis and functional analysis (Briggman et al., 2009). However, topological errors, such as loss of connectivity, are common in practice, even when pixel-wise accuracy is high. Standard pixel-based loss functions, such as Dice-loss, do not adequately address these issues. While they minimize pixel-level discrepancies, they do not take into account changes in topology, which may be caused by few or even single pixels. As a result, even small pixel-wise errors can lead to significant topological failures. Previous works have shown how different topology-aware methods can improve the integrity of target structures without sacrificing pixel-wise accuracy. Task-specific methods, such as those designed for tubular structure segmentation (Shit et al., 2021; Kirchhoff et al., 2024; Menten et al., 2023), are computationally efficient and perform well in their respective domains. However, they do not generalize effectively to other types of topological structures or datasets. In contrast, persistent homology (PH)-based methods can provide strong theoretical guarantees and deliver state-of-the-art performance (Hu et al., 2019; Stucki et al., 2023; Clough et al., 2020), but are computationally more demanding. Other topology-aware methods can be more versatile and computationally efficient, but lack theoretical guarantees for topological correctness (Mosinska et al., 2018; Funke et al., 2018; Hu et al., 2021). This work proposes a loss function that generalizes to various segmentation tasks where topology is crucial. Our method is based on a component graph that combines joint topological information of ground truth and prediction (see Figure 1). A theoretically founded analysis of the nodes in the graph allows the identification of topologically critical regions, which we then use for loss computation. Figure 1: Visualization of the proposed component graph representation. Left: Input image; Right: Overlay of the prediction (P𝑃Pitalic_P) and ground truth (G𝐺Gitalic_G). The bright green lines indicate the foreground structures in the ground truth, with (darker) green regions indicating correctly predicted foreground and pink regions representing incorrectly predicted foreground. A combined component graph 𝒢⁢(P,G)𝒢𝑃𝐺\mathcal{G}(P,G)caligraphic_G ( italic_P , italic_G ) is constructed to efficiently identify topological errors, which are used to compute a loss. Our contribution. We (1) establish a metric that captures topological correctness with strict theoretical guarantees, especially capturing the homotopy equivalence between union and intersection of a label/prediction pair, and (2) formulate a general topology-preserving loss for training arbitrary segmentation networks. Specifically, our loss formulation 1. surpasses existing methods in terms of topological correctness of predictions; 2. provides stricter topological guarantees than existing works, i.e., formal guarantees beyond the homotopy equivalence of ground truth and segmentation, by extending the enforced homotopy equivalence to their union and intersection through the respective inclusion maps, capturing the spatial correspondence of their topological properties; 3. is time and resource-efficient because of its low asymptotic complexity (O⁢(n⋅α⁢(n))𝑂⋅𝑛𝛼𝑛O(n\cdot\alpha(n))italic_O ( italic_n ⋅ italic_α ( italic_n ) )) and empirically low runtime; 4. and is flexible, making it applicable to arbitrary structures and image domains. We empirically validate the prediction performance on various public datasets for binary and multiclass segmentation tasks. Related Work Significant progress has been made in segmentation methods that aim to preserve topological accuracy. The use of persistent homology (PH)-based loss functions for training segmentation networks (Hu et al., 2019; Clough et al., 2020; Funke et al., 2018) ensures global topological correctness by aligning Betti numbers when minimized to zero. However, two issues persist: 1) most methods cannot guarantee spatially related matched structures, and 2) computational cost. Stucki et al. (2023) introduce the Betti Matching concept, which ensures spatially correct barcode matching. However, the cost of barcode computation is substantially higher compared to overlap based methods making its derived methods applicable only to relatively small patch sizes. PH-based methods typically use patch sizes of 48×48484848\times 4848 × 48, 65×65656565\times 6565 × 65, and 80×80808080\times 8080 × 80 respectively. These limit the applicability to medical and natural images, where whole images are commonly an order of magnitude larger. Another limitation is the gradient’s dependence on just two simplex values, which Nigmetov & Morozov (2024) recently showed to hinder optimization speed. While other methods are computationally more efficient (Hu et al., 2021; Mosinska et al., 2018), they offer limited guarantees of topological correctness. Task-specific, overlap-based approaches have been proposed for tubular structures where connectivity is the key topological feature. ClDice (Shit et al., 2021) calculates a loss term based on the union of ground truth skeletons and predicted volumes, a method extended in recent studies (Kirchhoff et al., 2024; Menten et al., 2023). Other approaches refine tubular-structure features through iterative feedback learning strategies (Cheng et al., 2021) or rely on post-processing networks (Li et al., 2023; Wu et al., 2024). However, none of these approaches generalize effectively to arbitrary structures."
https://arxiv.org/html/2411.03226v1,Kernel Orthogonality does not necessarily imply a Decrease in Feature Map Redundancy in CNNs: Convolutional Similarity Minimization,"Convolutional Neural Networks (CNNs) have been heavily used in Deep Learning due to their success in various tasks. Nonetheless, it has been observed that CNNs suffer from redundancy in feature maps, leading to inefficient capacity utilization. Efforts to mitigate and solve this problem led to the emergence of multiple methods, amongst which is kernel orthogonality through variant means. In this work, we challenge the common belief that kernel orthogonality leads to a decrease in feature map redundancy, which is, supposedly, the ultimate objective behind kernel orthogonality. We prove, theoretically and empirically, that kernel orthogonality has an unpredictable effect on feature map similarity and does not necessarily decrease it. Based on our theoretical result, we propose an effective method to reduce feature map similarity independently of the input of the CNN. This is done by minimizing a novel loss function we call Convolutional Similarity. Empirical results show that minimizing the Convolutional Similarity increases the performance of classification models and can accelerate their convergence. Furthermore, using our proposed method pushes towards a more efficient use of the capacity of models, allowing the use of significantly smaller models to achieve the same levels of performance.","Convolutional Neural Networks (CNNs) (LeCun et al., 1989) are known for being a powerful class of models in Deep Learning (DL). After more than two decades of development, one can find multiple complex convolutional architectures capable of advanced levels of classification (Krizhevsky et al., 2012; He et al., 2015; Simonyan and Zisserman, 2015), segmentation (Ronneberger et al., 2015) and generation (Sauer et al., 2023; Karras et al., 2021). Despite their substantial success, it has been observed that the capacity of CNNs is underused due to redundancy (Rodríguez et al., 2017; Cogswell et al., 2016; Wang et al., 2020; Jaderberg et al., 2014): models tend to learn redundant weights and hidden layers yield highly similar feature maps, thus wasting model capacity and limiting performance. This form of information redundancy manifests itself in both feature maps (Cogswell et al., 2016) and kernels (Denil et al., 2013; Wang et al., 2020), which should come as no surprise since, intuitively and in alignment with common belief, one would think that similar kernels yield similar features maps. Before delving into the core of the matter, it is necessary to define the term ""similarity"". From an information-theoretic perspective, it refers to the mutual information between parameters (linear neurons or convolutional kernels), i.e., given two parameters, the amount of information that can be predicted about one based on knowledge of the other. A pertinent example to illustrate that definition is (Denil et al., 2013), where the authors could successfully predict the values of the weights based on their knowledge of a small subset thereof. Redundancy can also be defined as one minus the relative entropy (the entropy divided by its maximum value) (Shannon, 1948), which can be interpreted as the part of the message that does not provide any new information. In this work, a linear approach is adopted rather than an information-theoretic one to quantify similarity: the inner product. Orthogonality is used as a metric and, therefore, linear redundancy is tackled in the present work, which is different from mutual information although they overlap. Redundancy in CNNs has been documented and studied in multiple existing works Cogswell et al. (2016); Denil et al. (2013); Bansal et al. (2018), most of which tackle it through kernel orthogonality. However, to the best of our knowledge, nowhere in the existing literature does the relationship between kernel and feature map redundancies arise in any in-depth manner despite its fundamental importance to the matter of redundancy in CNNs. Therefore, the main question that the present work answers is: how does kernel orthogonality affect feature map orthogonality? The pertinence of the question stems from the fact that the end goal of reducing redundancy in CNNs is not kernel orthogonality per se, but feature map orthogonality. If different kernels somehow yield similar feature maps, then limiting the scope to only kernel orthogonality would be of virtually no use vis-à-vis reducing redundancy in CNNs. Another question might arise, that regarding the utility of minimizing redundancy in CNNs. We consider that the objective is to ultimately have minimal redundancy in feature maps for more efficient use of CNNs’ capacity, which is known to be underused Rodríguez et al. (2017); Cogswell et al. (2016). In the absence of a redundancy-minimizing mechanism, CNNs lack the ability to advertently learn non-redundant features. A simple example to verify this would be to initialize the kernels of a CNN with the same values. After and throughout training, the kernels would remain equal, thus leading to equal feature maps. This indicates that the kernels of a channel are learned independently of each other. Therefore, the redundancy of feature maps depends solely on initialization, optimization, and the parameter solution space, which in turn depends on the dataset and model configuration. Given two feature maps F1subscript𝐹1F_{1}italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and F2subscript𝐹2F_{2}italic_F start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, a straightforward solution to reduce feature map similarity would be to explicitly minimize it by defining an objective function that constrains the inner product of F1subscript𝐹1F_{1}italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and F2subscript𝐹2F_{2}italic_F start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. However, in terms of scalability, an explicit minimization would not be ideal considering the potentially huge computational overhead it would introduce for large applications. A better solution would be one that implicitly reduces the inner product of F1subscript𝐹1F_{1}italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and F2subscript𝐹2F_{2}italic_F start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT by defining an objective function that depends only on kernels. Defining such a function requires a careful examination of the theory underlying feature map similarity with regard to kernels. In this context, the present work is, to the best of our knowledge, the first to theoretically and experimentally unfold the dynamics underlying feature map orthogonality with regard to kernels. The conclusion is that kernel orthogonality does not yield a reduction in feature map similarity. Even more, kernel orthogonality can increase feature map similarity in a volatile manner. Based on the subsequent theory, a novel loss function we call Convolutional Similarity is proposed. Minimizing Convolutional Similarity effectively minimizes feature map similarity. It can be minimized either during or prior to model training. In the latter case, it is used as an iterative initialization scheme. The formulation of the proposed method only depends on kernels, and thus has a computational complexity independent of the size of input data dimensions (in contrast to explicit feature map decorrelation (Cogswell et al., 2016), where the complexity is proportional to the input size). Our experiments show that minimizing Convolutional Similarity can significantly improve the classification accuracy of CNNs and accelerate convergence. Additionally, using our method allows using significantly smaller models as they can achieve performance on par with significantly bigger models that do not use our method. The paper is organized as follows: after presenting the related work and the necessary terminology in Section 3, we proceed to empirically identifying the limitation of kernel orthogonality with regard to feature map similarity minimization in Section 4.1. A theoretical analysis of feature map orthogonality with regard to kernels is then presented in Section 4.2, based on which the Convolutional Similarity loss is derived. In Section 4, all theoretical results are supported by numerical validation. In Section 5, the results are extended to all variants of the convolution and the cross-correlation operations. Afterward, experiments on CNN models are presented in Section 6 and the limitations in Section 7. Finally, the conclusion and future work are presented in Section 8."
https://arxiv.org/html/2411.03177v1,Three Things Everyone Should Know About Training Diffusion Models,"Large-scale training of latent diffusion models (LDMs) has enabled unprecedented quality in image generation. However, the key components of the best performing LDM training recipes are oftentimes not available to the research community, preventing apple-to-apple comparisons and hindering the validation of progress in the field. In this work, we perform an in-depth study of LDM training recipes focusing on the performance of models and their training efficiency. To ensure apple-to-apple comparisons, we re-implement five previously published models with their corresponding recipes. Through our study, we explore the effects of (i) the mechanisms used to condition the generative model on semantic information (e.g., text prompt) and control metadata (e.g., crop size, random flip flag, etc.) on the model performance, and (ii) the transfer of the representations learned on smaller and lower-resolution datasets to larger ones on the training efficiency and model performance. We then propose a novel conditioning mechanism that disentangles semantic and control metadata conditionings and sets a new state-of-the-art in class-conditional generation on the ImageNet-1k dataset – with FID improvements of 7% on 256 and 8% on 512 resolutions – as well as text-to-image generation on the CC12M dataset – with FID improvements of 8% on 256 and 23% on 512 resolution.","1 Introduction Diffusion models have emerged as a powerful class of generative models and demonstrated unprecedented ability at generating high-quality and realistic images. Their superior performance is evident across a spectrum of applications, encompassing image [7, 14, 39, 41] and video synthesis [35], denoising [52], super-resolution [49] and layout-to-image synthesis [51]. The fundamental principle underpinning diffusion models is the iterative denoising of an initial sample from a trivial prior distribution, that progressively transforms it to a sample from the target distribution. The popularity of diffusion models can be attributed to several factors. First, they offer a simple yet effective approach for generative modeling, often outperforming traditional approaches such as Generative Adversarial Networks (GANs) [3, 16, 24, 25] and Variational Autoencoders (VAEs) [29, 48] in terms of visual fidelity and sample diversity. Second, diffusion models are generally more stable and less prone to mode collapse compared to GANs, which are notoriously difficult to stabilize without careful tuning of hyperparameters and training procedures [23, 30]. Despite the success of diffusion models, training such models at scale remains computationally challenging, leading to a lack of insights on the most effective training strategies. Training recipes of large-scale models are often closed (e.g., DALL-E, Imagen, Midjourney), and only a few studies have analyzed training dynamics in detail [7, 14, 26, 27]. Moreover, evaluation often involves human studies which are easily biased and hard to replicate [17, 56]. Due to the high computational costs, the research community mostly focused on the finetuning of large text-to-image models for different downstream tasks [1, 4, 54] and efficient sampling techniques [34, 36, 45]. However, there has been less focus on ablating different mechanisms to condition on user inputs such as text prompts, and strategies to pre-train using datasets of smaller resolution and/or data size. The benefits of conditioning mechanisms are two-fold: allowing users to have better control over the content that is being generated, and unlocking training on augmented or lower quality data by for example conditioning on the original image size [39] and other metadata of the data augmentation. Improving pre-training strategies, on the other hand, can allow for big cuts in the training cost of diffusion models by significantly reducing the number of iterations necessary for convergence. Figure 1: Qualitative examples. Images generated using our model trained on CC12M at 512 resolution. Our work aims to disambiguate some of these design choices, and provide a set of guidelines that enable the scaling of the training of diffusion models in an efficient and effective manner. Beyond the main architectural choices (e.g., Unet vs. ViT), we focus on two other important aspects for generative performance and efficiency of training. First, we enhance conditioning by decoupling different conditionings based on their type: control metadata conditioning (e.g., crop size, random flip, etc.), semantic-level conditioning based on class names or text-prompts. In this manner, we disentangle the contribution of each conditioning and avoid undesired interference among them. Second, we optimize the scaling strategy to larger dataset sizes and higher resolution by studying the influence of the initialization of the model with weights from models pre-trained on smaller datasets and resolutions. Here, we propose three improvements needed to seamlessly transition across resolutions: interpolation of the positional embeddings, scaling of the noise schedule, and using a more aggressive data augmentation strategy. In our experiments we evaluate models at 256 and 512 resolution on ImageNet-1k and Conceptual Captions (CC12M), and also present results for ImageNet-22k at 256 resolution. We study the following five architectures: Unet/LDM-G4 [39], DiT-XL2 w/ LN [38], mDT-v2-XL/2 w/ LN [15], PixArt-α𝛼\alphaitalic_α-XL/2, and mmDiT-XL/2 (SD3) [14]. We find that among the studied base architectures, mmDiT-XL/2 (SD3) performs the best. Our improved conditioning approach further boosts the performance of the best model consistently across metrics, resolutions, and datasets. In particular, we improve the previous state-of-the-art DiT result of 3.04 FID on ImageNet-1k at 512 resolution to 2.76. For CC12M at 512 resolution, we improve FID of 11.24 to 8.64 when using our improved conditioning, while also obtaining a (small) improvement in CLIPscore from 26.01 to 26.17. See Fig. 1 for qualitative examples of our model trained on CC12M. In summary, our contributions are the following: • We present a systematic study of five different diffusion architectures, which we train from scratch using face-blurred ImageNet and CC12M datasets at 256 and 512 resolutions. • We introduce a conditioning mechanism that disentangles different control conditionings and semantic-level conditioning, improving generation and avoiding interference between conditions. • To transfer weights from pre-trained models we propose to interpolate positional embeddings, scale the noise schedule, and use stronger data augmentation, leading to improved performance. • We obtain state-of-the-art results at 256 and 512 resolution for class-conditional generation on ImageNet-1k and text-to-image generation and CC12M. Figure 2: Influence of control conditions. Images generated using the same latent sample. Top: Model trained with constant weighting of the size conditioning as used in SDXL [39], introducing undesirable correlations between image content and size condition. Bottom: Model trained using our cosine weighting of low-level conditioning, disentangling the size condition from the image content."
https://arxiv.org/html/2411.03169v1,Pre-trained Visual Dynamics Representations for Efficient Policy Learning,"Pre-training for Reinforcement Learning (RL) with purely video data is a valuable yet challenging problem. Although in-the-wild videos are readily available and inhere a vast amount of prior world knowledge, the absence of action annotations and the common domain gap with downstream tasks hinder utilizing videos for RL pre-training. To address the challenge of pre-training with videos, we propose Pre-trained Visual Dynamics Representations (PVDR) to bridge the domain gap between videos and downstream tasks for efficient policy learning. By adopting video prediction as a pre-training task, we use a Transformer-based Conditional Variational Autoencoder (CVAE) to learn visual dynamics representations. The pre-trained visual dynamics representations capture the visual dynamics prior knowledge in the videos. This abstract prior knowledge can be readily adapted to downstream tasks and aligned with executable actions through online adaptation. We conduct experiments on a series of robotics visual control tasks and verify that PVDR is an effective form for pre-training with videos to promote policy learning.","Recent years have seen groundbreaking advancements in Computer Vision (CV) and Natural Language Processing (NLP), notably enhanced by self-supervised pre-training [11, 16, 50, 18, 27], showcasing the efficacy of pre-training in distilling the prior knowledge about the world from vast data. As acknowledged few-shot learners [11], pre-trained models have also been applied in Reinforcement Learning (RL) [57]. Pre-training may improve the poor sample efficiency and generalization ability of online RL, mitigating the demand for extensive costly online interactions, especially in complex visual control tasks [62]. Compared to the readily available data in vision and natural language pre-training, the collection of data poses a challenge on RL pre-training. Early works use expert demonstrations [10], sub-optimal data [32], and reward-free data [87] for offline policy learning, necessitating specific collection efforts. Considering this, some researches [65, 6, 43] turn to more accessible and affordable video data. How in-the-wild videos can be used for RL pre-training becomes a promising and valuable topic to explore, which we term pre-traininig with videos. Pre-training with videos presents both potential and challenges. As an accessible and affordable data source, in-the-wild videos offer a rich repository of dynamic prior knowledge. Through self-supervised learning with video data, models can assimilate prior world knowledge pivotal for effective policy learning, including subject consistency, actions, and transitions [37, 80, 13]. Conversely, pre-training with the non-specifically collected videos encounters two main challenges. First, the lack of action annotations hinders the direct integration of videos in policy learning. Second, the common domain gap between videos and downstream tasks, given their content-related but not strictly identical nature, necessitates an online adaptation mechanism. Faced with these hurdles, existing methods generally fall into two pathways. Some works merely pre-train visual state representations with frames or temporal sequences from videos [59, 84, 35, 58, 51, 43]. The pre-trained representations benefit the understanding of visual states but inadequately leverage task-relevant dynamics knowledge. The other works view videos as a form of referential demonstration. These works pre-train an intrinsic reward function gauging task progress [43, 92, 12] or train an inverse dynamics model for action annotation [6, 85], thereby enabling the imitation of videos. Nonetheless, adapting such models for imitation to domain-gap downstream tasks remains challenging. From this, we question whether there is a more appropriate form of utilizing videos that harnesses the inherent dynamics knowledge and can also be easily adapted to domain-gap tasks. Intuitively, we posit that leveraging more abstract visual dynamics priors from videos holds greater promise for online adaptation. Generically collected videos are abundant in visual dynamics prior knowledge conducive to downstream tasks. For instance, human videos may exhibit continuous and purpose-driven behaviors, and robotic videos may show steady and goal-oriented posture and motion. These abstract visual dynamics priors, encapsulating common knowledge and the essence of motion, offer broad generalization capabilities, surpassing the inherent actions intended for imitation. In response to this insight, we propose Pre-trained Visual Dynamics Representations (PVDR) for efficient policy learning. In PVDR, the visual dynamics representations carrying visual dynamics prior knowledge are pre-trained via a video prediction task and online adapted to downstream tasks for policy learning. For PVDR learning, we design a Transformer-based [70] visual dynamics model employing a conditional variational autoencoder (CVAE) [64] to learn the visual dynamics representations. In the pre-training stage, the self-supervised video prediction task facilitates the latent variable of CVAE to effectively represent visual dynamics. We view the latent variables in the dynamics model as our visual dynamics representations. During inference, a batch of visual dynamics representations can be sampled from the learned prior to generate visual plans to choose from. During the online adaptation, we fine-tune the visual dynamics model with online experiences and train an action alignment module to turn the chosen plans into executable actions. For valid action alignment, we employ supervised learning with online experiences and RL with a reward signal related to the consistency between the chosen plan and the visual observations during action execution. Through this two-stage pipeline, the PVDR algorithm can learn visual dynamics representations from the pre-training videos and conduct planning-based inference in the downstream tasks, where the agent learns to turn plans into actions with feedback from the environment. Figure 1: Illustration of the pre-training video dataset and the downstream environments we used for experimental evaluations. To evaluate the proposed PVDR algorithm, we use the BAIR robot pushing dataset [19] recording the real-world robotic arm as the pre-training video dataset and test PVDR on goal-conditioned control tasks in the Meta-World [88] and RLBench [29] environments, which is illustrated in Figure 1. In the experiments, PVDR outperforms baselines, and ablation studies verify the effectiveness of the core design. Our experimental results indicate that visual dynamics representations are a suitable form to bridge domain-gap videos and downstream visual control tasks, and our online adaptation is conducive to turning visual dynamics representations into aligned actions. Thus, PVDR is an effective algorithm to potentially address the challenge of pre-trainning with videos."
https://arxiv.org/html/2411.03098v1,Local Lesion Generation is Effective for Capsule Endoscopy Image Data Augmentation in a Limited Data Setting,"Limited medical imaging datasets challenge deep learning models by increasing risks of overfitting and reduced generalization, particularly in Generative Adversarial Networks (GANs), where discriminators may overfit, leading to training divergence. This constraint also impairs classification models trained on small datasets. Generative Data Augmentation (GDA) addresses this by expanding training datasets with synthetic data, although it requires training a generative model. We propose and evaluate two local lesion generation approaches to address the challenge of augmenting small medical image datasets. The first approach employs the Poisson Image Editing algorithm, a classical image processing technique, to create realistic image composites that outperform current state-of-the-art methods. The second approach introduces a novel generative method, leveraging a fine-tuned Image Inpainting GAN to synthesize realistic lesions within specified regions of real training images. A comprehensive comparison of the two proposed methods demonstrates that effective local lesion generation in a data-constrained setting allows for reaching new state-of-the-art results in capsule endoscopy lesion classification. Combination of our techniques achieves a macro F1-score of 33.07%, surpassing the previous best result by 7.84 percentage points (p.p.) on the highly imbalanced Kvasir Capsule Dataset, a benchmark for capsule endoscopy. To the best of our knowledge, this work is the first to apply a fine-tuned Image Inpainting GAN for GDA in medical imaging, demonstrating that an image-conditional GAN can be adapted effectively to limited datasets to generate high-quality examples, facilitating effective data augmentation. Additionally, we show that combining this GAN-based approach with classical image processing techniques further enhances the results.","The classification of medical data using machine learning models presents a significant challenge [48, 23, 9] due to limited data availability [23, 49] and class imbalance [23, 106, 87], which often arises from natural disparities in pathology prevalence within a population. Gastrointestinal tract cancers, which account for 63% of cancer-related deaths and cause 2.2 million fatalities annually, underscore the critical need for accurate diagnostic tools [14]. Video capsule endoscopy (VCE) offers an alternative to traditional gastroscopy and colonoscopy, serving as a gold standard for diagnosing small bowel lesions [32, 24]. Unlike conventional endoscopy, VCE can visualize parts of the small intestine inaccessible through other methods, making it particularly useful for identifying occult bleeding and small mucosal lesions that standard imaging techniques might miss [69, 74]. Capsule variants, such as PillCam Colon, also allow for large intestine examination [2]. Additionally, VCE is less invasive than traditional methods and demonstrates superior sensitivity in detecting small bowel lesions compared to push enteroscopy [5, 2, 32, 24]. However, it does have limitations: VCE does not permit biopsy collection [24], and determining the exact location of a lesion remains challenging despite advancements in the field [84]. A major challenge in VCE is the sheer volume of data. The capsule moves through the gastrointestinal (GI) tract via peristalsis, generating lengthy videos with over 50,000 frames [102]. Manually analyzing such footage can take up to two hours [24, 102] of a healthcare professional time, which increases the likelihood of errors due to lapses in concentration or insufficient expertise [110, 21, 77]. An effective machine learning-based lesion classification method could accelerate this process, leading to faster diagnoses, earlier cancer detection, reduced mortality rates, and greater capacity for VCE examinations [79, 102]. Capsule endoscopy datasets are prone to issues commonly seen in medical imaging, such as data scarcity and class imbalance [79, 103]. The Kvasir Capsule dataset [79], the largest publicly available dataset for capsule endoscopy, suffers from a lack of diverse lesion representations, with some pathological classes containing images from only two or three patients. Lesions in the GI tract typically present as fine-grained, localized changes in tissue, often appearing in only one or two frames per video, and exhibit significant variation in color, shape, and size [102]. The arbitrary orientation of these images further complicates classification tasks, where even state-of-the-art methods achieve only a 25.23% macro F1-score in multiclass classification on the official data split. These models predominantly excel at identifying normal samples but misclassify most pathological cases. Over the past four decades, numerous methods have been developed to address issues related to low data availability and class imbalance. These include sampling techniques [34, 46, 85, 18, 26], transfer learning [68, 82], and data augmentation strategies [93, 51], particularly Generative Data Augmentation (GDA) [109, 101, 19, 8, 20]. Training effective models in a limited data regime is complicated. The most common use case for GDA is de novo generation using deep generative modes like GANs, VAEs, and diffusion models [12, 15, 86]. These models learn the data distribution from a training set and generate synthetic labeled examples to augment the original dataset. Empirical [101, 19, 8, 20] and theoretical [109] evidence shows that GDA improves model generalization, especially when overfitting occurs. Image inpainting [25, 10, 17, 54, 81] and image composition [62, 70] have also been employed for data augmentation. Image inpainting, dominated by GANs [81, 108, 54, 104, 61] and diffusion models [56, 83], modifies parts of an image by generating plausible content, whereas image composition combines elements from different images using techniques like Poisson Image Editing, known also as Poisson Blending [70]. However, most inpainting research focuses on removing objects, which contrasts with our need to generate lesions. Both methods have shown promise in augmenting data for image recognition tasks [93, 51, 38, 94, 83]. In this work, we introduce two data augmentation methods — one based on image composition, the other on image inpainting — that modify parts of healthy tissue images to generate synthetic lesions. The first method employs Poisson Blending Data Augmentation (PBDA) to realistically combine healthy and pathological tissue, while the second, Image Inpainting Data Augmentation (IIDA), introduces a novel generative approach. IIDA fine-tunes an image inpainting GAN to insert lesions into healthy tissue. Both techniques leverage the abundance of healthy tissue images in medical datasets to produce high-quality synthetic data, effectively augmenting small medical imaging datasets. By addressing the challenges of GAN and classifier overfitting, these approaches enhance data quality and model performance. Notably, when synthetic data created with both techniques is combined to enrich the training dataset for the classifier, the model achieved a new state-of-the-art performance on the Kvasir Capsule Dataset, with a 33.07% macro F1-score, surpassing the previous best result by 7.84 p.p. The contributions of this paper can be summarised as follows: • We propose IIDA, a novel data augmentation approach using a fine-tuned image inpainting model, particularly effective in low-data settings. This new approach leverages the abundance of healthy tissue, common in many medical imaging domains, to generate lesions. • Our qualitative and quantitative results demonstrate the superiority of the generated synthetic samples in improving classification performance. • We compare Poisson Blending, a classical image processing method, with deep generative models for augmenting local pathological changes, showing that both techniques can enhance machine learning models. IIDA shows better performance than PBDA as a standalone data augmentation technique, however their combination provides the best results. • We demonstrate that our local lesion generation methods outperform modern generative models, such as NVAE [88] and LDM [76], in data augmentation for classification tasks. The rest of the paper is organized as follows: Section 2 reviews the related literature. Section 3 introduces our two proposed data augmentation pipelines. In Section 4 we described the dataset and performed an extensive qualitative and quantitative study of proposed data augmentation techniques. Results and methods are discussed in Section 5 and conclusions are drawn in Section 6."
https://arxiv.org/html/2411.03086v1,HFGaussian: Learning Generalizable Gaussian Human with Integrated Human Features,"Recent advancements in radiance field rendering show promising results in 3D scene representation, where Gaussian splatting-based techniques emerge as state-of-the-art due to their quality and efficiency. Gaussian splatting is widely used for various applications, including 3D human representation. However, previous 3D Gaussian splatting methods either use parametric body models as additional information or fail to provide any underlying structure, like human biomechanical features, which are essential for different applications. In this paper, we present a novel approach called HFGaussian that can estimate novel views and human features, such as the 3D skeleton, 3D key points, and dense pose, from sparse input images in real time at 25 FPS. The proposed method leverages generalizable Gaussian splatting technique to represent the human subject and its associated features, enabling efficient and generalizable reconstruction. By incorporating a pose regression network and the feature splatting technique with Gaussian splatting, HFGaussian demonstrates improved capabilities over existing 3D human methods, showcasing the potential of 3D human representations with integrated biomechanics. We thoroughly evaluate our HFGaussian method against the latest state-of-the-art techniques in human Gaussian splatting and pose estimation, demonstrating its real-time, state-of-the-art performance.","Generating virtual photorealistic 3D human avatars is a long-standing challenge in the field of computer vision [1, 23]. These 3D models have diverse applications in fields such as augmented reality, virtual reality [32], entertainment, and the medical domain [53, 6]. The task of reconstructing a complete 3D human model with integrated structural properties [5, 76] in real time from images alone presents significant challenges. Classical approaches rely on complex multiview capture systems and body markers [48, 24] to obtain 3D models of humans, incorporating structural properties such as 3D pose involve fitting parametric body models such as SMPL [34] and STAR [43]. However, these methods require substantial resources and computational effort to generate each 3D model. In recent years, radiance field rendering becomes significantly popular [55, 63] for the scene representation capabilities. More recently, 3D Gaussian splatting (3DGS) [29] provides a new research direction and demonstrates notable improvements compared to neural rendering-based methods. 3D Gaussian splatting proposes a novel explicit representation that represents the scene using a set of 3D Gaussians for point-based rendering. The efficient representation of Gaussian splatting makes it particularly well-suited for real-time rendering applications. Subsequent researches apply 3DGS to various applications [37, 10, 62] including 3D human reconstructions [73, 22]. The existing methods employing 3D Gaussian Splatting (3DGS) for human avatar reconstruction either rely on parametric body models or fail to incorporate any underlying biomechanical features crucial for downstream applications [65, 20]. In this work, we propose a novel, generalizable approach for estimating a 3D human representation with integrated 3D pose and dense pose in real time, given sparse input images of the human subject. The proposed method, named Human Feature Gaussian (HFGaussian), uses Gaussian splatting to represent the human subject and its associated biomechanical features111In this study, ”biomechanical features” refer to components of the human musculoskeletal system, such as bones, muscles, ligaments, and joint locations, which are critical for human movement and function., which include the 3D skeleton, 3D keypoints, and dense pose. These biomechanical properties are essential for recreating natural human movements and interactions in the virtual world [15, 27]. One straightforward approach to representing human features while maintaining real-time rendering speed, is to directly parameterize the 3D Gaussian with additional human features. However, we point out that simply parameterizing the 3D Gaussian with these human features results in sub-optimal performance, as the same parameters like opacity, scaling, and rotation factor are not suitable for different human features. Instead of directly parameterizing the 3D Gaussian with human features, inspired by feature splatting [36], we learn these human features by optimizing additional feature parameters for each 3D Gaussian, which are then decoded into human features after rendering. Regarding 3D pose estimation, we find that even a subset of 3D Gaussians can serve as an effective point cloud for 3D pose estimation using a novel pose regression network based on DGCNN [60] and PointNet [50]. In conclusion, we propose HFGaussian, a human-centric Gaussian framework that enables real-time representation of human features through 3D Gaussians. Using GPS-Gaussian [73] as the backbone, HFGaussian can generalize to unseen human data without any fine-tuning. Building on this foundation, we additionally introduce feature splatting [36] to overcome the performance constraints of using the same set of Gaussians for various human features which may have different frequencies. Furthermore, HFGaussian employs a novel pose regression network to estimate the 3D pose from a subset of the 3D Gaussians, ensuring efficient estimation. HFGaussian is capable of simultaneously rendering novel poses, corresponding 3D poses, and human features in real time. Although this study focuses on human pose estimation, we believe that the HFGaussian can be extended to include other human features, such as body part segmentation. To evaluate our proposed method, we train our method in a large amount of human data generated from human scans and evaluate in real-world data. To the best of our knowledge, this is the first method to estimate 3D humans with biomechanics features and 3D pose in real time directly from images. The contributions of this work can be summarized as follows: ∙∙\bullet∙ We present a novel generalizable approach named HFGaussian that is capable of estimating human features and 3D human pose. ∙∙\bullet∙ The proposed method has demonstrated its ability to estimate 3D pose using a novel pose regression network and human features using feature splatting. ∙∙\bullet∙ We propose a generalizable approach to predicting human features, 3D pose, photometric, and geometric representations from 2D sparse images in real time. ∙∙\bullet∙ Our extensive experimental analysis across 3 datasets validates the applicability and versatility of our method."
https://arxiv.org/html/2411.03082v1,Self-supervised cross-modality learning for uncertainty-aware object detection and recognition in applications which lack pre-labelled training data,"This paper shows how an uncertainty-aware, deep neural network can be trained to detect, recognise and localise objects in 2D RGB images, in applications lacking annotated training datasets. We propose a self-supervising “teacher-student” pipeline, in which a relatively simple “teacher” classifier, trained with only a few labelled 2D thumbnails, automatically processes a larger body of unlabelled RGB-D data to teach a “student” network based on a modified YOLOv3 architecture. Firstly, 3D object detection with back projection is used to automatically extract and “teach” 2D detection and localisation information to the student network. Secondly, a weakly supervised 2D thumbnail classifier, with minimal training on a small number of hand-labelled images, is used to teach object category recognition. Thirdly, we use a Gaussian Process (GP) to encode and teach a robust uncertainty estimation functionality, so that the student can output confidence scores with each categorization. The resulting student significantly outperforms the same YOLO architecture trained directly on the same amount of labelled data. Our GP-based approach yields robust and meaningful uncertainty estimations for complex industrial object classifications. The end-to-end network is also capable of real-time processing, needed for robotics applications. Our method can be applied to many important industrial tasks, where labelled data-sets are typically unavailable. In this paper, we demonstrate an example of detection, localisation, and object category recognition of nuclear mixed-waste materials in highly cluttered and unstructured scenes. This is critical for robotic sorting and handling of legacy nuclear waste, which poses complex environmental remediation challenges in many nuclearised nations.","I-A Motivation This paper addresses the computer vision problems of detecting, recognising and localising objects. Our proposed method has broad potential to be used for many applications and object types. It is especially useful for industrial or applied problems, where large amounts of application-specific annotated training data are typically unavailable. We demonstrate such an application with a motivating example of robotics challenges in extreme environments, for example, robotic sorting of nuclear waste objects and materials, for the safe remediation of legacy nuclear facilities [1]. The UK alone contains an estimated 4.9 million tonnes of legacy nuclear waste [2], much of it dating back many decades. Waste items can comprise numerous objects, e.g. contaminated gloves, respirators, swabs, tools, containers, and pipework sections. At the Sellafield site (dating back to the 1940s), a new plant is being built which will use robot arms for the next 50 years. These will cut open old containers, for which there is some uncertainty about the contents. The robots must sort and identify waste items, separate them according to the estimated hazard level, and repackage them into safer modern containers. In addition to the potential for computer vision to help guide robots during e.g. pick and place operations, there is also a need to create inventory lists for the contents of the new containers. Since the quantities of waste are extremely large, automating such inventory generation will be necessary. It is also an essential requirement to estimate and document the uncertainty associated with the inventory for each storage container. This, and many other real-world industrial problems, pose particular challenges for modern computer vision approaches. Large, annotated, and ground-truthed data-sets are generally unavailable and may be prohibitively difficult, slow, or expensive to create. For example, it has been estimated that labelling the benchmark ImageNet dataset [3], with 14 million images, took approximately 22 human years of effort. Meanwhile, the objects and materials in industrial (or domestic) waste-handling problems are extremely diverse and unstructured, often appearing in arbitrary random heaps. For example, a contaminated rubber glove can appear in numerous different shapes and configurations. To incorporate such a perception system with robots, e.g. for autonomous grasping, relatively fast processing speeds are needed. Furthermore, for optimal robotic action planning [4, 5], the system needs to make explicit use of representations of uncertainty. I-B Background In recent years, modern computing hardware has enabled rapid advances in computer vision recognition tasks, via deep neural network structures. However, these methods are predominantly based on extensive supervised learning, depending on very large training data-sets, in which each image must be laboriously hand-labelled with ground-truth information. As a result, much of the deep learning computer vision literature is demonstrated on open-source benchmark data-sets. Many of these benchmark data-sets feature domestic objects, e.g. furniture, kitchen utensils etc., which do not readily transfer to practical industrial problems. In addition to the labour-intensive nature of collecting and hand-labelling data, such human labour can be prone to error. Sometimes an object may not be accurately bounded by bounding boxes or may be assigned a wrong class label. In some cases, it is difficult for a human annotator to categorize some images [6]. Many objects, e.g. a cat with a long tail, or a frying pan with a long handle, do not neatly fit within a bounding box. It is not clear what the correct definition of a bounding box should be, since a complete bounding box will contain large areas of non-object background pixels. Conversely, a box that is tightly fitted to the body of the cat or the frying pan, will omit key parts and features of these objects (the tail or handle). An uncertainty-aware approach to image-based learning, is valuable for such problems. State-of-the-art CNN based object detectors such as Mask Region-based convolutional neural network (Mask R-CNN) [7], Fast Region-based convolutional neural network (Fast R-CNN) [8] and Single Shot MultiBox Detector (SSD) [9] have demonstrated impressive object detection capabilities. However, most of these models are unable to estimate the uncertainty accompanying each detection or classification. More recently, object detection YOLOv3[10] network does assign a confidence estimate alongside its output detections. However, this capability is conventionally trained by inputting confidences that are derived from a simplistic calculation (essentially defining “confidence” as the proportion of the network’s output box which overlaps with the ground-truth bounding box). Figure 1 shows an image from our dataset [11], with detection results from a conventional YOLOv3 [10] network. It can be seen that this model was not successful in recognising the object categories accurately, with most of the objects labelled as “bottle”. Furthermore, the confidence scores assigned to each detection are questionable, e.g. a “plastic-pipe” is detected object as “bottle” with a high confidence score of 0.63. Figure 1: Detections on example image from our nuclear waste test dataset using the standard version of YOLOv3. Note how the conventional YOLOv3 can assign overly high confidence numbers to incorrect classifications. In this work, we have chosen to use YOLOv3 [12] over its more recent variants for the following reasons. You Only Look Once (YOLOv3) is a fast object detector that integrates the feature pyramids network and achieves a good balance between detection accuracy and detection speed, making it one of the most popular methods in this field. Redmon and Farhadi [10] proposed a balanced and optimised algorithm regarding the speed and accuracy of object detection. Later variants of YOLOv3 has been developed such as v4,v5,v6 and v7 [13]. New variants have developed an efficient backbone and a more understandable label assignment strategy and have minimal to no impact on calculation overhead. Despite that, YOLOv3 is still providing the base network to these variants. It is still very popular in the research community as it provides a simple implementation and deployment structure [14]. Ge et al. [15] articulate this perspective by stating that, while YOLOv4 and YOLOv5 have indeed made significant strides in object detection accuracy, they may potentially grapple with issues pertaining to over-optimization. The YOLOv3 algorithm is a popular choice in the industry for its high detection efficiency among the YOLO family, with a broad range of applications in various domains such as human nail abnormality detection [16], pavement distress detection [17], pedestrian detection [18], tracking smart robot car [19] apple growth stage detection [20], industrial distress detection [21], and perception systems for driver-less cars [22]. The main motivation for our use of YOLOv3 is that it incorporates functionality for explicitly encoding and outputting an estimate of confidence alongside its object categorization decisions. Figure 2: The outline of the proposed method for rapidly boot-strapping a learning system, in a semi-supervised manner, requiring relatively sparse data. This is accomplished by combining Gaussian Processes and YOLOv3 in a Knowledge Distillation paradigm. , Later versions of YOLO do not possess this functionality. In our work, we modify and enhance this uncertainty-awareness functionality by using a Gaussian Process to model uncertainty in a teacher classifier. The teacher then teaches robust uncertainty estimations to our modified YOLO3 classifier during teacher-student training. I-C Approach and novel contributions We use our previous work [23] as a baseline method, which also introduced our nuclear waste objects computer vision data-set. This method successfully detected objects, and accurately assigned category labels compared to contemporary methods from the literature. However, it was computationally expensive (execution time for detection was 100ms-200ms). It sometimes made false positive detections of background regions as objects, and object category assignment could be noisy and variable. It also struggled to detect small objects or partially occluded objects in cluttered scenes. Most importantly, this system also lacked an “uncertainty-aware” functionality. In this study, we address these problems. We describe a new approach which yields more accurate detections, with less computational complexity, while adding a new functionality enabling the system to output confidence estimates to accompany each detection. Common sense suggests that a robust model, with a meaningful and useful “uncertainty-awareness” capability, should output low confidence scores whenever it outputs false-positive detections or incorrect object category labels. In contrast, as seen in Fig. 1, in our example nuclear waste application we can see that the conventional approach to training confidence estimates in YOLOv3, often results in inappropriate output confidence scores during testing. To provide an improved uncertainty-awareness capability, this study proposes the fusion of a Gaussian Process (GPC) model for classification with a YOLOv3 detector, in a “teacher-student” paradigm, enabling real-time detection accompanied by robust and useful confidence scores. In contrast to previous methods for assigning confidences (discussed above), we adopt a “teacher-student” approach (related to “knowledge distillation” methods [24]. We use the GPC as the teacher and YOLOv3 as the student. The GPC proposes confidence scores associated with object image thumbnails and teaches these confidences to the YOLOV3 network during its object category recognition training. Figure 3: Deep kernel learning architecture with Stochastic variational inference procedure. Note that previously, “knowledge distillation” has been used in a very different way. Typically a complex (and computationally expensive) strong classifier is used as the “teacher”, and trains a simpler (and cheaper) classifier which serves as the “student” [25]. I.e. a large amount of knowledge, encoded in the large and complex teacher network, is “distilled” [26] into a much smaller and computationally cheaper student network. In contrast, a key novelty of our work is that we show how a relatively simple and cheap classifier can be bootstrapped as a “teacher”, which generates inputs to a much more complex and powerful “student” classifier during its training. The resulting strong classifier (student) then outperforms its teacher and also outperforms the same network structure when trained in a conventional way, without the teacher, on the same data-set. First, we use a 3D-detector from our previous work [23] to generate objectness proposals from RGB-D video streams, and generate corresponding 2D object thumbnails from the RGB-D data. We manually label a small number of these thumbnails. Some are retained for testing, and a few are used as a training input to a“weakly supervised” system. The system then bootstraps on this small input data, becoming “self-supervised’. I.e. based on this small labelled data, our system effectively creates and labels more training data, while training itself by using the teacher-student paradigm. We train the classic pre-trained Resnet-50v2 on this small labelled dataset, by using transfer learning [27]. Then we augment this Resnet network with a Gaussian Process (GP) model to provide a sophisticated functionality for learning uncertainty-awareness. The resulting “teacher” then generates a much larger scale of automatically labelled, or “self-labelled” data as inputs to the training of the YOLOV3 network. Meanwhile, the GP component of the teacher is used to provide input to the uncertainty-awareness learning component of our modified YOLOv3 network (in contrast to the more simplistic uncertainty learning approach of the original YOLOv3 as discussed above). The knowledge of the “teacher” network is thus “distilled” into a YOLOv3 “student” network, using the variation of loss for classification. This variation of loss is composed of knowledge distillation loss and the sum of squared loss. This technique improves the classification loss compared to the original YOLOV3 object detector method. The resulting network, informed by the GP component of the teacher during training, also generates significantly improved confidence/uncertainty values for each classification, compared to the original YOLOv3. The main contributions of this paper are as follows: Figure 4: Schematic of the Knowledge distillation pipeline for categorization. a) The transfer of knowledge from the teacher backbone, as shown in Figure 3, to the student backbone utilizing the YOLOv3 architecture. (b) Illustration of the YOLOv3 output structure, where bounding box coordinates are generated by a 3D detector, defining the spatial location and size of each detected object within the 3D space. The objectness score indicates the confidence level that the bounding box contains an object. The final part of the output comprises probabilistic class scores, which provide a probabilistic distribution over possible classes, thereby incorporating uncertainty in the classification process. 1. A self-supervised 2D objectness detection, trained by automatically extracting and labelling 2D RGB object thumbnails from 3D RGB-D data. We use 3D conditional clustering within the point clouds to automate the extraction and labelling of bounding boxes without human effort. This automatically generates 2D object bounding box annotations as inputs for training the YOLOV3 network, hence our term “self-supervised” learning. 2. A novel use of “teacher-student” and “knowledge distillation” concepts, to enable boot-strapping a weak classifier (based on a small amount of annotated training data) to train a more complex and strong classifier (by automatically generating and feeding it training examples). Not that this is significantly novel in contrast to conventional knowledge distillation methods. Such methods use complex, strong classifiers, to teach effective classification capabilities to a smaller, simpler classifier (e.g. for implementations on small processors). In contrast, we show how to invert this concept, using a weakly trained classifier to automatically generate large amounts of training data for teaching a larger and more complex student classifier, which eventually outperforms the teacher. 3. We propose a new way to enable a classifier network to learn uncertainty-awareness, i.e. the ability to output a confidence value alongside each object detection and classification decision. In contrast to the conventional YOLOv3 approach, by using items 1) and 2) we enable self-supervised training of a Gaussian Process Classifier (GPC) as part of the “teacher” in our teacher-student paradigm. The purpose of the GPC is to teach confidence/uncertainty scores to the YOLO student network, alongside its learning of objectness detection and object category values during teacher-student training. This yields significantly better quality confidence outputs than conventional approaches to YOLOV3 confidence training, in our example industrial waste objects application. 4. We redesign the loss function of classic knowledge distillation, which works more effectively with our waste object data-set and achieves SOTA performance, while reducing computational complexity. 5. Our semi-supervised and self-supervised methods can be readily applied to new industrial applications, where no large ground-truthed or annotated training data-sets exist. We demonstrate this capability by using our unique nuclear waste objects data-set, motivated by the robotics and AI challenges of environmental clean-up and remediation on legacy nuclear sites in hazardous environments,"
https://arxiv.org/html/2411.03053v1,Gradient-Guided Conditional Diffusion Models for Private Image Reconstruction: Analyzing Adversarial Impacts of Differential Privacy and Denoising,"We investigate the construction of gradient-guided conditional diffusion models for reconstructing private images, focusing on the adversarial interplay between differential privacy noise and the denoising capabilities of diffusion models. While current gradient-based reconstruction methods struggle with high-resolution images due to computational complexity and prior knowledge requirements, we propose two novel methods that require minimal modifications to the diffusion model’s generation process and eliminate the need for prior knowledge. Our approach leverages the strong image generation capabilities of diffusion models to reconstruct private images starting from randomly generated noise, even when a small amount of differentially private noise has been added to the gradients. We also conduct a comprehensive theoretical analysis of the impact of differential privacy noise on the quality of reconstructed images, revealing the relationship among noise magnitude, the architecture of attacked models, and the attacker’s reconstruction capability. Additionally, extensive experiments validate the effectiveness of our proposed methods and the accuracy of our theoretical findings, suggesting new directions for privacy risk auditing using conditional diffusion models.","In recent years, the fields of machine learning and federated learning have garnered significant attention due to their ability to harness large-scale data for various applications. However, large-scale data often carries private information, such as faces, genders, and so on. One critical concern that has emerged is the privacy of the training data. Gradients, a fundamental component in the optimization process of machine learning models, typically shared among different parties in federated learning, inherently contain private information about the training data. Data reconstruction attacks Zhu et al. (2019); Jeon et al. (2021a); Fang et al. (2023a), in particular, aim to reconstruct the original training data from the gradients, posing a severe threat to data privacy. When it comes to private image reconstruction, attackers are highly motivated to illicitly acquire private images, especially high-resolution images, due to commercial interests or other reasons, as there are many situations in which high-resolution images are needed. For instance, in fields like radiology, high-resolution images can make the difference between missing and identifying critical patient conditions. Detailed images help in diagnosing diseases, planning treatments, and monitoring patient progress. These images are costly to access and always contain private information. Existing gradient-based image reconstruction attacks are only effective for low-resolution images, with poor performance or very slow reconstruction speeds for high-resolution images. This is because current methods require repeated differentiation of randomly initialized images Zhu et al. (2019); Zhao et al. (2020), and as the dimensionality of these images increases, the differentiation becomes more difficult, leading to greater reconstruction errors. Reconstruction methods based on Generative Adversarial Networks (GANs) Jeon et al. (2021a); Fang et al. (2023a) or fine-tuned diffusion models Meng et al. (2024); Xue et al. (2024) introduce additional computational complexity and require certain prior conditions. For example, the distribution of the initial images must match that of the private images. Diffusion models Sohl-Dickstein et al. (2015); Ho et al. (2020); Song and Ermon (2019); Song et al. (2020b), inspired by the physical process of diffusion, iteratively add noise to data, break up the original data into pure Gaussian noise, and learn to reverse this process by predicting the added Gaussian noise, thereby generating new data from a Gaussian random vector by denoising the added Gaussian noise step by step. Conditional diffusion models Dhariwal and Nichol (2021); Ho and Salimans (2022); Chung et al. (2022c, b); Zhu et al. (2023); Bansal et al. (2023); Yu et al. (2023); He et al. (2023); Yang et al. (2024) extend this approach by conditioning the generation process on additional information, allowing for more controlled and targeted data generation. Due to the excellent image-generating ability of diffusion models, recent work Meng et al. (2024) has demonstrated that private gradients can serve as the guidance for fine-tuning pre-trained diffusion models to generate private images and the adversary can obtain a gradient-based fine-tuned diffusion model to conduct an effective image reconstruction attack. To mitigate the privacy risks associated with gradient leakage, one effective method is to add differentially private noise Dwork et al. (2014) to the original gradients such that the adversary could not infer private information from these noisy gradients Abadi et al. (2016); Zhu et al. (2019); Meng et al. (2024); Wang et al. (2022); El Mestari et al. (2024). Differential privacy Dwork et al. (2014) ensures that the added noise masks the private information, thereby reducing the risk of data reconstruction attacks. Based on the noising ability of differential privacy and the denoising ability of diffusion models, we are interested in the following issues: Given the capability of conditional diffusion models to generate high-resolution images, is it possible for an attacker to incorporate stolen or inadvertently leaked gradients into the conditional diffusion model as a guiding condition, thereby reconstructing the original image? Current reconstruction methods require certain priors. Could this prerequisite be eliminated in a conditional diffusion model without compromising the reconstruction quality? In other words, can a conditional diffusion model, starting from a randomly generated noise, utilize stolen or leaked gradients to reconstruct the original private image? Could diffusion models’ denoising ability contribute to the data reconstruction attack even if the adversary captures the noisy gradients with injected differentially private noise? By analyzing the interplay between the noise introduced by differential privacy and the denoising ability of diffusion models, we can better understand their adversarial impacts. In this paper, we delve into how to establish a gradient-guided conditional diffusion model to reconstruct private images and the adversarial impacts between differential privacy’s noising mechanism and the denoising ability of diffusion models. We aim to provide a comprehensive analysis of how these two factors interact and influence the overall privacy of private data. Through this investigation, we seek to advance the understanding of differential privacy-preserving techniques in the context of protecting private gradients. In detail, we first discuss how the attacker obtains conditional diffusion models based on gradients to reconstruct private images if they capture the original or differentially private gradients. Then, we study the adversarial impacts of differential privacy’s noising mechanism and the denoising ability of diffusion models by exploring how the differentially private noise scale influences the Jensen gap between the reconstructed and private images. This Jensen gap is highly correlated to the reconstruction ability of conditional diffusion models. In summary, our contributions are: • We propose two methods for constructing conditional diffusion models using leaked or stolen gradients to reconstruct private images. These methods require only minimal modifications to a few steps in the diffusion model’s generation process to reconstruct high-quality private images. Moreover, these methods do not require the attacker to have prior knowledge; as long as gradient information is captured, the attacker can reconstruct the private image starting from the initial noise image. • We conduct a detailed theoretical analysis of these methods, illustrating the impact of differential privacy noise magnitude and the type of attacked model on the quality of reconstructed private images. The analysis reveals the relationship among the denoising process of diffusion models, the magnitude of differential privacy noise, and the attacker’s reconstruction capability. It also explains why different attacked models exhibit varying levels of vulnerability to the same reconstruction attack. • We conduct extensive experiments to demonstrate the excellent capability of conditional diffusion models in data reconstruction. Our experiments also validate the correctness of the theoretical analysis, suggesting new research directions for privacy risk auditing using conditional diffusion models."
https://arxiv.org/html/2411.03047v1,GarVerseLOD: High-Fidelity 3D Garment Reconstruction from a Single In-the-Wild Image using a Dataset with Levels of Details,"Neural implicit functions have brought impressive advances to the state-of-the-art of clothed human digitization from multiple or even single images. However, despite the progress, current arts still have difficulty generalizing to unseen images with complex cloth deformation and body poses. In this work, we present GarVerseLOD, a new dataset and framework that paves the way to achieving unprecedented robustness in high-fidelity 3D garment reconstruction from a single unconstrained image. Inspired by the recent success of large generative models, we believe that one key to addressing the generalization challenge lies in the quantity and quality of 3D garment data. Towards this end, GarVerseLOD collects 6,000 high-quality cloth models with fine-grained geometry details manually created by professional artists. In addition to the scale of training data, we observe that having disentangled granularities of geometry can play an important role in boosting the generalization capability and inference accuracy of the learned model. We hence craft GarVerseLOD as a hierarchical dataset with levels of details (LOD), spanning from detail-free stylized shape to pose-blended garment with pixel-aligned details. This allows us to make this highly under-constrained problem tractable by factorizing the inference into easier tasks, each narrowed down with smaller searching space. To ensure GarVerseLOD can generalize well to in-the-wild images, we propose a novel labeling paradigm based on conditional diffusion models to generate extensive paired images for each garment model with high photorealism. We evaluate our method on a massive amount of in-the-wild images. Experimental results demonstrate that GarVerseLOD can generate standalone garment pieces with significantly better quality than prior approaches while being robust against a large variation of pose, illumination, occlusion, and deformation. Code and dataset are available at garverselod.github.io.","High-quality 3D garment models are critical assets for a large variety of applications, ranging from entertainment to professional concerns, such as visual effects, physical simulation, and VR/AR telepresence. In the production-level pipeline, independent garment pieces are more desirable than a single clothed human model, as the former allows layered compositions with an internal body mesh to ensure the realism of physical motion and the flexibility of garment transfer. However, unlike clothed human reconstruction that can directly utilize the latest advances of neural implicit representation (Saito et al., 2019, 2020; Xiu et al., 2022), standalone garment modeling mostly relies on deforming parametric templates with open boundaries due to its strict requirement of correct topology. Nonetheless, reconstructing high-fidelity 3D garment from a single image remains a nuisance to current vision algorithms. While the high diversity of garment styles and the scarcity of the inputs render the problem highly ill-posed, the complex deformations resulted from the cloth dynamics make the inference even more challenging. There are two mainstream approaches for estimating the deformations of standalone garments from posed humans. Linear blend skinning (LBS)-based methods (Jiang et al., 2020; Corona et al., 2021) focus on predicting the deformations caused by human poses, where the learned skinning weights of the garment mesh are either bound to the skeleton or the surface vertices of a parametric model of unclothed humans (e.g., SMPL (Loper et al., 2015)). While this line of approaches can effectively represent posed-induced deformations, they struggle to model other intricate deformations caused by the environments or physical dynamics. Feature-line-based methods (Zhu et al., 2020, 2022) reconstruct garment meshes from SMPL surfaces and further fit them with garments’ manifold boundaries, making it versatile to model any type of deformations. However, the problem of boundary estimation from single images itself is challenging, due to the severe occlusions and 2D-to-3D ambiguities. Apart from the technical challenges, the other obstacle to learning-based garment reconstruction is the limited quantity and quality of 3D dataset. Due to the lack of local geometry details in existing garment datasets, current LBS-based methods are incapable of learning fine-grained geometries (e.g., wrinkles), resulting in coarse 3D garment quality. ReEF (Zhu et al., 2022) annotates the feature lines for only 400 garment models in the RenderPeople dataset (RenderPeople, 2018). The limited data scale hampers the prior approaches from generalizing to unseen images and often leads to poor reconstruction quality of feature lines (i.e., garment boundaries). In this work, we strive to address the above issues for standalone 3D garment reconstruction from the perspectives of both data and algorithm. We thereby introduce GarVerseLOD, a dedicated dataset and framework that achieves unprecedented robustness in reconstructing high-fidelity 3D garments from a single in-the-wild image (Fig. 1). To promote the quantity and quality of 3D garment data, GarVerseLOD collects 6,000 high-quality hand-crafted garment meshes with fine-grained details created by professional artists. It covers 5 most commonly seen categories – each category shares the same mesh topology, facilitating cross-instance interpolation and construction of blendshape models. While garment shapes differ globally in terms of style and topology, the local deformations are determined by a wide range of factors, including body poses, garment-environment interactions, self-collisions, etc. We, therefore, propose to craft GarVerseLOD as a hierarchical dataset with levels of details (LOD) to accommodate this key observation. In particular, as shown in Fig. 2, GarVerseLOD contains three basic levels of databases: 1) Garment Style Database with T-posed and detail-free coarse garment; 2) Local Detail Database enclosing pairs of T-posed models with and without fine-level local geometric details; and 3) Garment Deformation Database consisting of pairs of T-posed garment and its deformed counterpart (i.e., with global deformations). As the mesh topologies are identical within each category, we can easily extract the local details and global deformations from paired models in the corresponding database and combine all levels of geometries to obtain the Fine Garment Dataset. The disentangled granularities of geometry allows us to make this highly underconstrained problem tractable by factorizing the inference into smaller tasks, each can be tackled with narrowed solution space. Furthermore, we introduce a novel data labeling paradigm to generate extensive paired images for each garment model. Specifically, we leverage the latest advances in conditional diffusion model to transfer the textureless renderings to photorealistic images with diverse appearances. This further elevates the generalization capability of GarVerseLOD in handling unconstrained images. Algorithm-wise, we propose to connect the good ends of both LBS and feature-line based approaches. We first build a parametric model of the T-posed coarse shapes in the garment style database. After estimating the blendshape coefficients of the coarse garment, we progressively refine the result by adding pose-induced global deformations and fine-scale local deformations. Thanks to the LOD structure of GarVerseLOD, these three steps can be performed in a disentangled manner with eased complexity. While we employ linear blend skinning to estimate deformations caused by body poses, an implicit garment representation is learned to capture pixel-aligned fine surface from estimated 2D normal maps. We then fit the posed coarse garment with fine surfaces by aligning their open boundaries for the purpose of transferring the local details to the globally deformed mesh with correct topology. To combat with the occlusions, we present a novel geometry-aware boundary prediction strategy that equips the 2D features with 3D information from the estimated fine surface for better localization of 3D boundaries. Our experimental results show that GarVerseLOD can effectively reconstruct garments with diversified shapes and intricate deformations, demonstrating significantly better generalization ability over the prior arts. We summarize our contributions as follows: • We present the GarVerseLOD dataset, a large collection of high-fidelity 3D hand-crafted garments. It encloses 6,000 professionally hand-crafted garments, covers 5 categories, and, for the first time, contains 3 disentangled levels of details to ease the learning task. • We propose a novel data simulation pipeline to generate extensive paired images for supporting single-view reconstruction. • We devise a specially-tailored coarse-to-fine approach to fully utilize the LOD structure of the GarVerseLOD dataset. Experimental results show that our method excels in reconstructing high-quality garments from single images."
https://arxiv.org/html/2411.03033v1,Rethinking Decoders for Transformer-based Semantic Segmentation: Compression is All You Need,"State-of-the-art methods for Transformer-based semantic segmentation typically adopt Transformer decoders that are used to extract additional embeddings from image embeddings via cross-attention, refine either or both types of embeddings via self-attention, and project image embeddings onto the additional embeddings via dot-product. Despite their remarkable success, these empirical designs still lack theoretical justifications or interpretations, thus hindering potentially principled improvements. In this paper, we argue that there are fundamental connections between semantic segmentation and compression, especially between the Transformer decoders and Principal Component Analysis (PCA). From such a perspective, we derive a white-box, fully attentional DEcoder for PrIncipled semantiC segemenTation (DEPICT), with the interpretations as follows: 1) the self-attention operator refines image embeddings to construct an ideal principal subspace that aligns with the supervision and retains most information; 2) the cross-attention operator seeks to find a low-rank approximation of the refined image embeddings, which is expected to be a set of orthonormal bases of the principal subspace and corresponds to the predefined classes; 3) the dot-product operation yields compact representation for image embeddings as segmentation masks. Experiments conducted on dataset ADE20K find that DEPICT consistently outperforms its black-box counterpart, Segmenter, and it is light weight and more robust. Our code and models are available at https://github.com/QishuaiWen/DEPICT.","Semantic segmentation has been a fundamental task in computer vision for decades. In the supervised setting, the task aims to segment an image into regions corresponding to different predefined classes. The dominant approaches for semantic segmentation have experienced significant shifts, in particular, from hand-crafted features [16] to deep learning, from Convolutional Neural Networks (CNNs) [25, 5] to Vision Transformers (ViT) [47, 36], and then from per-pixel classification to mask classification [8]. Recently, state-of-the-art methods [36, 8, 7, 46] for Transformer-based semantic segmentation [22] typically adopted the Transformer decoders inspired by DETR [2]. Although they vary among different methods, the Transformer decoders typically consist of cross-attention operators that extract additional embeddings (known as class embeddings [36, 46] or mask embeddings [8, 7]) from image embeddings, self-attention operators that refine either or both the additional embeddings and image embeddings, layer normalization (LN) [21] and feedforward neural networks (FFN), which are the default compositions of a Transformer block [38], and one (or two for mask classification) dot-product operation of the two types of embeddings. Here, we illustrate in Figure 1 two representative methods, i.e., Segmenter [36] and MaskFormer [8]. Figure 1: Illustration for Segmenter and MaskFormer. a) Segmenter. b) MaskFormer. c) Transformer block adopted by Segmenter. We omit the details of the Transformer decoder adopted by MaskFormer, which refines image embeddings and mask embeddings via self-attention respectively before the cross-attention operations. Despite the empirical designs of the Transformer decoders being intuitive and having achieved remarkable success [7, 20], there still lack theoretical justifications or interpretations, thus hindering potentially principled improvement (such as identifying and addressing performance bottlenecks). We believe that the first step toward white-box models for Transformer-based semantic segmentation is to answer the following questions: 1) Why do the Transformer decoders outperform a position-wise MultiLayer Perceptron (MLP) that independently classifies each image embedding [36]? 2) What is the underlying mechanism of the self- and cross-attention operators adopted by the Transformer decoders? 3) More importantly, is there a principle for designing and improving the Transformer decoders? In this paper, we argue that there are fundamental connections between semantic segmentation and compression, especially between the Transformer decoders and Principal Component Analysis (PCA), and that the principle of compression is all we need to derive a white-box decoder akin to the Transformer decoder. To be specific, we extend the objectives of PCA in the geometric and rank minimization views [39] to the context of the coding rate [11, 26, 44] and, following the derivation in CRATE [43], we derive self- and cross-attention operators for semantic segmentation by unrolling the optimization of these objectives. Contributions. The contributions of the paper are highlighted as follows. 1. We take a further step along the fundamental connections between semantic segmentation and compression, by introducing PCA for understanding the empirical designs of decoders for Transformer-based semantic segmentation. 2. We extend the objectives as well as the idea of PCA in terms of the coding rate to unrolled optimization, and thus derive a family of white-box fully attentional DEcoder for PrIncipled semantiC segmenTation (DEPICT). 3. We conduct extensive experiments to evaluate the performance of our DEPICT and find that our DEPICT consistently outperforms its black-box counterpart and shows desirable properties suggested by the derivation."
https://arxiv.org/html/2411.03013v1,"CRT-Fusion: Camera, Radar, Temporal Fusion Using Motion Information for 3D Object Detection","Accurate and robust 3D object detection is a critical component in autonomous vehicles and robotics. While recent radar-camera fusion methods have made significant progress by fusing information in the bird’s-eye view (BEV) representation, they often struggle to effectively capture the motion of dynamic objects, leading to limited performance in real-world scenarios. In this paper, we introduce CRT-Fusion, a novel framework that integrates temporal information into radar-camera fusion to address this challenge. Our approach comprises three key modules: Multi-View Fusion (MVF), Motion Feature Estimator (MFE), and Motion Guided Temporal Fusion (MGTF). The MVF module fuses radar and image features within both the camera view and bird’s-eye view, thereby generating a more precise unified BEV representation. The MFE module conducts two simultaneous tasks: estimation of pixel-wise velocity information and BEV segmentation. Based on the velocity and the occupancy score map obtained from the MFE module, the MGTF module aligns and fuses feature maps across multiple timestamps in a recurrent manner. By considering the motion of dynamic objects, CRT-Fusion can produce robust BEV feature maps, thereby improving detection accuracy and robustness. Extensive evaluations on the challenging nuScenes dataset demonstrate that CRT-Fusion achieves state-of-the-art performance for radar-camera-based 3D object detection. Our approach outperforms the previous best method in terms of NDS by +1.7%percent1.7\mathbf{+1.7\%}+ bold_1.7 %, while also surpassing the leading approach in mAP by +1.4%percent1.4\mathbf{+1.4\%}+ bold_1.4 %. These significant improvements in both metrics showcase the effectiveness of our proposed fusion strategy in enhancing the reliability and accuracy of 3D object detection.","3D object detection plays a crucial role in autonomous vehicles and robotics, leveraging sensors such as lidar, cameras, and radar to localize and classify objects in the environment. Extensive research has been conducted to explore various strategies for improving detection accuracy and robustness. One prominent approach is the integration of data across multiple timestamps, which aims to mitigate the inherent limitations associated with relying solely on instantaneous data. By incorporating historical information, this approach provides a more robust perception of the environment, addressing the challenges of incomplete data caused by occlusions, sensor failures, and other factors. Numerous studies have investigated the utilization of temporal information to enhance the performance of LiDAR-based and camera-based 3D object detection methods. Recent works have also explored the incorporation of temporal cues in radar-camera fusion methods [1, 2]. These methods generated bird’s-eye view (BEV) feature maps for each frame by fusing radar and camera data into a unified BEV representation. The resulting BEV feature maps are then concatenated across frames to create a comprehensive spatio-temporal representation, as illustrated in Figure 1(a). However, these approaches face limitations in effectively capturing object motion, as they merge data from different time intervals without explicitly considering the dynamics of moving objects. Consequently, the performance accuracy for dynamic objects is compromised. Figure 1: Comparison of temporal fusion methods: (a) Previous methods concatenate BEV feature maps without considering object motion. (b) CRT-Fusion estimates and compensates for object motion before concatenation. (c) Performance gain of CRT-Fusion over the direct concatenation method, showing CRT-Fusion’s superior accuracy across different object velocity ranges. To address these challenges, we propose a motion-aware approach, as illustrated in Figure 1 (b), which goes beyond simple concatenation of BEV feature maps. Our method first estimates the locations of dynamic objects with their corresponding velocity vector for each timestamped BEV feature map. Subsequently, we leverage this predicted information to rectify the motion of dynamic objects in each feature map and fuse them in a temporally consistent manner. Figure 1(c) presents a graph depicting the performance gain achieved by our proposed method over the direct concatenation of temporal BEV feature maps for different object velocity ranges. It is evident that our approach consistently outperforms the existing method across all velocity ranges, with a notable performance improvement for objects moving at medium velocities. This demonstrates the effectiveness of our motion-aware fusion strategy in capturing and compensating for object motion, leading to superior performance in 3D object detection. In this paper, we introduce CRT-Fusion, a novel approach for integrating temporal information into radar-camera fusion. Our framework comprises three modules: Multi-View Fusion (MVF), Motion Feature Estimator (MFE), and Motion Guided Temporal Fusion (MGTF). The MVF module generates radar-camera fused BEV feature maps for each timestamp. The MVF enhances image features with radar BEV features, achieving more precise depth predictions through Radar-Camera Azimuth Attention (RCA). The enhanced camera BEV features and radar BEV features are integrated through a gating operation. The MFE module predicts velocity information and performs BEV segmentation for each pixel in the fused BEV features to identify object regions and provide values for shifting the feature map spatially. Finally, the MGTF module generates the final feature map by leveraging the fused BEV feature maps, segmentation results, and velocity predictions. The MGTF module begins with the BEV features from the (t−N)𝑡𝑁(t-N)( italic_t - italic_N )th time step and aligns them with those from each subsequent time step. These aligned features are then aggregated one-by-one across all N𝑁Nitalic_N timestamps in a recurrent manner. Consequently, CRT-Fusion achieves state-of-the-art performance on the nuScenes 3D object detection benchmark for radar-camera fusion methods, with improvements of +1.7%percent1.7\mathbf{+1.7\%}+ bold_1.7 % in NDS and +1.4%percent1.4\mathbf{+1.4\%}+ bold_1.4 % in mAP compared to existing state-of-the-art approaches. In summary, the main contributions of this work are as follows: • We introduce CRT-Fusion, a novel framework that effectively integrates temporal information into radar-camera fusion for 3D object detection. By considering the motion of dynamic objects, CRT-Fusion significantly improves detection accuracy and robustness in complex real-world scenarios. • We design a Multi-View Fusion module that enhances depth prediction by leveraging radar features to improve image features before fusing them into a unified BEV representation. • We introduce an effective temporal fusion strategy through MFE and MGTF modules. MFE estimates pixel-wise velocity information, and MGTF iteratively aligns and fuses feature maps across multiple timestamps using the motion information obtained from MFE. • CRT-Fusion achieves state-of-the-art performance on the nuScenes dataset for radar-camera-based 3D object detection, surpassing previous best method by +1.7%percent1.7\mathbf{+1.7\%}+ bold_1.7 % in NDS and +1.4%percent1.4\mathbf{+1.4\%}+ bold_1.4 % in mAP."
https://arxiv.org/html/2411.02999v1,Precise Drive with VLM: First Prize Solution forPRCV 2024 Drive LM challenge,"This technical report outlines the methodologies we applied for the PRCV Challenge, focusing on cognition and decision-making in driving scenarios. We employed InternVL-2.0, a pioneering open-source multi-modal model, and enhanced it by refining both the model input and training methodologies. For the input data, we strategically concatenated and formatted the multi-view images. It is worth mentioning that we utilized the coordinates of the original images without transformation. In terms of model training, we initially pre-trained the model on publicly available autonomous driving scenario datasets to bolster its alignment capabilities of the challenge tasks, followed by fine-tuning on the DriveLM-nuscenes Dataset. During the fine-tuning phase, we innovatively modified the loss function to enhance the model’s precision in predicting coordinate values. These approaches ensure that our model possesses advanced cognitive and decision-making capabilities in driving scenarios. Consequently, our model achieved a score of 0.6064, securing the first prize on the competition’s final results.","Since the widespread adoption of ChatGPT, multimodal large language models have gained significant attention across various research fields due to their capability to process text and reason about non-textual data, such as images and videos. A substantial amount of research has also focused on applying large language models to the domain of autonomous driving. In this context, the competition aims to leverage the video comprehension abilities of large language models for cognitive and decision-making tasks in driving scenarios. By utilizing given multi-perspective images as input, the focus is on addressing questions related to perception, prediction, and planning within the autonomous driving system, ultimately assisting the vehicle in making interpretable driving decisions. This technical report will detail our experimental approach, including the format of the input data, enhancements made during the pretraining and finetuning phases, and the final experimental results."
https://arxiv.org/html/2411.02997v1,PV-faultNet: Optimized CNN Architecture to detect defects resulting efficient PV production,"The global shift towards renewable energy has pushed PV cell manufacturing as a pivotal point as they are the fundamental building block of green energy. However, the manufacturing process is complex enough to lose its purpose due to probable defects experienced during the time impacting the overall efficiency. However, at the moment, manual inspection is being conducted to detect the defects that can cause bias, leading to time and cost inefficiency. Even if automated solutions have also been proposed, most of them are resource-intensive, proving ineffective in production environments. In that context, this study presents PV-faultNet, a lightweight Convolutional Neural Network (CNN) architecture optimized for efficient and real-time defect detection in photovoltaic (PV) cells, designed to be deployable on resource-limited production devices. Addressing computational challenges in industrial PV manufacturing environments, the model includes only 2.92 million parameters, significantly reducing processing demands without sacrificing accuracy. Comprehensive data augmentation techniques were implemented to tackle data scarcity, thus enhancing model generalization and maintaining a balance between precision and recall. The proposed model achieved high performance with 91% precision, 89% recall, and a 90% F1 score, demonstrating its effectiveness for scalable quality control in PV production.","In the context of pursuing sustainable energy, solar power has been placed as the leading technology in renewable energy generation. Being abundant, solar energy plays a vital role in mitigating climate change while curbing the dependence on fossil fuels. According to Blakers [1], the total global Solar photovoltaic (PV) capacity has grown rapidly, with an annual increase of 22% and projections suggest it will reach 6 terawatts by 2031. Additionally, the global solar market is expected to reach USD 223.3 billion by 2026, driven by government incentives and decreasing PV module prices. However, ensuring the efficiency and quality of PV cells is essential for the continued sector. Maintaining the efficiency and integrity of PV (Photovoltaic) cells which are the bedrock units [2] that transform sunlight into electricity through the PV effect—is [3] pivotal for the sustained expansion of the sector [4] as they produce the highest power in their ideal state [5]. Even minor defects like cracks, contamination, or misalignment can significantly affect the efficiency of these cells, thereby hampering the overall performance of solar panels and putting them at risk in terms of the economic feasibility of solar projects. Research indicates that manufacturing anomalies can slash efficiency by 10–15%, which, when magnified across expansive solar farms, results in considerable energy losses and financial burdens on the industry [6]. The production of PV cells is complex, involving a series of stages each susceptible to defect introduction, such as wafer slicing, cell doping, and coating. Traditionally, manual inspections during production have been the norm, but this approach is laborious, error-prone, and unsuitable for high-volume production lines [7]. Consequently, with the demand for solar panels on an upward trajectory, there is a pressing need for automated, scalable, and precise defect detection systems. Nonetheless, challenges persist, including limitations in data availability, real-time monitoring, measurement accuracy, computational efficiency, and dataset distribution, all of which impact the effectiveness of defect detection in PV cell production [8]. In this context, Computer Vision [9], especially Convolutional Neural Networks (CNNs) [10], has shown great promise in automating defect detection in PV cells [11, 12, 13, 14, 15] . CNNs have established a distinctive identity in image recognition tasks [16], making them particularly suited for pinpointing defects by identifying subtle patterns and irregularities that may get overlooked during human inspections. Yet, deploying CNN models in production facilities is not without its hurdles, particularly for resource-limited edge devices that are far from the course in industrial settings. These devices often lack the ability required to process large CNN models in real time. The development of streamlined, lightweight CNN models tailored for the detection of defective PV cells would bring forth significant opportunities by reducing the computational demand while preserving detection accuracy, paving the way for real-time inspection on edge devices, and ensuring scalable and effective quality control in the manufacture of PV cells."
https://arxiv.org/html/2411.02979v1,CAD-NeRF: Learning NeRFs from Uncalibrated Few-view Imagesby CAD Model Retrieval,"Reconstructing from multi-view images is a longstanding problem in 3D vision, where neural radiance fields (NeRFs) have shown great potential and get realistic rendered images of novel views. Currently, most NeRF methods either require accurate camera poses or a large number of input images, or even both. Reconstructing NeRF from few-view images without poses is challenging and highly ill-posed. To address this problem, we propose CAD-NeRF, a method reconstructed from less than 10 images without any known poses. Specifically, we build a mini library of several CAD models from ShapeNet and render them from many random views. Given sparse-view input images, we run a model and pose retrieval from the library, to get a model with similar shapes, serving as the density supervision and pose initializations. Here we propose a multi-view pose retrieval method to avoid pose conflicts among views, which is a new and unseen problem in uncalibrated NeRF methods. Then, the geometry of the object is trained by the CAD guidance. The deformation of the density field and camera poses are optimized jointly. Then texture and density are trained and fine-tuned as well. All training phases are in self-supervised manners. Comprehensive evaluations of synthetic and real images show that CAD-NeRF successfully learns accurate densities with a large deformation from retrieved CAD models, showing the generalization abilities.","Figure 1: CAD-NeRF takes few-view images with unknown poses as inputs, and jointly optimizes density, texture, and poses with the help of priors from the CAD mini library. Reconstruction and synthesizing new views of 3D scenes from 2D images is a core problem in the field of 3D vision. With the development of differentiable rendering, NeRF (Mildenhall et al. 2021) and its subsequent works (Tancik et al. 2022; Barron et al. 2021; Martin-Brualla et al. 2021; Sun, Wu, and Gao 2024; Yan et al. 2024) have shown great potential in this task, with implicit representations of multi-layer perceptrons (MLPs) to represent 3D objects or scenes. NeRFs succeed in generating high-quality and realistic images of novel views but with some limitations. The limitations include the requirement of a large number of multi-view images (around 100 images for a scene), and the assumption of accurate camera poses. Meanwhile, each scene is trained from scratch which takes a long time to converge. They highly limit the practicality of NeRFs. Recent works (Chen et al. 2021; Yu et al. 2021a; Tancik et al. 2021; Jain, Tancik, and Abbeel 2021; Zhang et al. 2021; Wang et al. 2021a; Deng et al. 2022) explore to address the problem of novel view synthesis from few views. Most of them learn camera poses by pretrained networks. They are trained from several multi-view image datasets, which is time-consuming. DietNeRF (Jain, Tancik, and Abbeel 2021) takes advantage of semantic information using a pretrained visual encoder CLIP, to improve the few-shot quality. However, it does not pay attention to the geometry of the object, which may fail when the geometry is complicated or very different from the training set. DS-NeRF (Deng et al. 2022) proposes a depth loss to learn better geometry that utilizes depth supervision. Most importantly, all of these few-image works require accurate camera poses of input images. To solve this problem, NeRS (Zhang et al. 2021) softens the constraint of input poses by adding pose refinement in the pipeline, where they only require rough camera poses from fixed views as inputs. NeRS demonstrates that the method can be better applied to real scenes and used by amateur users without the requirement of accurate poses. In this paper, we aim to explore the reconstruction from a small number (<10absent10<10< 10) of uncalibrated images. As illustrated in Figure 1, we propose CAD-NeRF, a NeRF-based method that learns geometry and texture representations of objects from 3-10 uncalibrated images. The problem is challenging since camera poses are completely unknown, and input views are extremely few. To tackle the problem, we first construct a mini library of common objects from ShapeNet (Chang et al. 2015). Each object is pre-rendered from uniformly sampled poses. By the library, we can retrieve an initial pose and similar mesh for each image by comparing the silhouettes. We first retrieve a most similar mesh by voting of multiple input images and then we retrieve poses for input images. In this process, we find searching for a pose with the most similar silhouette for each input image may lead to problems of pose conflicts among views. Due to shape symmetries, some retrieved poses may have errors of 180 degrees. The retrieved poses will not follow the initial order of input images. To maintain the image order and get a coherent pose order, we propose a multi-view pose retrieval method that considers the order information of input images by utilizing the backtracking algorithm to search multi-view poses at the same time. The retrieved mesh serves as the supervision when learning the density fields. Later, the deformation of the density field and camera poses are trained jointly by a re-rendering loss of the rendered images. At last, the texture and density are trained and fine-tuned by the re-rendering loss of the rendered color images. Our main contributions are: • We propose a method that reconstructs NeRFs from very few images without camera poses, with the help of prior shapes from CAD datasets. From the results, objects that are very different from CAD models can still be successfully reconstructed. The method effectively boosts the generalization ability of NeRFs. • We propose a multi-view pose retrieval method, that considers the ordering of the input images, to prevent pose conflicts of input images. It is a new problem in NeRF-based methods. • On both synthetic and real objects, CAD-NeRF demonstrates significant improvement over state-of-the-art and NeRF baselines. Since the proposed method is not keypoint-based, it works for feature-sparse cases which is challenging for other methods. Figure 2: The CAD-NeRF pipeline. Input images are used to retrieve the model of poses from the mini library. The CAD model is treated as the supervision to pre-train the initial density field (phase one). In phase two, sampled rays are sent from retrieved poses to generate 3D points along the rays, for the deformation network to predict the offset and correction of each point. Poses are optimized at the same time. In phase three, a color network is added and three networks are trained together."
https://arxiv.org/html/2411.02974v1,Region-Guided Attack on the Segment Anything Model (SAM),"The Segment Anything Model (SAM) is a cornerstone of image segmentation, demonstrating exceptional performance across various applications, particularly in autonomous driving and medical imaging, where precise segmentation is crucial. However, SAM is vulnerable to adversarial attacks that can significantly impair its functionality through minor input perturbations. Traditional techniques, such as FGSM and PGD, are often ineffective in segmentation tasks due to their reliance on global perturbations that overlook spatial nuances. Recent methods like Attack-SAM-K and UAD have begun to address these challenges, but they frequently depend on external cues and do not fully leverage the structural interdependencies within segmentation processes. This limitation underscores the need for a novel adversarial strategy that exploits the unique characteristics of segmentation tasks. In response, we introduce the Region-Guided Attack (RGA), designed specifically for SAM. RGA utilizes a Region-Guided Map (RGM) to manipulate segmented regions, enabling targeted perturbations that fragment large segments and expand smaller ones, resulting in erroneous outputs from SAM. Our experiments demonstrate that RGA achieves high success rates in both white-box and black-box scenarios, emphasizing the need for robust defenses against such sophisticated attacks. RGA not only reveals SAM’s vulnerabilities but also lays the groundwork for developing more resilient defenses against adversarial threats in image segmentation.","The Segment Anything Model (SAM) [1] has emerged as a leading solution in image segmentation, demonstrating remarkable adaptability and performance across diverse datasets and prompts. Its architecture allows for seamless integration with various inputs, making it a pivotal tool for applications ranging from autonomous driving [2, 3] to medical imaging [4, 5]. However, this versatility also exposes SAM to vulnerabilities, particularly from adversarial attacks that can significantly degrade its performance [6, 7]. These attacks leverage subtle perturbations in the input data, misleading the model into producing incorrect segmentations, thereby raising concerns about the reliability of SAM in critical contexts. Previous adversarial attack methods, such as the Fast Gradient Sign Method (FGSM) [8] and Projected Gradient Descent (PGD) [9], primarily focus on classification models and often utilize global perturbations that affect the entire input. These methods can be less effective in segmentation tasks, where spatial relationships and contextual information are critical. Recent approaches like Attack-SAM-K [6] and UAD [7] have begun to explore the unique challenges associated with adversarial attacks on segmentation models, but many still rely on external prompts or do not fully exploit the structural dependencies inherent in the segmentation process. To effectively address these vulnerabilities, we propose the Region-Guided Attack (RGA), a novel adversarial attack strategy specifically designed for SAM. Unlike traditional adversarial methods that often rely on external prompts or global perturbations, RGA focuses on manipulating segmented regions directly through a Region-Guided Map (RGM). This approach allows for targeted adversarial perturbations that divide large segments into smaller fragments while merging smaller regions into larger areas, ultimately leading to misclassifications in SAM’s outputs. The innovation of RGA lies in its ability to exploit the structural dependencies within the segmentation task, leveraging the inherent characteristics of SAM to enhance the effectiveness of the attack. The significance of RGA is twofold. First, it provides a deeper understanding of the vulnerabilities inherent in advanced segmentation models like SAM, offering insights into how adversarial perturbations can be crafted more strategically. Second, RGA presents a more refined method of inducing segmentation errors that can be applied across various segmentation frameworks, highlighting the need for robust defenses against such targeted attacks. Through extensive experiments, we demonstrate the effectiveness of RGA, revealing its capability to achieve high attack success rates in both white-box and black-box scenarios while maintaining minimal perceptual distortion in the input images. In summary, the key contributions of RGA include: 1. Region-Guided Map (RGM) for Adversarial Guidance: RGA introduces a novel use of the RGM to directly guide the generation of adversarial examples. By utilizing RGM to define how SAM’s segmentation should be altered (i.e., splitting large regions into smaller ones and merging smaller regions into larger ones), RGA effectively guides perturbations to maximize the impact on segmentation quality. 2. Enhanced Attack Success and Transferability: By leveraging RGM, RGA achieves higher attack success rates and improved transferability. The adversarial examples are generated with a clear objective influenced by the segmentation output, which systematically guides perturbations, leading to more successful and transferable attacks against the SAM model. 3. Independent of External Prompts: Unlike many existing methods that rely heavily on specific prompts to guide attacks, RGA operates independently of external prompts, making the adversarial process more streamlined and broadly applicable. This independence ensures that RGA can be applied in scenarios where prompts are unavailable or unpredictable. 4. Insights into SAM’s Segmentation Vulnerabilities: RGA reveals particular vulnerabilities in SAM by focusing on regional manipulations instead of global input perturbations. The findings highlight how region-specific guidance, such as altering the size and boundaries of segmented areas, can degrade SAM’s segmentation performance significantly. This understanding provides valuable insights for designing more resilient segmentation models."
https://arxiv.org/html/2411.02972v1,Exploring Seasonal Variability in the Context of Neural Radiance Fields for 3D Reconstruction on Satellite Imagery,"In this work, the seasonal predictive capabilities of Neural Radiance Fields (NeRF) applied to satellite images are investigated. Focusing on the utilization of satellite data, the study explores how Sat-NeRF, a novel approach in computer vision, performs in predicting seasonal variations across different months. Through comprehensive analysis and visualization, the study examines the model’s ability to capture and predict seasonal changes, highlighting specific challenges and strengths. Results showcase the impact of the sun direction on predictions, revealing nuanced details in seasonal transitions, such as snow cover, color accuracy, and texture representation in different landscapes. Given these results, we propose Planet-NeRF, an extension to Sat-NeRF capable of incorporating seasonal variability through a set of month embedding vectors. Comparative evaluations reveal that Planet-NeRF outperforms prior models in the case where seasonal changes are present. The extensive evaluation combined with the proposed method offers promising avenues for future research in this domain.","Earth observation satellites are essential for monitoring the environment, and the amount of available data increases rapidly. Beneficial applications are found in e.g. meteorology [21], agriculture [2], forestry [18], biodiversity conservation [6], and regional planning [14]. As technology advances and the number of orbiting satellites increases, their significance grows [25]. However, the world we observe through earth observation satellites is far from static, and using these images for 3D reconstruction faces challenges. Due to variations, including seasonal changes, varying incoming light, and the presence of transient objects, multi-date imagery is frequently disregarded in novel view synthesis. At the same time, these extensive collections of images contain valuable information about how specific regions change over time. Recent advances in neural 3D reconstruction are neural radiance fields (NeRF) [15], a technique that has demonstrated impressive capabilities of creating accurate representations of 3D objects or scenes by representing them as a 5D-vector valued function or field. This function implicitly handles tasks such as synthesizing new views and reconstructing 3D information, matching geometry and colors with the camera projections from various angles. Since the initial research in this area [15], numerous researchers have explored different aspects [3, 11, 12, 13, 17]. Some have investigated how this approach copes with transient objects, such as cars [13], while others have explored its application to multi-date satellite images [3, 11, 12, 17]. Although previous works adapt the use of NeRF to satellite imagery by handling the occurrence of transient objects, an intriguing yet underexplored terrain lies in understanding its capabilities within dynamic environments subject to seasonal changes. Handling images from various seasons can pose challenges in creating an accurate 3D representation of the scene since the seasons contribute to changes in geometry, color, and illumination. Examples of variations include, among other things, varying amounts of snow, and the color and density appearance of vegetation in different seasons. Nevertheless, in the specific case of 3D reconstruction of real-world scenes captured by satellites, the handling of seasonal variability is very important. First of all, for accurate 3D reconstruction in the case of a sparse set of images where season changes between images are present. Second, for the purpose of novel view synthesis with different seasons. The contribution of this paper is twofold. First, an in-depth evaluation of the seasonal predictive capability of a publicly available method for satellite NeRF 3D reconstruction, Sat-NeRF [11]. Second, an extension to Sat-NeRF, called Planet-NeRF. A modular, simple, yet effective way of encoding seasons."
https://arxiv.org/html/2411.02969v1,Multi-modal NeRF Self-Supervision for LiDAR Semantic Segmentation,"LiDAR Semantic Segmentation is a fundamental task in autonomous driving perception consisting of associating each LiDAR point to a semantic label. Fully-supervised models have widely tackled this task, but they require labels for each scan, which either limits their domain or requires impractical amounts of expensive annotations. Camera images, which are generally recorded alongside LiDAR pointclouds, can be processed by the widely available 2D foundation models, which are generic and dataset-agnostic. However, distilling knowledge from 2D data to improve LiDAR perception raises domain adaptation challenges. For example, the classical perspective projection suffers from the parallax effect produced by the position shift between both sensors at their respective capture times. We propose a Semi-Supervised Learning setup to leverage unlabeled LiDAR pointclouds alongside distilled knowledge from the camera images. To self-supervise our model on the unlabeled scans, we add an auxiliary NeRF head and cast rays from the camera viewpoint over the unlabeled voxel features. The NeRF head predicts densities and semantic logits at each sampled ray location which are used for rendering pixel semantics. Concurrently, we query the Segment-Anything (SAM) foundation model with the camera image to generate a set of unlabeled generic masks. We fuse the masks with the rendered pixel semantics from LiDAR to produce pseudo-labels that supervise the pixel predictions. During inference, we drop the NeRF head and run our model with only LiDAR.We show the effectiveness of our approach in three public LiDAR Semantic Segmentation benchmarks: nuScenes, SemanticKITTI and ScribbleKITTI.","I INTRODUCTION LiDAR Semantic Segmentation (SemSeg) is the computer vision task of associating every sample from a LiDAR pointcloud to a semantic class. It is essential for autonomous driving, enabling the identification of drivable areas and object boundaries to make safe and informed decisions when navigating the vehicle’s surroundings [5] [6]. The availability of autonomous driving perception benchmarks with annotated LiDAR scans [11] [12] [13] [14] has enabled fully-supervised LiDAR SemSeg models [7] [8] [9] [10] to learn this task for narrow domains (e.g. specific dataset, selected cities/countries) where only limited amounts of labeled data are enough to train a well-performing perception model. However, for bringing fully automated driving functions to a world scale, we need models that perform well in a wide variety of scenarios, such as distinct continents, road agents, weather conditions, or driving styles. In the fully-supervised domain, this involves labeling huge amounts of LiDAR pointclouds, which is an expensive task that requires human supervision [15]. On the other hand, LiDAR pointclouds and camera images are cheap to collect compared with their annotation cost [17]. Generally, and for most of the public LiDAR perception benchmarks, LiDAR pointclouds are recorded alongside synchronized camera images at similar instants, with significant overlap in their field of view and a known transformation between the two sensor origins [11] [12] [13] [14]. The camera frames, unlike LiDAR pointclouds, can be processed by the widely available 2D foundation models [1] [2] [3], which are increasingly effective at helping with a wide range of tasks while being dataset agnostic. The Segment-Anything Model [3] (SAM) is a groundbreaking foundation model designed for zero-shot image segmentation. For each image, it generates masks that are agnostic to the instance definition and labeling format. Our work aims to distill the knowledge from SAM for the LiDAR modality during training, while having a LiDAR-only inference pipeline. Fig. 1 exemplifies this knowledge distillation. However, distilling knowledge from 2D foundation models into a 3D perception model raises the challenge of effectively bridging these data from distinct domains. There’s been extensive work tackling this challenge. The classical way of fusing 2D and 3D data, already known for over a decade, is the perspective projection [18] of LiDAR points into the camera plane given the camera parameters and transformation between sensors. This technique has been demonstrated useful in the past for assigning 3D pseudo-labels from 2D semantic labels, which are typically less expensive to produce, requiring less skilled annotators and more generic equipment [15]. Even though the perspective projection has proven effective and accurate in some scenarios, it suffers from the parallax effect, which is the apparent shift in the position of an object relative to a background when the observer’s viewpoint changes [16]. In our multi-modal setup, this occurs when the position of the LiDAR and camera at their respective capture times differ. This undesired effect becomes especially stronger at higher velocities and in setups where the camera is placed away from the LiDAR. Some works address this issue by exploiting the multi-view nature of video recordings at consecutive frames or by overlapping multiple-view cameras to select projections that are consistent from multiple views [19]. However, these works require pre-trained 2D SemSeg models and need to either train on data with multi-view cameras and big overlaps or deal with moving objects from neighboring frames. To address this, we include in our Semi-Supervised Learning (SSL) model a self-supervision technique to train on the unlabeled data which is inspired by the training mechanism of Neural Radiance Fields (NeRFs) [20]. We show how this technique can better leverage unlabeled images and scans than existing approaches such as the classical perspective projection. Our NeRF self-supervision allows the model to reason about occupancy and semantics along rays, rather than individual points. It consists of 1) an efficient Pixel-to-Ray Casting mechanism, 2) a N⁢e⁢R⁢F𝑁𝑒𝑅𝐹NeRFitalic_N italic_e italic_R italic_F Multi-Layer Perceptron (MLP) head, and 3) volumetric rendering equations. The whole NeRF self-supervision is only added during training and dropped for inference. Hence, once deployed, our model is as efficient as any LiDAR-only SemSeg method and doesn’t require any input data other than LiDAR. We demonstrate the benefit of our method in scenarios with scarcely labeled 3D data on three well-established public benchmarks for LiDAR SemSeg: nuScenes [13], SemanticKITTI [11], and its scarcely-labeled variation: ScribbleKITTI [12]."
https://arxiv.org/html/2411.02920v1,Domain Expansion and Boundary Growth for Open-Set Single-Source Domain Generalization,"Open-set single-source domain generalization aims to use a single-source domain to learn a robust model that can be generalized to unknown target domains with both domain shifts and label shifts. The scarcity of the source domain and the unknown data distribution of the target domain pose a great challenge for domain-invariant feature learning and unknown class recognition. In this paper, we propose a novel learning approach based on domain expansion and boundary growth to expand the scarce source samples and enlarge the boundaries across the known classes that indirectly broaden the boundary between the known and unknown classes. Specifically, we achieve domain expansion by employing both background suppression and style augmentation on the source data to synthesize new samples. Then we force the model to distill consistent knowledge from the synthesized samples so that the model can learn domain-invariant information. Furthermore, we realize boundary growth across classes by using edge maps as an additional modality of samples when training multi-binary classifiers. In this way, it enlarges the boundary between the inliers and outliers, and consequently improves the unknown class recognition during open-set generalization. Extensive experiments show that our approach can achieve significant improvements and reach state-of-the-art performance on several cross-domain image classification datasets.","When the training and test data are independently and identically distributed (i.i.d.), deep neural networks have shown remarkable performances in various computer vision tasks [1, 2, 3, 4]. However, the performances will suffer from a catastrophic drop once the training and test data have different distributions [5, 6, 7, 8]. Moreover, these distribution shifts are generally inevitable in real-world scenarios since the collection of training datasets is inherently a sub-sampling of the real-world data; thus, it is impossible to have the same distribution with unseen testing datasets that are usually collected separately under different conditions (e.g. time and space). Particularly, distribution shifts can appear as both domain shifts [9, 10] and label shifts [11, 12, 13] in the unseen testing datasets, resulting in target domains with different domain characteristics and larger label spaces compared to the source domain. To generalize to unseen target domains with unknown categories, open-set domain generalization (OS-DG) [14] is studied, with the aim of learning a generalizable model from multiple source domains. However, gathering annotated data from multiple domains can be both difficult and costly. As a result, a more practical and appealing approach involves using a single-source domain. This has spurred interest in the study of open-set single-source domain generalization (OS-SDG) [15], where a model is trained solely on a single source domain to identify known categories and discern unknown categories in unseen domains. Despite its appeal, this reliance on a singular source domain poses a significantly greater challenge compared to OS-DG. CrossMatch [15] is the first work solving OS-SDG. Built upon the adversarial data augmentation method [16, 17], CrossMatch generates two kinds of auxiliary training data: samples outside the source data distribution and samples outside the source label space, which are used to extend the source domain and act as unknown classes, respectively. However, the quality of samples generated through adversarial training can be unstable, resulting in unrealistic and subpar samples, particularly for unknown classes. On the other hand, SODG-Net [18] generates pseudo-open samples by combining the immediate features of two known classes. Nevertheless, this approach could potentially undermine the recognition of the known classes, as pseudo-open samples incorporate partial features from these known classes. Both of these prior works rely on the generation of pseudo-open samples, which may not accurately or sufficiently represent the actual open samples in the unseen domains. In this paper, we propose a novel Domain Expansion and BoUndary Growth based method, named DEBUG, that only augments source samples in terms of known classes, without generating any pseudo-open samples. An intuition stems from the observation that the capacity of the model in recognizing out-of-distribution known and unknown classes can be simultaneously enhanced once the known class distributions in the source feature space are adequately discriminative and separated by a large margin, as illustrated in Figure 1. The profound rationale for DEBUG is rooted in the theory that inductive bias aids in generalization [19, 20]. Enhancing the induction in semantic attribution is beneficial for generalization to unseen domains [21, 22]. However, training solely on a single source domain may inadvertently introduce a bias tied to domain-specific characteristics, which could adversely affect cross-domain generalization. We classify these domain characteristics into two distinct types: class-irrelevant content (primary background information) [23] and style[24]. To address these biases, we propose two techniques: background suppression (BS) and global probabilistic-based style augmentation (GPSA). More specifically, we use an off-the-shelf foreground-background segmentation method111Note that our method is relatively robust to the choice of the unsupervised segmentation methods, as demonstrated in Table X. to get a coarse yet free foreground mask for each sample. Based on the resultant coarse mask, we remove the irrelevant background regions from the original samples, referred to as background suppression. Additionally, we introduce a global probabilistic-based style augmentation method that globally models the feature statistics (i.e. mean and standard deviation) of the original and background-suppressed samples as Gaussian distributions. We augment both the original and background-suppressed samples by replacing their instance-level style statistics with randomly sampled style statistics from the global probabilistic distributions. Subsequently, with the paired augmented samples with and without backgrounds, we force the model to learn consistent representations via knowledge distillation. The consistency regularization between two samples with different content and style augmentations promotes domain-invariant representation learning. Moreover, the consistency with the background-suppressed augmentation can reduce the intra-class variance since the removal of a background can mitigate the spurious effect of irrelevant context. Despite the elimination of inductive biases related to domain characteristics, some class-unspecific features, such as common object parts (e.g., furry tails), may still increase the risk of misclassification, particularly for unknown classes that have not yet had their class-specific features captured. In response to this, we develop the Boundary Growth strategy, which diminishes the emphasis on common but class-unspecific features, which subsequently leads to further sufficient separation of class distributions and creates more room for open-set recognition. Specifically, we adopt the multi-binary classifiers [25] that train a one-vs-all classifier for each class to deal with open-set recognition, and we present a novel strategy based on edge maps that are considered as a new modality of the samples to train the multi-binary classifiers. Specifically, we extract edge maps from the original samples, treating them as additional samples with the same semantic yet different spreads in the feature space (see Figure 3). When training a binary classifier for a specific class, a boundary is learned between the edge maps of the positive samples and the hard negative samples, either in their original form or as edge maps. We conduct extensive experiments on a variety of cross-domain datasets, including PACS [26], Office31 [27], OfficeHome [27], and DomainNet126 [28]. Our proposed DEBUG shows significant and consistent improvements over CrossMatch, especially on unknown class recognition. This demonstrates the effectiveness of our proposed approach for open-set single-source domain generalization. Our contributions can be summarized as follows: • We offer a novel insight into OS-SDG: the recognition ability of out-of-distribution known and open-set unknown classes can be improved when there is clear discrimination and sufficient separation in the source feature space’s distribution of known classes. We achieve the discrimination and separation through our domain expansion and boundary growth design. • We introduce a comprehensive method for expanding the single-source domain: we augment the original samples by suppressing the background and adding style disturbances sampled from global probabilistic style statistics, and guide models to learn content-invariant information by distilling knowledge from these background-suppressed augmentations. • We propose a novel boundary growth technique to preserve spaces for unknown classes encountered in the target domains. We utilize edge maps as an additional modality and integrate them into the training of multi-binary classifiers. The edge maps expand the initial distribution of each class, and the optimization of the binary classifier for each class extends the class boundary of the initial distribution. • We demonstrate that our proposed approach consistently outperforms existing methods on various cross-domain image datasets in the challenging open-set single-domain generalization setting."
https://arxiv.org/html/2411.02902v1,Membership Inference Attacks againstLarge Vision-Language Models,"Large vision-language models (VLLMs) exhibit promising capabilities for processing multi-modal tasks across various application scenarios. However, their emergence also raises significant data security concerns, given the potential inclusion of sensitive information, such as private photos and medical records, in their training datasets. Detecting inappropriately used data in VLLMs remains a critical and unresolved issue, mainly due to the lack of standardized datasets and suitable methodologies. In this study, we introduce the first membership inference attack (MIA) benchmark tailored for various VLLMs to facilitate training data detection. Then, we propose a novel MIA pipeline specifically designed for token-level image detection. Lastly, we present a new metric called MaxRényi-K%, which is based on the confidence of the model output and applies to both text and image data. We believe that our work can deepen the understanding and methodology of MIAs in the context of VLLMs. Our code and datasets are available at https://github.com/LIONS-EPFL/VL-MIA.","The rise of large language models (LLMs) [9, 60, 45, 11] has inspired the exploration of large models across multi-modal domains, exemplified by advancements like GPT-4 [1] and Gemini [59]. These large vision-language models (VLLMs) have shown promising ability in various multi-modal tasks, such as image captioning [33], image question answering [13, 35], and image knowledge extraction [26]. However, the rapid advancement of VLLMs also causes user concerns about privacy and knowledge leakage. For instance, the image data used during commercial model training may contain private photographs or medical diagnostic records. This is concerning since early work has demonstrated that machine learning models can memorize and leak training data [3, 56, 63]. To mitigate such concerns, it is essential to consider the membership inference attack (MIA) [23, 53], where attackers seek to detect whether a particular data record is part of the training dataset [23, 53]. The study of MIAs plays an important role in preventing test data contamination and protecting data security, which is of great interest to both industry and academia [24, 19, 44]. When exploring MIAs in VLLMs, one main issue is the absence of a standardized dataset designed to develop and evaluate different MIA methods, which comes from the large size [16] and multi-modality of the training data, and the diverse VLLMs training pipelines [66, 35, 18]. Therefore, one of the main goals of this work is to build an MIA benchmark tailored for VLLMs. Beyond the need for a valid benchmark, we lack efficient techniques to detect a single modality in VLLMs. The closest work to ours is [30], which performs MIAs on multi-modal CLIP [46] by detecting whether an image-text pair is in the training set. However, in practice, it is more common to detect a single modality, as we care whether an individual image or text is in the training set. Therefore, we aim to develop a pipeline to detect the single modality from a multi-modal model. Moreover, existing literature on language model MIAs, such as Min-K% [52] and Perplexity [62], mostly are target-based MIAs, which use the next token as the target to compute the prediction probability. However, we can only access the image embedding instead of the image token in VLLMs, and thus only target-free MIAs [48] can be directly applied. Figure 1: MIAs against VLLMs. Top: Our image detection pipeline: In the generation stage, we feed the image and instruction to the target model to obtain a description; then during the inference stage, we input the image, instruction, and generated description to the model, and extract the logits slices to calculate metrics. Bottom: MaxRényi-K% metric: we first get the Rényi entropy of each token position, then select the largest k%percent𝑘k\%italic_k % tokens and calculate the average Rényi entropy. Therefore, we first propose a cross-modal pipeline for individual image or description MIAs on VLLMs, which is distinguished from traditional MIAs that only use one modality [61, 62]. We feed the VLLMs with a customized image-instruction pair from the target image or description. We show that we can perform the MIA not only by the image slice but also by the instruction and description slices of the VLLM’s output logits, see Figure 1. Such a cross-modal pipeline enables the usage of text MIA methods on image MIAs. We also introduce a target-free metric that adapts to both image and text MIAs and can be further modified to a target-based way. Overall, the contributions and insights can be summarized as follows. • We release the first benchmark tailored for the detection of training data in VLLMs, called Vision Language MIA (VL-MIA) (Section 4). By leveraging Flickr and GPT-4, we construct VL-MIA that contains two images MIA tasks and one text MIA task for various VLLMs, including MiniGPT-4 [66], LLaVA 1.5 [35] and LLaMA-Adapter V2 [18]. • We perform the first individual image or description MIAs on VLLMs in a cross-modal manner. Specifically, we demonstrate that we can perform image MIAs by computing statistics from the image or text slices of the VLLM’s output logits (Figures 1 and 5.1). • We propose a target-free MIA metric, MaxRényi-K%, and its modified target-based ModRényi (Section 5.2). We demonstrate their effectiveness on open-source VLLMs and closed-source GPT-4 (Section 6). We achieve an AUC of 0.815 on GPT-4 in image MIAs."
https://arxiv.org/html/2411.02890v1,Fried deconvolution,"In this paper we present a new approach to deblur the effect of atmospheric turbulence in the case of long range imaging. Our method is based on an analytical formulation, the Fried kernel, of the atmosphere modulation transfer function (MTF) and a framelet based deconvolution algorithm. An important parameter is the refractive index structure which requires specific measurements to be known. Then we propose a method which provides a good estimation of this parameter from the input blurred image. The final algorithms are very easy to implement and show very good results on both simulated blur and real images.","In long range imaging, the blur due to the atmosphere on the acquired image is non negligible. Two main issues can be observed depending on the level of turbulence: a blurring effect and a geometrical distortion effect. From a physics point of view, these effects are clearly correlated but very hard to model because of the different physical parameters (temperature, wind, humidity, wavelength, ……\ldots…). From the image processing point of view, it is easier to consider these two effects as two separate operators which we want to invert. In [8], the authors address the problem of correcting the geometrical distortions from an input sequence and use some usual deblurring algorithm based on a Gaussian kernel assumption at the end of the process to deal with the blur. This final processing about blur does not significantly improve the final image from the geometrically corrected one because of the ignorance of the blur kernel. At this moment, two methods are possible. The first one is to use some blind deconvolution algorithm but in the general case this kind of algorithm remains a very active field of research and no “simple” algorithms are available. Another method is to find some analytic expression which model the blurring effect of the atmosphere. Surprisingly, this model exists since ……\ldots… 1966! David Fried, well known in the optical science community for the definition of Fried’s seeing diameter used to characterize the optical resolution limit, proposed in [4] an analytical formulation to model the Modulation Transfer Function (MTF) of the atmosphere. This work was recently revisited by D. Tofsted in [5]. It appears that Fried’s MTF was not well-accepted by the optical science community. But recently, some experiments based on field trials show the effectiveness of this MTF to model real phenomena [1]. In this paper we propose to use the Fried kernel to deal with the atmospheric blur (the geometrical distortions are not taken into account here; however in the experiments we show some results using the output of the algorithm proposed in [8] as input to the deconvolution). In section 2, we give the analytical formulation of the Fried kernel. Some details are given about the different parameters and will show that if most of them are linked with the optical system, the last one, Cn2superscriptsubscript𝐶𝑛2C_{n}^{2}italic_C start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, is linked with the level of turbulence of the atmosphere and is difficult to know in practice. In section 3, we recall the nonblind framelet based deconvolution algorithm of [2, 3] which we will use in the following sections. In section 4, we present a very simple nonblind Fried deconvolution assuming that the Cn2superscriptsubscript𝐶𝑛2C_{n}^{2}italic_C start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT parameter is known. Some properties of the Fried deconvolution will be observed from different experiments done on simulated and real images. In section 5, we address the most practical case where the Cn2superscriptsubscript𝐶𝑛2C_{n}^{2}italic_C start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT parameter is unknown. We propose a criteria to find a good estimate of this parameter. Finally we introduce a blind Fried deconvolution algorithm. The different experiments show the effectiveness of the propose method on both simulated and real images."
https://arxiv.org/html/2411.02889v1,Turbulence stabilization,We recently developed a new approach to get a stabilized image from a sequence of frames acquired through atmospheric turbulence. The goal of this algorihtm is to remove the geometric distortions due by the atmosphere movements. This method is based on a variational formulation and is efficiently solved by the use of Bregman iterations and the operator splitting method. In this paper we propose to study the influence of the choice of the regularizing term in the model. Then we proposed to experiment some of the most used regularization constraints available in the litterature.,"These last few years show an increase of interest in the development of mitigation algorithms to deal with the atmospheric turbulence degradations. Indeed, turbulences can affect images in two major ways: a blurring effect and random geometric distortions. An interesting work about turbulence modelization for mitigation algorithms was made by Frakes [15, 16]. The authors modeled the turbulence phenomenon by using two operators: fi⁢(x)=Di⁢(H⁢(u⁢(x)))+noisesubscript𝑓𝑖𝑥subscript𝐷𝑖𝐻𝑢𝑥noisef_{i}(x)=D_{i}(H(u(x)))+\text{noise}italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_x ) = italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_H ( italic_u ( italic_x ) ) ) + noise (1) where u𝑢uitalic_u is the static original scene we want to retrieve, fisubscript𝑓𝑖f_{i}italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is the observed image at time i𝑖iitalic_i, H𝐻Hitalic_H is a blurring kernel, and Disubscript𝐷𝑖D_{i}italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is an operator which represents the geometric distortions caused by the turbulence at time i𝑖iitalic_i. Based on this model, the idea is to try to inverse the two operators H𝐻Hitalic_H and Disubscript𝐷𝑖D_{i}italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. Inversing H𝐻Hitalic_H is a deconvolution problem which will not be addressed here (we propose an original deconvolution approach in [20]), in this paper we will focus on the stabilization problem to remove the geometric distortions. In a recent work [29], we proposed a new variational framework based on a combination of a deformation flow estimation and a nonlocal regularization term to retrieve a stabilized image from a set of acquired frames. Our results clearly outperform other existing methods like the PCA-based algorithm [26] or the Lucky-Region Fusion approach [2]. If our algorihtm is very efficient and needs very few images (20 input frames seem enough in all the cases), its main drawback is the computational time needed to apply the nonlocal regularization. In this paper, we propose to study the impact, in terms of image quality reconstruction, the choice of other regularizers. The reminder of the paper is as follows. In section 2 we recall the model developped in [29]. Section 3 presents the different regularizer we choose to test instead of the nonlocal one. Corresponding experiments are presented in section 4 and some conclusions are provided in section 5."
https://arxiv.org/html/2411.02861v1,Centerness-based Instance-aware Knowledge Distillation with Task-wise Mutual Lifting for Object Detection on Drone Imagery,"Developing accurate and efficient detectors for drone imagery is challenging due to the inherent complexity of aerial scenes. While some existing methods aim to achieve high accuracy by utilizing larger models, their computational cost is prohibitive for drones. Recently, Knowledge Distillation (KD) has shown promising potential for maintaining satisfactory accuracy while significantly compressing models in general object detection. Considering the advantages of KD, this paper presents the first attempt to adapt it to object detection on drone imagery and addresses two intrinsic issues: (1) low foreground-background ratio and (2) small instances and complex backgrounds, which lead to inadequate training, resulting insufficient distillation. Therefore, we propose a task-wise Lightweight Mutual Lifting (Light-ML) module with a Centerness-based Instance-aware Distillation (CID) strategy. The Light-ML module mutually harmonizes the classification and localization branches by channel shuffling and convolution, integrating teacher supervision across different tasks during back-propagation, thus facilitating training the student model. The CID strategy extracts valuable regions surrounding instances through the centerness of proposals, enhancing distillation efficacy. Experiments on the VisDrone, UAVDT, and COCO benchmarks demonstrate that the proposed approach promotes the accuracies of existing state-of-the-art KD methods with comparable computational requirements. Codes will be available upon acceptance.","Unmanned Aerial Vehicle (UAV) equipment has played a pivotal role in numerous applications such as traffic management and security surveillance, due to its commendable flight capabilities and user-friendly operation, which stimulates a surge in the necessity for vision tasks, in particular object detection. Although general object detection has largely advanced during the last decade with sophisticated models and increasing accuracies delivered, relatively large computational requirements make it difficult to adapt them to resource-constrained drone hardware, thereby posing a significant challenge in developing effective and efficient detectors on drone imagery. (a) (b) Figure 1: Challenges to object detection on drone imagery (VisDrone): (a) low foreground-background ratio and (b) small instances and complex backgrounds. A number of efforts have been made to address the trade-off between accuracy and efficiency of detectors on drone images through model compression techniques, including lightweight network methods (Zhu et al., 2021; Zhou et al., 2022; Zhu et al., 2023), sparse convolution methods (Yang et al., 2022a; Du et al., 2023) and network pruning methods (Liu et al., 2021; Zhang et al., 2019). Nevertheless, lightweight network methods design elaborate modules to improve the performance without reducing much computational load, while network pruning methods tend to disregard essential calculations on small objects which are dominant in drone images, thus leading to a sub-optimal accuracy. Additionally, sparse convolution methods show a better speed-accuracy trade-off by implementing constrained sparse convolutions, albeit at the cost of compromising the generalizability of the model on drone platforms. Recently, Knowledge Distillation (KD) has emerged as a promising alternative to model compression, offering largely reduced training and deployment costs while maintaining competitive performance, and has proved effective in general object detection. KD typically follows a student-teacher framework, where knowledge of the larger teacher is distilled and transferred to the smaller student, thus generating a model of a compressed size with an enhanced accuracy. Unfortunately, it is not so straightforward to adapt KD to detectors on drone images, and two major challenges remain. First, as illustrated in Fig. 1 (a), drone images typically exhibit a low ratio between foreground and background, which limits the supervisory information conveyed by the teacher model in the foreground area. This limited supervision leads to insufficient distillation, resulting in a more pronounced performance gap between the teacher and student models. Second, when applying KD in object detection, a common practice is to extract additional valuable regions complementing positive samples for teacher supervision reinforcement. The strategies mainly include foreground-background region (Guo et al., 2021; Wang et al., 2019; Yang et al., 2022b), teacher-student comparison (Dai et al., 2021), and IoU-based selection (Zheng et al., 2022). However, regarding the case of drone imagery, instances are often small and backgrounds are usually complicated as shown in Fig. 1 (b), both of which make it more difficult to extract additional regions, further limiting the supervisory information, and thus incur an inadequate distillation effect. To address the aforementioned challenges, this paper proposes a novel approach with a task-wise Lightweight Mutual Lifting (Light-ML) module and a Centerness-based Instance-aware Distillation (CID) strategy for drone images. To mitigate the insufficient supervision caused by the low foreground-to-background ratio, the Light-ML module is designed to mutually harmonize the classification and localization branches through an innovative module consisting of channel shuffling and convolution. It enhances the input information for both branches, and integrates teacher supervision clues across different tasks during the back-propagation stage, thereby amplifying the impact of KD, and effectively reducing the performance gap between the teacher and student models. For the issue of insufficient distillation due to difficulties in extracting additional information from small instances in complex background, the CID strategy generates valuable regions around instances based on the centerness of the prediction of each anchor box or anchor point, which enables the smooth and flexible estimation of informative regions surrounding objects, particularly concerning small instances. The contribution of our work lies in three-fold: • We propose a novel KD approach to object detection in particular for drone images. To the best of our knowledge, this is the first attempt to introduce KD techniques for model compression to build detectors for drone imagery. • We design a Lightweight Mutual Lifting (Light-ML) module and a centerness-based instance-aware distillation (CID) strategy to address the inherent challenges of applying KD to drone imagery detection. These components enrich the supervision information during distillation, thereby enhancing the distillation effect. • We make extensive evaluation on three public benchmarks (VisDrone, UAVDT, and COCO) with various detection pipelines (e.g. GFL v1 and ATSS) and achieve the state-of-the-art accuracy, showing the great potential of KD in this field."
https://arxiv.org/html/2411.02860v1,Continual Audio-Visual Sound Separation,"In this paper, we introduce a novel continual audio-visual sound separation task, aiming to continuously separate sound sources for new classes while preserving performance on previously learned classes, with the aid of visual guidance. This problem is crucial for practical visually guided auditory perception as it can significantly enhance the adaptability and robustness of audio-visual sound separation models, making them more applicable for real-world scenarios where encountering new sound sources is commonplace. The task is inherently challenging as our models must not only effectively utilize information from both modalities in current tasks but also preserve their cross-modal association in old tasks to mitigate catastrophic forgetting during audio-visual continual learning. To address these challenges, we propose a novel approach named ContAV-Sep (Continual Audio-Visual Sound Separation). ContAV-Sep presents a novel Cross-modal Similarity Distillation Constraint (CrossSDC) to uphold the cross-modal semantic similarity through incremental tasks and retain previously acquired knowledge of semantic similarity in old models, mitigating the risk of catastrophic forgetting. The CrossSDC can seamlessly integrate into the training process of different audio-visual sound separation frameworks. Experiments demonstrate that ContAV-Sep can effectively mitigate catastrophic forgetting and achieve significantly better performance compared to other continual learning baselines for audio-visual sound separation. Code is available at: https://github.com/weiguoPian/ContAV-Sep_NeurIPS2024.","Humans can effortlessly separate and identify individual sound sources in daily experience [25, 7, 64, 33]. This skill plays a crucial role in our ability to understand and interact with the complex auditory environments that surround us [34]. However, replicating this capability in machines remains a significant challenge due to the inherent complexity of real-world auditory scenes [7, 77]. Inspired by the multisensory perception of humans [62, 60], audio-visual sound separation tackles this challenge by utilizing visual information to guide the separation of individual sound sources in an audio mixture. Recent advances in deep learning have led to significant progress in audio-visual sound separation [84, 23, 21, 67, 14, 65, 81, 63, 11, 71]. Benefiting from more advanced architectures (e.g., U-Net [84, 23], Transformer [14], and diffusion models [27]) and discriminative visual cues (e.g., grounded visual objects [67], motion [83], and dynamic gestures [21]), audio-visual separation models are able to separate sounds ranging from domain-specific speech, musical instrument sounds to open-domain general sounds within training sound categories. However, a limitation of these studies is their focus on scenarios where all sound source classes are presently known, overlooking the potential inclusion of unknown sound source classes during inference in real-world applications. This oversight leads to the catastrophic forgetting issue [32, 3], where the fine-tuning of models on new classes detrimentally impacts their performance on previously learned classes. Despite Chen et al. [14] demonstrating that their iQuery model can generalize to new classes well through simple fine-tuning, it still suffers from the catastrophic forgetting problem on old classes. This prevents the trained models from continuously updating in real-world scenarios, impeding their adaptability to dynamic environments. The question how to effectively leverage visual guidance to continuously separate sounds from new categories while preserving separation ability for old sound categories remains open. Figure 1: Top: Illustration of the continual audio-visual sound separation task, where the model (separator) learns from sequential audio-visual sound separation tasks. Bottom: Illustration of the catastrophic forgetting problem in continual audio-visual sound separation and its mitigation by our proposed method. Fine-tuning: Directly fine-tune the separation model on new sound source classes; Upper bound: Train the model using all training data from seen sound source classes. To bridge this gap, we introduce a novel continual audio-visual sound separation task by integrating audio-visual sound separation with continual learning principles. The goal of this task is to develop an audio-visual model that can continuously separate sound sources in new classes while maintaining performance on previously learned classes. The key challenge we need to address is catastrophic forgetting during continual audio-visual learning, which occurs when the model is updated solely with data from new classes or tasks, resulting in a significant performance drop on old ones. We illustrate our new task and the catastrophic forgetting issue in Fig. 1. Unlike typical continual learning problems such as task-, domain-, or class-incremental classification in visual domains [2, 57, 38, 53, 85], which result in progressively increasing logits (or probability distribution) across all observed classes at each incremental step, our task uniquely produces fixed-size separation masks throughout all incremental steps. In this context, each entry in the mask does not directly correspond to any specific classes. Additionally, the new task involves both audio and visual modalities. Therefore, simply applying existing visual-only methods cannot fully exploit and preserve the inherent cross-modal semantic correlations. Very recently, Pian et al. [53] and Mo et al. [44] extended continual learning to the audio-visual domain, but both focused on classification tasks. To address these challenges, in this paper, we propose a novel approach named ContAV-Sep (Continual Audio-Visual Sound Separation). Upon the framework, we introduce a novel Cross-modal Similarity Distillation Constraint (CrossSDC) to not only maintain the cross-modal semantic similarity through incremental tasks but also preserve previously learned knowledge of semantic similarity in old models to counter catastrophic forgetting. The CrossSDC is a generic constraint that can be seamlessly integrated into the training process of different audio-visual sound separators. To evaluate the effectiveness of our proposed ContAV-Sep, we conducted experiments on the MUSIC-21 dataset within the framework of continual learning, using the state-of-the-art audio-visual sound separation model iQuery [14] and a representative audio-visual sound separation model Co-Separation [23], as our separation base models. Experiments demonstrate that ContAV-Sep can effectively mitigate catastrophic forgetting and achieve significantly better performance than other continual learning baselines. In summary, this paper contributes follows: (i) To explore more practical audio-visual sound separation, in which the separation model should be generalized to new sound source classes continually, we pose a Continual Audio-Visual Sound Separation task that trains the separation model under the setting of continual learning. To the best of our knowledge, this is the first work on continual learning for audio-visual sound separation. (ii) We propose ContAV-Sep for the new task. It uses a novel cross-modal similarity distillation constraint to preserve cross-modal semantic similarity knowledge from previously learned models. (iii) Experiments on the MUSIC-21 dataset can validate the effectiveness of our ContAV-Sep, demonstrating promising performance gain over baselines."
https://arxiv.org/html/2411.02858v1,OLAF: A Plug-and-Play Framework for Enhanced Multi-object Multi-part Scene Parsing,"Multi-object multi-part scene segmentation is a challenging task whose complexity scales exponentially with part granularity and number of scene objects. To address the task, we propose a plug-and-play approach termed OLAF. First, we augment the input (RGB) with channels containing object-based structural cues (fg/bg mask, boundary edge mask). We propose a weight adaptation technique which enables regular (RGB) pre-trained models to process the augmented (5-channel) input in a stable manner during optimization. In addition, we introduce an encoder module termed LDF to provide low-level dense feature guidance. This assists segmentation, particularly for smaller parts. OLAF enables significant mIoU gains of 3.33.3\mathbf{3.3}bold_3.3 (Pascal-Parts-58), 3.53.5\mathbf{3.5}bold_3.5 (Pascal-Parts-108) over the SOTA model. On the most challenging variant (Pascal-Parts-201), the gain is 4.04.0\mathbf{4.0}bold_4.0. Experimentally, we show that OLAF’s broad applicability enables gains across multiple architectures (CNN, U-Net, Transformer) and datasets. The code is available at olafseg.github.io","Figure 1: The recipe for OLAF, our plug-and-play framework for enhanced multi-object multi-part scene parsing: 1 Augment RGB input with object-based channels (fg/bg, boundary edges) obtained from frozen pre-trained models (MO,MEsubscript𝑀𝑂subscript𝑀𝐸M_{O},M_{E}italic_M start_POSTSUBSCRIPT italic_O end_POSTSUBSCRIPT , italic_M start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT) 2 Use Low-level Dense Feature guidance from segmentation encoder (LDF, shaded green) 3 Employ targeted weight adaptation for stable optimization with augmented input. We show that following this recipe leads to significant gains (up to 4.04.0\mathbf{4.0}bold_4.0 mIoU) across multiple architectures and across multiple challenging datasets. Figure 2: The segmentation results for state-of-the-art approach FLOAT [55] and its limitations can be seen in the second column. In the first row, FLOAT completely fails to identify TV Frame and TV Screen. In the second row, FLOAT fails to capture the edge partition between Car-Body, Car-Tire and also between Car-Body, Car-Window. The third column shows results by incorporating our plug-and-play approach OLAF into FLOAT, leading to significantly improved object and part segmentation results. Multi-object multi-part segmentation is a challenging task that involves simultaneously segmenting multiple objects in an image while also segmenting their individual parts. The task goes beyond conventional object segmentation [72, 33, 71, 4, 58, 10, 3, 73] and aims to enable multi-granular scene understanding. The availability of granular semantic detail is crucial for applications in robotics [65, 47], visual question answering [27], object interaction and modeling [16, 1] and other domains [2, 11, 15, 32] where understanding the scene in terms of objects and their constituent parts is crucial. Related approaches primarily address simpler variants such as single-object part parsing [38, 36, 37, 48, 22, 61] or part parsing for objects with fewer or visibly larger parts [24]. Some recent methods [55, 45, 74, 57] have been developed to specifically tackle the more complex task of multi-object multi-part parsing. However, these suffer from three significant limitations: Limitation 1: Foreground (union of object regions) is often incorrectly segmented, impacting the constituent part segmentation (Figure 2, first row). Limitation 2: Crucial boundary details between objects and parts are not captured accurately (Figure 2, second row). Limitation 3: Small and thin parts especially fail to be segmented (Figure 4, Figure 5). To address Limitation 1 & 2, we first obtain a plausible boundary edge mask using a pre-trained network. We use another pre-trained network to obtain preliminary object segmentation and combine the object label channels to obtain a binary foreground mask. These masks are included as additional channels to constitute the 5-channel (3 RGB + 2 masks) input to the reference segmentation network (see 1 in Figure 1). The masks provide an inductive bias and guide the model to focus on relevant parts from the onset of training. We also propose a weight adaptation technique that enables the pre-trained segmentation encoder to process the new 5-channel input without destabilizing optimization ( 3 in Figure 1). To address Limitation 3 (i.e. small and thin parts), we introduce an encoder module termed Low-level Dense Feature (LDF) - see 2 in Figure 1. This module, in conjunction with augmented input representation, provides low-level dense feature guidance enabling better segmentation, especially for small/thin parts. To summarize our contributions: • Input Augmentation: We introduce an augmented 5-channel input representation with auxiliary channels containing object and boundary cues. • Weight Adaptation Technique: We introduce a targeted weight-adaptation training procedure that ensures stable optimization of pre-trained backbones on the augmented (5-channel) input. • Low-Level Dense Feature Guidance (LDF): We propose a generic encoder module called LDF which provides valuable low-level dense feature guidance, especially for small part segmentation. • Performance Boost: OLAF achieves significant mIoU improvements, surpassing state-of-the-art by 3.33.3\mathbf{3.3}bold_3.3 on Pascal-Parts-58, 3.53.5\mathbf{3.5}bold_3.5 on Pascal-Parts-108, and 4.04.0\mathbf{4.0}bold_4.0 on Pascal-Parts-201. • Generalizability: We show that OLAF enhances performance across multiple representative segmentation families (CNN, U-Net, Transformer) and multiple datasets (Pascal-Parts 58/108/201 and PartImageNet), suggesting broad applicability as a plug-and-play framework."
https://arxiv.org/html/2411.02844v1,Correlation of Object Detection Performance with Visual Saliency and Depth Estimation,"As object detection techniques continue to evolve, understanding their relationships with complementary visual tasks becomes crucial for optimising model architectures and computational resources. This paper investigates the correlations between object detection accuracy and two fundamental visual tasks: depth prediction and visual saliency prediction. Through comprehensive experiments using state-of-the-art models (DeepGaze IIE, Depth Anything, DPT-Large, and Itti’s model) on COCO and Pascal VOC datasets, we find that visual saliency shows consistently stronger correlations with object detection accuracy (mAρ𝜌\rhoitalic_ρ up to 0.459 on Pascal VOC) compared to depth prediction (mAρ𝜌\rhoitalic_ρ up to 0.283). Our analysis reveals significant variations in these correlations across object categories, with larger objects showing correlation values up to three times higher than smaller objects. These findings suggest incorporating visual saliency features into object detection architectures could be more beneficial than depth information, particularly for specific object categories. The observed category-specific variations also provide insights for targeted feature engineering and dataset design improvements, potentially leading to more efficient and accurate object detection systems.","Despite the progress being made in computer vision, object detection remains a fundamental challenge, with current approaches achieving impressive accuracy but still facing limitations in complex scenarios [1, 2]. Human perception is influenced by various factors, leading to selective attention towards certain elements in our environment. The same trait is followed in machines, which also rely on mechanisms that prioritise specific aspects of images [3]. Whilst advances in deep learning have led to significant improvements, understanding how different visual tasks relate to and potentially enhance object detection performance remains crucial for further progress. The relationships between complementary tasks such as depth estimation and visual saliency prediction could provide valuable insights for improving detection systems. Recent works in computer vision have explored multi-task learning approaches, combining object detection with either depth estimation or saliency prediction [4]. However, these studies typically focus on end-to-end performance rather than analysing the underlying correlations between these tasks. Understanding these correlations provides opportunities for advancement. These include improvement of object detection architectures while also improving computational efficiency in the process. Moreover, it also informs researchers on how datasets can be designed to improve these computer vision tasks. The challenges of detecting objects across varying scales and contexts [5, 6] make it essential to understand which complementary tasks provide the most beneficial information for object detection. Additionally, such advancements are also important when looking at AI techniques from a sustainable perspective. This paper presents an investigation of the relationships between depth estimation and visual saliency. We present an examination of how each factor correlates with object detection performance. Through analysis across different datasets and models, we aim to explore whether significant correlations exist between these visual tasks and object detection accuracy. This investigation focuses particularly on how these correlations vary across different object categories and scales since it provides insights that could inform more effective object detection architectures. The implications of the work presented in this paper extend beyond a theoretical understanding of computer vision. They present practical perspectives for refining object detection systems. The quantification of correlations between these tasks provides empirical evidence for which auxiliary features might be most beneficial for enhancing detection performance. Furthermore, our analysis of category-specific variations offers insights for targeted improvements in both model architecture and dataset design."
https://arxiv.org/html/2411.02840v1,Test-Time Dynamic Image Fusion,"The inherent challenge of image fusion lies in capturing the correlation of multi-source images and comprehensively integrating effective information from different sources. Most existing techniques fail to perform dynamic image fusion while notably lacking theoretical guarantees, leading to potential deployment risks in this field. Is it possible to conduct dynamic image fusion with a clear theoretical justification? In this paper, we give our solution from a generalization perspective. We proceed to reveal the generalized form of image fusion and derive a new test-time dynamic image fusion paradigm. It provably reduces the upper bound of generalization error. Specifically, we decompose the fused image into multiple components corresponding to its source data. The decomposed components represent the effective information from the source data, thus the gap between them reflects the Relative Dominability (RD) of the uni-source data in constructing the fusion image. Theoretically, we prove that the key to reducing generalization error hinges on the negative correlation between the RD-based fusion weight and the uni-source reconstruction loss. Intuitively, RD dynamically highlights the dominant regions of each source and can be naturally converted to the corresponding fusion weight, achieving robust results. Extensive experiments and discussions with in-depth analysis on multiple benchmarks confirm our findings and superiority. Our code is available at https://github.com/Yinan-Xia/TTD.","Image fusion jointly integrates complementary information from multiple sources, aiming to generate informative and high-quality fused images. With superior scene representation and enhanced visual perception, image fusion significantly benefits downstream vision tasks [1, 2]. Typically, image fusion can be categorized into multi-modal, multi-exposure, and multi-focus image fusion tasks. Multi-modal image fusion encompasses Visible-Infrared image Fusion (VIF) and Medical Image Fusion (MIF). For VIF [3, 4, 5], infrared images effectively highlight thermal targets especially under extreme conditions, while visible images provide texture details and ambient lighting. For MIF [6, 7], different medical imaging modalities emphasize various focal areas, enhancing diagnostic capabilities. Multi-exposure image Fusion (MEF) [8, 9, 10] bridges the gap between high dynamic range (HDR) natural scenes and low dynamic range (LDR) pictures, ensuring better detail preservation in varying lighting conditions. Multi-Focus image Fusion (MFF) [11, 12, 13] aims to produce all-in-focus images by combining multiple images focused at different depths. Numerous image fusion methods have been introduced, which can be mainly grouped into traditional techniques and deep learning approaches. Traditional image fusion methods, such as multi-scale decomposition-based models [14, 15] and sparse representation-based methods [16], rely on mathematical transformations to fuse images in the transform domain [17]. In contrast, deep learning-based methods employ data-driven schemes to fuse multi-source images, including convolutional neural network (CNN) based methods [11, 18], generative adversarial network (GAN) based methods [19, 4], and transformer-based methods [20]. The effectiveness of image fusion algorithms hinges on two critical factors: feature extraction [21] and feature fusion [22]. The aforementioned methods strive to achieve high-quality fused images by learning effective uni-source or multi-source feature representations through complex network structures or feature decomposition schemes. However, they often overlook the complexity of the real world, which necessitates dynamic feature fusion. Figure 1: We visualized the Relative Dominablity (RD) of each source on four tasks, which effectively highlights the dominance of uni-source in image fusion. Recently, some works have highlighted the importance of dynamism in image fusion. For instance, [23] pioneered the combination of image fusion with a Mixture of Experts (MoE), dynamically extracting effective and comprehensive information from the respective modalities. [24] utilized task-specific routing networks to extract task-specific information from different sources with dynamic adapters. Despite their empirically superior fusion performance, these dynamic fusion rules mainly rely on heuristic designs, lacking theoretical guarantees and interpretability. Moreover, they potentially lead to unstable and unreliable fusion results, especially in complex scenarios. To address these issues, we reveal the generalized form of image fusion and propose a new Test-Time Dynamic(TTD) image fusion paradigm with a theoretical guarantee. Given that the fused image integrates comprehensive information from different sources, it can be obtained by weighting the effective representation of each uni-source. By revisiting the relationship between fusion weights and image fusion losses from the perspective of generalization error [25], we decompose the fused image into multiple uni-source components and formulate the generalization error upper bound of image fusion. Based on generalization theory, we for the first time prove that dynamic image fusion is superior to static image fusion. The key to enhancing generalization lies in the negative correlation between fusion weight and uni-source component reconstruction loss. As fusion models are trained to extract complementary information from each source, the decomposed components represent the effective information from the source data. Thus, the fusion components can be estimated by source data with the fusion model, the losses of which represent the deficiencies of the source in constructing fusion images. Accordingly, we derive a pixel-level Relative Dominablity (RD) as the dynamic fusion weight, which theoretically enhances the generalization of the image fusion model and dynamically highlights the changing dominant regions of different sources as shown in Fig. 1. Extensive experiments on multiple datasets and diverse image fusion tasks demonstrate our superiority. Overall, our contributions can be summarized as follows: • This paper first theoretically proves the superiority of dynamic image fusion over static image fusion and provides the generalization error upper bound of image fusion by decomposing the fusion image into uni-source components provably. The proposed generalization theory reveals that the key to reducing the upper bound lies in the negative covariance between the fusion weight and uni-source reconstruction loss. • We proposed a simple but effective test-time dynamic fusion paradigm based on the generalization theory. By taking the uni-source’s Relative Dominability as the dynamic fusion weight, we theoretically enhance the generalization of the image fusion model and dynamically emphasize the dominant regions of each source. Notably, our method does not require additional training, fine-tuning, and extra parameters. • We conduct extensive experiments on multi-modal, multi-exposure, and multi-focus datasets. The superior performance across diverse metrics demonstrates the effectiveness and applicability of our approach. Moreover, an additional exploration of the gradient in constructing fusion weight demonstrates the reasonability of our theory and its expandability."
https://arxiv.org/html/2411.02833v1,Lost in Context: The Influence of Context on Feature Attribution Methods for Object Recognition,"Contextual information plays a critical role in object recognition models within computer vision, where changes in context can significantly affect accuracy, underscoring models’ dependence on contextual cues. This study investigates how context manipulation influences both model accuracy and feature attribution, providing insights into the reliance of object recognition models on contextual information as understood through the lens of feature attribution methods.We employ a range of feature attribution techniques to decipher the reliance of deep neural networks on context in object recognition tasks. Using the ImageNet-9 and our curated ImageNet-CS datasets, we conduct experiments to evaluate the impact of contextual variations, analyzed through feature attribution methods. Our findings reveal several key insights: (a) Correctly classified images predominantly emphasize object volume attribution over context volume attribution. (b) The dependence on context remains relatively stable across different context modifications, irrespective of classification accuracy. (c) Context change exerts a more pronounced effect on model performance than Context perturbations. (d) Surprisingly, context attribution in ‘no-information’ scenarios is non-trivial. Our research moves beyond traditional methods by assessing the implications of broad-level modifications on object recognition, either in the object or its context. Code available at https://github.com/nineRishav/Lost-In-Context","In Computer Vision, Context refers to information or signals not part of an object’s appearance, including visual scenes and other objects. It encompasses various types of information, such as the presence and relationships of co-occurring objects, the overall scene type, ambient lighting conditions, and spatial arrangements, all of which contribute to a comprehensive understanding of the visual scene. Context plays a vital role in humans’ ability to comprehend scene information. In situations where something is unclear, humans rely on contextual cues to understand the scene, such as when driving in foggy or rainy conditions, where visibility is low, but contextual information like the road layout and surrounding vehicles helps navigate safely. \Description An image showing the predictions emphasizing contextual information with feature attribution highlighting the object in the image. Figure 1. The predictions emphasize contextual information, while feature attribution for the top predicted class highlights the object in the image. Predictions were generated using a pre-trained ResNet50 model, and feature attribution was performed with GradCAM. The visual context of an object is an important source of information for recognition tasks in both human and computer vision. Understanding the importance of context in computer vision has become a significant field of study. One of the primary tasks in computer vision is Object Recognition, where deep learning models tend to surpass human-level performance (He et al., 2015), making it an ideal area to study the effects of context as perceived by these models. With the introduction of Deep Neural Networks (DNNs) in computer vision, their utility has grown exponentially. DNN models have surpassed traditional methods in various applications, including autonomous vehicles (Huang et al., 2020; González et al., 2016), medical imaging (Litjens et al., 2017; Patrício et al., 2023), face recognition (Schroff et al., 2015), and motion detection (Wang et al., 2016). In most cases, DNN models are used as black box models (Adadi and Berrada, 2018), with users primarily concerned about their performance. However, understanding how or why a model provides certain outputs is crucial for critical applications, such as medical imaging (Samek et al., 2017). For example, in Figure 1 (top row), feature attribution focuses on the object, a sofa, but the model’s prediction is influenced by the context, i.e., lakeside, coast. These concerns have led to the introduction of explainability in machine learning and deep learning models. Many works (Adebayo et al., 2020; Srinivas and Fleuret, 2019; Yang and Kim, 2019; Alsallakh et al., 2021) have been conducted in the visual explanation domain for computer vision applications. These visual explanation methods provide the regions the model looks at to generate a particular output prediction. Some past works show the effect of context on the performance of models. For instance, Xiao et al. (Xiao et al., 2021) have shown how context, or image background, plays a major role in the performance of models. Many past works (Chen et al., 2016; Divvala et al., 2009; Galleguillos et al., 2008; Mottaghi et al., 2014; Wang and Zhu, 2023) have shown how context helps provide information in different ways to improve the model’s accuracy. This paper provides insights into the effect of contextual information on the model’s prediction from the perspective of feature attribution methods. We extend this view by proposing methods and metrics to understand context effects using explainability methods. Explainability methods allow us to identify important areas, making it natural to use these methods to understand better how models perceive contextual information (Patrício et al., 2023). We have tried to interpret context effects such as context change and context perturbation from an explainability point of view and provide deeper insights using ImageNet-9 and ImageNet-CS (Sec 4.1.2), where perturbation is added to the context. In this paper, we propose a simple metric to compute the importance of context as provided by popular feature attribution methods in the literature. The major contributions of this paper are: • We propose a novel approach to understand the effect of context on object recognition models through feature attribution methods. By leveraging feature attribution maps and segmentation maps, we introduce a new metric for quantitatively analyzing the impact of context (Section 4.2). • Our experimental results demonstrate that context change significantly affects model performance more than context perturbation (Section 5.1). Additionally, we show that models trained on larger datasets exhibit reduced reliance on contextual pixels (Section 5.2). • Our study on object size reveals that contrary to expectations, context attribution does not vary significantly between larger and smaller objects, indicating that object size alone does not predict context dependence. This challenges the assumption that more context inherently increases context reliance, highlighting the complexity of these interactions (Section 5.4). • We reveal that misclassification is closely linked to context change or perturbation. Furthermore, our experiments indicate that ‘no information’ contexts tend to attract higher attribution than expected, highlighting the nuances of context influence (Section 5.5)."
https://arxiv.org/html/2411.02818v1,LiVOS: Light Video Object Segmentation with Gated Linear Matching,"Semi-supervised video object segmentation (VOS) has been largely driven by space-time memory (STM) networks, which store past frame features in a spatiotemporal memory to segment the current frame via softmax attention. However, STM networks face memory limitations due to the quadratic complexity of softmax matching, restricting their applicability as video length and resolution increase. To address this, we propose LiVOS, a lightweight memory network that employs linear matching via linear attention, reformulating memory matching into a recurrent process that reduces the quadratic attention matrix to a constant-size, spatiotemporal-agnostic 2D state. To enhance selectivity, we introduce gated linear matching, where a data-dependent gate matrix is multiplied with the state matrix to control what information to retain or discard. Experiments on diverse benchmarks demonstrated the effectiveness of our method. It achieved 64.8 𝒥&ℱ𝒥ℱ\mathcal{J\&F}caligraphic_J & caligraphic_F on MOSE and 85.1 𝒥&ℱ𝒥ℱ\mathcal{J\&F}caligraphic_J & caligraphic_F on DAVIS, surpassing all non-STM methods and narrowing the gap with STM-based approaches. For longer and higher-resolution videos, it matched STM-based methods with 53% less GPU memory and supports 4096p inference on a 32G consumer-grade GPU–a previously cost-prohibitive capability–opening the door for long and high-resolution video foundation models.","Video object segmentation (VOS) involves separating specific objects from the background in a sequence of video frames. It plays a critical role in various real-world applications, including robotics [40], video editing [28], and medical imaging [44]. Based on the level of user input, VOS tasks are generally categorized into three settings: unsupervised, semi-supervised (or one-shot), and interactive [20]. In this work, we focus on the semi-supervised setting, where the segmentation of the first frame is provided, and the model is required to propagate this segmentation across subsequent frames. Figure 1: Top: Conceptual comparison of softmax vs. linear matching in video object segmentation. Bottom: Softmax matching suffers from memory constraints due to its quadratic attention complexity, while linear matching achieves linear growth with a constant-size state. Curves are based on results in Tab. 3. Figure 2: CPU latency comparison between softmax matching and linear matching. Softmax attention scales linearly over time (i.e., the number of memory frames T𝑇Titalic_T) and quadratically with input resolution H⁢W𝐻𝑊HWitalic_H italic_W. Latency is measured on an Intel Core-i7 (2.80GHz) CPU with PyTorch 2.0, batch size 1, and fp32. Semi-supervised VOS is primarily driven by space-time memory (STM) networks [35, 12, 10, 14, 42], which store past frames and their segmentations as keys and values in the memory bank to help segment the query frame. Segmentation occurs by matching each pixel in the query frame to all spatiotemporal pixels in the memory frames via softmax attention [48]. This softmax matching process is highly inefficient due to the large attention matrix with space and time complexities of 𝒪⁢(H⁢W×T⁢H⁢W)𝒪𝐻𝑊𝑇𝐻𝑊\mathcal{O}(HW\times THW)caligraphic_O ( italic_H italic_W × italic_T italic_H italic_W ), where H𝐻Hitalic_H and W𝑊Witalic_W indicate the spatial dimensions of an image and T𝑇Titalic_T is the number of memory frames. Here the number of objects is not considered for simplicity. As shown in Fig. 2, the CPU latency of softmax matching increases linearly over time and quadratically with spatial dimensions. This is problematic as video length or resolution increase, making computations too slow or causing out-of-memory issues (Fig. 1). To alleviate these issues, one could limit the number of memory frames or downsample the video’s spatial resolution. However, a fixed-size memory bank may lead to failures in challenging scenarios such as for occlusions or fast-moving objects [10]. Further, downsampling a video’s spatial resolution may lose fine details in object masks, as shown in Fig. 3, leading to inaccurate segmentations. Therefore, memory networks relying on softmax matching face significant challenges as video lengths and resolutions increase, limiting their applicability on resource-constrained devices. To address these challenges, strategies such as knowledge distillation [33], recurrent feature embedding [29], and spatiotemporal redundancy compressing [52] have been explored. However, these methods still rely on softmax matching, which we identify as the core memory bottleneck. We address this limitation by introducing linear matching, reformulating the memory matching process into a recurrent framework that reduces the large attention matrix to a constant-size 2D state, yielding space and time complexities of 𝒪⁢(T⁢H⁢W)𝒪𝑇𝐻𝑊\mathcal{O}(THW)caligraphic_O ( italic_T italic_H italic_W ). To enhance selectivity, we further introduce gated linear matching, where the state is multiplied by a data-dependent transition matrix [57] that determines what information to retain or discard. Additionally, we leverage lightweight sensory memory [10] and object memory [14] to improve performance (we claim no contribution for these components). We thus present LiVOS, the first light memory network that maintains constant memory usage for arbitrarily long videos and achieves linear memory growth as video resolution increases. Figure 3: Masks of thin structures at different resolutions. Thin structures may lose fine details at 480p, the standard resolution for VOS. However, existing memory networks relying on softmax matching are not efficient for processing high resolution videos. We evaluated LiVOS on a diverse range of video benchmarks, including both short- and long-term video datasets, as well as high-resolution videos. It achieved 64.8 𝒥&ℱ𝒥ℱ\mathcal{J\&F}caligraphic_J & caligraphic_F on MOSE and 85.1 𝒥&ℱ𝒥ℱ\mathcal{J\&F}caligraphic_J & caligraphic_F on DAVIS, surpassing all non-STM methods and narrowing the gap with STM-based approaches. For longer and higher-resolution videos, it matched STM-based methods with 53% less GPU memory and supports 4096p inference on a 32G consumer-grade GPU, which is not easily possible with softmax attention. While our method is not optimized for high-resolution videos, it paves the way for developing foundation models tailored to the high-resolution regime. In summary, we introduce a lightweight memory network that: 1) employs linear instead of softmax attention for memory matching, enabling efficient video object segmentation, 2) maintains constant memory usage regardless of video length, and 3) supports 4096p high-resolution inference on 32G consumer-grade hardware. Our method demonstrates competitive performance across various video benchmarks, offering computational efficiency compared to state-of-the-art methods."
https://arxiv.org/html/2411.02816v1,ChatGPT in Research and Education: Exploring Benefits and Threats,"In recent years, advanced artificial intelligence technologies, such as ChatGPT, have significantly impacted various fields, including education and research. Developed by OpenAI, ChatGPT is a powerful language model that presents numerous opportunities for students and educators. It offers personalized feedback, enhances accessibility, enables interactive conversations, assists with lesson preparation and evaluation, and introduces new methods for teaching complex subjects. However, ChatGPT also poses challenges to traditional education and research systems. These challenges include the risk of cheating on online exams, the generation of human-like text that may compromise academic integrity, a potential decline in critical thinking skills, and difficulties in assessing the reliability of information generated by AI. This study examines both the opportunities and challenges ChatGPT brings to education from the perspectives of students and educators. Specifically, it explores the role of ChatGPT in helping students develop their subjective skills. To demonstrate its effectiveness, we conducted several subjective experiments using ChatGPT, such as generating solutions from subjective problem descriptions. Additionally, surveys were conducted with students and teachers to gather insights into how ChatGPT supports subjective learning and teaching. The results and analysis of these surveys are presented to highlight the impact of ChatGPT in this context.","Artificial intelligence has rapidly transformed various sectors over the past few decades, with education being one of its most significant beneficiaries. OpenAI’s ChatGPT, a conversational AI model, has emerged as a key tool with the potential to revolutionize education, particularly in developing countries [1]. By utilizing natural language processing, ChatGPT generates human-like responses, offering innovative ways to provide educational support and create guidelines. According to the World Bank (2024) [2], developing countries face significant challenges, including limited resources and inadequate infrastructure. The lack of access to quality education and trained teachers are major contributors to these challenges. ChatGPT can be crucial in bridging the educational gap by expanding learning opportunities and improving access to quality educational materials in underserved areas. In developing countries, ChatGPT presents significant potential for enhancing education for both learners and teachers. By analyzing large volumes of academic data, ChatGPT identifies trends and gaps that can help policymakers make informed, evidence-based decisions [3]. This technology addresses challenges like instant feedback, personalized learning, and academic support, making education more accessible, especially for those with geographic or economic barriers [4]. Unlike traditional classrooms, where one teacher handles many students, ChatGPT offers tailored content that adapts to each learner’s pace and style [5]. For teachers, it automates tasks, generates materials, and provides additional tutoring support, allowing them to focus on individualized instruction [5]. ChatGPT’s reach also extends to educational policy and guideline development, while its multilingual features enable learning in preferred languages, which is crucial for countries with diverse linguistic backgrounds. However, ethical and practical concerns must be addressed for effective implementation [6]. To maximize ChatGPT’s impact on education, developing countries need to address data privacy, digital access, and teacher training. Privacy measures are essential to protect student data, and affordable internet access and devices are crucial for equitable participation [7]. Public-private partnerships can help expand digital infrastructure, and teachers require training to use AI tools like ChatGPT effectively [8]. Addressing these challenges can enhance education across all levels, supporting both educational and economic growth [1]). Over the past year, many researchers have analyzed ChatGPT usage among students, teachers, and educators. Rahman et al. conducted a detailed study on ChatGPT’s role in education, highlighting its use in personalized feedback, accessibility, interactive learning, lesson preparation, and teaching complex concepts, particularly in programming [9]. They also identified risks, such as cheating, reduced critical thinking, and challenges in evaluating AI-generated content. The study tested ChatGPT’s programming assistance through code generation, pseudocode, and code correction, which were validated with an online judge system. Additionally, surveys gathered insights from students and teachers on ChatGPT’s effectiveness. Similarly, Lo et al. explored ChatGPT’s early challenges within the first three months of release [10]. Recent research also examines how students use ChatGPT to support learning [11, 12, 13, 14, 15, 16]. Studies focusing on educators reveal ChatGPT’s potential in supporting teaching routines and material preparation [17, 18, 19, 20, 2]. While ChatGPT offers numerous benefits for learners and educators, including enhanced learning experiences and streamlined content creation, it also presents challenges. A key concern is the risk of misinformation and bias in AI-generated content. Recent studies highlight issues like potential plagiarism and the spread of inaccuracies [21, 19, 22, 23, 24, 15, 25, 26, 13]. Although these studies provide insight into ChatGPT’s limitations, they often lack detailed analysis tailored to various educational contexts. Research has also examined ChatGPT’s use in specific subject areas, such as Economics [27], English Language [28], Law [15], Sports Science [25], Medical Education [29], Higher-Order Thinking [23], Mathematics [30], Programming [31, 9], and Software Testing [32]. However, there is a gap in understanding ChatGPT’s role in programming and engineering-related subjects. Little research has focused on how engineering students specifically use ChatGPT, their motivations, or the reliability of the AI content they rely on. Addressing this gap is crucial to better understand and support the unique needs of engineering students. To address the challenges and gaps identified in prior research, our study provides an in-depth analysis of ChatGPT’s use among engineering students and educators, examining how they can use the tool, their motivations, and their views on its reliability. This focused investigation sheds light on ChatGPT’s unique role in engineering education, enriching our understanding of its impact in this field. Additionally, we emphasize the strategic integration of ChatGPT in education, particularly in emerging and developing countries, by analyzing practical applications and their implications. Our key contributions are as follows: • First, we provide a thorough analysis of how ChatGPT is being deployed in these education systems, highlighting its potential benefits, such as enhancing access to and quality of education, while also addressing the associated risks and ethical considerations. Our work goes beyond the scope of existing studies by offering concrete solutions to the challenges they identified. • We collected and analyzed data from engineering students and educators to understand their specific use cases, motivations for using ChatGPT, and their perceptions of its reliability. This approach helps identify usage patterns and concerns unique to the engineering domain. Through real-world analysis, we provide actionable insights and practical recommendations for learners and educators, addressing ChatGPT-specific challenges in this field. Our dataset included responses from engineering students and teachers, with 15 survey questions covering ChatGPT usage in academic tasks such as research, problem-solving, programming, and essay writing. The survey aimed to evaluate the perceived benefits, challenges, and overall effectiveness of ChatGPT in education. Statistical analysis of the responses offers insights that can guide ChatGPT users and inform best practices for its application across various educational sectors. • Our findings aim to guide policymakers, educators, and stakeholders in effectively integrating ChatGPT into their educational frameworks, maximizing its benefits while minimizing potential drawbacks. By shedding light on these critical aspects, our study not only supports educational advancement but also fosters a more informed and strategic approach to leveraging AI tools in the engineering education sector."
https://arxiv.org/html/2411.02799v1,ERUP-YOLO: Enhancing Object Detection Robustness for Adverse Weather Condition by Unified Image-Adaptive Processing,"We propose an image-adaptive object detection method for adverse weather conditions such as fog and low-light. Our framework employs differentiable preprocessing filters to perform image enhancement suitable for later-stage object detections. Our framework introduces two differentiable filters: a Bézier curve-based pixel-wise (BPW) filter and a kernel-based local (KBL) filter. These filters unify the functions of classical image processing filters and improve performance of object detection. We also propose a domain-agnostic data augmentation strategy using the BPW filter. Our method does not require data-specific customization of the filter combinations, parameter ranges, and data augmentation. We evaluate our proposed approach, called Enhanced Robustness by Unified Image Processing (ERUP)-YOLO, by applying it to the YOLOv3 detector. Experiments on adverse weather datasets demonstrate that our proposed filters match or exceed the expressiveness of conventional methods and our ERUP-YOLO achieved superior performance in a wide range of adverse weather conditions, including fog and low-light conditions.","(a) Adverse weather images (b) Proposed method Figure 1: Failure cases of the conventional YOLOv3 detector on adverse weather images (left side) such as fog, rain, low-light and snow conditions, and the successful detection results after applying our proposed image processing filters (right side). Machine learning-based object detection methods [24, 25, 26, 7, 16, 28, 17, 1, 30, 31] are typically trained on images captured under normal weather and well-exposed conditions, and have achieved higher performance. However, their performance is degraded significantly when these detectors are applied to adverse weather scenarios such as foggy or low-light conditions. The adverse weather conditions cause domain shifts between the training and testing distributions. To mitigate these domain shifts, several approaches have been proposed. Some methods attempt to reduce the impact of weather by reconstructing the input image with defogging [4, 19], or low-light enhancement [6]. Another approach[3, 9, 27] treats adverse weather as a domain adaptation problem, aiming to align the feature distributions of normal and adverse condition domains. Furthermore, an emerging approach called image adaptive object detection that combines image processing and domain adaptation has been proposed. The image-adaptive methods [18, 12] integrate differentiable image processing filters into the object detection pipeline. These methods estimate the parameters of classical image processing filters such as white balance, gamma correction, tonemapping, contrast enhancement and sharpening from the input image using a Convolutional neural network (CNN) model. The classical filters with the estimated parameters are then applied to the input image as the preprocessing before feeding it to the object detector. While these image adaptive method shows promising results, these still have some problems regarding complexity of filter representations. The complexity includes combinations, order, and various parameter range of multiple filters. This complexity leads to redundant filter processing and requires manual customization of filter combinations for each adverse weather condition. As the number of filters increases, the complexity of such customization grows exponentially. Therefore, a simplified representation of preprocessing filters is crucial for achieving an efficient and customization-free image adaptive preprocessor. This paper proposes a novel image-adaptive object detection method with more simple and customization-free image processing filters. We reconsidered the role of the classical preprocessing filters, and assumed that these filters boosted object detection performance based global and local enhancement. Based on this assumption, we unify the classical filters, and represent these filters only by two simple and differentiable filters. Our contributions are as follows: • We propose an image-adaptive object detection method, called Enhanced Robustness by Unified Image Processing (ERUP)-YOLO. Our key idea is to unify and generalize classical image processing filters into two simple and differentiable filters: a Bézier curve-based pixel-wise (BPW) filter and a kernel-based local (KBL) filter. • Our method does not require data-specific customization of the filter combinations, parameter ranges. • Using our BPW filter, we additionally propose a domain-agnostic data augmentation, which can be universally applied to adverse weather conditions. We demonstrate that our proposed BPW and KBL filters match or exceed the expressiveness of the classical filters. We also show that ERUP-YOLO achieves the highest object detection accuracy based on YOLOv3 compared to state-of-the-art image adaptive methods on adverse weather datasets."
https://arxiv.org/html/2411.02794v1,"Real-Time Text Detection with Similar Mask in Traffic,
Industrial, and Natural Scenes","Texts on the intelligent transportation scene include mass information. Fully harnessing this information is one of the critical drivers for advancing intelligent transportation. Unlike the general scene, detecting text in transportation has extra demand, such as a fast inference speed, except for high accuracy. Most existing real-time text detection methods are based on the shrink mask, which loses some geometry semantic information and needs complex post-processing. In addition, the previous method usually focuses on correct output, which ignores feature correction and lacks guidance during the intermediate process. To this end, we propose an efficient multi-scene text detector that contains an effective text representation similar mask (SM) and a feature correction module (FCM). Unlike previous methods, the former aims to preserve the geometric information of the instances as much as possible. Its post-progressing saves 50%percent\%% of the time, accurately and efficiently reconstructing text contours. The latter encourages false positive features to move away from the positive feature center, optimizing the predictions from the feature level. Some ablation studies demonstrate the efficiency of the SM and the effectiveness of the FCM. Moreover, the deficiency of existing traffic datasets (such as the low-quality annotation or closed source data unavailability) motivated us to collect and annotate a traffic text dataset, which introduces motion blur. In addition, to validate the scene robustness of the SM-Net, we conduct experiments on traffic, industrial, and natural scene datasets. Extensive experiments verify it achieves (SOTA) performance on several benchmarks. The code and dataset are available at: https://github.com/fengmulin/SMNet.","Scene text detection is a foundational task in computer vision, focused on pinpointing text occurrences. While text detection and recognition in documents have advanced significantly, scene texts present challenges due to their diverse attributes like colors, sizes, fonts, shapes, orientations, and complex backgrounds. It is an essential prerequisite for miscellaneous applications, such as automatic driving, scene analysis [1], and intelligent transportation, so it attracts great interest in intelligent transportation communities. Figure 1: Texts in different scenes. Texts in the natural scene enjoy a complex background. In the industrial scene, low visual contrast and corroded surfaces make the difficult to detect text accurately. Motion blur and changes in weather and lighting are the main challenges for traffic scenes. Recently, scene text detection has made great strides due to the rapid advancements in semantic segmentation, instance segmentation, and object detection. However, numerous existing methods ignore some particular scenes, such as transportation and industrial manufacturing, warranting further exploration. Specifically, detecting text in industrial and traffic scenes proves more challenging than in natural scenes. Industrial scenes enjoy complex backgrounds, low visual contrast, and corroded surfaces. Weather, light, and motion blur influence traffic scene text detection more than other scenes. Some images from different scenes are visualized in Fig. 1. Moreover, due to their unique characteristics, industrial and traffic scenes demand both high accuracy and rapid inference speed, compared to general scenes. The other challenge is that there are relatively few datasets and studies in these fields. For the industrial scene, a challenging industrial text detection dataset and corresponding synthetic datasets [2] are proposed. For the intelligent transportation scene, there is a dataset [3] that only the test set is available, and a no-publicized transportation benchmark [4]. To further promote the development of transportation text detection, we establish a new dataset that considers the specific motion blur on traffic scenes, which includes 1,000 training images and 528 testing images. The proposed motion blur traffic scene text (MBTST) dataset encompasses various challenges found in traffic scenes and simulates the real world as much as possible. Apart from accuracy, high efficiency is crucial for transportation and industrial scene text detection, making segmentation-based methods the preferred option. The majority of these methods rely on predicting shrink masks and extending them to reconstruct text instances. The shrink mask is generated by shrinking the text region inward by a distance, which is computed by the area and perimeter of the instance. Specificially, PSENet [5] predicts different scale shrink masks and utilizes a progressive scale expansion algorithm to recover text contour, which is accurate but inefficient. Based on it, PAN [6] proposes a pixel aggregation method to speed up the post-processing. DBNet [7] introduces the Vatti clipping algorithm [8] to further simplify the post-processing. ADNet [9] and RSMTD [10] utilize the prediction of the neural network to replace the computing of the expansion distance. However, this shrinking method possesses two disadvantages: 1) It reconstructs text instances requiring complex operations like instance-level extendtion, area, and perimeter calculations. 2) It is an artificially defined concept with ambiguous definitions that humans cannot recognize its edge accurately, let alone models. Compared to the text region enjoys complete semantic features, it loses part geometry features associated with the instance contour. To address these limitations, we propose a new shrink calculation method coupled with efficient post-processing. Compared with the previous methods, it has several dominances: 1) Its simplistic post-progressing significantly improves overall efficiency. 2) Furthermore, it maximally preserves the geometric features of the text contour, which helps the model accurately recover them. The previous methods solely guide and adjust the model based on the gap between output and ground truth, lacking sufficient guidance for the intermediate process. The effective transfer of knowledge between input and output proves challenging due to the numerous interspersed modules. This process is similar to the teacher explaining challenging questions to students. If only the answer is provided to students without guidance during the intermediate steps, it may reduce students’ generalization ability. Specifically, when faced with a difficult problem, if the teacher only tells the correct result to the student with no intermediate steps, the student can only cope well with this kind of problem and not with similar ones. Inspired by this, to address this issue, we introduce a feature correction module that enhances the model at the feature level. This module encourages false positive features to keep away true positive ones, aiding the model in better distinguishing foreground and background, thereby significantly enhancing detection performance. Based on the above method, an effective and efficient multiple-scene text detector, which achieves SOTA performance on multiple benchmarks, is proposed. The main contributions of this paper are as follows: 1. A new text representation called similar mask (SM) is proposed, which retains geometry shape information and demonstrates greater robustness compared to prior methods. Furthermore, it incorporates an extremely simplified post-processing step, reducing computation on the CPU, and enhancing detection efficiency. 2. A feature correction module (FCM) is proposed, which guides the model in the intermediate process to suppress false positive predictions further. It helps the module distinguish foreground and background to improve detection performance without extra computation. 3. An efficient text detector SM-Net is proposed, which achieves SOTA performance across multiple datasets encompassing transportation, industrial, and various scenes. Moreover, this method successfully strikes a trade-off between accuracy and efficiency, enabling its seamless application in real-world scenarios. 4. We establish a transportation text dataset featuring diverse weather and illumination backgrounds. To further simulate real scenarios, we introduce motion blur to the images. In contrast to existing traffic text datasets, this incorporation of motion blur better mirrors the performance of various methods in real-world scenarios, significantly heightening the challenge. The remainder of this paper is structured as follows. Section II provides a review of related work on text detection in natural, industrial, and traffic scenes. In section III, we describe the established traffic scene text dataset in detail. We illustrate the proposed method and the used loss function in section IV. A series of experiments are performed in section V to demonstrate the superiority and effectiveness of the SM-Net. This paper is concluded in section VI."
https://arxiv.org/html/2411.02753v1,Label Critic: Design Data Before Models,"As medical datasets rapidly expand, creating detailed annotations of different body structures becomes increasingly expensive and time-consuming. We consider that requesting radiologists to create detailed annotations is unnecessarily burdensome and that pre-existing AI models can largely automate this process. Following the spirit don’t use a sledgehammer on a nut, we find that, rather than creating annotations from scratch, radiologists only have to review and edit errors if the Best-AI Labels have mistakes. To obtain the Best-AI Labels among multiple AI Labels, we developed an automatic tool, called Label Critic, that can assess label quality through tireless pairwise comparisons. Extensive experiments demonstrate that, when incorporated with our developed Image-Prompt pairs, pre-existing Large Vision-Language Models (LVLM), trained on natural images and texts, achieve 96.5% accuracy when choosing the best label in a pair-wise comparison, without extra fine-tuning. By transforming the manual annotation task (30–60 min/scan) into an automatic comparison task (15 sec/scan), we effectively reduce the manual efforts required from radiologists by an order of magnitude. When the Best-AI Labels are sufficiently accurate (81% depending on body structures), they will be directly adopted as the gold-standard annotations for the dataset, with lower-quality AI Labels automatically discarded. Label Critic can also check the label quality of a single AI Label with 71.8% accuracy when no alternatives are available for comparison, prompting radiologists to review and edit if the estimated quality is low (19% depending on body structures).","Publicly available abdominal CT datasets with per-voxel annotations have experienced rapid growth in recent years [1, 2]. In 2020, datasets like KiTS [3] and LiTS [4] offered a few hundred annotated CT scans. By 2023, datasets such as AbdomenAtlas [5] and FLARE [6] expanded these scans significantly, now exceeding 10,000 annotated scans. This growth is enabled by AI-assisted annotation, where AI performs the initial segmentation and radiologists review and edit errors made by AI [7, 8]. Despite AI assistance, the current scale—now with tens of thousands of annotations per dataset [9]—has made manual detection and editing of label errors increasingly impractical. This raises the question: Rather than having radiologists detect and edit AI errors, can we—again—use AI to automate these tasks and scale medical datasets? Automatic error detection is achievable most label errors in existing datasets because, simply put, critiquing is easier than creating. This paper builds on two main insights. First, most errors made by AI are easy to detect111A common AI error in abdominal CT scans is mislabeling the aortic arch. This error is obvious, as the aorta should appear curved in its top, forming an arch (see Fig. 1). Even non-experts can easily recognize such errors due to the aorta’s consistent size, position, and appearance across scans. and do not require the time and expertise of busy, costly radiologists. Second, when multiple labels are available222The number of public AI models quickly raises [10]. Medical segmentation benchmarks provide diverse datasets, where participants train different architectures, providing a variety of labels for Label Critic to choose from. E.g., for abdominal organ segmentation in CT, we easily find solutions to the FLARE challenge [11, 12, 13, 14], 11 models trained on AbdomenAtlas are already public, and more will be released after Touchstone Benchmark [9]., comparing them to identify the highest-quality label is even simpler. We discover that general-purpose Large Vision Language Models (LVLMs), like Llava and GPT-4V [15, 16], trained on massive text-image datasets, can detect errors in medical datasets and compare the label quality among multiple label options without additional fine-tuning. We present a new LVLM-based pipeline, Label Critic, which can effectively (1) detect a large portion (76.8%) of the obvious label errors in existing medical datasets and (2) select the Best-AI Label by comparing multiple AI Labels. We show that Label Critic can generalize to over 10,000 CT scans across 89 hospitals with minimal or no training data (≤\leq≤ 10). It detects 1,441 errors in the datasets, with overall accuracy of 96.5% in detecting label errors and identify the Best-AI Labels in a pair-wise comparison. The success of Label Critic is attributed to our innovative Input and Prompt designs specialized for 3D CT scans and the integration of prior knowledge about body structures. First, we design new inputs for LVLMs. Since most LVLMs are designed for 2D inputs, Label Critic uses 2D frontal projections of CT scans with transparent overlays of label projections, ensuring computational efficiency while preserving key volumetric information (§2.2). The projections resemble antero-posterior (AP) X-rays, making them familiar to general-purpose LVLMs. Second, we design new prompts for LVLMs. They incorporate step-by-step guidance, anatomical descriptions, Dual Confirmation, and variable examples ranging from zero-shot to in-context learning with up to 10 label examples (§2.3). This flexibility enables Label Critic to adapt quickly to new hospitals and segmentation classes, requiring few or no training samples, while avoiding overfitting to specific label error types (§2.3, §2.1). This is the first work to show LVLMs can compare semantic segmentations, using prior knowledge to choose the best AI model for each case and class. Related Work. Label quality control methods identify potential label errors by flagging uncertainty and inconsistency across AI models [17, 18, 19, 20, 21], but they do not specify which label is better, leaving radiologists to review each flagged case manually. In our dataset, this approach requires manual review of 4,348 labels across two AI models, a time-intensive task. Most existing QC methods are organ-specific (e.g., cardiac or muscle imaging [22, 23]), limiting their scalability to other body structures. There is no prior methods leverage large vision-language models (LVLMs) for label quality control. Our LVLM-based method can significantly reduce manual workload for multi-organ segmentation333Spleen, gallbladder, pancreas, postcava, aorta, kidneys, spleen, and liver. by comparing and selecting the best labels, discarding incorrect ones, and flagging only the most challenging cases for further manual review, streamlining the process efficiently. Fig. 1: (a) Public CT datasets with per-voxel labels are rapidly expanding, largely due to AI-assisted labeling. However, AI often makes obvious errors, exampled in the liver, IVC, and kidneys, highlighting the need for efficient, automated error detection. (b) Label Critic pipeline for comparing labels. (I) Frontally project (§2.2) the CT scan and overlay it with the projections of two candidate labels (red), y1subscript𝑦1y_{1}italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and y2subscript𝑦2y_{2}italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, creating two images; (II) verify the dice score (DSC) between the 2 label projections, skip the comparison if DSC is above a class-specific threshold—avoiding comparing overly similar labels; (III) ask a LVLM (§2.1) to compare the labels and choose the most correct. If y1subscript𝑦1y_{1}italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT is a dataset label we are evaluating, we consider it wrong if the LVLM prefers y2subscript𝑦2y_{2}italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, the output of an alternative public segmentation model. (c) 3-Step Prompt Design. Prompt 1 asks if the target organ should be in the CT, providing a skeleton projection as reference. If the LVLM says no, we select an empty label (if available) or flag the case for review. Otherwise, Prompt 2 asks the LVLM to compare two label overlays using class-aware prompts with anatomical guidance, optional in-context learning, and complexity based on the LVLM’s background knowledge of each class (§2.3). Prompt 3 asks the LVLM to summarizes its previous answer. Summarization provides an easily processable binary answer, but allows detailed justifications and step-by-step reasoning in earlier steps."
https://arxiv.org/html/2411.02747v1,Efficient Feature Aggregation and Scale-Aware Regression for Monocular 3D Object Detection,"Monocular 3D object detection has attracted great attention due to simplicity and low cost. Existing methods typically follow conventional 2D detection paradigms, first locating object centers and then predicting 3D attributes via neighboring features. However, these methods predominantly rely on progressive cross-scale feature aggregation and focus solely on local information, which may result in a lack of global awareness and the omission of small-scale objects. In addition, due to large variation in object scales across different scenes and depths, inaccurate receptive fields often lead to background noise and degraded feature representation. To address these issues, we introduces MonoASRH, a novel monocular 3D detection framework composed of Efficient Hybrid Feature Aggregation Module (EH-FAM) and Adaptive Scale-Aware 3D Regression Head (ASRH). Specifically, EH-FAM employs multi-head attention with a global receptive field to extract semantic features for small-scale objects and leverages lightweight convolutional modules to efficiently aggregate visual features across different scales. The ASRH encodes 2D bounding box dimensions and then fuses scale features with the semantic features aggregated by EH-FAM through a scale-semantic feature fusion module. The scale-semantic feature fusion module guides ASRH in learning dynamic receptive field offsets, incorporating scale priors into 3D position prediction for better scale-awareness. Extensive experiments on the KITTI and Waymo datasets demonstrate that MonoASRH achieves state-of-the-art performance.","In recent years, 3D object detection has emerged as a pivotal area of research, particularly driven by its critical role in autonomous systems, robotics, and augmented reality. The ability to accurately detect and localize objects in three-dimensional space is essential for applications that require a high degree of spatial awareness, especially for autonomous driving systems[1],[2]. Advancements in sensors such as LiDAR, radar, and stereo cameras have enhanced 3D object detection with point cloud data, radar signals, and depth maps. However, LiDAR’s high-resolution point clouds[3],[4],[5],[6] and stereo methods’ depth maps[7],[8] require additional calibration. CAD[9],[10] and multi-frame[11] approaches improve shape and movement tracking but are also computationally intensive. Compared to the above methods, monocular vision systems offer a more cost-effective and deployable solution, though they struggle with accurately extracting 3D information from 2D images due to the absence of direct depth perception. Therefore, current advancements in monocular 3D detection have focused on improving depth estimation accuracy. Methods like[12],[13],[14],[15],[16],[17] following the CenterNet[18] paradigm, use direct regression to estimate the 3D center depth of the target object.[9],[19],[20],[21],[22] incorporates geometric constraints into depth estimation to enhance target depth recovery.[23],[24],[25],[26],[27] effectively integrate geometric depth with regressed depth, improving the network’s generalization ability. However, these approaches often decouple 2D and 3D feature regression, predicting 2D attributes (x,y,w,h)𝑥𝑦𝑤ℎ\left({x,y,w,h}\right)( italic_x , italic_y , italic_w , italic_h ) and 3D attributes (x,y,z,w,h,l,y⁢a⁢w)𝑥𝑦𝑧𝑤ℎ𝑙𝑦𝑎𝑤\left({x,y,z,w,h,l,yaw}\right)( italic_x , italic_y , italic_z , italic_w , italic_h , italic_l , italic_y italic_a italic_w ) independently. This separation overlooks the potential relationship between 2D priors and the 3D position of objects. Briefly, distant objects often occupy small areas in images, while nearby objects dominate, leading to challenges in extracting relevant features using fixed receptive fields. This issue is evident in the contrast between the attention heatmaps of the networks shown in Fig. 1. It can be seen that DEVIANT[22] and MonoLSS[17] struggle to focus on smaller, distant objects, particularly in the pedestrian category. Moreover, DEVIANT[22] tends to inadvertently attend to irrelevant background noise. Some previous works attempted to address this issue. Chen et al. proposed a shape-aware auxiliary training task[28]. [29] integrated deformable convolutions[30] to enhance the model’s adaptability to features. Although these methods achieve dynamic receptive field adjustment to some extent, they have not explicitly considered object scale within a scene, nor dynamically adjusted the model’s attention based on different scales. (a) DEVIANT (b) MonoLSS (c) MonoASRH Figure 1: Visualization of attention heatmaps for different models: (a) DEVIANT[22], (b) MonoLSS[17], and (c) Our proposed MonoASRH. Previous methods struggle to effectively capture distant and occluded objects due to fixed receptive fields. In contrast, our MonoASRH overcomes these limitations by dynamically adjusting attention based on different scales, including for the car vs. pedestrian category. To address the aforementioned limitations, this paper proposes the novel Efficient Hybrid Feature Aggregation Module (EH-FAM) and Adaptive Scale-Aware 3D Regression Head (ASRH). Inspired by RT-DETR’s[31] success in 2D detection, EH-FAM integrates Vision Transformers with CNNs to efficiently aggregate visual features, significantly reducing model complexity compared to traditional methods like DLAUp[32]. The EH-FAM employs self-attention based within-scale feature interactions on the highest-level features, effectively capturing the semantic information of small-scale objects. Convolutional operations are then utilized for feature fusion, providing a more fine-grained cross-scale feature representation. Subsequently, ASRH encodes 2D bounding box dimensions to capture scale features, which are fused with semantic features extracted from the cross-scale feature through a specially designed scale-semantic feature fusion module. The fused features guide the learning of receptive field offsets, which are then applied to deformable convolutions, enhancing the model’s ability to adapt to varying object scales. Furthermore, ASRH utilizes a spatial variance-based attention mechanism to adaptively differentiate foreground objects from noise within semantic features. Additionally, we introduce the Selective Confidence-Guided Heatmap Loss to facilitate ASRH to prioritize high confidence detections and mitigates the impact of hard samples. Our contributions can be summarized as follows: • We introduce a plug-and-play module called Efficient Hybrid Feature Aggregation Module, which is designed for efficient cross-scale feature aggregation. • We propose a novel Adaptive Scale-Aware 3D Regression Head, which dynamically adjusts the network’s receptive field according to object scale. By leveraging 2D prior information, ASRH facilitates 3D bounding box regression. • Extensive experiments on the KITTI 3D object detection benchmark[33] and Waymo Open dataset[34] show that our MonoASRH achieves better results compared to previous state-of-the-art methods."
https://arxiv.org/html/2411.02733v1,DDFAV: Remote Sensing Large Vision Language Models Dataset and Evaluation Benchmark,"With the rapid development of large vision language models (LVLMs), these models have shown excellent results in various multimodal tasks. Since LVLMs are prone to hallucinations and there are currently few datasets and evaluation methods specifically designed for remote sensing, their performance is typically poor when applied to remote sensing tasks. To address these issues, this paper introduces a high-quality remote sensing LVLMs dataset, DDFAV, created using data augmentation and data mixing strategies. Next, a training instruction set is produced based on some high-quality remote sensing images selected from the proposed dataset. Finally, we develop a remote sensing LVLMs hallucination evaluation method RSPOPE based on the proposed dataset and evaluate the zero-shot capabilities of different LVLMs. Our proposed dataset, instruction set, and evaluation method files are available at https://github.com/HaodongLi2024/rspope.","Remote sensing technology uses image data acquired from high altitudes to analyze and understand various phenomena on the Earth’s surface. It is widely applied in fields such as land cover classification, disaster monitoring, and environmental protection. Recently, with the successful application of large vision language models (LVLMs) in natural scenes, researchers have begun to introduce these models into the field of remote sensing to achieve breakthroughs in tasks like image caption generation [35], scene classification [14], and complex reasoning [27]. However, general LVLMs designed for natural scenes tend to produce many hallucinations [7, 1, 8, 21, 16, 28, 9, 30, 33], and there are significant differences between remote sensing images and natural images. These differences are evident not only in the complexity of image content and target size but also in the requirement for complex spatial reasoning from a unique bird’s-eye perspective in remote sensing images. These factors often lead LVLMs to produce hallucinations in remote sensing tasks, as shown in Fig 1, revealing the limitations of existing remote sensing vision-language models and datasets. Figure 1: Comparison of hallucination phenomena of general LVLMs in image captioning tasks for remote sensing images. Most existing remote sensing LVLMs datasets are either restricted to a single task [34, 31] or lack diversity and detail [18], limiting the models’ generalization and multi-task processing capabilities. In addition, the annotation quality of many datasets is inconsistent, there is a lack of complex scene reasoning data [20], and the image caption instruction set caption is too short [32], which leads to significant bias or errors in the training models when processing diverse remote sensing images. Therefore, there is an urgent need for a high-quality remote sensing visual language dataset that covers a wider range of scenes, perspectives, and categories and can perform multiple tasks, ranging from simple image description to complex reasoning. Figure 2: The number of object categories and source distribution of our proposed remote sensing LVLMs dataset DDFAV. Current evaluation methods for remote sensing LVLMs also have notable shortcomings. For example, in image captioning tasks, most evaluation methods focus on the overall similarity between the generated text and the reference text, while overlooking the ability of LVLMs to accurately recognize specific objects in remote sensing images, such as small or overlapping objects. Metrics like BLEU [19] and METEOR [2] emphasize n-gram overlap between generated and reference captions, ROUGE-L [13] focuses on the longest common subsequence between texts, and CIDEr-D [24] measures consistency across multiple reference captions. For visual question answering tasks, methods such as RSVQA [17] include four types of questions: existence, comparison, rural/urban classification, and counting, seemingly providing a comprehensive evaluation of remote sensing LVLMs. However, the subjectivity and variability introduced by open-ended questions present a significant challenge for achieving consistent and reliable evaluation results. The main contributions of this paper can be summarized as follows: • We analyze the shortcomings of existing remote sensing LVLMs on dedicated datasets and propose a multi-category, multi-view, and more uniformly scaled remote sensing LVLMs dataset, DDFAV, created using data enhancement and data mixing techniques. • Using the proposed DDFAV dataset, we select a subset to create an instruction set for training remote sensing LVLMs, covering tasks such as image captioning, visual question answering, and complex reasoning. • We develop an RSPOPE evaluation method, based on POPE [12], for current remote sensing LVLMs using the proposed DDFAV dataset and employ mainstream LVLMs to evaluate their zero-shot remote sensing image recognition capabilities."
https://arxiv.org/html/2411.02715v1,"CIT: Rethinking Class-incremental Semantic Segmentation with a
Class Independent Transformation","Class-incremental semantic segmentation (CSS) requires that a model learn to segment new classes without forgetting how to segment previous ones: this is typically achieved by distilling the current knowledge and incorporating the latest data. However, bypassing iterative distillation by directly transferring outputs of initial classes to the current learning task is not supported in existing class-specific CSS methods. Via Softmax, they enforce dependency between classes and adjust the output distribution at each learning step, resulting in a large probability distribution gap between initial and current tasks. We introduce a simple, yet effective Class Independent Transformation (CIT) that converts the outputs of existing semantic segmentation models into class-independent forms with negligible cost or performance loss. By utilizing class-independent predictions facilitated by CIT, we establish an accumulative distillation framework, ensuring equitable incorporation of all class information. We conduct extensive experiments on various segmentation architectures, including DeepLabV3, Mask2Former, and SegViTv2. Results from these experiments show minimal task forgetting across different datasets, with less than 5% for ADE20K in the most challenging 11 task configurations and less than 1% across all configurations for the PASCAL VOC 2012 dataset.","Figure 1: The top panel illustrates the current iterative distillation pipeline, where knowledge is sequentially transferred and distilled across tasks. This sequential approach can cause an accumulation of errors over time. In contrast, the lower panel illustrates the use of independent logits, enabling the implementation of our proposed accumulative pipeline. Direct distillation from the source model for each class aids in preserving the integrity of acquired information during the learning sequence. Each square represents the addition of a new task at each stage. Semantic segmentation [29, 6, 13, 28, 50, 7, 18] is a fundamental task in scene understanding, which aims to assign a label to each pixel. While current deep-learning models [8, 9, 43] perform well on segmentation tasks, they are prone to the problem of catastrophic forgetting [17], which causes the model to lose accuracy on the previous categories as it learns new classes. Therefore, class-incremental semantic segmentation (CSS) [4, 14, 37, 34, 32] is an emerging field in computer vision, which aims to optimize the segmentation accuracy in continuously evolving environment. Unlike traditional semantic segmentation models [6, 45, 43, 9] trained on a static set of classes, CSS models continually learn to segment new classes without forgetting the previously seen categories. Such approaches are critical in real-world applications where new categories are frequently introduced, such as advancements in autonomous vehicles [1, 22] and robotics [2]. CSS methodologies often incorporate techniques like replay strategies [31], knowledge distillation [14, 33, 34], and network architecture adaptations [41, 36, 30, 25] that help in retaining knowledge of old classes while efficiently learning new ones. Among these techniques, knowledge distillation (KD) is prominent because it does not rely on storing previously seen data [34], thus protecting data privacy [14]. Specifically, when learning new classes, KD methods force the models to produce similar outputs to the previous checkpoint by distilling the probability output between the previous and the current step. Despite their remarkable success, existing CSS methods [14, 34, 37] suffer from several drawbacks. Firstly, these methods adopt conventional semantic segmentation techniques and rely on per-pixel label assignment for segmentation. At the output layer, segmentation models often adopt a Softmax operation to generate class-interdependent normalized likelihoods, ensuring that the total probability distribution of all learned categories sums to 1111. This design enforces a dependency within a set of target classes. Yet, classes of interest continually change at each CSS step, yielding the mismatch between the probability distribution at different learning steps. This probability mismatch makes the distillation between outputs of different steps non-optimal, potentially impairing the performance of the current model on prior tasks. Secondly, current methods adopt an iterative pipeline, where the predictions of the most recent model are used as the target to re-train the current model on previous categories, as shown in Fig. 1. However, this method causes the model’s performance on earlier tasks to deteriorate gradually with each iteration. This degradation, which results from re-distilling the previous tasks repeatedly, leads to incremental forgetting. Moreover, as the model goes through more incremental learning tasks, the accumulated errors and uncertainties on old classes from the learning process become more significant. To address these limitations in current class-incremental semantic segmentation (CSS) methods [31, 14, 33], we propose a novel approach, called class-independent transformation (CIT) that formulates the segmentation task as a class-agnostic prediction problem. This formulation eliminates the class interdependency enforced by Softmax, which leads to catastrophic forgetting and poor generalization in CSS. Our framework introduces a structural transformation, in which predictions from existing semantic segmentation methods are converted into class-independent binary likelihood forms. This conversion is achieved by Sigmoid, acts as a determination function, which ensures no compromise in the performance. By employing CIT, our framework simplifies the CSS training process, avoiding the class-interdependent normalization constraints. For previous categories, we apply a simple distillation loss, utilizing Kullback-Leibler (KL) divergence for the class and mask predictions. For the new categories with ground truth, we adopt the same supervised loss as in the original models, but in the CIT form. Note that, for the distillation process, the target is derived exclusively from supervised training, mitigating the error accumulation seen in previous methods. In contrast to previous distillation-based CSS, our framework introduces an advanced accumulative knowledge distillation approach, different from the iterative knowledge transfer employed in previous works [34, 37, 14]. This novel strategy focuses on combining logits of the source step checkpoints that correspond to the initial learning stage of each target class. By learning directly from the source model that first learned these target classes, our method avoids the problem of compounded forgetting that often occurs in conventional distillation processes. This accumulative distillation ensures that the current model retains and reinforces knowledge from the initial learning stage, which improves the stability of the base class recognition over time. Our contributions are as follows: • We empirically show that the interactive distillation and class interdependency enforced by Softmax cause incremental forgetting in continual learning. • We address the forgetting problem by introducing a simple yet effective class-independent transformation (CIT), which reformulates segmentation as a class-agnostic prediction problem. This formulation facilitates a more straightforward accumulative CSS training pipeline that can be integrated into existing semantic segmentation methods. • We demonstrate that the CIT, combined with the accumulative training pipeline, notably mitigates the forgetting issue, resulting in substantially reduced forgetting on the demanding ADE20K datasets compared to conventional CSS methods, and almost negligible forgetting (up to 1%) across all configurations on Pascal-VOC 2012."
https://arxiv.org/html/2411.02712v1,V-DPO: Mitigating Hallucination in Large Vision Language Models via Vision-Guided Direct Preference Optimization,"Large vision–language models (LVLMs) suffer from hallucination, resulting in misalignment between the output textual response and the input visual content. Recent research indicates that the over-reliance on the Large Language Model (LLM) backbone, as one cause of the LVLM hallucination, inherently introduces bias from language priors, leading to insufficient context attention to the visual inputs. We tackle this issue of hallucination by mitigating such over-reliance through preference learning. We propose Vision-guided Direct Preference Optimization (V-DPO) to enhance visual context learning at training time. To interpret the effectiveness and generalizability of V-DPO on different types of training data, we construct a synthetic dataset containing both response- and image-contrast preference pairs, compared against existing human-annotated hallucination samples. Our approach achieves significant improvements compared with baseline methods across various hallucination benchmarks. Our analysis indicates that V-DPO excels in learning from image-contrast preference data, demonstrating its superior ability to elicit and understand nuances of visual context. Our code is publicly available at https://github.com/YuxiXie/V-DPO.","Recent advancements in Large Language Models (LLMs) (Brown et al., 2020; Chowdhery et al., 2023; Touvron et al., 2023; Chiang et al., 2023; OpenAI, 2023) have catalyzed the evolution of Large Vision–Language Models (LVLMs) (Liu et al., 2023c, b; Dai et al., 2023; Anil et al., 2023) in understanding and reasoning across visual and textual modalities. Despite their impressive performance on various vision–language tasks, existing LVLMs still struggle with the issue of hallucination, where the model outputs are not factually grounded in the input visual contents (Rohrbach et al., 2018; Li et al., 2023b; Gunjal et al., 2024; Liu et al., 2024). Hallucination in LVLMs refers to non-existing or erroneous descriptions of visual contents, such as objects, attributes, and relationships, which is especially challenging to understanding unconventional images, as shown in Figure 1(a). (a) Hallucination examples on unconventional visual contents. (b) Distribution gaps. Figure 1: (a) Hallucination examples in visual question answering and region descriptions and (b) the model discriminative ability on the accurate and hallucinatory samples represented by difference in log-likelihoods. The phenomenon of hallucination in LVLMs can be attributed to the integration of pre-trained LLMs in the architecture. Recent works reveal that this issue is closely tied to insufficient context attention, where the model prioritizes language patterns and focuses on partial tokens rather than fully grounding the generated content in both visual and textual context (Lee et al., 2023; Wang et al., 2024). To mitigate the over-reliance on language priors, many efforts have been devoted to decoding optimization with penalties on over-trust candidates (Huang et al., 2023) or a focus on visual uncertainty (Chen et al., 2024). However, these methods require increased inference time and specific infrastructure designs (Lee et al., 2023), obstructing their generalizability and scalability across diverse data domains and sizes. In contrast, our study explores training strategies to alleviate the over-reliance on language priors via preference learning, enhancing visual understanding to mitigate hallucination in LVLMs. Given the difference in the likelihoods between accurate and hallucinatory samples on vision-conditioned p⁢(response∣image,query)𝑝conditionalresponseimagequeryp(\mathrm{response}\mid\mathrm{image},\mathrm{query})italic_p ( roman_response ∣ roman_image , roman_query ) and textual-only p⁢(response∣query)𝑝conditionalresponsequeryp(\mathrm{response}\mid\mathrm{query})italic_p ( roman_response ∣ roman_query ) distributions, Figure 1(b) illustrates the shifts of this difference after aligning the model with hallucination-free data via preference learning. Before alignment, the textual-only distributions dominate the model decision on determining accurate samples as preferred compared to hallucinatory ones, reflected by the distributions (in green) of the same shape for both probabilities. This dominance in pairwise preference illustrates the over-reliance on language priors in LVLMs, which is especially crucial for unseen images in training (e.g., Figure 1(a)), limiting the model generalizability across different data. Motivated by this challenge, we propose Vision-guided Direct Preference Optimization (V-DPO), a vision-specific variant of Direct Preference Optimization (DPO) (Rafailov et al., 2023), to employ visual guidance during preference learning for hallucination mitigation in LVLMs. We adapt Classifier-Free Guidance (CFG) (Ho and Salimans, 2022) to integrate the visual guidance into the optimization target, inspired by its effectiveness in improving the specificity of model generations tailored for specific contents (Sanchez et al., 2023; Kornblith et al., 2023). To assess the generalizability of V-DPO, especially on unconventional contents, we construct a synthetic dataset containing both response-contrast and image-contrast preference pairs, compared against existing human-annotated preferences such as RLHF-V (Yu et al., 2023). Our approach exhibits significant and stable performance improvements through extensive experiments on various hallucination benchmarks. Further analysis of the distribution shifts from training demonstrates the effectiveness of V-DPO in mitigating the over-reliance on language priors on both image- and response-contrast data."
https://arxiv.org/html/2411.02710v1,Full Field Digital Mammography Dataset from a Population Screening Program,"Breast cancer presents the second largest cancer risk in the world to women. Early detection of cancer has been shown to be effective in reducing mortality. Population screening programs schedule regular mammography imaging for participants, promoting early detection. Currently, such screening programs require manual reading. False-positive errors in the reading process unnecessarily leads to costly follow-up and patient anxiety. Automated methods promise to provide more efficient, consistent and effective reading. To facilitate their development, a number of datasets have been created. Such datasets can aid in learning-based development but many are not publicly available and do not draw directly from population screening programs. With the aim of specifically targeting population screening programs, we introduce NL-Breast-Screening, a dataset from a Canadian provincial screening program. The dataset consists of 5997 mammography exams, each of which has four standard views and is biopsy-confirmed. Cases where radiologists’ reading was a false-positive are identified. NL-Breast is made publicly available as a new resource to promote advances in automation for population screening programs.","Breast cancer prevalence increases with age and health authorities recommend that women over 40 (or 50 in some jurisdictions) receive an X-ray mammogram every two years. As a result, in Canada and the USA, about 40 million exams are performed each year. Of these, two million exams (5%) are deemed suspicious by the radiologist and referred for additional procedures. However, only 270,000 cases of breast cancer are reported each year. Assuming that all these cases are represented in the mammograms, the disease incidence in the mammograms is no greater than 0.7% (270k/40M). Ironically, although it appears that there is a marked tendency to over-diagnose, screening programs still miss up to 20% of the resident disease [1]. Attempts to mitigate errors include double reading and artificial intelligence (AI) assistance. These efforts improved accuracy but increased cost; they did not succeed in eliminating the false negative and false positive cases. In response to this diagnostic challenge, many research groups applied advanced automated detection and classification schemes to the problem of breast cancer detection in mammography[2]. The results, while promising, have not eclipsed those of existing programs [3, 4]. Since the onset of screening programs in the 1980’s there have been many technological developments. The first was optimization of x-ray beam quality through the use of appropriate anode material and aperture filters. During this period film technology received a great deal of attention, very fine structured films provided superior spatial resolution at the cost of a small dose increase. Film was supplanted by indirect technology, which in turn was supplanted by digital technology. Digital technology brought an opportunity to standardize data formats. Mammograms are now stored using the medical-standard DICOM image format [5]. This format is information rich in that it includes details of equipment settings and of automated data processing routines. Detector development continues to deliver ever higher resolution and ever more sensitive detectors. In addition, line source x-ray generators promise to reduce scatter and lower dose. To assist with the challenge of overlying tissue, panoramic cameras are now used to help with depth perception and increase conspicuity. There are now thousands of terabytes of mammography data, representing just about every variation of normal and pathological breast anatomy. This has proven to be an attractive resource for software engineers, who have produced a steady stream of “computer aided diagnostic” (CAD) applications to assist in identifying markers of disease. Early offerings used intensity measures to identify bright clusters in mammograms [6]. When present, these represent calcifications that, in certain configurations, are associated with carcinoma development. However, not all cancer containing breasts feature calcifications [7, 8]. So, the next focus was on identifying masses. These were detectable as discrete, anomalous areas of intermediate brightness. They represent a large class of breast cancer known as ductal carcinoma in situ [9]. Other cancers have a more complex structure, often with tendrils or diffuse edges that extend into seemingly normal tissue [9]. These were more difficult to detect using software algorithms, but were an important entity since they represent a second class of breast tumour known as invasive ductal carcinoma[10]. There are even more challenging cases. When the native breast tissue is fibrous, the contrast between normal and cancer tissues, already very subtle, is reduced [10]. Younger women tend to have these type of breasts; the detection efficiency is reduced for this age group meaning that some programs do not include women under fifty years of age [10]. As the algorithms have grown in complexity it has been more difficult to realize diagnostic gains in breast cancer screening programs. In addition, it is important not to increase the resource demands on this already expensive program [11]. Early studies determined that the use of CAD reduced the overall efficiency of a screening program as the radiologist was faced with many false positive findings that had to be cleared [12, 13, 14, 15]. Subsequent studies have provided a mixed bag of results. The performance of expert screeners did not improve with the use of CAD, while that of less experienced mammographers did. Despite this, software developers have pressed on, focusing their attention on differentiating benign and cancerous findings and on grading the severity of tumours found in an image. This focus has produced very complex CAD programs whose use requires specific training by the radiologist. Even though the number of false positives increased with CAD, its use in screening mammography is now widespread. CAD development has slowed somewhat and that may be attributed to the complexity of the remaining tasks. Generally in software development early applications develop quickly but have a narrow range of utility. As the scope of the undertaking increases, so too does the number of potential failure points. In very complex situations, such as differentiating tumour grade, there is an exponential increase in the number of potential failure points and a corresponding increase in the time required to resolve them. A second barrier to development is the requirement that software which is used in diagnosis must be approved by the cognate national authority. This authorization requires extensive independent testing, and that has highlighted another problem: the shortage of high quality testing datasets."
https://arxiv.org/html/2411.02697v1,Transferable polychromatic optical encoder for neural networks,"Artificial neural networks (ANNs) have fundamentally transformed the field of computer vision, providing unprecedented performance. However, these ANNs for image processing demand substantial computational resources, often hindering real-time operation. In this paper, we demonstrate an optical encoder that can perform convolution simultaneously in three color channels during the image capture, effectively implementing several initial convolutional layers of a ANN. Such an optical encoding results in 24,000×~{}24,000\times24 , 000 × reduction in computational operations, with a state-of-the art classification accuracy (∼73.2%similar-toabsentpercent73.2\sim 73.2\%∼ 73.2 %) in free-space optical system. In addition, our analog optical encoder, trained for CIFAR-10 data, can be transferred to the ImageNet subset, High-10, without any modifications, and still exhibits moderate accuracy. Our results evidence the potential of hybrid optical/digital computer vision system in which the optical frontend can pre-process an ambient scene to reduce the energy and latency of the whole computer vision system.","Visual information plays a crucial role in human response, particularly in situations where reaction time is limited to a few tens to hundreds of milliseconds [1, 2]. Though the human brain has efficiency far exceeding that of any other human-made computing systems, it still cannot process the entire collected visual data due to its massive amount of information. Most likely, our brain performs early visual processing to extract essential features for efficient and rapid interpretation without handling the entire visual data [3, 4, 5]. With the dramatic development of artificial intelligence (AI), computers can process the visual information like human brain, thanks to artificial neural network (ANN), enabling computer/machine vision [6, 7, 8, 9, 10]. Despite impressive progress, real-time inference with limited computational resources remains very challenging even with more efficient algorithms. For example, in a flying object (i.e., habitat drones [11]) on-site data processing is plagued by severe heating, battery capacity and weight handling challenges. Utilizing cloud based systems poses challenges associated with data security and additional data transfer latency [12, 13]. Optical neural networks have emerged as a potential platform to circumvent these trade-offs, since an optical system can process multidimensional information with large spatio-temporal bandwidth [14]. Recently, integrated photonics and free-space or fiber optics have been employed to implement some parts of an ANN for image compression/encryption [15, 16] and classification [17, 18, 19, 20, 21]. However, most of them are highly restricted on solving a relatively simple gray-scale datasets (i.e., MNIST and fashion-MNIST) and only a couple of systems have shown their implementation for more complicated multichannel datasets (i.e., CIFAR-10 and ImageNet) [17, 19]. For these complex datasets, the optical system often become extremely large (with multiple stacks of the photonic circuits) [19], otherwise the classification accuracy remains low (∼60%similar-toabsentpercent60\sim 60\%∼ 60 % accuracy for CIFAR-10 classification tasks) [17, 22, 23]. In addition, the most successful ANN architectures utilize nonlinear activation functions that are challenging to implement optically. Proposed solutions, including atomic vapor cells [24, 25] and image intensifiers [26], introduce significant experimental complexity, and additional power consumption. To leverage the strengths of both optical and digital computing systems, an encoder-decoder inspired hybrid optical/digital architecture is a promising approach [8, 27, 10, 28]. Specifically, an analog linear optical frontend (denoted as the optical encoder) performs bulk of linear computational tasks, while the digital backend implements the nonlinear operations. One intriguing possibility is to employ a static optical frontend, which is data agnostic, whereas the backend is trained and reconfigured. This resolves usual issues of modulation speed, errors, and system size in all-optical systems. An optical encoder is particularly suitable for convolutional neural network (CNN) architectures, where convolutional layers act as feature extractors, encoding high-dimensional images into low-dimensional features [29]. In fact, every free-space optic inherently performs a two-dimensional convolution operation during the imaging under incoherent light. The captured image is a convolution of the scene and the optic’s incoherent point-spread-function (PSF) [30]. Thus, by engineering the PSF, an optical encoder can perform the desired convolution and replace the initial layers of a CNN. Recently, PSF-engineered optical encoder has been employed to classify MNIST hand-written dataset and a reasonable classification accuracy with much less computational costs compared to the AlexNet is demonstrated [31]. We note that, however, MNIST images are monochrome, and is almost linearly separable (0.84%percent0.840.84\%0.84 % loss without any nonlinearity [32]). The monochrome nature of the images makes the PSF-engineering approach wavelength agnostic. On the other hand, datasets such as CIFAR-10 [33] or ImageNet subset (High-10) [34, 29] are not separable by linear layers. Moreover, they consist of colored images, where the actual color information is exploited in classification. Here, we demonstrate a polychromatic optical encoder with PSF-engineered meta-optics to classify the CIFAR-10 dataset. We first compressed the architecture into a single convolutional layer and two fully-connected layers using Knowledge Distillation. Then, we physically realized the convolution layer using an array of metasurfaces, where each metasurface, thanks to the inherent chromaticity, performs a separate convolution for each color channel. As a result, the hybrid CNN with an optical encoder reduces the total number of multiply–accumulate (MAC) operations at the digital backend by an factor of ∼24,000similar-toabsent24000\sim 24,000∼ 24 , 000. The reduction of number of MAC operation directly corresponds to the computational costs, i.e., power and latency [35]. It is worth noting that we always require an imaging system (i.e., lens and camera) to capture the image under ambient illumination, before we deliver the image data to the computational backend. Hence, with a single meta-optical encoder, we are not adding any additional optics, but simply replacing a conventional lens with PSF-engineered meta-optics. This makes our optical system compact and fully compatible with conventional optical imaging systems, while the other systems such as, integrated-photonic systems require pre-processing of the data[19] and in-sensor computing needs a customized sensor design [36]. Furthermore, we adopt the same meta-optics (optical convolutional layer) which was optimized for CIFAR-10 dataset to High-10 dataset to explore the generality of optical encoders. In practice, a static optical encoder should be applicable for any scene. While, one approach is to employ reconfigurable frontend, e.g. based on non-volatile phase change materials [37] or liquid crystals [38], the performance of these reconfigurable front end in terms of individual pixel control, power consumption, and operating speed are still inferior for practical deployment. Remarkably, with the same passive optical encoder (optimized for CIFAR-10 dataset), we achieved a high classification accuracy (for High-10 dataset) by fine-tuning the digital backend with additional fully-connected layer (via transfer learning approach). This ability to generalize the frontend is crucial for any ANNs as it enhances their versatility, efficiency, and robustness. A network that generalizes well can be applied to different tasks without extensive re-training, saving time, reducing costs for meta-surface fabrications, and conserving computational resources for real-world applications."
https://arxiv.org/html/2411.02673v1,Multi-Transmotion: Pre-trained Model for Human Motion Prediction,"The ability of intelligent systems to predict human behaviors is crucial, particularly in fields such as autonomous vehicle navigation and social robotics. However, the complexity of human motion have prevented the development of a standardized dataset for human motion prediction, thereby hindering the establishment of pre-trained models. In this paper, we address these limitations by integrating multiple datasets, encompassing both trajectory and 3D pose keypoints, to propose a pre-trained model for human motion prediction. We merge seven distinct datasets across varying modalities and standardize their formats. To facilitate multimodal pre-training, we introduce Multi-Transmotion, an innovative transformer-based model designed for cross-modality pre-training. Additionally, we present a novel masking strategy to capture rich representations. Our methodology demonstrates competitive performance across various datasets on several downstream tasks, including trajectory prediction in the NBA and JTA datasets, as well as pose prediction in the AMASS and 3DPW datasets. The code is publicly available: https://github.com/vita-epfl/multi-transmotion.","The research community has witnessed substantial advancements through the adoption of pre-trained models. In natural language processing (NLP), large language models (LLMs) have demonstrated remarkable interdisciplinary proficiency, excelling across a variety of downstream tasks [1, 2]. In contrast, pre-trained models in computer vision (CV) typically exhibit a greater degree of task specificity, which can be attributed to the multimodal nature of visual data [3, 4]. This specificity typically results in reduced efficiency when compared to NLP models. Nevertheless, recent developments in multitask pre-training, exemplified by MultiMAE [5] and 4M [6], have shown promising capabilities in transferring knowledge across a diverse array of CV tasks. However, this critical gap remains in the field of human motion prediction. Unlike fields such as NLP and CV, human motion incorporates rich representations and manifests through diverse modalities, including keypoints, trajectories, and bounding boxes, each reflecting different aspects of human movement. Despite this complexity, there currently exists no multimodal pre-trained model for accurately predicting human motion. Intuitively, human motion cannot be fully expressed by a single modalities. Thus, we argue that each modality can benefit from the others by integrating multiple modalities into the models. Consequently, the development of a multitask pre-trained model is imperative for this domain. Three principal challenges must be overcome to effectively pre-train a model for human motion prediction. First, the field lacks a comprehensive, large-scale dataset that encompasses various modalities of human motion. Second, a versatile framework is required to handle these diverse modalities, in contrast to previous approaches that typically addressed each modality in isolation. Third, the model must be robust when confronted with incomplete or noisy input data. To address these challenges, this paper proposes a novel approach that integrates multiple datasets, develops a flexible network architecture, and demonstrates its effectiveness in handling noisy data. Due to the absence of a large-scale multimodal human motion prediction dataset, we have undertaken the task of merging several existing datasets, namely Joint Track Auto (JTA) [7], Trajnet++ [8], JRDB-Pose [9], NBA [10], Human3.6M [11], AMASS [12] and 3DPW [13]. Each dataset was originally created with different data formats and frame settings. To streamline the training process, we have unified the data framework. This framework standardizes the observation and prediction horizons, as well as the frame rates, ensuring consistency across the merged datasets. This unified framework enables more efficient and effective pre-training of the model, addressing the complexities of multimodal human motion prediction. For the model design, we implement a tokenization strategy that maintains spatial-temporal information across all modalities by applying modality-specific linear projection layers to convert coordinates into hidden dimensions. To further enhance the model’s robustness and adaptability, we employ up-sampling padding, sampling masks, and a bi-directional temporal encoder. The up-sampling padding facilitates easy fine-tuning across different frame rates, while the sampling masks simulate various frame rates by masking tokens with different chunk sizes. Figure 1 shows an overview of our work. By unifying the datasets, we are able to pre-train a transformer-based model, Multi-Transmotion, that can predict future trajectories and informative 3D pose keypoints. The flexibility of our model architecture and data framework allows for easy fine-tuning to specific tasks with varying frame settings. Our model achieved competitive results on both tasks, demonstrating the effectiveness of our pre-training techniques, as shown by our ablation studies on few-shot learning and robustness. Figure 1: Overview. We propose a unified human motion data framework by standardizing the data format and frame settings. Based on that, we introduce a pre-trained transformer model with specialized masking techniques, validating its effectiveness and flexibility across different scenarios. We summarize the main contributions as follows: • Dataset: We create a unified human motion data framework by merging seven datasets with standardized settings. Additionally, this framework is flexible, allowing for the seamless addition of more datasets or adjustments to frame settings. • Method: We propose Multi-Transmotion, a pre-trained transformer-based model that flexibly adapts to different frame settings, demonstrating strong robustness and efficiency. This model outperforms previous models across several datasets."
https://arxiv.org/html/2411.02669v1,Semantic-Aligned Adversarial Evolution Triangle for High-Transferability Vision-Language Attack,"Vision-language pre-training (VLP) models excel at interpreting both images and text but remain vulnerable to multimodal adversarial examples (AEs). Advancing the generation of transferable AEs, which succeed across unseen models, is key to developing more robust and practical VLP models. Previous approaches augment image-text pairs to enhance diversity within the adversarial example generation process, aiming to improve transferability by expanding the contrast space of image-text features. However, these methods focus solely on diversity around the current AEs, yielding limited gains in transferability. To address this issue, we propose to increase the diversity of AEs by leveraging the intersection regions along the adversarial trajectory during optimization. Specifically, we propose sampling from adversarial evolution triangles composed of clean, historical, and current adversarial examples to enhance adversarial diversity. We provide a theoretical analysis to demonstrate the effectiveness of the proposed adversarial evolution triangle. Moreover, we find that redundant inactive dimensions can dominate similarity calculations, distorting feature matching and making AEs model-dependent with reduced transferability. Hence, we propose to generate AEs in the semantic image-text feature contrast space, which can project the original feature space into a semantic corpus subspace. The proposed semantic-aligned subspace can reduce the image feature redundancy, thereby improving adversarial transferability. Extensive experiments across different datasets and models demonstrate that the proposed method can effectively improve adversarial transferability and outperform state-of-the-art adversarial attack methods. The code is released at https://github.com/jiaxiaojunQAQ/SA-AET.","Vision-language pre-training (VLP) models achieve excellent performance across various downstream Vision-and-Language tasks, such as visual entailment [51], visual grounding [32], image captioning [25], and image-text retrieval [30]. However, they have been found to be vulnerable to adversarial examples [66; 42; 21; 22; 9; 43; 16]. Exploring adversarial vulnerabilities can inspire additional research dedicated to developing more robust and applicable VLP models. Figure 1: Comparison of Our Method and Set-Level Guided Attack (SGA) [42]. (a) illustrates the main concept of SGA, which involves performing data augmentations around online adversarial examples. (b) demonstrates the core idea of our SA-AET, where data augmentations are applied within the adversarial sub-triangle. The red and blue dots represent images sampled from this sub-triangle, with red dots highlighting the optimal samples chosen through a text-guided adversarial example selection strategy. The surrounding light red dots represent resized augmentations applied to these optimal samples, similar to the strategy used in SGA. (c) and (d) compare the adversarial transferability of our SA-AET against SGA using adversarial examples from ALBEF [34] and CLIPViTViT{}_{\text{ViT}}start_FLOATSUBSCRIPT ViT end_FLOATSUBSCRIPT [48] to attack CLIPCNNCNN{}_{\text{CNN}}start_FLOATSUBSCRIPT CNN end_FLOATSUBSCRIPT [48], respectively. Figure 2: The Pipeline of the Proposed SA-AET: (a) Pipeline for the Adversarial Evolution Triangle (AET) in Adversarial Image Generation. (b) Pipeline for the Adversarial Evolution Triangle (AET) in Adversarial Text Generation. (c) Pipeline for Extracting the Semantic Projection Matrix. Previous works mainly concentrate on exploring generating adversarial examples for VLP models in a white-box setting, in which the attacker can access model internal information, such as model parameters, etc. Some studies [18] have shown that adversarial examples generated on a victim model can successfully attack unseen target models, a phenomenon known as adversarial transferability. Given the limited access to detailed model structures in real-world scenarios, it is crucial to investigate the transferability of multimodal adversarial examples [70; 72]. A series of works focus on generating adversarial examples with high transferability for VLP models. For example, Lu et al. [42] propose to improve adversarial transferability for VLP models by introducing input diversity through data augmentation. Although previous works have achieved some effectiveness in boosting adversarial transferability in vision-language attacks, they mainly focus on maximizing the contrastive loss function in the image-text feature space to generate adversarial examples and increasing their diversity along the optimization path to improve adversarial transferability. While these methods predominantly enhance diversity in online adversarial examples, they still have a lot of room for improvement in improving adversarial transferability. Specifically, as shown in Figure 1, during each optimization iteration of the attack, previous works [42; 22] perform data augmentation on the generated adversarial examples (i.e., online adversarial image) to improve transferability. This adversarial strategy increases the variety of adversarial examples throughout the optimization pathway, consequently yielding improvements in their transferability. Nonetheless, the strategy still risks overfitting the victim model due to the heavy reliance on examples from the adversarial trajectory, which results in reduced attack success rates when the adversarial examples are transferred to other VLP models. To address these issues, as shown in Figure 2 (a) and (b), we first propose adopting the intersection evolution triangles along the adversarial trajectory to enhance the diversity of adversarial examples during optimization. Specifically, in each attack iteration, we propose constructing an adversarial evolution triangle that incorporates the original image, the adversarial image from the previous step, and the current adversarial image. Then, we propose to circumvent overfitting by strategic sampling within this evolution triangle, thereby avoiding excessive focus on adversarial example diversity only around adversarial images. We explore the impact of different adversarial evolution sub-triangle sampling on adversarial transferability and propose to sample from the adversarial evolution sub-triangle close to clean examples and previous adversarial examples. After obtaining the sampling samples, we generate the adversarial perturbations on the samples to stay away from the text. Subsequently, we apply these perturbations to current adversarial examples and select the one that diverges most significantly from the text. We also propose generating adversarial text that deviates from the previous adversarial evolution triangle along the optimization path rather than being distant from the last adversarial image example. We provide a theoretical analysis to illustrate the effectiveness of the proposed adversarial evolution triangle. In addition, image feature embeddings usually contain much information irrelevant to text features [39; 71; 26]. In the original image-text contrast space, only a limited number of feature dimensions in the image features may be activated, while the remaining dimensions remain inactive and redundant. These redundant dimensions may dominate the similarity calculation, potentially distorting the feature matching of images and texts. This feature distortion makes the generated adversarial examples highly dependent on the victim model, reducing their transferability. Hence, generating adversarial examples within the native image-text feature contrast space increases the likelihood of overfitting the victim model, diminishing the examples’ transferability. To overcome this limitation, we propose generating adversarial examples within the semantic image-text feature contrast space, which maps the original feature space into a semantic corpus subspace. Specifically, as shown in Figure 2 (c), we construct a semantic subspace through a series of independent text descriptions and project the image features of the original space into the semantic subspace. Then, we maximize the contrast loss between images and text in the semantic subspace to generate adversarial perturbations. By assembling the proposed methods, we conduct our vision-language attack by exploiting semantically aligned adversarial evolution triangle, i.e., SA-AET. We conduct a series of experiments to evaluate the effectiveness of the proposed method on the two widely used multimodal datasets, consisting of Flickr30K [47] and MSCOCO [41]. The evaluation experiments are also conducted on three vision-and-language downstream tasks, which include image-text retrieval (ITR), visual grounding (VG), and image captioning (IC). The experimental results indicate that the proposed method can significantly improve the transferability of multimodal adversarial examples, surpassing the state-of-the-art adversarial attack methods. Furthermore, when adversarial examples generated from ITR by the proposed method are applied to other vision-and-language downstream tasks, attack performance is significantly enhanced. Our main contributions are in five aspects: • We propose enhancing the diversity of adversarial examples during optimization by leveraging the intersection evolution triangle of adversarial trajectories, thereby improving the transferability of multimodal adversarial examples against VLP models. Furthermore, we provide a theoretical analysis to support the proposed adversarial evolution triangle. • We investigate how sampling from different adversarial evolution sub-triangles affects adversarial transferability and propose sampling from the evolution sub-triangle that is close to clean examples and previous adversarial examples. • We propose to generate the adversarial text by deviating from the final adversarial evolution triangle along the optimization trajectory rather than the final adversarial example, minimizing overfitting on the surrogate model to improve its transferability. • To further enhance the transferability of adversarial examples, we propose generating them in the semantic image-text feature contrast space by mapping the original feature space onto a subspace defined by a semantic corpus. • Our extensive experiments across various network architectures and datasets demonstrate that the proposed method significantly enhances the transferability of multimodal adversarial examples and outperforms state-of-the-art multimodal transfer adversarial attack methods. This paper is a journal extension of our conference paper [17] (called DRA). Compared to the preliminary conference version, we have made significant improvements and extensions in this version. The main differences are in four aspects: 1) In addition to sampling the intersection evolution triangle of adversarial trajectories proposed in the previous version, we explore the impact of different sampling strategies and propose to sample adversarial evolution triangles close to clean examples and previously generated adversarial examples in Section III-B. This can further improve the transferability of multimodal adversarial examples. We add a theoretical analysis to demonstrate the effectiveness of the proposed adversarial evolution triangle in Section III-D. 2) We propose to generate the adversarial examples in the semantic image-text feature contrast space in Section III-E, which can reduce reliance on victim models, thereby improving transferability. 3) We conduct more experiments and analyses, which include comparisons with state-of-the-art methods, ablation studies, and performance analyses. We adopt some state-of-the-art adversarial attack methods as the new comparison in Section IV-C. We add the ablation study versus the different proposed elements in Section IV-F. We analyze the effective performance of the proposed method in Section IV-G. 4) We have thoroughly revised the abstract, introduction, method, experiment, and conclusion sections to offer a more detailed overview of our motivation and approach. Furthermore, we have updated all figures and tables to enhance clarity and presentation."
https://arxiv.org/html/2411.02632v1,Intelligent Video Recording Optimization using Activity Detection for Surveillance Systems,"Surveillance systems often struggle with managing vast amounts of footage, much of which is irrelevant, leading to inefficient storage and challenges in event retrieval. This paper addresses these issues by proposing an optimized video recording solution focused on activity detection. The proposed approach utilizes a hybrid method that combines motion detection via frame subtraction with object detection using YOLOv9. This strategy specifically targets the recording of scenes involving human or car activity, thereby reducing unnecessary footage and optimizing storage usage. The developed model demonstrates superior performance, achieving precision metrics of 0.855 for car detection and 0.884 for person detection, and reducing the storage requirements by two-thirds compared to traditional surveillance systems that rely solely on motion detection. This significant reduction in storage highlights the effectiveness of the proposed approach in enhancing surveillance system efficiency. Nonetheless, some limitations persist, particularly the occurrence of false positives and false negatives in adverse weather conditions, such as strong winds.","The widespread installation of surveillance cameras has led to a significant increase in visual data, with estimates predicting over 1.4 billion cameras worldwide by 2024, generating vast amounts of video data daily. This surge poses major challenges in terms of storage, requiring hundreds of petabytes to manage this critical information. Efficient methods to manage and facilitate the search of this data have become imperative. Surveillance cameras, essential for security, create an information overload. A single HD camera can produce nearly 650 megabytes of data per minute, resulting in a substantial data management challenge when multiplied across thousands of cameras in urban areas. Much of this data is redundant or irrelevant, necessitating careful consideration of optimal storage methods to ensure the rapid retrieval of important information. The key problem is optimizing the storage of surveillance camera data to prevent storage congestion while maintaining necessary recordings. Object and motion detection algorithms emerge as promising solutions, filtering sequences to record only significant activities. This approach addresses the challenge of information overload in video surveillance. This study aims to optimize storage space through innovative methods using motion and object detection algorithms and to implement a solution capable of discriminating important scenes. This dual objective seeks to balance storage efficiency with the relevance of the recordings. The structure of the remain of this paper is as follows: Section 2 reviews related works in intelligent surveillance systems, comparing their strengths and weaknesses. In section 3, a proposed methodology of activity detection is introduced with a overall architecture of the proposed system. Implementation is presented in section 4, and experiment results and discussion in section 5. The paper concludes with a summary of findings and a discussion of future research directions."
https://arxiv.org/html/2411.02624v1,"Enhancing Indoor Mobility with Connected Sensor Nodes: A Real-Time, Delay-Aware Cooperative Perception Approach","This paper presents a novel real-time, delay-aware cooperative perception system designed for intelligent mobility platforms operating in dynamic indoor environments. The system contains a network of multi-modal sensor nodes and a central node that collectively provide perception services to mobility platforms. The proposed Hierarchical Clustering Considering the Scanning Pattern and Ground Contacting Feature based Lidar Camera Fusion improve intra-node perception for crowded environment. The system also features delay-aware global perception to synchronize and aggregate data across nodes. To validate our approach, we introduced the Indoor Pedestrian Tracking dataset, compiled from data captured by two indoor sensor nodes. Our experiments, compared to baselines, demonstrate significant improvements in detection accuracy and robustness against delays. The dataset is available in the repository111https://github.com/NingMingHao/MVSLab-IndoorCooperativePerception.","In recent years, intelligent indoor autonomy technology is gaining recognition and attention among healthcare professionals and researchers. Studies have shown that indoor transportation is the most urgent need from healthcare staff in hospitals and long-term care [1]. This rising demand is largely driven by workforce shortages and the high incidence of chronic injuries among healthcare staff, which often caused by transporting heavy materials. However, large scale commercial deployment of intelligent robotics platforms are still limited. Most existing indoor robots are designed to operate independently, relying on their built-in sensors to navigate and perform tasks. This restricts their effectiveness in the congested, dynamic, and unpredictable spaces of healthcare facilities. This paper presents a cooperative perception system consisting of a network of multiple sensor nodes, and a central node, to provide perception results/services to robotic mobility platforms. This system aimed to improve the operational safety and environmental awareness of intelligent robotic platforms, including autonomous hospital beds and delivery robots. Figure 1: Overview of the proposed cooperative perception system There are several challenges associated with developing a cooperative perception system in densely populated indoor environments, such as hospitals. One primary challenge for local perception is the fast and accurate fusion of perception data from multiple sensor nodes. This task is complicated by the dynamic behavior of people within a confined space, which involves close interactions between individuals. For instance, people travel in small groups, or crossing paths at close quarters. These situations pose significant difficulties in maintaining consistent tracking identities across different nodes and merging perception data effectively. The physical layout of indoor environments presents another significant challenge for local perception. Architectural features and decorative elements, such as corners, pillars, and mirrors, present significant challenges in achieving continuous and accurate coverage across the entire area. These environmental factors can obstruct the sensor field of view and distort the sensor signal, leading to gaps in coverage or inaccuracies in perception. Figure 2: The proposed delay-aware cooperative perception framework. The processing and communication delays poses a major challenge for global/cross-sensor perception in highly dynamic indoor environments. These cross-node delays can lead to the receipt of outdated or inaccurate representations of the dynamic environment at the center node. This impairs the center node’s ability to generate a cohesive and current understanding of the environment. To address these challenges, this paper proposes a delay-aware cooperative perception system designed for dynamic indoor environment. An overview of the proposed system is illustrated in Fig. 1. Our contribution can be summarized as follows: • An adaptive clustering method coupled with ground-contact point-based LiDAR-camera fusion, enhancing the accuracy and reliability of local perception. • A delay-aware global perception framework that accounts for messaging delays and latency, ensuring timely and cohesive environmental understanding. • The creation of a multimodal cooperative indoor perception dataset specifically designed for dynamic and crowded healthcare environments. This provids a valuable resource for further research and development in this field. The rest of the paper is organized as follows, in section II, the related methods and dataset are reviewed, in section III, the overview of our method is presented, in section IV, the experiments and discussion are presented, and finally in section V the impact of our work is concluded."
https://arxiv.org/html/2411.02592v1,Decoupled Data Augmentation for Improving Image Classification,"Recent advancements in image mixing and generative data augmentation have shown promise in enhancing image classification. However, these techniques face the challenge of balancing semantic fidelity with diversity. Specifically, image mixing involves interpolating two images to create a new one, but this pixel-level interpolation can compromise fidelity. Generative augmentation uses text-to-image generative models to synthesize or modify images, often limiting diversity to avoid generating out-of-distribution data that potentially affects accuracy. We propose that this fidelity-diversity dilemma partially stems from the whole-image paradigm of existing methods. Since an image comprises the class-dependent part (CDP) and the class-independent part (CIP), where each part has fundamentally different impacts on the image’s fidelity, treating different parts uniformly can therefore be misleading. To address this fidelity-diversity dilemma, we introduce Decoupled Data Augmentation (De-DA), which resolves the dilemma by separating images into CDPs and CIPs and handling them adaptively. To maintain fidelity, we use generative models to modify real CDPs under controlled conditions, preserving semantic consistency. To enhance diversity, we replace the image’s CIP with inter-class variants, creating diverse CDP-CIP combinations. Additionally, we implement an online randomized combination strategy during training to generate numerous distinct CDP-CIP combinations cost-effectively. Comprehensive empirical evaluations validate the effectiveness of our method.","Data augmentation is extensively employed to enhance neural network performance. Traditional data augmentation, such as random shifting, cropping, and rotation, are widely used due to their simplicity and effectiveness, becoming standard practice in nearly all training algorithms. Recently, two innovative types of data augmentation have shown potential for improving image classification: • Image-Mixing Data Augmentation. Generate augmented images by integrating two or more randomly picked natural images at the pixel or feature level, creating virtual data between classes. The online combination paradigm allows for the efficient production of many images with extensive pixel-level variations at a low cost, yet the images often look unrealistic and face fidelity problems, as noted by (Kang & Kim, 2023; Islam et al., 2024). • Generative Data Augmentation. This method leverages generative models to create images using prompts generated manually or via textual inversion to align with class labels. However, as noted by (Islam et al., 2024), this method is not yet mature for data-rich learning scenarios. Crafting prompts that ensure model-generated images match the actual data distribution is difficult, requiring expert knowledge to describe class objects and challenges in capturing the dataset’s style. Additionally, textual inversion often leads to limited image diversity due to information loss, reducing the diversity of the generated images, as mentioned by (Wang et al., 2024). Both forms of prompt guidance encounter issues of misalignment or limited variation, resulting in limited performance improvements. Readers can refer to Figure 1 for examples of various data augmentation methods. It is evident that a trade-off exists between semantic fidelity and diversity in these methods. Naturally, the question arises: ’How can semantic fidelity be preserved while simultaneously enhancing diversity?’ The prevailing practice of treating images as indivisible units in existing data augmentation methods presents a fundamental obstacle to achieving both fidelity and diversity. This whole-image paradigm, while enriching diversity, often results in excessive and detrimental variations to class-dependent objects, severely compromising fidelity. In contrast, viewing images from a disentangled perspective could alleviate this challenge by applying distinct strategies to class-dependent parts and class-independent parts: a conservative strategy on CDPs to maintain fidelity and an aggressive strategy on CIPs to enhance diversity. Based on this insight, we propose a novel data augmentation framework, Decoupled Data Augmentation (De-DA), which addresses the fidelity-diversity dilemma through a decoupling strategy. Specifically, we first separate images into class-dependent parts (CDPs) and class-independent parts (CIPs) using SAM (Kirillov et al., 2023), and then tailor our adaptive strategies for respective parts according to their distinct characteristics. To preserve semantic fidelity, we use class identifiers derived from intra-class CDPs as conditions to edit real CDPs with controlled strength, elaborately varying them while preserving their semantic consistency. To encourage diversity, we replace the original CIP of the images with a random CIP sampled from an inter-class image. Furthermore, we adopt an online randomized combination strategy, pairing one CDP (real or synthetic) with one CIP (cross-class real CIPs) at random positions and transformations to provide the model with various combinations during the training stage, further enhancing diversity. In summary, both conservatively translated CDPs and real CIPs align with the actual data, ensuring that the generated images maintain fidelity, while the semantic edits on CDPs and diverse CDP-CIP combinations significantly enrich variety. Compared to previous image-mixing methods, De-DA fuses CDPs and CIPs at the semantic level rather than the pixel level, thereby enhancing fidelity. De-DA also distinguishes itself from other generative methods via applying textual inversion (Gal et al., 2022) and SDEdit (Meng et al., 2021) to isolated CDPs instead of the entire image, thus avoiding the negative effects of noisy information in the image. Furthermore, De-DA’s decouple-and-combine paradigm enables the production of more images at a lower cost than prior generative methods. Our contributions include: • De-DA shows a solution to the fidelity-diversity dilemma in previous data augmentation methods by decoupling images into class-dependent parts and class-independent parts and managing these parts adaptively. • To our knowledge, we are the first to apply textual inversion and SDEdit to isolated CDPs instead of entire images in the field of data augmentation, which minimizes the negative impact from the noisy information in the images. Additionally, we propose truncated-timestep textual inversion to reduce the computational burden, enhancing practicability. • Extensive experiments on domain-specific classification, multi-label classification, and data-scarce learning scenarios comprehensively validate the effectiveness of De-DA. Table 1: Comparing data augmentation methods on fidelity and diversity. \rowcolormygray Image-Mixing Generative Image-Mixing + Generative \rowcolormygray Mixup 2018 CutMix 2019 Real-Guidance 2023 DA-Fusion 2024 Diff-Mix 2024 DiffuseMix 2024 De-DA (ours) Mixing Pixel-Wise Patch-Wise — — — Mask-Wise Semantic-Wise Prompt — — Label Description Derived from Intra-Class Images Derived from Inter-Class Images Style Prompt Derived from Intra-Class CDPs Fidelity Low Low High High Medium High High Diversity High High Medium Low High Medium High Figure 2: Illustration of the mechanisms of different data augmentation methods. Row 1: Image-mixing methods, such as Mixup (Zhang et al., 2018) and CutMix (Yun et al., 2019) create mixed images through pixel-level interpolation. DiffuseMix (Islam et al., 2024) uses style prompts (e.g., ”Sunset”) to transform input images, generating varied-style images which are then concatenated to form a hybrid image. DA-Fusion (Trabucco et al., 2024) uses the intra-class identifier V1imagesubscriptV1image{\color[rgb]{0.0,0.42,0.24}\definecolor[named]{pgfstrokecolor}{rgb}{% 0.0,0.42,0.24}\textbf{V1}_{\text{image}}}V1 start_POSTSUBSCRIPT image end_POSTSUBSCRIPT, while Diff-Mix (Wang et al., 2024) employs an another class’s identifier V2imagesubscriptV2image{\color[rgb]{0.0,0.42,0.24}\definecolor[named]{pgfstrokecolor}{rgb}{% 0.0,0.42,0.24}\textbf{V2}_{\text{image}}}V2 start_POSTSUBSCRIPT image end_POSTSUBSCRIPT to translate natural images with SDEdit, but these methods face issues of limited variety or constrained fidelity. Row 2: Our proposed De-DA maintains fidelity by editing CDPs conditioned with V1CDPsubscriptV1CDP{\color[rgb]{0.8,0.33,0.0}\definecolor[named]{pgfstrokecolor}{rgb}{% 0.8,0.33,0.0}\textbf{V1}_{\text{CDP}}}V1 start_POSTSUBSCRIPT CDP end_POSTSUBSCRIPT through a transparency image-to-image diffusion pipeline which is specifically designed for handling transparent images. It also enhances diversity by replacing CIPs and applying random transformations to CDPs, resulting in faithful and diverse images."
https://arxiv.org/html/2411.02588v1,TileTracker: Tracking Based Vector HD Mapping using Top-Down Road Images††thanks:This project was conducted during an internship at NVIDIA Corporation,"In this paper, we propose a tracking-based HD mapping algorithm for top-down road images, referred to as tile images. While HD maps traditionally rely on perspective camera images, our approach shows that tile images can also be effectively utilized, offering valuable contributions to this research area as it can be start of a new path in HD mapping algorithms. We modified the BEVFormer layers to generate BEV masks from tile images, which are then used by the model to generate divider and boundary lines. Our model was tested with both color and intensity images, and we present quantitative and qualitative results to demonstrate its performance.","Autonomous vehicles rely on accurate environmental data from various sensors, including cameras, lidar, radar, IMUs, and GPS. These sensors work together, with their data fused to generate precise commands for vehicle navigation [1]. Some methods estimate waypoints and navigational commands directly from sensor data in an end-to-end manner [2, 3, 4]. Large Visual Models (LVMs) derive actions from camera images [5], while Reinforcement Learning (RL) optimizes navigation based on reward signals [6]. High Definition (HD) maps serve as a valuable and detailed resource for navigating autonomous vehicles. To create these maps for various areas and cities, vehicles equipped with multiple sensors, such as cameras, lidar, and IMUs, traverse the roads, capturing necessary data. Human annotators then identify fixed road features like lane dividers, traffic lights, pedestrian crossings, and road boundaries. Following this, deep learning models have been employed to learn and estimate these road features for inclusion in the HD map based on the sensor data [7, 8, 9, 10]. One well-known method for HD mapping is MapTR [7, 8], which offers an end-to-end, transformer-based [11], unified permutation-equivalent modeling approach. This method models each map element as a point set with a group of equivalent permutations, enabling accurate shape descriptions and stabilizing the learning process. However, MapTR generates road features on a frame-by-frame basis, which can result in noisy and inconsistent HD maps. To address these issues, StreamMapNet [9] utilizes long-sequence temporal modeling of video data. It employs multi-point attention and temporal information to produce large-scale local HD maps with enhanced stability. Additionally, MapTracker [10] approaches the mapping problem as a tracking task, leveraging a memory of latents to ensure consistent reconstructions over time. This method adds a sensor stream into memory buffers of two latent representations: raster latents in bird’s-eye-view (BEV) space and vector latents over road elements. In these types of HD mapping methods, local HD maps are typically generated from perspective view (PV) images. This approach can limit model performance due to the need for PV-to-BEV conversion. In this work, we address this issue by developing HD maps based on MapTracker using top-down road images, known as tile images. We utilize NVIDIA’s road dataset to generate sequences of these top-down tile images. Additionally, we modify MapTracker to create BEV segmentation directly from the tile images, enabling the algorithm to generate vectors for local HD maps. These local maps are then integrated to produce comprehensive global HD maps."
https://arxiv.org/html/2411.02582v1,Real-Time Detection for Small UAVs: Combining YOLO and Multi-frame Motion Analysis,"Unmanned Aerial Vehicle (UAV) detection technology plays a critical role in mitigating security risks and safeguarding privacy in both military and civilian applications. However, traditional detection methods face significant challenges in identifying UAV targets with extremely small pixels at long distances. To address this issue, we propose the Global-Local YOLO-Motion (GL-YOMO) detection algorithm, which combines You Only Look Once (YOLO) object detection with multi-frame motion detection techniques, markedly enhancing the accuracy and stability of small UAV target detection. The YOLO detection algorithm is optimized through multi-scale feature fusion and attention mechanisms, while the integration of the Ghost module further improves efficiency. Additionally, a motion detection approach based on template matching is being developed to augment detection capabilities for minute UAV targets. The system utilizes a global-local collaborative detection strategy to achieve high precision and efficiency. Experimental results on a self-constructed fixed-wing UAV dataset demonstrate that the GL-YOMO algorithm significantly enhances detection accuracy and stability, underscoring its potential in UAV detection applications.","Since UAV technology emerged, its widespread application across various domains has raised significant safety risks and privacy concerns [1, 2, 3]. In response, the development of long-range drone detection technology has become critical, enabling the prompt identification, localization, and intervention of drones to safeguard public safety and personal privacy. However, existing detection technologies face considerable challenges when addressing small-pixel UAV targets at long distances [4, 5, 6]. Drones typically occupy less than 0.1%percent\%% of an image, leading to insufficient feature information. When combined with complex backgrounds, this results in decreased detection accuracy [7, 8]. Fig. 1 highlights some of the common challenges associated with detecting drone targets. Figure 1: Challenging conditions examples in UAV detections: 1) Minuscule targets with limited distinctive features; 2) Intricate backgrounds complicating target identification. While popular object detection methods like Fast-RCNN, YOLO, and DETR are highly effective for larger targets, they suffer from high false positive and false negative rates i+n long-range, small-object drone detection [9, 10, 11]. Recent researchers have developed specialized detection methods optimized for drone characteristics [12, 13, 14]. For example, by integrating appearance and motion features, using techniques like frame differencing or optical flow to extract moving objects, and applying classification methods to distinguish drone targets from other interfering objects. However, due to the diminutive size of drone targets, the amount of useful feature information available for classification is limited, and downsampling often leads to the loss of critical information. Additionally, the presence of diverse noise between drones and complex backgrounds makes it challenging to construct a precisely annotated dataset that can reliably distinguish targets from their surroundings. In this paper, we propose the GL-YOMO detection algorithm, which effectively integrates appearance and motion features. By enhancing the YOLO model, we aim to improve detection accuracy while reducing computational complexity. The algorithm incorporates multi-frame motion detection for secondary validation, ensuring precise detection of small objects without requiring a manually constructed classification dataset. The main contributions of this paper are as follows: 1) Development of the GL-YOMO Detection Algorithm: This algorithm combines YOLO object detection with multi-frame motion detection, leveraging YOLO’s efficient detection capabilities while incorporating motion feature capture to significantly enhance detection accuracy and stability. 2) Improvement of the YOLO Model: The enhancements increase detection accuracy and substantially reduce computational complexity and parameter count, resulting in a more efficient and lightweight model. 3) Design of a Template Matching-Based Motion Detection Algorithm: By analyzing pixel changes and displacement variations across three consecutive frames, this algorithm effectively detects extremely small objects, further improving small-object detection accuracy. 4) Construction of the Fixed-Wings Dataset: This dataset includes 13 video sequences and 24,951 frames, encompassing numerous UAV targets with an average image proportion of 0.01%, providing a robust resource for evaluating UAV detection algorithms. The paper is structured as follows: Section II reviews small object detection and related methods for UAV detection. Section III details our proposed method. Section IV presents experimental results and analysis. Section V concludes the paper and discusses future research directions."
https://arxiv.org/html/2411.02570v1,TI-PREGO: Chain ofThought andIn-Context Learning for Online Mistake Detection inPRoceduralEGOcentric Videos,"Identifying procedural errors online from egocentric videos is a critical yet challenging task across various domains, including manufacturing, healthcare, and skill-based training. The nature of such mistakes is inherently open-set, as unforeseen or novel errors may occur, necessitating robust detection systems that do not rely on prior examples of failure. Currently, however, no technique effectively detects open-set procedural mistakes online.We propose a dual-branch architecture to address this problem in an online fashion: one branch continuously performs step recognition from the input egocentric video, while the other anticipates future steps based on the recognition module’s output. Mistakes are detected as mismatches between the currently recognized action and the action predicted by the anticipation module. The recognition branch takes input frames, predicts the current action, and aggregates frame-level results into action tokens. The anticipation branch, specifically, leverages the solid pattern-matching capabilities of Large Language Models (LLMs) to predict action tokens based on previously predicted ones.Given the online nature of the task, we also thoroughly benchmark the difficulties associated with per-frame evaluations, particularly the need for accurate and timely predictions in dynamic online scenarios.Extensive experiments on two procedural datasets demonstrate the challenges and opportunities of leveraging a dual-branch architecture for mistake detection, showcasing the effectiveness of our proposed approach. In a thorough evaluation—including recognition and anticipation variants and state-of-the-art models—our method reveals its robustness and effectiveness in online applications.","Detecting procedural errors in videos has recently gained significant attention due to its potential to provide substantial benefits across various domains. Accurate mistake detection in videos of individuals performing tasks—such as recipe execution, object assembly, or managing complex workflows—holds promise for enhancing training and learning experiences by providing real-time feedback (see Figure 1). This immediate feedback allows for timely corrections, fostering faster skill development, skill acquisition, and a safer learning environment in high-stakes areas like surgery and aviation. Figure 1: Procedural Mistake Detection involves identifying errors within a procedural video, where each procedure consists of steps that can be correct or incorrect. The aim is to develop a method capable of analyzing a video and determining whether each frame contains a mistake. This concept is particularly useful when applied to egocentric videos, as they allow for direct feedback to the person performing the task. In the diagram, the leftmost column shows the steps that define a procedure, while the right side presents a sequence of frames, which can either indicate a mistake (red) or a correct action (green). The black arrow illustrates the flow of time, and the algorithm, depicted by a gear icon, takes the video sequence from time 00 to t−1𝑡1t-1italic_t - 1 as input, classifying the current frame t𝑡titalic_t as either correct (green) or a mistake (red). Advanced mistake detection models will become essential for ensuring precision, safety, and efficiency in procedural applications. This demand has driven the creation of new datasets [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12] and methodologies designed to advance procedure learning [13, 4, 14, 15, 1, 6, 16] and error detection models [3, 5, 8]. Currently, existing mistake detection approaches vary widely. Some methods emphasize action detection, identifying specific errors like missing steps or out-of-sequence actions [17], or incorrect oredering [16]. While other models bypass action detection altogether and monitor changes to the assembled object to ensure procedural accuracy [18, 19]. An ideal Procedural Mistake Detection (PMD) model should demonstrate two critical capabilities: robustness to diverse mistake types and the ability to provide online, timely feedback. First, a mistake detection system should identify any deviation from the correct procedure, regardless of the error’s nature. Since the range of correct methods is finite while possible mistakes are virtually unlimited, we use the One-Class Classification (OCC) framework. OCC trains on correct procedure videos only and tests on a mix of correct and incorrect cases, similar to anomaly detection [20, 21]. Second, the model should provide online feedback quickly enough to allow users to correct mistakes promptly, minimizing the reinforcement of incorrect actions. 111We distinguish an online setup, which allows slight processing delays while enabling effective feedback, from a real-time setup, which requires immediate responses. Additionally, an egocentric perspective enhances the model’s applicability by mirroring natural human perception, thereby increasing feasibility for real-world deployment in task execution scenarios. We propose a dual-branch approach combining step recognition and anticipation to detect mismatches in predicted and observed actions. The first branch, a step recognition module, processes the input video to identify the current action being performed. The second branch, a step anticipation module, predicts the next step based on previously recognized actions, utilizing a language model’s reasoning capabilities. Mistakes are flagged when there is a divergence between recognized and anticipated actions, highlighting inconsistencies between actual and expected steps. This paper extends our previous conference work, PREGO [22], by refining key components to improve procedural mistake detection. While PREGO’s evaluation of Large Language Models (LLMs) focused on a single model and fixed approach, our work conducts a comprehensive evaluation of LLMs as step anticipators, achieving state-of-the-art results on Assembly101-O and Epic-tent-O [22]. Specifically, we examine a broader range of LLMs, fine-tune them using Low-Rank Adaptation (LLoRA), and experiment with various prompting strategies, including Zero-Shot, Few-Shot, and Automatic Chain of Thought (Auto-CoT) [23], with Auto-CoT significantly enhancing the anticipation branch’s predictive abilities. Further, we address noise in the recognizer’s frame-by-frame predictions, which can disrupt action sequence continuity. To mitigate this, we extend beyond fixed-window aggregation to explore alternative strategies that reduce noise and improve prediction accuracy. We also introduce frame-level metrics for more precise evaluation, aligning the recognition module’s output with the anticipation module’s input. Given the online nature of this task, we emphasize accurate, timely per-frame evaluations in dynamic scenarios to ensure effective mistake detection and reduce response delays. Our contributions include (1) exploring and benchmarking multiple LLMs to advance PREGO’s foundational approach, (2) experimenting with various prompting techniques, including Auto-CoT, which enhances overall performance, (3) evaluating frame-level metrics for more precise system assessment compared to the original fixed window approach, and (4) investigating alternative aggregation techniques to improve mistake detection robustness and continuity. Extensive experimentation demonstrates the challenges and opportunities in dual-branch architectures and LLM-based step anticipation for open-set mistake detection. Our results underscore the potential of integrating step recognition and anticipation within a unified framework for online procedural error detection, offering a more flexible and effective solution for real-world applications."
https://arxiv.org/html/2411.02562v1,Segment Anything for Dendrites from Electron Microscopy††thanks:* Corresponding Authors,"Segmentation of cellular structures in electron microscopy (EM) images is fundamental to analyzing the morphology of neurons and glial cells in the healthy and diseased brain tissue. Current neuronal segmentation applications are based on convolutional neural networks (CNNs) and do not effectively capture global relationships within images. Here, we present DendriteSAM, a vision foundation model based on Segment Anything, for interactive and automatic segmentation of dendrites in EM images. The model is trained on high-resolution EM data from healthy rat hippocampus and is tested on diseased rat and human data. Our evaluation results demonstrate better mask quality compared to the original and other fine-tuned models, leveraging the features learned during training. This study introduces the first implementation of vision foundation models in dendrite segmentation, paving the path for computer-assisted diagnosis of neuronal anomalies.","Advancements in serial block-face scanning electron microscopy (SBF-SEM) allow researchers to capture brain tissue images at nanometer resolution across hundreds of micrometers [1, 2]. Segmenting complex ultrastructures such as axons, soma, and dendrites from electron microscopy (EM) data is critical for analyzing morphological parameters or reconstructing 3-dimensional (3D) neuronal networks. Dendrites, in particular, are intricate and varied, with numerous studies linking dendritic pathology to neurodegenerative diseases such as Alzheimer’s and Parkinson’s [3, 4, 5, 6]. While manual dendrite annotation has long been the benchmark, it is labor-intensive and time-consuming [7, 8]. Deep learning, especially convolutional neural networks (CNNs), has improved segmentation efficiency, but CNNs struggle with generalization across diverse datasets. Inspired by the success of large language models (LLMs), there is growing interest in applying foundation models to computer vision tasks. The name, foundation models, originates from their fundamentally central yet inherently incomplete nature, serving as a robust foundation that can be adapted to downstream tasks [9]. Models such as GPT-4 [10], PaLM-2 [11], and LLaMA [12] have benefited from the sheer scale of data they have been trained on and have demonstrated to generalize beyond the training data. Research on foundation models in computer vision, such as the segment anything model (SAM)[13] and segment-everything-everywhere model (SEEM) [14], demonstrates exceptional versatility in segmentation tasks. Notably, trained on around 1 billion masks from 11 million images, SAM is a state-of-the-art (SOTA) segmentation model. SAM has been widely studied and integrated into various downstream tasks, such as zero-shot abdominal organ segmentation in computed tomography (CT) [15] and skull-stripping in brain magnetic resonance imaging (MRI) [16]. SAM’s robustness has been found to be remarkable against various types of corruption, with the exception of blur-related corruption[17]. In spite of its generalizability gained from large training data, SAM’s performance degrades drastically when handling objects in medical imaging with blurred boundaries or weak contrast [18]. Therefore, further fine-tuning is essential for the precise segmentation of such objects. Here, we introduce DendriteSAM, a vision foundation model for segmenting dendrites from large-scale EM volumes, and evaluate it internally as well as externally and across subjects and species. Our contributions are: • we present the first application of a vision foundation model specialized in dendrite segmentation; • we evaluate DendriteSAM’s robustness both internally and externally through comprehensive quantitative and qualitative evaluation and demonstrate improved segmentation performance compared to SOTA segmentation models; • we investigate the impact of different image pre-processing methods on dendrite segmentation."
https://arxiv.org/html/2411.02553v1,Map++: Towards User-Participatory Visual SLAM Systems with Efficient Map Expansion and Sharing,"Constructing precise 3D maps is crucial for the development of future map-based systems such as self-driving and navigation. However, generating these maps in complex environments, such as multi-level parking garages or shopping malls, remains a formidable challenge. In this paper, we introduce a participatory sensing approach that delegates map-building tasks to map users, thereby enabling cost-effective and continuous data collection. The proposed method harnesses the collective efforts of users, facilitating the expansion and ongoing update of the maps as the environment evolves.We realized this approach by developing Map++, an efficient system that functions as a plug-and-play extension, supporting participatory map-building based on existing SLAM algorithms. Map++ addresses a plethora of scalability issues in this participatory map-building system by proposing a set of lightweight, application-layer protocols. We evaluated Map++ in four representative settings: an indoor garage, an outdoor plaza, a public SLAM benchmark, and a simulated environment. The results demonstrate that Map++ can reduce traffic volume by approximately 46% with negligible degradation in mapping accuracy, i.e., less than 0.03m compared to the baseline system. It can support approximately 2×2\times2 × as many concurrent users as the baseline under the same network bandwidth. Additionally, for users who travel on already-mapped trajectories, they can directly utilize the existing maps for localization and save 47% of the CPU usage.","Exploring and mapping uncharted environments has always been a captivating and enduring challenge, from the earliest human migrations to modern space exploration. Recently, with the advance of robotics and autonomous driving technology, high-resolution 3D maps have received a great deal of attention. Envision the following scenario: as you approach a massive, bustling parking garage unfamiliar to you, just minutes before a crucial meeting, you wish your car to autonomously locate an available parking spot and park itself securely. Given that numerous cars today can self-park (once the parking spot is identified), this aspiration is a reasonable leap forward. To realize this vision, a comprehensive, navigate-able 3D map of the garage is urgently needed. Figure 1. A User-Participatory SLAM system. Users upload data to contribute to the map (shown in the bottom left corner of the garage) on the server. The Simultaneous Localization and Mapping (SLAM) technique is crucial to building such 3D maps. SLAM enables continuous user localization/navigation while simultaneously modeling their environment using data collected from various sensors, such as cameras, depth sensors, LiDARs (Campos et al., 2021; Li et al., 2021), which can be attached to the user (e.g., the car, the smartphone) without any infrastructure within the environment. Despite extensive research on SLAM algorithms in the robotics domain, practical SLAM systems, particularly those capable of mapping sizable and complex areas such as multi-level parking garages, keeping the map up to date, and maintaining the map service for long periods of operations remain elusive. The main challenges in building a functional SLAM system arise from the difficulty of collecting comprehensive and fine-grained sensor data of the area of interest over a long period of time at a low cost (Lajoie et al., 2022; Zou et al., 2019). In this work, we propose collaborative user-participatory SLAM systems, shown in Fig. 1, that leverage the widespread availability of onboard cameras on users’ mobile devices or cars for gathering map data and constructing a global 3D map at edge/cloud servers. Users contribute to map construction in a laissez-faire fashion, not following instructions to move (Capponi et al., 2019). By harnessing the collective efforts of users, it facilitates convenient, low-cost, and continuous data collection, enabling the map to expand as users move through the space and keep updated as the environment changes. In turn, the users can obtain and utilize the up-to-date 3D map on their devices. Precisely, the devices capture the surroundings using cameras and upload the data to a map server, which then merges the data into the global map and conducts a global optimization process. We believe that this map-building and maintenance approach can provide effective solutions across numerous environments. The resulting 3D maps can serve as valuable additions to centralized commercial maps such as Google Maps, in terms of both map coverage and the ability to navigate mobile devices and cars. Despite its great promises, user-participatory SLAM faces several fundamental yet intertwined challenges. The primary challenge lies in the excessive map data redundancy. User-participatory mapping allows users to voluntarily contribute data, and upload it to the server while following their trajectories. When two devices capture the environment at similar locations, either simultaneously or at distinct instances, their sensing ranges are likely to overlap, producing redundant data. Given that most users traverse shared roads and paths, there exists a high degree of data redundancy, resulting in significant waste in network bandwidth, processing power, and memory usage. Additionally, the frequent transmission of map data may discourage user engagement due to limited (or, expensive) resources on users’ mobile devices (including cars). In addition to transmitting map data, several SLAM functions need to be performed on user devices, including pose estimation, map data generation, and local optimization, which can sum up to high resource consumption. In this work, we address this challenge by identifying the degree of redundancy between newly acquired map data and the existing global map and only requesting “fresh” data to be transmitted to the server. This can tremendously reduce resource consumption on the server as well as the participation cost of users. To achieve this goal, we devise a lightweight redundancy-checking mechanism utilizing two types of map metadata – the device dispatches, instead of the raw data, its pose (location and orientation) to the server, which then constructs a view cone representing the 3D field of view (FOV) of the camera at the pose. By evaluating the overlap between this view cone and the global map through an efficient spatial sampling technique, we can determine the overlap between the new map data and the global map with a minimal cost. Based on the overlap evaluation outcomes, we determine if the device’s current location is previously “seen” or new. If the location has been seen and mapped, the user does not need to upload map data, significantly reducing the processing/networking/memory resources. Meanwhile, the server shares the global map’s surrounding portion directly with the device. Leveraging the shared map, the device skips the expensive local optimization step, conserving computation resources and battery energy consumption. Furthermore, the server can suitably enlarge the shared map portion to include the device’s future locations, further reducing the system overhead. As such, for the first time, we can provide the 3D map service to passing-by devices, making the users feel more rewarded and worthwhile. If the location is new or partially new, the corresponding new map data must be uploaded to expand the global map. Towards this goal, we devise a redundancy control method, involving first removing all the map data that are redundant with the global map and then strategically injecting a minimal amount of redundant map data that are frequently observed and can thus be exploited for better map optimization purposes. In this work, we design Map++, an efficient user-participatory SLAM system that functions as a plug-and-play extension to support existing SLAM algorithms with minimal resource consumption. We have implemented Map++ and integrated it with the open-source project of Covins (Schmuck et al., 2021) based on ORB-SLAM3 (Campos et al., 2021), a state-of-the-art visual SLAM algorithm. To summarize, the main contributions of this paper are as follows: (1) We are the first, to the best of our knowledge, to propose a user-participatory SLAM framework that aims to build a shared map with low resource costs by exploiting user trajectory properties. Compared to a trajectory-unaware distributed SLAM system, our system maximizes the number of participating users under given resource constraints while maintaining SLAM accuracy. As the map expands, subsequent users can access the map as needed. Also, our system can support efficient map updates without incurring skyrocketing memory costs. (2) To minimize data redundancy for reduced computation and communication costs, we devise a set of protocols and algorithms, including metadata-based overlapping assessment, global map sharing for seen locations, and global map expansion for new locations. (3) We thoroughly evaluate the system in four distinct settings with heterogeneous cameras, including two real-world scenarios, a public dataset, and a simulated environment. Map++ manages to reduce approximately 46% traffic volume for map expansion with only a slight degradation in accuracy, i.e., less than 0.03m compared to the baseline system. Consequently, it can support approximately 2×2\times2 × as many users as the baseline under the same network bandwidth when they participate in mapping at the same time. Additionally, for users who travel on previously-mapped trajectories, they can directly utilize the existing maps for their operations such as localization and save 47% of the CPU usage. Figure 2. Overview of a vanilla shared-map architecture as discussed in (Schmuck et al., 2021). Each user uploads raw data (in the form of keyframes) to the server. The server merges the map from different users and conducts global optimization. Figure 3. Map++ Overview. The gray part (how to update) is not part of this work, and citations are given for further reference."
https://arxiv.org/html/2411.02545v1,TripletCLIP: Improving Compositional Reasoning of CLIP via Synthetic Vision-Language Negatives,"Contrastive Language-Image Pretraining (CLIP) models maximize the mutual information between textual and visual modalities to learn representations. However, the lack of compositional diversity in contemporary image-text datasets limits the compositional reasoning ability of CLIP. We show that generating “hard” negative captions via in-context learning and synthesizing corresponding negative images with text-to-image generators offers a solution. We introduce a novel contrastive pre-training strategy that leverages these hard negative captions and images in an alternating fashion to train CLIP. We demonstrate that our method, named TripletCLIP, when applied to existing datasets such as CC3M and CC12M, enhances the compositional capabilities of CLIP, resulting in an absolute improvement of over 9%percent99\%9 % on the SugarCrepe benchmark on an equal computational budget, as well as improvements in zero-shot image classification and image retrieval. Our code, models, and data are available at: \gradientRGBtripletclip.github.io92,51,23170, 108, 57.","Large-scale vision-language models, such as CLIP radford2021learning , have significantly advanced multi-modal learning by employing contrastive learning to acquire shared semantic representations from paired datasets. This approach has resulted in improved performance in vision-language tasks as well as zero-shot image classification song2022clip and segmentation jiao2023learning ; zhou2023zegclip . Beyond vision-language tasks, the individual components of these models, such as the vision encoder and the language encoder, are integral to several multimodal architectures and generative models such as multimodal large language models (MLLMs) NEURIPS2023_6dcf277e ; laurençon2024matters and text-to-image (T2I) diffusion models sauer2024fast ; patel2023eclipse ; patel2024eclipse . Yet, compositional reasoning remains challenging and multimodal models continue to exhibit naïve “bag of words” behavior, frequently failing to distinguish between expressions like “bulb in the grass” and “grass in the bulb” yuksekgonul2022and ; thrush2022winoground . Addressing this challenge remains critical for enhancing vision-language models and their downstream applications. Contrastive learning of representations benefits from “hard negative samples” (i.e., points that are difficult to distinguish from an anchor point) robinson2020contrastive . However, at each optimization step for training CLIP, image-text pairs are randomly sampled from the training dataset – this random sampling seldom exposes the model to highly similar negative pairs. We hypothesize that the limited compositional understanding of CLIP may stem from such issues in the optimization objective and sampling from training datasets. A straightforward solution could involve iteratively identifying hard negative pairs for each training iteration. However, due to the noisy captions and the scarcity of such pairs in existing datasets, prior work generates hard negative captions as a form of augmentation using rule-based strategies yuksekgonul2022and ; zhang2023contrasting . For instance, given an image-text pair labeled “a brown horse”, an additional negative caption “a blue horse” might be introduced. However in prior work, image data is not subjected to similar hard negative semantic augmentation during training; this is mainly because of the difficulty of making semantic perturbations at the pixel levels compared to sentence perturbation. While the text-only augmentation strategies have improved the models’ compositional understanding to a certain extent, it raises an intriguing question: could incorporating hard negative augmentation for both text and image modality further enhance the compositional reasoning capabilities of vision-language models? Motivated by this, in this paper, we introduce a novel, simple, and yet highly effective strategy for integrating hard negative images as well as hard negative text to enhance the compositional understanding of vision-language models. Recent developments in text-to-image diffusion models have opened up possibilities for performing semantic perturbations within images huang2024diffusion . Existing works have evaluated the impact of creating synthetic data for text-to-image generative models betker2023improving ; chatterjee2024getting . However, it remains less explored how these generative models can benefit the CLIP-like models. To tackle this challenge, our approach leverages the in-context learning capabilities of LLMs to produce realistic, linguistically accurate negative captions wang2022super . We then employ a pre-trained text-to-image diffusion model to create images corresponding to these captions, thereby enriching any given image-text dataset with valuable hard negatives that foster improved reasoning. This resulting TripletData comprises 13M image-text pairs to complement the CC3M and CC12M datasets Changpinyo_2021_CVPR . We developed TripletCLIP, which incorporates hard negative image-text pairs effectively by using them to optimize a novel triplet contrastive loss function. Extensive experiments on the CC3M and CC12M datasets and various downstream tasks with an equal compute budget demonstrate that TripletCLIP significantly enhances compositional reasoning. Notably, TripletCLIP results in more than 9% and 6% absolute improvement on the SugarCrepe benchmark compared to LaCLIP lai2023scarcity and NegCLIP yuksekgonul2022and , respectively. TripletCLIP also improves zero-shot classification and image-text retrieval performance with similar training-time concept diversity. An investigation into the effects of increasing training-time concept diversity revealed that baseline models consistently under-performed in compositional tasks despite an increase in integrated knowledge, while TripletCLIP demonstrated significant improvements. In summary, our key contributions are as follows: • We introduce a novel CLIP pre-training strategy that employs hard negative text and images in conjunction with triplet contrastive learning to enhance compositionality. • TripletCLIP consistently improves across downstream tasks, demonstrating the effectiveness of synthesizing hard negative image-text pairs. • Our extensive ablations on the choice of the loss function, modality-specific pre-training, the increase in concept diversity, and filtering high-quality TripletData provide deeper insights into the utility of hard negative image-text pairs for CLIP pre-training. • Ultimately, we present a promising avenue where synthetic contrastive datasets significantly improve reasoning capabilities, leading to the creation and release of the TripletData — a 13M contrastive image-text dataset."
https://arxiv.org/html/2411.02537v2,Inquire: A Natural WorldText-to-Image Retrieval Benchmark,"We introduce Inquire, a text-to-image retrieval benchmark fdesigned to challengemultimodal vision-language models. on expert-level queries Inquire includes iiNaturalist 2024 (iNat24) a new dataset of five million natural world images, along with 250 expert-level retrieval queries. These queries are paired with all relevant images comprehensively labeled within iNat24, comprising 33,000 total matches. Queries span categories such as species identification, context, behavior, and appearance, emphasizing tasks that require nuanced image understanding and domain expertise. Our benchmark evaluates two core retrieval tasks: (1) Inquire-Fullrank, a full dataset ranking task, and (2) Inquire-Rerank, a reranking task for refining top-100 retrievals. Detailed evaluation of a range of recent multimodal models demonstrates that Inquire poses a significant challenge, with the best models failing to achieve an mAP@50 above 50%. In addition, we show that reranking with more powerful multimodal models can enhance retrieval performance, yet there remains a significant margin for improvement. By focusing on scientifically-motivated ecological challenges, Inquire aims to bridge the gap between AI capabilities and the needs of real-world scientific inquiry, encouraging the development of retrieval systems that can assist with accelerating ecological and biodiversity research.","Recent advances in multimodal learning have resulted in advanced models Radford et al. [2021], Liu et al. [2023], Achiam et al. [2023] that demonstrate remarkable generalization capabilities in zero-shot classification Radford et al. [2021], Zhai et al. [2023], visual question-answering (VQA) Li et al. [2022], Yu et al. [2022], Alayrac et al. [2022], Li et al. [2023], and image retrieval Yu et al. [2022], Li et al. [2023]. These models offer the potential to assist in the exploration, organization, and extraction of knowledge from large image collections. However, despite this success, there remains a significant gap in the evaluation of these models on domain-specific, expert-level queries, where nuanced understanding and precise retrieval are critical. Addressing this gap is essential for future deployment in specialized fields such as biodiversity monitoring and biomedical imaging, among other scientific disciplines. Previous studies of the multimodal capabilities of this new generation of models have primarily focused on the task of VQA. In VQA, it has been demonstrated that there remains a large performance gap between state-of-the-art models and human experts in the context of challenging perception and reasoning queries such as those found on college-level exams Yue et al. [2024], Zhong et al. [2023]. However, no such expert-level benchmark exists for image retrieval. The most commonly used text-to-image retrieval benchmarks are derived from image captioning datasets, and contain simple queries related to common everyday categories Young et al. [2014], Lin et al. [2014]. Current multimodal models achieve near perfect performance on some of these benchmarks, indicating that they no longer pose a challenge (e.g., BLIP-2 Li et al. [2023] scores 98.9 on Flickr30K Young et al. [2014] top-10). Existing retrieval datasets are generally small Philbin et al. [2007, 2008], Young et al. [2014], Lin et al. [2014], limited to a single visual reasoning task (e.g., landmark-location matching Philbin et al. [2007, 2008], Weyand et al. [2020]), and lack concepts that would require expert knowledge Philbin et al. [2007, 2008], Weyand et al. [2020], Young et al. [2014], Lin et al. [2014]. These limitations impede our ability to track and improve image retrieval capabilities. Figure 2: Category breakdown for the fine-grained queries that make up Inquire. Each query category falls under one of the following supercategories: Species, Context, Behavior, or Appearance. A domain that is well-suited for studying this problem is the natural world, where images collected by enthusiast volunteers provide vast and largely uncurated sources of publicly available scientific data. In particular, the iNaturalist iNa [b] platform contains over 180 million species images and contributes immensely to research in biodiversity monitoring Chandler et al. [2017], Lohan [2024]. These images also contain a wealth of “secondary data” not reflected in their species labels Pernat et al. [2024], including crucial insights into interactions, behavior, morphology, and habitat that could be uncovered through searches. However, the time-consuming and expert-dependent analysis needed to extract such information prevents scientists from taking advantage of this valuable data at scale. This cost is amplified as scientists typically want to retrieve multiple relevant images for each text query, so that they can track changes of a property over space and time Young et al. [2019]. This domain serves as an ideal testbed for expert image retrieval, as these images contain expert-level diverse and composite visual reasoning problems, and progress in this field will enhance impactful scientific discovery. In this work, we introduce Inquire, a new dataset and benchmark for expert-level text-to-image retrieval and reranking on natural world images. Inquire includes the iNat24 dataset and 250 ecologically motivated retrieval queries. The queries span 33,000 true-positive matches, pairing each text query with all relevant images that we comprehensively labeled among iNat24’s five million natural world images. iNat24 is sampled from iNaturalist iNa [b], and contains images from 10,000 different species collected and annotated by citizen scientists, providing significantly more data for researchers interested in fine-grained species classification. The queries contained within Inquire come from discussions and interviews with a range of experts including ecologists, biologists, ornithologists, entomologists, oceanographers, and forestry experts. Our evaluation of multimodal retrieval methods demonstrates that Inquire poses a significant challenge, necessitating the development of models able to perform expert-level retrieval within large image collections. A key finding from our experiments is that reranking, a technique typically used in text retrieval Nogueira and Cho [2019], Khattab and Zaharia [2020], Karpukhin et al. [2020], offers a promising avenue for improvement in image retrieval. We hope that Inquire will inspire the community to build next-generation image retrieval methods towards the ultimate goal of accelerating scientific discovery. We make Inquire, the iNat24 dataset, pre-computed outputs from state-of-the-art models, and code for evaluation available at https://inquire-benchmark.github.io/. Figure 3: Proportion of queries in Inquire associated with each iconic group of species. Table 1: Comparison to common datasets used to evaluate text-to-image retrieval Gadre et al. [2023]. Unlike other datasets, Inquire has significantly more images and many matches per query rather than exactly one. MpQ: Matches per query Dataset Images Queries MpQ Expert Flickr30k Young et al. [2014] 1,000 5k 1 ✗ COCO Lin et al. [2014] 5,000 25k 1 ✗ Inquire 5,000,000 250 1–1.5k ✓"
https://arxiv.org/html/2411.02477v1,Building a Synthetic Vascular Model: Evaluation in an Intracranial Aneurysms Detection Scenario,"We hereby present a full synthetic model, able to mimic the various constituents of the cerebral vascular tree, including the cerebral arteries, bifurcations and intracranial aneurysms. This model intends to provide a substantial dataset of brain arteries which could be used by a 3D convolutional neural network to efficiently detect Intra-Cranial Aneurysms. The cerebral aneurysms most often occur on a particular structure of the vascular tree named the Circle of Willis. Various studies have been conducted to detect and monitor the aneurysms and those based on Deep Learning achieve the best performance. Specifically, in this work, we propose a full synthetic 3D model able to mimic the brain vasculature as acquired by Magnetic Resonance Angiography, Time Of Flight principle. Among the various MRI modalities, this latter allows for a good rendering of the blood vessels and is non-invasive. Our model has been designed to simultaneously mimic the arteries’ geometry, the aneurysm shape, and the background noise. The vascular tree geometry is modeled thanks to an interpolation with 3D Spline functions, and the statistical properties of the background noise is collected from angiography acquisitions and reproduced within the model. In this work, we thoroughly describe the synthetic vasculature model, we build up a neural network designed for aneurysm segmentation and detection, finally, we carry out an in-depth evaluation of the performance gap gained thanks to the synthetic model data augmentation.","\IEEEPARstart This work has been carried out in the context of a wide medical research project in which neuroradiologists intend to estimate the risk of occurrence and/or rupture of Intra-Cranial Aneurysms (ICA) [1]. The advent of ICA formation results from various factors, among which the genetic risk seems predominant [2]. However, it is commonly accepted among physicians that the geometric disposition of the cerebral vascular tree might explain why a weakened vessel wall (due to genetic or environmental factors) might give rise to an aneurysm. Untreated brain aneurysms pose a significant risk of rupture, which can result in a hemorrhagic stroke. In fact, this rupture can potentially lead to the patient’s death in as much as 50%percent5050\%50 % of all cases. Magnetic Resonance Angiography (MRA), Time of Flight (TOF) modality is frequently used for aneurysms detection [3]. Unlike other methods like Digital Subtraction Angiography (DSA) and Computed Tomographic Angiography (CTA), TOF is radiation-free and doesn’t require the administration of a contrast agent [4]. Given the mounting workload and the demanding nature of the detection process undertaken by radiologists, there is an increasing need for an automated tool to detect and monitor aneurysms at an early stage. Prior to the widespread adoption of Deep Learning (DL), research studies employed imaging filters or traditional machine learning techniques to detect aneurysms [5, 6]. Recent advances in artificial intelligence, particularly those involving deep Convolutional Neural Networks (CNNs), have significantly enhanced the development of automatic tools in the field of medical imaging [7]. To date, several deep learning based approaches have been proposed for ICA segmentation and/or detection [8, 9, 10, 11]. The ADAM Challenge compared 11 different DL approaches for detecting or/and segmenting ICAs on TOFs. The best algorithm, [12] achieved notable results, with an average sensitivity of 0.670.670.670.67 and a false positive rate of 0.130.130.130.13. It is important to note that a majority of the existing methods have been formulated using private clinical data that comes with meticulously refined manual annotations. Indeed, one of the obstacles in developing deep learning methods for medical imaging applications is the lack of large annotated datasets, particularly for the segmentation task. To mitigate this, Di Noto et al. [13] proposed the use of “weak” annotations and they obtained good results with an average lesion sensitivity of 0.830.830.830.83 and a false positive rate of 0.80.80.80.8. Other studies adopt non-voxel-based methods, such as mesh convolutional neural networks [14], to overcome limitations related to modality and scan acquisition parameters. The rationale behind our work is to try to reduce as much as possible, or even to free oneself from any manual labeling. In other words, we expect that using several hundreds or even thousands of modeled bifurcation to train a network might provide better performances than using only a couple of hundreds actual TOF segmentations. Unlike previous works, in our approach, we investigate the brain aneurysm detection task by exploiting synthetic data. While data augmentation stands out as a well-known technique for augmenting the number of data samples, its application requires careful consideration. In the context of medical images, such image manipulations might tamper with the geometrical or statistical properties in an undesirable way, i.e. render the augmented images too distant from their corresponding ground truth. In the past, several works have been devoted to the design of computer models intended to mimic arterial trees. At that time, the studies focused on constrained constructive optimization [15]. Some models were particularly designed to offer a high graphical fidelity through a better understanding of the biophysical properties [16]. A relatively nice rendering was obtained on liver vascular trees for instance in [17]. Such models were mostly designed in the aim to study angiogenesis (physiological process leading to the formation of new blood vessels.). More recently, the VascuSynth model [18] was proposed in the aim to produce vast amounts of volumetric vasculature images. Here, the aim was different, the authors intended to generate a synthetic dataset for image segmentation. All these computer models achieved a quite accurate modeling of the acquired medical images (mostly trying to mimic liver or lung vasculatures). However, modeling the cerebral vascular tree is more challenging, as the arteries are commonly longer, and may exhibit a stronger tortuosity. Moreover, in our study, the goal strongly differs. We intend to generate vast amounts of images to train a neural network for a pattern recognition task. Another interesting approach was proposed in [19, 20] where the authors generate synthetic MRI patches using Generative Adversarial Networks. Indeed, random patches were generated along with their underlying ground truth segmentation. This method has not been designed to generate a given target bifurcation, or to add an aneurysm onto the vascular tree, which is an issue we tackle in our work. In a previous study [21], we have proposed a fully synthetic model of 3D bifurcations and Intra-Cranial Aneurysms. In this work, we tried to mimic the geometrical shape of arterial bifurcations by generating linear segments to be later convolved by a spherical kernel, and apply some geometric distortions to model the tortuosity. A particular attention was devoted to the generation of a plausible background noise. Ultimately, an intracranial aneurysm was modeled and superimposed onto the bifurcation. Although this initial model proved to mimic relatively well 3D bifurcations, it showed some limitations, regarding the tortuosity, and the aneurysm location (slightly shifted away from the bifurcation). In the current work, our model accuracy is considerably increased, and we intend to propose a set of highly realistic bifurcations and aneurysms. Since our intended application involves identifying an aneurysm within a specific bifurcation or artery from a TOF scan, it is crucial to accurately model various essential arterial features: the shape, orientation, diameters, and tortuosity. As for the aneurysms, the model should allow to adjoin ICAs of various shapes and sizes onto different bifurcations. Finally, the performances in terms of image segmentation might depend on the accuracy of the modeled surrounding background noise, hence, it is important for the model to faithfully duplicate the background noise. In section 2, we thoroughly describe the synthetic model. Its three main features are presented, namely i) the arteries geometry, ii) the surrounding TOF noise and iii) the modeled aneurysm. Next, in section 3 we provide an in-depth description of the generated dataset, we describe the CNN architecture, evaluate both the ICA segmentation and detection performances. We try to assess the performance gain brought by using the synthetic images alongside the manually labeled ones. To do so, we run two separate experiments involving either the manually segmented images only or adjoining the modeled patches. Finally, in section 4, we discuss the benefit of using the synthetic model for intracranial aneurysm detection and conclude this work."
https://arxiv.org/html/2411.02470v1,Benchmarking XAI Explanations with Human-Aligned Evaluations,"In this paper, we introduce PASTA (Perceptual Assessment System for explanaTion of Artificial intelligence), a novel framework for a human-centric evaluation of XAI techniques in computer vision. Our first key contribution is a human evaluation of XAI explanations on four diverse datasets—COCO, Pascal Parts, Cats Dogs Cars, and MonumAI—which constitutes the first large-scale benchmark dataset for XAI, with annotations at both the image and concept levels. This dataset allows for robust evaluation and comparison across various XAI methods. Our second major contribution is a data-based metric for assessing the interpretability of explanations. It mimics human preferences, based on a database of human evaluations of explanations in the PASTA-dataset. With its dataset and metric, the PASTA framework provides consistent and reliable comparisons between XAI techniques, in a way that is scalable but still aligned with human evaluations. Additionally, our benchmark allows for comparisons between explanations across different modalities, an aspect previously unaddressed. Our findings indicate that humans tend to prefer saliency maps over other explanation types. Moreover, we provide evidence that human assessments show a low correlation with existing XAI metrics that are numerically simulated by probing the model.","As Deep Neural Networks (DNNs) systems are being used in increasingly high stakes domains (e.g., legal, medical) (Surden, 2021; Litjens et al., 2017), it is essential that humans are able to interpret how they reach their conclusions. (Bender et al., 2021). Their lack of transparency has led them to be characterized as “black boxes” (Castelvecchi, 2016), which is particularly problematic in critical applications where understanding the decision-making process is essential for trust and accountability (Vereschak et al., 2024), leading to the creation of a relatively new field: explainable AI (XAI) (Gunning et al., 2019). XAI aims to make the workings of deep learning models more transparent and interpretable. XAI methods fall into two main categories: post-hoc techniques (Selvaraju et al., 2017; Ribeiro et al., 2016; Lundberg & Lee, 2017) and ante-hoc techniques (Bennetot et al., 2022; Koh et al., 2020). Post-hoc techniques generally explain the output of a frozen, pretrained DNN, while ante-hoc techniques modify the architecture of the DNN to improve its interpretability from the outset. Each of these categories can be further subdivided into various sub-families, offering a wide array of XAI approaches. The diversity of XAI techniques calls for an effort to standardize their evaluation and comparison. Although there are toolkits in computer vision that offer a range of computational evaluation techniques (Hedström et al., 2023; Fel et al., 2022a), to our knowledge there has been no effort to standardize their evaluation from a perceptual point of view (Nauta et al., 2023), i.e., the way the explanation is perceived by the human for whom it was intended. Currently, prevalent approaches (Dawoud et al., 2023; Colin et al., 2022) to evaluating XAI techniques involve human annotators assessing and ranking their interpretability. This approach aligns with XAI’s goal of improving the human interpretability of DNN models. Yet, this method is costly since it requires paying annotators and is impractical for widespread use, as each new XAI technique necessitates a fresh round of human evaluation. It is also at risk of being inconsistent and unreliable since evaluations may differ from one annotator to another and depend on factors such as fatigue and even the time of day (Schmidt et al., 2007). To address the challenges associated with evaluating XAI techniques, we propose the Perceptual Assessment System for explanaTion of Artificial intelligence (PASTA). PASTA aims to automate the evaluation of XAI techniques by providing an evaluation metric that mimics human assessments. The first component of PASTA is a dataset composed of four diverse datasets (COCO, Pascal Part, Cats Dogs Cars, and Monumai), which includes both image and concept annotations. Using this dataset, we compare 21 XAI methods across multiple model architectures. We subject the resulting explanations to a rigorous evaluation by human annotators, along a comprehensive set of criteria that cover a variety of desired properties. The second component of PASTA is a metric designed to replicate human evaluation on the PASTA-dataset. While there are benchmarks that focus on perceptual evaluation of XAI methods (Colin et al., 2022; Dawoud et al., 2023), to the best of our knowledge, we are the first to integrate both saliency-based and concept-based explanations into a unified framework. Additionally, our approach addresses multiple dimensions of human assessment by incorporating a diverse set of questions for users. The primary contributions of this paper are as follows: • Comprehensive XAI Benchmark: We establish a dataset, the PASTA-dataset, designed to evaluate XAI methods across various modalities, including image and concept-based explanations (Sect. 3.1). • Extensive Evaluation of XAI Methods: We conduct a large-scale evaluation of 21 XAI methods, comparing both post-hoc and ante-hoc approaches across multiple datasets (Sect. 3.2—3.4). Our findings indicate that saliency and input perturbation-based techniques, such as LIME and SHAP, are favored for their effectiveness in interpreting model predictions (Sect. 3.5). • Human-AI Correlation: Our findings reveal a low correlation between widely used XAI metrics and human assessments, suggesting that these metrics cover complementary aspects (Sect 3.6). • Human-aligned Perception Metric for Explanations: We introduce a novel, data-based metric, which we call the PASTA-metric, that automates the scoring of XAI techniques along human-like interpretability criteria (Sect. 4). Automated yet human-aligned metrics such as the PASTA-metric may serve not only to streamline the evaluation process of XAI techniques but also to foster a more transparent and trustworthy AI ecosystem, where deep learning models are comprehensible and their decisions justifiable. The complete PASTA framework (code, annotation, and models) will be released after the reviews."
https://arxiv.org/html/2411.02456v1,A Study of Data Augmentation Techniques to Overcome Data Scarcity in Wound Classification using Deep Learning,"Chronic wounds are a significant burden on individuals and the healthcare system, affecting millions of people and incurring high costs. Wound classification using deep learning techniques is a promising approach for faster diagnosis and treatment initiation. However, lack of high quality data to train the ML models is a major challenge to realize the potential of ML in wound care. In fact, data limitations are the biggest challenge in studies using medical or forensic imaging today. We study data augmentation techniques that can be used to overcome the data scarcity limitations and unlock the potential of deep learning based solutions. In our study we explore a range of data augmentation techniques from geometric transformations of wound images to advanced GANs, to enrich and expand datasets. Using the Keras, Tensorflow, and Pandas libraries, we implemented the data augmentation techniques that can generate realistic wound images. We show that geometric data augmentation can improve classification performance, F1 scores, by up to 11% on top of state-of-the-art models, across several key classes of wounds. Our experiments with GAN based augmentation prove the viability of using DE-GANs to generate wound images with richer variations. Our study and results show that data augmentation is a valuable privacy-preserving tool with huge potential to overcome the data scarcity limitations and we believe it will be part of any real-world ML-based wound care system.","Wounds, the modern day focus of extensive medical and forensic research, play an important role in quality of life, causing an immense amount of complications, ranging from small to large-scale. A chronic wound is defined as a wound that has been open for more than a month and that has not healed normally [1]. Chronic wounds are a significant burden on individuals and the healthcare system, affecting millions of people and incurring high costs. Most common in elderly people, they affect around 8.2 million medicare beneficiaries in the United States, which shows the astounding ubiquity of wounds [1]. Chronic wound classes include, but are not limited to, diabetic, pressure, surgical, and venous ulcers. Each of these types of wounds are created by a multitude of factors, such as diabetes, surgery, and venous insufficiencies. More than being painful and inconvenient, chronic wounds are a financial burden as well. Costs of individual patient care alone for pressure ulcers is $20,900 minimum for each patient, with additional costs amounting to $43,180 per year [1]. Given the substantial impact a single wound can have on an individual’s quality of life, the cumulative effect on the larger population diagnosed with chronic wounds is significant. Therefore, developing methods and systems for efficient, effective and accurate wound care are of tremendous importance to society. The length and complexity of the medical care process often lead to individuals avoiding medical consultation, exacerbating their wound’s condition. Automated techniques based on deep learning algorithms can be used to classify wounds, allowing for quicker diagnosis and treatment initiation. They can assist doctors in saving time and focusing on more critical steps of wound care. ML(machine learning)-based wound classification can improve the quality of life in many people around the world. Many studies have concluded that application of automated techniques in wound classification and care is essential for the forensic and medical industry’s improvements [2], [3], [4]. However, large amounts of labeled data is required for developing deep learning models that can help solve wound classification. Collecting large amounts of real patient wound images is often not feasible due to a variety of privacy and legal concerns. In almost all use of deep learning techniques in forensic and medical sciences, there is the limitation of not having enough readily available images or data to train models – this is also called out in the conclusions of these wound classificiation studies [5], [6], and [7]). Such data limitations are a major challenge in realizing the potential that these ML techniques have in transforming wound care. Our study investigates the potential of state-of-the-art data augmentation techniques to overcome the data scarcity limitations in building real-world wound care systems. We explore two categories of data augmentation techniques, one based on geometric transformations and the other based on generative adversarial networks (GAN) [8]. Both can generate realistic wound images which can then be used to train deep learning models that achieve higher wound classification accuracy. Our study uses state-of-the-art computer vision (CV) models and transfer learning[9] as a baseline to mimic real-world ML-based wound care applications – as a part of this, we also show how modern CV models can be adapted to wound classification via transfer learning. Our results show that geometric data augmentation can provide significant improvements (up to 11% in F1 scores) on top of state-of-the-art models, and prove the viability of using GAN-based models to generate richer wound images. In the next section we describe various research studies that have shown the viability of deep learning for wound care, the applicability of data augmentation, and the broad limitations arising from lack of good labeled data. In Section III we outline the datasets we use, the augmentation and transfer learning[9] techniques we have implemented and evaluated. Section IV summarizes our key results and we discuss these results in Section V. The final section provides a conclusion and outlines future work."
https://arxiv.org/html/2411.02452v1,Goal-Oriented Semantic Communication for Wireless Visual Question Answering with Scene Graphs,"As demands for communication and computational capabilities escalate, traditional bit-oriented communication falls short of these stringent requirements, especially for mission-critical and computation-intensive applications. Visual Question Answering (VQA), a representative application, has adopted edge computing to mitigate local computational constraints and accelerate visual perception with natural language. However, it encounters significant communication challenges such as limited bandwidth, reduced transmission power, and increased noise levels, leading to considerable latency and reduced efficiency in image and question transmission. we propose a goal-oriented semantic communication (GSC) framework that focuses on effectively extracting and transmitting semantic information most relevant to the VQA goals, improving the answering accuracy and enhancing the effectiveness and efficiency. The objective is to maximize the answering accuracy, and we propose a scene graphs (SG)-based image semantic extraction and ranking approach to prioritize the semantic information based on the goal of questions. Experimental results demonstrate that our GSC framework improves answering accuracy by up to 59% under Rayleigh channels while reducing total latency by up to 65% compared to traditional bit-oriented transmission.","The rise of artificial intelligence (AI) has driven the need for highly efficient communication services to support computation-intensive applications that require real-time data processing. Visual Question Answering (VQA) is a prime example, combining visual perception with natural language processing to generate accurate responses [1]. Traditionally, VQA tasks are executed on local devices like phones, laptops, and UAVs, which suffer from high computation latency due to limited processing power. Edge computing mitigates these issues by offloading computation from local devices to edge servers [2]. However, this brings new communication challenges between the end devices and the edge, such as limited bandwidth, channel noise, and multipath effects [3]. This constraint poses challenges for VQA, where the fast and reliable transmission of questions and images is crucial. Consequently, communication efficiency is compromised, leading to delays or potential errors in answer generation. To tackle these limitations, goal-oriented semantic communication has been introduced [4]. Unlike traditional image compression techniques, which operate at the pixel level without considering the semantic significance of the data, semantic communication focuses on transmitting only semantically significant information, eliminating redundant data and improving the overall accuracy of VQA tasks [5]. Recently, advancements and applications of scene graph (SG)-based methods have been used to address VQA tasks [6]. SGs abstract information from images, with nodes representing detected objects and edges indicating the relationships between them, making this approach well-suited for VQA tasks involving complex relational questions. The work in [7] introduced a transformer-based iterative refinement process to enhance the accuracy of SGs by systematically removing unnecessary or irrelevant edges. Furthermore, a semantic scoring mechanism was introduced in [8], where the concept of image-to-graph semantic similarity (ISS) was used to rank semantic triplets by considering the frequency and probability of different categories in the dataset. However, these approaches were data-centric, focusing solely on data-related information in the SG refinement process, and were therefore unable to effectively handle VQA questions with varying emphases and structures. Additionally, little consideration was given to wireless channel conditions, such as noise, fading, and signal-to-noise ratio, and their potential impact on the quality and quantity of transmitted semantic information. To deal with these challenges, we define the goal-oriented semantic information to be extracted and transmitted in this paper, and propose a novel goal-oriented semantic communication (GSC) framework that focuses on effectively extracting and transmitting semantic information most relevant to the VQA goals, thereby improving the answering accuracy and enhancing the effectiveness and efficiency. We develop and evaluate bit-oriented and SG-based methods. To achieve high answering accuracy while maintaining low latency, we offer a comprehensive comparison and practical guidelines for appropriate transmission policies. The contributions of our research can be summarized as follows. • We propose an innovative GSC framework for an edge-enabled wireless VQA system operating over bandwidth-limited noisy wireless channels. The framework consists of an information base for knowledge sharing, a question parser for natural language programming, a semantic extractor for visual semantic extraction, GSC wireless communication with semantic ranking, and an answer reasoner. • We formulate the problem as maximizing the average answering accuracy, and propose a novel GSC solution that generates and prioritizes SGs using the proposed GSC ranking strategy to ensure the transmission of the most question-relevant objects. • We conduct extensive experiments to compare our proposed GSC framework with the traditional bit-oriented method and state-of-the-art semantic approaches. The results show that our GSC method outperforms the compared semantic methods, improving answering accuracy by up to 49%. Additionally, it reduces total question execution latency by up to 65% compared to bit-oriented transmission. The rest of the paper is organized as follows. In Section II, we introduce our system model and problem formulation. In Section III, we present the GSC framework design and the proposed GSC ranking method. Simulation results are presented in Section IV, followed by conclusions in Section V."
https://arxiv.org/html/2411.02445v1,WiCV@CVPR2024: The Thirteenth Women In Computer Vision Workshop at the Annual CVPR Conference,"In this paper, we present the details of Women in Computer Vision Workshop - WiCV 2024, organized alongside the CVPR 2024 in Seattle, Washington, United States. WiCV aims to amplify the voices of underrepresented women in the computer vision community, fostering increased visibility in both academia and industry. We believe that such events play a vital role in addressing gender imbalances within the field. The annual WiCV@CVPR workshop offers a) opportunity for collaboration between researchers from minority groups, b) mentorship for female junior researchers, c) financial support to presenters to alleviate financial burdens and d) a diverse array of role models who can inspire younger researchers at the outset of their careers. In this paper, we present a comprehensive report on the workshop program, historical trends from the past WiCV@CVPR events, and a summary of statistics related to presenters, attendees, and sponsorship for the WiCV 2024 workshop.","Despite remarkable progress in various computer vision research areas in recent years, the field still grapples with a persistent lack of diversity and inclusion. While the field of computer vision rapidly expands, female researchers remain underrepresented in the area, constituting only a small amount of professionals in both academia and industry. Due to this, many female computer vision researchers can feel isolated in workspaces which remain unbalanced due to the lack of inclusion. The WiCV workshop is a gathering designed for all individuals, irrespective of gender, engaged in computer vision research. It aims to appeal to researchers at all levels, including established researchers in both industry and academia (e.g. faculty or postdocs), graduate students pursuing a Masters or PhD, as well as undergraduates interested in research. The overarching goal is to enhance the visibility and recognition of female computer vision researchers across these diverse career stages, reaching women from various backgrounds in educational and industrial settings worldwide. There are three key objectives of the WiCV workshop: Networking and Mentoring The first objective is to expand the WiCV network and facilitate interactions between members of this network. This includes female students learning from seasoned professionals who share career advice and experiences. A mentoring banquet held alongside the workshop provides a casual environment for junior and senior women in computer vision to meet, exchange ideas and form mentoring or research relationships. Raising Visibility The workshop’s second objective is to elevate the visibility of women in computer vision, both at junior and senior levels. Senior researchers are invited to give high quality keynote talks on their research, while junior researchers are encouraged to submit their recent or ongoing work, with many of these being selected for oral or poster presentation through a rigorous peer review process. This empowers junior female researchers to gain experience presenting their work in a professional yet supportive setting. The workshop aims for diversity not only in research topics but also in the backgrounds of presenters. Additionally, a panel discussion provides a platform for female colleagues to address topics of inclusion and diversity. Supporting Junior Researchers Finally, the third objective is to offer junior female researchers the opportunity to attend a major computer vision conference that might otherwise be financially inaccessible. This is made possible through travel grants awarded to junior researchers who present their work during the workshop’s poster session. These grants not only enable participation in the WiCV workshop but also provide access to the broader CVPR conference."
https://arxiv.org/html/2411.02441v1,Cross-D Conv: Cross-Dimensional Transferable Knowledge Basevia Fourier Shifting Operation,"In biomedical imaging analysis, the dichotomy between 2D and 3D data presents a significant challenge. While 3D volumes offer superior real-world applicability, they are less available for each modality and not easy to train in large scale, whereas 2D samples are abundant but less comprehensive. This paper introduces the Cross-D Conv operation, a novel approach that bridges the dimensional gap by learning the phase shifting in the Fourier domain. Our method enables seamless weight transfer between 2D and 3D convolution operations, effectively facilitating cross-dimensional learning. The proposed architecture leverages the abundance of 2D training data to enhance 3D model performance, offering a practical solution to the multimodal data scarcity challenge in 3D medical model pretraining. Experimental validation on the RadImagenet (2D) and multimodal (3D) sets demonstrates that our approach achieves comparable or superior performance in feature quality assessment comparable to conventional methods. The enhanced convolution operation presents new opportunities for developing efficient classification and segmentation models in medical imaging. This work represents an advancement in cross-dimensional and multi-modal medical image analysis, offering a robust framework for utilizing 2D priors in 3D model pretraining or vice versa while maintaining computational efficiency.","The field of computer vision has traditionally focused on 2D image analysis due to its relative simplicity and suitability for a wide range of tasks, including those in biomedical imaging. However, the medical domain often requires the analysis of 3D images, as seen in modalities like computed tomography (CT), magnetic resonance imaging (MRI), and ultrasound (US). The emergence of large-scale volumetric biomedical datasets has recently marked a significant advancement in medical imaging research Li et al. . However, the transition from 2D to 3D analysis presents substantial challenges that demand a non-trivial solution. Two critical challenges stand at the forefront of this transition. First, while 2D datasets are abundant and well-curated across various modalities, their 3D counterparts remain limited in both scale and diversity. Second, the direct adaptation of existing 2D datasets for 3D applications poses significant technical hurdles, highlighting the growing importance of cross-dimensional learning approaches. These challenges motivate a fundamental research question: Can we design a novel convolutional operation that effectively bridges the gap between 2D and 3D training paradigms? Figure 1: Architectural diagram of the Cross-D Conv operation workflow. The process transforms 2D input tensors through: (1) rotation parameter generation from spatial coordinates, (2) Fourier transform and phase shifting, and (3) projection of 3D convolutional weights onto 2D kernels. The pipeline enables both 2D convolution with transformed weights and direct 3D convolution operations. Green blocks indicate trainable parameters, while orange blocks represent I/O tensors. To address this fundamental research question, we propose to investigate the intricate relationship between 2D and 3D convolutional operations in the context of cross-dimensional learning. Our hypothesis centers on the potential existence of meaningful correlations between 2D and 3D feature representations that could be leveraged for more efficient and robust volumetric analysis. If training models on 2D data can demonstrably enhance performance in 3D tasks, this would indicate a significant transferable knowledge base between dimensions. Such findings would pave the way for innovative hybrid architectures capable of simultaneous 2D and 3D processing, effectively combining the advantages of both paradigms. As a demonstrably enhanced convolution operation is developed to make pre-trained 2D architectures applicable to 3D images by Yang et al. Yang et al. (2021). They apply 2D convolutional filters along all three axes, thereby creating a pseudo-3D kernel known as ACS-convolution. Similar to our approach, this allows them to reuse information learned in the 2D domain, enhancing the effectiveness of 3D image analysis despite the additional computational load. Based on these points, the transition from 2D to 3D remains non-trivial. To address these issues, our work introduces the Cross-D Conv operation, a novel method that bridges the dimensional gap by shifting the phases of the kernels in the Fourier domain. This approach enables seamless weight transfer between 2D and 3D convolution operations, facilitating cross-dimensional learning, and demonstrably provides a correlation between 2D and 3D training. Experimental validations on the RadImagenet (2D) and multimodal (3D) datasets demonstrate that our method achieves comparable or superior performance in feature quality assessment with respect to conventional methods and representing a significant step forward in cross-dimensional and multi-modal medical image analysis."
https://arxiv.org/html/2411.02438v1,Entropic Hetero-Associative Memory,"The Entropic Associative Memory holds objects in a 2D relation or “memory plane” using a finite table as the medium. Memory objects are stored by reinforcing simultaneously the cells used by the cue, implementing a form of Hebb’s learning rule. Stored objects are “overlapped” on the medium, hence the memory is indeterminate and has an entropy value at each state. The retrieval operation constructs an object from the cue and such indeterminate content. In this paper we present the extension to the hetero-associative case in which these properties are preserved. Pairs of hetero-associated objects, possibly of different domain and/or modalities, are held in a 4D relation. The memory retrieval operation selects a largely indeterminate 2D memory plane that is specific to the input cue; however, there is no cue left to retrieve an object from such latter plane. We propose three incremental methods to address such missing cue problem, which we call random, sample and test, and search and test. The model is assessed with composite recollections consisting of manuscripts digits and letters selected from the MNIST and the EMNIST corpora, respectively, such that cue digits retrieve their associated letters and vice versa. We show the memory performance and illustrate the memory retrieval operation using all three methods. The system shows promise for storing, recognizing and retrieving very large sets of object with very limited computing resources.Keywords: hetero-associativity memory, declarative memory, indeterminacy of memory, memory and Hebb’s learning rule.","The Entropic Associative Memory (EAM) is a novel memory system in which recollections are stored diagrammatically in a table, or memory plane. The basic model defines three memory operations in relation to a cue: λ𝜆\lambdaitalic_λ-register, η𝜂\etaitalic_η-recognition and β𝛽\betaitalic_β-retrieval. Memory objects are represented as mathematical functions such that columns and rows of the table correspond to function arguments and values, respectively, which in turn correspond to the object’s attributes and values, as in standard feature-value structures [6]. Functions representing diverse memory objects can “overlap” on the memory medium. The system has been tested with the storage, recognition and retrieval of manuscript digits [14], manuscript digits and letters [12], phonetic representations [13], and images of clothes, bags and shoes [15]. From a computational perspective, the memory is declarative, abstractive, indeterminate and constructive. It supports the recovery not only of the cued objects but also of objects associated to the cue, as well as the production of association chains, possibly including objects of different classes[15], but the memory is mostly auto-associative. A more general form of associations can be established between objects of different domains and modalities, and a memory supporting this functionality is hetero-associative. Hetero-associative neural networks models were explicitly introduced with Kosko’s BAM model [7], which inspired a very large body of work [1, 9, 16, 18, 8]. In this paper we present the Entropic Hetero-Associative Memory (EHAM) as an extension of EAM [14, 12, 13, 15] that opposes BAM and related models in the same ways that EAM oppose auto-associative neural network-based models, such as Hopfield’s [5]. The structure of the paper is as follows: We present the extension of EAM to the hetero-associative model EHAM. The new model is tested with a heterogeneous corpus constituted by arbitrary associations between individual digits of MNIST and individual letters of EMNIST of designated associated classes. We illustrate the machinery with the object retrieved in both directions with three methods. Finally, we highlight the main features of the model, asses the results, and discuss some implications for research on associative memories."
https://arxiv.org/html/2411.03225v2,Knowledge Graphs of Driving Scenes to Empower the Emerging Capabilities of Neurosymbolic AI,"In the era of Generative AI, Neurosymbolic AI is emerging as a powerful approach for tasks spanning from perception to cognition. The use of Neurosymbolic AI has been shown to achieve enhanced capabilities, including improved grounding, alignment, explainability, and reliability. However, due to its nascent stage, there is a lack of widely available real-world benchmark datasets tailored to Neurosymbolic AI tasks. To address this gap and support the evaluation of current and future methods, we introduce DSceneKG – a suite of knowledge graphs of driving scenes built from real-world, high-quality scenes from multiple open autonomous driving datasets. In this article, we detail the construction process of DSceneKG and highlight its application in seven different tasks. DSceneKG is publicly accessible at: https://github.com/ruwantw/DSceneKG","Integrating intelligent behavior into AI systems requires both perception, processing raw sensor data, and cognition, using background knowledge for tasks like reasoning, planning, and decision-making [1]. Knowledge graphs play a crucial role in explicitly representing this background knowledge and enabling AI systems to perform cognitive tasks more effectively. Neural networks, while proficient in pattern recognition, often lack these explicit representations, limiting their ability to perform reliable reasoning. Neurosymbolic AI aims to overcome this limitation by combining symbolic knowledge representations (e.g., knowledge graphs, ontologies, logical rules) with sub-symbolic AI techniques, such as machine learning and deep learning. Recently, this approach has shown promise in improving reliability, explainability, and performance in handling tasks that demand higher levels of perceptual and cognitive abilities[2, 3]. However, evaluating such neurosymbolic AI capabilities is often constrained by the use of benchmark datasets that do not reflect the complexities of real-world scenarios, thereby limiting their practical relevance. A good example of this challenge is provided by knowledge graph completion (KGC), a key problem in knowledge representation and reasoning. Various link prediction (LP) methods have been developed to handle the inherent incompleteness of knowledge graphs by predicting new links in the graph in order to fill in the gaps. These methods are primarily evaluated on standard benchmark datasets like Freebase[4] and WordNet[5]. While such benchmark datasets offer a standardized platform for evaluating LP methods, they do not always accurately capture the complexities of real-world industrial applications. Industries such as automotive, manufacturing, healthcare, and finance are creating large-scale industrial KGs to represent domain-specific data and knowledge. For instance, in the automotive industry, there is a growing demand for large-scale knowledge graphs to represent multi-modal driving scene data from various sensors and cameras, conforming to domain-specific ontologies developed by subject matter experts (SMEs). The reliance on benchmark datasets for evaluating LP methods raises concerns about their applicability to industrial KGs. Real-world KGs differ significantly from benchmark datasets in terms of structure, modality, conformance to ontology, in/out degree, cardinality, etc. Industrial KGs often involve multimodal data, including text, images, and sensor data, and exhibit a higher degree of heterogeneity. This discrepancy highlights the need for more representative benchmarks that can better support the development and evaluation of neurosymbolic AI methods for use in real-world industrial settings. To address these challenges, we introduce DSceneKG, a suite of knowledge graphs representing real-world driving scenes sourced from multiple autonomous driving datasets. DSceneKG captures a broad spectrum of driving scenarios, including urban and rural environments, various weather conditions, and different traffic situations. By providing a rich symbolic representation of multi-modal data derived from LiDAR, cameras, and GPS sensors, DSceneKG serves as a valuable resource for advancing neurosymbolic AI methods, offering a more realistic and practical benchmark. We will demonstrate the applicability of DSceneKG in developing Neurosymbolic AI solutions for seven tasks: entity prediction, scene clustering/ typing, scene similarity, cross-modal retrieval, root-cause analysis, semantic search, and knowledge completion and augmentation."
https://arxiv.org/html/2411.03223v2,Beyond Grid Data: Exploring Graph Neural Networks for Earth Observation,"Earth Observation (EO) data analysis has been significantly revolutionized by deep learning (DL), with applications typically limited to grid-like data structures. Graph Neural Networks (GNNs) emerge as an important innovation, propelling DL into the non-Euclidean domain. Naturally, GNNs can effectively tackle the challenges posed by diverse modalities, multiple sensors, and the heterogeneous nature of EO data. To introduce GNNs in the related domains, our review begins by offering fundamental knowledge on GNNs. Then, we summarize the generic problems in EO, to which GNNs can offer potential solutions. Following this, we explore a broad spectrum of GNNs’ applications to scientific problems in Earth systems, covering areas such as weather and climate analysis, disaster management, air quality monitoring, agriculture, land cover classification, hydrological process modeling, and urban modeling. The rationale behind adopting GNNs in these fields is explained, alongside methodologies for organizing graphs and designing favorable architectures for various tasks. Furthermore, we highlight methodological challenges of implementing GNNs in these domains and possible solutions that could guide future research. While acknowledging that GNNs are not a universal solution, we conclude the paper by comparing them with other popular architectures like transformers and analyzing their potential synergies.","Earth Observation (EO) data, acquired by various sensors across different locations and times, presents as diverse modalities. Each of these modalities provides unique insights into the status of our planet, facilitating an understanding of the Earth. Deep learning (DL) is particularly appealing in parsing the abundant information in the EO field [1, 2, 3]. These data-driven models excel in extracting feature representations solely from the data, thereby eliminating the need for manually crafting features based on domain-specific knowledge. When it comes to images, they can be conceptualized as functions in Euclidean space, sampled on a grid where proximity is associated with local connectivity. Convolutional Neural Networks (CNNs) effectively harness the inductive bias of translation invariance and locality by employing convolutions in conjunction with other operations such as downsampling (pooling [4]). The convolution operation allows the extraction of local features shared across the entire image domain, as CNNs apply filters in a “sliding window” fashion across the input layer. This approach significantly reduces the number of learnable parameters, facilitating the training of very deep architectures [5]. Moreover, it enables CNNs to learn features independent of the specific region within the input [6]. These characteristics make CNNs well-suited for handling image datasets in computer vision applications [7, 8, 9]. However, EO data sometimes deviates from grids or regular formats alike, with an example being Light Detection and Ranging (LiDAR) point clouds, which are discrete points in space without predefined regularity between them. Moreover, EO data, in contrast to traditional vision images, are characterized by specific attributes such as geographical context, spatial-temporal dynamics, adherence to physical processes, and often limited visual saliency. These attributes necessitate a different space to decipher their complexities. Representing such data as graphs offers a compelling alternative, yet effectively handling graph-structured data poses significant challenges. Unlike images, which have a fixed size and spatial characteristics, graphs can exhibit arbitrary structures. They lack a well-defined beginning or end, where connected nodes may not necessarily be spatially adjacent. Besides, nodes in a graph may have a varying number of neighbors, a feature that hinders the straightforward application of two-dimensional (2D) convolution on graphs. Moreover, the traditional DL algorithms often assume that samples are independent of each other. However, this assumption may not hold for graph data, as the samples (nodes) are interconnected through links or edges. This interdependence among nodes adds a layer of complexity to the modeling process [10, 11]. In response to the challenges posed by graph data, a novel class of methods has emerged, known as Graph Neural Networks (GNNs) [12]. The essence of GNNs lies in the interactive aggregation and processing of the information across nodes and their connectivities (edges). These innovative approaches involve the development of new definitions and the generalization of existing operations from Euclidean space to non-Euclidean space. For instance, traditional 2D convolution is replaced with techniques better suited for graph structures [13, 14]. The advent of GNNs represents a paradigm shift in the field, enabling machine learning (ML) and DL models to effectively capture and leverage the complex relationships within graph data. The unique properties of EO data, coupled with the strengths of GNNs, have catalyzed the rapid adoption of GNNs in the EO field. The earliest and most straightforward use of GNNs in this field was through semi-supervised learning for fundamental remote sensing (RS) tasks such as image classification [15, 16, 17], segmentation [18], and change detection [19, 20]. Their ability to extract long-range contextual information and efficiency in organizing superpixels from grid-based inputs has made GNNs particularly prevalent in hyperspectral image analysis [21, 22]. Over time, GNNs have also been applied to specialized EO data types with inherent graph structures, such as those found in scattered monitoring stations, river networks, and road layouts. This has expanded their use in air quality monitoring [23, 24], flood detection [25, 26], and urban planning [27, 28]. Additionally, GNNs have shown promise in addressing global weather forecasting [29, 30], leading the way toward replacing traditional physical models with purely data-driven approaches. Furthermore, GNNs are adapted for dynamic complex systems. They are being explored for tasks such as natural resources exploitation [31], crop yield forecasting [32], seismic activity prediction [33], and other areas where understanding complex, interconnected processes is essential. The target of this survey is to provide an overview of GNN applications in the EO field. While many reviews on GNNs exist within the computer vision community [34, 35, 36, 37, 38], they often fall short of providing relevant guidance to geoscientific communities due to the significant gap between the two domains. Besides, although several reviews on the application of GNNs in the EO domain are available [39], they predominantly concentrate on traditional tasks such as RS image classification, segmentation, and change detection. However, real-world geoscientific problems are far more complex than these simplistic scenarios. Thus, there is a pressing need for a review that emphasizes geoscientific challenges and extends its focus beyond RS imagery to encompass diverse data sources such as monitoring stations and climate projections. More importantly, the challenges of implementing GNNs in EO have not been well summarized. Therefore, we recap fundamental generic problems from both geoscientific and methodological views, along with the promising research directions motivated by addressing these obstacles. The contributions of our work include • We provide a comprehensive review of GNNs on EO, with applications in atmospheric science, biogeosciences, climate, hydrological sciences, hazards, and seismology communities. • We summarize the key generic problems shared across the above domains and demonstrate how GNNs can effectively address them. • We analyze the major impediments preventing wider adoption of GNN-based methods in the EO domain, and offer potential solutions for overcoming these challenges. • We illustrate mind maps for conceptualizing and designing GNNs in the EO domain, and analyze the synergies between GNNs with other prominent DL models to encourage innovative solutions for real-world deployment. The paper is organized as follows. Chapter II presents fundamental background information on EO data types and GNNs. This chapter examines the limitations of traditional DL techniques in processing EO data and illustrates how GNNs can effectively address these challenges. In Chapter III, we provide an in-depth discussion on the common challenges associated with EO data shared across different EO-related use cases, and the advantage of GNNs in overcoming them. In Chapter IV, we delve into the case studies exemplifying the superiority of GNNs in addressing each EO application, including the methodologies for organizing EO data as graphs, designing appropriate architectures, and experimental results that show the real-world implications of GNNs. In Chapter V, we outline the major barriers to implementing GNNs in the EO domain, followed by potential solutions to narrow the gap between technical and domain science communities. Chapter VI compares GNNs with other prevalent architectures, such as CNNs and Transformers, exploring the opportunities for hybrid models. The paper concludes with future research directions of developing GNNs in the context of EO. In this contribution, we aim to assist domain experts in conceptualizing graph structures and designing suitable model architectures given EO data. Synergically, the methodological advance of GNN tools and domain-specific analysis demands opens up new areas of understanding our dynamic Earth."
https://arxiv.org/html/2411.03129v1,MA2: A Self-Supervised and Motion Augmenting Autoencoder for Gait-Based Automatic Disease Detection,"Ground reaction force (GRF) is the force exerted by the ground on a body in contact with it. GRF-based automatic disease detection (ADD) has become an emerging medical diagnosis method, which aims to learn and identify disease patterns corresponding to different gait pressures based on deep learning methods. Although existing ADD methods can save doctors time in making diagnoses, training deep models still struggles with the cost caused by the labeling engineering for a large number of gait diagnostic data for subjects. On the other hand, the accuracy of the deep model under the unified benchmark GRF dataset and the generalization ability on scalable gait datasets need to be further improved. To address these issues, we propose MA2, a GRF-based self-supervised and motion augmenting autoencoder, which innovatively models the ADD task as an encoder-decoder paradigm. In the encoder, we introduce an embedding block including the 3-layer 1D convolution for extracting the token and a mask generator to randomly mask out the sequence of tokens to maximize the model’s potential to capture high-level, discriminative, intrinsic representations. whereafter, the decoder utilizes this information to reconstruct the pixel sequence of the origin input and calculate the reconstruction loss to optimize the network. Moreover, the backbone of an autoencoder is multi-head self-attention that can consider the global information of the token from the input, not just the local neighborhood. This allows the model to capture generalized contextual information. Extensive experiments demonstrate MA2 has SOTA performance of 90.91%percent90.9190.91\%90.91 % accuracy on 1%percent11\%1 % limited pathological GRF samples with labels, and good generalization ability of 78.57%percent78.5778.57\%78.57 % accuracy on scalable Parkinson’s disease dataset. See supplementary materials for source code, and two pre-processed datasets are downloadable at https://github.com/EchoItLiu/MA2-PyTorch.","Bioinformatics is regarded as an important branch of modern biology and a powerful tool. In more detail, it takes advantage of biometrics to analyze various types of bioinformatics characteristics for revealing the mysteries of life activities. In recent years, in the field of bioinformatics for disease, indicators of bioinformatics have been playing an increasingly important role. That is to say, data-driven bioanalysis provides new standpoints and insights for the application of disease prevention, diagnostic precision, adjuvant therapy, disease surveillance, and personalized medical plans. Therefore, this interdisciplinary scientific field integrates the knowledge and technologies of multiple disciplines, including biometrics and computer science, as well as scholars in related fields who carry out the collection, storage, analysis, and interpretation of biometric data, which provides unprecedented opportunities for the medical field. Gait is one of the most important and unique biological characteristics of the human body. It refers to the distinct behavior pattern displayed by people during walking, which involves the trait of force, direction, and is the instantiation of people’s intrinsic walking habits during the stages of landing, starting, stance, and swing. Neurologists, physiotherapists, and orthopedic surgeons have all been working on the analysis of gait for a long time, so as to understand an individual’s health status and potential neurological or muscle diseases. However, these methods mainly rely on medical experience and are therefore not objective enough. Recently, by virtue of the accelerated growth of intelligent computing power, AI-based computational methods of bioanalysis have ushered in a development by leaps and bounds, the use of AI technology to automate the analysis of polymorphic bioinformation data such as gait data, which is mainly used to improve the detection accuracy of diseases. Gait data can be mainly divided into two categories: non-sensor data and sensor data. Among them, non-sensor data, including the video image and depth cameras, respectively. Besides, it has long-distance and non-cooperative characteristics during the collection process. On the other hand, sensor-based data, refers to the gait information directly collected by the devices installed in specific locations, such as the pelma, belts, joints, etc. These instruments can be accelerometers, gyroscopes, and plantar pressure sensors, which can be used for sampling the mechanical data generated during every stage of gait in real time. For example, ground reaction force (GRF), which is used as the key bioinformatic feature to describe and analyze gait. We can measure it based on our interaction with the ground. A person standing motionless on the ground exerts a contact force on it (equal to the person’s weight), and at the same time, an equal and opposite ground reaction force is exerted by the ground on the person. In summary, non-sensor or image-based gait technology is more suitable for identification and tracking, while gait feature information collected by sensors, especially GRF, has broader application prospects and potential in automatic disease detection (ADD), making it more suitable to build non-invasive tools for the classification of disease in clinical settings. For ADD methods based on GRF, some machine learning (ML) algorithms have been proposed in recent years. Slijepcevic et al. [1] investigate the helpful and explainable ML method with layer-wise relevance propagation (LRP), which can increase transparency in automated clinical gait classification based on a gait dataset comprising GRF measurements, and the experimental results are nearly consistent with those assessed by clinicians. Next, Teixeira et al. [2] put forward an extreme machine learning (ELM) for GRF gait disease detection to classify healthy and pathological gait on GaitRec [3] datasets, a large-scale ground reaction force dataset. Compared with linear regression, EML has the advantages of few training parameters, fast inference speed, and strong generalization ability. Furthermore, Siddiqui et al. [4] summarize four machine learning techniques as forecast primary methods, such as random forest for classifying the lower limb disorders affecting the knee, hip, and ankle joints from Tehsil Head Quarter hospital Sadiqabad. Meanwhile, the study emphasized the importance of preprocessing and feature extraction before analyzing the collected data. Since deep learning technologies [5] achieved outstanding results in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) with its end-to-end feature extraction capabilities, researchers have begun to consider how to employ these advanced technologies to GRF non-invasive disease detection, that is, extracting deeper, discriminative, and individual-differentiated features [6] from GRF data to improve the performance of prediction. Boompelli et al. [7] focus on the development of a telemetric gait analysis insole that works in conjunction with a mobile application and convolutional neural network to differentiate between healed and healing patients from the Austrian Workers’ Compensation Board (AUVA) and GaitRec. However, the 1D-CNN used in this article lacks sufficient comparative experiments with machine learning methods to demonstrate its advancement. Therefore, GaiRec-Net [8] was proposed for binary classification on the healthy control and gait disorders based on GaitRec and compared with three machine learning methods. The experiment proves that the proposed deep learning model is better for feature extraction, resulting in high accuracy. In addition to that, Yun [9] explores the application of residual network (ResNet) framework to human gait analysis for classification of lower limb injury on GaitRec dataset augumented by TimeGAN [10]. Experiments show that the resnet architecture outperformed unsupervised clustering models—self-organizing maps and k-means clustering by a large margin, despite the propensity for overfitting. Overall, deep learning networks for ADD in GRF have broad, developing prospects. However, to the best of our knowledge, there are still the following challenges to be addressed: 1. Uniform and standardized benchmark datasets still lack: Until now, GRF-based disease detection has utilized datasets from multiple hospitals and research institutes [11], but they are not unified and proprecessed with different modes. Hence, this makes it difficult to fairly evaluate the performance of different approaches, especially the computational advantages demonstrated by deep learning methods. 2. It’s not aligned with real clinical applications: Pathological gait samples (positive samples) are usually very scarce and more difficult to collect than healthy samples (negative samples). On the other hand, pathological samples must be diagnosed and evaluated by experienced physicians before they can be labeled, so they come with a high cost and are often poorly labeled [12]. Clinically, the main problem we face is how to train efficient deep models from fewer, and inadequately labeled pathological gait samples. 3. The flexibility of the model is limited: Models trained with supervised GRF samples may not be able to adequately learn the “intrinsic features” from the data, i.e., the implicit patterns and structures, and therefore are not suitable for the detection of various diseases [13]. To this end, we should consider exploring advanced methods based on self-supervised or unsupervised learning to improve the generalization of the model. To solve the problems above, we propose a pre-trained ADD method called motion augmenting autoencoder (MA2) and capture the high-level, discriminative locomotion of gait from limited pathological GRF-based samples with labels, thereby, enhancing the performance of classification. Concretely, our contributions to this paper are summarized into the following three parts: • GRF benchmarks in the real world: We integrated two canonical GRF datasets, GaitRec [3] and Gutenberg [14], as new benchmark datasets. Besides, from the perspective of real-world clinical application scenarios, we adjusted the ratio in the pathologic and healthy samples and performed channel-level minimum-maximum normalization [15]. Meanwhile, the multiple deep learning methods [7, 8, 9] for ADD in GRF are listed and conduct the experiments, so that researchers can reproduce these deep models in the future and make fair contrasts with each other. • The SOTA ADD method based on GRF: We proposed a self-supervised deep model called motion augmenting autoencoder (MA2) for extracting gait-based motion features from GRF data. The model adopts a masked auto-encoder structure based on Vision Transformer (ViT) for enhancing its ability to representation, and then fine-tuning with a small number of labeled samples to obtain a well-trained encoder. Multiple qualitative and quantitative experiments prove that MA2 can achieve an accuracy of over 90.91%percent\%% for ADD with only 1%percent\%% of labeled samples, which is beyond the reach of other comparative methods. • The satisfactory generalization: We transfer all the comparison methods and the proposed MA2 to a brand new GRF-based gait Parkinson’s disease detection dataset from the National Institutes of Health (NIH), and the experiments show that MA2 still achieves the best detection accuracy 78.57%percent\%% under the new dataset, indicating that MA2 has a better performance in terms of generalization."
https://arxiv.org/html/2411.03114v1,Investigating the Applicability of a Snapshot Computed Tomography Imaging Spectrometer for the Prediction of °Brix and pH of Grapes,"In this paper, a recently developed snapshot hyperspectral imaging (HSI) system based on Computed Tomography Imaging Spectroscopy (CTIS) is utilized to determine °Brix and pH values in Sheegene 20 table grapes through Partial Least Squares Regression (PLSR) modeling. The performance of the CTIS system is compared with that of a state-of-the-art line scan HSI system by imaging 100 grapes across both platforms. Reference measurements of °Brix and pH values are obtained directly using a refractometer and a pH meter, as these parameters are essential for assessing the quality of table and wine grapes. The findings indicate that the spectra captured by the CTIS camera correlate well with the reference measurements, despite the system’s narrower spectral range. The CTIS camera’s advantages, including its lower cost, portability, and reduced susceptibility to motion errors, highlight its potential for promising in-field applications in grape quality assessment.","Pre-harvest grape maturity and quality assessment are of utmost importance in the table grape and wine grape industry, as they directly influence the taste and aroma of the final product. Measurable indicators of maturity and quality in the lab include pH, titratable acidity (TA), total phenols and total anthocyanins, and soluble solids content (SSC) measured in °Brix, which represents the sugar content in the grape juice. For example, the °Brix/TA ratio was found to be a good indicator of the optimum harvesting stage of a number of different table grape varieties in [1]. Traditionally, °Brix values are measured using a refractometer, which requires extracting a sample of the grape juice. This process is time-consuming, and labor-intensive, it can introduce errors due to variations in juice extraction and may not capture all inter-row or inter-vine variations. Therefore it is of great interest to replace or complement these measurement methods with non-invasive spectroscopy [2] and hyperspectral imaging (HSI) [3]. Compared with traditional spectroscopy, HSI enables a two dimensional (2D) analysis of the reflectance spectra across the entire grape skin area, providing more information about the grape’s chemical composition including its SSC and pH. An early study of pH, °Brix and TA in table grapes, of the variety Crimson Seedless, based on line scan hyperspectral images (hereafter referred to as datacube) was presented in [3]. Several studies have since demonstrated the potential of line scan HSI for predicting °Brix values in table- and wine grapes [4]. An extensive study of different predictive data models that can be used to extract predictions for oenological parameters based on HSI is presented in [5]. All of the above studies employ line scan hyperspectral cameras with grapes put on a conveyor belt. The purpose of this paper is to compare the performance of a snapshot hyperspectral Computed Tomography Imaging Spectrometer (CTIS) with a line scan hyperspectral camera. The CTIS captures spatial and spectral information simultaneously by utilizing a 2D diffraction grating to disperse the light into a 3×3333\times 33 × 3 diffraction pattern of a central zeroth order and 8 surrounding first orders. From this diffraction image, a tomographic reconstruction is required to obtain a hyperspectral datacube [6, 7, 8]. The motivation for this comparison includes the ease of in-field deployment of the CTIS camera and the snapshot ability which means it will be less prone to motion errors when later deployed in the field [4]. We employ a newly developed CTIS system with a spectral range from 600-850 nm. We capture snapshot datacubes of 100 different grapes of the variety Sheegene 20, reconstruct the grape datacubes using both the standard expectation maximization (EM) algorithm and a U-Net neural network, and develop predictive Partial Least Squares Regression (PLSR) models that correlate the spectral information with corresponding °Brix- and pH measurements, acquired using a refractometer and a pH-meter, respectively. We compare the CTIS system performance with a state-of-the-art line scan HSI system with a spectral range from 532-1655 nm. The results of this study have the potential to improve the efficiency and accuracy of grape quality assessment in the wine industry and could have broader applications in the agriculture and food industries."
https://arxiv.org/html/2411.03064v1,Exploiting theSegment Anything Model(SAM) for Lung Segmentation in Chest X-ray Images,"Segment Anything Model (SAM), a new AI model from Meta AI released in April 2023, is an ambitious tool designed to identify and separate individual objects within a given image through semantic interpretation. The advanced capabilities of SAM are the result of its training with millions of images and masks, and a few days after its release, several researchers began testing the model on medical images to evaluate its performance in this domain. With this perspective in focus – i.e., optimizing work in the healthcare field – this work proposes the use of this new technology to evaluate and study chest X-ray images. The approach adopted for this work, with the aim of improving the model’s performance for lung segmentation, involved a transfer learning process, specifically the fine-tuning technique. After applying this adjustment, a substantial improvement was observed in the evaluation metrics used to assess SAM’s performance compared to the masks provided by the datasets. The results obtained by the model after the adjustments were satisfactory and similar to cutting-edge neural networks, such as U-Net.","Segment Anything Model (SAM) [6] is a tool that, since its release in April 2023, has proven to be very promising in the task of image segmentation. Its approach involves using a variety of input prompts to identify different objects in images, such as points and bounding boxes. To predict the masks, SAM uses three components: (i) an image encoder, (ii) a prompt encoder, and (iii) a mask decoder. Additionally, the model can automatically segment anything in an image and generate multiple valid masks for ambiguous inputs, which is innovative in the field. Given this and the immense amount of data used in its training – 11 million images and over 1 billion masks [6] – many researchers have recognized the potential of this technology in the medical field and have begun to investigate its effectiveness in this area. However, despite having this large volume of data in its training, there are no medical images among the domains in which SAM was trained, which makes its generalization ability moderate when it comes to this area [4, 8]. This study aims to advance the application of SAM in the field of medical image analysis, especially, for lung segmentation in chest X-ray images. Understanding the effectiveness of SAM in this domain is of paramount importance in the development of new technologies for the diagnosis, treatment and follow-up of lung diseases. We finetune SAM on two collections of chest X-ray images, known as the Montgomery and Shenzhen datasets [5]. This well-established practice aims to leverage the benefits of representations previously learned on a larger database to optimize the training of a network on a smaller dataset. Our exploration also involved testing SAM across such datasets using various input prompts, like bounding boxes and individual points. The obtained results show that our finetuned SAM can perform similar to state-of-the-art approaches for lung segmentation, like U-Net [9]."
https://arxiv.org/html/2411.03055v2,ATM: Improving Model Merging byAlternatingTuning andMerging,"Model merging has recently emerged as a cost-efficient paradigm for multi-task learning. Among current approaches, task arithmetic (Ilharco et al., 2022) stands out for its simplicity and effectiveness. In this paper, we motivate the effectiveness of task vectors by linking them to multi-task gradients. We show that in a single-epoch scenario, task vectors are mathematically equivalent to the gradients obtained via gradient descent in a multi-task setting, and still approximate these gradients in subsequent epochs. Furthermore, we show that task vectors perform optimally when equality is maintained, and their effectiveness is largely driven by the first epoch’s gradient. Building on this insight, we propose viewing model merging as a single step in an iterative process that Alternates between Tuning and Merging (ATM). This method acts as a bridge between model merging and multi-task gradient descent, achieving state-of-the-art results with the same data and computational requirements. We extensively evaluate ATM across diverse settings, achieving up to 20% higher accuracy in computer vision and NLP tasks, compared to the best baselines. Finally, we provide both empirical and theoretical support for its effectiveness, demonstrating increased orthogonality between task vectors and proving that ATM minimizes an upper bound on the loss obtained by jointly finetuning all tasks.","The pretrain-and-finetune paradigm has become the standard for many deep learning tasks, where a model pretrained on large-scale, unlabeled data is adapted to a specific downstream task with minimal tuning. However, when working with multiple tasks, a major drawback is the need to store separate finetuned models for each task. Model merging addresses this challenge by combining task-specific models into a single model capable of handling all tasks. This significantly reduces storage costs, as the unified model’s size remains comparable to that of a single-task model, regardless of the number of tasks. Among numerous model merging methods, task arithmetic (Ilharco et al., 2022) stands out for its simplicity and effectiveness. Given a pretrained model θ0subscript𝜃0\theta_{0}italic_θ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT and a model θisubscript𝜃𝑖\theta_{i}italic_θ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT finetuned on task tisubscript𝑡𝑖t_{i}italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, the task vector τi=θi−θ0subscript𝜏𝑖subscript𝜃𝑖subscript𝜃0\tau_{i}=\theta_{i}-\theta_{0}italic_τ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_θ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_θ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT is defined as the difference between the finetuned and pretrained weights. For multi-task learning with n𝑛nitalic_n tasks, task arithmetic sums the n𝑛nitalic_n task vectors, scales the sum with a coefficient α𝛼\alphaitalic_α, and adds the resulting vector back to the pretrained model. θbase(0)superscriptsubscript𝜃base0\theta_{\text{base}}^{(0)}italic_θ start_POSTSUBSCRIPT base end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPTθA(1)superscriptsubscript𝜃𝐴1\theta_{A}^{(1)}italic_θ start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPTθB(1)superscriptsubscript𝜃𝐵1\theta_{B}^{(1)}italic_θ start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPTτA+B(1)superscriptsubscript𝜏𝐴𝐵1\tau_{A+B}^{(1)}italic_τ start_POSTSUBSCRIPT italic_A + italic_B end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPTτA(1)superscriptsubscript𝜏𝐴1\tau_{A}^{(1)}italic_τ start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPTτB(1)superscriptsubscript𝜏𝐵1\tau_{B}^{(1)}italic_τ start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPTθbase(1)superscriptsubscript𝜃base1\theta_{\text{base}}^{(1)}italic_θ start_POSTSUBSCRIPT base end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPTθA(2)superscriptsubscript𝜃𝐴2\theta_{A}^{(2)}italic_θ start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 2 ) end_POSTSUPERSCRIPTθB(2)superscriptsubscript𝜃𝐵2\theta_{B}^{(2)}italic_θ start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 2 ) end_POSTSUPERSCRIPTτA+B(2)superscriptsubscript𝜏𝐴𝐵2\tau_{A+B}^{(2)}italic_τ start_POSTSUBSCRIPT italic_A + italic_B end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 2 ) end_POSTSUPERSCRIPTτA(2)superscriptsubscript𝜏𝐴2\tau_{A}^{(2)}italic_τ start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 2 ) end_POSTSUPERSCRIPTτB(2)superscriptsubscript𝜏𝐵2\tau_{B}^{(2)}italic_τ start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 2 ) end_POSTSUPERSCRIPTθbase(2)superscriptsubscript𝜃base2\theta_{\text{base}}^{(2)}italic_θ start_POSTSUBSCRIPT base end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 2 ) end_POSTSUPERSCRIPTθbase(h−1)superscriptsubscript𝜃baseℎ1\theta_{\text{base}}^{(h-1)}italic_θ start_POSTSUBSCRIPT base end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_h - 1 ) end_POSTSUPERSCRIPTθA(h)superscriptsubscript𝜃𝐴ℎ\theta_{A}^{(h)}italic_θ start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_h ) end_POSTSUPERSCRIPTθB(h)superscriptsubscript𝜃𝐵ℎ\theta_{B}^{(h)}italic_θ start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_h ) end_POSTSUPERSCRIPTτA+B(h)superscriptsubscript𝜏𝐴𝐵ℎ\tau_{A+B}^{(h)}italic_τ start_POSTSUBSCRIPT italic_A + italic_B end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_h ) end_POSTSUPERSCRIPTτA(h)superscriptsubscript𝜏𝐴ℎ\tau_{A}^{(h)}italic_τ start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_h ) end_POSTSUPERSCRIPTτB(h\tau_{B}^{(h}italic_τ start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_h end_POSTSUPERSCRIPTθbase(h)superscriptsubscript𝜃baseℎ\theta_{\text{base}}^{(h)}italic_θ start_POSTSUBSCRIPT base end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_h ) end_POSTSUPERSCRIPTTuneθA(h)=finetune⁢(θbase(h−1),A)superscriptsubscript𝜃𝐴ℎfinetunesuperscriptsubscript𝜃baseℎ1𝐴\theta_{A}^{(h)}=\text{finetune}(\theta_{\text{base}}^{(h-1)},A)italic_θ start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_h ) end_POSTSUPERSCRIPT = finetune ( italic_θ start_POSTSUBSCRIPT base end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_h - 1 ) end_POSTSUPERSCRIPT , italic_A )θB(h)=finetune⁢(θbase(h−1),B)superscriptsubscript𝜃𝐵ℎfinetunesuperscriptsubscript𝜃baseℎ1𝐵\theta_{B}^{(h)}=\text{finetune}(\theta_{\text{base}}^{(h-1)},B)italic_θ start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_h ) end_POSTSUPERSCRIPT = finetune ( italic_θ start_POSTSUBSCRIPT base end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_h - 1 ) end_POSTSUPERSCRIPT , italic_B )MergeτA+B(h)=τA(h)+τB(h)superscriptsubscript𝜏𝐴𝐵ℎsuperscriptsubscript𝜏𝐴ℎsuperscriptsubscript𝜏𝐵ℎ\tau_{A+B}^{(h)}=\tau_{A}^{(h)}+\tau_{B}^{(h)}italic_τ start_POSTSUBSCRIPT italic_A + italic_B end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_h ) end_POSTSUPERSCRIPT = italic_τ start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_h ) end_POSTSUPERSCRIPT + italic_τ start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_h ) end_POSTSUPERSCRIPTθbase(h)=θbase(h−1)+τA+B(h)superscriptsubscript𝜃baseℎsuperscriptsubscript𝜃baseℎ1superscriptsubscript𝜏𝐴𝐵ℎ\theta_{\text{base}}^{(h)}=\theta_{\text{base}}^{(h-1)}+\tau_{A+B}^{(h)}italic_θ start_POSTSUBSCRIPT base end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_h ) end_POSTSUPERSCRIPT = italic_θ start_POSTSUBSCRIPT base end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_h - 1 ) end_POSTSUPERSCRIPT + italic_τ start_POSTSUBSCRIPT italic_A + italic_B end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_h ) end_POSTSUPERSCRIPT Figure 1: The ATM method, illustrated up to iteration hℎhitalic_h with two tasks (A and B). In each iteration, the Tune step finetunes the pretrained model θbase(0)superscriptsubscript𝜃base0\theta_{\text{base}}^{(0)}italic_θ start_POSTSUBSCRIPT base end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT separately on both tasks, and the Merge step aggregates the task vectors and applies the resulting multi-task vector to the base model. This process repeats, with each iteration using the updated model as the new base, continuing until a stopping condition is met. We observe increased task vector orthogonality as ATM iterations progress. In this paper, we explain the effectiveness of task arithmetic by linking task vectors to the gradients of the average loss across all tasks. We start from a simple observation: when a model is finetuned for a single epoch using gradient descent (GD), the corresponding task vector is the additive inverse of the loss gradient, scaled by the learning rate. Similarly, the multi-task vector, obtained by summing individual task vectors, is equivalent to the additive inverse of the average loss gradient. Thus, task addition is analogous to performing a GD step on the sum of the average task losses. When finetuning spans multiple epochs, this equality becomes an approximation, with an error dependent on the learning rate. Despite the single-epoch assumption not always holding in practice, the gradient analogy still sheds light on why task vectors are effective. In fact, we reveal that the first epoch often contributes the most to the overall gradient norm during finetuning. Even when this is not the case, subsequent gradients tend to align with the first, confirming that the initial direction predominantly dictates the task vector’s effectiveness. In this view, aggregation and merging in task arithmetic correspond to a noisy GD step when finetuning on the combined tasks, using the sum of the average losses as the objective. In practice, this implies that the one-step nature of these techniques likely leads to overshooting the multi-task minimum, as they effectively perform GD on a multi-task dataset with a single, noisy step. The scaling factor optimized over the validation set essentially acts as the learning rate in this process. Contribution Building on these insights, we address the limitations of one-step task vector methods with Alternating Tuning and Merging (ATM) – a novel framework that iteratively alternates between finetuning and merging. This approach generalizes task arithmetic, allowing for a more gradual and refined integration of task-specific knowledge. Given a compute budget of b𝑏bitalic_b epochs per task, traditional methods allocate b𝑏bitalic_b epochs to a single finetuning pass. In contrast, ATM distributes this budget across k𝑘kitalic_k iterations, performing bk𝑏𝑘\frac{b}{k}divide start_ARG italic_b end_ARG start_ARG italic_k end_ARG epochs of finetuning in each iteration, followed by task vector aggregation. The unified model from each iteration serves as the starting point for the next, and after k𝑘kitalic_k iterations, the final unified model is deployed and evaluated. Notably, ATM is agnostic to the merging framework, allowing the integration of any interference-resolution techniques during the merge step to enhance performance. Overall, ATM significantly reduces time overhead compared to current baselines; extensive experiments in vision and NLP demonstrate its state-of-the-art results without hyperparameter tuning. All code, experiment configurations, and checkpoints are available for reproducibility 111https://github.com/LuckerZOfficiaL/Alternating-Tuning-and-Merging. To summarize, our contributions are four-fold: • We show that, under specific conditions, task vectors are equivalent to or approximate the gradients of the corresponding task losses. • We point out that existing one-shot merging frameworks often overshoot the multi-task optimum, especially when task vectors have large norms. • We introduce Alternating Tuning and Merging (ATM), a novel model merging framework that generalizes task arithmetic. Its flexibility allows it to incorporate any interference-resolution method with no additional overhead. • We empirically demonstrate that ATM increases task vector orthogonality compared to standard methods, and mathematically prove that it reduces the loss of a jointly finetuned multi-task model."
https://arxiv.org/html/2411.03019v1,FEDLAD: Federated Evaluation of Deep Leakage Attacks and Defenses,"Federated Learning is a privacy preserving decentralized machine learning paradigm designed to collaboratively train models across multiple clients by exchanging gradients to the server and keeping private data local. Nevertheless, recent research has revealed that the security of Federated Learning is compromised, as private ground truth data can be recovered through a gradient inversion technique known as Deep Leakage. While these attacks are crafted with a focus on applications in Federated Learning, they generally are not evaluated in realistic scenarios. This paper introduces the FEDLAD Framework (Federated Evaluation of Deep Leakage Attacks and Defenses), a comprehensive benchmark for evaluating Deep Leakage attacks and defenses within a realistic Federated context. By implementing a unified benchmark that encompasses multiple state-of-the-art Deep Leakage techniques and various defense strategies, our framework facilitates the evaluation and comparison of the efficacy of these methods across different datasets and training states. This work highlights a crucial trade-off between privacy and model accuracy in Federated Learning and aims to advance the understanding of security challenges in decentralized machine learning systems, stimulate future research, and enhance reproducibility in evaluating Deep Leakage attacks and defenses.","Marked by the proliferation of IoT devices, concerns regarding privacy, and information leakage have spurred the enactment of data protection legislation worldwide, highlighted by the GDPR in the European Union, and the Data Protection Act in the UK [7, 1]. Traditional centralized machine learning techniques face challenges in safeguarding privacy during data collection and sharing, prompting the rise of Federated Learning (FL) as a privacy-preserving distributed learning paradigm [19]. FL enables collaborative model training among participants without the exchange of local private data, a pivotal shift away from centralized methods. It operates by distributing the current model to each client, where local data is utilized for training. Gradient or model updates are subsequently shared with a central server for aggregation, as exemplified in algorithms like FedSGD and FedAvg respectively. This distributed approach not only addresses privacy concerns but also enhances scalability and security by minimizing data transmission, storage, and management overhead. FL finds application across diverse domains, from healthcare to autonomous vehicles, enabling the development of robust machine learning models while seemingly respecting data privacy and security regulations[22, 21]. Unfortunately, FL instils a misleading sense of security as in recent years a gradient inversion attack called Deep Leakage has emerged [34]. This attack aims to uncover private training data by analyzing the shared gradients transmitted to the server by the client. The attacker, often a curious server or a third party intercepting the communications, employs a randomly initialized dummy image and the shared global model to compute dummy gradients. The goal is then to minimize the distance between these dummy gradients and the intercepted shared gradients, thus optimizing the dummy image to closely resemble the ground truth. Although Deep Leakage was initially proposed within the context of Federated Learning, early iterations of the attack were primarily limited to untrained networks [34, 32]. Subsequent endeavours have aimed for more realistic scenarios by incorporating gradients from trained networks; however, these efforts typically assume a single steady-state model, failing to sufficiently consider how the recovery changes during training [11, 30]. In a realistic scenario with a sustained attack being undertaking throughout the training process the attacker can theoretically achieve more accurate recoveries[13, 14]. We call these approaches which leverage information from multiple timestamps Multi-Observation attacks. Previous Deep Leakage techniques have been assessed using differing metrics, across dissimilar datasets, and employing divergent attack protocols [34, 11, 13]. Prior codebases have utilized pretrained models in a stable state and lack implementation within a Federated Context or have minimal evaluation protocols in place[12, 15, 26]. In this paper, we present the FEDLAD Framework (Federated Evaluation of Deep Leakage Attacks and Defenses), which offers a diverse set of evaluation metrics across various datasets, incorporating implementations of numerous state-of-the-art attacks and defenses, and is designed to be extensible for future research. We hope this will help stimulate improved growth in the field, and better allow reproducibility in the future. In FEDLAD, we consider a server that is honest-but-curious. This server conducts training as intended but, in its curiosity, collects gradients at different timestamps to construct its own dataset using the recovered ground truths. To be clear, the attack does not adversely affect the performance of the final FL model, but rather leaks the private data held by the participants. Our threat model in our benchmark assumes the attacker only has access to information known to the FL server and does not allow the attacker to modify the model structure as this would alert the victim of the attack. Auxiliary information is permissible when derived from commonly shared information, such as batch normalization statistics from global models [29]. Finally, the attacker cannot know the quality of the recovery as they don’t have access to the ground truth. The code for our benchmark is available in the supplementary material. Our main contributions in this paper are as follows: • We present the FEDLAD Framework designed to assess single-observation and Multi-Observation attacks within the context of Federated Learning. • We unify various state-of-the-art attacks into a single joint formulation, and demonstrate the effectiveness of each approach across several widely recognised metrics for a variety of batch sizes, attack paradigms and defences. • We integrate FEDLAD with the popular Flower library for federated learning, and extend Flower to operate in FedSGD mode. We release all relevant code, ensuring that future works can also compare fairly and benefit from the new insights highlighted by the benchmark."
https://arxiv.org/html/2411.02992v1,Efficient and Effective Adaptation of Multimodal Foundation Models in Sequential Recommendation,"Multimodal foundation models (MFMs) have revolutionized sequential recommender systems through advanced representation learning. While Parameter-efficient Fine-tuning (PEFT) is commonly used to adapt these models, studies often prioritize parameter efficiency, neglecting GPU memory and training speed. To address this, we introduced the IISAN framework, significantly enhancing efficiency. However, IISAN was limited to symmetrical MFMs and identical text and image encoders, preventing the use of state-of-the-art Large Language Models. To overcome this, we developed IISAN-Versa, a versatile plug-and-play architecture compatible with both symmetrical and asymmetrical MFMs. IISAN-Versa employs a Decoupled PEFT structure and utilizes both intra- and inter-modal adaptation. It effectively handles asymmetry through a simple yet effective combination of group layer-dropping and dimension transformation alignment. Our research demonstrates that IISAN-Versa effectively adapts large text encoders, and we further identify a scaling effect where larger encoders generally perform better. IISAN-Versa also demonstrates strong versatility in our defined multimodal scenarios, which include raw titles and captions generated from images and videos. Additionally, IISAN-Versa achieved state-of-the-art performance on the Microlens public benchmark. We will release our code and datasets to support future research.","Recent advancements in recommendation algorithms have demonstrated that utilizing powerful large language models (LLMs) and vision encoders like GPT-4o111https://openai.com/index/hello-gpt-4o/, GPT-4 [1], DALL-E [2], Llama [3], and CLIP [4] can obtain a state-of-the-art recommendation performance [5, 6, 7, 8, 9]. These types of models typically combine a recommender model as the user encoder with multimodal foundation models (MFMs)222The definition of multimodal foundation models (MFMs) may vary. In this paper, we focus on separately pre-trained text and image encoders, following the approach in [10, 4], where the representations of image and text are learned in an end-to-end manner. Other MFM paradigms, such as [11, 12], which use a large language model (LLM) as the backbone and input image encoder features into the LLM, are beyond the scope of this paper. to encode the items. Many studies [5, 6, 7] have shown that fine-tuning the item encoder can lead to optimal performance. However, these approaches are highly inefficient due to the immense computational resources required to fine-tune such large encoders. A popular paradigm [13] for resolving the efficiency problem includes methods such as Adapter [14], LoRA [15], Bitfit [16], and Compacter [17]. These approaches integrate tunable neural network modules into the backbone multimodal foundation models, collectively referred to as embedded parameter-efficient fine-tuning (EPEFT) [18]. While EPEFT methods have gained popularity for addressing the efficiency of trainable parameters, they still face practical efficiency challenges, such as training time and GPU memory consumption. The introduction of the IISAN [18], with its decoupled structure and caching strategy for multimodal encoders, addresses these practical efficiency issues, outperforming both traditional full fine-tuning and EPEFT methods in terms of both performance and efficiency. We refer to this decoupled, parameter-efficient fine-tuning approach as DPEFT. It significantly reduces GPU memory usage by up to 15 times compared to FFT and 12 times compared to EPEFT (Adapter/LoRA). Additionally, it accelerates training time per epoch by up to 20 times compared to FFT and 16 times compared to EPEFT. IISAN leverages both intra- and inter-modal information from multimodal foundation models by exploiting the hidden states within the layers of backbone models to achieve fine-grained information merging. Despite these advancements, merging multimodal hidden states from two different encoders presents two limitations: (1) it can only be conducted for symmetrical multi-modal transformer encoders; (2) due to the first limitation, it is hard to explore whether scaling the text encoder in IISAN with recent state-of-the-art LLMs could yield performance improvements. These limitations do not align with the current research trend, where text encoders are often larger and more complex than visual transformers, as shown in Figure 1, which compares the state-of-the-art text and visual transformers. While vision transformers like ViT-e [19], ViT-G [20], and ViT-22B [21] have billions of parameters, they are all closed-source models. Therefore, the most commonly used vision transformers currently have fewer than 1 billion parameters [22]. Normally, larger pre-trained transformers generally offer better performance according to the scaling effect [23]. To maximize the potential of pre-trained models, it is advantageous to use larger models. However, finding a vision transformer that matches the size of a text encoder (LLM) is nearly impossible due to the disparity in model sizes. Therefore, addressing the issue of asymmetrical merging is of paramount importance. Figure 1: Comparisons of the model sizes of various popular text transformers and visual transformers using a logarithmic scale. The close-sourced is marked as ”Close-sourced” in the figure, the other are all open-sourced models. In this paper, we present an extension of the IISAN framework [18] called the Intra- and Inter-model Side Adapted Network for Versatile Multimodal Representation (IISAN-Versa). We specifically categorize IISAN-Versa into two variants: the symmetrical IISAN-Versa (IISAN-VS), which follows the standard IISAN configuration, and the newly proposed asymmetrical IISAN-Versa (IISAN-VA), which is capable of incorporating a larger text encoder to address asymmetry between text and vision pre-trained models. For IISAN-VA, we address the challenge of asymmetric multimodal backbone networks by proposing a simple yet effective strategy that combines group layer-dropping with dimension transformation alignment. The IISAN-VA variant, with a larger text encoder, significantly outperforms the standard symmetrical IISAN-VS, which uses BERT-base text encoders, while maintaining high efficiency. This demonstrates IISAN-Versa’s effectiveness in accommodating a larger and more powerful model. Moreover, we extend and validate the IISAN-Versa framework to a wider range of multimodal scenarios, particularly focusing on multimodal text, thereby validating its scalability. This extension involves the integration of diverse text forms (named multimodal text), such as titles, text captions from image covers, and video content, utilizing an open-source video recommendation dataset MicroLens [24]. We summarize the main changes made in this study compared to our published conference version in SIGIR2024 [18] as follows: • We revise the Introduction and Related Work sections to emphasize the motivation of achieving the versatility of the new IISAN-Versa, rather than focusing only on the efficiency of IISAN. • We expand the methodology section by categorizing IISAN [18] as the symmetrical IISAN-Versa (IISAN-VS) and introduced the asymmetrical IISAN-Versa (IISAN-VA) to incorporate LLMs as encoders. • We validate the effectiveness of the IISAN-VA, which incorporates state-of-the-art LLMs as text encoders, demonstrating significant performance improvements. Furthermore, we fully explore the effect of scaling laws on text encoders, observing that larger pre-trained text encoders typically lead to better performance. • To gain a clearer understanding of efficiency improvement during the training process, this paper analyzes it from the perspectives of forward and backward propagation. • We further validate IISAN-Versa’s performance on Microlens, a public multimodal recommendation benchmark, achieving state-of-the-art performance. • We also utilize pre-trained captioning models to generate multimodal texts from raw images and videos from Microlens and validated IISAN-Versa’s versatility in this multimodal text scenario. Our main contributions are the following: • We propose a versatile paradigm, IISAN-Versa, which adapts existing mainstream symmetric and asymmetric multimodal foundation models to enable efficient and effective multimodal sequential recommendation. It ensures the flexibility of multi-structure model adaptation and the efficiency of PEFT through minimal modifications of dimension transformation layers with LayerDrop. • We identify a scaling effect in our novel asymmetric IISAN-Versa by scaling the text encoder from a smaller language model to a larger language model. This provides a new inspiration that scaling LLMs with appropriate approaches, e.g. our IISAN-Versa, can effectively and efficiently improve multimodal recommendation. • We construct a new multimodal text recommendation scenario by extending the open-source Microlens dataset, i.e., generating captions from its original videos and images using a pre-trained video captioning generator, to validate the multi-scenario adaptation capabilities of the proposed IISAN-Versa. The new reconstructed dataset will be released to facilitate future research for the multimodal recommendation community."
https://arxiv.org/html/2411.02951v1,"LDPM: Towards undersampled MRI reconstruction with MR-VAE and Latent Diffusion Prior††thanks:The authors with††\dagger†contributed to the work equally and should be regarded as co-first authors.
Corresponding authors are marked with∗*∗.","Diffusion model, as a powerful generative model, has found a wide range of applications including MRI reconstruction. However, most existing diffusion model-based MRI reconstruction methods operate directly in pixel space, which makes their optimization and inference computationally expensive. Latent diffusion models were introduced to address this problem in natural image processing, but directly applying them to MRI reconstruction still faces many challenges, including the lack of control over the generated results, the adaptability of Variational AutoEncoder (VAE) to MRI, and the exploration of applicable data consistency in latent space. To address these challenges, a Latent Diffusion Prior based undersampled MRI reconstruction (LDPM) method is proposed. A sketcher module is utilized to provide appropriate control and balance the quality and fidelity of the reconstructed MR images. A VAE adapted for MRI tasks (MR-VAE) is explored, which can serve as the backbone for future MR-related tasks. Furthermore, a variation of the DDIM sampler, called the Dual-Stage Sampler, is proposed to achieve high-fidelity reconstruction in the latent space. The proposed method achieves competitive results on fastMRI datasets, and the effectiveness of each module is demonstrated in ablation experiments.","Magnetic Resonance Imaging (MRI) is a non-invasive medical imaging technique frequently used for disease diagnosis and treatment. However, the long scan time limits its broader application. To this end, k-space undersampling technique is employed to accelerate MRI acquisition. High acceleration factors (AF) can introduce aliasing artifacts, which need to be removed through reconstruction to achieve diagnostic-quality MRI[7]. Methods like parallel imaging[35, 36, 37] and compressed sensing[31, 32, 33, 34] were proposed to enhance MRI reconstruction, but they still suffer from limitations like residual artifacts and blurring[30]. In recent years, deep learning methods have been developed into the mainstream techniques for addressing undersampled MRI reconstruction problems[7, 6, 25, 24, 3, 23], especially those based on diffusion models (DMs, [13, 14, 20]). Chung et al.[1] proposed an SDE model that shows great reconstruction outcomes in various modalities and diverse body parts. Cao et al.[4] trained a DM with only high frequency MR k-space to preserve the consistency of the acquired low frequency information. Güngör et al.[8] utilized the sensitivity maps to enhance inference performance with large step diffusion. Peng et al.[12] proposed a model that visualizes multiple potential reconstructions and employing a novel method to estimate the most likely one. DM-based methods have demonstrated exceptional performance in reconstructing MR images[40, 5, 39]. Nevertheless, most of these methods operate directly in pixel domains (e.g., image domain and k-space), where optimization and inference are computationally demanding, making them more difficult to be applied in medical settings. Therefore, a more lightweight method is needed to enhance the accessibility of DM and reduce its significant resource consumption. Figure 1: Pipeline of the proposed LDPM method. 1) The Sketcher Module removes the artifacts and generates sketch image c𝑐citalic_c as the condition. 2) MRControlNet generates fully-sampled MRI prediction x^^𝑥\hat{x}over^ start_ARG italic_x end_ARG with MR-VAE and Dual-Stage sampler for higher fidelity. Utilizing latent diffusion models (LDMs)[21, 15] is one of the solutions for light-weighted natural image reconstruction[16, 27, 22, 28, 29]. However, direct application of LDMs to the task of MRI reconstructions still faces several challenges. Firstly, although the vanilla LDM framework exhibits superior performance in image generation, it may focus more on image quality rather than fidelity [16, 2] and requires appropriate control. Secondly, pre-trained variational autoencoders (VAEs)[18] are utilized in LDMs to map pixel-domain images into the latent space. As a lossy compression model[26], current VAEs pre-trained on natural images may lead to information misinterpretation on MRI images[22]. Finally, the widely used operation to ensure the fidelity of MRI reconstruction, data consistency (DC), needs to be modified to adapt to the LDM-based framework and address possible artifacts[9]. In order to fully exploit the advantages of LDM and solve these problems as much as possible, an undersampled MRI reconstruction method LDPM is proposed. To conclude, the main contributions of this work are: • An MR-VAE and Latent Diffusion Prior based undersample MRI reconstruction model (LDPM) is proposed. The latent diffusion model enables diffusion model training and inference on limited computational resources. Moreover, a sketcher module is utilized to produce conditional controls to ensure accurate detail generation in large scale undersample problems, which helps balance quality and fidelity of reconstructed MR images. • A MR-VAE is proposed and can be utilized as the backbone for the future MR-related tasks to transfer image to the latent domain. The MR-VAE is proved effective to minimize the loss in MRI images reconstruction and further improve the detail reconstruction results for undersampled MRI reconstruction tasks. • A variant of the DDIM sampler [10], named the Dual-Stage Sampler, is proposed to enable high-fidelity reconstruction to the LDM-based framework and address possible artifacts."
https://arxiv.org/html/2411.02888v1,A Symmetric Dynamic Learning Framework for Diffeomorphic Medical Image Registration,"Diffeomorphic image registration is crucial for various medical imaging applications because it can preserve the topology of the transformation. This study introduces DCCNN-LSTM-Reg, a learning framework that evolves dynamically and learns a symmetrical registration path by satisfying a specified control increment system. This framework aims to obtain symmetric diffeomorphic deformations between moving and fixed images. To achieve this, we combine deep learning networks with diffeomorphic mathematical mechanisms to create a continuous and dynamic registration architecture, which consists of multiple Symmetric Registration (SR) modules cascaded on five different scales. Specifically, our method first uses two U-nets with shared parameters to extract multiscale feature pyramids from the images. We then develop an SR-module comprising a sequential CNN-LSTM architecture to progressively correct the forward and reverse multiscale deformation fields using control increment learning and the homotopy continuation technique. Through extensive experiments on three 3D registration tasks, we demonstrate that our method outperforms existing approaches in both quantitative and qualitative evaluations.","\IEEEPARstart Deformable image registration is a crucial technique in medical image analysis to align anatomical structures in images [1]. This technique is essential for various clinical applications, including lesion identification [2], dose accumulation [3], motion tracking [4], and image reconstruction [5]. Traditional registration methods typically formulate image registration as a variational problem and solve it iteratively using optimization algorithms [6], such as Demons [7], B-spline [8], LDDMM [9], Diffeomorphic Demons [10], SyN [11], diffeomorphic image registration with control increment constraint [12], and their variants [13, 14, 15, 16, 17, 18, 19, 20]. Although these approaches preserve diffeomorphism and offer high registration accuracy, they are computationally expensive and slow because not only the time-dependent sequence operations but tuning hyper-parameter are needed for each image pair. As AlexNet [21] achieved success in ImageNet challenge, deep learning algorithms have been increasingly used in various image processing applications, achieving remarkable results in most tasks. In recent years, there have been many deep learning frameworks to solve medical image registration problems [1, 22, 23]. Initially, training neural networks requires the supervision of ground-truth deformation fields. Recently, unsupervised learning techniques employing a convolutional neural network (CNN), particularly U-net, have become the main focus of research in deep learning registration algorithms [24, 25, 26, 27, 28, 29, 30, 31, 32, 33]. Unlike traditional methods, unsupervised deep learning registrations have remarkably improved computational speed while maintaining accuracy [34]. Current learning methodologies, such as VoxelMorph [24], use two concatenated images as input and apply the U-net architecture to directly extract features, then generate deformation or velocity fields. However, we discuss that these straightforward methods may lack accuracy in complex scenarios. For complex or large-scale deformations, the VTN framework [26], which uses a cascade of multiple networks, proves to be an effective approach. Typically, these cascades consist of serially connected U-nets, where each progressively learns the deformation field and transforms the moving image to align with the fixed image through interpolation. However, this method involves high computational costs and tends to overfitting. It also accumulates errors during multiple interpolations, making it challenging to maintain the diffeomorphism. Moreover, most existing deep learning techniques are limited to unidirectional registration, neglecting the invertibility property of the smooth deformation field. Although SYM-net [28] and similar approaches have explored symmetrical registration, they still rely on a single U-net to learn spatial transformations and do not integrate cascaded and symmetrical registration. The scaling and squaring method [35] is widely adopted for diffeomorphic registration. However, these techniques are limited by the assumption of a constant velocity field, which may constrain their capability to capture fine-scale deformations. Additionally, the coupling nature of their iterative solutions can lead to interpolation errors, challenging practical application. Traditional methods are often considered more efficient than deep learning techniques in preserving diffeomorphism. Certain methods provide theoretical guarantees for diffeomorphic registration without requiring repeated interpolation. Zhang and Li [12] examined the optimal control relaxation method to indirectly determine the diffeomorphic transformation through the Jacobian determinant equation and investigated its applications in medical image registration. They developed the final deformation field by progressively incorporating control increment sequences that satisfy a particular PDE system into the previous deformation field. This inspired us to design a registration network that incorporates multiple incremental fields at different stages to compute the deformation field. This study investigates the mathematical diffeomorphic mechanisms proposed by Zhang and Li [12] and formulates diffeomorphic registration as a dynamic system. To address this system with a learning-based approach, we explore the long-term memory capabilities of the LSTM network to facilitate integrated multiscale cascade architecture and symmetrical registration path, and then propose a Diffeomorphic Cascaded CNN-LSTM Registration (DCCNN-LSTM-Reg) framework. This framework utilizes two U-nets that share the same parameters to extract multiscale features from a pair of images. We then develop an SR-module comprising a sequential CNN-LSTM architecture to iteratively align the images from coarse to fine levels using control increment learning and the homotopy continuation method. The suggested SR-module integrates a symmetric registration path based on its reversibility to further improve the performance of the progressive registration. In addition, we used intermediate deformation fields to progressively register the extracted features at subsequent finer scales, refining the registration accuracy. The main contributions of this work are summarized as follows: • Dynamical deformation framework: We model diffeomorphic image deformation using a dynamical system with control increments. Using homotopy continuation, we integrate all multiscale incremental fields to learn the evolving trajectory of diffeomorphic deformation fields. This technique enables us to obtain more flexible and accurate deformation fields. • Enhanced cascaded CNN-LSTM architecture: We propose a modified CNN-LSTM control increment module for cascaded transformation correction to achieve diffeomorphic multiscale registration. The proposed CNN-LSTM structure has advantages in capturing long-term dependencies of cascaded diffeomorphic registration. • Symmetric diffeomorphic registration: In the SR-module, we establish two symmetric registration paths with shared parameters which simultaneously generate symmetric deformation fields by reversing the order of input features. SR-module allows the invertibility of registration to be incorporated into the learning framework through optimizing the cyclic consistency loss, and not only yields symmetric deformation fields but also ensures diffeomorphism. • Pre-align of features: The deformation field obtained from the previous cascade is used to pre-align features of both the moving and fixed images in the subsequent cascade, thus increasing the accuracy of the registration."
https://arxiv.org/html/2411.02871v1,Enhancing Adversarial Robustness viaUncertainty-Aware Distributional Adversarial Training,"Despite remarkable achievements in deep learning across various domains, its inherent vulnerability to adversarial examples still remains a critical concern for practical deployment. Adversarial training has emerged as one of the most effective defensive techniques for improving model robustness against such malicious inputs. However, existing adversarial training schemes often lead to limited generalization ability against underlying adversaries with diversity due to their overreliance on a point-by-point augmentation strategy by mapping each clean example to its adversarial counterpart during training. In addition, adversarial examples can induce significant disruptions in the statistical information w.r.t. the target model, thereby introducing substantial uncertainty and challenges to modeling the distribution of adversarial examples. To circumvent these issues, in this paper, we propose a novel uncertainty-aware distributional adversarial training method, which enforces adversary modeling by leveraging both the statistical information of adversarial examples and its corresponding uncertainty estimation, with the goal of augmenting the diversity of adversaries. Considering the potentially negative impact induced by aligning adversaries to misclassified clean examples, we also refine the alignment reference based on the statistical proximity to clean examples during adversarial training, thereby reframing adversarial training within a distribution-to-distribution matching framework interacted between the clean and adversarial domains. Furthermore, we design an introspective gradient alignment approach via matching input gradients between these domains without introducing external models. Extensive experiments across four benchmark datasets and various network architectures demonstrate that our approach achieves state-of-the-art adversarial robustness and maintains natural performance. Systematical analyses further substantiate the effectiveness and generalization capability of our method across diverse experimental settings.","Although Deep Neural Networks (DNNs) have revolutionized numerous fields with superior performance, a growing body of research has underscored a critical vulnerability of DNNs: their susceptibility to adversarial examples—inputs subtly modified with visually imperceptible perturbations [1]. These tailored examples can easily circumvent the human observers to deceive DNNs with high confidence, posing potential security risks to deep learning-based systems. The growing vulnerabilities necessitate robust countermeasures against adversarial examples to safeguard the trustworthiness of DNNs in real-world applications [2]. (a) Illustration of the Decision Boundary (b) Feature Variance (c) Gradient Norm Figure 1: (a) The decision boundary of a standard adversarially trained model [3] can be overfitted to adversaries generated in the point-by-point strategy. A larger perturbation radius of adversarial examples leads to an increase in the average (b) feature variance values (%) and (c) gradient norm values (%). Among various defense methods against adversarial examples [4, 5, 6, 7], adversarial training has emerged as the most effective one for improving intrinsic network robustness via augmenting adversarial examples into training samples adaptively [8, 3, 9, 10, 11]. These works primarily rely on a point-by-point augmentation strategy, wherein each clean example is transformed into a single adversarial counterpart during the training stage. However, this deterministic training paradigm overlooks the multitude of latent adversarial examples around the decision boundaries, which compromises the performance of classification models [12]. Such an oversight can lead to an overfitted classifier that suffers from a distribution shift away from latent adversaries, resulting in suboptimal decision boundaries. Furthermore, adversarial examples can induce a significant disruption to the statistical information at the feature level within the target adversary domain, thereby exacerbating the complexity of modeling adversarial distributions. Hence, we hypothesize that the standard adversarially trained classifiers suffers from potential overfitting to adversaries generated via a point-by-point strategy, thus leading to poor generalization ability across unforeseen (latent) adversarial examples (see Figure 1(a)). Furthermore, the underlying distribution mismatch between clean and adversarial domains can exacerbate the statistical deviation in feature representations. To verify this hypothesis, we conduct an empirical analysis on the feature-level variance increment from clean examples to their adversarial counterparts using an adversarially trained model [3], as presented in Figure 1(b). The observed correlation between the escalation in feature variance and the increase of perturbation radius (i.e., attack strength) substantiates the premise that more potent adversarial examples precipitate a pronounced statistical deviation in feature representations. In the meantime, the gradient norm values w.r.t. the cross-entropy loss also suffer from an inevitable increase when enlarging the perturbation radius (see Figure 1(c)), further demonstrating the enhanced instability associated with an increased likelihood of misclassification for more potent adversarial examples. To mitigate the potential overfitting to a single adversary generated through the point-by-point strategy from each clean example, we propose a novel distributional adversary modeling method that leverages both the statistical information of adversarial examples and its corresponding uncertainty estimation to enhance the diversity of adversaries for improved adversarial training in the context of multi-class classification. Recognizing the detrimental impact brought by prediction alignment between adversaries and misclassified clean examples in adversarial training, we design a distributional refinement scheme to correct the alignment reference (i.e., clean examples) based on the statistical proximity to clean examples during adversarial training. Consequently, adversarial training is reframed into a distribution-to-distribution matching framework between clean and adversarial domains. This framework, termed Uncertainty-Aware Distributional Adversarial Training (UAD-AT), transcends traditional instance-specific schemes by considering both the underlying adversaries and their benignly refined counterparts to capture more generalizable and robust behavior. To enforce prediction invariance against adversarial perturbations, we develop an introspective gradient alignment approach by aligning input gradients across clean and adversarial domains without incorporating external models. Extensive experiments conducted across diverse datasets and architectures demonstrate that our UAD-AT method consistently outperforms the state-of-the-art adversarial training methods in terms of both natural performance and adversarial robustness. We also show that our method can be further adapted to auxiliary generated data for robustness improvement. Moreover, our method can serve as a plug-and-play module to boost the performance of single-step adversarial training at a marginal cost. We also provide systematic analyses to substantiate the effectiveness of our uncertainty-aware distributional adversary modeling. Our contributions can be summarized as follows: • By investigating the limited generalization ability towards underlying adversaries stemming from point-by-point adversary generation, we propose a novel distributional adversary modeling method that utilizes statistical information of adversaries and their corresponding uncertainty estimates for improved adversarial training. • To mitigate the detrimental impact of prediction alignment between misclassified examples, we design a distributional refinement scheme for each alignment reference, reframing adversarial training within a distribution-to-distribution matching paradigm. We also propose an introspective gradient alignment mechanism to enforce the prediction invariance against adversarial perturbation without the dependency on external models. • Comprehensive experiments and analyses demonstrate the effectiveness and generalization ability of our method compared with the state-of-the-art adversarial training approaches across diverse settings."
https://arxiv.org/html/2411.02855v1,Analyzing Poverty through Intra-Annual Time-Series: A Wavelet Transform Approach,"Reducing global poverty is a key objective of the Sustainable Development Goals (SDGs). Achieving this requires high-frequency, granular data to capture neighborhood-level changes, particularly in data scarce regions such as low- and middle-income countries. To fill in the data gaps, recent computer vision methods combining machine learning (ML) with earth observation (EO) data to improve poverty estimation. However, while much progress have been made, they often omit intra-annual variations, which are crucial for estimating poverty in agriculturally dependent countries. We explored the impact of integrating intra-annual NDVI information with annual multi-spectral data on model accuracy. To evaluate our method, we created a simulated dataset using Landsat imagery and nighttime light data to evaluate EO-ML methods that use intra-annual EO data. Additionally, we evaluated our method against the Demographic and Health Survey (DHS) dataset across Africa. Our results indicate that integrating specific NDVI-derived features with multi-spectral data provides valuable insights for poverty analysis, emphasizing the importance of retaining intra-annual information.","Accurately estimating poverty is essential for understanding socioeconomic disparities, guiding policy decisions, and tracking progress toward the United Nations Sustainable Development Goals (SDGs) [1]. Among these goals, the eradication of poverty is a primary objective, particularly emphasized in Goal 1, No poverty [2]. Reducing global poverty, particularly in low- and middle-income countries, is a central objective of SDGs. Achieving this goal requires policymakers to have access to reliable, high-resolution geo-temporal data that can track changes at the neighborhood level [3, 4, 5]. Poverty research is crucial because it illuminates the living conditions of marginalized populations, enabling the design of targeted and effective interventions. Despite the importance of such data, policymakers often rely on censuses and household surveys, such as the Demographic and Health Surveys (DHS) [6] and the Living Standards Measurement Study (LSMS)[7], which are infrequently collected and costly to implement, leading to a scarcity of timely poverty data essential for monitoring progress toward the SDGs [8, 9, 10]. Traditional poverty surveys, while valuable, are often prohibitively expensive and logistically challenging, particularly in remote or economically disadvantaged areas where continuous data collection is rare [11]. In response to these limitations, there has been growing interest in combining machine learning (ML) with earth observation (EO) data to estimate poverty at a neighborhood level [12, 13, 14, 15, 16, 17, 18, 19]. Although these emerging methods offer promise, the resulting poverty maps are often insufficiently accurate for precise policy applications. Current state-of-the-art models typically aggregate data over multiple years to address challenges such as missing data, noise from cloud cover, seasonal variations, and satellite instrument errors. Researchers often mitigate these issues by summarizing data through techniques like median value computation over extended periods. For example Yeh et.al. [16] and Pettersson et al. [20] used three-year median of Landsat data. While this approach effectively smooths out noise and reduces data volume, it risks overlooking critical intra-annual variations. These temporal variations—such as fluctuations in agricultural activity [21, 22] and economic events [23]—can provide valuable insights that enhance the accuracy of poverty predictions if properly accounted for. Thus, a balance must be struck between simplifying data for model manageability and preserving temporal detail crucial for accurate poverty estimation. The aim of this article is to address this question with the goal of improving poverty estimation accuracy that incorporates intra-annual time-series data. The article demonstrates the utility of wavelet-based feature extraction using a simulated dataset, where nighttime light data serves as a proxy for poverty estimation. Nighttime light, which correlates with economic activity, offers a unique and accessible measure for assessing poverty. The simulation uses nighttime light data as the target variable, representing a wealth index, while input data is derived from Landsat multispectral satellite images. Following this, the study applies the proposed methodology to real-world data, using the DHS dataset as a case study across the African continent. In this context, the time-series Normalized Difference Vegetation Index (NDVI), alongside raw Landsat spectral bands, is used in training a deep learning model to enhance the precision of poverty estimates. NDVI has demonstrated strong potential as an independent variable in economic analyses, particularly in agriculture-dependent economies [21, 24]. The other aspect of this research lies in its use of the wavelet transform as a tool for temporal data summarization. Unlike traditional methods that may overlook important intra-annual variations, the wavelet transform captures and summarizes temporal information at multiple scales, retaining critical details without the need to process the entire temporal dataset. By leveraging these temporal dynamics, this study aims to refine poverty estimation models and improve their predictive performance. This approach provides a novel framework for summarizing and analyzing temporal satellite data, potentially leading to more accurate and granular assessments of poverty and other socioeconomic indicators. Moreover, the integration of ML techniques with satellite data allows for the analysis of large datasets to identify economic indicators like nighttime lights, agricultural productivity, and infrastructure development. These indicators can generate high-resolution, timely poverty estimates, providing policymakers with the detailed data necessary to address socioeconomic challenges effectively. Poverty estimation. Datasets used in poverty studies encompass a wide range of sources, including household surveys, census data, administrative records, and satellite imagery. Satellite datasets are essential for poverty estimation, offering critical spatial and temporal information. Planetary-scale satellite imagery, as opposed to surveys, is accessible throughout Africa across a broad time span and geographic area. For instance, nighttime lights data from satellites like VIIRS show urbanization and economic activity levels [25] [26] [27] [28]. Bruederle et al. [28] developed a simple linear regression model combining nighttime lights data with demographic surveys, controlling for factors like population density and electrification. EO-ML methods have been on the rise the last decades [29], but their use for poverty estimation is recent. Elvidge et al. [30] were among the first researchers that deployed EO data to generate a poverty map. They defined poverty index as the value of population count divided by DMSP-OLS nighttime light value. Then, the index was calibrated by the national level poverty data from the World Development Indicators (WDI) 2006 edition. Wang et al. [31] used Principal Component Analysis (PCA) to extract the poverty index IPI from 17 socio-economic indexes. They showed that there is a high correlation between IPI and DMSP-OLS nighttime light data at a province-scale in China. Furthermore, Li et al. [32] showed that they can identify a high correlation rate between DMSP NL and Gross Domestic Product (GDP) in Zimbabwe for the period 1992 to 2009 when there was an economic decline. Jean et.al.[13] deployed 3-band high resolution optical (daylight satellite) data as input, for estimating poverty. They proposed using a deep-learning method for poverty estimation, and trained a ResNet18 deep model [33] with DMSP/OLS (NL) as the output. Then the deep network was frozen and a ridge regression layer was added to the network to predict the poverty index. The model was evaluated on five African countries including, Nigeria, Tanzania, Uganda, Malawi, and Rwanda. But to evaluate the proposed method between countries, the asset index was normalized for each country as a pre-processing step. Therefore, they ignore the wealth difference between countries and the temporal variability, and the model can show just the local variation of wealth index within each country. In a similar work Ni et al. [34] trained daylight images from Google Static Map API on NL as target data. They used VGG-Net, Inception-Net, ResNet, and DenseNet, to extract features from optical imagery and then applied LASSO regression for poverty prediction. They evaluated their method on four African countries including Malawi, Rwanda, Uganda, and Nigeria. In addition, Piaggesi et al. [12] followed a similar approach by deploying ResNet50 and VGG-F as deep models that were feed to a ridge regression model. They evaluated their method on a local-scale in Santiago (Chile), Los Angeles, Philadelphia, Boston, Chicago, and Houston (US). Building on Jean et al.’s [13] method, Perez et al. [35], but included additional experiments. For example they used multi-spectral Landsat-7 data instead of the optical data. More over, they used ResNet-34 and VGG-F deep models besides the ResNet-18 model. Tange et al. [21] used moderate-resolution vegetation index NDVI as an evidence of wealth level in low-income countries which are heavily dependent on agriculture. They also evaluated their method on Malawi, Nigeria, Rwanda, Tanzania, and Uganda. Yeh et.al. [16] deployed to ResNet18 networks, in which one was trained using Landsat optical data and the other was trained using NL data. Their outputs were concatenated and passed through a ridge regression layer to predict the poverty in several African countries. Furthermore, Chi et.al. [36] fused satellite data, mobile phone networks, topographic maps, and facebook data to predict Relative Wealth Index (RWI) in several low- and middle-income countries. They generated the poverty map at 2.4 km resolution. Vegetation indices from Moderate Resolution Imaging Spectroradiometer (MODIS), such as Normalized Difference Vegetation Index (NDVI) and Enhanced Vegetation Index (EVI), indicate agricultural productivity [24], which can be used for better poverty prediction. Although there is now a rich literature on using EO-ML methods for poverty estimation, none of the studies previously mentioned how much intra-annual data, such monthly variation, is informative for prediction accuracy. The importance of vegetation for poverty estimation. Time-series feature extraction from Earth observation data is crucial for understanding and predicting various phenomena, including poverty dynamics, agricultural productivity, and environmental changes. Time-series data, such as vegetation indexes, provide valuable insights into changes in environmental conditions over time, which are crucial for understanding socio-economic dynamics, especially in developing regions heavily reliant on agriculture. Tange et al. [21] demonstrated the effectiveness of utilizing the NDVI, derived from moderate-resolution satellite imagery (MODIS), in predicting poverty indicators among agricultural communities. The NDVI, a widely-used vegetation index, offers a measure of vegetation greenness and health, which is particularly relevant for regions where agriculture plays a significant role in livelihoods. By leveraging convolutional neural networks (CNNs) and transfer learning techniques, Tange et al. demonstrate how NDVI time-series data can be effectively harnessed to predict poverty measures such as consumption expenditure and wealth index at the community level. Tange et al.’s [21] approach involves fine-tuning a pre-trained CNN model on NDVI images to predict nighttime light intensities, which serve as intermediate labels. Subsequently, random forest regression models are trained on the extracted NDVI features to predict poverty indicators. This two-step procedure capitalizes on the temporal dynamics captured by NDVI time-series data and the spatial information provided by nighttime light intensities, resulting in accurate and timely poverty predictions. Moreover, the authors highlight the importance of considering the temporal dimension in poverty prediction by demonstrating the ability of their model to capture changes in consumption expenditure over time among poor communities. This sequential prediction aspect adds a valuable dimension to early warning systems and policy evaluation efforts. In summary, time-series feature extraction from EO data, particularly using NDVI, is crucial for understanding and predicting socio-economic and environmental phenomena in agricultural regions. Previous studies have highlighted the importance of capturing temporal NDVI dynamics for poverty prediction, providing valuable insights for policy evaluation. In the next subsection, we will further explore the role of NDVI in this context. Time-series feature extraction in EO data. The importance of time-series EO data analysis has been demonstrated in a range of applications, including wetland mapping [37, 38], change detection [39], cropland analysis and yield estimation [40], semantic segmentation [41], and vegetation phenological studies [42], which encompass both intra-annual and inter-annual variation studies. Among these applications, cropland analysis, traditionally reliant on NDVI, is particularly significant due to its strong correlation with agricultural economic analysis. Wavelet transform is one of the key methods used for decomposing time-series data. Unlike Empirical Mode Decomposition (EMD), which does not rely on predefined basis functions, wavelet transform uses a set of predefined basis functions to achieve data decomposition [43]. Karthikeyan et al. [44] conducted a comparison between wavelet-based methods and EMD for time-series modeling and forecasting. Their findings showed that wavelet-based methods outperformed EMD, demonstrating superior predictive accuracy, particularly in forecasting rainfall 12 months in advance across multiple locations. Martinez et al. [45] used wavelet transform to perform non-stationary and multiscale analysis of NDVI time series, capturing both short- and long-term vegetation variations. Their study emphasized how multi-resolution analysis enables the differentiation between intra-annual and inter-annual changes, identifying critical phenological features such as minimum NDVI values and the timing of peak vegetation. Similarly, Yan et al. [46] compared intra-annual NDVI with Annual Maximum NDVI (NDVImax) and found that time-series NDVI provided a more detailed representation of vegetation changes, revealing greater spatial and temporal heterogeneity and stronger correlations with climatic factors like precipitation and temperature. Rhif et al. [47] focused on improving trend analysis of non-stationary NDVI time series, particularly in monitoring long-term vegetation changes. They used Multi-Resolution Analysis Wavelet Transform (MRA-WT) to decompose NDVI series, and proposed a combined mother wavelet approach to better track vegetation trends, finding evidence of forest degradation and cropland improvement. In their subsequent work, Rhif et al. [48] introduced a hybrid approach, I-WT-LSTM (Improved Wavelet Long Short-Term Memory), which outperformed traditional models in forecasting non-stationary time series. Another study by Rhif et al. [49] optimized wavelet transform parameters for analyzing NDVI time series in the Mediterranean, determining that a decomposition level of 5 provided the best results in capturing trends and seasonal variations. These studies highlight the effectiveness of wavelet-based methods for feature extraction from time-series satellite data. Intra-annual NDVI analysis enables researchers to examine how various vegetation types respond to seasonal changes, which is essential for understanding vegetation phenology. Building on this, we applied NDVI time-series data in our study to extract more detailed and informative features for economic analysis. By utilizing wavelet transformation, we aimed to enhance the analysis of NDVI time-series, providing a richer dataset for economic studies."
https://arxiv.org/html/2411.02817v1,Conditional Vendi Score: An Information-Theoretic Approach to Diversity Evaluation of Prompt-based Generative Models,"Text-conditioned generation models are commonly evaluated based on the quality of the generated data and its alignment with the input text prompt. On the other hand, several applications of prompt-based generative models require sufficient diversity in the generated data to ensure the models’ capability of generating image and video samples possessing a variety of features. However, most existing diversity metrics are designed for unconditional generative models, and thus cannot distinguish the diversity arising from variations in text prompts and that contributed by the generative model itself. In this work, our goal is to quantify the prompt-induced and model-induced diversity in samples generated by prompt-based models. We propose an information-theoretic approach for internal diversity quantification, where we decompose the kernel-based entropy H⁢(X)𝐻𝑋H(X)italic_H ( italic_X ) of the generated data X𝑋Xitalic_X into the sum of the conditional entropy H⁢(X|T)𝐻conditional𝑋𝑇H(X|T)italic_H ( italic_X | italic_T ), given text variable T𝑇Titalic_T, and the mutual information I⁢(X;T)𝐼𝑋𝑇I(X;T)italic_I ( italic_X ; italic_T ) between the text and data variables. We introduce the Conditional-Vendi score based on H⁢(X|T)𝐻conditional𝑋𝑇H(X|T)italic_H ( italic_X | italic_T ) to quantify the internal diversity of the model and the Information-Vendi score based on I⁢(X;T)𝐼𝑋𝑇I(X;T)italic_I ( italic_X ; italic_T ) to measure the statistical relevance between the generated data and text prompts. We provide theoretical results to statistically interpret these scores and relate them to the unconditional Vendi score. We conduct several numerical experiments to show the correlation between the Conditional-Vendi score and the internal diversity of text-conditioned generative models. The codebase is available at https://github.com/mjalali/conditional-vendi.","Prompt-based generative models, including text-to-image and text-to-video generation schemes, are widely used in various artificial intelligence (AI) applications. In prompt-based generative AI, the sample creation process begins with a text input and produces a random output aligned with that text. The conditional nature of this sample generation distinguishes prompt-based generative models from standard unconditional generative models where the objective is to produce samples distributed similarly to real data without any guiding input prompt. Since most evaluation metrics for generative models had been developed for unconditional models in the previous decade, the recent literature has sought to create scores tailored specifically for text-conditioned generative models. The existing evaluation metrics for prompt-based generative models typically focus on fidelity and relevance in sample generation, i.e., they assess the visual quality of the produced samples and their alignment with the input prompt. Relevance is often measured by calculating a similarity score between a shared embedding of the text and image samples, e.g. in ClipScore [1] which utilizes the CLIP embeddings of text and image data. Such shared embedding-based evaluation mechanisms have been further adapted to quantify the aesthetics, semantic consistency, and compositional accuracy of the generated data based on the input text prompt. On the other hand, the diversity performance of prompt-based generative models has not been exclusively studied in the literature. The diversity scores proposed for unconditional generative models, such as Recall [2, 3], Coverage [4], Vendi [5, 6], and RKE [7], are often applied to quantify the variety of generated samples. However, in text-based generative models, the generated samples are typically produced in response to different input prompts, where the variation in input texts can significantly contribute to the diversity of the generated image or video samples. Thus, the diversity of data produced by prompt-based models is influenced by two main factors: 1) the variety of input prompts, and 2) the internal diversity of the model introducing randomness into the output samples. Figure 1 illustrates examples of prompt-induced diversity, where the variety of generated images is mostly due to the different prompts and the generated data has little variety in other details (images of similar people with similar poses), and model-induced diversity where the details not specified by the prompts vary significantly between generated images. This decomposition of diversity in text-based generative models has not been studied in the existing literature on conditional generative models including text-based image and video generation. In this work, we focus on quantifying the two diversity components mentioned for prompt-based generative models. To this end, we propose an information-theoretic decomposition of the diversity of the model’s output data, X𝑋Xitalic_X. The proposed decomposition is based on a classical identity in information theory, which shows that for variables X𝑋Xitalic_X and T𝑇Titalic_T, the Shannon entropy H⁢(X)𝐻𝑋H(X)italic_H ( italic_X ), representing the uncertainty of X𝑋Xitalic_X, can be decomposed into two terms as follows: H⁢(X)=H⁢(X|T)+I⁢(X;T)𝐻𝑋𝐻conditional𝑋𝑇𝐼𝑋𝑇H(X)\;=\;H\bigl{(}X|T\bigr{)}+I\bigl{(}X;T\bigr{)}italic_H ( italic_X ) = italic_H ( italic_X | italic_T ) + italic_I ( italic_X ; italic_T ) Here, H⁢(X|T)𝐻conditional𝑋𝑇H(X|T)italic_H ( italic_X | italic_T ) denotes the conditional entropy of data X𝑋Xitalic_X given the text variable T𝑇Titalic_T, which we interpret as the internal diversity of the text-based generative model not caused by variation in the input text T𝑇Titalic_T. Furthermore, the mutual information term I⁢(X;T)𝐼𝑋𝑇I(X;T)italic_I ( italic_X ; italic_T ) can be viewed as a measure of statistical relevance between the text T𝑇Titalic_T and the generated data X𝑋Xitalic_X, quantifying how much information the model’s output conveys about the input text. Figure 1: Illustration of prompt-induced diversity, where the diversity of generated images follows the variety of prompts and has little variation in other details, vs. model-induced diversity, where the diversity of images for similar prompts is due to the generation model. To mathematically define the entropy-based scores, we follow the kernel-matrix-based entropy definition, which has been applied by [5, 7, 6] to unconditional generative models. These references apply the matrix-based entropy in quantum information theory, that is the entropy of the eigenvalues of the kernel matrix of generated data X𝑋Xitalic_X, to measure the diversity of an unconditional model’s generated data. To extend the framework to conditional prompt-based generative models, we utilize the definition of matrix-based conditional entropy proposed by [8]. This work provides a definition for the conditional entropy of two general positive semi-definite matrices, which we select to be the kernel matrices of generated data X𝑋Xitalic_X and text T𝑇Titalic_T. Following these definitions, our work extends the entropy-based approach in [5, 7] to conditional generative models. We define the Conditional-Vendi and Information-Vendi scores which decompose the unconditional Vendi score to model-induced and prompt-induced diversity measures. To statistically interpret the defined scores, we first derive the statistic estimated by these scores from empirically generated samples. We show how the target statistic can be formulated in terms of the kernel covariance matrix of the Kronecker product of text T𝑇Titalic_T and data X𝑋Xitalic_X vectors. Also, to provide an operational meaning for the proposed score, we prove that given a mixture distribution PTsubscript𝑃𝑇P_{T}italic_P start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT for text data with a hidden group variable G∈{1,…,m}𝐺1…𝑚G\in\{1,\ldots,m\}italic_G ∈ { 1 , … , italic_m }, the defined conditional entropy approximates the expectation of the entropy H⁢(X|G=g)𝐻conditional𝑋𝐺𝑔H(X|G=g)italic_H ( italic_X | italic_G = italic_g ) given prompt type g∼PGsimilar-to𝑔subscript𝑃𝐺g\sim P_{G}italic_g ∼ italic_P start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT. This result connects our proposed diversity assessment approach to the Vendi [5, 6] and RKE [7] scores, showing that the Conditional-Vendi score is an aggregation of the unconditional Vendi scores for different prompt groups. We numerically evaluate the proposed diversity scores for standard text-to-image, text-to-video, and image-captioning generative models. In our experiments, we simulate text-based generative models for which the ground-truth rankings of internal diversity and relevance are known. Our experimental results validate the consistency of our proposed information-theoretic scores and the ground-truth ranking of the models. We further decompose the Conditional-Vendi score across different modes of input text data, evaluating the models’ internal diversity across different types of input text. The following is a summary of the contributions of this work: • Highlighting the diversity evaluation task in the context of conditional and prompt-based generative models, • Proposing an information-theoretic framework for decomposing the diversity of generated data into prompt-induced and model-induces components to evaluate the internal diversity of prompt-based generative models • Providing statistical analysis of the proposed scores and interpreting them as the conditional expectation of entropy scores given the text prompt • Presenting numerical results on the consistency between the conditional entropy score and the model-induced diversity of text-to-image and text-to-video generative models."
https://arxiv.org/html/2411.02796v1,Specialized Foundation Models Struggleto Beat Supervised Baselines,"Following its success for vision and text, the “foundation model” (FM) paradigm—pretraining large models on massive data, then fine-tuning on target tasks—has rapidly expanded to domains in the sciences, engineering, healthcare, and beyond. Has this achieved what the original FMs accomplished, i.e. the supplanting of traditional supervised learning in their domains? To answer we look at three modalities—genomics, satellite imaging, and time series—with multiple recent FMs and compare them to a standard supervised learning workflow: model development, hyperparameter tuning, and training, all using only data from the target task. Across these three specialized domains, we find that it is consistently possible to train simple supervised models—no more complicated than a lightly modified wide ResNet or UNet—that match or even outperform the latest foundation models. Our work demonstrates that the benefits of large-scale pretraining have yet to be realized in many specialized areas, reinforces the need to compare new FMs to strong, well-tuned baselines, and introduces two new, easy-to-use, open-source, and automated workflows for doing so.","Recent years have witnessed a shift towards large-scale pretraining across domains like computer vision and natural language processing. This workflow generally consists of two stages: pretraining on vast amounts of domain-specific data to capture general knowledge followed by fine-tuning on target tasks (Radford & Narasimhan, 2018). This pretrain-then-finetune paradigm has been tremendously successful, enabling foundation models (Bommasani et al., 2021) to consistently outcompete traditional supervised learning methods on a wide variety of downstream tasks in the vision and language domains (Dosovitskiy et al., 2021; Liu et al., 2021; Devlin et al., 2019). Driven by this success, the foundation model approach has been adapted to various specialized domains, which we define to be ML application areas—e.g. genomics, satellite imaging, and time series—whose data modalities lie outside those of classical AI tasks, i.e. natural images and text. These domains have seen the introduction of many new FMs claiming to leverage large, domain-specific pretraining datasets to achieve breakthrough performance on downstream tasks (Dalla-Torre et al., 2023; Nguyen et al., 2024; Zhou et al., 2023b; Avsec et al., 2021; Ji et al., 2021; Fuller et al., 2023; Cong et al., 2022; Mendieta et al., 2023). These claims underlie our study’s motivating question: Do these new specialized FMs outperform traditional supervised learning applied to the same tasks? Figure 1: Across three domains—genomics, satellite imaging, and time series—specialized FMs fail to significantly improve upon tuned supervised learning despite using two-to-five orders of magnitude more data. In contrast, breakthrough FMs such as BERT dramatically outperformed supervised baselines in NLP (top left), causing the field to switch to fine-tuning as the default approach. For each domain we plot total pretraining and fine-tuning data used vs. the mean improvement across tasks over the supervised state-of-the-art. Specifics of our evaluations on the last three domains are in Section 4, while the NLP results are derived from the GLUE benchmark (Wang et al., 2019). Note that in the x-axis of the top left figure we ignore tokens used to pretrain word embeddings. Answering this question is critical because supervised workflows are usually much less expensive to implement and deploy, but FMs that allow for effective transfer learning have the potential to fundamentally transform these domains, as we have seen with language and vision processing in the past decade. However, despite ongoing efforts to promote their fair and comprehensive evaluation (Liang et al., 2022; Bommasani & Liang, 2021), many new FMs have not been adequately compared to simpler, often more efficient baselines. Indeed, we found that many works only benchmark their proposed models against other FMs, essentially creating a comparison echo chamber (Fuller et al., 2023; Mendieta et al., 2023; Nguyen et al., 2024; Zhou et al., 2023b). We answer our motivating question by considering a reasonably representative set of three specialized domains—chosen according to the presence of multiple FMs and a standard set of evaluation tasks—and comparing their performance on those tasks with that of a traditional supervised learning workflow. As depicted in Figure 2, the latter is a model development, hyperparameter tuning, and training process in which all steps use only data from the target task, in contrast to the FM workflow, which uses vast amounts of pretraining data. By leveraging model selection tools ranging from classical information criteria to cutting-edge architecture search, we build automated pipelines that efficiently develop and train strong supervised models on over fifty tasks across three distinct domains. Our main result is negative: we find that, despite being pretrained on massive datasets, specialized FMs struggle and very often fail to outperform models trained exclusively on downstream task data with traditional supervised learning (c.f. Figure 1). Specifically, we show that lightly adapted convolutional neural network (CNN) architectures such as wide ResNet and UNet attain state-of-the-art on the Nucleotide Transformer benchmark in genomics and match the latest pretrained satellite FMs on downstream classification. Furthermore, we show that tuned linear auto-regression (AR) matches or outperforms every open-source time series FM on a standard suite of seven forecasting tasks, despite using four or more orders of magnitude fewer parameters and data. These results demonstrate that genomics, satellite imaging, and time series have not yet had their “BERT moment” (Devlin et al., 2019), i.e. these domains have not yet pretrained FMs that dominate traditional supervised approaches. This is despite the fact that all them have BERT-scale111Models with 100M+ parameters trained on 100x or more data than supervised tasks in the domain are given. FMs and the fact that many of them are already witnessing a shift towards not comparing with supervised approaches, as was seen in natural language processing (NLP) post-BERT. More broadly, since these domains are among the most high-profile areas with specialized FMs, our results challenge the prevailing assumption that pretrained models yield superior performance. They also reinforce the need for robust and well-tuned baselines, with surprising findings such as (a) simply tuning kernel sizes and dilation rates in standard CNN backbones dominates a genomics classification benchmark and (b) rescuing the century-old AR forecaster from obsolescence is as easy as considering lookback parameters larger than five and training on a GPU. To facilitate ongoing research in these and other domains, we make code associated with both our CNN-tuning pipeline (DASHA222https://github.com/ritvikgupta199/DASHA) and our AR-on-GPU workflow (Auto-AR333https://github.com/Zongzhe-Xu/AutoAR) publicly available."
https://arxiv.org/html/2411.02793v1,Toward Robust Incomplete Multimodal Sentiment Analysis via Hierarchical Representation Learning,"Multimodal Sentiment Analysis (MSA) is an important research area that aims to understand and recognize human sentiment through multiple modalities. The complementary information provided by multimodal fusion promotes better sentiment analysis compared to utilizing only a single modality. Nevertheless, in real-world applications, many unavoidable factors may lead to situations of uncertain modality missing, thus hindering the effectiveness of multimodal modeling and degrading the model’s performance. To this end, we propose a Hierarchical Representation Learning Framework (HRLF) for the MSA task under uncertain missing modalities. Specifically, we propose a fine-grained representation factorization module that sufficiently extracts valuable sentiment information by factorizing modality into sentiment-relevant and modality-specific representations through crossmodal translation and sentiment semantic reconstruction. Moreover, a hierarchical mutual information maximization mechanism is introduced to incrementally maximize the mutual information between multi-scale representations to align and reconstruct the high-level semantics in the representations. Ultimately, we propose a hierarchical adversarial learning mechanism that further aligns and adapts the latent distribution of sentiment-relevant representations to produce robust joint multimodal representations. Comprehensive experiments on three datasets demonstrate that HRLF significantly improves MSA performance under uncertain modality missing cases.","Multimodal sentiment analysis (MSA) has attracted wide attention in recent years. Unlike unimodal emotion recognition tasks [9, 63, 64, 53, 56], MSA understands and recognizes human emotions through multiple modalities, including language, audio, and visual [31, 58]. Previous studies have shown that combining complementary information among different modalities facilitates valuable semantic generation [41, 40, 61, 55, 62]. MSA has been well studied so far under the assumption that all modalities are available in the training and inference phases [12, 66, 54, 57, 56, 25, 59, 60]. Nevertheless, in real-world applications, modalities may be missing due to security concerns, background noises, sensor limitations and so on. Ultimately, these incomplete multimodal data significantly hinder the performance of MSA. For instance, as shown in Figure 1, the entire visual modality and some frame-level features in the language and audio modalities are missing, leading to an incorrect prediction. In recent years, many studies [8, 28, 26, 49, 37, 50, 76, 74, 68, 27, 23, 22] attempt to address the problem of missing modalities in MSA. For example, SMIL [29] estimates the latent features of the missing modality data via Bayesian Meta-Learning. However, these methods are constrained by the following factors: (i) Implementing complex feature interactions for incomplete modalities leads to a large amount of information redundancy and cumulative errors, resulting in ineffective extraction of sentiment semantics. (ii) Lacking consideration of semantic and distributional alignment of representations, causing imprecise feature reconstruction and nonrobust joint representations. Figure 1: A case of incorrect prediction by the traditional model with missing modalities. The pink and yellow areas indicate intra- and inter-modality missingness, respectively. To address the above issues, we propose a Hierarchical Representation Learning Framework (HRLF) for the MSA task under uncertain missing modalities. HRLF has three core contributions: (i) We present a fine-grained representation factorization module that sufficiently extracts valuable sentiment information by factorizing modality into sentiment-relevant and modality-specific representations through intra- and inter-modality translations and sentiment semantic reconstruction. (ii) Furthermore, a hierarchical mutual information maximization mechanism is introduced to incrementally align the high-level semantics by maximizing the mutual information of the multi-scale representations of both networks in knowledge distillation. (iii) Eventually, we propose a hierarchical adversarial learning mechanism to progressively align the latent distributions of representations leveraging multi-scale adversarial learning. Based on these components, HRLF significantly improves MSA performance under uncertain modality missing cases on three multimodal benchmarks."
https://arxiv.org/html/2411.02773v1,FedBlock: A Blockchain Approach to Federated Learning against Backdoor Attacks,"Federated Learning (FL) is a machine learning method for training with private data locally stored in distributed machines without gathering them into one place for central learning. Despite its promises, FL is prone to critical security risks. First, because FL depends on a central server to aggregate local training models, this is a single point of failure. The server might function maliciously. Second, due to its distributed nature, FL might encounter backdoor attacks by participating clients. They can poison the local model before submitting to the server. Either type of attack, on the server or the client side, would severely degrade learning accuracy. We propose FedBlock, a novel blockchain-based FL framework that addresses both of these security risks. FedBlock is uniquely desirable in that it involves only smart contract programming, thus deployable atop any blockchain network. Our framework is substantiated with a comprehensive evaluation study using real-world datasets. Its robustness against backdoor attacks is competitive with the literature of FL backdoor defense. The latter, however, does not address the server risk as we do.","Federated Learning (FL) [1] is a Machine Learning approach to learning a model using distributed training data that remain private and unmoved on local machines. A typical FL architecture consists of these local machines, hereafter referred to as the “clients”, which have the training data, and a central server to coordinate the training, called the “aggregation” server. The learning is an iterative procedure. In the first step, the aggregation server broadcasts a global learning model, initially random, to all clients. In the second step, each client in parallel performs local training on its own local data to improve this model. In the third step, the clients send their respective improved models to the server who in turn aggregates them to obtain a new global model. Then the first step is repeated until the global model converges. FL is elegant in idea and has widespread applications [2]. However, its success relies on the server working normally and clients being good citizens. This is not always true in practice. Indeed, FL is vulnerable to two types of security risks: server attack and backdoor attack. Server attack: Like in any client/server system, the centralized server is a security bottleneck. In FL, an attacker who gains control over the aggregation server can distribute bad global models to the clients, thus poisoning the entire system. Naturally, besides typical solutions designed to secure a server, the best way to avoid server attacks is by deploying multiple servers to decentralize this single point of failure. However, we then face a new challenge of how to coordinate these servers because the “coordinator” could itself become a new single point of failure. Backdoor attack: Dishonest clients can send fake local models to the server. In “untargeted” backdoor attacks, the poisoned global model leads to randomly bad predictions. In “targeted” backdoor attacks, the attacker’s goal is to make the global model always predict according to a targeted outcome. Backdoor attacks are common threats to computer systems. According to the IBM Security X-Force Threat Intelligence Index 2023, they are the top action by cybercriminals. Nearly a quarter of cyber incidents last year involved backdoor attacks. Recently, backdoor danger has been demonstrated with FL systems, raising serious concerns [3]. In this paper, we are interested in immunizing FL against the above attacks. Our approach is inspired by blockchain technology [4]. Blockchain is a computing solution for executing transactions in a way that is honest, immutable, and traceable. Because blockchain runs on a decentralized network of many autonomous computers with majority-based consensus, it is unaffected by any attack. Our idea is that, instead of running the centralized model aggregation on a server, we run it on the “blockchain computer”. Precisely, we implement the task of centralized aggregation as a smart contract to deploy on a blockchain network. An immediate benefit is that the risk due to server attacks becomes non-existent. Our goal then is narrowed to how to mitigate backdoor attacks. From the system-wide perspective, our work is a fresh direction compared to the literature. FL defense against server attacks and backdoor attacks mainly serves non-blockchain settings [5]. Meanwhile, blockchain use for FL has started only recently with purposes mostly not about backdoor risks, for example, to remove the dependency on the central server [6], incentivize clients to contribute training data [7], or provide robustness to non-backdoor security vulnerabilities [8]. Existing research on backdoor defense in blockchain-based FL is rare [9], which requires creating a dedicated blockchain network, a difficult task with limited adoption. In contrast, we make the following key contributions: • We propose FedBlock, a blockchain-based FL framework that provides simultaneous defense against server attacks and backdoor attacks, works at the smart contract level, and hence can run on any blockchain network. To our knowledge, this is the first framework with such features. • We propose an implementation of FedBlock integrating a realistic backdoor-defense technique. The result is a novel decentralized backdoor defense for FL. It uses an efficient amount of model data that is discriminative enough to distinguish compromised clients from the benign. • We justify and validate our technique with in-depth experiments. We evaluate comprehensive scenarios using various datasets, threat models, and attack scenarios. The results demonstrate FedBlock’s viability and superiority compared to the literature. The remainder of the paper is organized as follows. Related work is reviewed in Section II. Some preliminaries on federated learning and backdoor models are provided in Section III. The FedBlock framework is proposed in Section IV, followed by a specific backdoor-defense implementation in Section V. The evaluation results are discussed in Section VI. The paper concludes in Section VII with pointers to our future work."
https://arxiv.org/html/2411.02724v1,"TransUNext: towards a more advanced U-shaped
framework for automatic vessel segmentation in the
fundus image","Purpose: Automatic and accurate segmentation of fundus vessel images has become an essential prerequisite for computer-aided diagnosis of ophthalmic diseases such as diabetes mellitus. The task of high-precision retinal vessel segmentation still faces difficulties due to the low contrast between the branch ends of retinal vessels and the background, the long and thin vessel span, and the variable morphology of the optic disc and optic cup in fundus vessel images.Methods: We propose a more advanced U-shaped architecture for a hybrid Transformer and CNN: TransUNext, which integrates an Efficient Self-attention Mechanism into the encoder and decoder of U-Net to capture both local features and global dependencies with minimal computational overhead. Meanwhile, the Global Multi-Scale Fusion (GMSF) module is further introduced to upgrade skip-connections, fuse high-level semantic and low-level detailed information, and eliminate high- and low-level semantic differences. Inspired by ConvNeXt, TransNeXt Block is designed to optimize the computational complexity of each base block in U-Net and avoid the information loss caused by the compressed dimension when the information is converted between the feature spaces of different dimensions.Results: We evaluated the proposed method on four public datasets DRIVE, STARE, CHASE-DB1, and HRF. In the experimental results, the AUC (area under the ROC curve) values were 0.9867, 0.9869, 0.9910, and 0.9887, which exceeded the other state-of-the-art (SOTA) methods, respectively. In addition, evaluation metrics such as CAL (connectivity-area-length) were used in the ablation study to quantify the segmentation of coarse and fine vessels.Conclusion: The quantitative and qualitative results validate the superior performance of the proposed method for retinal vessel segmentation, and the robustness in challenging situations, such as lesions or fine vessels in the optic disc region, is well demonstrated. The Source code is available at https://github.com/lixiang007666/TransUNext.Keywords: Fundus image, vessel segmentation, TransNeXt Block, Global Multi-Scale Fusion","The essence of automatic segmentation of fundus vessel images is to dichotomize the vascular pixels in the image with the surrounding pixels. In clinical applications, fundus vessel images are more complex. They require experienced professionals to complete segmentation manually. Not only are they subjective and inefficient, but with the explosion of fundus image data, implementing computer-aided automatic segmentation of vessel networks in fundus images has crucial clinical value [17]. Currently, automatic segmentation methods for fundus vessel images are divided into two main categories: one is based on unsupervised methods, and the other is based on supervised methods, including machine learning and deep learning strategies. Based on unsupervised machine learning methods, Chaudhuri et al. [2] successfully implemented blood vessel segmentation of fundus images using a two-dimensional Gaussian matched filter. Afterward, some segmentation methods based on vessel morphology and particular pixels appeared. For example, Yang et al. [43] proposed a morphological processing method, which first enhances vessel features, suppresses background information, and then applies fuzzy clustering to achieve vessel segmentation; Zhao et al. [44] also proposed a segmentation method based on a deformable model, which uses regional information of different vessel types to achieve segmentation; Li et al. [13] optimized the matched filtering method and applied it to the vessel segmentation task. The segmentation method based on unsupervised learning is fast, but the segmentation result is rough and has low accuracy. The supervised machine learning segmentation method better extracts vessel feature information by strengthening the training model through manually labeled images. Staal et al. [23] and Soares et al. [22] used a two-dimensional filter to extract the overall features of the retinal image and then used naive Bayes to classify the retinal background and blood vessels. Ricci et al. [20] first extracted the green channel of the fundus image during image preprocessing and then used SVM to segment according to the difference in vessel width. Fraz et al. [6] proposed combining AdaBoost and Bagging models, integrating the results of complex feature extraction and the results of binary classification models, and using the supervised method to segment retinal vessel images automatically. Although the accuracy of the supervised machine learning method has been improved, because the algorithm itself cannot adapt to the shape, scale, and geometric transformation of blood vessels, there are still problems, such as low accuracy and low robustness when segmenting small vessels and vessel intersections, and it is difficult to provide an objective basis for clinical diagnosis. With the advent of CNN, the semantic segmentation method based on deep learning can accurately predict vessel and non-vessel pixels and provide descriptions of vessel scale, shape, multiple curvature, and other information. In medical image semantic segmentation methods, U-Net [21] is considered a very successful network, consisting of convolutional encoding and decoding units. It can use a few samples to complete training to perform segmentation tasks better. Its derivative works [18, 19, 37, 41] have also achieved advanced retinal blood vessel segmentation results. In order to further improve the accuracy of retinal blood vessel segmentation, Wang B et al. [26] proposed a variant of U-Net with dual encoders to capture richer context features. Li et al. [30] also proposed an improved end-to-end network of U-Net. This framework uses technologies including compression and excitation (SE) module, residual module, and circular structure and introduces enhanced super-resolution generative adaptive networks (ESRGANS) [34] and improved data enhancement methods to achieve retinal blood vessel segmentation. Wang B et al. [31] also provided a supervision framework for retinal vessel segmentation from thick to thin. Nevertheless, CNN-based approaches cannot model long-range dependencies due to inherent inductive biases such as locality and translational equivalence. Thus Transformer [25], which relies purely on attention mechanisms to build global dependencies without any convolutional operations, has emerged as an alternative architecture that provides better performance than CNNs in computer vision (CV) under pre-training conditions on large-scale datasets. Vision Transformer (Vit) [5] revolutionized the CV field by segmenting images into a sequence of tokens and modeling their global relationships with stacked Transformer blocks. Swin Transformer [15] can produce hierarchical features in a movable window with low computational complexity representation, achieving state-of-the-art performance in various CV tasks. However, the size of medical image datasets is much smaller than the pre-trained datasets in the above work (e.g., ImageNet-21k and JFT-300M). As a result, the Transformer produces unsatisfactory performance in medical image segmentation. Therefore, many hybrid structures combining CNN and Transformer have emerged, which have both advantages and are gradually becoming a compromise solution for medical image segmentation without needing pre-training on large datasets. We summarize several popular hybrid architectures based on Transformer and CNN in medical image segmentation. These hybrid architectures add the Transformer to a CNN-based backbone model or replace some architecture components. For example, UNETR [9] uses an encoder and decoder architecture where the encoder is composed of a cascaded block built with a pure Transformer, and the decoder is a stacked convolutional layer, see Fig. 1(a).TransBTS [33] and TransUNet [3] introduce a relationship between the encoder and decoder composed of a CNN a Transformer, see Fig. 1(b). coTr [39] bridges all stages from the encoder to the decoder through the Transformer, not only the adjacent stages, allowing to exploit the global dependencies at multiple scales, see Fig. 1(c). Furthermore, nn-Former [45, 1] interweaves Transformer and convolutional blocks into a hybrid model where convolution encodes precise spatial information while Transformer captures global context information, see Fig. 1(d). As seen from Fig. 1, these architectures implement a serial combination of Transformer and CNN from a macroscopic perspective. However, in these combinations, the convolutional and self-attention mechanisms cannot be applied throughout the network structure, making it challenging to model local and global features efficiently. Figure 1: Comparison of several hybrid architectures of Transformer and CNN. (Color figure online) In this paper, we propose a more advanced U-shaped architecture of a hybrid Transformer for fundus vessel image segmentation (TransUNext), the pattern of which is shown in Fig. 1(e), and TransUNext combines the advantages of both Fig. 1(c) and Fig. 1(d). It can take full advantage of the global dependencies at multi-scale, fuse high-level semantic and low-level detail information, and efficiently aggregate local and global features. In addition, inspired by ConvNeXt [16], we also optimize each base convolutional block in the U-shaped architecture to maintain excellent segmentation capability even at low computational complexity. We found that TransUNext can further improve the accuracy of vessel segmentation with the following main contributions: • TransNeXt Block is proposed as the base block in the U-shaped architecture. This base block avoids the information loss caused by the compressed dimension when the information is converted between the feature spaces of different dimensions and uses self-attention to capture global dependencies. At the same time, it avoids large-scale pre-training of the Transformer and has a more negligible computational overhead. • Upgrading the skip-connection of U-Net using Global Multi-Scale Fusion (GMSF) structure and propagating information through all-to-all attention in all tokens at each scale achieves semantic and spatial global multi-scale fusion, which can make full use of high-level semantic and low-level detail information. • The proposed method was evaluated on four public datasets, DRIVE, STARE, CHASE-DB1, and HRF, where the images of HRF are of high resolution. In addition, in addition to some general evaluation metrics, such as AUC, we used CAL for the specificity of the retinal vessel segmentation task [7]. The rest of this paper is organized as follows: Section II introduces the proposed method in detail. Section III describes the experimental implementation and illustrates the experimental results. Section IV gives the conclusions of this paper"
https://arxiv.org/html/2411.02704v1,RT-Affordance: Affordances are VersatileIntermediate Representations for Robot Manipulation,"We explore how intermediate policy representations can facilitate generalization by providing guidance on how to perform manipulation tasks. Existing representations such as language, goal images, and trajectory sketches have been shown to be helpful, but these representations either do not provide enough context or provide over-specified context that yields less robust policies. We propose conditioning policies on affordances, which capture the pose of the robot at key stages of the task. Affordances offer expressive yet lightweight abstractions, are easy for users to specify, and facilitate efficient learning by transferring knowledge from large internet datasets. Our method, RT-Affordance, is a hierarchical model that first proposes an affordance plan given the task language, and then conditions the policy on this affordance plan to perform manipulation. Our model can flexibly bridge heterogeneous sources of supervision including large web datasets and robot trajectories. We additionally train our model on cheap-to-collect in-domain affordance images, allowing us to learn new tasks without collecting any additional costly robot trajectories. We show on a diverse set of novel tasks how RT-Affordance exceeds the performance of existing methods by over 50%, and we empirically demonstrate that affordances are robust to novel settings. Videos available at https://snasiriany.me/rt-affordance","I INTRODUCTION In recent years, we have seen the rise of large pretrained models for learning robot policies. Vision-language-action (VLA) models [rt22023arxiv, kim24openvla], pretrained with large-scale robot data on top of vision-language models (VLMs) [geminiteam2024gemini] come with the promise of generalization to new objects, scenes, and tasks. However, VLAs are not yet reliable enough to be deployed outside of the narrow lab settings on which they are trained. While these shortcomings can be mitigated by expanding the scope and diversity of robot datasets, this is highly resource intensive and challenging to scale. Alternatively, there are various ways of interfacing with the policy that can potentially facilitate generalization by providing useful guidance on how to perform manipulation tasks. Examples of these policy representations include language specifications [zhang2024sprint, belkhale2024rth], goal images [black2023susie], goal sketches [sundaresan2024rtsketch], and trajectory sketches [gu2023rttrajectory]. These interfaces introduce mid-level abstractions that shield the policy from reasoning in a higher dimensional input space — leading to policies that can generalize over these intermediate representations. While one of the most common policy representations is conditioning on language, in practice most robot datasets are labeled with underspecified descriptions of the task and language conditioning does not reveal enough guidance on how to perform the task. Alternatively, goal image-conditioned policies provide detailed spatial context about the final goal configuration of the scene. However, goal-images are high-dimensional, which presents learning challenges due to over-specification issues [sundaresan2024rtsketch, shah2023mutex]. Furthermore, providing goal images at evaluation time is cumbersome for human users. This has lead to exploration of other intermediate representations — trajectory or goal sketches [gu2023rttrajectory, sundaresan2024rtsketch], or keypoints [yuan2024robopoint, fangandliu2024moka] — that attempt to provide spatial plans for the policy. While these spatial plans are informative, they still lack sufficient information for the policy on how to manipulate — e.g. what pose of the gripper should take when picking up a clothes hanger. Figure 1: Bridging robot and internet data via affordances. Prior work has shown the utility of co-training on robot and web datasets. However, robot actions and web content are still disjoint in their structure. We propose using affordances as a means to bridge this gap. Reasoning about affordances requires semantic and spatial reasoning, which is readily needed in VQA and spatial reasoning tasks such as object detection. By incorporating affordance reasoning explicitly in robot control tasks, we can better transfer knowledge from these web datasets to robot control tasks. Figure 2: Comparison of policy interfaces. Conditioning on language is intuitive, yet language typically does not provide enough guidance on how to perform the task. Goal images and trajectory sketches are typically over-specified and present learning challenges. We propose conditioning policies on intermediate affordance representations, which are expressive yet compact representations of tasks, making them easy to specify and to learn. In this work, we seek a policy representation that provides expressive yet lightweight abstractions for learning robust manipulation polices. We propose RT-Affordance, which is a policy conditioned on both language specifications and visual affordances. The visual affordances show the pose of the robot end effector at key stages of the task, visually projected onto the image input of the policy. By conditioning on affordances, the robot will have access to precise yet concise guidance on how to manipulate objects. To allow a seamless experience for the human user, we employ a hierarchical model that only requires task language from the user. The model first predicts the affordances given a task specification in language, and then leverages the affordances as an intermediate representation to steer the policy. The initial affordance prediction module can be trained on existing robot trajectories and web-scale datasets labeled with spatial information and affordances [Ego4D2022CVPR] (see Figure 1). We further enhance capabilities by training on a modest dataset of cheap-to-collect in-domain images annotated with affordances. This allows us to bypass costly robot teleoperation and learn novel tasks more scalably. We perform extensive experiments, where we show that RT-Affordance is effective across a broad range of real world tasks, achieving 69% overall success rate compared to 15% success rate for language-conditioned policies. We show how incorporating both web data and cheap-to-collect affordance images allows us to learn novel tasks without collecting any additional robot demonstrations. Additionally, we demonstrate that the resulting affordance prediction model is robust to distribution shifts, with overall performance on out of distribution settings within 10% of in-distribution evaluations."
https://arxiv.org/html/2411.02639v1,ACTIVE PROMPT TUNING ENABLES GPT-4o TO DO EFFICIENT CLASSIFICATION OF MICROSCOPY IMAGES,"Traditional deep learning-based methods for classifying cellular features in microscopy images require time- and labor-intensive processes for training models. Among the current limitations are major time commitments from domain experts for accurate ground truth preparation; and the need for a large amount of input image data. We previously proposed a solution that overcomes these challenges using OpenAI’s GPT-4(V) model on a pilot dataset (Iba-1 immuno-stained tissue sections from 11 mouse brains). Results on the pilot dataset were equivalent in accuracy and with a substantial improvement in throughput efficiency compared to the baseline using a traditional Convolutional Neural Net (CNN)-based approach.The present study builds upon this framework using a second unique and substantially larger dataset of microscopy images. Our current approach uses a newer and faster model, GPT-4o, along with improved prompts. It was evaluated on a microscopy image dataset captured at low (10x) magnification from cresyl-violet-stained sections through the cerebellum of a total of 18 mouse brains (9 Lurcher mice, 9 wild-type controls). We used our approach to classify these images either as a control group or Lurcher mutant. Using 6 mice in the prompt set the results were correct classification for 11 out of the 12 mice (92%) with 96% higher efficiency, reduced image requirements, and lower demands on time and effort of domain experts compared to the baseline method (snapshot ensemble of CNN models). These results confirm that our approach is effective across multiple datasets from different brain regions and magnifications, with minimal overhead.","Accurate assessments of cellular damage are essential for evaluating brain aging, neurotoxicology, and the efficacy of potential treatments for neurological conditions such as Parkinson’s, Huntington’s, and Alzheimer’s diseases. Previous work shows deep learning architectures can be trained to automatically quantify cell loss in defined regions of interest (ROIs) using disector-based multiple-input multiple-output (MIMO) Fig. 1: Illustration of the workflow for image classification with GPT-4o. frameworks [1, 2]. This approach shows comparable accuracy compared to manual stereology counts by humans (>>>90%) with improved throughput efficiency and less supervised time from experts. Further benefits of these frameworks include enhanced reproducibility (100% test-retest reliability) due to lower subjectivity and less variation from human factors such as training, experience, fatigue, etc. A subsequent study [3] demonstrated that snapshot ensembles of CNNs [4] trained on low-magnification (10-20x) images can make accurate predictions of microglial proliferation at the global level. This CNN-based approach showed a classification accuracy of 10 out of 11 cases correct (91%). A limitation of deep learning-based approaches for automatic cell counts is the requirement for quantitative ground truth, i.e., stereology-based counts, for training separate models for each cell type. Collection of this ground truth data cannot be crowd-sourced due to requirements for domain experts and integrated hardware-software stereology systems. Second, the requirement for labeling microscopy images (annotation) in specific regions of interest is laborious, time-intensive, and often requires a trained technician under the supervision of an experienced domain expert. Third, these approaches require a pre-trained or custom-built model followed by hyper-parameter optimization, training, and testing which can take months of effort by an AI expert. Finally, this labor-intensive, time-consuming, and computationally expensive process must be repeated for every new dataset (cell type). The recent introduction of Vision Language Models (VLMs) has the potential to reduce this bottleneck through in-context learning capabilities [5, 6] and simple prompting of a pre-trained model with a few examples at inference time, i.e., without the need for extensive fine-tuning. VLMs provide generative AI with the capability to understand, interpret, and analyze both text and images in concert. A further strength of VLMs is their ability to leverage knowledge from one modality to inform the analysis of another. Prior work with VLMs reports state-of-the-art results for general and domain-specific Visual Question Answering (VQA), image-text, text-image, and image-image retrieval [5, 7]. Most VLMs incorporate separate encoders for images and text and then use contrastive learning to capture the association between the text, i.e., a sentence, and an image. Learning of cross-modal representations is achieved by maximizing the similarity between matched image-text pairs while minimizing the similarity between unmatched pairs [8, 7]. GPT-4 omni (GPT-4o, for short) is the flagship multi-modal model from OpenAI. Following inputs of prompt sets, i.e., text descriptions of class-specific features in corresponding images, GPT-4o generates output in the form of classifications with text-based explanations for subsequent test images. These explanations can be accepted as is, or corrected and used as the prompt set for a subsequent iteration. As of May 2024, GPT-4o is the state-of-the-art model in vision understanding benchmarks, surpassing other leading models like GPT-4(V), Claude Opus, and Gemini 1.0 Ultra [9, 10]. Our previous work used the then state-of-the-art VLM model, GPT-4(Vision), for the classification of Iba-1 immunostained microglia cells in the hippocampus of tissue sections through brains of mice treated with either a powerful neurotoxin (tri-methyl tin, TMT) or saline [11]. A pilot study on a subset of data from the current study compared GPT-4(Vision) and GPT-4o. This experiment showed superior performance and reduced latency for GPT-4o. For this reason, we selected GPT-4o for subsequent experiments. In this work, we evaluated our Active Prompt Tuning [11] on a dataset of low-magnification (10x) images captured from mouse brain cerebellum sections stained with cresyl violet, a relatively low signal: noise general marker for all brain cells [12]. This dataset included images from 18 mice, a substantially larger number of mice than in our previous work with microglial cells from mouse hippocampus using the same few-shot prompting approach [11]. Specifically, we used images from 9 Lurcher mutants and 9 wild-types (controls) to assess the classification accuracy of the model using few-shot prompting. Notably, the number of images used for ground truth training (prompt sets) represents a small fraction (2%) of the image dataset required for the baseline CNN method. We also contrast the testing accuracy and time required to prepare the ground truth for the classification of two diverse datasets at different magnifications using our approach based on few-shot prompting compared to a traditional CNN approach [3]."
https://arxiv.org/html/2411.02637v1,"FuseCaps: Investigating Feature Fusion Based
Framework for Capsule Endoscopy Image Classification","In order to improve model accuracy, generalization, and class imbalance issues, this work offers a strong methodology for classifying endoscopic images. We suggest a hybrid feature extraction method that combines convolutional neural networks (CNNs), multi-layer perceptrons (MLPs), and radiomics. Rich, multi-scale feature extraction is made possible by this combination, which captures both deep and handmade representations. These features are then used by a classification head to classify diseases, producing a model with higher generalization and accuracy. In this framework we have achieved a validation accuracy of 76.2% in the capsule endoscopy video frame classification task.","Endoscopy is a procedure that is used to have a close look at the organs. This is done either to detect the diseases or observe the cellular patterns. Early detection of diseases is important as it helps to reduce the mortality rate and also improve the development of medicines. Here the subset of Machine Learning that is Deep Learning comes into play. Deep Learning has been implemented widely in the medical field to detect gastrointestinal and liver-related diseases. As a result, many models have been developed to classify capsule endoscopy images. Some of them involve the use of Convolution Neural Networks or Transfer Learning. But to ensure that the classification of the images is to the point, we have introduced the combination of Radiomics and Convolution Neural networks to enrich the feature dataset and lastly use that feature dataset to perform classification."
https://arxiv.org/html/2411.02627v1,Towards more efficient agricultural practices via transformer-based crop type classification††thanks:Accepted at NeurIPS 2024 Workshop on Tackling Climate Change with Machine Learning,"Machine learning has great potential to increase crop production and resilience to climate change. Accurate maps of where crops are grown are a key input to a number of downstream policy and research applications. In this proposal, we present preliminary work showing that it is possible to accurately classify crops from time series derived from Sentinel 1 and 2 satellite imagery in Mexico using a pixel-based binary crop/non-crop time series transformer model. We also find preliminary evidence that meta-learning approaches supplemented with data from similar agro-ecological zones may improve model performance. Due to these promising results, we propose further development of this method with the goal of accurate multi-class crop classification in Jalisco, Mexico via meta-learning with a dataset comprising similar agro-ecological zones.","1 Agriculture and AI Agriculture stands as the backbone of many economies worldwide, contributing as much as 60% to a country’s GDP and acting as a source of livelihood for over one billion people globally [1, 2, 3]. The sector is highly affected by climate change due to the increasing prevalence of adverse weather conditions such as prolonged droughts, increased temperatures and precipitation, and unpredictable pests and diseases, all culminating in declining land productivity [2, 3]. According to the FAO [4], about 757 million people worldwide face acute hunger in 2023. Moreover, the United Nations estimates a 25%percent\%% global population growth by 2050, with developing and emerging economies within Latin America and Sub-Saharan Africa expected to grow the most [5]. This will ultimately put more pressure on already strained food systems, and we cannot simply clear more agricultural land; doing so results in deforestation and excessive use of resources such as environmentally damaging fertilizers and chemicals to increase productivity. Instead of brute-forcing a solution to this problem, we must find ways to use current resources more efficiently. Accurate and timely crop classification is essential for centralized resource allocation, crop-specific yield prediction, and agricultural policy making. For instance, allocating sufficient resources to farmers throughout the year allows planting flexibility and greater efficiency. Also, crop suitability to specific climates is crucial, as some crops require either cooler or warmer conditions, dictating optimal planting times. Additionally, local demand plays a significant role: even if a farmer successfully grows a large quantity of a product, it may not be possible to sell that product locally if there is insufficient demand or storage capacity. Accurate and timely crop classification is therefore necessary for increasing land productivity. However, traditional crop classification methods like on-the-ground ‘field walks’ and manual satellite imagery classification are error-prone and costly due to the need for a human-in-the-loop. Machine learning methods do not have such downsides. These automatic techniques can be deployed at scale far more efficiently than traditional crop classification as they do not rely on manual intervention once trained. This paper’s contributions are two-fold: first, we present exploratory work on training a time series transformer classifier on satellite imagery time series to distinguish between agricultural and non-agricultural land. Second, we propose expanding this method to multi-class crop classification, addressing a critical data gap for policymakers in Jalisco, Mexico."
https://arxiv.org/html/2411.02619v1,Tracking Tumors under Deformation from Partial Point Clouds using Occupancy Networks,"To track tumors during surgery, information from preoperative CT scans is used to determine their position. However, as the surgeon operates, the tumor may be deformed which presents a major hurdle for accurately resecting the tumor, and can lead to surgical inaccuracy, increased operation time, and excessive margins. This issue is particularly pronounced in robot-assisted partial nephrectomy (RAPN), where the kidney undergoes significant deformations during operation. Toward addressing this, we introduce a occupancy network-based method for the localization of tumors within kidney phantoms undergoing deformations at interactive speeds. We validate our method by introducing a 3D hydrogel kidney phantom embedded with exophytic and endophytic renal tumors. It closely mimics real tissue mechanics to simulate kidney deformation during in vivo surgery, providing excellent contrast and clear delineation of tumor margins to enable automatic threshold-based segmentation. Our findings indicate that the proposed method can localize tumors in moderately deforming kidneys with a margin of 6mm to 10mm, while providing essential volumetric 3D information at over 60Hz. This capability directly enables downstream tasks such as robotic resection.","Kidney cancer is one of the most common forms of cancer in the US, with over 65,000 new patients being diagnosed every year, leading to over 15,000 deaths [1]. The standard treatment for localized small renal masses has shifted from radical nephrectomy (complete kidney removal) toward the more minimally invasive approach of partial nephrectomy (removal of the tumor, retaining partial kidney function). One of the main challenges during tumor removal is ensuring the resection of adequate tumor margins. The margin needs to ensure that no cancer cells remain in the kidney, while also avoiding excessive removal of healthy tissue to preserve organ function. For operations involving tumor resection, surgeons commonly interpret 2D preoperative scans and mentally construct a 3D anatomical model during the surgery. This process of creating a mental model can extend the duration of the procedure, increasing the risk of damaging adjacent tissues [2]. The difficulty is exacerbated by tissue deformation during surgery, causing the internalized mental model to become inaccurate over time. Registration methods that overlay pre-operative data onto the intra-operative scene can provide surgeons with valuable location information for regions of interest. Altamar et al. [3] develop a deformable registration method based on biomechanical elastic models to simulate in-vivo deformation during partial nephrectomy. While their method accurately analyzes kidney deformation, it doesn’t address the tumor and margin deformation. Y1.0e0]ptKidney Phantom Sensor Point Cloud Occupancy Point Cloud Figure 1: Given a known kidney phantom with two tumors and a pre-operative CT scan, we estimate, from a single sensor point cloud derived from a depth image, a dense occupancy point cloud that encodes the locations of each tumor. Point-based registration enables the calculation of precise transformations between markers in pre-operative images and those in the physical space, as shown on intra-operative images. In the absence of distinct anatomical landmarks on organs like the kidney, applying fiducials to the renal surface is advocated for point-based registration [4, 5]. Nimmagadda et al. [6] develop a touch-based registration approach using ink fiducials tattooed on the kidney surface. They employ the Iterative Closest Point (ICP) algorithm for initial alignment with a pre-operative point cloud, followed by point-based re-registration to account for kidney deformation. While precise, tattooing intra-operative tissue has been associated with increased patient morbidity [2]. Zhang et al. [7] propose a different approach. They gather surface data by capturing multiple views of the kidney, stitching them together to construct a 3D surface. They then apply Coherent Point Drift (CPD) for deformable registration. This technique achieves markerless deformable registration but requires multiple views of the organ, wherein the organ’s deformation during the image collection process could introduce errors. In previous work [8], we demonstrate a method for reconstructing deformed objects composed of multiple parts from single-viewpoint point clouds using a neural multi-class occupancy function. This approach utilizes a combination of occupancy networks [9] and PointNet++ [10] to infer a 3D object that aligns with the observation of the deformable real-world object. It relies on deforming a pre-operative 3D model digitally to generate ground truth occupancy samples and a sensor-based depth image. Through supervised learning, the network is trained to label occupancy samples correctly, learning the adaptation of the labelling to observed deformations directly from the data. A distinct advantage of this method is its ability to estimate the position of deformable parts without necessitating initial registration. To validate the accuracy of deformable registration and localization methods, organ ”phantoms” are widely used, which aim to replicate the mechanical properties and medical imaging results of real organs. Birnbaum et al. [11] customized renal inserts with different Hounsfield Unit (HU) values in anthropomorphic phantoms to produce varying brightness between kidney components under CT imaging. However, the analysis of mechanical tissue properties was not undertaken. In our previous work, we develop a hydrogel kidney phantom capable of reproducing both the mechanical and functional properties of living tissue [12]. This phantom is specifically designed for Robot-Assisted Partial Nephrectomy (RAPN) training, with a focus on mechanical fidelity over CT imaging characteristics. A method that can register or reconstruct deformable objects from an observation, can be applied to perform resection tasks autonomously. We recently presented the Autonomous System for Tumor Resection (ASTR) [13] method. It is a vision-guided robotic system that demonstrates success in tongue tumor resection (i.e. glossectomy). In this work, we present an occupancy network-based tumor localization approach designed to use pre-operative CT images with intra-operative RGBD sensor data. Our method only requires a single depth image from a single viewpoint to estimate the deformation of the kidney and locate embedded tumors at over 60⁢Hz60Hz60\text{Hz}60 Hz, see Figure 1. Therefore, it can track tumors, enabling the delineation of margins in the presence of deformations occurring during resection. Furthermore, we evaluate our method through the use of a novel 3D hydrogel kidney phantom, which is embedded with renal tumors. This phantom possesses realistic mechanical properties and allows easy automatic segmentation by providing varying brightnesses under CT imaging. The evaluation demonstrates the utility of our method in guiding a robotic resection, accounting for potential deformations during partial nephrectomy procedures."
https://arxiv.org/html/2411.02614v1,"Divergent Domains, Convergent Grading: Enhancing Generalization in Diabetic Retinopathy Grading","Diabetic Retinopathy (DR) constitutes 5% of global blindness cases. While numerous deep learning approaches have sought to enhance traditional DR grading methods, they often falter when confronted with new out-of-distribution data thereby impeding their widespread application. In this study, we introduce a novel deep learning method for achieving domain generalization (DG) in DR grading and make the following contributions. First, we propose a new way of generating image-to-image diagnostically relevant fundus augmentations conditioned on the grade of the original fundus image. These augmentations are tailored to emulate the types of shifts in DR datasets thus increase the model’s robustness. Second, we address the limitations of the standard classification loss in DG for DR fundus datasets by proposing a new DG-specific loss – domain alignment loss; which ensures that the feature vectors from all domains corresponding to the same class converge onto the same manifold for better domain generalization. Third, we tackle the coupled problem of data imbalance across DR domains and classes by proposing to employ Focal loss which seamlessly integrates with our new alignment loss. Fourth, due to inevitable observer variability in DR diagnosis that induces label noise, we propose leveraging self-supervised pretraining. This approach ensures that our DG model remains robust against early susceptibility to label noise, even when only a limited dataset of non-DR fundus images is available for pretraining. Our method demonstrates significant improvements over the strong Empirical Risk Minimization baseline and other recently proposed state-of-the-art DG methods for DR grading. Code is available at dg-adr.","Diabetic Retinopathy (DR) is a complication due to Diabetes Mellitus (DM). As of 2021, the global burden of DM surpassed 529 million people, a number projected to reach to a staggering 1.3 billion by 2050, with escalating prevalence rates worldwide [42], thus inducing a parallel escalation in DR instances. DR manifests in several symptoms which include retinal hemorrhages, abnormal growth of blood vessels, aneurysm, hard exudates and cotton wool spots, with the severity grouped into five main progressive grades/classes (Fig. 1). Early DR detection is pivotal for enabling timely intervention since the initial DR stages commonly unfold without noticeable symptoms [31]. Figure 1: Illustration of DR Progression: From an initial healthy state to advanced stages of DR [39]. NPDR stands for non-proliferative Diabetic Retinopathy. The traditional approach to DR screenings, involving fundus scans analyzed by highly-skilled ophthalmologists, faces significant challenges due to manual process errors, subjective diagnosis, low ophthalmologist-to-patient ratios and prolonged examination turnaround times in some countries [44], hindering efforts to address the rising incidences of DR within the DM population. In response to these challenges, various studies including [17, 32, 45], have focused on automating detection and grading using deep learning (DL) methods. However, these approaches face limitations, particularly in real-world deployment, as demonstrated by Google research team’s DR solution, which performed well in controlled lab settings but failed upon deployment [44]. This happens primarily due to the fact that DR fundus images suffer from shifts caused by the variation in acquisition procedures, diversity of population groups and dataset sampling. This domain shift, makes DR grading an even more challenging task since the grading already faces significant inter and intra-observer variability [9]. Our work aligns with the domain generalization (DG) research which aims to achieve robust model performance on DR out-of-distribution (OOD) fundus data [9, 25]. While a few studies have been devoted to DG for DR over recent years, still considerable efforts are required for these models to attain satisfactory real-world performance. DR fundus datasets are plagued by limited temporal continuity, privacy concerns affecting data accessibility, high annotation costs and constrained ethnic diversity hindering generalizable performance on new data with different distributions. We therefore address these challenges by introducing a novel approach to augmenting images, which not only increases source domains’ size at a relatively cheaper cost but also enhances semantic diversity, offering a solution which helps overcome domain shift in DR grading. We introduce domain generalizable DR fundus augmentations (DR-Aug), which produces varied and diagnostically relevant fundus images, that retain essential anatomical attributes while generating novel but clinically significant representations of diseased conditions. Notably, this capability is beyond what traditional augmentations (contrast adjustments, rotations etc.) can offer (see Fig. 2). The feature space of the fundus datasets inherently reveals clustering based on domain-specific information and at the same time the intra-domain shift (variations within the same domain) is also evident—Fig. 3. As such ensuring a domain generalizable model requires scattering features across all domains, realizing a feature space capable of accommodating new OOD data. Accordingly, we propose a novel DG-specific loss, namely domain alignment loss (DomAlign). This loss facilitates the aggregation of features corresponding to the same DR grade for all the source domains and simultaneously disperses the compactness of features originating from the same domain when they exhibit dissimilar semantic attributes, while accounting for potential intra-domain shift. Our DomAlign loss is flexible since it can be combined with any appropriate classification loss to demonstrate remarkable efficacy in ensuring consistent performance amidst domain shifts. Figure 2: Samples of DR-Aug augmented fundus images, when the prompts used correspond to the original image’s grade. First row corresponds to mild non-proliferative DR, the second row shows proliferative DR and third row no DR. The generated augmentations are consistent with the symptoms for the given text prompt and also exhibit some variations of the images present within the dataset (for that particular prompt), even when these variations are not within the original image itself. Temporal continuity for a particular DR grade is induced for a given original image e.g. second row. Figure 3: Representation of domain shift using t-SNE embeddings. Different colors correspond to distinct domains. Each plot represents a specific grade, ranging from 0 to 4. The final plot (right bottom) encapsulates the collective features of all grades within each respective domain. Furthermore, given the apparent high class imbalance in fundus datasets, it becomes crucial to devise a mechanism that effectively mitigates this imbalance. Consequently, we propose to leverage Focal loss [34] and integrate it with our DomAlign loss to simultaneously tackle both the high class imbalance and the domain shift. Finally, DR datasets demonstrate substantial observer variability, with inter-grader variability reaching up to 65% and intra-grader variability up to 60% [39], thereby introducing label noise. We therefore propose leveraging self-supervised learning (SSL) pretraining to reduce susceptibility to label noise during the initial training phase, guiding the model to handle noisy labels more effectively. Our overall framework, featuring the non-trivial integration of aforementioned novel componenets, coined as Domain-Generalized Augmentations for DR with Domain Alignment (DG-ADR) consistently exceeds the efficacy of recently proposed DG methods by notable margins across three different performance metrics."
https://arxiv.org/html/2411.02604v1,Computing critical exponents in 3D Ising model via pattern recognition/deep learning approach,"In this study, we computed three critical exponents (α,β,γ𝛼𝛽𝛾\alpha,\beta,\gammaitalic_α , italic_β , italic_γ) for the 3D Ising model with Metropolis Algorithm using Finite-Size Scaling Analysis on six cube length scales (L=20,30,40,60,80,90), and performed a supervised Deep Learning (DL) approach (3D Convolutional Neural Network or CNN) to train a neural network on specific conformations of spin states. We find one can effectively reduce the information in thermodynamic ensemble-averaged quantities vs. reduced temperature t (magnetization per spin <m>⁢(t)expectation𝑚𝑡<m>(t)< italic_m > ( italic_t ), specific heat per spin <c>⁢(t)expectation𝑐𝑡<c>(t)< italic_c > ( italic_t ), magnetic susceptibility per spin <χ>⁢(t)expectation𝜒𝑡<\chi>(t)< italic_χ > ( italic_t )) to six latent classes. We also demonstrate our CNN on a subset of L=20 conformations and achieve a train/test accuracy of 0.92 and 0.6875, respectively. However, more work remains to be done to quantify the feasibility of computing critical exponents from the output class labels (binned m,c,χ𝑚𝑐𝜒m,c,\chiitalic_m , italic_c , italic_χ) from this approach and interpreting the results from DL models trained on systems in Condensed Matter Physics in general.","In terms of dimensions, the three-dimensional Ising square lattice model is a simple extension of the two-dimensional Ising model, which Onsager solved analytically in 1944. However, to this day, no exact solution exists. Critical exponents are typically found using simulation results. This project was primarily motivated by two papers: a computational physics simulation study and a pattern recognition study using DL on a 2D Ising model. Sonsin et al. [1] simulated the 3D Ising system using a Monte Carlo method/Metropolis algorithm and found that for three system sizes of L=150,200,250, the Metropolis approach (local spin update) can reach equilibrium in a feasible amount of time (10000 Monte Carlo Sweeps (MCS)) and found the critical temperature Tcsubscript𝑇𝑐T_{c}italic_T start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT to be ≈4.512absent4.512\approx 4.512≈ 4.512 kB⁢T/Jsubscript𝑘𝐵𝑇𝐽k_{B}T/Jitalic_k start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT italic_T / italic_J from Binder’s Cumulant U4=1−<m4>3⁢<m2>2subscript𝑈41expectationsuperscript𝑚43superscriptexpectationsuperscript𝑚22U_{4}=1-\frac{<m^{4}>}{3<m^{2}>^{2}}italic_U start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT = 1 - divide start_ARG < italic_m start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT > end_ARG start_ARG 3 < italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT > start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG. Their approach contrasts the standard global spin update methods typically used for large system sizes of L (e.g., Wolff, Swendsen-Wang algorithms), which avoid the critical slowing down near TCsubscript𝑇𝐶T_{C}italic_T start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT. Holzbeck et al. [2] investigated the phase transition of the 2D Ising model using a DL approach to recognize patterns in spin conformations from equilibrated Monte Carlo Metropolis algorithm simulations. They found the trained DL model could distinguish between conformations with an inner region coupling constant Ji⁢n⁢n⁢e⁢r=2subscript𝐽𝑖𝑛𝑛𝑒𝑟2J_{inner}=2italic_J start_POSTSUBSCRIPT italic_i italic_n italic_n italic_e italic_r end_POSTSUBSCRIPT = 2 from conformations with an outer region coupling constant Jo⁢u⁢t⁢e⁢r=1subscript𝐽𝑜𝑢𝑡𝑒𝑟1J_{outer}=1italic_J start_POSTSUBSCRIPT italic_o italic_u italic_t italic_e italic_r end_POSTSUBSCRIPT = 1. The two separate but primary goals for this project were to: 1. Compute three critical exponents for the 3D Ising model using finite-size scaling analysis (FSSA) on simulation results, replicating the model and parameters in [1]. 2. Design, implement, and evaluate a supervised Deep Learning (DL) approach to recognize specific realizations of spin states, each categorized by a unique pattern or montage, extending the work from [2]. This paper was initially written as a final project for the PHYS 7350 Advanced Computational Physics class at the University of Houston (UH) in Spring 2020."
https://arxiv.org/html/2411.02572v1,ViTally Consistent: Scaling Biological Representation Learning for Cell Microscopy,"Large-scale cell microscopy screens are used in drug discovery and molecular biology research to study the effects of millions of chemical and genetic perturbations on cells. To use these images in downstream analysis, we need models that can map each image into a feature space that represents diverse biological phenotypes consistently, in the sense that perturbations with similar biological effects have similar representations. In this work, we present the largest foundation model for cell microscopy data to date, a new 1.9 billion-parameter ViT-G/8 MAE trained on over 8 billion microscopy image crops. Compared to a previous published ViT-L/8 MAE, our new model achieves a 60% improvement in linear separability of genetic perturbations and obtains the best overall performance on whole-genome biological relationship recall and replicate consistency benchmarks. Beyond scaling, we developed two key methods that improve performance: (1) training on a curated and diverse dataset; and, (2) using biologically motivated linear probing tasks to search across each transformer block for the best candidate representation of whole-genome screens. We find that many self-supervised vision transformers, pretrained on either natural or microscopy images, yield significantly more biologically meaningful representations of microscopy images in their intermediate blocks than in their typically used final blocks. More broadly, our approach and results provide insights toward a general strategy for successfully building foundation models for large-scale biological data.111Correspondence: kian.kd@recursion.com, info@rxrx.ai","Large-scale cell microscopy assays are used to discover previously unknown biological processes (Przybyla & Gilbert, 2022; Bock et al., 2022; Rood et al., 2024) and identify novel drug candidates and targets (Vincent et al., 2022). Labs are now able to achieve extremely high throughput by leveraging high content screening (HCS) systems that combine automated microscopy with robotic liquid handling (Boutros et al., 2015). Extracting meaningful features from microscopy images in large-scale screens has become increasingly difficult as this scale has increased. Public datasets like RxRx3 (Fay et al., 2023) and JUMP-CP (Chandrasekaran et al., 2023) now include millions of cellular images across 100,000s of unique chemical and genetic perturbations. In addition to limitations in expressiveness of the features that can be derived from them, traditional methods relying on customized pipelines for segmentation, feature extraction, and downstream analysis (Caicedo et al., 2017) struggle to handle this scale effectively (Chandrasekaran et al., 2021; Carpenter et al., 2006). The size and complexity of large-scale microscopy data demands image models that can extract rich biological features and do so consistently across experimental replicates, both of which are crucial for downstream biomedical applications. Rich, biologically meaningful representations reveal relationships between genes or compounds to drive the discovery of novel targets and drug candidates, while consistency in features extracted across replicates ensures that findings are reproducible and reliable for therapeutic development. Foundation models have been developed for representing high-dimensional unstructured biological data such as protein structures (Jumper et al., 2021) and transcriptomics (Hao et al., 2024), but the scale and dimensionality of large-scale microscopy data present unique challenges for generating representations that are both biologically informative and consistent across replicates. HCS datasets are often confounded by complex noise known as batch effects (Caicedo et al., 2017), stemming from differences between experimental batches and biological variability. These batch effects – including natural variation in cell populations – obscure the biological effects of perturbations and make it challenging to isolate the specific effects of the perturbations applied (Yang et al., 2019). Overcoming these obstacles with a model capable of generating robust, biologically meaningful representations can empower HCS to systematically interrogate gene function and identify novel drug candidates (Rood et al., 2024). State-of-the-art (SOTA) deep learning methods for microscopy leverage Vision Transformers (ViT) (Dosovitskiy et al., 2020) trained with self-supervised learning (SSL) techniques (Balestriero et al., 2023) to learn unbiased representations from large-scale screens (Doron et al., 2023; Kim et al., 2023; Bourriez et al., 2024). Recent studies have demonstrated that ViTs trained as Masked Autoencoders (MAEs) (He et al., 2022) can effectively scale beyond previous approaches and outperform various supervised and smaller SSL models in capturing biologically informative representations of cell images (Kraus et al., 2024). However, the level of consistency found in these representations across a large number of experimental replicates was not previously reported. Furthermore, compared to recent multi-billion parameter transformers developed for natural images (Dehghani et al., 2023) and natural language (Llama3, 2024), model scale in microscopy lags behind (Kraus et al., 2024; Chen et al., 2023a) despite the existence of massive datasets. In this work, we developed the largest foundation model to date for cell microscopy images, achieving SOTA results in both replicate consistency and biological recall of known gene-gene relationships. Specifically our work offers the following contributions: • We demonstrate that training on a curated microscopy dataset of statistically significant positive samples, named Phenoprints-16M, improves both recall of known gene-gene relationships and consistency of embeddings for gene knockout perturbations (Figure 1A). We describe components of this curation strategy that can be generalized to other scientific datasets (§ 3.1). • We present a new foundation model, MAE-G/8, a 1.86 billion parameter ViT-G/8 MAE trained on Phenoprints-16M over 48,000 H100 GPU hours on more than 8 billion samples from the curated dataset (Figure 1A, § 3.2). • We propose new set of biological linear probing tasks to evaluate representations learned by intermediate ViTs blocks for microscopy data (§ 4). Performance on these linear probing tasks are strongly correlated with performance on important whole-genome scale evaluation metrics while requiring significantly less resources to compute (Figure 4). • We find that using intermediate layers leads to better performance on these downstream whole-genome benchmarks at a lower computational inference cost, across SSL ViTs trained on microscopy or natural images. By taking advantage of our linear probing proxy task, we are able to cheaply find the best performing intermediate block (Eq. 1). Our results indicate that the biological scaling properties first identified by Kraus et al. (2023) extend to the multi-billion parameter regime (§ A.9). We show that our MAE-G/8 model produces a nearly 60% more phenotypically linearly separable latent space compared to previous approaches using the final block of MAE-L/8 (Figure 4), correlating with significant improvements in both recall and replicate consistency when benchmarking across the whole genome (Figure 1B). Figure 1: (A) Overview of performance gain from different foundation model pretraining and inference strategies. (B) Example whole-genome results for replicate consistency and biological relationship recall on StringDB for models trained with different combinations of strategies, by model name and dataset (left to right): MAE-L/8 (RPI-93M, block b=24𝑏24b=24italic_b = 24), MAE-L/8 trimmed to block b∗=15superscript𝑏15b^{*}=15italic_b start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT = 15, MAE-L/8 (Phenoprints-16M, block b∗=20superscript𝑏20b^{*}=20italic_b start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT = 20), MAE-G/8 (Phenoprints-16M, b∗=38superscript𝑏38b^{*}=38italic_b start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT = 38), where b∗superscript𝑏b^{*}italic_b start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT is the optimal block according to linear probes as defined in Equation 1."
https://arxiv.org/html/2411.02571v1,MM-Embed: Universal MultimodalRetrieval with Multimodal LLMs,"State-of-the-art retrieval models typically address a straightforward search scenario, where retrieval tasks are fixed (e.g., finding a passage to answer a specific question) and only a single modality is supported for both queries and retrieved results. This paper introduces techniques for advancing information retrieval with multimodal large language models (MLLMs), enabling a broader search scenario, termed universal multimodal retrieval, where multiple modalities and diverse retrieval tasks are accommodated. To this end, we first study fine-tuning an MLLM as a bi-encoder retriever on 10 datasets with 16 retrieval tasks. Our empirical results show that the fine-tuned MLLM retriever is capable of understanding challenging queries, composed of both text and image, but underperforms a smaller CLIP retriever in cross-modal retrieval tasks due to modality bias from MLLMs. To address the issue, we propose modality-aware hard negative mining to mitigate the modality bias exhibited by MLLM retrievers. Second, we propose to continually fine-tune the universal multimodal retriever to enhance its text retrieval capability while maintaining multimodal retrieval capability. As a result, our model, MM-Embed, achieves state-of-the-art performance on the multimodal retrieval benchmark M-BEIR, which spans multiple domains and tasks, while also surpassing the state-of-the-art text retrieval model, NV-Embed-v1, on MTEB retrieval benchmark. Finally, we explore to prompt the off-the-shelf MLLMs as the zero-shot rerankers to refine the ranking of the candidates from the multimodal retriever. We find that through prompt-and-reranking, MLLMs can further improve multimodal retrieval when the user queries (e.g., text-image composed queries) are more complex and challenging to understand. These findings also pave the way to advance universal multimodal retrieval in the future. We release the model weights at: https://huggingface.co/nvidia/MM-Embed.","Information retrieval is crucial for a variety of downstream tasks, such as question answering (Kwiatkowski et al., 2019), fact-checking (Thorne et al., 2018), and retrieval-augmented generation (Lewis et al., 2020). Existing state-of-the-art retrievers often focus on narrow scenarios. For example, LLM-based retrievers (Wang et al., 2023; Lee et al., 2024; Meng et al., 2024; Moreira et al., 2024) are limited to text-to-text retrieval tasks, where both the query and the retrieved results are text-only. Recent work on multimodal retrieval (Zhang et al., 2024; Jiang et al., 2024) focuses on specific tasks and assumes a homogeneous document format. However, in real-world applications, documents and queries often consist of diverse formats or modalities, such as text, images, and interleaved text and images. To advance information retrieval and support broader search scenarios, this work explores the use of multimodal LLMs (MLLMs; Dai et al., 2024; Liu et al., 2023a; 2024) for universal multimodal retrieval, accommodating diverse user-instructed tasks with multimodal queries and documents, as illustrated in Figure 1. We first explore to fine-tune MLLM-based bi-encoder retrievers with instructions as a guide (Asai et al., 2023) on 16 multimodal retrieval tasks from M-BIER (Wei et al., 2023). We find that MLLM-based retrievers significantly outperform CLIP-based retrievers in the challenging tasks, where interleaved text–image queries are given, such as visual question answering and composed image retrieval (tasks 3 and 7 in Figure 1). However, MLLM-based retrievers underperform in cross-modal retrieval tasks due to the modality bias from MLLMs. That is, given a text-based query with the instruction to retrieve an image (e.g., task 9 in Figure 1), an MLLM-based retriever tends to retrieve a relevant text-only rather than documents with images, especially when we improve the MLLM-based retriever’s text retrieval capability. To address the issue, we propose modality-aware hard negative mining in Section 4.1.1 and continual text-to-text retrieval fine-tuning in Section 4.1.2. Our final retriever, coined MM-Embed, is the first state-of-the-art universal multimodal retriever while maintaining competitive text-to-text retrieval performance across diverse tasks. Finally, we explore to prompt MLLMs as zero-shot rerankers. Surprisingly, we find that the zero-shot MLLM-based rerankers can further boost retrieval accuracy in the tasks, where user queries are interleaved text–image and more challenging to understand. For example, in the composed image retrieval dataset, CIRCO (Baldrati et al., 2023), the zero-shot rerankers are able to refine the ranked lists and significantly boosts the accuracy (mAP@5) over 7 points from the existing state-of-the-art composed-image retriever (Zhang et al., 2024) and our universal multimodal retrievers. This finding indicates that there is still room for improvement in such challenging tasks in order to tackle universal multimodal retrieval. Also, knowledge distillation from zero-shot or few-shot MLLM-based rerankers to retrievers is a promising direction. We summarize our contributions as follows: i) We present a study on applying MLLMs to universal multimodal retrieval. ii) We are the first to build MLLM-based universal multimodal retrievers. Notably, our MM-Embed, initialized from the existing best-performing text retriever (NV-Embed-v1; Lee et al., 2024), not only achieves state-of-the-art results in universal multimodal retrieval benchmark, M-BEIR (Wei et al., 2023), but also surpasses NV-Embed-v1 in text-to-text retrieval tasks on MTEB. iii) We are the first work to explore prompting MLLMs for zero-shot reranking. With a zero-shot MLLM-based reranker, we are able to boost the ranking accuracy over 7 points upon state-of-the-art retrievers in the composed image retrieval task, CIRCO (Baldrati et al., 2023). We organize the rest of the paper as follows. We discuss related work in § 2. We introduce the definition of universal multimodal retrieval in § 3 and present the proposed method in § 4. We report experiment results in § 5 and conclude the paper in § 6. Figure 1: Illustration of universal multimodal retrieval, where diverse tasks with instructions, queries and documents with multimodal formats are supported. In this work, we explore to fine-tune MLLM-based universal multimodal retriever, MM-Embed, and prompt an MLLM for reranking."
https://arxiv.org/html/2411.02547v1,Modeling Uncertainty in 3D Gaussian Splatting throughContinuous Semantic Splatting,"In this paper, we present a novel algorithm for probabilistically updating and rasterizing semantic maps within 3D Gaussian Splatting (3D-GS). Although previous methods have introduced algorithms which learn to rasterize features in 3D-GS for enhanced scene understanding, 3D-GS can fail without warning which presents a challenge for safety-critical robotic applications. To address this gap, we propose a method which advances the literature of continuous semantic mapping from voxels to ellipsoids, combining the precise structure of 3D-GS with the ability to quantify uncertainty of probabilistic robotic maps. Given a set of images, our algorithm performs a probabilistic semantic update directly on the 3D ellipsoids to obtain an expectation and variance through the use of conjugate priors. We also propose a probabilistic rasterization which returns per-pixel segmentation predictions with quantifiable uncertainty. We compare our method with similar probabilistic voxel-based methods to verify our extension to 3D ellipsoids, and perform ablation studies on uncertainty quantification and temporal smoothing.","In order to plan, robots require a world model which captures geometric detail and higher levels of information about their environment. Although some papers propose mapless navigation [1, 2, 3], maps are still widely used due to an interpretable world model which temporally adapts as robots explore their surroundings. Depending on the robot application, maps can store different types of information to increase scene understanding. For many robotic applications, uncertainty of the map is necessary to ensure safe planning in safety-critical environments. In these situations, robots must understand not only the type and location of objects, but confidence in the predictions as well. Uncertainty can arise from noisy perception networks, sensor noise, and sparse views which can ultimately result in incomplete maps. (a) Well fitted 3D-GS render. (b) Poorly fitted 3D-GS render. (c) Semantic prediction on poorly fitted render. (d) Semantic uncertainty on poorly fitted render. Figure 1: While 3D-GS may provide high quality renderings of the environment at novel views with sufficient training data, it may fail to render views which are occluded, unseen, or at different angles from the training data. In the above image, CSS produces semantic (c) and RGB (b) predictions at a novel view without sufficient training data, resulting in a blurry render and incorrect segmentation. Through probabilistic inference, CSS identifies blurs and gaps in the render which correlate with reconstruction quality (d). Continuous mapping combats sparse data by leveraging spatial relations of points to fill in gaps in the map from sparse data probabilistically and with quantifiable uncertainty [4, 5]. Continuous mapping has been successfully applied to applications such as elevation mapping [6] and semantic mapping, by incorporating measurements into nearby cells in the robotic map through a kernel [7]. The kernel effectively defines the influence of input points over nearby cells probabilistically, leading to a closed form update solution through Bayesian Kernel Inference (BKI). However, one challenge of BKI is defining the kernel function, which is generally hand-crafted and recently was shown to be learnable, resulting in 3D ellipsoid shapes [8]. Additionally, BKI has been limited to grid-based solutions which are prone to discretization errors and require accurate depth estimation. Separately, 3D Gaussian Splatting (3D-GS) proposes a new method for novel view synthesis, which learns to model the world explicitly as 3D ellipsoids, with high quality renderings from any angle without the discretization error of grid-based map representations [9]. 3D-GS has captured the attention of the robotics community, with many methods proposing to add additional features to 3D-GS [10, 11] and incorporate 3D-GS into simultaneous localization and mapping (SLAM) pipelines [12, 13]. Some works have recently explored quantifying information gain [14] or optimal ellipsoid pruning [15] in 3D-GS through Fisher Information, however quantifying uncertainty from noisy segmentation networks or novel views remains a challenge. In this work, we leverage the insight that 3D-GS learns valid kernels to propose a novel method for uncertainty quantification in 3D-GS. Our method, which we call Continuous Semantic Splatting (CSS), incorporates semantically labelled images in a Bayesian framework to capture the semantic uncertainty of each 3D ellipsoid. Additionally, through a novel rasterization method, we capture the semantic variance from noisy segmentation predictions in pixel space, as well as information on conflicting categories caused by poor renderings at novel views. To summarize, our contributions are: i. Extend continuous mapping literature from voxel grids to 3D-GS world representation. ii. Formulate novel 3D-GS semantic update with quantifiable semantic variance of ellipsoids. iii. Probabilistic semantic 3D-GS rasterization with quantifiable uncertainty."
https://arxiv.org/html/2411.02534v1,Multi-modal Spatial Clustering for Spatial Transcriptomics Utilizing High-resolution Histology Images††thanks:This study is funded by National Science Foundataion (NSF),"Understanding the intricate cellular environment within biological tissues is crucial for uncovering insights into complex biological functions. While single-cell RNA sequencing has significantly enhanced our understanding of cellular states, it lacks the spatial context necessary to fully comprehend the cellular environment. Spatial transcriptomics (ST) addresses this limitation by enabling transcriptome-wide gene expression profiling while preserving spatial context. One of the principal challenges in ST data analysis is spatial clustering, which reveals spatial domains based on the spots within a tissue. Modern ST sequencing procedures typically include a high-resolution histology image, which has been shown in previous studies to be closely connected to gene expression profiles. However, current spatial clustering methods often fail to fully integrate high-resolution histology image features with gene expression data, limiting their ability to capture critical spatial and cellular interactions.In this study, we propose the spatial transcriptomics multi-modal clustering (stMMC) model, a novel contrastive learning-based deep learning approach that integrates gene expression data with histology image features through a multi-modal parallel graph autoencoder. We tested stMMC against four state-of-the-art baseline models: Leiden, GraphST, SpaGCN, and stLearn on two public ST datasets with 13 sample slices in total. The experiments demonstrated that stMMC outperforms all the baseline models in terms of ARI and NMI. An ablation study further validated the contributions of contrastive learning and the incorporation of histology image features.","Biological tissue samples contain highly complex cellular processes, which are shaped by cell distribution patterns, cell types, cell states, composition, and cell-cell interactions [1]. Such information is crucial for understanding tissue development, repair, and responses to external signals [2, 1]. Single-cell RNA sequencing technology has evolved dramatically in recent years to be more efficient, accessible, and accurate, which enables researchers to obtain deep insights into cellular states and led to the discovery of new cell types [3, 4]. However, while single-cell sequencing provides valuable insights, the lack of crucial contextual information limits the understanding of how cells cohabit, interact, and communicate within their environments [5, 6]. Spatial transcriptomics (ST) addresses this gap by enabling transcriptome-wide gene expression profiling while preserving spatial context[7, 8], which enables researchers to move beyond the scope of cell clustering to find higher-order tissue structures. Spatial clustering on ST data has become a standard first step for any downstream analysis, such as tissue anatomy visualization, finding domain-dependent biomarkers, and identifying domain-specific molecular regulatory networks [9, 10, 11, 12]. As the amount of ST data rapidly expands, with technologies such as Visium [13], seqFISH+ [14], and MERFISH [15] becoming more accessible, there is a growing need for advanced spatial clustering methods that can handle this kind of complex data. Current ST procedures also contain a high-resolution histology image. While it has been proven in prior studies that histology image features and gene expression are closely linked [16, 17], most current analytical methods do not fully integrate spatial information and histology image with gene expression data. This limitation obstructs the clustering models from extracting critical information from the histology images, such as cell-cell interactions and spatial changes in cell states. Given these challenges, finding patterns in gene expression profiles with spatial and image context remains one of the significant challenges in spatial transcriptomics analysis. To address this, we propose a novel contrastive learning-based deep learning model named the spatial transcriptomics multi-modal clustering (stMMC) model. Our model integrates both gene expression and histology image features through a parallel graph autoencoder, leveraging contrastive learning to regulate feature extraction for each modality. This approach aims to provide a more accurate and comprehensive analysis of spatial transcriptomics data. Figure 1: The overall structure of the proposed model, stMMC is plotted here, where trapezoids represent the GCN layer, and rectangles represent extracted features. Dashed lines with double arrowheads represent that both GCNs share the same weight. stMMC takes two data modalities and passes them through the multi-modal parallel graph autoencoder (MPGA), where each modality is regulated by a contrastive learning mechanism. The detailed process of contrastive learning is shown in Figure 2. The MPGA reconstructs a refined gene expression data, which is then used for spatial clustering. Our contributions of this study are: i) we propose stMMC, a novel multi-modal contrastive learning-based clustering method using high-resolution histology images for spatial clustering in spatial transcriptomics data; ii) we demonstrate that combining gene expression data and learned histology image features improves the spatial clustering performance and paves a new way for future research; iii) we conduct experiments to show the proposed method achieves superior performance on benchmark datasets. Figure 2: The detailed process of contrastive learning mechanism for a random spot on any modality is plotted here, where the top row is the corrupted graph, the bottom row is the original graph, and there is three steps of the contrastive learning mechanism: 1) obtaining the learned spot feature by GCN; 2) computing the original local community representation and the corrupted one; 3) assigning positive pairs to the learned features and the community representation from the same graph, and negative pair to the learned feature and the community representation from different graphs. The positive pair is shown in blue and negative pair is shown in red."
https://arxiv.org/html/2411.02524v1,3DM-Explore: 3D Mapping for Multi-Robot Exploration for Unknown Indoor Environments,"Multi-Robot Exploration (MRE) is pivotal in advancing robotic autonomy, particularly for applications demanding sustained situational awareness such as , search and rescue, and intelligent transportation systems. Recent developments have focused on enhancing the efficiency and resilience of multi-robot SLAM algorithms, incorporating diverse sensor modalities and robust communication frameworks. While significant progress has been made with lidar-based collaborative SLAMs and large-scale visual SLAM systems, the research on visual SLAM in indoor environments has been comparatively limited. This gap is critical, as the dynamic interactions among robots in such settings present unique challenges. State-of-the-art approaches like Kimera-Multi and CORB-SLAM have demonstrated effectiveness in outdoor environments but struggle indoors, often leading to issues such as the ”ghosting trail effect” that compromise map quality. Our research proposes a fully distributed approach to multi-robot exploration in indoor settings, addressing the limitations of existing methods. By focusing on generating dense metric 3D mesh models and accounting for the complex geometric structures of indoor environments, we aim to improve mapping efficiency while adhering to strict communication bandwidth constraints, thereby advancing the field of collaborative visual SLAM in challenging indoor scenarios.","Multi-Robot Exploration (MRE) plays a crucial role in robotics research because it enhances situational awareness over extended periods. This capability is essential for a wide range of applications, including environmental monitoring and patrolling [espina2011multi], search and rescue operations [10611179], intelligent transportation systems [li2018corb]. The objective of MRE is to synergistically enhance the autonomous navigation and mapping capabilities of a coordinated robotic system, optimizing spatial comprehension, cost-effectiveness, travel time, and energy utilization. Recently, there have been substantial advancements in developing more efficient and resilient MRE algorithms and systems, incorporating various objectives, sensor modalities and communication frameworks. Frontier-Based exploration methods have garnered substantial attention for their capacity to accelerate exploration processes. However, the objectives of recent works are primarily focused on creating efficient 2D maps, optimizing cost, and minimizing travel time. Their suitability for 3D exploration remains inadequate for achieving notable performance enhancements. For instance, most of the existing literature relies on identifying frontiers using computer vision based approaches on grid maps. These approaches, while effective in 2D contexts, does not translate well to constructing efficient 3D maps, highlighting a critical gap in current methodologies. How we calculate the frontiers. In comparison to the experiments of lidar-based collaborative exploration and large-scale visual exploration frameworks, the evaluative research on visual MRE within indoor environments remains proportionally lacking. This shortfall is particularly significant given that the intrinsic dynamism of robot interactions within these settings presents unique challenges that must be addressed. The existing visual MRE state-of-the-art appraoches such as Kimera-Multi [tian2022kimera], CORB-SLAM [li2018corb] proven thier efficiency towards outdoor environments, where inter-robot visibility to each other is neglected. However, these methodologies often falter in indoor scenarios, leading to a ”ghosting trail effect” that significantly degrades the quality of the generated maps. Distinct from previous studies on vision-based MRE systems, our research emphasizes a fully distributed approach to multi-robot exploration. We enhance the efficiency of generating dense metric 3D mesh models by incorporating ”Mutual-Awareness” within environments in which the robots operate, all while operating under uncertain constraints on communication bandwidth. Overall, this paper proposes 3DM-Explore integrating exploration algo to enhance the efficacy of 3D exploration and localization within indoor scenarios. data efficiency and communication aspects. Contributions: Novelty:"
https://arxiv.org/html/2411.02482v1,NeRF-Aug: Data Augmentation for Roboticswith Neural Radiance Fields,"Training a policy that can generalize to unknown objects is a long standing challenge within the field of robotics. The performance of a policy often drops significantly in situations where an object in the scene was not seen during training. To solve this problem, we present NeRF-Aug, a novel method that is capable of teaching a policy to interact with objects that are not present in the dataset. This approach differs from existing approaches by leveraging the speed and photorealism of a neural radiance field for augmentation. NeRF-Aug both creates more photorealistic data and runs 3.83 times faster than existing methods. We demonstrate the effectiveness of our method on 4 tasks with 11 novel objects that have no expert demonstration data. We achieve an average 69.1% success rate increase over existing methods. See video results at https://nerf-aug.github.io.","Humans have an innate ability to interact with objects they have never encountered before. For instance, a person can intuitively approach an unknown object, pick it up, and interact with it. This is in stark contrast to existing robotic systems. For a robot, even the slightest differences in shape or color from the objects seen during training can prevent the robot from achieving success. This challenge of generalization to out-of-distribution samples is a fundamental issue in machine learning and robotics. Many prior works have explored methods to develop policies for robots that generalize to different objects. A straightforward approach is to simply collect demonstrations involving the novel object. However, this method has significant drawbacks because creating expert demonstrations is time-consuming and expensive as it requires a human to consciously control the robot’s movements. Collecting such human demonstrations is unfeasible at scale as every new object potentially requires many new demonstrations. Another approach is to use image editing tools, e.g., the latest diffusion-based image editing [1, 2, 3, 4, 5]. While these models can effectively edit images to insert new objects, they are often slow and struggle to render the exact object that will be encountered by the robot. This inaccuracy means the current object remains out of the domain of the training set which often causes these models to fail. Alternatively, some pipelines use depth images for object manipulation [6, 7]. Unfortunately, depth images in the real world suffer from noise and incompleteness [7]. This issue is exacerbated when using mounted gripper cameras, which amplify noise as they get closer to the object. Moreover, even though depth images disregard texture and color, small geometric differences between the original (training) and novel objects can still result in confusion and failure to complete the intended task. Figure 1: When a human provides expert demonstrations for training a behavior cloning model, the model is effective for the object in the demonstration, but will fail for novel objects. We propose NeRF-Aug, where we automatically learn NeRFs for the novel objects and inpaint them in the expert data. With this photorealistic synthetic data, the robot can learn to interact successfully with novel objects. In this work, we propose NeRF-Aug, a lightweight framework that streamlines and automates data collection for a wide range of novel objects. Our goal is to generate data samples for different tasks using novel objects without collecting more human demonstrations. To enable this, we follow the image editing paradigm, but instead of relying on slow generation frameworks, we propose grounding the editing process in the scene using the 3D model of a novel object with a Neural Radiance Field (NeRF) [8] representation. We augment the training data for the robot’s policy using this edited scene. Our framework uses existing demonstrations of a different object and generates NeRF-Augmented (NeRF-Aug) synthetic data (Fig. 2) that can be used in imitation learning policies. Figure 2: An illustration of our pipeline from beginning to end. We first train an object-level NeRF of a novel object (left). We then simultaneously erase the object in question with an off-the-shelf inpainter (top) and leverage NeRF to render images of a new object in the same position as the original object (bottom). We use the final synthetic dataset to train a new policy for the robot (right). We demonstrate that the synthetic data generated by the NeRF-Aug framework is almost indistinguishable from real-world data. Moreover, we show that compared to existing diffusion-based image editing techniques, our method runs significantly faster, creates photorealistic images, and can consistently render objects at a wider degree of viewpoints. We test our method on a variety of real world tasks, and achieve a 69.1% increase in success rate while rendering synthetic data 3.833.833.833.83 times faster than the baselines. To summarize, our contributions are as follows: • We propose a fast and photorealistic image editing framework to generate synthetic data that can be used in robot policy learning to generalize to novel objects. • We learn a NeRF of a novel object by using multi-view images of the object captured using a robot arm. • We edit videos of existing demonstrations by removing the training object via in-painting and blending the NeRF render of a novel object into the inpainted image to generate a synthetic dataset for training the robot policy. • We demonstrate effective generalization on four diverse tasks using the generated synthetic data for training."
https://arxiv.org/html/2411.02430v1,Generative Emotion Cause Explanation in Multimodal Conversations,"Multimodal conversation, a crucial form of human communication, carries rich emotional content, making the exploration of the causes of emotions within it a research endeavor of significant importance. However, existing research on the causes of emotions typically uses clause selection methods to locate the reason utterance, without providing a detailed explanation of the emotional causes. In this paper, we propose a new task, Multimodal Conversation Emotion Cause Explanation (MCECE), aiming to generate a detailed explanation of the emotional cause to the target utterance within a multimodal conversation scenario. Building upon the MELD dataset, we develop a new dataset (ECEM) that integrates video clips with detailed explanations of character emotions, facilitating an in-depth examination of the causal factors behind emotional expressions in multimodal conversations.A novel approach, FAME-Net, is further proposed, that harnesses the power of Large Language Models (LLMs) to analyze visual data and accurately interpret the emotions conveyed through facial expressions in videos. By exploiting the contagion effect of facial emotions, FAME-Net effectively captures the emotional causes of individuals engaged in conversations. Our experimental results on the newly constructed dataset show that FAME-Net significantly outperforms several excellent large language model baselines. Code and dataset are available at https://github.com/3222345200/ECEMdataset.git","In social life, multimodal conversation is crucial as it allows for a rich expression of emotions. Research Woo et al. (2020); Agnew and South (2014); Costa et al. (2013) indicates that exploring the causes of emotions is important in everyday life, as it can aid in promoting self-understanding. In recent years, researchers increasingly focus on how to explore the underlying causes of emotions. Consequently, a new task called Causal Emotion Entailment (CEE) Xia et al. (2019); Zheng et al. (2022) is proposed, along with the introduction of a benchmark dataset named RECCON Poria et al. (2021). The CEE task aims to identify the specific utterances that causally trigger the emotion expressed in the target utterance. Wang et al. further propose a new task, Multimodal Emotion-Cause Pair Extraction in Conversations (MECPE), which expands the research scope to multimodal content. However, the identification of emotional causes still relies on selective cause extraction rather than cause generation. Selective causes fail to consider emotional expressions that lack direct causes in conversations and require contextual reasoning for understanding; for those emotional utterances, if the causal relationships behind them are primarily conveyed through visual or auditory information, determining the specific causes that trigger emotions becomes quite challenging. The comparison between selective and generative is illustrated in Figure 1. To resolve these issues, we propose a novel task, called Multimodal Emotion Cause Explanation in Conversation (MECEC), which aims to uncover the underlying causes of emotions expressed in a target utterance within a multimodal conversation setting, encompassing text and visual modalities. We build an Emotion Cause Explanation (ECEM) based on the MELD dataset Poria et al. (2018) for MECEC. The ECEM dataset consists of 7,273 video clips, incorporating audio, visual and textual modalities, with each clip annotated with a thorough explanation of the emotional cause. To address the task of MECEC, we propose a new model, Facial-Aware Multimodal Emotion Explanation Network (FAME-Net), which is based on LLaVA Liu et al. (2023). FAME-Net integrates visual and facial emotion recognition to comprehensively understand multimodal emotional data, compensating for the limitations of text-only analysis and revealing deeper aspects of emotional contagion and hidden emotions. Facial expression recognition can assist in the task of discovering emotional causes; therefore, the model introduces a dedicated facial emotion recognition branch, designed as a two-stage pipeline. In the first stage, it performs scene and face detection to segment and track faces within videos; in the second stage, it employs multi-scale feature networks and global depth convolution techniques to accurately identify facial emotion features. Compared to other open-source LLMs, our model excels in text generation metrics, achieving the best results. Our main contributions are as follows: • We introduce a novel task named Multimodal Emotion Cause Explanation in Conversations (MECEC) along with a new dataset, Emotion Cause Explanation based on the MELD dataset (ECEM), which provides detailed natural language explanations for the emotions of target utterances and is specifically designed for training and evaluating the MECEC task. • We propose FAME-Net, which integrates visual modality and facial emotion recognition to achieve comprehensive analysis of emotional causes. Through a two-stage pipeline, it accurately extracts emotional features, enhancing the performance of multi-modal emotion causes explain and generation. • Experimental results on our annotated dataset indicate that our proposed model, FAME-Net, outperforms several existing open-source baseline models in the MECEC task."
https://arxiv.org/html/2411.02397v2,Adaptive Caching for Faster Video Generationwith Diffusion Transformers,"Generating temporally-consistent high-fidelity videos can be computationally expensive, especially over longer temporal spans. More-recent Diffusion Transformers (DiTs)— despite making significant headway in this context— have only heightened such challenges as they rely on larger models and heavier attention mechanisms, resulting in slower inference speeds. In this paper, we introduce a training-free method to accelerate video DiTs, termed Adaptive Caching (AdaCache), which is motivated by the fact that “not all videos are created equal”: meaning, some videos require fewer denoising steps to attain a reasonable quality than others. Building on this, we not only cache computations through the diffusion process, but also devise a caching schedule tailored to each video generation, maximizing the quality-latency trade-off. We further introduce a Motion Regularization (MoReg) scheme to utilize video information within AdaCache, essentially controlling the compute allocation based on motion content. Altogether, our plug-and-play contributions grant significant inference speedups (e.g. up to 4.7×\times× on Open-Sora 720p - 2s video generation) without sacrificing the generation quality, across multiple video DiT baselines.","Diffusion models (Ho et al., 2020; Song et al., 2020) have become the standard for generative modeling in recent years, arguably surpassing the quality of VAEs (Kingma, 2013; Rolfe, 2016), GANs (Karras et al., 2019; Goodfellow et al., 2020) and Auto-Regressive models (Chang et al., 2022, 2023). This observation holds in a wide-range of applications including image (Rombach et al., 2022; Saharia et al., 2022), video (Singer et al., 2022; Blattmann et al., 2023a), 3D (Poole et al., 2022; Liu et al., 2023a), and audio (Kong et al., 2020; Huang et al., 2023) generation, as well as image (Hertz et al., 2022; Avrahami et al., 2023) and video (Qi et al., 2023; Wu et al., 2023) editing. More recent Diffusion Transformers (DiTs) (Peebles and Xie, 2023; Ma et al., 2024a) show better promise in terms of scalability and generalization compared to prior UNet-based diffusion models (Rombach et al., 2022), revealing intriguing horizons in GenAI for the years to come. Despite the state-of-the-art performance, DiTs can also be computationally expensive both in terms of memory and computational requirements. This becomes especially critical when applied with a large number of input tokens (e.g. high-resolution long video generation). For instance, the reason for models such as Sora (OpenAI, 2024) not being publicly-served is speculated to be the high resource demands and slower inference speeds (Liu et al., 2024b). To tackle these challenges and reduce the footprint of diffusion models, various research directions have emerged such as latent diffusion (Rombach et al., 2022), step-distillation (Sauer et al., 2023; Yin et al., 2024), caching (Wimbauer et al., 2024; Ma et al., 2024c; Habibian et al., 2024), architecture-search (Zhao et al., 2023b; Li et al., 2024b), token reduction (Bolya and Hoffman, 2023; Li et al., 2024a) and region-based methods (Nitzan et al., 2024; Kahatapitiya et al., 2024). Fewer techniques transfer readily from UNet-based pipelines to DiTs, whereas others often require novel formulations. Hence, DiT acceleration has been under-explored as of yet. Figure 1: Effectiveness of Adaptive Caching: We show a qualitative comparison of AdaCache (right) applied on top of Open-Sora (Zheng et al., 2024) (left), a baseline video DiT. Here, we consider generating 720p - 2s video clips, and report VBench (Huang et al., 2024) quality and average latency (on a single A100 GPU) on the benchmark prompts from Open-Sora gallery. AdaCache generates videos significantly faster (i.e., 4.7×\times× speedup) with a comparable quality. Also, the number of computed steps varies for each video. Best-viewed with zoom-in. Prompts given in supplementary. Moreover, we note that not all videos are created equal. Some videos contain high-frequency textures and significant motion content, whereas others are much simpler (e.g. with homogeneous textures or static regions). Having a diffusion process tailored specifically for each video generation can be beneficial in terms of realizing the best quality-latency trade-off. This idea has been explored to some extent in region-based methods (Avrahami et al., 2023; Nitzan et al., 2024; Kahatapitiya et al., 2024), but not sufficiently in the context of video generation. Motivated by the above, we introduce Adaptive Caching (AdaCache) for accelerating video diffusion transformers. This approach requires no training and can seamlessly be integrated into a baseline video DiT at inference, as a plug-and-play component. The core idea of our proposal is to cache residual computations within transformer blocks (e.g. attention or MLP outputs) in a certain diffusion step, and reuse them through a number of subsequent steps that is dependent on the generated video. We do this by devising a caching schedule, i.e., deciding when-to-recompute-next whenever making a residual computation. This decision is guided by a distance metric that measures the rate-of-change between previously-stored and current representations. If the distance is high we would not cache for an extended period (i.e., #steps), to avoid reusing incompatible representations. We further introduce a Motion Regularization (MoReg) to allocate computations based on the motion content in the video being generated. This is inspired by the observation that high-moving sequences require more diffusion steps to achieve a reasonable quality. Altogether, our pipeline is applied on top of multiple video DiT baselines showing much-faster inference speeds without sacrificing the generation quality (see Fig. 1). Finally, we validate the effectiveness of our contributions and justify our design decisions through ablations and qualitative comparisons."
https://arxiv.org/html/2411.02395v1,Training-free Regional Promptingfor Diffusion Transformers,"Diffusion models have demonstrated excellent capabilities in text-to-image generation. Their semantic understanding (i.e., prompt following) ability has also been greatly improved with large language models (e.g., T5, Llama). However, existing models cannot perfectly handle long and complex text prompts, especially when the text prompts contain various objects with numerous attributes and interrelated spatial relationships. While many regional prompting methods have been proposed for UNet-based models (SD1.5, SDXL), but there are still no implementations based on the recent Diffusion Transformer (DiT) architecture, such as SD3 and FLUX.1. In this report, we propose and implement regional prompting for FLUX.1 based on attention manipulation, which enables DiT with fined-grained compositional text-to-image generation capability in a training-free manner. Code is available at https://github.com/antonioo-c/Regional-Prompting-FLUX.","Text-to-image models have been evolving over the past few years and have made great progress thanks to the emergence of diffusion models [1, 2, 3, 4, 5, 6, 7]. These models come with superior visual quality, capability of generating diverse styles and promising prompt adherence given short or descriptive prompts. However, despite these remarkable advances, they still face challenges in accurately processing prompts with complex spatial layouts [8]. On the one hand, it is very difficult to describe the exact spatial layout through natural language, especially when the number of objects increases or precise position control is required. For example, we rely on external conditional control (such as ControlNet [9]) to generate precise poses instead of describing the movements of the hand. On the other hand, although the ability of prompt adherence has been improved with the advancement of the model [4, 3, 10], when dealing with complex elements and relationships in long texts, the model still has drift problems such as confusing concepts and missing elements. Therefore, explicit spatial control is still necessary for compositional generation by now. To tackle these challenges, people have made different attempts [4, 3, 11, 10, 12]. With the base model, research shows that the prompt following ability of text-to-image generation depends largely on the representation ability of the text encoder; that is, a larger and stronger text model can significantly enhance the prompt following ability. For example, Stable Diffusion 3/3.5 [3] and FLUX.1 [4] additionally introduce T5-XXL [13] as one of the text encoders besides of the coarse-align CLIP [14] encoder, allowing the model to have the ability to render visual text. The Playground-3.0 [12] model further replaces the text encoder with a large language model (Lllma3 [15]) and uses the representation of the intermediate layer instead of the global pooling representation to achieve stronger text understanding capabilities. Clearly, these advances in base models improve overall semantic understanding but still fall short of precise compositional generation to meet user needs. In addition to improving the base model, some recent studies [16, 8, 17, 18, 19] have proposed to handle compositional control by providing spatial conditions (layout/box) and training a control module as a plugin on top of the base model, or to manipulate the attention score map using region masks in a training-free manner. For example, InstanceDiffusion [16] adds precise instance-level control via learnable UniFusion blocks to handle the additional per-instance conditioning. RPG [8] employs the Multi-modal Large Language Model (MLLM) [20] as a global planner to decompose the process of generating complex images into multiple simpler generation tasks within subregions, and proposes complementary regional diffusion to enable region-wise compositional generation. DenseDiffusion [18] and Omost [19] develop an attention modulation method that guides objects to appear in specific regions according to layout guidance. In this report, we are mainly inspired by Omost, but work on one of the recent diffusion transformer based models, FLUX.1-dev, which differs from previous base models mainly in its design of MMDiT structure where the prompt representation updates dynamically. We investigate training-free attention manipulation for this structure, so that it is not tied to a specific model and can be easily applied to models with similar designs. The code will be released and we hope the community can enjoy."
https://arxiv.org/html/2411.02394v1,AutoVFX: Physically Realistic Video Editing from Natural Language Instructions,"Modern visual effects (VFX) software has made it possible for skilled artists to create imagery of virtually anything. However, the creation process remains laborious, complex, and largely inaccessible to everyday users. In this work, we present AutoVFX, a framework that automatically creates realistic and dynamic VFX videos from a single video and natural language instructions. By carefully integrating neural scene modeling, LLM-based code generation, and physical simulation, AutoVFX is able to provide physically-grounded, photorealistic editing effects that can be controlled directly using natural language instructions. We conduct extensive experiments to validate AutoVFX’s efficacy across a diverse spectrum of videos and instructions. Quantitative and qualitative results suggest that AutoVFX outperforms all competing methods by a large margin in generative quality, instruction alignment, editing versatility, and physical plausibility.","Visual effects (VFX) combine realistic video footage with computer-generated imagery to create novel, photorealistic visuals. Recent advances in graphics, vision, and physical simulation have made it possible to produce VFX that depict virtually anything—even those that are too costly, time-consuming, dangerous, or impossible to capture in real life. As a result, VFX have become essential in modern filmmaking, ads, simulation, AR/VR, etc. However, the process remains laborious, complex, and expensive, requiring expert skills and professional software [18, 44, 17, 3], making it largely inaccessible to everyday users. A promising approach to democratizing VFX is to treat it as a generative video editing problem, where raw video and language prompts are used to generate new videos reflecting the original content and given instructions [4, 94, 69, 19, 32, 10, 92, 50, 64, 103, 65]. This method leverages advances in generative modeling, learning from large-scale internet data to produce controllable video. Successes have been seen in deepfake videos, fashion, driving, and robotics [15, 31, 87, 52, 102]. However, this purely data-driven generative editing approach hasn’t yet replaced traditional VFX pipelines due to challenges in achieving guaranteed physical plausibility, precise 3D-aware control, and various special effects. Another appealing alternative is to build a 3D representation from video input, apply edits like object insertion or texture changes, and then render the final output [53, 71, 36, 22, 63, 114, 113, 13, 25, 12, 100, 26, 56, 83, 57, 55]. While this approach aligns well with the VFX pipeline, it is often limited in editing capabilities and still requires manual interaction with cumbersome interfaces, making it difficult for everyday users. Bridging this gap is essential to make 3D scene editing capable of handling most visual effects while remaining accessible to everyone. In this work, we present AutoVFX, a framework that automatically creates realistic and dynamic VFX videos from a single video and natural language instructions. At the core of our method is a novel integration of neural scene modeling, LLM-based code generation, and physical simulation. First, we establish a holistic scene model that encodes rich geometry, appearance, and semantics from the input video. This model serves as the foundation for a variety of scene editing, simulation, and rendering capabilities, which we organize into a collection of executable functions. Next, AutoVFX takes simple language editing instructions and converts them into programs using large language models (LLMs). These programs consist of a sequence of calls to our predefined functions. Finally, the generated code is executed, producing a free-viewpoint video that reflects the instructed changes. Fig. 2 illustrates the overall framework. AutoVFX combines the strengths of generative editing and physical simulation, yet is uniquely set apart from both. Like traditional VFX, AutoVFX produces videos with physics-grounded, controllable, and photorealistic effects. At the same time, similar to generative editing, we support open-world natural language instructions, allowing anyone to edit a video by simply describing the desired effects. We conduct extensive experiments to validate AutoVFX’s efficacy across a diverse spectrum of videos and instructions. We also perform user studies and qualitative and quantitative comparisons with existing video and scene editing methods. Experimental results suggest AutoVFX outperforms all competing methods by a large margin in generative quality, instruction alignment, editing versatility, and physical plausibility. This demonstrates the effectiveness and convenience of our approach, highlighting its potential as a valuable framework to democratize VFX and pave the way for future integration of even more capabilities to further enhance realism in automatic VFX. Table 1: Comparison of existing and proposed methods for visual editing. Generative editing models lack physical plausibility and precise controllability. Existing physics-based editing methods have complicated interfaces and are limited in their range of editing capacities. Our method, AutoVFX, enjoys a convenient natural language interface while providing the widest range of capabilities. Method Input & Output Editing Capacities Real World Video Editing Free-Viewpoint Rendering Editing Interface Open-world Query Object Insertion Object Removal Object Rearrange Appearance Change Animated Objects Physics Simulation Particle Effects Visual Programming [35] ✓ × Natural Language ✓ ✓ ✓ ✓ ✓ × × × FRESCO [103] ✓ × Natural Language ✓ × × × ✓ × × × ClimateNeRF [53] ✓ ✓ Predefined Scripts × × × × ✓ × × ✓ Feature Splatting [73] ✓ ✓ Predefined Scripts ✓ ✓ ✓ ✓ ✓ ✓ ✓ × GaussianEditor [13] ✓ ✓ Graphical ✓ ✓ ✓ × ✓ × × × Gaussian Grouping [105] ✓ ✓ Graphical ✓ ✓ ✓ ✓ ✓ × × × PhysGaussian [98] ✓ ✓ Graphical × × × × × × ✓ × VR-GS [47] ✓ ✓ Graphical × ✓ ✓ ✓ × × ✓ × Gaussian Splashing [28] ✓ ✓ Graphical × ✓ ✓ ✓ × × ✓ ✓ DMRF [71] ✓ ✓ Graphical × ✓ × × × ✓ ✓ ✓ Instruct-N2N [36] ✓ ✓ Natural Language ✓ × × × ✓ × × × DGE [12] ✓ ✓ Natural Language ✓ × × × ✓ × × × Chat-Edit-3D [26] ✓ ✓ Natural Language ✓ ✓ ✓ ✓ ✓ × × × ChatSim [90] ✓ ✓ Natural Language × ✓ ✓ ✓ ✓ ✓ × × AutoVFX(Ours) ✓ ✓ Natural Language ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓"
https://arxiv.org/html/2411.02393v1,Adaptive Length Image Tokenization viaRecurrent Allocation,"Current vision systems typically assign fixed-length representations to images, regardless of the information content. This contrasts with human intelligence —and even large language models—which allocate varying representational capacities based on entropy, context and familiarity. Inspired by this, we propose an approach to learn variable-length token representations for 2D images. Our encoder-decoder architecture recursively processes 2D image tokens, distilling them into 1D latent tokens over multiple iterations of recurrent rollouts. Each iteration refines the 2D tokens, updates the existing 1D latent tokens, and adaptively increases representational capacity by adding new tokens. This enables compression of images into a variable number of tokens, ranging from 32 to 256. We validate our tokenizer using reconstruction loss and FID metrics, demonstrating that token count aligns with image entropy, familiarity and downstream task requirements. Recurrent token processing with increasing representational capacity in each iteration shows signs of token specialization, revealing potential for object / part discovery. Code available at https://github.com/ShivamDuggal4/adaptive-length-tokenizer.","Representation learning (Bengio et al., 2013), which involves extracting meaningful and useful information from input observations, is crucial for decision-making. An effective representation should be compact while encoding all relevant information. However, what constitutes “relevant” information varies based on the specific task; for example, a coarse classification task may require a different latent representation compression factor for satisfactory performance compared to a task demanding perfect pixel-level reconstruction, which necessitates denser representations. This notion of a useful representation aligns closely with aspects of human intelligence (Legg & Hutter, 2007), particularly the concept of adaptive and variable-compressible representations (Hutter, 2006). Similarly, language models can describe content at various levels of abstraction depending on complexity, context (Graves, 2016; Dehghani et al., 2018), and familiarity (Baevski & Auli, 2018). In contrast, most current visual systems, such as VAEs, VQGANs, and ViTs (Kingma & Welling, 2022; Esser et al., 2020; Dosovitskiy et al., 2020), generate fixed-size representations for all images. In this work, we take a step toward learning adaptive and variable-length visual representations, emphasizing that each image requires a different representation capacity (see Sec. 4). Figure 1: Adaptive Length Image Tokenization maps an image to multiple variable-length representations through a recurrent token allocation process, enabling task-specific sampling. We learn the tokenizer via image reconstruction as a self-supervised objective. While a compressed representation can be optimized for specific tasks (e.g., fewer tokens for “dog”, “leaf”, “grass” may suffice for a VLM task), reconstruction objective supports learning a universal, task-agnostic tokenizer. A common framework for learning image embeddings or representations is the encoder-decoder approach, where an encoder compresses input data into a compact latent representation, which can later be decoded and compared with the original image as a learning objective. While there are other encoder-only methods, such as contrastive learning (Chen et al., 2021) and self-distillation (Caron et al., 2021), we focus on encoder-decoder approaches because a reconstruction objective intuitively promotes the learning of adaptive representations by capturing varying level-of-details necessary for better reconstruction. The current state-of-the-art (transformer-based) encoder-decoder approaches (Dosovitskiy et al., 2020) operate in the discrete token space, by encoding images into learned tokens and then decoding them back to image pixels. To generate these tokens, these approaches compress (slightly) at the input patch-level and then maintain the number of tokens (=== number of patches) throughout the encoder-decoder network depth. Thus, the representation length for all images is fixed to the number of tokens, equivalent to the fixed patch-size decided by the human-engineer. Moreover, by having number of tokens equal to number of patches, such approaches are tied to the natural 2D inductive bias of images, preventing any form of adaptive representation or compression of different images. Moving away from this inductive bias and with the goal of having modality-agnostic architecture, Google DeepMind proposed Perceiver (Jaegle et al., 2021b; a), a transformer-based architecture which distills input data tokens to a set of fixed 1D tokens. This process of latent-token distillation refers to compressing a higher-dimensional input (e.g., 2D image tokens) into a more compact set of latent variables (1D tokens), capturing the most relevant features. Like Perceiver, we also fall into the category of latent-token distillation, where we encode 2D image tokens into much fewer 1D latent tokens via a self-supervised reconstruction objective. While 1D-tokenization of an image overcomes the patch to token constraint and allows much more efficient compression of the input image, a more universal tokenizer would be one which adaptively assigns variable tokens to each input based on content entropy, familiarity etc (Sec. 4). Figure 2: Adaptive Length Image Tokenizer (ALIT): Given an image, we first convert it into 2D image tokens before applying the 2D →→\rightarrow→ 1D latent distillation. ALIT recurrently distills 2D image tokens into variable 1D latent tokens, with each iteration adding new latent tokens and processing them with the existing 2D image tokens and the old latent tokens. Training focuses on reconstructing 2D image tokens through reverse distillation from latent 1D to masked 2D tokens. Based on token-reconstruction quality, we can optionally mask specific 2D tokens from further processing, enabling dynamic halting per token. Recurrent processing with Adaptive Memory leads to compressible representations, flexible tokenization & specialized tokens focusing on objects/parts. We tackle the challenge of adaptive or variable-length representation learning by auto-regressively distilling input visual observations into an increasing number of 1D latent tokens. To achieve this, we draw inspiration from foundational works on recurrent computation (Graves, 2016; Dehghani et al., 2018). Recurrent neural networks are often viewed as adaptive thinking modules (Schwarzschild et al., 2021), capable of enhancing the computational requirements of a specific input through recursive processing with the same neural network architecture. Thus, unlike the Matryoshka style (Kusupati et al., 2022) approach of learning multiple representations of varying lengths simultaneously in one-go, we adopt a recurrent computing approach for visual representation learning. In our framework, recurrent computing involves recursively distilling an input image or 2D image tokens into 1D latent tokens through a shared encoder-decoder architecture until each image token has been sufficiently processed/distilled into the latent tokens. At each iteration of this recurrent rollout, we provide additional computational resources in the form of new learnable latent tokens, enabling the model to learn adaptive and variable-length representations across different iterations. We refer to our approach as ALIT (Adaptive Length Image Tokenizer), and train it using self-supervised image reconstruction objective. Credited to the increasing representational capacity, each recurrent update leads to the latent tokens specializing and attending to localized regions, hinting at object / part discovery (see, Fig. 8, Fig. 9, Appendix Fig. 13 and Fig. 16). We validate the effectiveness of the learned tokenizer by demonstrating comparable reconstruction metrics (L1 loss and FID) and linear probing results on ImageNet-1K, relative to the 2D VQGAN tokenizer (Esser et al., 2020) and the fixed-latent 1D tokenizer, Titok (Yu et al., 2024), while also allowing for flexible token counts per image. By utilizing variable representations per image and introducing cumulative dataset representations, we emphasize key aspects of effective representations: the required capacity aligns with image’s information entropy, familiarity, and knowledge of downstream tasks / models."
https://arxiv.org/html/2411.02385v1,How Far Is Video Generation from World Model: A Physical Law Perspective,"OpenAI’s Sora highlights the potential of video generation for developing world models that adhere to fundamental physical laws. However, the ability of video generation models to discover such laws purely from visual data without human priors can be questioned. A world model learning the true law should give predictions robust to nuances and correctly extrapolate on unseen scenarios. In this work, we evaluate across three key scenarios: in-distribution, out-of-distribution, and combinatorial generalization. We developed a 2D simulation testbed for object movement and collisions to generate videos deterministically governed by one or more classical mechanics laws. This provides an unlimited supply of data for large-scale experimentation and enables quantitative evaluation of whether the generated videos adhere to physical laws. We trained diffusion-based video generation models to predict object movements based on initial frames. Our scaling experiments show perfect generalization within the distribution, measurable scaling behavior for combinatorial generalization, but failure in out-of-distribution scenarios. Further experiments reveal two key insights about the generalization mechanisms of these models: (1) the models fail to abstract general physical rules and instead exhibit “case-based” generalization behavior, i.e., mimicking the closest training example; (2) when generalizing to new cases, models are observed to prioritize different factors when referencing training data: color >>> size >>> velocity >>> shape. Our study suggests that scaling alone is insufficient for video generation models to uncover fundamental physical laws, despite its role in Sora’s broader success.","Foundation models (Bommasani et al., 2021) have emerged remarkable capabilities by scaling the model and data to an unprecedented scale (Brown, 2020; Kaplan et al., 2020). As an example, OpenAI’s Sora (Brooks et al., 2024) not only generates high-fidelity and surreal videos, but also has sparked a new surge of interest in studying world models (Yang et al., 2023). “Scaling video generation models is a promising path towards building general purpose simulators of the physical world.” — Sora Report (Brooks et al., 2024) World simulators are receiving broad attention from robotics (Yang et al., 2023) and autonomous driving (Hu et al., 2023) for the ability to generate realistic data and accurate simulations. These models are required to comprehend fundamental physical laws to produce data that extends beyond the training corpus and to guarantee precise simulation. However, it remains an open question whether video generation can discover such rules merely by observing videos, as Sora does. We aim to provide a systematic study to understand the critical role and limitation of scaling in physical law discovery. It is challenging to determine whether a video model has learned a law instead of merely memorizing the data. Since the model’s internal knowledge is inaccessible, we can only infer the model’s understanding by examining its predictions on unseen scenarios, i.e., its generalization ability. We propose a categorization (Figure 1) for comprehensive evaluation based on the relationship between training and testing data in this paper. In-distribution (ID) generalization assumes that training and testing data are independent and identically distributed (i.i.d.). Out-of-distribution (OOD) generalization, on the other hand, refers to the model’s performance on testing data that come from a different distribution than the training data, particularly when latent parameters fall outside the range seen during training. Human-level physical reasoning can easily extrapolate OOD and predict physical processes without encountering the exact same scenario before. Additionally, we also examine a special OOD capacity called combinatorial generalization, which assesses whether a model can combine two distinct concepts in a novel way, a trait often considered essential for foundation models in advancing toward artificial general intelligence (AGI) (Du & Kaelbling, 2024). Moreover, real-world videos typically contain complex, non-rigid objects and motions, which present significant challenges for quantitative evaluation and even human validation. The rich textures and appearances in such videos can act as confounding factors, distracting the model from focusing on the underlying physics. To mitigate these issues, we specifically focus on classical mechanics and develop a 2D simulator with objects represented by simple geometric shapes. Each video depicts the motion or collision of these 2D objects, governed entirely by one or two fundamental physical laws, given the initial frames. This simulator allows us to generate large-scale datasets to support the scaling of video generation models. Additionally, we have developed a tool to infer internal states (e.g., the position and size) of each object in the generated video from pixels. This enables us to establish quantitative evaluation metrics for physical law discovery. Figure 1: Categorization of generalization patterns.○○\ocircle○ denotes training data. ×\times× denotes testing data. We begin by investigating how scaling video generation models affects ID and OOD generalization. We select three fundamental physical laws for simulation: uniform linear motion of a ball, perfectly elastic collision between two balls, and parabolic motion of a ball. We scale the dataset from 30K to 3 million examples and increase the video diffusion model’s parameters from 22M to 310M. Consistently, we observe that the model achieves near-perfect ID generalization across all tasks. However, the OOD generalization error does not improve with increased data and model size, revealing the limitations of scaling video generation models in handling OOD data. For combinatorial generalization, we design an environment that involves multiple objects undergoing free fall and collisions to study their interactions. Every time, four objects from eight are selected to create a video. In total, 70 combinations (C84superscriptsubscript𝐶84C_{8}^{4}italic_C start_POSTSUBSCRIPT 8 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT) are possible. We use 60 of them for training and 10 for testing. We train models by varying the number of training data from 600K to 6M. We manually evaluate the generated test samples by labeling them as “abnormal” if the video looks physically implausible. The results demonstrate that scaling the data substantially reduces the percentage of abnormal cases, from 67%percent6767\%67 % to 10%percent1010\%10 %. This suggests that scaling is critical for improving combinatorial generalization. Our empirical analysis reveals two intriguing properties of the generalization mechanism in video generation models. First, these models can be easily biased by ""deceptive"" examples from the training set, leading them to generalize in a ""case-based"" manner under certain conditions. This phenomenon, also observed in large language models (Hu et al., 2024), describes a model’s tendency to reference similar training cases when solving new tasks. For instance, consider a video model trained on data of a high-speed ball moving in uniform linear motion. If data augmentation is performed by horizontally flipping the videos, thereby introducing reverse-direction motion, the model may generate a scenario where a low-speed ball reverses direction after the initial frames, even though this behavior is not physically correct. Second, we explore how different data attributes compete during the generalization process. For example, if the training data for uniform motion consists of red balls and blue squares, the model may transform a red square into a ball immediately after the conditioning frames. This behavior suggests that the model prioritizes color over shape. Our pairwise analysis reveals the following prioritization hierarchy: color > size > velocity > shape. This ranking could explain why current video generation models often struggle with maintaining object consistency. We hope these findings provide valuable insights for future research in video generation and the development of world models."
https://arxiv.org/html/2411.02372v1,Learning General-Purpose Biomedical Volume Representations using Randomized Synthesis,"Current volumetric biomedical foundation models struggle to generalize as public 3D datasets are small and do not cover the broad diversity of medical procedures, conditions, anatomical regions, and imaging protocols. We address this by creating a representation learning method that instead anticipates strong domain shifts at training time itself. We first propose a data engine that synthesizes highly variable training samples that enable generalization to new biomedical contexts. To then train a single 3D network for any voxel-level task, we develop a contrastive learning method that pretrains the network to be stable against nuisance imaging variation simulated by the data engine, a key inductive bias for generalization. This network’s features can be used as robust representations of input images for downstream tasks and its weights provide a strong, dataset-agnostic initialization for finetuning on new datasets. As a result, we set new standards across both multimodality registration and few-shot segmentation, a first for any 3D biomedical vision model, all without (pre-)training on any existing dataset of real images.","Figure 1: Representations produced by our framework, trained only on synthetic data, are approximately stable across imaging modalities, field-of-views, and poses on real unseen volumes from various datasets. For each anatomical region (rows), we show two example volumes with substantial variation (col. 1) and six arbitrarily selected network output channels (cols. 2–7) that illustrate this stability. These features and network weights can be used for several voxel-level tasks. Biomedical vision models trained on imaging studies with fixed protocols rarely generalize to new populations, medical procedures, and imaging devices. These domain shifts then necessitate clinically infeasible reannotation and retraining cycles, especially for adaptation to new tasks. Further, volumetric annotated biomedical datasets are especially limited in sample size and focused on specific medical procedures, diseases, or scales of anatomy, leading current networks to overfit to a small subset of biomedical tasks. To overcome this data scarcity and heterogeneity, we present a representation learning framework driven by a synthetic data engine. Our approach yields a generalist 3D network that performs well on diverse voxel-level tasks across a range of unseen biomedical contexts. Current biomedical foundation models are trained by aggregating publicly available datasets to cover multiple domains (Butoi et al., 2023; Chen et al., 2024a; Liu et al., 2023a; Ma & Wang, 2023; MH Nguyen et al., 2024; Pachitariu & Stringer, 2022; Xie et al., 2022). However, persistent gaps hinder their widespread adoption. For example, some methods operate only on specific modalities and regions that can be tractably scaled up in sample size, such as chest X-ray (Chen et al., 2024b), and thus cannot learn general representations for most other domains, such as in utero fetal MRI. Others treat 2D slices of volumetric images as independent data points (Butoi et al., 2023; Ma & Wang, 2023), often constructing training sets with high inter-sample correlation yielding models that fail to produce consistent 3D results. Further, existing foundation models almost exclusively focus on segmentation and classification and neglect other key vision tasks such as registration. To our knowledge, no biomedical vision foundation model has been demonstrated for multiple disparate 3D tasks yet. Contributions. This paper makes advances on two fronts. To gain robustness to large domain shifts in downstream deployment, we first propose a biomedically informed data engine whose samples encompass a wide range of appearances and semantics. This engine uses randomly sampled spatial configurations of biomedical shape templates to synthesize images with arbitrary resolutions, appearances, imaging physics, and crucially, minimal influence from any existing biomedical dataset. Unlike training on samples from GANs or diffusion models, which are limited to reproducing only their original training distribution, our engine synthesizes highly diverse samples useful for arbitrarily new biomedical contexts that we do not have training data for. We then develop a contrastive learning framework that uses paired samples from the data engine to pretrain a network for general voxel-level tasks using an inductive bias of approximate stability to nuisance imaging variation that does not change image semantics, a key property for generalization across datasets (Gruver et al., 2022). We experimentally demonstrate that the resulting features and weights enable broad generalization on the key biomedical tasks of 3D registration and segmentation across several diverse datasets. We achieve state-of-the-art unsupervised multimodality image registration by simply using the network’s approximately appearance invariant and pose equivariant representations (Fig. 1) to drive existing registration solvers. The proposed network can also be used as an off-the-shelf dataset-agnostic initialization for finetuning on any voxel-level task. Specifically, we demonstrate strong few-shot segmentation performance in a few-shot setting across highly diverse downstream datasets, thereby removing the need for cumbersome dataset-specific self-supervised pretraining. Our code and trained model weights are available at https://github.com/neel-dey/anatomix."
https://arxiv.org/html/2411.02354v1,Machine learning identification of maternal inflammatory response and histologic choroamnionitis from placental membrane whole slide images,"Introduction: The placenta forms a critical barrier to infection through pregnancy, labor and, delivery. Inflammatory processes in the placenta have short-term, and long-term consequences for offspring health. Digital pathology and machine learning can play an important role in understanding placental inflammation, and there have been very few investigations into methods for predicting and understanding Maternal Inflammatory Response (MIR). This work intends to investigate the potential of using machine learning to understand MIR based on whole slide images (WSI), and establish early benchmarks. Methods: To that end, use Multiple Instance Learning framework with 3 feature extractors: ImageNet-based EfficientNet-v2s, and 2 histopathology foundation models, UNI and Phikon to investigate predictability of MIR stage from histopathology WSIs. We also interpret predictions from these models using the learned attention maps from these models. We also use the MIL framework for predicting white blood cells count (WBC) and maximum fever temperature (Tm⁢a⁢xsubscript𝑇𝑚𝑎𝑥T_{max}italic_T start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT). Results: Attention-based MIL models are able to classify MIR with a balanced accuracy of up to 88.5% with a Cohen’s Kappa (κ𝜅\kappaitalic_κ) of up to 0.772. Furthermore, we found that the pathology foundation models (UNI and Phikon) are both able to achieve higher performance with balanced accuracy of 87.2 % and 88.5 % respectively, and κ𝜅\kappaitalic_κ of 0.751 and 0.772 respectively, compared to ImageNet-based feature extractor (EfficientNet-v2s) which achieves a balanced accuracy of 83.7% and κ𝜅\kappaitalic_κ of 0.724. For WBC and Tm⁢a⁢xsubscript𝑇𝑚𝑎𝑥T_{max}italic_T start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT prediction, we found mild correlation between actual values and those predicted from histopathology WSIs. Discussion: We used MIL framework for predicting MIR stage from WSIs, and compared effectiveness of foundation models as feature extractors, with that of an ImageNet-based model. We further investigated model failure cases and found them to be either edge cases prone to interobserver variability, examples of pathologist’s overreach, or mislabeled due to processing errors.","Maternal Inflammatory Response: Definitions and Pathogenesis Acute Placental Inflammation (API) is categorized into Maternal Inflammatory Response (MIR) or Fetal Inflammatory Response (FIR) depending on the origin of the inflammatory cells. MIR is characterized by extravasation of maternal neutrophils with movement towards the chorionic layer, followed by moving across the amnion and into amniotic space. MIR is divided into 3 stages. These stages are defined according to the Amsterdam criteria [35]. Stage 1 corresponds to subchorionitis, where where maternal neutrophils are limited to the cellular chorion in the membranes and subchorionic fibrin in the chorionic plate. Stage 2, chorioamnionitis is characterized by neutrophilic migration into the fibrous chorion and amniotic connective tissue. Stage 3, chorioamnionitis with amnion necrosis, is characterized by presence of neutrophil karyorrhectic debris, asbcess formation, and thickened basement membranes. MIR is classically seen in the presence of ascending infection by Group B. Streptococcus, E. coli and other pathogenic bacteria [39, 41]. However, organisms are not always identified and some authorities argue that MIR may be the result of sterile inflammation. [51, 42, 21, 23, 38, 32] Immediate and long-term implications of MIR MIR is associated with several adverse health outcomes. Bacteria causing MIR may enter the maternal or fetal circulation, causing puerperal fever or early onset neonatal sepsis, respectively. When MIR co-occurs with FIR, there is an increased risk of neonatal death [25, 27]. MIR2 is a risk factor for recurrent wheeze [22, 30], asthma [22, 16, 12], and chronic lung disease [12]. MIR has also been associated with lower mental development index [50], and increased risk of autism spectrum disorder [46]. In a small case-control study, severe MIR has also been linked to cerebral palsy albeit indirectly [37]. Challenges in analysing MIR The stages of MIR are broadly defined with significant variability in how different pathologists classify these patterns and the cut-off between different stages [17, 36]. In a study by Redline et al. [36], showed a moderate kappa agreement of 0.580.580.580.58 for the diagnosis of no MIR (MIR0) vs. any MIR (MIR1-3). An older study using a previous definition of histologic chorioamnionitis showed a kappa of 0.72 [43]. A study using secondary review of generalist pathologist diagnoses by an expert placental pathologist showed a kappa for any MIR of 0.6 [13]. This variability creates challenges for analysis. This may explain the relatively loose association between histologic chorioamnionitis and clinical chorioamnionitis or markers of maternal systemic inflammation [34, 32, 23, 28]. Machine Learning and Multiple Instance Learning Machine Learning along with digital pathology allows investigations to be conducted with larger datasets [5, 10, 44]. Consequently, there have been several studies conducted in recent years utilizing machine learning to build tools for pathological diagnoses, and analyse digital pathology datasets. These include both local-level tasks like nuclei segmentation, and global-level tasks prostate cancer detection [45]. Whole slide images are up to 100,000 x 60,000 pixels – too large for efficient computation. The earliest studies in computational pathology randomly selected patches from WSI in the hope that they would carry diagnostic material from the slide. Then convolutional neural networks (ResNet, EfficientNet) or attention-based models (ViT) were finetuned on the resultant dataset [24]. Later, multiple instance learning (MIL) based methods were proposed, that take all the patches into account, and learn to ascribe importance to patches. The importance scores can be calculated using probability of belonging to the positive class [4], as a proxy for selecting highly important patches. Another approach was proposed in [26], where attention mechanism was used to force the model to ascribe importance to patches during classification. These attention scores were calculated based on features extracted using some pretrained model e.g., ResNet50, instead of using raw patches directly. Recently, several groups have released pathology foundation models. Unlike ResNet or EfficientNet, these models use transformer architecture and have been trained on tens of thousands to millions of slides [15, 6, 53]. These models show superior performance on benchmarks. However, the training sets lack placental slides and benchmarks are nearly all on detection, classification, and mutation identification of neoplasia. Recently, several studies have been conducted employing machine learning and MIL for predicting and analyzing placenta [29]. Chen et al. [7] used CNNs to design a system for morphological characterization, and clinically meaningful feature analysis of placentas from photos. Zhang et al. [52] propose a Cycle-GAN along with attention module and saliency constraint to enable cross-domain image segmentation by translating target domain placenta pictures (from 1 hospital) into source domain (a different hospital), and adapting a pretrained segmentation model to segment them. Clymer et al. [9] designed multiresolution CNNs to classify desidual vasculopathy in placental membranes. Mobadersany et al. [31] used MIL framwork to improve prediction of gestational age from placental WSI. Andreasen et al. [3] proposed deep learning method for placenta segmentation from obstetric ultrasound. Goldstein et al. [18] used the MIL framework for classification of placental villous infarction, perivillous fibrin deposition, and intervillous thrombus. Patnaik et al. [33] used a pretrained ResNet-18 for feature extraction from placental histopathology images, and classify maternal vascular malperfusion. Irmakci et al. [19] investigated the challenges posed to Machine Learning models by presence of tissue contaminants e.g., floaters in WSIs. Studies have also been conducted to classify cell and regions in the placental disc [14, 49] Recently, Chou et al. [8] used machine learning and other quantitative techniques to characterize maternal inflammatory response (MIR) in placental membranes. In this study, we investigate MIR stage prediction from WSI."
https://arxiv.org/html/2411.02336v1,"MVPaint:
Synchronized Multi-View Diffusion for Painting Anything 3D","Texturing is a crucial step in the 3D asset production workflow, which enhances the visual appeal and diversity of 3D assets. Despite recent advancements in Text-to-Texture (T2T) generation, existing methods often yield subpar results, primarily due to local discontinuities, inconsistencies across multiple views, and their heavy dependence on UV unwrapping outcomes. To tackle these challenges, we propose a novel generation-refinement 3D texturing framework called MVPaint, which can generate high-resolution, seamless textures while emphasizing multi-view consistency. MVPaint mainly consists of three key modules. 1) Synchronized Multi-view Generation (SMG). Given a 3D mesh model, MVPaint first simultaneously generates multi-view images by employing an SMG model, which leads to coarse texturing results with unpainted parts due to missing observations. 2) Spatial-aware 3D Inpainting (S3I). To ensure complete 3D texturing, we introduce the S3I method, specifically designed to effectively texture previously unobserved areas. 3) UV Refinement (UVR). Furthermore, MVPaint employs a UVR module to improve the texture quality in the UV space, which first performs a UV-space Super-Resolution, followed by a Spatial-aware Seam-Smoothing algorithm for revising spatial texturing discontinuities caused by UV unwrapping. Moreover, we establish two T2T evaluation benchmarks: the Objaverse T2T benchmark and the GSO T2T benchmark, based on selected high-quality 3D meshes from the Objaverse dataset and the entire GSO dataset, respectively. Extensive experimental results demonstrate that MVPaint surpasses existing state-of-the-art methods. Notably, MVPaint could generate high-fidelity textures with minimal Janus issues and highly enhanced cross-view consistency.","3D texture generation remains a complex and critical aspect of asset creation, especially valuable in applications like gaming, animation, and virtual/augmented/mixed reality. Despite the scarcity of specialized 3D training data and the high computational demands of texture modeling, recent breakthroughs in text-to-image technologies [22, 17, 15, 44] have significantly advanced the field. These technologies facilitate Text-to-Texture (T2T) generation [6, 43, 60, 34] and their integration with 3D shapes [40, 49, 35], enhancing the visual diversity and realism of 3D models. However, achieving consistent and seamless textures across various viewing angles remains challenging, often hampered by local discontinuities and cross-view inconsistencies. Recently, many texture generation methods have focused on leveraging 2D diffusion priors for guiding the generation process, which often utilizes conditional controls [62] (e.g., depth) to produce more fitting textures. TEXTure [43] and Text2Tex [6] sample a series of camera viewpoints for iteratively rendering depth maps, which are then used to generate high-quality images through a pre-trained depth-to-image diffusion model. To avoid inconsistent textures due to using multiple independent generation processes, SyncMVD [34] introduced a method combining multi-view single-step denoising with UV space synchronization. However, its reused attention process is limited to nearby views, which frequently leads to Janus problems. Paint3D [60] developed a coarse-to-fine texture generation strategy, starting with a coarse texture obtained by iteratively painting from camera viewpoints, followed by inpainting and super-resolution in UV space. Similarly, Meta 3D TextureGen [4] also employs UV position maps for UV inpainting and enhancement. Despite achieving remarkable 3D texturing results, both tools [60, 4] depend heavily on continuous mesh UV unwrapping. Discontinuities in texture often arise in scenarios where the UV atlases are randomly packed within the UV images. Consequently, many challenges remain in 3D texturing: Multi-View Consistency: Ensuring consistency across multiple viewpoints to prevent local style discontinuities and the presence of numerous seams. Diverse Texture Details: Avoiding overly smooth textures that lack detail, while aiming for high-resolution outputs. UV Unwrapping Robustness: Developing a method that does not rely heavily on UV unwrapping results to achieve robust automated generation. To address these challenges, we propose MVPaint, a coarse-to-fine 3D texture generation framework capable of producing high-fidelity, seamless 3D textures while ensuring multi-view consistency and reducing dependence on UV unwrapping quality. MVPaint mainly consists of three stages for texture generation. (1) First, we employ the Synchronized Multi-view Generation (SMG) model that uses a multi-view diffusion model with cross-attention [51] and UV synchronization to initiate 3D texture generation conditioned on a given textural instruction, which effectively avoids the Janus problem and produces highly consistent multi-view images at low resolution. Following, we upsample and refine the coarse multi-view images by adding vivid texture details, subsequently projecting them into UV space (1K resolution) for further enhancement. (2) Second, we propose the Spatial-aware 3D Inpainting (S3I) method to ensure complete 3D texturing, particularly for areas that were not observed in the first stage. Specifically, S3I resolves the inpainting process in 3D space by considering the spatial relations among 3D points uniformly sampled from mesh surfaces. (3) Third, we introduce a UV Refinement (UVR) module, comprising a series of tailored texture enhancement operations in UV space. UVR first employs a super-resolution module to upscale the UV map to 2K resolution. Afterward, we introduce a Spatial-aware Seam-smoothing Algorithm (SSA) to revise spatial discontinuous textures, especially for repairing the seams caused by UV unwrapping. Consequently, high-quality 3D UV textures could be obtained. To facilitate the evaluation of T2T generation, we establish two benchmarks: the Objaverse [12] T2T benchmark and the GSO [16] T2T benchmark. The Objaverse T2T benchmark comprises 1000 high-quality 3D meshes curated from the Objaverse dataset. Given that most T2T models are trained on a subset of the Objaverse dataset, we further establish the GSO T2T benchmark, which leverages all 1032 3D models from the GSO dataset to assess the generalizability of T2T models. For each 3D mesh, textual annotations are generated utilizing a large language model (LLM). Extensive experimental results on the Objaverse [12] and the GSO [16] T2T benchmarks demonstrate that MVPaint could outperform existing State-of-The-Art (SoTA) methods for 3D texture generation. We would like to emphasize that MVPaint is a robust 3D texturing method, significantly reducing occurrences of failed generations, such as missing areas, large inconsistencies, over-smoothness, and Janus issues. Qualitative texturing results of MVPaint could be visualized in Fig. 1. Our contributions could be summarized as follows: 1) We propose a robust 3D texturing framework, entitled MVPaint, for generating diverse, high-quality, seamless 3D textures while ensuring multi-view consistency. 2) Various 3D texturing models, operations, and strategies, including SMG, S3I, and UVR modules, have been proposed, studied, and utilized in this work. We believe these contributions will significantly advance future research in 3D texture generation. 3) We conduct extensive experiments on the Objaverse and the GSO T2T benchmark, demonstrating that MVPaint achieves impressive 3D texture generation results, surpassing existing SoTA methods. Figure 2: The Framework Overview of MVPaint. Given an input mesh, Stage 1 of MVPaint utilizes a synchronized multi-view generation (SMG) model, consisting of a control-based T2MV model and an I2I model, for 3D texture initialization. In Stage 2, the synchronized views are reprojected back to UV space, where inpainting is performed on the 3D point cloud to fill the holes (shown in red dots), hence completing the UV map. In Stage 3, the completed UV map undergoes super-resolution, adding finer details, followed by seam detection and 3D-aware smoothing to achieve a complete, seamless, and multi-view consistent 3D texture."
https://arxiv.org/html/2411.02327v2,PPLLaVA: Varied Video Sequence Understanding With Prompt Guidance,"The past year has witnessed the significant advancement of video-based large language models. However, the challenge of developing a unified model for both short and long video understanding remains unresolved. Most existing video LLMs cannot handle hour-long videos, while methods custom for long videos tend to be ineffective for shorter videos and images. In this paper, we identify the key issue as the redundant content in videos. To address this, we propose a novel pooling strategy that simultaneously achieves token compression and instruction-aware visual feature aggregation. Our model is termed Prompt-guided Pooling LLaVA, or PPLLaVA for short. Specifically, PPLLaVA consists of three core components: the CLIP-based visual-prompt alignment that extracts visual information relevant to the user’s instructions, the prompt-guided pooling that compresses the visual sequence to arbitrary scales using convolution-style pooling, and the clip context extension designed for lengthy prompt common in visual dialogue. Moreover, our codebase also integrates the most advanced video Direct Preference Optimization (DPO) and visual interleave training. Extensive experiments have validated the performance of our model. With superior throughput and only 1024 visual context, PPLLaVA achieves better results on image benchmarks as a video LLM, while achieving state-of-the-art performance across various video benchmarks, excelling in tasks ranging from caption generation to multiple-choice questions, and handling video lengths from seconds to hours. Codes have been available at https://github.com/farewellthree/PPLLaVA.","Video Large Language Models (Video LLMs) have made significant advancements over the past year. Given the extensive resources and the scarcity of high-quality video-text data required for video pretraining, performing Image-to-Video transfer on powerful Image-domain Large Language Models has become a more practical approach for most Video LLMs. Building on the most advanced image LLMs (Liu et al., 2023a; 2024a; Dai et al., 2023), existing video LLMs typically address the modal differences between images and videos by video instruction data production (Maaz et al., 2023; Li et al., 2023b; Luo et al., 2023; Zhang et al., 2024b), temporal modeling (Liu et al., 2024c; d; Li et al., 2023c; Huang et al., 2023), or video token aggregation (Jin et al., 2023; Li et al., 2023d; Xu et al., 2024). Meanwhile, a wide range of video benchmarks and test tasks offer diverse perspectives and options for evaluating the capabilities of video LLMs, including video question answering (Maaz et al., 2023; Xu et al., 2016; Caba Heilbron et al., 2015; Wu et al., 2017), video dense captioning (Ren et al., 2024), multiple-choice questions (Li et al., 2023c; Fu et al., 2024), and long video assessment (Fu et al., 2024; Song et al., 2024; Zhang et al., 2024a). Figure 1: (a) An instance from VideoMME (Fu et al., 2024). The crucial information pertains to only a small portion of the video for different questions. (b) Performance comparison of PPLLaVA with recent strong Video LLM among video benchmarks, image benchmarks, and efficiency. All the models are based on Vicuna-7B. For temporal modeling, an intuitive approach is to directly input tokens from each frame into the LLM, a method proven effective in several studies (Liu et al., 2024d; b; Li et al., 2024). However, while this method leverages the LLM’s sequence modeling capabilities, it leads to an excessively long visual context. This not only increases computational resource consumption and processing time but also limits the model’s ability to handle extended videos. To address this issue, several alternative approaches exist. A commonly adopted method is average pooling across the temporal dimension, frequently seen in early video LLMs (Li et al., 2023b; Maaz et al., 2023; Luo et al., 2023; Liu et al., 2024c). While this approach maintains a constant context length, it significantly diminishes the model’s ability to capture temporal dynamics. Models designed specifically for long videos often incorporate unique structures, such as memory mechanisms (Ren et al., 2024; Zhang et al., 2024a; Zhou et al., 2024). Although these designs enable the models to handle hour-long videos, they offer limited utility for short videos or images. Another approach is the use of conditional token pooling or aggregation (Li et al., 2023d; Xu et al., 2024; Jin et al., 2023). Unlike global average pooling, this method reduces the context length while preserving some spatiotemporal structure, enabling more effective spatiotemporal modeling. However, pooling inevitably leads to performance loss compared to using the full set. So, how can we reduce the number of tokens while preserving the spatiotemporal modeling capabilities? We believe the solution lies in the inherent characteristics of the video. As proven by many previous works (Han et al., 2022; Liu et al., 2023b; Ma et al., 2022), videos contain significant redundancy, with key information often concentrated in just a few frames, which is particularly true for long videos. For video LLMs, this issue can be more pronounced. As shown in Fig. 1(a), the user’s instruction may pertain to only a small portion of the video, with the rest being redundant for correctly answering the question. Therefore, if we can extract crucial video information while compressing tokens, we can maintain or even enhance performance. In this context, Image LLMs have offered valuable inspiration. The BLIP series (Li et al., 2023a; Dai et al., 2023; Xue et al., 2024) and the LLaVA series (Liu et al., 2023a; 2024a; 2024b; Li et al., 2024) are the two most popular structures in multimodal LLM. BLIP uses a Q-Former for image-to-text mapping, while the LLaVA series employs simple linear projection or MLP. Recently, LLaVA-based models have demonstrated that simple mapping can achieve better results with less training (Liu et al., 2024a). However, despite requiring more computation resources and training stages, the Q-Former offers two key advantages: first, it significantly reduces visual tokens by converting them into fewer query tokens; second, through the interaction between text and visual tokens within the Q-Former, it enables more targeted extraction of video features relevant to the user’s instructions (Dai et al., 2023). Hence, can we develop a pooling method that retains LLaVA’s simple structure and powerful weights while reducing the number of tokens and enabling prompt-aware feature extraction? To this end, we propose Prompt-guided Pooling LLaVA (PPLLaVA), a novel method that combines visual pooling with instruction-aware visual feature extraction. Specifically, PPLLaVA first identifies prompt-relevant visual representations through fine-grained vision-prompt alignment. Then, using the prompt-vision relevance as a 3D convolutional kernel, PPLLaVA can compress the visual tokens to any desired three-dimensional size based on the specified output size or stride. Finally, recognizing that CLIP pretraining provides a limited context length and that training video LLMs—particularly for multi-turn dialogues—requires long text contexts, PPLLaVA also employs asymmetric positional embedding extensions to expand the text encoding capacity. As a result, PPLLaVA effectively extracts relevant visual features from both long texts and short phrases while compressing video tokens. PPLLaVA achieves over an 80% compression rate, supports ultra-long video inputs, and simultaneously improves performance on short videos. In fact, PPLLaVA functions similarly to a Q-Former within LLaVA, but it offers several advantages over directly training a Q-Former: (1) PPLLaVA introduces far fewer additional parameters and computational overhead, amounting to less than one-tenth of a Q-Former. (2) While a Q-Former requires a three-stage pretraining process—contrastive learning, alignment training, and instruction tuning—PPLLaVA can be utilized solely during instruction tuning, allowing for seamless transfer from image-domain LLMs. (3) PPLLaVA supports flexible output sizes for different modalities, whereas the number of queries in a Q-Former is fixed once set. As a result, different Q-Formers typically need to be trained separately for images and videos (Zhang et al., 2023; Li et al., 2023c). Extensive experiments on the latest multimodal LLM benchmarks have validated the superiority of PPLLaVA: with superior throughput, PPLLaVA has achieved top results across a wide range of test sets, including MSRVTT (Xu et al., 2016), MSVD (Wu et al., 2017), ActivityNet (Caba Heilbron et al., 2015), VCG Bench (Maaz et al., 2023), MVBench (Li et al., 2023c), and Video-MME (Fu et al., 2024). These benchmarks encompass tasks such as video question answering, detailed video captioning, and video multiple-choice questions, with video lengths ranging from seconds to hours. Furthermore, our codebase has integrated cutting-edge video LLM techniques, including video Direct Preference Optimization and video-image-multiple image interleave training. As shown in Fig. 1(b), compared to recent top Video LLMs, PPLLaVA demonstrates clear advantages across both video and image benchmarks, while responding 7x faster than LLaVA-Next-Video-7B."
https://arxiv.org/html/2411.02319v2,Gen𝒳𝒳\mathcal{X}caligraphic_XD: Generating any 3D and 4D Scenes,"Recent developments in 2D visual generation have been remarkably successful. However, 3D and 4D generation remain challenging in real-world applications due to the lack of large-scale 4D data and effective model design. In this paper, we propose to jointly investigate general 3D and 4D generation by leveraging camera and object movements commonly observed in daily life. Due to the lack of real-world 4D data in the community, we first propose a data curation pipeline to obtain camera poses and object motion strength from videos. Based on this pipeline, we introduce a large-scale real-world 4D scene dataset: CamVid-30K. By leveraging all the 3D and 4D data, we develop our framework, Gen𝒳𝒳\mathcal{X}caligraphic_XD, which allows us to produce any 3D or 4D scene. We propose multiview-temporal modules, which disentangle camera and object movements, to seamlessly learn from both 3D and 4D data. Additionally, Gen𝒳𝒳\mathcal{X}caligraphic_XD employs masked latent conditions to support a variety of conditioning views. Gen𝒳𝒳\mathcal{X}caligraphic_XD can generate videos that follow the camera trajectory as well as consistent 3D views that can be lifted into 3D representations. We perform extensive evaluations across various real-world and synthetic datasets, demonstrating Gen𝒳𝒳\mathcal{X}caligraphic_XD’s effectiveness and versatility compared to previous methods in 3D and 4D generation. The dataset and code will be made publicly available.","Generating 2D visual content has achieved remarkable success with diffusion (Rombach et al., 2022; Betker et al., 2023; Esser et al., 2024; Blattmann et al., 2023) and autoregressive modeling (Tian et al., 2024; Sun et al., 2024; Kondratyuk et al., 2023; Luo et al., 2024), which have already been used in real-world applications, benefiting society. In addition to 2D generation, 3D content generation is also of vital importance, with applications in video games, visual effects, and wearable mixed reality devices. However, due to the complexity of 3D modeling and the limitations of 3D data, 3D content generation is still far from satisfactory and is attracting more attention. In this paper, we focus on the unified generation of 3D and 4D content. Specifically, static 3D content involves only spatial view changes, referred to as 3D generation in this paper. In contrast, dynamic 3D content includes movable objects within the scene, requiring the modeling of both spatial view and dynamic (temporal) changes, which we term 4D generation. Most previous works (Liu et al., 2023b; Shi et al., 2023; Zhao et al., 2023; Xie et al., 2024; Tang et al., 2024; 2023) focus on 3D and 4D generation using synthetic object data. Synthetic object data are typically meshes, allowing researchers to render images and other 3D information (e.g., normals and depth) from any viewpoint. However, object generation is more beneficial to specialists than to the general public. In contrast, scene-level generation can help everyone enhance their images and videos with richer content. As a result, recent works (Gao et al., 2024; Wu et al., 2024b) have explored general 3D generation (both scene-level and object-level) in a single model, achieving impressive performance. Nonetheless, these works focus solely on static 3D generation, without addressing dynamics. In this paper, we propose a unified framework for general 3D and 4D generation, enabling the generation of images from different viewpoints and timesteps with any number of conditioning images (Fig. 1). The first and foremost challenge in 4D generation is the lack of general 4D data. In this work, we propose CamVid-30K, which contains approximately 30K 4D data samples. 4D data require both multi-view spatial information and temporal dynamics, so we turn to video data to obtain the necessary 4D data. Specifically, we need two key attributes from the video: the camera pose for each frame and the presence of movable objects. To achieve this, we first estimate the possible movable objects in the video using a segmentation model and then estimate the camera pose using keypoints in the static parts of the scene. While successful camera pose estimation ensures multiple views, we also need to ensure that moving objects are present in the video, rather than purely static scenes. To address this, we propose an object motion field that leverages aligned depth to estimate true object movement in the 2D view. Based on the object motion field, we filtered out static scenes, resulting in approximately 30K videos with camera poses. In addition, we propose a unified framework, Gen𝒳𝒳\mathcal{X}caligraphic_XD, to handle 3D and 4D generation within a single model. While there are similarities between 3D and 4D data in terms of their representation of spatial information, they differ in how they capture temporal information. Therefore, 3D and 4D generation can complement each other through the disentanglement of spatial and temporal information (see Appendix. C for details). To achieve this, we combine both 3D and 4D data during model training. To disentangle the spatial and temporal information, we introduce multiview-temporal modules in Gen𝒳𝒳\mathcal{X}caligraphic_XD. In each module, we use α𝛼\alphaitalic_α-fusing to merge spatial and temporal information for 4D data, while removing temporal information for 3D data. Previous works Xu et al. (2024); Voleti et al. (2024) typically use a fixed number of conditioning images (e.g., the first image). However, single-image conditioning can be more creative, whereas multi-image conditioning offers greater consistency. As a result, we implement masked latent conditioning in our diffusion model. By masking out the noise in the conditioning images, Gen𝒳𝒳\mathcal{X}caligraphic_XD can support any number of input views without modifying the network. With high-quality 4D data and a 4D spatio-temporal generative model, Gen𝒳𝒳\mathcal{X}caligraphic_XD achieves significant performance in both 3D and 4D generation using single or multiple input views. Our contributions are summarized as follows: • We design a data curation pipeline for obtaining high-quality 4D data with movable objects from videos and annotate 30,000 videos with camera poses. This large-scale dataset, termed CamVid-30K, will be made available for public use. • We propose a 3D-4D joint framework, Gen𝒳𝒳\mathcal{X}caligraphic_XD, which supports image-conditioned 3D and 4D generation in various settings (Tab. 1). In Gen𝒳𝒳\mathcal{X}caligraphic_XD, the multiview-temporal layer is introduced to disentangle and fuse multi-view and temporal information. • Using the proposed CamVid-30K along with other existing 3D and 4D datasets, Gen𝒳𝒳\mathcal{X}caligraphic_XD achieves performance comparable to or better than previous state-of-the-art and baseline methods in single-view 3D object generation, few-view 3D scene reconstruction, single-view 4D generation, and single/multi-view 4D generation. Table 1: Comparison among the settings of previous works. Method 3D Generation 4D Generation Object Scene Single View Multi-View Object Scene Single View Multi-View IM-3D ✓ ✗ ✓ ✗ ✗ ✗ ✗ ✗ RealmDreamer ✗ ✓ ✓ ✗ ✗ ✗ ✗ ✗ ReconFusion ✓ ✓ ✗ ✓ ✗ ✗ ✗ ✗ CAT3D ✓ ✓ ✓ ✓ ✗ ✗ ✗ ✗ Animate124 ✗ ✗ ✗ ✗ ✓ ✗ ✓ ✗ CameraCtrl ✗ ✗ ✗ ✗ ✗ ✓ ✓ ✗ SV4D ✓ ✗ ✓ ✗ ✓ ✗ ✓ ✓ CamCo ✗ ✓ ✓ ✗ ✗ ✓ ✓ ✗ Gen𝒳𝒳\mathcal{X}caligraphic_XD (Ours) ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓"
https://arxiv.org/html/2411.02299v1,Grouped Discrete Representation for Object-Centric Learning,"Object-Centric Learning (OCL) can discover objects in images or videos by simply reconstructing the input. For better object discovery, representative OCL methods reconstruct the input as its Variational Autoencoder (VAE) intermediate representation, which suppresses pixel noises and promotes object separability by discretizing continuous super-pixels with template features. However, treating features as units overlooks their composing attributes, thus impeding model generalization; indexing features with scalar numbers loses attribute-level similarities and differences, thus hindering model convergence. We propose Grouped Discrete Representation (GDR) for OCL. We decompose features into combinatorial attributes via organized channel grouping, and compose these attributes into discrete representation via tuple indexes. Experiments show that our GDR improves both Transformer- and Diffusion-based OCL methods consistently on various datasets. Visualizations show that our GDR captures better object separability.","Figure 1: Representation discretization: non-grouped vs grouped. (upper) Existing methods take features as units and use scalar number indexes to select template features from a codebook to discretize super-pixels. (lower) We take attributes as units and use tuple indexes to select template attributes from the grouped codebook. OCL is improved by reconstructing our grouped discrete representation. Under self or weak supervision, Object Centric Learning (OCL) [9, 4] can represent dense image or video pixels as sparse object feature vectors with corresponding segmentation masks as byproducts that reflect how well the object features are. This is also called object discovery [25]. OCL is metaphysically bio-plausible as we humans perceive visual scenes as objects for higher-level vision cognition, like understanding, reasoning, planning, and decision-making [2, 6, 19]. It is also physically desired as object-level representation of images or videos is more versatile for visual tasks involving multi-modalities [32, 29]. The training signal comes from reconstructing the input. Direct reconstruction of the input pixels [17, 9] struggles with complex-textured objects. Mixture-based OCL methods [14, 8] reconstructs more object-separable modalities like optical flow and depth map. Foundation-based [21, 33] ones take input’s foundation model features as the target. Transformer-based [22, 24] and Diffusion-based [30, 13] ones reconstruct input’s Variation Autoencoder (VAE) intermediate representation. With limited numbers of shared template features, i.e., codes in a codebook, continuous-valued super-pixels in VAE representation are discretized [11, 26], so that (super-)pixel noises are suppressed and object separability is promoted. The better object separability in the reconstruction target, the better training signal for object discovery, or “guidance”, there will be. However, these methods treat features as units and entangle the composing attributes together, thus impeding model generalization. The corresponding scalar number code indexes fail to capture super-pixels’ attribute-level similarities and differences, thus hindering model convergence. As illustrated in Fig. 1, consider a dataset described by two attribute groups: color (black or white) and shape (triangle, square, or circle); and an image in it with four objects, each down-sampled to a super-pixel in the feature map. Six code indexes are needed to select template features from a feature-level codebook, i.e., scalar numbers 0-5 referring to black-triangle, black-circle, black-square, etc.; then the feature map can be discretized as [0451]delimited-[]0451[\begin{smallmatrix}0&4\\ 5&1\end{smallmatrix}][ start_ROW start_CELL 0 end_CELL start_CELL 4 end_CELL end_ROW start_ROW start_CELL 5 end_CELL start_CELL 1 end_CELL end_ROW ]. Over the dataset, each code is reused with probability 1616\frac{1}{6}divide start_ARG 1 end_ARG start_ARG 6 end_ARG. But if we decompose that codebook into two attribute groups, super-pixels can be discretized as combinations of template attributes selected from these two groups, i.e., [0,01,11,20,1]delimited-[]00111201[\begin{smallmatrix}0,0&1,1\\ 1,2&0,1\end{smallmatrix}][ start_ROW start_CELL 0 , 0 end_CELL start_CELL 1 , 1 end_CELL end_ROW start_ROW start_CELL 1 , 2 end_CELL start_CELL 0 , 1 end_CELL end_ROW ]. The identical first/second numbers in these tuples indicate same color/shape attributes, and vice versa. This provides stronger guidance to OCL, enhancing model convergence. These attribute codes are reused more with probabilities 1212\frac{1}{2}divide start_ARG 1 end_ARG start_ARG 2 end_ARG and 1313\frac{1}{3}divide start_ARG 1 end_ARG start_ARG 3 end_ARG, respectively, benefiting model generalization. In short, our main contributions are as follows: • We propose Grouped Discrete Representation (GDR) to decompose features into attributes for VAE discrete representation that guides OCL better; • With GDR, we improve both the convergence and generalization of classical and even state-of-the-art OCL methods on object discovery tasks; • We interpret visually how GDR captures attribute-level similarities and differences and enhances object separability in VAE discrete representation."
https://arxiv.org/html/2411.02293v2,Tencent Hunyuan3D-1.0: A Unified Framework for Text-to-3D and Image-to-3D Generation,"While 3D generative models have greatly improved artists’ workflows, the existing diffusion models for 3D generation suffer from slow generation and poor generalization. To address this issue, we propose a two-stage approach named Hunyuan3D-1.0 including a lite version and a standard version, that both support text- and image-conditioned generation. In the first stage, we employ a multi-view diffusion model that efficiently generates multi-view RGB in approximately 4 seconds. These multi-view images capture rich details of the 3D asset from different viewpoints, relaxing the tasks from single-view to multi-view reconstruction. In the second stage, we introduce a feed-forward reconstruction model that rapidly and faithfully reconstructs the 3D asset given the generated multi-view images in approximately 7 seconds. The reconstruction network learns to handle noises and in-consistency introduced by the multi-view diffusion and leverages the available information from the condition image to efficiently recover the 3D structure. Our framework involves the text-to-image model, i.e., Hunyuan-DiT [22], making it a unified framework to support both text- and image-conditioned 3D generation. Our standard version has 3×3\times3 × more parameters than our lite and other existing model. Our Hunyuan3D-1.0 achieves an impressive balance between speed and quality, significantly reducing generation time while maintaining the quality and diversity of the produced assets.","3D generation has long been an attractive and active topic in the fields of computer vision and computer graphics, with significant applications spanning gaming, film, e-commerce, and robotics. Creating high-quality 3D assets is a time-intensive process for artists, making automatic generation a long-term goal for researchers. Early efforts in this field focused on unconditional generation within specific categories, constrained by 3D representation and data limitations. The recent success of scaling laws in large language models (LLMs), as well as in image and video generation, has illuminated a path toward this long-term vision. However, achieving similar advancements in 3D asset generation remains challenging due to the expressive nature of 3D assets and the limited availability of comprehensive datasets. The largest existing 3D dataset, Objarverse-xl [7], comprises only 10 million assets, which pales in comparison to the large-scale datasets available for language, image, and video tasks. Leveraging priors from 2D generative models presents a promising approach to address this limitation. To take advantage of 2D generative models, pioneering works have explored this problem and achieved notable advancements. Poole et al. [35] utilize Score Distillation Sampling (SDS) to distill a 3D representation, i.e., Nerf [31], via 2D image diffusion models. Despite issues with over-saturation and significant time costs, this approach inspired subsequent 2D lifting research. Follow-up works have explored to improve sampling efficiency [52], fine-tune diffusion models into multi-view diffusion frameworks [24, 41, 1], and replace sampling losses with regular rendering losses [23, 61, 26, 27]. However, these optimization-based methods remain time-consuming, requiring anywhere from 5 minutes to an hour to optimize the 3D representation [63, 50, 31, 58]. In contrast, feed-forward methods [13, 11, 60, 4, 44] can generate 3D objects in mere seconds but often struggle with generalization to unseen objects and fail to generate thin, paper-like structures. Disentangling single-view generation tasks into generating multi-view images and completing sparse-view reconstruction via feed-forward methods is a promising path to mitigate generalization issues and eliminate the optimization problem in SDS. Despite several works [59, 61, 40, 24, 1, 26, 27] in multi-view generation and sparse-view reconstruction, few have organized these approaches into a cohesive framework that addresses their combined challenges. First, widely used multi-view diffusion models are often criticized for multi-view inconsistency and slow denoising processes. Second, sparse-view reconstruction models typically rely solely on view-aware RGB images to predict 3D representations. Addressing these issues separately is challenging. Noticing the need to tackle these sub-tasks together, we propose Hunyuan3D-1.0, which integrates the strengths of multi-view diffusion models and sparse-view reconstruction models to achieve 3D generation in 10 seconds in the best-case scenario, achieving a subtle balance between generalization and quality. In the first stage, the multi-view diffusion model generates RGB to finish the 2D-to-3D lifting. We fine-tune a large-scale 2D diffusion model to generate multi-view images to enhance the model’s understanding of 3D information. Additionally, we set the 0-elevation camera orbit for the generated views to maximize the visible area between generated views. In the second stage, the sparse-view reconstruction model utilizes the imperfectly consistent multi-view images to recover the underlying 3D shape. Unlike most sparse-view reconstruction models that only use RGB images with known poses, we incorporate the conditional image, without the known view pose, to provide additional view information as an auxiliary input to cover the unseen part in the generated multi-view images. Furthermore, we employ a linear unpatchify layer operation to enrich details in the latent space without incurring additional memory or computational costs. Our contributions are summarized as follows: • We introduce a unified framework Hunyuan3D-1.0, support text- and image- condition 3D generation both. • We design the 0-elevation pose distribution in the multi-view generation, maximizing the visible area between generated views. • We introduce a view-aware classifier-free guidance that balances the controllability and diversity for different view generations. • We incorporate the hybrid input that involves the uncalibrated condition image as an auxiliary view in the sparse-view reconstruction process to compensate for the unseen part in the generated images. Figure 1: The overview of our Hunyuan3D-1.0. Given an input image, we first utilize a multi-view diffusion model to synthesize 6 novel views at fixed camera poses. Then we feed the generated multi-view images into a transformer-based sparse-view large reconstruction model to reconstruct a high-quality 3D mesh. The whole image-to-3D generation process takes only around 10 seconds."
https://arxiv.org/html/2411.02256v1,"Unified Speech Recognition: A Single Model for Auditory, Visual, and Audiovisual Inputs","Research in auditory, visual, and audiovisual speech recognition (ASR, VSR, and AVSR, respectively) has traditionally been conducted independently. Even recent self-supervised studies addressing two or all three tasks simultaneously tend to yield separate models, leading to disjoint inference pipelines with increased memory requirements and redundancies. This paper proposes unified training strategies for these systems. We demonstrate that training a single model for all three tasks enhances VSR and AVSR performance, overcoming typical optimisation challenges when training from scratch. Moreover, we introduce a greedy pseudo-labelling approach to more effectively leverage unlabelled samples, addressing shortcomings in related self-supervised methods. Finally, we develop a self-supervised pre-training method within our framework, proving its effectiveness alongside our semi-supervised approach. Despite using a single model for all tasks, our unified approach achieves state-of-the-art performance compared to recent methods on LRS3 and LRS2 for ASR, VSR, and AVSR, as well as on the newly released WildVSR dataset. Code and models are available at https://github.com/ahaliassos/usr.","Speech recognition can be achieved using auditory signals (known as auditory/automatic speech recognition; ASR) [1, 2], visual cues from lip movements (visual speech recognition; VSR) [3, 4], or both (audiovisual speech recognition; AVSR) [5, 6]. Audio typically offers the most relevant information in videos of talking faces, but lipreading can greatly enhance recognition, especially when the audio is noisy or wholly unavailable [6]. Despite the similarities between ASR, VSR, and AVSR, research in these fields has largely developed independently [7, 8, 3, 9]. The Transformer architecture’s versatility [10, 11, 12] has spurred efforts to unify speech recognition by pre-training a single model on various unlabelled inputs (visual, auditory, and audiovisual) through self-supervision [13, 14, 15]. However, these methods often require separate fine-tuning stages for ASR, VSR, and AVSR, leading to separate models for each task, which increases computational load and complexity. u-HuBERT [16] shows that a single pre-trained model can be fine-tuned for all three tasks, yet does not reach the performance of separately fine-tuned models [17, 18]. In this paper, we delve deeper into strategies for unified speech recognition (USR) by training a single model to perform ASR, VSR, and AVSR. We find that training such a model from scratch on the LRS3 dataset [19] achieves competitive performance on all tasks. This is notable given the known optimisation difficulties in VSR training, which previously required self-supervised pre-training [13], supervised feature extractor pre-training [6], or curriculum learning strategies [9]. Our findings suggest that including audio improves the optimisation landscape for VSR and AVSR supervised training, as observed in a different context by [20]. Furthermore, we propose a semi-supervised pseudo-labelling approach to leverage unlabelled audiovisual data, addressing shortcomings of standard fine-tuning in self-supervised methods [13, 14, 17, 18]. Fine-tuning often leads to overfitting due to using fewer samples than pre-training, requiring various “tricks” to reach optimal performance [13, 17]. This issue is particularly pronounced in encoder-decoder architectures where usually only the encoder is pre-trained, and attempts to pre-train the decoder have yielded inconsistent results [21, 22]. Our semi-supervised approach generates pseudo-labels via an encoder-decoder momentum-based teacher [23] to leverage unlabelled samples throughout training, effectively mitigating overfitting. Training on all three modalities simultaneously helps alleviate the computational cost of pseudo-labelling as the cost is amortised across the inputs. Lastly, inspired by recent self-supervised works, we design a pre-training method within our unified framework. We combine pre-training with pseudo-labelling and show that our semi-supervised approach is complementary to self-supervision. Our final unified models achieve state-of-the-art results across multiple settings, surpassing existing methods that use separate models for each task."
https://arxiv.org/html/2411.02236v1,3D Audio-Visual Segmentation,"Recognizing the sounding objects in scenes is a longstanding objective in embodied AI, with diverse applications in robotics and AR/VR/MR. To that end, Audio-Visual Segmentation (AVS), taking as condition an audio signal to identify the masks of the target sounding objects in an input image with synchronous camera and microphone sensors, has been recently advanced. However, this paradigm is still insufficient for real-world operation, as the mapping from 2D images to 3D scenes is missing. To address this fundamental limitation, we introduce a novel research problem, 3D Audio-Visual Segmentation, extending the existing AVS to the 3D output space. This problem poses more challenges due to variations in camera extrinsics, audio scattering, occlusions, and diverse acoustics across sounding object categories. To facilitate this research, we create the very first simulation based benchmark, 3DAVS-S34-O7, providing photorealistic 3D scene environments with grounded spatial audio under single-instance and multi-instance settings, across 34 scenes and 7 object categories. This is made possible by re-purposing the Habitat simulator [32] to generate comprehensive annotations of sounding object locations and corresponding 3D masks. Subsequently, we propose a new approach, EchoSegnet, characterized by integrating the ready-to-use knowledge from pretrained 2D audio-visual foundation models synergistically with 3D visual scene representation through spatial audio-aware mask alignment and refinement. Extensive experiments demonstrate that EchoSegnet can effectively segment sounding objects in 3D space on our new benchmark, representing a significant advancement in the field of embodied AI. Project page: https://surrey-uplab.github.io/research/3d-audio-visual-segmentation/","Human perception of the real world, both visual and acoustic, predominantly occurs in three dimensions. Prior psychology literature [37] has highlighted humans’ remarkable ability to correspond across multiple modalities, often involving the association of events across these modalities. For instance, we can effortlessly ground emergent surround sound with its potential source in 3D visuals [26]. Inspired by this capability, a crucial aspect in the development of embodied AI systems is their ability to integrate cues from synchronous multimodal input streams and establish targets corresponding to their goals. In this work, we aim to build a machine model to achieve this multimodal correspondence, particularly targeted towards the task of audio-visual segmentation (AVS) in 3D. Albeit AVS has been widely explored within audio-visual scene analysis and correspondence learning, prominent research in this field has focused on 2D environments involving mono (single channel) sound sources, thus devoid of spatial presence entirely. In this paper, we take the first step towards exploring 3D AVS and introduce a large benchmark, 3DAVS-S34-O7. Our exploration is rooted in a fundamental grounding problem: given an embodied agent equipped with a camera and a binaural microphone, can we teach the agent to obtain fine-grained localization of potential sounding objects (generally by predicting a segment-level mask of the object in 3D) while also utilizing spatial audio cues? (see Fig. 1) Furthermore, we extend our benchmark to include a more competitive multi-instance setup where, although multiple instances of the same object might be present in the scene, the goal is to segment only the sounding instance. This setup helps us testify to the efficacy of spatial presence harnessed from the input binaural audio samples. Recently, 3D Gaussian Splatting (3D-GS) [15] has emerged as a prospective method for modeling static 3D scenes directly from input RGB frames. Owing to its explicit Gaussian based representation, it has paved a natural pathway for 3D visual segmentation [13, 41, 33]. Deriving inspiration from human spatial memory in indoor environments, we design EchoSegnet, a purely training-free pipeline for 3D AVS within a 3D-GS representation. EchoSegnet leverages 2D foundation models (namely SAM [17] and ImageBind [9]) to first obtain 2D AVS masks on the input RGB frames. These 2D AVS masks are further used to segment the Gaussians in the learned 3D-GS representation to obtain multi-view masks to achieve a consistent 3D segmentation. Figure 1: Comparison of the existing 2D AVS task with our proposed 3D AVS. Former task utilizes single channel audio to generate pixel-level masks of the potential sounding object in the input RGB frame. 3D AVS on the other hand is aimed at generating 3D masks (from which multi-view consistent 2D masks can be rendered) while utilizing multichannel (spatial) audio. To summarize, we make the following contributions: (1) the first 3D audio-visual segmentation benchmark composing of fairly complex indoor room scenes with integrated spatial sound cues; (2) a training-free AVS framework, EchoSegnet, capable of syncing across sequential frames from 3D environments; (3) a novel Audio-Informed Spatial Refinement Module AISRM, designed to enhance 3D segmentation and resolve ambiguities in complex, multi-instance environments by leveraging spatial audio intensity maps. We perform a comprehensive evaluation of EchoSegnet on the proposed 3DAVS-S34-O7 for both single-instance and multi-instance scenarios, along with an ablative comparison with existing 2D AVS models, highlighting their shortcomings in aligning audio-visual cues within 3D scenes -establishing their adaptation to 3DAVS-S34-O7 as non-trivial."
https://arxiv.org/html/2411.02229v2,FewViewGS: Gaussian Splatting with Few View Matching and Multi-stage Training,"The field of novel view synthesis from images has seen rapid advancements with the introduction of Neural Radiance Fields (NeRF) and more recently with 3D Gaussian Splatting. Gaussian Splatting became widely adopted due to its efficiency and ability to render novel views accurately. While Gaussian Splatting performs well when a sufficient amount of training images are available, its unstructured explicit representation tends to overfit in scenarios with sparse input images, resulting in poor rendering performance. To address this, we present a 3D Gaussian-based novel view synthesis method using sparse input images that can accurately render the scene from the viewpoints not covered by the training images. We propose a multi-stage training scheme with matching-based consistency constraints imposed on the novel views without relying on pre-trained depth estimation or diffusion models. This is achieved by using the matches of the available training images to supervise the generation of the novel views sampled between the training frames with color, geometry, and semantic losses. In addition, we introduce a locality preserving regularization for 3D Gaussians which removes rendering artifacts by preserving the local color structure of the scene. Evaluation on synthetic and real-world datasets demonstrates competitive or superior performance of our method in few-shot novel view synthesis compared to existing state-of-the-art methods.","Reconstruction of a 3D scene representation from sparse 2D observations that can render novel views unseen during training has been an active field of research with wide applications in VR/AR and navigation. Recently, neural radiance fields (NeRF) [20] utilizing differentiable volume rendering and implicit scene representation achieved great results in novel view synthesis (NVS). The NeRF framework was extended to the sparse-view setting (few-shot NVS) using efficient training schemes, regularization losses, depth consistency, and image priors [11, 22, 8, 34]. While being good at rendering, NeRFs typically take a long time to be optimized, and their rendering speed is far from real-time which greatly limits their practical application. 3D Gaussian Splatting [13] has introduced an unstructured radiance field represented with 3D Gaussians. Gaussians are initialized from a sparse Structure-from-Motion (SfM) [29] point cloud and dynamically added or removed from the scene during training. With images observing a static scene from different viewpoints, Gaussian parameters are optimized using photometric loss. This method became widely adopted due to its training efficiency, unprecedented rendering speed, and quality. However, the rendering performance dramatically drops when fewer images are used for training. While well-observed regions can be accurately rendered, less-supervised scene geometry is under-reconstructed, Gaussian parameters are overfitted to the views observing the scene [44], geometry artifacts [38] appear. In addition, optimization is highly dependent on the SfM initialization point cloud whose quality is also affected by the sparsity of the training views. There are several concurrent works exploring few-shot Gaussian splatting [15, 38, 24, 6, 46]. However, all of them rely on depth estimation [15, 38, 24, 6, 46] or diffusion priors [38]. In these methods, depth-estimation networks predict depth up to scale on unseen scenes, making them sensitive to domain shifts, while diffusion models increase training time. In this paper, we introduce FewViewGS, a Gaussian splatting-based novel view synthesis method achieving state-of-the-art rendering results in a few-shot setup. We break the training process into the pre-training, intermediate, and tuning stages to better propagate the information to the novel views. During the pre-training stage, we use only training views to get a basic representation of the scene. This enables us to obtain a fundamental point cloud and scaled training view depth maps. In the intermediate stage, the emphasis is on optimizing the new perspectives. We use multi-view geometry and a novel view interpolation sampling to render unseen views coherent with the training images. For this, we match the pairs of training images, robustly warp the matches to the randomly sampled novel views between them, and enforce novel view consistency by applying geometry, color, and semantic losses. In the final stage, the scene representation undergoes refinement through a limited number of iterations, utilizing only the known views. During training, the 3D Gaussians are regularized by our proposed locality preserving regularization to maintain their local properties, eliminating the artifacts on the novel views. In summary, our contributions are as follows: ∙∙\bullet∙ A few-shot NVS Gaussian splatting-based system not relying on pre-trained depth estimation or diffusion models achieving SoTA rendering results. ∙∙\bullet∙ A multi-stage training scheme enabling seamless knowledge transfer from known to novel views. ∙∙\bullet∙ A robust warping-based novel view consistency constraint ensuring the coherence of the synthesized unseen images. ∙∙\bullet∙ 3D Gaussian locality preserving regularization handling visual artifacts."
https://arxiv.org/html/2411.02220v1,SIRA: Scalable Inter-frame Relation and Association for Radar Perception,"Conventional radar feature extraction faces limitations due to low spatial resolution, noise, multipath reflection, the presence of ghost targets, and motion blur. Such limitations can be exacerbated by nonlinear object motion, particularly from an ego-centric viewpoint. It becomes evident that to address these challenges, the key lies in exploiting temporal feature relation over an extended horizon and enforcing spatial motion consistency for effective association. To this end, this paper proposes SIRA (Scalable Inter-frame Relation and Association) with two designs. First, inspired by Swin Transformer, we introduce extended temporal relation, generalizing the existing temporal relation layer from two consecutive frames to multiple inter-frames with temporally regrouped window attention for scalability. Second, we propose motion consistency track with the concept of a pseudo-tracklet generated from observational data for better trajectory prediction and subsequent object association. Our approach achieves 58.1158.1158.1158.11 mAP@⁢0.5@0.5@0.5@ 0.5 for oriented object detection and 47.7947.7947.7947.79 MOTA for multiple object tracking on the Radiate dataset, surpassing previous state-of-the-art by a margin of +4.114.11+4.11+ 4.11 mAP@⁢0.5@0.5@0.5@ 0.5 and +9.949.94+9.94+ 9.94 MOTA, respectively.","Automotive perception involves the interpretation of the external driving environment and internal vehicle cabin conditions with an array of perception sensors to achieve robust safety and driving autonomy [40]. Compared to optical camera and lidar sensors, radar is cost-effective, friendly to sensor maintenance and calibration, and has distinct advantages in providing long-range perception capabilities in adverse weather and lighting conditions [59]. Nevertheless, a notable limitation of radar-based automotive perception is its low spatial resolution in the azimuth and elevation domains, and its inherent noise including multipath reflection, ghost targets, and motion blur. As a result, its ability to detect and track objects lags behind the requirements for fully autonomous driving capabilities. Recently, standalone radar-only perception has been investigated in [60, 38, 14, 1, 27, 39, 28]. Li et al. [27] proposed a framework called TempoRadar to study temporal attention to features from 2222 ego-centric bird-eye-view (BEV) radar frames. It has shown promising performance gains when evaluated on the large-scale open Radiate [47] dataset. Figure 1: Conventional radar perception pipelines such as TempoRadar [27] (Bottom Row) rely on a limited number (one or two) of frames and the limited time horizon may lead to incorrect feature-level and object-level association (e.g., t=T−1𝑡𝑇1t=T-1italic_t = italic_T - 1) and propagate to subsequent frames (e.g., t=T𝑡𝑇t=Titalic_t = italic_T). In contrast, SIRA (Top Row) accounts for joint spatio-temporal consistency over an extended temporal horizon (e.g., all 3333 frames here), allowing for more accurate association in nonlinear motion scenarios even in an ego-centric viewpoint. However, such limitations can be exacerbated by nonlinear object motion, particularly from an ego-centric BEV. In particular, low frame rates result in significant influence from the nonlinearity of object motion, leading to frequent tracking errors. Conventional radar perception pipelines such as TempoRadar enables prediction based on information from the previous frame, but in the case of objects with fast and nonlinear motion within radar frames, such information is inadequate (Bottom of Fig. 1). Although applying Kalman filter (KF [24])-based algorithms [4, 61, 7, 12], is possible, radar perception is difficult to relate accurately due to a complex combination of factors, including the effects of high-speed nonlinear motion dynamics and the lack of detailed appearance features due to low resolution. To address these limitations and improve radar perception for object detection and tracking, we propose a framework called scalable inter-frame relation & association (SIRA). SIRA consists of two modules: extended temporal relation (ETR) and motion consistency track (MCTrack). The contributions of this study are as follows: • We introduce ETR, generalizing the existing temporal relation layer from two consecutive frames to multiple inter-frames with temporally regrouped window attention for scalability. It emphasizes the temporal consistency of moving objects by enabling accurate detection while maintaining computational efficiency over long time horizon. This can facilitate easy detection through consistent correlations across multiple frames at the object level. • We designed MCTrack based on the concept of pseudo-tracklets, which are generated by using a learnable module to predict the arbitral nonlinear motion of an object between multiple frames, and the association caused by these pseudo-tracklets enhances spatial consistency during inference. Thus, MCTrack enables more reliable position predictions, even in scenarios with fast-moving objects and low frame rates. • We propose SIRA that adopts a loss function for the end-to-end learning of these two modules, achieving stable predictions that capture the spatio-temporal consistency of nonlinear moving objects. • We evaluate SIRA on Radiate [47], a BEV radar dataset. Our approach achieves 58.1158.1158.1158.11 mAP@⁢0.5@0.5@0.5@ 0.5 for oriented object detection and 47.7947.7947.7947.79 MOTA for multiple object tracking on the Radiate dataset, surpassing previous state-of-the-art by a margin of +4.114.11+4.11+ 4.11 mAP@⁢0.5@0.5@0.5@ 0.5 and +9.949.94+9.94+ 9.94 MOTA, respectively."
https://arxiv.org/html/2411.02210v1,One VLM to Keep it Learning: Generation and Balancing for Data-free Continual Visual Question Answering,"Vision-Language Models (VLMs) have shown significant promise in Visual Question Answering (VQA) tasks by leveraging web-scale multimodal datasets. However, these models often struggle with continual learning due to catastrophic forgetting when adapting to new tasks. As an effective remedy to mitigate catastrophic forgetting, rehearsal strategy uses the data of past tasks upon learning new task. However, such strategy incurs the need of storing past data, which might not be feasible due to hardware constraints or privacy concerns. In this work, we propose the first data-free method that leverages the language generation capability of a VLM, instead of relying on external models, to produce pseudo-rehearsal data for addressing continual VQA. Our proposal, named as GaB, generates pseudo-rehearsal data by posing previous task questions on new task data. Yet, despite being effective, the distribution of generated questions skews towards the most frequently posed questions due to the limited and task-specific training data. To mitigate this issue, we introduce a pseudo-rehearsal balancing module that aligns the generated data towards the ground-truth data distribution using either the question meta-statistics or an unsupervised clustering method. We evaluate our proposed method on two recent benchmarks, i.e. VQACL-VQAv2 and CLOVE-function benchmarks. GaB outperforms all the data-free baselines with substantial improvement in maintaining VQA performance across evolving tasks, while being on-par with methods with access to the past data. Code will be made public soon.","Visual Question Answering (VQA) has been a prominent research challenge since its introduction a decade ago [4], aiming to mimic the real-world scenarios in which an agent can interpret and reason about the visual content, e.g. the object attributes, their relations, and commonsense [13, 16, 42, 59] through natural language questions. One of the main challenges of VQA is to bridge the multimodal gap as both visual and textual understanding are required, and their knowledge representation should be integrated. Recent Vision Language Models (VLMs), e.g., BLIP-2 [23] and LLaVa [26], have greatly enhanced such multimodal understanding thanks to their web-scale pre-training with image-text pairs. Figure 1: Key ideas. Top: we explore the language generation capability of VLMs to synthesize pseudo-rehearsal data of previous tasks to mitigate catastrophic forgetting in continual VQA. Bottom: as pseudo-rehearsal data tends to skew towards particular question types, we further propose a pseudo-rehearsal balancing module to align such skewed distribution towards the ground-truth meta-statistics, effectively improving the task performance while avoiding forgetting. VLMs in addressing the VQA task, have demonstrated a good generalization ability, yet they require continuous model updates to cope with new knowledge and to handle new types of reasoning skills [42, 56]. The main challenge in updating VQA models in such continual learning setup, a.k.a. VQACL [59], is to avoid the famous catastrophic forgetting problem, i.e., without hindering the performance of previously learned tasks upon learning new ones [48, 42, 59, 14, 15]. One common remedy to catastrophic forgetting is via data rehearsal, i.e., replay of the data samples of prior tasks during the model updates for new tasks [34, 8, 5]. However, such prior data might not be available due to privacy or proprietary concerns [47]. Can we address VQACL without the access to data samples of previous tasks? Prior methods leverage pseudo-rehearsal by generating data samples of prior task [22, 42, 38]. Such pseudo-rehearsal methods are capable of mitigating catastrophic forgetting in a data-free manner. However, existing methods leverage additional models to produce rehearsal samples [22, 38] or computationally demanding adversarial samples [42]. In the era of VLMs, can we exploit their generative language model to generate data samples? In this work, we explore this direction, instead of relying on additional models. We propose GaB, the first data-free method that leverages the language module of a VLM to address VQACL by generating question-answer pairs to mimic old tasks. Specifically, we generate questions and pseudo-answers about previous tasks by conditioning on the current visual data at hand. Crucially, the lack of diversity of questions related to the same task leads to generation collapse to data frequently occurring questions, inhibiting the VQA model capabilities (as shown in Fig. 1). To mitigate this problem, we balance the generated multimodal data: we initially learn to categorize the questions that are posed during learning new tasks and later categorize generated data accordingly. Hence, the rehearsal buffer is re-sampled ensuring that training data and generated question type distributions align. We evaluate GaB on continual learning VQA benchmarks, namely VQACL-VQAv2 [59] and CLOVE-function [22] and compare the proposed approach with respect to state-of-the-art and continual learning baselines. Results show a large margin improvement on all considered settings among data-free methods. We also provide thorough analysis for each proposed element and verify our proposed method can achieve real-data alike distribution in the Q&A generation, being effective with less memory buffer. To summarize, our contributions are three-fold: • We propose the first data-free method for VQACL, leveraging the language model inherent in VLMs to generate pseudo-rehearsal data, achieving the best performance among data-free methods on VQACL-VQAv2 [59] and CLOVE-function [22]. • We propose to jointly learn a task-specific projection module for generating question-answer pairs conditioned on images of the current task, to then synthesize data for the the rehearsal buffer. • We propose a novel and effective pseudo-rehearsal balancing module to mitigate the skewed distribution of the VLM-generated question types. Figure 2: Architecture of the proposed data-free method GaB for addressing VQACL. (a) At task t𝑡titalic_t, past task-specific projection heads fv→q⁢as,s=1⁢.⁣.⁢t−1subscriptsuperscript𝑓𝑠→𝑣𝑞𝑎𝑠1..𝑡1f^{s}_{v\to qa},s=1\mathinner{\ldotp\ldotp}t-1italic_f start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v → italic_q italic_a end_POSTSUBSCRIPT , italic_s = 1 start_ATOM . . end_ATOM italic_t - 1 are used to generate pseudo-rehearsal data with question-answer pairs about old tasks on current task images. (b) Pseudo-rehearsal samples undergo a balancing process through a module designed to ensure that under-represented question types are adequately represented. Finally (c) uses the pseudo-rehearsal data to mitigate the forgetting in the sequential learning scenario and data Dtsubscript𝐷𝑡D_{t}italic_D start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT to learn the current question-answer projector fv→q⁢atsubscriptsuperscript𝑓𝑡→𝑣𝑞𝑎f^{t}_{v\to qa}italic_f start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v → italic_q italic_a end_POSTSUBSCRIPT."
https://arxiv.org/html/2411.02188v3,Digi2Real: Bridging the Realism Gap in Synthetic Data Face Recognition via Foundation Models,"The accuracy of face recognition systems has improved significantly in the past few years, thanks to the large amount of data collected and the advancement in neural network architectures. However, these large-scale datasets are often collected without explicit consent, raising ethical and privacy concerns. To address this, there have been proposals to use synthetic datasets for training face recognition models. Yet, such models still rely on real data to train the generative models and generally exhibit inferior performance compared to those trained on real datasets. One of these datasets, DigiFace, uses a graphics pipeline to generate different identities and different intra-class variations without using real data in training the models. However, the performance of this approach is poor on face recognition benchmarks, possibly due to the lack of realism in the images generated from the graphics pipeline. In this work, we introduce a novel framework for realism transfer aimed at enhancing the realism of synthetically generated face images. Our method leverages the large-scale face foundation model, and we adapt the pipeline for realism enhancement. By integrating the controllable aspects of the graphics pipeline with our realism enhancement technique, we generate a large amount of realistic variations— combining the advantages of both approaches. Our empirical evaluations demonstrate that models trained using our enhanced dataset significantly improve the performance of face recognition systems over the baseline. The source code and datasets will be made available publicly: https://www.idiap.ch/paper/digi2real","Face recognition (FR) technology has seen widespread adoption due to its accuracy and ease of use. This high performance could be attributed to the advancements in deep learning architectures, margin-based loss functions [6, 12], and large-scale public datasets [35, 9]. However, the acquisition of many of these datasets often occurs without explicit user consent, posing significant legal and ethical challenges, particularly in light of the European Union’s General Data Protection Regulation (GDPR). As a result of these regulations, several datasets have been withdrawn, complicating the availability of training data. Consequently, there is a growing interest in generating high-performance synthetic face recognition datasets for training face recognition models as evidenced in public competitions on this topic [16, 19, 5, 17]. Over the past few years, several synthetic face datasets have been introduced for training face recognition models. Most of them rely on generative models like StyleGAN [11] and Diffusion Models [24]. These models are often trained on real datasets such as FFHQ [11] to model the distribution of faces. Most of them also use pretrained face recognition networks in the generation pipeline to induce the notion of identity in the sampling process. However, these datasets suffer from issues like a limited number of identities and limited intra-class variation. Preserving the identity while generating intra-class variations is another challenging issue here as these are sampled from a continuous latent space. These have resulted in reduced accuracy of face recognition models trained with these synthetic datasets, which limits the practical use of these models as the performance is much worse than those trained using real data. It is also to be noted that the diversity and accuracy of these models could vary based on the training data used to train the generative model in the pipeline. DigiFace-1M [1] presents an alternative to generative models for data creation, utilizing a graphics rendering pipeline to produce images without needing large-scale real images or a face recognition network to produce these images. This method leverages the rendering pipeline described in [31] to combine facial geometry, texture, and hairstyle. It facilitates the generation of intra-class variations by simply rendering additional images with varying poses, expressions, lighting, and accessories. This approach holds significant potential for creating a large number of identities with diverse intra-class variations and ethnicities. Interestingly, it also allows for controlled generation processes by selecting specific attributes. However, the major limitation of this approach is the lack of realism of the generated samples, which results in poor face recognition performance for the models trained using this dataset. The computational requirement to produce these samples is another constraint. In our proposed method, we introduce a novel approach to enhance the realism of the procedurally generated DigiFace dataset. By reusing the existing DigiFace samples as our source dataset, we obviate the need for recomputation and demonstrate through empirical evidence that our approach substantially boosts performance. Our approach significantly improves the face recognition performance compared to DigiFace and achieves comparable performance to other state-of-the-art synthetic data methods. The main contributions of this work are listed below: • We introduce a novel framework for generating realistic sample images from those generated by a graphics pipeline. • We introduce a new synthetic face recognition dataset called Digi2Real with a large number of identities and intra-class variations • We conduct a thorough analysis using the generated dataset, demonstrating the advantages of our approach. Additionally, the dataset and the code necessary to reproduce the results will be made publicly available. Figure 2: The images on the left show variations of the one identity from the DigiFace dataset, and the images on the right show the realism-enhanced images generated from our approach"
https://arxiv.org/html/2411.02179v1,CleAR: Robust Context-Guided Generative Lighting Estimation for Mobile Augmented Reality,"High-quality environment lighting is the foundation of creating immersive user experiences in mobile augmented reality (AR) applications. However, achieving visually coherent environment lighting estimation for Mobile AR is challenging due to several key limitations associated with AR device sensing capabilities, including limitations in device camera FoV and pixel dynamic ranges. Recent advancements in generative AI, which can generate high-quality images from different types of prompts, including texts and images, present a potential solution for high-quality lighting estimation. Still, to effectively use generative image diffusion models, we must address their key limitations of generation hallucination and slow inference process. To do so, in this work, we design and implement a generative lighting estimation system called CleAR that can produce high-quality and diverse environment maps in the format of 360∘superscript360360^{\circ}360 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT images. Specifically, we design a two-step generation pipeline guided by AR environment context data to ensure the results follow physical environment visual context and color appearances. To improve the estimation robustness under different lighting conditions, we design a real-time refinement component to adjust lighting estimation results on AR devices. To train and test our generative models, we curate a large-scale environment lighting estimation dataset with diverse lighting conditions. Through quantitative evaluation and user study, we show that CleAR outperforms state-of-the-art lighting estimation methods on both estimation accuracy and robustness. In particular, CleAR achieves 56% - 51% (an average of 53%) accuracy improvement on virtual object renderings across objects with three distinctive types of materials over different reflective properties. Moreover, CleAR supports real-time refinement of lighting estimation results, ensuring robust and timely environment lighting updates for AR applications. Our end-to-end generative estimation takes as fast as 3.2 seconds, outperforming state-of-the-art methods by 110×\times×.","As new augmented reality (AR) hardware and software enter consumer markets, mobile AR technologies have positively impacted various industries, including e-commerce, education, and engineering (Chylinski et al., 2020; Rauschnabel et al., 2019). The growing public adoption of AR technologies demands new standards for content quality and application user experiences, particularly emphasizing the need for visual coherency between virtual and physical content to ensure high-quality user experiences. To create visual coherency, AR applications require an accurate and robust environment lighting estimation, which ensures that virtual objects blend naturally with the physical environment. We define the lighting estimation task for AR systems as estimating a complete environment map (a 360∘ HDR image) from partial observation of the environment (an LDR image with limited FoV) in real-time. This task is critical for supporting three key aspects of photorealistic rendering in real-time AR systems. (i) Reflective object rendering, which requires a complete environment map with visually coherent details. (ii) Highlights and shadows rendering, which requires HDR pixel information in environment maps. (iii) Temporally consistent visual coherency, which requires the system to robustly adapt to changing environmental lighting in real-time. To address this challenging task, traditional systems adopt autoregressive models (Zhao and Guo, 2021). These models extract low-frequency information based on camera input (shown in Fig. 1a). While these methods are able to capture coarse information about environmental lighting conditions, high-frequency details are missing. These details are important for creating a visually coherent AR system. To obtain the details, recent works (Somanath and Kurz, 2021; Yang et al., 2023; Wang et al., 2022) leverage the advancement of controllable generative models. These models have the potential to extract fine-grained environment details (shown in Fig. 1a), empowering AR systems with a better rendering effect. While generative-model-based methods seem promising, we identify two key challenges in using them on mobile AR systems. a Autoregressive Lighting Estimation b Generative Lighting Estimation Figure 1. Comparison between autoregressive and generative lighting estimation methods. (1a) An autoregressive lighting estimation system, Xihe (Zhao and Guo, 2021), estimates omnidirectional low-frequency lighting information from camera images with autoregressive models. The low-frequency lighting information misses important visual details, as visualized in the example environment map. (1b) Generative lighting estimation models can create high-frequency environment lighting estimation results from limited environment observations. The estimation process can be conditioned in several ways, such as partial environment observations and text prompts. First, robust lighting estimation demands accurate estimation under challenging lighting conditions. However, through a measurement study on existing datasets, we found that existing lighting estimation datasets have several distribution biases in key lighting properties, including light intensity and color temperatures. In further tests, we found that these data biases will affect the generalization and robustness of multiple recent lighting estimation models (Wang et al., 2022; Akimoto et al., 2022). Therefore, data diversity and fairness must be ensured when training and testing generative lighting estimation systems. Second, interactive mobile AR applications demand timely updates of lighting estimation results. However, generative models usually experience long inference latency. Without system latency optimization, naively integrating generative lighting estimation models into AR applications can easily violate the temporal constituency of immersive user experiences. In this paper, we address the above issues with CleAR, an edge-assisted novel generative lighting estimation system for mobile AR. To tackle AR device limitations in observation capability, we first design a two-step generative pipeline that estimates 360∘ HDR environment maps from partial LDR environment observations. Our key design insight is to separate the generative model training objective into two domains: LDR environment map completion and high-intensity pixel value estimation. This novel learning objective design addresses the practical challenges of the scarcity of high-quality lighting estimation training data, and allows us to leverage pre-trained large model to tackle each generation step effectively. On top of the two-stepped generative pipeline, CleAR uses controlling signals extracted from AR context data to ensure the estimated environment map aligns with the environment of physical AR devices. Specifically, CleAR uses environment semantic maps, which control the environment map visual details, and device ambient light sensor data, which informs the lighting intensity and color temperatures during generation. Specifically, this information is used as image and text inputs for ControlNet models. This design helps the generative pipeline to handle the significant information increases during environment map generation. Generative model inference latency is high, even on the edge servers. Therefore, we design an edge-device collaborative estimation system architecture with on-device refinement components to adjust edge estimation results to real-time lighting conditions. Specifically, we design a multi-output estimation strategy with an on-device adaptive output selection component. The estimation strategy is configured with the optimal number of generation outputs based on our performance measurement results. Additionally, CleAR uses a color appearance matching technique to efficiently and effectively adjust edge estimation results, even for challenging environment lighting conditions. To evaluate the lighting estimation quality of CleAR, we integrate CleAR into a mobile AR application using Unity, Python, and a recent AR data streaming framework ARFlow (Zhao and Guo, 2024). We compare CleAR’s virtual object rendering quality to ones using environment lighting acquired from three representative baselines: unwrapping a mirror ball (physical reference) (Debevec, 2006), ARKit (Apple, 2022) (commercial), and LitAR (Zhao et al., 2022) (academic). Our evaluation shows that CleAR can generate environment maps with better image details to support more visually coherent virtual object rendering. To quantitatively understand CleAR’s performance, we evaluate CleAR with standard testing dataset (Phongthawee et al., 2023) and compare with state-of-the-art lighting estimation models (Phongthawee et al., 2023; Wang et al., 2022; Akimoto et al., 2022; Gardner et al., 2017). Our evaluation shows that CleAR outperforms DiffusionLight (Phongthawee et al., 2023) by up to 56% using the three-sphere evaluation protocol (Wang et al., 2022). Also, notably, CleAR achieves 110X estimation latency reduction compared to DiffusionLight due to our efficient generative pipeline and generation control design. Furthermore, we verify the robustness of CleAR under different lighting conditions by testing CleAR on an augmented Laval dataset with diverse lighting conditions of light intensity and color temperatures. Qualitatively, our user study also confirms the effectiveness and robustness of CleAR, which shows at least 12% higher quality ratings. Related works on mobile AR lighting estimation systems seek to extract environment information from physical light probes (Prakash et al., 2019), user dynamics, and learning-based solutions (Zhao and Guo, 2021; Gardner et al., 2019; Wang et al., 2022; Yang et al., 2023). While physical light probes provide the most comprehensive environment observations, their use in practical applications is typically constrained due to the need for physical light probe presence. While environment lighting can also be extracted from dynamic environment observations, the estimation is usually incomplete as AR devices often do not have comprehensive environment observations. In contrast, leveraging learned models to estimate environment lighting from AR device camera images is a more feasible solution for AR applications. Over the past couple of decades, learning-based methods have evolved from discovering scene lighting cues from image details, such as highlights and shadows (Yu et al., 1999), to regress omnidirectional environment lighting representations (Zhao and Guo, 2020, 2021; Gardner et al., 2019). However, autoregressive models cannot effectively tackle the environment information generation in lighting estimation. For example, Xihe (Zhao and Guo, 2021) provides real-time low-frequency lighting estimation to AR applications but fails to support detailed environment reflections on object rendering. More recently, a new opportunity has arisen as generative models can support highly detailed image content for environment map estimation (Wang et al., 2022; Yang et al., 2023). In this work, we focus on novel AR system integrations with image-generative models to provide high-quality environment lighting estimation. We specifically focus on adapting generative lighting estimation models to achieve robust estimation under challenging environmental lighting conditions. We summarize our main contribution as the following: • We design and implement a novel AR context-guided generative lighting estimation system, CleAR. Our design leverages generative models to tackle the environmental observation limitation of AR devices. Specifically, our system uses a novel two-step generative lighting estimation pipeline to estimate an accurate environment map with visual details that match the AR user’s physical environment. To train the generative models, we craft a large-scale lighting estimation dataset with approximately 30K data items. • We present a measurement study on recent lighting estimation datasets to understand the complexity of environmental lighting conditions and explore the challenges of achieving robust lighting estimation. With the insights from this study, we design and conduct lighting estimation robustness testing experiments to evaluate several lighting estimation systems, including ours. • We develop two real-time estimation refinement techniques to improve the estimation quality of our system. Our first refinement component can automatically select the best environment map from generated results to match the current lighting conditions. Our second refinement component can match the color appearances between generated environment maps with real-time camera view to improve estimation robustness. • We implement CleAR as an end-to-end edge-assisted framework that can be integrated into third-party AR applications. We demonstrate the integration with an example object placement-based AR application. We will provide the dataset, system source code, and demo application links once the paper is accepted for publication to encourage follow-up research. • We conduct comprehensive experiments to evaluate the effectiveness of CleAR. Specifically, we tested CleAR with SoTA lighting estimation systems on standard testing datasets and our robustness testing dataset. We also conduct a quality assessment study with human perception feedback on the lighting estimation results. Overall, the CleAR rating score ranked the top with 12% higher than the second best method. Additionally, the overall standard deviation of the rating values for CleAR is also lower than the second best method (1.67) by 7%."
https://arxiv.org/html/2411.02149v2,Improving Domain Generalization in Self-Supervised Monocular Depth Estimation via Stabilized Adversarial Training,"Learning a self-supervised Monocular Depth Estimation (MDE) model with great generalization remains significantly challenging. Despite the success of adversarial augmentation in the supervised learning generalization, naively incorporating it into self-supervised MDE models potentially causes over-regularization, suffering from severe performance degradation. In this paper, we conduct qualitative analysis and illuminate the main causes: (i) inherent sensitivity in the UNet-alike depth network and (ii) dual optimization conflict caused by over-regularization. To tackle these issues, we propose a general adversarial training framework, named Stabilized Conflict-optimization Adversarial Training (SCAT), integrating adversarial data augmentation into self-supervised MDE methods to achieve a balance between stability and generalization. Specifically, we devise an effective scaling depth network that tunes the coefficients of long skip connection and effectively stabilizes the training process. Then, we propose a conflict gradient surgery strategy, which progressively integrates the adversarial gradient and optimizes the model toward a conflict-free direction. Extensive experiments on five benchmarks demonstrate that SCAT can achieve state-of-the-art performance and significantly improve the generalization capability of existing self-supervised MDE methods.","Monocular depth estimation (MDE) plays an important role in various 3D perceptional fields such as robotic navigation [17], autonomous driving [16] and 3D reconstruction [30]. However, due to the dynamic nature of the real world, even minor perturbations in the environment can result in significant domain shifts in the visual observations, which makes the trained model hard to generalize into unseen scenarios and restricts its application in the physical world. To improve the generalization capability, several studies [28, 19, 66] utilize data augmentation methods to generate synthetic data and diversify the training environments, yielding considerable performance improvements. However, existing methods such as [19] mostly select some specific data augmentation schemes for target scenarios, yielding poor generalization performance in environments varying far from the augmented images. Compared with above mentioned offline data augmentation, adversarial data augmentation (ADA) does not make any target distribution assumption and synchronously optimizes the augmenter during the training phase, providing a promising pre-processing solution. Unfortunately, there is a dilemma in naively incorporating ADA into self-supervised MDE. Although adversarial data augmentation can effectively improve generalization capability in multiple supervised visual tasks [19], self-supervised MDE algorithms are quite sensitive to such excessive perturbation, resulting in significant performance degradation and training collapse. Therefore, it is necessary to rethink why self-supervised MDE cannot benefit from ADA as much as supervised learning. Figure 1: Visualization of the domain generalization over various methods.(a) Comparisons with offline scenario-specific data augmentation methods.(b) Comparisons with vanilla Gaussian noise and vanilla adversarial data augmentation.(c) Comparisons of different LSCs scaling factors. The results show that our SCAT has excellent generalization performance under multiple unseen domains and retains the performance on the original training set. In this work, we first conduct extensive quantitative analysis to investigate the causes of performance degradation when applying adversarial data augmentation to common self-supervised MDE models. There are two primary factors for this phenomenon: (i) inherent sensitivity of long skip connections (LSC) in UNet-alike depth estimation networks; (ii) dual optimization conflict caused by over-regularization. Specifically, as a core component in the MDE, LSC is widely adopted in the UNet-alike Depth Networks to combine multi-scale features and preserve low-level detail, yielding better prediction performance. However, the presence of these shortcut connections amplifies adversarial gradients, when coupled with pixel-level adversarial data augmentation, leading to severe training instability and collapse. Moreover, compared to offline data augmentation, adversarial augmentation data commonly act as the worst-case training examples [36, 37] and often provides over-regularization for the model training, which results in optimization gradients that oppose those of the original data, leading to severe gradient conflicts and a subsequent decline in convergence performance. To tackle these issues, we propose a general adversarial training framework, named Stabilized Conflict-optimization Adversarial Training (SCAT), tailored for self-supervised monocular depth estimation. SCAT is designed to enhance both generalization capability and stability. In particular, we develop an innovative scaling depth network that adjusts the coefficients of long skip connections within UNet architecture, thereby ensuring a more stable training regime through a theoretically supported. Furthermore, we advance a Conflict Gradient Surgery method, progressively blending the adversarial gradient to guide the model optimization along a direction devoid of conflicts. It is worth noting that one of the merits of our method SCAT is model-agnostic, and the stabilized adversarial training can be applied to diverse SOTA methods, yielding consistent generalization enhancement. To validate the effectiveness of SCAT, we conduct extensive experiments on both KITTI and KITTI-C datasets, demonstrating its cross-domain generalization capability on Foggy CityScapes, DrivingStereo and NuScenes datasets. In summary, our contributions are summarized as follows: ∙∙\bullet∙ We point out the inherent sensitivity of UNet-based depth networks and the dual optimization conflict caused by over-regularization, providing a theoretical analysis of the self-supervised MDE model’s instability. ∙∙\bullet∙ We propose a general adversarial training framework named Stabilized Conflict-optimization Adversarial Training (SCAT), which efficiently incorporates adversarial augmentation into self-supervised MDE, and significantly improves the model generalization capabilities across multiple unseen domains. ∙∙\bullet∙ We developed Conflict Gradient Surgery (CGS) to address the dual optimization conflicts induced by adversarial over-regularization, achieving a balance between model stability and generalization."
https://arxiv.org/html/2411.02136v1,Advanced computer vision for extracting georeferenced vehicle trajectories from drone imagery11footnotemark:1,"This paper presents a comprehensive framework for extracting georeferenced vehicle trajectories from high-altitude drone footage, addressing key challenges in urban traffic monitoring and limitations of traditional ground-based systems. Our approach employs state-of-the-art computer vision techniques and deep learning models to create an end-to-end pipeline that significantly enhances vehicle detection, tracking, and trajectory stabilization. Conducted in the Songdo International Business District, South Korea, the study utilized a multi-drone experiment over 20 intersections, capturing approximately 12TB of ultra-high-definition video data over four days. We developed a novel track stabilization method that uses detected vehicle bounding boxes as exclusion masks during image registration, which, combined with advanced georeferencing techniques, accurately transforms vehicle coordinates into real-world geographical data. Additionally, our framework includes robust vehicle dimension estimation and detailed road segmentation, allowing for an in-depth analysis of traffic dynamics. The framework produced two high-quality datasets: the Songdo Traffic dataset, comprising nearly 1 million unique vehicle trajectories, and the Songdo Vision dataset, containing over 5,000 human-annotated frames with about 300,000 vehicle instances in four classes. Comparisons between drone-derived data and high-precision sensor data from an instrumented probe vehicle highlight the accuracy and consistency of our framework’s extraction in dense urban settings. By publicly releasing these datasets and the pipeline source code, this work sets new benchmarks for data quality, reproducibility, and scalability in traffic research. The results demonstrate the potential of integrating drone technology with advanced computer vision tools for precise and cost-effective urban traffic monitoring, providing valuable resources for the research community to develop intelligent transportation systems and improve traffic management strategies.","Traditional traffic monitoring methods, such as loop detectors and manual counting, are becoming obsolete in the era of smart cities due to their inflexibility and limited scope [1]. Stationary camera surveillance, although effective, presents economic challenges due to high installation costs and limited field of view (FoV), leading to blind spots. Data from global navigation satellite system (GNSS) and connected vehicles also lack the versatility needed for comprehensive traffic monitoring. Several vehicle detection and tracking technologies have emerged, including camera-based [2, 3, 4], LiDAR-based [5, 6], magnetics-based [7, 8], and radar-based [9, 10] systems. While each offers certain advantages, they still face limitations in adaptability and coverage. Accurate estimation of traffic states allows for adjustments in traffic control strategies [11], enhancing network utilization [12, 13], improving road user experiences, and yielding economic and ecological benefits. The integration of unmanned aerial vehicles, commonly known as drones, and computer vision (CV) techniques presents a promising solution to these challenges. Originally developed for military purposes [14], UAVs have since found applications in various civilian domains, including urban traffic monitoring and management [15, 16, 17]. Advances in drone technology, combined with sophisticated CV tools, enable the extraction of detailed vehicle trajectories from high-altitude drone footage, facilitating spatio-temporal analyses of dynamic entities such as vehicles, pedestrians, and cyclists [18, 19, 20]. Compared to traditional sensors, these trajectories offer new opportunities for studying complex traffic phenomena, such as large-scale network modeling [21], on-street parking [22], as well as emission [23] and noise [24] estimation. This aerial perspective, free from the constraints of fixed infrastructure, provides an adaptable and cost-effective means to observe urban mobility patterns in congested environments, thereby supporting responsive traffic management strategies in smart cities [25, 26, 27]. In a collaborative effort between KAIST and EPFL, a unique multi-drone experiment was conducted in the Songdo International Business District, South Korea, from October 4 to 7, 2022, excluding Monday due to adverse weather conditions. A fleet of 10 commercial-off-the-shelf drones synchronized their operations to monitor 20 busy intersections, as depicted in Fig. 1. Advanced flight plans, combining both hovering and transitioning phases, were executed to optimize coverage. The drones maintained an altitude of 150 meters, with some reduced to 140 meters to mitigate collision risks. Adopting a bird’s-eye view (BEV) perspective, the drones captured footage at 4K ultra-high-definition (UHD) resolution (3,840×2,160384021603{,}840\times 2{,}1603 , 840 × 2 , 160 pixels) at a smooth frame rate of 29.97 frames per second (FPS). Each day included 10 synchronized flight sessions per drone during peak morning and afternoon periods, each session lasting approximately 30 minutes and followed by a brief battery replacement. This experiment generated 12TB of raw video data and utilized 13 real-time kinematic (RTK)-calibrated ground control points for precise georeferencing. A dedicated drone captured overlapping images at an altitude of 75 meters to create a highly accurate orthophoto of the experiment area, providing a solid foundation for advanced transportation analysis. Figure 1: Left: Map of the Songdo International Business District, indicating the locations of the 20 monitored intersections labeled A to U (excluding D, which denotes the drones). Right: Images of the 20 intersections as seen by our DJI Mavic 3 drones at 4K resolution. This work introduces a comprehensive end-to-end trajectory extraction pipeline that employs advanced CV algorithms to derive detailed traffic data from high-altitude drone footage. Although a detailed experiment design and an in-depth analysis of the extracted data are outside this paper’s scope, our contributions significantly expand the resources available to the research community. Specifically, we introduce the Songdo Traffic dataset [28], comprising nearly 1 million unique vehicle trajectories, making it one of the largest public traffic datasets collected from a smart city worldwide. This extensive collection provides unparalleled insights into urban mobility, serving as a valuable resource for traffic management and research. The dataset includes trajectory data at a high frequency of 29.97 points per second, complemented by various metadata. Songdo Traffic encompasses vehicle trajectories in multiple coordinate systems, including global geographic, local Cartesian, and orthophoto CS. Each trajectory is further annotated with metadata such as vehicle identifier (ID), type, and, when feasible, vehicle dimension estimates. Additionally, each trajectory point is tagged with a precise ISO-formatted timestamp, along with instantaneous speed and acceleration estimates, road section and lane number information, and a visibility flag, where available. Complementing Songdo Traffic, the Songdo Vision dataset [29] addresses the scarcity of detailed vehicle annotations necessary for training object detection models, particularly from high-altitude BEV aerial perspectives, where smaller vehicles, like motorcycles, are often less discernible. This dataset contains over 5,000 human-annotated video frames, featuring nearly 300,000 vehicle instances across four distinct classes: cars (including vans and light-duty vehicles), buses, trucks, and motorcycles. To facilitate broader use, annotations are provided in multiple formats compatible with various deep learning (DL) models, making the Songdo Vision dataset a valuable resource for improving and validating vehicle detection algorithms. Our commitment to open science drives us to release these datasets [28, 29] alongside the source codes [30, 31, 32] for our extraction and stabilization routines. These resources are poised to set new benchmarks for data quality and reproducibility in traffic research that leverages CV and DL techniques. Our extraction methodology introduces several innovative techniques that significantly enhance the accuracy and reliability of vehicle trajectory data from drone imagery. Firstly, we trained a highly accurate object detector using a state-of-the-art (SOTA) model architecture with the Songdo Vision dataset, supplemented by eight curated public datasets, resulting in superior detection performance as validated on the Songdo Vision test set. Secondly, to address the challenges of drone movements, we developed a novel track stabilization method that uses detected vehicle bounding boxes as no-feature masks during image registration, thereby improving stabilization accuracy and computational efficiency. Additionally, we introduce a robust vehicle dimension estimation that uses azimuth-based filtering and ratio-based criteria to ensure accurate measurements under varying conditions. Our methodology further includes comprehensive speed and acceleration computation, incorporating visibility filtering and Gaussian smoothing to derive realistic traffic dynamics. Advanced georeferencing techniques, employing a high-resolution orthophoto enhanced with 13 ground control points, ensure precise mapping of trajectories into real-world coordinates. We optimized the orthophoto creation and image registration through an extensive parameter search, resulting in highly accurate geospatial data for traffic analysis. During the Songdo experiment, various traffic events, including severe congestion and post-accident traffic dynamics, were captured alongside an instrumented autonomous vehicle (AV) from Stanford University equipped with high-precision real-time kinematic GNSS (RTK-GNSS) sensors. Although the collision itself occurred during a battery swap and was not recorded, the prolonged presence of the involved vehicles allowed us to capture the resulting traffic disruptions over time. The AV data provided a valuable resource for comparison with our drone-derived trajectories and speed profiles, demonstrating very high consistency between the two measurement methods. Furthermore, our methodology underwent rigorous hyperparameter tuning for track stabilization and a series of benchmarks to assess image registration and georeferencing accuracy. For vehicle dimension estimation, our results closely aligned with known dimensions of specific vehicles, such as the AV and recognizable bus types, confirming the method’s capability to produce realistic distributions. By addressing key challenges in drone-based traffic monitoring through CV, as discussed in Section 2, this research presents a comprehensive end-to-end framework for extracting georeferenced trajectories from high-altitude drone footage. We enhance detection and tracking algorithms to handle unique perspectives and dynamic conditions, surpassing traditional ground-based methods. Our innovative track stabilization, accurate vehicle dimension estimation, speed and acceleration computations, and precise georeferencing contribute to the extraction of high-quality trajectory data. Comparison with AV data shows that our methods produce realistic speed, acceleration, and dimension distributions. By disseminating these methods through open-source datasets and code, we establish a new resource for data quality, facilitating more responsive, scalable, and cost-effective traffic management solutions in smart cities."
https://arxiv.org/html/2411.02112v1,Multi-modal biometric authentication: Leveraging shared layer architectures for enhanced security,"In this study, we introduce a novel multi-modal biometric authentication system that integrates facial, vocal, and signature data to enhance security measures. Utilizing a combination of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), our model architecture uniquely incorporates dual shared layers alongside modality-specific enhancements for comprehensive feature extraction. The system undergoes rigorous training with a joint loss function, optimizing for accuracy across diverse biometric inputs. Feature-level fusion via Principal Component Analysis (PCA) and classification through Gradient Boosting Machines (GBM) further refine the authentication process. Our approach demonstrates significant improvements in authentication accuracy and robustness, paving the way for advanced secure identity verification solutions.","In the advent of an increasingly digital and interconnected world, the security of personal and organizational data has risen to the forefront of technological innovation and research. Within this context, biometric authentication systems, which leverage unique physiological and behavioral characteristics for identity verification, have emerged as a critical component in enhancing security infrastructures. Despite their growing ubiquity and importance, traditional biometric systems often grapple with challenges related to accuracy, privacy, and susceptibility to sophisticated attacks. To address these challenges, advancements in machine learning (ML) models have provided promising avenues for developing more secure, accurate, and efficient biometric authentication methods. This paper introduces a novel approach to biometric authentication by proposing a multi-modal system that integrates facial images, voice recordings, and signature data, leveraging the inherent strengths of each modality. Our research is driven by the hypothesis that a fusion of multiple biometric modalities, processed through a carefully architected network of shared and modality-specific layers, can significantly enhance the robustness and reliability of authentication systems. The shared layers, utilizing Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), are designed to extract and learn from the common features across modalities, while the modality-specific layers focus on the unique attributes of each biometric trait. This dual approach not only broadens the system’s ability to accommodate a wider variance in biometric data but also reduces the bias inherent in single-modality systems. However, integrating diverse biometric data introduces complexities in ensuring data privacy and security, necessitating advanced solutions to protect against potential vulnerabilities. To this end, our model incorporates sophisticated data preprocessing and feature integration techniques, including Principal Component Analysis (PCA) for dimensionality reduction and Gradient Boosting Machines (GBM) for classification, enhancing the system’s capability to thwart security threats effectively. Figure 1 offers a visual representation of our innovative architecture, emphasizing a dual-layer strategy that harnesses both shared and modality-specific layers to process and analyze facial images, voice recordings, and signature data. The system’s shared layers employ CNNs to analyze spatial attributes and RNNs to manage temporal data, facilitating comprehensive feature extraction across modalities. This structure allows the system to capture a rich set of biometric features, enhancing its capacity to recognize and authenticate identities with high accuracy. Following the shared processing, modality-specific layers are dedicated to refining these features further, with tailored CNNs focusing on detailed aspects of facial and signature recognition, and specific RNN configurations handling the dynamic elements of voice and signature sequences. This tailored approach ensures that each biometric characteristic’s unique properties are effectively captured and integrated, reinforcing the system’s robustness against various forms of spoofing and falsification. Furthermore, our sophisticated preprocessing routines, including PCA for reducing dimensionality and GBM classifiers for effective threat detection and response, equip the system with the necessary tools to offer enhanced security and privacy assurances, preparing it to handle the complexities of modern biometric authentication challenges effectively. The primary contributions of our research are threefold and pivot around the novel shared layer mechanism. • Firstly, the introduction of this dual shared layer architecture represents a significant advancement in the field of biometric authentication, enabling a more sophisticated and nuanced analysis of multi-modal biometric data. • Secondly, the integration of modality-specific layers alongside the shared layers enhances the system’s ability to accurately identify individuals by accommodating a broader spectrum of variability and reducing bias. • Thirdly, the fusion of learning from both the shared and modality-specific layers into a cohesive authentication decision mechanism marks a pioneering step towards achieving higher accuracy and security in biometric authentication systems. Figure 1: Architectural Diagram We organize the paper as follows. Section II discusses the related work and motivating factors for our proposed method. Section III outlines the methodology, including data collection and preprocessing, design of the dual shared layer architecture with modality-specific enhancements, and the training process integrating both shared and modality-specific layers. Section IV provides experimental results and performance comparisons with legacy systems, demonstrating the effectiveness of our approach. Lastly, Section V concludes with insights into the future work inspired by the outcomes of the current research."
https://arxiv.org/html/2411.02099v2,Differentially Private Integrated Decision Gradients (IDG-DP) for Radar-based Human Activity Recognition,"Human motion analysis offers significant potential for healthcare monitoring and early detection of diseases. The advent of radar-based sensing systems has captured the spotlight for they are able to operate without physical contact and they can integrate with pre-existing Wi-Fi networks. They are also seen as less privacy-invasive compared to camera-based systems. However, recent research has shown high accuracy in recognizing subjects or gender from radar gait patterns, raising privacy concerns. This study addresses these issues by investigating privacy vulnerabilities in radar-based Human Activity Recognition (HAR) systems and proposing a novel method for privacy preservation using Differential Privacy (DP) driven by attributions derived with Integrated Decision Gradient (IDG) algorithm. We investigate Black-box Membership Inference Attack (MIA) Models in HAR settings across various levels of attacker-accessible information. We extensively evaluated the effectiveness of the proposed IDG-DP method by designing a CNN-based HAR model and rigorously assessing its resilience against MIAs. Experimental results demonstrate the potential of IDG-DP in mitigating privacy attacks while maintaining utility across all settings, particularly excelling against label-only and shadow model black-box MIA attacks. This work represents a crucial step towards balancing the need for effective radar-based HAR with robust privacy protection in healthcare environments.","Demographic projections indicate that the worldwide population of individuals aged 60 and above will surge to approximately 1.4 billion people by 2030 [65], leading to an increased reliance on advanced healthcare monitoring technologies in patients’ homes [23]. Human motion analysis shows significant potential for healthcare monitoring and early disease detection [14, 13]. Deep Neural Networks (DNNs) have exhibited remarkable efficacy in Human Activity Recognition (HAR) for monitoring patients and detecting abnormalities [24, 1]. Especially, in home settings these technologies can play a key role in preventive and proactive healthcare strategies by enabling personalised care systems [84, 23]. However, while DNNs excel at encoding input features, this capability also renders them vulnerable to privacy breaches [53, 39]. For instance, Membership Inference Attacks (MIA) [69, 35, 74] and Model Inversion (MI) attacks [27] can disclose private information about the patients and their training data by accessing only the pre-trained model outputs. To mitigate such risks, privacy-preserving techniques needs to be incorporated into HAR systems to safeguard sensitive user information. Differential Privacy (DP) is a commonly used technique to enhance privacy in deep learning models [8, 22]. DP quantifies the risk of an individual’s information being disclosed by ensuring that the model’s output is not significantly affected by the inclusion or exclusion of any single individual’s data. The implementation of DP involves introducing controlled perturbations to the data. Recently, human motion sensing with radar emerges as a prominent sensing technology for continuous monitoring thanks to its non-intrusive nature [25]. This makes it suitable for privacy-sensitive environments such as assisted-living facilities, hospitals and homes. Nevertheless, recent studies have revealed high accuracy in subject recognition from radar human gait patterns, challenging the perception of privacy in radar-based systems [50, 51]. This underscores the need for implementing privacy safeguards in human motion sensing systems, regardless of whether the output is visually identifiable by humans. Previous research in human motion analysis has primarily focused on privacy preservation of Red Green and Blue (RGB) videos on a frame by frame basis [21]. Some methods use anonymisation techniques by replacing the face or whole body with synthetic data[33], while other approaches use obfuscation of sensitive attributes [82]. In most cases, the utility of the data is severely compromised, hindering the application of these methods in healthcare. More recently, it has been recognised that other human motion tracking modalities such as accelerometer, gyroscope sensor data and radar data also record sensitive information that can help identify users and track them during their daily activities [22, 37, 50, 51]. However, to our knowledge, there is no systematic work modeling threats and evaluating the robustness of privacy preservation techniques under sophisticated machine learning (ML)-driven attacks in radar-based systems. Here we firstly define black-box MIA that are relevant to HAR setting with radar data. We assume that the adversary has access to the logit space of the model and it is also possible to gain partial access to the training data or their underlying distribution. In these ways, the adversary can orchestrate attacks to identify individuals. Subsequently, we propose a novel method, named IDG-DP, based on DP and the Integrated Decision Gradient (IDG) attribution algorithm [80]. The IDG was selected for its ability to compute attributions precisely at the model’s decision points, making it a superior attribution algorithm with enhanced performance [80]. IDG-DP injects more noise in input features that contribute more towards the subject identification than to the activity recognition and thus it preserves privacy, while it maintains high performance in HAR. Our paper presents the following contributions: 1. To our knowledge, we are the first to systematically investigate state-of-the-art threat models that are relevant to HAR in a home setting with radar technology. This enables the identification of sensitive information leakage that is not perceptible to a human observer. 2. We introduce a novel methodology that drives DP by identifying the model’s highest attributions during training. In this way, we achieve a better balance between data utility and privacy preservation. 3. We devise a rigorous evaluation strategy of the mitigation capabilities of the proposed approach against black-box MIA attacks. We exploit a publicly available dataset on HAR with radar data [20] to assess the effectiveness of the proposed IDG-DP privacy method under various black-box MIA attacks. We demonstrate promising results in balancing data utility and privacy in the data."
https://arxiv.org/html/2411.02074v1,GraphVL: Graph-Enhanced Semantic Modeling via Vision-Language Models for Generalized Class Discovery,"Generalized Category Discovery (GCD) aims to cluster unlabeled images into known and novel categories using labeled images from known classes. To address the challenge of transferring features from known to unknown classes while mitigating model bias, we introduce GraphVL, a novel approach for vision-language modeling in GCD, leveraging CLIP. Our method integrates a graph convolutional network (GCN) with CLIP’s text encoder to preserve class neighborhood structure. We also employ a lightweight visual projector for image data, ensuring discriminative features through margin-based contrastive losses for image-text mapping. This neighborhood preservation criterion effectively regulates the semantic space, making it less sensitive to known classes. Additionally, we learn textual prompts from known classes and align them to create a more contextually meaningful semantic feature space for the GCN layer using a contextual similarity loss. Finally, we represent unlabeled samples based on their semantic distance to class prompts from the GCN, enabling semi-supervised clustering for class discovery and minimizing errors. Our experiments on seven benchmark datasets consistently demonstrate the superiority of GraphVL when integrated with the CLIP backbone.","Deep learning models have excelled in visual recognition tasks (Krizhevsky et al., 2017), enabling novel machine learning approaches beyond supervised learning (Wang et al., 2020; Van Engelen and Hoos, 2020). So far, the importance of open-set recognition (OSR) and semi-supervised learning has been widely acknowledged for the downstream tasks. Semi-supervised learning, as described by (Sohn et al., 2020), operates by integrating both labeled and unlabeled data within a closed-set framework. In contrast, OSR (Ge et al., 2017) deals with known and previously unknown classes in the unlabeled test set, often assigning an “unknown” label to potential novel class samples. However, clustering novel class data instead of labeling them could be more cost-effective. Novel class discovery (NCD) (Fini et al., 2021) aims to transfer knowledge from labeled to unlabeled datasets with distinct class labels for semantic clustering. Generalized class discovery (GCD) (Vaze et al., 2022) relaxes NCD’s assumptions for real-world applications, accounting for mixed known and novel class samples in the unlabeled set. However, despite its potential, GCD encounters several challenges. Existing models exhibit bias toward known classes due to labeled data availability and rely on vision-based backbones with limited semantic richness, limiting their utility on fine-grained datasets. Large-scale vision-language models, like CLIP (Radford et al., 2021), create semantically rich embeddings from image-text pairs. CLIP’s potential for GCD remains largely unexplored, despite its success. K-means with pre-trained CLIP features outperforms ViT (Han et al., 2022a) by around 10% on CIFAR-100 (Krizhevsky, 2009). Bias issues persist, particularly for novel classes. Thus, our research question is: “How can we harness CLIP for an unbiased, discriminative semantic space for GCD?” Introducing GraphVL: To tackle these challenges, we present GraphVL, a novel framework for Generalized Category Discovery (GCD) that utilizes CLIP frozen backbones alongside a learnable Graph Convolutional Network (GCN) module in the semantic space. We propose three key considerations for learning an unbiased and discriminative embedding space, enhancing unlabeled data clustering (Fig. 1). Our approach enhances the semantic space by introducing a learnable GCN on CLIP’s class embeddings, preserving neighborhood structures. This GCN maintains class relationships, ensuring that the neighborhood connections of pre-trained CLIP embeddings persist. A visual projector network adapts CLIP’s vision encoder for task-specific use. Our training combines two metric-based objectives: increasing similarity among same-class samples and maximizing separation for different-class data in the non-semantic space. Additionally, a margin-based image-text contrastive loss ensures a discriminative non-semantic space, yielding more generalized and unbiased visual samples. We also perform prompt learning similar to COOP, aligning textual prompts from known classes with the semantic feature space obtained from the GCN layer. This alignment enhances the contextual meaning of the embeddings and improves the overall performance of our approach. For novel class discovery, we adapt the semi-supervised K-means algorithm (Vaze et al., 2022), utilizing similarity distributions based on learned class embeddings from the GCN. Our approach excels in GCD literature (Vaze et al., 2022; Zhang et al., 2023) by systematically addressing bias and enhancing discriminativeness. Our main contributions are: - We tackle the challenging GCD problem, emphasizing bias reduction and enhanced discriminativeness. Our proposal combines the strengths of Vision-Language Models (VLMs), metric learning, and GCN. Our semi-supervised clustering in the semantic similarity-driven feature space improves class discovery. - To combat bias in labeled classes, we maintain semantic class relations through a learnable GCN module in a discriminative embedding space achieved via visual domain and cross-domain metric losses. We also use the prompt guided alignment between the generated graph and learned prompt embeddings. - We conduct extensive experiments comparing our GraphVL with benchmark NCD and GCD models re-implemented with the CLIP backbone, as well as the traditional DINO-based backbone (Vaze et al., 2022). Our findings consistently demonstrate the comprehensive superiority of GraphVL by a substantial margin of alteast 2%."
https://arxiv.org/html/2411.02065v1,AM Flow: Adapters for Temporal Processing in Action Recognition,"Deep learning models, in particular image models, have recently gained generalisability and robustness. In this work, we propose to exploit such advances in the realm of video classification. Video foundation models suffer from the requirement of extensive pretraining and a large training time. Towards mitigating such limitations, we propose ”Attention Map (AM) Flow” for image models, a method for identifying pixels relevant to motion in each input video frame. In this context, we propose two methods to compute AM flow, depending on camera motion. AM flow allows the separation of spatial and temporal processing, while providing improved results over combined spatio-temporal processing (as in video models). Adapters, one of the popular techniques in parameter efficient transfer learning, facilitate the incorporation of AM flow into pretrained image models, mitigating the need for full-finetuning. We extend adapters to ”temporal processing adapters” by incorporating a temporal processing unit into the adapters. Our work achieves faster convergence, therefore reducing the number of epochs needed for training. Moreover, we endow an image model with the ability to achieve state-of-the-art results on popular action recognition datasets. This reduces training time and simplifies pretraining. We present experiments on Kinetics-400, Something-Something v2, and Toyota Smarthome datasets, showcasing state-of-the-art or comparable results. Our code will be made available on github.","Currently, foundation models have advanced in multiple domains, on the task, for which they were explicitly trained, as well as on other implicit applications. For example, (Karamcheti et al. 2023) show that LLMs work well as pattern prediction machines, in addition to language-related tasks. The hypothesis behind this work is inspired by this phenomenon. While image models do not comprise motion, attention maps computed in transformer blocks are endowed with the ability to derive pixels which are pertinent to motion. Figure 1: Intuition of AM flow. No motion in the videos (frames) and minimal change in the attention map are represented by white/transparent. Here, rows with changes in the attention map correspond to input patches with motion, for example second and third patches have motion and the second and third rows of the attention maps have change. Note: the attention map is hand-crafted only for explanation and the colours have no intended meaning. Despite their remarkable capabilities, video foundation models come with significant limitations pertaining to their resource-intensive nature. Specifically, such models necessitate a substantial amount of pretraining data and training time, posing practical constraints. This work addresses this limitation using an image foundation model as a backbone along with the proposed modules: AM Flow and temporal processing adapters - an alternative to video processing backbones, increasing performance and reducing training time compared to the state-of-the-art (SOTA). Image foundation models have seen massive growth with works such as Dinov2 (Oquab et al. 2023) and Hiera (Ryali et al. 2023). This growth is due to improved spatial attention, which can be exploited to alleviate the above challenge with video models. Instead of learning fine-grained temporal relations directly from videos, the absolute difference of attention maps (taken from transformer encoders) for two consecutive frames provides simplified, encoded information about the motion in the frames. Here, we refer to this difference as Attention Map flow or AM flow. Figure 1 illustrates the intuition behind computing AM flow. This rich information can be achieved with only an absolute difference operation performed on the attention maps, and therefore resulting in a reduced training and inference time. We aim to remove temporal processing from the backbone, in order to expedite convergence and let temporal processing adapters account for time. It is counter-intuitive that separating spatial and temporal processing improves performance. However, end-to-end training allows the network to learn the relationship between space and time through backpropagation. Adapters are a parameter-efficient transfer learning technique, in which modules are added to a frozen pre-trained backbone and only these modules are trained during the new transfer learning protocol. They work by adapting the distribution of the original network to the new task, data, or even a new modality. Therefore, by combining temporal processing adapters and AM flow, we propose an expedite computation of spatio-temporal relations for video classification. AM flow can simply be concatenated with the input to the adapters. Section ”Methodology” discusses this in detail. A benefit of our method has to do with the downsampled embedding from this adapter, providing rich spatial information, pertaining to motion. We conduct experiments with three temporal processing modules, namely transformer encoder, TCN, and LSTM. These are added to the temporal processing adapters and our experiments suggest that all perform well. To summarise, the contributions of our work include the following. • We introduce AM flow, an efficient method to compute motion between consecutive frames. • We propose two methods to compute AM flow, depending on camera motion. • We propose a novel architecture to incorporate AM flow using temporal processing adapters. • We achieve SOTA performance on three datasets, namely Kinetics-400, Something-something v2, and Toyota Smarthome. Figure 2: The middle part of the figure shows the frozen image model (ViT) (in red colour) with trainable additions: temporal processing adapters (in green) containing temporal processing units (in purple). The adapter across MHSA takes AM flow as input along with the input to the transformer block. XAtsubscript𝑋subscript𝐴𝑡X_{A_{t}}italic_X start_POSTSUBSCRIPT italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT is shown as computed inside M⁢H⁢S⁢A𝑀𝐻𝑆𝐴MHSAitalic_M italic_H italic_S italic_A on the left (in yellow). On the right, XAtsubscript𝑋subscript𝐴𝑡X_{A_{t}}italic_X start_POSTSUBSCRIPT italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT and XAt+1subscript𝑋subscript𝐴𝑡1X_{A_{t+1}}italic_X start_POSTSUBSCRIPT italic_A start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT are used to compute A⁢M⁢f⁢l⁢o⁢w𝐴𝑀𝑓𝑙𝑜𝑤AMflowitalic_A italic_M italic_f italic_l italic_o italic_w (in yellow). t𝑡titalic_t and t+1𝑡1t+1italic_t + 1 signify different time-steps for the input frames. (Violet) shows the global temporal processing unit (TPU) and the classification head (squeeze and linear) added to it. All logits received from the temporal processing module (TPM) and the frozen model branch are averaged to obtain the final classification logits."
https://arxiv.org/html/2411.02057v1,Exploiting Unlabeled Data with Multiple Expert Teachers for Open Vocabulary Aerial Object Detection and Its Orientation Adaptation,"In recent years, aerial object detection has been increasingly pivotal in various earth observation applications. However, current algorithms are limited to detecting a set of pre-defined object categories, demanding sufficient annotated training samples, and fail to detect novel object categories. In this paper, we put forth a novel formulation of the aerial object detection problem, namely open-vocabulary aerial object detection (OVAD), which can detect objects beyond training categories without costly collecting new labeled data. We propose CastDet, a CLIP-activated student-teacher detection framework that serves as the first OVAD detector specifically designed for the challenging aerial scenario, where objects often exhibit weak appearance features and arbitrary orientations. Our framework integrates a robust localization teacher along with several box selection strategies to generate high-quality proposals for novel objects. Additionally, the RemoteCLIP model is adopted as an omniscient teacher, which provides rich knowledge to enhance classification capabilities for novel categories. A dynamic label queue is devised to maintain high-quality pseudo-labels during training. By doing so, the proposed CastDet boosts not only novel object proposals but also classification. Furthermore, we extend our approach from horizontal OVAD to oriented OVAD with tailored algorithm designs to effectively manage bounding box representation and pseudo-label generation. Extensive experiments for both tasks on multiple existing aerial object detection datasets demonstrate the effectiveness of our approach. The code is available at https://github.com/lizzy8587/CastDet.","Object detection in aerial images aims at localizing and categorizing objects on the earth’s surface, which is fundamental to a wide range of remote sensing applications, such as urban planning, environmental surveillance, and disaster response [1, 2, 3]. Despite significant advancements in aerial object detectors powered by deep learning [4, 5, 6, 7, 8, 9], they are still weak at detecting objects beyond the training categories. To expand the object detectors to novel categories, a naive idea is to collect and annotate large-scale aerial images with rich object categories. However, obtaining accurate yet sufficient annotations is time- and labor-expensive, even requiring human experts involvement. As a result, despite extensive efforts paid in the collection of existing aerial object detection datasets [10, 11, 12, 13, 14, 15], they are still much smaller and less diverse than natural image datasets [16, 17, 18], as depicted in Fig. 1. It hinders the detector scaling up in open-world scenarios. Therefore, this paper advocates more flexible object detectors capable of detecting novel object categories unseen during training, referred to as open vocabulary object detection (OVD). This allows us to characterize new objects that emerged in the earth observation data without extra annotations in open scenarios. Figure 1: Comparisons of the amount of categories and images of 19 common aerial and natural datasets. Challenge 1: Aerial datasets are much smaller in size and category vocabularies compared with natural image datasets. Drawing inspiration from the recent success of OVD in natural images [19, 20, 21], we intend to explore ways in the challenging task, namely open vocabulary aerial object detection (OVAD), where objects often vary widely in scale, orientation, and exhibit weak feature appearance [22]. Unlike natural images, which often feature clear contours and textures, allowing class-agnostic region proposal networks (RPNs) to effectively generalize to novel categories [23, 24]; aerial images captured from overhead perspectives usually display weak surface features, where objects may blend into the background. This complicates object detection, for instance, AIRPORT may be confused with HIGHWAY, and thus be wrongly classified as background, as illustrated in Fig. 2(b). Consequently, the recall of novel categories in aerial images is significantly lower than that in natural images, as shown in Fig. 2(c). In addition, objects in aerial images exhibit great orientation diversity. Using horizontal bounding boxes for these rotated objects could bring in substantial background clutter, leading to inaccurate localization. However, current open-vocabulary detectors primarily support horizontal detection, which limits their effectiveness in aerial images. These factors, on the one hand, pose challenges to directly apply current OVD methods for natural images to aerial images; and, on the other hand, spur us to develop extensible object detectors for aerial images, covering more object categories without extra annotation. Figure 2: Challenge 2: The recall of novel objects in aerial images is much lower than that in natural images due to the highly complex backgrounds. (a)(b) Aerial images from DIOR [11]. Objects in aerial images exhibit background interference. (c) Class-agnostic RPN recall statistics of novel categories in the natural dataset COCO [16] and the aerial dataset VisDroneZSD [25] (i.e., 77% v.s. 48%). To address these challenges, we propose a simple yet effective open-vocabulary aerial detection framework, namely CastDet, a CLIP-activated student-teacher detector. It comprises three key models: a student model and two teacher models for localization and classification, respectively. The student model is responsible for training both horizontal and oriented open-vocabulary detectors. The former adopts methods such as Faster R-CNN [26], and the later exploits the Oriented R-CNN [27]. The localization teacher model focuses on discovering and localizing potential objects, while the other teacher model provides classification guidance for novel categories. We jointly train the student model and teacher models in a self-learning manner, where the student is optimized with pseudo-labels generated by the teachers, and the localization teacher is updated from the continually learned student model. As exemplified by the literature [28], the slowly progressing localization teacher model acts as an ensemble of the previous student models, exhibiting superior capabilities that enable it to generate more reliable proposals. These proposals are then filtered based on their confidence scores, such as scale jittering variance, before being presented to the external teacher. While the student-teacher learning paradigm is powerful for enhancing the capabilities of the student model under the guidance of the teacher model [29, 30, 31, 28, 32], they operate in the closed-set setting. This leads to poor performance in discovering and recognizing novel object categories that are not encountered in the training data. To overcome this limitation, RemoterCLIP [33] that has been pre-trained on massive remote sensing image-text pairs and equips with strong zero-shot classification capabilities is exploited as an external teacher model. Thus, the learning process could benefit from the extra knowledge dedicated to interpreting aerial images brought by the RemoteCLIP. In addition to involving the RemoteCLIP, maintaining high-quality pseudo-labels is necessary. To this end, after predicting the category labels for the pseudo proposals, a dynamic label queue is proposed to store and iteratively update the pseudo-labels. Unlike previous CLIP-based methods [34, 35, 23] that directly transfer knowledge from CLIP for open-vocabulary recognition, our CLIP-activated student-teacher self-learning framework incorporates high-confidence knowledge from RemoteCLIP as an incentive to guide both the student and localization teacher. This self-learning mechanism facilitates a “flywheel effect” wherein the external teacher transfers knowledge to strengthen the localization teacher to identify potential regions of novel objects, thereby enhancing pseudo box quality. In turn, the refined pseudo boxes enable the external teacher to produce more precise pseudo-labels. Through our student-teacher self-learning process, the detection model can be progressively updated to localize and recognize continuously expanded object category vocabularies. The preliminary content of this paper has partially appeared in the conference version, with the detector named CastDet [36] (ECCV 2024). The overall contributions of this extended journal version are as follows111This journal version extends the previous conference version (ECCV 2024) [36], particularly in the following aspects: 1) We rewrite the full paper with comprehensive discussions and more details; 2) The proposed CastDet framework is now extended to support open-vocabulary oriented detection, addressing the specific challenges of detecting objects with arbitrary orientations in aerial scenes; 3) We introduce two new oriented box selection strategies: box scale jittering variance and angle jittering variance, which take both the scale and orientation of objects into account to filter robust pseudo boxes; 4) We integrate multiple open-vocabulary oriented detection algorithms into a unified code repository, including Oriented CastDet, Oriented ViLD, Oriented GLIP and Oriented GroundingDINO; 5) We conduct more ablation experiments to demonstrate the effectiveness of the proposed techniques; 6) We build a more comprehensive benchmark for the open-vocabulary aerial detection task, including both horizontal object detection and oriented object detection.: 1. Our work pioneers the open vocabulary aerial object detection (OVAD) to conquer fundamental challenges in interpreting earth observation images: the relatively small scale of annotated data in terms of both the number of training samples and the object categories; and the unique characteristics of aerial images, such as arbitrary orientations and weak feature appearances. 2. We introduce the CastDet, a flexible open-vocabulary detection framework incorporating the student-(multi)teacher self-learning paradigm. The proposed framework enables the detection model to progressively expand its object vocabulary and achieve accurate localization of novel objects using horizontal or oriented boxes without extra annotation efforts. 3. We propose a series of box selection strategies to preserve high-quality pseudo-labels via filtering, including RPN Score, box jittering variance (BJV) and regression jittering variance (RJV) for horizontal boxes; scale jittering variance (SJV) and angle jittering variance (AJV) for oriented boxes. 4. We devise a dynamic label queue for storing and incrementally updating high-quality pseudo-labels generated by the teachers. By this way, richer and more accurate labels could be dynamically maintained. 5. We implement multiple open-vocabulary oriented object detection methods, including Oriented ViLD, Oriented GLIP and Oriented GroundingDINO. This marks the first effort to open-vocabulary oriented detection. 6. We utilize several public aerial image datasets to establish the open-vocabulary aerial detection benchmark and conduct extensive experiments to validate our methods, achieving significant performance improvement."
https://arxiv.org/html/2411.01988v1,QCS:Feature Refining from Quadruplet Cross Similarity for Facial Expression Recognition,"On facial expression datasets with complex and numerous feature types, where the significance and dominance of labeled features are difficult to predict, facial expression recognition(FER) encounters the challenges of inter-class similarity and intra-class variances, making it difficult to mine effective features. We aim to solely leverage the feature similarity among facial samples to address this. We introduce the Cross Similarity Attention (CSA), an input-output position-sensitive attention mechanism that harnesses feature similarity across different images to compute the corresponding global spatial attention. Based on this, we propose a four-branch circular framework, called Quadruplet Cross Similarity (QCS), to extract discriminative features from the same class and eliminate redundant ones from different classes synchronously to refine cleaner features. The symmetry of the network ensures balanced and stable training and reduces the amount of CSA interaction matrix. Contrastive residual distillation is utilized to transfer the information learned in the cross module back to the base network. The cross-attention module exists during training, and only one base branch is retained during inference. our proposed QCS model outperforms state-of-the-art methods on several popular FER datasets, without requiring additional landmark information or other extra training data. The code is available at https://github.com/birdwcp/QCS.","Facial expression is crucial for machines to comprehend human emotions. Facial expression recognition (FER) holds vast research potential and application worth in human-computer interaction, emotion analysis, and intelligent healthcare. Over decades, deep learning advancements in computer vision have propelled significant progress in facial expression recognition. However, FER is still a complex task facing many difficulties such as Inter-class similarity and intra-class variances. Faces with varying expressions retain the common features that constitute a human face, yet same expressions can vary widely in pose, identity, illumination, etc. Traditional object classification methods struggle with complex face datasets due to assumptions of labeled feature dominance. Dividing datasets by labels for training can result in the learning of unlabeled redundant features if they’re non-randomly distributed, impacting classification. Fig. 1 illustrates an extreme case of this impact. In reality, interference features are varied, numerous, and unpredictable, exerting varying degrees of influence. Figure 1: Assuming that unlabeled gender features and labeled expression features both exhibit similar distributions in the training set, leading to the non-dominance of significant expression features. It is difficult to explain whether the trained model is an expression classifier or of another type. How to extract discriminative features and how to eliminate the interference of redundant features are the core issues of FER. Using self-attention, some deep learning methods (Wang et al. 2020; Xue, Wang, and Guo 2021; Xue et al. 2022; Ma, Sun, and Li 2021) focus on key features, enhancing model discrimination. However, model performance hinges on the amount and quality of training data. Some methods (Xie, Hu, and Wu 2019; Zhang et al. 2021; Cai et al. 2021; Ruan et al. 2021, 2022) study separating redundant from expression features using GANs or transformers. They extract expression information via decomposition and reconstruction, but often need redundant feature labels, such as age, gender, race, or rely on identity recognition modules. Additionally, interference decoupling FER (Li, Deng, and Du 2017) relies on one or several interference feature labels and can only address limited and specific interference factors. However, interference features are often unclear in type and quantity, and these methods that rely on labeled data often fail to achieve good performance when the types, quantities, and quality of labeled data are limited. Furthermore, when the expression feature labels are ambiguous (She et al. 2021; Zhang, Wang, and Deng 2021), it is unrealistic to clearly label all interference features. Unlike the aforementioned supervised methods, we introduce a straightforward yet potent approach. It harnesses similarities among same-class image pairs to extract discriminative label features and employs similarities in cross-class image pairs to separate unlabeled, redundant interference features, to alleviate the problem of relying on annotated features to be dominant in the training set. Firstly, We propose Cross Similarity Attention (CSA) tailored for mining fine-grained feature similarity between different images. It distinguishes from NLP Transformer’s cross-attention(Vaswani et al. 2017) for language correlations and Vision Transformer’s self-attention(Dosovitskiy et al. 2021) for intra-image correlations. CSA calculates Euclidean distance similarity between fine-grained features of images, focusing solely on the importance of the features in the spatial dimension of the queried image(key). This compensates for ViT’s matrix multiplication-based attention, which lacks input-output positions correlation in cross-image similarity. By leveraging CSA for interaction module construction, we enhance the distinct shared features between images within the same class. Inspired by triplet loss (Schroff, Kalenichenko, and Philbin 2015), we apply the interaction module to extract and diminish similar features across different class image pairs, thereby separating redundant and interfering features. To ensure equal feature refinement across input branches, we introduce a four-branch configuration, with each class contributing two images. This forms a centrally symmetric closed-loop network utilizing four sets of interaction modules for stable joint training. This approach simultaneously minimizes intra-class feature differences and maximizes inter-class differences, mimicking the effect of triplet loss at the fine-grained feature level. During inference, only one image is input, requiring the removal of the multi-branch interaction module, retaining a single base network. To effectively integrate the refined information from cross modules, we introduce residual connections that minimize gradient conflicts in feature maps, creating a self-distillation-like structure. In summary, our main contributions are summarized as follows: • We introduce Cross Similarity Attention (CSA), a tailored mechanism for mining fine-grained feature similarities across different images, utilizing a spatially dimensioned and input-output position-sensitive attention approach. • We propose Quadruplet Cross Similarity (QCS), a four-branch, centrally symmetric closed-loop joint training framework based on CSA. It refines features by mining discriminative features within the same class and separating interfering features across different classes synchronously. • Our method achieves state-of-the-art performance on several FER datasets without requiring additional landmark information or pre-training on FER external datasets. Figure 2: The framework of Quadruplet Cross Similarity Network. Joint training is performed on 4 classifiers C⁢l⁢sb⁢a⁢s⁢e𝐶𝑙subscript𝑠𝑏𝑎𝑠𝑒{{Cls}_{base}}italic_C italic_l italic_s start_POSTSUBSCRIPT italic_b italic_a italic_s italic_e end_POSTSUBSCRIPT based on a weight-shared backbone and 4 classifiers C⁢l⁢sc⁢r⁢o⁢s⁢s𝐶𝑙subscript𝑠𝑐𝑟𝑜𝑠𝑠{{Cls}_{cross}}italic_C italic_l italic_s start_POSTSUBSCRIPT italic_c italic_r italic_o italic_s italic_s end_POSTSUBSCRIPT enhanced by cross-module, with only the red branch retained during inference. Anchor and pos are in the same category, so are neg and neg2. The matrix S⁢D𝑆𝐷{SD}italic_S italic_D performs attention refinement on matrix S𝑆{S}italic_S and matrix D𝐷{D}italic_D by rows or columns."
https://arxiv.org/html/2411.01981v2,Typicalness-Aware Learning for Failure Detection,"Deep neural networks (DNNs) often suffer from the overconfidence issue, where incorrect predictions are made with high confidence scores, hindering the applications in critical systems. In this paper, we propose a novel approach called Typicalness-Aware Learning (TAL) to address this issue and improve failure detection performance. We observe that, with the cross-entropy loss, model predictions are optimized to align with the corresponding labels via increasing logit magnitude or refining logit direction. However, regarding atypical samples, the image content and their labels may exhibit disparities. This discrepancy can lead to overfitting on atypical samples, ultimately resulting in the overconfidence issue that we aim to address. To tackle the problem, we have devised a metric that quantifies the typicalness of each sample, enabling the dynamic adjustment of the logit magnitude during the training process. By allowing atypical samples to be adequately fitted while preserving reliable logit direction, the problem of overconfidence can be mitigated. TAL has been extensively evaluated on benchmark datasets, and the results demonstrate its superiority over existing failure detection methods. Specifically, TAL achieves a more than 5% improvement on CIFAR100 in terms of the Area Under the Risk-Coverage Curve (AURC) compared to the state-of-the-art. Code is available at https://github.com/liuyijungoon/TAL.","Failure detection plays a vital role in machine learning applications, particularly in high-risk domains where the reliability and trustworthiness of predictions are crucial. Applications such as medical diagnosis Esteva et al. (2017), autonomous driving Janai et al. (2020); Yang et al. (2023, 2024), and other visual perception tasks Lai et al. (2023); Tian et al. (2020); Tang et al. (2024b); Luo et al. (2024); Tian et al. (2023); Peng et al. (2024); Tian et al. (2019); Peng et al. (2023); Tian et al. (2022b) require accurate assessments of prediction confidence before making critical decisions. The goal of failure detection is to enhance the reliability and trustworthiness of predictions, ensuring that high-confidence predictions are relied upon while low-confidence predictions are appropriately rejected Moon et al. (2020); Zhang et al. (2020). This is essential for maintaining the safety and effectiveness of these applications. Indeed, deep neural networks (DNNs) trained using the cross-entropy loss often suffer from the issue of overconfidence. This leads to unreliable confidence scores, which in turn hinder the effectiveness of failure detection methods. It is common for models to make incorrect predictions with high confidence scores, sometimes even close to 1.0. A recent study called LogitNorm Wei et al. (2022) has shed light on this problem. It reveals that the softmax cross-entropy loss can cause the magnitude of the logit vector to continue increasing, even when most training examples are correctly classified. This phenomenon contributes to the model’s overconfidence. To alleviate the overconfidence problem, LogitNorm Wei et al. (2022) proposes assigning a constant magnitude to decouple the influence of the magnitude during optimization. The cross-entropy loss with the decoupled logit vector 𝒇=f⁢(𝒙;Θ)𝒇𝑓𝒙Θ\boldsymbol{f}=f(\boldsymbol{x};\Theta)bold_italic_f = italic_f ( bold_italic_x ; roman_Θ ) is defined as what follows: ℒCE⁢(𝒇,y)=−log⁡e‖𝒇‖⋅𝒇^y∑i=1Ce‖f‖⋅𝒇^i,subscriptℒCE𝒇𝑦superscript𝑒⋅norm𝒇subscriptbold-^𝒇𝑦superscriptsubscript𝑖1𝐶superscript𝑒⋅norm𝑓subscriptbold-^𝒇𝑖\mathcal{L}_{\mathrm{CE}}(\boldsymbol{f},y)=-\log\frac{e^{\|\boldsymbol{f}\|% \cdot\boldsymbol{\hat{f}}_{y}}}{\sum_{i=1}^{C}e^{||f||\cdot\boldsymbol{\hat{f}% }_{i}}},caligraphic_L start_POSTSUBSCRIPT roman_CE end_POSTSUBSCRIPT ( bold_italic_f , italic_y ) = - roman_log divide start_ARG italic_e start_POSTSUPERSCRIPT ∥ bold_italic_f ∥ ⋅ overbold_^ start_ARG bold_italic_f end_ARG start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT end_POSTSUPERSCRIPT end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPT italic_e start_POSTSUPERSCRIPT | | italic_f | | ⋅ overbold_^ start_ARG bold_italic_f end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT end_ARG , (1) where ΘΘ\Thetaroman_Θ is the parameters of the DNN model, 𝒙𝒙\boldsymbol{x}bold_italic_x is the input image with label y𝑦yitalic_y, the logit vector 𝒇𝒇\boldsymbol{f}bold_italic_f is decoupled into the magnitude ‖𝒇‖norm𝒇\|\boldsymbol{f}\|∥ bold_italic_f ∥ and the directions 𝒇^^𝒇\hat{\boldsymbol{f}}over^ start_ARG bold_italic_f end_ARG. Based on the decomposition of the logit vector in Eq. (1), we can observe that the overconfidence issue could stem from either increasing ‖𝒇‖norm𝒇\|\boldsymbol{f}\|∥ bold_italic_f ∥ or decreasing α𝛼\alphaitalic_α (the angle between the directions of prediction and label) during training. Key observation & Motivation. Following the exclusion of the logit magnitude’s impact by LogitNorm Wei et al. (2022), we posit that the risk of the overconfidence issue still arises from logit directions. Typical samples, which have clear contextual information, help models generalize well. However, optimizing the direction for ambiguous atypical samples can still cause overconfidence. In these cases, the labels do not match the image context well. Aligning the logit direction of atypical samples may still lead to high softmax scores near 1.0 which worsens the overconfidence problem. According to previous work Zhu et al. (2023b); Yuksekgonul et al. (2024), the definitions of typical and atypical samples are based on their semantic similarity to most samples and the ease with which the model learns them. Specifically: • Typical samples are those that exhibit similarity to a majority of other samples at the semantic level. These samples possess typical features that are easier for deep neural networks to learn and generalize. • Atypical samples, on the other hand, differ significantly from other samples at the semantic level. They pose a challenge for the model to generalize due to their uniqueness. These samples are often located near the decision boundary, causing the model to have higher uncertainty in making predictions for them. In Fig. 1, we present an atypical example to illustrate the issue at hand. Despite the ground-truth label being a horse, the image depicts a horse with a human body, which could reasonably be predicted as either a human or a horse with a confidence score of around 50%. However, the model incorrectly predicts the image as a horse with an excessively high confidence score of 95%. Upon examining Eq. (1), we observe that the confidence score is determined by two crucial factors: magnitude and direction. This prompts an important question regarding the decoupling of these factors to determine which one is more reliable in accurately approximating real confidence. Addressing this inquiry is essential for effective failure detection. Figure 1: Illustration of the motivation. We observe that directly aligning the predictions of atypical samples to the target label is not appropriate, causing overconfidence (horse with 95% confidence). Instead, the confidence should be aligned with the human perception. During training, the cross-entropy loss increases the magnitude ‖𝒇‖norm𝒇\|\boldsymbol{f}\|∥ bold_italic_f ∥ and adjusts their direction towards the target (represented by the angle α𝛼\alphaitalic_α). Consider this example where an image of a human body with a horse head is presented, the loss may optimize towards 𝒇2subscript𝒇2\boldsymbol{f}_{2}bold_italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT in the blue box, which is not the ideal outcome direction. Instead, it would be better to optimize towards 𝒇1subscript𝒇1\boldsymbol{f}_{1}bold_italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, rather than being biased towards either one, ensuring a more balanced and unbiased representation and allowing for a more accurate estimation of confidence. Our approach. Based on the aforementioned observations, we propose a novel approach called Typicalness-Aware Learning (TAL). TAL dynamically adjusts the magnitudes of logits based on the typicalness of the samples, allowing for differentiated treatments of typical and atypical samples. By doing so, TAL aims to mitigate the adverse effects caused by atypical samples and emphasizes that the direction of logits serves as a more reliable indicator of model confidence. In the blue dashed box of Fig. 1, we provide an example that illustrates the impact of TAL on an atypical sample. The logit vector could be changed from 𝒇2subscript𝒇2\boldsymbol{f}_{2}bold_italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT to 𝒇1subscript𝒇1\boldsymbol{f}_{1}bold_italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, indicating that the scores obtained with 𝒇^^𝒇\hat{\boldsymbol{f}}over^ start_ARG bold_italic_f end_ARG for both ""horse"" and ""human"" become nearly equal. This alignment better aligns with human perception, highlighting the effectiveness of TAL in improving model confidence estimation. The proposed TAL approach is model-agnostic, making it easily applicable to models with various architectures, such as CNN He et al. (2016b) and ViT Dosovitskiy et al. (2020). Experimental results on benchmark datasets, including CIFAR10, CIFAR100, and ImageNet, demonstrate the superiority of TAL over existing failure detection methods. Specifically, on CIFAR100, our method achieves a significant improvement of more than 5% in terms of the Area Under the Risk-Coverage Curve (AURC) compared to the state-of-the-art method Zhu et al. (2022). In summary, the main contributions of this paper are as follows: • We propose a new insight that the overconfidence might stem from the presence of atypical samples, whose labels fail to accurately describe the images. This forces the models to conform to these imperfect labels during training, resulting in unreliable confidence scores. • In order to mitigate the issue of overfitting on atypical samples, we introduce the Typicalness-Aware Learning (TAL), which enables the identification and separate optimization of typical and atypical samples, thereby alleviating the problem of overconfidence. • Extensive experiments demonstrate the effectiveness and robustness of TAL. Besides, TAL has no structural constraints to the target model and is complementary to other existing failure detection methods."
https://arxiv.org/html/2411.01975v1,SPECTRUM: Semantic Processing and Emotion-informed video-Captioning Through Retrieval and Understanding Modalities,"Capturing a video’s meaning and critical concepts by analyzing the subtle details is a fundamental yet challenging task in video captioning. Identifying the dominant emotional tone in a video significantly enhances the perception of its context. Despite a strong emphasis on video captioning, existing models often need to adequately address emotional themes, resulting in suboptimal captioning results. To address these limitations, this paper proposes a novel Semantic Processing and Emotion-informed video-Captioning Through Retrieval and Understanding Modalities (SPECTRUM) framework to empower the generation of emotionally and semantically credible captions. Leveraging our pioneering structure, SPECTRUM discerns multimodal semantics and emotional themes using Visual-Text Attribute Investigation (VTAI) and determines the orientation of descriptive captions through a Holistic Concept-Oriented Theme (HCOT), expressing emotionally-informed and field-acquainted references. They exploit video-to-text retrieval capabilities and the multifaceted nature of video content to estimate the emotional probabilities of candidate captions. Then, the dominant theme of the video is determined by appropriately weighting embedded attribute vectors and applying coarse- and fine-grained emotional concepts, which define the video’s contextual alignment. Furthermore, using two loss functions, SPECTRUM is optimized to integrate emotional information and minimize prediction errors. Extensive experiments on the EmVidCap, MSVD, and MSR-VTT video captioning datasets demonstrate that our model significantly surpasses state-of-the-art methods. Quantitative and qualitative evaluations highlight the model’s ability to accurately capture and convey video emotions and multimodal attributes.","I INTRODUCTION Recent advancements in computer vision and natural language processing have led to emerging joint vision-language research domains. Semantically describing videos represents a significant interdisciplinary challenge within these domains, aiming to generate precise textual descriptions from visual features automatically. Achieving a meaningful understanding of video content and translating it into precise and concise corresponding sentences constitutes the most critical challenge in video captioning. Considering the diverse underlying spatial and temporal relationships between visual features and their corresponding semantic connections is crucial to resolving this issue,[1], [2], [3]. Video subtitling, video summarization, content-based video retrieval, human-computer interaction, assistance for the visually impaired, and video surveillance represent just a few of the myriad applications of semantic video description, [4], [5], [6]. Emotions and motions significantly enhance the richness of video captions due to their strong interconnection [7]. Researchers increasingly use emotion detection in natural language processing (NLP) to understand emotions in various contexts, classifying and measuring emotional intensities [8], [9]. Effective video emotional captioning aims to improve sentence expressiveness by incorporating multimodal analysis—visual, audio, and textual information—thereby completely understanding a video’s thematic and emotional contexts. This paper introduces procedures designed to enhance vision-language correspondence and derive emotionally-informed and field-acquainted concepts, focusing on multimodal semantic analysis. The primary challenges associated with these procedures include the accurate extraction of attribute features and their effective utilization. Previous research has often failed to simultaneously consider both emotional and factual concepts in video description, highlighting a clear gap in the existing literature, [10], [11]; our model is designed to be emotionally-informed and field-acquainted, utilizing a fine-to-coarse concept investigation approach to perform multimodal semantic analysis. Attribute features encompass both emotional and factual features, where factual words typically include concepts such as beings, objects, scenes, and most importantly, motions. Emotional concepts can be categorized into 34 distinct bags of emotions, [9], [12]. Each ’bag of emotions,’ considered a coarse emotional category, comprises a set of finer emotional terms that convey the same concept. For example, the ""Calm"" category, a bag of emotions, includes finer terms such as ""peaceful,"" ""quiet,"" ""calmly,"" ""quietly,"" and ""peacefully"" all of which reflect the same underlying concept as the coarse term. Hence, for each query video, a holistic theme oriented around both emotional and factual concepts predominates in the generated captions. Consider Fig 1 as an example; our proposed model aims to identify the emotions, actions, and associated descriptive fields that comprehensively describe the video, thereby conceptually guiding its orientation. it becomes challenging to specify the emotion associated with playing music without employing a concept-oriented theme that is holistically emotional. This is because the motion may arise from a range of emotions such as joy, happiness, sadness, calmness, anger, frustration, surprise, or excitement. In most mainstream studies, the coarse-to-fine investigation of attribute features often overlooks the combined impact of emotional and factual concepts, resulting in incomplete and partial captions. Figure 1: Diagram of the SPECTRUM framework illustrating the caption generation procedure of a query video based on the emotional spectrum. The SPECTRUM architecture is designed to generate emotionally-informed and factually-acquainted captions through the use of a concept investigation unit (CIU), including video-text attribute investigation (VTAI) and holistic concept-oriented theme (HCOT). In this paper, a novel Semantic Processing and Emotion-informed video-Captioning Through Retrieval and Understanding Modalities (SPECTRUM) framework is designed to support the generation of credible semantic descriptions of videos, focusing on the dominant emotion associated with each motion. Moreover, we enrich our proposed architecture by incorporating the contrastive vision-language pre-trained model, CLIP [13], for encoding and video-to-text retrieval procedures. In the Concept Investigation Unit (CIU) of the proposed architecture, the Video-Text Attribute Investigation (VTAI) stage conducts multimodal semantic analysis. It evaluates the likelihood of attribute concepts appearing in ground truth captions. In the second stage, the Holistic Concept-Oriented Theme (HCOT) aims to define the conceptual orientation of captions, reflecting emotionally-informed and field-acquainted sentences. This stage leverages the Coarse-to-Fine Block (CFB) and the Attribute Embedding Block (AEB) to prevent the emergence of semantically unrelated descriptions. By utilizing both textual and video features as inputs, the CIU enhances the model’s robustness against ambiguities and inaccuracies during the semantic description of the video. Our contributions to this work are as follows: I: We present SPECTRUM, a novel framework for video captioning that incorporates the Concept Investigation Unit (CIU) as its core component. SPECTRUM enhances the generation of accurate semantic video descriptions by integrating multimodal feature fusion with emotionally-informed and field-acquainted knowledge. II: We devise a Video-Text Attribute Investigation (VTAI) module that derives multimodal concepts to generate meaningful captions. This module predicts the probability of attribute cues in each caption, encompassing objects, scenes, actions, and emotions. III: We developed the Holistic Concept-Oriented Theme (HCOT) module to guide the semantic direction of captions with emotional context, minimizing misalignment. The HCOT module features two key components: the Coarse-to-Fine Block (CFB) and the Attribute Embedding Block (AEB). These components investigate query videos and their associated captions to derive fine-to-coarse concepts and ascertain each video concept’s emotion and factual field. IV: We incorporate the pre-trained model, CLIP, to enhance our proposed architecture through text and image encoding and video-to-text retrieval procedures. V: Extensive experiments on EmVidCap [12], MSVD [14], and MSR-VTT [15] demonstrate that our proposed network achieves performance comparable to state-of-the-art methods in video captioning, with a specific focus on integrating emotional, factual, and holistic concepts."
https://arxiv.org/html/2411.01969v1,Active Gaze Behavior Boosts Self-Supervised Object Learning,"Due to significant variations in the projection of the same object from different viewpoints, machine learning algorithms struggle to recognize the same object across various perspectives. In contrast, toddlers quickly learn to recognize objects from different viewpoints with almost no supervision. Recent works argue that toddlers develop this ability by mapping close-in-time visual inputs to similar representations while interacting with objects. High acuity vision is only available in the central visual field, which may explain why toddlers (much like adults) constantly move their gaze around during such interactions. It is unclear whether/how much toddlers curate their visual experience through these eye movements to support learning object representations. In this work, we explore whether a bio-inspired visual learning model can harness toddlers’ gaze behavior during a play session to develop view-invariant object recognition. Exploiting head-mounted eye tracking during dyadic play, we simulate toddlers’ central visual field experience by cropping image regions centered on the gaze location. This visual stream feeds a time-based self-supervised learning algorithm. Our experiments demonstrate that toddlers’ gaze strategy supports the learning of invariant object representations. Our analysis also reveals that the limited size of the central visual field where acuity is high is crucial for this. We further find that toddlers’ visual experience elicits more robust representations compared to adults’ mostly because toddlers look at objects they hold themselves for longer bouts. Overall, our work reveals how toddlers’ gaze behavior supports self-supervised learning of view-invariant object recognition.","Toddlers learn visual representations that support the recognition of object instances observed from different viewpoints within their first year of life (Kraebel & Gerhardstein, 2006; Ayzenberg & Behrmann, 2024). This early emergence of view-invariant recognition and the ease with which adults perform this skill hide the complexities of learning it. Images reaching the retina vary drastically when objects are turned in depth. Even state-of-the-art machine learning methods still make absurd recognition mistakes when faced with unusual viewpoints of objects (Dong et al., 2022; Abbas & Deny, 2023; Ruan et al., 2023). It raises the question of what learning mechanisms support such a view-invariant recognition in humans. One of the main theories of object recognition posits that the development of view-invariant object recognition rests on the brain’s ability to construct visual representations that slowly change over time (Földiák, 1991; Li & DiCarlo, 2008; Miyashita, 1988). The main idea is that learners abundantly manipulate (or walk around) objects while watching them, giving access to different views of a single object over a short period of time. By learning slowly changing representations, a brain discards rapidly changing information from an image (here, information about the view) and naturally builds view-invariant representations. Following this idea, recent computational studies proposed to simulate humans’ visual experience by generating or curating large-scale temporal sequences of rotating objects (Aubret et al., 2022; Schneider et al., 2021; Yu et al., 2023); they confirm that learning slowly changing representation may indirectly induce view-invariant object recognition. However, it is currently unclear if a toddler’s actual gaze behavior supports this learning mechanism. The significance of active gaze behavior stems from the limited area of high-acuity vision in humans. This area, known as the central visual field, covers only a few degrees of visual angle. This high-acuity processing is predominant in extracting semantic information in cortex areas related to object recognition (Quaia & Krauzlis, 2024; Yu et al., 2015). Such a small area of the visual field may be semantically unstable over time, as humans make three saccades per second on average. However, toddlers curate their own visual experience; compared to adults, objects held by toddlers appear bigger in the field of view due to their shorter arms (Bambach et al., 2018), their visual stimuli are simpler (Anderson et al., 2024) and their visual inputs semantically change on a slower timescale (Sheybani et al., 2023). The latter point may be critical to make slowness-based learning operational. In this paper, we explore whether a bio-inspired model of visual learning can utilize the actual eye-tracking derived visual experience of toddlers to develop robust object representations. For this, we leverage a dataset of head-camera recordings and gaze tracking from toddlers and adults during play sessions (Bambach et al., 2018). To simulate central visual experience, we crop image patches centered on tracked gaze locations. Then, we train a previously introduced self-supervised learning (SSL) model of temporal slowness (Schneider et al., 2021). Our experiments demonstrate that toddlers’ gaze behavior supports the learning of view-invariant object representations within a single unsupervised 12-minute play session. Our analysis shows that: 1) toddlers’ gaze strategy boosts visual learning in comparison to several baselines; 2) modeling the central visual field crucially enables object learning. Furthermore, we discover that representations learned from toddlers’ visual experiences are also better than adults’, presumably because toddlers look longer at objects that they hold. In sum, our work provides a plausible model of how toddlers may efficiently learn view-invariant object recognition without supervision by exploiting their active gaze behavior during interactions with objects."
https://arxiv.org/html/2411.01966v1,UnSegMedGAT: Unsupervised Medical Image Segmentation using Graph Attention Networks Clustering,"The data-intensive nature of supervised classification drives the interest of the researchers towards unsupervised approaches, especially for problems such as medical image segmentation, where labeled data is scarce. Building on the recent advancements of Vision transformers (ViT) in computer vision, we propose an unsupervised segmentation framework using a pre-trained Dino-ViT [1]. In the proposed method, we leverage the inherent graph structure within the image to realize a significant performance gain for segmentation in medical images. For this, we introduce a modularity-based loss function coupled with a Graph Attention Network (GAT) to effectively capture the inherent graph topology within the image. Our method achieves state-of-the-art performance, even significantly surpassing or matching that of existing (semi)supervised technique such as MedSAM [2] which is a Segment Anything Model in medical images. We demonstrate this using two challenging medical image datasets ISIC-2018 [3] and CVC-ColonDB [4]. This work underscores the potential of unsupervised approaches in advancing medical image analysis in scenarios where labeled data is scarce. The github repository of the code is available on https://github.com/mudit-adityaja/UnSegMedGAT.","Image segmentation is a crucial downstream task in healthcare and computer vision, with many methods developed primarily using supervised deep learning techniques that require extensive annotated data. This is particularly challenging in medical image segmentation due to the difficulty of obtaining such annotated data. We propose a novel method in unsupervised image segmentation, an area that has been comparatively less explored. Convolutional Neural Networks (CNNs) have become popular for image segmentation, with architectures like UNet [5], SegNet [6], and DeepLab [7] effectively extracting features from input images. However, these supervised methods depend on labeled datasets, which are resource-intensive to create and often limited by privacy issues, particularly in healthcare. Additionally, models trained on specific datasets may not generalize well to new data due to variations in imaging protocols and subjects. The MedSAM model [2], a recent advancement in medical image segmentatin, uses a vision transformer (ViT) [1] architecture and is trained on a large dataset to improve generalization across diverse anatomical structures and conditions. Despite its advancements, MedSAM struggles with modality imbalance, leading to bias towards majority classes. Moreover, it has primarily been tested on cancer data only. Recent approaches combining ViT with Graph Convolution Networks (GCN) [8] have shown promising results [9],[10, 11] in computer vision. In our work, we utilize a Dino-ViT [1] model to extract pretrained features from images and create a complete graph structure from these features. We prune edges based on features dissimilarity, and optimize segmentation clusters using Modularity loss [12] in an unsupervised manner. The contribution of this paper can be summarized below: • We propose a novel unsupervised approach in medical imaging domain by incorporating modularity[13] and attention mechanism [14] based criteria. This approach leverages features extracted from a pre-trained Vision Transformer [1] along with the intrinsic graph structure of the images for efficient image segmentation. • We deploy an ensemble of multi-level Graph attention networks [15] for underlying graph clustering required for effective image segmentation. • Our method, UnSegMedGAT, demonstrates superior performance compared to other state-of-the-art techniques, including MedSAM, on the ISIC-2018 dataset. Additionally, it significantly surpasses existing unsupervised methods on the CVC-ColonDB dataset. Figure 1: UnSegMedGAT Pipeline: we i) extract features f𝑓fitalic_f of all (overlapping) image patches using vision transformer (ViT) and formulate a (complete) Graph 𝒢𝒢\mathcal{G}caligraphic_G (few nodes shown, for illustration, in the same color as image patch windows), ii) then apply similarity (normalized f⁢fT𝑓superscript𝑓𝑇ff^{T}italic_f italic_f start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT) threshold to select important edges in 𝒢𝒢\mathcal{G}caligraphic_G, iii) aggregate and normalize features in GAT, darker node colors represent aggregation per series of GATs , iv) apply a fully connected network (FCN) with softmax activation to finally obtain node level clusters. vi) The modularity and regularization-based loss is finally used to train the model. vii-viii) At inference, edge refinement [16] is used over the predicted mask."
https://arxiv.org/html/2411.01948v1,Learning Where to Edit Vision Transformers,"Model editing aims to data-efficiently correct predictive errors of large pre-trained models while ensuring generalization to neighboring failures and locality to minimize unintended effects on unrelated examples. While significant progress has been made in editing Transformer-based large language models, effective strategies for editing vision Transformers (ViTs) in computer vision remain largely untapped. In this paper, we take initial steps towards correcting predictive errors of ViTs, particularly those arising from subpopulation shifts. Taking a locate-then-edit approach, we first address the “where-to-edit” challenge by meta-learning a hypernetwork on CutMix-augmented data generated for editing reliability. This trained hypernetwork produces generalizable binary masks that identify a sparse subset of structured model parameters, responsive to real-world failure samples. Afterward, we solve the “how-to-edit” problem by simply fine-tuning the identified parameters using a variant of gradient descent to achieve successful edits. To validate our method, we construct an editing benchmark that introduces subpopulation shifts towards natural underrepresented images and AI-generated images, thereby revealing the limitations of pre-trained ViTs for object recognition. Our approach not only achieves superior performance on the proposed benchmark but also allows for adjustable trade-offs between generalization and locality. Our code is available at https://github.com/hustyyq/Where-to-Edit.","In many scientific and engineering disciplines, computational models serve as approximations of complex real-world phenomena. As a consequence, they are inherently prone to predictive errors, aptly encapsulated by George Box’s adage: “All models are wrong, but some are useful.” Model editing [55, 6, 9, 40] has emerged as a promising technique to make (large) pre-trained models more useful by enabling targeted updates to model behavior on specific inputs or tasks in a data-efficient manner without pre-training again from scratch. An ideal model editing method should satisfy three major desiderata [9, 61]: 1) reliability, ensuring the model behavior is effectively updated for the current sample; 2) generalization, so that the changes extend to neighboring samples; and 3) locality, meaning the edit should have minimal impact on the model behavior on unrelated samples. Model editing has allowed many fascinating applications, including error correction, factual knowledge update, bias mitigation, policy compliance, and personalization, though most of them have predominantly been within large language models (LLMs) [15, 1, 12] in the natural language processing (NLP) community [9, 40]. With the enormous and often inaccessible pre-training datasets and the ever-growing model sizes that make retraining computationally demanding, the need for effectively editing computer vision (CV) models is also becoming urgent. Adapting model editing techniques from NLP to CV is non-trivial and presents unique challenges. From the data perspective, NLP deals with one-dimensional, discrete signals that are highly semantic and information-dense, whereas CV requires processing high-dimensional continuous sensor data that is spatially redundant. From the model perspective, lots of model editing methods in NLP are specially designed for LLMs with unidirectional (i.e., autoregressive) attention, such as GPT-3 [15] and GPT-4 [1]. In contrast, CV models have primarily been based on convolutional networks [33, 52, 24], with more recent implementations using vision Transformers (ViTs) [11, 35] that otherwise employ bidirectional attention. These differences in data formats and model structures make targeted edits more challenging to implement in CV models, and when such edits are achieved, they often result in suboptimal performance. In this paper, we take initial steps towards editing pre-trained ViTs for object recognition [10], aiming to correct predictive errors without the need for costly and time-consuming retraining. Specifically, we take a locate-then-edit approach, which breaks down model editing into two key subproblems: where-to-edit and how-to-edit. Moreover, we prioritize learning where to edit rather than how to edit to facilitate a simpler yet better trade-off between generalization and locality, without needing to store previously trained data. For the where-to-edit phase, we first narrow the editing scope using a greedy search-based heuristic. Next, inspired by the proven effectiveness of meta-learning [14] in optimizing training strategies for individual samples, we meta-train a hypernetwork to generate a binary task, indicating which parameters are critical for the editing sample. To address the issue of limited data, the hypernetwork is trained solely using pseudo-samples, each comprising a natural image paired with its CutMix version [62] (see Fig. 1). The optimization objective is to align the predicted probability distribution of the CutMix sample to that of the original. By controlling the sizes of patches used in CutMix and randomly varying their locations, we simulate distribution shifts in backgrounds, contextual objects, and object attributes, creating opportunities to learn generalizable binary masks that effectively respond to real-world failures. Additionally, we apply a sparsity constraint to the binary masks, acting as an indirect, data-free regularizer to promote locality. Once the where-to-edit problem is solved, the how-to-edit phase becomes straightforward: we simply fine-tune the selected parameters using a variant of gradient descent to apply targeted edits. To validate our method, we construct an editing benchmark that exposes the weaknesses of pre-trained ViTs by introducing two types of subpopulation shifts. The first is a natural subpopulation shift [45, 50], with underrepresented natural images of certain categories efficiently identified by the maximum discrepancy (MAD) competition [58]. The second is an artificial subpopulation shift, introduced by synthesized images from high-quality text-to-image generative models like Stable Diffusion [46]. In summary, our key contributions are as follows: ∙∙\bullet∙ a first-of-its-kind model editing method for pre-trained ViTs that leverages meta-learning to prioritize the where-to-edit phase; ∙∙\bullet∙ an editing benchmark that provides valuable resources for future model editing research in CV; ∙∙\bullet∙ an extensive experimental demonstration that our method achieves the best Pareto front between generalization and locality on the proposed benchmark, while offering flexible trade-offs in the how-to-edit phase."
https://arxiv.org/html/2411.01925v1,Exploiting Contextual Uncertainty of Visual Data for Efficient Training of Deep Models,"Objects, in the real world, rarely occur in isolation and exhibit typical arrangements governed by their independent utility, and their expected interaction with humans and other objects in the context. For example, a chair is expected near a table, and a computer is expected on top. Humans use this spatial context and relative placement as an important cue for visual recognition in case of ambiguities. Similar to human’s, DNN’s exploit contextual information from data to learn representations. Our research focuses on harnessing the contextual aspects of visual data to optimize data annotation and enhance the training of deep networks. Our contributions can be summarized as follows: (1) We introduce the notion of contextual diversity for active learning CDAL (Agarwal et al., 2020) and show its applicability in three different visual tasks semantic segmentation, object detection and image classification, (2) We propose a data repair algorithm (Agarwal et al., 2022) to curate contextually fair data to reduce model bias, enabling the model to detect objects out of their obvious context, (3) We propose Class-based annotation (Agarwal et al., 2023), where contextually relevant classes are selected that are complementary for model training under domain shift. Understanding the importance of well-curated data, we also emphasize the necessity of involving humans in the loop to achieve accurate annotations and to develop novel interaction strategies that allow humans to serve as fact-checkers. In line with this we are working on developing image retrieval system for wildlife camera trap images and reliable warning system for poor quality rural roads. For large-scale annotation, we are employing a strategic combination of human expertise and zero-shot models, while also integrating human input at various stages for continuous feedback.","“For me context is the key - from that comes the understanding of everything.” - Kenneth Noland Objects in the actual world exhibit typical arrangements, giving information of their interaction with other objects and the context of the overall scene. Humans use this spatial context as an essential cue for visual recognition in natural settings (Biederman et al., 1982). When we look at a complex scene, we perceive it effortlessly and identify the objects even without recognizing them. Context of the objects in the real world helps us solve perceptual inference faster and more accurately. It’s generally observed that objects appearing in a consistent or familiar background are detected more accurately than objects in an inconsistent environment. For instance, we perceive the round disc-like object on the dining table from a distance as a diner plate. Spatial context is an essential factor in facilitating scene understanding and object recognition for both machines and humans. Similar to how humans learn concepts and objects by observing their surroundings, deep learning models leverage large amounts of data, often annotated with labels, to learn diverse representations that can generalize to unseen data. One of the reasons why deep neural networks have done exceptionally well in the past decade is the availability of large diverse datasets. While data is crucial for deep learning, it is often overlooked in a learning setting. The more quality data a model is exposed to, the better it can capture intricate patterns, relationships, and nuances, leading to higher accuracy and robustness. While this works in theory, the sheer scale of data for practical applications comes at a high labor cost to label, especially in very specialized fields like medical or autonomous driving domains where the cost of running simulations to produce ground truth is very expensive. Possible algorithmic solutions like active learning comes to let the algorithm iteratively pick most informative data examples to be labeled from unlabeled datasets in a manner such that it is representative of the underlying data distribution to a near-optimal learner (Settles, 2012). Traditional AL techniques (Lewis and Catlett, 1994; Lewis and Gale, 1994; Joshi et al., 2009) have mostly been based on uncertainty and have exploited the ambiguity in the predicted output of a model. Existing approaches that leverage these cues are still insufficient in adequately capturing the spatial and semantic context within an image and across the selected set. Considering the critical role that data plays in model training, “we argue that along with the quantity of data, the quality of data needs attention.” To this end, we propose that model training should be designed so that models are trained using contextually diverse data to ensure they are accurate and unbiased while being efficiently trained. We thus investigate different aspects of contextual information from the available data and need of human in the loop for efficient annotation."
https://arxiv.org/html/2411.01916v1,Masked Autoencoders are Parameter-Efficient Federated Continual Learners,"Federated learning is a specific distributed learning paradigm in which a central server aggregates updates from multiple clients’ local models, thereby enabling the server to learn without requiring clients to upload their private data, maintaining data privacy. While existing federated learning methods are primarily designed for static data, real-world applications often require clients to learn new categories over time. This challenge necessitates the integration of continual learning techniques, resulting in federated continual learning (FCL). Although advanced prompt-based continual learning methods leverage pre-trained transformers to mitigate catastrophic forgetting, they do not adequately address the non-IID challenges in federated learning. To address both catastrophic forgetting and non-IID issues, we propose to use masked autoencoders (MAEs) as parameter-efficient federated continual learners, called pMAE. pMAE learns reconstructive prompt on the client side through image reconstruction using MAEs. On the server side, it reconstructs the uploaded restore information to capture the data distribution across previous tasks and different clients, using these reconstructed images to finetune discriminative prompt and classifier parameters designed for classification, thereby alleviating catastrophic forgetting and non-IID challenges on a global scale. Experimental results demonstrate that pMAE achieves performance comparable to existing prompt-based methods and can enhance their effectiveness, particularly when using self-supervised pre-trained transformers as the backbone. Code is available at: https://github.com/ycheoo/pMAE.","Federated learning [27, 42, 18, 16] is a privacy-preserving decentralized learning paradigm that enables collaborative model training without requiring clients to upload their private data. Traditional federated learning assumes static data, but in real-world scenarios, clients’ data is often dynamic, with old task data being removed and new task data emerging. To address these dynamic scenarios, continual learning [5, 26, 37, 51] techniques need to be integrated into federated learning, a combination known as federated continual learning (FCL) [44, 8], as shown in Figure 1. In FCL scenarios, new task data is continuously generated on clients, while old task data cannot be fully utilized for model updates. This limitation results in catastrophic forgetting [20, 30], where the model’s performance on previous tasks degrades significantly after training on new tasks. Additionally, due to the non-IID (non-independent and identically distributed) [48, 22] nature of FCL, local data distributions across clients may vary significantly, resulting in a suboptimal global model [22, 46]. Figure 1: Illustration of federated continual learning (FCL), wherein clients sequentially learn over T𝑇Titalic_T tasks. Each client continuously updates its local model with class-imbalanced private data specific to each task, and further transfers the well-chosen parameters to server for aggregation. The aggregated global model needs to maintain discriminability across all observed classes within the task set. The performance drop caused by catastrophic forgetting is due to the absence of old task data, which leads the model to overfit the current task’s data [20, 30]. Similarly, the performance drop caused by non-IID arises because the local model fits the non-IID data of individual clients, preventing the aggregation of a global model that reflects the overall data distribution [22, 46]. If we could effectively capture the distribution of old task data and account for the differing data distributions across clients, both catastrophic forgetting and the non-IID problem could be significantly alleviated. Building on the insights from Zhai et al. [45], we propose to use masked autoencoders (MAEs) as parameter-efficient federated continual learners, called pMAE. Originally introduced as scalable vision learners, MAEs [13] leverage self-supervised learning [4, 13, 53] to learn effective visual representations. Instead of focusing solely on representation learning, pMAE emphasizes the ability of MAEs to reconstruct images from restore information. In pMAE, client-side learning not only prioritizes classification as the primary task but also focuses on reconstruction as a secondary task. On the server side, the images reconstructed from uploaded restore information are utilized to finetune model parameters tailored for classification. Reconstructed images represent distributions from old tasks and diverse client data, thereby enabling the global mitigation of catastrophic forgetting and non-IID challenges in a parameter-efficient manner. In FCL, the limitations of client-side computational resources make it burdensome to train a transformer-based model (such as MAEs) [34, 7, 9] from scratch; therefore, we employ pre-trained transformers [12] as the backbone. To further enhance parameter efficiency, we freeze the parameters of the pre-trained transformer and leverage prompt tuning [21, 17]. Specifically, clients utilize discriminative prompt and classifier parameters for classification while employing reconstructive prompt for reconstructing masked images. After completing training, clients randomly extract restore information from a subset of the images and upload them to the server. The server then reconstructs images using reconstructive prompt and restore information, finetuning discriminative prompt and classifier parameters in the process, which mitigates performance degradation from catastrophic forgetting and non-IID challenges on a global scale. Given the generality of our approach, it can be directly integrated with existing prompt-based methods [39, 38, 31] to enhance their performance. Our contributions can be summarized as follows: - We propose to use masked autoencoders (MAEs) for FCL tasks, mitigating catastrophic forgetting and non-IID issues globally by reconstructing images on the server. - We introduce prompt tuning as a parameter-efficient method for finetuning pre-trained transformers, employing discriminative prompt for the classification task and reconstructive prompt for the reconstruction task. - Experimental results indicate that pMAE achieves performance comparable to existing prompt-based methods and can be directly integrated to enhance their effectiveness, particularly when utilizing self-supervised pre-trained transformers as the backbone."
https://arxiv.org/html/2411.01893v1,A Global Depth-Range-Free Multi-View Stereo Transformer Network with Pose Embedding,"In this paper, we propose a novel multi-view stereo (MVS) framework that gets rid of the depth range prior. Unlike recent prior-free MVS methods that work in a pair-wise manner, our method simultaneously considers all the source images. Specifically, we introduce a Multi-view Disparity Attention (MDA) module to aggregate long-range context information within and across multi-view images. Considering the asymmetry of the epipolar disparity flow, the key to our method lies in accurately modeling multi-view geometric constraints. We integrate pose embedding to encapsulate information such as multi-view camera poses, providing implicit geometric constraints for multi-view disparity feature fusion dominated by attention. Additionally, we construct corresponding hidden states for each source image due to significant differences in the observation quality of the same pixel in the reference frame across multiple source frames. We explicitly estimate the quality of the current pixel corresponding to sampled points on the epipolar line of the source image and dynamically update hidden states through the uncertainty estimation module. Extensive results on the DTU dataset and Tanks&Temple benchmark demonstrate the effectiveness of our method. The code is available at our project page: https://zju3dv.github.io/GD-PoseMVS/.","Multi-view stereo matching (MVS) is a crucial technique in 3D reconstruction, which aims to recover robust and reliable 3D representations from multiple RGB images [1; 2; 3]. Traditional methods [4; 5; 6] rely on hand-crafted similarity metrics and regularizations to compute dense correspondences between the input images. These methods are prone to degradation in challenging scenarios, such as varying illumination, textureless regions, and occlusion regions. Recently, learning-based methods [7; 8; 9; 10; 11] directly learn discriminative features from the input images through neural networks such as CNN and Transformers. By sampling some possible depth hypothesis within a given depth range, they warp the features from the source images to the reference view (i.e., the plane sweep algorithm [12]) and compute the cost volume, which is then regularized also through the neural network to obtain the final depth maps. However, obtaining a suitable depth range is non-trivial when applied in real-world scenarios while these methods are generally sensitive to the depth range, which limits their application. To get rid of the dependence on depth range, some methods [13; 14; 15] transform the regression problem in the given depth space into a matching problem on the epipolar lines. Similar to optical flow [16] and feature matching [17; 18; 19; 20], these methods also adopt a pair-wise manner. For example, DispMVS [14] computes the depth map of the source image multiple times through pairs that contain different source images and computes the final depth map by weight of sum. However, the pair-wise manner neglects the inter-image correspondence between the source images and could lead to sub-optimal solutions. Meanwhile, although DispMVS mitigates the influence of depth priors on constructing the 3D cost volume, its initialization based on depth range can still lead to significant performance degradation when the depth range error is too large, as shown in Fig. 1. We argue that these methods need to consider all the source images at the same time. Our ideas are inspired by the recent methods [21; 22] of optical flow which concurrently estimate optical flows for multiple frames by sufficiently exploiting temporal cues. However, we find these frameworks cannot be trivially applied in the task of multi-view stereo. The reasons are twofold. First, a strong cue in the multi-frame optical flow estimation is that the flow originating from the same pixel belongs to a continuous trajectory in the temporal dimension. Additionally, the frames are sequentially aligned along this temporal dimension. Such inductive bias makes it easy to learn. But in the context of multi-view stereo, the source images may be captured in no particular order, lacking a similar constraint of continuity. Unlike optical flow, the input images in multi-view stereo are unordered. These distinctions pose a significant challenge when attempting to adapt the multi-frame optical flow framework for use in multi-view stereo. Second, the arbitrary positions and viewing angles of the source images, coupled with potentially large temporal gaps between captures, exacerbate issues such as varying illumination, significant viewport differences, and occlusions which call for new designs. Based on the above observations, in this paper, we propose a novel framework that gets rid of the depth range assumption. Unlike some recent methods [13; 14; 15] that work in a pair-wise manner, the proposed method estimates the depth maps of a reference image by simultaneously considering all the source images. To address the first issue, we design careful injection of geometric information into disparity features using 3D pose embedding, followed by multi-frame information interaction through an attention module. Subsequently, we encode multi-view relative pose information and geometric relationships between specific sampled points into 3D pose embedding, which is subsequently transferred to the Multi-view Disparity Attention (MDA) module. This method efficiently incorporates the relationship between depth and pixels within the network, facilitating improved information integration across multiple frames. Second, to mitigate the challenge of fluctuating image quality stemming from occlusion and other factors, we maintain and update the disparity hidden features to reflect the depth uncertainty of the current sampling point for each iteration. We design the disparity feature encoding module to learn disparity features along the epipolar lines of multi-view frames. This approach enables us to explicitly characterize occlusion scenarios for each pixel across diverse source images and dynamically adapt them during epipolar disparity flow updates. Consequently, the auxiliary information is furnished for subsequent information fusion within the module. Furthermore, we designed a novel initialization method to further eliminate the influence of the depth range compared to DispMVS [14]. In summary, our contributions can be highlighted as follows: (1) A multi-view disparity transformer network, which facilitates the fusion of information across multi-view frames, (2) A specially designed 3D pose embedding which is utilized to implicitly construct relationships of the epipolar disparity flow among multi-view frames, and (3) An uncertainty estimation module and dynamically updated hidden states representing the quality of source images during iterations. We evaluate our method against other MVS methods on the DTU dataset [23] and Tanks&Temple dataset [24], and demonstrate its generalization in Fig. 1. Figure 1: The robustness testing on the depth range. Under identical training configurations, our method exhibits superior robustness to variations in depth range compared with two state-of-the-art methods [13; 14]. The red markings denote the actual depth range used during training."
https://arxiv.org/html/2411.01889v1,LiDAttack: Robust Black-box Attack on LiDAR-based Object Detection,"Due to the significantly increasing number of autonomous driving systems incorporated with LiDAR sensors, the point cloud captured by LiDAR for object detection plays an indispensable role. In particular, deep neural network (DNN) based point cloud object detection has achieved dominant performance compared with traditional methods. However, since DNN is vulnerable to carefully crafted adversarial examples, adversarial attack on LiDAR sensors have been extensively studied. In all, these attacks are challenged from three aspects: (i) effective - they’re generally effective in white-box scenarios, but their performance will significantly degrade in black-box scenarios; (ii) robustness - they are much less effective when exposed to the diverse changes in the real world (i.e., angle and distance changes);(iii) concealing - attacks will be defended since most of them overlook the stealthy goal of escaping detection. To address these challenges, we introduce a robust black-box attack dubbed LiDAttack. It utilizes a genetic algorithm with a simulated annealing strategy to strictly limit the location and number of perturbation points, achieving a stealthy and effective attack. And it simulates scanning deviations, allowing it to adapt to dynamic changes in real world scenario variations. Extensive experiments are conducted on 3 datasets (i.e., KITTI, nuScenes, and self-constructed data) with 3 dominant object detection models (i.e., PointRCNN, PointPillar, and PV-RCNN++). The results reveal the efficiency of the LiDAttack when targeting a wide range of object detection models, with an attack success rate (ASR) up to 90%. To evaluate the robustness of LiDAttack, experiments at different distances and angles are conducted, and the results show that LiDAttack can maintain consistent performance over a certain range. The practicality of LiDAttack is further validated in the physical world both indoors and outdoors. The results show that LiDAttack-generated adversarial objects can still maintain effective attack in the physical world. In order to achieve a concealed attack, we limit the volume of the generated adversarial object to less than 0.1% of the volume of the target object to achieve ASR up to 90%. The code is available at https://github.com/Cinderyl/LiDAttack.git.","LiDAR sensor is an advanced technology that uses lasers to analyze the surrounding environment for wide-ranging areas, such as autonomous driving system. It calculates distance and 3D effects by measuring the time gap to fire a laser pulse, and returns it from the target. Among other things, the LiDAR sensor is capable of generating a point cloud for object detection[1, 2, 3], enabling real-time monitoring of obstacles, pedestrians, and other vehicles around the target one. Thus, the autonomous driving system can make accurate decisions to ensure safe driving. From the perspective of representation learning strategies of the irregular laser point cloud, object detection can be broadly cast into three categories, i.e., point-based methods[3, 22, 23, 24], voxel-based methods[1, 25, 26, 27], and point-voxel based methods[28, 29, 30, 31]. Most of them rely on deep neural network (DNN) to process point cloud data due to the help of DNN’s powerful extraction and learning capabilities. Not surprisingly, DNN-based point cloud object detection has achieved dominant performance. However, recent research[7, 6] has revealed that DNN is susceptible to adversarial attack, in which imperceptibly perturbed examples will easily manipulate the DNN make incorrect predictions. Due to DNN’s wide application in some safety-critical areas such as autonomous driving systems’ LiDAR-based object detection[4, 5], the carefully crafted adversarial example of LiDAR point cloud will be a serious security threat. Furthermore, the vulnerability of LiDAR sensors towards physical attack will threat the autonomous driving system in a more direct manner. These attacks can be categorized into three groups, i.e., laser-based attack, object-based attack, and location-based attack. The laser-based attacks [8, 11, 10, 9, 12] are conducted by manipulating the signal detection system of a LiDAR sensor. Specifically, an attacker utilizes a photodiode to receive laser pulses from the LiDAR, and triggers the attack by activating a delay component, thereby simulating a real echo pulse. Generally, these attacks require prior knowledge of the victim’s LiDAR laser emission frequency, so as to take the synchronization of the jammer and the attacked LiDAR into account. They fall under the category of white-box attack. Laser-based attack exhibits heightened sensitivity to the movement of the target object, resulting in a lack of robustness. For object-based attacks[10, 19, 16, 14, 18, 17], objects with adversarial shapes are proposed to attack LiDAR sensors. These adversarial objects are large and unusually shaped, resulting in inadequate concealment. The location-based attack[20] aims to find the key confrontation location of the target object and fly the drone at the target location, but they are mostly effective in some scenarios, and the target is obvious and expensive. In all, the existing works are still challenged from three aspects. First, they lack concealability, leading to easy detection. Second, they are susceptible to changes in the environment and targets, thus they lack adversarial robustness. Third, most of these works are generally designed for specific scenario, while difficult to transfer to other applications, thus limited in generalizability. To address these issues, we ask: “ Can we implement a black-box attack to generate perturbation points to achieve a stealthy and robust physical attack with a high attack success rate (ASR) ?” To answer this question, a novel black-box adversarial attack is proposed based on genetic simulated annealing algorithm (GSA), denoted as LiDAttack. Specifically, two scenarios are considered, i.e., hiding the target vehicle and detecting the target object as some others. In both scenarios, they can be successfully launched by placing a 3D printing optimally obtained from perturbation points near the target object. As shown in Fig. 1, the left column indicates that the object detection model can correctly detect the target object in normal cases, and when the adversarial object is generated using LiDAttack and placed at the target location shown as the right column, object detection models fail to detect the target object or identify the target object as others, so that LiDAttack conducts the attack successfully. Figure 1: Normal detection and adversarial scenarios. The left column indicates that the target object can be detected normally, and the right column indicates that the target object cannot be detected or misdetected by adding adversarial objects. In specific, the research questions are addressed as follows. (i) Can concealed adversarial objects be placed next to the target object for the purpose of a stealthy adversarial attack? To address this challenge, the position of the perturbation point was carefully adjusted during the generation process to ensure that it remained within 0.2 meters away from the target object. This strategic placement is aimed at achieving effective adversarial attacks and sustained concealment. (ii) Whether the black-box attack used can achieve a high ASR? Trained adversarial examples are evaluated on three state-of-the-art point cloud object detection models, i.e., PointRCNN[3], PointPillar[1], and PV-RCNN++[34]. The experiment proves that the ASR can reach more than 90% on the object detection models. (iii) How robust is LiDAttack? The angular and distance deviations that exist in LiDAR are simulated in the digital domain, and the experimental results show that a certain range of deviations has only a small effect on attack performance. Furthermore, the robustness of LiDAttack as an effective physical attack in real-world scenarios is further verified. In summary, the main contributions are as follows. • A novel black-box attack for point cloud object detection using GSA is proposed. LiDAttack achieves a highly stealthy attack by strictly controlling the distance between the perturbation point and the object as well as the distance between the perturbation points, so that they naturally blend into the background. • An adaptive genetic algorithm is used to improve attack effectiveness and convergence speed. In the local search, simulated annealing algorithm is used to find a better solution, which involves accepting the probability of a worse solution to avoid falling into the local optimal solution trap. This approach combines the advantages of global and local search to find better solutions in a wider search space and improve the robustness of the algorithm. • LiDAttack shows good performance on three datasets (i.e., KITTI, nuScenes, and self-constructed data) against wide-ranging detection models, covering point-based (e.g., PointRCNN), voxel-based (e.g., PointPillars), and hybrid point-voxel based (e.g., PV-RCNN++). LiDAttack can effectively break through the simple random sampling (SRS) defense mechanism. In addition, by applying LiDAttack combined with adversarial training, the robustness of the target detection models can be significantly enhanced."
https://arxiv.org/html/2411.01870v1,Mining and Transferring Feature-Geometry Coherence for Unsupervised Point Cloud Registration,"Point cloud registration, a fundamental task in 3D vision, has achieved remarkable success with learning-based methods in outdoor environments. Unsupervised outdoor point cloud registration methods have recently emerged to circumvent the need for costly pose annotations. However, they fail to establish reliable optimization objectives for unsupervised training, either relying on overly strong geometric assumptions, or suffering from poor-quality pseudo-labels due to inadequate integration of low-level geometric and high-level contextual information. We have observed that in the feature space, latent new inlier correspondences tend to cluster around respective positive anchors that summarize features of existing inliers. Motivated by this observation, we propose a novel unsupervised registration method termed INTEGER to incorporate high-level contextual information for reliable pseudo-label mining. Specifically, we propose the Feature-Geometry Coherence Mining module to dynamically adapt the teacher for each mini-batch of data during training and discover reliable pseudo-labels by considering both high-level feature representations and low-level geometric cues. Furthermore, we propose Anchor-Based Contrastive Learning to facilitate contrastive learning with anchors for a robust feature space. Lastly, we introduce a Mixed-Density Student to learn density-invariant features, addressing challenges related to density variation and low overlap in the outdoor scenario. Extensive experiments on KITTI and nuScenes datasets demonstrate that our INTEGER achieves competitive performance in terms of accuracy and generalizability. [Code Release]","Point cloud registration is a fundamental task in autonomous driving and robotics. It aims to align two partially overlapping point clouds with a rigid transformation. Learning-based methods have achieved remarkable success in outdoor point cloud registration[1, 2, 3, 4, 5]. PCAM[1] pioneered the integration of low-level geometric and high-level contextual information, inspiring subsequent works[2, 3, 4, 5]. However, these supervised methods suffer from poor generalizability and reliance on costly pose annotations[6, 7, 8], underscoring the need for unsupervised methods to address these challenges in real-world applications. Despite recent progress[9, 10, 11, 12] in unsupervised registration methods, the task remains challenging and underexplored, especially in outdoor scenarios where LiDAR point clouds are large-scale and complexly distributed. Some methods[9, 10, 11] optimize photometric and depth consistency, limiting their applicability to indoor scenarios where RGB-D data and differentiable rendering are feasible. Others[13, 14] learn global alignment and neighborhood consensus, but struggle with low overlap and density variation in outdoor settings. Recent advances resort to pseudo-label-based frameworks, achieving promising results in outdoor scenarios[12, 15]. However, they rely solely on geometric cues to mine and filter pseudo-labels, neglecting the complementarity of high-level contextual information in feature space. Their partiality results in incomplete scene perception, leading to noisy and suboptimal optimization objectives. Figure 1: (1) Motivation: new inliers (outliers) tend to cluster around latent positive (negative) anchors that represent existing inliers (outliers) in the feature space, respectively. (2) Performance: pseudo-labels from INTEGER are more robust and accurate than the previous state-of-the-art EYOC[12]. Various 2D [16, 17] and 3D vision tasks [18, 19, 20, 2, 5, 4] have benefited from integrating both low-level and high-level information. In point cloud registration, as illustrated in Fig. 1 (Left), we observe that potential inliers (outliers) tend to cluster around positive (negative) anchors that summarize the features of existing inliers (outliers) in the feature space, respectively. This suggests that high-level contextual information is adept at discovering inliers from a global perspective of the scene. Meanwhile, low-level geometric cues have proven effective in rejecting outliers[13, 21, 22, 23]. Inspired by this, we propose a novel method, termed INTEGER, which adopts a teacher-student framework to mINe and Transfer fEature-GEometry coheRence for unsupervised point cloud registration. Specifically, our method starts by initializing a teacher with synthetic pairs generated from each point cloud scan, and then transfers to real point cloud pairs with a teacher-student framework. Building upon our observations, we introduce the Feature-Geometry Coherence Mining (FGCM) module for the teacher, which first adapts the teacher to each mini-batch of real data to establish a denoised feature space. Reliable pseudo-labels, including correspondences and anchors, are then generated based on our key observation by iteratively mining potential inliers based on their similarity to anchors and rejecting outliers via spatial compatibility [21]. These robust pseudo-labels mined by FGCM not only accurately include inlier correspondences as shown in Fig. 1 (Right), but also aggregate effective representations of inliers and outliers from the teacher. We refer to this characteristic as feature-geometry coherence. To further enhance robustness and transfer feature-geometry coherence to the student, we propose Anchor-Based Contrastive Learning (ABCont) for contrastive learning with anchors. Meanwhile, we design a succinct and efficient Mixed-Density Student (MDS) for the student to learn density-invariant features using teacher’s anchors, overcoming density variation and low overlap in distant scenarios. We extensively evaluate our method on two large-scale outdoor datasets, KITTI and nuScenes. By exploiting feature-geometry coherence for reliable optimization objectives, INTEGER outperforms existing unsupervised methods by a considerable margin. It even performs competitively compared to state-of-the-art supervised methods, especially in distant scenarios. To the best of our knowledge, our approach is the first to integrate both low-level and high-level information for producing pseudo-labels of unsupervised point cloud registration. Overall, our contributions are threefold: • We propose INTEGER, a novel method to exploit low-level and high-level information for unsupervised point cloud registration, achieving superior performance in complex outdoor scenarios. • We introduce FGCM and MDS for the teacher and student, respectively, to mine reliable pseudo-labels and learn density-invariant features. • We design ABCont to mitigate pseudo-label noise and facilitate contrastive learning with anchors for a robust feature space."
https://arxiv.org/html/2411.01853v2,GVKF: Gaussian Voxel Kernel Functions for Highly Efficient Surface Reconstruction in Open Scenes,"In this paper we present a novel method for efficient and effective 3D surface reconstruction in open scenes. Existing Neural Radiance Fields (NeRF) based works typically require extensive training and rendering time due to the adopted implicit representations. In contrast, 3D Gaussian splatting (3DGS) uses an explicit and discrete representation, hence the reconstructed surface is built by the huge number of Gaussian primitives, which leads to excessive memory consumption and rough surface details in sparse Gaussian areas. To address these issues, we propose Gaussian Voxel Kernel Functions (GVKF), which establish a continuous scene representation based on discrete 3DGS through kernel regression. The GVKF integrates fast 3DGS rasterization and highly effective scene implicit representations, achieving high-fidelity open scene surface reconstruction. Experiments on challenging scene datasets demonstrate the efficiency and effectiveness of our proposed GVKF, featuring with high reconstruction quality, real-time rendering speed, significant savings in storage and training memory consumption. Project page: https://3dagentworld.github.io/gvkf/.","3D surface reconstruction in open scenes holds great significance in various practical applications, such as autonomous driving, virtual reality, urban planning and etc. However, achieving high-fidelity and efficient open scene reconstruction has been a longstanding challenge, due to the trade-off between the rendering quality and the required resources for optimization. In pursuit of this goal, two predominant approaches are Neural Radiance Fields (NeRF) nerf ; snerf ; barron2021mipnerf ; guo2023streetsurf ; kulhanek2023tetranerf and 3D Gaussian Splatting (3DGS) kerbl3Dgaussians ; chen2024periodic ; lin2024vastgaussian ; yan2023streetgaussians based methods. On one hand, NeRF-based implicit representations typically require extensive training and rendering time, which limits the practical use in large-scale scene reconstruction guo2023streetsurf ; wang2023f2nerf ; mi2023switchnerf ; tancik2022blocknerf . On the other hand, 3DGS kerbl3Dgaussians adopts explicit representations, which enables high-quality novel view synthesis while achieving real-time rendering. This makes 3DGS more feasible for efficient scene reconstruction in the applications such as autonomous driving and virtual reality. Recently, there are studies using 3DGS technology for novel view synthesis and surface reconstruction in street scenes and urban environments chen2024periodic ; lin2024vastgaussian ; yan2023streetgaussians ; matchingcubes . For instance, SuGaR guédon2023sugar attempts to reconstruct the 3D surfaces based on Gaussian points. However, it has been noted that overly large and sparse Gaussian points can significantly affect the geometric representations of the scene, particularly in background areas. To overcome these challenges, the 2D Gaussian Splatting (2DGS) huang20242d proposes to use Gaussian surfaces as surfels to represent complex geometries Surfels , thereby improving the surface reconstruction quality. Particularly, 2DGS faces challenges when processing large-scale scenes, as it requires the explicit representation of a large number of Gaussian primitives, leading to significant GPU memory consumption. Therefore, 2DGS still exhibits limitations in novel view synthesis capabilities and the geometric representation of large-scale scenes. In Table 1, we summarize the comparison of 3DGS rendering and volume rendering. To fully leverage the fast rendering advantages of Gaussian alpha blending while achieving effective implicit scene representation, we propose a novel Gaussian Voxel Kernel Functions (GVKF) method. Firstly, GVKF utilizes voxelization to implicitly represent 3DGS, managing the growth and pruning of Gaussian splats. This approach retains the expressive power of explicit Gaussian splats while enabling efficient management of these splats. Secondly, we carefully analyze the intrinsic connection between Gaussian splatting alpha blending rendering and traditional volume rendering from a mathematical perspective. We establish a 3DGS-based method to represent continuous scene opacity density fields through kernel regression. This makes it possible for discrete Gaussians to represent continuous scenes. By replacing the discrete opacity values in original 3DGS rendering pipeline (which can be viewed as collapsed kernel functions) with Gaussian kernel functions, we maintain the advantages of the original 3DGS alpha blending while optimizing the representation of continuous scenes. Moreover, we demonstrate that our proposed rendering method is mathematically consistent with traditional volume rendering. Thirdly, based on our constructed scene opacity representation, which is also known as the scene opacity field, we derive the bidirectional mapping relationship between opacity and the scene surface. This enables direct mesh extraction for scene surface. In summary, our contributions are as follows: • We propose GVKF, an implicit continuous scene reconstruction method that integrates the effectiveness of implicit representation with the fast rasterization advantages of Gaussian Splatting, without the need for computationally intensive volume rendering. • Based on GVKF, we further propose implicit representation of the scene surface, achieving efficient and high-quality scene surface reconstruction. • Experiments demonstrate the usefulness of GVKF in open scenes, showcasing high-quality surface reconstruction accuracy, real-time rendering speeds, and significant savings in storage and memory consumption. Table 1: Comparison of 3DGS rendering and volume rendering methods. Method Math Expression Pros Cons 3DGS Rendering Discrete summation Fast rendering High Mem consumption, Hard to fit 3D continuous surface Volume Rendering Continuous integration Better 3D surface representation Low rendering speed due to continuous sampling"
https://arxiv.org/html/2411.01851v1,Silver medal Solution for Image Matching Challenge 2024,"Image Matching Challenge 2024 is a competition that aims to build 3D maps from sets of images taken in different scenes and environments. The competition requires participants to solve the fundamental computer vision problem of image matching, including dealing with complex factors such as different viewing angles, lighting and seasonal changes. The diversity of image shooting angles and environments places higher demands on the robustness and diversity of the participating algorithms. There are many challenges in image matching, such as differences in surface texture and surroundings that may lead to significant degradation in algorithm performance. Especially when matching images taken from different viewpoints, traditional methods are difficult to cope with these variations, so new methods need to be developed to improve the accuracy and stability of matching. This project develops a Pipeline method, which consists of the following steps: firstly, image data features are extracted using the pre-trained EfficientNet-B7 model, and similar image pairs in the image set are filtered and sorted according to the cosine distance. Then, two key point feature extraction methods, KeyNetAffNetHardNet and SuperPoint, are used to obtain feature point locations. Next, AdaLAM and SuperGlue are used for keypoint matching. Finally, 3D spatial location relations are performed by Pycolmap to determine the final matching results of image pairs. The methodology of this project has achieved an excellent score on the leaderboard of Image Matching Challenge 2024. The project scored 0.167 on the private leaderboard and performed well on the public test set. The experimental results show that the combination of KeyNetAffNetHardNet and SuperPoint has a significant advantage in keypoint detection and matching. The project conducted several experiments to verify the performance of different feature extraction and matching methods and found that the combination of KeyNetAffNetHardNet and SuperPoint worked best.","Image Matching Challenge 2024 [1] The aim of the competition is to construct 3D maps using sets of images from different scenarios, environments, and domains, such as drone shots, deep in dense forests, and nighttime shots. Compared to last year’s Image Matching Challenge 2023 [2], the requirements are very high, such as model diversity, variability, and robustness. Because each photo is taken from a slightly different angle, the shadows may also vary depending on the time of day and season in which the photo was taken. One photo may have been taken from the ground, another from a step, and yet another from a drone. Matching images from different viewpoints is a fundamental computer vision problem that has not yet been fully solved. Factors such as surface texture or surroundings can cause performance degradation in an otherwise well-performing algorithm. The project developed a pipeline approach , the specific process is that the image set of each scene The image data features are first extracted using ImageNet weights from the pre-training model efficientnet-b7 [3], filtered based on the cosine distance, and the first n image pairs of the image set are sorted according to their similarity. Then the retrieved image pairs one by one use two keypoint feature detectors to extract the relevant feature point locations, use two keypoint matching algorithms to match all the matched point locations for the matching calculation, and save the matched pairs (match pairs) successfully. Finally, the image pairs of matched successful points (match pairs) are integrated to exclude the same and then superimposed into pycolmap to calculate the final 3d spatial positional relationship (position and pose estimation). The score of this pipeline is 0.168089 in leaderboard’s private list test and public test. previous study In order to complete the Image Matching Challenge 2024, this project refers to the work of Image Matching Challenge 2023 or 2022, many researchers have disclosed their own use of the method, most of them are divided into the following four parts of image retrieval, feature extraction, matching, 3D reconstruction are implemented separately, this This project introduces the general method of these four parts 1.1 Image Retrieval Image retrieval is a key task in computer vision aimed at identifying highly similar image pairs from large-scale image sets. In recent years, with the advancement of deep learning, most image retrieval methods now rely on deep learning techniques to extract global features of an image. These methods mainly use convolutional neural networks (CNNs) or transformer networks. Convolutional Neural Networks (CNNs) are widely used for image retrieval due to their ability to efficiently capture spatial hierarchies in images.NetVLAD [4] is a notable CNN-based approach that combines the benefits of the VLAD ( Vector Local Aggregation Descriptor ) aggregation layer with deep learning features to significantly improve retrieval performance on a variety of datasets. EfficientNet is another noteworthy CNN architecture that balances the depth, width, and resolution of the network to achieve state-of-the-art results with fewer parameters and computational resources.ConvNeXt [5] is a modernised CNN architecture that combines the design elements of transformer [6,7] networks to further enhance its retrieval capabilities. Transformer networks have received increasing attention for their effectiveness in capturing long-range dependencies and contextual information in images.DINOv2 [8] is a self-supervised transformer-based model that learns robust visual representations without the need for labelled data and performs well in image retrieval tasks.CLIP [9] and its variants EVA-CLIP [10] exploits large-scale image-text pair pre-training to achieve zero-sample migration learning, and achieves significant results in image retrieval benchmarks.ViT [11] introduces a visual transformer architecture and applies the transformer model directly to an image block, setting new records in a variety of visual tasks, including image retrieval. 1.2 Feature extraction With the development of SIFT [12], local features have become an important part of computer vision.The classical SIFT-based approach involves three steps: keypoint detection, orientation estimation, and descriptor extraction. Other method AKAZE [13], which is implemented using a combination of fast significant diffusion and nonlinear scale space. Recently descriptor extraction usually trains deep networks on patches, usually from SIFT keypoints. They include L2-Net [14], HardNet [15], TFeat [16]. There have also been several attempts by researchers to learn keypoint detectors at descriptors alone, including TCDet [17], Keynet [18]. There is also another end-to-end training method that contains SuperPoint [19],D2-Net [20],R2D2 [21],ALIKED [22]. 1.3 Match Local feature matching is usually done by examining keypoints, computing visual descriptors, and using Nearest Neighbor (NN) [23] to search for matches to these features, filtering out incorrect matches. Others are finding matches using robust solvers such as RANSAC [24]. Recent approaches based on deep learning matching, such as SuperGlue [25], LightGlue [26], the former based on a flexible context aggregation mechanism and graph structure based on attention, and the latter based on transformer as well as self-attention, cross-attention. 1.4 Rebuild The most popular frameworks in SFM are VisualSFM [27] and COLMAP [28], the latter being commonly used for projects that require it as it produces ground truth and also serves as a backbone for multi-view tasks."
https://arxiv.org/html/2411.01846v1,KptLLM: Unveiling the Power of Large Language Model for Keypoint Comprehension,"Recent advancements in Multimodal Large Language Models (MLLMs) have greatly improved their abilities in image understanding. However, these models often struggle with grasping pixel-level semantic details, e.g., the keypoints of an object. To bridge this gap, we introduce the novel challenge of Semantic Keypoint Comprehension, which aims to comprehend keypoints across different task scenarios, including keypoint semantic understanding, visual prompt-based keypoint detection, and textual prompt-based keypoint detection. Moreover, we introduce KptLLM, a unified multimodal model that utilizes an identify-then-detect strategy to effectively address these challenges. KptLLM underscores the initial discernment of semantics in keypoints, followed by the precise determination of their positions through a chain-of-thought process. With several carefully designed modules, KptLLM adeptly handles various modality inputs, facilitating the interpretation of both semantic contents and keypoint locations. Our extensive experiments demonstrate KptLLM’s superiority in various keypoint detection benchmarks and its unique semantic capabilities in interpreting keypoints.","Recent advancements in deep learning and natural language processing have facilitated the rise of Large Language Models (LLMs) that display human-like fluency in text comprehension and generation [1, 2, 3, 4, 5, 6, 7]. By incorporating visual information, researchers have developed Multimodal Large Language Models (MLLMs) [8, 9, 10, 11, 12], specifically designed for visual-language tasks, showcasing remarkable abilities in image understanding. However, these models encounter difficulties in capturing fine-grained semantic details, particularly at the point level, which are crucial for various real-world applications. The exploration of MLLMs for keypoint comprehension remains under-explored in the literature. Keypoint detection is a fundamental aspect of computer vision that supports various applications such as controllable image/video generation [13, 14, 15], human-centric perception [16, 17], and AR/VR systems [18, 19, 20]. Initially, research in this field focused on closed-set problems, aiming to predict the locations of predefined semantic keypoints of a certain object category (e.g. human body). As the demand for generalization grew, researchers started investigating the detection of keypoints for novel objects by providing visual prompts (i.e., a support image of a novel object with its keypoint definitions) [21, 22] or utilizing textual prompts (i.e., keypoint names) [23]. Despite significant progress in these areas, existing models still fall short of achieving genuine semantic comprehension of keypoints akin to humans. These models primarily rely on direct learning of visual patterns for keypoint localization through extensive data fitting, while neglecting semantic understanding of the keypoints, thus leading to misinterpretation of the prompts and inaccurate predictions. Moreover, the input-output structures are designed in fixed and predefined formats, restricting their usage to predetermined methods and impeding the flexibility required for interfacing with users. Figure 1: This work aims to address the problem of semantic keypoint comprehension, which aims to understand keypoints across different task scenarios: (a) Keypoint Semantic Understanding takes the object image and a keypoint prompt (i.e., the position of the target keypoint) as inputs, then generate responses that interpret keypoint semantics; (b) Visual Prompt-based Keypoint Detection takes a query image and a support image with a keypoint prompt as inputs and then outputs the corresponding keypoint positions and semantics of the query image; (c) Textual Prompt-based Keypoint Detection utilizes detailed descriptions of keypoints through extensive text, to perform more generalizable keypoint detection. Motivated by the aforementioned challenges, this paper delves into a more comprehensive problem of Semantic Keypoint Comprehension to evaluate the model capability of comprehensively understanding keypoints both visually and semantically. As shown in Fig. 1, we investigate three distinct capabilities via different task instructions: (a) Keypoint Semantic Understanding aims to infer the desired keypoint semantics, given the target image and a keypoint prompt (i.e., the position of the target keypoint) as inputs. It provides the potential for an AI model with high-level visual understanding and analytical capabilities, crucial for tasks such as structural comprehension, action recognition, and medical image analysis. (b) Visual Prompt-based Keypoint Detection, also referred to as category-agnostic pose estimation, takes a query image and a labeled support image with the keypoint annotation as inputs and then outputs the corresponding keypoint positions in the query image. This capability requires the model to acquire keypoint definitions from visual prompts, enabling it to perform cross-class and cross-keypoint localization tasks using sample images provided by users. (c) Textual Prompt-based Keypoint Detection, also known as open-vocabulary keypoint detection, aims to utilize detailed descriptions of keypoints through extensive text for keypoint localization. The keypoint detectors directly receive the human language guidance, facilitating keypoint localization on arbitrary object and keypoint categories in a zero-shot manner. We introduce KptLLM, a novel framework that utilizes an identify-then-detect strategy to address the challenging problem of semantic keypoint comprehension. It formulates all three capabilities depicted in Fig. 1, by first identifying the semantic meaning of keypoints and then detecting their positions via a chain-of-thought approach, akin to human cognition. KptLLM is a unified framework that comprises four key components designed to accommodate various modality inputs and infer both the semantics and location of the keypoint. Specifically, we first extract visual features of both query and support images to obtain query visual tokens and support image features. Secondly, we encode the support keypoint prompt, which describes the position of keypoint on the support image, to generate keypoint prompt embedding. Thirdly, prompt-oriented features are derived by integrating support image features with keypoint prompt embedding, and are utilized to form keypoint prompt tokens. Lastly, LLMs take query visual tokens, keypoint prompt tokens, and task-related language tokens as input, and then generate the semantic description of the target keypoint and its corresponding position on the query image. By harnessing commonsense knowledge in LLMs, KptLLM can assist in keypoint localization of novel object categories, potentially leading to enhanced generalizability in performance. In addition, the chain-of-thought design elicits the powerful keypoint understanding capabilities of LLMs, which helps to distinguish visually ambiguous keypoints (e.g. left and right arms). Extensive experiments demonstrate KptLLM’s superiority on semantic keypoint comprehension, showcasing its unique semantic understanding capabilities in interpreting keypoints and state-of-the-art performance in various keypoint detection benchmarks. In summary, the contributions of this work are three-fold: (1) We pioneer the investigation of a novel problem in semantically interpretable keypoint analysis, termed Semantic Keypoint Comprehension, which aims to enhance MLLMs with improved image understanding at a finer-grained keypoint level; (2) We introduce KptLLM, a unified multimodal model that utilizes an identify-then-detect strategy to effectively address three tasks of semantic keypoint comprehension. KptLLM underscores the initial discernment of semantic significance in keypoints, followed by the precise determination of their positions through a chain-of-thought process. (3) We demonstrate KptLLM’s superiority in various existing keypoint detection benchmarks and its unique semantic capabilities in interpreting keypoints. We hope our work could inspire future research on keypoint understanding and localization, while also fostering enhanced human-AI interface in fine-grained visual understanding."
https://arxiv.org/html/2411.01822v1,Distribution alignment based transfer fusion frameworks on quantum devices for seeking quantum advantages,"The scarcity of labelled data is specifically an urgent challenge in the field of quantum machine learning (QML). Two transfer fusion frameworks are proposed in this paper to predict the labels of a target domain data by aligning its distribution to a different but related labelled source domain on quantum devices. The frameworks fuses the quantum data from two different, but related domains through a quantum information infusion channel. The predicting tasks in the target domain can be achieved with quantum advantages by post-processing quantum measurement results. One framework, the quantum basic linear algebra subroutines (QBLAS) based implementation, can theoretically achieve the procedure of transfer fusion with quadratic speedup on a universal quantum computer. In addition, the other framework, a hardware-scalable architecture, is implemented on the noisy intermediate-scale quantum (NISQ) devices through a variational hybrid quantum-classical procedure. Numerical experiments on the synthetic and handwritten digits datasets demonstrate that the variational transfer fusion (TF) framework can reach state-of-the-art (SOTA) quantum DA method performance.","Quantum computation is demonstrated to take advantage of the properties of quantum mechanics to enhance the performance of computational process [1, 2, 3, 4]. Based on the quantum entanglement and superposition operations, the quantum computing is merged with machine learning (ML) resulting in the field of quantum machine learning (QML) [5]. On the one hand, the application scope of ML can be extended by the QML with performance advantages [6, 7, 8]; on the other hand, the QML methods have effectively promoted the performance in dealing with a variety of tasks compared to the classical ML methods [9, 10, 11]. At present, the QML has developed into a comprehensive and systematic research field. In terms of the types of learning task, both the discriminative [12, 13, 14, 15, 16, 17] and generative tasks [18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30] can be efficiently accomplished with quantum advantages. In addition, the QML methods for different quantum devices such as the universal quantum computer and the NISQ devices are proposed for seeking quantum advantages under different circumstances. The goal of the QML is not to defeat the classical ML, but to enlarge the ML community for further exploration in computing science. As the increase of the scale of available data, the ML method has to deal with learning tasks from different domains simultaneously with domain shift. In addition, data from specific domains such as quantum data are expensive and unlabelled. Traditional ML methods, which obey the independent and identically distributed (IID) assumption, may not be able to handle this situation. In the field of ML, the transfer learning (TL) devotes to accomplish the task in an unprocessed target domain with the acquired knowledge of a processed source domain [31, 32]. As an important sub-realm of TL, domain adaptation (DA) attempts to predict the labels of the unprocessed target domain data set based on the labelled source domain data set. As a crucial method of DA, the distribution domain adaptation (DDA) assumes that the distributions of the labelled source domain data and the unlabelled target domain data are generated from different data distributions. The goal of the DDA is to predict the target labels of an unprocessed dataset by minimizing the distribution discrepancies between the labelled source domain data and the unlabelled target domain data together [33, 34, 35]. In order to achieve a competitive adaptation performance, the distribution adaptation considers to approximate both the marginal and the conditional distributions of the two domains. However, the classical distribution adaptation algorithm can be time-consuming with the increase of the number and dimension of the given data. In addition, the computational complexity of the classical DDA can be prohibited in the quantum data scenario. In addition to the encoding from classical data, quantum data are mostly generated from the evolution of quantum systems. On the one hand, the quantum data and the corresponding labels are rare and expensive to achieve; on the other hand, quantum tomography operations which are extremely time-consuming are necessary for classical ML algorithm to handle the domain shift between different quantum data domains. To deal with this dilemma, different types of quantum transfer learning methods are proposed. Ref. [36] proposed a framework for achieving the procedure of TL through the variational quantum neural networks. They discussed different extensive TL scenarios and presented many representative examples to demonstrate the efficiency of the quantum transfer learning (QTL). Furthermore, for quantum domain adaptation (QDA), a quantum marginal distribution adaptation algorithm, namely the quantum transfer component analysis (QTCA) [37], is proposed to achieve the domain adaptation procedure by approximating the marginal distributions of the source and target domain. In addition, the quantum feature alignment method [38, 39, 40, 41], a representative type of quantum subspace learning algorithm, can be implemented to achieve the procedure of DA on both the universal and NISQ devices. The works above only consider to align the marginal probability distributions of the source and target domain, or roughly the data matrix. The intra-class feature divergence between the two domains are neglected, which may harm the transfer performance. In this paper, to achieve the procedure of TL efficiently, the quantum information infusion channel ℰℰ\mathcal{E}caligraphic_E is designed to fuses the quantum source state ρssubscript𝜌𝑠\rho_{s}italic_ρ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT and the quantum target state ρtsubscript𝜌𝑡\rho_{t}italic_ρ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. Combined with quantum measurement operations and classical post-processing, the target labels can be predicted by aligning both the marginal and conditional probability distributions as depicted in Fig. 1. Concretely, two distribution alignment based TF frameworks are implemented on the universal and NISQ quantum devices respectively. The QBLAS-based TF framework can accomplish the procedure of DA on a universal quantum computer with quadratic quantum speedup compared to the classical TL methods. The variational quantum TF (VQTF) framework can align the distributions between the source and target domain through a variational hybrid quantum-classical procedure on NISQ devices. The quantum advantages of the quantum TF (QTF) frameworks in our work mainly comes from the quantum superposition encoding and the quantum kernel method. In addition to the theoretical computational complexity analysis, numerical experiments on the synthetic dataset for the quantum data scenario and the handwritten digits dataset for the classical data scenario are implemented, demonstrating that the VQTF can achieve SOTA quantum DA performance. The remainder of this paper is arranged as follows. In section 2, the procedure of domain distribution adaptation (DDA) and basic quantum computing operations are briefly overviewed. In the next, the implementation of the QBLAS-based TF is proposed in section 3.1. Subsequently, the implementation of the VQTF on the NISQ devices is provided in section 3.2. To demonstrate the feasibility and effectiveness of the NISQ implementation, two numerical experiments are presented in section 4. Finally, the whole contents are concluded in section 5."
https://arxiv.org/html/2411.01819v1,DiffuMask-Editor: A Novel Paradigm of Integration Between the Segmentation Diffusion Model and Image Editing to Improve Segmentation Ability,"Semantic segmentation models, like mask2former, often demand a substantial amount of manually annotated data, which is time-consuming and inefficient to acquire. Leveraging state-of-the-art text-to-image models like Midjourney and Stable Diffusion has emerged as an effective strategy for automatically generating synthetic data instead of human annotations. However, prior approaches have been constrained to synthesizing single-instance images due to the instability inherent in generating multiple instances with Stable Diffusion. To expand the domains and diversity of synthetic datasets, this paper introduces a novel paradigm named DiffuMask-Editor, which combines the Diffusion Model for Segmentation with Image Editing. By integrating multiple objects into images using Text2Image models, our method facilitates the creation of more realistic datasets that closely resemble open-world settings while simultaneously generating accurate masks. Our approach significantly reduces the laborious effort associated with manual annotation while ensuring precise mask generation. Experimental results demonstrate that synthetic data generated by DiffuMask-Editor enable segmentation methods to achieve superior performance compared to real data. Particularly in zero-shot backgrounds, DiffuMask-Editor achieves new state-of-the-art results on Unseen classes of VOC 2012. The code and models will be publicly available soon.","Semantic segmentation [1, 2] is a critical computer vision component with profound implications across diverse applications. It is the cornerstone for achieving nuanced scene understanding, enabling advancements in autonomous driving, intelligent surveillance, and robotic navigation by deciphering intricate visual environments. Additionally, the precision of semantic segmentation significantly enhances object recognition and localization, thereby improving tasks like image retrieval and object identification. However, existing semantic segmentation models largely depend on robust, manually annotated data, which can be prohibitively expensive. For example, precise pixel-level annotations are required in medical imaging to delineate the boundaries of organs and structures, demanding expertise and meticulous attention. To address this issue, some previous works have shifted focus to weak supervision, which involves training models with a smaller scale of data, such as image-level labels [3, 4, 5, 6, 7] and bounding boxes [8]. This approach aims to balance annotation costs with testing performance. Unfortunately, it still suffers from drawbacks such as low accuracy and complex training strategies. An alternative method involves synthesizing unrealistic datasets with pixel-wise labels. Previous researches [9, 10, 11], primarily based on Generative Adversarial Networks (GANs), have aimed to generate synthetic data to support supervised learning, thus reducing the need for extensive human annotation. However, due to the inherent limitations of GANs, such as heavy reliance on hyperparameters and difficulties in capturing complex open-world correspondences between semantic information and objectives, these methods still face issues with accuracy and stability during training. With the advancement of image generation technologies, diffusion models [12, 13, 14] have garnered considerable attention in the content generation community due to their stability and fidelity. Building on this technology, recent works such as DiffuMask [15] and DiffusionSeg [16] have successfully generated pairs of synthetic images and corresponding segmentation masks. DiffuMask [15] effectively uses intuitive textual prompts like ’a photograph depicting a [class label]’ to generate image-mask pairs. Conversely, DiffusionSeg [16] specializes in generating synthetic datasets tailored for object discovery, which involves identifying prominent objects within an image. While these methods are limited to generating a single object segmentation mask per image, Dataset Diffusion [17] has been used to produce multiple objects along with precise segmentation masks. However, the accuracy of these masks can be significantly affected by occlusion between different objects. Based on the description provided, we introduce DiffuMask-Editor, a novel module for generating synthetic images and masks. Unlike previous work, such as DiffuMask [15], this module can simultaneously generate multiple objects and their corresponding masks in a single image. The method utilizes the advanced text-to-image diffusion model, Stable Diffusion [18], which is trained on large-scale web datasets. As a zero-shot pre-trained model, it enables the generation of any type of picture without the constraints of closely related training sets, which will provide extremely rich data for segmentation models to improve performance. Building on DiffuMask [15], our core lies in how we add multiple objects to a single-instance image. To this end, we creatively combine semantic segmentation and image editing, driven by artificially imposed regulations that allow for accurate mask annotations. Specifically, we introduce three techniques: Adaptive Matching Thesaurus, which helps us select the most coherent and logical objects to add; Foreground Object Location, which identifies the precise location for the placement of objects chosen by the Adaptive Matching Thesaurus; and Image Harmonization, which integrates foreground objects seamlessly with the background. These synthetic data can then be utilized to train any semantic segmentation methods, such as mask2former [19], replacing real data and thus enhancing their robustness. In summary, the main contributions of our work are as follows: • We reverse the traditional segmentation task from image to mask to a process that starts with precise masks and then generates images when synthesizing segmentation datasets. This approach not only simplifies the acquisition of accurate masks for segmentation tasks but also enriches the data domain and enhances performance in open-world scenarios by adding foreground objects. • We introduce the novel Adaptive Matching Thesaurus to determine and filter which types of objects can and cannot be added to the generated images by the diffusion model. We also introduce the Foreground Object Location to select appropriate locations, leveraging search-derived valid positions for mask annotation, and apply Image Harmonization to enhance the realism of synthetic images. • We present the DiffuMask-Editor framework, capable of generating multiple objects in one image along with their accurate masks. To the best of our knowledge, this is the first work to innovatively collaborate on the diffusion model for segmentation and image editing. We have conducted extensive experiments and achieved promising performance gains on two large-scale datasets and open-world scenes."
https://arxiv.org/html/2411.01801v1,Bootstrapping Top-down Information for Self-modulating Slot Attention,"Object-centric learning (OCL) aims to learn representations of individual objects within visual scenes without manual supervision, facilitating efficient and effective visual reasoning. Traditional OCL methods primarily employ bottom-up approaches that aggregate homogeneous visual features to represent objects. However, in complex visual environments, these methods often fall short due to the heterogeneous nature of visual features within an object. To address this, we propose a novel OCL framework incorporating a top-down pathway. This pathway first bootstraps the semantics of individual objects and then modulates the model to prioritize features relevant to these semantics. By dynamically modulating the model based on its own output, our top-down pathway enhances the representational quality of objects. Our framework achieves state-of-the-art performance across multiple synthetic and real-world object-discovery benchmarks.","Object-centric learning (OCL) is the task of learning representations of individual objects from visual scenes without manual labels. The task draws inspiration from the human perception which naturally decomposes a scene into individual entities for comprehending and interacting with the real world visual environment. Object-centric representations provides improved generalization and robustness [9], and have been proven to be useful for diverse downstream tasks such as visual reasoning [44], simulation [45], and multi-modal learning [24]. In this context, OCL which learns such representations without labeled data has gained increasing attention. A successful line of OCL builds upon slot attention [30]. This method decomposes an image into a set of representations, called slots, that iteratively compete with each other to aggregate image features. Reconstructing the original image from the slots, they are encouraged to capture entities constituting the scene. This simple yet effective method has been further advanced by novel encoder or decoder architectures [33, 19, 46, 42], optimization technique [18, 6], and new query initialization strategies [18, 24]. It is worth noting that all these methods are fundamentally considered bottom-up models, as they rely on aggregating visual features without incorporating high-level semantic information from the beginning. This bottom-up approach assumes that visual features within an object are homogeneous and can be clustered in the feature space, which only holds for simplistic objects that can be identified using low-level cues such as color [20]. In complex real-world scenarios where visual entities of the same semantics exhibit diverse appearances, this homogeneity often breaks down, leading to suboptimal object representations [22, 47]. Thus, we take an approach different from the previous line of research: introducing top-down information into slot attention, such as object categories and semantic attributes. Incorporating top-down information enables slot attention to specialize in discerning objects within specific semantic categories. For instance, identifying vehicles in a complex urban environment can be challenging due to the diverse and cluttered nature of the scene. Top-down information can guide the model to prioritize vehicle-specific features, such as wheels and windows. This inhibits the contributions of irrelevant features when computing slots, and enhances the aggregation of visual features of individual vehicles into slots. Nevertheless, devising such a top-down approach is not straightforward since OCL assumes an unsupervised setting without any labeled data, making it hard to identify and exploit the high-level semantics typically obtained from annotated datasets. Figure 1: The overall pipeline of our framework. A top-down pathway is introduced into slot attention to utilize top-down information. The pathway consists of two parts: bootstrapping top-down knowledge and exploiting them. Firstly, semantic information is bootstrapped from slot attention outputs by mapping slots to discrete codes from a learned codebook through vector quantization. Secondly, slot attention is modulated using these codes and its attention maps, transforming it into a self-modulating module. Inner activations are modulated across channels with codes and across space with centered attention maps. Slot attention is then repeated with these modulated activations, yielding more representative slots. We propose a novel framework that incorporates a top-down pathway into slot attention to provide and exploit top-down semantic information; Fig. 1 illustrates of our framework. The pathway consists of two parts: bootstrapping semantics and exploiting them for better representations. Firstly, top-down semantic information is bootstrapped from the output of slot attention itself, by mapping continuous slots to discrete codes selected from a finite learned codebook. Such an approach allows the codebook to learn prevalent semantics in the dataset, with each code representing a specific semantic concept. Thus, semantic information can be bootstrapped without any object-level annotations and used to provide top-down semantic information. Secondly, slot attention is modulated using bootstrapped top-down cues obtained from the first phase, which we call self-modulation. In this phase, the top-down pathway dynamically guides the slot attention by re-scaling its inner activations based on the top-down information. This self-modulation process enables the model to focus on feature sub-spaces where object homogeneity is more consistent, thereby improving its performance in diverse and realistic settings. Our contributions are threefold: • We introduce a method to bootstrap top-down semantic information from the output of slot attention, without requiring any object-level annotations. This allows the extraction of high-level semantic cues from an unsupervised learning process. • We propose a self-modulation scheme that dynamically guides the slot attention’s inner activations to enhance object representation, successfully incorporating the top-down semantic cues extracted. • By integrating the proposed top-down pathway into slot attention, we demonstrate that the performance of object discovery is largely improved on various OCL benchmarks."
https://arxiv.org/html/2411.01800v1,Expanding Sparse Tuning for Low Memory Usage,"Parameter-efficient fine-tuning (PEFT) is an effective method for adapting pre-trained vision models to downstream tasks by tuning a small subset of parameters. Among PEFT methods, sparse tuning achieves superior performance by only adjusting the weights most relevant to downstream tasks, rather than densely tuning the whole weight matrix. However, this performance improvement has been accompanied by increases in memory usage, which stems from two factors, i.e., the storage of the whole weight matrix as learnable parameters in the optimizer and the additional storage of tunable weight indexes. In this paper, we propose a method named SNELL (Sparse tuning with kerNELized LoRA) for sparse tuning with low memory usage. To achieve low memory usage, SNELL decomposes the tunable matrix for sparsification into two learnable low-rank matrices, saving from the costly storage of the whole original matrix. A competition-based sparsification mechanism is further proposed to avoid the storage of tunable weight indexes. To maintain the effectiveness of sparse tuning with low-rank matrices, we extend the low-rank decomposition by applying nonlinear kernel functions to the whole-matrix merging. Consequently, we gain an increase in the rank of the merged matrix, enhancing the ability of SNELL in adapting the pre-trained models to downstream tasks. Extensive experiments on multiple downstream tasks show that SNELL achieves state-of-the-art performance with low memory usage, endowing PEFT with sparse tuning to large-scale models. Codes are available at https://github.com/ssfgunner/SNELL.","Fine-tuning has become a predominant way for adapting large pre-trained models to downstream tasks with limited training samples [13, 9, 24, 23]. Nevertheless, fine-tuning all model parameters requires substantial memory usage and is susceptible to over-fitting, making it costly and infeasible for large-scale models [58, 2, 11]. To address these limitations, parameter-efficient fine-tuning (PEFT) [64, 27, 62, 30, 8, 22] has been proposed to tune a small subset of parameters while keeping other parameters frozen. PEFT methods can be categorized into addition-based and reparameterization-based methods. The former attaches additional parameters to a frozen pre-trained backbone, while the latter adjusts the original parameters in the pre-trained backbone. Figure 1: (a) The high memory usage of sparse tuning arises from taking the whole weight matrix as learnable parameters, in addition to the storage of the tunable weight indexes (typically represented as a binary mask). (b) Our framework (SNELL) only stores the learnable low-rank matrices in the optimizer. (c) Memory usage comparison on pre-trained models with different depths. Addition-based methods [53, 62, 30] have achieved remarkable performance on vision tasks. However, adopting additional parameters incurs extra computational costs during the inference process. In contrast, reparameterization-based methods [5, 7, 27] directly fine-tune the original parameters. These methods select specific parameters, involving reduced memory usage compared to full-parameter fine-tuning. Based on the granularity of parameter selection, one primary approach focuses on specific parameter matrices. For example, Bitfit [5] only adjusts bias to reduce the volume of tunable parameters while Partial-k [30] fine-tunes the last few layers to avoid back-propagation through the entire pre-trained backbone. To further reduce memory usage, LoRA [27] optimizes each selected weight matrix using two low-rank matrices to achieve memory-efficient fine-tuning. Although sufficient in reducing memory usage, these methods usually gain inferior performance compared to addition-based methods [30]. Recently, SPT [22] and GPS [63] found that combining existing PEFT methods with sparse tuning, which only adjusts the most task-related weights in a matrix, can achieve state-of-the-art performance on vision tasks. Concurrently, the effectiveness of sparse tuning has also been observed in NLP tasks [18]. By focusing on individual weights in a matrix, sparse tuning allows for more precise adjustments, thus achieving good performance and mitigated over-fitting risks [18]. However, the performance gained from sparse tuning has been accompanied by high memory usage, as Figure 1(a) shows. Although sparse tuning only updates part of weights in the pre-trained weight matrix, the whole matrix still needs to be stored as learnable parameters in the optimizer and computed for their corresponding gradients in practice. Additionally, sparse tuning necessitates storing the tunable weight indexes, further aggravating the memory demands. The above observation indicates that sparse tuning gains no advantage over full fine-tuning regarding memory usage, especially given the increasing parameter volumes in pre-trained models [58, 2]. A sparse tuning method with low memory usage is urgently required for applications on large-scale pre-trained models. In this paper, we propose a method that conducts Sparse tuning with kerNELized LoRA (SNELL) shown in Figure 1(b). SNELL can adapt pre-trained models to downstream tasks with both low memory usage and strong performance. To reduce memory usage, we decompose the tunable matrix for sparsification into low-rank learnable matrices to store fewer parameters in the optimizer and develop a competition-based method to avoid storing the tunable weight indexes. To improve the performance on downstream tasks, we extend LoRA from a kernel perspective and merge low-rank matrices with nonlinear kernel functions to obtain matrices with higher ranks. Specifically, SNELL updates the pre-trained weight matrix using a sparse low-rank adaptation matrix. This adaptation matrix is first merged with two low-rank learnable matrices and then sparsified toward effective fine-tuning. Compared to storing the whole adaptation matrix, storing low-rank matrices in the optimizer results in lower memory usage. For the sparsification process, we propose a competition-based method inspired by the neuron competition phenomenon in neuroscience [49], avoiding the storage of the tunable weight indexes that incur additional memory usage. The proposed method promotes competition among weights based on their absolute values. Most task-relevant weights are encouraged to have larger absolute values and survive during the fine-tuning process. By setting a sparsity ratio as the hyperparameter and determining tunable weights based on their absolute values in an end-to-end manner, we can eliminate the storage of the tunable weight indexes. In addition to low memory usage, the performance is also critical for model fine-tuning. However, directly merging two low-rank matrices through the inner product leads to the low-rank structure of the adaptation matrix, which narrows the optimization scope of tunable matrices and further limits the expressiveness of sparse tuning. To overcome this bottleneck, we draw inspiration from DyN [45] on weight matrix interpretation based on low-dimensional dynamical systems, and reformulate the merging process with nonlinear kernel functions that increase the rank of the merged adaptation matrix. This new formulation enables a more expressive sparse tuning while maintaining a compact representation with low memory. Extensive experiments are conducted on 24 downstream visual recognition tasks with both plain and hierarchical vision Transformer backbones under supervised and self-supervised pre-training. Results show that SNELL can gain the performance improvement of sparse tuning and the low memory usage of LoRA concurrently. SNELL obtains the state-of-the-art performance on FGVC (91.8% vs. 90.7%) and VTAB-1k (74.6% vs. 74.1%) benchmark with LoRA-level memory usage. Moreover, as Figure 1(c) shows, the low memory-usage advantage of SNELL becomes increasingly apparent as the model size grows, enabling sparse tuning on larger models."
https://arxiv.org/html/2411.01788v1,Non rigid geometric distortions correction - Application to atmospheric turbulence stabilization,"A novel approach is presented to recover an image degraded by atmospheric turbulence. Given a sequence of frames affected by turbulence, we construct a variational model to characterize the static image. The optimization problem is solved by Bregman Iteration and the operator splitting method. Our algorithm is simple, efficient, and can be easily generalized for different scenarios.","In the last decade, long range imaging systems have been developed to improve target identification. One of the main visual effects is distortion due to atmospheric turbulence (known in the literature as “image dancing”). It may occur in many other scenarios: for example, underwater imaging systems, which are subject to scattering effects and video shooting in the summer, suffers from hot air near the ground, and so on. Weak turbulence does not really affect human observers, but it can cause problems for an automatic target recognition algorithm because the shape of the object may be very different from those learned by the algorithm. Fig. 1 shows some examples obtained by a camera in real scenarios. For each video we arbitrarily choose three frames to display here. Figure 1. Sample images. Each row contains three arbitrary frames from different testing turbulence videos. Previous methods have been developed to deal with the turbulence effect in astronomical images. In [19], local filters (Wiener filter, Laplacian regularization and so on) were utilized and local properties were obtained by block partitioning of the image. As a result, some block artifacts appear on the restored images. An interesting work about turbulence modelization for mitigation algorithms was made by Frakes [11, 12]. The authors modeled the turbulence phenomenon by using two operators: fi⁢(x)=Di⁢(H⁢(u⁢(x)))+noisesubscript𝑓𝑖𝑥subscript𝐷𝑖𝐻𝑢𝑥noisef_{i}(x)=D_{i}(H(u(x)))+\text{noise}italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_x ) = italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_H ( italic_u ( italic_x ) ) ) + noise (1) where u𝑢uitalic_u is the static original scene we want to retrieve, fisubscript𝑓𝑖f_{i}italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is the observed image at time i𝑖iitalic_i, H𝐻Hitalic_H is a blurring kernel, and Disubscript𝐷𝑖D_{i}italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is an operator which represents the geometric distortions caused by the turbulence at time i𝑖iitalic_i. Based on this model, the authors of [14] proposed a scheme to evaluate the H−1superscript𝐻1H^{-1}italic_H start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT and D−1superscript𝐷1D^{-1}italic_D start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT operators. The H−1superscript𝐻1H^{-1}italic_H start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT operator is obtained by blind deconvolution, while the correction of the geometrical distortions D−1superscript𝐷1D^{-1}italic_D start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT is computed by an elastic registration algorithm based on diffeomorphic mappings. This approach gives good results but it has two main drawbacks. First, note that it is time consuming to perform the calculations due to the two iterative processes involved in the algorithm. Secondly, the performance is sensitive to the choice of the parameters. Another kind of approach for this problem is to utilize the Kalman filter, which is a statistical tool that recovers a static object from a time series of observations. In [27], the authors successfully use this filter in the turbulence reconstruction problem. However, this method requires a strong time dependence of the frames, therefore the frame rate has to be sufficiently high. It treats the warped frames ordered in time as governed by fluid dynamics, and thus can be characterized by time-dependent differential equations, which is not a practical assumption in some applications. More recently, some efforts were made to propose new mitigation algorithms. In [20], assuming long exposure video capture, the authors propose to use Principal Component Analysis to find the statistically best restored image from a sequence of acquired frames. In [2], the authors use the assumption that for a fixed location in the image, its neighborhood has some high probability to appear with better quality through the time. Then the restored region is a fusion of the best ones. Some spatially variant deblurring was proposed in [18] but the algorithm does not specifically address the problem of geometrical distortions. Following the modelization of Frakes [11], in [30] the authors propose to invert the geometric distortions and the blur. They use some B-Spline registration algorithm embedded in a Bayesian framework with bilateral total variation (TV) regularization. The goal of this paper is to propose a new approach on this problem, principally on the correction of geometrical distortions. We develop a unified framework which uses both an optical flow scheme to estimate the geometrical distortion, and a nonlocal TV based regularization process to recover the original observed scene. The paper is organized as follows: in section 2, we describe the basic model used throughout the whole paper. Section 3 deals with Bregman iteration and the operator splitting operator used in the optimization process. Section 4 provides the whole algorithm and implementation details. Section 5 presents many numerical results obtained by the proposed method on real data. Concluding remarks are provided in section 6."
https://arxiv.org/html/2411.01781v2,MSTA3D: Multi-scale Twin-attention for 3D Instance Segmentation,"Recently, transformer-based techniques incorporating superpoints have become prevalent in 3D instance segmentation. However, they often encounter an over-segmentation problem, especially noticeable with large objects. Additionally, unreliable mask predictions stemming from superpoint mask prediction further compound this issue. To address these challenges, we propose a novel framework called MSTA3D. It leverages multi-scale feature representation and introduces a twin-attention mechanism to effectively capture them. Furthermore, MSTA3D integrates a box query with a box regularizer, offering a complementary spatial constraint alongside semantic queries. Experimental evaluations on ScanNetV2, ScanNet200 and S3DIS datasets demonstrate that our approach surpasses state-of-the-art 3D instance segmentation methods.","Given 3D point clouds, 3D instance segmentation refers to a task that involves identifying and separating individual objects within a 3D scene, including detecting object boundaries and assigning a unique label to each identified object. Its significant role in computer vision has surged corresponding to the demand for 3D perception in various applications, such as augmented/virtual reality (Park et al., 2020; Lehtola et al., 2017), autonomous driving (Yurtsever et al., 2020), robotics (Xie et al., 2021), and indoor scanning (Li et al., 2022). Figure 1. The proposed MSTA3D, a 3D instance segmentation framework, tackles existing challenges by leveraging multi-scale feature representation and spatial query/regularizer. In the literature, 3D instance segmentation methods are commonly categorized into four main approaches: proposal-based (Sun et al., 2023b; Yang et al., 2019; Qi et al., 2017; Liu et al., 2020; Engelmann et al., 2020; Kolodiazhnyi et al., 2024; Hou et al., 2019), grouping-based (Lahoud et al., 2019; Jiang et al., 2020; Han et al., 2020; Liang et al., 2021; Chen et al., 2021; Dong et al., 2022; Vu et al., 2022; Zhong et al., 2022; Vu et al., 2023; Pham et al., 2019), kernel-based (He et al., 2021; Jiang et al., 2020; He et al., 2022; Wu et al., 2022; Ngo et al., 2023), and transformer-based methods (Sun et al., 2023a; Schult et al., 2023; Lai et al., 2023; Lu et al., 2023). Proposal-based methods begin by generating a 3D bounding box and then using it to segment into an instance mask. Grouping-based methods aggregate points into clusters using per-point features, such as semantic or geometric cues, and then segment instances based on these clusters. Kernel-based methods are similar to grouping-based techniques but treat each potential instance as a kernel for dynamic convolution. However, these methods require a high-quality proposal or clustering algorithm that heavily relies on per-point prediction, resulting in a significant demand for computational resources. To address these issues, transformer-based methods have been proposed, treating each potential instance as an instance query and refining it through a series of transformer decoder blocks. However, predicting instances from point clouds inherently presents substantial challenges due to their typically lacking clear structure, unlike the regular grid arrangement found in images. Moreover, managing large-scale input point clouds further requires costly computations and extensive memory resources. Thus, recent transformer approaches leverage superpoints, which roughly offer contextual relationships between object parts with reduced memory usage. Nevertheless, existing transformer-based approaches employing superpoints often suffer from performance degradation due to over-segmentation problems and the unreliability of mask predictions. These challenges are described as follows: (1) Existing methods are prone to over-segmentation, especially when dealing with large objects such as doors, curtains, bookshelves, and backgrounds. Additionally, converting labels from superpoints to points can introduce unreliability into the categorical grouping. (2) The learning process for point-wise classification and aggregation encounters challenges due to the sparse and irregular distribution of observed scene points. Hence, we propose a novel framework that leverages multi-scale superpoint features and simultaneously incorporates global/local spatial constraints. This framework is aimed at mitigating previously mentioned over-segmentation challenges and overcoming the limitations in point-wise classification. Specifically, to capture features at various scales, we generate superpoints at different scales, enabling effective feature representation of large objects and backgrounds as well as small objects. Correspondingly, we introduce a novel attention scheme, named twin-attention, to effectively fuse features at different scales. Moreover, we introduce the concept of box query, in addition to semantic query, which is trained using the proposed twin-attention decoder and refined by the spatial regularizer to enhance the confidence of mask predictions. Furthermore, we utilize this bounding box prediction to enhance the reliability of mask predictions during the inference phase. In summary, the contributions of this study are as follows: • We propose a twin-attention-based decoder for effectively representing multi-scale features to tackle over-segmentation challenges observed in large objects and backgrounds. • We introduce the notion of box query with box regularizer to provide complementary supervision without additional annotation requirements. This enforces spatial constraints for each instance during the query learning process, resulting in enhanced object localization and reduced background noise. • Extensive experiments are conducted on widely-used benchmark datasets, including ScanNetV2 (Dai et al., 2017), ScanNet200 (Rozenberszki et al., 2022), and S3DIS (Armeni et al., 2016), demonstrating the effectiveness of the proposed approach and achieving state-of-the-art results."
https://arxiv.org/html/2411.01777v1,Learning predictable and robust neural representations by straightening image sequences,"Prediction is a fundamental capability of all living organisms, and has been proposed as an objective for learning sensory representations. Recent work demonstrates that in primate visual systems, prediction is facilitated by neural representations that follow straighter temporal trajectories than their initial photoreceptor encoding, which allows for prediction by linear extrapolation. Inspired by these experimental findings, we develop a self-supervised learning (SSL) objective that explicitly quantifies and promotes straightening. We demonstrate the power of this objective in training deep feedforward neural networks on smoothly-rendered synthetic image sequences that mimic commonly-occurring properties of natural videos. The learned model contains neural embeddings that are predictive, but also factorize the geometric, photometric, and semantic attributes of objects. The representations also prove more robust to noise and adversarial attacks compared to previous SSL methods that optimize for invariance to random augmentations. Moreover, these beneficial properties can be transferred to other training procedures by using the straightening objective as a regularizer, suggesting a broader utility for straightening as a principle for robust unsupervised learning.","All organisms make predictions, and their survival generally depends on the accuracy of these predictions. In simple organisms, these predictions are reflexive and operate over short time scales (e.g., moving toward or away from light, heat, or food sources). In more complex organisms, they involve internal state (memories, plans, emotions) and operate over much longer timescales. Prediction has the potential to provide an organizing principle for overall brain function, and a source of inspiration for learning representations in artificial systems. However, natural visual scenes evolve according to highly nonlinear dynamics that make prediction difficult. Recent experiments – both perceptual (in humans) and neurophysiological (in macaques) – indicate that the visual system transforms these complex pixel dynamics into straighter temporal trajectories [henaff2019perceptual, henaff2021primary]. As an alternative to full temporal prediction, straight representations facilitate predictability through linear extrapolations (Fig. 1A, red arrows). Yet, the utility of straightening as a learning principle for organizing representations remains underexplored. Here, we ask whether straightening is sufficiently powerful to be used as a primary objective in self-supervised learning (SSL). Our contributions are as follows: • We developed an SSL objective function that aims to straighten spatio-temporal visual inputs. We demonstrated on simulated data that this objective, coupled with a whitening regularizer that prevents representational collapse, can successfully straighten visual inputs containing both geometric and photometric transformations. • We show that the trained network is effective in extracting and predicting various visual attributes including object identity, location, size, and orientations. • We provide geometric intuition for how straightening yields class separability. • We show that representations learned by straightening are significantly more robust than those learned by multi-view invariance, when trained on the same model architecture and dataset. Moreover, straightening can be used as a regularizer to enhance the robustness of state-of-the-art SSL methods. The implementation can be found at https://github.com/xyniu1/learning-by-straightening. Figure 1: Learning straightened representations. A. Illustration of temporal trajectories, in the space of pixel intensities (left), and in a straightened representation (right). Color indicates digit identity. B. Two-dimensional t-SNE rendering of actual trajectories for the translating digits in our sequential MNIST dataset. Initial (pixel intensity) representation is highly curved and entangled (left). Although the straightening objective is unsupervised (no object labels), the learned representation clearly isolates the trajectories corresponding to different digits (right). 1.1 Related work Temporal invariance. Many successful SSL methods aim to learn representations that are invariant to simple transformations, with additional regularization incorporated to prevent representational collapse (e.g., constant response independent of input). Depending on the exact implementation of these components, three categories of SSL objectives are identified in review [balestriero2023cookbook]: 1) contrastive SSL [chen2020simple, dwibedi2021little], which encourages representation similarity between two augmented views of the same image (positive pairs) and dissimilarity of different images (negative pairs); 2) self-distillation [grill2020bootstrap, chen2021exploring, caron2021emerging], which uses two different encoders to process two views of the same image, and maps the two representations by a predictive projection; and 3) canonical correlation analysis [vicreg, zbontar2021barlow, ermolov2021whitening] that aims to whiten the cross-correlation matrix of neural representations estimated over augmented pairs of the same image. Most of the invariance-based methods subserve tasks that operate on static images rather than spatio-temporal inputs. This is because invariance is a strict constraint and equating representations over time necessarily means losing the time-varying features in the inputs. For example, for a video that contains moving objects, learning invariant representations across frames may help to encode the identity of the object but not its location or relative size. In contrast, straightening is designed to not only capture all predictable features in the spatio-temporal inputs, static and dynamic alike, but also predict their future states using a predefined operator. Temporal prediction. Temporal prediction as a fundamental goal for learning visual representations dates back to [foldiak1991learning]. Many theories rooted in predictability have been successful in characterizing the properties of early visual areas [berkes2005slow, palmer2015predictive, fiquet2024polar]. Invariance is the simplest form of prediction. Linear predictions have been extensively studied. Notably, [oord2018representation, henaff2020data] achieved great success in unsupervised object recognition by learning a linear predictor that maps current states to future states for each step into the future. In [jiang2024dynamic], this paradigm was extended to allow a context-dependent, dynamic selection of linear predictors. Straightening differs from these methods in that straightening is parameter-free and the prediction can adapt to different contexts, while previous methods rely on parametrization that scales quadratically with the feature dimension. The work most similar to ours is [goroshin2015learning], which uses auxiliary architectural elements such as phase-pooling and further relies on an autoencoder structure and a pixel-level prediction loss to prevent information collapse. Our solution uses a much simpler architecture, and we also provide a more extensive quantitative evaluation of the resulting straightened representations. Straightening and robustness. Although straightening has been documented in human perception and macaque physiology, it is not an inherent property of deep neural networks [henaff2019perceptual], including supervised and self-supervised recognition networks, and video prediction networks [toosi2023brain, harrington2022exploring]. Some non-parametric formulations of early visual processes demonstrate a degree of straightness, but the effect does not seem to survive to downstream layers [henaff2019perceptual]. Recently, [toosi2023brain, harrington2022exploring] demonstrated that straightening can be an emerging property of robustified networks: if networks are trained to tolerate Gaussian noise or adversarial perturbations, they would generate straightened responses without being explicitly trained to do so. In this work, we provide the complementary observation: if networks are trained to straighten, the representation is robust to corruptions including Gaussian noise and adversarial perturbations. Apart from straightening, other learning objectives that exploit the temporal structures of natural video statistics have also been shown to improve adversarial robustness, such as temporal classification (classifying frames to the episode they belong to) and temporal contrastive learning (temporally adjacent frames are used as positive examples) in [kong2021models]."
https://arxiv.org/html/2411.01759v1,Automatic Structured Pruning for Efficient Architecture in Federated Learning,"In Federated Learning (FL), training is conducted on client devices, typically with limited computational resources and storage capacity. To address these constraints, we propose an automatic pruning scheme tailored for FL systems. Our solution improves computation efficiency on client devices, while minimizing communication costs. One of the challenges of tuning pruning hyper-parameters in FL systems is the restricted access to local data. Thus, we introduce an automatic pruning paradigm that dynamically determines pruning boundaries. Additionally, we utilized a structured pruning algorithm optimized for mobile devices that lack hardware support for sparse computations. Experimental results demonstrate the effectiveness of our approach, achieving accuracy comparable to existing methods. Our method notably reduces the number of parameters by 89% and FLOPS by 90%, with minimal impact on the accuracy of the FEMNIST and CelebFaces datasets. Furthermore, our pruning method decreases communication overhead by up to 5x and halves inference time when deployed on Android devices. 111The code implementation is available online: https://github.com/NguyenThaiVu/prune˙fl˙project.","Recently, we have witnessed an increasing interest in shifting cloud computing to edge computing (li2021talk). In fact, bringing computation to the edge of computer networks can reduce latency, benefiting real-time applications, such as autonomous driving (jin2024fractional). On the other hand, the data generated, at an unprecedented rate, from billions of edge devices can be used for training and improving AI models (jia2024fedlps; nguyen2023attention). In such scenarios, Federated Learning (FL) has emerged as a promising alternative to process such an extensive amount of data while preserving users’ privacy (mcmahan2017communication). This is achieved by decentralizing the training process of machine learning (ML) models and keeping personal user data on clients’ devices. Thus, training is performed locally and only the model parameters are sent to the central server. The final global model is attained via aggregation of the parameters from all clients. In the FL settings, where the primary training process occurs on devices with limited computational and storage resources, there is growing interest in developing strategies to decrease the model’s footprint. Several studies, for instance, have proposed neural network pruning to reduce model size. The frameworks proposed in (li2021lotteryfl) and (isik2023sparse) apply the lottery ticket hypothesis to the FL settings. The work presented in (qiu2022zerofl) selects the top-k weights of local models to be sent back to the central server for aggregation. In (Bibikar_Vikalo_Wang_Chen_2022), authors propose a sparse training approach tailored specifically for the FL settings. A crucial limitation of such approaches is that they require pruning to be performed on the local devices. This results in higher computational costs when compared to the normal local training. Additionally, the differences in local models lead to different pruning masks, complicating the aggregation of these masks into the unified global model. To overcome these problems, we propose a pruning procedure that is executed on the central server rather than on individual local devices. It is important to note that the aforementioned works rely on unstructured pruning, which removes individual weight elements and results in a sparse network with many zero values. Performing fast computations on sparse matrices requires support from specialized libraries, such as cuSPARSE, or hardware (e.g., NVIDIA Ampere GPU) and clients’ devices participating in FL training often lack such requirements. Additionally, maintaining sparse data structures requires extra storage for information like compressed sparse rows or binary masks. Thus, inspired by the structured pruning technique presented in (li2017pruning), which prunes entire filters rather than individual weights, this paper proposes pruning filters in convolutional networks, maintaining the dense structure of the global model while ensuring compatibility with the simple computational capabilities of client devices. Figure 1: An overview of our pruning scheme in Federated Learning system. While centralized learning allows access to the training data and therefore proper tuning of pruning hyper-parameters, such as the sparsity fraction and the number of pruned layers, decentralized learning presents the inability to examine the whole training data directly during training. This makes it difficult to choose a suitable model architecture. For example, in the FL settings, we cannot perform a grid search to determine the optimal number of filters in a convolutional neural network (CNN). To address this challenge, we propose an automatic pruning algorithm, which dynamically defines the pruning boundaries and removes redundant filters without disrupting the FL procedure. The proposed framework for pruning CNNs in the FL settings is illustrated in Figure 1. The FL training procedure is divided into two sub-stages. First, we perform automatic pruning to identify the well-suited architecture, as depicted in Figure 1-a. This phase involves dynamically eliminating unnecessary filters to reduce the model’s size without yet considering the accuracy of the global model. After obtaining the final global model architecture, we proceed with the standard FL training phase (Figure 1-b). The second stage is dedicated to training the final global model to achieve the desired accuracy for the specific task. Below, we summarize the four main contributions of our work: • We designed a pruning scheme for FL that significantly reduces redundant parameters/FLOPS in the global model while maintaining the desired accuracy. Additionally, the pruning procedure runs on the central server, reducing computational burdens for local clients. • We proposed a structured pruning algorithm that automatically determines the pruning boundary and preserves a dense network structure, ensuring fast computation on basic client hardware. • Extensive experiments show that our pruning algorithm is effective across various architectures, including CNNs, ResNet, and Inception. Additionally, the pruned models significantly reduce inference time on real-world Android devices. • We also present supplemental studies showing improvements in communication costs and consistency when training with varying random numbers of selected clients in the FL system. The rest of the paper is organized as follows. Section 2 presents the related work in network pruning, besides summarizing the differences between our paper and existing work. Section 3 introduces our proposed method to prune convolutional neural networks in the FL settings. In Section 4, we conduct extensive experiments to validate the superior performance of our method, followed by the concluding remarks in Section 5."
https://arxiv.org/html/2411.01756v1,ChatTracker: Enhancing Visual Tracking Performance via Chatting with Multimodal Large Language Model,"Visual object tracking aims to locate a targeted object in a video sequence based on an initial bounding box. Recently, Vision-Language (VL) trackers have proposed to utilize additional natural language descriptions to enhance versatility in various applications. However, VL trackers are still inferior to State-of-The-Art (SoTA) visual trackers in terms of tracking performance. We found that this inferiority primarily results from their heavy reliance on manual textual annotations, which include the frequent provision of ambiguous language descriptions. In this paper, we propose ChatTracker to leverage the wealth of world knowledge in the Multimodal Large Language Model (MLLM) to generate high-quality language descriptions and enhance tracking performance. To this end, we propose a novel reflection-based prompt optimization module to iteratively refine the ambiguous and inaccurate descriptions of the target with tracking feedback. To further utilize semantic information produced by MLLM, a simple yet effective VL tracking framework is proposed and can be easily integrated as a plug-and-play module to boost the performance of both VL and visual trackers. Experimental results show that our proposed ChatTracker achieves a performance comparable to existing methods.","Visual object tracking stands as a foundational and challenging task in the computer vision realm. It aims to locate an object in each frame of a video given an initial object box. Recently, Vision-Language (VL) trackers leverage additional natural language descriptions to boost their efficacy. For instance, the shape of a target may change during tracking. However, the semantic information of the target, such as its category or material, remains the same. This makes language text more potential and stable to describe such an appearance-changing object than an image template solely. Despite these advantages, current VL trackers Li et al. (2023); Zhou et al. (2023); Guo et al. (2022) are still inferior to SoTA Visual Trackers Wei et al. (2023); Cui et al. (2022) on mainstream benchmarks Fan et al. (2021); Muller et al. (2018). We identify the following reasons for this: 1) VL trackers heavily rely on manual annotations, which often contain ambiguous language descriptions. 2) Manual textual annotations primarily focus on the tracking target and neglect the semantic information embedded in the text, such as the presence of various background objects and their relations to the target. However, most VL trackers mainly focus on better aligning the vision-language modal features Ma et al. (2024b); Zhou et al. (2023); Li et al. (2023), overlooking how inaccurate textual annotations in the dataset can adversely affect VL trackers performance. In the last few years, Large Language Models (LLMs) Achiam et al. (2023) and Multimodal Large Language Models (MLLMs) Touvron et al. (2023); Zhu et al. (2023); Yang et al. (2023) have progressed rapidly. The wealth of world knowledge encoded in the pre-trained LLMs and MLLMs, along with their capabilities in processing and understanding VL information, has attracted immediate attention from the research communities. Inspired by these advancements, we contemplate whether they can be utilized to achieve better language descriptions for visual tracking. However, we find that directly using the language descriptions generated by MLLMs hardly improves tracking performance as shown in Fig. 1. Two primary causes are identified: 1) The VL tracker is unable to comprehend the language descriptions directly from the MLLM, resulting in the VL trackers identifying incorrect targets. This is because the MLLM and VL trackers are trained on different datasets, leading to a mismatch between the text generated by the MLLM and the visual content in the VL tracker’s latent space. 2) The inherent limitations of MLLMs in understanding alternate modalities exacerbate the phenomenon of “hallucination” in multi-modal contexts Cui et al. (2023), leading to outputs that are inaccurate or even erroneous. To address the aforementioned issues, we propose a novel framework, ChatTracker, to integrate MLLMs into visual object tracking. By utilizing the capabilities of MLLMs, a Reflection-based Prompt Optimization (RPO) module is introduced to generate accurate language descriptions of both foreground and background objects. The core idea is to provide feedback to the MLLM about inaccuracies or incomprehensible content of initial language outputs with the VL tracker. This feedback mechanism drives the iterative refinement of the MLLM’s output, making it more aligned with the image content and more understandable for the VL tracker, thus effectively addressing the above mentioned issues. In addition, a novel semantic tracking module is proposed to effectively utilize the semantic information obtained from the MLLM and yield the final tracking results. Comprehensive experiments on several widely recognized public datasets are conducted, including LaSOT Fan et al. (2021), TrackingNet Muller et al. (2018), TNL2K Wang et al. (2021), and OTB Li et al. (2017), to demonstrate the effectiveness and efficiency of our proposed method. Figure 1: Comparison of different text generation methods. (a) shows manual descriptions and GPT-4V generated descriptions of the tracking target, which are both sub-optimal for tracking. (b) illustrates the generation method used in ChatTracker. Our main contributions are summarized as follows: 1. We propose ChatTracker, a novel framework that leverages MLLMs for visual object tracking. To the best of our knowledge, this is the first work to incorporate MLLMs into tracking frameworks. It offers a plug-and-play module enhancement for existing visual and VL trackers with limited computational overhead. 2. We introduce a Reflection-based Prompt Optimization (RPO) module to narrow the knowledge gap between the VL tracker and the MLLM. By reflecting on the feedbacks from tracking, the RPO module can iteratively optimize the prompt for the MLLM and finally produces accurate and relevant descriptions for tracking targets. These descriptions are superior in both tracking performance and image-text alignment compared to manually annotated texts in datasets. 3. Our proposed ChatTracker achieves SoTA performance on several tracking datasets. We conduct extensive experiments including ablation studies to demonstrate the effectiveness of the proposed method and its individual modules."
https://arxiv.org/html/2411.01748v1,Rotation Perturbation Robustness in Point Cloud Analysis: A Perspective of Manifold Distillation,"Point cloud is often regarded as a discrete sampling of Riemannian manifold and plays a pivotal role in the 3D image interpretation. Particularly, rotation perturbation, an unexpected small change in rotation caused by various factors (like equipment offset, system instability, measurement errors and so on), can easily lead to the inferior results in point cloud learning tasks. However, classical point cloud learning methods are sensitive to rotation perturbation, and the existing networks with rotation robustness also have much room for improvements in terms of performance and noise tolerance. Given these, this paper remodels the point cloud from the perspective of manifold as well as designs a manifold distillation method to achieve the robustness of rotation perturbation without any coordinate transformation. In brief, during the training phase, we introduce a teacher network to learn the rotation robustness information and transfer this information to the student network through online distillation. In the inference phase, the student network directly utilizes the original 3D coordinate information to achieve the robustness of rotation perturbation. Experiments carried out on four different datasets verify the effectiveness of our method. Averagely, on the Modelnet40 and ScanobjectNN classification datasets with random rotation perturbations, our classification accuracy has respectively improved by 4.92% and 4.41%, compared to popular rotation-robust networks; on the ShapeNet and S3DIS segmentation datasets, compared to the rotation-robust networks, the improvements of mIoU are 7.36% and 4.82%, respectively. Besides, from the experimental results, the proposed algorithm also shows excellent performance in resisting noise and outliers.","Point cloud, collections of densely distributed unordered points in three-dimensional space, is often regarded as two-dimensional manifolds embedded in three-dimensional space, offering rich information for numerous tasks such as 3D reconstruction [1], object recognition [2], and scene understanding [3]. In recent years, deep learning has achieved remarkable accomplishments in the field of point cloud interpretation [4, 5, 6]. This technological breakthrough is primarily attributed to the efficient learning and representation capabilities of deep neural networks that empower them to discover and harness the abundant geometric information and spatial structures embedded within point cloud data. To be specific, existing algorithms typically perceive point cloud as two-dimensional manifolds embedded in a vertically oriented three-dimensional space, leveraging hierarchical structures to extract intrinsic feature information of point cloud so as to realize high-precision classification and segmentation tasks [7, 8, 9]. Nevertheless, in real-world point cloud acquisition processes, apart from random errors such as noise and outliers [10], systematic errors including rotation perturbation caused by equipment offset or system instability [11] also pose a significant challenge to the robustness and stability of point cloud analysis. Taking the classical network PointNet++ [12] as an example, one can see from Fig. 1 that when unforeseen deviations occur in the test data, the segmentation results are quite different compared with the original segmentation results. Figure 1: Different segmentation results of PointNet++. (a) Original segmentation results; (b) The segmentation results when the axis is rotated 30°. In order to solve this complex issue, data augmentation is usually seen as a direct solution, but maintaining efficient and effective information learning capabilities is difficult due to the redundant calculations and imbalanced data distribution [13]. Furthermore, some cutting-edge researches also focus on achieving rotation robustness within the architectures of networks, which can be mainly classified to three categories. The first one is committed to reconstructing the local coordinate system of manifold to accurately extract geometric features such as distances and angles between adjacent points [14]. The second one performs regularizations on the manifold coordinates to correct the pose and embed them into the Euclidean space from different perspectives [15]. The last one maps the manifold coordinates from the Euclidean space to the spherical space, and then slides a spherical voxel convolution kernel in the space to achieve rotation robustness [16]. Nevertheless, these methods, while enhancing the model’s adaptability to rotation perturbation, often overemphasize the extraction of rotation-robust information, compromising generalization capabilities in the presence of random errors (such as noise and outliers) [17]. Besides, they typically introduce extra computational burdens during the inference process as well, owing to the required coordinate transformation. Therefore, to achieve robustness of rotation perturbation efficiently while maintaining the ability to resist random errors still remains an unresolved issue. For this goal, we propose a novel augmentation strategy based on manifold distillation, using a teacher-student architecture with several designed models to achieve the robustness against rotation perturbation and random errors without relying on complex coordinate transformation and increasing the number of inference parameters during inference phase. The contributions of this paper are summarized below: • We remodel point cloud from the perspective of manifold and construct a hierarchical network architecture to learn the mappings between different manifolds while extracting the structural information of manifolds with a graph representation method. • A novel augmentation approach based on manifold distillation is introduced. During the training phase, a teacher-student architecture is utilized to align the feature information of the student network with the rotation-robust manifold provided by the teacher network. In the inference phase, the teacher network is removed, and the student network directly leverages the original coordinates to achieve the robustness against the rotation perturbation. • To avoid the degradation and distortion of information caused by direct manifold alignment, we use low-rank decomposition to extract manifold structural components, and further utilize the local Euclidean space properties of manifold to design an alignment model. Meanwhile, to eliminate the additional computational overhead introduced by above processes, a reparameterization technique is also proposed. • Experiments conducted on two classification datasets ModelNet40, ScanobjectNN and two segmentation datasets PartNet, S3DIS show that our method not only realizes the robustness against the rotation perturbation, but also successfully resists noise and outliers. The remainder of this paper is organized as follows. In Section II, we discuss the existing works about the point cloud-related tasks. Section III introduces how to realize anti-perturbation learning in primitive Euclidean space by distillation learning. Section IV shows experimental results. The limitations of our method are presented in Section V and conclusions are shown in Section VI."
https://arxiv.org/html/2411.01739v2,"Not Just Object, But State: Compositional Incremental Learning without Forgetting","Most incremental learners excessively prioritize coarse classes of objects while neglecting various kinds of states (e.g. color and material) attached to the objects. As a result, they are limited in the ability to reason fine-grained compositionality of state-object pairs. To remedy this limitation, we propose a novel task called Compositional Incremental Learning (composition-IL), enabling the model to recognize state-object compositions as a whole in an incremental learning fashion. Since the lack of suitable benchmarks, we re-organize two existing datasets and make them tailored for composition-IL. Then, we propose a prompt-based Composition Incremental Learner (CompILer), to overcome the ambiguous composition boundary problem which challenges composition-IL largely. Specifically, we exploit multi-pool prompt learning, which is regularized by inter-pool prompt discrepancy and intra-pool prompt diversity. Besides, we devise object-injected state prompting by using object prompts to guide the selection of state prompts. Furthermore, we fuse the selected prompts by a generalized-mean strategy, to eliminate irrelevant information learned in the prompts. Extensive experiments on two datasets exhibit state-of-the-art performance achieved by CompILer. Code and datasets are available at: https://github.com/Yanyi-Zhang/CompILer.","Class Incremental Learning (class-IL) CL_survey_1 ; CL_survey_2 ; LwF ; EWC gathers increasing attention due to its ability to make the models learn new tasks rapidly, without forgetting previously acquired knowledge. Yet, traditional class-IL sets a strict limit on the old classes such that they should not recur in newly incoming tasks. To break such a strict limitation, recent studies develop a new setting mostly called Blurry Incremental Learning (blur-IL) MVP ; CLIB , where the incremental sessions allow the recurrence of previous classes, resulting in a more realistic and flexible scenario. Despite such empirical progresses on incremental learning, they aim to improve object classification only, while overlooking fine-grained states attached to the objects. For instance, analyzing how the clothing styles (akin to states) have changed over time is important for forecasting the future trends that will emerge. Figure 1: Differences between Class Incremental Learning (class-IL), Blurry Incremental Learning (blur-IL), and Compositional Incremental Learning (composition-IL). The object classes are not allowed to recur in the class-IL scenario, whereas they may recur randomly in the blur-IL scenario. Different from them, the classes in composition-IL involve state-object compositions apart from the object classes. Besides, the compositions do not reoccur, but the primitives (states or objects) may randomly reappear across incremental sessions. To simultaneously model objects and their states, some efforts are dedicated to Compositional Learning whose aim is how to equip the models with compositionality CognitiveScience1 ; CognitiveScience2 ; CognitiveScience3 . The core of compositional learning lies in the structure of class labels, which conceptualizes a state-object pair (e.g. “Brown Pants” and “Yellow Dress”) as a whole, rather than a lonely object label. In this way, the model can dissect and reassemble learned knowledge, achieving a more fine-grained understanding about the objects. However, existing works are mainly focused on zero-shot generalization from seen compositions to unseen ones AttrAsOpe ; Task-driven ; CZSL_AAAI_2024 ; CZSL_CVPR_2024 , whereas none of them consider the challenging fact that the model must deal with a significantly larger number of composition classes than object classes. As a result, it is hardly feasible to learn all compositions by training the model once. To remedy the limitations inherent in incremental learning and compositional learning, we conceive a novel task named Compositional Incremental Learning (composition-IL), enabling the model to continually learn new state-object compositions in an incremental fashion. As compared in Fig. 1, we can see that composition-IL integrates the characteristics of class-IL and blur-IL. Although the composition classes are disjoint across incremental tasks, the primitive classes (i.e. objects and states) encountered in old tasks are allowed to reappear in new tasks. Unfortunately, existing incremental learning approaches are challenged by such a compositional scenario, because their models excessively prioritize the object primitives while neglecting the state primitives. Consequently, the compositions with the same object but with different states become ambiguous and indistinguishable. To tackle the problem, we propose a rehearsal-free and prompt-based Compositional Incremental Learner (CompILer). Specifically, our model comprises of three primary components: multi-pool prompt learning, object-injected state prompting, and generalized-mean prompt fusion. Firstly, we construct three prompt pools for learning the states, objects and compositions individually. Upon that, we add extra restrictions to regularize the inter-pool prompt discrepancy and intra-pool prompt diversity. This multi-pool prompt learning paradigm strengthens the fine-grained understanding and reasoning towards primitive concepts and their compositions. In addition, as the state classes are more difficult to distinguish than the object ones, we propose object-injected state prompting which incorporates object prompts to guide the selection of state prompts. Furthermore, we fuse the selected prompts by a generalized-mean fusion manner, which helps to adaptively eliminate irrelevant information learned in the prompts. Last but not least, we also leverage symmetric cross-entropy loss to alleviate the impact of noisy data during training. In summary, the main contributions in this work are encapsulated as follows: (1) We devise a new task coined compositional incremental learning (composition-IL). It enables learning fine-grained state-object compositions continually while the isolated primitive concepts can randomly recur in incremental tasks. (2) To address the lack of datasets, we re-organize two existing datasets such that they are tailored specifically for composition-IL. For the two new datasets (Split-Clothing and Split-UT-Zappos), we split them into 5 and 10 incremental tasks for evaluating the methods. (3) We propose a novel learning-to-prompt model for composition-IL, namely CompILer. Our state-of-the-art results on Split-Clothing and Split-UT-Zappos validate the effectiveness of CompILer for incrementally learning new compositions without forgetting old ones."
https://arxiv.org/html/2411.01734v1,Next Best View For Point-Cloud Model Acquisition: Bayesian Approximation and Uncertainty Analysis,"The Next Best View problem is a computer vision problem widely studied in robotics. To solve it, several methodologies have been proposed over the years. Some, more recently, propose the use of deep learning models. Predictions obtained with the help of deep learning models naturally have some uncertainty associated with them. Despite this, the standard models do not allow for their quantification. However, Bayesian estimation theory contributed to the demonstration that dropout layers allow to estimate prediction uncertainty in neural networks.This work adapts the point-net-based neural network for Next-Best-View (PC-NBV). It incorporates dropout layers into the model’s architecture, thus allowing the computation of the uncertainty estimate associated with its predictions. The aim of the work is to improve the network’s accuracy in correctly predicting the next best viewpoint, proposing a way to make the 3D reconstruction process more efficient.Two uncertainty measurements capable of reflecting the prediction’s error and accuracy, respectively, were obtained. These enabled the reduction of the model’s error and the increase in its accuracy from 30% to 80% by identifying and disregarding predictions with high values of uncertainty. Another method that directly uses these uncertainty metrics to improve the final prediction was also proposed. However, it showed very residual improvements.","Developments in the fields of robotics and artificial intelligence have been allowing several improvements in some scientific and non-scientific areas, revolutionizing everyday tasks in the process. This influence can also be visible in the domains of archaeology, conservation and restoration, where their strategic integration has become an invaluable asset, aiding men in tasks that require lots of precision and time investment. The RePair project [1] is proof of this. It focuses on the use of intelligent robotics in this area, with the goal of making the reconstruction process of fragmented Pompeii frescoes more efficient. Some methodologies for reconstruction rely on computational help - it is possible to use computer algorithms that, just like solving a jigsaw puzzle, are able to perform the reconstructions. For this, however, they require the 3D models of the fragments, which is the task that inspired this work. We study an algorithm for the efficient 3D model acquisition of the fresco fragments. The 3D data collection method must be as efficient as possible. This means that the algorithm needs to find a way of getting the most information of the fragments surface with as few scans as possible. To achieve this goal the algorithm has to find the camera viewpoints (i.e. camera pose with respect to the object being scanned) that will get the most new information on the object being scanned. This is known as the Next Best View (NBV) problem, which the algorithm must adeptly solve to achieve its intended outcome. Recent approaches are based on Deep Learning architectures such as [2], [3], [5] and [4], showed very good results in performing 3D model reconstructions in an efficient manner. However, they do not account for the uncertainty of their predictions which, in real world scenarios, can be vital to guarantee a good reconstruction performance. Hence, in this work, a NBV that predicts the coverage of the viewpoints - PC-NBV [5] - is analyzed in a more detailed way, and modifications are made to its architecture to make it account for uncertainty. The uncertainty of the estimated coverage scores is calculated using the idea of Monte Carlo sampling to estimate the uncertainty of the output of a Neural Network. To do this, the deterministic output becomes probabilistic through the application of dropout layers. The goal is for the model to output a prediction along with an uncertainty measurement that reflects how sure the model is of its predictions, in an attempt of enhancing precision with the information given by this measurement. This can subsequently improve the efficiency of the 3D model acquisition process. Two types of metrics that give two different kinds of uncertainty measurements were obtained. One is error related, as it estimates the prediction uncertainty of the coverage estimation. Another one is accuracy related, as it estimates the prediction uncertainty related to the ability of the network on choosing the right NBV. These metrics allow to improve the performance of the NBV selection."
https://arxiv.org/html/2411.01725v1,A Probabilistic Formulation of LiDAR Mapping with Neural Radiance Fields,"In this paper we reexamine the process through which a Neural Radiance Field (NeRF) can be trained to produce novel LiDAR views of a scene. Unlike image applications where camera pixels integrate light over time, LiDAR pulses arrive at specific times. As such, multiple LiDAR returns are possible for any given detector and the classification of these returns is inherently probabilistic. Applying a traditional NeRF training routine can result in the network learning “phantom surfaces” in free space between conflicting range measurements, similar to how “floater” aberrations may be produced by an image model. We show that by formulating loss as an integral of probability (rather than as an integral of optical density) the network can learn multiple peaks for a given ray, allowing the sampling of first, nthsuperscriptnth\text{n}^{\text{th}}n start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPT, or strongest returns from a single output channel. Code is available at https://github.com/mcdermatt/PLINK","Neural Radience Fields (NeRFs) provide continuous representations of scenes by storing information about the surrounding world inside the weights of a neural network \parencitenerf. Recent works have extended NeRFs from camera images to LiDAR point clouds for use in localization \parencitepan2024pin, odometry \parencitenerfloam, path planning \parenciteshubPathPlanning, and data augmentation \parenciteNFL, lidarNeRF. To date, LiDAR applications of NeRFs have assumed a deterministic model of the scene. In existing approaches, a LiDAR depth map is computed from the network output in the same way as one may be produced from a conventional NeRF, where the weighted sum of opacity at various test points along each ray is used to estimate depth in that direction \parenciteNFL, lidarNeRF, nerflidar. Researchers have identified several geometry-based tweaks to enhance the accuracy of LiDAR NeRFs within this deterministic formulation. For instance, Huang et al. introduced a beam spreading model to enable their algorithm NFL to compute range and reflectance accurately for oblique surfaces \parenciteNFL. Tao et al. take a simpler approach, with their algorithm LiDAR-NeRF, masking oblique surfaces to avoid associated complications \parencitelidarNeRF. NeRF-LOAM improves sample efficiency by learning an octree map to focus the network on occupied regions of the scene \parencitenerfloam. Similarly, SHINE Mapping uses a hierarchical spatial structure to encode information and varying levels of detail \parenciteshine. Rather than learning a single large network, PIN-SLAM achieves a scene representation through many small localized networks in a voxel structure, each relative to a learnable pose parameter, rather than a fixed location in SO(3), which allows for loop closure without retraining the model \parencitepan2024pin. Despite their impressive accuracy over most segments of a scene, existing algorithms struggle to handle viewpoints from which multiple returns are recieved, since the measured range value is essentially probabilistic. Of the above methods, only NFL attempts to address the problem, electing to learn two separate depth channels to represent the first and second returns from any viewpoint \parenciteNFL. However, NFL always assumes exactly two returns, never more and never fewer. In this paper, we contend that a small but impactful change to NeRF allows for the direct representation of multiple possible returns, as can occur when a LiDAR views a surface and beyond (as for a window or loose foliage). Acknowledging an element of randomness in the training data, we seek to learn a probabilistic representation of depth, rather than a single optimal depth value along each ray emanating from the sensor. As such, the network is trained to represent the world as a probabilistic density, rather than as a deterministic optical density, as in the case of a traditional NeRF. Browning et al. introduced a similar stochastic volumetric world model for LiDAR sample generation \parencitebrowning20123d, however, to the best of our knowledge, such a representation has not been combined with NeRF or other differentiable rendering techniques. The remainder of this paper explains our contribution and demonstrates performance in generating continuous world representations from real LiDAR data. In section II, we concisely but quantitatively define the problems associated with LiDAR data reconstruction. In sections III and IV, we propose a solution and provide implementation details. In Section V we introduce and discuss the results of our proposed method, which we implement and share in our Probabilistic LiDAR NeRF Codebase (PLiNK), which we evaluate across two experiments. The first experiment involves an architectural scene from the Newer College Dataset \parencitenewerCollege, and the second involves a simulated drive through an urban environment in the Mai City Dataset \parencitemaicity. Finally, in section VI we discuss our results and suggest directions for future work."
https://arxiv.org/html/2411.01683v1,ROAD-Waymo: Action Awareness at Scale for Autonomous Driving,"Autonomous Vehicle (AV) perception systems require more than simply seeing, via e.g., object detection or scene segmentation. They need a holistic understanding of what is happening within the scene for safe interaction with other road users. Few datasets exist for the purpose of developing and training algorithms to comprehend the actions of other road users. This paper presents ROAD-Waymo, an extensive dataset for the development and benchmarking of techniques for agent, action, location and event detection in road scenes, provided as a layer upon the (US) Waymo Open dataset. Considerably larger and more challenging than any existing dataset (and encompassing multiple cities), it comes with 198k annotated video frames, 54k agent tubes, 3.9M bounding boxes and a total of 12.4M labels. The integrity of the dataset has been confirmed and enhanced via a novel annotation pipeline designed for automatically identifying violations of requirements specifically designed for this dataset. As ROAD-Waymo is compatible with the original (UK) ROAD [57] dataset, it provides the opportunity to tackle domain adaptation between real-world road scenarios in different countries within a novel benchmark: ROAD++. Dataset and code are available at: Dataset 111https://github.com/salmank255/Road-waymo-dataset, Code 222https://github.com/salmank255/ROAD_Waymo_Baseline.","Several large-scale, comprehensive AV training datasets have recently been released [9, 20, 69, 56, 65, 45], leveraging multiple sensors (e.g. cameras, LiDAR, GPS, etc.) for the purposes of object detection, semantic segmentation, object tracking and trajectory forecasting. However, a safe interaction with other road users requires much more than simply seeing things: rather, it requires understanding actions and events being conducted or happening in the scene. To this end, Singh et al. [57] introduced the ROAD dataset, which provides multi-label annotations of agents and actions for event awareness in autonomous driving. ROAD represents a starting point along the path to enable AVs to understand events. It features some degree of diversity in terms of weather conditions (including bright sunshine, wet roads, rain, snow and fog) and can be used as a benchmark for object/agent detection, action detection and event detection in a natural multi-label setting. This was indeed the case for the ICCV 2021 ROAD workshop and challenge 333https://sites.google.com/view/roadchallangeiccv2021/challenge, which saw a combined 200+ entries for the aforementioned tasks. Since then, the problem of situation awareness for AVs has gained even more traction [22, 70]. Figure 1: Exemplary annotations of road users in ROAD-Waymo style, explained from the (ego) viewpoint of an AV. Each road user is annotated with three distinct labels: agent, action, and location. Only agents with a distinct status are labeled to avoid confusion from overlapped labels. Nevertheless, ROAD remains a small-scale dataset, lacking in both scale and true diversity to allow the further demonstration and testing of the benefits of event awareness. It is limited to scenes from just one, relatively small city - Oxford, UK (population 160,000) - with a high density of narrow streets, relatively low-speed driving on few routes. This makes it suitable for early stage development, but is significantly less challenging than real-world driving in complex environments. This paper introduces ROAD-Waymo, which tackles the diversity and scale issues of ROAD [57], building on the Waymo-Open dataset [61]. Spatiotemporal [57] annotations, in the form of agent tracks plus action and location labels, are densely provided in the scene to study event awareness, action and event detection. As such annotations are expensive - we inherited 52⁢K52𝐾52K52 italic_K tracks from the Waymo dataset, adding extensive labelling based upon the road event = {{\{{ agent, action, location }}\}} protocol (illustrated in Figure 1), resulting in 4.1⁢M4.1𝑀4.1M4.1 italic_M bounding boxes with 4.1⁢M4.1𝑀4.1M4.1 italic_M agent, 4.3⁢M4.3𝑀4.3M4.3 italic_M action, and 4.3⁢M4.3𝑀4.3M4.3 italic_M location label instances. In combination with the 7⁢K7𝐾7K7 italic_K tracks present in ROAD, ROAD-Waymo provides an extensive playground for the development of advanced perception systems for autonomous vehicles and the tackling of novel problems, with a focus on domain adaption (in particular, between cities in the UK and the US) for action/agent/event detection tasks. Contributions: The contributions of this work are: 1. ROAD-Waymo A large-scale multi-label dataset for action and event detection in autonomous vehicles. Provided as an extension to the Waymo AV dataset [61], it is considerably more comprehensive and challenging than any existing such benchmark, including ROAD. 2. Scale: ROAD-Waymo contains 4.3⁢M4.3𝑀4.3M4.3 italic_M action labels, making it 7777 times larger than the original ROAD dataset, and the largest known dataset for this purpose. At the same time, it extends the Waymo dataset by providing a total of 12.7⁢M12.7𝑀12.7M12.7 italic_M additional labels. 3. Multi-City: The dataset encompasses 4 cities from several US states, providing a plethora of different driving scenarios in a range of operating domains. 4. Complexity: The labelling provided is highly comprehensive - actors are labelled at great distances from the ego vehicle, with many active participants in each scene. Several complex scenes contain the presence of emergency vehicles (11 times more than the ROAD), six weather conditions (two times of ROAD), many lanes, busy roads, pedestrians and so on. Thus, ROAD-Waymo provides a considerably more challenging perception testbed than any other dataset of this kind. 5. Verified Annotations: Importantly, ROAD-Waymo is the first AV dataset guaranteed to be compliant with a set of commonsense constraints. Each annotation was automatically verified to be compliant with 251 domain-dependent logical requirements elicited for this task. 6. ROAD++ domain adaptation framework: The new ROAD-Waymo dataset is fully compatible with the original ROAD data. Together, they form the basis for a benchmarking framework for domain adaptation in autonomous driving (between two countries UK and the US and their 5 different cities), termed ROAD++. Tasks and Baselines: Experiments were conducted to provide a benchmark for the following perception tasks: 1. Object/Agent, Action, and Event detection tasks using various types of models. 2. Cross-country (datasets) and cross-city real-to-real unsupervised domain adaptation for same tasks. 3. Neuro-symbolic prediction enhancement (i.e. the use of domain knowledge to guide the training of the neural models to ensure compliance with commonsense rules)."
https://arxiv.org/html/2411.01656v1,Degradation-Aware Residual-Conditioned Optimal Transport for Unified Image Restoration,"Unified, or more formally, all-in-one image restoration has emerged as a practical and promising low-level vision task for real-world applications. In this context, the key issue lies in how to deal with different types of degraded images simultaneously. Existing methods fit joint regression models over multi-domain degraded-clean image pairs of different degradations. However, due to the severe ill-posedness of inverting heterogeneous degradations, they often struggle with thoroughly perceiving the degradation semantics and rely on paired data for supervised training, yielding suboptimal restoration maps with structurally compromised results and lacking practicality for real-world or unpaired data. To break the barriers, we present a Degradation-Aware Residual-Conditioned Optimal Transport (DA-RCOT) approach that models (all-in-one) image restoration as an optimal transport (OT) problem for unpaired and paired settings, introducing the transport residual as a degradation-specific cue for both the transport cost and the transport map. Specifically, we formalize image restoration with a residual-guided OT objective by exploiting the degradation-specific patterns of the Fourier residual in the transport cost. More crucially, we design the transport map for restoration as a two-pass DA-RCOT map, in which the transport residual is computed in the first pass and then encoded as multi-scale residual embeddings to condition the second-pass restoration. This conditioning process injects intrinsic degradation knowledge (e.g., degradation type and level) and structural information from the multi-scale residual embeddings into the OT map, which thereby can dynamically adjust its behaviors for all-in-one restoration. Extensive experiments across five degradations demonstrate the favorable performance of DA-RCOT as compared to state-of-the-art methods, in terms of distortion measures, perceptual quality, and image structure preservation. Notably, DA-RCOT delivers superior adaptability to real-world scenarios even with multiple degradations and shows distinctive robustness to both degradation levels and the number of degradations. Source code is publicly available at: https://github.com/xl-tang3/DA-RCOT.","Image restoration (IR) holds a fundamental position in low-level computer vision, aiming to address the degradation (e.g., most commonly noise, blur, rain, haze, low light) in a degraded image. Traditional methods focus on designing an optimization problem that exploits priors of natural image [1], [2], [3], [4], [5] or corresponding image-induced signals such as gradients [6], [7] and residuals or noise maps [8], [9], [10]. Recent advances in deep learning techniques [11], [12], [13], [14] have triggered great achievements in image restoration, in which most methods [15], [16], [17], [18], [19], [20], [21] [22], [23] train task-specific restoration models over degraded-clean image pairs of a single known degradation. However, this specificity limits their practicability in real-world applications, e.g., autonomous navigation [24], [25], surveillance systems [26], and digital photography [27], where varied and unexpected degradations usually occur. Therefore, there is an emerging field known as “All-in-One” image restoration (AIR) that aims to address different degradations simultaneously. To tackle the AIR problem, several works [28], [29], [30], [31], [32], [33], [34] directly fit joint regression models over multi-domain degraded-clean image pairs, minimizing pixel-wise distortion measures such as L1subscript𝐿1L_{1}italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT or L2subscript𝐿2L_{2}italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT distances. Particularly, these methods derive degradation embeddings from the degraded image as task indicators, enabling the model to perform appropriate restoration while perceiving the degradation type. However, the severe ill-posedness of the regression with heterogeneous degradations still poses unique challenges for these methods, including perceiving the degradation (type and level) inadequately and leading to ”average” or suboptimal restoration maps. Consequently, they tend to produce results with compromised structural details [35], [36], [37], e.g., distorted color and excessively smoothed textures (see Figure 1 (b)). Moreover, existing AIR methods rely on large amounts of paired data for supervised learning, which is usually conducted using synthetic data due to the difficulty in acquiring real-world degraded-clean pairs. As a result, the learned models tend to over-fit to the synthetic data and are vulnerable in real-world scenarios, where the availability of real-world image pairs, coupled with the complexity and diversity of real-world degradations, further hinders their applicability. Figure 1: (a) The core idea of DA-RCOT is to model AIR as an OT problem and then condition the OT map with the customized multi-scale residual embeddings, yielding a degradation-aware and structure-preserving DA-RCOT map. (b) A denoising demo under noise level σ=50𝜎50\sigma=50italic_σ = 50. DA-RCOT produces a noise-free image with more faithful textures. The challenges of AIR boil down to: i) seeking the optimal transformation of heterogeneous degraded images into clean ones while ii) properly perceiving the degradation semantics and prompting the preservation of visual structures, and iii) extending to unpaired or partially paired data. To address these challenges, this work is motivated to tackle the AIR problem from an optimal transport (OT) perspective for both unpaired and paired settings. The core idea is to seek the OT map that minimizes the transport distance between the distribution of multi-domain degraded images and the distribution of clean images, while informing the OT map with the novel transport residual (i.e., the domain gap between degraded and clean images) that contains degradation-specific knowledge about the degradation and image structures (see Figure 1 (a)). In this sense, the computed OT map has the potential to perceive the degradation (type and level) and further preserve the structural contents. Specifically, we present a Degradation-Aware Residual-Conditioned Optimal Transport (DA-RCOT) approach, introducing the novel transport residual as a degradation-specific cue for both the transport cost and the transport map. Firstly, we formalize AIR as an OT problem between the distribution of multi-domain degraded images and the distribution of clean images with a Fourier residual-guided OT (FROT) objective that exploits the frequency patterns of the residual for the transport cost. Secondly and most crucially, we design the transport map as a two-pass DA-RCOT map, in which the transport residual is computed in the first pass and then encoded as multi-scale residual embeddings to condition the second-pass restoration. This multi-scale conditioning mechanism injects intrinsic degradation knowledge (e.g., degradation type and level) and structural information from the multi-scale residual embeddings into the OT map, which thereby can dynamically adjust its behaviors for structure-preserving all-in-one restoration. Our contributions can be mainly summarized as follows: • We advocate an OT solution dubbed DA-RCOT for AIR, learning to transform the distribution of multi-domain degraded images to that of clean ones with a minimal cost. Uniquely, DA-RCOT introduces the transport residual as a degradation-specific cue for both the transport cost and transport map. • We integrate the degradation knowledge into the transport cost by exploiting the frequency patterns of transport residuals, yielding the FROT objective. • We design the transport map as a two-pass DA-RCOT map, which dynamically conditions the OT map with degradation-specific low-level and high-level features from the multi-scale residual embeddings, enhancing its capability to perceive the degradation semantics and preserve the structures. • Extensive experiments on synthetic and real-world data across five degradations show that DA-RCOT achieves state-of-the-art performance in all-in-one and task-specific settings. DA-RCOT also exhibits superior generalization to unseen degradation levels and robustness to the number of degradation types. This work extends our conference paper [38] published at ICML. In the previous version, we learned a separate model based on OT theories to address single degradation, using single-scale residual embedding for structure preservation. In this work, we address the more challenging All-in-One problem by learning a unified OT map to handle different types of degradations simultaneously. Moreover, we extract multi-scale residual embeddings to condition the OT map for degradation-aware restoration. Specifically, we additionally make the following contributions: 1) We formulate AIR as an OT problem that seeks the optimal transportation from the distribution of multi-domain degraded images to that of the clean ones. 2) We employ gating-based Transformer blocks with a contrastive loss to extract multi-scale residual embeddings as conditions, which enhance the OT map with abundant degradation semantics and structural information. 3) We perform extensive experiments under AIR setting on a mixed dataset collected from multiple benchmarks, including Rain100L [39] for deraining, BSD68 [40] for denoising, SOTS [41] for dehazing, GoPro [42] for deblurring, and LOL [43] for low-light enhancement. The results substantiate the effectiveness of the proposed method and demonstrate its robustness against varying degradation numbers and unseen degradation levels as compared to the conference version and other AIR methods."
https://arxiv.org/html/2411.01647v1,MedSora: Optical Flow Representation AlignmentMamba Diffusion Model for Medical Video Generation,"Medical video generation models are expected to have a profound impact on the healthcare industry, including but not limited to medical education and training, surgical planning, and simulation. Current video diffusion models typically build on image diffusion architecture by incorporating temporal operations (such as 3D convolution and temporal attention). Although this approach is effective, its oversimplification limits spatio-temporal performance and consumes substantial computational resources. To counter this, we propose Medical Simulation Video Generator (MedSora), which incorporates three key elements: i) a video diffusion framework integrates the advantages of attention and Mamba, balancing low computational load with high-quality video generation, ii) an optical flow representation alignment method that implicitly enhances attention to inter-frame pixels, and iii) a video variational autoencoder (VAE) with frequency compensation addresses the information loss of medical features that occurs when transforming pixel space into latent features and then back to pixel frames. Extensive experiments and applications demonstrate that MedSora exhibits superior visual quality in generating medical videos, outperforming the most advanced baseline methods. Further results and code are available at https://wongzbb.github.io/MedSora/.","With the development and integration of technologies such as diffusion (Ho et al., 2020; Sohl-Dickstein et al., 2015), multimodal (Radford et al., 2021; Ramesh et al., 2021), pre-trained models (Brown et al., 2020), and model fine-tuning (Hu et al., 2021; Li & Liang, 2021), artificial intelligence generated content (AIGC) has achieved remarkable progress, sparking widespread interest in interdisciplinary medical fields. Ranging from image reconstruction (Liu et al., 2023) and translation (Wang et al., 2024) to the generation of virtual cases and the creation of simulation data (Özbey et al., 2023), AIGC underpin numerous innovations. This has significantly advanced the frontiers of computer-assisted diagnosis and precision medicine. Recently, advances in video generation models (Blattmann et al., 2023; An et al., 2023; Huang et al., 2024) have propelled AIGC to new heights. However, due to the considerable complexity and significant resource demands of clinical video generation, research in this area remains in its nascent stages. In this context, our work aims to explore whether it is possible to create temporally coherent and realistic clinical medical videos. Significant progress has been made in generating realistic medical images using generative adversarial networks (GANs) (Goodfellow et al., 2014) and diffusion models (Rombach et al., 2022). However, video is composed of pixel arrays spanning both temporal and spatial dimensions, necessitating meticulous attention to fine temporal dynamics and the maintenance of temporal consistency across frames. Consequently, generating stable, high-quality videos is a non-trivial task. To achieve this, most recent works in computer vision attempt to extend the spatial self-attention mechanism of images to spatio-temporal self-attention (Khachatryan et al., 2023; Qi et al., 2023), using pseudo-3D (Singer et al., 2022) or serial 2D+1D convolutions (Xing et al., 2024a) to apply image diffusion models to video generation. To the best of our knowledge, the sole existing study in the medical field on video generation employs the spatio-temporal self-attention (Li et al., 2024). Those strategies integrate features from patches across different video frames via an expanded attention or convolution module, as illustrated in Figure 1. Figure 1: Illustration of spatial attention, pseudo-3D or 2D+1D convolution, spatio-temporal attention, and our spatio-temporal Mamba. 𝐱ksuperscript𝐱𝑘\mathbf{x}^{k}bold_x start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT represents the feature map of the k𝑘kitalic_k-th video frame. Patches marked with ’*’ are computed together with the colored patches to aggregate their features. Despite their effectiveness in capturing contextual information across space and time, these methods have significant drawbacks: i) spatio-temporal self-attention requires each patch attends to all other patches in the video, which not only poses the risk of misdirecting the attention mechanism with patches that are irrelevant to video generation (Liu et al., 2022), but also significantly increases the computational complexity, presenting significant challenges in terms of resource allocation and maintaining cross-frame coherence, and ii) convolutional methods focus only on local areas and cannot capture the global receptive field, thereby reducing model performance. To mitigate these challenges, some recent works have incorporated Mamba (Gu & Dao, 2023; Dao & Gu, 2024) into video generation, they typically replace the original convolution or attention modules of GANs or diffusion models with Mamba (Gao et al., 2024; Mo & Tian, 2024), or merely alter the scanning order of patches (Park et al., 2024) without thorough exploration. These Mamba-based video generation models exhibit two primary limitations: i) existence of pixel-level bias. Recent advances in diffusion models for image generation illustrate that Mamba underperforms relative to certain Transformer in generating pixel-level data (Wang et al., 2024). And ii) absence of explicit spatio-temporal modeling. These models either maintain the same structural design of image generation models (Mo & Tian, 2024; Gao et al., 2024) or simplistically modify the sequence of video frames (Park et al., 2024), assuming it to suffice for spatio-temporal modeling. We aim to address these challenges by advancing our approach with an optimized design of spatio-temporal Mamba and attention blocks. Spatio-temporal self-attention frameworks typically model the content within each video frame before conducting temporal modeling of patches at corresponding positions across frames (Diba et al., 2023). However, this approach has a significant limitation as it presupposes linear motion of patches along the temporal axis. This assumption leads to inadequate mutual attention among patches following non-linear trajectories across frames. To overcome existing drawbacks, we propose a supervised, optical flow representation alignment method seamlessly integrated into the video diffusion model. By using optical flow (Fleet & Weiss, 2006; Shi et al., 2023) to implicitly control key features, our method addresses the visual consistency problems identified in previous work. An primary benefit is the facilitation of precise information transfer between frames through optical flow guidance, stabilizing the generated visual content throughout the video. Specifically, we initially employ a pre-trained optical flow estimation model (Teed & Deng, 2020) to determine the optical flow of the source video. Following this, a self-supervised foundation model (Caron et al., 2021) is utilized to extract key features of the optical flow from various perspectives. These key features are then sampled and leveraged to supervise the training of video diffusion model, without the addition of any extra training parameters. Video generation models process high-dimensional videos as cubic arrays, necessitating substantial memory and computational resources, particularly for high-resolution and extended-duration videos. To reduce the computational load, numerous video diffusion models (Blattmann et al., 2023; An et al., 2023) are not trained on raw pixels, but first employ an autoencoder (Kingma & Welling, 2013) to project video frames into low-dimensional latent space and then model this latent distribution. However, the pre-train time-agnostic image autoencoder can distorts the temporal information in the latent space (Zhou et al., 2022), i.e., the change △z=zi→zjsubscript△𝑧subscript𝑧𝑖→subscript𝑧𝑗\triangle_{z}={z_{i}\to z_{j}}△ start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT = italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT → italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT from the i𝑖iitalic_i-th frame to the j𝑗jitalic_j-th frame in the latent space suffers from frame consistency degradation or misalignment compared to the change △x=xi→xjsubscript△𝑥subscript𝑥𝑖→subscript𝑥𝑗\triangle_{x}={x_{i}\to x_{j}}△ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT → italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT in the original pixel space. To address this challenge, Lin et al. (2023) using additional modules, and Xiang et al. (2023a) retrained the decoder for image autoencoder. Researchers have recently begun training video autoencoders on large-scale video datasets (Yang et al., 2024). However, the limited availability of medical videos means that these video autoencoders have not been adequately trained on medical datasets. We incorporate frequency compensation modules into the video VAE to preserve spatio-temporal consistency, particularly during the projection and reconstruction processes of medical videos under spectral constraints. The primary contributions of this paper are: i) proposing a novel video diffusion model framework that introduces spatio-temporal capabilities while minimizing resource requirements, featuring powerful spatio-temporal Mamba module and local attention mechanism. ii) optical flow representation alignment is incorporated into the video diffusion model to improve temporal coherence and smoothness of the generated videos. iii) development of a medical video autoencoder incorporating frequency compensation to enhance the reconstruction quality of medical videos. Comprehensive experiments verify that our method MedSora establishes a new benchmark in state-of-the-art performance among existing medical video generation models, particularly in sustaining visual consistency and diminishing computational burden."
https://arxiv.org/html/2411.01624v1,PreCM: The Padding-based Rotation Equivariant Convolution Mode for Semantic Segmentation,"Semantic segmentation is an important branch of image processing and computer vision. With the popularity of deep learning, various deep semantic segmentation networks have been proposed for pixel-level classification and segmentation tasks. However, the imaging angles are often arbitrary in real world, such as water body images in remote sensing, and capillary and polyp images in medical field, and we usually cannot obtain prior orientation information to guide these networks to extract more effective features. Additionally, learning the features of objects with multiple orientation information is also challenging, as most CNN-based semantic segmentation networks do not have rotation equivariance to resist the disturbance from orientation information. To address the same, in this paper, we first establish a universal convolution-group framework to more fully utilize the orientation information and make the networks rotation equivariant. Then, we mathematically construct the padding-based rotation equivariant convolution mode (PreCM), which can be used not only for multi-scale images and convolution kernels, but also as a replacement component to replace multiple convolutions, like dilated convolution, transposed convolution, variable stride convolution, etc. In order to verify the realization of rotation equivariance, a new evaluation metric named rotation difference (RD) is finally proposed. The experiments carried out on the datesets Satellite Images of Water Bodies, DRIVE and Floodnet show that the PreCM-based networks can achieve better segmentation performance than the original and data augmentation-based networks. In terms of the average RD value, the former is 0% and the latter two are respectively 7.0503% and 3.2606%.","Semantic segmentation has been a fundamental and challenging task in computer vision for many years [1]. Since it can provide category information at the pixel level, it has been widely used in various fields, such as autonomous vehicles [2], water detection [3], road segmentation [4], and defect detection [5]. Within the context of traditional machine learning, semantic segmentation methods are normally constructed as classifiers, like Support Vector Machine (SVM) [6] and Random Forest (RF) [7]. In addition, considering the use of texture information to ensure the consistency between pixel labels, context models such as Markov Random Fields (MRF) [8] and Conditional Random Fields (CRF) [9] are also utilized as classical frameworks for semantic segmentation. Although these algorithms are convenient and easy to implement, their poor feature extraction ability limits their generalization capability and segmentation accuracy. With the development of deep learning, CNNs have proven their ability to extract more advanced features of images for semantic segmentation. Specifically, the translation equivariance of convolution kernels enables CNNs to better capture spatial features in images [10]. In [11], Long et al. used CNN-based fully convolutional networks to produce correspondingly-sized output with efficient inference and learning for semantic segmentation. Chen et al. [12] combined deep convolutional networks, atrous convolution, and fully connected conditional random fields to achieve semantic segmentation. Yang et al. [13] proposed Densely connected Atrous Spatial Pyramid Pooling (DenseASPP) for street scenes segmentation, which connected a set of atrous convolutional layers in a dense way to produce multi-scale features. Although these networks have achieved significant success in different semantic segmentation tasks, the convolutions often fail to guarantee the rotation equivariance of networks. So, when extracting the pose and texture features of objects, the networks are inevitably affected by the orientational information [14]. To tackle this issue, some works have been done from two aspects, i.e., extending data and designing networks. For the former, data augmentation technology is frequently utilized. In [15], Mikołajczyk et al. investigated the influence of multiple data augmentation methods on the image classification task, and proposed a new data augmentation method. Choi et al. [16] presented a data augmentation method based on Generative Adversarial Networks (GANs), which could enhance the performance of segmentation networks on the target domain. Olsson et al. [17] developed a semi-supervised semantic segmentation algorithm that used ClassMix, a new data augmentation technique to generate augmentation by mixing unlabeled samples. Although data augmentation technology can effectively help networks reduce their attentions to irrelevant features by introducing various rotated data, it also has two obvious limitations. One is the more orientation changes are considered, the more time costs and calculation budgets need to be consumed [18]. The other is adding rotated samples during the training stage only can make the segmentation results approximate to rotation equivariance. In regard to designing networks, existing works mainly focus on the study of the rotation equivariance of {0∘, 90∘, 180∘, 270∘} [19, 20, 21, 22]. This is because, when performing rotation operations, images typically require resampling with the interpolation technique, like Bilinear Interpolation. Therefore, the rotation equivariance of the angles except for {0∘, 90∘, 180∘, 270∘} is hard to accomplish [23]. Even so, for these works, strict constraints on the sizes of images and convolution kernels as well as the hyperparameters (e.g., stride and padding) are still needed [19], which in turn limits their further applications. Hence, it is necessary and meaningful for us to continue exploring how to keep the rotation equivariance of networks without these limitations. For this goal, this paper proposes one padding-based rotation equivariant convolution mode (PreCM), which thoroughly solves the rotation equivariance issue of {0∘, 90∘, 180∘, 270∘} from the viewpoint of mathematics. Owing to the strict mathematical derivations, PreCM can be directly used as a replacement component to replace traditional convolutions, such as dilated convolution, transposed convolution, variable stride convolution, and so on. In a nutshell, our contributions are three-fold: • Based on the group theory, we first build a rotation equivariant convolution-group framework to extract the features related to orientational information, and also mathematically prove its rotation equivariance. • For the proposed rotation equivariant convolution-group framework, we further give a concrete implementation, i.e., designing the padding-based rotation equivariant convolution mode (PreCM) that can not only be used as a replacement component to replace convolution operations for making networks rotation equivariant, but also realize the rotation equivariance under feature maps and convolution kernels of different scales as well as networks of different hyperparameters. • To verify the effectiveness of PreCM, a new quantitative metric named RD (rotation difference) is correspondingly proposed. The experimental results carried out on three different segmentation tasks, i.e. binary, small-sample, and multi-class segmentations, prove the effectiveness of PreCM in semantic segmentation. Also, in comparison with the data augmentation-based networks, the PreCM-based networks show better segmentation performance without increasing the training samples. The rest of this paper is organized as follows. Related work is discussed in Section II. Section III describes the methodology. The experimental results and analyses are given in Section IV. Section V concludes this paper."
https://arxiv.org/html/2411.01618v1,VQ-Map: Bird’s-Eye-View Map Layout Estimation in Tokenized Discrete Space via Vector Quantization,"Bird’s-eye-view (BEV) map layout estimation requires an accurate and full understanding of the semantics for the environmental elements around the ego car to make the results coherent and realistic. Due to the challenges posed by occlusion, unfavourable imaging conditions and low resolution, generating the BEV semantic maps corresponding to corrupted or invalid areas in the perspective view (PV) is appealing very recently. The question is how to align the PV features with the generative models to facilitate the map estimation. In this paper, we propose to utilize a generative model similar to the Vector Quantized-Variational AutoEncoder (VQ-VAE) to acquire prior knowledge for the high-level BEV semantics in the tokenized discrete space. Thanks to the obtained BEV tokens accompanied with a codebook embedding encapsulating the semantics for different BEV elements in the groundtruth maps, we are able to directly align the sparse backbone image features with the obtained BEV tokens from the discrete representation learning based on a specialized token decoder module, and finally generate high-quality BEV maps with the BEV codebook embedding serving as a bridge between PV and BEV. We evaluate the BEV map layout estimation performance of our model, termed VQ-Map, on both the nuScenes and Argoverse benchmarks, achieving 62.2/47.6 mean IoU for surround-view/monocular evaluation on nuScenes, as well as 73.4 IoU for monocular evaluation on Argoverse, which all set a new record for this map layout estimation task. The code and models are available on https://github.com/Z1zyw/VQ-Map.","Surround-View Images Groundtruths BEVFusion [1] VQ-Map (Ours) Figure 1: We showcase the prediction results in various environmental conditions (day, rainy and night from top to bottom). Our VQ-Map produces more reasonable results, even for areas that are not directly visible, while significantly reducing artifacts. Color scheme is the same as in [1]. BEV layouts represent high-dimensional structured data that encompasses significant prior knowledge, particularly regarding road structures. While current methods for BEV map layout estimation mainly focus on constructing dense BEV features [2, 1, 3] for semantic segmentation as map prediction, they often overlook the incorporation of map prior knowledge. Additionally, occlusion and inherent challenges in depth estimation often lead to inaccuracies in dense features, especially in the areas that are corrupted or invalid in the PV. These factors contribute to incoherent and unrealistic BEV layout results, often with numerous artifacts (see Fig. 1). Yet, humans can rely solely on partial observations of a scene in the PV to imagine the entire coherent BEV layout elements. A natural approach to imitating the human imagination process is to leverage generative models to learn the prior knowledge from the groundtruth BEV map layouts. However, the question is how to align the PV features with the generative models to facilitate BEV map estimation. To this end, we propose a novel pipeline called VQ-Map (see Fig. 2), which aligns the generative models well in the spirit of discrete tokens. In specific, VQ-Map utilizes a generative model similar to VQ-VAE [4] to encode the groundtruth BEV semantic maps into tokenized, discrete and sparse BEV representations, termed BEV tokens, accompanied with a discrete embedding space (i.e., the codebook embedding). Each BEV token is the index of the nearest neighbor in the codebook embedding for an encoded BEV patch feature, representing the high-level semantics of a BEV patch. BEV tokens serve as a new classification label to directly supervise the PV feature learning via a specialized token decoder in our pipeline. The training of the generative model and the token decoder module is separated. By aligning with the sparse BEV tokens, our token decoder module is able to rely solely on sparse backbone features directly queried by token queries for BEV token prediction using an arbitrary transformer-like architecture [5, 6, 7]. Simultaneously, directly employing these sparse features for token prediction bypasses the challenges of building accurate dense BEV features in common practice. The predicted tokens can be integrated into BEV embeddings through the off-the-shelf codebook embedding for generating the final high-quality BEV semantic maps. This process is similar to the human brain’s memory mechanism [8], where the targets (BEV map layouts) are encoded into highly abstract, sparse representations (BEV embeddings) through memory neurons (BEV tokens) that can be activated by specific visual signals (generated based on token queries). We evaluate our proposed VQ-Map on both the surround-view and monocular map estimation tasks, and our method sets new records in both tasks, achieving 62.2/47.6 mean IoU for surround-view/monocular evaluation on nuScenes [9], as well as 73.4 IoU for monocular evaluation on Argoverse [10]. In summary, our contributions are as follows: (1) We propose a novel pipeline VQ-Map exploring a discrete codebook embedding to generate high-quality BEV semantic map layouts. The acquired prior knowledge subsequently helps to effectively align the sparse backbone image features with the generative models based on a specialized token decoder, leading to more accurate BEV map layout estimation with generation. (2) By formulating map estimation as the alignment of perception and generation, our achieved BEV codebook embedding serves as a bridge between PV and BEV, and can be used in the off-the-shelf manner. (3) Extensive experiments show that our VQ-Map establishes new state-of-the-art performance on camera-based BEV semantic segmentation. Meanwhile, we confirm that as a PV-BEV alignment method, token classification is more effective than value regression."
https://arxiv.org/html/2411.01613v1,ANNE: Adaptive Nearest Neighbors and Eigenvector-based Sample Selection for Robust Learning with Noisy Labels,"An important stage of most state-of-the-art (SOTA) noisy-label learning methods consists of a sample selection procedure that classifies samples from the noisy-label training set into noisy-label or clean-label subsets. The process of sample selection typically consists of one of the two approaches: loss-based sampling, where high-loss samples are considered to have noisy labels, or feature-based sampling, where samples from the same class tend to cluster together in the feature space and noisy-label samples are identified as anomalies within those clusters. Empirically, loss-based sampling is robust to a wide range of noise rates, while feature-based sampling tends to work effectively in particular scenarios, e.g., the filtering of noisy instances via their eigenvectors (FINE) sampling exhibits greater robustness in scenarios with low noise rates, and the K nearest neighbor (KNN) sampling mitigates better high noise-rate problems. This paper introduces the Adaptive Nearest Neighbors and Eigenvector-based (ANNE) sample selection methodology, a novel approach that integrates loss-based sampling with the feature-based sampling methods FINE and Adaptive KNN to optimize performance across a wide range of noise rate scenarios. ANNE achieves this integration by first partitioning the training set into high-loss and low-loss sub-groups using loss-based sampling. Subsequently, within the low-loss subset, sample selection is performed using FINE, while the high-loss subset employs Adaptive KNN for effective sample selection. We integrate ANNE into the noisy-label learning state of the art (SOTA) method SSR+, and test it on CIFAR-10/-100 (with symmetric, asymmetric and instance-dependent noise), Webvision and ANIMAL-10, where our method shows better accuracy than the SOTA in most experiments, with a competitive training time. The code is available at https://github.com/filipe-research/anne.","Deep neural networks have demonstrated remarkable performance in tackling various computer vision applications [1, 2, 3]. To achieve high generalization capacity, these models require a large amount of well-curated training data sets with high-quality labels [4]. However, in real-world problems, the annotation of large-scale data is costly and time-consuming. Although crowd-sourcing [5] can reduce the laborious labelling process, it may produce low-quality labels due to human annotation mistakes. Moreover, in challenging domains like medicine, labelling can be subject to ambiguity among specialists, making precise and consistent annotations critical for model accuracy. The main issue of errors in the annotation process is that incorrect labels in the training set can severely harm training, leading to a significant decrease in model generalization [6]. In fact, Zhang et al. [7] showed that deep learning models can easily memorize label noise, and this process is independent of the model architecture or optimization method [8]. Training strategies for learning with noisy labels [9] aim to alleviate the adverse effects of label noise using several strategies that usually depend on a sample selection method to subdivide the training set into clean and noisy-label subsets [10, 11]. After such subdivision, the noisy-label subset is either discarded [12], relabeled [13] or used in a semi-supervised learning approach [10, 14]. A particularly simple and successful sample selection strategy, which works effectively for a wide range of label noise rates, is the loss-based selection that assumes that small-loss samples have clean labels and high-loss samples have noisy labels [15, 10, 16]. An alternative strategy is based on feature-based sampling, where samples belonging to the same class typically form clusters in the feature space, with samples carrying noisy labels recognized as anomalies within these clusters [17, 13]. As an example of feature-based sampling, the filtering noisy instances via their eigenvectors (FINE) [17] method discriminates clean and noisy-label samples via the distance to class-specific eigenvectors. In this technique, clean-label samples tend to lie closer to the class-specific dominant eigenvector of the latent representations than the noisy-label samples. FINE is highly effective in low noise-rate scenarios, but tends to degrade in high-noise rate scenarios [17]. Conversely, SSR+ [13] incorporates a sample selection criterion using K𝐾Kitalic_K nearest neighbor (KNN) [18] classification in the feature space. While effective in high noise-rate scenarios, SSR+ exhibits limitations in low-noise situations [13]. Existing noisy-label learning methods in the literature apply just a single sample selection strategy (e.g., based on small loss hypothesis, Eigenvector decomposition, or KNN) [9]. Our hypothesis is that by dynamically adapting the selection criterion during the training process and leveraging loss-based selection together with FINE and KNN, we can design a more efficacious sample selection strategy. The top row of Fig. 1 shows an analysis of two feature-based selection strategies, SSR+ (using KNN) [13] and FINE [17], on CIFAR-100 on low (i.e. 20%) and high (i.e. 80%) symmetric noise rate. In that figure, we measure the Precision (P) and Recall (R) of the detection of clean or noisy-label training samples, and the classification accuracy. The bottom row of Fig. 1 shows P and R of the detection of clean or noisy-label training samples on the high confidence subset (HCS) and low confidence subset (LCS), built using the small loss strategy [10], where HCS contains samples with low loss (i.e., samples classified as clean) and LCS has samples with high loss (i.e., samples classified as noisy). In the top row of Fig. 1, we note that FINE has higher accuracy, P and R for the low-noise rate scenario, while SSR+ (using KNN) has higher accuracy and P, but comparable R for the high-noise rate problem. In the bottom row of Fig. 1, we note that FINE tends to perform better than SSR+ for the HCS samples in both datasets, while SSR+ appears to be better for the LCS samples in both datasets, so we hypothesise that FINE should be used for the HCS samples, while SSR+ should be applied to the LCS samples. Figure 1: Mean classification accuracy, Precision (P), and Recall (R) of the detection of clean or noisy-label samples done by SSR+ (using KNN) [13] and FINE [17] on the last 10 epochs (out of 300) on CIFAR-100 with symmetric noise rate of 20% (a) and 80% (b). The top row shows the P and R of the clean or noisy-label sample detection, together with the CIFAR-100 classification accuracy for SSR+ and FINE. The bottom row shows the P and R of the clean or noisy-label classification for the High Confidence Set (HCS) and Low Confidence Set (LCS), formed with the small-loss selection from DivideMix [10]. On the top row, note that FINE works better (higher accuracy, P and R) for the low-noise rate scenario, while SSR+ is better for high-noise rate (higher accuracy and P, but comparable R). On the bottom row, FINE shows better sample selection results for the HCS samples of both datasets, while SSR+ is better for the LCS samples from both datasets. This paper proposes the Adaptive Nearest Neighbors and Eigenvector-based (ANNE) sample selection strategy for robustly learning with noisy labels. In this strategy, we first subdivide the training samples into HCS and LCS using a loss-based selection method [10]. Subsequently, HCS samples are sub-divided into clean and noisy using FINE, while the remaining LCS samples are sub-divided into clean and noisy using the proposed Adaptive k𝑘kitalic_k-nearest neighbours (AKNN) method. The AKNN method adjusts the value of K𝐾Kitalic_K based on the neighbour density in the feature space. The combination of FINE and AKNN is designed to be highly robust to both low and high noise rates. To the best of our knowledge, this is the first approach that investigates the combination of multiple sample selection strategies to split clean and noisy-label samples. Our selection method is integrated into the iterative selection and relabeling training process of the Sample Selection and Relabelling (SSR+) method [13], which is a state-of-the-art (SOTA) noisy-label learning method based on sample selection. To summarise, the main contributions of the proposed method are as follows: 1. A new sample selection methodology for noisy-label learning methods, leveraging the benefits of employing different sample selection strategies, based on small-loss hypothesis, adaptive K nearest neighbours (AKNN), and Eigenvector decomposition (FINE); 2. An adaptive k𝑘kitalic_k-nearest neighbor approach for selecting low-confidence clean-label samples, by dynamically adjusting the k𝑘kitalic_k-value based on neighborhood density, to make the approach robust to both high and low label noise rate scenarios; 3. Incorporation of ANNE into SSR+ [13], which is a SOTA noisy-label learning method based on sample selection, allowing us to validate our methodology on real-world datasets. Empirical results on CIFAR-10/-100 [19] under symmetric, asymmetric and instance-dependent noise, show that ANNE outperforms previous SOTA approaches. For real-world data sets, like Webvision, ANNE presents the best results in the field. Our results also show that ANNE has a competitive training time compared to SOTA approaches."
https://arxiv.org/html/2411.01602v1,DreamPolish: Domain Score Distillation With Progressive Geometry Generation,"We introduce DreamPolish, a text-to-3D generation model that excels in producing refined geometry and high-quality textures. In the geometry construction phase, our approach leverages multiple neural representations to enhance the stability of the synthesis process. Instead of relying solely on a view-conditioned diffusion prior in the novel sampled views, which often leads to undesired artifacts in the geometric surface, we incorporate an additional normal estimator to polish the geometry details, conditioned on viewpoints with varying field-of-views. We propose to add a surface polishing stage with only a few training steps, which can effectively refine the artifacts attributed to limited guidance from previous stages and produce 3D objects with more desirable geometry. The key topic of texture generation using pretrained text-to-image models is to find a suitable domain in the vast latent distribution of these models that contains photorealistic and consistent renderings. In the texture generation phase, we introduce a novel score distillation objective, namely domain score distillation (DSD), to guide neural representations toward such a domain. We draw inspiration from the classifier-free guidance (CFG) in text-conditioned image generation tasks and show that CFG and variational distribution guidance represent distinct aspects in gradient guidance and are both imperative domains for the enhancement of texture quality. Extensive experiments show our proposed model can produce 3D assets with polished surfaces and photorealistic textures, outperforming existing state-of-the-art methods.","Generating 3D assets benefits a series of downstream applications like virtual reality, movies, video game, 3D printing, etc. Existing 3D generation methods can be divided into 3D native method and 2D/3D hybrid methods. 3D native generation methods [22, 34, 23] directly model 3D data, and demonstrate considerable success in producing basic 3D objects. However, these methods often struggle to produce complex objects with intricate geometry details and photorealistic textures, primarily due to a shortage of high-quality training data. Recent advances in text-to-image diffusion models [12, 31, 44] demonstrate the effectiveness of large-scale pretraining on high-quality image-text pairs. The 2D/3D hybrid models, which combine such 2D diffusion models with 3D neural representations, have shown promising outcomes. The Score Distillation Sampling (SDS) objective introduced in DreamFusion [29] is widely used in the 2D/3D hybrid models. It serves as a bridge to bring photorealistic generation capability embedded in 2D text-to-image models to 3D content generation. During training, renderings with random viewpoints are sampled and sent to a pretrained 2D text-to-image diffusion model (e.g.,[31, 1, 28]). The pretrained 2D diffusion model is incorporated to leverage the difference in distribution between the renderings and images posteriorly generated by 2D text-to-image models. However, using SDS as the sole optimization criterion can lead to inconsistent geometry among viewpoints. Furthermore, to maintain training stability, the SDS loss demands an excessively high classifier-free guidance (CFG) weight, leading to overly saturated outcomes. Recent trials [21, 30, 38] propose to decompose the text-to-3D process into multiple phases (e.g., geometry construction and texture generation). Under such a strategy, researchers could leverage varying priors on decomposed sub-tasks, hence boosting the generation performance. Despite these advancements, the quality of generated 3D content has not reached the level of handcrafted and professionally produced 3D assets. In the geometric construction phase, the utilization of a view-conditioned diffusion prior leads to artifacts on the generated surfaces, mainly because such a model cannot to offer adequate supervision on geometric intricacies such as surface details. In the texture generation phase, the quality of the resulting texture continues to be inferior compared to those produced by text-to-image diffusion models. Moreover, obtaining higher-quality textures often requires compromising on stability, presenting the challenge of balancing texture photorealism with training stability. We introduce DreamPolish, a text-to-3D generation method that produces 3D objects with polished geometry and photorealistic textures. We first focus on the geometric surface refinement. We employ a variety of neural implicit and explicit representations to progressively retrieve geometry details, and we utilize an additional polishing stage that boosts the surface smoothness with the help of a pretrained normal estimation prior. We further propose a score distillation objective, dubbed domain score distillation (DSD), designed to identify the target domain that can ensure stability throughout the distillation process and enhance the photorealism of textures. Our pipeline consists of two phases: progressive geometry polishing and domain-guided texture enhancing. In the progressive geometry polishing phase, we start with a text prompt and its corresponding image, and construct a neural representation with image-level supervision in the reference view and employ vanilla SDS from a view-conditioned diffusion prior [22] in novel views. We use NeRF [26] in the early iterations and progressively change the neural representation to advanced surface representations (i.e., NeuS [40] and DMTet [35]). However, the sampled field-of-view remains constrained by the pretrained distribution of the view-conditioned diffusion prior, and there is no surface supervision in novel views. To further polish the geometry surface, we incorporate a diffusion prior that produces 3D-consistent normal estimation [9] to refine and polish the surface in the novel views. The initial limitation to the field-of-view, dictated by the pretrained distribution of 3D diffusion priors, is eventually relaxed, enabling the normal estimator to address and rectify any preceding artifacts. In the domain-guided texture enhancing phase, we first demonstrate through empirical analysis why the vanilla SDS tends to be unstable, requiring a disproportionately high weight for CFG. Subsequently, we illustrate why recent works [41, 38] using guidance from variational domains improve the texture quality and effectively reduce the CFG weight to its normal ranges. In the conditional text-to-image generation, the use of classifier-free guidance leveraging unconditional image domains is shown to mitigate the decline in generated performance observed when text conditioning is introduced. This suggests that the unconditional image domain is beneficial for generation diversity and stability. Building upon these analyses, we propose DSD, aiming to balance the generation quality and stability. DSD mimics the classifier-free guidance generation process and proposes to use the unconditional gradient direction to guarantee the stability of score distillation. As illustrated in Figure 1, DreamPolish demonstrates its capability in generating 3D content featuring polished geometry and photorealistic textures. We have conducted extensive experiments to showcase the superior performance of DreamPolish compared to previous methods. Additionally, we have performed ablation studies to validate the effectiveness of our proposed modules. The complete implementation will be made publicly available upon acceptance of our work."
https://arxiv.org/html/2411.01595v1,RS-MoE: Mixture of Experts for Remote Sensing Image Captioning and Visual Question Answering,"Remote Sensing Image Captioning (RSIC) presents unique challenges and plays a critical role in applications such as environmental monitoring, urban planning, and disaster management. Traditional RSIC methods often struggle to produce rich and diverse descriptions. Recently, with significant advancements in Vision-Language Models (VLMs), efforts have emerged to integrate these models into the remote sensing domain and to introduce richly descriptive datasets specifically designed to enhance VLM training. However, most current RSIC models generally apply only fine-tuning to these datasets without developing models tailored to the unique characteristics of remote sensing imagery. This paper proposes RS-MoE, a first Mixture of Expert based VLM specifically customized for remote sensing domain. Unlike traditional MoE models, the core of RS-MoE is the MoE Block, which incorporates a novel Instruction Router and multiple lightweight Large Language Models (LLMs) as expert models. The Instruction Router is designed to generate specific prompts tailored for each corresponding LLM, guiding them to focus on distinct aspects of the RSIC task. This design not only allows each expert LLM to concentrate on a specific subset of the task, thereby enhancing the specificity and accuracy of the generated captions, but also improves the scalability of the model by facilitating parallel processing of sub-tasks. Additionally, we present a two-stage training strategy for tuning our RS-MoE model to prevent performance degradation due to sparsity. We fine-tuned our model on the RSICap dataset using our proposed training strategy. Experimental results on the RSICap dataset, along with evaluations on other traditional datasets where no additional fine-tuning was applied, demonstrate that our model achieves state-of-the-art performance in generating precise and contextually relevant captions. Notably, our RS-MoE-1B variant achieves performance comparable to 13B VLMs, demonstrating the efficiency of our model design. Moreover, our model demonstrates promising generalization capabilities by consistently achieving state-of-the-art performance on the Remote Sensing Visual Question Answering (RSVQA) task.","Image captioning integrates computer vision and natural language processing to automatically generate descriptive text for images, effectively bridging the information gap between visual and linguistic domains. Unlike captioning of common natural images, remote sensing image captioning (RSIC) poses greater challenges due to the inherent characteristics of remote sensing images, which cover larger geographic areas and more diverse geographical objects. Consequently, RSIC tasks demand not only the description of geographic object information but also a comprehensive understanding of the relationships and interactions among various geographic objects. Early studies of RSIC focused on traditional methodologies, including template-based [1] and retrieval-based methods [2, 3]. However, these methods cannot generate rich and varied descriptive sentences. Recent studies [4, 5, 6, 7] commonly adopt the encoder-decoder architecture, which divides the RSIC task into an image encoding phase that extracts semantic features from the input image, and a sequence modeling phase that uses the extracted features to generate text and sentences. Depending on the specific models employed, encoder-decoder approaches can be further categorized into CNN-based encoders with RNN/LSTM decoders, and CNN-based encoders with Transformer decoders. Although these methods have achieved satisfying performance on the RSIC task, they typically generate only one or two simple sentences for captioning, limiting their practical application. This limitation typically arises due to two principal reasons: the simplicity and repetitiveness of the sentences in the datasets used for training models; and the relatively limited ability of these models to extract semantic features and generate complex descriptions. The Large Language Models (LLMs) and Vision Language Models (VLMs) have recently achieved significant success across multiple fields, including computer vision [8], natural language processing [9], and robotics [10]. Particularly, VLMs [11, 12, 13] have effectively narrowed the gap between visual images and textual natural language by advancing the understanding of intermodal relationships, reaching a level of visual comprehension comparable to human capabilities. Therefore, some researchers [14, 15, 16, 17, 18, 19, 20] have recently shifted their focus to applying VLMs to remote sensing visual interpretation tasks. Specifically, for RSIC tasks, the RSGPT [15] introduced the human-annotated RSICap dataset, which provides high-quality and detailed descriptions for remote sensing images. The remote sensing captions in this dataset include summarized scene descriptions, object information (color, shape, and count), and object relationships (relative position). However, RSGPT merely fine-tunes an existing VLM model on this dataset without proposing a novel model to explore and utilize the provided detailed descriptions deeply. In this paper, we propose a novel Vision-Language Model (VLM) based on the Mixture of Experts (MoE) framework, named RS-MoE, specifically customized for the Remote Sensing Image Captioning (RSIC) task. To the best of our knowledge, this is the first work that applies the MoE framework to VLMs in the remote sensing domain. We extend the core principles of MoE to specifically address the unique challenges of remote sensing images. By decomposing the RSIC task into specialized subtasks, we leverage different expert models to facilitate more precise and contextually aware caption generation. Specifically, RS-MoE consists of three key components: the Image Encoder, the VLM Encoder, and the MoE Block. Unlike conventional MoE modules, our MoE Block consists of a novel Instruction Router and multiple lightweight LLMs. The Instruction Router is designed to generate tailored prompts for each LLM. In our experiments, these prompts focus on theme comprehension, object recognition, and relationship inference. The Instruction Router dynamically adjusts prompts based on visual features and task instructions, offering a more precise level of control and enabling the model to generate descriptions specifically tailored to each captioning objective. Furthermore, unlike traditional MoE models that typically use feed-forward networks (FFNs) as experts, we incorporate lightweight LLMs as expert models. This design allows each LLM to process complex, task-specific linguistic information efficiently and generate detailed, contextually aware descriptions while ensuring computational feasibility. In addition to the RS-MoE model, we propose a two-stage training strategy to fine-tune the MoE framework for the RSIC task, addressing potential model degradation caused by the sparsity that typically arises when applying MoE to remote sensing images. This sparsity arises from the lack of representation of remote sensing-specific features in models pretrained on natural images, leading to inefficiencies in capturing the complex characteristics of remote sensing imagery, which ultimately affects model performance. Moreover, to significantly reduce the number of trainable parameters, we incorporate the Low-Rank Adaptation (LoRA) training strategy into the RS-MoE tuning stages. After fine-tuning our model on the RSICap dataset using our proposed training strategy, we evaluated it on the RSICap dataset and directly tested it on traditional datasets, such as UCM-Captions, Sydney-Captions, and RSICD, without any additional fine-tuning on these datasets. Experimental results demonstrate that our RS-MoE-7B model achieves state-of-the-art performance, while the RS-MoE-1B variant achieves performance comparable to 13B VLMs in generating precise and contextually relevant captions. Furthermore, we extend our model to the Remote Sensing Visual Question Answering (RSVQA) task and demonstrate promising generalization capabilities by consistently achieving state-of-the-art performance on this task. Our contributions are summarized as follows: • We are the first to introduce the MoE framework within the domain of remote sensing. By drawing on the idea behind MoE for handling complex tasks through task decomposition, we break down the intricate challenges of remote sensing image captioning into distinct subtasks, enabling specialized expert models to more effectively address the diversity and complexity inherent in remote sensing data. • We propose a novel Vision-Language Model, RS-MoE, specifically designed for remote sensing image captioning. Unlike traditional MoE models, our model introduces two key innovations: a novel Instruction Router module that dynamically generates task-specific prompts and lightweight LLMs as expert models. These innovations optimize the model’s ability to handle the complexity and diversity of remote sensing images while maintaining computational efficiency. • We present a two-stage training strategy for the RS-MoE model tailored to the RSIC task, which incorporates a proper initialization process to prevent model degradation due to the sparsity that typically results from directly applying MoE. Additionally, we employ the LoRA technique to significantly reduce the number of trainable parameters, thereby enhancing the efficiency and manageability of the model during the training process. • Extensive experiments demonstrate that, even with fine-tuning on only one dataset containing around 3,000 images, our model achieves state-of-the-art performance across four RSIC datasets and exhibits strong generalization capabilities on an RSVQA dataset. Notably, our lightweight RS-MoE-1B model achieves comparable performance to larger 13B VLM methods for remote sensing image captioning, while offering significantly greater efficiency. Figure 1: Overview of the proposed RS-MoE model, which consists of three key components: the Image Encoder, the VLM Encoder, and the MoE Block. The MoE Block comprises an Instruction Router that dynamically generates task-specific prompts and three lightweight LLMs, which focus on different aspects of the captioning task. In the generated captions, shown in the top right corner of the figure, distinct colors represent each aspect: orange for the overall theme, purple for specific objects, and green for relationships between objects. RS-MoE is trained using a novel two-stage training strategy specifically designed for remote sensing image captioning. In Stage I (a), the VLM Encoder and the LLM Block are fine-tuned to initialize model weights specifically designed for the RSIC task. In Stage II (b), the MoE Block is fine-tuned to produce more detailed captions for RSIC tasks."
https://arxiv.org/html/2411.01593v1,High-Fidelity Virtual Try-on with Large-Scale Unpaired Learning,"Virtual try-on (VTON) transfers a target clothing image to a reference person, where clothing fidelity is a key requirement for downstream e-commerce applications. However, existing VTON methods still fall short in high-fidelity try-on due to the conflict between the high diversity of dressing styles (e.g. clothes occluded by pants or distorted by posture) and the limited paired data for training. In this work, we propose a novel framework Boosted Virtual Try-on (BVTON) to leverage the large-scale unpaired learning for high-fidelity try-on. Our key insight is that pseudo try-on pairs can be reliably constructed from vastly available fashion images. Specifically, 1) we first propose a compositional canonicalizing flow that maps on-model clothes into pseudo in-shop clothes, dubbed canonical proxy. Each clothing part (sleeves, torso) is reversely deformed into an in-shop-like shape to compositionally construct the canonical proxy. 2) Next, we design a layered mask generation module that generates accurate semantic layout by training on canonical proxy. We replace the in-shop clothes used in conventional pipelines with the derived canonical proxy to boost the training process. 3) Finally, we propose an unpaired try-on synthesizer by constructing pseudo training pairs with randomly misaligned on-model clothes, where intricate skin texture and clothes boundaries can be generated. Extensive experiments on high-resolution (1024×76810247681024\times 7681024 × 768) datasets demonstrate the superiority of our approach over state-of-the-art methods both qualitatively and quantitatively. Notably, BVTON shows great generalizability and scalability to various dressing styles and data sources.","Virtual try-on, fitting the target clothes onto a reference person, have achieved great progress in recent years [26, 7, 25, 13]. Well-designed architectures are frequently proposed to build the relationship between the in-shop clothes and the reference person. However, we still observe three major problems in current try-on pipelines: 1) Low clothing fidelity of current (“conventional”) try-on pipelines which ignore the actual shape of target clothing images. Clothing fidelity requires that the original clothing characteristics, especially the irregular designs, are well preserved. 2) Insufficient data on the widely-used clothes-model pairs. 3) Low-resolution of the results generated by most of the current methods [26, 3, 4, 25, 7]. As resolution is one of the most important factors in downstream e-commerce applications, designing high-fidelity try-on pipelines with high-resolution output is essential. Earlier efforts in improving clothing fidelity lie on two key modules: characteristics-preserving deformation module and accurate semantic prediction module. From the very first Thin-plate Spline (TPS) based methods [6, 19, 14, 26], various pioneering deformation methods are proposed such as Moving Least Squares (MLS) based method [25] and flow-based methods [4, 7]. With the maximized flexibility, flow-based methods can model any transformation with the regularized training objectives. On the other hand, modeling the accurate after-try-on semantics is also crucial, as addressed in [26, 25]. With limited paired data, learning the target semantic layout conditioned on the target clothes is a challenging problem. To alleviate the problem, RT-VTON proposes a tri-level attention mechanism by modeling the semantic prediction as a long-range correspondence learning that achieves state-of-the-art semantic accuracy in previous methods. However, the aforementioned methods adopt the same conventional design which directly preserves the bottom clothes when fitting the target clothes at the reference person, ignoring the intricate shape details as well as the correct length of the target clothing, as in Fig. 1. A trivial fix can be achieved by removing the bottom clothes in the semantic prediction modules, but it severely suffers from another problem with the ambiguous wearing styles of the model images, which degenerates the stability of semantic prediction. To tackle the aforementioned three major problems in current pipelines, we propose a novel framework, Boosted Virtual Try-on (BVTON) which generates high-resolution (1024×76810247681024\times 7681024 × 768) results by leveraging the large-scale unpaired learning for high-fidelity try-on. Specifically, BVTON consists of four major modules as shown in Fig. 2. The first module, as the key of our solution, is the Clothes Canonicalization Module (CCM). The CCM predicts a compositional canonicalizing flow that maps on-model clothes into pseudo in-shop clothes, dubbed canonical proxy. The second part is the Layered Mask Generation Module (L-MGM), which predicts the Layered semantic masks of the reference person wearing the target clothes. As opposed to prior arts, our L-MGM is not trained with in-shop clothes pairs but solely on large-scale fashion images, which is achieved by using the canonical proxy generated by the pre-trained CCM. Notably, our L-MGM is a plug-and-play module that can take the target clothes as input in the inference phase, so it can be used in any semantic-based pipeline [2, 26, 25]. The third part is the Mask-guided Clothes Deformation Module (M-CDM) that predicts deformation flow to warp the target clothes onto the reference person, guided by the layered semantic masks generated by L-MGM. Finally, with the predicted segmentation, we propose an Unpaired Try-on Synthesizer Module (UTOM) by constructing pseudo training pairs with randomly misaligned on-model clothes, where intricate skin texture and clothes boundaries can be generated. The UTOM also acts as a plug-and-play module that takes the warped clothes as input in the inference phase; it generates the final results according to the predicted layered masks. In this way, the spatial misalignment of the deformed clothes can be tolerated with guidance of layered semantic masks. Our contributions can be summarized as follows: 1) We design a principled try-on paradigm, i.e., BVTON, which generates high-resolution (1024 × 768) results by leveraging additional large-scale unpaired learning for high-fidelity try-on. Intricate clothing details such as laces, over-long clothes, and asymmetrical clothes bottoms can be well preserved. 2) Our unified framework is the first cloth-to-model try-on approach that can adapt seamlessly to model-to-model virtual try-on without retraining. We demonstrate the incapability of baseline methods on model-to-model try-on setting in supp.. Note that we do not claim superiority over other model-to-model works, but only demonstrate as an extra application. 3) We propose a novel unpaired try-on synthesizer that decouples the conventional try-on synthesis training from the limited paired data to boost the generative capabilities with large-scale unpaired learning on fashion images. 4) BVTON greatly outperforms three lastest state-of-the-art methods [25, 7, 13] across three different test sets (TEST1, TEST2 and VITON [6]). In the conventional setting (retaining bottom clothes), significant gains are achieved by 35.2% in FID, 34.2% in LPIPS, and 5.7% in SSIM (TEST1 compared to [13]). Some extra baselines [4, 1] are given in quantitative results for reference."
https://arxiv.org/html/2411.01584v1,One for All: Multi-Domain Joint Training forPoint Cloud Based 3D Object Detection,"The current trend in computer vision is to utilize one universal model to address all various tasks. Achieving such a universal model inevitably requires incorporating multi-domain data for joint training to learn across multiple problem scenarios. In point cloud based 3D object detection, however, such multi-domain joint training is highly challenging, because large domain gaps among point clouds from different datasets lead to the severe domain-interference problem. In this paper, we propose OneDet3D, a universal one-for-all model that addresses 3D detection across different domains, including diverse indoor and outdoor scenes, within the same framework and only one set of parameters. We propose the domain-aware partitioning in scatter and context, guided by a routing mechanism, to address the data interference issue, and further incorporate the text modality for a language-guided classification to unify the multi-dataset label spaces and mitigate the category interference issue. The fully sparse structure and anchor-free head further accommodate point clouds with significant scale disparities. Extensive experiments demonstrate the strong universal ability of OneDet3D to utilize only one trained model for addressing almost all 3D object detection tasks (Fig. 1).","(a) (b) Figure 2: Illustration of existing 3D detectors (a) and ours (b). Existing detectors can be divided into point-based (up) and voxel-based (down). Our model has the capacity for joint training on multi-domain point cloud data. 3D point cloud based object detection aims to predict the oriented 3D bounding boxes and the corresponding semantic category tags for the real scenes given a point set. Unlike mature 2D detectors ren2015faster ; he2017mask ; tian2019fcos ; carion2020end , which once trained, can generally conduct inference on different types of images in various scenes and environments, current 3D detectors still follow a single-dataset training-and-testing paradigm, i.e., point clouds used during inference should be from the totally same domain as that used during training. Whether indoor qi2019deep ; zhang2020h3dnet ; xie2021venet ; wang2022cagroup3d or outdoor yan2018second ; lang2019pointpillars ; shi2020points ; shi2020pv , existing point cloud based 3D detectors can only be trained on datasets from one specific domain, then be tested on the same domain data. Such restriction of training and testing on a single dataset severely hampers the generalization ability of 3D detectors, resulting in a significant lag in the progress of 3D detection compared to 2D in terms of universality. To address this issue, multi-domain joint training (i.e., multi-dataset joint training) should be introduced into point cloud based 3D object detection, to allow 3D detectors to learn from point clouds of different domains through large-scale joint training. In this way, a 3D detector, once trained, can well generalize across various domains of point clouds. The ultimate goal is to obtain a 3D detector that can support unified 3D object detection across different domains with only one set of parameters, thereby achieving the target of universal 3D object detection. The motivation of multi-domain joint training is to learn universal 3D knowledge by leveraging point clouds from different sources and domains, thereby establishing a general representation from 3D data to 3D spatial positions. Through this, a model independent of point cloud source, collection, and domain can be achieved. With common 3D knowledge from diverse point clouds, it can effectively serve as a universal 3D detector and function as a 3D foundation model. However, achieving this is highly challenging and difficult. As can be seen in Fig. 1, due to the significant domain gaps (e.g., point cloud ranges, scenes, object sizes, sparsity, etc.), existing 3D detectors fail to support this. Specifically, current 3D detectors can be generally divided into point-based and voxel-based ones. For point-based 3D detectors (the upper part of Fig. 2(a)) qi2019deep ; zhang2020h3dnet ; liu2021group , it is difficult to apply the same sampling and grouping technique for different domain data. For voxel-based 3D detectors (the lower part of Fig. 2(a)) yan2018second ; yin2021center ; li2022unifying ; wang2024uni3detr , which usually require converting from sparse to dense features for 3D box prediction, the scale differences between indoor and outdoor point clouds make it difficult to represent them using dense features of the same size. This consequently limits existing models to learning domain-specific knowledge, restricting their ability to acquire generalized 3D knowledge. In this paper, we propose OneDet3D, a unified point cloud based 3D detector with only one set of parameters through multi-domain joint training. As in Fig. 2(b), we employ 3D sparse convolution for feature extraction, which is more robust to domain gaps compared to point-based feature extractors qi2017pointnet ; qi2017pointnet++ , making it well-suited for adapting to point clouds from different domains. Subsequently, we utilize an anchor-free detection head, where objects are represented by center points tian2019fcos ; yin2021center ; rukhovich2022fcaf3d , enabling direct compatibility with sparse convolution and avoiding the constraints of fixed-size dense features. Such a fully sparse structure, together with the anchor-free detection head using center point representation, provides an effective architecture for multi-domain joint training. Based on the model architecture, during multi-domain joint training, the domain-interference issue should be further addressed. This issue primarily comprises two aspects: data-level interference caused by differences in point clouds themselves, and category-level interference caused by label conflict among categories across different domains. To mitigate the data-level interference, we employ domain-aware partitioning, which partitions parameters where the interference problem mainly exists to be domain-specific and keeps the vast majority shared among different domains. The data-level interference can thus be effectively prevented without increasing the model complexity too much. Specifically, we partition re-scaling in normalization layers to maintain the consistency of the data scatter, and parameters about context learning for reducing the effect of range disparities. They are guided by a domain router implemented by a domain classifier. To alleviate category-level interference, we employ language-guided classification, leveraging the text modality to alleviate conflict issues. We utilize a combination of fully connected layers and sparse convolution for class-specific and class-agnostic classification to ensure compatibility with the anchor-free head. Our main contributions can be summarized as follows: • We propose OneDet3D, a multi-domain point cloud joint training model for universal 3D object detection. To the best of our knowledge, this is the first 3D detector that supports point clouds from domains in both indoor and outdoor simultaneously with only one set of parameters. • We propose the domain-aware partitioning in scatter and global context, guided by the domain routing mechanism. In this way, the data-level interference issue caused by point cloud disparities can be alleviated during multi-dataset joint training. • We integrate the text modality into the anchor-free head classification. Through employing both fully connected layers and 3D sparse convolution for the dual-level of class-agnostic and class-specific classification, the issue of category-level interference can be mitigated. Extensive experiments demonstrate the one-for-all ability of our OneDet3D. OneDet3D possesses the strong generalization ability in both category and scene, thus effectively achieving the goal of universal 3D object detection. In the close-vocabulary setting, it achieves comparable performance using only one set of parameters. In the open-vocabulary setting, it obtains more than 7% performance."
https://arxiv.org/html/2411.01573v1,Conditional Controllable Image Fusion,"Image fusion aims to integrate complementary information from multiple input images acquired through various sources to synthesize a new fused image. Existing methods usually employ distinct constraint designs tailored to specific scenes, forming fixed fusion paradigms. However, this data-driven fusion approach is challenging to deploy in varying scenarios, especially in rapidly changing environments. To address this issue, we propose a conditional controllable fusion (CCF) framework for general image fusion tasks without specific training. Due to the dynamic differences of different samples, our CCF employs specific fusion constraints for each individual in practice. Given the powerful generative capabilities of the denoising diffusion model, we first inject the specific constraints into the pre-trained DDPM as adaptive fusion conditions. The appropriate conditions are dynamically selected to ensure the fusion process remains responsive to the specific requirements in each reverse diffusion stage. Thus, CCF enables conditionally calibrating the fused images step by step. Extensive experiments validate our effectiveness in general fusion tasks across diverse scenarios against the competing methods without additional training. The code is publicly available.*** https://github.com/jehovahxu/CCF","Image fusion aims at integrating complementary information from multi-source images, fusing a new composite image containing richer details [1]. It has been applied in various scenarios that single image contains incomplete information, such as multi-modal fusion (MMF) [2, 3], multi-exposure fusion (MEF) [4, 5], multi-focus fusion (MFF) [6], and remote sensing fusion [2]. The fused image inherits the strengths of both modalities, resulting in a composite with enhanced visual effects [7]. These fusion tasks have diverse downstream applications in computer vision, including object detection [8, 9, 10], semantic segmentation [11, 12], and medical diagnosis [13] because the comprehensive representation of images with multi-scene information contributes to the improved performance of applications. Figure 1: The conditions selection statistics during the sampling process of the LLVIP dataset. The distinct process of sampling has different favor of the conditions. The crucial role that diverse conditions play in controlling various image generation processes. Throughout the diffusion sampling, different conditions are dynamically selected to best suit the generation requirement at each stage. Recently, numerous image fusion methods [14, 15, 16, 17] have been proposed, such as traditional fusion methods [18], CNN-based fusion methods [19, 20] and GAN-based methods [21]. While these methods produce acceptable fused images in certain scenarios, they are also accompanied by significant drawbacks and limitations: (i) They are often tailored for specific scenarios or individual tasks, limiting their adaptability across diverse applications; (ii) These methods necessitate training and consume substantial computational resources, posing limitations in terms of time and resource requirements. Lately, denoising diffusion probabilistic models (DDPM) have emerged as an iterative generation framework, showcasing impressive capabilities in unconditional generation. Inspiringly, numerous researchers explored its controllable aspects. ILVR [22] proposed iterative latent variable refinement with a reference image to control image translation. Some recent works [23, 24] employed the diffusion model for image fusion, which fuses images in fixed fusion paradigms (fixed fusion conditions) by using its inherent reconstruction capacity. However, these approaches are not qualified for sample-customized fusion with dynamic conditions. At present, general image fusion with controllable diffusion models is still a challenging problem, warranting further exploration. In this paper, we propose a diffusion-based controllable conditional image fusion (CCF) framework, which controls the fusion process by adaptively selecting optimization conditions. We construct a condition bank of generally used conditions, categorizing them into basic, enhanced, and task-specific conditions. CCF dynamically assigns fusion conditions from the condition bank and continuously injects them into the sampling process of diffusion. To enable flexible integrated conditions, we further propose a sampling-adaptive condition selection (SCS) mechanism that tailors condition selection at different denoising steps. The iterative refinements of the sampling are based on the pre-trained diffusion model without additional training. It is worth noting that the estimated fused images are conditionally controllable during the iterative denoising process. The diffusion process seamlessly integrates these conditions during the sampling process, decreasing potential impacts. As illustrated in Fig. 1, the generation process emphasizes different aspects at various sampling steps. In the initial stages, the condition selection is influenced by random noise, resulting in a random selection. During the intermediate stages, there is a shift towards content components. In the final stage, the emphasis moves to generating and selecting texture details. These various conditional factors contribute to different aspects of fusion results and demonstrate the necessity and effectiveness of introducing specific conditions in different stages. To the best of our knowledge, we for the first time propose a conditional controllable framework for image fusion. The main contributions are summarized as follows: • We propose a pioneering conditional controllable image fusion (CCF) framework with a condition bank, achieving controllability in various image fusion scenarios and facilitating the capability of dynamic controllable image fusion. • We propose a sampling-adaptive condition selection mechanism to subtly integrate the condition bank into denoising steps, allowing adaptive condition selection on the fly without additional training and ensuring the dynamic adaptability of the fusion process. • Extensive experiments on various fusion tasks have confirmed our superior fusion performance against the competing methods. Furthermore, our approach qualifies for interactive manipulation of the fusion results, demonstrating our applicability and efficacy."
https://arxiv.org/html/2411.01564v1,ParseCaps: An Interpretable Parsing Capsule Networkfor Medical Image Diagnosis,"Deep learning has excelled in medical image classification, but its clinical application is limited by poor interpretability. Capsule networks, known for encoding hierarchical relationships and spatial features, show potential in addressing this issue. Nevertheless, traditional capsule networks often underperform due to their shallow structures, and deeper variants lack hierarchical architectures, thereby compromising interpretability. This paper introduces a novel capsule network, ParseCaps, which utilizes the sparse axial attention routing and parse convolutional capsule layer to form a parse-tree-like structure, enhancing both depth and interpretability. Firstly, sparse axial attention routing optimizes connections between child and parent capsules, as well as emphasizes the weight distribution across instantiation parameters of parent capsules. Secondly, the parse convolutional capsule layer generates capsule predictions aligning with the parse tree. Finally, based on the loss design that is effective whether concept ground truth exists or not, ParseCaps advances interpretability by associating each dimension of the global capsule with a comprehensible concept, thereby facilitating clinician trust and understanding of the model’s classification results. Experimental results on CE-MRI, PH2, and Derm7pt datasets show that ParseCaps not only outperforms other capsule network variants in classification accuracy, redundancy reduction and robustness, but also provides interpretable explanations, regardless of the availability of concept labels.","Deep learning methods of medical image classification provides consistent, rapid predictions that often exceed human capabilities in detecting subtle abnormalities. However, obtaining extensive, high-quality datasets is challenging and poor interpretability limits their clinical application. Capsule networks (CapsNets) have shown potential to enhance interpretability by maintaining hierarchical relationships and spatial orientations within images (Sabour, Frosst, and Hinton 2017; Hinton, Sabour, and Frosst 2018). CapsNets not only excel in generalizing from small datasets, but also improve classification accuracy by capturing relationships between disease markers and normal anatomical structures (Akinyelu et al. 2022; Ribeiro et al. 2022; Patrick et al. 2022). CapsNets utilize vectors called capsules to replace single neuron. Each capsule vector’s length represents the presence probability of specific entity in the input image, and its direction encodes the captured features (Sabour, Frosst, and Hinton 2017). Each dimension of the capsule vector, termed an instantiation parameter, represents the direction of the capsule, thus conferring inherent physical meanings. These parameters hold potential for concept interpretability, as each corresponds to a human-understandable and meaningful concept. Detailed introduction of CapsNets is in supplementary material A. Existing CapsNets face challenges to assign clear meaning to instantiation parameters; however, integrating a parse-tree-like structure could map part-to-whole relationships similarly to human cognitive processes (Sabour, Frosst, and Hinton 2017), thereby enhancing concept interpretability. In this structure, each node is a capsule, and through routing, active capsules select parent capsules from the upper layer. Because (Sabour, Frosst, and Hinton 2017) does not adhere to a strict tree structure where each child node connects to only one parent node, it is referred to as parse-tree-like. The top layer features a single “global capsule” that provides a comprehensive view of the entire image, encapsulating entities and their hierarchical relationships. This allows each instantiation parameters to align with an interpretable and conceptually meaningful image entity. The parse tree provides a carrier for interpretability through the global capsule. Coupled with loss constraints, they create a capsule network with concept interpretability. However, the current implementation of parse tree faces challenges. First, it lacks a suitable routing algorithm that effectively supports the parse-tree-like structure. Dynamic routing process tends to create a fully connected structure between sub-capsules and parent capsules (Peer, Stabinger, and Rodriguez-Sanchez 2018; Jeong, Lee, and Kim 2019), which undermines the desired selective connection essential for clear hierarchical relationships. Additionally, although existing attention routing method sparsifies the coupling coefficients between capsules (Geng et al. 2024), it does not see the significance of instantiation parameters within capsules, which adversely affects the global capsule’s instantiation parameters. Second, certain CapsNet layers contradict the parse tree by increasing the number of capsules without correspondingly enhancing their dimension (Rajasegaran et al. 2019; Choi et al. 2019; Geng et al. 2024)., which is detailed in Sec. 3. They fail to concentrate features of entire image into a global capsule; instead, as the number of capsules increases, features become more dispersed, leading to redundancy in feature representation and a diluted hierarchical structure. Furthermore, existing CapsNets lack loss functions to promote concept interpretability. This paper presents ParseCaps, a novel capsule network featuring three enhancements corresponding to above challenges. First, following (Geng et al. 2024), a sparse axial attention (SAA) routing is proposed, which not only sparsifies coupling coefficients but also weighs instantiation parameters within each capsule. The sparsity inherent in SAA routing restricts routing process, excluding sub-capsules with weaker connections to parent capsules, thus benefiting a hierarchical parse-tree-like structure. Second, we introduce a parse convolutional capsule (PConvCaps) layer. This layer generates capsules whose prediction strictly aligns with the parse tree, reducing the number of capsules while increasing their dimensionality as layer depth increases, thus forming an interpretable global capsule. Third, we design loss functions to enable the model to provide conceptual explanations that aligns each dimension of the capsule vector with a human-understandable concept, regardless of whether concept ground truth are available or not. The motivation of this paper is shown in Fig. 1. Contributions 1) ParseCaps forms a parse-tree-like structure by creating PConvCaps layer and SAA routing to optimize the network’s hierarchical structure, especially focusing on instantiation parameter weights in SAA routing. 2) It enhances interpretability in CapsNets with a parse-tree-like structure and loss functions, aligning instantiation parameters of global capsule with human-understandable concepts. 3) It outperforms existing CapsNets in medical image classification and provides interpretable explanations regardless of concept label availability. Figure 1: The visualization of motivation. Using a parse-tree-like structure as a carrier, combined with the constraints of loss, an interpretable capsule network with instantiation parameters has been achieved. Each dimensional instantiation parameter of the global capsule corresponds to a human-understandable concept."
https://arxiv.org/html/2411.01545v1,Towards Small Object Editing: A Benchmark Dataset and A Training-Free Approach,"A plethora of text-guided image editing methods has recently been developed by leveraging the impressive capabilities of large-scale diffusion-based generative models especially Stable Diffusion. Despite the success of diffusion models in producing high-quality images, their application to small object generation has been limited due to difficulties in aligning cross-modal attention maps between text and these objects. Our approach offers a training-free method that significantly mitigates this alignment issue with local and global attention guidance , enhancing the model’s ability to accurately render small objects in accordance with textual descriptions. We detail the methodology in our approach, emphasizing its divergence from traditional generation techniques and highlighting its advantages. What’s more important is that we also provide SOEBench (Small Object Editing), a standardized benchmark for quantitatively evaluating text-based small object generation collected from MSCOCO(Lin et al., 2014) and OpenImage(Kuznetsova et al., 2020). Preliminary results demonstrate the effectiveness of our method, showing marked improvements in the fidelity and accuracy of small object generation compared to existing models. This advancement not only contributes to the field of AI and computer vision but also opens up new possibilities for applications in various industries where precise image generation is critical. We will release our dataset on our project page: https://soebench.github.io/","The realm of text-to-image generation has witnessed tremendous advancements with the advent of recent diffusion models(Podell et al., 2024; Rombach et al., 2022; Nichol et al., 2021; Ho et al., 2020; Song et al., 2020b), which have successfully revolutionized various tasks, including photo editing(Kawar et al., 2023; Wang et al., 2023), and inpainting(Avrahami et al., 2022; Saharia et al., 2022). These models have demonstrated remarkable capabilities in producing and manipulating salient objects within a picture, like the main subject of a picture, under the description guidance. The success of such models can be attributed to the effective cross-modal feature alignment between textual descriptions and the corresponding visual objects during the synthesis process. Such alignment facilitates a coherent and accurate translation of textual descriptions into visual representations, resulting in impressive outcomes in both image editing and in-painting tasks. Figure 2. Comparison between our multi-scale joint attention guidance method and traditional text-based image in-painting methods. Traditional text-based image in-painting methods can only edit objects with a large scale. The first column: The input image with mask condition. The second column: Small mask condition is more likely leads to wrong attention map. The third column: Our method multi-scale joint attention guidance method can obtain the refined cross-attention map for accurate small object editing. The last column: The output image generated by refined cross-attention map. However, a severe issue arises when the pending object is small. As shown in Fig. 2 (b), using small masks to guide the target object editing can lead to problems such as attribute leakage, poor quality, and missing entities. This issue is primarily due to the model’s inability to focus on such a small region of interest in the description, which can result in the model’s inability to generate objects that align well with the textual descriptions. For example, consider an image with a size of 512×512512512512\times 512512 × 512 and a small object whose bounding box typically occupies only 64×64646464\times 6464 × 64 pixels. When performing multi-level cross-modal feature alignment, with U-Net as the backbone, the cross-attention map is progressively down-sampled to a small resolution of 8×8888\times 88 × 8 as the network deepens. At the same time, our target region may encompass within only a 1×1111\times 11 × 1 grid, which is too small for the model to effectively focus on. Consequently, the model may struggle to generate objects that align well with the textual descriptions within such a small area, leading to poor quality and even missing entities. Facing the abovementioned limitations, we introduce a new task called Small Object Editing (SOE) in this work. Specifically, the SOE task requires the model to perform editing that is consistent with the given textual descriptions, seamlessly integrates with the surrounding context of the original image, and is precise in the desired small region. The key to this task is to prevent mismatches between the desired masked region and the intended textual description, ensuring that the model generates accurate and high-quality small objects. To evaluate the performance of models on this SOE task, we construct a comprehensive benchmark dataset, i.e., SOEBench. This benchmark allows us to evaluate the effectiveness of small object editing on various models and includes 4000 objects from two established datasets, MSCOCO (Lin et al., 2014) and OpenImages (Kuznetsova et al., 2020). SOEBench holds two sets, SOE-2k and SOG-4k, where SOE-2k contains 2000 objects from OpenImages for editing and SOE-4k contains an additional 2000 objects from MSCOCO for editing. For each small object editing prompt, we provide both label-only and label-with-color templates in the description. This allows us to evaluate the models’ ability to generate small objects that align with both textual descriptions and color specifications. Building upon the constructed SOEBench, we further provide a strong baseline method for small object editing. As discussed above, the quality of the cross-attention map is crucial in small object editing. To this end, we propose a new joint attention guidance method to enhance the accuracy of the cross-attention map alignment from both local and global perspectives. In particular, we first develop a local attention guidance strategy to enhance the foreground cross-attention map alignment and then introduce a global attention guidance strategy to enhance the background cross-attention map alignment. Our proposed baseline method is training-free but highly effective in addressing the SOE problem. The contributions of our work are summarized as below: • By recognizing the limitations of current research, we define a new task of small object editing and come up with a new benchmark dataset SOEBench to evaluate the model’s ability on small object editing. • We introduce a novel training-free baseline approach to address the small object editing challenge by considering both global and local perspectives. • The proposed method is evaluated on the proposed SOEBench dataset and achieves the state-of-the-art results."
https://arxiv.org/html/2411.01512v1,InstantGeoAvatar:Effective Geometry and Appearance Modeling of Animatable Avatars from Monocular Video,"We present InstantGeoAvatar, a method for efficient and effective learning from monocular video of detailed 3D geometry and appearance of animatable implicit human avatars. Our key observation is that the optimization of a hash grid encoding to represent a signed distance function (SDF) of the human subject is fraught with instabilities and bad local minima. We thus propose a principled geometry-aware SDF regularization scheme that seamlessly fits into the volume rendering pipeline and adds negligible computational overhead. Our regularization scheme significantly outperforms previous approaches for training SDFs on hash grids. We obtain competitive results in geometry reconstruction and novel view synthesis in as little as five minutes of training time, a significant reduction from the several hours required by previous work. InstantGeoAvatar represents a significant leap forward towards achieving interactive reconstruction of virtual avatars.","Enabling the reconstruction and animation of 3D clothed avatars is a key step to unlock the potential of emerging technologies in fields such as augmented reality (AR), virtual reality (VR), 3D graphics and robotics. Interactivity and fast iteration over reconstructions and intermediate results can help speed up workflows for designers and practitioners. Different sensors are available for learning clothed avatars, including monocular RGB cameras, depth sensors and 4D scanners. RGB videos are the most widely available, yet provide the weakest supervisory signal, making this configuration elusive. Figure 1: InstantGeoAvatar. We introduce a system capable of reconstructing the geometry and appearance of animatable human avatars from monocular video in less than 10 minutes. In order to attain high quality geometry reconstructions, we propose a smoothing term directly on the learned signed distance field during optimization, requiring no extra computation or sampling and delivering noticeable qualitative improvements. Traditional depth-based methods fusing depth measurements over time [73, 74, 50, 8] produce compelling reconstructions but need complex and expensive setups to avoid sensor noise. Dense multi-view capture systems [1, 20, 34, 49, 64, 94, 96, 97, 41, 91, 30, 105, 9, 79] offer detailed 3D reconstructions by leveraging multi-view images and cues like silhouette or stereo. However, they can only be applied in controlled camera studios. Moreover, neither depth-based nor dense multi-view approaches can effectively work from RGB inputs alone or produce satisfactory results within short fitting times. Mesh-based approaches [2, 3, 6, 7, 10, 44, 12, 28, 71, 53, 4] struggle with garment deformations and are restricted to low-resolution topologies. Point cloud-based techniques [66, 68, 116, 60, 98, 59, 120, 80, 121, 117, 61] have shown promising outcomes, but are not yet capable of fast-training times with RGB supervision only. The advent of neural radiance fields (NeRFs) [70] enabled techniques for novel view synthesis and animation of human avatars from RGB image supervision only [26, 38, 109, 93]. Volume rendering-based approaches typically learn a canonical representation that is deformed with linear blend skinning [65] and an additional non-rigid deformation [77, 104, 115]. Despite producing good renderings of human avatars, these techniques lack awareness of the underlying geometry. In parallel, some works have adopted a signed distance function (SDF) [111] as basic primitive for learning clothed human avatars from 3D scans [85, 5, 107]. To remove the need for 3D supervision, some works have embedded SDFs within a volumetric rendering pipeline [29, 103, 77]. Significant steps have been taken to speed up training of NeRF-based approaches [26, 38] by leveraging an efficient hash-grid spatial encoding [72]. Subsequent work has tried to improve training on such unstable and noisy hash grids [56, 23] with some success. Up to date, however, fast geometry learning in general and effective use of hash grid-based representations in particular for clothed human avatars with RGB supervision only remains elusive. The challenges faced by NeRF- and SDF-based approaches trained with volume rendering can be succinctly reduced to effectively capturing realistic non-rigid deformations, dealing with noisy pose and camera estimates, slow training, and unstable training in the case of hash grid-based methods. In this paper, we specifically focus on the last two challenges, and aim at significantly advancing towards the realization of interactive use of human avatar modelling. We propose InstantGeoAvatar, a system capable of yielding good rendering and reconstruction quality in as little as 5 minutes of training, down from several hours as in prior work. Building on recent advances for fast training of NeRF based systems [72, 38] and efficient training of hash grid encodings [56, 23], we demonstrate that even in combination such prior improvements and techniques are insufficient for fast and effective learning of 3D clothed humans. Thus we propose a simple yet effective regularization scheme that imposes a local geometric consistency prior during optimization, effectively removing undesired artifacts and defects on the surface. The proposed approach, which effectively constrains surface curvature and torsion over continuous SDFs along ray directions, is easy to implement, fits neatly within the volume rendering pipeline, and delivers noticeable improvements over our base model without additional cost. Our experiments demonstrate the effectiveness of the proposed method for effective and fast learning of animatable 3D human avatars from monocular video. At the short-training regime, InstantGeoAvatar yields superior geometry reconstruction and rendering quality compared to previous work in less than 10 minutes (see Fig. 1). While SoTA methods can yield more accurate reconstructions after several hours upon convergence, InstantGeoAvatar still shows comparable and even superior results with out of distribution (OOD) poses. In addition, the presented ablation demonstrates that previous work on improving training of hash grid-based representations is insufficient for obtaining satisfying geometry reconstructions, highlighting the suitability of our proposal."
https://arxiv.org/html/2411.01499v1,Polar R-CNN: End-to-End Lane Detection with Fewer Anchors,"Lane detection is a critical and challenging task in autonomous driving, particularly in real-world scenarios where traffic lanes can be slender, lengthy, and often obscured by other vehicles, complicating detection efforts. Existing anchor-based methods typically rely on prior lane anchors to extract features and subsequently refine the location and shape of lanes. While these methods achieve high performance, manually setting prior anchors is cumbersome, and ensuring sufficient coverage across diverse datasets often requires a large amount of dense anchors. Furthermore, the use of Non-Maximum Suppression (NMS) to eliminate redundant predictions complicates real-world deployment and may underperform in complex scenarios. In this paper, we propose Polar R-CNN, an end-to-end anchor-based method for lane detection. By incorporating both local and global polar coordinate systems, Polar R-CNN facilitates flexible anchor proposals and significantly reduces the number of anchors required without compromising performance. Additionally, we introduce a triplet head with heuristic structure that supports NMS-free paradigm, enhancing deployment efficiency and performance in scenarios with dense lanes. Our method achieves competitive results on five popular lane detection benchmarks—Tusimple, CULane, LLAMAS, CurveLanes, and DL-Rail—while maintaining a lightweight design and straightforward structure. Our source code is available at https://github.com/ShqWW/PolarRCNN.","Lane detection is a critical task in computer vision and autonomous driving, aimed at identifying and tracking lane markings on the road [1]. While extensive research has been conducted in ideal environments, it is still challenging in adverse scenarios such as night driving, glare, crowd, and rainy conditions, where lanes may be occluded or damaged [2]. Moreover, the slender shapes and complex topologies of lanes further complicate detection efforts [3]. In the past few decades, a lot of methods primarily focus on handcrafted local feature extraction and lane shape modeling. Techniques such as the Canny edge detector[4], Hough transform[5], and deformable templates[6] have been widely employed for lane fitting. However, these approaches often face limitations in real-world scenarios, especially when low-level and local features lack clarity and distinctiveness. In recent years, advancements in deep learning and the availability of large datasets have led to significant progress in lane detection, especially deep models such as Convolutional Neural Networks (CNNs)[2] and transformer-based architectures [7]. Based on this, earlier approaches typically framed lane detection as a segmentation task [8], which, despite its straightforward, required time-consuming computations. There are still some methods that rely on parameter-based models, which directly output lane curve parameters rather than pixel locations [3][7][9]. Although these segmentation-based and parameter-based methods provide end-to-end solutions, their sensitivity to lane shape compromises their robustness. (a) (b) (c) (d) Figure 1: Anchor (i.e., the yellow lines) settings of different methods and the ground truth lanes. (a) The initial anchor settings of CLRNet. (b) The learned anchor settings of CLRNet trained on CULane. (c) The flexible proposal anchors of our method. (d) The ground truth. (a) (b) (c) (d) Figure 2: Comparison of NMS thresholds in sparse and dense scenarios. (a) and (b) Ground truths in the dense and sparse scenarios, respectively. (c) Predictions with large NMS thresholds in a dense scenario, resulting in a lane prediction being mistakenly suppressed. (d) Predictions with small NMS thresholds in a sparse scenario, where redundant prediction results are not effectively removed. Drawing inspiration from object detection methods such as YOLO [10] and Faster R-CNN [11], several anchor-based approaches have been introduced for lane detection, with representative works including LaneATT [12] and CLRNet [13]. These methods have shown superior performance by leveraging anchor priors (as shown in Fig. 1) and enabling larger receptive fields for feature extraction. However, anchor-based methods encounter similar drawbacks to those in general object detection, including the following: • As shown in Fig. 1(a), a large amount of lane anchors are predefined in the image, even in sparse scenarios—the situations where lanes are distributed widely and located far apart from each other, as illustrated in the Fig. 1(d). • Non-Maximum Suppression (NMS) [14] post-processing is required to eliminate redundant predictions but may struggle in dense scenarios where lanes are close to each other, such as forked lanes and double lanes, as illustrated in the Fig. 2(a). Regrading the first issue, [13] introduced learned anchors that optimize the anchor parameters during training to better adapt to lane distributions, as shown in Fig. 1(b). However, the number of anchors remains excessive to adequately cover the diverse potential distributions of lanes. Furthermore, [15] proposes flexible anchors for each image by generating start points with directions, rather than using a fixed set of anchors. Nevertheless, these start points of lanes are subjective and lack clear visual evidence due to the global nature of lanes. In contrast, [16] uses a local angle map to propose sketch anchors according to the direction of ground truth. While this approach considers directional alignment, it neglects precise anchor positioning, resulting in suboptimal performance. Overall, the abundance of anchors is unnecessary in sparse scenarios. Regarding the second issue, nearly all anchor-based methods [12][13][15][16] rely on direct or indirect NMS post-processing to eliminate redundant predictions. Although it is necessary to eliminate redundant predictions, NMS remains a suboptimal solution. On one hand, NMS is not deployment-friendly because it requires defining and calculating distances between lane pairs using metrics such as Intersection over Union (IoU). This task is more challenging than in general object detection due to the intricate geometry of lanes. On the other hand, NMS can struggle in dense scenarios. Typically, a large distance threshold may lead to false negatives, as some true positive predictions could be mistakenly eliminated, as illustrated in Fig. 2(a)(c). Conversely, a small distance threshold may fail to eliminate redundant predictions effectively, resulting in false positives, as shown in Fig. 2(b)(d). Therefore, achieving an optimal trade-off across all scenarios by manually setting the distance threshold is challenging. To address the above two issues, we propose Polar R-CNN, a novel anchor-based method for lane detection. For the first issue, we introduce Local Polar Module based on the polar coordinate system to create anchors with more accurate locations, thereby reducing the number of proposed anchors in sparse scenarios, as illustrated in Fig. 1(c). In contrast to State-Of-The-Art (SOTA) methods [13][17], which utilize 192 anchors, Polar R-CNN employs only 20 anchors to effectively cover potential lane ground truths. For the second issue, we have incorporated a triplet head with a new heuristic Graph Neural Network (GNN) [18] block. The GNN block offers an interpretable structure, achieving nearly equivalent performance in sparse scenarios and superior performance in dense scenarios. We conducted experiments on five major benchmarks: TuSimple [19], CULane [2], LLAMAS [20], CurveLanes [21], and DL-Rail [22]. Our proposed method demonstrates competitive performance compared to SOTA approaches. Our main contributions are summarized as follows: • We design a strategy to simplify the anchor parameters by using local and global polar coordinate systems and applied these to the two-stage lane detection framework. Compared to other anchor-based methods, this strategy significantly reduces the number of proposed anchors while achieving better performance. • We propose a novel triplet detection head with a GNN block to implement a NMS-free paradigm. The block is inspired by Fast NMS, providing enhanced interpretability. Our model supports end-to-end training and testing while still allowing for traditional NMS post-processing as an option for a NMS version of our model. • By integrating the polar coordinate systems and NMS-free paradigm, we present a Polar R-CNN model for fast and efficient lane detection. And we conduct extensive experiments on five benchmark datasets to demonstrate the effectiveness of our model in high performance with fewer anchors and a NMS-free paradigm. Figure 3: An illustration of the Polar R-CNN architecture. It has a similar pipeline with the Faster R-CNN for the task of object detection, and consists of a backbone, a Feature Pyramid Network with three levels of feature maps, respectively denote by 𝑷1subscript𝑷1\boldsymbol{P}_{1}bold_italic_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, 𝑷2subscript𝑷2\boldsymbol{P}_{2}bold_italic_P start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT and 𝑷3subscript𝑷3\boldsymbol{P}_{3}bold_italic_P start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT, followed by a Local Polar Module, and a Global Polar Module for lane detection. Based on the designed lane representation and lane anchor representation in polar coordinate system, the local polar module can propose sparse line anchors and the global polar module can produce the final accurate lane predictions. The global polar module includes a triplet head, which comprises the one-to-one (O2O) classification subhead, the one-to-many (O2M) classification subhead, and the one-to-many (O2M) regression subhead."
https://arxiv.org/html/2411.01494v1,Finding NeMo: Negative-mined Mosaic Augmentation for Referring Image Segmentation,"Referring Image Segmentation is a comprehensive task to segment an object referred by a textual query from an image. In nature, the level of difficulty in this task is affected by the existence of similar objects and the complexity of the referring expression. Recent RIS models still show a significant performance gap between easy and hard scenarios. We pose that the bottleneck exists in the data, and propose a simple but powerful data augmentation method, Negative-mined Mosaic Augmentation (NeMo). This method augments a training image into a mosaic with three other negative images carefully curated by a pretrained multimodal alignment model, e.g., CLIP, to make the sample more challenging. We discover that it is critical to properly adjust the difficulty level, neither too ambiguous nor too trivial. The augmented training data encourages the RIS model to recognize subtle differences and relationships between similar visual entities and to concretely understand the whole expression to locate the right target better. Our approach shows consistent improvements on various datasets and models, verified by extensive experiments.","Figure 1: Diverse visual and linguistic challenges of referring scenarios. In (a), query (1) demands discernment among three road signs, while query (2) involves identifying a “woman”, relatively easier due to a single instance. NeMo, our method, in (b) uses similar negative images to generate a mosaic. Query (2) becomes harder as the augmented image contains additional instances of “woman” (e.g., women standing or sitting), and thus “in front of the wall” becomes crucial hint to solve the problem. Referring Image Segmentation (RIS) is a fundamental task in computer vision that aims to segment objects described in a natural language expression within a given scene. Central to RIS is not merely the visual recognition of objects but also the intricate understanding of the interrelationships among these objects, interpreted through linguistic cues. Each RIS problem often requires a different level of multimodal understanding capabilities, depending on visual ambiguity as well as linguistic complexity. For instance, having visually similar objects to the referent in an image complicates locating and identifying the correct object. In such a case, precise comprehension of the referring expression becomes a key to find the right target. Table 1: Statistics of representative Referring Image Segmentation (RIS) Datasets Dataset RefCOCO RefCOCO+ G-Ref # Images 19,994 19,992 26,711 # Ref. Exp. 142,209 141,564 85,474 Query length 3.61 3.53 8.43 Obj. per query 1.76 1.67 3.03 Table 2: mIoU & oIoU on 100 easy and hard samples from G-Ref UMD test set mIoU oIoU Models Easy Hard Easy Hard LAVT [50] 78.26 54.61 79.16 47.40 CRIS [45] 76.89 52.97 78.81 43.20 CGFormer [43] 79.86 61.22 79.95 53.27 Figure 2: Data samples from RIS benchmarks and augmented samples using our NeMo. RefCOCO and RefCOCO+ are characterized by relatively easier scenarios with simple referring expressions, whereas G-Refs encompass more challenging sets. Fig. 1(a) illustrates varying degrees of difficulties even within the same image. The sentence (1) is relatively harder, since three road signs exist and the model needs to find the target by precisely understanding the entire phrase. The expression (2), on the other hand, is relatively easier, since there is only one woman, so one can easily find her regardless of the rest of the phrase. Many existing RIS datasets, however, have not been created considering such challenge levels; rather, many examples can be solved by simply finding an object corresponding to the referred class. RefCOCO and RefCOCO+ [54], for example, contain easier scenarios with less visual ambiguity and linguistic complexity. On the contrary, G-Ref [36] is considered harder. As in Tab. 1 and Fig. 2, it contains more objects in each image on average, and queries are relatively longer. The level of difficulty significantly varies even within the same dataset. To illustrate, we manually select 100 easy and hard samples from the G-Ref UMD test set. We select ‘easy’ samples containing only a single object per its category, straightforward to identify the target without ambiguity. For instance, in the third image in Fig. 2, there is only a single woman holding a glass, so just finding ‘a woman’ will suffice. Conversely, ‘hard’ examples contain multiple objects within the same category, necessitating a detailed perception to distinguish the intended target. Specific easy and hard examples are provided in Appendix 0.A. Tab. 2 shows a significant performance gap between the easy and hard samples by recent RIS models. This indicates that they are capable of picking the right object without ambiguity, but they tend to lack in understanding delicate meaning in the referring expressions and using them to distinguish multiple objects in the same (or similar) categories. To further improve the RIS performance, this observation reveals that we may need to revisit if the models have been provided with sufficiently difficult training data to learn from. Given this problem landscape, we aim to improve the performance by tackling the data part of the training. Specifically, we postulate that amplifying the exposure of the models to challenging examples at training could fortify their capability to understand the subtle dynamics between visual and linguistic components. Such complexity often arises when multiple objects, potentially of the same class, coexist within an image, encouraging the model to fully understand both the scene and the given referring expression. However, manually labeling such ‘hard’ data examples is prohibitively expensive. Recognizing the key factors behind the difficulty and quality of RIS training data, we introduce a simple but universally applicable data augmentation method, Negative-mined Mosaic Augmentation (NeMo). Inspired by the mosaic augmentation in YOLO v4 [3], NeMo augments each training image by combining it with three other images in a 2×2222\times 22 × 2 formation, showing four times more objects on average. However, NeMo differs from the previous method in that the extra three images are not chosen at random, but are carefully selected to create a properly challenging training example. Specifically, we propose considering relevance between the referring expression and candidate images, measured by a cross-modal retrieval model, e.g., CLIP. To build a mosaic, our method selects negative images containing objects from the same or similar category to the referred object, retrieved based on the relevance to the referring expression. Augmented mosaic images mimic challenging referring examples as in Fig. 1(b), encouraging the model to learn subtle visual differences and to concretely understand the given referring expression to better locate the target. One might concern if combining similar images may create false positives, where the correct object in an image becomes invalid due to the objects in the other quadrants. We study the possibility and impact of such false positives, and discover that it is indeed critical for the mosaic to have the right level of difficulty to be maximally effective. Based on our observations, we present a strategic retrieval process to make the mosaic neither too hard nor too easy. From extensive experiments with five state-of-the-art RIS models, we verify that NeMo consistently improves performance across all models on multiple datasets. Furthermore, we exhibit that NeMo encourages a model to make better connections between words and visual components, grasping fine details in the scene and the referring expression. We expect our study to support the primary aim of the RIS task, distinguishing multiple candidate objects in the scene and recognizing the target based on the textual description. Our main contributions are summarized as follows: 1. We introduce NeMo, a simple but powerful labor-free data augmentation method for Referring Image Segmentation (RIS), effective across various datasets and models. 2. We discover that it is critical to adjust the level of difficulty to successfully apply a mosaic augmentation, and propose a systematic way to tune this difficulty by generating training examples at a properly controlled difficulty. 3. We empirically verify that NeMo enhances both visual and textual understanding capabilities for segmenting the right target."
https://arxiv.org/html/2411.01492v1,EEE-Bench: A Comprehensive Multimodal Electrical And Electronics Engineering Benchmark,"Recent studies on large language models (LLMs) and large multimodal models (LMMs) have demonstrated promising skills in various domains including science and mathematics. However, their capability in more challenging and real-world related scenarios like engineering has not been systematically studied. To bridge this gap, we propose EEE-Bench, a multimodal benchmark aimed at assessing LMMs’ capabilities in solving practical engineering tasks, using electrical and electronics engineering (EEE) as the testbed. Our benchmark consists of 2860 hand-picked and carefully curated multiple-choice and free-form problems spanning 10 essential subdomains such as analog circuits, control systems, etc. Compared to benchmarks in other domains, engineering problems are intrinsically 1) more visually complex and versatile and 2) less deterministic in solutions. Successful solutions to these problems often demand more-than-usual rigorous integration of visual and textual information as models need to understand intricate images like abstract circuits and system diagrams while taking professional instructions, making them excellent candidates for LMM evaluations. Alongside EEE-Bench, we provide extensive quantitative evaluations and fine-grained analysis of 17 widely-used open and closed-sourced LLMs and LMMs. Our results demonstrate notable deficiencies of current foundation models in EEE, with an average performance ranging from 19.48% to 46.78%. Finally, we reveal and explore a critical shortcoming in LMMs which we term “laziness”: the tendency to take shortcuts by relying on the text while overlooking the visual context when reasoning for technical image problems. In summary, we believe EEE-Bench not only reveals some noteworthy limitations of LMMs but also provides a valuable resource for advancing research on their application in practical engineering tasks, driving future improvements in their capability to handle complex, real-world scenarios.","Electrical and electronics engineering (EEE) embodies a profound synthesis of theoretical knowledge and practical application, serving as the backbone of modern digital and power technological advancements (Bose, 2000; 2020; Irwin & Nelms, 2020). From designing intricate circuitry to advancing power systems, this discipline demands rigorous logical reasoning and a deep understanding of physical laws (Brophy & Voigt, 2014; Saadat et al., 1999). Addressing problems in EEE requires that researchers and engineers not only grasp the related theoretical knowledge but also comprehend the associated visual contexts, such as circuit diagrams and signal waveforms. Leveraging machine learning models equipped with robust knowledge and reasoning capabilities, alongside accurate perception of visual contexts, can significantly enhance various aspects of our lives. These models can aid in intricate hardware design processes (Chang et al., 2023; Liu et al., 2023c; Wu et al., 2024), optimize operations within the electric energy sector (Majumder et al., 2024), drive innovation by proposing novel approaches to traditional scientific challenges (Taylor et al., 2022), and assist in solving complex educational problems. Recent Large Language Models (LLMs) (Brown, 2020; Jiang et al., 2024; Achiam et al., 2023; Chiang et al., 2023; Touvron et al., 2023) and Large Multimodal Models (LMMs) (Team et al., 2023; Liu et al., 2024b; Zhang et al., 2023; Dai et al., 2023) have achieved much progress and demonstrated remarkable capabilities in various domains, e.g., mathematical reasoning (Yue et al., 2023; Yu et al., 2023) and general science (Lu et al., 2022a). To evaluate the abilities of these foundation models, various specialized or versatile benchmarks (Lu et al., 2023; Zhang et al., 2024c; Yue et al., 2024a) have been developed. However, the reasoning ability of these foundation models on engineering problems with visual contexts has not been systematically examined. Therefore, to better harness the potential of foundation models, it is crucial to develop a new benchmark specific to engineering problems, and we select EEE as our case study for reasons mentioned before. This will facilitate the evaluation of LLMs and LMMs in their ability to tackle rigorous engineering reasoning tasks, thereby advancing research progress in this field. Figure 1: Comparison between EEE-Bench and MathVista problems: EEE problems feature significantly more complex and intricate visual information compared to mathematical problems, requiring specific technical knowledge to interpret diagrams effectively. Compared with other subjects such as mathematics, the visual contexts in EEE are often more abstract and heavily logic-oriented. Solving EEE problems demands specialized technical knowledge, as illustrated in Figure 1. Specifically, unlike mathematical problems (Lu et al., 2023; Zhang et al., 2024b; Wang et al., 2024b) which might involve clear numerical data and operations, visual representations in EEE, such as circuit diagrams, are intricate and logical, often containing minimal textual information (more example images can be found in Appendix F). Hence effective problem-solving in this domain requires a deep understanding of the logical relationships between different components of the diagram and the application of specific technical knowledge to interpret these visuals. In view of the above, the proficiency of existing LMMs in mathematical domains does not guarantee their effectiveness in EEE tasks. Owing to the same cause, existing benchmarks in other domains (Lu et al., 2022b; 2023; Zhang et al., 2024b; Wang et al., 2024b; Yue et al., 2024a; Yun et al., 2024; Doris et al., 2024) also lack some necessary rigor to accurately faithfully inspect LMMs’ problem-solving capability in challenging real-world scenarios where the visual context is crucial but involves serious complexity. Consequently, there is a pressing need for a specialized benchmark focused on EEE to thoroughly assess the multimodal reasoning abilities of LMMs within this specific field as well as to provide a broader outlook of their problem-solving capabilities in these practical engineering problems in the real world. To this end, we present EEE-Bench, a pioneering multimodal electrical and electronics engineering (EEE) reasoning benchmark in visual contexts. To ensure that EEE-Bench encompasses a thorough range of knowledge in EEE, we initially identified ten pivotal topics for problem collection: (1) Digital Logic Circuits and Microprocessor Design, (2) Circuit Theory and Network Analysis, (3) Analog Circuits, (4) Power Electronics and Power Systems, (5) Signals and Systems, (6) Communication Systems, (7) Control Systems, (8) Electronic Devices, (9) Electrical Machines, and (10) Electromagnetics. These topics comprehensively cover both theoretical foundations and practical applications within EEE. When collecting problems for EEE-Bench, we include a broad range of visual contexts such as electric and digital circuits, system diagrams, abstract scenes, electronic component images, and assorted tables, charts, and plots. It’s important to note that many figures within EEE-Bench feature combinations of these visual contexts, enhancing their complexity and educational value. Overall, EEE-Bench comprises 2,860 entirely new problems. We conduct extensive experiments using EEE-Bench to assess the reasoning abilities of 17 leading foundation models, including both open-source (Chen et al., 2024a; Liu et al., 2024a; b) and closed-source (OpenAI, 2024; Team et al., 2023). Figure 2 illustrates the overall experimental results. We discover that most existing LMMs struggle to understand EEE diagrams and perform poorly on EEE-Bench. Our results show that GPT-4o (OpenAI, 2024) achieves the best overall performance across different topics with 46.78% accuracy. Notably, closed-source models generally outperform open-source models. When evaluating performance across various subjects, we observed that many closed-source models excel in subjects with straightforward visual contexts but require significant computational resources, such as Electronic Devices. However, these models tend to struggle with subjects that involve complex visual diagrams, such as Circuit Theory and Network Analysis. These findings underscore the proficiency of closed-source LMMs in numerical computation, but also reveal their limitations in processing intricate visual information. Through the fine-grained error analysis, we found that current LMMs struggle to understand EEE diagrams and reason effectively. Taking GPT-4o as an example, we found that over 50% of the errors stem from reasoning issues, and 26.5% are due to errors in image perception. These findings suggest that better EEE visual understanding capabilities and more robust EEE reasoning for LMMs could be the potential for future advancement in addressing multi-modal EEE problems. Lastly, we analyze which modality—vision or text—LMMs predominantly rely on. Our investigation revealed a ’laziness’ phenomenon within existing LMMs. In contrast to previous studies that concentrated on hallucinations of nonexistent objects with natural images (Liu et al., 2024c; Qian et al., 2024; Liu et al., 2023a), our focus is on the robustness of the understanding and reasoning process in LMMs dealing with technical images. Specifically, we found that when presented with spurious captions that contradict the accompanying images, LMMs tend to disregard essential visual information, relying instead on the textual content and reasoning, even if it is misleading. This tendency leads to flawed reasoning processes within the models. Consequently, augmenting original question text with such spurious captions results in significant accuracy declines—7.79% for GPT-4o and 6.78% for Gemini-Pro 1.5 on the EEE-Bench. These results reveal that existing LMMs mainly rely on text information, and intriguingly, they may overlook visual information when it is explicitly provided in text. The contributions of this paper can be summarized as follows: • We introduce EEE-Bench, a novel multimodal benchmark designed for assessing the reasoning abilities of LMMs in electrical and electronics engineering (EEE) problems. EEE-Bench consists of 2860 samples spanning 10 essential subjects in EEE, featuring a diverse range of visual contexts such as electric and digital circuits, system diagrams, and others. This benchmark is specifically crafted to evaluate how well LMMs can handle complex visual and logical challenges within the EEE field. • We conduct extensive experiments using EEE-Bench to assess the reasoning abilities of 17 leading open-source and closed-source foundation models. We found that most existing LMMs struggle to deal with EEE problems, especially in subjects with complex visual contexts, yielding an average performance ranging from 19.48% to 46.78%. We also demonstrate that reasoning errors and image perception errors are the main problems for current LMMs. These findings provide insights for future improvement. • We further explore which modality—text or vision—current LMMs primarily rely on. Our research identifies a ’laziness’ phenomenon in these models: LMMs will not try to read images if relevant visual information for reasoning and solving problems appears in the text, even if this extra-textual information is spurious. Our experiments show that introducing additional misleading captions into the text leads to a 7.79% drop in accuracy for GPT-4o on the EEE-Bench. Figure 2: Overview of performance of left: open source models and right: close source models."
https://arxiv.org/html/2411.01472v1,Adaptive Domain Learning for Cross-domain Image Denoising,"Different camera sensors have different noise patterns, and thus an image denoising model trained on one sensor often does not generalize well to a different sensor. One plausible solution is to collect a large dataset for each sensor for training or fine-tuning, which is inevitably time-consuming. To address this cross-domain challenge, we present a novel adaptive domain learning (ADL) scheme for cross-domain RAW image denoising by utilizing existing data from different sensors (source domain) plus a small amount of data from the new sensor (target domain). The ADL training scheme automatically removes the data in the source domain that are harmful to fine-tuning a model for the target domain (some data are harmful as adding them during training lowers the performance due to domain gaps). Also, we introduce a modulation module to adopt sensor-specific information (sensor type and ISO) to understand input data for image denoising. We conduct extensive experiments on public datasets with various smartphone and DSLR cameras, which show our proposed model outperforms prior work on cross-domain image denoising, given a small amount of image data from the target domain sensor.","Noise generated by electronic sensors in a RAW image is inevitable. Over the past few years, learning-based methods have made significant progress in RAW image denoising chen2018learning ; lehtinen2018noise2noise ; maleky2022noise2noiseflow ; wei2020physics . However, building a large-scale real-world dataset with noise-clean pairs for training a denoising model is time-consuming and labor-intensive. It is hard to collect ground truth that is noise-free and has no misalignment with the input noisy data. Moreover, due to the different noise distributions of different sensors (such as read noise and shot noise), the collected data from a particular sensor usually cannot be used to train the denoising model of other sensors, which causes a waste of resources. Therefore, it is important to develop a method to solve this problem. Existing solutions to data scarcity in RAW image denoising can be divided into two categories, noise calibration wei2020physics ; zhang2021rethinking ; monakhova2022dancing and self-supervise denoising lehtinen2018noise2noise ; krull2019noise2void ; maleky2022noise2noiseflow ; Wang_2022_CVPR ; laine2019high . Noise synthesis and calibration methods first build a noise model, optimize for noise parameters according to a particular camera, and then synthesize training pairs from the noise model to train a network. Self-supervised denoising is designed based on the blindspots schemes. When the input noisy image masks out some pixels and forms a similar but different image from the input, the network learns to denoise instead of identity mapping. Therefore, the network can learn to denoise without pairwise noise-clean data. ††footnotetext: † Corresponding author. While the noise synthesis and calibration methods are top-performing ones for RAW data denoising and self-supervised denoising does not need to collect pairwise data, both of them have their practical limitations. First, noise synthesis and calibration methods are not able to obtain the exact noise model of the real noise. For example, fixed pattern noise such as dark signal non-uniformity (DSNU) and Photo-response non-uniformity (PRNU) are not included in the model. As a result, some of the sampled noise training pairs might be harmful to the training of the denoising model (i.e., decrease in performance). Second, building a calibration model still needs to collect data under particular circumstances. Third, these models can only be used to synthesize training data for specific sensors, which leads to a waste of resources. On the other hand, self-supervised denoising is designed under some unverified assumptions of noise distribution. First, the noise distribution has zero means. Second, the noise in different pixels is independent of each other. These assumptions do not match the noise in the real world, especially when the noise distribution is complicated. Therefore, self-supervised denoising does not achieve state-of-the-art denoising performance. Different from prior work, we solve this problem by proposing a cross-domain RAW image denoising method, adaptive domain learning (ADL). Our method can utilize existing RAW image denoising datasets from various sensors (source domains) combined with very little data from a new sensor (target domain) together to train a denoising model for that new sensor. Some data in a source domain may be harmful to fine-tuning a model due to the large domain gap: for instance, synthetic data may be harmful to training a model for real-world applications if the synthetic data imposes unrealistic and unreasonable assumptions. In such cases, our method dynamically evaluates whether a data sample from a source domain is beneficial or harmful by evaluating the performance on a small validation set of the target domain, before and after fine-tuning the model on this data sample. If the performance improves after fine-tuning, we can use this data sample for training; otherwise, we should ignore it. As for the network architecture, we design a modulation network that takes sensor-dependent information as input (sensor types and ISO), which aligns the features from different sensors into the same space and ensembles useful common knowledge for denoising. To evaluate our proposed model with ADL, we compare our model against prior methods on diverse real-world public datasets abdelhamed2018high ; wei2020physics ; chen2018learning captured by both smartphone and DLSR cameras. The results demonstrate that our method outperforms the prior work and shows consistent state-of-the-art performance with ADL on RAW data denoising, given a small amount of data in the target domain. We also demonstrate that our ADL can be applied to fine-tuning existing noise calibration models with cross-domain data to further improve its performance. The contributions of this work can be summarized as follows. • We propose a novel adaptive domain learning (ADL) strategy that can train a model with little data from a new sensor (target domain), by automatically leveraging useful information and removing useless data from existing RAW denoising data from other sensors (source domains). • A customized modulation strategy is applied to provide sensor-specific information, which helps our network adapt to different sensors and noise distributions. • Our model outperforms prior methods in cross-domain image denoising in the target domain with little data. Figure 1: The overall pipeline of our adaptive domain learning (ADL) algorithm. The network parameter θ0subscript𝜃0\theta_{0}italic_θ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT is first initialized, then the small target domain training set will be used to train a model with parameter θ𝜃\thetaitalic_θ. In the source domain adaptive learning stage, in iteration t𝑡titalic_t, data from the source domain will be used to update the network parameter from θt−1subscript𝜃𝑡1\theta_{t-1}italic_θ start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT to θ′superscript𝜃′\theta^{\prime}italic_θ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT. Then a dynamic validation set will judge whether the data is useful. If so, set θt=θ′subscript𝜃𝑡superscript𝜃′\theta_{t}=\theta^{\prime}italic_θ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_θ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT and repeat the process. If not, retrieve the network parameter from θ′superscript𝜃′\theta^{\prime}italic_θ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT to θt−1subscript𝜃𝑡1\theta_{t-1}italic_θ start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT. Finally, the target domain data will be used to fine-tune θ′superscript𝜃′\theta^{\prime}italic_θ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT to θTsubscript𝜃𝑇\theta_{T}italic_θ start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ."
https://arxiv.org/html/2411.01465v1,Efficient Non-Exemplar Class-Incremental Learning with Retrospective Feature Synthesis,"Despite the outstanding performance in many individual tasks, deep neural networks suffer from catastrophic forgetting when learning from continuous data streams in real-world scenarios. Current Non-Exemplar Class-Incremental Learning (NECIL) methods mitigate forgetting by storing a single prototype per class, which serves to inject previous information when sequentially learning new classes. However, these stored prototypes or their augmented variants often fail to simultaneously capture spatial distribution diversity and precision needed for representing old classes. Moreover, as the model acquires new knowledge, these prototypes gradually become outdated, making them less effective. To overcome these limitations, we propose a more efficient NECIL method that replaces prototypes with synthesized retrospective features for old classes. Specifically, we model each old class’s feature space using a multivariate Gaussian distribution and generate deep representations by sampling from high-likelihood regions. Additionally, we introduce a similarity-based feature compensation mechanism that integrates generated old class features with similar new class features to synthesize robust retrospective representations. These retrospective features are then incorporated into our incremental learning framework to preserve the decision boundaries of previous classes while learning new ones. Extensive experiments on CIFAR-100, TinyImageNet, and ImageNet-Subset demonstrate that our method significantly improves the efficiency of non-exemplar class-incremental learning and achieves state-of-the-art performance.","Automatic visual recognition based on deep neural networks is a fundamental task in computer vision that has undergone substantial advancements in recent years [1, 2, 3]. However, most existing methods assume a static scenario, where the entire training process occurs in a single session with all training data fully available. In contrast, real-world scenarios are dynamic, with new data continuously emerging, requiring continuous updates to the model. Therefore, it is essential to develop efficient visual recognition algorithms that can learn continuously, much like the human ability to acquire new concepts without forgetting previously learned knowledge. ((a)) Baseline ((b)) Gaussian Noise Aug ((c)) Prototype Mixing ((d)) Our Method Figure 1: Idea illustration. (a) Baseline: Use the stored prototypes as the classification features of the old classes to rebuild the classification boundaries. (b) Gaussian Noise Aug [4]: Use random Gaussian noise to enhance the prototypes, but it introduces inaccurate features and overlaps between class distributions due to noise interference to the representation space. (c) Prototype Mixing [5]: Combine the prototypes of the old classes randomly linearly to form the hybrid prototypes, but these obtained prototypes lack authenticity. (d) Our Method: Model the representation space of each old class using a multivariate Gaussian distribution, then generate classification features for old classes by sampling from the high-likelihood regions, thereby better capturing the diversity and accuracy of the generated representations. Class incremental learning [6, 7, 8] is a real-world scenario that considers adding new classes to an already trained classification model. This approach aims to mitigate the catastrophic forgetting that neural networks underfit previously learned classes while absorbing new class data [9]. Catastrophic forgetting typically arises due to the overlap or confusion between old and new class representations within the feature space [4]. As a large number of new class instances are introduced, the unified classifier is biased to these new classes, causing the decision boundaries of previous classes to shift significantly[4]. Some methods [10, 11, 12, 13, 14] mitigate this issue by replaying a limited set of exemplars from old classes while learning new classes. However, past data is often unavailable due to privacy concerns or restrictions on the long-term storage of raw samples in real-world applications. Therefore, in this paper, we focus on Non-Exemplar Class-Incremental Learning (NECIL), a more challenging scenario where no raw data from previous tasks is retained. In NECIL research, one of the primary challenges is to reconstruct the decision boundaries of previous classes without accessing their original samples. Some methods [15, 16, 17, 18] leverage deep generative models to generate pseudo-samples of previous classes. However, these methods frequently encounter issues like model collapse and unreliable sample generation. To address these limitations, prototype-based methods have gained popularity for their ability to balance privacy security with robust representational capability [4, 19, 20, 5, 9]. As shown in Figure 1(a), PASS [4] first proposes storing a representative prototype for each old class, typically the class mean in the deep feature space. When learning new classes, these stored prototypes are used as old class representations to inject previous information. Furthermore, as illustrated in Figure 1(b), PASS [4] augments these prototypes by adding Gaussian noise to increase the diversity of the injected representations. However, this method struggles with maintaining representation quality and ensuring clear distinctions between classes, as the added noise can interfere with the feature space. RRFE [5], depicted in Figure 1(c), randomly combines old class prototypes linearly to create hybrid prototypes with mixed labels. While innovative, these hybrid prototypes lack authenticity, making it difficult to accurately reconstruct the original class boundaries. Other methods, such as SSRE [19] and FeTrIL [9], either directly use these augmented prototype variants or introduce enhancements based on them. Overall, these methods often overlook the distribution’s shape and scope in the feature space of old classes, resulting in imprecise or limited diversity in class representations. To overcome this challenge, we propose a Multivariate Gaussian Sampling (MGS) strategy, which offers a more robust generation of old class representations, replacing traditional prototypes. As depicted in Figure 1(d), our approach models the feature space of each old class using a multivariate Gaussian distribution. By sampling from the high-likelihood regions of these distributions, we generate diverse and high-quality representations of old classes. This strategy is inspired by recent work in out-of-distribution detection [21]. Unlike prototype-based methods, which focus primarily on the class mean, our approach accounts for both the shape and scope of the entire feature distribution space. As a result, the generated old class representations exhibit both good diversity and higher accuracy and can more effectively reconstruct the decision boundaries of previous classes. In addition to generating old class representations, another often overlooked challenge in NECIL is the increasing deviation between the classifier and the generated old class representations. Specifically, as the model continuously learns new class data, both the feature extractor and the classifier must be updated to maintain the model’s plasticity. This ongoing process gradually leads to the outdated of the old class representations, whether obtained through prototype-based methods or our proposed strategy [20]. In NECIL, this deviation is particularly problematic because it cannot be directly measured without access to raw samples from the old classes. To alleviate this issue, PRAKA [20] introduces random bidirectional interpolation between new class features and stored old class prototypes to enhance the quality of old class representations. While this random interpolation somewhat mitigates representation deviation in scenarios with a limited number of incremental phases, it also introduces instability in the generated features due to its reliance on a random interpolation factor. As a result, representation deviation may increase over longer sequences of incremental tasks, leading to performance degradation, as demonstrated in the experimental results in Section IV-A. To further address this bottleneck, we propose a Similarity-based Feature Compensation (SFC) mechanism to reduce the deviation between generated representations and the evolving classifier. Specifically, we use normalized cosine similarity to measure the similarity between the arriving new class features and generated old class features, selecting the most similar new class feature for each generated old class feature. We then apply a simple yet effective element-wise averaging between the generated and most similar feature sets to form the retrospective representations of old classes. These compensated features, along with their original labels, are used to retain the previously acquired knowledge. This compensatory mechanism ensures that synthesized old class representations not only remain closely aligned with real distributions over longer incremental tasks but also preserve a stable ability to reconstruct classification boundaries. Finally, we build a robust baseline model that integrates self-supervised label augmentation [22] and knowledge distillation [23] to combine the two key components described above, forming the proposed efficient non-exemplar class-incremental learning method with Retrospective Feature Synthesis, termed RFS. Our main contributions are as follows: • We propose an effective old class representation generation strategy for NECIL, which models the feature space with multivariate Gaussian distribution and sample representations from high-likelihood regions to resist catastrophic forgetting. • We contribute a similarity-based feature compensation mechanism, which selects the most similar new class features to compensate for the generated old class features through element-wise averaging, further improving the efficiency of incremental learning. • Extensive experiments and analysis demonstrate that our method achieves state-of-the-art performance on the NECIL benchmarks of CIFAR-100, TinyImageNet, and ImageNet-Subset."
https://arxiv.org/html/2411.01455v1,HiMemFormer: Hierarchical Memory-Aware Transformer for Multi-Agent Action Anticipation,"Understanding and predicting human actions has been a long-standing challenge and is a crucial measure of perception in robotics AI. While significant progress has been made in anticipating the future actions of individual agents, prior work has largely overlooked a key aspect of real-world human activity – interactions. To address this gap in human-like forecasting within multi-agent environments, we present the Hierarchical Memory-Aware Transformer (HiMemFormer), a transformer-based model for online multi-agent action anticipation. HiMemFormer integrates and distributes global memory that captures joint historical information across all agents through a transformer framework, with a hierarchical local memory decoder that interprets agent-specific features based on these global representations using a coarse-to-fine strategy. In contrast to previous approaches, HiMemFormer uniquely hierarchically applies the global context with agent-specific preferences to avoid noisy or redundant information in multi-agent action anticipation. Extensive experiments on various multi-agent scenarios demonstrate the significant performance of HiMemFormer, compared with other state-of-the-art methods.","Action detection [8] or anticipation [21] systems aim at forecasting future states of single or multiple agents from history. The recent advances in these areas facilitate embodied or virtual AI systems with the ability to perceive and interact with other agents and complex environments [33, 37, 44]. Such ability plays a pivotal role in numerous applications, such as autonomous driving [41], collaborative robotics [30], and home automation [32], where understanding and predicting the actions of various entities in a shared environment can significantly enhance safety, efficiency, and coordination. Agent memory plays an important role in conducting action anticipation due to the innate dependencies among actions [39, 36, 14, 37, 44]. LSTR [39] proposes to capture both long-term and short-term memory, while MAT [36] additionally incorporates future content in seen scenarios. In the multi-agent scenarios [33], each agent can be arbitrary or affected by the environment, which suggests one key to the success of a multi-agent system: how to effectively capture agent behavior at various time and social scales. A prominent line of research exploits the ways to obtain a unified single global feature representing time, e.g., [23, 4], and social relations, e.g., [17, 29]. AgentFormer [42] and HiVT [45] further explore combining time and social features with a overall global representation. Despite the significance, these state-of-the-art systems overlook the individual perspective of the problem: different agents may need time and social features at different scales. From the time perspective, some agent actions heavily rely on long-term memory, e.g., if they belong to a complex multi-step action sequence, while some actions are only relevant to short memory, e.g., an instant response to a rapid environment change. From the social perspective, similarly, the actions of some agents are much correlated with others during collaboration, while some of mostly stand-alone. To capture these agent-specific preferences in feature utilization, we propose to hierarchically capture the time and social features for each agent-specific decoder to include these global or contextual features with the desired granularity and discard unnecessary information that may introduce noise or latency to each specific agent. To achieve customized and flexible global feature utilization automatically, we propose the Hierarchical Memory-Aware Transformer (HiMemFormer), a novel approach that simultaneously learns feature representations from both contextual and agent-specified dimensions through a dual-hierarchical framework. Specifically, its Agent-to-Context Encoder augments the agents’ long-term history through cross-attention with global long-term memory. Then, the encoded long-term memory is further processed through a hierarchical Agent-to-Context Decoder that offers a coarse prediction given augmented long-term memory and contextual short-term memories. Finally, the coarse prediction is gradually refined by each agent-specific network augmented with individual short-term memory to get the anticipated actions. Through the dual-hierarchical network, HiMemFormer manages to model agent’s unique short-term memory while learning useful correlations from the contextual memories. This allows us to effectively compress the long-range contextual information without losing important lower level feature information. In summary, our contributions are three-fold: • We propose a transformer-based method to capture and utilize the global features in multi-agent scenarios in a flexible way responding to each agent’s preference. • We design a hierarchical memory encoder that follows a specific-to-general paradigm to learn long-term joint-memory features and a hierarchical memory decoder that learns an agent’s future action by a coarse-to-fine strategy. • We carry out exhaustive experiments on various multi-agent action anticipation scenarios and outperform existing baseline models."
https://arxiv.org/html/2411.01445v1,A Visual Question Answering Method for SAR Ship: Breaking the Requirement for Multimodal Dataset Construction and Model Fine-Tuning,"Current visual question answering (VQA) tasks often require constructing multimodal datasets and fine-tuning visual language models, which demands significant time and resources. This has greatly hindered the application of VQA to downstream tasks, such as ship information analysis based on Synthetic Aperture Radar (SAR) imagery. To address this challenge, this letter proposes a novel VQA approach that integrates object detection networks with visual language models, specifically designed for analyzing ships in SAR images. This integration aims to enhance the capabilities of VQA systems, focusing on aspects such as ship location, density, and size analysis, as well as risk behavior detection. Initially, we conducted baseline experiments using YOLO networks on two representative SAR ship detection datasets, SSDD and HRSID, to assess each model’s performance in terms of detection accuracy. Based on these results, we selected the optimal model, YOLOv8n, as the most suitable detection network for this task. Subsequently, leveraging the vision-language model Qwen2-VL, we designed and implemented a VQA task specifically for SAR scenes. This task employs the ship location and size information output by the detection network to generate multi-turn dialogues and scene descriptions for SAR imagery. Experimental results indicate that this method not only enables fundamental SAR scene question-answering without the need for additional datasets or fine-tuning but also dynamically adapts to complex, multi-turn dialogue requirements, demonstrating robust semantic understanding and adaptability.","In recent years, deep learning has driven major advancements in object detection, natural language processing, and large language models[1]. Object detection models analyze images to provide information about the location and size of objects within them. Natural language processing models respond to user input and requirements to generate answers. Visual Question Answering (VQA) systems attempt to interpret images by combining image data with prompts and leveraging their general knowledge to respond to users[2]. Recently, VQA has gained attention in the fields of Geoscience and remote sensing, with studies such as [3],[4] advancing the application of VQA for remote sensing information. As a form of remote sensing imagery, Synthetic Aperture Radar (SAR) offers all-weather monitoring capabilities unaffected by weather conditions[5], holding significant promise for maritime traffic applications. You Only Look Once (YOLO) [6], [7], [8], [9] excels in object detection with fast and accurate results, making it effective for various tasks, though its applications are relatively limited. Vision Language Models (VLMs) like those used in VQA typically require extensive data and training to achieve good performance, particularly when applied to specialized tasks such as SAR imagery in remote sensing. Parameter-efficient fine-tuning (PEFT) [10] with methods like LoRA [11] is often necessary but demands considerable resources and high-performance hardware. To ensure efficiency, lower-parameter models are preferred but generally lack broad scientific knowledge. Alternatively, multimodal fusion networks can integrate image and text to answer questions; however, approaches like those by Lobry et al. [12] and Li et al. [3] that use linear layers for classification are constrained to predefined question types and struggle with complex, open-ended queries. Researchers have developed various vision-language models, like Qwen2-VL [13] and BLIP-2 [14], which possess extensive general knowledge and are well-suited for VQA tasks without the need for additional fine-tuning or dataset creation. However, SAR imagery poses unique challenges due to its high noise levels that can interfere with the model’s understanding, and the often unclear ship features, particularly in nearshore scenes or when dealing with small targets. Additionally, although these vision-language models recognize image content well, they struggle to extract specific details such as target location and size, which are essential for accurately addressing user queries. Based on the background outlined above, this letter presents a novel visual question answering approach that focuses on rapidly transferring visual language question answering tasks to the maritime traffic domain using SAR imagery, without the need for additional dataset construction or model fine-tuning. The specific contributions are as follows: 1) We propose an efficient method for ship-related querying in SAR images by connecting an object detection network with a vision-language model, thereby eliminating the need for model fine-tuning or multimodal dataset construction. 2) Baseline experiments using YOLO series models were conducted on the SSDD and HRSID datasets to systematically assess each model’s accuracy and efficiency in SAR ship detection. The optimal model was selected to construct prompts based on predicted bounding box information. Q&A experiments on representative samples demonstrate that the proposed method enables more accurate and reliable responses, offering new insights for future multimodal SAR image analysis and maritime traffic monitoring."
https://arxiv.org/html/2411.01443v1,Activating Self-Attentionfor Multi-Scene Absolute Pose Regression,"Multi-scene absolute pose regression addresses the demand for fast and memory-efficient camera pose estimation across various real-world environments. Nowadays, transformer-based model has been devised to regress the camera pose directly in multi-scenes. Despite its potential, transformer encoders are underutilized due to the collapsed self-attention map, having low representation capacity. This work highlights the problem and investigates it from a new perspective: distortion of query-key embedding space. Based on the statistical analysis, we reveal that queries and keys are mapped in completely different spaces while only a few keys are blended into the query region. This leads to the collapse of the self-attention map as all queries are considered similar to those few keys. Therefore, we propose simple but effective solutions to activate self-attention. Concretely, we present an auxiliary loss that aligns queries and keys, preventing the distortion of query-key space and encouraging the model to find global relations by self-attention. In addition, the fixed sinusoidal positional encoding is adopted instead of undertrained learnable one to reflect appropriate positional clues into the inputs of self-attention. As a result, our approach resolves the aforementioned problem effectively, thus outperforming existing methods in both outdoor and indoor scenes.","Camera pose estimation is a fundamental and essential computer vision task, adopted in numerous applications such as augmented reality and autonomous driving. Geometric pipelines [1, 2, 3, 4, 5, 6] with 2D and 3D data have been a mainstream with high accuracy. After extracting features and matching 2D-3D correspondences, camera pose is approximated via Perspective-n-Points (PnP) algorithm and RANSAC [7]. However, there still remains several challenges for real-world applications, including high computational cost and a huge amount of 3D point cloud. Absolute Pose Regression (APR) tackles these issues by directly estimating the 6-DoF pose from a single RGB image in an end-to-end manner. First introduced by Kendall et al. [8], subsequent APR methods [9, 10, 11, 12, 13, 14, 15, 16, 17, 18] have been devised based on convolutional neural networks (CNN). However, they still demand multiple models and individual optimization to be applied in real-world multi-scene scenarios. In this regard, Multi-Scene Absolute Pose Regression (MS-APR) has emerged to satisfy the needs of speed and memory efficiency across multiple scenes [19]. MSTransformer [20] pioneers a streamlined one-stage MS-APR approach with transformer architecture. It leverages the transformer decoder not only to improve memory efficiency but also to enhance accuracy significantly. However, we point out that the learning capacity of transformer encoders is underutilized. As shown in Tab. 1, MSTransformer’s encoder self-attention modules do not significantly improve or even degrade performance. Although we discovered low-rank, collapsed attention maps, which are known to cause gradient vanishing [21] or training instability [22], general solutions [22, 23, 24] were not correct the problem in the case of MSTransformer. This challenge motivates us to elucidate the phenomenon from another perspective in the context of MS-APR. Therefore, this work proposes brand new analysis: distortion of query-key embedding space, where the attention score originates. We statistically substantiate that queries and keys are mapped in completely separated regions in the embedding space while only a few keys are blended into the query region. In this situation, attention maps inevitably collapse, as all queries are represented similarly to those few keys. Intuitively, it means that all image patches are just represented by only a few image patches, as illustrated in Fig. 1. We conjecture that the task difficulty contributes to the issue; the model should extrapolate the camera pose from a single RGB image across multiple scenes. To support this, we empirically reveal that the model tends to avoid exploring self-relationships at the beginning of the training. These findings align with our additional observations that learnable positional embeddings used in self-attention are also undertrained. Nevertheless, we expect that image features incorporating global self-relations will be valuable to estimate the camera pose by capturing salient features such as long edges and corners. This work thus introduces simple but effective solutions that activate encoder self-attention modules for MS-APR. Concretely, we design an auxiliary loss which aligns the query region and the key region. By forcing all queries and keys to be mapped into the close and dense space, they are highly encouraged to interact with each other by self-attention—like putting boxers in the ring and blowing the whistle! Furthermore, we explore various positional encoding methods based on empirical evidence since current undertrained one confuses the model to estimate the camera pose with incorrect positions. Finally, fixed 2D sinusoidal encoding is adopted instead of learnable parameter-based methods [25, 26]. This enables the model to consider appropriate positional clues of each image feature during exploring self-relationships from the beginning of the training. As such, the model can obtain rich global relations from activated self-attention as shown in Fig. 1. These relations are incorporated into image features, acquiring more informative encoder output. Extensive experiments demonstrate that our solution recovers the self-attention successfully by preventing the distortion of query-key space and keeping high capacity of self-attention map [22]. As a result, our model outperforms existing MS-APR methods in both outdoor [8] and indoor [27] scenes without additional memory during inference, upholding the original purpose of MS-APR. Table 1: Ablation on encoder self-attention modules in MSTransformer [20]. We report the average of median position and orientation errors for each experiment. Outdoor [8] Indoor [27] MST [20] 1.28⁢m/2.73⁢°1.28m2.73°1.28\text{m}/$$1.28 m / 2.73 ⁢ ° 0.18⁢m/7.28⁢°0.18m7.28°0.18\text{m}/$$0.18 m / 7.28 ⁢ ° MST w/o encoder SA 1.21⁢m/2.84⁢°1.21m2.84°1.21\text{m}/$$1.21 m / 2.84 ⁢ ° 0.18⁢m/7.49⁢°0.18m7.49°0.18\text{m}/$$0.18 m / 7.49 ⁢ ° Figure 1: The figure shows query-key spaces, self-attention maps, and attended keys from the orientation transformer encoder of the baseline and ours, respectively. (a) In the case of the baseline, queries and keys are mapped in separate regions, while only a few keys are blended into the query region. Consequently, the self-attention map collapses and whole image features are represented by meaningless few keys, indicating waste of learning capacity of transformer encoder. (b) However, our solution makes queries and keys interact with each other, activating self-attention. This allows the model to obtain crucial global relations within image features, capturing salient global features."
https://arxiv.org/html/2411.01432v1,Meta-Exploiting Frequency Prior for Cross-Domain Few-Shot Learning,"Meta-learning offers a promising avenue for few-shot learning (FSL), enabling models to glean a generalizable feature embedding through episodic training on synthetic FSL tasks in a source domain. Yet, in practical scenarios where the target task diverges from that in the source domain, meta-learning based method is susceptible to over-fitting. To overcome this, we introduce a novel framework, Meta-Exploiting Frequency Prior for Cross-Domain Few-Shot Learning, which is crafted to comprehensively exploit the cross-domain transferable image prior that each image can be decomposed into complementary low-frequency content details and high-frequency robust structural characteristics. Motivated by this insight, we propose to decompose each query image into its high-frequency and low-frequency components, and parallel incorporate them into the feature embedding network to enhance the final category prediction. More importantly, we introduce a feature reconstruction prior and a prediction consistency prior to separately encourage the consistency of the intermediate feature as well as the final category prediction between the original query image and its decomposed frequency components. This allows for collectively guiding the network’s meta-learning process with the aim of learning generalizable image feature embeddings, while not introducing any extra computational cost in the inference phase. Our framework establishes new state-of-the-art results on multiple cross-domain few-shot learning benchmarks.","Meta-learning Finn et al. (2017); Lee et al. (2019); Rusu et al. (2019); Zhmoginov et al. (2022); Baik et al. (2020) represents a potent paradigm within the domain of FSL Vinyals et al. (2016); Snell et al. (2017); Huang et al. (2022); Zhang and Huang (2022); Chen et al. (2021). This paradigm harnesses a feature embedding network to capture task-agnostic meta-knowledge, facilitating generalization to novel tasks. To this end, meta-learning systematically samples a sequence of FSL episodes in the source domain to supervisedly enforcing learn an effective feature embedding network that assimilating cross-task transferable essentials and generalize well to novel target tasks. Due to its exceptional learning-to-learn capabilities, meta-learning has established itself as the de facto approach for the development of effective few-shot solvers Vinyals et al. (2016); Snell et al. (2017); Huang et al. (2022); Zhang and Huang (2022); Finn et al. (2017); Lee et al. (2019); Rusu et al. (2019); Zhmoginov et al. (2022); Baik et al. (2020). However, in practical cross-domain scenarios where the target task exhibits a noticeable distribution discrepancy from that in the source domain, meta-learning based methods are susceptible to over-fitting. This phenomenon can be attributed to two main reasons. Firstly, tasks randomly sampled in source domain often come from one or several fixed patterns, and thus the continual switching of episodes training may cause the model to over-fit on some task-specific priors. For instance, in tasks involving the discrimination between tigers and giraffes, meta-learning methods may compel the model to emphasize appearance outlines, while in tasks focused on fine-grained bird identification, models tend to prioritize local discriminative textures. Yet, these task-specific priors prove challenging to transfer across different tasks Lyu et al. (2021); Zhou et al. (2023). Secondly, the iterative episodic training in the source domain can result in the model over-fitting to semantic prior properties specific to that domain. For example, source domains comprised of natural scene images often exhibit obvious semantic priors, whereas specialized target domains like medical image analysis or remote sensing may lack clear semantic concepts. This overall domain bias also hampers the model’s cross-domain generalization. For all these challenges, the underlying evil lies in the absence of cross-domain invariant priors to guide meta-learning in the source domain. To address this challenge, we introduce a novel framework, Meta-Exploiting Frequency Prior for Cross-Domain Few-Shot Learning. Inspired by classical image transform theories (Fourier Nussbaumer and Nussbaumer (1982) or wavelet Zhang and Zhang (2019)), where each image can be decomposed into low-frequency content details and high-frequency structural characteristics, despite which domain it belongs to, we attempt to cast such a cross-domain invariant image property into appropriate frequency priors and utilize them to guide the meta-learning in source domain. Following this idea, we decompose each query image into a high-frequency and a low-frequency parts, and feed each into the feature embedding network for final category prediction, mirroring the process applied to the original query image. These allows for the independent feature learning in both spatial and frequency domains. In addition, the low-frequency and high-frequency branch will separately exploit the complementary image content and structures for feature enhancement, which are often concealed in the spatial domain of original query image. More importantly, we further develop two frequency priors, namely a feature reconstruction prior and a prediction consistency prior, which separately forces the original query image and its decomposed frequency components to produce the consistent intermediate feature representation as well as the final category prediction. In a specific, the feature reconstruction prior requires to reconstruct the feature of original image through fusing the features of both decomposed frequency parts using a deep projection network. The prediction consistency prior aims to minimize the separate Kullback-Leibler divergence between the prediction scores produced by the original query image and its each frequency component. By doing these, meta-learning in the source domain can be appropriately regularized and produce the exceptional cross-domain generalizable feature embeddings. Moreover, such frequency priors only perform in the meta-learning phase without introducing any extra computational cost in inference. Through a series of rigorous experiments, our framework establishes itself as a front-runner, achieving state-of-the-art results across multiple cross domain FSL benchmarks. Additionally, our method exhibits significant efficiency advantages. The primary contributions of this study can be summarized as follows: • We present a novel insightful meta-learning framework that exploits cross-domain invariant frequency priors to alleviate the over-fitting problems of classic meta-learning in cross-domain FSL tasks. • We propose two frequency prior, namely a prediction consistency prior and a feature reconstruction prior, to collectively guide the meta-learning procedure. • We achieve state-of-the-art results on multiple cross-domain FSL benchmarks."
https://arxiv.org/html/2411.01408v1,HeightMapNet: Explicit Height Modeling for End-to-End HD Map Learning,"Recent advances in high-definition (HD) map construction from surround-view images have highlighted their cost-effectiveness in deployment. However, prevailing techniques often fall short in accurately extracting and utilizing road features, as well as in the implementation of view transformation. In response, we introduce HeightMapNet, a novel framework that establishes a dynamic relationship between image features and road surface height distributions. By integrating height priors, our approach refines the accuracy of Bird’s-Eye-View (BEV) features beyond conventional methods. HeightMapNet also introduces a foreground-background separation network that sharply distinguishes between critical road elements and extraneous background components, enabling precise focus on detailed road micro-features. Additionally, our method leverages multi-scale features within the BEV space, optimally utilizing spatial geometric information to boost model performance. HeightMapNet has shown exceptional results on the challenging nuScenes and Argoverse 2 datasets, outperforming several widely recognized approaches. The code will be available at https://github.com/adasfag/HeightMapNet/.","In the domain of autonomous driving, the ability to accurately and comprehensively interpret the environmental context surrounding the ego vehicle is paramount for ensuring safe and effective operational decisions. Surround-view methods, recognized for their cost-efficiency and broad applicability, have garnered significant advancements in this area. Current techniques predominantly fall into two categories based on their intermediary representations: sparse query-based methods and dense BEV-based methods. Drawing inspiration from the DETR architecture [2, 26], sparse query-based approaches [16, 15, 14] utilize learnable global queries representing detection elements, refined through interactions with surround-view image features. Although this strategy effectively controls the proliferation of queries, its reliance on static global queries limits adaptability in dynamic environments, often resulting in detection inaccuracies at extended distances. Conversely, BEV-based methods convert Perspective View (PV) into BEV representations using a perspective transformation module. Subsequent feature processing is performed via a map detector head. BEV methods [33, 9, 12, 13, 17] demonstrate state-of-the-art performance in online mapping, and have recently dominated the field. Existing BEV methods typically utilize enhanced LSS [20] or attention mechanism [25] as baselines for the perspective transformation module. LSS-based methods [13, 10] often require auxiliary losses to accelerate detector convergence speed, while attention-based methods [33, 5, 21, 23, 18, 3] usually need additional modules to improve the output BEV features. These methods normally overlook the vertical dimension of road features during the PV-to-BEV transformation, thus compromising their capacity to delineate complex environmental details accurately. Additionally, most existing studies [11, 12, 16] inadequately address the challenge of filtering non-critical elements such as the sky and other extraneous background features during the processing of image features from multi-view inputs. This oversight in selective background filtration foregoes crucial noise reduction opportunities, consequently rendering the models vulnerable to disruptions caused by irrelevant data. Such disruptions significantly compromise the accuracy and reliability of the resultant perceptual outputs. Moreover, while prevailing researches [11, 12, 13] tend to concentrate on exploiting single-layer image features for computational effectiveness, it largely neglects the benefits and possibilities afforded by multi-scale feature fusion within the BEV space. This limitation undermines the model’s effectiveness in navigating complex road environments. To address these challenges, we propose a new view transformation paradigm that forges a nuanced relationship between image features and road surface height distributions, integrating height priors to refine the accuracy of BEV features. By explicitly modeling ground height during vehicle operation, the capacity to delineate complex environmental details is significantly improved. In addition, we develop a foreground-background separation network with self-supervised learning. This network optimizes the extraction and utilization of road features, effectively minimizing non-critical background elements, thereby enhancing the clarity and quality of the input features. Such enhancements substantially improve the reliability of the perceptual outcomes. Furthermore, our methodology leverages multi-scale feature fusion within the BEV space, bolstering the precision and robustness of map construction across complex road environments. In summary, we propose a novel framework for HD map construction with the following key contributions. • We develop an advanced view transformation module that dynamically links image features to road surface height distributions, significantly enhancing spatial comprehension by integrating seamlessly with attention-based neural networks. • We introduce a foreground-background separation network optimized for road environments, using self-supervised learning to improve road feature extraction and remove irrelevant background elements. Besides, a coupled multi-scale feature fusion mechanism is proposed to achieve a more robust BEV representation. • Extensive experiments on two challenging HD map construction datasets validate the effectiveness of our framework, demonstrating robust performance and suitability across diverse driving scenarios."
https://arxiv.org/html/2411.01399v1,MambaReg: Mamba-Based Disentangled Convolutional Sparse Coding for Unsupervised Deformable Multi-Modal Image Registration,"Precise alignment of multi-modal images with inherent feature discrepancies poses a pivotal challenge in deformable image registration. Traditional learning-based approaches often consider registration networks as black boxes without interpretability. One core insight is that disentangling alignment features and non-alignment features across modalities bring benefits. Meanwhile, it is challenging for the prominent methods for image registration tasks, such as convolutional neural networks, to capture long-range dependencies by their local receptive fields. The methods often fail when the given image pair has a large misalignment due to the lack of effectively learning long-range dependencies and correspondence. In this paper, we propose MambaReg, a novel Mamba-based architecture that integrates Mamba’s strong capability in capturing long sequences to address these challenges. With our proposed several sub-modules, MambaReg can effectively disentangle modality-independent features responsible for registration from modality-dependent, non-aligning features. By selectively attending to the relevant features, our network adeptly captures the correlation between multi-modal images, enabling focused deformation field prediction and precise image alignment. The Mamba-based architecture seamlessly integrates the local feature extraction power of convolutional layers with the long-range dependency modeling capabilities of Mamba. Experiments on public non-rigid RGB-IR image datasets demonstrate the superiority of our method, outperforming existing approaches in terms of registration accuracy and deformation field smoothness.","Image registration is a fundamental task in the computer vision field. Multi-modal image registration (MIR) aims to spatially align two images from different modalities, which capture the same scene from different perspectives, such as pairs [1] of RGB images that provide intensity and texture details and infrared images that give information on thermal properties that are not visible in the RGB spectrum. This alignment is crucial for subsequent multi-modal tasks like image fusion [2] and segmentation [3]. However, these multi-modal images are acquired by different sensors, they are often misaligned and contain both features responsible and not responsible for alignment, requiring further attention. Classical registration methods mostly involve an iterative optimization of similarity metrics like mutual information [4], correlation ratio [5], and structural feature descriptors [6, 7] to maximize the similarity between modalities. These classical methods suffer from time-consuming and poor performance compared with deep-learning-based models. Recently, with the success of deep learning, many learning-based MIR methods have emerged and achieved remarkable success in inference speed [8] and registration accuracy, broadly classified into two categories. One category [9, 10, 11] trains deep networks to optimize certain similarity metrics for multi-modal registration. For example, de Vos et al. [10] proposed a twin CNN to minimize the normalized cross-correlation metric between multi-modal images for registration. However, these methods heavily rely on the effectiveness of similarity metrics. The other category [12, 13, 14] adopts image-to-image translation to reduce MIR to a simpler mono-modal registration task. Arar et al. [14] employed a GAN to translate one modality to another, and then performed registration within the same modality. However, training GANs is challenging and may introduce artificial noise[15], impeding registration. These methods consider the networks as black-boxes and fail to provide better interpretability and relationship between modalities. One core insight is disentangling features that are responsible for alignment and features that are not responsible for alignment from different modalities is beneficial for multi-modal registration. This raises an intriguing question: how to build registration models capable of recognizing different features for alignment and non-alignment? Meanwhile, the prominent methods for image registration tasks, such as convolutional neuron networks, do not have the ability to capture long-range dependencies by their local receptive fields. The methods often fail when the given image pair has a large misalignment due to CNNs being lack of effectively learn long-range dependencies and correspondences. Existing methods typically require that image pairs are roughly aligned. Although vision transformers can exploit long-range dependencies by their attention mechanism, they suffer from high quadratic complexity and the requirements of massive data. Therefore, another question is raised: how to equip registration models with the ability to exploit long-term dependencies without too much resource consumption? To address the aforementioned issues, we propose a Mamba-based multi-modal image registration network, named MambaReg, for deformable multi-modal registration tasks. Our proposed MambaReg consists of two interacted branches to disentangle alignment and non-alignment features from different modalities by our proposed modules. In order to exploit long-term dependencies, we integrate Mamba’s [16] long sequence modeling capabilities into our model. In summary, our contributions in this paper are as follows: • We propose a Mamba-based multi-modal image registration network, named MambaReg, for deformable multi-modal registration tasks. To the best of our knowledge, the proposed MambaReg is the first unsupervised Mamba-based image registration framework. • We proposed different modules to disentangle modality-independent features responsible for registration from modality-dependent, non-aligning features. • To address data scarcity in this domain, we reconstructed a plant RGB-IR registration dataset using the publicly available MSU-PID dataset. • Extensive experiments on non-grid RGB-IR image datasets demonstrate that MambaReg outperforms existing methods and achieves state-of-the-art performance."
https://arxiv.org/html/2411.01390v1,A NEW LOGIC FOR PEDIATRIC BRAIN TUMOR SEGMENTATION,"In this paper, we present a novel approach for segmenting pediatric brain tumors using a deep learning architecture, inspired by expert radiologists’ segmentation strategies. Our model delineates four distinct tumor labels and is benchmarked on a held-out PED BraTS 2024 test set (i.e., pediatric brain tumor datasets introduced by BraTS). Furthermore, we evaluate our model’s performance against the state-of-the-art (SOTA) model using a new external dataset of 30 patients from CBTN (Children’s Brain Tumor Network), labeled in accordance with the PED BraTS 2024 guidelines. We compare segmentation outcomes with the winning algorithm from the PED BraTS 2023 challenge as the SOTA model. Our proposed algorithm achieved an average Dice score of 0.642 and an HD95 of 73.0 mm on the CBTN test data, outperforming the SOTA model, which achieved a Dice score of 0.626 and an HD95 of 84.0 mm. Our results indicate that the proposed model is a step towards providing more accurate segmentation for pediatric brain tumors, which is essential for evaluating therapy response and monitoring patient progress.","Automated brain tumor segmentation provides precise and objective quantification, enhancing the accuracy and consistency of tumor burden assessment and treatment response monitoring, which could significantly improve disease prognosis and support personalized treatment planning. This is valuable for patients irrespective of age. However, pediatric brain tumor segmentation presents unique challenges compared to adult cases due to several factors [1]: anatomical differences, tumor heterogeneity, limited data, and ethical considerations. Additionally, pediatric brain tumors vary significantly between age groups within the pediatric population. The constant development and plasticity of the pediatric brain introduces further complexity, as evolving brain structures may impact tumor appearance and growth patterns. Addressing these challenges requires the development of specialized segmentation algorithms that can effectively handle the unique characteristics associated with pediatric brain tumors. Towards the dataset problem, over the past two years, the Brain Tumor Segmentation (BraTS) challenge, a community based initiative to develop machine learning models for automating brain tumor segmentation, introduced the pediatric (PED) segmentation task to establish benchmark methods in this area. Moreover, Children’s Brain Tumor Network (CBTN), established in 2011, is a collaborative consortium with a data repository for pediatric brain tumor research. CBTN and PED BraTS initiated efforts to drive research development, with the introduction of pediatric brain tumor segmentation task at BraTS 2023 [1, 2]. We leverage these data sources in this study, and propose a novel deep-learning segmentation architecture incorporating simple radiological reasoning to label tumor components more accurately. Our contributions are summarized as follows: Fig. 1: Our model architecture utilizing two nnU-Net frameworks, one trained on ET, CC and ED detection and the other trained to detect WT. 1. We developed a novel yet simple deep learning architecture, inspired by radiological reasoning for 3D multi-class delineation of brain tumors, to segment pediatric brain tumors and their counterparts. 2. We evaluated the robustness of our proposed architecture across multiple frameworks, demonstrating significant improvements over existing baseline models. 3. Our model outperformed the state-of-the-art (SOTA) model—the winner of the PED BraTS 2023 challenge— when tested on an out-of-distribution, real-world dataset from CBTN."
https://arxiv.org/html/2411.01351v1,"Guided Synthesis of Labeled Brain
MRI Data Using Latent Diffusion
Models for Segmentation of
Enlarged Ventricles","Deep learning models in medical contexts face challenges like data scarcity, inhomogeneity, and privacy concerns. Synthetic data can help, but often underperforms compared to real data. This study focuses on improving ventricular segmentation in brain Magnetic Resoncance Imaging (MRI) images, particularly for cases with enlarged ventricles, using synthetic data. We employed two latent diffusion models (LDMs): a mask generator trained using 10,000 masks, and a corresponding SPADE image generator optimized using 6,881 scans to create an MRI conditioned on a 3D brain mask. Conditioning the mask generator on ventricular volume in combination with classifier-free guidance enabled the control of the ventricular volume distribution of the generated synthetic images. Next, the performance of the synthetic data was tested using three nnU-Net segmentation models trained on a real (N=1,000), augmented (N=1,512) and entirely synthetic data (N=1,000), respectively, where the synthetic data contains a more uniform distribution of ventricular volumes compared to the real data. The resulting models were tested on a completely independent hold-out dataset of patients with enlarged ventricles (N=42), with manual delineation of the ventricles used as ground truth performed by a trained neuroradiologist. The model trained on real data showed a mean absolute error (MAE) of 9.09±12.18plus-or-minus9.0912.189.09\pm 12.189.09 ± 12.18 mL in predicted ventricular volume, while the models trained on synthetic and augmented data showed MAEs of 7.52±4.81plus-or-minus7.524.817.52\pm 4.817.52 ± 4.81 mL and 6.23±4.33plus-or-minus6.234.336.23\pm 4.336.23 ± 4.33 mL, respectively. Both the synthetic and augmented model also outperformed the state-of-the-art model SynthSeg, which due to limited performance in cases of large ventricular volumes, showed an MAE of 7.73±12.12plus-or-minus7.7312.127.73\pm 12.127.73 ± 12.12 mL with a factor of 3 higher standard deviation. The model trained on augmented data showed the highest Dice score of 0.892±0.05plus-or-minus0.8920.050.892\pm 0.050.892 ± 0.05, slightly outperforming SynthSeg (0.874±0.06plus-or-minus0.8740.060.874\pm 0.060.874 ± 0.06) and on par with the model trained on real data (0.891±0.05plus-or-minus0.8910.050.891\pm 0.050.891 ± 0.05). The synthetic model (0.872±0.06plus-or-minus0.8720.060.872\pm 0.060.872 ± 0.06) performed similar to SynthSeg. In summary, we provide evidence that guided synthesis of labeled brain MRI data using LDMs improves the segmentation of enlarged ventricles and outperforms existing state-of-the-art segmentation models.","The success of machine learning largely depends on the quantity and quality of available training data. As models grow increasingly large, they require immense datasets, which has culminated in some of the most successful models trained on so many instances of data they need not see the same sample twice (Brown et al. (2020)). While such abundance can be found in natural images or natural language processing tasks, this wealth of data is not universally accessible. In medical imaging, data scarcity, variability, and privacy constraints present significant challenges to building similarly large models. Patients with normal pressure hydrocephalus (NPH) exemplify this issue, displaying an abnormal buildup of cerebrospinal fluid in the brain’s ventricle system, consequently leading to unique and abnormally enlarged ventricles. Contemporary state-of-the-art models for brain segmentation of magnetic resonance imaging (MRI) data (Billot et al. (2023a)) struggle to accurately segment the enlarged ventricle systems of NPH patients. Furthermore, obtaining a large training set for these patients is difficult due to their uniqueness and the diverse nature of the disease. Generating synthetic data offers a promising alternative to the prohibitively costly process of collecting real data. Modern image generation models, such as diffusion models, initially seem to provide limitless, highly customizable data with perceptual quality nearly indistinguishable from real images (Podell et al. (2023)). However, extensive research has shown that purely synthetic training data cannot match the effectiveness of real training data (Marwood et al. (2023); Fernandez et al. (2022, 2024)). Instead, synthetic data is best used to supplement real data to enhance model performance (Azizi et al. (2023)). In the field of MRI, several studies (Pinaya et al. (2022); Han et al. (2023)) have demonstrated the remarkable perceptual quality of synthetic images generated using diffusion models. Although these synthetic images still do not match the quality of real data, Fernandez et al. (2022) made significant progress in narrowing the performance gap between real and synthetic training data for brain MRI segmentation models. Additionally, conditioning the models on scalar values can produce increasingly realistic and morphology-preserving images. For example, Pinaya et al. (2022) trained a latent diffusion model (LDM) on a large sample of 3D T1-weighted MRI data (N=31,740) from the UK Biobank dataset (Sudlow et al. (2015)) while conditioning on several parameters such as ventricular volume, thus being able to generate synthetic data with varying ventricle sizes. Achieving remarkable visual quality, Pinaya et al. (2022) used their model to create an open large synthetic dataset consisting of 100,000 synthetic 3D MRI images (LDM100k dataset). In this study, we extend prior work by presenting two LDMs: a mask generator and an image generator. Combined, these models create fully synthetic labeled segmentation data for brain MRI. By conditioning our mask generator on ventricular volume, we enable control over the distribution in the generated data. We generate synthetic images and labels with out-of-distribution (OOD) ventricle sizes using our LDMs to address data scarcity for patients with enlarged ventricles. Finally, we train a ventricle segmentation model using this synthetic data and compare the performance against state-of-the-art models, using a completely independent test set consisting of NPH patients with enlarged ventricles. Code to reproduce this work is publicly available on GitHub.111See https://github.com/vaynonym/diffusion_abnormal_brain_anatomy"
https://arxiv.org/html/2411.01327v1,Visual Fourier Prompt Tuning,"With the scale of Transformer-based vision models continuing to grow, finetuning these large-scale pretrained models for new tasks has become increasingly parameter-intensive. Visual prompt tuning is introduced as a parameter-efficient finetuning (PEFT) method to this trend. Despite its successes, a notable research challenge persists within almost all PEFT approaches: significant performance degradation is observed when there is a substantial disparity between the datasets used in pretraining and finetuning phases. To address this challenge, we draw inspiration from human visual cognition, and propose the Visual Fourier Prompt Tuning (VFPT) method as an effective and efficient solution for adapting large-scale Transformer-based models. Our approach innovatively incorporates the Fast Fourier Transform into prompt embeddings, seamlessly integrating both spatial and frequency domain information. Apart from its inherent simplicity and intuitiveness, VFPT exhibits superior performance across various tasks, offering a general solution to address the data disparity challenge. Empirical results demonstrate that our approach outperforms several state-of-the-art baselines on two benchmarks, with low parameter usage (e.g., 0.57% of model parameters on VTAB-1k) and notable performance enhancements (e.g., 73.20% of mean accuracy on VTAB-1k). Our code is avaliable at https://github.com/runtsang/VFPT.","“Fourier’s theorem is not only one of the most beautiful results of modern analysis, but it may be said to furnish an indispensable instrument in the treatment of nearly every recondite question in modern physics.” −-- Lord William Thomson Kelvin Thomson_Tait_2009 Prompt tuning lester2021power ; aprompt is initially introduced for parameter-efficient adaptation of large foundation models in natural language processing (NLP). As vision models continue to scale for enhanced performance, visual prompt tuning jia2022visual has been applied to various vision domains (e.g., image classification han20232vpt , segmentation xu2024spectral ; zhu2023segprompt , detection nie2023pro ), demonstrating superior performance and lower parameter usage compared to other parameter-efficient fine-tuning (PEFT) methods. However, a common challenge within the research community remains unaddressed: significant performance degradation occurs when there is a substantial disparity between the data used in pretraining and finetuning han2024facing ; chavan2023one . This issue hinders the broader application of visual prompt tuning. Consequently, a natural question arises: ① Can prompt tuning generalize across datasets with varying disparities? As researchers commonly draw insights from human to replicate the principles in intelligent machines zhao2023brain ; hassabis2017neuroscience ; salehi2018emerging ; shneiderman2022human , we consider to answer this question from the human visual cognition’s perspective. While humans comprehend the world through past experiences/knowledge, it is essential to generalize and adapt this understanding to new tasks efficiently and effectively. The robust and rapid adaptability of human visual cognition thus arises from various domain analysis, capturing the new patterns from different channels and perspectives quigley2000fade ; viola2020visual ; burgess2019monet . Interestingly, we find that the paradigm of visual prompt tuning is conceptually analogous to human visual cognition. While the frozen large-scale vision model functions as accumulated knowledge, the fast adaptation mechanism resembles visual prompt tuning, requiring the incorporation of diverse domains of information (e.g., time, frequency) to achieve comprehensive understandings allen2004signal ; reddy1998fast ; chi2020fast . The Fast Fourier Transform (FFT) allen2004signal ; reddy1998fast ; chi2020fast , renowned for its ability to convert signals from their original domain (e.g., time or spatial) to the frequency domain and vice versa, serves as an ideal tool for contributing informative insights in the frequency domain. By leveraging the capabilities of FFT, visual prompts can naturally integrate both spatial and frequency domain information during finetuning, thereby enabling the frozen vision model to achieve consistent and robust performance across datasets with varying disparities. Consequently, our research question evolves into: ② How can FFT be integrated into visual prompt tuning to emulate the human visual mechanism? To this end, we employ a simple yet effective strategy that utilizes the Fourier operations to facilitate visual prompt tuning (see Fig. 1(c)). By integrating frequency domain information into learnable prompt embeddings, our approach elegantly assimilates data from both spatial and frequency domains, simulating the human visual cognition. We name our approach Visual Fourier Prompt Tuning (VFPT), which exhibits several compelling advantages: ❶ Simplicity. The intuitive application of FFT in prompt tuning emulates the rapid processing capabilities of the human visual system, making VFPT both elegant and straightforward to implement (see §2.1). ❷ Generality. By incorporating frequency domain information, the search space for latent embeddings of prompts is naturally expanded, resulting in advanced enhancement in performance across different datasets and tasks with varying data disparities (see §4.2). The generality of our model is further illustrated through our analysis of the optimization process, which enables smoother navigation towards local minima, increasing flatness around them and exhibiting apparent convexity. ❸ Interpretability. To intuitively demonstrate the advantages of Fourier components, we visually illustrate that the introduction of Fourier transform in visual prompt tuning results in a markedly higher concentration of attention scores within the Transformer’s input space, which correlates positively with enhancements in performance (see §4.4). This observation, in turn, explains the effectiveness of our approach. Comprehensive experiments are conducted to evaluate the performance of VFPT. In §2, we conduct a literature review and discuss relevant works. Our approach is presented in §3, where we describe how we simple yet effectively integrate FFT into visual prompt tuning.absent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT In §4.2, weabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT presentabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT compellingabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT experimentalabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT resultsabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT onabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT variousabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT benchmarks,absent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT backbones,absent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT andabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT differentabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT pretrainingabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT objectives, achieving superior performance without complex engineering design. Specifically, ourabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT approachabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT achievesabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT anabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT averageabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT improvementabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT of 7.63% in accuracy on VTAB-1k comparedabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT toabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT fullabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT finetuning, and 3.77% compared to VPT jia2022visual . In §4.4, we demonstrate that the FFT prompts significantly enhance the activation of the frozen vision model. Additionally, we study the optimization process of prompt tuning approaches, indicating that VFPT provides a more favorable optimization process. Finally,absent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT weabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT demonstrateabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT theabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT strongabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT algorithmicabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT generalizationabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT ofabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT ourabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT approachabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT toabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT theabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT languageabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT domain, and show additional visual explanations inabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT theabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT Appendix. Weabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT trustabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT thatabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT thisabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT workabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT providesabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT valuableabsent{}_{\!}start_FLOATSUBSCRIPT end_FLOATSUBSCRIPT insights."
https://arxiv.org/html/2411.01293v1,Diffusion Models as Cartoonists!The Curious Case of High Density Regions,"We investigate what kind of images lie in the high-density regions of diffusion models. We introduce a theoretical mode-tracking process capable of pinpointing the exact mode of the denoising distribution, and we propose a practical high-probability sampler that consistently generates images of higher likelihood than usual samplers. Our empirical findings reveal the existence of significantly higher likelihood samples that typical samplers do not produce, often manifesting as cartoon-like drawings or blurry images depending on the noise level. Curiously, these patterns emerge in datasets devoid of such examples. We also present a novel approach to track sample likelihoods in diffusion SDEs, which remarkably incurs no additional computational cost.","Recently, Karras et al. (2024a) attributed the empirical success of guided diffusion models to their ability to limit outliers, i.e. samples 𝒙0∼p0similar-tosubscript𝒙0subscript𝑝0{\bm{x}}_{0}\sim p_{0}bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∼ italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT with low likelihood p0⁢(𝒙0)subscript𝑝0subscript𝒙0p_{0}({\bm{x}}_{0})italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ). We argue that these models in fact also elude samples with very high likelihoods. Our assertion stems from investigating a key question: what manifests if we bias the sampler towards high-density regions of p0subscript𝑝0p_{0}italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT? However, an immediate hurdle is the inability of stochastic diffusion models to track their own likelihood (Song et al., 2020c; 2021). Figure 1: High-density samples y0|xtconditionalsubscript𝑦0subscript𝑥𝑡{\bm{y}}_{0}|{\bm{x}}_{t}bold_italic_y start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT | bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT resemble cartoon drawings. This is in contrast to regular denoising samples 𝒙0∼p⁢(𝒙0|𝒙t)similar-tosubscript𝒙0𝑝conditionalsubscript𝒙0subscript𝒙𝑡{\bm{x}}_{0}\sim p({\bm{x}}_{0}|{\bm{x}}_{t})bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∼ italic_p ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT | bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) or expectations 𝔼⁢[𝒙0|𝒙t]𝔼delimited-[]conditionalsubscript𝒙0subscript𝒙𝑡\mathbb{E}[{\bm{x}}_{0}|{\bm{x}}_{t}]blackboard_E [ bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT | bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ]. The data contains no cartoons. First, we show that likelihood can be tracked in diffusion models with novel augmented stochastic differential equations (SDE), which govern the evolution of a sample with its log-density log⁡pt⁢(𝒙t)subscript𝑝𝑡subscript𝒙𝑡\log p_{t}({\bm{x}}_{t})roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) under the optimal (unknown) model. For approximate models, we provide a formula for the bias in the log-density estimate. The evaluation of the log-density estimate comes at no additional cost, and can be used with any pretrained model without further tuning. Then, we introduce a theoretical mode-tracking process, which finds the exact mode of the denoising distribution p⁢(𝒙0|𝒙t)𝑝conditionalsubscript𝒙0subscript𝒙𝑡p({\bm{x}}_{0}|{\bm{x}}_{t})italic_p ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT | bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) under some technical assumptions, albeit at a high computational cost. We propose an approximative high probability sampler whose images almost always have higher likelihood than regular samples. We leverage these findings to give insights into the diffusion model probability landscape. We find that dramatically higher likelihood samples exist that a regular sampler never returns in practise. We observe that the high density samples tend to be (i) blank images for high noise levels, (ii) cartoon drawings for moderate noise, and (iii) blurry images for low noise (See Figure 1). This is despite the datasets not containing any cartoon drawings. Curiously, on FFHQ-256 we observe 97% correlation between model’s likelihood estimates and the amount of information in the image. We summarize our contributions in Figure 2. Summary of our contributions Augmented SDEs (section 2 + section 3) Exact log⁡pt⁢(xt)subscript𝑝𝑡subscript𝑥𝑡\log p_{t}(x_{t})roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) when ∇log⁡pt⁢(x)∇subscript𝑝𝑡𝑥\nabla\log p_{t}(x)∇ roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_x ) and pTsubscript𝑝𝑇p_{T}italic_p start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT are known subsection 2.2 Estimated log⁡ptSDE⁢(xt)subscriptsuperscript𝑝SDE𝑡subscript𝑥𝑡\log p^{\mathrm{SDE}}_{t}(x_{t})roman_log italic_p start_POSTSUPERSCRIPT roman_SDE end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) for approximate ∇log⁡pt⁢(x)∇subscript𝑝𝑡𝑥\nabla\log p_{t}(x)∇ roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_x ) subsection 2.3 Variational gap log⁡p0SDE⁢(𝒙0)−ELBO⁢(𝒙0)superscriptsubscript𝑝0SDEsubscript𝒙0ELBOsubscript𝒙0\log p_{0}^{\mathrm{SDE}}({\bm{x}}_{0})-\mathrm{ELBO}({\bm{x}}_{0})roman_log italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_SDE end_POSTSUPERSCRIPT ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) - roman_ELBO ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) formula Equation 13 Estimation error analysis subsection 3.2 Mode-tracking (section 4): Evolution of the mode of p⁢(xs|xt)𝑝conditionalsubscript𝑥𝑠subscript𝑥𝑡p(x_{s}|x_{t})italic_p ( italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) Theorem 5 High-Probability ODE approximation 1 General instantaneous change of variables 1 Diffusion probability landscape (section 5): Unrealistic images have the highest likelihoods Figure 1 Blurring an image increases its likelihood Figure 10 Likelihood estimates correlate with PNG size Figure 8 Figure 2: Our contributions."
https://arxiv.org/html/2411.01263v1,Confidence Aware Learning for Reliable Face Anti-spoofing,"Current Face Anti-spoofing (FAS) models tend to make overly confident predictions even when encountering unfamiliar scenarios or unknown presentation attacks, which leads to serious potential risks. To solve this problem, we propose a Confidence Aware Face Anti-spoofing (CA-FAS) model, which is aware of its capability boundary, thus achieving reliable liveness detection within this boundary. To enable the CA-FAS to “know what it doesn’t know”, we propose to estimate its confidence during the prediction of each sample. Specifically, we build Gaussian distributions for both the live faces and the known attacks. The prediction confidence for each sample is subsequently assessed using the Mahalanobis distance between the sample and the Gaussians for the “known data”. We further introduce the Mahalanobis distance-based triplet mining to optimize the parameters of both the model and the constructed Gaussians as a whole. Extensive experiments show that the proposed CA-FAS can effectively recognize samples with low prediction confidence and thus achieve much more reliable performance than other FAS models by filtering out samples that are beyond its reliable range.","Face recognition models are widely applied in security fields such as access control and payment systems nowadays. However, the emergence of various presentation attacks, such as deceiving the system with a video or a printed photo, leads to serious security risks. To defend face recognition models from presentation attacks, face anti-spoofing (FAS) methods are proposed to serve as a protective measure. Existing FAS works have demonstrated excellent detection performance in intra-dataset settings [1, 2, 3, 4, 5, 6, 7]. However, when these FAS models encounter challenging cases from unseen scenarios (unseen domains) or unknown presentation attacks, they usually suffers from a noticeable performance decline. Some studies [8, 9] attempt to identify unknown attacks through one-class classification or by reconstructing live faces. However, these methods perform poorly when encountering live faces under various domains. Figure 1: The relationship between the traditional binary prediction and the proposed confidence aware prediction. In binary prediction, the model uses a prediction boundary to divide the test data. However, due to unseen scenarios or unknown attacks, the differences between test and training feature distributions can lead to misclassifications, which poses serious risks. In contrast, the confidence aware method sets a confidence threshold to filter out samples beyond the reliable range, ensuring that only the samples with high prediction confidence are subjected to the classifier. The low confidence samples are rejected and passed for manual review, reducing the risk of misclassification. Recently, many works aim to enhance the cross-domain generalization of FAS models when the distribution of the target test dataset is inconsistent with the source training data. Domain adaptation (DA) methods are adopted to improve the model’s performance on the target dataset by using a small amount of data from that dataset [10, 11, 12, 13, 14, 15]. However, it is often impossible to know in advance what the unknown presentation attacks are in practical scenarios, making it challenging to obtain data from the target dataset for DA-based methods. Another category of works adopts the domain generalization (DG) methods, in which they simultaneously utilize multiple diverse training datasets to enhance the model’s cross-domain generalization. A commonly used method of most DG-based FAS works is to align the feature space of different domains, i.e., various training datasets, for seeking domain invariant features[16, 17, 18, 19, 20]. Subsequent works resorts to feature disentanglement by removing biased factors in the training datasets, which enhances the model’s generalization capability [21, 22, 23, 24, 25, 26]. Additionally, meta-learning also serves as a crucial approach for achieving FAS domain generalization via simulating domain shifts in the training stage [27, 28, 29, 30]. Although the DG-based methods can exploit the domain invariant feature from multiple datasets, they are still constrained by the limited types of presentation attacks in the training datasets. The models trained on these datasets always have a limited capability boundary and can hardly perform well on all the unknown presentation attacks. After analyzing the shortcomings of existing FAS methods, we find that the generalization ability of liveness detection models is inherently limited due to the constraints of the scenarios and attack types included in the training datasets. However, in this era of rapidly evolving attack techniques, new presentation attack methods will inevitably emerge that exceed the capabilities of current FAS models. As mentioned earlier, FAS models play a crucial role in safeguarding critical security domains where they are employed to protect face recognition systems. Any inaccuracies in the detection can result in serious consequences. Therefore, we propose the importance of reliable face anti-spoofing, which requires the FAS model to be aware of its own capability boundary. The model should operate within its capability boundary and reject the “unknown samples” outside this boundary. Only when the model “knows what it doesn’t know” can security issues caused by overly confident misclassification be avoided. In this work, we propose a Confidence Aware Face Anti-spoofing (CA-FAS) model, which can achieve reliable face anti-spoofing by measuring the prediction confidence of any input sample. Specifically, we build Gaussian distributions in the feature space for the live faces and each of the known presentation attacks, and realize the confidence aware prediction as shown in Figure 1. Each Gaussian represents a reliable range for the feature from a category, i.e., live or one of the known attacks. If an input’s feature is close to a particular Gaussian, we consider the model’s prediction for that sample to be reliable. Conversely, if a sample’s feature is far from any known Gaussian distribution, we consider the model’s prediction confidence for that sample to be low, and the model will refuse to make a prediction on it to avoid misclassification. We apply the Mahalanobis distance to measure the distance between an input feature and a constructed Gaussian, as it considers both the mean and covariance of the distribution and makes a more accurate assessment. During training, we introduce a Mahalanobis distance-based triplet mining to optimize the parameters of both the FAS feature extractor and the constructed Gaussians. The triplet loss simultaneously pulls close the Mahalanobis distance between the feature of each category, i.e., live faces or each known attack, and their corresponding Gaussian, while pushing far the Mahalanobis distance between the Gaussians of different categories. Finally, we design experiments with eight FAS datasets to evaluate the performance of FAS models on unseen scenarios and unknown presentation attacks. The results show that our CA-FAS model can effectively recognize samples beyond its capability boundary and achieves reliable liveness detection under different experimental settings. The main contributions of this work are summarized as follows: • We pull up the importance of reliable face anti-spoofing, which requires the FAS model to recognize its own capability boundary. This helps avoid making overly confident predictions on uncertain samples with unknown scenarios or presentation attacks, thereby mitigating potential security risks. • We propose the Confidence Aware Face Anti-spoofing (CA-FAS) model, which establishes its reliable range in the feature space and evaluates the prediction confidence through Mahalanobis distance. During inference, the proposed model can provide the prediction confidence, which reduces the risk of overly confident misclassification and achieves reliable face anti-spoofing. • We conduct experiments to evaluate the performance of FAS models when faced with unknown scenarios and presentation attacks. The results demonstrate the proposed CA-FAS achieves much more reliable performance by filtering out samples that are beyond its capability boundary."
https://arxiv.org/html/2411.01226v1,MonoPlane: Exploiting Monocular Geometric Cues forGeneralizable 3D Plane Reconstruction,"This paper presents a generalizable 3D plane detection and reconstruction framework named MonoPlane. Unlike previous robust estimator-based works (which require multiple images or RGB-D input) and learning-based works (which suffer from domain shift), MonoPlane combines the best of two worlds and establishes a plane reconstruction pipeline based on monocular geometric cues, resulting in accurate, robust and scalable 3D plane detection and reconstruction in the wild. Specifically, we first leverage large-scale pre-trained neural networks to obtain the depth and surface normals from a single image. These monocular geometric cues are then incorporated into a proximity-guided RANSAC framework to sequentially fit each plane instance. We exploit effective 3D point proximity and model such proximity via a graph within RANSAC to guide the plane fitting from noisy monocular depths, followed by image-level multi-plane joint optimization to improve the consistency among all plane instances. We further design a simple but effective pipeline to extend this single-view solution to sparse-view 3D plane reconstruction. Extensive experiments on a list of datasets demonstrate our superior zero-shot generalizability over baselines, achieving state-of-the-art plane reconstruction performance in a transferring setting. Our code is available at https://github.com/thuzhaowang/MonoPlane.","I INTRODUCTION Reconstructing 3D planes from images is a fundamental task in 3D vision that has wide applications in downstream tasks, such as robotics, mixture reality and interior modeling. The goal of this task is to simultaneously detect planes and estimate plane parameters from single or multiple images. Traditional methods rely on multi-view cues [1], depth sensor inputs [2, 3], or certain heuristic scene assumptions [4], and utilize robust estimators like RANSAC [5] and MRFs [6] optimization to fit plane proposals and label pixels. These methods achieve desirable plane reconstruction results in the presence of depth or multi-view input. However, their performance may drastically degrade when the heuristic assumptions are broken, or encountering low-texture images, motion blurs and noisy or incomplete depth. Learning-based methods have been proposed to reconstruct 3D planes from a single image [7, 8, 9], multi-view images [10, 11, 12] or videos [13]. Thanks to the construction of large-scale benchmark datasets [14, 15], these methods have showcased remarkable performance in segmenting plane instances and regressing plane parameters on particular dataset. To achieve better plane reconstruction, researchers have proposed to leverage more advanced network architectures [7, 16], or utilize geometric primitives such as line segments [9] to assist, or optimize multi-view planes and camera pose in a joint manner [17, 12]. Despite the significant progress, such data-driven methods still require costly groundtruth labeling and more importantly, have shown poor generalization ability onto out-of-distribution data as shown in our experiments. However, transferring across different environments is a crucial capacity in real-world applications such as robot navigation or augmented reality. Recently, several works explore the possibility of achieving generalizable depth or normal estimation from single-view images [18, 19, 20]. These works leverage rich depth-relevant data from different sources such as stereo, RGB-D video, 3D movies, and mix them together to obtain large-scale training datasets. These trained neural networks demonstrate superior zero-shot accuracy on in-the-wild data. Some recent studies have leveraged these pre-trained models to assist in different 3D vision tasks such as neural 3D reconstruction and rendering [21, 22]. Indoor - Matterport [15] Outdoor - Synthia [23] In-the-wild Data [24] Figure 1: Sample results from diverse scenarios. Our method achieves superior generalizable 3D plane detection and reconstruction by incorporating generalizable cues and robust designs. Inspired by these recent successes using pretrained monocular geometric models, we aim to exploit the potential of monocular geometric cues in 3D plane reconstruction. Taking a single RGB image as input, the pretrained networks are employed to predict depth and surface normals. To make fully use of these geometric cues, we revisit the classical RANSAC [5] pipeline, and further propose a monocular cue guided proximity modeling approach upon the graph-cut RANSAC [25]. This specially-designed RANSAC pipeline is able to effectively utilize the monocular geometric information while being robust to handle noisy input. Then, an image-level multi-plane joint optimization is conducted via dense CRFs [26] to encourage more consistent plane masks by combining the RGB color information. In this way, our method, termed MonoPlane, without plane labels and domain-specific training, effectively predicts satisfactory 3D plane reconstruction results from a single RGB image. We further demonstrate that our single-view solution can be easily extended to sparse uncalibrated images. Experimental results across different benchmarks and in-the-wild data (Figure 1) showcase that our proposed framework is accurate, generalizable and scalable. To summarize, our contributions are three-fold: • We explore the use of monocular geometric cues for 3D plane reconstruction, delivering generalizable 3D plane reconstruction from a single or sparse RGB images. • We propose a point-proximity based graph-cut RANSAC algorithm by incorporating monocular geometric cues, which greatly enhanced the robustness on handling noisy input or complex scenes. • We build a generalizable 3D plane reconstruction system for both single image and sparse-view images without any plane groundtruth or domain-specific finetuning. Experiments on various data from diverse environments clearly demonstrate the effectiveness of our system."
https://arxiv.org/html/2411.01225v1,RLE: A Unified Perspective of Data Augmentation for Cross-Spectral Re-identification,"This paper makes a step towards modeling the modality discrepancy in the cross-spectral re-identification task. Based on the Lambertain model, we observe that the non-linear modality discrepancy mainly comes from diverse linear transformations acting on the surface of different materials. From this view, we unify all data augmentation strategies for cross-spectral re-identification by mimicking such local linear transformations and categorizing them into moderate transformation and radical transformation. By extending the observation, we propose a Random Linear Enhancement (RLE) strategy which includes Moderate Random Linear Enhancement (MRLE) and Radical Random Linear Enhancement (RRLE) to push the boundaries of both types of transformation. Moderate Random Linear Enhancement is designed to provide diverse image transformations that satisfy the original linear correlations under constrained conditions, whereas Radical Random Linear Enhancement seeks to generate local linear transformations directly without relying on external information. The experimental results not only demonstrate the superiority and effectiveness of RLE but also confirm its great potential as a general-purpose data augmentation for cross-spectral re-identification. The code is available at https://github.com/stone96123/RLE.","Identity recognition has attracted intensive attention in the last few years due to its wide applications in surveillance systems [43, 26, 9, 29]. Since silicon-based digital cameras are naturally sensitive to near-infrared (NIR), most cameras provide infrared (IR) images instead of visible (VIS) images for better visual quality under poor illumination conditions. In practice, this puts the re-identification (Re-ID) problem in a cross-spectral setting and requires the approaches to properly handle both the intra-class variance and the more significant modality discrepancies between cross-spectral images [32, 37, 10]. Encouraged by the great success of single-modality re-identification, substantial research efforts in cross-spectral re-identification attempt to transform the cross-spectral re-identification challenge into a single-modality learning task. To achieve this goal, previous efforts utilize DNN-based image processing such as Generative Adversarial Networks (GANs) [11], to construct the translation from one spectrum to another. These methods [30, 31] generally provide good visual effects and adjustability. However, the limited visual quality of the generated images and the lack of large-scale databases providing cross-spectral image pairs make GAN training challenging, thus limiting the performance of these methods. Another mainstream strategy focuses on the channel difference between infrared images [42, 44]. Methods such as grayscale transformation and random channel selection attempt to use image transformation strategies to mimic the transformation between cross-spectral images, thereby pushing the network to adapt to such a transformation. While these methods make sense and decrease the modality discrepancy, lacking the modeling of cross-spectral transformation, they usually tend to pursue the similarity in human visual perception rather than real cross-spectral transformation. Figure 1: Illustration of the cross-spectral transformation. G refers to the green channel of the visible image. Under the same illumination, the cross-spectral transformation could be described as a linear transformation in a material-similar surface. Still, in the whole image level, the transformation is nonlinear due to the diversity of materials. Since the Re-ID image pairs are not well aligned, we select the cross-spectral image pairs from [1]. In this paper, we attempt to explore the possibility of modeling the multi-spectral transformation to provide more interpretability, and thus further push the boundary of cross-spectral Re-ID approaches. Based on the Lambertian reflection model [14, 7, 28], we find that the illuminations of the same region in VIS and NIR photos should be able to be described using a simple linear model, as long as the region is composed of one consistent material (details are discussed in Sec. 3). This is illustrated in Figure 1. Here, we use paired VIS-NIR images from the dataset in [1]. For the red and yellow regions in the middle of the image, with a simple linear model, we can accurately predict the pixel values of the NIR image based on the VIS image, as long as the region only has one material. Although the linear transformation exists at the pixel level of cross-spectral image pairs, the material’s reflection function determines the linear factor. It means that the linear factor is inconsistent across different surfaces, resulting in an image-level non-linear transformation. In Section 3, we analyze and visualize the result to confirm whether the different linear factors on different surfaces are the main culprit that induces the modality discrepancy in the cross-spectral images. It is interesting to find that the modality discrepancy occurs when using variable linear factors among different patches in the image. The above observation provides us with a fresh perspective on the cross-spectral Re-ID task. Empirically, adopting observation in image generation seems to be the most intuitive way. As long as we are able to identify regions’ materials with their visible or infrared input and calculate the linear coefficients to transform the input image from one spectrum to another, the modality discrepancy would be easy to bridge. Unfortunately, the correlation between visible or infrared input and regional materials is quite limited, which also confines generative strategies in this task to a clear upper bound. Besides exposing the bottleneck of the generative strategy, the observation also provides a unified perspective to rethink the augmentation strategies within this topic. From this perspective, we discover that data augmentation for cross-spectral re-identification is formed to achieve non-linear transformations with different distinct local linear factors, thus encouraging the network to be robust to such a transformation. Therefore, under this view, we can easily categorize all the data augmentation strategies designed for cross-spectral re-identification into moderate transformation and radical transformation based on the extent of changes to images. We assign moderate transformation as a strategy that can still keep the original linear correlation after the transformation. Generally, achieving moderate transformation may require precise material labels on each pixel. However, with benefits from the diversity of different channels in visible images, we can obtain a moderate transformation by a linear calculation based on the original image channels. Methods like channel exchange and grayscale transformation are both special cases under moderate transformation. Within the unified formulation of moderate transformation, in this paper, we further provide a more general moderate transformation as Moderate Random Linear Enhancement (MRLE), which aims to use an unfixed mixing of different channels to provide more diverse augmentation results. In contrast to moderate transformations, radical transformations attempt to apply linear transformations to randomly selected local areas. Compared to moderate transformations, which have a limited transformation space and are only effective on multi-channel visible images, radical transformations can produce a more diverse range of results even on single-channel infrared images. However, due to the lack of constraints, these transformations often introduce additional noise into the original image. Methods such as random erasing [49] and channel random erasing [42] can be considered special cases of radical transformation where the linear factor is set to 0. Similarly, based on the above perspective, we also provide a Radical Random Linear Enhancement strategy, that yields competitive augmentation results by directly applying linear transformations to randomly selected local areas. In summary, our contributions are threefold: • As an effort to model the transformation behind the modality discrepancy in the cross-spectral Re-ID task, we discover that the cross-spectral modality discrepancy mainly comes from different local linear transformations caused by the diversity of materials. Based on this observation, we further categorize the cross-spectral data augmentation strategies into moderate and radical transformations under a unified perspective. • By extending the observation, we propose a Random Linear Enhancement (RLE) strategy, which includes Moderate Random Linear Enhancement (MRLE) and Radical Random Linear Enhancement (RRLE). The RLE effectively takes advantage of the aforementioned unified perspective and embeds it in a controllable linear transformation. • Extensive experiments on cross-spectral re-identification datasets demonstrate the effectiveness and superior ability of the proposed RLE, which can boost performance under various scenarios."
https://arxiv.org/html/2411.01218v1,Real-Time Spatio-Temporal Reconstruction of Dynamic Endoscopic Scenes with 4D Gaussian Splatting,"Dynamic scene reconstruction is essential in robotic minimally invasive surgery, providing crucial spatial information that enhances surgical precision and outcomes. However, existing methods struggle to address the complex, temporally dynamic nature of endoscopic scenes. This paper presents ST-Endo4DGS, a novel framework that models the spatio-temporal volume of dynamic endoscopic scenes using unbiased 4D Gaussian Splatting (4DGS) primitives, parameterized by anisotropic ellipses with flexible 4D rotations. This approach enables precise representation of deformable tissue dynamics, capturing intricate spatial and temporal correlations in real time. Additionally, we extend spherindrical harmonics to represent time-evolving appearance, achieving realistic adaptations to lighting and view changes. A new endoscopic normal alignment constraint (ENAC) further enhances geometric fidelity by aligning rendered normals with depth-derived geometry. Extensive evaluations show that ST-Endo4DGS outperforms existing methods in both visual quality and real-time performance, establishing a new state-of-the-art in dynamic scene reconstruction for endoscopic surgery.","Accurately modeling dynamic scenes from 2D images and rendering lifelike novel views in real-time are critical capabilities in surgical robotics, particularly for endoscopic surgery. Dynamic scene reconstruction significantly enhances the accuracy of robotic-assisted minimally invasive procedures by accurately restoring the spatial and physical properties of human tissues [1]. This, in turn, contributes to better patient outcomes by supporting higher precision in surgical interventions [2, 3]. Advanced techniques such as Neural Radiance Fields (NeRF) [4] and 3D Gaussian Splatting (3DGS) [5] have recently shown remarkable potential in rendering highly realistic static scenes. EndoNeRF was the first to apply NeRF in endoscopic scenes, modeling tissue deformation and canonical density to enhance dynamic scene reconstruction accuracy [1]. EndoSurf advanced this by introducing signed distance functions to impose self-consistency constraints on neural fields, improving geometric fidelity, though at the cost of extended training times [6]. SurgicalGS achieves computational efficiency through explicit Gaussian primitives and splatting-based rendering, yet lacking temporal resolution for complex surgical dynamics [7]. However, adapting these methodologies to dynamic endoscopic scene reconstruction presents substantial challenges. Fig. 1: Diagram of the ST-Endo4DGS framework, demonstrating the transition from 4D Gaussians to 2D projections, effectively capturing dynamic endoscopic scenes. Temporal and spatial correlations are preserved through marginalization, with ENAC ensuring alignment of rendered and depth-derived geometry for enhanced fidelity. The continuous motion and temporal dynamics in surgical settings complicate the adaptation of conventional methods to endoscopic applications. As endoscopic procedures capture dynamic scenes through monocular video streams [8], it is challenging to implement traditional approaches that train static representations per frame and merge them into a dynamic model. Endo-4DGS [9] tackled real-time dynamics using a 4D Gaussian deformation field with pseudo-depth data from Depth-Anything [10], enabling real-time reconstruction without ground-truth depth. LGS further optimized 4DGS for surgical settings by incorporating deformation-aware pruning and feature compression, reducing memory and computational costs while maintaining quality [11]. Additionally, EndoGaussian introduced holistic Gaussian initialization to enhance tracking stability and computational efficiency [12]. EndoGS integrates depth-guided supervision, spatio-temporal weighting, and surface-aligned regularization to improve geometric fidelity and robustness against occlusions [13]. Despite these advancements, accurately capturing the spatio-temporal structure of dynamic scenes and managing scale and deformation modeling remain challenging, as many methods still experience local overfitting and limited alignment with true surface geometry. To address these issues, this study redefines the task by approximating an endoscopic scene’s underlying spatio-temporal 4D volume through a set of 4D Gaussian functions, offering a flexible and unified approach to representing complex endoscopic dynamics [14]. By enabling 4D rotations, these Gaussians adapt to the 4D manifold, effectively capturing the intrinsic motion in endoscopic scenes. Furthermore, we extend endoscopic spherindrical harmonics, a generalization of 4D spherical harmonics [14, 15], to simulate the time-evolved appearance of dynamic endoscopic scenes, allowing for realistic lighting and appearance changes over time. Moreover, inspired by unbiased depth rendering techniques [16] and normal consistency [17], which are effective in preserving surface detail and reducing local overfitting in complex scenarios, we introduce an endoscopic normal alignment constraint (ENAC) to enhance alignment with true surface geometry, thereby improving the geometric fidelity of dynamic endoscopic scenes. Therefore, the primary contributions are: (i) We propose ST-Endo4DGS, a novel framework that utilizes unbiased 4DGS for precise spatio-temporal modeling of dynamic endoscopic scenes, effectively capturing the complex spatial and temporal dynamics of deformable tissues. (ii) We extend endoscopic spherindrical harmonics to model time-evolved appearance, capturing complex lighting and appearance changes in endoscopic scenes with improved temporal fidelity and interpretability. (iii) The endoscopic normal alignment constraint (ENAC) is proposed to enhance alignment between rendered and depth-derived normals, improving the accuracy of monocular depth estimation. (iv) Extensive validation demonstrates that ST-Endo4DGS outperforms existing methods in visual quality and real-time performance."
https://arxiv.org/html/2411.01212v1,Infinite-Resolution Integral Noise Warpingfor Diffusion Models,"Adapting pretrained image-based diffusion models to generate temporally consistent videos has become an impactful generative modeling research direction. Training-free noise-space manipulation has proven to be an effective technique, where the challenge is to preserve the Gaussian white noise distribution while adding in temporal consistency. Recently, Chang et al. (2024) formulated this problem using an integral noise representation with distribution-preserving guarantees, and proposed an upsampling-based algorithm to compute it. However, while their mathematical formulation is advantageous, the algorithm incurs a high computational cost. Through analyzing the limiting-case behavior of their algorithm as the upsampling resolution goes to infinity, we develop an alternative algorithm that, by gathering increments of multiple Brownian bridges, achieves their infinite-resolution accuracy while simultaneously reducing the computational cost by orders of magnitude. We prove and experimentally validate our theoretical claims, and demonstrate our method’s effectiveness in real-world applications. We further show that our method readily extends to the 3-dimensional space.","The success of diffusion models in image generation and editing (Rombach et al., 2022; Nichol et al., 2021; Ho et al., 2020; Zhang et al., 2023a) has spurred significant interest in lifting these capacities to the video domain (Singer et al., 2022; Durrett, 2019; Gupta et al., 2023; Blattmann et al., 2023; Ho et al., 2022; Guo et al., 2024). While training video diffusion models directly on spatiotemporal data is a natural idea, practical concerns such as limited availability of large-scale video data and high computational cost have motivated investigations into training-free alternatives. One such approach is to use pre-trained image models to directly generate video frames, and utilize techniques such as cross-frame attention, feature injection and hierarchical sampling to promote temporal consistency across frames (Ceylan et al., 2023; Zhang et al., 2023b; Khachatryan et al., 2023; Cong et al., 2023). Among these techniques, the controlled initialization of noise has been consistently shown to be an important one (Ceylan et al., 2023; Khachatryan et al., 2023). However, most existing approaches for noise manipulation either compromise the noise Gaussianity (and subsequently introduce a domain gap at inference time), or are restricted to simple manipulations such as filtering and blending which are insufficient for capturing complex temporal correlations. Recently, Chang et al. (2024) proposed a method that both preserves Gaussian white noise distribution and well captures temporal correlations via integral noise warping: each warped noise pixel integrates a continuous noise field over a polygonal deformed pixel region, which is computed by summing subpixels of an upsampled noise image. However, their method’s theoretical soundness and effectiveness are followed by its high-end computational cost in both memory and time, which not only incurs a significant overhead at inference time but also limits its useability in novel applications (Kwak et al., 2024). In this paper, we introduce a new noise-warping algorithm that dramatically cuts down the cost of Chang et al. (2024) while fully retaining its virtues. Our key insight for achieving this lies in that, when adopting an Eulerian perspective (as opposed to the original Lagrangian one), the limiting-case algorithm of Chang et al. (2024) for computing a warped noise pixel reduces to summing over increments from multiple Brownian bridges (Durrett, 2019, Section 8.4). In place of the costly upsampling procedure, sampling the increments of a Brownian bridge can be done efficiently in an autoregressive manner (2). We build upon this to devise the infinite-resolution integral noise warping algorithm (1) which directly resolves noise transport in the continuous space, when given an oracle that returns the overlapping area between a pixel square and a deformed pixel region (Section 2.3). We propose two concrete ways to compute this oracle, leading to a grid-based and a particle-based variant of our method. Similar to Chang et al. (2024), the grid-based variant (Algorithm 2) computes the area by explicitly constructing per-pixel deformed polygons, and is exactly equivalent to the existing approach (Chang et al., 2024) with an infinite upsampling resolution, while running 8.0×8.0\times8.0 × to 19.7×19.7\times19.7 × faster and using 9.22×9.22\times9.22 × less memory111Since the official code of Chang et al. (2024) is not available, performance is compared using our reimplementation in Taichi (Hu et al., 2019), which we find to be faster than as reported in the original paper.. Inspired by hybrid Eulerian-Lagrangian fluid simulation (Brackbill et al., 1988), our novel particle-based variant (Algorithm 3) computes area in a fuzzy manner, which not only offers a further 5.21×5.21\times5.21 × speed-up over our grid-based variant, but is also agnostic to non-injective maps. In real-world scenarios, the particle-based variant shows no compromise in generation quality compared to the grid-based one (see video results), while offering superior robustness, efficiency, simplicity, and extensibility to higher dimensions. In summary, we propose a new noise-warping method to facilitate video generation by lifting image diffusion models. Through analyzing the limiting case of the current state-of-the-art method (Chang et al., 2024) with an infinite upsampling resolution, we derive its continuous-space analogy, which fully retains its distribution-preserving and temporally-coherent properties, while achieving orders-of-magnitude speed-up, warping 1024×1024102410241024\times 10241024 × 1024 noise images in ∼0.045similar-toabsent0.045\sim 0.045∼ 0.045s (grid variant) and ∼0.0086similar-toabsent0.0086\sim 0.0086∼ 0.0086s (particle variant) using a laptop with a Nvidia RTX 3070 Ti GPU. Figure 1: When the image grid deforms, the Lagrangian view tracks a deformed pixel region, while the Eulerian view tracks the undeformed pixel square as it gets partitioned into multiple regions. On the right, we leverage the exchangeability of upsampled subpixels to convert the Lagrangian gathering procedure into scattering noise subpixels to overlapped deformed pixel regions."
https://arxiv.org/html/2411.01208v1,MultiPull: Detailing Signed Distance Functions by Pulling Multi-Level Queries at Multi-Step,"Reconstructing a continuous surface from a raw 3D point cloud is a challenging task. Recent methods usually train neural networks to overfit on single point clouds to infer signed distance functions (SDFs). However, neural networks tend to smooth local details due to the lack of ground truth signed distances or normals, which limits the performance of overfitting-based methods in reconstruction tasks. To resolve this issue, we propose a novel method, named MultiPull, to learn multi-scale implicit fields from raw point clouds by optimizing accurate SDFs from coarse to fine. We achieve this by mapping 3D query points into a set of frequency features, which makes it possible to leverage multi-level features during optimization. Meanwhile, we introduce optimization constraints from the perspective of spatial distance and normal consistency, which play a key role in point cloud reconstruction based on multi-scale optimization strategies. Our experiments on widely used object and scene benchmarks demonstrate that our method outperforms the state-of-the-art methods in surface reconstruction. Project page: https://takeshie.github.io/MultiPull","Reconstructing surfaces from 3D point clouds is an important task in computer vision. It is widely used in various real-world scenarios such as autonomous driving, 3D scanning and other downstream applications. Recently, using neural networks to learn signed distance functions from 3D point clouds has made huge progress [1, 2, 3, 4, 5, 6, 7, 8]. An SDF represents a surface as the zero-level set of a 3D continuous field, and the surface can be further extracted using the marching cubes algorithm [9]. In supervised methods [10, 11, 12, 13], a continuous field is learned using signed distance supervision. Some methods employ multi-level representations [14, 15], such as Fourier layers and level of detail (LOD) [16, 17], to learn detailed geometry. However, these methods require 3D supervision, including ground truth signed distances or point normals, calculated on a watertight manifold. To address this issue, several unsupervised methods [18, 19, 20, 21, 22] were proposed to directly infer an SDF by overfitting neural networks on a single point cloud without requiring ground truth signed distances and point normals. They usually need various strategies, such as geometric constraints [18, 19, 20] and consistency constraints [22, 23], for smoother and more completed signed distance field. However, the raw point cloud is a highly discrete approximation of the surface, learning SDFs directly from the point cloud is often inaccurate and highly ambiguous. This makes it hard for the network to learn accurate SDFs on local details, resulting in over-smooth reconstruction. To address this issue, we propose MultiPull, to learn an accurate SDF with multi-scale frequency features. It enables network to predict SDF from coarse to fine, significantly enhancing the accuracy of the predictions. Furthermore, to optimize the SDF at different scales simultaneously, we introduce constraints on the pulling process. Specifically, given query points sampled around 3D space as input, we use a Fourier transform network to represent them as a set of Fourier features. Next, we design a network that can leverage multi-scale Fourier features to learn an SDF fields from coarse to fine. To optimize the signed distance fields with multi-scale features, we introduce a loss function based on gradient consistency and distance awareness. Compared with Level of Detail (LOD) methods [16, 17, 24], we can optimize the signed distance fields effectively without a need of signed distance supervision, recovering more accurate geometric details. Evaluations on widely used benchmarks show that our method outperforms the state-of-the-art methods. Our contribution can be summarized as follows. • We propose a novel framework that can directly learn SDFs with details from raw point clouds, progressing from coarse to fine. This provides a new perspective for recovering 3D geometry details. • We introduce a multi-level loss function based on gradient consistency and distance awareness, enabling the network to geometry details. • Our method outperforms state-of-the-art methods in surface reconstruction in terms of accuracy under widely used benchmarks. Figure 1: Visualization of the 3D shape reconstruction. In (a), (b) and (c), SDFs are learned from a point cloud by optimizing multi-level query points at multi-step. At each step, we optimize query points at one level with frequency features at this specific level as conditions. This enables the network to progressively recover coarse-to-fine geometry details."
https://arxiv.org/html/2411.01179v1,Hollowed Net for On-Device Personalization of Text-to-Image Diffusion Models,"Recent advancements in text-to-image diffusion models have enabled the personalization of these models to generate custom images from textual prompts. This paper presents an efficient LoRA-based personalization approach for on-device subject-driven generation, where pre-trained diffusion models are fine-tuned with user-specific data on resource-constrained devices. Our method, termed Hollowed Net, enhances memory efficiency during fine-tuning by modifying the architecture of a diffusion U-Net to temporarily remove a fraction of its deep layers, creating a hollowed structure. This approach directly addresses on-device memory constraints and substantially reduces GPU memory requirements for training, in contrast to previous methods that primarily focus on minimizing training steps and reducing the number of parameters to update. Additionally, the personalized Hollowed Net can be transferred back into the original U-Net, enabling inference without additional memory overhead. Quantitative and qualitative analyses demonstrate that our approach not only reduces training memory to levels as low as those required for inference but also maintains or improves personalization performance compared to existing methods.","Recent research on text-to-image (T2I) diffusion models [1, 2], which generate high-resolution images from text prompts, has increasingly focused on personalizing and customizing these generative models effectively [3, 4, 5, 6, 7]. A primary approach, termed subject-driven generation [5], involves fine-tuning pre-trained diffusion models with a few user-specific images to generate varied representations of a subject using simple text prompts. This allows users to create personalized images of specific subjects, such as family, friends, pets, or personal items, with preferred appearances, backgrounds, and styles. Such capabilities enable creative applications including art renditions, property modifications, and accessorization. From a practical standpoint, implementing subject-driven generation on-device offers significant benefits in efficiency and privacy. By operating independently of congested cloud servers or networks, users can generate personalized images anywhere at no additional cost and do not need to compromise their privacy as all data and personal information remain on the device. Despite extensive research aimed at efficiently personalizing diffusion models, limited attention has been paid to memory I/O, a critical bottleneck in on-device learning. Recent studies have mainly explored two strategies: (1) decreasing the number of training steps and (2) reducing the number of updating parameters. The first methods [8, 9, 10, 11, 12] utilize additional large pre-trained models to generate a set of personalized Low-Rank Adaptation (LoRA) parameters [13], text embeddings, or image prompts from a user-specific image. This strategy provides a better initial setup for personalizing the diffusion models, effectively reducing required training steps. Some models [10, 11, 12] even support zero-shot personalization, although they underline that further fine-tuning can enhance personalization quality and address failure cases. Nonetheless, these methods are not viable for environments with severely limited computational resources, as they necessitate additional inference using large pre-trained models (e.g., 2.7B parameters for BLIP-2 in BLIP-Diffusion [12] and 2.5B for apprentice models in SuTI [11]), which are substantially larger than standard diffusion models (e.g., 1B for Stable Diffusion v2 [2]), making their application challenging in on-device settings. The second approach [7, 14], often involving LoRA, aims to reduce the number of updating parameters by limiting updates to specific layers or decomposing weight matrices. However, even with fewer parameters to update, these parameters reside within large pre-trained models, and thus the backward pass through the large models is required to compute gradients. Given limited computational resources, where even simple inference tasks with diffusion models can strain GPU memory, performing backpropagation while keeping the entire diffusion model in GPU memory remains a significant limitation. A promising approach to address these challenges is side-tuning [15, 16, 17, 18], which fine-tunes a smaller auxiliary network rather than directly updating the parameters of a large pre-trained network. This method significantly reduces the heavy memory costs associated with computing backpropagation on the larger network. Particularly for Natural Language Processing (NLP) tasks, Ladder Side Tuning (LST) [18] has proven effective, reducing the memory costs required for fine-tuning large language models (LLMs) by 69 percent. However, applying LST directly to diffusion U-Nets presents significant challenges. Unlike transformer layers in LLMs, which maintain consistent input and output dimensions, diffusion U-Nets have varying spatial dimensions and channels, as well as skip-connections across different blocks. Additionally, the requirements for structural pruning and weight initialization to build side-tuning networks further complicate the rapid adaptability of LST to personalization tasks across different subjects and domains. To this end, we introduce a novel personalization technique called Hollowed Net, which is illustrated in Fig. 1. Based on our observation that deep layers in the middle of diffusion U-Nets play significantly less important roles than the rest of the layers, we propose to fine-tune LoRA parameters for the personalization using Hollowed Net, a layer-pruned U-Net featuring a central hollow, which is constructed by temporarily removing the middle deep layers from the pre-trained diffusion U-Net. By utilizing the symmetrical ""U-shape"" architecture of the diffusion U-Net, we avoid complicated processes of applying structural pruning and weight initialization to build a side network, and neither additional models nor extensive pre-training with large datasets are required. By fine-tuning LoRA parameters using Hollowed Net, we can significantly reduce the memory needed for storing model weights in GPU. Once the LoRA parameters are fine-tuned with Hollowed Net, they can be seamlessly transferred back to the original Diffusion U-Net for inference, without requiring any additional memory beyond the small set of transferred parameters. Our experiments demonstrate that Hollowed Net enables achieving performance that is comparable to or better than the direct fine-tuning with LoRA, while using 26 percent less GPU memory, which is only 11 percent increased GPU memory relative to an inference. To the best of our knowledge, Hollowed Net is the first technique that addresses subject-driven generation in terms of memory efficiency. Our method shows how T2I diffusion models can be fine-tuned under extremely limited computational resources with as low GPU memory as required for inference. Furthermore, it is important to note that our method does not preclude the use of previously described strategies for efficient personalization. Both enhanced parameter-efficient strategies and improved initializations with additional pre-trained models can be integrated with our approach to further increase efficiency according to given resource constraints. Our contributions can be summarized as follows: • We introduce Hollowed Net, a novel personalization technique for T2I diffusion models under limited computational resources. Our method significantly reduces the memory demands on GPU to levels as low as those required for inference, while maintaining a high-fidelity personalization capacity. This demonstrates its potential as a feasible on-device learning solution for resource-constrained devices. • Our method provides a scalable and controllable solution for on-device learning. As this method does not require any additional models or pre-training with large datasets, it is easily scalable to other architectures such as SDXL and Transformers. Moreover, we can simply adjust the fraction of hollowed layers to control the trade-offs between performance and memory requirements, depending on the target application and resources. • Unlike previous side-tuning methods, Hollowed Net does not need to be retained for inference. The LoRA parameters fine-tuned with Hollowed Net can be seamlessly transferred back to its original network, enabling inference with no additional memory cost. Figure 1: The LoRA personalization with Hollowed Net for resource-constrained environments. The input image is from the DreamBooth dataset [5]."
https://arxiv.org/html/2411.01172v1,Covariance-based Space Regularization for Few-shot Class Incremental Learning,"Few-shot Class Incremental Learning (FSCIL) presents a challenging yet realistic scenario, which requires the model to continually learn new classes with limited labeled data (i.e., incremental sessions) while retaining knowledge of previously learned base classes (i.e., base sessions). Due to the limited data in incremental sessions, models are prone to overfitting new classes and suffering catastrophic forgetting of base classes. To tackle these issues, recent advancements resort to prototype-based approaches to constrain the base class distribution and learn discriminative representations of new classes. Despite the progress, the limited data issue still induces ill-divided feature space, leading the model to confuse the new class with old classes or fail to facilitate good separation among new classes. In this paper, we aim to mitigate these issues by directly constraining the span of each class distribution from a covariance perspective. In detail, we propose a simple yet effective covariance constraint loss to force the model to learn each class distribution with the same covariance matrix. In addition, we propose a perturbation approach to perturb the few-shot training samples in the feature space, which encourages the samples to be away from the weighted distribution of other classes. Regarding perturbed samples as new class data, the classifier is forced to establish explicit boundaries between each new class and the existing ones. Our approach is easy to integrate into existing FSCIL approaches to boost performance. Experiments on three benchmarks validate the effectiveness of our approach, achieving a new state-of-the-art performance of FSCIL.","Provided with a substantial amount of stationary data, recent advancements in deep learning have enabled neural networks to excel in classification tasks [17, 9, 35, 30, 31]. However, it is not feasible to directly deploy neural networks into the open world, where the data may emerge in a non-stationary way, such as recognizing new types of diseases or new vehicles in autonomous driving. Simply retraining or fine-tuning the model with new data introduces the well-known catastrophic forgetting problem [24, 7]. To address this challenge, Class Incremental Learning (CIL) has been extensively researched to broaden the application scope of neural networks [52, 33]. By simulating the scenario where disjoint new data appears in incremental sessions, CIL aims to learn new concepts without forgetting old knowledge. The conventional CIL setting assumes the amount of new data is usually sufficient, which may not be realistic as labeling new data can be expensive. To handle this issue, few-shot class incremental learning (FSCIL) has attracted much attention recently, where only a few training samples from new classes are available during incremental sessions [32, 18, 47]. Figure 1: (a) Prototype-based models demonstrate compact representations of old classes, conserving space for new classes, though risking confusion due to mixed class distributions. (b) The proposed approach aims to regularize each class distribution within a fixed span by constraining covariance and to enhance class separation through the learning of perturbed new class data. The framework of FSCIL typically involves two key stages. Initially, the model is trained on a base dataset, where all classes (referred to as base classes or old classes) contain sufficient instances. Subsequently, the model engages in incremental sessions, where it is required to learn new classes with limited samples in each session without access to previous data. After training, the model is evaluated on all previously encountered classes. Due to the scarcity of new data, the model is more vulnerable to overfit new classes and hence suffers catastrophic forgetting of old classes during incremental learning sessions [26, 20, 52, 23]. To overcome both overfitting and catastrophic forgetting issues of FSCIL, recent wisdom resorts to prototype-based models [18, 54, 10, 44]. Specifically, prototype-based models [28, 42] replace the linear classifier with learnable prototypes, aiming to learn the most representative point (e.g., center point) of each class. Recent FSCIL works involve two stages, i.e., base session learning and incremental session learning. In the base learning stage, prototype-based models are utilized to learn compact representations of base classes [25, 10, 44]. During the incremental sessions, the feature extractor is frozen, and new prototypes are computed using the mean of each new class. The frozen feature extractor can effectively alleviate the catastrophic forgetting problem, and the prototype classifier can mitigate the overfitting problem. However, as shown in Fig. 1(a), in the fixed feature space, the estimated new class prototypes may lie very close to base class distributions due to the scarcity of the new data. Thus, class distributions can be ill-located during incremental sessions, i.e., new class distribution may lie close to old classes or mixed with other new class distributions [44, 38]. This dilemma may then lead the model to confuse the class distributions in the feature space. Besides, the computed class prototypes can be affected by data noise, exacerbating the confusion dilemma. To resolve this dilemma, we argue that each class should occupy the same amount of feature space. Motivated by this, we dynamically relocate each new class during incremental sessions to prevent overlapping. Without loss of generality, we assume each class follows the Gaussian distribution, where the mean controls the location and the covariance controls the scope of each feature distribution. To ensure each class takes up the same amount of feature space, we first regularize the covariance of each class in the base session. However, the direct optimization of the distribution-related statistics during training is often inefficient, as it requires model inferences throughout the entire dataset. Inspired by previous works in variational inference [16, 15, 48], we adopt a similar approach to estimate and regularize the class distributions efficiently. Specifically, we derive an evidence lower bound for classification and distribution learning. Maximizing the lower bound equals to maximize the confidence of prediction and minimize the KL divergence between the estimated distribution and the prior distribution. We set the fixed covariance of the prior distribution to be equal to all classes, which formulates the KL divergence to a covariance constraint loss, encouraging each class distribution to have the same covariance during training. The covariance constraint loss serves as an upper bound of KL divergence, which leads to a tighter evidence lower bound. Furthermore, to reallocate the feature space for the few-shot new classes during incremental sessions, we propose a perturbation approach to expand the distributions of few-shot new classes by generating perturbed samples for new classes, then pushing these perturbed samples away from semantically similar classes. In detail, we introduce a learnable prior distribution for each few-shot sample based on semantic similarity. We first obtain each training sample’s softmax score towards other classes, then we use the weighted mean and the fixed covariance as a prior distribution. We adopt one linear layer to estimate the mean and variance of the new class distribution. The estimated distribution is supervised by minimizing the KL divergence between the estimated distribution and the prior. Next, we multiply the predicted variance with the extracted features and add the predicted mean to create perturbed samples, as shown in Fig. 1(b). The perturbed samples are treated as new training samples, learning which pushes new classes away from the overlapping distributions. In this manner, the new class distribution is expanded by assigning the fixed variance. Learning semantic-guided perturbed samples also facilitates better separation between classes. Our approach is easy to implement and can be integrated with other approaches. We choose several recent state-of-the-art FSCIL models as baselines [49, 29] and apply the proposed approach to these methods. Extensive experiments on three benchmarks validate the effectiveness of our approach. Our main contributions can be summarized as follows: (1) We propose a covariance constraint loss (CCL) to regularize the class distributions, constraining the span of each distribution within the same range.(2) We propose a semantic guided perturbation approach to perturb the few-shot new data, aiming to learn extensive and discriminative new class distributions. (3) Our proposed method is easy to incorporate into current FSCIL models to boost their performance. Experimental results on the FSCIL benchmark datasets validate the effectiveness of our approach."
https://arxiv.org/html/2411.01171v1,Fast and Memory-Efficient Video Diffusion Using Streamlined Inference,"The rapid progress in artificial intelligence-generated content (AIGC), especially with diffusion models, has significantly advanced development of high-quality video generation. However, current video diffusion models exhibit demanding computational requirements and high peak memory usage, especially for generating longer and higher-resolution videos. These limitations greatly hinder the practical application of video diffusion models on standard hardware platforms. To tackle this issue, we present a novel, training-free framework named Streamlined Inference, which leverages the temporal and spatial properties of video diffusion models. Our approach integrates three core components: Feature Slicer, Operator Grouping, and Step Rehash. Specifically, Feature Slicer effectively partitions input features into sub-features and Operator Grouping processes each sub-feature with a group of consecutive operators, resulting in significant memory reduction without sacrificing the quality or speed. Step Rehash further exploits the similarity between adjacent steps in diffusion, and accelerates inference through skipping unnecessary steps. Extensive experiments demonstrate that our approach significantly reduces peak memory and computational overhead, making it feasible to generate high-quality videos on a single consumer GPU (e.g., reducing peak memory of AnimateDiff from 42GB to 11GB, featuring faster inference on 2080Ti)111Code available at: https://github.com/wuyushuwys/FMEDiffusion.","Recent years have witnessed continual progress and advancements in artificial intelligence-generated content (AIGC). Among them, diffusion models allow artists and amateurs to create visual content with text prompts, advancing the development of image and video generation in both academia and industry. For video diffusion models, the latest works such as SVD-XT [3], Gen2 [29], Pika [20], and notably the more advanced Sora [28], demonstrate impressive capabilities in producing visually striking and artistically effective videos. Despite their great performance, video diffusion models also exhibit high computational requirements and substantial peak memory, particularly when generating longer videos with higher resolutions. For instance, SVD-XT generates 25 frames simultaneously with a resolution of 576×10245761024576\times 1024576 × 1024, while Sora expands these capabilities by supporting the generation of longer videos (over a minute) at a higher resolution of 1080×1920108019201080\times 19201080 × 1920. Given the trends of generating longer videos with higher quality, the escalating memory and computation demands have impeded practical applications of these large-scale video diffusion models on various platforms. Existing model compression methods to reduce peak memory and latency, such as weight pruning [38, 12, 35, 6, 48, 23, 42, 44, 47], quantization [37, 22, 32], and distillation [18, 10, 43, 39], typically require substantial retraining or fine-tuning of the compressed model to recover performance. This process is costly, time-consuming, and may raise data privacy concerns. Applying these methods in zero-shot avoids the expensive retraining, but leads to severe performance degradation. Furthermore, the variety and complexity of video diffusion architectures further complicate the model optimization. Therefore, it is challenging yet crucial to develop an effective and efficient video diffusion framework with reduced computations, smaller peak memory and less data (no re-training) requirements for its wide applications. To address the above challenges, we first identify the sources of the high computation and memory cost, which scale up with the iterative denoising process and the simultaneous processing of multiple frames. We further observe that the feature maps of certain layers may exhibit high similarity between multiple consecutive denoising steps due to the temporal property of videos, enabling further optimizations for acceleration. Based on that, we propose a training-free framework named Streamlined Inference, by leveraging the temporal and spatial characteristics of video diffusion models to effectively reduce peak memory and computational demands. Our framework contains three core components: Feature Slicer, Operator Grouping, and Step Rehash, which work together closely and comprehensively with different focuses on peak memory reduction or inference acceleration. Our Feature Slicer performs lossless feature slicing in both temporal and spatial layers, raising the possibility of peak memory reduction through processing smaller features. However, the feature slicer alone is not able to decrease peak memory as we still need to store all intermediate results of one layer for all sliced features to form a complete intermediate feature map for the next layer. To reduce peak memory practically, we further propose Operator Grouping to group homogeneous and consecutive operators in the computational graph. Within each operator group, the intermediate result of one sliced feature can be directly sent to the next operator/layer without waiting for aggregation with all other intermediate results, achieving the full potential of Feature Slicer to reduce the peak memory. Furthermore, a pipeline technique is proposed to accelerate the computations of the same operator group for multiple sliced feature inputs, with improved parallelism. Figure 2: Comparison on Animatediff and SVD inference using our Streamlined Inference. Memory requirement is crucial as “Out of Memory” errors prevent the GPU from performing inference. Moreover, observing the high similarity of certain features between multiple consecutive denoising steps, we propose Step Rehash to reuse the generated features for a few following steps due to their high similarity, skipping the exact expansive and repetitive generation of similar features and thereby accelerating the video diffusion significantly. With this framework, we can generate high-quality videos in a fast and memory-efficient manner on a single consumer GPU, as shown in Fig. 2. For example, the peak memory of AnimateDiff [11] can be reduced significantly from 41.7GB to 11GB, featuring inference on a typical consumer GPU 2080Ti. We summarize our contributions as follows: • We propose a novel training-free framework that can significantly reduce the peak memory and computation cost for the inference of video diffusion models by leveraging the spatial and temporal characteristics of video diffusion models. • Our approach can be seamlessly integrated into existing video diffusion models. Our extensive experiments on SVD, SVD-XT, and AnimateDiff demonstrate our effectiveness to reduce peak memory and accelerate inference without sacrificing quality. • Our approach offers a new research perspective for fast and memory-efficient video diffusion, enabling the generation of higher quality and longer videos on consumer-grade GPUs."
https://arxiv.org/html/2411.01153v1,Designing a Robust Radiology Report Generation System,"Recent advances in deep learning have enabled researchers to explore tasks at the intersection of computer vision and natural language processing, such as image captioning, visual question answering, visual dialogue, and visual language navigation. Taking inspiration from image captioning, the task of radiology report generation aims at automatically generating radiology reports by having a comprehensive understanding of medical images. However, automatically generating radiology reports from medical images is a challenging task due to the complexity, diversity, and nature of medical images. In this paper, we outline the design of a robust radiology report generation system by integrating different modules and highlighting best practices drawing upon lessons from our past work and also from relevant studies in the literature. We also discuss the impact of integrating different components to form a single integrated system. We believe that these best practices, when implemented, could improve automatic radiology report generation, augment radiologists in decision making, and expedite diagnostic workflow, in turn improve healthcare and save human lives.","With the advances in deep neural network architectures, availability of large-scale annotated datasets, and rise of graphics processing units (GPUs), the field of deep learning (Goodfellow et al., 2016) has shown remarkable performance in areas such as computer vision, natural language processing, and speech processing. Although research has been advancing in standalone fields for many decades, there has been rise in interest in multimodal learning (Liang et al., 2024; Singh, 2018), making models capable of processing information from different modalities, including images, videos, speech, and text. Taking inspiration from image captioning (Vinyals et al., 2015), the task of generating a description of an image, radiology report generation is a multimodal task that aims to transform radiology images into reports. In radiology practice, radiologists routinely examine medical images and conclude their findings in the form of radiology reports. The radiology report indicates what abnormalities are present as well as highlights what is normal in the medical image. In Figure 1, we show a sample chest X-ray and its corresponding radiology report from the Indiana University Chest X-ray Collection (IU-CXR) (Demner-Fushman et al., 2016). The radiology report has a structure with sections, namely, Comparison, Indication, Findings, and Impression. The task of examining medical images and writing radiology reports is tedious, time-consuming, subjective, and error-prone. With increasing population and improving imaging techniques, the demand for medical scans has been on rise. Automated radiology report generation are assistive systems that take as input the medical images and generates a textual radiology report that describes the radiological findings and interpretation. The radiology report generation system offer the potential to improve radiology practice by automating the repetitive process of radiology report drafting, identifying possible medical conditions, and reducing diagnostic errors. These systems when implemented in real clinical settings have the potential to expedite clinical workflow by triaging patients depending upon the level of urgency, assisting radiologists by providing “second opinion”, and highlighting relevant regions of interest in the images, potentially saving human lives. Comparison: None. Indication: Chest pain, feels out of it. Findings: The Cardiomediastinal silhouette and pulmonary vasculature are wining normal limits in size. The lungs are clear of focal airspace disease, pneumothorax, or pleural effusion. There are no acute bony findings. Impression: No acute cardiopulmonary findings. Figure 1: An example of a chest X-ray and its corresponding radiology report in the IU-CXR dataset (Demner-Fushman et al., 2016)."
https://arxiv.org/html/2411.01139v1,HIP: Hierarchical Point Modeling and Pre-training for VisualInformation Extraction,"End-to-end visual information extraction (VIE) aims at integrating the hierarchical subtasks of VIE, including text spotting, word grouping, and entity labeling, into a unified framework. Dealing with the gaps among the three subtasks plays a pivotal role in designing an effective VIE model. OCR-dependent methods heavily rely on offline OCR engines and inevitably suffer from OCR errors, while OCR-free methods, particularly those employing a black-box model, might produce outputs that lack interpretability or contain hallucinated content. Inspired by CenterNet, DeepSolo, and ESP, we propose HIP, which models entities as HIerarchical Points to better conform to the hierarchical nature of the end-to-end VIE task. Specifically, such hierarchical points can be flexibly encoded and subsequently decoded into desired text transcripts, centers of various regions, and categories of entities. Furthermore, we devise corresponding hierarchical pre-training strategies, categorized as image reconstruction, layout learning, and language enhancement, to reinforce the cross-modality representation of the hierarchical encoders. Quantitative experiments on public benchmarks demonstrate that HIP outperforms previous state-of-the-art methods, while qualitative results show its excellent interpretability.","Visual information extraction (VIE) from scanned documents and captured photos has gained increasing attention because of its wide range of applications in various domains. Typical applications usually involve receipts of shopping, business cards, product manuals, and bills of entry (Yang et al. 2023). While VIE greatly frees efforts from manually processing a great number of documents, it is accompanied by stringent requirements and a low tolerance for errors. The latest research trend in VIE focuses on closing the gap between stages in the pipeline: namely text spotting (Huang et al. 2022a; Ye et al. 2023), word grouping (Yu et al. 2023), and entity labeling (Zhang et al. 2020; Xu et al. 2020; Yang et al. 2023). These three tasks are mutually interdependent: incorrect text spotting results in erroneous entity values, while inaccurate word grouping leads to imprecise entity boundaries. OCR-dependent methods, such as LayoutLM (Xu et al. 2020) and its variants (Xu et al. 2021b; Huang et al. 2022b; Xu et al. 2021a; Gu et al. 2022), rely heavily on the OCR engines and inevitably suffer from OCR errors. OCR-free methods (Kim et al. 2021), which usually employ a black-box model, might generate outputs that lack interpretability or may contain hallucinated content. Figure 1: Illustration of the proposed HIP framework and the idea of hierarchical points. VIE is decomposed into three successive tasks: word spotting, word grouping, and entity labeling, each of which is accompanied by two pre-training strategies to learn visual, geometric, and semantic clues. To enable early mitigation of OCR errors, joint learning methods incorporate OCR as an integral part, and they allow for synergistic optimization from text detection to information extraction. TRIE (Zhang et al. 2020) and VIES (Wang et al. 2021a) both designed networks for training multiple tasks simultaneously. They both explicitly classified multi-modal information to get final entities without any general visual-language pre-training, which may lead to weak linguistic representation. StrucTexTv2 (Yu et al. 2023) proposes a stronger representation for document understanding, but it requires additional decoding for OCR and VIE. In this paper, we propose HIP, a joint learning framework for visual information extraction by hierarchically modeling entities as character-level, word-level, and entity-level points. As shown in Fig.1, we first represent each character sequence as ordered points. After the decoding stage of the text spotter, we obtain the requisite contents and locations at the word level. Then, we take the central point of each word as a word-level representation. By calculating the average of all the central points within an entity, we derive an entity-level point that serves as the representation of the entity. Compared to other text representations (Liao et al. 2020a; Wang et al. 2020), the point representation serves as a more flexible intermediary (Duan et al. 2019; Ye et al. 2023), whether it is used for text spotting or word grouping. To enhance the cross-modality representation of the hierarchical encoders, we propose corresponding hierarchical pre-training strategies: (1) Regarding the spotting task, char-level and word-level mask image modeling tasks (corresponding to CMIM and WMIM) are introduced to reconstruct character and word regions of images, respectively. CMIM and WMIM effectively enhance the model’s ability to accurately recognize individual characters and grasp the semantic context of the text. (2) For the grouping task, bottom-up (Word to Block, WTB) and top-down (Entity Detection, ETD) tasks are devised to encourage the model to fully learn layout information. (3) In the entity labeling stage, mask language modeling (MLM) and reading order reasoning (ROR) tasks are employed to strengthen the model’s linguistic capabilities. Although the previous work ESP (Yang et al. 2023) represents entities as points, there is only one entity granularity in ESP, and it is unable to perform token-level recognition and more flexible entity boundary distinction. In contrast, our method yields better recognition results and a more flexible entity granularity. Ultimately, we achieved state-of-the-art (SOTA) performance on the FUNSD dataset, surpassing StrucTextv2 (Yu et al. 2023) by over 5.2%, which is a new record. On CORD and SROIE, our method also obtains SOTA performance in terms of end-to-end F-score. The major contributions of HIP are summarized as follows: - We propose HIP, a joint learning framework for VIE. HIP models entities as hierarchical points to better fit the hierarchical nature of the VIE task. - We devise hierarchical pre-training strategies, categorized as image reconstruction, layout learning, and language enhancement, which facilitate the model to better learn complex visual, geometric and semantic clues. - Experiments on typical VIE benchmarks demonstrate the effectiveness and interpretability of the proposed HIP."
https://arxiv.org/html/2411.01123v1,X-Drive: Cross-modality consistent multi-sensor data synthesis for driving scenarios,"Recent advancements have exploited diffusion models for the synthesis of either LiDAR point clouds or camera image data in driving scenarios. Despite their success in modeling single-modality data marginal distribution, there is an under-exploration in the mutual reliance between different modalities to describe complex driving scenes. To fill in this gap, we propose a novel framework, X-Drive, to model the joint distribution of point clouds and multi-view images via a dual-branch latent diffusion model architecture. Considering the distinct geometrical spaces of the two modalities, X-Drive conditions the synthesis of each modality on the corresponding local regions from the other modality, ensuring better alignment and realism. To further handle the spatial ambiguity during denoising, we design the cross-modality condition module based on epipolar lines to adaptively learn the cross-modality local correspondence. Besides, X-Drive allows for controllable generation through multi-level input conditions, including text, bounding box, image, and point clouds. Extensive results demonstrate the high-fidelity synthetic results of X-Drive for both point clouds and multi-view images, adhering to input conditions while ensuring reliable cross-modality consistency. Our code will be made publicly available at https://github.com/yichen928/X-Drive.","Autonomous driving vehicles perceive the world with multiple sensors of different kinds, where LiDAR and cameras play crucial roles by capturing point clouds and multi-view images. They provide complementary geometric measurements and semantic information about the surrounding environment, significantly benefiting tasks such as object detection (Liu et al., 2023b; Xie et al., 2023), motion planning (Sobh et al., 2018), scene reconstruction (Huang et al., 2024; Zhou et al., 2024), and self-supervised representation learning (Yang et al., 2024a; Xie et al., 2024). However, these advancements hinge on access to large amounts of aligned multi-modality data, specifically well-calibrated LiDAR and multi-view camera inputs that describe the same scene. Scaling up the collection of such high-quality multi-modality data is costly and a non-trivial effort. The high-quality sensors are expensive, and the calibration process demands intensive human efforts. Additionally, real-world driving data suffers from the severe long-tailed distribution problem, making it an obstacle for corner case collection such as in extreme weather conditions. This raises a natural question: Can we use a controllable way to synthesize aligned multi-modality data? Given the success in other fields (Rombach et al., 2022; Blattmann et al., 2023; Liu et al., 2023a), generative models offer a promising solution. Current research focuses either on synthesizing point clouds (Zyrianov et al., 2022; Hu et al., 2024a; Ran et al., 2024) or multi-view images (Gao et al., 2023; Wang et al., 2023; Wen et al., 2024), with limited attention paid to generating multi-modality data. A simple combination of these single-modality algorithms results in serious cross-modality mismatches in the synthetic scenes (Fig. 1(a)). Such inconsistencies create ambiguities and even contradictory inputs or supervision signals, thus hindering the performance of downstream tasks. Cross-modality consistency serves as the key desiderata of multi-modality data generation. However, there are several challenges in the generation of consistent LiDAR and camera data. Firstly, synthetic point clouds and multi-view images must be spatially aligned in all the local regions since they describe the same driving scene, i.e. the shapes and layouts of both foregrounds and backgrounds must be matched. Secondly, unlike those 2D pixel-level tasks (Zhang et al., 2023), point clouds and multi-view images have distinct geometrical spaces and data formats. Multi-view images are represented by RGB values in camera perspective views, while point clouds are XYZ coordinates in the 3D space. Thirdly, the 3D spatial information is ambiguous during generation for both point clouds and multi-view images without reliable point location or pixel depth in the denoising process. In this paper, we fill in this gap by proposing X-Drive, a novel framework for the joint generation of LiDAR and camera data, as demonstrated in Fig. 1(b). We design a dual-branch architecture with two latent diffusion models separately dedicated to the synthesis of point clouds and multi-view images, while a key cross-modality condition module enhances cross-modality consistency between them. To model the joint distribution and ensure local spatial alignment, we perform the cross-modality conditions locally with only corresponding regions from the other modality considered. An explicit transform bridges the two different geometrical spaces, converting the cross-modality condition to match each other’s noisy latent space. To handle the positional ambiguity, we resort to a 3D-aware design based on epipolar lines on range images and multi-view images, allowing the cross-attention module to adaptively determine the cross-modality correspondence without explicit 3D positions. In consequence, X-Drive is able to exploit existing single-modality data as conditions to seamlessly generate data in the other modality. Additionally, X-Drive enhances controllability by introducing 3D bounding boxes for geometrical layout control and text prompts for attribute control (e.g. weather and lighting), enabling more flexible, fine-grained, and precise control over both modalities. Extensive experiments demonstrate the great ability of X-Drive in generating realistic multi-modality sensor data. It notably outperforms previous specialized single-modality algorithms in the quality of both synthetic point clouds and multi-view images. More importantly, for the first time, it demonstrates reliable cross-modality consistency in synthetic scenes with comprehensive and flexible conditions. Our contributions are summarized as follows. • We introduce X-Drive, a dual-branch multi-modality latent diffusion framework that, for the first time, enables controllable and reliable synthesis of aligned LiDAR and multi-view camera data. • Our cross-modality epipolar condition module bridges the geometrical gap under spatial ambiguity between point clouds and multi-view images, significantly enhancing modality consistency. • Extensive experimental results demonstrate the effectiveness of X-Drive, with notable MMD (for point cloud generation) and FID (for multi-view generation) improvements, establishing a new state-of-the-art for remarkable cross-modality consistency."
https://arxiv.org/html/2411.01122v1,OnlineTAS: An Online Baseline for TemporalAction Segmentation,"Temporal context plays a significant role in temporal action segmentation. In an offline setting, the context is typically captured by the segmentation network after observing the entire sequence. However, capturing and using such context information in an online setting remains an under-explored problem. This work presents the an online framework for temporal action segmentation. At the core of the framework is an adaptive memory designed to accommodate dynamic changes in context over time, alongside a feature augmentation module that enhances the frames with the memory. In addition, we propose a post-processing approach to mitigate the severe over-segmentation in the online setting. On three common segmentation benchmarks, our approach achieves state-of-the-art performance.","This work addresses online temporal action segmentation (TAS) of untrimmed videos. Such videos typically feature procedural activities consisting of multiple actions or steps in a loose temporal sequence to achieve a goal survey . For example, “making coffee” has actions: ‘take cup’, ‘pour coffee’, ‘pour water’, ‘pour milk’, ‘pour sugar’ and ‘stir coffee’. Standard TAS models ms-tcn ; yi2021asformer ; liu2023diffusion ; singhania2023c2f are offline and segment-only videos of complete procedural activities. An online TAS model, in contrast, segments only up to the current time point and does not have access to the entire video and, therefore, the entire activity. Online TAS faces challenges similar to other online tasks lstr ; ctvis in establishing a scalable network that can retain useful information from an ever-increasing volume of data and facilitate effective retrieval when required. Additionally, over-segmentation is a common issue for offline TAS, where the segmentation model divides an action into many discontinuous sub-segments, leading to fragmented outputs. This issue is exacerbated in the online setting, as partial data at the onset of an action may lead to erratic predictions and increased over-segmentation. Most relevant to our task is online temporal action detection (TAD) xu2019temporal . Online TAD aims to identify whether an action is taking place and the action category. TAD targets datasets like THUMOS thumos , TVSeries detec2 , and HACS Segment hacs . Among these, 90.8% videos of THUMOS thumos feature only multiple instances of the same action, while TVSeries detec2 comprises diverse yet independent actions (e.g., ‘open door’, ‘wave’ and ‘write’) in one video. These actions do not necessarily correlate with one another or impose specific temporal constraints. As such, a direct adaptation of popular online TAD approaches like LSTR lstr and MAT unders1 to online segmentation is non-ideal. For instance, these models encode temporal context with a fixed set of tokens, which may limit their capability to handle the relations of procedural videos. Furthermore, these models are typically trained to prioritize frame-level accuracy while neglecting temporal continuity, which invariably leads to over-segmentation. To address the online action segmentation task, this work proposes a novel framework centered on a context-aware feature augmentation module and an adaptive memory bank . The memory bank, per-video, tracks short-term and long-term context information. The augmentation module uses an attention mechanism to allow frame features to interact with context information from the memory bank and integrate temporal information into standard frame representations. Finally, we introduce a post-processing technique for online boundary adjustment that imposes duration and prediction confidence constraints to mitigate over-segmentation. Summarizing our contributions, 1) We establish an online framework for TAS; 2) We propose a feature augmentation module that generates context-aware representations by incorporating an adaptive memory, which accumulates temporal context collectively. The module operates on frame features independently of model architecture, enabling flexible integration; 3) We present a simple post-processing technique for online prediction adjustment, which can effectively mitigate the over-segmentation problem; and 4) Our framework achieves the state-of-the-art online segmentation performance on three TAS benchmarks."
https://arxiv.org/html/2411.01119v1,AquaFuse: Waterbody Fusion for Physics GuidedView Synthesis of Underwater Scenes,"We introduce the idea of AquaFuse, a physics-based method for synthesizing waterbody properties in underwater imagery. We formulate a closed-form solution for waterbody fusion that facilitates realistic data augmentation and geometrically consistent underwater scene rendering. AquaFuse leverages the physical characteristics of light propagation underwater to synthesize the waterbody from one scene to the object contents of another. Unlike data-driven style transfer, AquaFuse preserves the depth consistency and object geometry in an input scene. We validate this unique feature by comprehensive experiments over diverse underwater scenes. We find that the AquaFused images preserve over 94% depth consistency and 90-95% structural similarity of the input scenes. We also demonstrate that it generates accurate 3D view synthesis by preserving object geometry while adapting to the inherent waterbody fusion process. AquaFuse opens up a new research direction in data augmentation by geometry-preserving style transfer for underwater imaging and robot vision applications. keywords–View Synthesis; Robot Vision; Image Processing.","Light attenuates exponentially underwater with propagation distance [2, 44] due to two physical characteristics of light: scattering and absorption. Forward scattering is responsible for blur whereas backscatter causes contrast reduction [43, 17]. Besides, light absorption by water has a prominent spectral dependency, which depends on the specific optical properties of a waterbody [3], distance of light sources, salinity, and many other factors. These physical characteristics are represented by the underwater image formation model [2], which, in recent years, have been successfully applied for image restoration [3] and scene rendering tasks [20, 41]. In particular, underwater scene synthesis approaches extend the state-of-the-art (SOTA) neural rendering methods such as NeRF [30] and Gaussian splatting [19] for domain-aware learning. They incorporate the physical properties of underwater image formation into the rendering pipeline for accurate view synthesis [20, 41]. This helps compensate for the inherent attenuation and optical artifacts in underwater imagery and ensures geometrically consistency. Various image recognition models and 3D reconstruction pipelines also use them for improved visual perception [43, 38]. Figure 1: AquaFuse is a physics-guided waterbody fusion method to fuse waterbody properties of a reference image to an input image. It leverages closed-form solutions to estimate and exploit the scene depth, background light, backscatter, and medium illumination parameters for guiding the fusion process. As shown, AquaFused scenes are perceptually realistic and also meet the underwater image formation constraints. In this paper, we explore a new technique for domain-aware data augmentation by fusing waterbody across different underwater scenes. The traditional approaches of data augmentation rely on preserving perspective or isometric constraints (e.g., rotations, translations, scaling, shear, dilation) or altering basic photometric properties (e.g., brightness, color, contrast, saturation). We hypothesize that physically accurate waterbody fusion can be an effective way to augment data for underwater image recognition and scene rendering research; see Fig. 1. Note that the image style transfer literature [8, 46] offers solutions that learn artistic or photometric features from data. These methods rely on generative or adversarial networks to adapt one image domain’s style to another [10, 26]. While they excel at perceptual enhancements and artistic blending [17], physical constraints of underwater image formation [2] are not preserved. Hence, data augmentation by such domain transfer is not suitable if physical realism and object geometry preservation are crucial, as in 3D view synthesis, scene reconstructions, robotic mapping, and visual servoing tasks [41, 39]. We address these issues in AquaFuse by leveraging the revised underwater image formation model (UIFM) [2, 3] to fuse the optical characteristics of various waterbody types while preserving the object geometry on the scene. We formulate a nonlinear domain projection method to estimate scene depth in real-time, which facilitates an empirical estimation of the veiling light and illumination map. Veiling light is the global background light content of the backscatter signal, which decays exponentially with depth (optical distance) [3, 23, 22]. We decouple this global component from a reference image and integrate it into another (input) scene for backscatter fusion. The fused background light and backscatter signal drive the waterbody fusion, while the direct attenuation and illumination map remain preserved. We then reconstruct the AquaFused image by following the revised UIFM, retaining its physical properties. In addition to the theoretical analysis, we present the engineering constructs to tackle the practicalities of waterbody fusion across multiple scenes. We demonstrate how AquaFuse can be used for waterbody crossover, image enhancement by fusing with a clear reference scene, and in a generative pipeline for data augmentation. For qualitative and quantitative validation, we collect data at multiple ocean sites over diverse waterbody types and depth levels (10′superscript10′10^{\prime}10 start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT-80′superscript80′80^{\prime}80 start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT). Our experiments show that AquaFuse (i) performs perceptually realistic waterbody fusion; (ii) ensures geometric consistency across underwater scenes; (iii) is invariant to dataset bias and overfitting unlike learning-based approaches; and (iv) offers fast end-to-end computational being a closed-form solution. In particular, our analyses of N𝑁Nitalic_N images across six water body types show that AquaFused images preserve over 94%percent9494\%94 % depth consistency and 90909090-95%percent9595\%95 % structural similarity of input scenes. The performance margins vary 1111-5555% with increasing water depths from 10′superscript10′10^{\prime}10 start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT to 80′superscript80′80^{\prime}80 start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT. Moreover, we integrate AquaFuse within a 3D view synthesis pipeline using Gaussian Splatting [19] for underwater scene reconstruction across multiple waterbodies. We find that AquaFuse can generate highly detailed 3D reconstructions that accurately represent the original scene’s geometric characteristics in the fused scenes. As annotated underwater image databases are scarce, physics-guided data augmentation by AquaFuse can facilitate a multi-fold increase in training samples, which can be used for both 2D image recognition and 3D scene reconstruction [41, 45, 20]. This covers a wide spectrum of robot vision tasks, empowering learning-based models to be more robust and generalizable [44, 1]. The computational efficiency of AquaFuse also enables it to run in real-time for active vision, which data-driven style transfer approaches generally fail to offer."
https://arxiv.org/html/2411.01116v1,Test-Time Adaptation in Point Clouds: Leveraging Sampling Variation with Weight Averaging,"Test-Time Adaptation (TTA) addresses distribution shifts during testing by adapting a pretrained model without access to source data. In this work, we propose a novel TTA approach for 3D point cloud classification, combining sampling variation with weight averaging. Our method leverages Farthest Point Sampling (FPS) and K-Nearest Neighbors (KNN) to create multiple point cloud representations, adapting the model for each variation using the TENT algorithm. The final model parameters are obtained by averaging the adapted weights, leading to improved robustness against distribution shifts. Extensive experiments on ModelNet40-C, ShapeNet-C, and ScanObjectNN-C datasets, with different backbones (Point-MAE, PointNet, DGCNN), demonstrate that our approach consistently outperforms existing methods while maintaining minimal resource overhead. The proposed method effectively enhances model generalization and stability in challenging real-world conditions. The implementation is available at: https://github.com/AliBahri94/SVWA_TTA.git.","Deep neural networks have recently demonstrated impressive capabilities in classifying 3D point clouds [1, 2, 3, 4, 5, 6]. However, this success typically relies on the assumption that the test data is drawn from the same distribution as the training data. In real-world applications, this assumption is often invalid. When the test distribution (target) differs from the training distribution (source), the challenge of distribution shifts arises. In 3D data, such differences can vary widely, as they may be caused by various factors including the type sensor (e.g., RGB-D camera or Lidar), conditions of the environment (e.g., low light for RGB-D camera), and occlusions. This make it impractical to pretrain the network for every possible shift encountered during testing. It is thus essential to develop methods that can adapt to these distribution changes in real-time, and without supervision, during the test phase. By addressing a more realistic setting where distribution shifts can also occur after training, Test-Time Adaptation (TTA) recently became a focal point for researchers in machine learning and computer vision [7, 8, 9, 10, 11, 12]. TTA uses unlabeled test data to adapt a source-pretrained model to distribution shifts occurring in the testing phase. In this paper, we consider the fully-TTA setting where the model is pretrained on source data in a standard supervised manner, without any additional mechanism for adaptation, and the model is only adapted in testing. This setting contrast with Test-Time Training (TTT), where specialized strategies for adaptation are incorporated during source pretraining. In recent years, various TTA methods have been introduced in the 2D image domain. Key strategies include regularizing the classifier on test data using objective functions based on the prediction entropy [13, 14], or updating batch normalization statistics to align with the test data distribution [15]. In the context of 3D point cloud classification, TTA is a relatively new and emerging field, with only two approaches proposed for this task: MATE [16] and BFTT3D [17]. However, MATE [16] can technically be categorized as Test-Time Training rather than TTA, as it involves using a masked autoencoder during the source pretraining phase. On the other hand, BFTT3D [17] employs a set source prototypes to adapt to new target domains. While this prototype memory maintains privacy, it does not fully align with the core principle of TTA which aims to avoid reliance on source data during adaptation. In this paper, we propose the first fully-TTA strategy for 3D point cloud classification. Our approach is inspired by the concept seeking flat minima via weight averaging as highlighted in the SWA [18] and SWAD [19] papers. Focusing on flat minima, our method aims to enhance model robustness against distribution shifts, which are common in real-world scenarios. A key innovation in our approach is the use of sampling variation to drive the adaptation. Specifically, we employ Farthest Point Sampling (FPS) and K-Nearest Neighbors (KNN) to generate multiple variations of sampled points within the point cloud, thereby introducing controlled stochasticity during adaptation. By combining the weights obtained using differently-sampled point clouds, the model is steered away from sharp minima which are more prone to overfitting and less robust to distribution shifts. The iterative adaptation process of our method, which is guided by prediction entropy minimization strategy of TENT [20], ensures that each variation in sampling contributes to a broader exploration of the loss landscape. By saving the model weights after each adaptation and subsequently averaging them, we converge to a flatter and more stable region in the loss landscape. This weight averaging technique, inspired by the SWAD approach, mitigates the impact of noise and outliers within individual samples, leading to a more robust and generalizable model. We outline the main contributions of our work as follows: • Novelty: Addressing the lack of studies in this field, we introduce the first fully-TTA method specifically designed for 3D point cloud classification. Our method proposes a novel strategy for this challenging task, which combines sampling variation and weight averaging at test time. • Robustness: Our method, which achieves complete TTA without accessing any source data, demonstrates superior efficiency compared to leading approaches like TENT even with very small batch sizes. • State-of-art performance: Through an extensive set of experiments involving three datasets modeling a broad range of corruptions and three different backbones for point cloud classification, we show that our method achieves state-of-art performance in most test cases. Figure 1: Overview of our 3D TTA methodology. First, FPS is applied to generate different samplings from the input point cloud. Patchification is then performed using FPS for patch centers and KNN to form patches (a and b). The Normalization Layer (NL) weights are adapted using the TENT algorithm for each sampling. Finally, weight averaging is applied across all adapted weights to enhance robustness and generalization."
https://arxiv.org/html/2411.01106v1,LoRA-Contextualizing Adaptation ofLarge Multimodal Models for Long Document Understanding,"Large multimodal models (LMMs) have recently shown great progress in text-rich image understanding, yet they still struggle with complex, multi-page visually-rich documents. Traditional methods using document parsers for retrieval-augmented generation suffer from performance and efficiency limitations, while directly presenting all pages to LMMs leads to inefficiencies, especially with lengthy ones. In this work, we present a novel framework named LoRA-Contextualizing Adaptation of Large multimodal models (LoCAL111The source code is available at: https://github.com/puar-playground/LoCAL), which can broaden horizons of any LMM to support long-document understanding. We demonstrate that LMMs themselves can be an effective multimodal retriever to fetch relevant pages and then answer user questions based on these pages. LoCAL is implemented with two specific LMM adapters, one for evidence page retrieval and the other for question answering. Empirical results show state-of-the-art performance on public benchmarks, demonstrating the effectiveness of LoCAL.","Documents serve as a critical medium for the preservation and dissemination of information, with millions produced annually. These documents are not limited to simple text; they encompass complex layouts and a variety of modalities such as text, tables, charts, and images. Visually-rich document understanding (VDU) is thus an essential and challenging area of research. Recently, Large Multimodal Models (LMMs) has emerged, showcasing remarkable abilities to process and understand documents. These models span both proprietary and open-source domains, like GPT-4o (OpenAI, 2023), Gemini-1.5 (Team et al., 2023), and Claude-3 among closed-source models, and InternLM-XC2-4KHD (Dong et al., 2024), InternVL-Chat (Chen et al., 2023b), LLaVA-NeXT (Liu et al., 2024a), Mini-CPM (Hu et al., 2024), mPLUG-DocOwl (Ye et al., 2023b), and TextMonkey (Liu et al., 2024d) in open-source space. Their performance has been particularly notable in single-page DU tasks demonstrated on datasets like DocVQA (Mathew et al., 2021), ChartQA (Masry et al., 2022) and InfoVQA (Mathew et al., 2022). In real-world applications, they often present documents that are much longer, containing dozens or hundreds of pages(Ma et al., 2024c; Tanaka et al., 2023; Islam et al., 2023; Zhu et al., 2021). Addressing the understanding of such lengthy documents presents LMMs with new challenges (Ma et al., 2024c). One way is to utilize a classical document parser (Rausch et al., 2021) to extract information and formulate a prompt for LLM (Wang et al., 2023; Lamott et al., 2024), which is difficult to recover the layout in prompts and suffers performance degeneration from the document parser. The other way is to exploit the long context windows of large models, allowing them to take multiple pages at once. However, most of the input pages are not relevant to user requests, and efficiency will be compromised when the document contains hundreds of pages Ma et al. (2024c); Islam et al. (2023) or there is a document collection (Tito et al., 2021). In this work, we first retrieve evidence pages to obtain relevant information within a vast and varied landscape of content. Unlike using a classical document parser, we propose using LMMs as the information encoder, which have shown great generalization ability as they have been trained on a huge text corpus. After obtaining the embedding of each page, we further utilize contextualized late interaction for relevance scoring (Khattab & Zaharia, 2020). This design shows significantly better efficiency and accuracy than using the classical document parser to extract information. Top-k𝑘kitalic_k pages are then selected from hundreds of pages and provided to LMMs to answer user questions on documents. Figure 1: Overview of the LoCAL pipeline. The multi-page document and query are encoded by a customized LMM (yellow). The most relevant page is retrieved through similarity-based matching, and a fine-tuned LMM (blue) generates the final answer from the evidence. Based on this design demonstrated in Figure 1, we introduce the LoCAL framework for multi-page document understanding, which includes modules for evidence page retrieval and answer generation. Our contributions can be summarized as follows. • We propose a novel framework named LoCAL to broaden the horizons of LMMs, where we use intermediate LMMs hidden embedding for efficient question-based evidence page retrieval. • We finetune LMMs through dual LoRA adapters for evidence page retrieval and question answering, respectively, enabling LoCAL to be edge-friendly with great memory efficiency. • We collect a visually-rich document QA dataset, LoCAL-bench, comprising nine domains including magazine, flyer, newsletter, product manual, and presentations, etc. This dataset is built upon web-crawl documents, containing 226 documents and 471 question answer pairs. • We empirically show that LoCAL, with only 4B parameters, achieves state-of-the-art performance on LoCAL-bench and four public benchmarks, rivaling Gemini-1.5-pro on MMLongBench-Doc and demonstrating its effectiveness."
https://arxiv.org/html/2411.01099v1,Few-Class Arena: A Benchmark for Efficient Selection of Vision Models and Dataset Difficulty Measurement,"We propose Few-Class Arena (FCA), as a unified benchmark with focus on testing efficient image classification models for few classes. A wide variety of benchmark datasets with many classes (80-1000) have been created to assist Computer Vision architectural evolution. An increasing number of vision models are evaluated with these many-class datasets. However, real-world applications often involve substantially fewer classes of interest (2-10). This gap between many and few classes makes it difficult to predict performance of the few-class applications using models trained on the available many-class datasets. To date, little has been offered to evaluate models in this Few-Class Regime. We conduct a systematic evaluation of the ResNet family trained on ImageNet subsets from 2 to 1000 classes, and test a wide spectrum of Convolutional Neural Networks and Transformer architectures over ten datasets by using our newly proposed FCA tool. Furthermore, to aid an up-front assessment of dataset difficulty and a more efficient selection of models, we incorporate a difficulty measure as a function of class similarity. FCA offers a new tool for efficient machine learning in the Few-Class Regime, with goals ranging from a new efficient class similarity proposal, to lightweight model architecture design, to a new scaling law. FCA is user-friendly and can be easily extended to new models and datasets, facilitating future research work. Our benchmark is available at https://github.com/fewclassarena/fca.","The de-facto benchmarks for evaluating efficient vision models are large scale with many classes (e.g. 1000 in ImageNet [1], 80 in COCO [2], etc.). Such benchmarks have expedited the advance of vision neural networks toward efficiency [3, 4, 5, 6, 7, 8, 9, 10] with the hope of reducing the financial and environmental cost of vision models [11, 12]. More efficient computation is facilitated by using quantization [13, 14, 15], pruning [16, 17, 18, 19], and data saliency [20]. Despite efficiency improvements such as these, many-class datasets are still the standard of model evaluation. Real-world applications, however, typically comprise only a few number of classes (e.g, less than 10) [21, 22, 23] which we termed Few-Class Regime. To deploy a vision model pre-trained on large datasets in a specific environment, it requires the re-evaluation of published models or even retraining to find an optimal model in an expensive architectural search space [24]. (a) Accuracies for sub-models (blue) and full models (red). (b) Zoomed window shows accuracy values and range for full and sub-models in the few-class range. (c) Zoomed window shows (c.1) drop of accuracy as NC⁢Lsubscript𝑁𝐶𝐿N_{CL}italic_N start_POSTSUBSCRIPT italic_C italic_L end_POSTSUBSCRIPT decreases, (c.2) accuracy scales with model size for full models in the few-class range. (d) Zoomed window shows (d.1) rising accuracy as NC⁢Lsubscript𝑁𝐶𝐿N_{CL}italic_N start_POSTSUBSCRIPT italic_C italic_L end_POSTSUBSCRIPT decreases, (d.2) accuracy does not scale with model size for sub-models in the few-class range. Figure 1: Top-1 accuracies of various scales of ResNet, whose model sizes are shown in the legend, and whose plots vary from dark to light by decreasing size. Plots range along number of classes NC⁢Lsubscript𝑁𝐶𝐿N_{CL}italic_N start_POSTSUBSCRIPT italic_C italic_L end_POSTSUBSCRIPT from the full ImageNet size (1000) down to the Few-Class Regime. Each model is tested on 5 subsets whose NC⁢Lsubscript𝑁𝐶𝐿N_{CL}italic_N start_POSTSUBSCRIPT italic_C italic_L end_POSTSUBSCRIPT classes are randomly sampled from the original 1000 classes. (a) Plots for sub-models trained on subsets of classes (blue) and full models trained on all 1000 classes (red). (b) Zoomed window shows the standard deviation of subset’s accuracies is much smaller than for the full model. (c.1) Full model accuracies drop when NC⁢Lsubscript𝑁𝐶𝐿N_{CL}italic_N start_POSTSUBSCRIPT italic_C italic_L end_POSTSUBSCRIPT decreases. (c.2) Full model accuracies increase as model scales up in the Few-Class Regime. (d.1) Sub-model accuracies grow as NC⁢Lsubscript𝑁𝐶𝐿N_{CL}italic_N start_POSTSUBSCRIPT italic_C italic_L end_POSTSUBSCRIPT decreases. (d.2) Sub-model accuracies do not increase when model scales up in the Few-Class Regime. One major finding is that, apart from scaling down model and architectural design for efficiency, dataset difficulty also plays a vital role in model selection [25] (described in Section 4.3). Figure 1 summarizes several key findings under the Few-Class Regime. On the left graph in red are accuracy results for a range of number of classes NC⁢Lsubscript𝑁𝐶𝐿N_{CL}italic_N start_POSTSUBSCRIPT italic_C italic_L end_POSTSUBSCRIPT for what we call the “full model”, that is ResNet models pre-trained on the full 1000 classes of ImageNet (generally available from many websites). On the right are accuracy results for what we call “sub-models”, each of which is trained and tested on the same NC⁢Lsubscript𝑁𝐶𝐿N_{CL}italic_N start_POSTSUBSCRIPT italic_C italic_L end_POSTSUBSCRIPT, where this number of classes is sampled from the full dataset down to the Few-Class Regime. Findings include the following. (a) Sub-models attain higher upper-bound accuracy than full models. (b) The range of accuracy widens for full models at few-classes, which increases the uncertainty of a practitioner selecting a model for few classes. In contrast, sub-models narrow the range. (c) Full models follow the scaling law [26] in the dimension of model size - larger models (darker red) have higher accuracy from many to few classes. (d) Surprisingly, the scaling law is violated for sub-models in the Few-Class Regime (see the zoomed-in subplot) where larger models (darker blue) do not necessarily perform better than smaller ones (lighter blue). From these plots, our key insight is that, instead of using full models, researchers and practitioners in the Few-Class Regime should use sub-models for selection of more efficient models. However, obtaining sub-models involves computationally expensive training and testing cycles since they need to be converged on each of the few-class subsets. By carefully studying and comparing the experiment and evaluation setup of these works in the literature, we observe that, how models scale down to Few-Class Regime is rarely studied. The lack of comprehensive benchmarks for few-class research impedes both researchers and practitioners from quickly finding models that are the most efficient for their dataset size. To fill this need, we propose a new benchmark, Few-Class Arena (FCA), with the goal of benchmarking vision models under few-class scenarios. To our best knowledge, FCA is the first benchmark for such a purpose. We formally define Few-Class Regime as a scenario where the dataset has a limited number of classes. Real-world applications often comprise only a few number of classes (e.g. NC⁢L<10subscript𝑁𝐶𝐿10N_{CL}<10italic_N start_POSTSUBSCRIPT italic_C italic_L end_POSTSUBSCRIPT < 10 or 10%percent1010\%10 % classes of a dataset). Consequently, Few-Class Arena refers to a benchmark to conduct research experiments to compare models in the Few-Class Regime. This paper focuses on the image classification task, although Few-Class Regime can generalize to object detection and other visual tasks. Statement of Contributions. Four contributions are listed below: • To be best of our knowledge, we are the first to explore the problems in the Few-Class Regime and develop a benchmark tool Few-Class Arena (FCA) to facilitate scientific research, analysis, and discovery for this range of classes. • We introduce a scalable few-class data loading approach to automatically load images and labels in the Few-Class Regime from the full dataset, avoiding the need to duplicate data points for every additional few-class subset. • We incorporate dataset similarity as an inverse difficulty measurement in Few-Class Arena and propose a novel Silhouette-based similarity score named SimSS. By leveraging the visual feature extraction power of CLIP and DINOv2, we show that SimSS is highly correlated with ResNet performance in the Few-Class Regime with Pearson coefficient scores ≥0.88absent0.88\geq 0.88≥ 0.88. • We conduct extensive experiments that comprise ten models on ten datasets and 2-1000 numbers of classes on ImageNet, totalling 1591 training and testing runs. In-depth analyses on this large body of testing reveal new insights in the Few-Class Regime."
https://arxiv.org/html/2411.01048v1,MultiDepth: Multi-Sample Priors for Refining Monocular Metric Depth Estimations in Indoor Scenes,"Monocular metric depth estimation (MMDE) is a crucial task to solve for indoor scene reconstruction on edge devices. Despite this importance, existing models are sensitive to factors such as boundary frequency of objects in the scene and scene complexity, failing to fully capture many indoor scenes. In this work, we propose to close this gap through the task of monocular metric depth refinement (MMDR) by leveraging state-of-the-art MMDE models. MultiDepth proposes a solution by taking samples of the image along with the initial depth map prediction made by a pre-trained MMDE model. Compared to existing iterative depth refinement techniques, MultiDepth does not employ normal map prediction as part of its architecture, effectively lowering the model size and computation overhead while outputting impactful changes from refining iterations. MultiDepth implements a lightweight encoder-decoder architecture for the refinement network, processing multiple samples from the given image, including segmentation masking. We evaluate MultiDepth on four datasets and compare them to state-of-the-art methods to demonstrate its effective refinement with minimal overhead, displaying accuracy improvement upward of 45%.","Accurate pixel-wise depth estimation is critical to various applications such as indoor 3D scene reconstruction, autonomous driving, and robotics. For indoor 3D scene modeling in real-world use cases, attaining posed image sequences is uncommon, especially on edge devices. Thus, monocular metric depth estimation becomes an important step in surface reconstruction for this purpose. However, variables such as hardware limitations, scene complexity, and view-dependent effects (VDE) present a unique challenge, which we propose to solve by using the task of Monocular Metric Depth Refinement (MMDR). While advancements in monocular metric depth refinement (MMDE) methods [unidepth, monodepth2] with the introduction of ViT [vit, xformers] demonstrate impressive metric accuracy, detail, and camera invariance, the change in image resolution, scene complexity, and boundary frequency impact their capabilities significantly. Such inconsistencies across image capture factors can be seen when we apply sampling algorithms such as pixel-unshuffle [pixelshuffle] and random subsampling. Fig. 1 demonstrates the inconsistency in UniDepth [unidepth] between image pairs that have been pixel-unshuffled. For forgiveness around extremely high-frequency surface areas, we give less weight to the boundary with canny edge detection. As shown, it struggles to predict the depth when a lower-resolution image is fed into the network. Similarly, Fig 2 shows inconsistency between subsampled and original image predictions. Although some surfaces are accurately predicted, they completely miss certain separable areas, displaying the model’s dependency on object relationships derived from its self-attention layer. As such discrepancies display an area of improvement, we directly address these issues in our proposed method. (a) GT sample (b) UniDepth sample (c) Sample Error Map Figure 2: UniDepth [unidepth] prediction inconsistency between sub-sampled images. (a) shows ground-truth sub-sample. (b) shows UniDepth prediction on a sub-sampled image. (c) shows L1 loss between ground truth and depth prediction, where green indicates greater loss. To improve upon some of the discrepancies in the predicted depth maps, iterative depth refinement methods [irondepth, iterdepth, semilocaldepth] have been proposed. However, current methods largely rely on segmented object normals as priors. Such methods produce impressive results if accurate normal estimation is made. However, the presence of a large prediction error causes such an error to propagate to the depth refinement. In addition to this limitation, it raises model size and complexity, which is often a limiting factor for devices on Edge. In this work, we attempt to address the task of iteratively refining the predicted depth map without significantly increasing the model size or complexity. Our proposed method, MultiDepth, is the first to delve into scene complexity to improve model accuracy and detail. Unlike existing refinement pipelines, MultiDepth does not predict normal maps of objects, which creates unnecessary overhead. Instead, we employ various sampling strategies for quick iterations, which improve Edge compatibility. Due to its lightweight nature, it can be quickly utilized for mesh initialization for surface reconstruction methods. To solve the task of MMDR by refining depth estimations generated by state-of-the-art MMDE models, we propose an iterative mechanism for leveraging various image sampling strategies to obtain view-independent depth maps that are less sensitive to boundary frequency and image scale while being aware of model size to allow easier integration on edge devices. The proposed architecture can be finetuned on any depth estimation backbone, improving its flexibility for future research in depth estimation. Our major contributions are listed as follows: • We employ multiple sampling strategies such as segmentation, random subsampling, and pixel-unshuffling to input into a depth refinement network for better environment-independent depth-map prediction of the given scene from a single image. • We propose a novel depth refinement method to iteratively improve the monocular metric depth estimation output result through a lightweight encoder-decoder network that can be run on Edge. • We design an aggregation method that considers multi-sample depth maps and concatenates them into the most probable depth map estimation."
https://arxiv.org/html/2411.01025v1,FISHing in Uncertainty: Synthetic Contrastive Learning for Genetic Aberration Detection,"Detecting genetic aberrations is crucial in cancer diagnosis, typically through fluorescence in situ hybridization (FISH). However, existing FISH image classification methods face challenges due to signal variability, the need for costly manual annotations and fail to adequately address the intrinsic uncertainty. We introduce a novel approach that leverages synthetic images to eliminate the requirement for manual annotations and utilizes a joint contrastive and classification objective for training to account for inter-class variation effectively. We demonstrate the superior generalization capabilities and uncertainty calibration of our method, which is trained on synthetic data, by testing it on a manually annotated dataset of real-world FISH images. Our model offers superior calibration in terms of classification accuracy and uncertainty quantification with a classification accuracy of 96.7% among the 50% most certain cases. The presented end-to-end method reduces the demands on personnel and time and improves the diagnostic workflow due to its accuracy and adaptability. All code and data is publicly accessible at: https://github.com/SimonBon/FISHing.","Evaluating genetic aberrations is crucial in cancer diagnostics, including breast [22], lung cancer [25] or neuroblastoma [2]. Fluorescence in situ hybridization (FISH) followed by fluorescence microscopy, allows for the visualization of gene copies in cell nuclei. Fluorophore-labeled probes are employed to bind to DNA sequences of a gene of interest (target) and a reference gene and are imaged using distinct wavelengths. FISH signals manifest as red and green spots in an RGB color scheme, with the nucleus stained blue as illustrated in Fig. 1A for HER2 in breast cancer and Fig. 1B for MYCN in neuroblastoma risk stratification [7]. Challenges: Assessing gene copy number in FISH images requires expert manual evaluation to count the signals. It is a tedious and subjective process, which embodies inherent uncertainty, particularly when signal proximity or cluster formation complicates the precision of gene copy number determinations (see Fig. 1A). This necessitates multiple expert reviews, leading to increasing costs and time. Figure 1: FISH-image patches with stained cell nuclei in blue image channel for row A and B. A: HER2 (target) as red signals, CEN17q (reference) as green signals. B: Synthetic images of MYCN FISH, MYCN (target) as green signals, NMI (reference) as red signals, showing diagnostic classes: MYCN Normal (cNsubscript𝑐𝑁c_{N}italic_c start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT), Gain (cGsubscript𝑐𝐺c_{G}italic_c start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT), and Amplified (cAsubscript𝑐𝐴c_{A}italic_c start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT) along real world image examples. n indicates the number of MYCN signals. Related Work: Current FISH image analysis approaches, including machine and deep learning[11, 9, 29, 5], aim at direct spot identification or appearance standardization for easier application of traditional methods like thresholding or gradient techniques [4, 15, 26]. While certain methods excel with images having clear spot-like signals, they falter with FISH image variability, especially in case of gene amplifications lacking defined spot appearances (see Fig. 1B n=20). In [31] a RetinaNet-based dual-network approach has been devised for HER2 status identification in breast cancer. It classifies nuclei and signals independently, but inaccurately estimates gene copy counts in clusters, defaulting to a fixed number - not necessarily mirroring reality - and inadequately addresses prediction uncertainty. In [12] a two-stream model is introduced by combining whole image and spot information, for FISH image classification, requiring minimal manual annotations, but raising questions about detection reliability and lack of uncertainty quantification. In [14] point clouds were applied to represent localization patterns in synthetic single molecule FISH images effectively, yet this method demands pre-extraction of point positions, limiting its direct applicability to imaging data. To date, no studies have successfully integrated uncertainty modeling into FISH classification. Contribution: Our work provides four major contributions: (i) An innovative methodology for the precise classification of single-cell FISH images, leveraging synthetic data to bypass the need for costly manual annotations. (ii) A technique for generating the aforementioned synthetic FISH images. (iii) An innovative strategy for embedding inter-class variation into classification through a contrastive learning-based joint objective. (iv) Our approach enables the accurate quantification of classification uncertainty, demonstrating its concordance with human expert judgment and its applicability in the diagnostic processes."
https://arxiv.org/html/2411.00997v1,Identifying Implicit Social Biases in Vision-Language Models,"Vision-language models, like CLIP (Contrastive Language Image Pretraining), are becoming increasingly popular for a wide range of multimodal retrieval tasks. However, prior work has shown that large language and deep vision models can learn historical biases contained in their training sets, leading to perpetuation of stereotypes and potential downstream harm. In this work, we conduct a systematic analysis of the social biases that are present in CLIP, with a focus on the interaction between image and text modalities. We first propose a taxonomy of social biases called So-B-IT, which contains 374 words categorized across ten types of bias. Each type can lead to societal harm if associated with a particular demographic group. Using this taxonomy, we examine images retrieved by CLIP from a facial image dataset using each word as part of a prompt. We find that CLIP frequently displays undesirable associations between harmful words and specific demographic groups, such as retrieving mostly pictures of Middle Eastern men when asked to retrieve images of a “terrorist”. Finally, we conduct an analysis of the source of such biases, by showing that the same harmful stereotypes are also present in a large image-text dataset used to train CLIP models for examples of biases that we find. Our findings highlight the importance of evaluating and addressing bias in vision-language models, and suggest the need for transparency and fairness-aware curation of large pre-training datasets.","Machine learning has seen rapid advances in Vision-Language (VL) models that learn to jointly represent image and language data in a shared embedding space (Radford et al. 2021; Jia et al. 2021). Recent advances on a range of multi-modal tasks are exemplified by the VL model CLIP (Radford et al. 2021), leading to state-of-the-art performance on several zero-shot retrieval tasks (Xu et al. 2021) as well as being integrated into various VL models such as LLaVA (Liu et al. 2024) and BLIP (Li et al. 2022a), which combine the frozen vision encoder with language models for enhanced multi-modal understanding and alignment, Stable Diffusion, which leverages CLIP embeddings for refined text-to-image generation (Rombach et al. 2022) and various other VL models. These successes have spurred several VL models in end-user applications, such as facial recognition systems where CLIP enhances zero-shot face recognition (Zhao and Patras 2023), and multimedia event extraction, as well as event detection in images and captions (Li et al. 2022b; Lu et al. 2024). However, recent works show that large pre-trained models that operate over vision (May et al. 2019; Park et al. 2021), language (Bender et al. 2021; Guo and Caliskan 2020; Zhang et al. 2020a) or both learn social biases from training data (Barocas, Hardt, and Narayanan 2017; Corbett-Davies and Goel 2018), which risks perpetuating bias into downstream retrieval and generation tasks (Silva, Tambwekar, and Gombolay 2021; Luccioni et al. 2023; Weidinger et al. 2021). In VL models specifically, terms related to race have been found to be associated more with people of color (Agarwal et al. 2021), and women are generally underrepresented in image retrieval tasks (Wang, Liu, and Wang 2021). However, these existing works focus only on very specific forms of bias while probing disparities using a small set of curated words (Bhargava and Forsyth 2019). Figure 1: Identifying biases in CLIP using word associations. Given that more extensive and intersectional forms of bias may exist in VL models, there is a need to expand these experiments to a richer taxonomy of potential biases. Further, the size of current datasets used to train such models makes it more difficult for humans to effectively identify low-quality, toxic, or harmful samples (Hanna and Park 2020; Kreutzer et al. 2022). Methods to find and describe biases in datasets are crucial to ensure safe adoption of VL models, yet few methods exist. Recent findings of child sexual abuse images (Thiel 2023) in LAION-5B (Schuhmann et al. 2022), a popular training VL training dataset (Ilharco et al. 2021), further highlights the need to audit the relationships learned by VL models in particular. In this work, we target identifying and describing bias in pre-trained VL models at scale. We first propose a large new taxonomy, called Social Bias Implications Taxonomy (So-B-IT), which spans ten different categories of biases. So-B-IT allows us to examine bias much more broadly than prior works, including biases associated with discrimination based on the model’s implicit assumptions on images of faces. For instance, So-B-IT implements new categories of biased description, such as Appearance and Occupation, and extends word lists used by prior works (May et al. 2019; Steed and Caliskan 2021; Berg et al. 2022), allowing for finer grained analysis. Including new categories is crucial to investigate bias in VL models, as past work has targeted crime-related words (Bhargava and Forsyth 2019) or self-similarity across different demographic groups (Wolfe and Caliskan 2022). Using So-B-IT, we then investigate bias in VL models by retrieving images from FairFace (Kärkkäinen and Joo 2019) — a dataset containing pictures of peoples’ faces along with their age, gender, and race — that the model associates with the words in our taxonomy. For each category in So-B-IT, we quantify the demographic distributions of these retrieved images. As each image contains only a persons face, the association that a VL model makes between these images and the words in our taxonomy should be exclusively explained by the biases inherent to the model itself (Figure 1). Our analysis, based on studying four CLIP-based models (OAICLIP (Radford et al. 2021), OpenCLIP (Ilharco et al. 2021), FaceCLIP (Zheng et al. 2022), and DebiasCLIP (Berg et al. 2022), confirms that these systems encode significant racial and gender biases. Because So-B-IT is more fine-grained than prior work, we uncover previously-unknown, intersectional biases in CLIP models. For example, OpenCLIP not only strongly associates Homemaker with Women significantly more than it does with Men (Stanovsky, Smith, and Zettlemoyer 2019; De-Arteaga et al. 2019), but overwhelmingly associates Homemaker with Indian Women more than it does for women of other races, which is previously uncharacterized in VL models. Our analysis also uncovers that debiasing VL models for Gender can significantly increase the racial bias of the model. This extends prior work showing the propensity of vision models to lean more strongly on remaining shortcuts after debiasing (Li et al. 2023) to VL models. We also extend our experiments to seek the sources of bias in VL training data. Our investigation into training data associated with biased terms confirms the non-representative demographic distributions we identify experimentally. While our experiments are based on CLIP due to its ubiquity (Rombach et al. 2022; Gao et al. 2023; Zhou et al. 2023), our analysis and the So-B-IT taxonomy is directly applicable to any VL model with a joint image and text encoding. Our contributions can be summarized as follows: • We propose a taxonomy, So-B-IT, that covers more categories of bias than prior work and at a finer grain. So-B-IT allows us to categorize a VL model’s capacity to perpetuate societal bias in more representative tasks, and can be used broadly for vision and language auditing. • Using So-B-IT, we audit four different versions of CLIP, finding that these models encode various forms of societal bias and stereotyping across gender and racial groups. • Our findings indicate that debiasing with respect to one sensitive attribute, such as gender, does not necessarily eliminate other forms of bias, particularly racial bias. • We investigate the source of such biases using CLIP’s pre-training data, finding that disproportionate demographic representation may be a root cause of identified biases."
https://arxiv.org/html/2411.00991v1,"Re-thinking
Richardson-Lucy without Iteration Cutoffs: Physically Motivated Bayesian Deconvolution","Richardson-Lucy deconvolution is widely used to restore images from degradation caused by the broadening effects of a point spread function and corruption by photon shot noise, in order to recover an underlying object. In practice, this is achieved by iteratively maximizing a Poisson emission likelihood. However, the RL algorithm is known to prefer sparse solutions and overfit noise, leading to high-frequency artifacts. The structure of these artifacts is sensitive to the number of RL iterations, and this parameter is typically hand-tuned to achieve reasonable perceptual quality of the inferred object. Overfitting can be mitigated by introducing tunable regularizers or other ad hoc iteration cutoffs in the optimization as otherwise incorporating fully realistic models can introduce computational bottlenecks. To resolve these problems, we present Bayesian deconvolution, a rigorous deconvolution framework that combines a physically accurate image formation model avoiding the challenges inherent to the RL approach. Our approach achieves deconvolution while satisfying the following desiderata:deconvolution is performed in the spatial domain (as opposed to the frequency domain) where all known noise sources are accurately modeled and integrated in the spirit of providing full probability distributions over the density of the putative object recovered;the probability distribution is estimated without making assumptions on the sparsity or continuity of the underlying object;unsupervised inference is performed and converges to a stable solution with no user-dependent parameter tuning or iteration cutoff;deconvolution produces strictly positive solutions; andimplementation is amenable to fast, parallelizable computation.","Deconvolution improves contrast and recovers small, or equivalently, high spatial frequency features in images obtained by diffraction-limited optical equipment such as telescopes and microscopes. However, diffraction of light ultimately limits spatial frequencies supported by the optics, leading to attenuation and loss of high-frequency features [1]. All information from spatial frequencies above the microscope bandpass is lost in the image formation process making the inverse problem of recovering the underlying object ill-posed. The ill-posedness is exacerbated by the Poisson noise originating from the quantum nature of photons, the detector noise, and inherent data pixelization. In light of the stochasticity introduced by the image formation model, the full inverse problem is naturally re-pitched probabilistically where high-frequency features outside the bandpass set by the Fourier transform of the point spread function (PSF) cannot be resolved with certainty. In the 1970s, William H. Richardson and Leon B. Lucy independently proposed a computationally inexpensive algorithm to restore diffraction-degraded images through an iterative process maximizing the observation likelihood, assuming Poisson noise only. The resulting algorithm is now known as Richardson-Lucy (RL) deconvolution [2, 3]. While Poissonian noise is a reasonable approximation in the higher signal-to-noise ratio (SNR) regime, with large photon counts, the true noise model deviates considerably when the detector noise is significant [4]. Furthermore, maximum likelihood methods are prone to overfitting data and, in particular, the RL algorithm prefers sparse solutions and exhibits overfitting by amplifying noise and generating artifacts within an image [5]. A contemporaneous alternative, maximum entropy deconvolution [6], was later combined with RL to help identify an iteration stop criterion [7]. To keep the noise model simple and avoid extra parameters irrelevant to high SNR data, many modern approaches still only consider Poissonian noise and only improve upon the likelihood maximization problem by introducing regularization. Regularizers are added to the likelihood and a two-term convex optimization problem is then solved for a single point object estimate. While these approaches have been successful in many contexts, they often require hand-crafted regularizers with carefully tuned hyperparameters [8, 9]. Most commonly used regularizers can be classified according to the types of structures they are designed to encourage to help reduce ill-posedness: smooth or sparse. Regularizers such as total variation (TV) [10] and Hessian [11] encourage object smoothness while others such as compressed sensing encourage sparsity [12]. The TV regularizer is the most popular regularization technique and involves minimizing the total variation in the deconvolved image, which encourages smoothness while preserving sharp edges. Recent implementations of TV have leveraged fast iterative shrinkage-thresholding algorithm (FISTA) [13] and alternating direction method of multipliers (ADMM) [14]. However, these techniques often result in restored images manifesting staircase artifacts whereby reconstructed images exhibit piecewise constant structures and loss of fine details (e.g., textures) [10, 15] visually adjusted via the regularization parameter. Hessian matrices are yet another regularizer encouraging smoothness but avoids oversharpening of boundaries between regions of significantly different intensities [11]. Here, unlike TV penalty where the integral over the first order derivatives or pixel-to-pixel variation is minimized, integration is performed over second-order derivatives enforcing differentiability of the object at every point. However, while differentiability encourages smoothness of transitions between regions, it may result in loss of resolution. On the other hand, compressed sensing [12] is an optimization technique that allows deconvolving or denoising images where the data is sparse in some basis (for instance, the Fourier basis or a wavelet basis [16]). A combination of sparsity and continuity assumptions have also been used in image restoration [17]. Such strategies require advance knowledge of the underlying object of interest, which may not always be available. More recently, neural networks have allowed fast deconvolutions without requiring any knowledge of the noise sources. On one hand, prior knowledge of diffraction or blurring physics can be incorporated implicitly by parameterizing the unblurred image using a specific network architecture. For example, using either convolutional neural networks (CNNs) as in the deep image prior (DIP) approach [18] or coordinate-based neural radiance fields as implicit neural representations [19] have been successful in various contexts. However, these networks are typically optimized iteratively using the input raw images only and have a tendency to overfit data without a suitable stopping criterion [18, 19]. On the other hand, many fully supervised methods [20, 21, 22, 23, 24] have been developed demanding large amounts of input network training data before deconvolution can be performed and raising concerns as to their generalizability in deconvolving different structures. In light of these issues, we take a different approach and avoid hand-crafted regularizers or neural networks. Instead, we retain a physical image formation model as shown in Fig. 1 and present an unsupervised, tuning-free, and parallelizable Bayesian image deconvolution framework incorporating an accurate noise model. This framework intrinsically treats sparse and continuous samples on the same footing, and incorporates the physics of the optical setup. We mitigate the effect of overfitting from high spatial frequencies beyond the optical bandpass by placing a prior on spatial frequencies in accordance with those allowed by the optical transfer function (OTF) and by using the mean of the Monte Carlo samples to recover a smoothed object. This physically-motivated approach allows for a robust Bayesian deconvolution redressing overfitting issues inherent to likelihood-based approaches while rigorously propagating error from noise sources. Figure 1: Bayesian deconvolution alternative to Richardson-Lucy. a. Image formation model where: 1) the imaged object ρ𝜌\rhoitalic_ρ convolved with the PSF to produce an expected convolved image μ𝜇\muitalic_μ; 2) Poisson distributed photon counts φ𝜑\varphiitalic_φ were replicated by corrupting the image with shot noise; and finally 3) normally distributed measurements were reproduced by further degrading with camera noise and pixelization. b. Inverse strategy where samples are generated from a posterior probability distribution using a Monte Carlo scheme. c. Mean of N Monte Carlo samples drawn from the posterior along with their Fourier spectra. Red circles represent the diffraction limit. d. Increasing peak sign-to-noise ratio (PSNR) and decreasing root mean square error (RMSE) as a function of the number of collected samples is desired and achieved by the method proposed herein."
https://arxiv.org/html/2411.00988v1,Retrieval-enriched zero-shot image classificationin low-resource domains,"Low-resource domains, characterized by scarce data and annotations, present significant challenges for language and visual understanding tasks, with the latter much under-explored in the literature. Recent advancements in Vision-Language Models (VLM) have shown promising results in high-resource domains but fall short in low-resource concepts that are under-represented (e.g. only a handful of images per category) in the pre-training set. We tackle the challenging task of zero-shot low-resource image classification from a novel perspective. By leveraging a retrieval-based strategy, we achieve this in a training-free fashion. Specifically, our method, named CoRE (Combination of Retrieval Enrichment), enriches the representation of both query images and class prototypes by retrieving relevant textual information from large web-crawled databases. This retrieval-based enrichment significantly boosts classification performance by incorporating the broader contextual information relevant to the specific class. We validate our method on a newly established benchmark covering diverse low-resource domains, including medical imaging, rare plants, and circuits. Our experiments demonstrate that CoRE outperforms existing state-of-the-art methods that rely on synthetic data generation and model fine-tuning.","Low-resource domains refer to those rare domains where the data or its annotation is truly scarce. Similarly, low-resource languages are those that have significantly less content available online Magueresse et al. (2020) with respect to other high-resource languages, like English. There exist abundant research on the topic in the context of natural language processing Ranathunga et al. (2023); Adams et al. (2017); Fadaee et al. (2017); Pan et al. (2017). However, surprisingly, the vision counterpart, i.e. low-resource visual domains, is much under-explored despite the numerous practical applications. In this paper, we focus on classifying images in low resource domains, i.e. where we can find only a handful of images per category. The causes for such limited data can be various: for example, when only certain devices are capable of capturing the visual content, e.g. astronomy or medical imaging; the visual content itself is sensitive or private, e.g. due to privacy issues, or rarely appears in nature, e.g. deep ocean animals, or other long-tailed categories. Their associated annotations can also be limited due to the expertise requires, in particular for niche fields, e.g. electric design or phytology. Figure 1: Our retrieval-based solution enriches both images and textual descriptors with real-world captions which contain domains and classes. Even when the captions are generic (third row for each example), they can still restrict the focus to the correct domain. Recent large vision-language models (VLMs) have fostered a paradigm shift in image classification. Their flexibility and generalization, enabled by web-scale pre-training with text-image pairs, makes them versatile tools in many sub-fields of computer vision. Numerous works have appeared, with the objective of tuning VLMs, e.g. CLIP Radford et al. (2021) or SigLIP Zhai et al. (2023), to address image zero-shot Jia et al. (2022) or few-shot Chowdhury et al. (2023); da Costa et al. (2023) image classification. However, the images involved in those studies are mostly in high-resource image domains, where there exist thousands of images on the Internet for VLMs to learn from during the web-scale pre-training Udandarao et al. (2024). However, directly performing zero-shot classification in low-resource domains does not yield satisfactory performance due to the data scarcity in pre-training. Even supervised fine-tuning might fall short in learning the underlying data distribution due to the very limited amount of data and annotation. Among the techniques that have been explored in the pioneering work Zhang et al. (2024), one prominent recipe is to fine-tune the VLMs on data augmented via synthetic generation (e.g. Stable Diffusion (Rombach et al., 2021)). Despite the performance improvements, by analyzing the generated images, we observe that image generation models are also affected by the low-resource nature of the task. The generation quality is largely dependent on the noise injected on the real samples: by injecting limited noise, the synthetic images appear very similar to the original samples, being correct but not diverse, while by injecting more noise, the synthetic images diversify in appearance, but are mostly semantically incorrect and exhibit domain-specific rule violations. This is because the data distribution of rare domains is not well-represented in the generative models latent space (Mokady et al., 2022; Trabucco et al., 2024). Instead of generating synthetic images as data augmentation, we explore the possibility of retrieving relevant information from a textual corpus, crawled from the Internet, to enrich the data representation at inference time, as shown in Fig. 1. It turns out that retrieval is also non-trivial in the low-resource regime as (i) pre-trained models generally under-represent the low-resource domains, thus greatly limiting the retrieval efficacy; (ii) large web-crawled databases can contain noisy or incorrect content, a problem that is more severe in low-resource domains. Thus, a careful design is required to leverage the retrieved data. In this work, we propose the first training-free and retrieval-based method, CoRE (Combination of Retrieval Enrichment), to tackle low-resource image classification. Following a VLM-based zero-shot classification paradigm, we propose to enrich the representation for both the query image and the class prototypes with textual content retrieved with different encoder backbones from large web-crawled databases. Specifically, for the query image, we employ the pre-trained image encoder from a VLM as our vision retrieval backbone. We perform image-to-text retrieval, obtaining the most relevant captions with respect to the query image. From our preliminary analysis, we observe that although the specific category (e.g. “LED”) appears sparsely in the retrieval, its broader category (“circuit”) does occur frequently. Previous studies have empirically demonstrated that enriching the prompt with the broader concept, together with noise, can significantly boost the zero-shot recognition performance Roth et al. (2023). We thus enrich the image embedding by combining it with the textual embedding from image-to-text retrieval. Similarly, we construct the enriched class prototypes. For each class, we form its corresponding text prompt and embed it with a pre-trained text encoder to retrieve captions that are most relevant. Then, the retrieved captions are encoded with the VLM text encoder and aggregated together with the textual embedding of the original class prompt. The final categorization is obtained by computing the cosine similarity between the enriched visual representation against the enriched textual class prototypes. To validate the effectiveness of our proposed method, we also collect a set of datasets that covers diverse low-resource domains, including medical imaging, rare plants, and circuits. CoRE can effectively improve the data representation in a training-free fashion, with a noticeable improvement in image classification performance on all the datasets, outperforming the state-of-the-art method that involves synthetic image generation and model fine-tuning. To summarize, our contributions are: • We propose the first training-free retrieval-based method CoRE for addressing zero-shot low-resource image classification; • we propose a data representation enrichment strategy for both query image and class prototypes, using the textual content retrieved from the database; • we establish a benchmark featuring zero-shot low-resource image classification, composed of representative datasets and VLM-based baselines and state-of-the-art methods; • our training-free method is effective in classifying low-resource images, outperforming competitors with training and other training-free baselines by a large margin."
https://arxiv.org/html/2411.00967v1,Raspberry PhenoSet: A Phenology-based Dataset for Automated Growth Detection and Yield Estimation,"The future of the agriculture industry is intertwined with automation. Accurate fruit detection, yield estimation, and harvest time estimation are crucial for optimizing agricultural practices. These tasks can be carried out by robots to reduce labour costs and improve the efficiency of the process. To do so, deep learning models should be trained to perform knowledge-based tasks. Which outlines the importance of contributing valuable data to the literature. In this paper, we introduce Raspberry PhenoSet, a phenology-based dataset designed for detecting and segmenting raspberry fruit across seven developmental stages. To the best of our knowledge, Raspberry PhenoSet is the first fruit dataset to integrate biology-based classification with fruit detection tasks, offering valuable insights for yield estimation and precise harvest timing. This dataset contains 1,853 high-resolution images, the highest quality in the literature, captured under controlled artificial lighting in a vertical farm. The dataset has a total of 6,907 instances of mask annotations, manually labelled to reflect the seven phenology stages. We have also benchmarked Raspberry PhenoSet using several state-of-the-art deep learning models, including YOLOv8, YOLOv10, RT-DETR, and Mask R-CNN, to provide a comprehensive evaluation of their performance on the dataset. Our results highlight the challenges of distinguishing subtle phenology stages and underscore the potential of Raspberry PhenoSet for both deep learning model development and practical robotic applications in agriculture, particularly in yield prediction and supply chain management. The dataset and the trained models are publicly available for future studies.","Estimating when and how much yield a farm will produce is critical in the food supply chain. Farmers face financial penalties due to contractual obligations for under and over-delivering perishable goods to retailers and distributors. Another benefit of accurate yield estimation is that it supports a better market position and pricing stability. Shortages may cause price hikes and dissatisfaction among buyers, while surpluses can force distributors to sell at lower prices, negatively impacting the market position of both farmers and distributors. Finally, accurate estimates allow for better transportation, storage, and sales planning, reducing the costs associated with underutilized logistics or the need for last-minute adjustments [1]. To achieve accurate yield estimation, farmers need to have a deep understanding of the speed of fruit or vegetable ripening and developmental stages of the fruit (phenology) as well as the labour to conduct regular visual inspections to monitor crop development, which is a time-consuming and costly task. However, finding specific targets using visual observations is a well-studied task in computer vision, called object detection [2]. In some cases such as fruit sorting, a fixed camera and a computer are sufficient to carry out [3, 4]. However, in many cases of fruit detection and yield estimation (such as raspberries) the leaves and branches grow randomly, therefore, a fixed camera would be incapable of detecting the desired targets. This outlines the need to have a robot with a camera attached to its arm to move around and capture images from appropriate angles. For robots to predict yield, they need to detect fruits and classify each fruit into a developmental stage. With recent advances in parallel computing, artificial intelligence models (especially deep learning (DL) models) have outperformed other techniques and have become the state-of-the-art method in the field [2]. Yet, deep neural networks (DNNs) feed on large datasets and they need to be trained on a task-specific level to achieve desired outcomes (see Fig. 1). We can leverage transfer learning [5] and fine-tune a network that is pre-trained on a large dataset using a relatively small dataset. Therefore, developing task-specific datasets is crucial to make automation feasible using DNNs. Figure 1: A sample image from the Raspberry PhenoSet, taken at the vertical farming facility at the Center of Urban Innovation of Toronto Metropolitan University. a) Original image with no annotations. b) Mask annotations of all instances present in the image; Labels A-G correspond to the seven phenology stages of raspberries. Figure 2: Phenology reference of raspberry development stages used for annotating the images. The stages are labelled a-g, representing the seven development stages. a) Buds b) Open Flower c) Fruit Initiation d) Green Fruit e) Growing Fruit (yellow colour) f) Semi-mature (pink) fruit g) Mature (red) fruit Large and publicly available datasets such as COCO [6] and LVIS [7] are not fruit-specific, and lack the proper environment, illumination, and abundance. Many fruit-specific datasets such as [8, 9, 10, 11, 12] are not publicly available. There are available datasets on strawberries, grapes, and tomatoes [13, 14, 15, 8, 10, 12], however, they possess only a few hundred images or their images have low resolution both of which prevent achieving good accuracy with DL models. More importantly, even the datasets that are fruit-specific, publicly available, and contain a sufficient number of images, lack biological relevance from which the timing of harvest cannot be elucidated. For instance, Afonso et al. [16] have labelled their tomato dataset simply by colour i.e. green, red, etc. and Cossio-Montefinale et al. [17] have too broad categories (green, unripe, ripe) to derive meaningful data to determine precise harvest dates. This makes them unpractical from a supply chain and yield estimation point of view. Moreover, object detection models are sensitive to illumination and all of the the previous datasets were captured in conventional farms or greenhouses which use sunlight, whereas vertical farms use artificial lights and have a different illumination. In this paper, we present the Raspberry PhenoSet for raspberry detection, segmentation, and yield estimation. Raspberry PhenoSet is a large and high-quality dataset with 1853 images and 6907 mask annotations, that were manually labelled. The images are provided in two different sizes, a smaller size suitable for current widely-used GPUs and a larger size suitable for future hardware upgrades. To the best of our knowledge, Raspberry PhenoSet is the first phenology-based dataset, the first dataset gathered in a vertical farming environment, and also has the largest image size amongst the fruit datasets [17] and it is publicly available111https://sites.google.com/view/raspberry-phenoset. The images are annotated and classified according to the development stages of raspberry plants (see Fig. 2), i.e. the remaining days before harvest are known for each class, making them suitable to estimate the yield and the harvest time precisely, therefore, fulfilling the supply chain needs discussed earlier. In the rest of the paper, we discuss the recent advances in the field in Sec. II, provide the details and considerations of creating the dataset in Sec. III, describe the models and metrics used for the evaluation of the dataset in Sec. IV, present the results and benchmark the dataset in Sec. V, discuss the results and performance of different models in Sec. VI, and finally provide conclusions and recommendations in Sec. VII. TABLE I: An Overview of the Fruit Detection Datasets in the Literature Fruit Dataset Size Image Size Farm Type Availability Phenology-based Apples 300 640×640640640640\times 640640 × 640 Conventional Private No [8] Apples 1,386 600×400600400600\times 400600 × 400 Conventional Private No [18] Apples 267 416×416416416416\times 416416 × 416 Conventional Private No [19] Apples 1,200 416×416416416416\times 416416 × 416 Conventional Private No [20] Apples 2,298 1280×96012809601280\times 9601280 × 960 Conventional Public No [21] Apples 1,000 1280×72012807201280\times 7201280 × 720 Conventional Public No [22] Apples 285 640×480640480640\times 480640 × 480 Greenhouse Private No [23] Citrus 4,855 1920×1080192010801920\times 10801920 × 1080 Conventional Public No [24] Citrus 579 2448×3264244832642448\times 32642448 × 3264 Conventional Public No [25] Grapes 300 1365×2048136520481365\times 20481365 × 2048 Conventional Public No [14] Grapes 961 300×450300450300\times 450300 × 450 Conventional Private No [12] Mangoes 1,100 800×600800600800\times 600800 × 600 Conventional Private No [11] Raspberry 2039 1773×1773177317731773\times 17731773 × 1773 Conventional Public No [26] Strawberries 2,400 Not Available Conventional Private No [9] Strawberries 177 1280×72012807201280\times 7201280 × 720 Greenhouse Public No [13] Tomatoes 996 512×512512512512\times 512512 × 512 Greenhouse Private No [10] Tomatoes 318 504×377504377504\times 377504 × 377 Greenhouse Private No [27] Tomatoes 250 2000×2000200020002000\times 20002000 × 2000 Greenhouse Public No [15] Raspberry(Current Study) 1853 5184×345651843456\textbf{5184}\times\textbf{3456}5184 × 3456 Vertical Farm Public Yes"
https://arxiv.org/html/2411.00916v1,Enhancing Osteoporosis Detection: An Explainable Multi-Modal Learning Framework with Feature Fusion and Variable Clustering,"Osteoporosis is a common condition that increases fracture risk, especially in older adults. Early diagnosis is vital for preventing fractures, reducing treatment costs, and preserving mobility. However, healthcare providers face challenges like limited labeled data and difficulties in processing medical images. This study presents a novel multi-modal learning framework that integrates clinical and imaging data to improve diagnostic accuracy and model interpretability. The model utilizes three pre-trained networks—VGG19, InceptionV3, and ResNet50—to extract deep features from X-ray images. These features are transformed using PCA to reduce dimensionality and focus on the most relevant components. A clustering-based selection process identifies the most representative components, which are then combined with preprocessed clinical data and processed through a fully connected network (FCN) for final classification. A feature importance plot highlights key variables, showing that Medical History, BMI, and Height were the main contributors, emphasizing the significance of patient-specific data. While imaging features were valuable, they had lower importance, indicating that clinical data are crucial for accurate predictions. This framework promotes precise and interpretable predictions, enhancing transparency and building trust in AI-driven diagnoses for clinical integration.","Osteoporosis is a common metabolic bone disorder in the elderly, which is characterized by a low bone mineral density (BMD) [1, 2]. It often leads to bone pain and an increased risk of fragility fractures, which reduces the quality of life [3, 4, 5]. This condition has become an increasingly serious public health concern, particularly in countries with an aging population. Osteoporosis-related fractures, especially hip fractures, are among the leading causes of disability and mortality worldwide, imposing a significant social and economic burden on society [3, 6]. According to a systematic review and meta-analysis conducted in 2022, the global prevalence of osteoporosis and osteopenia was 19.7% and 40.4%, respectively [4]. Early identification of osteoporosis risk factors and timely referral to specialized care improve treatment outcomes and prognosis. Improving the diagnosis quality is crucial, because it promotes equity within the healthcare system by reducing waiting times for specialized care and it enhances the overall treatment process. Early diagnosis of osteoporosis enables the prevention of osteoporotic fractures and lowers costs to the public health system, since effective early-stage treatments can reduce expenses associated with surgeries and hospitalizations before fractures occur [7, 8]. Although currently, dual-energy X-ray absorptiometry (DXA) is considered the gold standard for diagnosing osteoporosis, mass screening of individuals at high risk for osteoporosis limited. This limitation stems primarily from the high cost of DXA and its restricted availability in specialized hospitals, which makes the situation more severe in developing countries [7]. Replacing DXA with standard X-rays for osteoporosis diagnosis offers a more cost-effective and accessible alternative [9, 10]. When compared with standard X-ray, DXA is superior because it directly measures BMD using two different energy levels of X-rays, allowing for precise detection of early-stage osteoporosis and small changes in bone mass. The technical complexity and the level of expertise necessary to formulate a diagnosis based on the imaging results contributes to the higher cost and limited availability of DXA. In contrast, standard X-rays are widely available and more affordable, but this technology lacks the sensitivity to detect early bone loss, relying instead on visual assessments of bone structure. To bridge this performance gap, advanced image processing techniques are necessary. To address this, researchers propose deep learning models to automatically extract relevant image features [2, 11, 12, 13]. By using these AI techniques, it is possible to enhance the diagnostic capability of standard X-rays, extracting detailed bone density information and making them a viable alternative to DXA [2, 14]. Furthermore, this AI-driven approach can democratize osteoporosis screening and significantly reduce costs without sacrificing diagnostic accuracy [15]. However, decision support, delivered by deep learning models, is intransparent, because the decision system was created by tuning a large number of parameters during learning. In other words, you can recreate and understand the design steps of deep AI models, but retracing and validating the effects of learning on these models remains a challenge due to their complexity. This is a problem, because it is difficult to estimate or predict the effects of bias in the training data and in the selected deep learning algorithms. This makes it difficult to trust that the osteoporosis detection models can perform well in clinical settings. While deep learning models have achieved impressive results in medical diagnostics [14, 16], their adoption in clinical settings faces challenges due to the black-box nature of these models. Explainable AI (XAI) approaches provide a solution by making model predictions more transparent and interpretable. In healthcare, where decisions must be understood and trusted by both professionals and patients, explainability is critical. Our approach focuses on enhancing explainability through feature selection and clustering, ensuring that every feature contributing to the final diagnosis is interpretable, thus promoting trust and transparency in the use of AI for osteoporosis detection. Our main contributions to this study are outlined below: • The data preprocessing involved cropping the primary tissues from the X-ray images and accurately removing irrelevant features from the screening data. • An ensemble transfer learning system is introduced that leverages three pre-trained networks, selected through their performances, to extract various features and patterns present in X-ray images. • By introducing a feature fusion layer, we combine features from pre-trained networks, eliminating null spaces and creating a more interpretable feature space. This enhances both performance and explainability, ensuring the model’s decisions are more transparent and understandable. • A component selection and synergistic combination (CSSC) layer carefully screen the components got from the feature fusion layer using a clustering-based variable selection approach. It combines these components with preprocessed clinical data, integrating hard and soft information to enhance transparency. • Cropping out X-ray images, utilizing pre-trained networks, employing feature fusion layers, and excluding redundant components contribute significantly to reducing computational costs. The synergistic feature combination achieves the desired performance metrics in osteoporosis detection. The rest of this study is organized: Section II begins with an overview of the dataset and the identification of its key features, followed by a discussion on the feature fusion and component selection processes, and concludes with an analysis of the proposed system’s architecture. The findings and their analysis are detailed in Section III. Section IV-A discusses potential directions for future research. Last, Section V provides the concluding remarks."
https://arxiv.org/html/2411.00915v1,V-LoRA: An Efficient and Flexible System Boosts Vision Applications with LoRA LMM,"Large Multimodal Models (LMMs) have shown significant progress in various complex vision tasks with the solid linguistic and reasoning capacity inherited from large language models (LMMs). Low-rank adaptation (LoRA) offers a promising method to integrate external knowledge into LMMs, compensating for their limitations on domain-specific tasks. However, the existing LoRA model serving is excessively computationally expensive and causes extremely high latency. In this paper, we present an end-to-end solution that empowers diverse vision tasks and enriches vision applications with LoRA LMMs. Our system, V-LoRA, enables accurate and efficient vision tasks by 1) an accuracy-aware LoRA adapter generation approach that generates LoRA adapters rich in domain-specific knowledge to meet application-specific accuracy requirements, 2) an adaptive-tiling LoRA adapters batching operator that efficiently computes concurrent heterogeneous LoRA adapters, and 3) a flexible LoRA adapter orchestration mechanism that manages application requests and LoRA adapters to achieve the lowest average response latency. We prototype V-LoRA on five popular vision tasks on three LMMs. Experiment results reveal that V-LoRA improves 24-62% of the accuracy compared to the original LMMs and reduces 20-89% of the latency compared to the state-of-the-art LoRA model serving systems.","Encouraged by the success of LLMs in NLP applications (LLM12App, ; 7AppLLM, ; indataAppLLM, ; LLMUseCase, ; li2024personal, ), Large Multimodal Models (LMMs) (zhu2023minigpt, ; gpt4o, ; claude, ) have attracted great attention from both academia and industry. They enhance LLMs by perceiving and interpreting multimodal signals (e.g., visual inputs (liu2024visual, ; bai2023qwen, ; team2023gemini, )) and well accomplish many complex multi-modal tasks that prior models cannot. For example, GPT-4o (gpt4o, ) achieves leading accuracy on many multimodal tasks such as visual question answering (goyal2017making, ). Yet when applied to practical applications requiring domain-specific knowledge, LMMs often show suboptimal performance, similar to the early LLMs that experienced hallucinations (zhang2023siren, ). Low-rank adaptation (LoRA) (hu2021lora, ; dettmers2024qlora, ) provides a promising way to integrate the external knowledge into LMM. It fine-tunes a small portion of model parameters, known as LoRA adapters, on domain-specific datasets to learn target knowledge, and freezes the base model to preserve its original capability (more in §2). LLMs often leverage retrieval-augmented generation (RAG) (lewis2020retrieval, ) to meet this goal. Unlike LoRA, which modifies model parameters, RAG appends the retrieved knowledge (e.g., documents) onto requests (i.e., input data) for accurate response. However, this data-augmented method is not appropriate for time-sensitive vision applications. Its retrieval process and appended long-context requests incur ¿10×\times× response delay (jin2024ragcache, ). Conversely, LoRA merges fine-tuned adapters into LMMs at runtime and efficiently generates high-quality and consistent domain-specific responses without additional overhead. Despite these advantages of LoRA, it introduces complex system challenges. Recent work (chen2024punica, ; sheng2023slora, ; wu2024dlora, ), focusing on system optimization for linguistic applications with LoRA LLM, has made noticeable progress. Punica (chen2024punica, ) and S-LoRA (sheng2023slora, ) propose unmerged inference to overcome the limitation of merged inference that can merge only one adapter at once. It computes multiple LoRA adapters in parallel while batching the shared base model computation across different requests, to boost system efficiency. dLoRA (wu2024dlora, ) further balances the throughput and latency by merged and unmerged inference mode switch (more in §2). However, these efforts fail to meet the high efficiency and diverse requirements of vision applications (more in §3.2). In this paper, we answer the following research question: Can we leverage LoRA LMMs to enrich vision applications while meeting their performance requirements? We argue that yes, but need to tackle the following challenges. First, many existing small models, trained on domain-specific datasets and used in current vision applications, outperform LMMs on target tasks. To keep accuracy, their knowledge must be integrated into LMM with LoRA adapters. However, simply training one LoRA adapter for each task is uneconomical, while fusing too much external knowledge (e.g., multiple small models) into a single adapter causes inevitable accuracy degradation. Second, a vision application often involves diverse external knowledge. When serving multiple vision applications, in particular, LoRA LMM is very likely asked to execute multiple LoRA adapters simultaneously. Although unmerged inference can compute heterogeneous adapters in parallel, they introduce excessive delays. Therefore, our LoRA LMM inference system must efficiently compute concurrent heterogeneous adapters with low latency. Lastly, different vision applications have distinct performance requirements. Real-time video analytics application (yuan2022PacketGame, ; AccDecoder, ) needs low latency, while visual retrieval (kang13blazeit, ) prefers high throughput. To meet these needs, carefully managing LoRA adapters and flexibly scheduling application requests is necessary. Typical LoRA adapter manager (e.g., dLoRA) is not designed for vision applications, which incurs excessive overhead. Solely its inference mode switcher costs over half of LMM inference time. This paper presents V-LoRA, an end-to-end LoRA LMM serving system, including LoRA adapter preparation (§4.2) and inference runtime (§4.3, §4.4), to empower vision applications. V-LoRA addresses the above challenges with the following techniques: Accuracy-aware LoRA adapter generation. We propose a LoRA generator (§4.2) that prepares domain-specific LoRA adapters to generate accurate results on target tasks with external knowledge. Considering the complex accuracy variations of knowledge fusion (§3.2), LoRA adapters generation can be formulated as a constrained bin packing problem, that given external knowledge, i.e., small models and domain-specific datasets, to generate the minimum number of LoRA adapters, ensuring the accuracy specified by vision applications. We design an accuracy-aware knowledge-fusion algorithm with a greedy heuristic to solve it. Additionally, we introduce vision task heads, incorporated as part of the LoRA adapter, enabling low-latency response for vision tasks. Adaptive-tiling LoRA adapters batching. We propose a concurrent LoRA adapters batching method (§ 4.3), comprised of the Adaptive-Tiling Matrix Multiplication (ATMM) operator and its optimal tiling search algorithm, for efficient heterogeneous LoRA adapters computation. The offline search algorithm identifies the optimal tiling configurations for each possible input matrix shape, builds a hash table storing these input-optimal tiling pairs, and compiles their code implementations for standby. At runtime, ATMM adaptively selects the optimal tiling configuration in the hash table, according to the input shapes of both concurrent requests and invoked LoRA adapters, then executes the corresponding code implementation in an extreme efficiency. Flexible LoRA adapters orchestration. For diverse requirements of vision applications, we propose an orchestrator (§4.4) that efficiently and flexibly orchestrates LoRA adapters at runtime. Two tools are developed to facilitate high efficiency. A switcher leverages ATMM and unified memory management to enable swift inference mode switch and LoRA adapters swap, and a mixture inference mode, deLoRA, mitigates the starvation. Using the above tools, we design an algorithm to dynamically switch between three inference modes, schedule requests, and manage LoRA adapters to satisfy the performance requirement of each application. We summarize our key contributions as follows: ∙∙\bullet∙ To the best of our knowledge, we are the first to identify and solve the key problems in empowering vision applications with LoRA LMM. ∙∙\bullet∙ We prototype V-LoRA ***We will release the code and documentation after this paper is accepted. that enables accurate, efficient, and flexible LoRA LMM serving for vision applications, which involves accuracy-aware LoRA adapter generation, adaptive-tiling LoRA adapters batching, and efficient and flexible LoRA adapters orchestration. ∙∙\bullet∙ We implement V-LoRA and conduct evaluations for five popular analytical tasks with real-world trace on three LMMs. Experimental results show that V-LoRA achieves 24-62% accuracy compared to the original LMMs, and 20-89% latency compared to the state-of-the-art methods. This work does not raise any ethical issues."
https://arxiv.org/html/2411.00898v1,Replace-then-Perturb: Targeted Adversarial Attacks With Visual Reasoning for Vision-Language Models,"The conventional targeted adversarial attacks add a small perturbation to an image to make neural network models estimate the image as a predefined target class, even if it is not the correct target class. Recently, for visual-language models (VLMs), the focus of targeted adversarial attacks is to generate a perturbation that makes VLMs answer intended target text outputs. For example, they aim to make a small perturbation on an image to make VLMs’ answers change from “there is an apple” to “there is a baseball.” However, answering just intended text outputs is insufficient for tricky questions like “if there is a baseball, tell me what is below it.” This is because the target of the adversarial attacks does not consider the overall integrity of the original image, thereby leading to a lack of visual reasoning. In this work, we focus on generating targeted adversarial examples with visual reasoning against VLMs. To this end, we propose 1) a novel adversarial attack procedure—namely, Replace-then-Perturb and 2) a contrastive learning-based adversarial loss—namely, Contrastive-Adv. In Replace-then-Perturb, we first leverage a text-guided segmentation model to find the target object in the image. Then, we get rid of the target object and inpaint the empty space with the desired prompt. By doing this, we can generate a target image corresponding to the desired prompt, while maintaining the overall integrity of the original image. Furthermore, in Contrastive-Adv, we design a novel loss function to obtain better adversarial examples. Our extensive benchmark results demonstrate that Replace-then-Perturb and Contrastive-Adv outperform the baseline adversarial attack algorithms. We note that the source code to reproduce the results will be available.","Recently, vision-language models (VLMs) such as GPT-4 [1], Claude 3.5 [2], LLAVA 1.6 [3], and Gemini [4] have garnered significant research attention. The key abilities of VLMs are their capacity to understand and generate human-like responses to multi-modal inputs, thereby opening up new possibilities in numerous applications. For example, VLMs excel at processing and integrating both visual and textual information, enabling image captioning, visual question answering, and cross-modal retrieval. However, as VLMs continue to advance, their potential impact on both industry and academia is immense, thereby driving the need for further exploration into their capabilities and vulnerabilities. Figure 1: An example comparing embedding-based adversarial attacks and the proposed method. The original image depicts books, alphabet blocks, pencils, a drawing, and an apple. The target object is the apple in the image, and the desired prompt is “a baseball.” (Left) In embedding-based adversarial attacks, the image is recognized as a baseball; however, due to a lack of visual reasoning, the VLMs provide unnatural outputs (Q2, Q3). (Right) In the proposed method, incorporating visual reasoning, the VLMs generate natural outputs, correctly replacing the target object (apple) with a (baseball). TABLE I: Summary of the existing studies on adversarial attacks against VLMs. Ref. # Application Attack Type Targeted Visual Perturbation Generalization Visual Reasoning Shared Vision-Language Semantic Space Pub. year [5] VQA Backdoor ✓ ✓ ✗ ✗ - 2024 [6] VQA Adversarial ✗ ✓ ✓ ✗ - 2022 [7] VQA Adversarial ✓ ✓ ✗ ✗ - 2018 [8] VQA Adversarial ✓ ✗ ▲ ✗ ✓ 2024 [9] Image Captioning Adversarial ✓ ✓ ✗ ✗ ✓ 2023 [10] image-text retrieval Backdoor ✓ ✓ ✗ ✗ ✓ 2023 [11] VQA Adversarial ✓ ✓ ✗ ✗ ✓ 2024 [12] VQA Adversarial ✓ ✓ ✗ ✗ ✓ 2024 Ours VQA / Image Captioning Adversarial ✓ ✓ ✓ ✓ ✗ - Motivation Driven by research interests in the vulnerabilities of VLMs, a key research direction is generating adversarial examples (i.e., adversarial attacks), as they are closely related to several security and privacy applications such as adversarial training [13, 14], privacy protection [15, 16], and steganography [17]. Recently, several studies have focused on targeted and type-II111Adversarial attacks aim to deceive neural network models either by producing the same output with a significant change in the input (type-I) or by producing different outputs with negligibly small changes in the input (type-II) [18]. adversarial attacks [12, 8, 9, 10]. However, as depicted in Fig. 1, previous embedding-based methods lack visual reasoning, thereby providing unnatural responses to questions. In this study, we aim to answer the following research question: “How can we preserve the overall integrity of adversarial examples against VLMs?”. I-A Existing Studies and Challenges Existing works Several previous studies have proposed methods for generating adversarial examples against VLMs [19, 5]. In [5], the authors propose a targeted adversarial attack algorithm aimed at forcing VLMs to produce predefined answers (e.g., “I don’t know”) when the input prompt includes a trigger pattern. Similarly, the authors of [7] have developed methods for generating adversarial examples against Visual Question Answering (VQA) models. In [6], the authors introduce a non-targeted adversarial attack on VLMs, aiming to cause VLMs to fail in providing proper answers. Challenge 1—Visual Reasoning Unlike classifiers, targeted adversarial attacks on VLMs need to preserve the outputs for untargeted objects, whether or not adversarial perturbations are applied. For instance, in classifiers trained on ImageNet, it is sufficient for attackers to change the output from “cat” to “dog.” However, for VLMs, attackers must manipulate the original image so that VLMs recognize only the specific target as changed while preserving the interpretation of the untargeted objects. In the example shown in Fig. 1, the image contains books, alphabet blocks, pencils, a drawing, and an apple. In this example, the target is the apple in the image, and the prompt is a baseball. This ensures that only the intended target is altered, while the overall context and responses to other parts of the input remain unchanged. In previous studies, the authors of [9] have changed a specific part of an output caption by introducing an adversarial perturbation. However, this method only alters a specific output token to another targeted token, limiting its generalization capabilities. Another approach involves updating the visual encoder [10, 11], which shares the same semantic space with a text encoder. This method perturbs images using text-guided features. In [12], a similar approach has been proposed, where the focus is on multi-modal architectures to enhance adversarial attacks. Additionally, adversarial perturbations can be crafted by targeting the interactions between image and text modalities. Several studies have investigated adversarial attacks on contrastive language-image pre-training (CLIP)-like models, utilizing gradient-based optimization to create perturbations that disrupt multi-modal consistency [20, 21]. These methods aim to misalign the shared semantic space between images and texts, causing the model to generate incorrect outputs. However, despite numerous studies, none has addressed visual reasoning in adversarial attacks. Challenge 2—Effective Adversarial Example Algorithms Adversarial examples in VLMs pose a unique challenge, especially since models such as CLIP, which are multi-modal in nature, have shown inherent robustness against adversarial attacks due to their cross-modal learning ability [22]. CLIP models, by mapping images and texts into a shared semantic space, make it harder to generate adversarial perturbations that consistently fool both the vision and text encoders. This robustness stems from the model’s ability to align multi-modal features, which adds an extra layer of complexity for adversarial attacks. In previous studies [23, 24, 25, 26, 27], several adversarial attack algorithms have been proposed; however, these algorithms focus on other issues of adversarial attacks—namely, transferability. To the best of the authors’ knowledge, while multi-modal models like CLIP have demonstrated strong adversarial robustness, continued research is necessary to develop more effective adversarial example algorithms that can break this resilience. This includes refining techniques for creating targeted attacks that perturb the model’s understanding of a specific visual object without affecting unrelated parts of the input. I-B Summary of Our Contributions To address these challenges, we propose generating targeted adversarial examples through two steps: 1) applying a mask to the relevant part of the image corresponding to the target object, and 2) altering the masked region based on the desired prompt. This process creates a target image for adversarial attacks, where the generated examples visually resemble the original image but are recognized by the model as containing the intended synthesized object. Our key contributions in this work are as follows: • Replace-then-Perturb: We propose a novel framework for generating targeted adversarial examples for VLMs, ensuring that only the specified target is altered while maintaining the integrity of the rest of the input. • Contrastive-Adv: We introduce an adv aboveersarial gradient descent algorithm tailored for VLMs. • Benchmark Dataset Creation: We develop a benchmark dataset for targeted adversarial examples in VLMs, providing a valuable resource for future research. • Comprehensive Evaluation: We conduct extensive experiments using various adversarial example generation techniques to evaluate their effectiveness. I-C Paper Organization The remainder of this paper is organized as follows. Section II provides the background and preliminaries with a review of relevant literature and essential concepts. The proposed approach (Replace-then-Perturb and Contrastive-Adv) is described in Section III. Section IV introduces the construction of our proposed dataset, TA-VLM. The experimental results in Section V present and analyzes the proposed method by comparing it with baselines. Finally, the conclusion summarizes the finding and suggests directions for future research. Figure 2: An illustration of the detailed procedure of Replace-then-Perturb, where the target object is the stop sign in the original image. In this adversarial attack, we aim to change the stop sign into a 50 mph speed limit sign."
https://arxiv.org/html/2411.00868v1,Unsupervised Object Discovery:A Comprehensive Survey and Unified Taxonomy,"Unsupervised object discovery is commonly interpreted as the task of localizing and/or categorizing objects in visual data without the need for labeled examples. While current object recognition methods have proven highly effective for practical applications, the ongoing demand for annotated data in real-world scenarios drives research into unsupervised approaches. Furthermore, existing literature in object discovery is both extensive and diverse, posing a significant challenge for researchers that aim to navigate and synthesize this knowledge. Motivated by the evidenced interest in this avenue of research, and the lack of comprehensive studies that could facilitate a holistic understanding of unsupervised object discovery, this survey conducts an in-depth exploration of the existing approaches and systematically categorizes this compendium based on the tasks addressed and the families of techniques employed. Additionally, we present an overview of common datasets and metrics, highlighting the challenges of comparing methods due to varying evaluation protocols. This work intends to provide practitioners with an insightful perspective on the domain, with the hope of inspiring new ideas and fostering a deeper understanding of object discovery approaches.",sec1_intro
https://arxiv.org/html/2411.00862v1,A Simple and Effective Temporal Grounding Pipeline for Basketball Broadcast Footage,"We present a reliable temporal grounding pipeline for video-to-analytic alignment of basketball broadcast footage. Given a series of frames as input, our method quickly and accurately extracts time-remaining and quarter values from basketball broadcast scenes. Our work intends to expedite the development of large, multi-modal video datasets to train data-hungry video models in the sports action recognition domain. Our method aligns a pre-labeled corpus of play-by-play annotations containing dense event annotations to video frames – enabling quick retrieval of labeled video segments. Unlike previous methods – we forgo the need to localize game clocks – fine-tuning an out-of-the-box object detector to find semantic text regions directly. Our end-to-end approach improves the generality of our work. Additionally, interpolation and parallelization techniques prepare our pipeline for deployment in a large computing cluster. All code is made publicly available: https://github.com/leharris3/contextualized-shot-quality-estimation/tree/temporal-grounding-pipeline","I-A Motivation Text-to-video temporal grounding is a growing field of interest and essential to developing large, multi-modal video datasets [1], [2]. In particular, fine-grained temporal alignment of sports broadcast footage and play-by-play annotations spans academic and industrial interests. Sports action recognition benchmarks offer a popular method of evaluating video model capabilities in academia. Multi-sport and sport-specific datasets [3] provide practical test beds for state-of-the-art video understanding systems. In particular, we are interested in basketball broadcast video datasets aligned to play-by-play annotations (see Figure 1). These datasets offer enormous cost savings and potential for scalability – due to the ready availability of play-by-play annotations and broadcast clips through content providers like Hudl. Mainly, deeply indexed game footage enables rapid, semi-supervised generation of action-recognition datasets in a semi-supervised manner. While not the focal point of our work, our method may also be applicable in an industrial setting. Temporal alignment of game broadcasts and play-by-play logs may aid the development of automatic highlight detection systems, player recognition/tracking software [4], automated game video annotation pipelines, and segmentation and analysis tools. Figure 1: We motivate our pipeline with the figure above. Given pre-annotated game logs and unlabeled video footage, we devise a method to align both modalities. The final result of our temporal grounding pipeline is a video-to-text aligned corpus intended to enable rapid dataset development. I-B Challenges Extracting text from broadcast scenes poses several unique challenges. Primarily, ensuring the generality of a text-extraction method for many different broadcast providers, basketball leagues, and video resolutions is especially difficult. Basketball broadcasters use unique game clock graphics and fonts – making any hard-coded solution impractical. Additionally, our unprocessed video corpus includes irrelevant moments (ad breaks, breaks in play) and unpredictable occlusions of key text regions. We meet these challenges by training our custom text detection model on a robust, proprietary dataset. Additionally, we rigorously test our pipeline in a custom test bed before deployment."
https://arxiv.org/html/2411.00857v1,Deep Learning for3D Point Cloud Enhancement: A Survey,"Point cloud data now are popular data representations in a number of three-dimensional (3D) vision research realms. However, due to the limited performance of sensors and sensing noise, the raw data usually suffer from sparsity, noise, and incompleteness. This poses great challenges to down-stream point cloud processing tasks. In recent years, deep-learning-based point cloud enhancement methods, which aim to achieve dense, clean, and complete point clouds from low-quality raw point clouds using deep neural networks, are gaining tremendous research attention. This paper, for the first time to our knowledge, presents a comprehensive survey for deep-learning-based point cloud enhancement methods. It covers three main perspectives for point cloud enhancement, i.e., (1) denoising to achieve clean data; (2) completion to recover unseen data; (3) upsampling to obtain dense data. Our survey presents a new taxonomy for recent state-of-the-art methods and systematic experimental results on standard benchmarks. In addition, we share our insightful observations, thoughts, and inspiring future research directions for point cloud enhancement with deep learning.","Three-dimensional (3D) point clouds are essential data representations in 3D vision [1], such as robotics, autonomous driving, augmented reality, and metaverse [2]. Due to the limited performance of existing ranging sensors as well as sensing noise, the raw point cloud is often noisy, incomplete, and sparse [3, 4]. As such, point cloud enhancement is critical to generate high-quality 3D point clouds for the down-stream point-cloud-based tasks. Hence, this paper is dedicated to the point cloud enhancement process, which typically involves point cloud denosing, completion, and upsampling. Figure 1: Illustration of 3D point cloud enhancement. It aims to generate smooth, complete, and dense point clouds from low-quality raw point clouds. In point cloud enhancement, point cloud denoising, completion, and upsampling are three main tasks. Specifically, denosing aims to eliminate or rectify noisy points; completion is to recover the missing part in point clouds; upsampling is to enhance the resolution of the point cloud. The goal of 3D point cloud enhancement is to resample points upon given input point cloud to generate high-quality points, which are expected to be clean, complete, and dense. In recent years, deep-learning-based methods for point cloud enhancement spring up rapidly, thank to the advancements of several large-scale 3D vision datasets, such as ShapeNet [5], KITTI [6] and ScanNet [7]. They enable the neural networks to learn discriminative feature representations from raw point clouds and predict their high-quality data variations [8]. Owing to the strong representation ability of deep neural networks, the performance of deep-learning-based point cloud enhancement methods significantly outperforms traditional ones based on optimization and interpolation [9]. Thus, performing point cloud enhancement with deep neural networks becomes a dominating solution. Unfortunately, existing 3D point cloud enhancement methods are only discussed partially in survey papers addressing other topics such as 3D point cloud learning [10], 3D object detection [11], and 3D scene completion [12]. Figure 2: A taxonomy of 3D point cloud enhancement. Motivated by this, we present a comprehensive survey on deep-learning-based 3D point cloud enhancement to fill the gap. We concentrate on three critical enhancement problems (Fig. 1), including (i) denoising, (ii) completion, and (iii) upsampling. These tasks can enhance the smoothness, completeness, resolution of the raw point cloud, respectively. Our taxonomy of existing deep-learning-based 3D point cloud enhancement methods is shown in Fig. 2. Compared with the published surveys related to point cloud enhancement [13, 14, 15, 16], our survey has the following traits. First, our survey focuses on systematically analyzing point cloud enhancement in a global viewpoint, instead of illustrating methods in a sub-field of point cloud enhancement. For example, Zhou et al. [14] provided a broad overview of denoising methods, categorizing them into filter-based, optimization-based, and deep learning-based approaches. However, their review includes an extensive examination of classical methods and is not exclusively centered on deep learning. By contrast, our work is dedicated solely to deep learning techniques, offering a more detailed and specific discussion of these methods. Second, our literature review is more comprehensive. For instance, Kwon et al. [16] categorized point cloud upsampling techniques into surface consolidation and edge consolidation based on the target area of upsampling. Despite their valuable contributions, their review lacks comprehensiveness and covers a limited scope of the literature. Third, we propose a new taxonomy for deep learned point cloud enhancement methods. Fei et al. [15] classified existing architectures into point-based, view-based, convolution-based, graph-based, generative model-based, and transformer-based methods based on network structures. By contrast, our taxonomies for point cloud denoising, completion and upsampling methods are from different perspectives. Fourth, we present a more comprehensive experimental comparison by considering representative and the most recent methods in the realm. Lastly, we offer new insights to the community, contributing to a deeper understanding and fostering further advancements in this field. To summarize, the major contributions of this work are as follows: • Comprehensive review and systematic taxonomy. To the best of our knowledge, this is the first survey paper to comprehensively review recent deep learning methods for point cloud enhancement, offering a systematic taxonomy and a comprehensive literature coverage. • Benchmark overview and performance comparison. The experimental datasets and evaluation metrics for point cloud enhancement are systematically summarized. We also present thorough comparative results of representative state-of-the-art methods on standard benchmarks. • Outlook on future directions. The traits, merits, and demerits of existing methods have been highlighted. We present insightful discussions on current challenges and several future research directions to inspire further advancement in this field. The remainder of this paper is organized as follows. Sec. II details the background of 3D point cloud enhancement. Then, we survey the specific tasks in point cloud enhancement, i.e., point cloud denoising in Sec. III, point cloud completion in Sec. IV, and point cloud upsampling in Sec. V. Sec. VI presents current challenges and opportunities. Finally, Sec. VII concludes the survey."
https://arxiv.org/html/2411.00842v1,Video prediction using score-basedconditional density estimation,"Temporal prediction is inherently uncertain, but representing the ambiguity in natural image sequences is a challenging high-dimensional probabilistic inference problem. For natural scenes, the curse of dimensionality renders explicit density estimation statistically and computationally intractable. Here, we describe an implicit regression-based framework for learning and sampling the conditional density of the next frame in a video given previous observed frames. We show that sequence-to-image deep networks trained on a simple resilience-to-noise objective function extract adaptive representations for temporal prediction. Synthetic experiments demonstrate that this score-based framework can handle occlusion boundaries: unlike classical methods that average over bifurcating temporal trajectories, it chooses among likely trajectories, selecting more probable options with higher frequency. Furthermore, analysis of networks trained on natural image sequences reveals that the representation automatically weights predictive evidence by its reliability, which is a hallmark of statistical inference111Code will be released upon acceptance..","All organisms make temporal predictions, and more complex organisms make prediction about more complex aspects of their environment. Even in the simplest cases, temporal prediction is difficult: sensory measurements are incomplete and insufficient to fully specify the future, making prediction uncertain even at very short timescales. Consider a movie: the next frame of an image sequence is an event with several possible outcomes and a deterministic (e.g., least-squares) prediction computes an average over this multi-modal distribution, resulting in blurred predictions. Traditional image processing methods based on optic flow are prone to errors at occlusion boundaries, where motion is discontinuous and multiple next frames are possible. Point estimates are inadequate for handling such ambiguous situations and we consider a probabilistic formulation instead: video prediction amounts to estimating and sampling from the conditional density of the next frame given the previous frames in a sequence: p⁢(xt+1|xt,…,xt−τ+1)𝑝conditionalsubscript𝑥𝑡1subscript𝑥𝑡…subscript𝑥𝑡𝜏1p(x_{t+1}|x_{t},\dots,x_{t-\tau+1})italic_p ( italic_x start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_t - italic_τ + 1 end_POSTSUBSCRIPT ), where τ𝜏\tauitalic_τ is the memory length. Abstractly, temporal prediction can be thought of as an inverse problem that requires prior information in order to recover the part of the image sequence obfuscated by the arrow of time. Learning a density from data is generally considered intractable for high-dimensional signals such as videos: the sample complexity of density estimation is exponential in the signal dimensionality (the so-called “curse of dimensionality”). As a result, the statistical difficulty of high-dimensional density modeling can only be approached by making strong assumptions, and many methods have been proposed to tackle the case of video prediction (Oprea et al., 2020). Each of these methods imposes inductive biases through choices of objective function and/or prediction architecture. But, overall, video prediction remained largely out of reach for statistical machine learning methods until the advent of “diffusion models”, which have offered a new and highly successful paradigm for learning generative models (Song & Ermon, 2019; Ho et al., 2020; Song et al., 2021). They have also been applied to video generation where they achieve impressive empirical results (Ho et al., 2022; Brooks et al., 2024; Bar-Tal et al., 2024). These results typically rely on huge datasets, opaque networks, and sophisticated text conditioning. In this paper, we will consider a simplified diffusion-like framework where the learned representation can be analyzed. Diffusion models are based on score-matching estimation and bypass the normalization constant that plagues maximum likelihood estimation (Hyvärinen, 2005). This simplification arises from considering the gradient of the logarithm of the data distribution—known as the score function—which does not depend on the problematic normalization constant. Score-based estimation has its roots in the “empirical Bayes” subfield of statistical inference, and the central role of the score of the noisy data distribution can be generalized to many other types of estimation (Raphan & Simoncelli, 2011). The empirical Bayes formulation is appealing because of its simplicity and generality: given data and a distortion process, least-squares regression gives access to the score of the noisy data. Traditional statistical machine learning frameworks such as variational autoencoders (Kingma & Welling, 2014), or normalizing flows (Dinh et al., 2014), build an explicit model of the data distribution. In contrast, score-based modeling focuses on learning a mapping that approximates the score function and leaves the density model implicit. Remarkably, even vanilla convolutional deep networks can achieve excellent approximations of the optimal denoiser, thereby providing access to the score of the noisy data distribution, and enabling sampling from this distribution (Kadkhodaie & Simoncelli, 2021). The similarities and differences of these two probabilistic modeling frameworks are illustrated in Figure 1. Crucially, the score-based modeling framework offers the possibility of analyzing the representation, and here, we aim to describe how trained networks adapt to the content of the image being processed. Figure 1: Modeling frameworks for probabilistic prediction. Both the classical and the proposed approach are trained unsupervised on image sequences and learn the distribution of the next frame conditioned on the recent past. Both approaches make predictions by sampling from this learned conditional density, but the density can be either explicit or implicit. Left. Traditional framework: the model is explicit and its parameters are learned through repeated iterations of inference and sampling. The learning objective is applied end-to-end: from past frames, to inferred latent representation, to generated next frame. Right. Proposed score-based framework: train a denoiser via regression, i.e., a mapping from past frames and noisy observation to an estimate of the clean next frame. The trained denoiser approximates the score function and implicitly represents the probabilistic model (denoted by a funnel). To sample from this implicit model, iterative partial denoising gradually transforms noise into a predicted next frame. This work proposes a simple probabilistic approach to video prediction that enables analysis of the learned representation, making three contributions: in Section 2, we describe a score-based framework for conditional density estimation applied to video prediction; then in Section 3.1, we analyze a representative case of bifurcating trajectory that arises due to occlusion boundaries; finally in Section 3.2, we reveal the adaptivity of representations in trained networks, making their performance partially interpretable."
https://arxiv.org/html/2411.00838v1,Task-Oriented Real-time Visual Inference for IoVT Systems: A Co-design Framework of Neural Networks and Edge Deployment,"As the volume of collected video data continues to grow, data-oriented cloud computing in Internet of Video Things (IoVT) systems faces increasing challenges, such as system latency and bandwidth pressure. Task-oriented edge computing, by shifting data analysis to the edge, enables real-time visual inference. However, the limited computational power of edge devices presents challenges for effectively executing computationally intensive visual tasks. Existing methods struggle with the trade-off between high model performance and low resource consumption. For instance, lightweight neural networks often deliver limited performance, while Neural Architecture Search (NAS)-based model design incurs high computational and commercial costs at training Inspired by hardware/software co-design principles, we propose, for the first time, a co-design framework at both the model and system levels to optimize neural network architecture and deployment strategies during inference. This framework maximizes the computational efficiency of edge devices to achieve high throughput, enabling real-time edge inference. Specifically, we implement a dynamic model structure based on the re-parameterization principle, coupled with a Roofline-based model partitioning strategy to synergistically enhance the computational performance of heterogeneous devices. Furthermore, we employ a multi-objective co-optimization approach for the above strategies to balance throughput and accuracy. We also conduct a thorough analysis of the mathematical consistency between the partitioned and original models, in addition to deriving the convergence of the partitioned model. Experimental results show that, compared to baseline algorithms, our method significantly improves throughput (12.05% on MNIST, 18.83% on ImageNet) while also achieving higher classification accuracy. Additionally, it ensures stable performance across devices with varying computational capacities, highlighting its generalizability and practical application value. Simulated experiments further validate that our method enables high-accuracy real-time detection in IoVT systems, particularly for small object detection.","Real-time visual task inference is a core function of the Internet of Visual Things (IoVT) systems[22, 23], particularly for time-sensitive tasks such as object detection[25] and object tracking[26]. Real-time inference can effectively prevent task failures caused by latency. Currently, many IoVT systems rely on a centralized cloud computing architecture, as shown in Fig. 1, where data collected by terminal sensing devices is transmitted to cloud servers for centralized inference. However, as the number of terminal devices increases and the volume of visual data grows exponentially, the long-distance transmission of large-scale data creates immense bandwidth pressure, severely compromising the system’s real-time performance[24, 27]. Thus, this centralized cloud computing design principle is not well-suited for machine vision applications. Edge computing[28], as an emerging computational paradigm, as illustrated in Fig. 1, shifts data processing and analysis tasks closer to the data source, avoiding the need for massive data transmission, and thus holds greater potential for enhancing real-time system performance.[29] For example, studies [27] and [30] deploy lightweight object detection models on edge devices (e.g., drones) to achieve real-time power inspection, while study [31] deploys deep learning models on edge servers to avoid the latency issues associated with transmitting visual data to the cloud. However, a key challenge is limited computational resources of each edge device, which are insufficient to meet high-performance demands of complex visual tasks[32]. Although the computational power of edge devices has improved, it still lags significantly behind cloud servers, making it difficult to fully satisfy the computational requirements of visual inference tasks [33, 34]. Despite neural network edge deployment solutions being proposed to address this issue[40, 39], there remain significant limitations in achieving real-time inference, where balancing performance and throughput proves challenging[38]. To explore this, we introduce several key edge deployment approaches and analyze their drawbacks: • General lightweight models (e.g., the Mobilenet family[42] and GhostNet[43]): These models focus on lightweight design to reduce computational complexity, making them suitable for embedded devices with limited resources[41]. However, despite reducing computational load, their feature representation capabilities are limited, resulting in suboptimal performance for high-accuracy visual inference tasks. • Neural Architecture Search (NAS)-based methods: These methods employ automated network structure search strategies to design models optimized for specific edge devices[46, 45]. While theoretically capable of customizing models for device-specific deployment, the process incurs significant computational and financial costs[44]. Moreover, NAS-generated models are typically device-specific, lacking generalizability across devices, which severely limits their practical application value. • Split learning approaches: Split learning methods [35] partition large-scale models across different devices for distributed sequential inference. Studies [27] and [37] introduce a split learning strategy for model distribution deployment in industrial IoVT systems to achieve real-time visual inference. Study [11] proposes a model partitioning deployment method based on roofline analysis to enhance inference throughput. However, relying solely on the model partitioning strategy still has limited effectiveness in maximizing the device’s computational performance to improve throughput. Figure 1: The comparison between centralized cloud computing and edge computing approach in the IoVT system. Centralized cloud computing is a typical data-driven design, requiring all data to be uploaded to a central server for inference. In contrast, edge computing is a task-oriented paradigm, where data analysis tasks are offloaded from the cloud to the edge, closer to the data collection points, in order to reduce system latency. To address these issues, we propose, for the first time, a co-design method for neural network architecture and edge deployment tailored for real-time visual inference in IoVT systems. This approach performs multi-objective joint optimization of dynamic model architecture design and deployment strategies to enhance the compatibility between the model and edge devices, maximizing computational power and improving throughput while ensuring high accuracy. At the system level, we adopt a Roofline-based[56] model partitioning strategy that determines the optimal partitioning scheme based on the peak computational performance and memory bandwidth of the hardware, fully leveraging their computational resources to boost throughput. At the model level, we employ a dynamic architecture based on a reparameterization strategy[57], which adaptively adjusts the neural network during inference to reduce computational complexity while maintaining model performance, further optimizing throughput. To simultaneously enhance throughput and maintain model accuracy, we utilize a multi-objective co-optimization approach that coordinates both model architecture and deployment strategies. Unlike traditional optimization methods, our approach dynamically adjusts the pre-trained model during inference, enabling it to adapt to different devices with greater generalizability, making it more suitable for IoVT systems where frequent dynamic device participation occurs. Experimental results demonstrate that, compared to six state-of-the-art (SOTA) methods, the proposed approach significantly improves throughput by an average of 12.05% on MNIST, 13.75% on CIFAR-100, and 18.83% on ImageNet, while achieving satisfactory classification accuracy. Additionally, it achieves excellent throughput and accuracy across different devices, highlighting its generalizability. Furthermore, we evaluated the practical application of the proposed method in a simulated industrial IoVT system for object detection. The results show that our approach enables successful edge-side model deployment, performing real-time and accurate small object detection. The main contributions of this paper are summarized as follows: • We propose a co-design strategy for neural network architecture and partitioned deployment to maximize edge devices’ computing performance in IoVT systems. This strategy significantly improves system throughput to enables real-time edge inference. • A multi-objective co-optimization approach is adopted to effectively improve throughput while maintaining model performance. • The co-design process is performed during the application phase, adjusting the pre-trained model to adapt to different devices. This applies to practical IoVT systems with varying device registration and deregistration scenarios. • Experimental results demonstrate that the proposed method effectively enhances edge inference speed while preserving model performance. The co-design strategy at both the model and system levels proves to be highly practical."
https://arxiv.org/html/2411.00837v1,Longitudinal Mammogram Exam-based Breast Cancer Diagnosis Models: Vulnerability to Adversarial Attacks,"In breast cancer detection and diagnosis, the longitudinal analysis of mammogram images is crucial. Contemporary models excel in detecting temporal imaging feature changes, thus enhancing the learning process over sequential imaging exams. Yet, the resilience of these longitudinal models against adversarial attacks remains underexplored. In this study, we proposed a novel attack method that capitalizes on the feature-level relationship between two sequential mammogram exams of a longitudinal model, guided by both cross-entropy loss and distance metric learning, to achieve significant attack efficacy, as implemented using attack transferring in a black-box attacking manner. We performed experiments on a cohort of 590 breast cancer patients (each has two sequential mammogram exams) in a case-control setting. Results showed that our proposed method surpassed several state-of-the-art adversarial attacks in fooling the diagnosis models to give opposite outputs. Our method remained effective even if the model was trained with the common defending method of adversarial training.","Mammography-based AI is at the forefront of medical AI research and many AI products are being translated for clinical deployment. The cyber-security of such models is therefore becoming a paramount need to ensure AI integrity. Although medical IT system is relatively closed, but cyber-attacks still occur, quite often, through infiltration of the IT systems [1] or by internal hackers [2]. For example, hospitals had to pay ransom when their data are hacked [3]. In fact, 94 % of U.S. hospitals are affected by healthcare data breaches [4], showing high vulnerability and risks to adversarial attacks. As elucidated in [5], adversarial attacks can occur with real world bad intentions for data-based ransom, insurance fraud, clinical trial effect manipulation, etc. In clinical radiology, the established practice of comparing prior data with current data serves as a cornerstone for radiologists to perform lesion detection and diagnosis. The emergence of deep learning models has refined this process by integrating temporal sequential data, such as using multiple mammograms, magnetic resonance imaging, computed tomography, etc., as model inputs. Recent evidences indicate that these longitudinal models outperform those relying solely on a single time-point input. Cui et al. [6] shows that applying convolutional neural networks to follow-up MRI scans can effectively diagnose Alzheimer’s disease. Dadsetan et al. [7] proposes a novel deep learning model on longitudinal mammogram showing superior effects compared to the model using a single exam. Lee et al. [8] leverages a Transformer decoder to incorporate prior images showing improved performance on breast cancer risk prediction. In the biomedical domain, longitudinal models using sequential imaging scans are gaining popularity, where these models are effective by utilizing spatiotemporal relationships of longitudinal data. Adversarial attacks on models using an individual time-point scans have received considerable attention [9]. However, there has been little to none exploration into attacks on longitudinal models. In this work, we focus on the task of diagnosing breast cancer through a Transformer decoder architecture [10] using sequential mammogram exams. We examine the susceptibility of this longitudinal model to adversarial attacks. We propose a novel attack method tailored to combine cross-entropy loss (to guide adversarial samples across the decision boundary) and distance metric learning (to modify the relationship between sequential imaging exams), aiming to fool the diagnosis models to give rise to an opposite output. We followed the attack transferring scheme and showed through experiments that, even when adversarial training is employed to enhance the model’s robustness, our attacking method can still effectively degrade the model’s performance. Our method also outperforms several other compared methods. Our study highlights a significant vulnerability of longitudinal models to adversarial attacks, which urges the needs of enhancing such model’s safety against adversarial attacks. The contributions of our work can be summarized as: (1) We studied a novel topic of investigating adversarial attacks on longitudinal imaging-based diagnostic models; (2) We evaluated the method of integrating cross-entropy loss with distance metric learning to implement the attack, which exhibited stronger attaching effects in leading to misclassification of breast cancer, compared to several state-of-the-art attack techniques; and (3) We showed that in distance metric learning, medical knowledge can be leveraged in selecting effective adversarial samples aiming to fool a diagnosis model."
https://arxiv.org/html/2411.00836v1,DynaMath: A Dynamic Visual Benchmarkfor Evaluating Mathematical ReasoningRobustness of Vision Language Models,"The rapid advancements in Vision-Language Models (VLMs) have shown great potential in tackling mathematical reasoning tasks that involve visual context. Unlike humans who can reliably apply solution steps to similar problems with minor modifications, we found that state-of-the-art VLMs like GPT-4o can consistently fail in these scenarios, revealing limitations in their mathematical reasoning capabilities. In this paper, we investigate the mathematical reasoning robustness in VLMs and evaluate how well these models perform under different variants of the same question, such as changes in visual numerical values or function graphs. While several vision-based math benchmarks have been developed to assess VLMs’ problem-solving capabilities, these benchmarks contain only static sets of problems and cannot easily evaluate mathematical reasoning robustness. To fill this gap, we introduce DynaMath, a dynamic visual math benchmark designed for in-depth assessment of VLMs. DynaMath includes 501 high-quality, multi-topic seed questions, each represented as a Python program. Those programs are carefully designed and annotated to enable the automatic generation of a much larger set of concrete questions, including many different types of visual and textual variations. DynaMath allows us to evaluate the generalization ability of VLMs, by assessing their performance under varying input conditions of a seed question. We evaluated 14 state-of-the-art VLMs with 5,010 generated concrete questions (10 per seed question). Our results show that the worst-case model accuracy, defined as the percentage of correctly answered seed questions in all 10 variants, is significantly lower than the average-case accuracy. In addition, many models show high consistency in answering these questions – the incorrectness of a certain variant of a seed question is not due to inherent randomness. Our analysis emphasizes the need to study the robustness of VLMs’ reasoning abilities, and DynaMath provides valuable insights to guide the development of more reliable models for mathematical reasoning.","Leveraging pretraining on vast Internet-scale datasets, Large Language Models (LLMs) (Brown, 2020; Ouyang et al., 2022; Touvron et al., 2023; Achiam et al., 2023) and Multi-modal Large Language Models (MLLMs) (Team et al., 2023; Bai et al., 2023; Liu et al., 2024c; a) have achieved remarkable performance across a wide range of tasks. Among them, Vision-Language Models (VLMs) (Zhu et al., 2023; Zhang et al., 2024b) stand out, showing exceptional promise as versatile assistants capable of integrating vision and language for problem-solving. Among their visual comprehension abilities across different domains, mathematical reasoning (Lightman et al., 2023; Zhang et al., 2024f) stands out as a crucial measure of human-like intelligence, requiring both math knowledge and logical thinking. Recent work has proposed many benchmarks for evaluating the mathematical reasoning ability of VLMs. MATHVISTA (Lu et al., 2023) was the first benchmark specifically designed to evaluate visual mathematical reasoning. Recent closed-source models, such as Claude 3.5 Sonnet and GPT-4o, along with open-source models like LLaVA-OneVision (Li et al., 2024), have demonstrated average performance surpassing that of humans. Benchmarks such as MATH-V (Wang et al., 2024a) and MATHVERSE (Zhang et al., 2024e) demonstrate the current limitations of VLMs in handling challenging mathematical problems and understanding mathematical diagrams. Following typical evaluation pipelines, these benchmarks contain a static set of testing questions on which a VLM will be scored. Figure 1: An example of consistent failures in GPT-4o. Seed question 78 in our DynaMath benchmark generates a graph of a shifted absolute value function. GPT-4o consistently provides incorrect answers for variant 9 (left) with 90% repetition consistency, while it can successfully answer variant 7 (right) with 100% repetition consistency. We tested 7 other variants involving non-zero shifts of the absolute value function, and in each case, GPT-4o insists incorrectly that the “sharp corner” is at x=0𝑥0x=0italic_x = 0, leading to incorrect answers for all 7 variants. More failure examples are in Appendix F. Our work is inspired by recent studies (Nezhurina et al., 2024; Zheng et al., 2023; Zong et al., 2023; Mirzadeh et al., 2024), which revealed that even powerful LLMs struggle to reliably solve simple text reasoning problems under different input values or conditions. We found that this issue is even more pronounced in VLMs due to the added complexity of visual context. In the setting of math problems, we identified consistent failure cases on variations of simple questions. As illustrated in Figure 1, we identify a simple question asking whether a shifted absolute value function f⁢(x)=|x−a|𝑓𝑥𝑥𝑎f(x)=|x-a|italic_f ( italic_x ) = | italic_x - italic_a | is differentiable at x=0𝑥0x=0italic_x = 0. Despite the shift, this question is still quite simple and poses no challenges to humans. While GPT-4o can give correct answers for some values of a𝑎aitalic_a, it consistently gives a wrong answer for many different values of a≠0𝑎0a\neq 0italic_a ≠ 0. Drawing inspiration from human reasoning, where the same steps can be applied to solve similar problems with varying conditions, a robust reasoning model should exhibit the same ability. This raises important questions about the robustness of VLMs’ reasoning abilities: are the reasoning procedures in VLMs robust to problem variations that pose no challenge to humans? To address this question, we comprehensively study the robustness of mathematical reasoning in VLMs by introducing a new benchmark, DynaMath. DynaMath is a dynamic visual math benchmark designed for an in-depth assessment of VLMs’ reasoning robustness. Unlike existing benchmarks, which contain a static dataset of benchmarking questions, DynaMath contains 501 high-quality seed questions covering multiple mathematical topics: Plane Geometry, Solid Geometry, Analytic Geometry, Algebra, Puzzle Tests, Graph Theory, Statistics, Scientific Figures, and Arithmetic. Each seed question is represented as a carefully designed Python program; upon running, a program generates diverse concrete instances of one seed question with random variations in its conditions. The program is individually written for each seed question and considers multiple possible types of variations in each question, such as variations of numerical values, function types, graph structure, geometry, mathematical operations, etc. The questions also span varying difficulty levels, from elementary school to high school and undergraduate, with the latter two dominating. The process of dynamic benchmark generation and evaluation is presented in Figure 2. During generation, many concrete questions are created from a single seed question, and thus the actual number of questions evaluated can be much greater (e.g., 10×\times× more) than the number of seed questions. We conducted extensive experiments on DynaMath to evaluate the reasoning robustness of current state-of-the-art (SOTA) closed-source models, including GPT-4o, Gemini Pro, and Claude-3.5 Sonnet, as well as open-source VLMs such as the InternVL2 series (Chen et al., 2024), LLaVA-v1.6 series (Liu et al., 2024b), Qwen2-VL (Wang et al., 2024c), DeepSeek-VL (Lu et al., 2024), and Llama 3.2 (Dubey et al., 2024). For each seed question, we randomly generated 10 variants, resulting in an evaluation dataset of 5,010 concrete problems. On these problems, we evaluate both average-case accuracy and worst-case accuracy. The worst-case accuracy is defined as the percentage of correctly answered seed problems in all 10 variants. We observe that all considered VLMs have a worst-case accuracy that is close to or less than 50% of their average-case accuracy, signifying their unreliability in handling question variations. In addition, we also evaluate the repetition consistency on these VLMs, which characterizes the models’ inherent randomness to ensure that a low worst-case accuracy is not caused by occasional random errors but consistent errors on certain variants of a seed problem. Our main contributions and findings can be summarized as: Figure 2: The dynamic benchmark generation procedure in DynaMath. A seed question is represented as a program that can generate many concrete questions with different variations. The plots for concrete questions are randomly generated along with the corresponding ground-truth answers. During evaluation, all concrete variants of the seed questions are considered, allowing us to evaluate the worst-case model performance and robustness. • We are the first to study the mathematical reasoning robustness of VLMs and identified a new weakness in VLMs: they may consistently fail on certain variants of simple math questions that pose no challenges to humans. Such a weakness is prevalent in many state-of-the-art VLMs. • We introduce DynaMath, a dynamic benchmark comprising 501 individually designed programs capable of generating a large number of question variants across different types. Our work is the first dynamically generated benchmark for evaluating the math capability of VLMs. • Based on 5,010 concrete questions generated by DynaMath, we conduct an extensive evaluation of both SOTA closed-source and open-source VLMs. We find a noticeable gap between the average-case accuracy and worst-case accuracy among all models, indicating that many VLMs do not have robust reasoning capabilities even on relatively simple mathematical questions."
https://arxiv.org/html/2411.00831v1,Saliency-Based diversity and fairness Metric and FaceKeepOriginalAugment: A Novel Approach for Enhancing Fairness and Diversity,"Data augmentation has become a pivotal tool in enhancing the performance of computer vision tasks, with the KeepOriginalAugment method emerging as a standout technique for its intelligent incorporation of salient regions within less prominent areas, enabling augmentation in both regions. Despite its success in image classification, its potential in addressing biases remains unexplored. In this study, we introduce an extension of the KeepOriginalAugment method, termed FaceKeepOriginalAugment, which explores various debiasing aspects—geographical, gender, and stereotypical biases—in computer vision models. By maintaining a delicate balance between data diversity and information preservation, our approach empowers models to exploit both diverse salient and non-salient regions, thereby fostering increased diversity and debiasing effects. We investigate multiple strategies for determining the placement of the salient region and swapping perspectives to decide which part undergoes augmentation. Leveraging the Image Similarity Score (ISS), we quantify dataset diversity across a range of datasets, including Flickr Faces HQ (FFHQ), WIKI, IMDB, Labelled Faces in the Wild (LFW), UTK Faces, and Diverse Dataset. We evaluate the effectiveness of FaceKeepOriginalAugment in mitigating gender bias across CEO, Engineer, Nurse, and School Teacher datasets, utilizing the Image-Image Association Score (IIAS) in convolutional neural networks (CNNs) and vision transformers (ViTs). Our findings shows the efficacy of FaceKeepOriginalAugment in promoting fairness and inclusivity within computer vision models, demonstrated by reduced gender bias and enhanced overall fairness. Additionally, we introduce a novel metric, Saliency-Based Diversity and Fairness Metric, which quantifies both diversity and fairness while handling data imbalance across various datasets.","Deep learning has shown remarkable success across various domains, such as image processing [25, 40, 39, 37, 1, 19], audio analysis [34, 23, 6, 47, 46, 24], and numerous other fields [5, 2, 36, 15, 16, 20, 49, 43, 42, 22, 35, 18]. But bias has been in each domain. Computer vision models and their applications often exhibit various social biases, including gender bias [4, 3], geographical bias [29, 32], and racial bias [4, 13]. For instance, facial recognition systems tend to be less accurate for individuals with darker skin tones and for females [4]. When deploying a model trained on a dataset that doesn’t represent the demographics of the patient population, biases can skew results. For instance, if a model trained on mammograms of white patients is used on non-white patients with higher breast density, it may show decreased sensitivity, leading to more missed or delayed diagnoses and potentially worse outcomes for non-White patients [45]. The root cause of these biases often lies in the datasets used to train the models, which propagate biases when deployed in real-time applications. Dataset compilation methods typically involve gathering images from the internet, leading to the creation of biased datasets [14]. The auditing of visual datasets for faces has primarily focused on race and gender [29, 4, 13], highlighting the importance of addressing biases in facial regions. Figure 1: Overall architecture of the proposed approach Figure 2: Where to place the salient region? Figure 3: Which part should be augmented? To mitigate these biases, several methods have been proposed. Zhang et al. [50] investigated debiasing in image classification tasks by generating adversarial examples to balance training data distribution. Kim et al. [17] introduced Biaswap, a method that debiases deep neural networks without prior knowledge of bias types, utilizing unsupervised sorting and style transfer techniques. Lee et al. [28] proposed DiverseBias, a feature-level data augmentation technique for improving the debiasing of image classification models by synthesizing diverse bias-conflicting samples through disentangled representation learning. Other related approaches including SalfMix [8], KeepAugment [12], and Randaugment [9], have also tackled feature fidelity challenges. However, it’s worth noting that while these methods enhance data diversity, they may introduce noise and compromise feature diversity, thereby affecting overall model performance. For example, SalfMix may induce overfitting by repetitively incorporating the salient part of the image [8], and KeepAugment might introduce a domain shift between salient and non-salient regions, impeding contextual information [12]. In this work, we present a pioneering approach named FaceKeepOriginalAugment, detect the salient region and place in any non-salient region as shown in Fig. 1, aimed at tackling biases in computer vision. We delve into various strategies for determining the optimal placement of the salient region and meticulously assess the efficacy of our approach in mitigating biases across diverse datasets and occupational categories. In this endeavor, we address the following pivotal questions by extending our previous work [21]: • To what extent does FaceKeepOriginalAugment enhance dataset diversity, as measured by the Image Similarity Score (ISS), across various datasets like Flickr Faces HQ (FFHQ), WIKI, IMDB, Labelled Faces in the Wild (LFW), and UTK? • To what extend does FaceKeepOriginalAugment contribute to mitigate gender bias in CNNs and ViTs, thereby promoting fairness in computer vision models across various datasets? • Can we measure diversity and fairness in dataset while dealing data imbalance ? The remainder of our work is organized as follows: Section 2 discusses the relevant literature, Section 3 describes our methodology, Section 4 outlines the experimental setup and results, and finally, Section 5 concludes our work."
https://arxiv.org/html/2411.00828v1,Dreaming Out Loud: A Self-Synthesis Approach For Training Vision-Language Models With Developmentally Plausible Data,"While today’s large language models exhibit impressive abilities in generating human-like text, they require massive amounts of data during training. We here take inspiration from human cognitive development to train models in limited data conditions. Specifically we present a self-synthesis approach that iterates through four phases: Phase 1 sets up fundamental language abilities, training the model from scratch on a small corpus. Language is then associated with the visual environment in phase 2, integrating the model with a vision encoder to generate descriptive captions from labeled images. In the “self-synthesis” phase 3, the model generates captions for unlabeled images, that it then uses to further train its language component with a mix of synthetic, and previous real-world text. This phase is meant to expand the model’s linguistic repertoire, similar to humans self-annotating new experiences. Finally, phase 4 develops advanced cognitive skills, by training the model on specific tasks such as visual question answering and reasoning. Our approach offers a proof of concept for training a multimodal model using a developmentally plausible amount of data.","Figure 1: Self-Synthesis Training Framework. Our model BabyLLaMA is trained in four phases that connect fundamental language abilities to vision by learning to describe unlabeled visual experiences. We divided our approach in 4 phases, each feeding its best snapshot in terms of validation loss to the next phase. Phase 1 is concerned with fundamental language skill acquisition using 50M words. Phase 2 combines visual and text data (35 M words) to learn to describe objects and scenes. In phase 3 - making our approach one revolving around self-synthesis - we generate captions from images and use this synthesized text (i.e., 0 words from real-world corpora) to further train the model’s language decoder. Phase 4 closes the loop using 15M words to develop skills for advanced visuo-linguistic tasks such as question answering and reasoning about the environment. Recent advances in machine learning have produced large language models (LLMs) that, after training on massive text corpora, are capable of generating human-like text. However, when comparing LLM training to human development, the amount of data required for successful model training far exceeds the quantities that humans learn from during their development (Warstadt et al., 2023a). The human brain is thus often seen as a more sample-efficient learning machine than contemporary artificial neural network approaches Marcus (2020). In this work, we take inspiration from human cognitive development to build new models under limited data conditions that more closely resemble the language experience of humans. Specifically, humans learn language in combination with other senses, and use it to reflect on their experiences. We implement this idea via a self-synthesis approach that combines vision and language such that the model learns on external (real-world) text as well as its own (synthetic) description of unlabeled visual experiences (Figure 1). Self-synthesis can also be seen as analogous to the process of dreaming, which neuroscience research suggests functions as providing “augmented samples of waking experiences,” helping to shape neural representations and prevent overfitting to those experiences (Hoel, 2021; Prince and Richards, 2021)."
https://arxiv.org/html/2411.00827v1,IDEATOR: Jailbreaking VLMs Using VLMs,"As large Vision-Language Models (VLMs) continue to gain prominence, ensuring their safety deployment in real-world applications has become a critical concern. Recently, significant research efforts have focused on evaluating the robustness of VLMs against jailbreak attacks. Due to challenges in obtaining multi-modal data, current studies often assess VLM robustness by generating adversarial or query-relevant images based on harmful text datasets. However, the jailbreak images generated this way exhibit certain limitations. Adversarial images require white-box access to the target VLM and are relatively easy to defend against, while query-relevant images must be linked to the target harmful content, limiting their diversity and effectiveness. In this paper, we propose a novel jailbreak method named IDEATOR, which autonomously generates malicious image-text pairs for black-box jailbreak attacks. IDEATOR is a VLM-based approach inspired by our conjecture that a VLM itself might be a powerful red team model for generating jailbreak prompts. Specifically, IDEATOR employs a VLM to generate jailbreak texts while leveraging a state-of-the-art diffusion model to create corresponding jailbreak images. Extensive experiments demonstrate the high effectiveness and transferability of IDEATOR. It successfully jailbreaks MiniGPT-4 with a 94% success rate and transfers seamlessly to LLaVA and InstructBLIP, achieving high success rates of 82% and 88%, respectively. IDEATOR uncovers previously unrecognized vulnerabilities in VLMs, calling for advanced safety mechanisms.Disclaimer: This paper contains content that may be disturbing or offensive.","With the popularity of OpenAI’s GPT-4 (Achiam et al., 2023) and Google’s Gemini (Team et al., 2023), increasing attention has been paid to large Vision-Language Models (VLMs) and their trustworthy deployment in real-world applications. Although VLMs are built on well-aligned Large Language Models (LLMs), the integration of both textual and visual modalities introduces new vulnerabilities. Recent studies have revealed that VLMs are extremely vulnerable to jailbreak attacks, where a malicious jailbreak prompt can trick the model to output harmful content that it would normally be restricted from generating, making these risks a critical concern for VLMs’ safe release. However, evaluating the robustness of VLMs against jailbreak attacks presents notable challenges. This is partly because multi-modal evaluation data remains relatively scarce when compared to the abundant textual corpora available in the LLM domain. As such, current VLM jailbreak methods (Qi et al., 2024; Wang et al., 2024a) rely on existing LLM jailbreak datasets to craft adversarial images or texts to evaluate VLM robustness. While these methods are effective, they require white-box access to the target model which is less practical in real-world scenarios. Moreover, the generated adversarial images and texts are either severely distorted or semantically meaningless, making them easily detectable by the underlying safety mechanism of VLMs (Nie et al., 2022; Zhang et al., 2023a). This has motivated the development of manually designed pipelines for generating jailbreak images, such as combining typographic attacks with query-relevant images (Liu et al., 2023b). One major limitation of manual methods is their heavy reliance on human-engineered processes, which restricts the flexibility and scalability of generating diverse jailbreak data for robustness evaluation. Figure 1: Comparison of different jailbreak attacks on VLMs. Existing attacks generate adversarial (left) or query-relevant (middle) images, potentially boosted with adversarial text suffixes or rephrasing (red texts). Our IDEATOR attack (right) is capable of generating diverse and chained image-text pairs as stronger jailbreak prompts. To address the above limitations, in this paper, we propose the idea of “using VLMs against VLMs”, that is, leveraging a VLM to generate effective, transferable, and diverse multimodal jailbreak data. Our idea is inspired by the red-teaming techniques (Chao et al., 2023) for LLMs, where a powerful red team model can automatically generate diverse jailbreak texts to test the robustness of LLMs. In this work, we extend red-teaming to the field of VLMs, and utilize an advanced VLM as a jailbreak agent. Considering VLMs can only output texts, the attacker VLM is then integrated with state-of-the-art image-generation models to generate subtle images, making the jailbreak prompts multimodal. By integrating images, the VLM can bypass safeguards more effectively, such as by concealing malicious content within images or using visuals to strengthen role-playing scenarios. Figure 1 shows an example of the jailbreak texts and images generated by our proposed attack IDEATOR. Particularly, the attacker VLM in IDEATOR acts as an “ideator” who simulates an adversary interacting with the target VLM. The attacker VLM iteratively refines its attack strategy based on all previous responses of the target model, while the target model processes only the current inputs and cannot access the historical conversations. IDEATOR also employs multiple concurrent attack streams to allow the attacker to explore different jailbreak strategies simultaneously. This dual focus on depth and breadth enables a comprehensive examination of how the attacker can jailbreak the target model toward generating harmful outputs while bypassing safety mechanisms. The main contributions of our work are as follows: • We study the robustness of VLMs to jailbreak attacks and proposed a novel framework named IDEATOR that leverages VLMs and diffusion models to generate multimodal jailbreak prompts. To the best of our knowledge, IDEATOR is the first red team model for VLMs, establishing a new paradigm in this direction. • IDEATOR simulates an adversarial user who iteratively evolves its jailbreak strategies by interacting with the target VLM under a black-box setting, with the assistance of a diffusion model. By considering both breadth and depth in its attack strategy, IDEATOR provides a comprehensive examination of the VLM’s multimodal vulnerabilities. • Extensive experiments on benchmark datasets demonstrate that IDEATOR can successfully jailbreak MiniGPT-4 with a success rate of 94%. Moreover, the multimodal jailbreak prompts generated by IDEATOR transfer seamlessly to other VLMs, achieving high success rates of 82% and 88% on LLaVA and InstructBLIP, respectively."
https://arxiv.org/html/2411.00826v1,Uncertainty Quantification via Hölder Divergence for Multi-View Representation Learning,"Evidence-based deep learning represents a burgeoning paradigm for uncertainty estimation, offering reliable predictions with negligible extra computational overheads. Existing methods usually adopt Kullback-Leibler divergence to estimate the uncertainty of network predictions, ignoring domain gaps among various modalities. To tackle this issue, this paper introduces a novel algorithm based on Hölder Divergence (HD) to enhance the reliability of multi-view learning by addressing inherent uncertainty challenges from incomplete or noisy data. Generally, our method extracts the representations of multiple modalities through parallel network branches, and then employs HD to estimate the prediction uncertainties. Through the Dempster-Shafer theory, integration of uncertainty from different modalities, thereby generating a comprehensive result that considers all available representations. Mathematically, HD proves to better measure the “distance” between real data distribution and predictive distribution of the model and improve the performances of multi-class recognition tasks. Specifically, our method surpass the existing state-of-the-art counterparts on all evaluating benchmarks. We further conduct extensive experiments on different backbones to verify our superior robustness. It is demonstrated that our method successfully pushes the corresponding performance boundaries. Finally, we perform experiments on more challenging scenarios, i.e., learning with incomplete or noisy data, revealing that our method exhibits a high tolerance to such corrupted data.","Figure 1: The confident (a-d) and uncertain (e-h) sample-depth pairs predicted by our method on the SUNRGBD [1] dataset. The comparison reveals the discrepancies between high-confident and uncertain predictions, demonstrating the capacity of our method in handling challenging cases. Recently, multi-view learning has become pivotal in machine learning, addressing diverse forms of multi-view data [2, 3]. In the field of multi-view learning, researchers have found that the performance of models can be improved by estimating the uncertainty of data distribution. However, incorporating uncertainty considerations in each modality for reliable predictions remains a gap. There are two categories in the estimation methods of uncertainty. The first category often assigns equal weights to modalities, lacking practicality [4]. The second dynamically assigns weights to each modality, considering uncertainty to avoid unreliable predictions [5]. And Confident (a-d) and uncertain (e-h) samples from the SUN RGB-D test set is shown in Fig. 1. Regardless of the approach, estimating the uncertainty in the classification results, especially the distribution uncertainty, is critical to the reliability of the model. Current methods often use the Kullback-Leibler divergence (KLD) [6] to estimate the uncertainty of the classification results, but challenges persist in accurately discerning distribution uncertainty [7]. To address this, we use HD [8], superior in clustering experiments, replaces KLD in the models for more precise classification outcomes. Our method outperforms existing methods, offering a systematic analysis, identification of critical determinants, and empirical validation across four multi-view scenario datasets. In addition, we also test the performance of the attention mechanism in the field of multimodal image classification. Specifically, this study uses the attention mechanism to extract image features of different modalities, and uses the Visual Transformer (ViT) model [9] and the Mamba model [10] to explore the application of different types of attention mechanisms in the field of image recognition. In summary, the contributions of this paper can be encapsulated as follows: • Enhanced Objective Function: Through an exploration of Hölder divergence’s mathematical properties, we elevate the ETMC model’s objective function, resulting in the creation of the HDMVL model. Experimental results across four multi-view scenario datasets conclusively demonstrate that the HDMVL model outperforms the original ETMC model in terms of classification accuracy. • Divergence Formulas: We have delved into the impact of utilizing diverse divergence formulas to formulate objective functions concerning classification outcomes. This exploration yields fresh insights into the enhancement of multi-view classification and clustering models, affirming that an improved objective function can significantly boost classification and clustering efficacy. Furthermore, it underscores the favorable influence of Hölder divergence on classification and clustering accuracy and model performance within the realm of multi-view learning tasks. • Empirical Validation: Our extensive empirical experiments provide concrete evidence that Hölder divergence excels over KLD in the context of multi-class classification and clustering tasks, emphasizing its superior performance. It also highlights the adaptability of Hölder divergence to a variety of multi-class classification and clustering tasks, offering the potential for reduced computational costs through adjustments in the Hölder index. Additionally, the experiment proves that the global attention mechanism can better integrate information between different modalities and improve the performance of multimodal classification models. And the main notations used in this work is shown in Table I."
https://arxiv.org/html/2411.00824v1,Leaving Some Facial Features Behind,"Facial expressions are crucial to human communication, offering insights into emotional states. This study examines how specific facial features influence emotion classification, using facial perturbations on the Fer2013 dataset. As expected, models trained on data with the removal of some important facial feature experienced up to an 85% accuracy drop when compared to baseline for emotions like happy and surprise. Surprisingly, for the emotion disgust, there seem to be slight improvement in accuracy for classifier after mask have been applied. Building on top of this observation, we applied a training scheme to mask out facial features during training, motivating our proposed Perturb Scheme. This scheme, with three phases—attention-based classification, pixel clustering, and feature-focused training, demonstrates improvements in classification accuracy. The experimental results obtained suggests there are some benefits to removing individual facial features in emotion recognition tasks.","Understanding emotions plays a pivotal role in how we perceive and interact with others. When interpreting facial emotions, key landmarks such as the eyes and mouth provide significant insights into a person’s emotional state [2]. Additionally, faces can be divided into two halves, focusing on the eyes and eyebrows on one side and the mouth on the other [8]. To enhance our comprehension of facial emotions, it is essential to understand how these facial landmarks contribute to emotion prediction. To investigate the impact of facial landmarks on emotion classification, we introduced perturbations such as a physical mask to Fer2013, a facial emotion dataset, through similar procedure to that of [3]. In this paper, the masked facial emotion dataset will be referred to as MaskFer111Dataset can be accessed at https://github.com/chengq220/MaskFerDataset. Models were trained on both Fer2013 and MaskFer. The performance results for accuracy are presented in Table 1. The accuracy ratio for each emotion class generally decreased for model trained on MaskFer when compared to model trained on Fer2013. This is to be expected because by applying a mask, important features are removed from the context, resulting in less information models to determine the emotion. However, the percent change in accuracy ratio varied across different classes. For example, the accuracy for emotions like happiness dropped by around 60% while for fear, there was only a slight drop of around 10%. Surprisingly, for the emotion sad, there’s even a slight increase in accuracy ratio which means that the model trained on MaskFer performed better than its counterpart trained on Fer2013. For this specific emotion, we hypothesized that masking the mouth allowed the model to extract better information relevant to the emotion of sad such as the eyebrow. Emotion [5] [1] [9] [4] [12] Ensemble Angry -39% -35% -32% -20% -23% -32% Happy -49% -63% -42% -55% -35% -42% Sad -12% -24% -4% -16% -8% -4% Neutral -51% -60% -53% -60% -43% -53% Surprise -19% -26% -35% -19% -20% -35% Disgust -49% -59% -39% -49% -49% -39% Fear -20% -23% -13% -18% -17% -13% Table 1: Percent accuracy gain/loss for each emotion class with models trained on MaskFer compared to models trained on FER2013 By examining the saliency map shown in Figure 1, it becomes evident that for emotions such as disgust, anger, and sadness, the model trained on Fer2013 may not have learned the optimal salient features. In contrast, the model trained on MaskFer was able to capture the features in the eyebrows when identifying an angry face. It might be case that models trained on Fer2013 do not effectively leverage all the information present in the vicinity of major facial landmarks like the eyebrows. While classifying emotions based on overall facial landmarks may provide a global context that’s beneficial to emotion classification, considering them as individual features provide better localized context that’s better suited for classification of specific emotions. Figure 1: Saliency map for trained models on Fer2013 and MaskFer"
https://arxiv.org/html/2411.00822v1,"EEG-based Multimodal Representation Learningfor Emotion Recognition††thanks:This work was supported by the National Research Foundation of Korea (NRF) grant funded by the MSIT (No. 2022-2-00975, MetaSkin: Developing Next-generation Neurohaptic Interface Technology that enables Communication and Control in Metaverse by Skin Touch) and the Institute of Information & Communications Technology Planning & Evaluation (IITP) grant, funded by the Ministry of Science and ICT (MSIT) (No. RS-2019-II190079, Artificial Intelligence Graduate School Program).","Multimodal learning has been a popular area of research, yet integrating electroencephalogram (EEG) data poses unique challenges due to its inherent variability and limited availability. In this paper, we introduce a novel multimodal framework that accommodates not only conventional modalities such as video, images, and audio, but also incorporates EEG data. Our framework is designed to flexibly handle varying input sizes, while dynamically adjusting attention to account for feature importance across modalities. We evaluate our approach on a recently introduced emotion recognition dataset that combines data from three modalities, making it an ideal testbed for multimodal learning. The experimental results provide a benchmark for the dataset and demonstrate the effectiveness of the proposed framework. This work highlights the potential of integrating EEG into multimodal systems, paving the way for more robust and comprehensive applications in emotion recognition and beyond.","I INTRODUCTION Multimodal representation learning has gained significant attention in the field of artificial intelligence, particularly in tasks that involve complex human behaviors such as emotion recognition [1]. By combining data from multiple modalities, such as video, audio, and physiological signals, multimodal systems can capture diverse and complementary information to improve model performance [2, 3, 4]. However, integrating electroencephalogram (EEG) data into such frameworks introduces unique challenges due to the inherent variability, noise, and limited availability of EEG datasets compared to other modalities. Despite its potential to provide direct insights into brain activity, the effective utilization of EEG data in multimodal settings remains an open research problem. EEG data, widely used in neuroscience and clinical research [5], offer a non-invasive window into the electrical activity of the brain. This modality has the advantage of capturing cognitive and emotional states in real-time, making it particularly valuable for emotion recognition tasks [6]. However, EEG signals are often noisy [7], highly variable across subjects and sessions, and recorded at a higher dimensionality than traditional modalities like video and audio. These characteristics complicate the feature extraction process and hinder the straightforward integration of EEG into multimodal learning frameworks [8]. Existing work on emotion recognition has largely focused on conventional modalities, with many studies opting to exclude EEG due to these challenges [9]. As a result, the full potential of EEG data in improving emotion recognition systems remains underexplored [10, 11]. Figure 1: Overview of the proposed attention-based multimodal emotion recognition framework, extracting EEG, audio, and visual features with specifically tailored transformers and integrating them through self-attention fusion. Recent advances in multimodal learning have explored various fusion strategies to integrate heterogeneous data sources, such as feature concatenation [12], attention mechanisms [13], and joint embedding spaces [14]. These methods have shown promise in combining modalities like video and audio, where data characteristics are more homogeneous. However, the integration of EEG data presents a distinct set of limitations [15]. Current models often struggle to dynamically adjust to the variability in feature importance across different modalities [16], especially when EEG is involved. Additionally, existing multimodal systems may not be flexible enough to handle the varying input sizes and feature distributions that arise when combining EEG with other sensory data [17]. This often results in suboptimal performance or requires manual tuning to accommodate the unique nature of EEG signals [18, 19]. In this paper, we propose a novel multimodal framework for emotion recognition that integrates EEG, video, and audio data. Our approach dynamically adjusts attention weights to prioritize key features from each modality and adapts to varying input sizes. Evaluated on a new multimodal emotion recognition dataset, our model sets a new benchmark, highlighting the potential of incorporating EEG in emotion recognition. By addressing the limitations of existing multimodal frameworks and incorporating EEG data, our work paves the way for more robust and versatile applications in emotion recognition. The proposed framework highlights the value of using diverse data sources to capture complex emotional states, offering a more comprehensive understanding of human emotions than unimodal systems alone."
https://arxiv.org/html/2411.00818v1,On the Black-box Explainability of Object Detection Models forSafe and Trustworthy Industrial Applications,"In the realm of human-machine interaction, artificial intelligence has become a powerful tool for accelerating data modeling tasks. Object detection methods have achieved outstanding results and are widely used in critical domains like autonomous driving and video surveillance. However, their adoption in high-risk applications, where errors may cause severe consequences, remains limited. Explainable Artificial Intelligence (XAI) methods aim to address this issue, but many existing techniques are model-specific and designed for classification tasks, making them less effective for object detection and difficult for non-specialists to interpret. In this work we focus on model-agnostic XAI methods for object detection models and propose D-MFPP, an extension of the Morphological Fragmental Perturbation Pyramid (MFPP), which uses segmentation-based mask generation. Additionally, we introduce D-Deletion, a novel metric combining faithfulness and localization, adapted specifically to meet the unique demands of object detectors. We evaluate these methods on real-world industrial and robotic datasets, examining the influence of parameters such as the number of masks, model size, and image resolution on the quality of explanations. Our experiments use single-stage object detection models applied to two safety-critical robotic environments: i) a shared human-robot workspace where safety is of paramount importance, and ii) an assembly area of battery kits, where safety is critical due to the potential for damage among high-risk components. Our findings evince that D-Deletion effectively gauges the performance of explanations when multiple elements of the same class appear in the same scene, while D-MFPP provides a promising alternative to D-RISE when fewer masks are used.","In recent years, Artificial Intelligence (AI) has emerged as a transformative force across various domains, especially in human-machine interaction, where it has enabled significant advancements in data-driven decision-making processes. Among these advances, object detection has become a key component, finding application in critical areas such as autonomous driving, security surveillance, industrial automation, and robotics Zou et al. (2023); Muhammad et al. (2020). State-of-the-art object detection models, including Faster-RCNN Ren et al. (2017), DETR Carion et al. (2020), and the YOLO series Terven et al. (2023), have demonstrated impressive performance in identifying and localizing objects within images. Despite their success, the adoption of these models in highly sensitive environments remains limited, particularly in domains where errors could result in serious consequences such as injury, equipment damage, or operational failures. One of the primary reasons for this hesitancy is the black-box nature of object detectors implemented as Deep Learning models, which to date amount to the majority of proposals in the literature. The internal activations of these are not inherently interpretable, making it challenging for end-users to trust the predictions issued by object detectors, especially in high-risk environments operating in open-world environments such as autonomous vehicles and industrial robotics. In this context, the field of Explainable AI (XAI) Barredo Arrieta et al. (2020) aims to enhance the interpretability of AI systems by their audience and ultimately, to enhance the user’s trust in the output of AI-based systems. Leaving aside the category of transparent AI models (which are inherently interpretable and do not require any explanations for a user to understand how they work), explainability methods in XAI can be broadly categorized into white-box and black-box approaches. White-box XAI methods require access to the internal workings of the model, such as weights, activations, or gradients (e.g., Grad-CAM Selvaraju et al. (2017)). While these methods can provide powerful insights, they are often limited by their dependence on specific model architectures, making them difficult to generalize across different models and less accessible to users unfamiliar with AI research/tools. In contrast, black-box XAI methods treat the model as an opaque entity, providing explanations based solely on the model’s input-output behavior without requiring any access to its internal components. However, most black-box XAI methods are designed for classification tasks rather than for object detection Ribeiro et al. (2016); Lundberg and Lee (2017); Petsiuk (2018); Ali et al. (2023a). While classification models produce a single label per image, object detection models must identify and localize multiple objects within an image. Therefore, they need to explain not only the class prediction for each detected object – what they detect– but also the spatial reasoning behind the bounding boxes that define the object’s location – where the object is positioned within the image. Balancing these dual aspects complicates the explanation process and requires more sophisticated techniques than those used for classification tasks. In this paper, we address the gap in XAI methods for object detection by focusing on model-agnostic, black-box XAI techniques. We propose and evaluate novel black-box XAI methods and XAI metrics that are specifically tailored for object detection models, without requiring access to internal model details. Our proposed methods are generalizable to object detection frameworks beyond those utilized in our experiments. Specifically, the contributions of this work can be summarized as follows: • We formally define a quantitative evaluation metric, D-Deletion, which extends the existing Deletion metric Bach et al. (2015); Petsiuk (2018) proposed for classification tasks. This metric is adapted to handle the unique challenges of object detection, including localization (as seen in Figure 4), which is of utmost importance when multiple instances of the same object appear in the same scene. • By using the similarity score of D-RISE Petsiuk et al. (2021), we analyze multiple mask generation methods’ performance and introduce D-MFPP, an extension of MFPP Yang et al. (2020) originally developed for classification tasks. D-MFPP utilizes segmentation-based mask generation to improve explanations for object detection models. • We analyze the impact of key parameters, such as image dimensions and the model sizes within the YOLOv8 architecture utilized in our experiments, which can significantly influence the quality of the resulting explanations. • Last but not least, we facilitate the broader adoption of the developed techniques for object detection in real-world use cases by releasing the code publicly in a repository https://github.com/aklein1995?tab=repositories 111To be published upon article acceptance.. The remainder of this paper is structured as follows: in Section 2, we first review literature related to XAI for object detection. In Section 3, we provide the necessary background on object detection and XAI to familiarize the reader with the key concepts used in the definitions of D-RISE and Deletion. Next, Section 4 presents the experimental setup, including datasets, object detection training configuration, employed XAI methods, and evaluation metrics. In this section we also introduce our proposed D-MFPP method and D-Deletion metric. We discuss our results in Section 5. Finally, Section 6 concludes the paper with a summary of our key findings and directions for future research."
https://arxiv.org/html/2411.02347v1,Physically Based Neural Bidirectional Reflectance Distribution Function,"We introduce the physically based neural bidirectional reflectance distribution function (PBNBRDF), a novel, continuous representation for material appearance based on neural fields. Our model accurately reconstructs real-world materials while uniquely enforcing physical properties for realistic BRDFs, specifically Helmholtz reciprocity via reparametrization and energy passivity via efficient analytical integration. We conduct a systematic analysis demonstrating the benefits of adhering to these physical laws on the visual quality of reconstructed materials. Additionally, we enhance the color accuracy of neural BRDFs by introducing chromaticity enforcement supervising the norms of RGB channels. Through both qualitative and quantitative experiments on multiple databases of measured real-world BRDFs, we show that adhering to these physical constraints enables neural fields to more faithfully and stably represent the original data and achieve higher rendering quality.","Representational learning has become the standard method for modeling complex spatial distributions and functions from measured data in computer graphics and vision. Implicit neural representations (INRs) utilize neural fields, such as multi-layer perceptrons (MLPs), to estimate functions that represent a signal continuously by training on discretely sampled data. These representations have applications ranging from 2D images (Sitzmann et al. 2020) to directional reflectance functions (Rainer et al. 2019; Sztrajman et al. 2021), 3D surfaces (Wang et al. 2021), and scene density and radiance (Mildenhall et al. 2020; Rainer et al. 2022). They have demonstrated superior expressiveness, interpolation ability, and fidelity to the underlying data compared to analytic parametric models. However, this expressiveness comes with a lack of constraints on the function learned by the network. This issue is often addressed through the use of priors (e.g., Pérez De Jesús, Linot, and Graham 2023), minimizing total variation (Yeh et al. 2022), or incorporating inductive biases relevant to the problem (e.g., Wang et al. 2021). In the domain of material appearance modeling, the most common function used for rendering is the bidirectional reflectance distribution function (BRDF) (Nicodemus et al. 1977b). Implicit neural representations for BRDF models (e.g., neural BRDFs (NBRDFs) (Sztrajman et al. 2021)) have proven efficient in modeling material appearance due to their expressiveness and compactness, coupled with high fidelity to real-world training data. However, BRDFs are derived from first principles and must obey strict physical constraints: Helmholtz reciprocity ensures that reflected and absorbed energy are independent of the direction of light travel, and energy passivity ensures that a point cannot reflect more light than it receives (see Sec. 3 for more details). Parametric BRDF models (Guarnera et al. 2016b) inherently respect these constraints, whereas NBRDFs, optimized to match measured data accurately, are not guaranteed to comply. This non-compliance may lead to significant artifacts in physical simulations of light transport, such as path tracing, where BRDFs are expected to satisfy strict physical constraints (e.g., Figs. 1 and 3). In this paper, we propose the physically based neural BRDF (PBNBRDF), which combines multiple novel methods to ensure physical plausibility and improve the perceptual fidelity of the underlying BRDFs. Through qualitative and quantitative evaluations, we demonstrate the effectiveness of our methods for creating physically sound and accurate neural BRDFs. We highlight the necessity of physically based neural fields to bridge the gap between well-established models like BRDFs and real-world data."
https://arxiv.org/html/2411.02334v1,Diffusion-based Generative Multicasting with Intent-aware Semantic Decomposition,"Generative diffusion models (GDMs) have recently shown great success in synthesizing multimedia signals with high perceptual quality enabling highly efficient semantic communications in future wireless networks. In this paper, we develop an intent-aware generative semantic multicasting framework utilizing pre-trained diffusion models. In the proposed framework, the transmitter decomposes the source signal to multiple semantic classes based on the multi-user intent, i.e. each user is assumed to be interested in details of only a subset of the semantic classes. The transmitter then sends to each user only its intended classes, and multicasts a highly compressed semantic map to all users over shared wireless resources that allows them to locally synthesize the other classes, i.e. non-intended classes, utilizing pre-trained diffusion models. The signal retrieved at each user is thereby partially reconstructed and partially synthesized utilizing the received semantic map. This improves utilization of the wireless resources, with better preserving privacy of the non-intended classes. We design a communication/computation-aware scheme for per-class adaptation of the communication parameters, such as the transmission power and compression rate to minimize the total latency of retrieving signals at multiple receivers, tailored to the prevailing channel conditions as well as the users’ reconstruction/synthesis distortion/perception requirements. The simulation results demonstrate significantly reduced per-user latency compared with non-generative and intent-unaware multicasting benchmarks while maintaining high perceptual quality of the signals retrieved at the users. For a typical setup of multicasting street scene images to 10 users, our proposed framework achieves a 15.4%percent15.415.4\%15.4 % reduction in per-user latency at a fixed power budget, or equivalently 50%percent5050\%50 % reduction in the transmission power required to achieve a fixed per-user latency, compared with non-generative multicasting.","Semantic Communication (Semcom) is envisioned to play a crucial role in future wireless networks, specifically in emerging applications where communication of large multimodal signals with stringent latency and reliability constraints is required, e.g. the wireless metaverse and digital twins [1, 2, 3], extended/mixed reality (XR/MR) [4, 5], and holographic teleportation and the internet of senses [6, 7]. In many such applications, multicasting of multimodal signals is required, where the wireless and network resource utilization can be significantly improved by transmitting the same desired content to multiple users over shared resources. For example in the wireless metaverse, multicasting is used to communicate updates, states, or events of the virtual world to multiple users simultaneously in real-time to ensure all users remain synchronized. Another example is XR/MR streaming, where the same content may be distributed to multiple users simultaneously. In these applications, Semcom can further reduce the wireless and network resource utilization by extracting and communicating only the semantic content of interest based on the users’ intent [8, 9, 10, 11, 12]. Recently, the Generative AI (GenAI) models have shown great success in developing efficient low bitrate Generative Semantic Communication (Gen Semcom) systems [13, 14, 15, 16]. Generative models are capable of learning the general distribution of natural signals during training and synthesizing new samples with high perceptual quality at inference time. The generation process can be guided by multi-modal prompts and conditioning signals to produce high quality outputs with a desired semantic content. The emerging Generative Foundation Models and Multimodal Large Language Models (MLLMs), e.g. Sora [17], Lumiere [18], and DALL.E [19], etc., provide ample opportunities to develop efficient and universal generative Semcom frameworks [20, 21, 22, 23]. As these models possess a vast general knowledge captured via intensive pre-training on huge amount of data, they alleviate the need for a shared Knowledge Base (KB) between the semantic transmitter and receiver, thereby reducing the corresponding knowledge sharing overheads. This vast general knowledge also makes the generative Semcom framework applicable to various datasets and tasks thereby achieving universality. Moreover, generative Semcom offers an inherent advantage in privacy preservation, as only the intended features of the original signal are communicated, and the non-intended details are generated locally from random noise inputs at the receiver utilizing the generative model. Such details are thereby not precisely reproducible and are kept private from the receiver, without requiring separate anonymization techniques The GenAI models are trained to maximize the perceptual quality of the synthesized signal, and the theoretical limits of generative Semcom are governed by the rate-distortion-perception theory [24, 25, 26, 27]. This theory explores the threefold trade-off between rate, distortion, and the perceptual quality of the synthesized signal. While the rate-distortion-perception function is analytically derived only for a few source distributions, it is generally estimated empirically for natural signals, e.g. images, audio/video, point cloud, etc., by training deep source encoders with a perceptual loss function, e.g. Wasserstein distance, on large datasets [24, 25]. In generative Semcom, the transmitter extracts the intended semantics, e.g. in form of textual prompts [28, 29], compressed embeddings [30, 31], semantic/edge map [15, 21, 32] etc., which are then transmitted over the channel. The receiver uses these semantics to guide a generative model, synthesizing a signal that is semantically consistent and highly realistic. Despite the above existing works, multiuser generative semantic communications, specifically in the multicasting setup, and its scalability to increasing number of users under latency constraints remains less studied. In this paper, we develop a generative semantic multicasting framework with adaptive intent-aware resource allocation utilizing pre-trained diffusion models. We claim two key benefits for the proposed framework: Firstly, generative multicasting in our Semcom framework allows preserving privacy of the non-intended signals, while improving utilization of the wireless and network resources compared with non-generative and intent-unaware multicasting benchmarks. This also reduces the total latency, while maintaining the reconstruction/synthesis distortion/perception quality. Secondly, the adoption of pre-trained diffusion models allows a source-channel separation-based Semcom architecture, thereby alleviating the need for end-to-end joint training of the transmitter and receiver, which is required in many existing Semcom frameworks [8, 9, 10, 11]. Such a separation-based architecture offers improved adaptability to varying channel conditions, improved scalability to increasing number of users, and better compatibility with the existing design of wireless communication networks. The contributions of this work are three-fold: • We develop a generative multicasting framework where a semantic decomposition scheme splits the source signal into multiple semantic classes at the transmitter based on the multi-user intent, i.e. each user is assumed to need only a subset of the semantic classes based on its communication intent. The transmitter sends to each user only its intended classes over orthogonal wireless resources, and multicasts to all users a highly compressed semantic map over shared resources. Each user then locally synthesizes the non-intended classes using a pre-trained generative diffusion model. • For a typical image synthesis task, we train a generative diffusion model with classifier-free semantic guidance to derive estimates of the corresponding rate-distortion/perception curves achieved by the model through pre-training. We use these curves to determine the communication parameters, i.e. the transmission power and compression rate in our proposed framework. • We design a communication/computation-aware scheme for per-class adaptation of the communication parameters to minimize the total latency for varying channel conditions and users’ reconstruction/synthesis distortion/perception requirements. The signal retrieved at each user is partially reconstructed and partially synthesized utilizing the pre-trained diffusion model, and the communication parameters are optimized for latency-aware synchronisation of the synthesized and reconstructed datastreams based on the prevailing channel conditions and multi-user intent. The rest of this paper is organized as follows. In Section II we provide the related works. In Section III, we present our proposed diffusion-based generative multicasting framework and the corresponding intent and latency-aware adaptive semantic communication scheme. Simulation results are provided in Section IV, and we conclude the paper in Section V. Notations: Boldface lower and upper-case symbols denote column vectors and matrices, respectively. Calligraphic letters denote mathematical operators. [K]delimited-[]𝐾[K][ italic_K ] is {1,2,…,K}12…𝐾\{1,2,...,K\}{ 1 , 2 , … , italic_K }, and |𝐯|𝐯\left|{\bf v}\right|| bold_v | denotes the length of vector 𝐯𝐯{\bf v}bold_v. Additionally, ⊕direct-sum\oplus⊕ and ⊙direct-product\odot⊙ denote point/pixel/voxel-wise summation and product, and ⋃\bigcup⋃ is the union operator. Expectation is denoted by 𝔼[.]\mathbb{E}[.]blackboard_E [ . ]. Finally, 𝒰[.,.]\mathcal{U}[.,.]caligraphic_U [ . , . ] and 𝒩[.,.]\mathcal{N}[.,.]caligraphic_N [ . , . ] denote the uniform and Gaussian probability distributions, respectively."
https://arxiv.org/html/2411.02281v1,Conformal-in-the-Loop for Learning with Imbalanced Noisy Data,"Class imbalance and label noise are pervasive in large-scale datasets, yet much of machine learning research assumes well-labeled, balanced data, which rarely reflects real-world conditions. Existing approaches typically address either label noise or class imbalance in isolation, leading to suboptimal results when both issues coexist. In this work, we propose Conformal-in-the-Loop (CitL), a novel training framework that addresses both challenges with a conformal prediction-based approach. CitL evaluates sample uncertainty to adjust weights and prune unreliable examples, enhancing model resilience and accuracy with minimal computational cost. Our extensive experiments include a detailed analysis showing how CitL effectively emphasizes impactful data in noisy, imbalanced datasets. Our results show that CitL consistently boosts model performance, achieving up to a 6.1% increase in classification accuracy and a 5.0 mIoU improvement in segmentation. Our code is publicly available: CitL.","The success of Deep Neural Networks (DNNs) in areas such as image classification [58], object detection [41], and medical image analysis [8, 9] has largely hinged on access to large-scale, accurately labeled datasets. However, producing these high-quality annotations is costly and labor-intensive, especially in fields like medical imaging, where expert annotation is essential. As a result, more cost-effective methods, such as web scraping or crowd-sourcing, have become prevalent [39]. While these approaches facilitate data collection, they also introduce noisy labels due to misannotations and ambiguities. When coupled with the strong fitting capacity of DNNs, such label noise can significantly degrade model performance [49]. Recent approaches to address label noise in datasets generally fall into two categories: Loss/label correction methods [26, 24], which aim to adjust noisy labels by estimating a noise transition matrix or using model predictions, or sample selection methods [57, 54, 62], which identify and select possibly clean data out of noisy ones, and then use them to update the deep networks. Intuitively, if the training data can become less noisy, better generalization can be achieved. Beyond label noise, another critical challenge in real-world datasets is class imbalance [16, 1]. This imbalance arises when certain classes, often referred to as “head” classes, are represented by a large number of samples, while others, or “tail” classes, are underrepresented. Datasets with such imbalance tend to bias models toward the majority classes, leading to underrepresentation and under-learning of minority classes. Imbalance-related biases are well-documented: models trained on such data generally optimize for majority-class accuracy, resulting in reduced performance on minority categories. Common approaches to address class imbalance include resampling and reweighting, which adjust either the sample distributions or the class-specific weights to encourage learning from minority classes. In many real-world scenarios, noisy labels and class imbalance coexist, presenting a multifaceted problem that single-focus methods struggle to solve. However, techniques aimed at mitigating label noise typically assume a balanced dataset, ignoring the complexities that arise in imbalanced data. Similarly, methods addressing class imbalance generally presume that labels are accurate, failing to account for noise in the underrepresented classes. In such cases, label noise can exacerbate learning bias: when minority classes contain mislabeled samples, models may treat hard-to-learn but clean samples as noise, neglecting genuine examples from the tail classes. This overlap complicates the model’s ability to distinguish between true minority samples and noisy samples from majority classes. These interconnected challenges highlight the need for a more robust approach to training DNNs on imbalanced and noisy data. To address this, we introduce Conformal-in-the-Loop (CitL), a novel uncertainty-aware training method that dynamically adjusts sample weights and prunes data in response to uncertainty levels calculated through conformal prediction. Specifically, CitL applies conformal prediction to assess sample uncertainty, which is then used to strategically modify training in two primary ways: (1) Sample Weighting – assigning higher weights to samples with moderate uncertainty that are likely to be informative, thereby enhancing learning in minority classes, and (2) Sample Pruning – identifying and removing samples with high uncertainty that are likely mislabeled, reducing the noise that would otherwise degrade model performance. By leveraging these two mechanisms, CitL selectively refines the training data, ensuring the model focuses on reliable, informative samples. Through this targeted curation of data, CitL addresses the dual challenges of imbalance and noise while maintaining computational efficiency. Extensive experimental evaluations demonstrate that CitL significantly improves model performance across common data challenges of imabalanced, noisy labels. To summarize, our contributions in this paper are: 1. We demonstrate that conformal prediction is an effective tool for balancing easy and challenging examples during training. 2. We design a weighting and pruning mechanism that performs well under label noise, showing particular effectiveness at lower noise levels commonly encountered in practice. 3. We prune highly uncertain examples at training time by leveraging the Least-Ambiguous set-valued Classifier (LAC)’s ability to produce empty prediction sets. 4. We apply dynamic weighting to examples at different stages of training to address class imbalance, with higher weights early on to condition the network on difficult features. 5. We evaluate CitL across multiclass classification and semantic segmentation tasks, demonstrating consistent performance improvements in both settings. 6. We achieve minimal computational overhead, increasing training time per step by only 11% for classification and 4% for segmentation tasks."
https://arxiv.org/html/2411.02184v1,Double Descent Meets Out-of-Distribution Detection: Theoretical Insights and Empirical Analysis on the role of model complexity,"While overparameterization is known to benefit generalization, its impact on Out-Of-Distribution (OOD) detection is less understood. This paper investigates the influence of model complexity in OOD detection. We propose an expected OOD risk metric to evaluate classifiers confidence on both training and OOD samples. Leveraging Random Matrix Theory, we derive bounds for the expected OOD risk of binary least-squares classifiers applied to Gaussian data. We show that the OOD risk depicts an infinite peak, when the number of parameters is equal to the number of samples, which we associate with the double descent phenomenon. Our experimental study on different OOD detection methods across multiple neural architectures extends our theoretical insights and highlights a double descent curve. Our observations suggest that overparameterization does not necessarily lead to better OOD detection. Using the Neural Collapse framework, we provide insights to better understand this behavior. To facilitate reproducibility, our code will be made publicly available upon publication.","In recent years, large neural networks have seen increased use in Machine Learning due to their impressive generalization properties (Brown, 2020; Dubey et al., 2024). While empirical evidence suggests that rich machine learning systems obtain near-optimal generalization results when trained to interpolate training data (Zhang et al., 2021), the classical bias-variance trade-off theory (Geman et al., 1992) suggests that such models overfit and generalize poorly. Indeed, the classical literature describes the generalization error with respect to the model complexity as a U-shaped curve and suggests finding a model between underfitting and overfitting, i.e., a model rich enough to express underlying structure in the data and simple enough to avoid fitting spurious patterns. To bridge the gap between the classical theory and the modern practice, Belkin et al. (2019) introduced the concept of “double descent” within a unified generalization error curve. In this setting, for “small” model complexities, the generalization error curve exhibits the U-shaped curve described by the bias-variance trade-off. However, when the model complexity is higher than the interpolation threshold, i.e., when the model is rich enough to fit the training data, increasing the model complexity leads to a second decrease in the generalization error. A popular intuitive explanation of this phenomenon is that by considering large model complexities that contain more candidate predictors compatible with the training data, we are also able to find interpolating functions that are “simpler” and are smoother to follow a form of Occam’s razor (Belkin et al., 2019). Although the double descent phenomenon provides valuable insights to understand generalization of rich models on unseen data, its understanding on Out-Of-Distribution (OOD) detection has received less attention. OOD detection addresses a distinct challenge in deep neural networks (DNNs): their tendency to make high-confidence predictions, even for inputs that differ significantly from the training data. While generalization focuses on the model’s ability to classify data that has shifted, OOD detection emphasizes the model’s capacity to recognize when a shift is too large and refrain from confident predictions. In real-world applications, reliable OOD detection is crucial to ensuring the safety and reliability of AI systems. This includes fields such as healthcare (Schlegl et al., 2017), industrial inspection (Paul Bergmann & Stege, 2019), and autonomous driving (Kitt et al., 2010). Therefore, understanding the influence of the model complexity on OOD detection is crucial for model selection in critical tasks. While overparameterization is known to benefit generalization, its effects on OOD detection still remain limited. In this work, we investigate the role of the model complexity in OOD detection. In particular, we highlight a double descent phenomenon similar to the one observed for generalization error. To the best of our knowledge, the double descent phenomenon has never been observed in OOD detection. The results of our theoretical and empirical analyses are outlined below. Contributions. We make the following contributions, taking a step towards a better theoretical and empirical understanding of the influence of model complexity on OOD detection: 1. We propose an expected OOD risk metric to evaluate classifiers confidence on both training and OOD samples. 2. Using Random Matrix Theory, we derive bounds for both the expected risk and OOD risk of binary least-squares classifiers applied to Gaussian data with respect to the model complexity. We show that both risks exhibit an infinite peak, when the number of parameters is equal to the number of samples, which we associate with the double descent phenomenon. 3. We empirically observe a double descent phenomenon curve in various OOD detection methods and across multiple neural architectures, including transformer-based (ViT, Swin) and convolutional-based models (ResNet, CNN). 4. We also observe that OOD detection in the overparametrized regime is not guaranteed to be better than in the underparametrized regime. Using the Neural Collapse framework Papyan et al. (2020), we propose to better explain this architecture-dependant improvement on OOD detection with overparametrization. To facilitate reproducibility, our code will be made publicly available upon publication."
https://arxiv.org/html/2411.02175v1,SAFE: Slow and Fast Parameter-Efficient Tuning for Continual Learning with Pre-Trained Models,"Continual learning aims to incrementally acquire new concepts in data streams while resisting forgetting previous knowledge. With the rise of powerful pre-trained models (PTMs), there is a growing interest in training incremental learning systems using these foundation models, rather than learning from scratch. Existing works often view PTMs as a strong initial point and directly apply parameter-efficient tuning (PET) in the first session for adapting to downstream tasks. In the following sessions, most methods freeze model parameters for tackling forgetting issues. However, applying PET directly to downstream data cannot fully explore the inherent knowledge in PTMs. Additionally, freezing the parameters in incremental sessions hinders models’ plasticity to novel concepts not covered in the first session. To solve the above issues, we propose a Slow And Fast parameter-Efficient tuning (SAFE) framework. In particular, to inherit general knowledge from foundation models, we include a transfer loss function by measuring the correlation between the PTM and the PET-applied model. After calibrating in the first session, the slow efficient tuning parameters can capture more informative features, improving generalization to incoming classes. Moreover, to further incorporate novel concepts, we strike a balance between stability and plasticity by fixing slow efficient tuning parameters and continuously updating the fast ones. Specifically, a cross-classification loss with feature alignment is proposed to circumvent catastrophic forgetting. During inference, we introduce an entropy-based aggregation strategy to dynamically utilize the complementarity in the slow and fast learners. Extensive experiments on seven benchmark datasets verify the effectiveness of our method by significantly surpassing the state-of-the-art. Code will be available at https://github.com/MIFA-Lab/SAFE.","Continual Learning (CL) requires deep learning models to incrementally incorporate new concepts from open-world data streams, while retaining previously learned knowledge. This presents a more challenging yet practical setting compared to traditional deep learning, which typically recognizes only closed-set categories. A variety of methods have been proposed for continual learning, including regularization-based [17, 47, 20], rehearsal-based [31, 5, 14], and dynamic network-based approaches [1, 44, 43]. These methods often assume that the model is trained from scratch, resulting in a substantial performance gap when compared to the joint training upper-bound. Most recently, with the emergence of powerful pre-trained models, there has been growing interest in utilizing these foundational models as starting points for continual learning [52, 49, 23]. Pre-Trained Models (PTMs) which are often trained on vast datasets, encapsulate a wealth of general knowledge, effectively enhancing the performance of deep learning models in continual learning scenarios. As shown in the left part of Fig. 1(a), for adapting PTMs from pre-training datasets to continual learning datasets, prevailing works resort to parameter-efficient tuning (PET) techniques [6, 21, 16] in the first session. To restrain catastrophic forgetting, in incremental sessions, these works set parameters of the adapted model frozen [15, 52, 23, 2] and only update the classification weights in a training-free manner (i.e., without gradient updates) to accommodate novel classes. Figure 1: Comparisons of (a) prevailing PTM-based CL methods [52, 23, 2] and our Slow And Fast parameter-Efficient tuning (SAFE). The right part (b) illustrates several parameter-efficient tuning (PET) blocks: Adapter [6], Scale & Shift (SSF) [21], and Visual Prompt Tuning (VPT) [16]. However, the above methods have two main limitations. First, direct parameter-efficient tuning in the first session will largely lose the general knowledge inherent in PTMs. This is because PTMs are pre-trained on a multitude of datasets while the dataset in the first session only contains relatively limited samples. Without proper transfer mechanisms, the knowledge from PTMs may be overwritten by the adapted model, which impedes the model’s generalizability to unseen classes. Second, freezing parameters in the following sessions will hinder the plasticity of the model to further absorb new concepts not learned in the first session, resulting in a sub-optimal solution. Although several efforts have been made to mitigate the second limitation, existing works still face certain constraints such as additional storage requirement [49, 35], inferior online branch performance [8] and linearly increased model complexity [54]. Based on the above observations, in this paper, we propose Slow And Fast parameter-Efficient tuning (SAFE) to address existing challenges. In particular, SAFE demonstrates a unified framework that effectively inherits the generalizability of PTMs using slow parameter-efficient tuning (S-PET) and provides sufficient plasticity to learn task-specific knowledge in each incremental session using the fast one (F-PET). Meanwhile, SAFE does not require storing class distributions for data replay and only incurs constant-level additional computation and memory costs. To achieve the above goals, SAFE employs distinct strategies for the first and subsequent sessions. In the first session, we focus on explicitly transferring general knowledge from pre-trained models (PTMs) by introducing a knowledge transfer loss. This involves computing a correlation matrix between feature embeddings from the PTM and the model with parameter-efficient tuning (PET). The diagonal elements of this matrix are maximized to ensure that the features remain consistent across both models, effectively aligning the PET-applied model’s performance with that of the PTM. Simultaneously, minimizing the off-diagonal elements reduces redundancy in the embeddings, enhancing feature discriminability. After this tuning process, parameters can retain generalizable knowledge from the PTM. To prevent forgetting this knowledge, these trained parameters are subsequently frozen, with only the classification weights being updated, thus designating this model as the slow learner. In the incremental sessions, to address the plasticity limitations of the slow learner, we introduce a fast learner capable of continuously integrating new concepts. Given the persistent challenge of catastrophic forgetting in continual learning, the slow learner guides the training of the fast learner. Concretely, we employ a feature alignment loss to minimize the distance between the embeddings of both learners on a hypersphere. Additionally, a cross-classification loss is proposed to ensure compatibility between the features of the fast learner and the classification weights of the slow learner, and vice versa. This approach allows the fast learner to assimilate new knowledge without storing exemplars or distributions, while also mitigating forgetting. For robust predictions, an entropy-based aggregation strategy is implemented during inference to dynamically leverage the complementary strengths of the slow and fast learners. To summarize, the contributions of our paper are three-fold: • To inherit the generalizable knowledge in PTMs that has been overlooked in existing continual learning works, we propose to explicitly transfer knowledge from the PTM to a slow learner. Once trained, the slow learner can generalize well to classes in incremental sessions. • For improving the plasticity of CL models, we include a fast learner with guidance from the slow learner to continuously incorporate novel concepts. Moreover, by aggregating both slow and fast learners into a unified framework SAFE, robust predictions can be further made. • The superiority of SAFE is validated on seven continual learning datasets where our method consistently achieves remarkable state-of-the-art performance. For example, our method surpasses the second-best result on ImageNet-A over 4%percent44\%4 %."
https://arxiv.org/html/2411.02068v1,Model Integrity when Unlearning with T2I Diffusion Models,"The rapid advancement of text-to-image Diffusion Models has led to their widespread public accessibility. However these models, trained on large internet datasets, can sometimes generate undesirable outputs. To mitigate this, approximate Machine Unlearning algorithms have been proposed to modify model weights to reduce the generation of specific types of images, characterized by samples from a “forget distribution”, while preserving the model’s ability to generate other images, characterized by samples from a “retain distribution”. While these methods aim to minimize the influence of training data in the forget distribution without extensive additional computation, we point out that they can compromise the model’s integrity by inadvertently affecting generation for images in the retain distribution. Recognizing the limitations of FID and CLIPScore in capturing these effects, we introduce a novel retention metric that directly assesses the perceptual difference between outputs generated by the original and the unlearned models. We then propose unlearning algorithms that demonstrate superior effectiveness in preserving model integrity compared to existing baselines. Given their straightforward implementation, these algorithms serve as valuable benchmarks for future advancements in approximate Machine Unlearning for Diffusion Models.","Context Recent years have witnessed rapid advancements in text-to-image generative models, particularly Diffusion Models, leading to the widespread availability of powerful models like Stable Diffusion and Midjourney (Rombach et al., 2021; Midjourney et al., 2023). However, these models may inadvertently learn undesirable concepts from their training data, posing challenges at deployment (Fan et al., 2024; Heng and Soh, 2023; Wu et al., 2024a). Consequently, research into methods for removing such concepts from model weights has gained significant traction. One promising avenue is Machine Unlearning (MU), which broadly aims to mitigate the influence of specific training data subsets (forming the “forget set” 𝒟fsubscript𝒟𝑓\mathcal{D}_{f}caligraphic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT) after pre-training (Shaik et al., 2023). Due to the substantial computational costs associated with pre-training large models, both retraining and exact MU approaches (Guo et al., 2019; Thudi et al., 2022; Bourtoule et al., 2019) prove impractical. This has spurred the development of more feasible approximate MU techniques (Shaik et al., 2023; Izzo et al., 2021; Fan et al., 2024), often referred to as “forgetting” or “erasing concepts” in the context of Diffusion Models (Gandikota et al., 2023; Heng and Soh, 2023). Motivation: Ensuring Model Integrity After Unlearning Evaluating the effectiveness of approximate MU algorithms necessitates assessing both the extent of forgetting and the preservation of model quality/integrity on the retain dataset 𝒟rsubscript𝒟𝑟\mathcal{D}_{r}caligraphic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT. Existing approaches for diffusion models primarily rely on FID (Heusel et al., 2017) and CLIPScore (Hessel et al., 2021) metrics, applied to a set of retain prompts. However, recent work (Zhang et al., 2024c) has criticized this methodology, advocating for the development of new benchmark datasets and the use of fine-tuned classifiers to gauge in-domain and cross-domain retention because quantifying retention using a single metric lacks precision in the generative context (Zhang et al., 2024c, Sec. 2, pg. 4). While acknowledging the potential benefits of such an approach (e.g. precision and granularity on sub-domains of the retain dataset), we highlight several limitations. Firstly, developing new benchmarks or fine-tuning classifiers incurs significant costs. Additionally, using multiple fine-tuned classifiers results in a non-well-ordered set of in-domain and out-of-domain scores, complicating evaluation. Furthermore, classifiers may not adequately capture subtle qualitative changes in model outputs after unlearning (Fig. 2, §4). Similarly, FID and CLIPScore may fail to detect unintended changes on specific subsets of the retain set, as illustrated in Fig. 1, where forgetting Van Gogh’s style inadvertently impacts the generation of Vermeer’s style. While FID could be computed on subsets of the retain set, this is computationally expensive due to its distributional nature and potential scaling issues across subsets (see, for example, (Lyu et al., 2024, Tab. 1) where FID ranges can change significantly across subsets). To address these challenges, we propose a novel integrity metric (Sec. 3) that directly quantifies the similarity between images generated by the unlearned and original checkpoints on the retain set. This approach by-passes the need for ad-hoc evaluation datasets or fine-tuned classifiers, offering a more practical alternative to estimating model closeness via parameter posteriors or model predictions (Hu et al., 2022). Our empirical analysis reveals that current methods often struggle to maintain integrity, leading to unintended consequences such as the removal of multiple artistic styles when targeting a single style. This underscores a gap in existing approaches, motivating our development of two novel algorithms that prioritize integrity preservation. Contributions We summarize our contributions as follows: 1. Integrity Metric: We introduce the integrity metric, ℐℐ\mathcal{I}caligraphic_I (§3), providing a quantitative measure to assess preservation of generated outputs after unlearning. 2. Integrity-Driven Algorithms: We present two novel algorithms designed to prioritize integrity preservation, addressing both supervised and unsupervised scenarios. 3. Empirical validation: Our proposed algorithms demonstrate: • Improved integrity and reduced side effects: Superior performance in maintaining integrity and minimizing unintended consequences (Table 1, Figs. 1, 2). • Simplicity: straightforward implementation, facilitating future research by serving as effective baselines (implementation details available in the Appendix A.2). • Better behavior in data-limited scenarios: they outperform current methods when one needs to rely on generated images as the original training data is inaccessible (§4). Reproducibility We include Python code snippets of the implementation in Appendix A.2."
https://arxiv.org/html/2411.02038v1,Addressing Representation Collapse in Vector Quantized Models with One Linear Layer,"Vector Quantization (VQ) is a widely used method for converting continuous representations into discrete codes, which has become fundamental in unsupervised representation learning and latent generative models. However, VQ models are often hindered by the problem of representation collapse in the latent space, which leads to low codebook utilization and limits the scalability of the codebook for large-scale training. Existing methods designed to mitigate representation collapse typically reduce the dimensionality of latent space at the expense of model capacity, which do not fully resolve the core issue. In this study, we conduct a theoretical analysis of representation collapse in VQ models and identify its primary cause as the disjoint optimization of the codebook, where only a small subset of code vectors are updated through gradient descent. To address this issue, we propose SimVQ, a novel method which reparameterizes the code vectors through a linear transformation layer based on a learnable latent basis. This transformation optimizes the entire linear space spanned by the codebook, rather than merely updating the code vector selected by the nearest-neighbor search in vanilla VQ models. Although it is commonly understood that the multiplication of two linear matrices is equivalent to applying a single linear layer, our approach works surprisingly well in resolving the collapse issue in VQ models with just one linear layer. We validate the efficacy of SimVQ through extensive experiments across various modalities, including image and audio data with different model architectures. The results show that SimVQ not only effectively addresses the problem of representation collapse but also proves highly adaptable and easy to implement, suggesting its broad applicability in diverse machine learning contexts. Our code is available at https://github.com/youngsheen/SimVQ.","In recent years, vector quantization (VQ) (van den Oord et al., 2017; Razavi et al., 2019) has emerged as a foundational technique in unsupervised representation learning (Baevski et al., 2020; Bruce et al., 2024) and latent generative models (Rombach et al., 2022; Yu et al., 2022a; b; Borsos et al., 2023; Wang et al., 2023; Zhu et al., 2024b). By converting continuous representations into discrete codes, VQ models can effectively identify the inherent structure of data and enable various discrete modeling methods on continuous data, from high-quality image generation (Esser et al., 2021) to audio synthesis (Défossez et al., 2023). The recent success of Large Language Models (LLMs) (Achiam et al., 2023) has highlighted the effectiveness of next-token prediction as a powerful and versatile training objective. Consequently, VQ models are taken as the direct method to transform data from various modalities (Zhang et al., 2023a; Sun et al., 2024; Team, 2024) or scientific domains (Gao et al., 2024) to discrete sequences for next token prediction training. However, attempts to integrate VQ models as multimodal tokenizers to leverage the scaling laws of LLMs face significant challenges because of the difficulty of expanding the codebook. For example, the Chameleon model (Team, 2024) constrains its codebook size to 8⁢k8𝑘8k8 italic_k, which is significantly trailing behind the vocabulary size of LLMs (e.g., LLaMA3’s vocabulary size is 128⁢k128𝑘128k128 italic_k (Dubey et al., 2024)). Figure 1: Comparison of Vanilla VQ and SimVQ. (a): (left) Disjoint optimization in Vanilla VQ. Only the nearest codes are updated, resulting in a high percentage of “dead” codes that are not updated. (b): (right) Joint optimization in SimVQ. The entire codebook is updated with a latent basis, ensuring all codes remain active. There is a broad agreement that increasing vocabulary size can consistently improve the performance of LLMs (Tao et al., 2024). However, recent studies (Zhu et al., 2024a) indicate that traditional VQ models often fail to utilize the additional parameters introduced by codebook expansion, leaving most codes inactive during training. The contradiction between codebook expansion and low codebook utilization in VQ models is known as the representation collapse problem (Roy et al., 2018), where increasing the codebook size fails to improve the performance. To address these discrepancies, we conduct a theoretical analysis of the optimization procedure of VQ models and identify that the disjoint optimization of the codebook is the root cause of representation collapse. As illustrated in Fig. 1(a), the core mechanism of VQ models involves a nearest-neighbor replacement strategy, where the encoder’s output features are replaced by the nearest vector in the codebook to serve as input to the decoder. The indices of the nearest vector are taken as the discrete representation of the data. This nearest-selection operator results in only a subset of codes being updated through gradient descent, while the remaining codes remain unchanged. Recently, some approaches have been proposed to mitigate representation collapse. FSQ (Mentzer et al., 2024), LFQ (Yu et al., 2024) and ViTVQGAN (Yu et al., 2022a) reduce the dimension of the latent space to a very small scale (e.g., 8 vs. 128) to alleviate the curse of dimensionality, thereby improving the overlap between the encoder’s features and the codebook. However, while these methods enhance codebook utilization, they do so at the cost of model capacity, leading to worse performance compared to vanilla VQ models when the codebook size is small and representation collapse is not severe. Another approach, VQGAN-LC (Zhu et al., 2024a), initializes the codebook with features extracted from the pre-trained CLIP model (Radford et al., 2021) to create a well-structured latent space that better matches the distribution of the encoder output. Nevertheless, the latent space defined by an external pre-trained model limits the model’s ability to generalize to diverse datasets and reaches a performance plateau as the codebook size increases. These limitations highlight the need for a more effective method to improve codebook utilization without compromising model capacity or relying on external models. We critically assess prevalent methodologies and reveal that optimizing the latent space rather than individual code vectors is key to preventing representation collapse. Building on this insight, we introduce a simple yet effective method, termed SimVQ, to directly update the latent space spanned by the codebook by linear transforming the code vectors via a learnable latent basis. Specifically, the vectors in the codebook are reparameterized as a linear combination of the basis in the learnable linear layer 𝑾𝑾\bm{W}bold_italic_W: 𝑪∈ℝK×d⇒𝑪⁢𝑾⁢with⁢𝑾∈ℝd×d,𝑪superscriptℝ𝐾𝑑⇒𝑪𝑾with𝑾superscriptℝ𝑑𝑑\bm{C}\in\mathbb{R}^{K\times d}\Rightarrow\bm{CW}~{}\text{with}~{}\bm{W}\in% \mathbb{R}^{d\times d},bold_italic_C ∈ blackboard_R start_POSTSUPERSCRIPT italic_K × italic_d end_POSTSUPERSCRIPT ⇒ bold_italic_C bold_italic_W with bold_italic_W ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_d end_POSTSUPERSCRIPT , (1) where K𝐾Kitalic_K denotes the codebook size and d𝑑ditalic_d represents the dimension of latent space. This reparameterization with linear transformation disentangles the optimization of the codebook into two components: the coefficient matrix 𝑪𝑪\bm{C}bold_italic_C and the basis of linear space 𝑾𝑾\bm{W}bold_italic_W respectively. As illustrated in Fig. 1(b), by optimizing the basis matrix 𝑾𝑾\bm{W}bold_italic_W, the latent space spanned by 𝑪⁢𝑾𝑪𝑾\bm{CW}bold_italic_C bold_italic_W is rotated and stretched to match encoder’s output feature. The entire codebook is updated jointly to prevent the representation collapse problem. The simplicity of the proposed method makes it highly portable and easily adaptable for improving VQ-based models across a wide range of domains, requiring only one linear layer. In summary, our contributions to vector quantized models are as follows: • We theoretically analyze the representation collapse problem of VQ models and reveal that optimizing the latent space spanned by the codebook, rather than focusing on the individual code vectors, is crucial to addressing this issue. • We propose a novel method, SimVQ, which reparameterizes the codebook vectors in VQ models via a linear transformation with a learnable latent basis. This simple yet effective approach is highly adaptable and easy to implement, making it broadly applicable across various machine learning contexts. • We conduct an extensive evaluation of SimVQ across diverse modalities, including image and audio with different model architectures. The results show that SimVQ not only effectively addresses the representation collapse problem by achieving near-complete codebook utilization regardless of the codebook size, but also establishes new state-of-the-art performance. Furthermore, when scaling up the codebook size, SimVQ consistently delivers improved results."
https://arxiv.org/html/2411.01919v1,Real-Time Polygonal Semantic Mapping for Humanoid Robot Stair Climbing,"We present a novel algorithm for real-time planar semantic mapping tailored for humanoid robots navigating complex terrains such as staircases. Our method is adaptable to any odometry input and leverages GPU-accelerated processes for planar extraction, enabling the rapid generation of globally consistent semantic maps. We utilize an anisotropic diffusion filter on depth images to effectively minimize noise from gradient jumps while preserving essential edge details, enhancing normal vector images’ accuracy and smoothness. Both the anisotropic diffusion and the RANSAC-based plane extraction processes are optimized for parallel processing on GPUs, significantly enhancing computational efficiency. Our approach achieves real-time performance, processing single frames at rates exceeding 30⁢H⁢z30𝐻𝑧30~{}Hz30 italic_H italic_z, which facilitates detailed plane extraction and map management swiftly and efficiently. Extensive testing underscores the algorithm’s capabilities in real-time scenarios and demonstrates its practical application in humanoid robot gait planning, significantly improving its ability to navigate dynamic environments.","The capability to perceive the environment accurately and construct effective traversable semantic maps is crucial for humanoid robots navigating complex environments. In particular, for humanoid robots, securing stable footholds in sufficiently large planar areas is essential for safety and accuracy in movement planning. Due to their greater degrees of freedom, humanoid robots can undertake more complex tasks, such as jumping and climbing stairs, compared to traditional wheeled robots. However, these complex movements are based on the robot’s ability to precisely and efficiently sense its surroundings and build reliable semantic maps. Hence, enhancing planar semantic mapping for humanoid robots is paramount and warrants further research and discussion. The construction of reliable, traversable planar semantic maps for humanoid robots has long captured the attention of multiple disciplines including computer vision and SLAM (Simultaneous Localization and Mapping). In the domain of computer vision, numerous effective algorithms have been proposed to enhance performance metrics such as real-time responsiveness and Intersection over Union (IOU) by focusing on extracting planes from ordered structures like depth images, which generally perform better in real-time and accuracy than methods based on unordered point clouds [1]. Additionally, the field of SLAM has utilized planar and curved spatial semantic information for robot localization and mapping [2, 3], with several successful projects now open-sourced, such as elevation mapping with GPU [4]. Figure 1: Planar polygon semantic mapping results of spiral and straight stairs Figure 2: Overview of the Planar Polygonal Semantic Mapping System Framework. The system inputs are depth images and robot pose estimates, which are processed to generate a terrain’s polygonal planar semantic map. This map can be directly utilized to plan humanoid robot gaits. However, current methods often overlook the impact of filtering techniques on extracting planes from raw data, which can lead to significant discrepancies in performance between noise-free simulated data and real depth camera data. While results showing an IOU of over 95% in simulations drop below 80% with real sensor data may be acceptable for quadruped robots, the requirement for larger stable areas for humanoid robots makes resolving this issue indispensable. The accuracy of the extracted plane results, including the planar normal vectors and centroid heights, is crucial as it directly influences the safety of the robot during its operation. Moreover, this thesis also considers real-time performance as a critical metric, placing computationally intensive and iterative image processing algorithms on the GPU to ensure that the processing time for each frame remains below the sensor cycle time. Issues such as dynamic obstacles and odometric drift also need to be addressed in map construction. Our previous work [5] proposed using supervoxel-based plane segmentation for humanoid robot stairs. Supervoxel is a point cloud clustering method that is time-consuming. Thus, in this paper, we propose a system that performs real-time extraction of planar polygons and incremental semantic map management. This system first applies anisotropic diffusion filtering [6] to significantly suppress noise in depth images, thereby enhancing the quality of depth images. It then uses edge detection algorithms to extract contours of planes from transformed normal vector images and simplifies these contours into polygons. Subsequently, the RANSAC algorithm fits the optimal plane equation for each polygonal region from the depth images. This process yields the results of polygonal plane extraction from single frames. To construct a global polygonal semantic map, the system integrates robot pose estimates obtained from odometry/SLAM, allowing newly observed polygons to be added to the map. During the polygon merging process, a simple method is employed to estimate and compensate for odometric drift in the vertical direction, ensuring more precise mapping results. Fig. 1 illustrates the mapping results for different staircases. Building on this, we have also integrated the humanoid robot’s gait planning algorithm with our mapping system, as shown in Fig. 2. The code can be accessed at https://github.com/BTFrontier/polygon_mapping."
https://arxiv.org/html/2411.01904v1,FPPL: An Efficient and Non-IID RobustFederated Continual Learning Framework,"Federated continual learning (FCL) aims to learn from sequential data stream in the decentralized federated learning setting, while simultaneously mitigating the catastrophic forgetting issue in classical continual learning. Existing FCL methods usually employ typical rehearsal mechanisms, which could result in privacy violations or additional onerous storage and computational burdens. In this work, an efficient and non-IID robust federated continual learning framework, called Federated Prototype-Augmented Prompt Learning (FPPL), is proposed. The FPPL can collaboratively learn lightweight prompts augmented by prototypes without rehearsal. On the client side, a fusion function is employed to fully leverage the knowledge contained in task-specific prompts for alleviating catastrophic forgetting. Additionally, global prototypes aggregated from the server are used to obtain unified representation through contrastive learning, mitigating the impact of non-IID-derived data heterogeneity. On the server side, locally uploaded prototypes are utilized to perform debiasing on the classifier, further alleviating the performance degradation caused by both non-IID and catastrophic forgetting. Empirical evaluations demonstrate the effectiveness of FPPL, achieving notable performance with an efficient design while remaining robust to diverse non-IID degrees. Code is available at: https://github.com/ycheoo/FPPL.","Federated Learning (FL) [29, 43, 18, 16] is considered as a specific distributed learning paradigm in which clients update their models by aggregating information from different client-side local models without uploading private data, thereby ensuring data privacy. Most existing FL methods assume that the data in each client is static without dynamically changing over time. However, in many real-world applications, the data of each client accumulates continuously, which would lead to the emergence of new classes or potential shifts in data distribution. Typical static FL methods become not applicable, the catastrophic forgetting issue, degrading performance on previous tasks, would significantly occur when data dynamically grows [20, 31], which could be exacerbated by the non-IID (non-independent and identically distributed) issue of FL [48, 24]. Therefore, it becomes natural to consider the Federated Continual Learning (FCL) [44, 7]. FCL aims to establish federated learning methods that can adapt to new data streams and mitigate catastrophic forgetting [31] on previous tasks. However, prevalent methods often resort to rehearsal [7, 30], a common technique in continual learning [31, 41, 49], where data from prior tasks is utilized for subsequent training to mitigate catastrophic forgetting. This practice, integrating old task data into the current task, raises privacy concerns or adds storage and computational overhead in FCL. For instance, GLFC [7] locally store old task data for rehearsal, posing privacy risks and introducing additional burdens on clients. Moreover, FedCIL [30] uses generative replay to produce synthetic data, avoiding raw data maintenance but still incurring additional storage and computational costs due to the use of generative models. Rehearsal-based methods heavily rely on incorporating old task data into subsequent training, potentially leading to privacy issues or increased storage and training pressure on the local model. As a consequence, these methods lack scalability when faced with an increasing number of new tasks. Instead, the most efficient approach is to update the model by training solely on new data, but such a rehearsal-free method may result in catastrophic forgetting. Fortunately, the utilization of pre-trained models [9] has emerged as a promising solution to mitigate catastrophic forgetting without the need for rehearsal[40, 39, 32]. FCL methods utilizing pre-trained transformers [36, 6, 8] have gained prominence [27, 1]. The use of pre-trained transformers eliminates the need for rehearsal, providing an efficient alternative for mitigating catastrophic forgetting. For instance, recent work [1] proposes a rehearsal-free FCL approach based on pre-trained Vision Transformer [8], demonstrating a communication-efficient transmission strategy. Regrettably, methods based on pre-trained transformers have not sufficiently addressed the critical challenge of non-IID issue in federated learning [48, 24], lacking developed methodologies to mitigate resulting performance degradation effectively. In federated learning, the non-IID nature introduces data heterogeneity, leading to significant bias in models trained on different clients, and aggregating these biased models directly may result in a suboptimal global model [24, 46]. Similar to conventional federated learning, FCL encounters performance degradation with increasing non-IID degrees, as outlined in [47]. Additional knowledge sharing among clients is necessary to mitigate the impact of non-IID. To address the challenge of mitigating catastrophic forgetting without rehearsal, we draw inspiration from prompt-based continual learning methods [40, 39, 32], which utilize lightweight prompts [22, 25, 17] to instruct pre-trained transformers. The lightweight nature of prompts makes them well-suited for federated learning scenarios, where they can effectively reduce communication cost of clients, yielding promising performance without rehearsal. To tackle the issue of non-IID, lightweight prototypes [34], defined as the mean of extracted features from data, have been employed as efficient and privacy-preserving information carriers in federated learning [35, 15]. Building on the insights from [35, 15], we propose using prototypes as a superior alternative to rehearsal for addressing performance degradation caused by non-IID issue. In this work, an efficient and non-IID robust federated continual learning framework, called Federated Prototype-Augmented Prompt Learning (FPPL), is proposed. The proposed FPPL leverages both prompt tuning and prototypes to tackle the challenges of non-IID and catastrophic forgetting. Specifically, task-specific prompts are integrated with a fusion function to fully leverage acquired knowledge, thereby mitigating catastrophic forgetting. To address performance degradation caused by both non-IID data and catastrophic forgetting, a classifier debiasing mechanism based on local prototypes is designed. These local prototypes are aggregated into global prototypes at the server and returned to clients, achieving unified representations across clients and further addressing the non-IID issue. The main contributions can be summarized as: - The prompt tuning based on pre-trained transformers is introduced into FCL, eliminating the need for rehearsal and reducing communication cost; - The prototypes are employed to address the non-IID issue in FL through lightweight transmission while stabilizing local training and mitigating catastrophic forgetting globally; - Extensive experiments on ImageNet-R, CUB-200 and CIFAR-100 can validate the effectiveness of FPPL, achieving notable performance with strong non-IID robustness."
https://arxiv.org/html/2411.01833v1,OwMatch: Conditional Self-Labeling with Consistency for Open-World Semi-Supervised Learning,"Semi-supervised learning (SSL) offers a robust framework for harnessing the potential of unannotated data. Traditionally, SSL mandates that all classes possess labeled instances. However, the emergence of open-world SSL (OwSSL) introduces a more practical challenge, wherein unlabeled data may encompass samples from unseen classes. This scenario leads to the misclassification of unseen classes as known ones, consequently undermining classification accuracy. To overcome this challenge, this study revisits two methodologies from self-supervised and semi-supervised learning, self-labeling and consistency, tailoring them to address the OwSSL problem. Specifically, we propose an effective framework called OwMatch, combining conditional self-labeling and open-world hierarchical thresholding. Theoretically, we analyze the estimation of class distribution on unlabeled data through rigorous statistical analysis, thus demonstrating that OwMatch can ensure the unbiasedness of the self-label assignment estimator with reliability. Comprehensive empirical analyses demonstrate that our method yields substantial performance enhancements across both known and unknown classes in comparison to previous studies. Code is available at https://github.com/niusj03/OwMatch.","Deep learning has made remarkable success in various tasks by leveraging substantial labeled training data [22, 21, 13]. However, the costly and time-consuming labeling process limits their application in practical scenarios. Semi-supervised learning (SSL) significantly reduces the dependency on labeled data by exploring the inherent structure of unlabeled data [16]. Despite promising results, SSL methods assume a closed-world scenario where, though limited, all classes possess labeled instances. This assumption may be violated due to difficulties in data collection, such as in medical diagnostics, where it is common to encounter new symptoms or fail to annotate due to technical constraints. As a result, only a subset of the categories can be precisely labeled during the annotation process. Recently, numerous studies have sought to identify such novel classes effectively. Open-world SSL (OwSSL) is innovative in promoting dual objectives: classifying instances of seen classes and discovering instances of novel classes [5]. A notable challenge in OwSSL is the confirmation bias of model: model tends to predict instances as seen classes owing to the lack of ground-truth supervision of novel-class instances. To eliminate this bias, existing works utilize unsupervised clustering methods, including contrastive loss and binary cross-entropy (BCE) loss, to group pairs identified by similarity metrics [5, 15]. Among these unsupervised techniques, self-labeling [1, 6] has shown remarkable success, which involves assigning self-labels to unlabeled data, with the generation of high-quality self-labels being the key factor. Previous studies utilize optimal transport to align the self-labels for unlabeled data with a given distribution. However, this self-label generation fully relies on the accurate prior distribution and lack of consideration of the supervision of labeled data. In TRSSL [34], the unlabeled data are assigned with a soft self-label based on the inaccurate class distribution, which raises a biased estimation. Moreover, the confirmation bias still exists even if we use the ground-truth distribution to align the unlabeled data in the same process. In addition to confirmation bias, a new issue called clustering misalignment arises when self-labeling depends solely on unlabeled data: without proper guidance, the self-labeling process may adopt varying criteria for clustering. For example, it might cluster data based on superficial features like color rather than high-level semantic information. This misalignment can lead to results that deviate from expected outcomes and even contradict the classification criteria established by labeled data. Consequently, we propose a new self-labeling scheme, conditional self-labeling, designed to address the challenges of OwSSL, particularly targeting issues related to confirmation bias and misalignment. This scheme limits self-labels for each class and incorporates labeled data to generate debiased and informative self-label assignments for all training data, further mitigating the confirmation bias as shown in Figure 1(a). Additionally, as illustrated in Figure 1(b), seen classes typically exhibit higher predictive confidence, while novel clusters demonstrate variability in their internal learning progresses. The disparities in learning paces between seen and novel classes, coupled with their distinct behaviors, necessitate the selection of appropriate thresholds to facilitate cluster learning. To address these challenges and ensure a balanced learning process across classes, we propose a hierarchical thresholding scheme. We demonstrate our contributions as follows: 1) We introduce a novel conditional self-labeling method to incorporate labeled data into the clustering process, reducing confirmation bias and misalignment. 2) We design a hierarchical thresholding strategy that balances learning difficulties across different classes, helping unstable clusters gradually form. 3) Our theoretical analysis rigorously discusses the unbiasedness and reliability of conditional self-labeling estimator from population-level statistics. To the best of our knowledge, this is the first work proposing the expectation of chi-square statistics (ECS) to evaluate the reliability of self-label assignment estimation. 4) We conduct extensive experiments on various datasets, demonstrating the effectiveness of our approach, OwMatch, through detailed comparisons. On CIFAR-10, OwMatch significantly outperforms FixMatch [36] by up to 47.3% in all-class accuracy, while on CIFAR-100, it enhances TRSSL [34], the state-of-the-art model in OwSSL, by up to 14.6% in novel-class and 7.2% in all-class accuracy. (a) (b) Figure 1: Experimental results on the OwSSL problem. (a) Self-label assignment of seen classes (1-5) and novel classes (6-10) with or without conditional component in self-labeling. (b) Predictive confidence and hierarchical threshold for each class."
https://arxiv.org/html/2411.01758v1,Disentangled PET Lesion Segmentation,"PET imaging is an invaluable tool in clinical settings as it captures the functional activity of both healthy anatomy and cancerous lesions. Developing automatic lesion segmentation methods for PET images is crucial since manual lesion segmentation is laborious and prone to inter- and intra-observer variability. We propose PET-Disentangler, a 3D disentanglement method that uses a 3D UNet-like encoder-decoder architecture to disentangle disease and normal healthy anatomical features with losses for segmentation, reconstruction, and healthy component plausibility. A critic network is used to encourage the healthy latent features to match the distribution of healthy samples and thus encourages these features to not contain any lesion-related features. Our quantitative results show that PET-Disentangler is less prone to incorrectly declaring healthy and high tracer uptake regions as cancerous lesions, since such uptake pattern would be assigned to the disentangled healthy component.","Positron emission tomography (PET) is a medical imaging modality that captures functional, metabolic activity within the body. PET is used in many areas of medicine, especially in oncology for cancer staging, diagnosis, and monitoring [1]. Due to the nature of the modality, areas of high metabolic activity, including both healthy anatomy and cancerous lesions, correspond to high intensity values. Expert clinicians’ analysis of PET images is laborious and can suffer from inter- and intra-observer variability, neccessitating automated methods. Over the past few decades, automatic PET lesion segmentation methods have evolved from various intensity thresholding-based approaches, active contours, and region growing algorithms [2] to the current state-of-the-art methods that rely on deep learning (DL) to optimize models to learn disease features [3]. A promising DL-based approach that has yet to be explored in PET lesion segmentation is image disentanglement, which attempts to separate distinct sources of variation within images into independent representations. Image disentanglement has been shown to be beneficial in a wide variety of medical image analysis applications [4]. However, there is no work on evaluating the utility of an image disentanglement-based approach for PET images, a challenging modality owing to the presence of noise, low signal-to-contrast ratio, and high activity regions corresponding to both healthy and disease activity. In this work, we leverage lesion segmentation with a disentanglement framework and introduce PET-Disentangler, a novel PET segmentation method that disentangles 3D PET images in the latent space into disease features and normal healthy anatomical features for a robust and explainable segmentation method. PET-Disentangler uses the disease features for lesion segmentation prediction, the healthy features to estimate pseudo-healthy images per input, and re-entangles both healthy and disease features for a full reconstruction. PET-Disentangler enhances the lesion segmentation task by providing explainability in the form of a pseudo-healthy image as to what the model expects the lesion-free image to look like per given input. Furthermore, PET-Disentangler shows that learning the healthy component provides a solution to a critical challenge in PET lesion segmentation [5], where segmentation models can incorrectly segment healthy, high-intensity areas as disease. This challenge has seen solutions that focus on localizing healthy, high intensity regions [6] whereas the proposed PET-Disentangler can both capture the healthy anatomy features and delineate lesions through the learned disentangled representations."
https://arxiv.org/html/2411.01742v1,Learning From Convolution-based Unlearnable Datasets,"The construction of large datasets for deep learning has raised concerns regarding unauthorized use of online data, leading to increased interest in protecting data from third-parties who want to use it for training. The Convolution-based Unlearnable DAtaset (CUDA) method aims to make data unlearnable by applying class-wise blurs to every image in the dataset so that neural networks learn relations between blur kernels and labels, as opposed to informative features for classifying clean data. In this work, we evaluate whether CUDA data remains unlearnable after image sharpening and frequency filtering, finding that this combination of simple transforms improves the utility of CUDA data for training. In particular, we observe a substantial increase in test accuracy over adversarial training for models trained with CUDA unlearnable data from CIFAR-10, CIFAR-100, and ImageNet-100. In training models to high accuracy using unlearnable data, we underscore the need for ongoing refinement in data poisoning techniques to ensure data privacy. Our method opens new avenues for enhancing the robustness of unlearnable datasets by highlighting that simple methods such as sharpening and frequency filtering are capable of breaking convolution-based unlearnable datasets. 111Our code is available at https://github.com/aseriesof-tubes/RSK","Widespread data scraping has raised concerns about privacy and securing important data away from malicious actors. Desai and Johnson [5] mention the need and goal to scale to training on hundreds of millions of images, which is a daunting task without the ability to scrape the web. Additionally, the usage of private data may inadvertently lead to the susceptibility of inference attacks [12]. While the intentions of scrapers may not be malicious in nature, concerns of privacy remain. Poisoning can be used as a countermeasure against web scraping, which negatively affects the performance of a model that is trained on said dataset [1]. Another kind of poisoning attack, backdoor poisoning [2], poses a threat to learning systems by causing targeted misclassifications. To prevent the unauthorized scraping of data on the web, unlearnable datasets have been introduced as a way to render images unusable. Unlearnable dataset methods seek to find out the best way to imperceptibly modify a training dataset so that a deep neural network (DNN) trained on the resulting data exhibits the largest generalization gap. The idea is that if data can be rendered useless, third parties are disincentivized from using the modified data. Two types of unlearnable datasets exist: bounded and unbounded. Bounded methods [9, 6, 16, 17] attempt to make the perturbation as imperceptible to humans, while unbounded methods [14, 19] are more easily perceptible. Both types of unlearnable datasets face challenges. Existing works that explore bounded unlearnable methods have shown that Adversarial Training (AT) [9, 6, 21] and JPEG compression [11, 4] can increase the learnability of unlearnable data. We propose a method for learning from unbounded unlearnable datasets like the Convolution-based Unlearnable DAtaset (CUDA). As bounded methods show weakness whether it be by AT or other transforms, Sadasivan et al. [14] argue that unbounded unlearnable methods are valid because 1) they do not obscure the semantics of the dataset, and 2) their class-wise blurring makes the model learn the relationship between the class-wise filters and their corresponding labels. We demonstrate that it is possible to learn from CUDA datasets [14], despite their unbounded perturbations. We achieve a high test accuracy by sharpening the images from the CUDA dataset using our sharpening kernels followed by frequency filtering, achieving a 55%, 36%, and 40% improvement over CUDA standard training for CIFAR-10, CIFAR-100, and ImageNet-100, respectively. Our results suggest using random sharpening kernels should be a baseline against convolution-based perturbations. Figure 1: Sharpening and Frequency Filtering of a CIFAR-10 Image. We analyze the effect of standard and random sharpening kernels, both with a center value of 2.5. We find that the randomized sharpening kernel, denoted RSK, ensures images of the same class are sharpened differently. After sharpening, we decompose the image into spatial frequencies using DCT and filter out high frequencies (see Section 3.2)."
https://arxiv.org/html/2411.01713v1,Rethinking Weight Decay for Robust Fine-Tuning of Foundation Models,"Modern optimizers such as AdamW, equipped with momentum and adaptive learning rate, are designed to escape local minima and explore the vast parameter space. This exploration is beneficial for finding good loss basins when training from scratch. It is not necessarily ideal when resuming from a powerful foundation model because it can lead to large deviations from the pre-trained initialization and, consequently, worse robustness and generalization. At the same time, strong regularization on all parameters can lead to under-fitting. We hypothesize that selectively regularizing the parameter space is the key to fitting and retraining the pre-trained knowledge. This paper proposes a new weight decay technique, Selective Projection Decay (SPD), that selectively imposes a strong penalty on certain layers while allowing others to change freely. Intuitively, SPD expands and contracts the parameter search space for layers with consistent and inconsistent loss reduction, respectively. Experimentally, when equipped with SPD, Adam consistently provides better in-distribution generalization and out-of-distribution robustness performance on multiple popular vision and language benchmarks. Code available at https://github.com/GT-RIPL/Selective-Projection-Decay.git.","Modern optimizers, such as Adam [1], LARS [2], and LAMB [3] usually include momentum and adaptive learning rates. They help optimizers avoid local minima and accelerate learning [4, 5] to explore wider parameter spaces. However, we hypothesize that this behavior is not always beneficial for fine-tuning from a well pre-trained foundation model, especially when fine-tuning a few layers is already sufficient for fitting the target data [6, 7, 8, 9]. Several prior works have found that unnecessary exploration will lead to large deviation from the initialization and worse robustness [10, 11], and constraining the deviation can improve a model’s generalization on in-distribution (ID) data and robustness to out-of-distribution (OOD) data [12, 13, 14]111In this paper, ID generalization and OOD robustness refer to test accuracy on the fine-tuning distribution and robust accuracy on other shifted distributions, respectively.. For example, L2-SP [13] imposes a regularization term on the distance between the current and pre-trained models. More recently, TPGM [10] and FTP [11] propose to learn different hard constraints for each layer. These new works have demonstrated impressive results on benchmarks. However, they are either difficult to tune, specialized to specific settings, or require significant computation and storage overhead. This motivates us to ask whether a simple few-liner solution exists for this fundamental problem. We propose re-examining the existing methods and summarizing their findings to find this solution. Starting from the simplest: L2-SP [13]. Specifically, L2-SP adds an L2 regularization term to the original objective function. Formally, ℒ⁢(θ)=ℒ~⁢(θ)+λ2⁢‖θ−θ0‖22ℒ𝜃~ℒ𝜃𝜆2subscriptsuperscriptnorm𝜃subscript𝜃022\displaystyle\mathcal{L(\theta)}=\tilde{\mathcal{L}}(\theta)+\frac{\lambda}{2}% \|\theta-\theta_{0}\|^{2}_{2}caligraphic_L ( italic_θ ) = over~ start_ARG caligraphic_L end_ARG ( italic_θ ) + divide start_ARG italic_λ end_ARG start_ARG 2 end_ARG ∥ italic_θ - italic_θ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT (1) where θ𝜃\thetaitalic_θ denotes the model parameters, θ0subscript𝜃0\theta_{0}italic_θ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT the initialization, ℒ~⁢(θ)~ℒ𝜃\tilde{\mathcal{L}}(\theta)over~ start_ARG caligraphic_L end_ARG ( italic_θ ) the original objective function, and λ𝜆\lambdaitalic_λ the hyper-parameter for regularization strength. When θ0=𝟎subscript𝜃00\theta_{0}=\mathbf{0}italic_θ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = bold_0, L2-SP reduces to an ordinary weight decay. This simple method should be effective enough to constrain the model, as our experiments show it can reduce the deviation between the fine-tuned and pre-trained models (Sec. 4.1). However, it is held back by an important design choice: the penalty is always applied to all model parameters. Our empirical results identify that a large λ𝜆\lambdaitalic_λ prevents every layer from deviating too much and leads to poor fitting, while a small λ𝜆\lambdaitalic_λ cannot provide enough regularization. This significantly limits the otherwise effective design (Sec. 4.1). So, what is missing in this algorithm? Recent works in robust fine-tuning and parameter-efficient fine-tuning (PEFT) have shown that customizing constraints for each layer and selectively choosing layers for fine-tunning can improve robustness [10, 11, 6]. Inspired by these findings, we hypothesize that selectively imposing the regularization to different layers is the key. Therefore, we propose a simple selective version of L2-SP weight decay: selective projection decay (SPD). This new algorithm innovates in two aspects: a selection condition and a regularization strength ratio. The former determines when to apply regularization to a layer, and the latter determines the strength of regularization for intuitive hyper-parameter tuning. Specifically, we derive the selection condition from hyper-optimization [15, 16, 17] by treating the condition as an optimizable parameter (Sec. 3.3), and the regularization strength ratio by re-writing L2-SP as a projection operation (Sec. 3.4). Intuitively, when the condition is met, the algorithm imposes large regularizations on selected layers. This allows the algorithm to avoid unnecessary deviation and simultaneously fit into the fine-tuning data. We test SPD on large-scale computer vision, and NLP benchmarks with popular foundation models and test ID and OOD performance on various distribution and domain shifts. SPD achieves SOTA performance while being much simpler than other competing methods. Our contributions are: • We propose a selective projection decay, a selective variant of the popular L2-SP/weight decay regularization methods, for robust fine-tuning of large foundation models. We show that selectivity is important to make regularization effective. • We conduct a detailed study of ID and OOD performance on image classification and semantic segmentation with natural distribution and domain shifts. SPD improves ID and OOD performance on these benchmarks. • We show that SPD consistently improves the performance of PEFT methods (e.g. LoRA [7] and adapters [9]) on 8 common sense reasoning language tasks with LLaMA-7B (-13B)."
https://arxiv.org/html/2411.01669v1,MamT4: Multi-view Attention Networks for Mammography Cancer Classification,"In this study, we introduce a novel method, called MamT4, which is used for simultaneous analysis of four mammography images. A decision is made based on one image of a breast, with attention also devoted to three additional images: another view of the same breast and two images of the other breast. This approach enables the algorithm to closely replicate the practice of a radiologist who reviews the entire set of mammograms for a patient. Furthermore, this paper emphasizes the preprocessing of images, specifically proposing a cropping model (U-Net based on ResNet-34) to help the method remove image artifacts and focus on the breast region. To the best of our knowledge, this study is the first to achieve a ROC-AUC of 84.0 ± 1.7 and an F1 score of 56.0 ± 1.3 on an independent test dataset of Vietnam digital mammography (VinDr-Mammo), which is preprocessed with the cropping model.","Breast cancer is a leading cause of cancer-related deaths among women [1]. Regular screening is essential for early detection, with mammography being the primary screening tool [2]. Mammography utilizes low-dose X-rays to detect tissue changes in the breast, making it effective in detecting malignancies like microcalcifications and clusters of calcifications [3]. Radiologists interpret mammograms based on standard terminology and the BI-RADS classification system, facilitating standardized reporting and risk assessment [4]. Although mammography is effective, it can result in false positives or negatives, requiring additional tests like biopsies [5]. To improve the efficiency of early screening, automated approaches in mammography, such as computer-aided diagnosis (CAD) systems, as well as solutions using machine learning and deep learning (DL) technologies, are being actively developed, assisting radiologists in interpreting mammogram [6]. Deep Learning has emerged as a highly effective method for image classification [7]. Furthermore, DL has become one of the popular methods for detection of cancer pathologies, particularly, on mammograms [8]. A key aspect of mammographic examinations is the acquisition of images in different projections for each breast, requiring four images in total – two for each breast (MLO and CC). Radiologists analyze the symmetry of lesions [9]. This unique feature affects the training of DL models and the use of multi-view, a novel approach based on learning four projections at once, presented in this study. Given the widespread significance of breast cancer diagnosis, this research on utilizing deep learning methods offers a valuable chance to enhance the automation of breast pathology detection and streamline the tasks of radiologists. To sum up, the contributions of our paper are: 1. We propose MamT4: a novel classification framework based on Transformer Encoder that utilizes feature vector representations from four views of mammography studies and outperforms single-view methods in classifying cancer status. 2. To the best of our knowledge, this paper is the first to achieve ROC-AUC of 84.0±1.7plus-or-minus84.01.784.0\pm 1.784.0 ± 1.7 and F1 of 56.0±1.3plus-or-minus56.01.356.0\pm 1.356.0 ± 1.3 on the VinDr-Mammo dataset (test subset). 3. As a preprocessing, we propose the following method: cropping the breast along its border using U-Net to increase the quality of classification."
https://arxiv.org/html/2411.01611v1,Stochastic Communication Avoidance for Recommendation Systems,"One of the major bottlenecks for efficient deployment of neural network based recommendation systems is the memory footprint of their embedding tables. Although many neural network based recommendation systems could benefit from the faster on-chip memory access and increased computational power of hardware accelerators, the large embedding tables in these models often cannot fit on the constrained memory of accelerators. Despite the pervasiveness of these models, prior methods in memory optimization and parallelism fail to address the memory and communication costs of large embedding tables on accelerators. As a result, the majority of models are trained on CPUs, while current implementations of accelerators are hindered by issues such as bottlenecks in inter-device communication and main memory lookups. In this paper, we propose a theoretical framework that analyses the communication costs of arbitrary distributed systems that use lookup tables. We use this framework to propose algorithms that maximize throughput subject to memory, computation, and communication constraints. Furthermore, we demonstrate that our method achieves strong theoretical performance across dataset distributions and memory constraints, applicable to a wide range of use cases from mobile federated learning to warehouse-scale computation. We implement our framework and algorithms in PyTorch and achieve up to 6×6\times6 × increases in training throughput on GPU systems over baselines, on the Criteo Terabytes dataset.","A significant portion of machine learning research has advanced due to better memory and computational speeds of accelerators, alongside faster interconnects and more efficient parallelization in large systems. However, accelerators often have limited memory compared to CPUs, rendering many memory-intensive algorithms infeasible for deployment. One approach to mitigate this issue is to increase memory, but this cannot keep up with the rapid growth of machine learning models. An alternative is to develop new parallelization strategies that balance memory usage and communication, as explored in strategies like those in [1, 2, 3]. Another optimization strategy involves quantization [4, 5, 6], which aims to minimize the memory footprint and computational requirements without significantly impacting accuracy. Embedding tables, which map sparse categorical features to dense vectors [7, 8], are often prime targets for quantization due to their large sizes. By quantizing the embeddings to lower bit-width representations in DLRMs [9], such as 4-bit [10, 11], or performing tensor train decomposition [12], memory usage can be significantly reduced, making it more feasible to train and inference. These methods primarily focus on reducing the embedding size [13], while another solution is to modify the training process to save memory. Examples of this approach include reversible networks [14], which change the model’s structure, and techniques like Checkmate [15], which alter the model’s execution pattern by adding additional operations to backpropagation to decrease the number of intermediate values that need to be stored in memory. Recent algorithms have significantly increased the scale of NLP and CV models by reducing the memory demands per GPU [16, 17, 18], allowing the use of accelerators for extremely large models. However, these methods struggle with models that have large embedding tables, which are not easily managed by pipeline parallelism and remain large in parameter count. Data parallelism also falls short as it is better suited for compute-heavy tasks rather than memory-intensive embedding operations. Furthermore, techniques like recomputation or checkpointing do not suit embeddings well, as their high memory cost does not justify the modest savings from managing intermediate activations. This forces the use of model parallelism, but oftentimes the number of accelerators required to fit the large embedding tables is too great to make model parallelism a financially viable solution. These embeddings can often be terabytes large, but as observed in practice, they are not accessed uniformly at random. In real datasets, the access pattern of these embeddings varies, generally with a small portion of embeddings being accessed far more frequently than others[19]. Existing methods [20] have explored the usage of distributed communication to decrease the communication cost, but the theoretical bounds of communication efficiency has not been analyzed in previous work. In addition, there has been no existing work exploring how various methods of distributing embeddings across GPUs and CPUs impact the system performance. To summarize, our contributions are as follows: 1. We develop a simple framework to calculate the expected communication cost under a training regime. And we expand this framework to address considerations such as determining the optimal levels of communication and caching, as well as methods for adjusting them accordingly. 2. We use the above framework to obtain communication strategies that minimize expected communication costs without caching. We demonstrate that these methods also decrease main memory I/O proportionally to the decrease in communication. 3. We demonstrate how assumptions about ML training motivate different strategies for caching through empirical analysis with our framework. 4. We extensively test our algorithms on a variety of datasets and models. Furthermore, we also test various theoretical distributions and observe that our algorithms can generalize well, with performance gains on a wide range of potential synthetic datasets."
https://arxiv.org/html/2411.01608v1,GITSR: Graph Interaction Transformer-based Scene Representation for Multi Vehicle Collaborative Decision-making,"In this study, we propose GITSR, an effective framework for Graph Interaction Transformer-based Scene Representation for multi-vehicle collaborative decision-making in intelligent transportation system. In the context of mixed traffic where Connected Automated Vehicles (CAVs) and Human Driving Vehicles (HDVs) coexist, in order to enhance the understanding of the environment by CAVs to improve decision-making capabilities, this framework focuses on efficient scene representation and the modeling of spatial interaction behaviors of traffic states. We first extract features of the driving environment based on the background of intelligent networking. Subsequently, the local scene representation, which is based on the agent-centric and dynamic occupation grid, is calculated by the Transformer module. Besides, feasible region of the map is captured through the multi-head attention mechanism to reduce the collision of vehicles. Notably, spatial interaction behaviors, based on motion information, are modeled as graph structures and extracted via Graph Neural Network (GNN). Ultimately, the collaborative decision-making among multiple vehicles is formulated as a Markov Decision Process (MDP), with driving actions output by Reinforcement Learning (RL) algorithms. Our algorithmic validation is executed within the extremely challenging scenario of highway off-ramp task, thereby substantiating the superiority of agent-centric approach to scene representation. Simulation results demonstrate that the GITSR method can not only effectively capture scene representation but also extract spatial interaction data, outperforming the baseline method across various comparative metrics.","Autonomous vehicles have garnered significant research attention over the past two decades, driven by their substantial potential for societal and economical advancement. The efficient coordination of driving decisions among CAVs promises not only to enhance safety and operational efficiency but also to reduce energy consumption [1]. However, in dynamic traffic scenarios, the intricate interplay between scenarios and traffic participants presents formidable challenges for CAVs in making decisions that are safe, efficient, and comfortable [2]. The Internet of Vehicles (IoV) technology integrates Vehicle-to-Vehicle (V2V) and Vehicle-to-Infrastructure (V2I) communications with artificial intelligence (AI) to offer innovative solutions for CAVs to process dynamic traffic scene information and perform collaborative driving decisions [3]. In this context, methods based on deep reinforcement learning (DRL) are becoming more and more popular because the intelligent agent can continuously learn through interaction with the driving environment, extract environmental information through deep learning, and make decisions through reinforcement learning algorithms [4]. However, modeling and representing scene information effectively, processing and calculating it to adapt to various complex traffic environments, while achieving high-quality collaborative decision-making in real-time dynamic settings, has emerged as a formidable research challenge. Concurrently, the research on autonomous vehicle decision-making is increasingly focusing on more complex scenarios. The crux of the challenge lies in the representation of the state, which must encompass the elements, characteristics, and interactions in the dynamic scene. Addressing this will become one of the key issues of the DRL methods [5]. To this end, we introduce GITSR, a novel graph interaction Transformer-based scene representation framework for multi-vehicle collaborative decision-making. This framework leverages the Transformer architecture to capture scene information and employs a graph structure to model spatial interaction, thereby enhancing the multi-vehicle collaborative decision-making ability of reinforcement learning. Firstly, we extract features from the dynamic driving environment within the context of intelligent networking, meticulously considering both the local interaction and global communication attributes of CAVs. We perform local reconstruction reasoning on scene input information, introduce the Transformer module to process information and enhance understanding of surrounding traffic scene for CAVs. We conduct local reconstruction reasoning on the input scene information and introduce the Transformer module to process this data, thereby enhancing the CAVs’ comprehension of the surrounding traffic environment. Then, we represent the dynamic traffic scene as a graph, based on global communication attributes, and introduce GNN to extract spatial interaction features. This approach is advantageous as it optimally utilizes the information from all CAVs within dynamic traffic scenarios. It aids CAVs in scene comprehension and the transmission of upstream and downstream information. Moreover, it establishes the spatial interaction dynamics of the traffic environment, optimizing the collaborative driving decision-making capabilities. The main contributions of this article can be summarized as follows: 1) A collaborative decision-making framework for intelligent connected vehicles that integrates Transformer and GNN is designed, which is tailored for scene extraction and interaction modeling from the perspective of state representation, thus significantly enhancing the state representation to improve the reinforcement learning effect. 2) A local representation method based on Transformer to reconstruct reasoning from scene features is proposed. This method reconstructs the scene representation with a focus on all CAVs and employs GNN to extract spatial interaction behaviors between the motion information of traffic participants. The GITSR framework can make full use of the information extracted from features to assist all CAVs in comprehending both local scene details and global interaction dynamics. 3) The framework is verified in a challenging interactive collaborative driving environment. The results show that GITSR has advantages over advanced algorithms in terms of safety, efficiency, and task success rate. At the same time, we have conducted an assessment of the influence of various components within the GITSR framework on its overall performance."
https://arxiv.org/html/2411.01579v1,Flexible Coded Distributed Convolution Computing for Enhanced Fault Tolerance and Numerical Stability in Distributed CNNs,"Deploying Convolutional Neural Networks (CNNs) on resource-constrained devices necessitates efficient management of computational resources, often via distributed systems susceptible to latency from straggler nodes. This paper introduces the Flexible Coded Distributed Convolution Computing (FCDCC) framework to enhance fault tolerance and numerical stability in distributed CNNs. We extend Coded Distributed Computing (CDC) with Circulant and Rotation Matrix Embedding (CRME) which was originally proposed for matrix multiplication to high-dimensional tensor convolution. For the proposed scheme, referred to as Numerically Stable Coded Tensor Convolution (NSCTC) scheme, we also propose two new coded partitioning schemes: Adaptive-Padding Coded Partitioning (APCP) for input tensor and Kernel-Channel Coded Partitioning (KCCP) for filter tensor. These strategies enable linear decomposition of tensor convolutions and encoding them into CDC subtasks, combining model parallelism with coded redundancy for robust and efficient execution. Theoretical analysis identifies an optimal trade-off between communication and storage costs. Empirical results validate the framework’s effectiveness in computational efficiency, fault tolerance, and scalability across various CNN architectures.","In the rapidly evolving domain of distributed machine learning, Convolutional Neural Networks (CNNs) have become fundamental due to their exceptional capabilities in image feature extraction and classification [1, 2, 3], as well as their versatility enabled by transfer learning [4, 5, 6]. A significant trend in this field, particularly relevant to Internet of Things (IoT) applications, is the shift towards edge computing, where data processing is conducted directly on edge devices [7]. This paradigm reduces dependency on cloud resources, minimizing latency [8] and enhancing data privacy [9]. Deploying CNNs in distributed systems, especially on resource-constrained devices, poses significant challenges due to intensive computational requirements, particularly within convolutional layers (ConvLs). Convolution operations represent over 90% of the Multiply-Accumulate operations (MACs) in mainstream CNN architectures[10], including AlexNet [11], VGGNet [12], GoogleNet [13] and ResNet [14], and account for more than 80% of the computational time during inference [15]. Collaborative inference across multiple devices has emerged as a viable approach to mitigate the computational burden on individual devices and enhance CNN deployment efficiency [16]. However, inference latency is often significantly impacted by slow worker nodes, commonly referred to as stragglers. These stragglers, arising from hardware heterogeneity and variable network conditions [17], can lead to performance degradation and potential failures, particularly in IoT systems where data loss rates may exceed 70% per layer [15]. Coded Distributed Computing (CDC) has been introduced to enhance computational resilience and efficiency in distributed systems. By dividing a task into m𝑚mitalic_m subtasks and introducing redundancy through an (n,m)𝑛𝑚(n,m)( italic_n , italic_m ) error correction code (with n>m𝑛𝑚n>mitalic_n > italic_m workers), CDC allows the master node to obtain the desired solution by waiting for the fastest k𝑘kitalic_k workers (m≤k<n𝑚𝑘𝑛m\leq k<nitalic_m ≤ italic_k < italic_n), referred to as the recovery threshold, thereby significantly reducing computation time against the straggler effect. CDC has been successfully applied in Coded Matrix-Matrix Multiplication (CMMM) and other matrix multiplication-based algorithms [18, 19, 20, 21, 22, 23], due to the ease of linear decomposition of these operations. The application of CDC in distributed deep learning systems has garnered significant attention. Dutta et al. introduced a unified coded deep neural network (DNN) scheme utilizing Generalized PolyDot codes [24], which demonstrated the feasibility of deploying large DNNs on unreliable nodes prone to soft errors. Subsequent works, such as [25, 15], further integrated CDC at the model parallelism level in DNNs. However, these CMMM-based methods are not directly extensible to tensor convolutions in CNNs. Specifically, the decomposition of tensors 𝐓∈ℝd1×⋯×dn𝐓superscriptℝsubscript𝑑1⋯subscript𝑑𝑛\mathbf{T}\in\mathbb{R}^{d_{1}\times\cdots\times d_{n}}bold_T ∈ blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT × ⋯ × italic_d start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_POSTSUPERSCRIPT for CDC subtasks introduces multiple splitting dimensions, necessitating specialized partitioning schemes. Numerical stability is also a critical concern for CDC in DNNs due to the substantial propagation depth and cumulative errors associated with deep architectures [26]. Most existing CDC schemes—including Polynomial Codes [19], MatDot Codes [22], PolyDot Codes [20], and Lagrange Codes [23]—rely on polynomial methods that perform well over finite field, while encountering challenges in the real field. Specifically, the condition number of a real Vandermonde matrix, used in the recovery process during decoding of polynomial methods, grows exponentially with its dimension [27], exacerbating numerical instability. To address this issue, Ramamoorthy et al. enhanced numerical stability in CMMM by incorporating Circulant and Rotation Matrix Embeddings (CRME) into polynomial codes [28]. This approach exploits the well-conditioned nature of complex Vandermonde matrices with parameters on the unit circle while performing computations over the real field ℝℝ\mathbb{R}blackboard_R. This advancement suggests the potential for employing CRME into the real-valued applications, such as tensor convolutions in CNNs. Integrating CDC into CNNs poses significant challenges due to the complex and tightly coupled nature of ConvLs. These layers involve intricate interactions between three-dimensional input tensors (feature maps) and four-dimensional filter tensors (kernels), requiring careful management during tensor decomposition to preserve spatial continuity at the model parallelism level [29]. Existing research on CDC within this field was limited to one-dimensional vector convolutions [30, 31], which were insufficient for CNN architectures. Recent efforts, such as the Resilient, Secure, and Private Coded Convolution (RSPCC) scheme by Qiu et al. [32], have sought to broaden CDC’s applicability to ConvLs by adapting the im2col-based algorithm [33], transforming tensor convolutions into matrix-vector multiplications. However, due to this specific pre-encoding transformation, RSPCC exhibits limited compatibility with alternative convolution techniques, including FFT-based methods [34], Winograd-based algorithms [35], and approximation methods [36]. Additionally, RSPCC requires each node to retain complete filter, thereby increasing storage requirement. It also relies on finite field computations, which introduce numerical stability challenges in real-valued environments [28]. Addressing these challenges is crucial for the effective deployment of CDC in large-scale distributed CNNs. New tensor block encoding and decoding strategies are required to manage high-dimensional structures, ensuring numerical stability while optimizing the trade-off between communication and storage costs [37]. This paper introduces a Flexible Coded Distributed Convolution Computing (FCDCC) framework designed specifically for ConvLs in CNNs within distributed environments. The framework enhances model parallelism through a numerically stable coded computing scheme, improving both efficiency and robustness while minimizing computational and memory requirements. The primary contributions of this work are as follows: • Numerically Stable Coded Tensor Convolution (NSCTC): We extend the CDC scheme based on Circulant and Rotation Matrix Embedding (CRME) from matrix multiplication to high-dimensional tensor convolution, by introducing a new tensor-matrix multiplication operation for encoding and decoding. This approach leverages complex Vandermonde matrices computed over the real field to achieve numerical stability, achieving a maximum mean squared error (MSE) of 10−27superscript102710^{-27}10 start_POSTSUPERSCRIPT - 27 end_POSTSUPERSCRIPT for AlexNet’s ConvLs in a distributed setting with 20 worker nodes. This represents the first CDC scheme specifically designed for high-dimensional tensor operations. • Adaptive-Padding Coded Partitioning (APCP): We introduce an adaptive partitioning strategy that divides the input tensor along its spatial dimensions (height H𝐻Hitalic_H or width W𝑊Witalic_W), based on kernel size (KH,KWsubscript𝐾𝐻subscript𝐾𝑊K_{H},K_{W}italic_K start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT , italic_K start_POSTSUBSCRIPT italic_W end_POSTSUBSCRIPT) and stride s𝑠sitalic_s, with the addition of coded redundancy. This reduces communication cost and workload per node compared to single node scheme. • Kernel-Channel Coded Partitioning (KCCP): We employ a partitioning approach for the filter tensor along the output channel dimension N𝑁Nitalic_N, generating coded partitions to enhance resilience, while also lowering storage cost and per-node workload compared to RSPCC scheme. • Framework Optimization: The FCDCC framework is then analyzed to determine optimal partitioning parameters kAsubscript𝑘𝐴k_{A}italic_k start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT and kBsubscript𝑘𝐵k_{B}italic_k start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT, balancing communication and storage costs while maintaining a fixed number of subtasks (where kA⁢kBsubscript𝑘𝐴subscript𝑘𝐵k_{A}k_{B}italic_k start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT is constant). • Generality: The framework is applicable to CNN libraries such as PyTorch and various CNN models, including LeNet, AlexNet, and VGGNet. Figure 1: The FCDCC framework demonstrating the main workflow and tensor definitions in a comprehensive 3-D representation. We assume 3 kinds of straggler problems in this diagram: Upload failures, computing failures, and download failures. The remainder of the paper is organized as follows: Section II presents the system model, Section III introduces the NSCTC scheme, Section IV describes the FCDCC framework and cost optimization, Section V provides theoretical analysis and complexity evaluation, Section VI offers experimental validation, and Section VII concludes with future research directions. Notations: The imaginary unit is denoted by i=−1i1\mathrm{i}=\sqrt{-1}roman_i = square-root start_ARG - 1 end_ARG. The set of integers modulo m𝑚mitalic_m is represented by 𝒵m={0,1,…,m−1}subscript𝒵𝑚01…𝑚1\mathcal{Z}_{m}=\{0,1,\ldots,m-1\}caligraphic_Z start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT = { 0 , 1 , … , italic_m - 1 }, and the cardinality of a finite set ℐℐ\mathcal{I}caligraphic_I is denoted by |ℐ|ℐ|\mathcal{I}|| caligraphic_I |. We employ various matrix and tensor operations, including the Kronecker product (⊗tensor-product\otimes⊗) and tensor convolution (∗*∗). For a matrix 𝐌𝐌\mathbf{M}bold_M, 𝐌⁢(i,j)𝐌𝑖𝑗\mathbf{M}(i,j)bold_M ( italic_i , italic_j ) denotes its (i,j)𝑖𝑗(i,j)( italic_i , italic_j )-th entry; 𝐌i,jsubscript𝐌𝑖𝑗\mathbf{M}_{i,j}bold_M start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT represents its (i,j)𝑖𝑗(i,j)( italic_i , italic_j )-th block sub-matrix; and 𝐌isubscript𝐌𝑖\mathbf{M}_{i}bold_M start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT denotes its i𝑖iitalic_i-th column block sub-matrix. For a tensor 𝐓∈ℝN1×N2×N3𝐓superscriptℝsubscript𝑁1subscript𝑁2subscript𝑁3\mathbf{T}\in\mathbb{R}^{N_{1}\times N_{2}\times N_{3}}bold_T ∈ blackboard_R start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT × italic_N start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT × italic_N start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT, (𝐓)i,j,ksubscript𝐓𝑖𝑗𝑘(\mathbf{T})_{i,j,k}( bold_T ) start_POSTSUBSCRIPT italic_i , italic_j , italic_k end_POSTSUBSCRIPT (or ti,j,ksubscript𝑡𝑖𝑗𝑘t_{i,j,k}italic_t start_POSTSUBSCRIPT italic_i , italic_j , italic_k end_POSTSUBSCRIPT) refers to its (i,j,k)𝑖𝑗𝑘(i,j,k)( italic_i , italic_j , italic_k )-th entry; similar notation applies to higher-dimensional tensors. We denote a 1×Uk1subscript𝑈𝑘1\times U_{k}1 × italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT tensor block list as 𝐓′=[𝐓0′,𝐓1′,…,𝐓Uk−1′]superscript𝐓′subscriptsuperscript𝐓′0subscriptsuperscript𝐓′1…subscriptsuperscript𝐓′subscript𝑈𝑘1\mathbf{T}^{\prime}=[\mathbf{T}^{\prime}_{0},\,\mathbf{T}^{\prime}_{1},\,% \ldots,\,\mathbf{T}^{\prime}_{U_{k}-1}]bold_T start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = [ bold_T start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , bold_T start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_T start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - 1 end_POSTSUBSCRIPT ] or {𝐓i′}i=0Uk−1superscriptsubscriptsubscriptsuperscript𝐓′𝑖𝑖0subscript𝑈𝑘1\{\mathbf{T}^{\prime}_{i}\}_{i=0}^{U_{k}-1}{ bold_T start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - 1 end_POSTSUPERSCRIPT, where each tensor block 𝐓i′subscriptsuperscript𝐓′𝑖\mathbf{T}^{\prime}_{i}bold_T start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT has identical dimensions."
https://arxiv.org/html/2411.01575v1,HC3L-Diff: Hybrid conditional latent diffusion with high frequency enhancement for CBCT-to-CT synthesis,"Background: Cone-beam computed tomography (CBCT) plays a crucial role in image-guided radiotherapy, but artifacts and noise make them unsuitable for accurate dose calculation. Artificial intelligence methods have shown promise in enhancing CBCT quality to produce synthetic CT (sCT) images. However, existing methods either produce images of suboptimal quality or incur excessive time costs, failing to satisfy clinical practice standards.Methods and materials: We propose a novel hybrid conditional latent diffusion model for efficient and accurate CBCT-to-CT synthesis, named HC3L-Diff. We employ the Unified Feature Encoder (UFE) to compress images into a low-dimensional latent space, thereby optimizing computational efficiency. Beyond the use of CBCT images, we propose integrating its high-frequency knowledge as a hybrid condition to guide the diffusion model in generating sCT images with preserved structural details. This high-frequency information is captured using our designed High-Frequency Extractor (HFE). During inference, we utilize denoising diffusion implicit model to facilitate rapid sampling. We construct a new in-house prostate dataset with paired CBCT and CT to validate the effectiveness of our method.Result: Extensive experimental results demonstrate that our approach outperforms state-of-the-art methods in terms of sCT quality and generation efficiency. Moreover, our medical physicist conducts the dosimetric evaluations to validate the benefit of our method in practical dose calculation, achieving a remarkable 93.8% gamma passing rate with a 2%/2mm criterion, superior to other methods.Conclusion: The proposed HC3L-Diff can efficiently achieve high-quality CBCT-to-CT synthesis in only over 2 mins per patient. Its promising performance in dose calculation shows great potential for enhancing real-world adaptive radiotherapy.","1 Introduction Radiotherapy is an integral part of cancer treatment and it uses radiation either in the form of X-ray or energetic particles to deposit dose in the tumor to achieve local control [12]. However, due to the nature of the dose deposition curve of photon and particle beam, radiation toxicity is an inevitable consequence of radiotherapy [41]. The main maxim of radiotherapy is therefore to maximise the tumor control while minimizing the normal tissue complication probability. Achieving this balance relies heavily on accurate dose calculation, which is essential for effective and safe treatment delivery. There is an increasing number of proton therapy facilities being established globally due to the Bragg peak characteristic of the proton dose deposition curve which improves the dose conformity compared to conventional radiotherapy [25]. There is clinical evidence suggests a correlation between more conformal dose distributions and reduced side effects. This has been observed in head and neck cancers and the treatment of central nervous system (CNS) tumors [2]. Despite the dosimetric advantage of proton therapy, the integrity of the dose distribution is more susceptible to anatomical changes [30]. Advancement in image-guided radiotherapy (IGRT) especially the development of cone-beam computed tomography (CBCT) is a cost-effective method to acquire 3D volumetric information of the daily patient’s anatomy [13]. However, the major drawback of CBCT is the low image quality due to scatters and motions compared with CT, which result in inaccurate Hounsfield units (HU) and make the images unsuitable for dose calculation [20]. The ability to calculate doses on the daily CBCT empowers the clinic to make informed clinical decisions based on the individual target and organ-at-risk (OAR) doses. These decisions could be to continue treatment or to arrange for a new CT scan and a replan because the target coverage has been compromised; all of these are essential in moving towards adaptive radiotherapy (ART) [1]. In this regard, improving the quality of CBCT to the level of CT is crucial and highly demanded to benefit ART workflow. Some approaches have been proposed in the literature to enhance CBCT image, including early physics based-method [18] and Hounsfield look-up table (HLUT) [21]. More recently, artificial intelligence (AI) and deep learning methods are emerging and showing great potential to fundamentally change the radiation oncology workflow and pave the path towards online ART, which allows for rapid adaptation of treatment plans to accommodate daily anatomical changes, ensuring consistent target coverage and sparing of surrounding healthy tissues [24, 10, 38]. These AI methods have shown promising results in improving the quality of CBCT to the level of CT. The task is commonly named as CBCT-to-CT synthesis with the improved CBCT known as synthetic CT (sCT) [36]. For instance, convolutional neural networks (CNN) [6] have been used for sCT generation, and have benefitted ART [42]. Next, generative adversarial networks (GANs) have gradually played an important role in sCT generation. Consisting of a generator and a discriminator, GANs can be optimized through adversarial training, resulting in higher-quality image synthesis compared to traditional CNNs. Several studies have utilized GAN-based methods to improve CBCT-to-CT synthesis [8, 27]. However, training GANs is often unstable and optimization can be challenging, leading to issues such as model collapse [3]. Recently, diffusion model has revolutionized the field of image synthesis [9], image editing [45], super-resolution [46], and there are also some literatures that utilize diffusion model for CBCT-to-CT synthesis, showing state-of-the-art results [14]. Diffusion model was originally devised for simulating substance diffusion in space [34], and the advent of denoising diffusion probabilistic models (DDPM) [16] brings the diffusion model to the field of image synthesis by adding noise followed by progressive denoising. Diffusion model involves a process of adding noise to images and then iteratively denoising them based on conditional probability distributions, effectively reconstructs images by sampling from the model’s output distributions. Compared to the GAN-based method, it possesses progressive refinement through denoising, improving the detail and clarity of the image [9]. For example, Peng et al. [29] use a conditional DDPM to generate sCT images based on CBCT images. The CBCT images are concatenated with the noise samples along the channel dimension as the condition, which is used to guide each sampling generation step of CT, and ensures that the generated sCT images are accurate counterparts of the input CBCT images, effectively enhancing the overall quality of the generated images. Fu et al. [14] also utilize the diffusion model and design an energy-guided function that differentiates between domain-independent and domain-specific features, enhancing sCT generation by preserving relevant details and discarding less critical ones. Although these pilot studies using diffusion model further improve the quality of sCT images, we discern two principal limitations within these literatures, including low efficiency and detail deficiency in sCT generation. (i) The iterative nature of diffusion model-based image generation incurs significant computational time, often exceeding an hour to produce a 3D medical volume of a patient, which does not meet the needs of clinicians, as the maximum inference time to produce sCT for one patient should be less than 5 minutes. This strict time constraint is crucial for online ART, ensuring rapid adaptation and timely treatment when substantial anatomical changes require a new treatment plan, minimizing patient discomfort and optimizing outcomes [15]. Recently, the latent diffusion model has emerged and gained a lot of attention [32]. Different from the original diffusion models, the latent diffusion model uses autoencoders to compress original images into a lower dimensional latent space, enabling more efficient model training and image generation. It has been applied to various domains in general computer vision of image generation and editing [32, 44]. Also in medical domain, it has shown promise in MRI image synthesis [17]. These achievements inspired us to explore the potential of latent diffusion model in CBCT-to-CT generation. (ii) The images generated by existing models often exhibit structural ambiguities or inconsistencies, which fail to reflect the complex anatomical structures in CT imaging. The underlying reason may be that current models are primarily conditioned on the original CBCT images, which are insufficient for describing detailed anatomical structures. How to wisely extract and incorporate other informative modalities to augment diffusion process is crucial to improve sCT generation. For example, high-frequency features, can offer additional fine structural information like edges and textures, which can improve the representation of intricate anatomical details. In this paper, we propose a novel Hybrid Conditional Latent Diffusion model for efficient and accurate CBCT-to-CT image synthesis, named HC3L-Diff. Specifically, we first propose the unified feature encoder (UFE) to compress images into a lower-dimensional yet perceptually equivalent latent embedding space. This compression can reduce the training time, computational resources required for the diffusion model, and meanwhile, accelerate the generation process. Moreover, we propose to include both CBCT and its corresponding high-frequency information as a hybrid condition, to guide the generation of sCT images in the latent space. We design a high-frequency extractor (HFE) to effectively capture the high-frequency components from CBCT images. By wisely integrating high-frequency embedding and CBCT features, this hybrid condition can provide comprehensive guidance that ensures the preservation and reconstruction of fine anatomical structures in the diffusion process, therefore enhancing the quality of the generated images. Additionally, we employ the denoising diffusion implicit model (DDIM) during inference, to further speed up the sampling process and dramatically reduce the time for sCT image generation. We introduce a new in-house prostate dataset and conduct extensive experiments on it, employing widely used image similarity metrics and clinical dosimetric studies in collaboration with our medical physicist team. Our method outperforms state-of-the-art approaches across all evaluation metrics for CBCT-to-CT synthesis. The main contributions of this paper are as follow: • We propose a novel hybrid conditional latent diffusion model for CBCT-to-CT synthesis, which leverages both CBCT and its high-frequency image as guidance in diffusion process. We employ UFE to map both images into the latent space and utilize DDIM for generation, to enhance the efficiency of image synthesis. • We design a new HFE to capture high-frequency knowledge from CBCT images. Integrated with CBCT in the latent space, it serves as a hybrid conditional input to the diffusion model, improving sCT image quality by preserving fine anatomical details. • We construct an in-house prostate dataset to facilitate this task, showcasing that HC3L-Diff achieves performance gain in image similarity metrics with reduced inference cost, superior to previous state-of-the-arts. • We study the potential usage of our method in improving clinical dose calculation. Our model outperforms others in dosimetric evaluation metrics, achieving a 93.8% gamma passing rate (GPR) with a 2%/2mm criterion. The promising result highlights its great potential for effective treatment planning and adaptive radiotherapy in real-world practice."
https://arxiv.org/html/2411.01547v1,Decoupling Dark Knowledge via Block-wise Logit Distillation for Feature-level Alignment,"Knowledge Distillation (KD), a learning manner with a larger teacher network guiding a smaller student network, transfers dark knowledge from the teacher to the student via logits or intermediate features, with the aim of producing a well-performed lightweight model. Notably, many subsequent feature-based KD methods outperformed the earliest logit-based KD method and iteratively generated numerous state-of-the-art distillation methods. Nevertheless, recent work has uncovered the potential of the logit-based method, bringing the simple KD form based on logits back into the limelight. Features or logits? They partially implement the KD with entirely distinct perspectives; therefore, choosing between logits and features is not straightforward. This paper provides a unified perspective of feature alignment in order to obtain a better comprehension of their fundamental distinction. Inheriting the design philosophy and insights of feature-based and logit-based methods, we introduce a block-wise logit distillation framework to apply implicit logit-based feature alignment by gradually replacing teacher’s blocks as intermediate stepping-stone models to bridge the gap between the student and the teacher. Our method obtains comparable or superior results to state-of-the-art distillation methods. This paper demonstrates the great potential of combining logit and features, and we hope it will inspire future research to revisit KD from a higher vantage point.","\IEEEPARstart Deep neural networks have shown exceptional performance in several domains, including computer vision [19, 12, 13] and natural language processing [9, 35]. Nevertheless, it is common for robust and powerful networks to get advantages from large model capacity, hence resulting in significant computational and storage expenses. Various strategies have been developed to train fast and compact neural networks that are both efficient in terms of speed and size, including the invention of novel architectures [7, 17, 43, 63], network pruning [25, 11, 31], quantization [20], and knowledge distillation [16, 41, 51]. Among the investigated strategies, knowledge distillation (KD) has emerged as one of the most promising techniques to obtain a lightweight model with practicality and efficiency. KD refers to a set of methods concentrating on transferring “dark knowledge” from a large model (teacher) to a light one (student), which are applicable to a variety of network architectures and can be combined with numerous other techniques, such as network pruning and quantization [55] toward lightweight models. Figure 1: Method Illustration. (a) Logit distillation transfers the entire dark knowledge based solely on output logits. (b) Feature distillation further implements feature alignment at multiple levels. (c) The proposed block-wise logit distillation framework accomplishes implicit feature alignment via output logits of stepping-stones. We refine the distillation into logit-only objectives, with the transfer of dark knowledge decoupled at the block level. The primary approaches used in KD can be categorized into two main streams: 1) logit distillation and 2) feature distillation. The earliest concept of KD was proposed based on logits [16], which transfers only logit-level knowledge from teacher to student by minimizing the KL-Divergence between prediction logits (Fig. 1a). Since then, towards better utilization of teacher knowledge [41], the majority of KD study has been dedicated to investigating intermediate layers and performing distillation via the alignment of feature distributions [14, 5], which are referred to as feature distillation (Fig. 1b). Nevertheless, recent work [65] reveals and improves the limitations of the classical KD loss and re-examines the potential of the logits, bringing the simple logit-based KD back into the limelight. Logits or features? Although both logit-based and feature-based methods implement the distillation, they do so with quite different perspectives on the ”dark knowledge”, deciding between the two very hard. It’s time to gaze at the gap between them to discover an avenue to connect the two lines together. For logit-based methods, ”dark knowledge” is thought to be fully contained in the output logits, so aligning the logits is sufficient to transfer the ”dark knowledge” from teacher to student (Fig. 1a); however, for features-based methods, this alignment requirement is extended to the hidden features across different block-levels in an effort to transfer different levels of ”dark knowledge” by aligning block-wise features (Fig. 1b). Clearly, the features-based methods are more hypothetical and implemented with more stringent constraints in the hopes of facilitating the transfer of ”dark knowledge” to the student at finer levels; as a result, this approach may frequently converge faster and achieve a good performance. Meanwhile, in some cases, end-to-end logit-based alignment is more secure for transferring ”clear” knowledge since features-based alignment may also preserve task-irrelevant noise items throughout the distillation process. We argue that, although feature-based approaches have more success than logit-based methods at conveying ”dark knowledge” through block-wise alignments, they are restricted by the need for excessively stringent alignment constraints. Inspired by this observation, we propose a new distillation framework called block-wise logit distillation, which implements the alignment of feature-level “dark knowledge” based on the unified logit-based distillation of intermediate stepping-stone models (Fig. 1c) to fill in the gap between features and logits. As the implicit intermediate models during training, the student’s shallow blocks are gradually substituted with the teacher’s to establish stepping-stones, which are eventually abandoned during inference. We further evaluate the proposed methods on several visual benchmark datasets, including CIFAR100, ImageNet, and MSCOCO datasets for image classification and object detection tasks. The experimental results demonstrate that our method consistently outperforms both logit- and feature-based distillation methods. Further experiments on natural language processing tasks based on the BERT model have also been included in the experimental section to demonstrate the versatility of our approach. Additionally, a comparative analysis of training costs against related distillation methods yields several key insights: 1. Compared to feature-based KD methods, our approach not only achieves superior performance but does so with a similar degree of added complexity, highlighting the efficiency of the proposed framework without significantly increasing computational demands. 2. Regarding adaptability, we have formulated a lightweight version of the original framework that considerably diminishes the complexity typical of the standard method while still preserving high accuracy, which offers a feasible solution for contexts where computational resources are scarce. 3. The framework is compatible with recent logit-based KD approaches, providing an alternative option that can further enhance performance, which underscores the versatility of our method, rendering it an invaluable asset for augmenting the effectiveness of logit-based knowledge distillation."
https://arxiv.org/html/2411.01487v1,DSDE: Using Proportion Estimation to Improve Model Selection for Out-of-Distribution Detection,"Model library is an effective tool for improving the performance of single-model Out-of-Distribution (OoD) detector, mainly through model selection and detector fusion. However, existing methods in the literature do not provide uncertainty quantification for model selection results. Additionally, the model ensemble process primarily focuses on controlling the True Positive Rate (TPR) while neglecting the False Positive Rate (FPR). In this paper, we emphasize the significance of the proportion of models in the library that identify the test sample as an OoD sample. This proportion holds crucial information and directly influences the error rate of OoD detection. To address this, we propose inverting the commonly-used sequential p-value strategies. We define the rejection region initially and then estimate the error rate. Furthermore, we introduce a novel perspective from change-point detection and propose an approach for proportion estimation with automatic hyperparameter selection. We name the proposed approach as DOS-Storey-based Detector Ensemble (DSDE). Experimental results on CIFAR10 and CIFAR100 demonstrate the effectiveness of our approach in tackling OoD detection challenges. Specifically, the CIFAR10 experiments show that DSDE reduces the FPR from 11.07%percent11.0711.07\%11.07 % to 3.31%percent3.313.31\%3.31 % compared to the top-performing single-model detector.","Deep neural networks (DNNs) demonstrate significant fitting capabilities. Generally, a DNN’s accuracy and robustness can be improved with an adequate number of training samples. However, this also introduces new risks, such as well-trained DNNs producing highly confident predictions on unfamiliar samples. For example, when a neural network classifier trained on the CIFAR-10 dataset [18] is evaluated on the SVHN dataset [12], it can confidently misclassify number plates as dogs, birds, and airplanes with nearly 100%percent100100\%100 % certainty. Likewise, a model trained on the MNIST dataset [7] can make predictions with exceptionally high confidence when presented with the grayscale version of the CIFAR-10 dataset. Figure 1 illustrates how the model incorrectly identifies a dog, cat, airplane, and truck as the numbers 7, 6, 2, and 7, respectively, with a confidence level of at least 98%percent9898\%98 %. These instances highlight the unreliability of deep neural networks, as they generate highly confident predictions for samples that were not encountered during training. Figure 1: Example of high-confidence predictions on images from CIFAR10. Most deep neural network models are trained in a closed-world environment, assuming that test data follows the same distribution as training data [19]. This assumption necessitates these models to generate predictions for all inputs. However, a dependable classifier should refrain from making uninformed predictions on Out-of-Distribution (OoD) data as it impacts the model’s reliability and generalization. This emphasizes the significance of OoD detection [24]. For instance, in autonomous driving, if a model fails to handle unforeseen events, it can result in accidents and put the driver at risk. Incorporating OoD detection enables the system to notify the driver and transfer control, mitigating the risk of accidents. Likewise, in medical imaging, deep learning models aid physicians in detecting anomalies. If these models fail to accurately identify abnormal regions or assess new cases, it can lead to misdiagnoses and delayed treatments, thereby complicating patient care and placing a burden on healthcare facilities. In recent years, research on OoD detection techniques has predominantly focused on single-model methods, with limited exploration of multi-model perspectives [8, 32, 25, 5]. Morningstar et al. enhanced OoD detection performance by combining multiple test statistics derived from generative models [22]. Haroush et al. employed Simes’ and Fisher’s methods to combine p-values calculated for each channel and layer of a deep neural network [9]. Bergamin et al. showed that combining different types of test statistics using Fisher’s method improves the accuracy of OoD detection [3]. Magesh et al. introduced a multi-detection framework for OoD detection, inspired by multiple testing, that combines statistics from neural network models [21]. Xue et al. proposed the ZODE algorithm, which integrates multiple pre-trained models from a model zoo and selects those capable of identifying test inputs as OoD samples [31]. The ZODE method standardizes multiple detection scores into p-values and employs the Benjamini &\&& Hochberg (BH) method [2] for OoD decision-making. Detecting OoD samples using multiple pre-trained models presents more complex challenges compared to single-model OoD detection, i.e. controlling True Positive Rate (TPR). While controlling the TPR is typically done for a single-model detector, managing the combined TPR rate becomes necessary when employing multiple OoD detectors. Generally, controlling TPR is derived from the upper bound of the false discovery rate (FDR) using intricate sequential p-value rejection methods based on observed data [21, 31]. A sequential p-value method produces an estimate k^^𝑘\hat{k}over^ start_ARG italic_k end_ARG, where the detectors associated with p(1),p(1),⋯,p(k^)subscript𝑝1subscript𝑝1⋯subscript𝑝^𝑘p_{(1)},p_{(1)},\cdots,p_{(\hat{k})}italic_p start_POSTSUBSCRIPT ( 1 ) end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT ( 1 ) end_POSTSUBSCRIPT , ⋯ , italic_p start_POSTSUBSCRIPT ( over^ start_ARG italic_k end_ARG ) end_POSTSUBSCRIPT identify the test input as an OoD sample. Here, p(1)≤p(2)≤⋯≤p(m)subscript𝑝1subscript𝑝2⋯subscript𝑝𝑚p_{(1)}\leq p_{(2)}\leq\cdots\leq p_{(m)}italic_p start_POSTSUBSCRIPT ( 1 ) end_POSTSUBSCRIPT ≤ italic_p start_POSTSUBSCRIPT ( 2 ) end_POSTSUBSCRIPT ≤ ⋯ ≤ italic_p start_POSTSUBSCRIPT ( italic_m ) end_POSTSUBSCRIPT represent the ordered observed p-values. Nevertheless, this procedure exhibits several significant weaknesses. Firstly, k^^𝑘\hat{k}over^ start_ARG italic_k end_ARG is a random variable without a measure of its uncertainty. Ideally, TPR≥1−αTPR1𝛼\text{TPR}\geq 1-\alphaTPR ≥ 1 - italic_α, but the reliability of these methods varies on a case-by-case basis. Another weakness is that the TPR is controlled concurrently for all values of m0subscript𝑚0m_{0}italic_m start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT (the number of pre-trained models identifying the test input as an ID sample), without utilizing any information about m0subscript𝑚0m_{0}italic_m start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT in the observed p-values. Incorporating this information can result in a less stringent and more sensitive procedure for OoD data while maintaining robust TPR control. This paper aims to employ conventional and straightforward statistical concepts to control the TPR. While a sequential p-value method fixes the error rate and estimates the corresponding rejection region, we propose the reverse approach: fixing the rejection region and estimating its associated error rate. We incorporate the Storey correction method [26] into OoD Detection and develop a novel algorithm to leverage multiple pre-trained models. For each test input, we empirically estimate the number of pre-trained models that classify the test input as an ID sample. We utilize this information to enhance the OoD detector. However, hyperparameter tuning remains a challenging task, lacking consensus on the optimal approach [15]. Like the existing sequential p-value approaches [21, 31], our proposed approach also faces challenges of high variance in model selection results. To mitigate this issue, [17] introduce a hyperparameter selection approach named ’DOS-Storey’ that leverages insights from change-point detection. This method approximates the p-value plot using a piecewise linear function with a single change-point, enabling the selection of the hyperparameter based on the p-value at the change-point location. The DOS-Storey estimator automatically selects the optimal hyperparameter value and demonstrates reduced bias and variance, thereby enhancing performance stability. Consequently, we employ the DOS-Storey estimator to determine the number of pre-trained models that identify the test input as an OoD sample. We name the proposed approach as DOS-Storey-based Detector Ensemble (DSDE). The primary contributions and findings of this study are outlined below: 1. Contrary to the sequential p-value method, where the error rate is determined to infer its corresponding rejection region, our approach fixes the rejection region first and then estimates the error rate. We utilize a model library to determine the proportion of models capable of classifying the test input as OoD samples. By employing this approach, we mitigate the issue of missed detections for OoD samples and improve detection accuracy. 2. We leverage the insight from change-point detection and utilize the DOS-Storey estimator to select optimal hyperparameter values. Unlike the Storey proportion estimator method, the DOS-Storey estimator autonomously selects λ𝜆\lambdaitalic_λ values, resulting in lower bias levels and reduced variances. This characteristic ensures a more stable performance. 3. In this study, we evaluate the effectiveness of the proposed DSDE detector using comprehensive experiments. On the CIFAR10 dataset, the experimental results show that the proposed algorithm reduces the average FPR from 11.07%percent11.0711.07\%11.07 % to 3.31%percent3.313.31\%3.31 % compared to the top-performing single-model detector. Similarly, on CIFAR100, a reduction in FPR from 48.75%percent48.7548.75\%48.75 % to 41.28%percent41.2841.28\%41.28 % is achieved. Furthermore, this algorithm can be integrated with various baseline methods for post-detection, and the DSDE-KNN detector exhibits superior performance. On the CIFAR10 dataset, the DSDE-KNN detector achieves a significant reduction in FPR from 20.74%percent20.7420.74\%20.74 % to 3.31%percent3.313.31\%3.31 % compared to the top-performing baseline method. Similarly, on CIFAR100, the enhanced detector achieves a reduction in FPR from 64.14%percent64.1464.14\%64.14 % to 41.28%percent41.2841.28\%41.28 %."
https://arxiv.org/html/2411.01409v1,Classifier-guided Gradient Modulation forEnhanced Multimodal Learning,"Multimodal learning has developed very fast in recent years. However, during the multimodal training process, the model tends to rely on only one modality based on which it could learn faster, thus leading to inadequate use of other modalities. Existing methods to balance the training process always have some limitations on the loss functions, optimizers and the number of modalities and only consider modulating the magnitude of the gradients while ignoring the directions of the gradients. To solve these problems, in this paper, we present a novel method to balance multimodal learning with Classifier-Guided Gradient Modulation (CGGM), considering both the magnitude and directions of the gradients. We conduct extensive experiments on four multimodal datasets: UPMC-Food 101, CMU-MOSI, IEMOCAP and BraTS 2021, covering classification, regression and segmentation tasks. The results show that CGGM outperforms all the baselines and other state-of-the-art methods consistently, demonstrating its effectiveness and versatility. Our code is available at https://github.com/zrguo/CGGM.","Humans perceive the world in a multimodal way, such as sight, touch and sound. These multimodal features can provide comprehensive information to help us understand and explore the environment. Recent years have witnessed great success in multimodal learning, such as visual question answering [2], multimodal sentiment analysis [18] and multimodal retrieval [26, 13]. Although multimodal learning has made significant progress in recent years, inadequate use of different modality information during training remains a challenge. Theoretically, for example, Wu et al. [25] put forward the greedy learner hypothesis which states that a multimodal model learns to rely on one of the input modalities, based on which it could learn faster, and does not continue to learn to use other modalities. Huang et al. [12] find that during joint training, multiple modalities will compete with each other and some modalities will fail in the competition. Experimentally, on some multimodal datasets, there is little improvement in accuracy between training with only one modality and training with all modalities [18, 21]. These theoretical analyses and experimental results demonstrate the inefficiency of multimodal learning to fully utilize and integrate information from different modalities. To deal with this problem, recent studies [25, 17, 15, 8, 28, 9] investigate the training process of multimodal learning and propose gradient modulation strategies to better integrate the information of different modalities and balance the training process in some situations. However, all of these methods can not be applied easily for some limitations. For example, Wu et al. [25], Peng et al. [17], Li et al. [15] and Hua et al. [11] propose balancing methods based on cross-entropy loss for classification tasks. For regression tasks or other tasks, we can not use these strategies. Besides, most of these methods can just deal with situations where there are only two modalities. For example, Wu et al. [25] propose the conditional learning speed which is difficult to calculate and employ if there are more than two modalities. For situations where there are more modalities, these methods can not be applied. Furthermore, most of these methods only consider modulating the magnitude of the gradients while ignoring the directions of the gradients. Based on the above observations, in this paper, we propose a novel method to balance multimodal learning with Classifier-Guided Gradient Modulation (CGGM). In CGGM, we consider a more general situation with no limitations on the type of tasks, optimizers, the number of modalities, etc. Additionally, we consider both the magnitude and directions of the gradients to fully boost the training process of multimodal learning. Specifically, we add classifiers to evaluate the utilization rate of each modality and obtain the unimodal gradients. Then, we leverage the utilization rate to adaptively modulate the magnitude of the gradients of encoders and use the unimodal gradients to instruct the model to optimize towards a better direction. We conduct extensive experiments on four multimodal datasets: UPMC-Food 101 [23], CMU-MOSI [27], IEMOCAP [3], and BraTS 2021 [1]. UPMC-Food 101 and IEMOCAP are classification tasks, CMU-MOSI is a regression task, and BraTS 2021 is a segmentation task. CGGM outperforms all the baselines and other state-of-the-art methods, demonstrating its effectiveness and universality. In summary, our contributions are as follows: • We propose CGGM to balance multimodal learning by both considering the magnitude and direction of the gradients. • CGGM can be easily applied to many multimodal tasks and networks with no limitations on the type of tasks, optimizers, the number of modalities, etc. which indicates its versatility. • Our proposed CGGM brings consistent improvements to various tasks, including classification, regression and segmentation tasks. Extensive experiments show that CGGM outperforms other state-of-the-art methods, demonstrating its effectiveness."
https://arxiv.org/html/2411.01403v1,TPOT: Topology Preserving Optimal Transport in Retinal Fundus Image Enhancement,"Retinal fundus photography enhancement is important for diagnosing and monitoring retinal diseases. However, early approaches to retinal image enhancement, such as those based on Generative Adversarial Networks (GANs), often struggle to preserve the complex topological information of blood vessels, resulting in spurious or missing vessel structures. The persistence diagram, which captures topological features based on the persistence of topological structures under different filtrations, provides a promising way to represent the structure information. In this work, we propose a topology-preserving training paradigm that regularizes blood vessel structures by minimizing the differences of persistence diagrams. We call the resulting framework Topology Preserving Optimal Transport (TPOT). Experimental results on a large-scale dataset demonstrate the superiority of the proposed method compared to several state-of-the-art supervised and unsupervised techniques, both in terms of image quality and performance in the downstream blood vessel segmentation task. The code is available at https://github.com/Retinal-Research/TPOT.","Fig. 1: (A) illustrates the TPOT framework. The structures of Gθsubscript𝐺𝜃G_{\theta}italic_G start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT and Dβsubscript𝐷𝛽D_{\beta}italic_D start_POSTSUBSCRIPT italic_β end_POSTSUBSCRIPT follows the design outlined in [1, 2], and SNsubscript𝑆𝑁S_{N}italic_S start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT follows the design presented in [3]. (B) represents the changes in segmentation masks of the images during training. The orange box highlights regions with complex topological structures. (C) provides an example of how our topology-preserving regularization operates. The yellow crosses and blue dots represent corresponding persistent features in SN⁢(𝐱)subscript𝑆𝑁𝐱S_{N}(\mathbf{x})italic_S start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ( bold_x ) and SN⁢(Gθ⁢(𝐱))subscript𝑆𝑁subscript𝐺𝜃𝐱S_{N}(G_{\theta}(\mathbf{x}))italic_S start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ( italic_G start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_x ) ), respectively. The green dots indicate persistent noise features that must be removed during training. The model penalizes the differences between corresponding points to encourage topological consistency. Further details are discussed in Sec. 1. Retinal color fundus photography (CFP) plays a critical role in diagnosing ocular diseases [4, 5]. However, retinal fundus imaging equipment, particularly nonmydriatic cameras, faces challenges such as artifacts and blurring caused by uncontrollable factors (e.g., variability in ambient light conditions, capture errors, or operator mistakes). These issues often result in degraded retinal images. Some types of degradation, such as contrast mismatch, inconsistent illumination, and vessel occlusions, can impair the accurate visualization of lesions and blood vessels, essential for diagnosing retinal diseases such as diabetic retinopathy. Therefore, developing a robust framework to enhance low-quality images is vital for improving diagnostic accuracy[1, 2]. Since it is both difficult and costly to collect paired degraded and clean retinal images, research on retinal fundus image quality enhancement has shifted from supervised algorithms [6] to unsupervised algorithms, leading to remarkable improvements in image quality. These unsupervised approaches typically model retinal quality enhancement as an end-to-end image-to-image (I2I) translation task using Generative Adversarial Networks (GANs), where low-quality images X𝑋Xitalic_X are transformed into their high-quality counterparts Y𝑌Yitalic_Y. Specifically, CycleGAN [7] serves as a common strategy, as its cycle consistency regularization enables learning meaningful bidirectional mappings and improves generation alignment, albeit with additional computational cost and potential artifacts. To address these limitations, Wang et al. [8] applied optimal transport (OT) theory to learn optimal mappings between two image domains, Zhu et al. [1, 2] introduced to use the Structural Similarity Index (SSIM) to regularize the generator and prevent excessive distortion of lesion structures. However, these methods often prove suboptimal, such as suffering from mode collapse, when handling images with complex structures or multimodal distributions. To mitigate this, Dong et al. [9] proposed the unpaired neural schrödinger bridge method to reserve the smooth and probabilistically consistent transformation between distributions. However, their iterative learning process can smooth high-frequency information, producing suboptimal structure preservation. Topology preservation has become a key focus in segmentation tasks. Specifically, Hu et al. [10] extended beyond simple geometric structures by constructing a differentiable topology loss based on persistence diagrams and incorporating it into the segmentation training process. However, existing fundus image enhancement methods primarily focus on maintaining semantic consistency in the feature space [11] without considering introducing topology preservation and leveraging the natural advantage(e.g., vessel topology structures). In this paper, we propose a novel topology-preserving retinal fundus image enhancement framework, named Topology Preserving Optimal Transport (TPOT), which leverages Optimal Transport (OT) theory to learn the optimal mappings between low-quality and high-quality images while incorporating topology regularization to preserve the complex blood vessel structures. Our main contributions are threefold: (i) To the best of our knowledge, this is the first work to introduce topology-preserving into OT-GAN-based retinal enhancement methods. (ii) We extend the application of topology loss beyond segmentation tasks, demonstrating its effectiveness in preserving topological structures during image generation. (iii) We conduct experiments on public retinal fundus datasets, showing our method outperforms various supervised and unsupervised approaches in denoising and downstream vessel segmentation."
https://arxiv.org/html/2411.01291v1,Deep Multi-contrast Cardiac MRI Reconstruction via vSHARP with Auxiliary Refinement Network,"Cardiac MRI (CMRI) is a cornerstone imaging modality that provides in-depth insights into cardiac structure and function. Multi-contrast CMRI (MCCMRI), which acquires sequences with varying contrast weightings, significantly enhances diagnostic capabilities by capturing a wide range of cardiac tissue characteristics. However, MCCMRI is often constrained by lengthy acquisition times and susceptibility to motion artifacts. To mitigate these challenges, accelerated imaging techniques that use k𝑘kitalic_k-space undersampling via different sampling schemes at acceleration factors have been developed to shorten scan durations. In this context, we propose a deep learning-based reconstruction method for 2D dynamic multi-contrast, multi-scheme, and multi-acceleration MRI. Our approach integrates the state-of-the-art vSHARP model, which utilizes half-quadratic variable splitting and ADMM optimization, with a Variational Network serving as an Auxiliary Refinement Network (ARN) to better adapt to the diverse nature of MCCMRI data. Specifically, the subsampled k𝑘kitalic_k-space data is fed into the ARN, which produces an initial prediction for the denoising step used by vSHARP. This, along with the subsampled k𝑘kitalic_k-space, is then used by vSHARP to generate high-quality 2D sequence predictions. Our method outperforms traditional reconstruction techniques and other vSHARP-based models.","Cardiac Magnetic Resonance Imaging (CMRI) is a cornerstone clinical imaging modality that provides detailed 4D (3D + dynamic) images of the heart, essential for assessing and evaluating cardiovascular health, structure, and function. Multi-contrast CMRI (MCCMRI) enhances this capability by acquiring multiple imaging sequences with varying contrast weightings, such as cardiac cine, mapping, tagging, phase-contrast, and dark-blood [16]. These sequences capture different anatomical views, including long-axis, short-axis, outflow tract, and aortic, offering a comprehensive assessment of the heart. Despite the clinical importance of MCCMRI, it is often hindered by extensive acquisition times, which increases susceptibility to periodic or aperiodic body motion and causes patient discomfort. A viable solution is to reduce these lengthy acquisition times by subsampling the k𝑘kitalic_k-space during the MCCMRI acquisition. This approach accelerates the process by scanning fewer k𝑘kitalic_k-space data points than required by the Nyquist sampling theorem [6, 11]. This is typically achieved by applying a variety of subsampling patterns up to a specific acceleration factor. Common techniques include Cartesian trajectories such as equidistant sampling (each column sampled at a uniform distance from the next and previous), random-column sampling, variable density point sampling, pseudo-radial sampling, etc. [21, 22]. While these patterns are often applied uniformly across all time frames of the dynamic acquisition for ease of implementation, distinct patterns can also be applied to each time frame [14], exploiting potential cross-frame complementary information. Although subsampling the k𝑘kitalic_k-space accelerates MCCMRI, it also degrades the quality of the reconstructed images, rendering them less useful for clinical practice. The Cardiac MRI Reconstruction (CMRxRecon) challenge [4] held at MICCAI in 2023 demonstrated that deep learning (DL)-based approaches represent the state-of-the-art (SOTA) compared to traditional methods. In this work, we build upon the method that achieved one of the highest reconstruction quality submissions (2nd place in the challenge with minimal differences from the winning team) and demonstrated high reconstruction speed (more than 5×5\times5 × faster than the winning team) [19]. Specifically, the authors used the variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse-Problems (vSHARP) [20], which was adapted for 2D dynamic CMRI reconstruction. They demonstrated that a unified model was able to quickly (2-15s) and effectively reconstruct 4D CMRI volumes of varying contrast (Cine, T1, T2), views (short or long-axis), and acceleration factors (4×4\times4 ×, 8×8\times8 ×, 10×10\times10 ×). Building on vSHARP, we introduce an enhanced version with an Auxiliary Refinement Network (ARN) for multi-contrast, multi-view, multi-scheme, and multi-acceleration 2D dynamic reconstruction. Specifically, we employ a Variational Network [12] as the ARN, which generates an initial state for the auxiliary variable in vSHARP’s ADMM denoising step, using the subsampled k𝑘kitalic_k-space measurements as input to inform the reconstruction process of the specific type of input data. We outline 2D dynamic MRI reconstruction in Section 2, introduce our proposed methods in Section 3, present our experiments in Section 4, and conclude with a discussion of our results in Section 5."
https://arxiv.org/html/2411.01227v1,Rotational Odometry using Ultra Low Resolution Thermal Cameras,"[gagraphic.png] This letter provides what is, to the best of our knowledge, a first study on the applicability of ultra-low-resolution thermal cameras for providing rotational odometry measurements to navigational devices such as rovers and drones. Our use of an ultra-low-resolution thermal camera instead of other modalities such as an RGB camera is motivated by its robustness to lighting conditions, while being one order of magnitude less cost-expensive compared to higher-resolution thermal cameras. After setting up a custom data acquisition system and acquiring thermal camera data together with its associated rotational speed label, we train a small 4-layer Convolutional Neural Network (CNN) for regressing the rotational speed from the thermal data. Experiments and ablation studies are conducted for determining the impact of thermal camera resolution and the number of successive frames on the CNN estimation precision. Finally, our novel dataset for the study of low-resolution thermal odometry is openly released with the hope of benefiting future research.","\IEEEPARstart Odometry estimation is a fundamental aspect of any navigational device such as rovers, drones and cars [1, 2]. Using odometry, navigational devices can estimate key inertial measures such as their acceleration, their rotational and translational speed, and their position in the environment [3]. Traditionally, odometry is provided by using Inertial Measurement Units (IMUs) embedding an accelerometer, a gyroscope and a magnetic compass in one integrated sensor [4]. Thanks to Micro-Electro-Mechanical System (MEMS) technology, small-size IMU chips have become ubiquitous in many robotics and navigational applications [5]. On the other hand, using IMUs alone is known to suffer from growing estimation errors as the inertial measurement provided by the IMU are integrated through time [2]. This is due to the slowly-varying biases and non-idealities affecting the IMU readout [6]. For this reason, IMUs are often fused with visual data (from e.g., an RGB camera) forming a visual-inertial odometry system (VIO) [7]. The VIO fusion approach has been successfully used in many navigational settings to provide more precise positioning [8, 9], as well as in Simultaneous Localization and Mapping (SLAM) setups in order to concurrently map new environments while localizing the navigational device into the map [10, 11]. But using RGB cameras to form VIO systems also comes with the fundamental issue that cameras are greatly affected by lighting conditions [16]. This can lead to a significant degradation in the odometry estimation when using VIO systems in low-light and nigh-time conditions [9]. Hence, in order to increase the robustness of VIO systems to lighting conditions, the use of other sensing modalities such as radar, LIDAR, high-dynamic-range (HDR) cameras, event-based cameras and high-resolution thermal cameras have been explored in literature [15, 16, 9, 12]. Among these modalities, the use of thermal cameras has recently attracted a large attention due to its advantages in terms of sensing robustness, payload size and power consumption compared to the other aforementioned modalities [12, 17]. Indeed, radars are known to be power-hungry due to their use of multiple antennas with multiple power amplifiers used to attain the emitted power required at high frequency (e.g., 79-GHz is a typical frequency) [15]. LIDARs are still bulky [16] and both HDR and event-based cameras can be expensive while still not being immune to total night-time conditions [13, 19]. On the other hand, even though high-resolution thermal cameras can be both power- and size-efficient, they can still be expensive, costing in the ∼500similar-toabsent500\sim 500∼ 500 $currency-dollar\$$ range [14, 18]. Figure 1: Data acquisition setup. The 24×32243224\times 3224 × 32 thermal camera is connected to a readout board which translates its I2C interface to a serial interface via USB. A 100⁢μ100μ100\mu100 italic_μF decoupling capacitor is used for providing a stable power supply to the thermal camera. The thermal camera is mounted on top of a servo motor controlled by a micro-controller via serial interface over USB. This setup enables the acquisition of thermal camera data while rotating the camera at precisely-controlled speeds. Figure 2: CNN architecture for the estimation of rotational speed from thermal camera data. The CNN is composed of two convolutional layers (with max pooling in between), followed by two fully-connected layers and an linear output layer. This small-size architecture has been designed with the aim of reducing the CNN compute complexity for potential implementation in CNN accelerator hardware [20]. In order to reduce the system cost of thermal-based odometry systems, this paper present what is, to the best of our knowledge, the first demonstration of thermal-based rotational odometry using an ultra-low-resolution (24×32243224\times 3224 × 32) thermal camera (reducing the sensor costs to the ∼50similar-toabsent50\sim 50∼ 50 $currency-dollar\$$ range) [14, 18]. The contributions of this paper are the following: 1. We build a custom data acquisition setup for acquiring low-resolution (24×32243224\times 3224 × 32) thermal camera data with a precise control of the camera’s azimuth rotation speed in order to obtain a labelled dataset of thermal camera frames and rotational speed. 2. We study the use of small-scale Convolutional Neural Networks (CNNs) for regressing the rotational speed from the thermal camera frames. 3. We provide a study on the impact of thermal camera resolution and the number of consecutive input frames on the CNN odometry accuracy. 4. We release our dataset as open-source to help future research. This letter is organized as follows. Section 2 provides a description of our dataset and data acquisition hardware. Section 3 describes our CNN design choices and training approach. Section 4 describes our experimental results. Finally, Section 5 provides conclusions."
https://arxiv.org/html/2411.01200v1,GarmentLab: A Unified Simulation and Benchmark for Garment Manipulation,"Manipulating garments and fabrics has long been a critical endeavor in the development of home-assistant robots. However, due to complex dynamics and topological structures, garment manipulations pose significant challenges. Recent successes in reinforcement learning and vision-based methods offer promising avenues for learning garment manipulation. Nevertheless, these approaches are severely constrained by current benchmarks, which offer limited diversity of tasks and unrealistic simulation behavior. Therefore, we present GarmentLab, a content-rich benchmark and realistic simulation designed for deformable object and garment manipulation. Our benchmark encompasses a diverse range of garment types, robotic systems and manipulators. The abundant tasks in the benchmark further explores of the interactions between garments, deformable objects, rigid bodies, fluids, and human body. Moreover, by incorporating multiple simulation methods such as FEM and PBD, along with our proposed sim-to-real algorithms and real-world benchmark, we aim to significantly narrow the sim-to-real gap. We evaluate state-of-the-art vision methods, reinforcement learning, and imitation learning approaches on these tasks, highlighting the challenges faced by current algorithms, notably their limited generalization capabilities. Our proposed open-source environments and comprehensive analysis show promising boost to future research in garment manipulation by unlocking the full potential of these methods. We guarantee that we will open-source our code as soon as possible. You can watch the videos in supplementary files to learn more about the details of our work. Our project page is available at: https://garmentlab.github.io/","The next-generation assistant robots should possess not only the abilities to separately manipulate a wide variety of objects, including rigid, articulated[59], and deformable objects[58], but also the capability to leverage interactions between those physical media, including flow and fluids, in order to assist humans[39]. Among various daily tasks [69, 59, 56], garment manipulation stands out as one of the most challenging, crucial, and extensively discussed tasks in the robotics and computer vision, due to its demanding requirements for understanding dynamic properties of physical instances and interactions between them. For instance, washing clothes entails the interaction between garments and fluids, while dressing up requires collaboration between robots and humans. Figure 1: GarmentLab provides realistic simulation for diverse garments with different physical propoerties, benchmarking various novel garment manipulation tasks in both simulation and the real world. Garment manipulation tasks mainly presents three challenges. Firstly, each individual garment possesses nearly infinite states and exhibits complex kinematic and dynamic properties. Therefore, it is crucial for models to comprehend the various self-deform states of garments, which usually requires large amount of training data [17, 6] (C1). Secondly, garment manipulation involves interactions with various types of objects, including rigid (e.g., clothes hanger) and articulated (e.g., wardrobe) objects, as well as fluids and human body. Consequently, enabling models to understand these interactions across diverse physical media presents great significance (C2). Finally, considering that strategies for manipulating garments are often highly complex, and visual perception of garments is more challenging due to their diverse states and patterns, manipulating garments faces a greater sim2real gap [63, 29] (C3). Training a powerful agent capable of overcoming these challenges necessitates a vast amount of data encompassing robot-object interactions. However, directly collecting data from the real world is impractical. Thus, researchers have long pursued benchmarks for garment manipulation [30, 4, 6, 64]. Current deformable simulations suffer from drawbacks such as missing garment meshes [30]. Additionally, they offer a limited range of tasks, hindering further research endeavors. Therefore, we present GarmentLab (Figure 1), a unified environment and benchmark for garment manipulation. GarmentLab has three novel components to satisfy the demands for diversity and realism: The powerful GarmentLab Engine, which is built upon Omniverse Isaac Sim [71] and supports a variety of physical simulation methods. The simulator not only supports Particle-Based-Dynamic (PBD) [3], Finite-Element-Method (FEM) [11], to simulate garments, fluid and deformable objects but also makes integration with ROS [42] to provide an efficient teleoperation pipeline for data collection. GarmentLab Assets is a large-scale indoor dataset comprising 1) garments models covering 11 categories of daily garments from ClothesNet [70] 2) various kinds of robot end-effector including gripper, suction and dexterous hands. 3) high-quality 3D assets including 20 scenes and 9000+ object models from ShapeNet [7]. Based on realistic simulation and rich assets, we propose GarmentLab Benchmark containing 20 tasks divided into 5 groups to evaluate state-of-the-art vision-based and reinforcement learning based algorithm. To tackle above challenges, our environment has three characteristics:1) Efficient. Garment manipulation involves nearly infinite object state and action spaces, requiring substantial data for models to understand garment structure and deformation. To meet this demand, our highly parallelized GPU-based simulator provides a significant training advantage. Larger batch sizes enhance RL-based algorithms [33], while faster data collection speeds reduce training time for perception-based algorithms (tackling C1). 2) Rich. The richness of our simulator can be categorized into two aspects: the diversity of simulation content offered by GarmentLab Assets and the depth of physical interaction facilitated by GarmentLab Engine. We emphasize multi-physics simulation, encompassing rigid-articulated, deformable-garment, fluid dynamics, and flow, along with their interactions. This focus is vital for training agents capable of comprehending real-world physical properties [48] (tackling C2). You can refer to videos in supplementary material for our simulation effects. 3) Real. As the sim-to-real gap emerges as the main obstacle in developing embodied agents, GarmentLab Engine surpasses Omniverse capabilities by providing mature sim-to-real algorithms, such as Teleoperation [41] utilized in the RL field, and the Visual Sim-Real Alignment Algorithm employed in perception algorithms. We also make integration with ROS [42] and MoveIt [10], which is beneficial for narrowing sim2real gap by introducing real-world robot motions into simulation (tackling C3). Our benchmark experiments highlight the significant challenges current algorithms face, even with seemingly simple tasks like unfolding. These difficulties arise from a lack of understanding of physical interactions and the complexities of high-dimensional states. Vision-based algorithms demonstrate limited generalization, with performance strongly affected by the initial state of objects. RL-based algorithms also encounter difficulties with tasks requiring long-horizon planning. These insights offer valuable guidance for improving methods for garment and deformable object manipulation. In summary, we have made the following contributions in GarmentLab: • We propose GarmentLab Environment, a realistic and rich environment for garment manipulation, featuring diverse simulation methods, assets, object physics and multi-material interactions. • Based on GarmentLab Environment, we propose GarmentLab Benchmark, benchmarking a large variety of garment manipulation tasks, and providing the first real-world garment manipulation benchmark that can be reproduced internationally. • We integrate different sim2real methods into GarmentLab, providing solutions to narrowing the sim2real and further facilitating the real-world applications. • Extensive experiments and detailed analyses of different types of garment manipulation algorithms facilitate and enlight future research on garment manipulation."
https://arxiv.org/html/2411.01173v1,Reasoning Limitations of Multimodal Large Language Models. A case study of Bongard Problems,"Abstract visual reasoning (AVR) encompasses a suite of tasks whose solving requires the ability to discover common concepts underlying the set of pictures through an analogy-making process, similarly to human IQ tests. Bongard Problems (BPs), proposed in 1968, constitute a fundamental challenge in this domain mainly due to their requirement to combine visual reasoning and verbal description. This work poses a question whether multimodal large language models (MLLMs) inherently designed to combine vision and language are capable of tackling BPs. To this end, we propose a set of diverse MLLM-suited strategies to tackle BPs and examine four popular proprietary MLLMs: GPT-4o, GPT-4 Turbo, Gemini 1.5 Pro, and Claude 3.5 Sonnet, and four open models: InternVL2-8B, LLaVa-1.6 Mistral-7B, Phi-3.5-Vision, and Pixtral 12B. The above MLLMs are compared on three BP datasets: a set of original BP instances relying on synthetic, geometry-based images and two recent datasets based on real-world images, i.e., Bongard-HOI and Bongard-OpenWorld. The experiments reveal significant limitations of MLLMs in solving BPs. In particular, the models struggle to solve the classical set of synthetic BPs, despite their visual simplicity. Though their performance ameliorates on real-world concepts expressed in Bongard-HOI and Bongard-OpenWorld, the models still have difficulty in utilizing new information to improve their predictions, as well as utilizing a dialog context window effectively. To capture the reasons of performance discrepancy between synthetic and real-world AVR domains, we propose Bongard-RWR, a new BP dataset consisting of real-world images that translates concepts from hand-crafted synthetic BPs to real-world concepts. The MLLMs’ results on Bongard-RWR suggest that their poor performance on classical BPs is not due to domain specificity but rather reflects their general AVR limitations.","Analogy-making is a critical aspect of human cognition, tightly linked with fluid intelligence, the capacity to apply learned skills in novel settings (Lake et al., 2017). Several approaches have been proposed to build systems capable of making analogies. Notably, the structure-mapping theory explores methods for discovering structural correspondences between pre-existing object representations (Winston, 1982; Gentner, 1983; Carbonell, 1983; Falkenhainer et al., 1989; Holyoak & Thagard, 1989). However, these approaches often overlook the perceptual aspect, assuming object representations are already given. Chalmers et al. (1992) highlight that forming useful representations is an intricate challenge. In particular, perception is not merely a passive reception of sensory data, but rather an active interpretation influenced by prior knowledge. This process involves the detection of patterns, recognition of analogies, and abstraction of concepts. The resultant representations may vary significantly depending on the context, which underscores the importance of modeling perception and cognition jointly (Hofstadter, 1995). Multiple problems that necessitate combined perception and reasoning have been identified (Hofstadter, 1999). Among these tasks are Bongard Problems (BPs), introduced by Bongard (1968; 1970). Initial BPs were designed manually, leading to the formulation of a few hundred task instances by individual contributors (Foundalis, 2006b). A typical BP consists of two sides, left and right, each comprising six image panels arranged in a grid. All images on one side illustrate a shared concept absent in the images on the opposite side. The task is to identify the underlying rule that differentiates the sides and articulate it in natural language. Initial BPs (Bongard, 1968), akin to human IQ tests, featured abstract 2D geometric shapes, putting the focus on abstract reasoning. However, recent works have expanded the set of BPs to include real-world images, which broadens the scope of presented objects, attributes and relations. Specifically, the matrices in Bongard HOI (Jiang et al., 2022) depict human-object interactions, while Bongard-OpenWorld (Wu et al., 2024) employs open-world free-form concepts, increasing the diversity of featured scenes. Figs. 1(a), 1(c), 1(d) illustrate examples of problems from the three above-mentioned datasets. A central theme in BPs is recognition of concepts in a context-dependent manner, as object representations need to be formed specifically for the presented matrix, rather than described a priori (Linhares, 2000). For example, consider the matrix in Fig. 1(a) – an analysis restricted to its left side may yield multiple concepts, such as the presence of curves or an object centered in the image. Only through a comprehensive understanding of both matrix sides one can recognize that the left side depicts a single line, while the right side presents two lines. Such concept-based tasks were argued to promote a more accurate evaluation of a system’s generalization ability and its capacity for abstraction (Mitchell, 2021; Odouard & Mitchell, 2022). Moreover, the concepts in BPs are illustrated with several image examples, which positions the task within a few-shot learning setting (Fei-Fei et al., 2006; Wang et al., 2020). In contrast to other abstract reasoning problems, such as Raven’s Progressive Matrices (RPMs) (Raven, 1936; Raven & Court, 1998; Małkiński & Mańdziuk, 2022) that have recently witnessed the development of large-scale benchmarks (Barrett et al., 2018; Zhang et al., 2019), BPs allow to assess system’s ability to derive concepts from a limited set of examples (typically six images per matrix side). The above aspects make BPs a valuable testbed for assessing abstract reasoning abilities of AI models. Motivation. The quest to build systems capable of forming abstract concepts dates back to the 1950s (McCarthy et al., 2006). The advent of Deep Learning (DL) opened new possibilities to tackle BPs (Kharagorgiev, 2018; Nie et al., 2020). However, despite significant advancements, methods for consistently solving BPs (and other problems that involve abstract reasoning) are still lacking (Mitchell, 2021; van der Maas et al., 2021; Stabinger et al., 2021). Typically, DL approaches omit the generation of natural language answers by casting BP into a binary classification task, in which a test image had to be assigned to the matching side of the matrix. Conversely, a parallel stream of research on large language models (LLMs) demonstrated promising results in open-ended language generation (Brown et al., 2020). In particular, LLMs were applied to selected AVR tasks (Webb et al., 2023), though, lately Xu et al. (2024a) pointed certain LLM limitations in solving AVR problems represented as text despite using information lossless translation through direct-grid encoding. Recent works have combined the vision and language modalities into multimodal large language models (MLLMs) (Achiam et al., 2023; Reid et al., 2024; Anthropic, 2024), inviting their application to diverse tasks (Yin et al., 2023; Wu et al., 2023). Motivated by these recent developments we examine the reasoning capabilities of MLLMs in solving BPs. Contributions. The main contribution of this paper is four-fold. (1) For the first time in the literature, we consider BPs in the context of MLLMs and propose a diverse set of strategies to solve BP instances in two setups: open-ended language generation and binary classification. (2) We evaluate 4444 state-of-the-art proprietary MLLMs and 4444 open MLLMs on both synthetic and real-world BPs, and identify their severe abstract reasoning limitations. (3) To further examine the main difficulties faced by MLLMs in solving both types of BPs (synthetic and real world ones) we introduce a focused dataset of BPs (Bongard-RWR) comprising real-world images that represent concepts from synthetic BPs using real world images. Thanks to relying on the same abstract concepts as synthetic BPs, Bongard-RWR facilitates direct comparisons of the MLLMs performance in both domains. (4) We perform a detailed comparative analysis of 8888 MLLMs on Bongard-RWR vs. synthetic BPs, shedding light on the reasons of their generally poor performance."
https://arxiv.org/html/2411.01144v1,LEARNER: Learning Granular Labels from Coarse Labels using Contrastive Learning,"A crucial question in active patient care is determining if a treatment is having the desired effect, especially when changes are subtle over short periods. We propose using inter-patient data to train models that can learn to detect these fine-grained changes within a single patient. Specifically, can a model trained on multi-patient scans predict subtle changes in an individual patient’s scans?Recent years have seen increasing use of deep learning (DL) in predicting diseases using biomedical imaging, such as predicting COVID-19 severity using lung ultrasound (LUS) data. While extensive literature exists on successful applications of DL systems when well-annotated large-scale datasets are available, it is quite difficult to collect a large corpus of personalized datasets for an individual.In this work, we investigate the ability of recent computer vision models to learn fine-grained differences while being trained on data showing larger differences. We evaluate on an in-house LUS dataset and a public ADNI brain MRI dataset. We find that models pre-trained on clips from multiple patients can better predict fine-grained differences in scans from a single patient by employing contrastive learning.","Fig. 1: In this work, we show that using contrastive loss helps models learn finer differences from coarse differences in the labels. The figure shows that the embeddings distribution are more spread out in the case of our proposed contrastive loss (Eq 3) than using only MSE loss. This helps models to predict subtle changes in scans of a given patient when pre-trained on data comparing scans from multiple patients. In recent years, automated patient monitoring has become crucial for sustaining medical infrastructure, particularly evident during the COVID-19 pandemic when clinicians had to make life-altering tough decisions on resource allocation, such as access to oxygen [1]. A key question in active patient care is determining if a treatment is effective. Clinicians need to know if a patient is improving to adjust the treatment if necessary. However, detecting treatment effects in lung ultrasound is challenging because changes are slow and progressive, making subtle differences difficult to capture. This issue arises because a patient’s scans are highly correlated, allowing models to easily recognize if scans belong to the same patient [6], which complicates the detection of subtle changes. We propose leveraging inter-patient data to train models to learn intra-patient changes. It is easier to distinguish between scans from different patients (inter-patient differences) than between scans from the same patient (intra-patient changes). In this work, we investigate recent computer vision models’ ability to learn fine-grained differences in data while being trained on data that show larger/coarse-grained differences. Given LUS clips from multiple patients, could a model be trained to predict differences in scans of a single patient on a fine-grained scale? We evaluate on two longitudinal datasets, a) an in-house lung ultrasound dataset (LUS) wherein the aim is to track subtle changes in the patients SpO2/FiO2 (S/F) ratio (a measure of hypoxemia in patients) over subsequent scans. b) the public ADNI Alzheimer’s Disease MRI dataset wherein the goal is to track changes in a patient’s Mini-mental state examination (MMIM) over subsequent scans. In this work, we wish to extend the previous idea to tackle a particular problem, can LUS (MRI) clips of multiple patients - which are coarsely different in nature - be used to detect fine-grained changes observed in a particular patient (which is not seen during training)? We propose, using a contrastive training routine based on the S/F ratio (MMIM) differences between multiple video clips in a batch to help the model learn further during its training phase. We hypothesize that in learning differences between latent representations of the data, the model might be able to figure out fine differences which could then be used for characterizing changes in a single patient’s LUS clips. Contrastive learning [2] has been applied in previous works in many different formats [10, 7, 9]. The basic idea of contrastive learning includes utilizing latent feature representations of input data along with the fact that intra-class features are expected to show more similarity than features from different classes. In the past, contrastive learning has been applied to classify between COVID-19, healthy, and other pneumonic diseases. [10, 7] applied classic contrastive learning on a few-shot COVID-19 prediction problem and provided insightful results. Other works such as [9] utilized a multimodal strategy by incorporating textual data along with LUS clips in a contrastive learning routine. Recently, [4] demonstrated a weakly supervised training method that applied contrastive learning to predict COVID-19 severity using noisy labels."
https://arxiv.org/html/2411.01053v1,Contrasting with Symile: Simple Model-Agnostic Representation Learning for Unlimited Modalities,"Contrastive learning methods, such as CLIP, leverage naturally paired data—for example, images and their corresponding text captions—to learn general representations that transfer efficiently to downstream tasks. While such approaches are generally applied to two modalities, domains such as robotics, healthcare, and video need to support many types of data at once. We show that the pairwise application of CLIP fails to capture joint information between modalities, thereby limiting the quality of the learned representations. To address this issue, we present Symile, a simple contrastive learning approach that captures higher-order information between any number of modalities. Symile provides a flexible, architecture-agnostic objective for learning modality-specific representations. To develop Symile’s objective, we derive a lower bound on total correlation, and show that Symile representations for any set of modalities form a sufficient statistic for predicting the remaining modalities. Symile outperforms pairwise CLIP, even with modalities missing in the data, on cross-modal classification and retrieval across several experiments including on an original multilingual dataset of 33M image, text and audio samples and a clinical dataset of chest X-rays, electrocardiograms, and laboratory measurements. All datasets and code used in this work are publicly available at https://github.com/rajesh-lab/symile.","Contrastive learning leverages naturally paired data to learn general representations that transfer efficiently to downstream tasks [53, 35, 3]. A common contrastive approach is to maximize the mutual information between the paired modalities, ensuring that the learned representations retain sensitivity to all correlations between them. While SimCLR [12] popularized the use of the mutual information estimator InfoNCE [38] for data augmentations, CLIP [40] applied the approach to distinct modalities—for example, images and their corresponding text captions—where representations are learned using any encoder for each modality. While contrastive approaches are generally applied to two modalities, there is a rapidly expanding range of domains that require the integration of many types of data at once. For example, in robotics, agents combine information from visual, proprioceptive, and tactile sensors [18, 28]; healthcare providers analyze various types of patient data including imaging, biosignals, and genomics [10, 29]; and video encompasses RGB frames, audio waveforms, and text transcripts [55]. One strategy for handling multimodal data has been to design specialized architectures capable of processing all data types at once, which limits their general applicability and increases operational complexity [2, 47]. Another common approach is to apply two-modality contrastive objectives, such as CLIP, to pairs of available modalities [15, 44]. In this paper, we show that, despite its popularity, the pairwise application of CLIP fails to capture higher-order conditional information between modalities, thereby limiting the quality of the representations it learns. For instance, given three modalities 𝐚𝐚\mathbf{a}bold_a, 𝐛𝐛\mathbf{b}bold_b, and 𝐜𝐜\mathbf{c}bold_c, pairwise CLIP captures dependencies between 𝐚𝐚\mathbf{a}bold_a and 𝐛𝐛\mathbf{b}bold_b, 𝐛𝐛\mathbf{b}bold_b and 𝐜𝐜\mathbf{c}bold_c, and 𝐚𝐚\mathbf{a}bold_a and 𝐜𝐜\mathbf{c}bold_c, yet cannot capture any conditional dependencies, such as between 𝐚𝐚\mathbf{a}bold_a and 𝐛𝐛\mathbf{b}bold_b given 𝐜𝐜\mathbf{c}bold_c. We show in Section 2.2 that even in a simple one-dimensional controlled setting where the target 𝐛𝐛\mathbf{b}bold_b is perfectly predictable from 𝐚𝐚\mathbf{a}bold_a and 𝐜𝐜\mathbf{c}bold_c, CLIP performs no better than random chance. Effective contrastive learning for more than two modalities requires a model-agnostic approach capable of learning modality-specific representations—like CLIP—yet also captures higher-order information between any number of modalities—unlike CLIP. Methodological contributions. This paper presents Symile, a simple contrastive learning approach that captures higher-order information between any number of modalities. Symile provides a flexible, architecture-agnostic objective for learning modality-specific representations. To develop Symile’s objective, we derive a total correlation estimator, employing a generalization of inner products to more than two vectors that allows for the simultaneous contrasting of all modalities and enables zero-shot applications such as classification and retrieval. We then show that the representations produced by Symile for any set of modalities form a sufficient statistic for predicting the remaining modalities not considered in the set. Because it targets total correlation, Symile captures strictly more information than CLIP, guaranteeing performance that matches or surpasses CLIP, except in cases where it known that only pairwise statistics are relevant. Given that such prior knowledge is rarely available, Symile should be favored over CLIP. Empirical contributions. We demonstrate that Symile outperforms pairwise CLIP on cross-modal classification and retrieval across several experiments including on a multilingual dataset of images, text and audio of over 33M examples and a clinical dataset of chest X-rays, electrocardiograms, and laboratory measurements. We show that Symile retains its advantage over pairwise CLIP even with modalities missing in the data. We publicly release both the multilingual and the clinical datasets, which are specifically designed to test a model’s ability to capture higher-order information between three distinct high-dimensional data types."
https://arxiv.org/html/2411.01034v1,Evaluation Metric for Quality Control and Generative Models in Histopathology Images,"Our study introduces ResNet-L2 (RL2), a novel metric for evaluating generative models and image quality in histopathology, addressing limitations of traditional metrics, such as Fréchet inception distance (FID), when the data is scarce. RL2 leverages ResNet features with a normalizing flow to calculate RMSE distance in the latent space, providing reliable assessments across diverse histopathology datasets. We evaluated the performance of RL2 on degradation types, such as blur, Gaussian noise, salt-and-pepper noise, and rectangular patches, as well as diffusion processes. RL2’s monotonic response to increasing degradation makes it well-suited for models that assess image quality, proving a valuable advancement for evaluating image generation techniques in histopathology. It can also be used to discard low-quality patches while sampling from a whole slide image. It is also significantly lighter and faster compared to traditional metrics and requires fewer images to give stable metric value.","Figure 1: The process of computing RL2. In the training phase, the ResNet-normalizing flow network is trained on the given real (high-quality) images. In the evaluation phase, real and generated (or evaluation) images are passed through the network, and the L2 distance between mean of latent vectors of real and generated (or evaluation) images is used as the final metric. Generative models, particularly generative adversarial networks (GAN), variational autoencoders (VAE), and diffusion are used in histopathology for various tasks, such as data augmentation [1], anomaly detection [2], and synthetic image generation [3], for improving diagnostic models by addressing scarcity of data. GANs and diffusion generate realistic images to train models for tasks, such as cancer detection [4], while VAEs aid in anomaly detection by reconstructing healthy tissues and identifying deviations [5]. These models also facilitate style transfer between staining protocols, thus reducing costs and enhancing cross-modal comparisons. Overall, generative models enhance data efficiency and diagnostic accuracy in histopathology. Evaluating the quality and performance of generative models is challenging due to the lack of well-established, theory-backed metrics. This complexity stems from the need to assess various aspects of generated images, such as quality, aesthetics, realism, and diversity, all of which are inherently subjective. While human evaluation is effective, it is costly, time-consuming, and impractical for large datasets. As a result, researchers rely on automated evaluation methods that should ideally respond consistently to image degradation, accurately measure image quality, and remain computationally efficient, particularly in resource-limited environments. Commonly used metrics for evaluating generative models include Inception Score (IS) [6], Kernel Inception Distance (KID) [7], Fréchet Inception Distance (FID) [8], and contrastive language-image pre-training (CLIP) Maximum Mean Discrepancy (CMMD) [9]. KID, FID and CMMD require thousands of real and generated images to compute a stable metric which becomes a big issue in medical domain with data scarcity. Flow-based Likelihood Distance [10] have been proposed as an alternative, functioning effectively with fewer data, but they come with the complexity of training large normalizing flows, which is inefficient due to the need to maintain the large latent space. Another limitation of these metrics is their reliance on Inception-V3 features, which are trained on natural image datasets such as ImageNet. While effective for evaluating generative models that produce natural images, this approach performs poorly in domains, such as medical imaging, particularly in histopathology, where the image characteristics differ significantly from natural images. Additionally, a quality metric trained on real patches of high quality can also be used to filter out low-quality patches from a whole slide image while training and testing deep learning pipelines for weakly supervised learning in histopathology [11]."
https://arxiv.org/html/2411.01019v1,A lightweight Convolutional Neural Network based on U shape structure and Attention Mechanism for Anterior Mediastinum Segmentation,"To automatically detect Anterior Mediastinum Lesions (AMLs) in the Anterior Mediastinum (AM), the primary requirement will be an automatic segmentation model specifically designed for the AM. The prevalence of AML is extremely low, making it challenging to conduct screening research similar to lung cancer screening. Retrospectively reviewing chest CT scans over a specific period to investigate the prevalence of AML requires substantial time. Therefore, developing an Artificial Intelligence (AI) model to find location of AM helps radiologist to enhance their ability to manage workloads and improve diagnostic accuracy for AMLs. In this paper, we introduce a U-shaped structure network to segment AM. Two attention mechanisms were used for maintaining long-range dependencies and localization. In order to have the potential of Multi-Head Self-Attention (MHSA) and a lightweight network, we designed a parallel MHSA named Wide-MHSA (W-MHSA). Maintaining long-range dependencies is crucial for segmentation when we upsample feature maps. Therefore, we designed a Dilated Depth-Wise Parallel Path connection (DDWPP) for this purpose. In order to design a lightweight architecture, we introduced an expanding convolution block and combine it with the proposed W-MHSA for feature extraction in the encoder part of the proposed U-shaped network. The proposed network was trained on 2775 AM cases, which obtained an average Dice Similarity Coefficient (DSC) of 87.83%, mean Intersection over Union (IoU) of 79.16%, and Sensitivity of 89.60%. Our proposed architecture exhibited superior segmentation performance compared to the most advanced segmentation networks, such as Trans_Unet, Attention_Unet, Res_Unet, and Res_Unet++.","The global utilization of Computed Tomography (CT) scans is on the rise. The UNSCEAR 2022 report estimated that globally, from 2009 to 2018, the number of examinations nearly doubled compared to 2006. Chest CT scans accounted for 12.2% of the total, making them the second most frequently performed type of CT scan, following head CTs [1]. With the increase in chest CT examinations, the detection of Anterior Mediastinal Lesions (AMLs) may also rise. Although their frequency is reported to be less than 1% [2, 3, 4], identifying these lesions on CT is crucial for determining subsequent management [5]. With the increase in radiological examinations, radiologist burnout has emerged as a significant issue in the medical field. One potential solution being explored is the use of Artificial Intelligence (AI) [6]. In the domain of chest radiology, the implementation of comprehensive lung cancer screening programs is anticipated to lead to an increase in the use of Low-dose Chest CT (LDCT) scans [7]. Detecting incidental AMLs on LDCT is important, as these findings may indicate the presence of tumors. This area represents a promising opportunity for radiologists to utilize AI assistance, enhancing their ability to manage workloads and improve diagnostic accuracy. An automated segmentation model particularly tailored for the AM is a crucial prerequisite for automatically identifying AMLs in the AM, akin to lung cancer screening, which poses a significant hurdle. Retrospectively reviewing chest CT scans over a specific period to investigate the prevalence of AML requires substantial time. Additionally, the lack of commercially available AI models makes utilizing AI for such research difficult. Developing an AI model for AML is particularly challenging because the AM does not have clear boundaries on CT scans, unlike lung parenchyma. This study establishes a hypothetical boundary for the AM and subsequently develops an AM segmentation AI model to assist in investigating the prevalence of AML. Radiologists can now get assistance utilizing Computer-Aided Detection (CADe) techniques. By highlighting potential organ areas within CT images, CADe methods make screening more efficient and cost-effective [8] [9]. Adhering to segmentation methods is an effective strategy for successfully implementing a CADe system. Segmentation methods are useful in delineating the outlines of lesions and organs, which are essential factors for assessing the malignancy of the lesions [10]. These processes emphasize anomalies of CT imaging at the pixel level, which allows them to be distinguished from other procedures. Some methods in [11] and [12] were considered for lesion and organ segmentation, but those methods depend on handwrought features that need several manual procedures. In recent years, Deep Neural Networks (DNN) have become a professional strategy to solve problems in many areas [13, 14, 15], and medical image segmentation is one of them [16, 17]. The main issues in medical image segmentation for training DNN are lack of dataset, diversity of shapes, domain shift, etc. Using pre-train models, designing lightweight and sophisticated models, and K-fold cross-validation are some methods that are used to solve issues in medical image segmentation. This work suggests a U shape architecture for segmenting the AM organ from chest CT images to aid radiologists. The proposed model identifies location of AM without any pre-processing or fine-tuning. It employs two distinct attention processes to give a rich feature map. According to the results, the proposed architecture performed better than state-of-the-art (SOTA) segmentation networks involving Res_Unet[18], Res_Unet++ [19], Trans_Unet [20], Attention_Unet [21], and Unet. The main contributions of this paper are as follows: • This research introduced a U shape structure network for AM segmentation which uses expanded convolution and wide multi-head self-attention in the encoder. This attention effectively allows the model to learn different attention components inside separate subspaces and long range dependencies. • Specifically, we used Channel Depth-Wise Cross-Correlation Attention (CD-WCC) in the encoder to find similarities between the encoder and decoder to improve organ localization. • The proposed architecture is more lightweight than the SOTA network, with just 6.7 million parameters while it has achieved acceptable and higher performance in all metrics. • To show that our proposed architecture is not limited to our proposed encoder, we used Resnet18 [22] and the encoder part of the original Unet as the encoder part in the proposed network. These results illustrates the effectiveness of the proposed structure. The rest of the paper is organized as follows: Section II provides a summary of the related research on image segmentation using the Unet framework. In Section III, we elaborate each datails of the proposed Unet. Next, Section IV describes an overview of datasets and the methodologies used to analyze and validate the accuracy of AM segmentation and experimental results. In Section V, we show the advantage of the proposed network compared to SOTA networks. Finally, the conclusions can be found in Section VI."
https://arxiv.org/html/2411.01001v1,"Automated Assessment of Residual Plots with Computer Vision
Models","Plotting the residuals is a recommended procedure to diagnose deviations from linear model assumptions, such as non-linearity, heteroscedasticity, and non-normality. The presence of structure in residual plots can be tested using the lineup protocol to do visual inference. There are a variety of conventional residual tests, but the lineup protocol, used as a statistical test, performs better for diagnostic purposes because it is less sensitive and applies more broadly to different types of departures. However, the lineup protocol relies on human judgment which limits its scalability. This work presents a solution by providing a computer vision model to automate the assessment of residual plots. It is trained to predict a distance measure that quantifies the disparity between the residual distribution of a fitted classical normal linear regression model and the reference distribution, based on Kullback-Leibler divergence. From extensive simulation studies, the computer vision model exhibits lower sensitivity than conventional tests but higher sensitivity than human visual tests. It is slightly less effective on non-linearity patterns. Several examples from classical papers and contemporary data illustrate the new procedures, highlighting its usefulness in automating the diagnostic process and supplementing existing methods.","Plotting residuals is commonly regarded as a standard practice in linear regression diagnostics (Belsley, Kuh, and Welsch 1980; Cook and Weisberg 1982). This visual assessment plays a crucial role in identifying whether model assumptions, such as linearity, homoscedasticity, and normality, are reasonable. It also helps in understanding the goodness of fit and various unexpected characteristics of the model. Generating a residual plot in most statistical software is often as straightforward as executing a line of code or clicking a button. However, accurately interpreting a residual plot can be challenging. A residual plot can exhibit various visual features, but it is crucial to recognize that some may arise from the characteristics of predictors and the natural stochastic variation of the observational unit, rather than indicating a violation of model assumptions (Li et al. 2024). Consider Figure 1 as an example, the residual plot displays a triangular left-pointing shape. The distinct difference in the spread of the residuals across the fitted values may result in the analyst suggesting that there may be heteroskedasticity, however, it is important to avoid over-interpreting this visual pattern. In this case, the fitted regression model is correctly specified, and the triangular shape is actually a result of the skewed distribution of the predictors, rather than indicating a flaw in the model. The concept of visual inference, as proposed by Buja et al. (2009), provides an inferential framework to assess whether residual plots indeed contain visual patterns inconsistent with the model assumptions. The fundamental idea involves testing whether the true residual plot visually differs significantly from null plots, where null plots are plotted with residuals generated from the residual rotation distribution (Langsrud 2005), which is a distribution consistent with the null hypothesis H0subscript𝐻0H_{0}italic_H start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT that the linear regression model is correctly specified. Typically, the visual test is accomplished through the lineup protocol, where the true residual plot is embedded within a lineup alongside several null plots. If the true residual plot can be distinguished from the lineup, it provides evidence for rejecting H0subscript𝐻0H_{0}italic_H start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. The practice of delivering a residual plot as a lineup is generally regarded as a valuable approach. Beyond its application in residual diagnostics, the lineup protocol has been integrated into the analysis of diverse subjects. For instance, Loy and Hofmann 2013; 2014; 2015 illustrated its applicability in diagnosing hierarchical linear models. Additionally, Widen et al. (2016) and Fieberg, Freeman, and Signer (2024) demonstrated its utility in geographical and ecology research respectively, while Krishnan and Hofmann (2021) explored its effectiveness in forensic examinations. A practical limitation of the lineup protocol lies in its reliance on human judgements (see Li et al. 2024, about the practical limitations). Unlike conventional statistical tests that can be performed computationally in statistical software, the lineup protocol requires human evaluation of images. This characteristic makes it less suitable for large-scale applications, given the associated high labour costs and time requirements. There is a substantial need to develop an approach to substitute these human judgement with an automated reading of data plots using machines. The utilization of computers to interpret data plots has a rich history, with early efforts such as “Scagnostics” by Tukey and Tukey (1985), a set of numerical statistics that summarize features of scatter plots. Wilkinson, Anand, and Grossman (2005) expanded on this work, introducing scagnostics based on computable measures applied to planar proximity graphs. These measures, including, but not limited to, “Outlying”, “Skinny”, “Stringy”, “Straight”, “Monotonic”, “Skewed”, “Clumpy”, and “Striated”, aimed to characterize outliers, shape, density, trend, coherence and other characteristics of the data. While this approach has been inspiring, there is a recognition (Buja et al. 2009) that it may not capture all the necessary visual features that differentiate true residual plots from null plots. A more promising alternative entails enabling machines to learn the function for extracting visual features from residual plots. Essentially, this means empowering computers to discern the crucial visual features for residual diagnostics and determining the method to extract them. Modern computer vision models are well-suited for addressing this challenge. They rely on deep neural networks with convolutional layers (Fukushima and Miyake 1982). These layers use small, sliding windows to scan the image, performing a dot product to extract local features and patterns. Numerous studies have demonstrated the efficacy of convolutional layers in addressing various vision tasks, including image recognition (Rawat and Wang 2017). Despite the widespread use of computer vision models in fields like computer-aided diagnosis (Lee and Chen 2015), pedestrian detection (Brunetti et al. 2018), and facial recognition (Emami and Suciu 2012), their application in reading data plots remains limited. While some studies have explored the use of computer vision models for tasks such as reading recurrence plots for time series regression (Ojeda, Solano, and Peramo 2020), time series classification (Chu et al. 2019; Hailesilassie 2019; Hatami, Gavet, and Debayle 2018; Zhang et al. 2020), anomaly detection (Chen, Su, and Yang 2020), and pairwise causality analysis (Singh et al. 2017), the application of reading residual plots with computer vision models is a new field of study. In this paper, we develop computer vision models and integrate them into the residual plots diagnostics workflow, addressing the need for an automated visual inference. The paper is structured as follows. Section 2 discusses various specifications of the computer vision models. Section 3 defines the distance measure used to detect model violations, while Section 4 explains how the computer vision models estimate this distance measure. Section 5 covers the statistical tests based on the estimated distance, and Section 6 introduces a Model Violations Index, which offers a quicker and more convenient assessment. Sections 7, 8, and 9 detail the data preparation, model architecture, and training process, respectively. The results are presented in Section 10. Example dataset applications are discussed in Section 11. Finally, we conclude with a discussion of our findings and propose ideas for future research directions. Figure 1: An example residual vs fitted values plot (red line indicates 0 corresponds to the x-intercept, i.e. y=0𝑦0y=0italic_y = 0). The vertical spread of the data points varies with the fitted values. This often indicates the existence of heteroskedasticity, however, here the result is due to skewed distribution of the predictors rather than heteroskedasticity. The Breusch-Pagan test rejects this residual plot at 95% significance level (p⁢-value=0.046𝑝-value0.046p\text{-value}=0.046italic_p -value = 0.046)."
https://arxiv.org/html/2411.00956v1,AI-EDI-SPACE: A Co-designed Dataset for Evaluating the Quality of Public Spaces,"Advancements in AI heavily rely on large-scale datasets meticulously curated and annotated for training. However, concerns persist regarding the transparency and context of data collection methodologies, especially when sourced through crowdsourcing platforms. Crowdsourcing often employs low-wage workers with poor working conditions and lacks consideration for the representativeness of annotators, leading to algorithms that fail to represent diverse views and perpetuate biases against certain groups. To address these limitations, we propose a methodology involving a co-design model that actively engages stakeholders at key stages, integrating principles of Equity, Diversity, and Inclusion (EDI) to ensure diverse viewpoints. We apply this methodology to develop a dataset and AI model for evaluating public space quality using street view images, demonstrating its effectiveness in capturing diverse perspectives and fostering higher-quality data.","Current advancements in AI heavily rely on the availability of large-scale datasets meticulously curated and annotated for training purposes. The significance of such datasets has been underscored by the success of models like ChatGPT, which leverages Reinforcement Learning with Human Feedback (RLHF) to fine-tune models based on human input [1]. However, concerns persist regarding the transparency and context of data collection methodologies, particularly in instances where annotations are sourced through crowdsourcing platforms. For instance, reports indicate that annotations for training ChatGPT were gathered from workers in Kenya under conditions of low pay and poor labor standards [15]. This reliance on crowdsourcing, often driven by cost-effectiveness, perpetuates the invisibility and exploitation of workers, particularly those from the global south [10]. Moreover, the failure to acknowledge the socio-cultural context within which data is produced can introduce biases into datasets. For example, algorithms trained on datasets devoid of the historical context of segregation may inadvertently perpetuate biases against certain minority groups [12]. Furthermore, the identities of workers involved in annotations are frequently overlooked, leading to a lack of diversity in viewpoints captured within datasets. This bias is compounded by the common practice of aggregating annotations through majority voting [5]. To address these limitations, we propose a methodology grounded in a specific socio-cultural context for dataset collection and AI model development. Our approach centers on a co-design model that actively involves stakeholders at key stages of the AI model development, including dataset creation. Additionally, we integrate principles of Equity, Diversity, and Inclusion (EDI) to ensure diverse viewpoints are represented within the dataset. We argue that this approach not only mitigates biases within datasets but also fosters the creation of higher-quality data reflecting diverse perspectives. We apply this methodology to the development of a dataset and AI model capable of evaluating the quality of public spaces using street view images. Assessing public space quality is inherently subjective, as demonstrated by research showing variations across cultural groups [13, 9]. Leveraging our proposed methodology grounded in co-design and EDI principles, we curated a dataset of streetview images annotated by a diverse group of citizens. Using this dataset, we trained a baseline AI model to score public space images along various dimensions. Finally, we propose several fairness metrics to assess the model’s ability to capture diverse viewpoints within the population. Figure 1: Methodology used to create the dataset and the AI model to evaluate the quality of public spaces."
https://arxiv.org/html/2411.00922v1,Lung tumor segmentation in MRI mice scans using 3D nnU-Net with minimum annotations,"In drug discovery, accurate lung tumor segmentation is an important step for assessing tumor size and its progression using in-vivo imaging such as MRI. While deep learning models have been developed to automate this process, the focus has predominantly been on human subjects, neglecting the pivotal role of animal models in pre-clinical drug development. In this work, we focus on optimizing lung tumor segmentation in mice. First, we demonstrate that the nnU-Net model outperforms the U-Net, U-Net3+, and DeepMeta models. Most importantly, we achieve better results with nnU-Net 3D models than 2D models, indicating the importance of spatial context for segmentation tasks in MRI mice scans. This study demonstrates the importance of 3D input over 2D input images for lung tumor segmentation in MRI scans. Finally, we outperform the prior state-of-the-art approach that involves the combined segmentation of lungs and tumors within the lungs. Our work achieves comparable results using only lung tumor annotations requiring fewer annotations, saving time and annotation efforts. This work111https://anonymous.4open.science/r/lung-tumour-mice-mri-64BB is an important step in automating pre-clinical animal studies to quantify the efficacy of experimental drugs, particularly in assessing tumor changes.","Lung cancer is the leading cause of global cancer incidence and mortality, and it is characterized by the growth of abnormal cells in the tissues of lungs [30, 4, 15]. The detection of those abnormal cells has been revolutionized by deep learning which accelerated and facilitated the process of detecting, classifying, and annotating tumors, often with human-like accuracy [11]. Although advanced models like the segment anything model (SAM), transformers, or stable diffusion are being designed and applied at the intersection of oncology and medical imaging for cancer patients [19, 26, 8, 34], increasingly greater focus in research is also being put on animals. Animal models like mice or rats are essential in the pre-clinical pipeline of drug discovery and development due to their biological similarities to human genetic and physiological characteristics. It provides a critical platform for metabolic, safety, and efficacy studies [27], allowing one to develop a full understanding of a drug and disease [25] and translate it onto humans [22]. Despite inherent limitations and ethical considerations, the use of animal models is a regulatory requirement, and corroborating drugs selected for clinical trials have a solid basis for pre-clinical validation. The conventional methods for quantifying tumors in the pre-clinical domain are laborious, time-intensive, and prone to observer variability. Magnetic resonance imaging (MRI) is a type of biological imaging method, a non-invasive method to render images of living tissues. Although the segmentation of tumors within small animals’ organs is on the rise, majority of research is focusing on brain tumor segmentation with the use of MRI datasets [9, 23, 1]. While MRI scans have also been used for cardiac strain segmentation in rats [7, 29], the lung tumor segmentation in mice focuses only on utilizing CT scans [6, 21, 3] which involve potentially harmful X-ray radiation [16]. Although CT scans are the usual imaging technique for lung tumor detection, tumors can be also detected with the help of MRI which is non-invasive and can be safely applied to children or pregnant women regularly, allowing for tracking of the tumor growth over time [17]. Furthermore, such 2D and 3D MRI segmentation of lung tumor was successfully achieved in human patients [32], highlighting a potential gap in in-vivo animal image segmentation. One potential reason for lack of research in the field of pre-clinical lung tumor segmentation using MRI scans could be the lack of data. To date, there is only one publicly available MRI dataset222https://zenodo.org/records/7014776 with lungs and tumors annotated, acquired by [17] who introduced a method DeepMeta which outperformed U-Net on the task of lung and tumor segmentation. Although with the use of this model, the authors were capable of recognizing growth pattern of tumors, the segmentation was conducted for both lungs and tumors, indicating that tumor segmentation could rely heavily on contextual information coming from background organs. While such a model is somewhat viable from clinical and anatomical perspectives, it would be beneficial to also have a sole-tumor segmentation model as often clinicians might not be interested in lung segmentation. However, such sole-tumor segmentation would be much more challenging to achieve due to the small sizes of tumors and the contrast being much less clear. Furthermore, even though the provided MRI dataset contained 3D scans, the DeepMeta study only focused on 2D models, neglecting potentially useful spatial context. This is likely due to GPU constraints as training a model on 3D images can be computationally expensive [31]. Although tumors can be successfully segmented using 2D images, there is evidence that 3D segmentation improves the overall performance for isotropic datasets [12]. Therefore, although 2D models can be computationally cheaper, it would be highly desirable to have an architecture that is capable of effectively segmenting lung tumors in both 2D and 3D settings. While 3D models can leverage the spatial advantages of 3D imaging, they are computationally more expensive. In contrast, 2D models offer greater flexibility in terms of resource and time availability but lack the spatial context. One such architecture could be ""no-new-Net"" (nnU-Net), a deep learning based segmentation method that can automatically configure the network architecture, training environment, and pre-processing stage [13]. The framework configures itself automatically based on the data fingerprint and set of defined rules which aims for best performance and computationally efficient results. Even though the framework is based on the U-Net architecture, with a few modifications, it outperformed state-of-the-art approaches on numerous occasions, highlighting its simplicity and importance of data-oriented approach [14]. Due to its automatic pre-processing strategy, it enables for 2D and 3D segmentation, allowing to examine the importance of spatial context in in-vivo imaging [20]. In this work, we aim to address the gap in the field of in-vivo medical imaging and examine nnU-Net performance on the task of tumor segmentation in 2D and 3D MRI scans of mice. We reproduced the results of the U-Net, U-Net3+, and DeepMeta models, benchmarked them against the nnU-Net framework, and then extended the work to the new task of sole-tumor segmentation using both 2D and 3D images. The main contributions of this paper are as follows: • We demonstrated that 3D models outperformed 2D models when we evaluated their performance using nnU-Net. The results indicate the importance of 3D spatial context for 3D lung tumor segmentation tasks in MRI mice scans. • We trained the nnU-Net model that outperformed the previous state-of-the-art models for tumor segmentation in MRI mice scans including U-Net, U-Net3+ and DeepMeta models. • We trained segmentation models using only tumor annotations whereas the previous study used both lungs and tumor annotations [17]. Our models performed on par with the previous ones but required less human annotations."
https://arxiv.org/html/2411.00919v1,Benchmark of Deep Learning-based Imaging PPG in Automotive Domain,"Imaging photoplethysmography (iPPG) can be used for heart rate monitoring during driving, which is expected to reduce traffic accidents by continuously assessing drivers’ physical condition. Deep learning-based iPPG methods using near-infrared (NIR) cameras have recently gained attention as a promising approach. To help understand the challenges in applying iPPG in automotive, we provide a benchmark of a NIR-based method using a deep learning model by evaluating its performance on MR-NIRP Car dataset. Experiment results show that the average mean absolute error (MAE) is 7.5 bpm and 16.6 bpm under drivers’ heads keeping still or having small motion, respectively. These findings suggest that while the method shows promise, further improvements are needed to make it reliable for real-world driving conditions.","In future vision of mobility safety [1], continuous monitoring of drivers’ vital signs like heart rate is essential. To achieve this goal, Imaging photoplethysmography (iPPG) is applied. iPPG estimates drivers’ pulse signal in a non-contact manner by detecting subtle color changes in facial skin, captured by a camera installed in the vehicle. However, varying lighting conditions while driving significantly affect the results of RGB three-channel cameras largely. In contrast, a NIR camera with a specific optical density bandpass filter is more robust to light variation during driving[2], offering a promising alternative. Recent studies[3][4][5] have utilized NIR cameras in combination with using deep learning model to explore accurate drivers’ heart rate estimation methods. To help understand the advances and challenges in drivers’ heart rate estimation, we provide a benchmark of a representative deep learning-based approach[3] in this report on the MR-NIRP Car dataset[2]."
https://arxiv.org/html/2411.00911v1,Zero-Shot Self-Consistency Learning for Seismic Irregular Spatial Sampling Reconstruction,"Seismic exploration is currently the most important method for understanding subsurface structures. However, due to surface conditions, seismic receivers may not be uniformly distributed along the measurement line, making the entire exploration work difficult to carry out. Previous deep learning methods for reconstructing seismic data often relied on additional datasets for training. While some existing methods do not require extra data, they lack constraints on the reconstruction data, leading to unstable reconstruction performance. In this paper, we proposed a zero-shot self-consistency learning strategy and employed an extremely lightweight network for seismic data reconstruction. Our method does not require additional datasets and utilizes the correlations among different parts of the data to design a self-consistency learning loss function, driving a network with only 90,609 learnable parameters. We applied this method to experiments on the USGS National Petroleum Reserve–Alaska public dataset and the results indicate that our proposed approach achieved good reconstruction results. Additionally, our method also demonstrates a certain degree of noise suppression, which is highly beneficial for large and complex seismic exploration tasks.","Over the past few decades, seismic exploration has been one of the most effective means of understanding subsurface structures, achieving significant success in fields such as oil and gas development Malehmir et al. (2012), regional geological studies Shen et al. (2024), and crustal research Tuve et al. (1954). Seismic exploration mainly analyzes the reflected or refracted waves from underground, and their reception requires the arrangement of receivers along the survey line on the surface. However, the conditions on the surface are always very complex, and the undulations of the terrain and the distribution of surface objects can pose significant challenges to the layout of the entire observation system. By using some proposed methods, we can eliminate the impact of terrain variations on observation data; these methods are typically referred to as static correction Cox (1999). Besides, there is usually another situation: due to various surface conditions, such as obstacles or areas with restricted access, some area in the exploration region may not be able to accommodate the deployment of receivers, resulting in an irregular spatial arrangement of the observation system Chen et al. (2019). The seismic data observed by receivers with irregular spatial distribution is incomplete, which can severely hinder subsequent data processing and even the entire seismic exploration work. To address this issue of reconstructing seismic data from irregular spatial sampling, many researchers have proposed a variety of methods. Various methods based on sparse transformations are the most widely used approaches Candès et al. (2006). The assumption is that complete regular sampling of seismic data can be sparsely represented, while irregular spatial sampling affects this sparsity, resulting in many small amplitude coefficients in the sparse domain. By filtering out the small amplitude coefficients, reconstruction of the seismic data can be achieved. Typically, to achieve satisfactory results, the above steps need to be repeated multiple times. For such methods, they can be classified based on the type of sparse transformation basis functions, including Fourier transform Sacchi et al. (1998), curvelet transform Candes et al. (2006), wavelet transform Antonini et al. (1992), etc, and different choices exhibit varying sparse representation performance. In addition to using fixed basis functions, many researchers have proposed using learnable basis functions as an alternative, those methods commonly referred to as dictionary learning Kreutz-Delgado et al. (2003) Tošić and Frossard (2011). However, the greatest challenge of methods based on sparse transformations lies in the filtering of small amplitude coefficients, which typically relies on manually set parameters. Achieving satisfactory results often requires multiple iterations to determine the relevant parameters. Besides the aforementioned methods, methods based on rank-reduction have also received significant attention from researchers Trickett et al. (2010) Abbad et al. (2011) Huang et al. (2016). These methods assume that seismic data possesses a low-rank structure, and irregular spatial sampling disrupts this low-rank structure, leading to an increase in the rank of the data. Singular value decomposition (SVD) is the most commonly used rank-reduction operator for extracting the singular matrix of seismic data, followed by imposing constraints on the singular values. These methods also face challenges related to parameter selection. In recent years, deep learning LeCun et al. (2015) has been widely applied in various fields and such applications include image processing Tian et al. (2020), speech processing Wang and Chen (2018), and text translation, where deep learning has significantly improved performance and efficiency. Deep learning methods have also been widely applied in seismic data processing, including tasks such as noise attenuation Saad and Chen (2020) Chen and Sacchi (2015) Peng et al. (2024), fault identification Wu et al. (2019) Hu et al. (2020), and seismic reconstruction Ovcharenko and Hou (2020) Liu et al. (2022) Liu et al. (2021). In a strict sense, most work using deep learning for seismic data reconstruction is not significantly different from seismic data noise processing. It treats the missing areas of data as targets for regression, effectively transforming the entire task into a regression problem. However, inspired by some works in the field of image generation Goodfellow et al. (2020) Ho et al. (2020), many researches have begun to introduce deep learning generative models into the seismic reconstruction domain Siahkoohi et al. (2018) Oliveira et al. (2018a) Deng et al. (2024). Essentially, various traditional computational methods reconstruct seismic data by examining the internal relationships between observed data and the spatial relationships between observed and target data while the methods based on generative models treat the observed irregular spatially sampled data as constraints for generating seismic data in the missing areas Oliveira et al. (2018b) Chang et al. (2020). One important reason for the success of deep learning is the use of additional datasets for training; the large training sets provide effective constraints for the model’s parameters. However, due to the lack of interpretability, the blind use of additional data, even if it yields seemingly good results, makes it difficult to trust the outcomes. At the same time, the generalization and performance of various deep learning methods largely depend on the dataset, which greatly limits the application of deep learning approaches in the reconstruction of irregularly sampled seismic data. Therefore, we believe that for the reconstruction of irregularly sampled seismic data, it is necessary to return to the intrinsic relationships within the data itself. To solve this, we proposed a new deep learning paradigm aimed at exploring these intrinsic relationships through simple deep learning model, called zero-shot self-consistancy learning. The relationships within the data itself, which we refer to as self-consistency, are often difficult to measure directly. Besides, this self-consistency is independent of other data, so our proposed deep learning method does not require additional data for training, which is often referred to as zero-shot learning Pourpanah et al. (2022). Zero-shot learning does not require a large amount of additional data to construct datasets, nor does it necessitate modeling numerous data features. As a result, the number of parameters can be significantly reduced compared to traditional deep learning methods Mansour and Heckel (2023). We applied the proposed method to the USGS National Petroleum Reserve–Alaska Bird and Houseknecht (2002) Miller et al. (2000) seismic data to evaluate its performance. In the NPRA, we used seismic data spanning approximately 2015 km, with a total recording time of about 91.6 hours. The results indicate that our proposed method can efficiently and effectively reconstruct complex and large seismic datasets, and we further observed its resistance to noise interference. This has very positive implications for large-scale seismic exploration projects, effectively addressing the data collection challenges posed by terrain limitations."
https://arxiv.org/html/2411.00900v1,Intensity Field Decomposition forTissue-Guided Neural Tomography,"Cone-beam computed tomography (CBCT) typically requires hundreds of X-ray projections, which raises concerns about radiation exposure. While sparse-view reconstruction reduces the exposure by using fewer projections, it struggles to achieve satisfactory image quality. To address this challenge, this article introduces a novel sparse-view CBCT reconstruction method, which empowers the neural field with human tissue regularization. Our approach, termed tissue-guided neural tomography (TNT), is motivated by the distinct intensity differences between bone and soft tissue in CBCT. Intuitively, separating these components may aid the learning process of the neural field. More precisely, TNT comprises a heterogeneous quadruple network and the corresponding training strategy. The network represents the intensity field as a combination of soft and hard tissue components, along with their respective textures. We train the network with guidance from estimated tissue projections, enabling efficient learning of the desired patterns for the network heads. Extensive experiments demonstrate the proposed method significantly improves the sparse-view CBCT reconstruction with a limited number of projections ranging from 10 to 60. Our method achieves comparable reconstruction quality with fewer projections and faster convergence compared to state-of-the-art neural rendering based methods.","Cone-beam computed-tomography (CBCT) can provide high-resolution volumetric images of hard tissue and becomes one of the most widely used imaging modalities in dentistry and oral maxillofacial surgery [1]. Despite numerous benefits, the techniques used by commercial CBCT machines often require hundreds of X-ray projections, which thus raises concerns about the associated radiation dose and its potential adverse effects [2]. To mitigate this issue, reducing the number of projections for CBCT reconstruction becomes a longstanding topic. Classical analytical or iterative computed-tomography (CT) reconstruction approaches, such as Feldkamp-Davis-Kress (FDK) algorithm [3] and Simultaneous Algebraic Reconstruction Technique (SART) [4] have been widely used in medical imaging due to their effectiveness in reconstruction with sufficient projections. However, these methods often struggle from sparse-view conditions. This inadequacy arises due to the ill-posed nature of sparse-view reconstruction, wherein there is insufficient measurement data to provide comprehensive information about the imaged objects. Existing works incorporate prior knowledge to deal with this issue, e.g. adding regularization terms into the optimization process or applying denoising techniques to the reconstructed image [5, 6]. With the rapid development of deep learning in computer vision, deep models have garnered considerable attention and extensive investigation into the medical image reconstruction problem [7, 8, 9, 10]. Nevertheless, achieving end-to-end CBCT reconstruction devoid of physical constraints remains a challenging task for deep neural networks, which requires substantial volumes of training data and computational resources [9]. Consequently, a majority of these endeavors continue to adhere to the traditional analytical or iterative reconstruction paradigm. A noteworthy advancement in 3D reconstruction involves neural implicit scene representation and neural rendering [11]. Applying neural rendering based reconstruction to CT scans is undoubtedly a highly promising avenue since the neural field can naturally incorporate physical constraints into the deep learning reconstruction framework. We have witnessed some pioneer works on CBCT reconstruction with neural rendering [12, 13] and some attempts on sparse view conditions [14, 15, 16, 17, 18]. However, many assumptions on improving sparse-view rendering quality for natural scenes do not hold for CT images, particularly in the case of geometric prior. For instance, the depth continuity [19, 20, 21] and visibility constraint [22] which are widely used as regularization that can introduce substantial improvement, do not exist in CT images due to the penetrative nature of X-ray. Similarly, the patch similarity [23] does not necessarily apply because of parallax, especially in maxillofacial regions where complex anatomical structures present. Moreover, despite the advancements, existing works on neural rendering for sparse-view CBCT mainly build upon techniques for natural scenes or are designed for generic volumetric images where the intensity field is typically represented by a single neural network output. However, we contend that capturing the diverse features of CBCT in an end-to-end manner poses a formidable challenge for a single neural network. When the number of projections is sufficient, this limitation can be mitigated by leveraging the information with complex neural network architectures. However, under sparse-view scenarios, relying on a single network with large capacity becomes a drawback by overfitting to the limited observations. In this article, we aim to improve the sparse-view CBCT reconstruction with neural fields by addressing the aforementioned challenges. Our method is based on the following observations: 1. The soft and hard tissue differ both physiologically and radiologically, this dissimilarity is evident in their nature; Moreover, CBCT has a low radiometric resolution [24], leading to less variation within the two tissue components. Therefore, we assume that the soft and hard tissue in CBCT exhibit distinct distributions and should not be treated as a homogeneous entity. 2. We observed that a single X-ray image is a routinely used diagnostic tool in orthodontics, where a cephalometry is used to evaluate the shape of the skull [25], which indicates that the bony structures can be inferred from a single X-ray projection. We hypothesize that tissue shape can be predicted from projections and used as a geometric prior for the sparse-view reconstruction. This is an analogy to the depth regularization in the 3D reconstruction of natural scenes, the accurate estimation of which greatly assists sparse-view rendering [26, 27]. Among the related researches for natural scenes, it is widely applied to decouple the shape and appearance, so as to offer better control of an object [28, 29, 30], or disassemble to enable efficient inference [31, 32]. In addition, the network capacity may be limited in certain branches to regularize the selected feature [33]. Likewise, to leverage the aforementioned insights, we propose using a combination of neural fields, each specialized in modeling decoupled features of the soft and hard tissue, while regularizing the tissue shape and texture to impose prior to the reconstruction process. Specifically, we present a straightforward and intuitive strategy to segregate the volume density into soft and hard tissue shape and texture components. The proposed shapes are essentially the segmentation masks of the tissue, which are learned with the help of the predicted tissue projections (Fig. 1). The textures are automatically adjusted to be smooth by the limited capacity of the neural network to provide limited variations among each tissue component. Our approach achieves satisfying reconstruction quality of oral-maxillofacial region in varying degrees of sparsity from 10 to 60 projections, within a relatively low computational complexity. The main contributions of this work are two-fold: First, we propose a novel and meaningful intensity field decomposition that has direct clinical implications. The decomposition simplifies the learning process by modeling the diverse set of features present in CBCT with their specialized network while increasing the explainability and controllability of the neural field. Second, we leverage the tissue information from X-ray projection and incorporate tissue-specific guidance into the training process, which significantly accelerates convergence speed and enhances reconstruction quality. Figure 1: Overview of the proposed method. Instead of directly regress a neural field to the projections, we introduce a novel approach by disentangling the field into 4 semantically meaningful components, which are then supervised with tissue projections that have clear clinical significance."
https://arxiv.org/html/2411.00894v1,Multiscale texture separation,"In this paper, we investigate theoretically the behavior of Meyer’s image cartoon + texture decomposition model. Our main results is a new theorem which shows that, by combining the decomposition model and a well chosen Littlewood-Paley filter, it is possible to extract almost perfectly a certain class of textures. This theorem leads us to the construction of a parameterless multiscale texture separation algorithm. Finally, we propose to extend this algorithm into a directional multiscale texture separation algorithm by designing a directional Littlewood-Paley filter bank. Several experiments show the efficiency of the proposed method both on synthetic and real images.","In the last decade, based on the work of Meyer [14], many papers were published on cartoon + textures decomposition models for images. Some of them address numerical issues [1, 2, 16], others the modeling aspects [3, 5, 9, 13, 15], a few focus on applications [2, 4, 6, 7, 8, 10]. Finally a very small number of publications tries to characterize the solutions of such models with respect to the choice of the parameters [5, 10]. In [21], Tadmor et al. proposed a multiscale cartoon representation of an image. They iterate the Rudin-Osher-Fatemi model to consecutively extract objects that belong to different scales. This work does not deal with textures at different scales but it is interesting as it provides the relationship between the notion of scale and the choice of the regularizing parameter of the algorithm. Initially, the idea of decomposing an image is to separate different kind of information: objects and textures. Indeed, in term of analysis (like, for example a segmentation task) it is useful to have separated specific information. For example, a classical way to analyze textures is to use some wavelet type filtering and then use the obtained coefficients to build a feature vector which can be provided to some classifier. While the idea of extracting such texture feature vectors directly from the texture part of the decomposition seems natural, no publication really addresses the construction of a well-defined texture separation algorithm based on decomposition models. In this paper, we investigate the possibility to optimally combine image decomposition and a well chosen filtering to extract specific textures in an image. Based on this result, we propose a multiscale texture separation algorithm. To do this program, we propose a general formulation to decompose an image f∈L2⁢(ℝ2)𝑓superscript𝐿2superscriptℝ2f\in L^{2}({\mathbb{R}^{2}})italic_f ∈ italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) into three parts u,v𝑢𝑣u,vitalic_u , italic_v and w𝑤witalic_w. The first one represents the objects contained in f𝑓fitalic_f, the second part is a residual term while the last one models the highest oscillating parts in the image. In our work, we model u,v𝑢𝑣u,vitalic_u , italic_v and w𝑤witalic_w by three different functional spaces, B⁢V,L2𝐵𝑉superscript𝐿2BV,L^{2}italic_B italic_V , italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT and the space G𝐺Gitalic_G which is, in a sense defined below, the dual of B⁢V𝐵𝑉BVitalic_B italic_V. Moreover, we use two complementary parameters λ>0𝜆0\lambda>0italic_λ > 0 and μ>0𝜇0\mu>0italic_μ > 0 to control the behavior of the algorithm. Finally, the optimal decomposition must minimize J⁢(u,v,w)=‖u‖B⁢V+λ⁢‖v‖L22+μ⁢‖w‖G𝐽𝑢𝑣𝑤subscriptnorm𝑢𝐵𝑉𝜆superscriptsubscriptnorm𝑣superscript𝐿22𝜇subscriptnorm𝑤𝐺J(u,v,w)=\|u\|_{BV}+\lambda\|v\|_{L^{2}}^{2}+\mu\|w\|_{G}italic_J ( italic_u , italic_v , italic_w ) = ∥ italic_u ∥ start_POSTSUBSCRIPT italic_B italic_V end_POSTSUBSCRIPT + italic_λ ∥ italic_v ∥ start_POSTSUBSCRIPT italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_μ ∥ italic_w ∥ start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT (1) over all possible decompositions f=u+v+w𝑓𝑢𝑣𝑤f=u+v+witalic_f = italic_u + italic_v + italic_w of f𝑓fitalic_f. We will see that this program works only if the parameters λ>0𝜆0\lambda>0italic_λ > 0 and μ>0𝜇0\mu>0italic_μ > 0 are, a posteriori, fixed accordingly to the processed image. If μ𝜇\muitalic_μ is too large, we necessarily have w=0𝑤0w=0italic_w = 0 and the decomposition algorithm is equivalent to the well known Rudin-Osher-Fatemi (ROF) [17] algorithm. If μ𝜇\muitalic_μ is too small (0<μ<4⁢π0𝜇4𝜋0<\mu<4\pi0 < italic_μ < 4 italic_π) then we have u=0𝑢0u=0italic_u = 0 and the algorithm degenerates. The remainder of the paper is as follows. In section 2 we recall the definition of the used function spaces and some of their properties. Section 3 gives a detailed presentation of the decomposition model and recall some of its properties which will be useful to prove our main result. In section 4, we prove a theorem which states that we can retrieve, almost perfectly, some specific textures from the texture part and a well chosen Littlewood-Paley filter. Section 5 provides a more precise result in the case of noisy images. A multiscale texture separation algorithm is proposed in section 6 and is extended to a directional multiscale texture separation algorithm in section 7. We conclude this work in section 8."
https://arxiv.org/html/2411.00888v1,"Topology-Aware Graph Augmentation for
Predicting Clinical Trajectories in
Neurocognitive Disorders","Brain networks/graphs derived from resting-state functional MRI (fMRI) help study underlying pathophysiology of neurocognitive disorders by measuring neuronal activities in the brain. Some studies utilize learning-based methods for brain network analysis, but typically suffer from low model generalizability caused by scarce labeled fMRI data. As a notable self-supervised strategy, graph contrastive learning helps leverage auxiliary unlabeled data. But existing methods generally arbitrarily perturb graph nodes/edges to generate augmented graphs, without considering essential topology information of brain networks. To this end, we propose a topology-aware graph augmentation (TGA) framework, comprising a pretext model to train a generalizable encoder on large-scale unlabeled fMRI cohorts and a task-specific model to perform downstream tasks on a small target dataset. In the pretext model, we design two novel topology-aware graph augmentation strategies: (1) hub-preserving node dropping that prioritizes preserving brain hub regions according to node importance, and (2) weight-dependent edge removing that focuses on keeping important functional connectivities based on edge weights. Experiments on 1,68816881,6881 , 688 fMRI scans suggest that TGA outperforms several state-of-the-art methods.","1 INTRODUCTION Brain networks/graphs derived from resting-state functional MRI (fMRI) help objectively study underlying pathophysiology of neurocognitive disorders by measuring brain neuronal activities. Some fMRI-based studies utilize learning-based approaches for brain network analysis and imaging biomarker identification. But existing methods often suffer from poor model generalizability due to limited labeled data [1]. As a promising self-supervised learning strategy, graph contrastive learning provides an effective solution to pretrain a generalizable encoder on auxiliary data without requiring task-specific labels [2]. In general, it first performs graph data augmentation for each input graph to generate two views, and then optimizes the agreement of graph representations from these two views to achieve self-supervised model training [3]. However, existing studies typically randomly perturb graph nodes or edges to generate augmented graphs for contrastive learning [4], ignoring the crucial topological information conveyed in functional brain networks. To this end, we propose a topology-aware graph augmentation (TGA) framework for clinical trajectory prediction of HIV-associated neurocognitive disorders (HAND) with resting-state fMRI. As shown in Fig. 1, the TGA consists of a pretext model to train a generalizable graph convolutional network (GCN) encoder on large-scale unlabeled fMRI cohorts and a task-specific model to fine-tune the encoder for HIV-associated disorder identification and clinical score prediction on a target HAND dataset. We design two novel topology-aware graph augmentation strategies including (1) hub-preserving node dropping that prioritizes preserving brain hub regions according to node importance, and (2) weight-dependent edge removing that focuses on keeping important functional connectivities based on edge weights. Experiments on 1,68816881,6881 , 688 fMRI scans validate the superiority of our TGA in classification and regression tasks. The TGA incorporates a learnable attention mask to automatically detect HIV-related brain regions and functional connectivities, providing potential imaging biomarkers for early intervention. Fig. 1: Illustration of the proposed topology-aware graph augmentation (TGA) framework for functional MRI analysis."
https://arxiv.org/html/2411.00869v1,Federated Learning for Diabetic Retinopathy Diagnosis: Enhancing Accuracy and Generalizability in Under-Resourced Regions,"Diabetic retinopathy is the leading cause of vision loss in working-age adults worldwide, yet under-resourced regions lack ophthalmologists. Current state-of-the-art deep-learning systems struggle at these institutions due to limited generalizability. This paper explores a novel federated learning system for diabetic retinopathy diagnosis with the EfficientNetB0 architecture to leverage fundus data from multiple institutions to improve diagnostic generalizability at under-resourced hospitals while preserving patient-privacy. The federated model achieved 93.21% accuracy in five-category classification on an unseen dataset and 91.05% on lower-quality images from a simulated under-resourced institution. The model was deployed onto two apps for quick and accurate diagnosis.","Diabetes mellitus often leads to diabetic retinopathy (DR), a condition that damages retinal blood vessels and can cause severe vision loss or blindness. Currently, 103 million people worldwide are affected by DR, with this number expected to rise to 161 million by 2045. Projections indicate that DR prevalence in under-resourced regions like the Middle East and Africa could increase by 20.6% to 47.2% by 2030, outpacing growth in high-income areas like the United States [1]. I-A Motivations I-A1 Ophthalmologists Shortage Early detection of DR is crucial to prevent vision loss, yet many regions worldwide face a shortage of ophthalmologists required for diagnosis. In Sub-Saharan Africa countries, there is an average of approximately 2.5 ophthalmologists per a million population, compared with a ratio of 56.8 ophthalmologists per a million population in the United States [2, 3]. This limited access to ophthalmologists leads to delayed diagnosis and increased risk of blindness for people in these under-resourced regions. Thus, there is a need for alternative diagnosis methods. I-A2 Current Challenges of AI in DR Diagnosis While advances in AI have improved DR diagnosis, current AI-diagnostic systems struggle to be effective in real-world settings, due to over-fitting to a specific institution’s data and poor generalization across diverse populations. For instance, Google’s 2020 DR detection model, despite its 90% proposed accuracy, failed in real-world testing due to poor generalization and long screening times [4]. I-A3 Resource Constraints in DR Imaging In addition to having a lack of trained ophthalmologists, many under-resourced healthcare institutions are located in sparse, remote regions. The retinal images collected in under-resourced regions are also of lower quality due to a lack of adequate imaging devices. Deep learning algorithms require large datasets of high-quality images to perform reliably and accurately. The inadequacy in both the amount and quality of retinal images at medical institutions of under-resourced regions makes it difficult for these institutions to build accurate DR-diagnosis deep learning systems. Thus, it is crucial for medical institutions in under-resourced regions to have access to high-quality DR data to improve their diagnosis models. I-B Solutions One approach is to gather fundus data from various global medical institutions and train a centralized DR diagnosis model on this combined dataset (Fig. 1A). However, centralized data-sharing faces significant challenges due to patient privacy, intellectual property, and data ownership concerns. Regulations like Europe’s GDPR and the U.S.’s HIPAA [5, 6] restrict sharing patient data across institutions, making international implementation of a centralized data-sharing model difficult. Federated learning (FL) enables collaborative, decentralized training of models across multiple clients without transferring data between them. Instead of gathering data from various locations, FL trains a shared model on a central server while keeping the data at its origin by learning from CNN architectures. This approach allows medical institutions to contribute to AI-driven systems for diagnosis, treatment, and disease monitoring without compromising patient privacy [7]. This approach can be valuable in regions like Sub-Saharan Africa, where quality data and trained ophthalmologists are scarce. In this work, we explore the use of federated learning for DR diagnosis and severity assessment, demonstrating with experiments how federated learning can also be a solution for under-resourced regions. Figure 1: Centralized data-sharing system (a) vs Federated learning-driven system (b)"
https://arxiv.org/html/2411.00860v1,Survey of Cultural Awareness in Language Models: Text and Beyond,"Large-scale deployment of large language models (LLMs) in various applications, such as chatbots and virtual assistants, requires LLMs to be culturally sensitive to the user to ensure inclusivity. Culture has been widely studied in psychology and anthropology, and there has been a recent surge in research on making LLMs more culturally inclusive in LLMs that goes beyond multilinguality and builds on findings from psychology and anthropology. In this paper, we survey efforts towards incorporating cultural awareness into text-based and multimodal LLMs. We start by defining cultural awareness in LLMs, taking the definitions of culture from anthropology and psychology as a point of departure. We then examine methodologies adopted for creating cross-cultural datasets, strategies for cultural inclusion in downstream tasks, and methodologies that have been used for benchmarking cultural awareness in LLMs. Further, we discuss the ethical implications of cultural alignment, the role of Human-Computer Interaction in driving cultural inclusion in LLMs, and the role of cultural alignment in driving social science research. We finally provide pointers to future research based on our findings about gaps in the literature.111We additionally organize the papers covered by this survey at https://github.com/siddheshih/culture-awareness-llms.git.","Language models are deployed in various user-facing applications, such as recommender systems (Bao et al., 2023), customer service (Pandya and Holia, 2023), and search applications (Xiong et al., 2024), which are increasingly used by people in all aspects of their life including education (Kasneci et al., 2023), public health (De Angelis et al., 2023), and professional writing (Jakesch et al., 2023a). These models reflect the Western perspective, predominantly trained on Western-centric data (Durmus et al., 2023). This skewed perspective can lead to stereotyping and alienation of users, propagation of stereotypes due to a lack of cultural understanding (e.g., flattening of cultural identities), or responding in a culturally insensitive way (Cao et al., 2022, 2023). Therefore, cultural awareness is one of the critical factors that should be considered while creating NLP models. In this work, we provide a comprehensive survey of the steps that the NLP community has taken to make language models more culturally inclusive. Furthermore, with advancements in multimodal foundation models and their adaption on NLP tasks (Fei et al., 2022), we also examine efforts towards cultural inclusion in multimodal NLP systems (i.e., multimodal systems with language understanding as one of their components). As the notion of culture used by the NLP community (to define and ensure cultural inclusion in NLP systems) is adopted from social science research, we start by defining ‘cultural awareness in LLMs’ based on definitions of culture in psychology and anthropology literature. We then consolidate the works that look into cultural inclusion in LLMs and multimodal models, including benchmark creation, training data creation, alignment methodologies, and evaluation methodologies. We also discuss the role of cultural alignment in accelerating social research. Human-computer interaction (HCI) also plays a role in ensuring cultural alignment in LLMs, as how studying different cultures reacts to certain levels of cultural (mis)alignment and matching varied expectations of people falls under the realm of HCI research (Weidinger et al., 2023). Finally, we discuss the ethical and safety implications of current research directions and provide potential research avenues that the community could take to foster cultural inclusion in language models. While recent surveys (Liu, Gurevych, and Korhonen, 2024; Adilazuarda et al., 2024) focus on the cultural alignment of LLMs in NLP and provide a taxonomy for grouping current cultural alignment works, we consolidate the literature from a broader scope. We survey and compare efforts towards incorporation and conceptualization of culture in NLP systems, and our survey spans several modalities, including images, videos, and audio, along with text. We position our survey at the intersection of NLP, multimodality, and social science. The key contributions and research goals of this survey are as follows: 1. We review 300+ papers to provide an overview of the current state of benchmarks and methods used for cultural inclusion in multimodal language models (we organize the papers in §4, §5, §6); 2. We provide an overview of common data sources used for creating cultural alignment datasets and how current benchmark creation and culturally relevant fine-tuning dataset creation methodologies leverage these common sources (§3); we also discuss ethical implications and limitations of the dataset creation methodologies (§8); 3. We provide an overview of the coverage of current datasets for geographical regions and cultures (§7) and discuss measures that the community could take to foster equity in cultural inclusion (§9); 4. We also examine the societal impact and implications of deploying LLMs with or without cultural awareness and discuss the role of Human-Computer Interaction (HCI) research in cultural alignment (§8). Literature Collection Strategy. As our paper focuses on multimodal and text-based NLP, we consider papers published in conferences including ACL and regional ACL chapters, EMNLP, ICLR, and ICML, computer vision conferences such as ICCV and CVPR as well as papers published in the ACL Anthology. The inclusion of cultural aspects in the NLP and CV community has been a recent one, with most (benchmark) papers published post-2016, so we consider cultural inclusion benchmarks post-2016. We also consider recent submissions to Arxiv to include recent NLP and social science papers, as the publication cycles for social science journals are typically 1–3 years. For alignment methodologies, we specifically focus on recent works published after 2022, following the release of ChatGPT OpenAI . We define culture in §2 and organize our paper into three major parts. The first part discusses data sources and methodologies the community has used to create datasets and benchmarks for the cultural inclusion of LLMs (§3). The second part discusses the methodologies and state of benchmarks that have been used or created for improving cultural awareness in LLMs across modalities (§4, §5, §6). Finally, we discuss our observations: the state of cultural inclusion (§7), ethical issues related to cultural alignment, and the role of cultural alignment in accelerating social science research (§8), and future research directions (§9) in the last part. In each of the subsections in §4, §5, and §6, we identify specific research gaps and, based on the research gaps, provide concrete suggestions for future research in §9."
https://arxiv.org/html/2411.00855v1,Vision-Language Models Can Self-Improve Reasoning via Reflection,"Chain-of-thought (CoT) has proven to improve the reasoning capability of large language models (LLMs). However, due to the complexity of multimodal scenarios and the difficulty in collecting high-quality CoT data, CoT reasoning in multimodal LLMs has been largely overlooked. To this end, we propose a simple yet effective self-training framework, R3V, which iteratively enhances the model’s Vision-language Reasoning by Reflecting on CoT Rationales. Our framework consists of two interleaved parts: (1) iteratively bootstrapping positive and negative solutions for reasoning datasets, and (2) reflection on rationale for learning from mistakes. Specifically, we introduce the self-refine and self-select losses, enabling the model to refine flawed rationale and derive the correct answer by comparing rationale candidates. Experiments on a wide range of vision-language tasks show that R3V consistently improves multimodal LLM reasoning, achieving a relative improvement of 23% to 60% over GPT-distilled baselines. Additionally, our approach supports self-reflection on generated solutions, further boosting performance through test-time computation. 111 Our code is available at https://github.com/njucckevin/MM-Self-Improve.","Humans often rely on intuitive Chain-of-Thought (CoT) to perform complex reasoning Ericsson and Simon (1980). Previous studies have shown that this CoT capacity also emerges in Large Language Models (LLMs) Wei et al. (2022). Through simple prompting or fine-tuning Cobbe et al. (2021); Kojima et al. (2022); Hsieh et al. (2023), CoT enhances the reasoning performance of LLMs while providing insights into their decision-making process. Recently, OpenAI o1 further advances reasoning by producing long internal CoT sequences, taking LLMs intelligence to a new level. Figure 1: Results of Qwen-VL on TabMWP, a visual mathematical reasoning dataset. Qwen-VL exhibits weak zero-shot CoT reasoning performance, while our R3V iteratively self-improves, surpassing the GPT-distilled baseline by a large margin. While CoT reasoning has significantly advanced LLMs in textual domains, extending CoT to multimodal settings remains an open problem. Unlike the abundant, unsupervised text-based CoT in pre-training corpora Kojima et al. (2022); Wei et al. (2022), multimodal CoT resources are scarce in the text-dominated internet collections Dai et al. (2023), hindering the full realization of Multimodal LLMs’ (MLLMs) reasoning potential. Recent studies show that open-sourced MLLMs struggle to integrate visual cues into their reasoning process, resulting in weak CoT performance Zhang et al. (2024a); Shi et al. (2024). Consistent with our observations in Figure 1, CoT prompting provides minimal gains over direct prediction Chen et al. (2024a) and falls far behind GPT-4o. One potential solution is to construct multimodal CoT annotations for post-training; however, manual annotation is prohibitively expensive and hard to scale. This raises our first research question: can MLLMs self-improve the reasoning capabilities through bootstrapping on CoT samples? Orthogonal to fine-tuning on curated CoT annotations, relying solely on positive samples can lead to suboptimal policy due to insufficient exploration of reasoning paths. Inspired by human thinking, another promising direction involves learning from trial-and-errors Yuan et al. (2024); Song et al. (2024), where mistakes are not failures but key opportunities to enhance reasoning. A few multimodal approaches use corrupted prompts to create negative samples for preference learning, aiming to improve image comprehension Wang et al. (2023); Deng et al. (2024). However, these methods fail to generate reasoning-aligned positive and negative CoT solutions, making them unsuitable for complex multimodal reasoning tasks. Thus, it remains unaddressed: how can MLLMs efficiently learn from mistakes to improve their reasoning skills? To address the above two questions, this paper proposes R3V, a self-training framework that enables the model to Reflect on bootstrapped CoT Rationales, thereby strengthening its Vision-Language Reasoning. Firstly, we leverage MLLM’s pre-existing but weak CoT ability to bootstrap both rationales and answers for a given question, enabling the collection of a large number of positive and negative solutions based on answer correctness. Secondly, we introduce a reflection mechanism on negative solutions to help the model learn from mistakes. Specifically, we design self-refine and self-select losses that guide the model to correct flawed rationales and derive the correct answer by comparing rationale candidates, respectively. The above synergistic process can be repeated, with improved samples boosting MLLM’s reasoning and the enhanced model further improving rationale generation. Additionally, through self-select training, our model can derive the superior solution from multiple samples, further boosting performance via test-time computation. We conduct experiments across a wide range of multimodal reasoning benchmarks, including charts, geometry, commonsense, science, mathematics, etc. R3V progressively enhances the reasoning ability of MLLMs, delivering a 23%-60% relative accuracy improvement compared to GPT distillation, and consistently outperforming the strong self-training baseline, STaR Zelikman et al. (2022). Moreover, our test-time selection is robust and effective, consistently surpassing Pass@1 and majority voting, even in OOD scenarios. Our main contributions are as follows: • We introduce an iterative self-training framework R3V that leverages CoT bootstrapped by MLLM itself for self-improvement. To our knowledge, this is the first attempt to apply self-training in vision-language reasoning. • We propose learning from mistakes through self-reflection, with support for test-time computation to further improve reasoning performance. • We perform extensive evaluations across 6 different multimodal domains to validate the effectiveness of R3V. Our analysis reveals the key factors driving the success of multimodal self-training."
https://arxiv.org/html/2411.00839v1,CausAdv: A Causal-based Framework for Detecting Adversarial Examples,"Deep learning has led to tremendous success in many real-world applications of computer vision, thanks to sophisticated architectures such as Convolutional neural networks (CNNs). However, CNNs have been shown to be vulnerable to crafted adversarial perturbations in inputs.These inputs appear almost indistinguishable from natural images, yet they are incorrectly classified by CNN architectures. This vulnerability of adversarial examples has led researchers to focus on enhancing the robustness of deep learning models in general, and CNNs in particular, by creating defense and detection methods to distinguish adversarials inputs from natural ones. In this paper, we address the adversarial robustness of CNNs through causal reasoning.We propose CausAdv: a causal framework for detecting adversarial examples based on counterfactual reasoning. CausAdv learns causal and non-causal features of every input, and quantifies the counterfactual information (CI) of every filter of the last convolutional layer. Then we perform statistical analysis on the filters’ CI of every sample, whether clan or adversarials, to demonstrate how adversarial examples indeed exhibit different CI distributions compared to clean samples. Our results show that causal reasoning enhances the process of adversarials detection without the need to train a separate detector. In addition, we illustrate the efficiency of causal explanations as a helpful detection technique through visualizing the causal features. The results can be reproduced using the code available in the repository: https://github.com/HichemDebbi/CausAdv","Deep neural networks (DNNs), which represent an important class of ML models have shown a great success in many applications such as image classification [1], natural language processing [2], and two-player games [3]. DNNs could achieve good levels of precision, but they have a black box nature. As a result, extending their application to other fields such as: autonomous driving and avionics [4] and healthcare [5], the safety and trustworthiness of DNNs have been raised [6]. Compared to other ML models such as decision trees and Bayesian networks, diagnosing the inference process in DNNs is more challenging, due to the large number of learnable parameters, which increases as much as we go deeper in the network. In addition, they have various components such as activation functions, connectivity patterns and properties of the learning procedure, such as regularization and employed cost functions. From what precedes, it is evident that the ability of human users to understand and trust the decisions of DNNs is a very challenging task. This blackbox nature of DNNs leads to discovering one of their most important vulnerabilities, which is the adversarial examples. [7] and [8] show that every DNN model is vulnerable to adversarial examples, which refer to malicious perturbations on the input image. Although these perturbations are imperceptible to humans, yet they succeed to fool the state-of-the-art models [9, 10, 11, 12]. It has been found that adversarial examples can be applied across different models [12, 13, 14] and be harmful even in the real world [15], which raises many safety concerns especially in autonomous driving [16] and healthcare [17]. The act of generating adversarial examples to threat the security of DNNs systems is called adversarial attacks. Adversarial attacks range in different categories. Regarding adversary’s knowledge, we have white and black box attacks. For white box attacks, the adversary has a full knowledge about the trained classification model including model architecture, hyperparameters, activation functions, and model weights. These attacks usually take the gradient of the predictions probabilities with respect to particular pixels. Some famous white box attacks are FGSM [18], PGD [19], BIM [20], and C&W [21]. Most adversarial attacks are white-box attacks, however, due to the transferability of adversarial examples[22], they can be transferred to blackbox attack. In contrast to white box attacks, black box attacks such as Zoo[23] do not need to have access to the internal architecture of the pretarined models, and have knowledge only of the model’s output, which reflects the real world scenario. Chen et al. [23] proposed Zeroth-Order Optimization (ZOO), a black-box attack that does not require gradients, and can be applied to both targeted and untargeted attacks. Adversarial training which represents a famous defense technique could remain actually vulnerable to black-box attacks. From the target label’s perspective, we have targeted and untargeted attacks. Consider an input image x𝑥xitalic_x that is fed into a classifier Mθ(.)M_{\theta}(.)italic_M start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( . ), where θ𝜃\thetaitalic_θ refers to the model’s parameters. An adversarial example against Mθ(.)M_{\theta}(.)italic_M start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( . ) is defined then as another image x^^𝑥\hat{x}over^ start_ARG italic_x end_ARG such that ‖x^−x‖norm^𝑥𝑥\parallel\hat{x}-x\parallel∥ over^ start_ARG italic_x end_ARG - italic_x ∥ is small, but the classifier prediction is no longer the same, i.e Mθ⁢(x^)≠Mθ⁢(x)subscript𝑀𝜃^𝑥subscript𝑀𝜃𝑥M_{\theta}(\hat{x})\neq M_{\theta}(x)italic_M start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( over^ start_ARG italic_x end_ARG ) ≠ italic_M start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x ). While untargeted attacks aim to generate any x^^𝑥\hat{x}over^ start_ARG italic_x end_ARG that fools the model, targeted attacks in contrast generate x^^𝑥\hat{x}over^ start_ARG italic_x end_ARG given a specific prediction label y^^𝑦\hat{y}over^ start_ARG italic_y end_ARG such that Mθ⁢(x^)=y^subscript𝑀𝜃^𝑥^𝑦M_{\theta}(\hat{x})=\hat{y}italic_M start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( over^ start_ARG italic_x end_ARG ) = over^ start_ARG italic_y end_ARG and y^≠Mθ⁢(x)^𝑦subscript𝑀𝜃𝑥\hat{y}\neq M_{\theta}(x)over^ start_ARG italic_y end_ARG ≠ italic_M start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x ). The small difference between the original image x𝑥xitalic_x and the adversarial one x^^𝑥\hat{x}over^ start_ARG italic_x end_ARG is subjected to a perceptibility threshold ϵitalic-ϵ\epsilonitalic_ϵ : ‖x^−x‖≤ϵnorm^𝑥𝑥italic-ϵ\parallel\hat{x}-x\parallel\leq\epsilon∥ over^ start_ARG italic_x end_ARG - italic_x ∥ ≤ italic_ϵ, where ϵ>0italic-ϵ0\epsilon>0italic_ϵ > 0. Here the perceptible difference ∥∥˙\parallel\dot{\parallel}∥ over˙ start_ARG ∥ end_ARG is usually obtained using the Euclidean norm ∥∥˙2\parallel\dot{\parallel}_{2}∥ over˙ start_ARG ∥ end_ARG start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT or the max-norm ∥∥˙∞\parallel\dot{\parallel}_{\infty}∥ over˙ start_ARG ∥ end_ARG start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT, which is reduced to an optimization problem. While l∞subscript𝑙l_{\infty}italic_l start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT measures the maximum change that can be made for all the pixels in the adversarial examples, l2subscript𝑙2l_{2}italic_l start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT norm measures the Euclidean distance between x𝑥xitalic_x and x^^𝑥\hat{x}over^ start_ARG italic_x end_ARG. Many defense methods have been proposed in order to make CNNs more secure and robust against adversarial attacks. According to Papernot et al. [22], the works on defending against adversarial examples can be grouped into tow main categories: adversarial training and robust network architectures. Adversarial training (AT-based) techniques rely on injecting adversarial generated data in the training phase, and it is treated as a minimax optimization problem. Recently many other variants of AT have been proposed [24, 25, 26]. [27] proposed the Misclassification-Aware-AdveRsarial-Training (MART) process that supports adversarial data for model training by simultaneously applying the misclassified natural data. As AT leads to robustneess of CNNs, in the same time it could decrease the natural accuracy, in this regard, [28] proposed TRADES to achieve a better trade-off of adversarial robustness and accuracy. Moreover, different regularization techniques such as layer-wise feature matching [29, 30] and Lipschitz regularity ([31]) have been proposed to support AT. Although adversarial training represents the current state-of-the-art robustness method against white-box attacks [32], it could remain actually vulnerable to black-box attacks. Although AT has been shown to be the most effective strategy, from the activation perspective, the behavior of DNNs is still not understood how given the small perturbations accumulating across intermediate layers fool the network, and as a result, how adversarial training can deal with these activations. In this regard, many papers tried to tackle this aspect by proposing robust architectures designs [33, 34, 35], which represent the second category of defense methods. We have also detection-based methods, which are about testing whether an input image meets some criteria, which are satisfied by natural images in general, even those which are incorrectly classified, and they are not satisfied by adversarially perturbed images. The general assumption is that input images that are subjected to perturbations force the network architectures to behave slightly, at least to natural images. These works tackle this issue from many perspectives such as statistics[36, 37, 38, 39], detector training [40, 41] and prediction inconsistency[42, 43] where the two later can be combined. Detector training follows similar approach to adversarial training, where adversarial examples are used to train the detector. Based on the assumption that causal reasoning could help to bring robustness, and thus it could cope with adversarial attacks, many works tried recently to investigate this direction[44, 45]. This paper proposes CausAdv: a causal-based framework for detecting adversarial examples. Our approach is based on two main steps, in the first step we perform a causal learning process to identify causal robust features, and then, in the second step, we perform statistical analysis based on the outcome of the first step. In CNNs, filters are the main elements of the network that contribute to a decision, and thus, in our setting, filters are considered as actual causes. Based on the causality abstraction principle [46], we consider only the filters of the last convolutional layer. We perform a causal learning process for each prediction, in which, filters can be either causal, or non causal for such a classification. For each filter, we assign contribution score in term of the difference in prediction probability when this filter is removed. Such a value would help to measure the importance of this filter with respect to the predicted class. We denote these values by Counterfactual information (CI), and they represent the output result of the first step. In the second step we perform many detection strategies based on statistical analysis of the filters’ CI in order to distinguish clean samples from adversarial ones. Finally, we show how visualizing the causal features as possible explanations would serve as well as a helpful detection technique. The main contributions of this paper are: • We propose CausAdv, a detection framework based on causality that is able to detect adversarial examples. • CausAdv employs statistical analysis on the filters’ counterfactual information (CI) of each sample in order to distinguish adversarial examples from natural ones. • CausAdv can be performed on the top of any CNN architecture. • In contrast to existing defense methods that add noise, CausAdv does not need any modification of the input image. In addition, no modification of the architecture is needed, and no training is needed in contrast to adversarial training approaches. • CausAdv was able to achieve 100% detection with BIM attack by only basic analysis of the CI. • CausAdv has a powerful explanation and interpretation ability that helps to distinguish natural samples from adversarial ones."
https://arxiv.org/html/2411.00830v1,Unsupervised Training of a Dynamic Context-Aware Deep Denoising Framework for Low-Dose Fluoroscopic Imaging,"Fluoroscopy is critical for real-time X-ray visualization in medical imaging. However, low-dose images are compromised by noise, potentially affecting diagnostic accuracy. Noise reduction is crucial for maintaining image quality, especially given such challenges as motion artifacts and the limited availability of clean data in medical imaging. To address these issues, we propose an unsupervised training framework for dynamic context-aware denoising of fluoroscopy image sequences. First, we train the multi-scale recurrent attention U-Net (MSR2AU-Net) without requiring clean data to address the initial noise. Second, we incorporate a knowledge distillation-based uncorrelated noise suppression module and a recursive filtering-based correlated noise suppression module enhanced with motion compensation to further improve motion compensation and achieve superior denoising performance. Finally, we introduce a novel approach by combining these modules with a pixel-wise dynamic object motion cross-fusion matrix, designed to adapt to motion, and an edge-preserving loss for precise detail retention. To validate the proposed method, we conducted extensive numerical experiments on medical image datasets, including 3500 fluoroscopy images from dynamic phantoms (2,400 images for training, 1,100 for testing) and 350 clinical images from a spinal surgery patient. Moreover, we demonstrated the robustness of our approach across different imaging modalities by testing it on the publicly available 2016 Low Dose CT Grand Challenge dataset, using 4,800 images for training and 1,136 for testing. The results demonstrate that the proposed approach outperforms state-of-the-art unsupervised algorithms in both visual quality and quantitative evaluation while achieving comparable performance to well-established supervised learning methods across low-dose fluoroscopy and CT imaging. The related source code will be available at https://github.com/sunyoungIT/UDCA-Net.git.","Figure 1: Typical fluoroscopic images obtained from (a) a static pelvis phantom and (b) a dynamic needle tip with a spherical lesion phantom. Fluoroscopy is critical in medical imaging, enabling the real-time generation and visualization of X‑ray images. It is invaluable in understanding internal body structures and organs and imaging medical devices, such as catheters. Consequently, fluoroscopy is an indispensable tool used in various diagnostic examinations, including catheter insertion, vascular imaging, and monitoring during orthopedic surgery. However, the inherent ionizing radiation from X-rays presents considerable risks to patients and medical personnel [1]. Reducing the X-ray intensity is recommended to mitigate this risk. Adopting low-dose X-ray fluoroscopy is a standard practice in monitoring interventions. Although lower doses reduce radiation exposure, they can produce images with increased noise and artifacts [2]. Such imperfections can obscure vital details, potentially affecting clinical decision-making. Thus, efficient noise reduction techniques tailored to low-dose X-ray fluoroscopic images are necessary to maintain essential features, including medical instruments and vital anatomical structures. Various restoration techniques have been introduced to mitigate noise-caused image degradation. Their efficiency is closely related to the suitability of the noise model. Many of these methods characterize the noise as either an additive spatially invariant Gaussian [3, 4] or a signal-dependent Poisson [5, 6]. However, these noise-specific assumptions may not universally apply to clinics equipped with various fluoroscopy devices. Thus, demand exists for denoising algorithms without a specific noise model. Because of the significant noise levels in fluoroscopic images [6], restoring signals obscured by noise in a single fluoroscopic frame is unattainable. For instance, as demonstrated in Fig. 1(a), bone tissue obscured in a single frame (a1) becomes evident when averaging multiple frames of fluoroscopic image sequences (a2). Therefore, denoising fluoroscopic images must be performed using multi-frame image sequences, not just a single image. Multi-frame processing algorithm using traditional temporal filters has been proposed [6, 7, 8, 9, 10]. These traditional filter-based methods have successfully reduced noise and improved the signal-to-noise ratio (SNR) in low-dose fluoroscopic images while minimizing processing time. However, depending on the specific filter applied, the resulting filtered images may exhibit motion-induced blurring in the temporal direction. For instance, as depicted in Fig. 1(b), when applying a recursive filter to a multi-frame sequence image (b1), significant blurring is caused by motion that is not present in the single frame image (b2). To address motion-blurring problems, many researchers have developed methods to enhance the quality of sequential fluoroscopic images by diminishing noise artifacts [11, 12, 13, 14, 15, 16, 17]. For example, Amiot et al. [15] applied a motion-compensated temporal filtering technique operating on multi-scale coefficients to image sequences, successfully capturing subtle features with low contrast while minimizing boundary blurring. However, when acquiring images in X-ray fluoroscopy sequences involving complex motion, motion artifacts, such as blurring and trailing, can occur [18, 19, 10]. This problem underscores the need for proper motion compensation when handling multi-frame sequence images. With the recent emergence of deep learning technology, significant advancements have been achieved in various image processing tasks, including image denoising [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34], to enhance denoising performance. This development has yielded remarkable results, demonstrating the successful application of deep learning in image denoising. Furthermore, several convolutional neural network (CNN)-based methods have been proposed in the field of fluoroscopy images [35, 4, 36, 37, 38, 39]. These approaches successfully reduce noise while retaining image details, displaying exceptional performance. However, most approaches for denoising fluoroscopy images employ supervised learning and rely on a substantial dataset of paired images. These paired images include noisy low- and high-dose X-ray images, which are essential to learn the mapping function between the two image types effectively. Obtaining such paired data in a clinical interventional setting presents significant challenges due to patient motion over fluoroscopy image sequences and the potential risks associated with increased ionizing radiation exposure. To address this problem, self-supervised or unsupervised learning-based methods have been proposed for medical image denoising, particularly in the context of CT imaging [40, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49]. However, research on fluoroscopy image denoising remains limited, and studies in this field are notably scarce [50]. This represents a significant gap in the application of advanced denoising techniques to fluoroscopy. Despite the limited number of studies on unsupervised and self-supervised methods for fluoroscopy noise reduction, a few notable examples do exist. For instance, Liu et al. [51] proposed a three-stage, self-supervised framework for denoising fluoroscopy videos, involving stabilization, mask-based RPCA decomposition, and spatiotemporal bilateral filtering. This approach underscores the potential of enhancing self-supervised learning techniques in fluoroscopy applications. Similarly, Sandersonet al. [52] introduced DDPM-X, a diffusion model-based denoising method for planar X-ray images, capable of handling Gaussian and Poisson noise without network modifications. However, Liu et al. [51] utilized the Self2Self [53], making it challenging to specify the noise model, particularly in real-world scenarios where such models are crucial. Furthermore, preserving edge details is essential for accurate diagnostics, and Sanderson et al. [52] highlights the need for proper motion compensation methods for moving fluoroscopy sequences. These challenges underscore the key requirements for effective fluoroscopic image denoising algorithms, which include 1) not relying on a specific noise model, 2) the necessity of multi-frame sequence images, 3) the need for proper motion compensation in the temporal domain, and 4) the requirement for unsupervised learning due to patient movement. Given these requirements, we propose an unsupervised, two-step denoising framework that effectively utilizes multi-frame sequence images to meet these critical needs. Our approach specifically addresses motion compensation, multi-frame utilization, and unsupervised learning, while also enhancing edge preservation and improving overall image quality in fluoroscopy. In the first step, we pretrain the multi-scale recurrent attention U-Net (MSR2AU-Net) using a self-supervised approach that predicts the middle frame from a series of sequential fluoroscopic images. In the following step, we incorporate a correlated noise suppression module to manage denoising in pixel regions with significant inter-frame motion and an uncorrelated noise suppression module for areas with less motion. The correlated noise suppression module operates recursive filtering on sequence images that have undergone motion compensation. We adopt a knowledge distillation approach for the uncorrelated noise suppression module, using the pre-trained MSR2AU-Net as a teacher model to train a student denoising model. We aim to preserve image edges and ensure outstanding denoising outcomes by fusing these modules and tailoring them to the dynamic object motion on a pixel-by-pixel basis. We evaluate the denoising performance of the unsupervised two-step framework using dynamic phantom and clinical, in vivo datasets. We also used the publicly available CT dataset from the “2016 NIH-AAPM-Mayo Clinic Low Dose CT Grand Challenge” dataset [54]. We compare it with the current state-of-the-art unsupervised and supervised learning methods. The experimental results indicate that the proposed framework effectively minimizes noise levels while maintaining the intricate details of microstructures. The primary contributions of this paper are summarized as follows: • An unsupervised dynamic context-aware denoising framework is proposed to mitigate both correlated and uncorrelated motion-induced quantum noise in low-dose fluoroscopy images. To the best of our knowledge, this is the first attempt at an unsupervised deep learning-based denoising approach specifically designed for dynamic low-dose X-ray fluoroscopy. • Based on dynamic context extraction modules, motion compensation and recursive filtering modules are integrated into the network to enhance denoising capabilities and effectively address issues like motion blurring by considering temporal correlations between frames. • In the edge-preserving extraction module, a pixel-wise cross-fusion matrix is designed to capture changes between the motion-corrected image and noise reduction, maintaining sharp edges and ensuring optimal denoising results. This module combines correlated and uncorrelated noise reduction techniques, adapting to the dynamic motion of the object on a pixel-by-pixel basis. • To improve the network’s ability to maintain sharpness and enhance perceptual quality, an improved loss function is proposed to maximize the extraction of high-frequency features by combining Wavelet and Fourier Transform techniques. • The evaluations on clinical and dynamic-phantom fluoroscopy datasets, as well as low-dose CT, reveal that the proposed algorithm not only surpasses existing leading unsupervised algorithms but also matches the performance of supervised learning-based approaches. Additionally, our framework has been effectively applied to both low-dose CT and fluoroscopy, showcasing strong generalization across different medical imaging modalities. I-A Related Works I-A1 Low-dose CT Denoising Various CNN-based methods, such as U-Net [55] and DnCNN [56], have been employed to reduce noise in low-dose CT (LDCT) images. Similarly, RED-CNN [57] utilizes a combination of ResNet and Autoencoder architectures to further improve denoising performance. These CNN-based approaches showcase the considerable potential of deep learning techniques in advancing medical imaging. However, these techniques often produce overly smooth images due to the mean squared error (MSE) loss. To address this, alternatives like perceptual and GAN losses have been introduced. For instance, exemplified in Yang et al.’s WGAN-VGG framework [58], effectively address this smoothing issue, resulting in more realistic image details and improving overall image quality. Kim et al. [59] proposed a method combining pixel-wise losses for high objective quality with perceptual and wavelet losses to preserve fine details and edges, leveraging wavelet transform properties for enhanced image quality. In addition, CCN-CL [60] enhances denoising accuracy using a content-noise complementary learning strategy, attention mechanisms, and deformable convolutions. Transformer-based models like CTformer [61] introduce pure transformer approaches to LDCT denoising, outperforming traditional methods. StruNet [62] presents a novel Swin transformer-based residual U-shape network, showing promise in handling diverse noise artifacts across various imaging modalities. However, these networks generally rely on supervised learning with paired LDCT and normal-dose CT datasets, a method challenging for fluoroscopy imaging, which involves capturing moving objects. Recently, self-supervised models that only require noisy natural images (e.g., BM3D [63], NLM [64], Noisier2Noise [65], Noise2Void [66], and Noisy-As-Clean [67]) have been applied for denoising tasks in both LDCT and fluoroscopy imaging. These self-supervised approaches have shown promising results, demonstrating their potential to effectively reduce noise without the need for clean reference images, making them valuable tools for improving image quality in challenging medical imaging scenarios. Nonetheless, these methods rely on specific assumptions about noise characteristics in the images, and therefore, their application in fluoroscopic imaging might result in suboptimal performance. This highlights the importance of exploring unsupervised learning techniques, which can provide more flexible and generalized solutions for noise reduction without relying on such assumptions. MM-Net [42], a framework based on unsupervised learning, incorporates multi-mask patching with high-frequency components, allowing it to surpass existing unsupervised algorithms in both qualitative and quantitative assessments across diverse clinical and animal data domains. DenoisingGAN [68] leverages unpaired datasets and integrates a CycleGAN variation with a memory-efficient architecture, demonstrating superior performance in both objective and perceptual quality. These unsupervised learning approaches offer significant advantages by not relying on paired NDCT and LDCT images, making them more adaptable and easier to implement in various clinical settings. However, their clinical application remains limited due to the current limitations in denoising performance. I-A2 Low-dose X-ray Fluoroscopy Denoising While there are relatively fewer deep learning-based denoising algorithms for fluoroscopy compared to LDCT, recent advancements have been made in this area. For instance, Zhang et al. [36] proposed a hybrid three-dimensional (3D)/two-dimensional (2D)-based deep CNN framework for X-ray angiography using a stack of consecutive frames as input. Unlike single-frame approaches, which struggle with motion artifacts due to their limited temporal context, this method harnesses both spatial and temporal information from multiple frames. This comprehensive approach not only significantly enhances image quality but also effectively reduces motion artifacts and preserves fine details, demonstrating a marked improvement over traditional single-frame denoising techniques. In addition, Wu et al. [38] introduced the multi-channel DnCNN (MCDnCNN) based on DenseNet [69] and DnCNN [56], demonstrating enhanced denoising capabilities for fluoroscopic sequence images. Van Veen et al. [37] showcased clinically viable video denoising results for fluoroscopic imaging using a network based on FastDVDNet [70], which incorporates multiple adjacent frames. This study integrates motion estimation directly into the network architecture, significantly enhancing runtime efficiency while maintaining or improving video quality and reducing radiation dose. Moreover, Luo et al. proposed UDDN [4], utilizing dense connections for information reuse in CNN frameworks, thereby enhancing denoising performance. They further improved this approach in EEDN [35] by incorporating an edge-enhancement module, which significantly sharpens edges and fine details, resulting in clearer and more defined images. However, all these algorithms are based on supervised learning, making their application to clinical fluoroscopy imaging quite challenging."
https://arxiv.org/html/2411.00813v1,Personality Analysis from Online Short Video Platforms with Multi-domain Adaptation,"Personality analysis from online short videos has gained prominence due to its applications in personalized recommendation systems, sentiment analysis, and human-computer interaction. Traditional assessment methods, such as questionnaires based on the Big Five Personality Framework, are limited by self-report biases and are impractical for large-scale or real-time analysis. Leveraging the rich, multi-modal data present in short videos offers a promising alternative for more accurate personality inference. However, integrating these diverse and asynchronous modalities poses significant challenges, particularly in aligning time-varying data and ensuring models generalize well to new domains with limited labeled data. In this paper, we propose a novel multi-modal personality analysis framework that addresses these challenges by synchronizing and integrating features from multiple modalities and enhancing model generalization through domain adaptation. We introduce a timestamp-based modality alignment mechanism that synchronizes data based on spoken word timestamps, ensuring accurate correspondence across modalities and facilitating effective feature integration. To capture temporal dependencies and inter-modal interactions, we employ Bidirectional Long Short-Term Memory networks and self-attention mechanisms, allowing the model to focus on the most informative features for personality prediction. Furthermore, we develop a gradient-based domain adaptation method that transfers knowledge from multiple source domains to improve performance in target domains with scarce labeled data. Extensive experiments on real-world datasets demonstrate that our framework significantly outperforms existing methods in personality prediction tasks, highlighting its effectiveness in capturing complex behavioral cues and robustness in adapting to new domains. We open our datasets and code at https://github.com/Anne6645/personality_analysis","Personality analysis has long been a central topic in psychological science and has gained increasing importance in recent years due to its wide-ranging applications. It plays a crucial role in various domains such as personalized recommendation systems [1, 2], sentiment analysis [3, 4], and human-computer interaction. Accurately identifying an individual’s personality can enable tailored experiences and services, enhancing user satisfaction and engagement. However, personality traits are inherently latent characteristics that are not directly observable, making the assessment of personality a challenging task. Traditionally, psychologists have employed structured methods to evaluate an individual’s personality. One of the most widely accepted models is the Big Five Personality Traits (as shown in Figure 1), which assesses personality across five key dimensions: openness, conscientiousness, extraversion, agreeableness, and neuroticism. To determine an individual’s position on these dimensions, conventional approaches often rely on well-designed questionnaires and psychological inventories that analyze self-reported responses. While these methods are grounded in rigorous psychometric principles, they have notable limitations. Self-reported data can be influenced by social desirability bias, where respondents tailor their answers to be viewed favorably. Additionally, administering and processing these surveys can be time-consuming and resource-intensive, making them less practical for large-scale or real-time applications. Figure 1: Big Five Personality Traits With the advent of online video social platforms like TikTok111https://www.tiktok.com/ and others, there is a growing opportunity to analyze personality traits through digital means. Users increasingly share selfie videos online, providing a wealth of data that captures not only their visual appearance but also their speech patterns, facial expressions, and environmental context. Compared to static questionnaires, these multi-modal data offer richer insights into an individual’s intrinsic traits. Unlike traditional social media platforms that primarily feature text or images, video platforms enable the observation of dynamic behaviors and interactions, which are crucial for understanding personality. This shift opens up new possibilities for applications such as online job interviews, remote education, and personalized content delivery, where assessing personality from videos can significantly enhance outcomes. Recent research has begun to explore the potential of analyzing personality traits through online media instead of traditional surveys. Behavioral observations from personal photographs [5, 6, 7] and short videos [8] have been utilized to glean personality insights. For instance, [5] analyzed Facebook profile pictures to infer personality traits, while [6] and [7] leveraged social media images for similar purposes. However, photograph-based approaches have limitations, as individuals often curate their online images, sharing selective moments that may not accurately represent their typical behaviors or personality, leading to biased data and potentially inaccurate predictions. In contrast, short videos provide a more comprehensive medium for personality analysis. They capture changes in facial expressions, body movements, speech patterns, and contextual scenes—all of which are significant indicators in psychological assessments of personality. Recognizing this, researchers have started to model the audio, visual, and textual features present in short videos [9, 10, 11]. For example, [9] developed a Deep Bimodal Regression model combining audio and visual modalities to predict scores on the Big Five personality traits. Similarly, [10] employed convolutional neural networks to extract visual features and linear regression for audio features, while [11] conducted an in-depth analysis using logistic regression on audio, video, and text features. Figure 2: Comparison of Existing Works Despite these advances, existing multi-modal personality prediction methods often rely on large volumes of high-quality short videos with high-resolution visuals and clear audio to achieve satisfactory performance. Moreover, many approaches depend heavily on supervised learning techniques that require extensive labeled datasets. Collecting and annotating such multi-modal data is both expensive and time-consuming. Manual annotation introduces the potential for subjectivity and inconsistency, which can affect the reliability of the analysis. Consequently, detecting personality traits from online video platforms presents significant challenges, particularly in the following areas: The First Challenge is identifying the most important features from multiple modalities to optimize the use of a limited number of short videos for accurate personality analysis. The complexity arises from the need to effectively integrate diverse data types—visual cues, auditory signals, textual content, and contextual information—each contributing uniquely to personality inference. Existing methods may not adequately address the alignment and synchronization of these modalities, leading to fragmented or incomplete representations that hinder predictive accuracy. The Second Challenge is effectively utilizing a small number of high-quality short videos to achieve strong generalization in personality analysis. Models trained on specific datasets may struggle to generalize across different domains due to variations in cultural contexts, linguistic expressions, and recording conditions. The scarcity of labeled data in new or underrepresented domains exacerbates this issue, limiting the applicability of the models in real-world scenarios where data diversity is the norm. In this paper, we propose an effective multi-modal personality analysis framework designed to overcome these challenges. To address the first challenge, we introduce a semantic unit method for feature extraction and alignment, which synchronizes multi-modal data based on spoken words. This ensures that features from different modalities correspond accurately at each moment in the video, facilitating effective integration. Within this module, we employ self-attention mechanisms to discern the significance of features across various modalities. By assigning weights to features based on their relevance to personality prediction, the model focuses on the most informative aspects of the data, enhancing analytical accuracy. To tackle the second challenge, we propose a multi-domain adaptation method that transfers domain knowledge across multiple domains to alleviate the data sparsity problem. This approach leverages information from data-rich source domains to enhance learning in data-scarce target domains. By computing gradient similarities between source and target domains, our model adapts to emphasize learning from source domains that are most relevant to the target domain. This method improves the model’s generalization capabilities, enabling more accurate predictions even when limited data is available in certain domains. Our main contributions are summarized as follows: • We propose an effective multi-modal personality analysis framework that effectively integrates facial expressions, audio signals, textual content, and background information from short videos for personality prediction. • We introduce a semantic unit modality alignment mechanism that synchronizes multi-modal data based on spoken word timestamps, ensuring accurate correspondence across modalities and enhancing feature representation. • We develop a gradient-based domain adaptation method that transfers knowledge from multiple source domains to target domains with limited labeled data, enhancing model generalization and performance in few-shot learning scenarios. • We validate the effectiveness of our proposed framework through extensive experiments on real-world datasets, demonstrating significant improvements over existing methods in personality prediction tasks. By addressing both the feature integration and domain adaptation challenges, our framework advances in personality analysis from online short videos. The rest of the paper is organized as follows. Section II introduces the related work of personality analysis and domain adaption. The problem definition is in Section III. Section IV elucidates the detailed methodology, and Section V presents the results of the experiments and the analysis. The last section is the conclusion of the paper."
https://arxiv.org/html/2411.00776v1,Randomized Autoregressive Visual Generation,"This paper presents Randomized AutoRegressive modeling (RAR) for visual generation, which sets a new state-of-the-art performance on the image generation task while maintaining full compatibility with language modeling frameworks. The proposed RAR is simple: during a standard autoregressive training process with a next-token prediction objective, the input sequence—typically ordered in raster form—is randomly permuted into different factorization orders with a probability r𝑟ritalic_r, where r𝑟ritalic_r starts at 1111 and linearly decays to 00 over the course of training. This annealing training strategy enables the model to learn to maximize the expected likelihood over all factorization orders and thus effectively improve the model’s capability of modeling bidirectional contexts. Importantly, RAR preserves the integrity of the autoregressive modeling framework, ensuring full compatibility with language modeling while significantly improving performance in image generation. On the ImageNet-256 benchmark, RAR achieves an FID score of 1.48, not only surpassing prior state-of-the-art autoregressive image generators but also outperforming leading diffusion-based and masked transformer-based methods. Code and models will be made available at https://github.com/bytedance/1d-tokenizer.","AutoRegressive (AR) models have driven remarkable advancements across both natural language processing and computer vision tasks in recent years. In language modeling, they serve as the fundamental framework for Large Language Models (LLMs) such as GPT [43], Llama [59, 60], and Gemini [57], along with other state-of-the-art models [67, 1]. In the realm of computer vision, autoregressive models111While MaskGIT-style models [10] could be classified as “generalized autoregressive models” as defined in [36], in this paper, we primarily use the term “autoregressive” to refer to GPT-style models [22, 69, 52], which are characterized by causal attention, next-token prediction, and operate without the need for mask tokens as placeholders. have also shown substantial potential, delivering competitive performance in image generation tasks [22, 69, 70, 35, 52, 39] to diffusion models [18, 6, 45, 36] or non-autoregressive transformers [10, 71, 72, 73, 65]. More importantly, autoregressive modeling is emerging as a promising pathway toward unified models across multiple modalities and tasks [9, 66, 14, 5, 55, 56]. Figure 1: Comparison among different language modeling compatible autoregressive (AR) image generators. The proposed RAR demonstrates significant improvements over previous AR methods. RAR-B, with only 261M parameters, achieves an FID score of 1.95, outperforming both LlamaGen-XXL (1.4B parameters) and Open-MAGVIT2-XL (1.5B parameters). Figure 2: Overview of the proposed Randomized AutoRegressive (RAR) model, which is fully compatible with language modeling frameworks. Left: RAR introduces a randomness annealing training strategy to enhance the model’s ability to learn bidirectional contexts. During training, the input sequence is randomly permuted with a probability r𝑟ritalic_r, which starts at 1 (fully random permutations) and linearly decreases to 0, transitioning the model to a fixed scan order, such as raster scan, by the end of training. Right: Randomly selected images generated by RAR, trained on ImageNet. Despite the dominance of autoregressive models in language modeling, they often yield suboptimal performance in comparison to diffusion models or non-autoregressive transformers in visual generation tasks [52, 39]. This discrepancy can be attributed to the inherent differences between text and visual signals. Text is highly compact and semantically meaningful, while visual data tends to be more low-level and redundant [29, 73], making bidirectional context modeling more critical. For instance, several studies [36, 21, 7] have demonstrated that causal attention applied to image tokens leads to inferior performance compared to bidirectional attention in vision tasks. To address this, recent works [58, 36] have attempted to reintroduce bidirectional attention by redesigning the autoregressive formulation, achieving state-of-the-art results in image generation. However, these approaches often deviate from the traditional autoregressive paradigm. For example, VAR [58] shifts from next-token prediction to next-scale prediction, enabling bidirectional attention within each scale, and MAR [36] generalizes MaskGIT-style framework [10] to the autoregressive definition, which naturally introduces back the bidirectional attention. While effective, these modifications complicate their integration into universal transformer architectures that aim to unify different modalities, which proves to work well with conventional autoregressive modeling [55, 56]. In this paper, we aim to enhance the generation quality of autoregressive image models while preserving the core autoregressive structure, maintaining compatibility with language modeling frameworks. Specifically, we enable bidirectional context learning within an autoregressive transformer by maximizing the expected likelihood over all possible factorization order. In this way, all tokens will be trained and predicted under all possible contexts, facilitating learning bidirectional representation. Moreover, we introduce a permutation probability r𝑟ritalic_r, which controls the ratio of training data between a random factorization order and the standard raster order. Initially, r𝑟ritalic_r is set to 1111 (fully random factorization) and it linearly decays to 00 over the course of training, gradually reverting the model to the raster order commonly used by other autoregressive image generators. To this end, we present a simple, effective, and scalable autoregressive model training paradiam named Randomized AutoRegressive modeling (RAR). RAR retains the original autoregressive model architecture and formulation, ensuring full compatibility with language modeling. At the same time, it significantly improves the generation quality of autoregressive models at no additional cost. On the ImageNet-256 benchmark [16], RAR achieves an FID score of 1.481.481.481.48, substantially outperforming previous state-of-the-art autoregressive image generators, as illustrated in Fig. 1. By addressing the limitations of unidirectional context modeling, RAR represents a critical step towards autoregressive visual generation and opens up new possibilities for further advancements in the field."
https://arxiv.org/html/2411.00771v1,CityGaussianV2: Efficient and Geometrically Accurate Reconstruction for Large-Scale Scenes,"Recently, 3D Gaussian Splatting (3DGS) has revolutionized radiance field reconstruction, manifesting efficient and high-fidelity novel view synthesis. However, accurately representing surfaces, especially in large and complex scenarios, remains a significant challenge due to the unstructured nature of 3DGS. In this paper, we present CityGaussianV2, a novel approach for large-scale scene reconstruction that addresses critical challenges related to geometric accuracy and efficiency. Building on the favorable generalization capabilities of 2D Gaussian Splatting (2DGS), we address its convergence and scalability issues. Specifically, we implement a decomposed-gradient-based densification and depth regression technique to eliminate blurry artifacts and accelerate convergence. To scale up, we introduce an elongation filter that mitigates Gaussian count explosion caused by 2DGS degeneration. Furthermore, we optimize the CityGaussian pipeline for parallel training, achieving up to 10×\times× compression, at least 25% savings in training time, and a 50% decrease in memory usage. We also established standard geometry benchmarks under large-scale scenes. Experimental results demonstrate that our method strikes a promising balance between visual quality, geometric accuracy, as well as storage and training costs. The project page is available at https://dekuliutesla.github.io/CityGaussianV2/.","3D scene reconstruction is a long-standing topic in computer vision and graphics, with its core pursuit of photo-realistic rendering and accurate geometry reconstruction. Beyond Neural Radiance Fields (NeRF) (Mildenhall et al., 2021), 3D Gaussian Splatting (3DGS) (Kerbl et al., 2023) has become the predominant technique in this area due to its superiority in training convergence and rendering efficiency. 3DGS represents the scene with a set of discrete Gaussian ellipsoids and renders with a highly optimized rasterizer. However, such primitives take an unordered structure and do not correspond well to the actual surface of the scene. This limitation impairs its synthesis quality at extrapolated views and hinders its downstream application in editing, animation, and relighting (Guédon & Lepetit, 2024). Recently, many excellent works (Guédon & Lepetit, 2024; Huang et al., 2024; Yu et al., 2024c) have been proposed to address this issue. Despite their great success in single objects or small scenes, devils emerge when applying them directly to complex, large-scale scenes. On the one hand, existing methods face significant challenges related to scalability and generalization ability. For example, SuGaR (Guédon & Lepetit, 2024) binds meshes with Gaussians for refinement. However, in large-scale and complex scenarios, meshes can become extensive and finely detailed, resulting in substantial memory demands and capacity limitations. GOF (Yu et al., 2024c) struggles with large, over-blurred Gaussians. These Gaussians not only severely deteriorate its rendering and geometry quality, but also generate a shell-like mesh that is non-trivial to remove, as validated in Fig. 7 and Fig. 8 in Appendix. While 2DGS (Huang et al., 2024) exhibits better generalization ability, as shown in Tab. 1, its convergence is hindered by the blurred Gaussians illustrated in part (b) of Fig. 1. Additionally, when scaling up through parallel training, it suffers from a Gaussian count explosion, as depicted in Fig. 3. Another challenge lies in the evaluation protocol: due to insufficient observations in boundary regions, geometry estimation becomes error-prone and unstable in these areas. As a result, the metrics can significantly fluctuate and underestimate actual performance (Xiong et al., 2024), making it difficult to objectively evaluate and compare algorithms. On the other hand, achieving efficient parallel training and compression is critical to realizing geometrically accurate reconstruction of large-scale scenes. As shown in Tab. 2, the total number of Gaussians can increase to 19.3 million during parallel training, resulting in a storage requirement of 4.6 GB and a memory cost of 31.5 GB, while rendering speed drops below 25 FPS. Additionally, existing VastGaussian (Lin et al., 2024) costs nearly 3 hours for training, and CityGaussian Liu et al. (2024) consumes 4 hours to finish both training and compression. For reconstruction on low-end devices or under strict time constraints, these training costs and rendering speeds are unacceptable. Therefore, there is an urgent need for an economical parallel training and compression strategy. In response to these challenges, we introduce CityGaussianV2, a geometrically accurate yet efficient strategy for large-scale scene reconstruction. We take 2DGS as primitive due to its favorable generalization capabilities. To accelerate reconstruction, we employ depth regression guided by Depth-Anything V2 (Yang et al., 2024) and Decomposed-Gradient-based Densification (DGD). As shown in part (b) of Fig. 1 and Tab. 2, DGD effectively eliminates blurred surfels, crucial for performance improvement. To address scalability, we introduce an Elongation Filter to mitigate the Gaussian count explosion problem associated with 2DGS degeneration during parallel training. To reduce the burden of single GPU, we conduct parallel training based on CityGaussian’s block partitioning strategy. And we streamline the process by omitting time-consuming post-pruning and distillation steps of CityGaussian. Instead, we implement spherical harmonics of degree 2 from scratch and integrate contribution-based pruning into per-block fine-tuning. As demonstrated in part (c) of Fig. 1, it scales up the surface quality of complex structures while significantly reducing training costs. Furthermore, our contribution-based vectree quantization enables a tenfold reduction in storage requirements for large-scale 2DGS. For evaluation, we introduce TnT-style (Knapitsch et al., 2017) protocol along with a visibility-based crop volume estimation strategy, which can efficiently exclude underobserved regions and bring stable and consistent assessment. In summary, our contributions are four-fold: • A novel optimization strategy for 2DGS, that accelerates its convergence under large-scale scenes and enables it to be scaled up to high capacity (Sec. 3.2). • A highly optimized parallel training pipeline that significantly reduces training costs and storage requirements while enabling real-time rendering performance (Sec. 3.3). • A TnT-style standardized evaluation protocol tailored for large, unbounded scenes, establishing a geometric benchmark for large-scale scene reconstruction (Sec. 4). • To the best of our knowledge, our CityGaussianV2 is among the first to implement the Gaussian radiance field in large-scale surface reconstruction. Experimental results confirm our state-of-the-art performance in both geometric quality and efficiency."
https://arxiv.org/html/2411.00769v1,GameGen-𝕏𝕏\mathbb{X}blackboard_X: Interactive Open-world Game Video Generation,"We introduce GameGen-𝕏𝕏\mathbb{X}blackboard_X, the first diffusion transformer model specifically designed for both generating and interactively controlling open-world game videos. This model facilitates high-quality, open-domain generation by simulating an extensive array of game engine features, such as innovative characters, dynamic environments, complex actions, and diverse events. Additionally, it provides interactive controllability, predicting and altering future content based on the current clip, thus allowing for gameplay simulation. To realize this vision, we first collected and built an Open-World Video Game Dataset (OGameData) from scratch. It is the first and largest dataset for open-world game video generation and control, which comprises over one million diverse gameplay video clips sampling from over 150 games with informative captions from GPT-4o. GameGen-𝕏𝕏\mathbb{X}blackboard_X undergoes a two-stage training process, consisting of foundation model pre-training and instruction tuning. Firstly, the model was pre-trained via text-to-video generation and video continuation, endowing it with the capability for long-sequence, high-quality open-domain game video generation. Further, to achieve interactive controllability, we designed InstructNet to incorporate game-related multi-modal control signal experts. This allows the model to adjust latent representations based on user inputs, unifying character interaction and scene content control for the first time in video generation. During instruction tuning, only the InstructNet is updated while the pre-trained foundation model is frozen, enabling the integration of interactive controllability without loss of diversity and quality of generated video content. GameGen-𝕏𝕏\mathbb{X}blackboard_X represents a significant leap forward in open-world video game design using generative models. It demonstrates the potential of generative models to serve as auxiliary tools to traditional rendering techniques, effectively merging creative generation with interactive capabilities. The project will be available at https://github.com/GameGen-X/GameGen-X.","Generative models (Croitoru et al. (2023); Ramesh et al. (2022); Tim Brooks & Ramesh (2024); Rombach et al. (2022b)) have made remarkable progress in generating images or videos conditioned on multi-modal inputs such as text, images, and videos. These advancements have benefited content creation in design, advertising, animation, and film by reducing costs and effort. Inspired by the success of generative models in these creative fields, it is natural to explore their application in the modern game industry. This exploration is particularly important because developing open-world video game prototypes is a resource-intensive and costly endeavor, requiring substantial investment in concept design, asset creation, programming, and preliminary testing (Anastasia (2023)). Even early development stages of games still involved months of intensive work by small teams to build functional prototypes showcasing the game’s potential (Wikipedia (2023)). Several pioneering works, such as World Model (Ha & Schmidhuber (2018)), GameGAN (Kim et al. (2020)), R2PLAY (Jin et al. (2024)), Genie (Bruce et al. (2024)), and GameNGen (Valevski et al. (2024)), have explored the potential of neural models to simulate or play video games. They have primarily focused on 2D games like “Pac-Man”, “Super Mario”, and early 3D games such as “DOOM (1993)”. Impressively, they demonstrated the feasibility of simulating interactive game environments. However, the generation of novel, complex open-world game content remains an open problem. A key difficulty lies in generating novel and coherent next-gen game content. Open-world games feature intricate environments, dynamic events, diverse characters, and complex actions that are far more challenging to generate (Eberly (2006)). Further, ensuring interactive controllability, where the generated content responds meaningfully to user inputs, remains a formidable challenge. Addressing these challenges is crucial for advancing the use of generative models in game content design and development. Moreover, successfully simulating and generating these games would also be meaningful for generative models, as they strive for highly realistic environments and interactions, which in turn may approach real-world simulation (Zhu et al. (2024)). In this work, we provide an initial answer to the question: Can a diffusion model generate and control high-quality, complex open-world video game content? Specifically, we introduce GameGen-𝕏𝕏\mathbb{X}blackboard_X, the first diffusion transformer model capable of both generating and simulating open-world video games with interactive control. GameGen-𝕏𝕏\mathbb{X}blackboard_X sets a new benchmark by excelling at generating diverse and creative game content, including dynamic environments, varied characters, engaging events, and complex actions. Moreover, GameGen-𝕏𝕏\mathbb{X}blackboard_X enables interactive control within generative models, allowing users to influence the generated content and unifying character interaction and scene content control for the first time. It initially generates a video clip to set up the environment and characters. Subsequently, it produces video clips that dynamically respond to user inputs by leveraging the current video clip and multimodal user control signals. This process can be seen as simulating a game-like experience where both the environment and characters evolve dynamically. GameGen-𝕏𝕏\mathbb{X}blackboard_X undergoes a two-stage training: foundation model pre-training and instruction tuning. In the first stage, the foundation model is pre-trained on OGameData using text-to-video generation and video continuation tasks. This enables the model to learn a broad range of open-world game dynamics and generate high-quality game content. In the second stage, InstructNet is designed to enable multi-modal interactive controllability. The foundation model is frozen, and InstructNet is trained to map user inputs—such as structured text instructions for game environment dynamics and keyboard controls for character movements and actions—onto the generated game content. This allows GameGen-𝕏𝕏\mathbb{X}blackboard_X to generate coherent and controllable video clips that evolve based on the player’s inputs, simulating an interactive gaming experience. To facilitate this development, we constructed Open-World Video Game Dataset (OGameData), the first large-scale dataset for game video generation and control. This dataset was collected from over 150 next-generation games and was built by using a human-in-the-loop proprietary data pipeline that involves scoring, filtering, sorting, and structural captioning. OGameData contains one million video clips from two subsets including OGameData-GEN and OGameData-INS, providing the foundation for training generative models capable of producing realistic game content and achieving interactive control, respectively. In summary, GameGen-𝕏𝕏\mathbb{X}blackboard_X offers a novel approach for interactive open-world game video generation, where complex game content is generated and controlled interactively. It lays the foundation for a new potential paradigm in game content design and development. While challenges for practical application remain, GameGen-𝕏𝕏\mathbb{X}blackboard_X demonstrates the potential for generative models to serve as a scalable and efficient auxiliary tool to traditional game design methods. Our main contributions are summarized as follows: 1) We developed OGameData, the first comprehensive dataset specifically curated for open-world game video generation and interactive control, which contains one million video-text pairs. It is collected from 150+ next-gen games, and empowered by GPT-4o. 2) We introduced GameGen-𝕏𝕏\mathbb{X}blackboard_X, the first generative model for open-world video game content, combining a foundation model with the InstructNet. GameGen-𝕏𝕏\mathbb{X}blackboard_X utilizes a two-stage training strategy, with the foundation model and InstructNet trained separately to ensure stable, high-quality content generation and control. InstructNet provides multi-modal interactive controllability, allowing players to influence the continuation of generated content, simulating gameplay. 3) We conducted extensive experiments comparing our model’s generative and interactive control abilities to other open-source and commercial models. Results show that our approach excels in high-quality game content generation and offers superior control over the environment and character."
https://arxiv.org/html/2411.00762v1,Face Anonymization Made Simple,"Current face anonymization techniques often depend on identity loss calculated by face recognition models, which can be inaccurate and unreliable. Additionally, many methods require supplementary data such as facial landmarks and masks to guide the synthesis process. In contrast, our approach uses diffusion models with only a reconstruction loss, eliminating the need for facial landmarks or masks while still producing images with intricate, fine-grained details. We validated our results on two public benchmarks through both quantitative and qualitative evaluations. Our model achieves state-of-the-art performance in three key areas: identity anonymization, facial attribute preservation, and image quality. Beyond its primary function of anonymization, our model can also perform face swapping tasks by incorporating an additional facial image as input, demonstrating its versatility and potential for diverse applications. Our code and models are available at https://github.com/hanweikung/face_anon_simple.","In the digital age, our identity and privacy are more vulnerable than ever. People have shared personal information and photos online over recent decades, while advancements in facial recognition technology have made it easier to identify individuals from a single image. This combination allows for the potential linking of our faces to personal information, posing a significant threat to our privacy and identity. In response, various regions have enacted privacy protection laws. These include the European Union’s General Data Protection Regulation (GDPR) [1], California’s Consumer Privacy Act, and Japan’s amended Act on the Protection of Personal Information. Such legislation mandates that organizations implement security measures and maintain transparency in their handling of personal data. Face anonymization is essential for protecting individuals in photos and videos, thereby reducing the risk of personal data being compromised or misused. Traditional methods like blurring and pixelation are common but have significant drawbacks. These techniques are vulnerable to reconstruction attacks [52], degrade image quality, and apply a uniform transformation across the image without considering which areas are most critical to anonymize. These limitations make traditional methods impractical for professionals who need to preserve facial expressions and backgrounds. For example, medical practitioners may need to anonymize patient images for case studies or research while retaining crucial facial cues that indicate symptoms. In creative fields, documentary filmmakers might want to protect interviewees’ privacy without losing the narrative impact of their facial expressions and reactions. They may also wish to replace an interviewee’s face with a specific virtual identity to enhance storytelling clarity. In contrast, recent advances in deep learning have led to more effective anonymization techniques that enhance both privacy protection and usability. Generative Adversarial Networks (GANs) [17], in particular, can anonymize faces by replacing the original with computer-generated alternatives [25, 37, 10, 50]. However, these methods are not without challenges. Some fail to produce natural-looking faces [37], while others [25] struggle to preserve crucial elements like facial expressions, eye direction, head orientation, background details, clothing, and accessories. These limitations greatly restrict the practical application of these techniques. This paper presents a diffusion-based method for face anonymization. Our goal is to ensure that de-identified facial images remain useful for facial analysis tasks, including pose estimation, eye-gaze tracking, and expression recognition, as well as for broader uses such as interviews and films. Therefore, we approach face anonymization similarly to face swapping, aiming to generate an image where a person’s face is replaced by another person’s face while maintaining the original facial expression, pose, eye gaze, and background. We designed a framework that initially performs realistic and seamless face swaps given both source and driving images. At its core is a denoising UNet architecture, similar to those used in text-to-image diffusion models, which generates the final output. We enhance this with an image feature extraction mechanism that transfers fine details from input images to the synthesized output throughout the diffusion process. The model is then trained in a dual setting: conditionally with a source image and unconditionally without a source image. This dual method allows the model to replace faces using one single image input. To create a distinct anonymized identity, the system reverses the original face’s most distinctive features. This technique produces a believable anonymized face while preserving the original image’s quality and essential facial characteristics. In summary, our contributions are: • A convenient method that produces realistic anonymized faces while preserving attributes, without needing external data like facial landmarks or masks as required by existing techniques. • A diffusion-based network that achieves good performance with a single, simple loss function, in contrast to GAN-based models requiring multiple, carefully designed loss functions. • Simple control of the anonymization level using a single parameter. • Versatility beyond anonymization, including the ability to perform face swapping tasks with an additional facial image input."
https://arxiv.org/html/2411.00729v1,Autobiasing Event Cameras,"This paper presents an autonomous method to address challenges arising from severe lighting conditions in machine vision applications that use event cameras. To manage these conditions, the research explores the built-in potential of these cameras to adjust pixel functionality, named bias settings. As cars are driven at various times and locations, shifts in lighting conditions are unavoidable. Consequently, this paper utilizes the neuromorphic YOLO-based face tracking module of a driver monitoring system as the event-based application to study. The proposed method uses numerical metrics to continuously monitor the performance of the event-based application in real-time. When the application malfunctions, the system detects this through a drop in the metrics and automatically adjusts the event camera’s bias values. The Nelder-Mead simplex algorithm is employed to optimize this adjustment, with fine-tuning continuing until performance returns to a satisfactory level. The advantage of bias optimization lies in its ability to handle conditions such as flickering or darkness without requiring additional hardware or software. To demonstrate the capabilities of the proposed system, it was tested under conditions where detecting human faces with default bias values was impossible. These severe conditions were simulated using dim ambient light and various flickering frequencies. Following the automatic and dynamic process of bias modification, the metrics for face detection significantly improved under all conditions. Autobiasing resulted in an increase in the YOLO confidence indicators by more than 33% for object detection and 37% for face detection highlighting the effectiveness of the proposed method.","Event cameras have emerged as a reliable solution for computer vision applications, primarily due to their unique approach to scene capture. Unlike frame-based cameras, these sensors record changes in light and movement in the environment by adjusting the values of individual pixels independently [4]. When the sensor detects a change in light intensity due to movement, it generates an event that includes the X and Y positions of the pixel, the timestamp of occurrence (in microsecond), and the polarity of the change which can be positive or negative depending on the change in the light. This variable data captured by event cameras effectively mitigates the issue of under-sampling encountered in conventional frame-based cameras [38]. This feature is particularly beneficial in automotive technologies where precise and detailed information is essential. Event cameras are also noted for their low latency and high temporal resolution, which enhances their suitability for various automotive applications, including out-of-cabin object detection and in-cabin driver monitoring systems [38]. Recent years have witnessed a growing interest among researchers in event-based driver monitoring systems (DMS), leading to the development of several initiatives [35, 38, 34]. In the design of DMS, it is crucial to consider that vehicles operate under diverse lighting conditions, ranging from daylight to nighttime, and across varying weather conditions, such as sunny or cloudy skies, and in different environments like open roads or tunnels. In addition to these variations in lighting, modern vehicles contain numerous sources of noise and flickering, including large screens and various sensors, which can negatively impact the functionality of any type of camera. The inherent sensitivity of event cameras to changes in ambient light can present challenges when implementing them in DMS applications. However, event cameras offer an advantage in addressing lighting challenges due to their flexibility in allowing users to adjust sensor-level settings to adapt to ambient lighting conditions. This flexibility, provided by event camera manufacturers, is referred to as bias [24]. By adjusting the bias settings, it becomes possible to tune the characteristics of the internal circuits of the sensor, particularly different filters, to ensure compatibility with various conditions. These settings include the camera’s sensitivity to positive or negative light changes, determining the minimum light change required to trigger an event. Additionally, two other biases control the sharpness of a low-pass filter, which removes background noise, and a high-pass filter, responsible for filtering out low-speed movements from the event stream. Lastly, another bias that determines the sleeping time for each pixel after each event generation. More details about the event camera hardware and these biases are presented in section 2 [24]. This study investigates the use of bias settings in event cameras to enhance the adaptability of DMS applications with diverse lighting conditions. Specifically, the research aims to explore the implementation of such adaptation mechanisms, including when and how the bias settings should be adjusted, the extent of the required changes, and the specific biases that need modification. Recognizing the impracticality of manual adjustments by drivers and passengers, the study seeks to develop an automated system capable of monitoring camera performance across various lighting conditions. This system would dynamically adjust the bias settings in response to changing conditions, ensuring optimal camera functionality without requiring manual intervention. In this study, the term ""autobiasing"" refers to the automatic and dynamic adjustment of biases within event cameras. Previous research has explored autobiasing but focused primarily on single bias adjustments, such as Nair et al.’s feedback control loop that regulates the refractory period bias to maintain event count within a desired range [28]. In contrast, this study introduces a novel approach by simultaneously tuning all biases, thus providing a more versatile solution applicable to various event-based applications. Unlike previous systems, which were designed to address specific issues and incorporated separate monitoring blocks, the proposed algorithm continuously adapts to any bias-related changes and optimizes performance dynamically. Additional research by Dilmaghani et al. has demonstrated the impact of bias changes on event camera output sharpness and DMS functionality [3, 36], while another study investigated auto-focus capabilities [39]. This study’s significance lies in its ability to ensure optimal performance across diverse lighting and noise conditions, addressing a critical challenge in event-based systems and enhancing the reliability of event cameras compared to traditional camera types. According to [36], face detection and tracking are critical for the effective operation of Driver Monitoring Systems (DMS), as they underpin other components such as eye tracking and blink counting. This project aims to continuously monitor face tracking metrics to evaluate the DMS’s face detection capabilities. This study utilizes a YOLO V3-based [32] network for face detection and assess the proposed bias optimization method based on YOLO’s output confidences. It is important to note that no retraining, fine tuning, or evaluation of YOLO was done in this study as such efforts would be out of scope. Dynamic bias adjustment will be implemented by feeding monitoring results into a bias controller. If DMS functionality metrics fall below a specified threshold, the simplex algorithm will optimize the bias values until performance metrics exceed the threshold. The proposed algorithm includes the following main improvements on the state of the art: • Represents the first known approach to optimize all biases in an event camera system simultaneously. This method effectively addresses issues related to adverse lighting conditions, such as flickering and darkness, significantly improving performance. • Continuously monitors the performance of DMS components using quantitative metrics, focusing particularly on face tracking capabilities. This real-time tracking allows for immediate detection and correction of any performance declines. • Evaluates the effectiveness of real-time autobiasing in enhancing DMS functionality. Employing YOLO V3 for face detection and a simplex algorithm for bias adjustments, the study demonstrates significant improvements in system performance and reliability. The paper is organized as follows: the process of event generation and the impact of biases are described in section 2. A summary of the state of the art is given in Section 3. In Section 4, the problem addressed in this study and the approach taken to solve it are explained in extensive detail. The algorithm test results are proposed in Section 5, and the conclusion is covered in Section 6."
https://arxiv.org/html/2411.00715v1,B-cosification: Transforming Deep Neural Networksto be Inherently Interpretable,"B-cos Networks have been shown to be effective for obtaining highly human interpretable explanations of model decisions by architecturally enforcing stronger alignment between inputs and weight. B-cos variants of convolutional networks (CNNs) and vision transformers (ViTs), which primarily replace linear layers with B-cos transformations, perform competitively to their respective standard variants while also yielding explanations that are faithful by design. However, it has so far been necessary to train these models from scratch, which is increasingly infeasible in the era of large, pre-trained foundation models. In this work, inspired by the architectural similarities in standard DNNs and B-cos networks, we propose ‘B-cosification’, a novel approach to transform existing pre-trained models to become inherently interpretable. We perform a thorough study of design choices to perform this conversion, both for convolutional neural networks and vision transformers. We find that B-cosification can yield models that are on par with B-cos models trained from scratch in terms of interpretability, while often outperforming them in terms of classification performance at a fraction of the training cost. Subsequently, we apply B-cosification to a pretrained CLIP model, and show that, even with limited data and compute cost, we obtain a B-cosified version that is highly interpretable and competitive on zero shot performance across a variety of datasets. We release our code and pre-trained model weights at https://github.com/shrebox/B-cosification.","Despite their strong performance on a variety of tasks, understanding decisions of deep neural networks (DNNs) remains challenging. Explanation methods, such as feature attributions [45, 47, 52, 5], have been proposed in an attempt to explain such decisions post-hoc, but have often found to be unfaithful to the model being explained [2, 3, 39, 58]. Inherently interpretable Deep Neural Network (DNN) models have recently gained popularity. In contrast to the common approach of explaining existing DNNs in a post-hoc fashion, these models typically feature certain architectural constraints that allow for extracting human-interpretable, model-faithful simplifications of the models’ computations by design; examples of this include prototype-based [13, 16, 31], dynamic linear [8, 9], or concept-bottleneck models [27, 56, 32, 41]. However, given those architectural changes, this comes at a price: specifically, the models need to be trained from scratch, which—especially in the case of large foundational models, which are increasingly popular—can cost millions of dollars. To mitigate this, in this work, we explore a novel approach of fine-tuning DNNs for inherent interpretability and propose to ‘B-cosify’ existing DNNs. Specifically, we investigate whether pre-trained DNNs can simply be efficiently fine-tuned to obtain a similar degree of interpretability as the recently proposed B-cos Networks [9, 10]. In contrast to the original B-cos Networks, which leverage existing architectures to obtain performant and interpretable models, we investigate whether we can additionally leverage the existing pre-trained weights, thus aiming to take advantage of the significant amount of resources that have been invested in training existing models. As a result, we hope to make inherently interpretable models more easily accessible to the community. To do so, we first conduct a detailed analysis of how B-cos DNNs differ from their conventional counterparts. Interestingly, we find that many existing models can be converted into functionally equivalent B-cos models by a small set of targeted implementational modifications (Tab. 1). To increase the interpretability of the models, we then increase the ‘alignment pressure’ [9] via the parameter B of the B-cos transformations and fine-tune the models on their respective tasks, which leads to significantly more interpretable explanations (Fig. 2). On supervised settings, we find that B-cosified models often outperform both conventional and B-cos DNNs at a fraction of the full training cost (Fig. 1, left), whilst exhibiting a similar degree of interpretability as the original B-cos DNNs (Fig. 1, right). We further apply B-cosification to a pre-trained CLIP model [38], a large foundational vision-language model (VLM), and show that despite using comparatively limited data and compute cost, B-cosified CLIP models yield highly interpretable explanations whilst being competitive on zero-shot performance across a variety of downstream datasets. Our work thus opens a new perspective on how to design inherently interpretable models in a cost-effective manner. Importantly, on the one hand it highlights that conventional models might be closer to inherently interpretable models than previously understood. On the other hand, it highlights the benefits of designing inherently interpretable models via minor architectural modifications, such as e.g. the B-cos DNNs, as this can allow for leveraging the large array of existing, pre-trained DNNs. Fig. 1: B-cosification: Obtaining inherently interpretable models with competitive accuracy at low cost. Left: Accuracy progression over epochs for a DenseNet-121 and a ViT-S, comparing B-cosified (blue) and B-cos (orange) training curves. B-cosified models achieve equivalent accuracy with a substantial reduction in training time, yielding 4.7x speedup for DenseNet-121 and 9.0x speedup for ViT-S. Right: Qualitative comparison of explanations for various images for B-cos [10] and our B-cosified models at various stages of training. Specifically, we show the dynamic linear mappings 𝐖⁢(𝐱)𝐖𝐱\mathbf{W}(\mathbf{x})bold_W ( bold_x ) computed by the models in color as in [9, 10]; note that by formulating conventional models (‘initial’ in the plot) as a specific version of B-cos models, we are able to visualise the corresponding explanations in color too, see Sec. 3.2.1 for further details. We find that after only one epoch of training, the B-cosified models exhibit similar explanations as B-cos models. In summary, our contributions are: • We propose B-cosification, a novel technique to ‘fine-tune for interpretability’, that addresses the problem of high training cost associated with obtaining inherently intepretable models such as B-cos DNNs. Our B-cosified DNNs are highly interpretable while often outperforming both standard and B-cos DNNs. • We thoroughly study different design choices to find an optimal strategy for B-cosification. • We apply B-cosification to supervised image classifiers on ImageNet [15], including both CNNs and ViTs, and show that the B-cosified variants perform on par on interpretability metrics while often outperforming in terms of accuracy. Overall, we find that B-cosifying a pre-trained black box DNN to be superior on both metrics as compared to training a B-cos DNN from scratch, while being computationally significantly cheaper. • We extend B-cosification to CLIP, a foundational VLM, and show that B-cosified CLIP remains highly competitive on zero-shot performance across a variety of downstream datasets, while also yielding similar interpretability benefits as B-cos models. Fig. 2: B-cosified CLIP Models. After B-cosifying a CLIP model and fine-tuning it according to our proposed B-cosification scheme, see Sec. 3.2, we find that it is possible to endow the model with the same level of inherent interpretability as the B-cos models proposed in [10], whilst maintaining CLIP’s zeroshot ability (see Fig. 5). The resulting linear summaries of the models (𝐖⁢(𝐱)𝐖𝐱{\mathbf{W}}({\mathbf{x}})bold_W ( bold_x )) can be visualised in color (row 3) and provide significantly more detail than GradCAM explanations (row 2), which are often used to explain conventional CLIP models."
https://arxiv.org/html/2411.00711v1,Debiasify: Self-Distillation for Unsupervised Bias Mitigation,"Simplicity bias is a critical challenge in neural networks since it often leads to favoring simpler solutions and learning unintended decision rules captured by spurious correlations, causing models to be biased and diminishing their generalizability. While existing solutions rely on human supervision, obtaining annotations of the different bias attributes is often impractical. To tackle this, we present Debiasify, a novel self-distillation approach that works without any prior information about the nature of biases. Our method leverages a new distillation loss to distill knowledge within a network; from a deep layer where complex, highly-predictive features reside, to a shallow layer where simpler yet attribute-conditioned features are found in an unsupervised manner. In this way, Debiasify learns robust, debiased representations that generalize well across various biases and datasets, enhancing worst-group performance and overall accuracy. Extensive experiments on computer vision and medical imaging benchmarks show the efficacy of our method, significantly outperforming the previous unsupervised debiasing methods (e.g., a 10.13% improvement in worst-group accuracy on Wavy Hair classification in CelebA) while achieving comparable or superior performance to supervised methods. Our code is publicly available at the following link:Debiasify.","Deep neural networks have emerged as a fundamental technology in numerous applications that profoundly impact various aspects of society, such as facial recognition [16], AI-enabled recruitment [19], and healthcare diagnostics [10, 3]. Given the significant societal implications of these algorithms, it is increasingly crucial to ensure their resilience against simplicity bias [34, 30]; in other words, these networks’ learning process should not prioritize weak predictive features over complex features that underpin the actual mechanisms of the task of interest. For instance, on the CelebA dataset [23], which is a real-world dataset where different attributes are strongly correlated, networks tend to classify hair color based on gender, frequently associating Blond Hair with Female. Such an unintended rule performs adequately across the majority of training instances but leads to unforeseen extreme errors in minority examples that lack the spurious correlation, thereby hindering the model’s ability to adapt to new testing scenarios that exhibit changes in data distributions. Effective ways for network debiasing include upweighting or upsampling of examples that lack spurious correlations [28], data augmentation [11], adversarial learning [36, 10], robust learning [30], and architecture optimization [2]. Nevertheless, most of these efforts rely on explicit bias attribute labels in their debiasing recipes. This compromises their practicality, as identifying and manually labeling the types of biases, to determine which attributes involve spurious correlations without a thorough analysis of the model and dataset, present significant challenges. Only recently, the focus has been shifted towards debiasing without the bias attribute labels. This is usually achieved by identifying the minority group within each class – flagged based on indicators such as misclassification [21], high loss [27], or sensitive representations [8], and subsequently upweighting/upsampling them during training. Despite being promising, these methods have two major drawbacks. First, they are heavily dependent on hyperparameter tuning using bias attribute information in the validation set, which might not be accessible for datasets in the real world [18]. Second, they are designed to address only a single bias attribute within a class, neglecting the potential existence of multiple bias sources within the same class; e.g., skin tone, gender, image background. Figure 1: Debiasify leverages clustering in the feature space of a shallow layer in the network to identify attribute-conditioned groups (3 groups shown) for each class (e.g., Attractive), where images in each group are clustered based on common, non-target bias attributes (e.g., Female, Smiling, etc.) To overcome the aforementioned problems, we propose Debiasify, a simple yet effective unsupervised debiasing technique via feature clustering and self-distillation. Rooted in the observations that images sharing the same label for certain bias attribute(s) (other than the target attribute) tend to have similar representations in the feature space, particularly in the shallow layers of the neural network stack [9, 36], we propose clustering the shallow layer features to identify attribute-conditioned groups for each class, as depicted in Figure 1. To exploit these groups for learning debiased representations, we introduce a novel self-distillation loss that encourages their distributions to converge while simultaneously aligning them with the distribution of their highly-predictive class features in the deep layer. In summary, our contributions are: • We introduce Debiasify, a new method for unsupervised bias mitigation through a self, deep-to-shallow, distillation technique. • We propose a hybrid loss that maintains high classification performance while effectively debiasing representations by minimizing the distance between class-specific, bias attribute-conditioned groups in the shallow layers and their corresponding class attribute-agnostic distributions in the deep layers. • We conduct experiments and ablation studies on CelebA, Waterbirds, and Fitzpatrick, benchmarking against bias-unsupervised methods, including previous SOTA: CFix [6], and the uper-bound supervised method: GDRO [31]. Our results highlight Debiasify’s superior performance, especially in worst-group accuracy."
https://arxiv.org/html/2411.00705v1,ReMatching Dynamic Reconstruction Flow,"Reconstructing dynamic scenes from image inputs is a fundamental computer vision task with many downstream applications. Despite recent advancements, existing approaches still struggle to achieve high-quality reconstructions from unseen viewpoints and timestamps. This work introduces the ReMatching framework, designed to improve generalization quality by incorporating deformation priors into dynamic reconstruction models. Our approach advocates for velocity-field-based priors, for which we suggest a matching procedure that can seamlessly supplement existing dynamic reconstruction pipelines. The framework is highly adaptable and can be applied to various dynamic representations. Moreover, it supports integrating multiple types of model priors and enables combining simpler ones to create more complex classes. Our evaluations on popular benchmarks involving both synthetic and real-world dynamic scenes demonstrate a clear improvement in reconstruction accuracy of current state-of-the-art models.","This work addresses the challenging task of novel-view dynamic reconstruction. That is, given a set of images of a dynamic scene evolving over time, the task objective is to render images from any novel view or intermediate point in time. Despite significant progress in dynamic reconstruction (Lombardi et al., 2021; Fridovich-Keil et al., 2023; Yunus et al., 2024), effectively learning dynamic scenes still remains an open challenge. The main hurdle arises from the typically sparse nature of multi-view inputs, both temporally and spatially. While tackling sparsity often involves incorporating some form of prior knowledge into the dynamic reconstruction model - either from a physical prior such as rigidity (Sorkine & Alexa, 2007), or learnable priors derived from large foundation models (Ling et al., 2024; Wang et al., 2024) - the optimal scheme for integrating these priors without compromising the fidelity of model reconstructions remains unclear. To address this issue, this paper presents the ReMatching framework, a novel approach for designing and integrating deformation priors into dynamic reconstruction models. The ReMatching framework has three core goals: i) suggest an optimization objective that aims at achieving a reconstruction solution that is closest to satisfying the prior regularization; ii) ensure applicability to various model functions, including time-dependent rendered pixels or particles representing scene geometry; and, iii) provide a flexible design of deformation prior classes, allowing more complex classes to be built from simpler ones. To support the usage of rich deformation prior classes, we advocate for priors expressed through velocity fields. A velocity field is a mathematical object that describes the instantaneous change in time the deformation induces. As such, a velocity field can potentially provide a simpler characterization of the underlying flow deformation. For example, the complex class of volume-preserving flow deformations is characterized by the condition of being generated by divergence-free velocity fields (Eisenberger et al., 2019). However, representing a deformation through its generating velocity field typically necessitates numerical simulation for integration, leading to training instability and expressivity challenges. Nevertheless, recent progress in flow-based generative models (Ben-Hamu et al., 2022; Lipman et al., 2022; Albergo et al., 2023) supports simulation-free flow training, inspiring this work to explore simulation-free training for flow-based dynamic reconstruction models. Therefore, our framework is specifically designed to integrate with dynamic reconstruction models that represent dynamic scenes directly through time-dependent reconstruction functions (Pumarola et al., 2021; Yang et al., 2023). Exploiting the simplicity offered by velocity-field-based deformation prior classes, we observe that the projection of a time-dependent reconstruction function onto a velocity-field prior class can be framed as a flow-matching problem, solvable analytically. The opportunity to access the projected flow is reminiscent of the Alternating Projections Method (APM) (Deutsch, 1992), a greedy algorithm guaranteed in finding the closest points between two sets. Therefore, we suggest an optimization objective aimed at re-projecting back onto the set of reconstruction flows. This corresponds to a flow-matching loss that we term the ReMatching loss. Our hypothesis is that by mimicking the APM, this optimization would converge to solutions that not only meet the reconstruction objective, but also reach the closest possible alignment to the required prior class. By doing so, we achieve the desired goal of improving generalization without compromising solutions’ fidelity levels. We instantiate our framework with a dynamic model based on the popular Gaussian Splats (Kerbl et al., 2023) rendering model. We explore several constructions for deformation prior classes including piece-wise rigid and volume-preserving deformations. Additionally, we demonstrate our framework’s usability for two different types of time-dependent functions: rendered image intensity values, and particle positions representing scene geometry. Lastly, we evaluate our framework on standard dynamic reconstruction benchmarks, involving both synthetic and real-world scenes, and showcase clear improvement in generalization quality."
https://arxiv.org/html/2411.00683v1,TaxaBind: A Unified Embedding Space for Ecological Applications,"We present TaxaBind, a unified embedding space for characterizing any species of interest. TaxaBind is a multimodal embedding space across six modalities: ground-level images of species, geographic location, satellite image, text, audio, and environmental features, useful for solving ecological problems. To learn this joint embedding space, we leverage ground-level images of species as a binding modality. We propose multimodal patching, a technique for effectively distilling the knowledge from various modalities into the binding modality. We construct two large datasets for pretraining: iSatNat with species images and satellite images, and iSoundNat with species images and audio. Additionally, we introduce TaxaBench-8k, a diverse multimodal dataset with six paired modalities for evaluating deep learning models on ecological tasks. Experiments with TaxaBind demonstrate its strong zero-shot and emergent capabilities on a range of tasks including species classification, cross-model retrieval, and audio classification. The datasets and models are made available at https://github.com/mvrl/TaxaBind.","Fine-grained species classification is a challenging task in computer vision, which is often necessary for ecologists to automatically label images of rare species. A related and arguably a more important task is species distribution mapping which aims to map the presence of a given species of interest. Until now, both tasks were addressed using separate frameworks and methodologies, often requiring different datasets. In this work, we propose learning a unified embedding space over six modalities that is useful for several downstream ecological tasks including but not limited to species distribution mapping, fine-grained classification, and audio classification. The presence of a particular species at a given geographic location can reveal several important characteristics of that species. Previous studies attempted to implicitly learn the relationship between geographic location and the presence of species by considering either environmental features [1] or satellite images [2, 3, 4] describing the location. This leads to learning an effective representation of any geographic location which is useful for species distribution mapping. However, this type of modeling often overlooks important species attributes, such as their taxonomic hierarchy or audio signatures. Recent works such as BioCLIP [5] and ArborCLIP [6] have demonstrated impressive zero-shot species classification capabilities. However, these frameworks are restricted to image and text modalities, ignoring crucial geographic, audio, and habitat characteristics of species. Multimodal embedding frameworks like ImageBind [7] and GRAFT [8] have shown that it is possible to learn a joint representation space by aligning all available modalities to the ground-level image modality. This allows for training modality-specific encoders using only image-paired datasets. One potential downside of such methods is that they perform locked tuning with the ground-level image modality. This means that the ground-level image encoder is kept frozen, while the other modalities are trained to project to the existing learned space of the ground-level image modality. This can lead to sub-optimal performance since task-specific unique information of each modality is lost [9]. To this end, we propose multimodal patching, building upon patching [10], a framework to distill knowledge from various modalities while still preserving the original embedding space of the binding modality. We show that multimodal patching can improve zero-shot classification performance of the binding modality. We create a joint embedding space containing six modalities (Figure 1). To facilitate future research and evaluation of ecological models, we present TaxaBench-8k, a truly multimodal dataset containing six paired modalities. The contributions of our work are fivefold: 1. Multimodal Patching. We propose a simple yet effective patching technique that improves over the ImageBind framework. 2. Multimodal Models for Ecological Applications. We propose modality-specific encoders that can handle various ecological tasks over six modalities: ground-level image, geographic location, satellite image, text, audio, and environmental features. 3. Multimodal datasets. We compiled two large-scale novel cross-view datasets: i) iSoundNat: ground-level images of species with their corresponding audio; and ii) iSatNat: ground-level images of species with their corresponding satellite imagery. 4. TaxaBench-8k. We present TaxaBench-8k, a benchmarking dataset containing six paired modalities for evaluating multimodal ecological models. 5. We demonstrate our models’ effectiveness and emergent properties on several benchmarking and zero-shot tasks."
https://arxiv.org/html/2411.00652v1,Towards High-fidelity Head Blending with Chroma Keyingfor Industrial Applications,"We introduce an industrial Head Blending pipeline for the task of seamlessly integrating an actor’s head onto a target body in digital content creation. The key challenge stems from discrepancies in head shape and hair structure, which lead to unnatural boundaries and blending artifacts. Existing methods treat foreground and background as a single task, resulting in suboptimal blending quality. To address this problem, we propose CHANGER, a novel pipeline that decouples background integration from foreground blending. By utilizing chroma keying for artifact-free background generation and introducing Head shape and long Hair augmentation (𝑯𝟐superscript𝑯2\bm{H^{2}}bold_italic_H start_POSTSUPERSCRIPT bold_2 end_POSTSUPERSCRIPT augmentation) to simulate a wide range of head shapes and hair styles, CHANGER improves generalization on innumerable various real-world cases. Furthermore, our Foreground Predictive Attention Transformer (FPAT) module enhances foreground blending by predicting and focusing on key head and body regions. Quantitative and qualitative evaluations on benchmark datasets demonstrate that our CHANGER outperforms state-of-the-art methods, delivering high-fidelity, industrial-grade results.","In the realm of modern digital content creation, Head Blending, the seamless integration of an actor’s head onto a body filmed in separate takes or contexts is a critical yet under-explored task. We focus on a such process, which is essential for various applications such as visual effects (VFX) post-production, digital human creation, and virtual avatar generation. In these scenarios, integrating an actor’s head with footage where the body and surrounding environment may differ significantly is often necessary. The main challenge of Head Blending arises from the discrepancies between the actor’s head and the target body, including differences in head shape and hair structure. These discrepancies often lead to unnatural boundaries or blending artifacts, which can be particularly problematic in professional applications where high fidelity and visual coherence are ultimate. The existing method, Head2Scene Blender (H2SB) [16], approaches this task by treating the foreground and background generation as a single process. H2SB shows unsatisfactory results (Figure 2(a), (b)), especially around the boundary regions. Although the generation region has two distinct background and foreground parts, H2SB considers the region at once, which results in an unclear border of a human and artifacts. Moreover, H2SB lacks in mimicking the cross-identity head blending and fails to cover large inpainting regions. To this end, we propose CHANGER, a novel pipeline for Consistent Head blending with predictive AtteNtion Guided foreground Estimation under chroma key setting for industRial applications. We decompose the problem into two distinct sub-tasks: background integration and foreground blending. This decomposition allows for a more focused treatment of each aspect of the task, ensuring higher fidelity in both the background and foreground. The background integration challenge is addressed by incorporating chroma keying [14], a widely used technique in content production where a uniformly colored background (e.g., a green screen) is replaced with the desired scene. This allows for flawless background generation, eliminating the artifacts that arise when the foreground and background are blended simultaneously. By decoupling the foreground blending from the background, we ensure that the visual integrity of the scene is preserved, even in complex environments. For the foreground blending, we tackle the problem of seamlessly integrating the actor’s head onto the body of the target, particularly in cases where significant differences exist in head shape and hair structure. To generate the high-fidelity foreground, we devise two contributions, one from a data-centric and the other from a model-centric perspective. First, we propose a novel data augmentation technique called Head shape and long Hair augmentation (𝑯𝟐superscript𝑯2\bm{H^{2}}bold_italic_H start_POSTSUPERSCRIPT bold_2 end_POSTSUPERSCRIPT augmentation), which simulates a wide range of head shapes and hair styles in the self-supervised training. This enables our model to better generalize to real-world variations and handle the significant visual discrepancies that often arise in professional content production. Second, we introduce the Foreground Predictive Attention Transformer (FPAT), a novel architecture designed for foreground blending. FPAT predicts the exact regions of the head and body that require attention and apply targeted attention to these areas during the blending process. By explicitly restricting the attention to these key regions, FPAT enhances the blending quality, particularly in areas where head shape and hair differences pose a challenge. To summarize, we propose the first comprehensive solution for the Head Blending task in industrial content production. Unlike the previous approach that treats this process as part of general head or face swapping, our method focuses explicitly on the seamless blending of an actor’s head with the target body, ensuring realistic and high-quality results. Our method, CHANGER, significantly outperforms state-of-the-art techniques, as demonstrated through both quantitative metrics and qualitative evaluations on benchmark datasets. Figure 3: Network overview of CHANGER. (a) We visualize how we conduct the input of the network (X𝑋Xitalic_X) at the train (blue) and the test (red). We apply H2superscript𝐻2H^{2}italic_H start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT augmentation during the training to improve the fidelity of the generated image by improving the diversity of the input. (b) We visualize the network of CHANGER. The head colorizer colorizes the gray head of X𝑋Xitalic_X, and the body blender inpaints the hidden body with a foreground mask-aware attention mechanism. Please refer to the detailed explanations of H2superscript𝐻2H^{2}italic_H start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT augmentation and FPAT in Section 3.2 and 3.3, respectively. In summary, our main contributions are as follows: • We propose CHANGER, a novel pipeline that utilizes chroma keying for the first time to decouple background integration from the head blending process, addressing the common artifacts seen in prior methods. • We introduce H2superscript𝐻2H^{2}italic_H start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT augmentation, a data-centric approach designed to handle significant variations in head shape and hair structure, enhancing the robustness of the Head Blending process. • We present the FPAT module, which uses predictive attention to focus on key regions of the head and body, resulting in high-fidelity blending with minimized artifacts. • CHANGER significantly outperforms existing methods on benchmark datasets, both quantitatively and qualitatively, showcasing its effectiveness in industrial content production scenarios."
https://arxiv.org/html/2411.00639v1,Event-guided Low-light Video Semantic Segmentation,"Recent video semantic segmentation (VSS) methods have demonstrated promising results in well-lit environments. However, their performance significantly drops in low-light scenarios due to limited visibility and reduced contextual details. In addition, unfavorable low-light conditions make it harder to incorporate temporal consistency across video frames and thus, lead to video flickering effects. Compared with conventional cameras, event cameras can capture motion dynamics, filter out temporal-redundant information, and are robust to lighting conditions. To this end, we propose EVSNet, a lightweight framework that leverages event modality to guide the learning of a unified illumination-invariant representation. Specifically, we leverage a Motion Extraction Module to extract short-term and long-term temporal motions from event modality and a Motion Fusion Module to integrate image features and motion features adaptively. Furthermore, we use a Temporal Decoder to exploit video contexts and generate segmentation predictions. Such designs in EVSNet result in a lightweight architecture while achieving SOTA performance. Experimental results on 3 large-scale datasets demonstrate our proposed EVSNet outperforms SOTA methods with up to 11×11\times11 × higher parameter efficiency.","Video semantic segmentation, a problem of assigning a category label to each pixel in the video frames, has become a hot research topic in recent years. It plays a fundamental role in a wide range of multimedia and computer vision applications including video parsing [29, 35], video processing [3, 70, 33], and autonomous driving [68, 75, 74, 23]. Figure 1: Comparison among (a) normal light frame, (b) low-light frame, (c) events generated from normal light frames, and (d) events generated from low-light frames. Events demonstrate robustness against lighting changes and effectively capture temporal motion features in low-light environments. While video semantic segmentation of normal light scenes has made tremendous achievements [48, 33, 40, 53, 73], low-light scenarios are still challenging due to limited visibility and degraded image quality. In low-light conditions, conventional frame-based cameras have difficulty capturing a wide range of brightness levels, resulting in low contrast and loss of textures. The reduced contrast hinders accurate discrimination of object boundaries, ultimately diminishing clarity and fidelity of the captured images. In addition to limited visibility, low-light semantic segmentation is also challenging because of luminance noise. Severe noises caused by constrained photon counts and imperfections in photodetectors manifest as random bright or dark pixels scattered throughout the whole image. Such noises often lead to inaccurate segmentation predictions. To resolve the above issues, researchers have explored event cameras as an alternate sensing modality. Event sensors asynchronously measure sparse data streams at high temporal resolution (10µs vs 3ms), higher dynamic range (140dB vs 60dB), and significantly lower energy (10mW vs 3W) compared to conventional cameras. In recent years, it has been increasingly utilized in the computer vision [16, 34, 5, 22, 47, 25] and robotics [57, 52] community. Researchers have explored event modality in many tasks such as 3D reconstruction [76], pose estimation [5], and simultaneous localization and mapping (SLAM) [17]. Figure 2: Different from existing approach which fuse event features in the encoder, our model utilizes event features to learn short-term and long-term motions and leverage these motion features to gain video temporal contexts. Instead of capturing an image at a fixed interval, the event cameras, such as the Dynamic Vision Sensor (DVS), only record a single event based on the brightness changes at each pixel. This makes it suitable for edge-case scenarios and video-related tasks. For low-light video semantic segmentation task, event modality offers two advantages: (1) it facilitates the learning of temporal consistency; (2) it provides an alternative perspective through intensity changes. Figure. 1 explains the potential of event modality in low-light conditions. Although bringing a new paradigm shift, event modality presents two challenges: firstly, it only captures pixels in motion, leading to sparse information; secondly, it demonstrates distinct attributes compared with visual image modality, highlighting the significance of effectively modeling event features. To address the above challenges, we propose a lightweight framework, namely EVSNet, that exploits both image and event modalities for low-light video semantic segmentation. Figure. 2 illustrates the difference between existing works and our approach. The architecture of EVSNet is shown in Figure. 3. It consists of three parts: an Image Encoder, a Motion Extraction Module, a Motion Fusion Module, and a Temporal Decoder. Specifically, we first adopt a lightweight backbone as the Image Encoder to extract image features. Inspired by Atkinson-Shiffrin memory model [2] which hypothesizes that the human memory consists of short memory and long memory, we then propose a Motion Extraction Module to extract short-term and long-term motion features and acquire the video contexts to guide semantic understanding through such motions. We apply Event Encoder to extract event features (can be seen as short-term motions between 2 consecutive frames) and leverage the Temporal Convolutional Block to learn long-term motions. Furthermore, we devise a Motion Fusion Module to integrate image and motion features adaptively. It contains a Channel Attention layer and a Spatial Attention Layer to blend cross-channel and spatial information. By leveraging both modalities, EVSNet extracts richer and complementary information, leading to more accurate segmentation compared to the single modality. Finally, we use a Temporal Decoder to exploit video contexts and generate final predictions. Extensive evaluations using three large-scale low-light datasets show EVSNet results in better semantic segmentation results. In summary, our contributions to this paper include: • We propose EVSNet, a lightweight framework that exploits image and event modality. To the best of our knowledge, we are the first to introduce event modality to video semantic segmentation task. Event information focuses on the motion changes and thus can be used to learn better short and long-term temporal consistent representations. • We propose a Motion Extraction Module (MEM) and a Motion Fusion Module (MFM) for learning temporal motion and adaptively learning the spatial and channel-wise relationship between image and motion features. Unlike existing extraction and fusion modules, our design alleviates misalignment while lowering computational cost. • We conduct experiments to evaluate our EVSNet model using three large-scale low-light video semantics segmentation datasets and demonstrate its effectiveness using standard segmentation metrics. Compared to SOTA models with similar parameter efficiency and inference cost, our EVSNet achieves superior performance on these 3 datasets."
https://arxiv.org/html/2411.00632v1,PCoTTA: Continual Test-Time Adaptation forMulti-Task Point Cloud Understanding,"In this paper, we present PCoTTA, an innovative, pioneering framework for Continual Test-Time Adaptation (CoTTA) in multi-task point cloud understanding, enhancing the model’s transferability towards the continually changing target domain. We introduce a multi-task setting for PCoTTA, which is practical and realistic, handling multiple tasks within one unified model during the continual adaptation. Our PCoTTA involves three key components: automatic prototype mixture (APM), Gaussian Splatted feature shifting (GSFS), and contrastive prototype repulsion (CPR). Firstly, APM is designed to automatically mix the source prototypes with the learnable prototypes with a similarity balancing factor, avoiding catastrophic forgetting. Then, GSFS dynamically shifts the testing sample toward the source domain, mitigating error accumulation in an online manner. In addition, CPR is proposed to pull the nearest learnable prototype close to the testing feature and push it away from other prototypes, making each prototype distinguishable during the adaptation. Experimental comparisons lead to a new benchmark, demonstrating PCoTTA’s superiority in boosting the model’s transferability towards the continually changing target domain. Our source code is available at: https://github.com/Jinec98/PCoTTA.","Recent advancements in 3D point cloud understanding have marked a significant leap in the field of computer vision [20, 38, 52, 41] and 3D processing [9, 8, 12, 29]. Current methods [34, 47] primarily concentrate on training and testing on a single domain [33, 19]. Nevertheless, they encounter noticeable performance drops on other target data. Different datasets have domain gaps, also known as domain shifts. For instance, models trained on meticulously structured synthetic data, such as ModelNet40 [51], may encounter difficulties in adapting to intricate and noisy real-world data, such as ScanObjectNN [42]. Figure 1: (a) Previous UDA approaches on point cloud suffer from catastrophic forgetting and error accumulation toward the continually changing target domains. (b) In contrast, we present an innovative framework PCoTTA to address these issues, enhancing the model’s transferability. To mitigate domain shifts, recent researchers have introduced Unsupervised Domain Adaptation (UDA) techniques [64, 27, 63, 65, 62] into point cloud understanding. Some studies synthesize diverse training data [56, 44, 61], and others leverage adversarial learning [36, 60, 26], pseudo labeling [53, 50, 54, 18, 37], consistency learning [44, 48, 30, 49], feature disentanglement [22] or self-supervised learning [1, 39, 66, 24] to align the latent features across different domains. Nonetheless, these methods still face challenges especially when the target domain is streaming online and the whole training set of the target domain is inaccessible. As such, Test-Time Adaptation is introduced into point cloud [21, 16, 17] where the model can adapt to target distributions in an online manner at test-time without requiring any prior knowledge of the whole target domain. However, these methods may still fail when the target domain is continually changing, referred to as Continual Test-Time Adaptation (CoTTA), and such an open problem is rarely explored in point cloud understanding contexts. On the one hand, due to the lack of specific designs for 3D data, current CoTTA methods [45, 10, 40, 31, 2, 46, 13, 32, 58] that are designed for 2D images are inapplicable to 3D point cloud tasks or exhibit less desired performance. On the other hand, few works like MM-CCTA [3] target the CoTTA problem in 3D point cloud tasks. Although MM-CTTA [3] designs a Continual Cross-Modal Adaptive Clustering (CoMAC) approach for 3D semantic segmentation, it suffers from two primary limitations: (1) it is specifically designed for one task only, and cannot handle other point cloud tasks such as point cloud reconstruction, denoising, and registration. Redesigning and retraining a CoTTA method for each task is cost-expensive. (2) The adapted model would inevitably forget the previously learned data (catastrophic forgetting) and accumulate the model errors (error accumulation) during the continual adaptation, limiting the model’s transferability toward the target domains. Motivated by the above analysis, we present PCoTTA, an innovative, pioneering framework for Continual Test-Time Adaptation (CoTTA) in multi-task point cloud understanding, enhancing the model’s transferability towards the continually changing target domain. Also, we introduce a multi-task setting for PCoTTA, which is practical and realistic, handling multiple tasks within one unified model during the adaptation. In particular, given an off-the-shelf model pre-trained on the source domains, our PCoTTA aims to bridge the gap between the source and continually changing target domains by dynamically scheduling the shifting amplitude at test time. Our PCoTTA mainly consists of three novel modules. Firstly, to prevent catastrophic forgetting, we propose an automatic prototype mixture (APM) strategy that automatically mixes the source prototypes with the learnable target prototypes based on the automatic similarity balancing factor (ASBF), which avoids straying too far from its original source model. Secondly, to mitigate error accumulation, we present Gaussian Splatted feature shifting (GSFS) that dynamically shifts the testing sample toward the source domain based on the distance between the testing features and the shared prototype bank. In addition, we also introduce Gaussian weighted graph attention to further adaptively schedule the shifting amplitude in a learnable manner at test time. Our insight is to highlight the similarity between the target sample and its similar prototypes and suppress the dissimilar weights. It therefore mitigates the risk of catastrophic forgetting. Finally, we devise the contrastive prototype repulsion (CPR) to pull the nearest learnable prototype close to the testing feature and push it away from other prototypes, making learnable prototypes more distinguishable. Furthermore, we present a new benchmark. We meticulously select a total of 30,9543095430,95430 , 954 point cloud samples from 4444 datasets, including 2222 synthetic datasets (ModelNet40 [51] and ShapeNet [5]) and 2222 real-world datasets (ScanNet [7] and ScanObjectNN [42]), encompassing 7777 same object categories, and generate corresponding ground truth for 3333 different tasks (reconstruction, denoising, and registration). Our main contributions are three-fold: • We present PCoTTA, an innovative, pioneering, and unified framework for Continual Test-Time Adaptation (CoTTA) in multi-task point cloud understanding, enhancing the model’s transferability towards the continually changing target domain. We introduce a multi-task setting with a new benchmark for PCoTTA, which is practical and realistic in the real world. • We devise three innovative modules for PCoTTA, i.e., automatic prototype mixture (APM), Gaussian Splatted feature shifting (GSFS), and contrastive prototype repulsion (CPR) strategies, where APM avoids straying too far from its original source model, mitigating the risk of catastrophic forgetting, and GSFS dynamically shifts the testing sample toward the source model, alleviating error accumulation, and CPR pulls the nearest learnable prototype close to the testing feature and pushes it away from other prototypes. • Extensive experimental results with analysis demonstrate the effectiveness and superiority of our presented method, surpassing the state-of-the-art approaches by a large margin."
https://arxiv.org/html/2411.00630v1,STAA: Spatio-Temporal Attention Attribution for Real-Time Interpreting Transformer-based Video Models,"Transformer-based models have achieved state-of-the-art performance in various computer vision tasks, including image and video analysis. However, Transformer’s complex architecture and black-box nature pose challenges for explainability, a crucial aspect for real-world applications and scientific inquiry. Current Explainable AI (XAI) methods can only provide one-dimensional feature importance, either spatial or temporal explanation, with significant computational complexity. This paper introduces STAA (Spatio-Temporal Attention Attribution), an XAI method for interpreting video Transformer models. Differ from traditional methods that separately apply image XAI techniques for spatial features or segment contribution analysis for temporal aspects, STAA offers both spatial and temporal information simultaneously from attention values in Transformers. The study utilizes the Kinetics-400 dataset, a benchmark collection of 400 human action classes used for action recognition research. We introduce metrics to quantify explanations. We also apply optimization to enhance STAA’s raw output. By implementing dynamic thresholding and attention focusing mechanisms, we improve the signal-to-noise ratio in our explanations, resulting in more precise visualizations and better evaluation results. In terms of computational overhead, our method requires less than 3% of the computational resources of traditional XAI methods, making it suitable for real-time video XAI analysis applications. STAA contributes to the growing field of XAI by offering a method for researchers and practitioners to analyze Transformer models.","Video understanding has emerged as a cornerstone of modern artificial intelligence, revolutionizing critical applications from autonomous driving safety systems [1] to precision medical diagnostics [2]. The introduction of Transformer architectures, exemplified by TimeSformer [3] and ViViT [4], represents a paradigm shift in video analysis, consistently outperforming traditional convolutional neural networks (CNNs) [5, 6] across diverse benchmarks. However, this leap in performance introduces new challenges: as model architectures grow more sophisticated, their decision-making processes become increasingly opaque, necessitating advanced explainable AI (XAI) approaches [7] to ensure accountability and trust in critical applications. Video understanding presents unique challenges compared to static image analysis, primarily due to its inherent spatio-temporal complexity. While spatial features capture frame-level information such as object configurations and contextual relationships, temporal features encode crucial dynamic patterns—motion trajectories, state transitions, and temporal dependencies [8]. The significance of temporal modeling is evidenced by breakthroughs such as Temporal Segment Networks (TSN) [8], which demonstrate how precise temporal feature analysis enables the discrimination of visually similar but temporally distinct actions. This spatio-temporal interplay forms the foundation of advanced video understanding tasks, where the precise sequencing and timing of spatial transformations define complex actions [6]. Understanding and explaining these intricate spatio-temporal relationships becomes crucial as video AI systems increasingly influence high-stakes decisions in autonomous systems [1] and healthcare [2]. While significant advances have been made in developing XAI methods for computer vision [9], particularly in CNN-based image analysis [10], extending these approaches to Transformer-based video models presents unprecedented challenges. These challenges manifest in two critical dimensions: algorithmic complexity and computational efficiency, both of which fundamentally impact the practical deployment of explainable video AI systems. Algorithmic Challenges: The distinctive architecture of Transformers, centered on self-attention mechanisms, represents a fundamental departure from traditional CNNs, rendering many established XAI methods ineffective or theoretically incompatible [10, 11]. Besides, architectural divergence is compounded by the inherent complexity of video data, where temporal dynamics create additional complexity beyond what static image-based XAI methods can address [12]. Current approaches often artificially dichotomize spatial and temporal analysis, producing fragmented explanations that fail to capture the holistic nature of video understanding. This limitation creates a critical gap in our ability to explain how models interpret and process the dynamic interplay between spatial and temporal features in video content. Computational Efficiency Challenges: The computational demands of existing post-hoc explanation methods present a significant barrier to real-world deployment. These methods typically require multiple model inference passes, creating substantial computational overhead for video analysis tasks. This inefficiency becomes particularly acute in scenarios demanding real-time explanations, such as autonomous systems or live video monitoring applications [13]. The computational burden is especially problematic for long video sequences, where the resource requirements can grow exponentially [14, 15] due to the algorithm of post-hoc methods. These challenges underscore the need for XAI methods to adapt to transformer architectures, provide spatio-temporal explanations, and achieve high computational efficiency. We pose the following research questions: RQ1: Can we develop a novel XAI method for video Transformer models that simultaneously captures spatial and temporal feature explanations? RQ2: Can our proposed XAI method improve faithfulness, monotonicity, and computational efficiency compared to traditional model-agnostic methods in video analysis tasks? RQ3: Can we design and implement a cloud-based XAI service architecture that enables real-time explanations for edge-side video? To the best of our knowledge, there are several critical gaps in the current video AI explainability: (1) Despite the widespread deployment of Transformer-based video [3, 16, 9], there remains an absence of methods for video Transformer architectures [17]. (2) Current approaches provide a single type of feature explanation, failing to simultaneously capture both spatial and temporal features in video models. (3) The absence of real-time capabilities for video analysis creates a significant barrier for edge computing applications [18]. Traditional post-hoc approaches fail to address the computational demands of video analysis. These gaps collectively impede practitioners from understanding and trusting video AI systems, highlighting the urgent need for novel XAI approaches that can address both the theoretical and practical challenges of explaining video Transformer models. To address these critical gaps, we propose STAA (Spatio-Temporal Attention Attribution), a novel XAI method for video Transformer models. STAA achieves three key advantages: (1) simultaneous capture of spatial and temporal feature importance through a single forward pass, (2) improved explanation faithfulness through direct alignment with the model’s decision-making process, and (3) significantly reduced computational overhead by utilizing the model’s internal attention mechanisms rather than requiring multiple inference passes. In this paper, the main contributions are: Novel XAI Architecture: STAA fundamentally addresses the architectural mismatch between current XAI methods and video Transformer models. Empirical Validation: Rigorous evaluation of STAA against adapted versions of SHAP and LIME on the Kinetics-400 dataset, demonstrating significant improvements in both explanation quality and computational efficiency. Real-Time XAI Framework: Design and implementation of a cloud-based service architecture that enables real-time video explanations with sub-100ms latency, making STAA practical for edge computing applications. The remainder of this paper is organized as follows: Section II reviews related work in video Transformers and current XAI approaches, highlighting limitations in explaining video models. Section III presents our STAA method and adaptations of SHAP and LIME for video analysis. Section IV details our experimental setup using the Kinetics-400 dataset and TimeSformer architecture, introducing metrics for assessing video XAI quality. Section V demonstrates STAA’s performance improvements in both explanation quality and computational efficiency, including our real-time implementation results. Section VI concludes with key findings and future directions."
https://arxiv.org/html/2411.00627v1,Investigating the Gestalt Principle of Closure in Deep Convolutional Neural Networks,"Deep neural networks perform well in object recognition, but do they perceive objects like humans? This study investigates the Gestalt principle of closure in convolutional neural networks. We propose a protocol to identify closure and conduct experiments using simple visual stimuli with progressively removed edge sections. We evaluate well-known networks on their ability to classify incomplete polygons. Our findings reveal a performance degradation as the edge removal percentage increases, indicating that current models heavily rely on complete edge information for accurate classification. The data used in our study is available on Github.","Neural networks were designed inspired by the working mechanism of the human brain and have since achieved remarkable success across various fields. While psychology still aims to better understand the human brain, computer science strives to enhance understanding of neural networks. The primary goal in neural network research is to develop models capable of performing similar tasks as the human brain, rather than to recreate it. Interestingly, neural networks tend to exhibit more human-like behavior than expected, despite not being explicitly designed for this purpose. In particular, recent findings suggest that CNNs may exhibit certain aspects of Gestalt laws of perceptual organization [1], which explain how the human brain interprets complex visual stimuli, albeit possibly subject to certain thresholds and limitations. The narrow scope of previously explored neural network architectures, coupled with limited datasets and insufficient experiments, necessitates a more comprehensive investigation. We focus on the Gestalt principle of closure, which states that the human brain naturally fills in gaps to perceive figures as complete wholes, when parts are occluded or fragmented. We present a dataset specifically designed to examine closure from various psychology-based perspectives and conduct experiments across a broad range of CNNs to investigate their alignment with this principle. Our work provides a comprehensive analysis of CNNs regarding closure, identifying limits and thresholds that define their applicability in performing closure over gradually manipulated stimulus classes."
https://arxiv.org/html/2411.00626v1,ZIM: Zero-Shot Image Matting for Anything,"The recent segmentation foundation model, Segment Anything Model (SAM), exhibits strong zero-shot segmentation capabilities, but it falls short in generating fine-grained precise masks. To address this limitation, we propose a novel zero-shot image matting model, called ZIM, with two key contributions: First, we develop a label converter that transforms segmentation labels into detailed matte labels, constructing the new SA1B-Matte dataset without costly manual annotations. Training SAM with this dataset enables it to generate precise matte masks while maintaining its zero-shot capability. Second, we design the zero-shot matting model equipped with a hierarchical pixel decoder to enhance mask representation, along with a prompt-aware masked attention mechanism to improve performance by enabling the model to focus on regions specified by visual prompts. We evaluate ZIM using the newly introduced MicroMat-3K test set, which contains high-quality micro-level matte labels. Experimental results show that ZIM outperforms existing methods in fine-grained mask generation and zero-shot generalization. Furthermore, we demonstrate the versatility of ZIM in various downstream tasks requiring precise masks, such as image inpainting and 3D NeRF. Our contributions provide a robust foundation for advancing zero-shot matting and its downstream applications across a wide range of computer vision tasks. The code is available at https://github.com/naver-ai/ZIM.","Image segmentation, which divides an image into distinct regions to facilitate subsequent analysis, is a fundamental task in computer vision. Recent breakthroughs in segmentation models have made significant strides in this area, particularly with the emergence of the segmentation foundation model, Segment Anything Model (SAM) [27]. SAM is trained on the SA1B dataset [27] containing 1 billion micro-level segmentation labels, where its extensiveness enables SAM to generalize effectively across a broad range of tasks. Its strong zero-shot capabilities, powered by visual prompts, have redefined the state of the art in zero-shot interactive segmentation and opened new avenues for tackling more complex tasks within the zero-shot paradigm. Despite these achievements, SAM often struggles to generate masks with fine-grained precision (see Figure 1). To address this limitation, recent studies [59, 33, 60] have extended SAM to the image matting task, which focuses on capturing highly detailed boundaries and intricate details such as individual hair strands. These approaches achieve enhanced mask precision by fine-tuning SAM on publicly available matting datasets [30, 44, 56]. However, this fine-tuning process can undermine the zero-shot potential of SAM, since most public matting datasets contain only macro-level labels (e.g.,formulae-sequence𝑒𝑔e.g.,italic_e . italic_g . , entire human portrait) rather than the more detailed micro-level labels (e.g.,formulae-sequence𝑒𝑔e.g.,italic_e . italic_g . , individual body parts), as illustrated in Figure 2. Fine-tuning with macro-level labels can cause SAM to overfit to this macro-level granularity, resulting in catastrophic forgetting of its ability to generalize at the micro-level granularity, as shown in Figure 1. Moreover, the scarcity of large-scale matting datasets with micro-level matte labels poses a significant obstacle in developing effective zero-shot matting solutions. In this paper, we introduce a pioneering Zero-shot Image Matting model, dubbed ZIM, that retains strong zero-shot capabilities while generating high-quality micro-level matting masks. A key challenge in this domain is the need for a matting dataset with extensive micro-level matte labels, which are costly and labor-intensive to annotate. To address this challenge, we propose a novel label conversion method that transforms any segmentation label into a detailed (pseudo) matte label. For more reliable label transformation, we design two effective strategies, i.e.,formulae-sequence𝑖𝑒i.e.,italic_i . italic_e . , Spatial Generalization Augmentation and Selective Transformation Learning, to reduce noise and yield high-fidelity matte labels (Section 3.1). Subsequently, we construct a new dataset, called SA1B-Matte, which contains an extensive set of micro-level matte labels generated by transforming segmentation labels from the SA1B dataset via the proposed converter (see Figure 2). By training SAM on the SA1B-Matte dataset, we introduce an effective foundational matting model with micro-level granularity while preserving the zero-shot ability of SAM (see Figure 1). To further ensure effective interactive image matting, we enhance the major bottleneck in the network architecture of SAM that impedes capturing robust and detailed feature maps. Specifically, SAM employs a pixel decoder with simple two transposed convolutional layers to generate mask feature maps with a stride of 4, which is susceptible to checkerboard artifacts and often falls short in capturing fine details. To mitigate this, we implement a more elaborated pixel decoder with a hierarchical feature pyramid design, inspired by [58], enabling more robust and richer mask feature representations (Section 3.2). Furthermore, inspired by the Mask2Former [8] framework, we introduce a prompt-aware masked attention mechanism that leads to the improvement of interactive matting performance by allowing the model to focus on regions specified by visual prompts. To validate our zero-shot matting model, we present a new test set, called MicroMat-3K, consisting of 3,000 high-quality micro-level matte labels. Our experiments on this dataset demonstrate that while SAM exhibits strong zero-shot capabilities, it struggles to deliver precise mask outputs. In contrast, existing matting models show limited zero-shot performance. ZIM, however, not only maintains robust zero-shot functionality but also provides superior precision in mask generation. Additionally, we highlight the foundational applicability of ZIM in several downstream tasks requiring precise masks, such as image inpainting [64] and 3D NeRF [6]. We hope this work provides valuable insights to the research community, encouraging further development and utilization of zero-shot matting models. Figure 2: Qualitative samples from each dataset: Public matting datasets [30, 31, 44, 56] with macro-level fine labels, the SA1B datset [27] with micro-level coarse labels, and our proposed SA1B-Matte dataset incorporating the micro-level labels with fine details."
https://arxiv.org/html/2411.00608v1,HopTrack: A Real-time Multi-Object Tracking System for Embedded Devices,"Multi-Object Tracking (MOT) poses significant challenges in computer vision. Despite its wide application in robotics, autonomous driving, and smart manufacturing, there is limited literature addressing the specific challenges of running MOT on embedded devices. State-of-the-art MOT trackers designed for high-end GPUs often experience low processing rates (<11fps) when deployed on embedded devices. Existing MOT frameworks for embedded devices proposed strategies such as fusing the detector model with the feature embedding model to reduce inference latency or combining different trackers to improve tracking accuracy, but tend to compromise one for the other. This paper introduces HopTrack, a real-time multi-object tracking system tailored for embedded devices. Our system employs a novel discretized static and dynamic matching approach along with an innovative content-aware dynamic sampling technique to enhance tracking accuracy while meeting the real-time requirement. Compared with the best high-end GPU modified baseline Byte (Embed) and the best existing baseline on embedded devices MobileNet-JDE, HopTrack achieves a processing speed of up to 39.29 fps on NVIDIA AGX Xavier with a multi-object tracking accuracy (MOTA) of up to 63.12% on the MOT16 benchmark, outperforming both counterparts by 2.15% and 4.82%, respectively. Additionally, the accuracy improvement is coupled with the reduction in energy consumption (20.8%), power (5%), and memory usage (8%), which are crucial resources on embedded devices. HopTrack is also detector agnostic allowing the flexibility of plug-and-play.","Multi-Object Tracking (MOT) aims to detect and track multiple objects in video frames while preserving each object’s unique identity across the frame sequence. This is usually accomplished by first running a detection model on a sequence of frames to identify objects, followed by a data association algorithm to link the same objects across frames. Figure 1: Performance comparison of HopTrack with baselines on embedded devices on MOT16. Circle size indicates the memory usage, while the number above the circle represents processing rate in fps. The challenge of MOT is two-fold. First, there can be drastic variations in the status and location of objects between frames, primarily caused by a low capture rate or algorithms that selectively process frames [1, 2, 3]. Second, there is the issue of occlusion111Occlusion is a common term in computer vision and means that one object is partially or fully hiding one or more other objects in the same frame. among objects in crowded scenes [4, 5]. The association between the same object across frames is typically achieved in two ways. The first approach uses a constant velocity model to predict the location of each object in frames following the detection frame [1, 6]. Then, it constructs a cost matrix based on the intersection over union (IoUs) of the actual detection bounding box in the subsequent frame and the predicted detection bounding box (i.e., complete overlap means the cost is 0, no overlap means the cost is 1). The object association across frames is formulated as a linear assignment problem, which aims to minimize the cost by associating the detected bounding box with the predicted bounding box that has the highest IoU. Framework Testing Device Benchmark Dataset Real Time222The general definition of real-time processing rate is 24 frames per second (fps) as the typical video hardware capture rate ranges from 24 to 30 fps. In this work, we refer to a processing rate of 24-30 fps on NVIDIA Jetson AGX Xavier as near real-time and ≥30absent30\geq 30≥ 30 fps as real-time. The reported fps calculation includes both detection and association latency FPS333For fair comparison, we downloaded all baseline frameworks where code was available or reimplemented them and then ran experiments on NVIDIA Jetson AGX Xavier. Only for REMOT, we used their reported metric values and used these to calculate the MOTA metric.(AGX fps) Open Source Tracker Requires Training Detector Independent SORT [1] Intel i7 @ 2.5GHz MOT15 ✔ 260 (association only) ✔ ✗ ✔ DeepSort [2, 3] NVIDIA GTX 1050 MOT16 ✗ 13.8 (5.9) ✔ ✔ ✗ JDE [7] NVIDIA Titan xp MOT16 ✗ 18.8 (9.08) ✔ ✔ ✗ StrongSort [8] Tesla V100 MOT17/20 ✗ 7.4 (3.2) ✔ ✔ ✗ ByteTrack [6] Tesla V100 MOT16/17/20 ✗ 29.6 (10.11) ✔ ✗ ✔ OCSort [9] NVIDIA RTX 2080Ti MOT17/20 ✗ 28 (10.72) ✔ ✗ ✔ RTMOVT [10] Jetson TX2 MOT16 ✗ 30 (24.1) ✗ ✔ ✗ MobileNet-JDE [11] Jetson AGX Xavier MOT16 ✗ 4.0 - 12.6 ✗ ✔ ✗ REMOT [12] Jetson Xavier NX MOT16/17 ✔ 58 - 81 ✗ ✔ ✗ HopTrack (Ours) Jetson AGX Xavier MOT16/17/20 ✔ 30.61 ✔ ✗ ✔ TABLE I: Comparison of HopTrack and other MOT methods on embedded devices as well as high-end GPUs. An alternative approach involves training a feature extractor model (embedding) that extracts deep features from the objects and uses those deep features to perform association through similarity comparison between objects across two frames [2, 3]. Recent advancements involve the fusion of detection and embedding models to produce a joint detection and embedding (JDE) model to reduce the latency [7, 8, 13]. However, these tracking methods predominantly rely on high-end GPUs. On the other hand, there are a growing number of applications, such as autonomous driving [14, 15], smart city surveillance [16, 17], and multi-robot collaboration in manufacturing [18, 19], where an accurate and fast MOT is needed but a high-end GPU is not practical due to physical, cost, and design constraints. Offloading computation to edge server or cloud [20, 21, 22, 23] is a complementary approach as it can still benefit from more efficient local processing, which we provide. Further, offloading requires stable network connections, which are not always available in our target environments. Designing an MOT system on embedded devices is challenging, because it is a resource-intensive, time-sensitive task, and the resources such as GPU power and memory are limited on these devices. Existing works such as REMOT [12], MobileNet-JDE [11], and RTMOVT [10] have attempted to address these challenges by exploiting the latency-friendly JDE architecture and performing detection on keyframes only, with tracking on the rest. However, these frameworks struggle with delivering high-quality results while meeting real-time processing needs. For example, MobileNet-JDE [11] operates at just 13 fps and RTMOVT [10] achieves only 45% tracking accuracy on the MOT16 test dataset. Our solution: HopTrack. In this paper, we present a real-time, multi-object tracking system, HopTrack, specifically designed for embedded devices. HopTrack brings three innovations to solve the problem. First, it dynamically samples the video frames (Section III-A) for detection based on the video content characteristics, e.g., complex scenes with plenty of objects and occlusions. Then, it employs two different data association strategies (Section III-B), Hop Fuse and Hop Update, for fusing the detection results with existing track results and correcting tracking errors. HopTrack uses innovative discretized static and dynamic matching techniques to analyze simple appearance features, such as pixel intensity distribution of different channels, and a trajectory-based data association method (Section III-C) that can be computed efficiently on the CPU on every frame (Section III-D) to achieve real-time, high-quality MOT. Table I shows a comparative analysis of existing frameworks on both high-end GPUs and embedded devices (Jetson AGX). Figure 1 illustrates HopTrack’s balanced performance in accuracy, processing rate, energy and memory usage comparing with baseline frameworks. Figure 11 highlights the gradual accuracy improvement on embedded devices over the years, emphasizing the need for further exploration in this area. We summarize our main contributions below. 1. We introduce HopTrack, a real-time multi-object tracking framework for embedded device that achieves 63.12% MOTA at around 30 fps on embedded device. 2. We propose a dynamic and content-aware sampling algorithm that adjusts the running frequency of the detection algorithm. 3. We present a two-stage tracking heuristic called Hop Fuse and Hop Update, which achieves an average processing speed of 30.61 fps and an average MOTA of 62.91% on MOT16, 63.18% on MOT17 and 45.6% on MOT20 datasets. 4. We release our source code and models for the community to access and build on it. We better all existing solutions for embedded devices in the accuracy or the processing speed (or both). Our evaluation across multiple datasets (MOT16, MOT17, MOT20, KITTI), on a representative embedded device (NVIDIA AGX Xavier) brings out the following insights: (i) HopTrack betters the state-of-the-art in accuracy, while maintaining real-time tracking (anything above 24 fps), with the closest competitor being Byte(Embed) [6]; (ii) Reaching this involves a subtle interplay between detection and tracking on different frames and our microbenchmarks bring out that estimating trajectories of objects of different speeds is supremely important; (iii) It is an important advantage if processing can be largely on the CPU and in parallel; (iv) One has to carefully consider the power and the execution time to determine if a MOT solution is suitable for an embedded platform — HopTrack achieves the state-of-the-art in memory, power, and energy consumption. We recognize the rapid pace of hardware development; however, these advancements are orthogonal to our research. For example, the Jetson Orin Nano (March 2023) offers 20 and 40 TOPS versions, comparable to the Xavier AGX’s 32 TOPS but at a quarter of the cost and 2.7 times smaller. Our framework remains a more affordable, space-efficient solution as new hardware becomes available. The rest of the paper is organized as follows. In Section II, we provide a problem statement and discuss the challenges that motivate the development of HopTrack. Section III describes HopTrack’s framework in detail, including the algorithms of each component. In Section IV, we present the experimental results, which demonstrate the effectiveness of HopTrack in various settings on different benchmarks. Section V presents a comprehensive review of related work. Section VI discusses the implications of our findings, and their potential applications in different domains, and outlines future directions. Section VII offers concluding remarks."
https://arxiv.org/html/2411.00578v1,Federated Voxel Scene Graph for Intracranial Hemorrhage,"Intracranial Hemorrhage is a potentially lethal condition whose manifestation is vastly diverse and shifts across clinical centers worldwide. Deep-learning-based solutions are starting to model complex relations between brain structures, but still struggle to generalize. While gathering more diverse data is the most natural approach, privacy regulations often limit the sharing of medical data. We propose the first application of Federated Scene Graph Generation. We show that our models can leverage the increased training data diversity. For Scene Graph Generation, they can recall up to 20202020% more clinically relevant relations across datasets compared to models trained on a single centralized dataset. Learning structured data representation in a federated setting can open the way to the development of new methods that can leverage this finer information to regularize across clients more effectively.","Intracranial Hemorrhage (ICH) is a potentially lethal condition, which requires swift detection and treatment to improve the patients’ odds of survival [9, 12]. However, the term ICH captures a variety of situations. For instance, hypertension can cause the spontaneous rupture of a blood vessel and lead to a subarachnoidal hemorrhage under the brain. In contrast, trauma patients will more often have a subdural or epidural hemorrhage along the skull. The difference for such cases are visualized in Fig. 2. Treatment decisions remain clinically challenging as they need to be 1) patient-centered despite the diversity of manifestations of ICH and 2) swift as the patient outcome worsens shortly after ICH onset [1]. The clinical routine involves the acquisition of head CTs for diagnosis . We can employ Deep Learning (DL) on such images to support clinicians in their decisions and improve the treatment of ICH patients. Clinical centers worldwide see local shifts in disease manifestation, which makes it problematic for purely supervised representation learning to perform to its full potential. Especially with patient data privacy, available data is often scarce. Federated Learning (FedL) of visual representation has gained traction in recent years [10], as it enables learning to address diversity issues without sharing the private data. This is especially relevant for medical data, since many hospitals own data and have capacities to gather annotations, but these data are usually patient data that need to be protected. By using FedL, one can leverage the heterogeneity of the available data to improve the models’ generalizability while preserving the privacy of the patients. However, pure visual representation learning even using FedL only offers a superficial understanding of the clinical case, especially compared to the structured approach clinicians often use. Figure 1: Overview of the origin and diversity of the four datasets used for this study: INSTANCE2022, BHSD, CQ500, and a private cohort from Germany. We show the outline of ICH, the ventricle system, and midline. Bleeding 1 from INSTANCE2022, CQ500, and the private cohort all involve the ventricle system, which often serves as a buffer for other brain structures. The ventricle system can compress to absorb external pressure, or conversely it can fill with blood with possible expansion. Such changes are often accompanied by a midline shift, as in the samples of the INSTANCE2022, BHSD and private cohort datasets. Additionally, some images show the results of a previous surgical operation such as the presence of a ventricular drainage (appearing as a white dot within the slice) or even a craniectomy, see the red arrows. Sec. 4 offers detailed statistics over these cohorts. The majority of existing DL work focuses on the detection or segmentation of ICH [3, 6, 11, 18, 19, 25, 27, 29, 30, 32]. These models are trained using centralized learning on individual datasets, and often fail to generalize well to other data distributions. While segmentation is useful for computing the volume of the hemorrhage, it is ill-suited for the detection of individual bleeding [25]. Even detecting ICH accurately is not enough from a clinical perspective, as no clinical complication caused by the bleeding are modeled. The involvement of the ventricular system through hemorrhage expansion or the bleeding-induced shift of midline can occur and are both strong predictors of poor patient outcome [18, 8, 34]. The clinical utility of DL lies in analyzing the structure of the clinical cerebral scene using a specialized representation. Recently, Voxel Scene Graph Generation (V-SGG) [26] has shown promising results in modeling the clinical cerebral scene through a structured representation incorporating both ICH localization and the relations between ICH and adjacent brain structures. Likewise to other studies using Centralized Learning, the models detected 24242424% fewer relations when evaluated for Scene Graph Generation on an external cohort with a tangible data shift. We introduce Federated Voxel Scene Graph Generation. Motivated by Neural Motifs [35] and Iterative Message Passing [31], we propose the Fed-MOTIF and Fed-IMP methods, which learn a common relation distribution across clients in a federated setup and to minimize the bias towards client-local distributions. We validate our methods on four datasets originating from all over the world. Fig. 1 gives an overview of the data origins, as well as how anatomically dissimilar two ICH cases can be. Nevertheless, clinical decisions still depend on the same set of complex relations, independently of the precise ICH manifestation. Our models trained with FedL can recall up to 20202020% more clinically relevant relations compared to models trained on a single centralized dataset for Scene Graph Generation. With this work, we pioneer Federated Voxel Scene Graph111Code available at https://github.com/MECLabTUDA/VoxelSceneGraph, which generalizes across four datasets sourced worldwide and offer improved ICH detection for each bleeding type. (a) Spontaneous subarachnoid hemorrhage (b) Intraventricular hemorrhage (c) Intraparenchymal hemorrhage (d) Epidural hemorrhage in a trauma patient. Figure 2: Examples of the diversity in manifestation of ICH. The outline of the bleeding is shown in yellow. Hemorrhages such as in (a) may require a surgical intervention to repair any ruptured blood vessel or the placement of a drainage to relieve pressure. Similarly, involvement of the ventricular system as in (b) can cause occlusive hydrocephalus and will also require a drainage for the accumulating cerebrospinal fluid. Intraparenchymal (c) and epidural (d) hemorrhages, while dissimilar in appearance, can both cause midline shifts (c and d). Such a shift is associated with increased intracranial pressure and may require surgery."
https://arxiv.org/html/2411.00576v1,Handheld Video Document Scanning: A Robust On-Device Model for Multi-Page Document Scanning,"Document capture applications on smartphones have emerged as popular tools for digitizing documents. For many individuals, capturing documents with their smartphones is more convenient than using dedicated photocopiers or scanners, even if the quality of digitization is lower. However, using a smartphone for digitization can become excessively time-consuming and tedious when a user needs to digitize a document with multiple pages.In this work, we propose a novel approach to automatically scan multi-page documents from a video stream as the user turns through the pages of the document. Unlike previous methods that required constrained settings such as mounting the phone on a tripod, our technique is designed to allow the user to hold the phone in their hand. Our technique is trained to be robust to the motion and instability inherent in handheld scanning. Our primary contributions in this work include: (1) an efficient, on-device deep learning model that is accurate and robust for handheld scanning, (2) a novel data collection and annotation technique for video document scanning, and (3) state-of-the-art results on the PUCIT page turn dataset.","The use of smartphone cameras for document digitization, whether through the default camera or a dedicated app, has become commonplace. While dedicated scanners can achieve higher quality scans and specialized feed-forward scanners are significantly faster, the convenience and availability of using a smartphone are often necessary, particularly for remote workers. Much research has been conducted to bridge the gap between dedicated document scanners and smartphone document scanning, including: improved OCR and Handwriting Recognition for camera-captured documents[1], automatic document dewarping[2][3], robust document binarization[4], shadow removal[5], automatically finding the corners and edges of a document page[6], and automatic capture of multiple document pages from a video stream[7][8]. From a practical perspective, another important consideration for smartphone document digitization is the need for efficient, on-device machine learning models. As recent research increasingly makes use of deep learning, many models are too computationally expensive to run on smartphones. While hosting large models behind an API may work in some cases, server costs, network availability, and latency may make such models impractical. In this work, we propose a novel approach to automatically scan multi-page documents from a video stream as the user turns through the pages of a document. The general process is shown in Figure 1 where our system classifies video frames as: capture frames, page change frames, and problematic frames (in this case a hand obscuring content). Our system is implemented using efficient on-device models so the classification is real-time, giving the user live feedback during the capture. Our primary contributions in this work are: 1. An efficient on-device, cascading model for video capture as a live-streaming experience as opposed to post-processing of an entire video. 2. A novel approach to efficiently collect and annotate document scanning videos. 3. State-of-the-art results on the PUCIT Page Turn dataset."
https://arxiv.org/html/2411.00561v1,Automated Classification of Cell Shapes: A Comparative Evaluation of Shape Descriptors,"This study addresses the challenge of classifying cell shapes from noisy contours, such as those obtained through cell instance segmentation of histological images. We assess the performance of various features for shape classification, including Elliptical Fourier Descriptors, curvature features, and lower-dimensional representations. Using an annotated synthetic dataset of noisy contours, we identify the most suitable shape descriptors and apply them to a set of real images for qualitative analysis. Our aim is to provide a comprehensive evaluation of descriptors for classifying cell shapes, which can support cell type identification and tissue characterization—critical tasks in both biological research and histopathological assessments.","Cell morphology is an essential factor in the study, diagnosis, prognosis, and treatment of diseases, since morphological abnormalities can signal pathological conditions [1, 2]. Cell shape (i.e., the outline of a cell) is a critical component of cell morphology, closely related to cellular function and behavior. In both biological research and clinical settings, automated cell shape classification—when integrated with molecular and physiological data, along with other morphological characteristics—can aid in identifying cell types [3, 4, 5] and quantitatively characterizing tissue cytoarchitecture. [6, 7, 8]. In this study, we focus on the challenge of classifying cells extracted from digitized histological brain images into distinct shape classes. We consider five shape classes, as illustrated in Fig. 1: circular, elliptical/spindle-like, teardrop-like/unipolar, triangular/pyramidal, and irregular/multipolar. These classes represent the typical shapes of neuronal and glial cell bodies [9] observable in histological images stained with the Nissl method (as shown in Fig. 2), which is widely regarded as the most effective technique for highlighting cell body morphology in tissue sections [10]. Scalar features such as axis ratio and solidity have been used to characterize cell shapes. However, these features are often insufficient for consistently and accurately discriminating between shapes. While more complex solutions are available (cf. Section 2), to our knowledge, no comprehensive, quantitative comparison of shape descriptors for cell shape classification has been conducted, and this study seeks to begin addressing that gap. As outlined in Section 3, we extract multiple shape descriptors to characterize cell contours, followed by classification and feature importance evaluation. The feature set that best performs on a synthetic dataset of 100k noisy contours is tested on CytoDArk0 (an open dataset of Nissl-stained histological images of the mammalian brain) for qualitative validation, as discussed in Section 4. Fig. 1: Noisy cell shapes synthetized for model training. Fig. 2: Qualitative results on real data."
https://arxiv.org/html/2411.00560v1,Topology and Intersection-Union Constrained Loss Function for Multi-Region Anatomical Segmentation in Ocular Images,"Ocular Myasthenia Gravis (OMG) is a rare and challenging disease to detect in its early stages, but symptoms often first appear in the eye muscles, such as drooping eyelids and double vision. Ocular images can be used for early diagnosis by segmenting different regions, such as the sclera, iris, and pupil, which allows for the calculation of area ratios to support accurate medical assessments. However, no publicly available dataset and tools currently exist for this purpose. To address this, we propose a new topology and intersection-union constrained loss function (TIU loss) that improves performance using small training datasets. We conducted experiments on a public dataset consisting of 55 subjects and 2,197 images. Our proposed method outperformed two widely used loss functions across three deep learning networks, achieving a mean Dice score of 83.12% [82.47%, 83.81%] with a 95% bootstrap confidence interval. In a low-percentage training scenario (10% of the training data), our approach showed an 8.32% improvement in Dice score compared to the baseline. Additionally, we evaluated the method in a clinical setting with 47 subjects and 501 images, achieving a Dice score of 64.44% [63.22%, 65.62%]. We did observe some bias when applying the model in clinical settings. These results demonstrate that the proposed method is accurate, and our code along with the trained model is publicly available.","Myasthenia gravis (MG) is a chronic rare disease caused by neuromuscular transmission disorder [1]. With timely symptom recognition and treatment, patients can significantly improve their quality of life. MG has five types: ocular, mild systemic, moderate systemic, acute severe, delayed severe, and muscular atrophy [2]. Ocular Myasthenia Gravis (OMG) is the mildest form, involving only eye muscle weakness [3], with symptoms such as eyelid drooping or diplopia [4]. An automated method to identify ocular regions can help quantify disease severity, aiding in diagnosis and large-scale analysis. Some related research, such as Aayush et al.[5] and Rot et al.[6], has proposed models for eye structure segmentation in ocular image; however, these works primarily focus on healthy subjects. Currently, no open-source tool exists for this purpose. Deep learning-based segmentation methods can achieve high performance by learning powerful feature representations and optimizing pixel-level accuracy [7]. However, these methods often face limited training data due to high data collection costs [8] and may overlook biological structures, causing inconsistencies in segmentation results [9]. Topology-based loss functions have been proposed to preserve relationships between regions in medical images [10, 11]. Fu et al. [11] also showed improved performance using topological loss with small training sets. Despite this, few methods consider the intersections and relationships between regions, which is crucial for segmenting eye regions where the iris surrounds the pupil, and both the pupil and sclera are mutually exclusive. We propose a new loss function that leverages topology constraints and the intersection and union relationships between eye regions to enhance iris segmentation performance, particularly with small training sets. The method integrates with pixel-level loss functions like cross-entropy or Dice loss and was evaluated using three deep learning models. Developed on a public dataset with healthy subjects, it was tested on both public and clinical datasets. Results consistently showed performance improvements across different training set sizes."
https://arxiv.org/html/2411.00553v1,Is Multiple Object Tracking a Matter of Specialization?,"End-to-end transformer-based trackers have achieved remarkable performance on most human-related datasets. However, training these trackers in heterogeneous scenarios poses significant challenges, including negative interference – where the model learns conflicting scene-specific parameters – and limited domain generalization, which often necessitates expensive fine-tuning to adapt the models to new domains. In response to these challenges, we introduce Parameter-efficient Scenario-specific Tracking Architecture (PASTA), a novel framework that combines Parameter-Efficient Fine-Tuning (PEFT) and Modular Deep Learning (MDL). Specifically, we define key scenario attributes (e.g., camera-viewpoint, lighting condition) and train specialized PEFT modules for each attribute. These expert modules are combined in parameter space, enabling systematic generalization to new domains without increasing inference time. Extensive experiments on MOTSynth, along with zero-shot evaluations on MOT17 and PersonPath22 demonstrate that a neural tracker built from carefully selected modules surpasses its monolithic counterpart. We release models and code.","Video Surveillance is essential for enhancing security, supporting law enforcement, improving safety, and increasing operational efficiency across various sectors. In this respect, Multiple Object Tracking (MOT) is a widely studied topic due to its inherent complexity. Nowadays, MOT is commonly tackled with two main paradigms: tracking-by-detection (TbD) [3, 51, 61, 28, 43, 38, 29] or query-based tracking [56, 58, 63, 12] (i.e., tracking-by-attention). Although tracking-by-detection methods have proven effective across multiple datasets, their performance struggles to scale on larger datasets due to the non-differentiable mechanism used for linking new detections to existing tracks. To this end, query-based methods are being employed to unify the detection and association phase. Nevertheless, training such end-to-end transformer-based methods presents significant challenges, as they tend to overfit specific scenario settings [37, 55] (e.g., camera viewpoint, indoor vs. outdoor environments), require vast amounts of data [63], and incur substantial computational costs. Moreover, these methods degrade under domain shifts, struggling to outperform traditional TbD methods. In light of these challenges, we propose a novel framework, Parameter-efficient Scenario-specific Tracking Architecture (PASTA), aimed at reducing the computational costs and enhancing the transfer capabilities of such models. Leveraging Parameter Efficient Fine-Tuning (PEFT) techniques [16, 34] can significantly decrease computational expenses and training time, starting with a frozen backbone pre-trained on synthetic data. However, the model may still experience negative interference [49, 50, 37, 55], a phenomenon for which training on multiple tasks (or scenarios) causes the model to learn task-specific parameters that may conflict. For instance, if the model learns parameters tailored for an indoor sports activity, it could detrimentally affect its performance on a novel outdoor scene depicting people walking. To this end, inspired by Modular Deep Learning (MDL) [35], we employ a lightweight expert module for each attribute, learn them separately, and finally compose them efficiently [17]. This approach – depicted in Fig. 1 – is akin to a chef preparing a pasta dish. Each ingredient (i.e., module) is prepared individually to preserve its unique flavor and then combined harmoniously to create a balanced dish. Moreover, as pasta must be perfectly al dente to serve as the ideal base for various sauces, the pre-trained backbone should be robust and well-tuned to serve as the foundation for the modules. These modules must be combined effectively to ensure the model performs well across diverse scenarios. Conversely, the result will be sub-optimal if incompatible modules are mixed – analogous to combining ingredients that do not complement each other. Indeed, combining contrasting modules can lead to ineffective handling of diverse tasks. Figure 1: Given a scene, we select the modules corresponding to its attributes, such as lighting and indoor/outdoor. These modules are composed and then deployed, yielding a specialized model. Notably, such a modular framework brings two advantages: it avoids negative interference and enhances generalization by leveraging domain-specific knowledge. Firstly, starting from a pre-trained backbone, we train each module independently to prevent parameter conflicts, ensuring that gradient updates are confined to the relevant module for the specific scenario. This assures that parameters learned for one attribute do not negatively impact the performance of another. Secondly, the modular approach allows us to exploit domain knowledge fully, even when encountering a novel attribute combination. Indeed, as shown in Sec. 5.5, our approach is effective even in a zero-shot setting (i.e., without further fine-tuning on the target dataset). Moreover, the selection of the modules may be done automatically or in a more realistic production environment by video surveillance operators. To evaluate our approach, we conduct extensive experiments on the synthetic MOTSynth [10] and the real-world MOT17 [8] and PersonPath22 [44] datasets. The results show that PASTA can effectively leverage the knowledge learned by the modules to improve tracking performance on both the source dataset and in zero-shot scenarios. To summarize, we highlight the following main contributions: • We propose PASTA, a novel framework for Multiple Object Tracking built on Modular Deep Learning, enabling the fine-tuning of query-based trackers with PEFT techniques. • By incorporating expert modules, we improve domain transfer and prevent negative interference while fine-tuning MOT models. • Comprehensive evaluation confirms the validity of our approach and its effectiveness in zero-shot tracking scenarios."
https://arxiv.org/html/2411.00552v1,Tracking one-in-a-million: Large-scale benchmark for microbial single-cell tracking with experiment-aware robustness metrics,"Tracking the development of living cells in live-cell time-lapses reveals crucial insights into single-cell behavior and presents tremendous potential for biomedical and biotechnological applications. In microbial live-cell imaging (MLCI), a few to thousands of cells have to be detected and tracked within dozens of growing cell colonies. The challenge of tracking cells is heavily influenced by the experiment parameters, namely the imaging interval and maximal cell number. For now, tracking benchmarks are not widely available in MLCI and the effect of these parameters on the tracking performance are not yet known.Therefore, we present the largest publicly available and annotated dataset for MLCI, containing more than 1.41.41.41.4 million cell instances, 29292929k cell tracks, and 14141414k cell divisions. With this dataset at hand, we generalize existing tracking metrics to incorporate relevant imaging and experiment parameters into experiment-aware metrics. These metrics reveal that current cell tracking methods crucially depend on the choice of the experiment parameters, where their performance deteriorates at high imaging intervals and large cell colonies. Thus, our new benchmark quantifies the influence of experiment parameters on the tracking quality, and gives the opportunity to develop new data-driven methods that generalize across imaging and experiment parameters. The benchmark dataset is publicly available at https://zenodo.org/doi/10.5281/zenodo.7260136.","Detecting objects, segmenting their visual appearance into pixel-precise masks and tracking their movement through time is a fundamental challenge of computer vision providing crucial scene understanding necessary for autonomous driving [11], pedestrian management [27], sports or robotics [11]. Especially, in biomedical imaging, tracking the development of individual living cells allows gaining insights into the basic principles of life and diseases. For instance, single-cell tracking allows studying virus infections [34], pathogenic bacteria [15], cell aging [26], and cell interactions [41, 14] at the single-cell level. In particular, microbial live-cell imaging (MLCI) is a technology that performs high-throughput screening of the temporal developments of individual cells (see Figure 1). Herein, living microbial cells are introduced into microfluidic chip devices and trapped within thousands of micrometer-sized microfluidic structures called cultivation chambers. Within these structures, the cells grow in monolayers while their temporal development is recorded using automated microscopy. The microscope scans the cultivation chambers one by one, takes an image and repeatedly performs this within a loop, recording a time-lapse that captures the temporal development of the independent cell colonies. As a result, a single MLCI experiment records dozens of time-lapses and produces hundreds of gigabytes of raw imaging data. Clearly, automated segmentation and tracking methods are essential to extract information from the time-lapse images and to gain insights into microbial colony development and single-cell behavior. Figure 1: Data acquisition in microbial live-cell imaging. A microfluidic cultivation device is mounted to an automated microscope (A, B). The device contains hundreds to thousands of rectangular cultivation chambers (C) that are imaged one-by-one, moving the microscope stage and capturing images at a series of time-points (D). This imaging-and-movement loop, indicated by the purple line (C), is repeated with a pre-set imaging interval and leads to time-lapse recordings capturing the temporal development of the cell colony from a few cells up to several thousands (E). The images in (E) depict a 80×90⁢μ⁢m8090𝜇𝑚80\times 90\mu m80 × 90 italic_μ italic_m region. The figure is adapted from Blöbaum et al. [4]. However, tracking living microbial cells presents unique challenges distinct from those usually encountered in general object tracking. First, living microbial cells divide frequently, with division times ranging from a few minutes to hours. In MLCI experiments, few cells grow exponentially into dense and large colonies with up to thousands of cells captured in a single microscopy image (Figure 1E) while their total number is limited by the size of the cultivation chamber. Second, microbial cells in phase-contrast microscopy are visually hard to distinguish, making it hard to track them by their appearance (see Figure 1D). Third, the time-lapse recordings are affected by parameters such as the choice of the imaging interval between two consecutive phase contrast images and the number of concurrently monitored cultivation chambers. Moreover, both parameters are interdependent: lower imaging intervals usually simplify the tracking challenge but enforce shorter movement cycles of the microscope and, consequently, limit the number of concurrently monitored cultivation chambers. Notably, the imaging procedure itself may influence the growth behavior of the cells. While phase contrast imaging is not considered to impact microbial growth, fluorescence imaging may lead to phototoxicity and -bleaching effects [16]. Therefore, MLCI experiments are usually conducted with relatively low imaging rates. This leads to frequent cell divisions and larger cell displacement between consecutive frames. Consequently, we argue that MLCI not only needs robust and highly automated cell tracking, but also informed choices of experiment parameters, namely the imaging interval and the upper cell count limit. In recent years, the rapid development in deep learning (DL) methods has driven segmentation and tracking methods [13, 20, 28, 9, 44, 43, 42, 19]. Moreover, the increased availability of large-scale datasets such as ImageNet [29], KITTI [11], CityScapes [6], SA-1B [19], and LAION-5B [31] has shown to be a crucial driver for method development. Within the life sciences, we have seen a similar rapid development of methods. DL segmentation approaches have been developed [7, 35, 18, 40, 30], driven by annotated datasets [39, 35, 5, 8, 32, 12]. For cell tracking, special methods have been developed that incorporate cell divisions[32, 17, 36, 23, 10, 22, 25]. However, public datasets for cell tracking [39, 32, 1] primarily focus on microscopy images with eukaryotic cells. In contrast to microbial cells, eukaryotes usually show more distinctive visual features, less frequent cell division and the images contain fewer cell instances. For microbial cells, only few tracking datasets [26] are available, making it difficult to train data-driven tracking methods and benchmarking suitable experiment parameters. Due to the lack of datasets, the importance of the experiment parameters for high-quality automated tracking has not been investigated before. Therefore, we establish a novel benchmark for cell tracking in MLCI and extend existing tracking metrics with experiment parameters to quantify their effect on the tracking performance. Our contributions are threefold: (1) we introduce a new annotated time-lapse dataset, recorded with low imaging interval for the segmentation and tracking of Corynebacterium glutamicum cells containing roughly 1.41.41.41.4 million cell masks and about 14141414k cell divisions. (2) We introduce experiment-aware metrics that extend existing metrics and incorporate the choice of the imaging interval and the maximum number of cells into the evaluation. (3) We evaluate state-of-the-art (SOTA) tracking methods using our devised metrics across a broad range of experiment parameters. We thereby show that the performance of SOTA tracking algorithms deteriorates, especially at lower imaging rates and higher cell counts. Notably, this fact that has not yet been quantified by the CTC community. Therefore, our benchmark represents a step forward to towards fully automated and robust data-driven microbial single-cell tracking and raises awareness about the importance of experiment parameters for cell tracking in MLCI experiments."
https://arxiv.org/html/2411.00548v1,Generative AI-based Pipeline Architecture for Increasing Training Efficiency in Intelligent Weed Control Systems,"In automated crop protection tasks such as weed control, disease diagnosis, and pest monitoring, deep learning has demonstrated significant potential. However, these advanced models rely heavily on high-quality, diverse datasets, which are often scarce and costly to obtain in agricultural settings. Traditional data augmentation techniques, while useful for increasing the volume of the dataset, often fail to capture the real-world variability and conditions needed for robust model training. In this paper, we present a new approach for generating synthetic images for improved training of deep learning-based object detection models in the context of intelligent weed control. The presented approach is designed to improve the data efficiency of the model training process. The architecture of our GenAI-based image generation pipeline integrates the Segment Anything Model (SAM) for zero-shot domain adaptation with a text-to-image Stable Diffusion Model, enabling the creation of synthetic images that can accurately reflect the idiosyncratic properties and appearances of a variety of real-world conditions. We further assess the application of these synthetic datasets on edge devices by evaluating state-of-the-art lightweight YOLO models, measuring data efficiency by comparing mAP50 and mAP50-95 scores among different proportions of real and synthetic training data. Incorporating these synthetic datasets into the training process has been found to result in notable improvements in terms of data efficiency. For instance, most YOLO models that are trained on a dataset consisting of 10% synthetic images and 90% real-world images typically demonstrate superior scores on mAP50 and mAP50-95 metrics compared to those trained solely on real-world images. This shows that our approach not only reduces the reliance on large real-world datasets but at the same time also enhances the models’ predictive performance. The integration of this approach opens opportunities for achieving continual self-improvement of perception modules in intelligent technical systems.","In the transformation towards more sustainable agriculture, the adoption of smart technology for crop protection is crucial to minimize the application of pesticides. Here, deep learning (DL)-based algorithms embedded in intelligent agricultural technology systems emerge as a promising frontier in the area of automated crop protection [1]. DL facilitates these tasks through a range of computer vision techniques, including image classification, object detection, and segmentation. A vast amount of data, however, is a prerequisite for the satisfactory performance of DL models. In agricultural scenarios, high-quality labeled open-source datasets reflecting the heterogeneity present, for instance, in the various field conditions are yet scarce. Moreover, collecting and annotating images for new datasets is a highly labor-intensive and expensive process [2].To alleviate this data scarcity issue, data augmentation is a popular tool in DL to increase both the volume of and variation from the available datasets [3]. Different approaches to data augmentation exist. Classical operations such as flipping, rotating, and blurring of images are straightforward to implement and use but lack the important property of introducing real variability into the training data. This is, however, important to yield robustly working models generalizing well to unseen scenes. Recent approaches such as those proposed in [4] deal with generating artificial training data. We provide a brief overview in Section 2. This article expands upon our previous work presented at the Architecture of Computing Systems (ARCS) conference [5], in which we developed a novel pipeline architecture for synthetic image generation adapted to weed detection, with applicability to other object detection tasks. Our approach combined the zero-shot transfer technique with the Segment Anything Model (SAM) [6] and the Stable Diffusion Model [7]. This integration aimed to generate synthetic image data that not only mimics the inherent style of real images but also enhances the natural variation within image datasets used to train deep neural network models. Validation was carried out using a sugar beet weed detection dataset from a current research project on smart weed control 111https://www.photonikforschung.de/projekte/sensorik-und-analytik/projekt/hopla.html (accessed on April 22, 2024) (see Acknowledgement 6), which exhibits distinctive characteristics of weeds and background conditions from experimental settings. In this paper, we extend our prior work [5] by offering an in-depth evaluation of data efficiency through the gradual substitution of real-world images with synthetic counterparts in training lightweight You Only Look Once (YOLO) models [8], intending to deploy for real-time weed detection tasks. Additionally, we perform an extensive comparison of synthetic versus real-world image quality using various no-reference image quality assessment (NR-IQA) metrics. Furthermore, we discuss a newly implemented automated annotation process incorporated into our pipeline, utilizing the full potential of the YOLOv8x (extra-large) model to effectively annotate synthetic images, thus enhancing our GenAI-based pipeline infrastructure for intelligent weed management systems. Next to a brief purely qualitative comparison of the generated synthetic images with the real images, we conduct a quantitative image quality assessment as well as a comprehensive task-specific evaluation of the downstream task of weed detection in Section 3.5. For task-specific evaluation, we focused on data efficiency by targeting edge devices with less computational power for real-time weed detection tasks. Consequently, we tested the nano and small variants of the latest YOLO models to evaluate their efficiency by integrating synthetic images into the training dataset. In Section 6 we discuss our findings and briefly touch upon how our approach can be utilized to enhance the self-improvement capabilities of Organic Computing and other intelligent technical systems by introducing self-reflection. Finally, the paper is closed with an outlook on future research. The contributions of this paper are summarized as follows: 1. Novel image data augmentation methodology: We show how to combine the segment anything foundation model (SAM) with a fine-tuned Stable Diffusion Model within a dedicated data processing pipeline. This allows a) automatic transformation of an object detection dataset into an instance segmentation dataset to subsequently mask the relevant object classes and thereby get rid of the complex background while preserving the exact image shape, and, b) further use of these masks for training a text-guided image generation method, allowing to synthesize specific and tailored artificial training data. 2. Increase data efficiency: Thereby, our method facilitates increased exploitation of the available training data (next to only feeding to the training process) and is expected to allow for alleviation of the vast training data requirements of deep learning for obtaining robust models. 3. Increase autonomous learning ability: Integration of our method into intelligent technical systems architectures potentially serves as one way to increase their autonomous learning and self-improvement capabilities by including synthetic image generation and training into a continual self-reflection and learning loop."
https://arxiv.org/html/2411.00543v2,3D Equivariant Pose Regression via Direct Wigner-D Harmonics Prediction,"Determining the 3D orientations of an object in an image, known as single-image pose estimation, is a crucial task in 3D vision applications. Existing methods typically learn 3D rotations parametrized in the spatial domain using Euler angles or quaternions, but these representations often introduce discontinuities and singularities. SO(3)-equivariant networks enable the structured capture of pose patterns with data-efficient learning, but the parametrizations in spatial domain are incompatible with their architecture, particularly spherical CNNs, which operate in the frequency domain to enhance computational efficiency. To overcome these issues, we propose a frequency-domain approach that directly predicts Wigner-D coefficients for 3D rotation regression, aligning with the operations of spherical CNNs. Our SO(3)-equivariant pose harmonics predictor overcomes the limitations of spatial parameterizations, ensuring consistent pose estimation under arbitrary rotations. Trained with a frequency-domain regression loss, our method achieves state-of-the-art results on benchmarks such as ModelNet10-SO(3) and PASCAL3D+, with significant improvements in accuracy, robustness, and data efficiency.","Figure 1: Types of representations for 3D rotation prediction. Existing methods consider predicting 3D rotations in the spatial domain. Our method predicts Wigner-D coefficients in the frequency domain, to obtain accurate pose in continuous space using an SO(3)-equivariant network. Predicting the 3D pose of objects, i.e., position and orientation, in 3D space from an image is crucial for numerous applications, including augmented reality [59], robotics [69, 63, 5, 4], autonomous vehicles [21, 50], and cryo-electron microscopy [79]. Estimating 3D orientation is particularly challenging due to rotational symmetries and the non-linear nature of rotations. In addition, unlike translations, rotations introduce unique challenges such as gimbal lock and the requirement for continuous, singularity-free representations. Existing methods often learn 3D rotations using spatial domain parameterizations like Euler angles, quaternions, or axis-angle representations, as illustrated in Figure 1. However, these parameterizations suffer from issues such as discontinuities and singularities [58, 54, 80], which can hinder the performance and reliability. SO(3)-equivariance enables accurate 3D pose estimation and improves generalization to unseen rotations. It ensures that outputs consistently change with the 3D rotation of the input, maintaining rotational consistency between the input and output across network layers. Despite its importance, many existing methods [80, 3, 52, 75, 44] often design networks without considering SO(3)-equivariance, resulting in suboptimal performance when dealing with 3D rotations. In addition, in the context of spherical CNNs [10, 16, 12, 18, 17, 36, 9] for efficient SO(3)-equivariant operations, the 3D rotation parametrization in the spatial domain is inadequate because these SO(3)-equivariant networks operate in the frequency domain. To address these challenges, we propose an SO(3)-equivariant pose harmonics regression network that directly predicts Wigner-D coefficients in the frequency domain for 3D rotation regression. Building on prior work [35, 28], our method leverages the properties of spherical CNNs [11], which operate in the frequency domain, to guarantee SO(3)-equivariant output representation. By directly regressing Wigner-D matrix coefficients, our approach eliminates the need to convert outputs into spatial representations during training, ensuring alignment with the operations of spherical CNNs. This design allows us to bypass the limitations inherent in traditional spatial parameterizations—such as discontinuities and singularities [58, 54, 80]—resulting in more precise and continuous pose estimation. We further introduce a frequency-domain MSE loss to enable continuous training of 3D rotations, with the flexibility to incorporate distributional losses [52] for effectively capturing rotational symmetries in objects. Our method achieves state-of-the-art performance on standard single object pose estimation benchmarks, including ModelNet10-SO(3) and PASCAL3D+, demonstrating high sampling efficiency and strong generalization to unseen 3D rotations."
https://arxiv.org/html/2411.00499v1,Cross-modal semantic segmentation for indoor environmental perception using single-chip millimeter-wave radar raw data,"In the context of firefighting and rescue operations, a cross-modal semantic segmentation model based on a single-chip millimeter-wave (mmWave) radar for indoor environmental perception is proposed and discussed. To efficiently obtain high-quality labels, an automatic label generation method utilizing LiDAR point clouds and occupancy grid maps is introduced. The proposed segmentation model is based on U-Net. A spatial attention module is incorporated, which enhanced the performance of the mode. The results demonstrate that cross-modal semantic segmentation provides a more intuitive and accurate representation of indoor environments. Unlike traditional methods, the model’s segmentation performance is minimally affected by azimuth. Although performance declines with increasing distance, this can be mitigated by a well-designed model. Additionally, it was found that using raw ADC data as input is ineffective; compared to RA tensors, RD tensors are more suitable for the proposed model.","Since the 21st century, while society and the economy have developed rapidly, the number of fires has also increased annually [1]. Statistics show that most firefighter casualties occur during fire rescues [2]. In fire scenes, dim environments and heavy smoke severely limit the environmental perception of firefighters, negatively impacting rescue efficiency. Impaired perception can lead to disorientation and falls, causing injuries or fatalities, especially in complex indoor environments. Additionally, the air respirators for firefighters typically last only 30 minutes [3], after which they face the risk of asphyxiation or poisoning, making rescue efficiency even more critical. Therefore, enhancing the environmental perception in fire scenes is essential to reducing casualties and improving rescue efficiency. Indoor fire scene perception aims to use sensors to efficiently and accurately detect and visualize the environment, providing vital information for personnel and laying a foundation for post-processing applications. Currently, cameras [4] and LiDAR [5].are the primary tools for indoor environment perception. However, in smoke-filled and dim environments, cameras fail to properly expose and capture images, and LiDAR suffers from scattering in smoke [6], making both unsuitable for fire scenes.. In recent years, Frequency Modulated Continuous Wave (FMCW) mmWave radar emerged as a promising solution for environmental sensing in autonomous driving [7], advanced driver-assistance systems (ADAS) [8], and automatic emergency braking (AEB) [9]. Besides its low cost, its robustness in harsh conditions like fog and smoke [10] makes it particularly suitable for fire scene perception. Essentially, indoor environmental perception forms the basis for mapping [11, 12], localization [13, 14], trajectory tracking [15, 16], and navigation [17]. For fire scenes, where traditional sensors fail, mmWave radar must independently perform environment perception. However, mmWave radar is limited by hardware constraints and signal processing methods The point clouds generated by mmWave radar are significantly inferior to those of LiDAR, not only being sparse and unstable [18] but also prone to interference from multipath effects [19], particularly with single-chip mmWave radar. These limitations greatly hinder its application in indoor environment perception. Various radar data processing methods have been proposed to improve point cloud accuracy. Constant False Alarm Rate (CFAR) detectors are widely used in point cloud generation, designed to detect target signals amid noise [20]. However, CFAR algorithms, such as cell averaging (CA) CFAR [21], greatest of (GO) CFAR [22], ordered statistic (OS) CFAR [23], etc. are prone to false alarms and missed detections. Weak signals may contain rich environmental information, while strong signals could be cluttered. In complex indoor environments with multiple reflections, the detection performance of CFAR degrades significantly, and it cannot effectively eliminate multipath effects. Direction of Arrival (DOA) estimation is another critical step in point cloud generation. Super-resolution algorithms like multiple signal classifier (MUSIC) [24], iterative adaptive approach (IAA) [25], etc. offer better accuracy than traditional Fast Fourier Transform (FFT)-based azimuth estimation methods, but they have high computational complexity and limited super-resolution capabilities. Recent advances in machine learning have introduced learning-based methods for DOA estimation and radar detection. Brodeski et al. [26] a used an RCNN model for DOA estimation of single targets, and Cheng et al. [18] developed RPDNet, a radar detector based on Range-Doppler Maps (RDM), which improves point cloud accuracy and density while mitigating multipath effects. However, the precision of point clouds still lags behind LiDAR. Semantic segmentation of navigable space, instead of point cloud-based obstacle detection, is another promising approach. Orr et al. [27] proposed a cross-modal semantic segmentation model based on deep neural networks (DNNs). This model takes raw RD tensor as input and directly performs navigable space segmentation in the overlapping field of view (FoV) with camera images, demonstrating the potential of DNNs to interpret abstract mmWave radar signals and selectively filter them. Jin et al. expanded this work by adding multi-class functionality [28] and introducing a transformer structure [29, 30]. While these efforts were conducted in outdoor driving scenarios, this idea also holds great potential for indoor environmental perception. To achieve indoor fire scene perception, common indoor environment perception must first be realized, which is the focus of this paper. Point clouds are not the only solution for rescue operations. Direct segmentation of navigable space offers more intuitive guidance for personnel, eliminating the need for sparse point cloud-based obstacle detection and path planning. However, applying cross-modal semantic segmentation to indoor environment perception tasks faces several key obstacles. Previous studies primarily used high-resolution automotive radars, while the potential of the more affordable single-chip mmWave radar remains unexplored. Compared to high-resolution radars, single-chip radars have fewer antennas, providing less environmental data. Moreover, indoor environments are more complex, posing significant challenges for cross-modal models. The design of segmentation tasks and labels is crucial, as it directly affects model performance. This paper addresses these challenges and contributes as follows: • The feasibility of applying a cross-modal semantic segmentation model based on single-chip mmWave radar for indoor environment perception has been validated. A U-Net-based semantic segmentation model was proposed, demonstrating excellent performance on this task. • To reduce labor and time costs, standardize the labeling process, and improve label quality, an efficient automatic labeling method based on LiDAR point clouds and occupancy grid maps was introduced and implemented. • The point cloud-based environmental perception method and the proposed segmentation-based method were intuitively compared and evaluated. The characteristics of the proposed model and method for the segmentation task were thoroughly discussed from the perspectives of distance, azimuth, and scene complexity. A detailed analysis of their respective advantages and disadvantages was provided, along with suggestions for future optimization. Furthermore, the model’s adaptability to different types of input data was also explored. It is important to note that this paper focuses on the feasibility of using segmentation models for indoor perception, with factors related to smoke left for future exploration. The remainder of this paper is structured as follows: Section 2 introduces the dataset and LiDAR-based labeling method. Section 3 describes the semantic segmentation model, and Section 4 outlines the training details. Section 5 compares the traditional point cloud method with the proposed method, evaluating model performance and discussing characteristics and ablation experiments. Finally, Section 6 concludes the paper."
https://arxiv.org/html/2411.00485v1,Zoom Out and Zoom In: Re-Parameterized Sparse Local Contrast Networks for AirborneSmall Target Detection,"Drone-based target detection presents inherent challenges, such as the high density and overlap of targets in drone-based images, as well as the blurriness of targets under varying lighting conditions, which complicates identification. Traditional methods often struggle to recognize numerous densely packed small targets under complex background. To address these challenges, we propose LAM-YOLO, an object detection model specifically designed for drone-based. First, we introduce a light-occlusion attention mechanism to enhance the visibility of small targets under different lighting conditions. Meanwhile, we incroporate incorporate Involution modules to improve interaction among feature layers. Second, we utilize an improved SIB-IoU as the regression loss function to accelerate model convergence and enhance localization accuracy. Finally, we implement a novel detection strategy that introduces two auxiliary detection heads for identifying smaller-scale targets.Our quantitative results demonstrate that LAM-YOLO outperforms methods such as Faster R-CNN, YOLOv9, and YOLOv10 in terms of mAP@0.5 and mAP@0.5:0.95 on the VisDrone2019 public dataset. Compared to the original YOLOv8, the average precision increases by 7.1%. Additionally, the proposed SIB-IoU loss function shows improved faster convergence speed during training and improved average precision over the traditional loss function.","Unmanned Aerial Vehicles (UAVs) provide exceptional flexibility in aerial surveying, allowing for takeoff and landing at virtually any location and enabling the rapid completion of tasks. Their operational costs are significantly lower than those of traditional aircraft and helicopters. Consequently, drone-based target detection monitors are increasingly pivotal in fields such as mining surveys [1], traffic monitoring [2], and agricultural assessments [3]. The detection of small targets in complex scenes captured by UAVs is particularly important. However, the varying altitude and angles of drone capture image, combined with environmental factors, cause targets to exhibit different sizes and lighting conditions across images. Additionally, when the imaging distance is excessively large, small targets are susceptible to clustering, overlapping, and occlusion, which can result in missed detections and false positives[4]. As illustrated in Fig. 1(a)-(d), drone-based target detection faces significant challenges. Overall, the detection of small targets in drone imagery presents several notable obstacles: 1. Diverse Target Sizes: Drone-based target detection images often include targets of varying dimensions, with small objects particularly susceptible to interference. This complicates accurate identification and reduces detection accuracy. 2. Occlusion and Overlap: Small targets frequently experience occlusion and overlap, leading to missed detections and false alarms. 3. Environmental Variability: Variations in flight position, lighting conditions, weather, and sensor noise contribute to reduced contrast and sharpness of small targets in images, further complicating detection efforts. Figure 1: The complex background faced by image target detection, the red and green bounding boxes are zoom in area: (a)Dense target of different types such as pedestrians and cars, (b)Target occlusion by other objects such as trees, (c)Strong lighting conditions of sunlight, and (d) Dim lighting conditions during nights. With advancements in deep learning techniques, convolutional neural networks (CNN) have emerged as prominent methods for target detection [5, 6], particularly the YOLO (You Only Look Once) series methods [7, 8]. To address the challenges mentioned above for drone-based target detection, several researchers have made concerted efforts in this area. Xu et al. [9] employ spatial and channel attention prior to feature extraction to retain as much target information as possible. Chen et al. [7] address the low detection rates and high miss rates by enhancing the convergence speed without increasing the model parameters, utilizing the WIoU-v2 loss function for optimization. Although Wang et al. [8] introduce a small target detection head to capture extremely small targets. However, these methods fail to consider that the addition of attention modules before the backbone network introduces more interference signals in the retained information, and whether the loss function can accurately regress between the ground truth and predicted bounding boxes ultimately affects the detection of extremely small targets. Currently, the detection of small targets in drone-based aerial images has seen significant progress. While there are still unresolved issues in feature extraction that hinder high-precision on drone-based object detection, current approaches tend to serve as remedies for specific problems rather than fundamentally addressing the core challenges. The shortcomings of existing methods are shown in several key areas:1) the problem of missed and false detections is exacerbated. In drone-based aerial images, the scale of small targets varies significantly due to factors such as occlusion and distance. Without a specialized auxiliary detection head to enhance the recognition ability of these small targets, the model may miss detections. 2) Attention linear squeeze computing: Traditional attention mechanisms, utilizing linear computing similar to pooling in Squeeze and excitation networks (SENet) [10] and Pyramid Vision Transformer (PVT) [11], unintentionally merge target features with primary background noise, diluting the target features within the background. To address these problems, we need to propose a learning paradigm that not only tackles the challenge of small targets changing size and scale with varying drone heights but also mitigates missed detections caused by lighting and occlusion affecting small targets in complex scenes through a specially designed attention mechanism. To address the complexities faced by small targets in drone aerial imagery, this study aims to improve detection accuracy through enhancements in feature extraction, detection, and training strategies. We proposes LAM-YOLO, which builds upon YOLOv8 architecture by integrating multiple Lighting-Occlusion Attention Modules (LAM). This design facilitates the cross-scale integration of deep and shallow image information, thereby enhancing the model’s capability for precise target localization. We utilizes Involution blocks to improve the interaction between feature maps of different scales, thereby allowing for the capture of subtle target characteristics. To optimize model training, we introduces an improved loss function that enhances the regression capability of predicted bounding boxes. Additionally, to address the uncertainty regarding target size, we incorporates an auxiliary detection head for small targets on top of the original detection head, thereby increasing the accuracy of detecting extremely small objects. The main contributions of this article are listed as follows: 1. Enhanced Attention Mechanism: We proposes the Lighting-Occlusion Attention Module (LAM), which includes channel attention, self-attention, and overlapping cross-attention mechanisms. Additionally, this study introduces these modules to strengthen multi-scale feature interaction, enabling the model to capture more information about small targets while also paying closer attention to occluded targets. 2. Refined Regression Loss: We integrate the SIB-IoU (Soft Intersection Bounding Box IoU) into the bounding box regression loss and utilizes scaling factors to generate auxiliary bounding boxes of varying sizes for loss computation. This approach facilitates faster and more efficient model training, thereby accelerating model convergence and improving precision 3. Auxiliary Feature Detection: To address the significant size variations of targets in drone-based images, we has designed an auxiliary feature detection strategy. Building upon the three existing detection heads in standard YOLOv8, this study adds two auxiliary detection heads specifically for extremely small targets, enhancing sensitivity and the ability to detect small objects. 4. Experimental Validation: We conducted detection experiments LAM-YOLO model. Results indicate a 7.1 % improvement in accuracy on the VisDrone2019 dataset compared to baseline YOLOv8. Meanwhile, compare experiments also demonstrating superior detection performance to other state-of-the-art methods. The remainder of this paper is organized as follows: Sec. II reviews several deep learning-based object detection methods. Sec. III provides a detailed description of the proposed model, while Sec. IV presents the experimental results. Finally, Sec. V concludes the paper."
https://arxiv.org/html/2411.00472v1,MV-Adapter: Enhancing Underwater Instance Segmentation via Adaptive Channel Attention,"Underwater instance segmentation is a fundamental and critical step in various underwater vision tasks. However, the decline in image quality caused by complex underwater environments presents significant challenges to existing segmentation models. While the state-of-the-art USIS-SAM model has demonstrated impressive performance, it struggles to effectively adapt to feature variations across different channels in addressing issues such as light attenuation, color distortion, and complex backgrounds. This limitation hampers its segmentation performance in challenging underwater scenarios. To address these issues, we propose the MarineVision Adapter (MV-Adapter). This module introduces an adaptive channel attention mechanism that enables the model to dynamically adjust the feature weights of each channel based on the characteristics of underwater images. By adaptively weighting features, the model can effectively handle challenges such as light attenuation, color shifts, and complex backgrounds. Experimental results show that integrating the MV-Adapter module into the USIS-SAM network architecture further improves the model’s overall performance, especially in high-precision segmentation tasks. On the USIS10K dataset, the module achieves improvements in key metrics such as mAP, AP50, and AP75 compared to competitive baseline models.","I-A Research Background: Underwater vision tasks are a core technology in various fields such as marine science research, resource management, and environmental protection[1]. They are particularly essential in marine ecological monitoring, seabed resource exploration, and blue carbon ecosystem studies, where underwater image processing provides crucial technical support for acquiring high-precision data. However, underwater image processing faces numerous challenges, primarily due to the unique optical characteristics of the underwater environment. As depth increases, light transmission in water gradually attenuates, with red wavelengths disappearing rapidly, resulting in severe color distortion in underwater images[2]. This phenomenon not only reduces the visual quality of the images but also increases the difficulty in detecting and recognizing targets. In addition, underwater scenes typically have low contrast and complex background structures, where the boundaries between prominent targets and the background are blurred, significantly increasing the difficulty of instance segmentation. Light scattering, the presence of suspended particles, and uneven illumination at different depths further exacerbate these issues, leading to reduced image clarity and information loss. These factors make traditional image processing techniques often ineffective in underwater environments, especially in prominent target segmentation, where model performance is often affected by the surrounding environment, making high-precision segmentation difficult to achieve. Therefore, improving the robustness and adaptability of segmentation algorithms, particularly in addressing the unique optical challenges of underwater environments, has become a crucial direction in advancing underwater vision tasks, supporting key missions in marine science and environmental protection. I-B Current Research Status: Technologies based on the Segment Anything Model (SAM) have introduced new perspectives to underwater salient instance segmentation[3]. In the literature[4], researchers proposed the USIS-SAM model and created the largest underwater salient instance segmentation dataset to date—USIS10K. This model not only inherits SAM’s powerful performance in segmentation tasks but also undergoes deep optimization based on the unique characteristics of underwater images. By introducing the Salient Feature Prompt Generator (SFPG), the model can automatically generate foreground prompts, reducing reliance on manual prompts and enhancing adaptability in complex underwater scenes. Experimental results show that the model performs excellently on the USIS10K dataset, significantly improving segmentation accuracy and robustness, effectively addressing challenges such as light attenuation, color distortion, and complex backgrounds in underwater environments. Despite the significant progress made by the USIS-SAM model in underwater salient instance segmentation, it still faces some limitations when dealing with complex underwater environments. First, when handling light attenuation, USIS-SAM struggles to maintain stable segmentation performance under different depths and lighting conditions. Particularly in deep-water environments, as red light attenuates rapidly, the problem of color distortion worsens, and the model fails to effectively adapt to these optical changes, leading to a decline in segmentation performance. Second, color distortion is also a critical issue, especially in scenes with significant light scattering and many suspended particles, where color shifts can cause the model to misjudge object boundaries and categories. Additionally, the complexity of the background also significantly affects the model’s segmentation performance. When there is high similarity or mixing between the underwater background and foreground objects, the model struggles to separate salient objects from the background, making it difficult to accurately capture object contours and shapes. Therefore, although USIS-SAM improves segmentation performance by introducing the Salient Feature Prompt Generator (SFPG), it still has shortcomings in adapting to variations in channel features, particularly in terms of segmentation accuracy and robustness in complex scenes. This indicates that USIS-SAM still requires optimization in areas such as handling changes in lighting conditions, color correction, and complex background processing to better address the diversity and challenges of underwater environments. I-C Research Objectives This study aims to develop an innovative adapter module, called the MarineVision Adapter (MV-Adapter), which integrates an adaptive channel attention mechanism. This module can dynamically adjust channel feature weights based on the characteristics of underwater images, thereby enhancing segmentation performance in complex underwater scenes. The design of the MV-Adapter addresses the challenges faced by existing SAM models in underwater environments, including light attenuation, color distortion, and salient target detection. Through this improved module, the model can significantly enhance segmentation accuracy and stability, adapting to the varying conditions of different underwater environments. 1. Proposing the MarineVision Adapter (MV-Adapter): This study designs and proposes the MV-Adapter module, which integrates an adaptive channel attention mechanism. The module can dynamically adjust the feature weights of each channel based on the optical characteristics of underwater images and diverse scene features. This enables the model to maintain stable segmentation performance under conditions of light attenuation and color distortion, making it particularly suitable for complex underwater environments. 2. Improving the model’s robustness and adaptability in complex underwater scenes: Through the adaptive adjustment mechanism of the MV-Adapter, the model can effectively cope with changes in lighting, background interference, and color shifts in complex underwater environments. Experiments show that this module significantly enhances the model’s robustness and accuracy in handling diverse underwater conditions, especially improving segmentation performance in deep water and complex background scenes. 3. Achieving outstanding performance on the USIS10K dataset: Experimental results demonstrate that the MV-Adapter module significantly outperforms existing baseline models on various metrics in the USIS10K dataset. The MV-Adapter showcases excellent segmentation performance in key metrics such as mAP, AP50, and AP75, particularly excelling in high-precision segmentation tasks, further validating its effectiveness and adaptability."
https://arxiv.org/html/2411.00462v1,Target-Guided Adversarial Point Cloud Transformer Towards Recognition Against Real-world Corruptions,"Achieving robust 3D perception in the face of corrupted data presents an challenging hurdle within 3D vision research. Contemporary transformer-based point cloud recognition models, albeit advanced, tend to overfit to specific patterns, consequently undermining their robustness against corruption. In this work, we introduce the Target-Guided Adversarial Point Cloud Transformer, termed APCT, a novel architecture designed to augment global structure capture through an adversarial feature erasing mechanism predicated on patterns discerned at each step during training. Specifically, APCT integrates an Adversarial Significance Identifier and a Target-guided Promptor. The Adversarial Significance Identifier, is tasked with discerning token significance by integrating global contextual analysis, utilizing a structural salience index algorithm alongside an auxiliary supervisory mechanism. The Target-guided Promptor, is responsible for accentuating the propensity for token discard within the self-attention mechanism, utilizing the value derived above, consequently directing the model attention towards alternative segments in subsequent stages. By iteratively applying this strategy in multiple steps during training, the network progressively identifies and integrates an expanded array of object-associated patterns. Extensive experiments demonstrate that our method achieves state-of-the-art results on multiple corruption benchmarks.","3D point cloud recognition has garnered significant interest owing to its promising implications for robotics and autonomous driving. Prevailing techniques [11, 12, 20, 13] have been predominantly designed and evaluated on clean data [22, 1], overlooking the extensive corruptions present in real-world scenarios arising from sensor inaccuracies and physical constraints and leading to suboptimal performance when models are exposed to such conditions. Therefore, enhancing the resilience of point cloud models against real-world corruption emerges as a paramount yet daunting task. Figure 1: Overall motivation. We advocate for the model to broaden its attention to diverse patterns, mitigating the tendency to overfit to localized patterns. The left segment of the figure contrasts the confusion matrices of the standard transformer with our approach. The right portion showcases the performances of both the standard transformer and our methodology when confronted with objects exhibiting similar local patterns. Tokens with high / low contributions to classification are in red / blue, respectively. Standard transformer tends to overfit to localized patterns. While our method, by modulating tokens with significant contributions, enables the model to garner features from a varied spectrum of target segments, thereby ensuring greater robustness. Due to their elevated performance, existing transformer-based models [4, 15] have emerged as the mainstream choice. Despite their effectiveness on clean datasets, these methods generally falter when faced with corrupted data. Upon closer examination, we discerned that prevailing models have a propensity to overfit to specific patterns (Fig. 1a). Such patterns can degrade in the presence of real-world corruptions, undermining the model reliability. Relying solely on these localized patterns for predictions can be fragile, especially with corrupted data. Thus, we theorize that if the model is encouraged to extract features from a broader region of the object during training, gathering more diverse perception cues, it would be more resilient, as proved in Fig. 1b. This is because, even if some local patterns are compromised in corrupted data, the model could still source information from other intact areas to make accurate predictions. To address the aforementioned limitation, we present a novel adversarial representation learning method, named Target-Guided Adversarial Point Cloud Transformer(APCT), for robust 3D perception. This method adversarially weakens the dominant patterns and gathers more sub-important perception cues during training on clean samples. By progressive pattern excavation, the proposed approach prompts the model to delve into objects and acquire a broader range of patterns, consequently elevating the robustness of the model. Specifically, we first partition and extract local geometries from the point cloud by a mini-PointNet [11], resulting in a subset of tokens that encode these features. The tokens are then fed into stacked transformer blocks with two core modules, namely the Adversarial Significance Identifier and the Target-guided Promptor. The former module utilizes a dominant feature index mechanism along with an auxiliary supervision, to discern significance of all tokens, thus integrating global contextual analysis. Simultaneously, it signs a proportion of dominant tokens that are significant for perception in the current phase. Subsequently, the Target-guided Promptor, increases the dropping likelihood of token signed above during the self-attention process, thereby compelling the network to focus on less dominant tokens and to extract perceptive clues from alternative patterns. By iteratively engaging in such an adversarial process, the network gradually excavates and assimilates an extended array of patterns from objects (in Fig. 2a), thus making precise predictions against corruption. We have extensively validated the effectiveness of our proposed method on multiple benchmarks, including ModelNet-C [15] and the more challenging ScanObjectNN-C [19]. The results significantly underscore improvements in robustness and establish state-of-the-art performance on these datasets. In addition, our algorithm demonstrates pronounced generalization capabilities in downstream tasks, as evidenced by its adeptness in shape segmentation under corruptions in ShapeNet-C [14]. These findings collectively underscore the method’s generality and effectiveness in enhancing the robustness of point cloud perception models. Our contributions can be summarized as follows: • We present a new framework, APCT, which effectively improves the resilience of point cloud model against various types of corruptions by leveraging adversarial mining strategy. • We propose a novel adversarial dropout method that can stimulate the model to embrace broader valuable patterns by adversarially lifting dropout probability of specific patterns. • We demonstrate the effectiveness of APCT through extensive experiments on multiple point cloud benchmarks under diverse corruption scenarios."
https://arxiv.org/html/2411.00448v1,ConceptFactory: Facilitate 3D Object Knowledge Annotation with Object Conceptualization,"We present ConceptFactory, a novel scope to facilitate more efficient annotation of 3D object knowledge by recognizing 3D objects through generalized concepts (i.e. object conceptualization), aiming at promoting machine intelligence to learn comprehensive object knowledge from both vision and robotics aspects. This idea originates from the findings in human cognition research that the perceptual recognition of objects can be explained as a process of arranging generalized geometric components (e.g. cuboids and cylinders). ConceptFactory consists of two critical parts: i) ConceptFactory Suite, a unified toolbox that adopts Standard Concept Template Library (STL-C) to drive a web-based platform for object conceptualization, and ii) ConceptFactory Asset, a large collection of conceptualized objects acquired using ConceptFactory suite. Our approach enables researchers to effortlessly acquire or customize extensive varieties of object knowledge to comprehensively study different object understanding tasks. We validate our idea on a wide range of benchmark tasks from both vision and robotics aspects with state-of-the-art algorithms, demonstrating the high quality and versatility of annotations provided by our approach. Our website is available at https://apeirony.github.io/ConceptFactory.","In the current data-driven era, the availability of a large amount of training data with dense annotations has become an indispensable factor for the successful implementation of deep neural networks in a wide range of 3D object understanding tasks. Particularly, for tasks like segmentation, pose estimation and more sophisticated robot manipulation, current approaches [1, 2, 3, 4, 5, 6, 7, 8] require a substantial volume of annotations of semantic, pose and affordance knowledge to fully demonstrate their power. However, there are two primary issues demanding attention in 3D object knowledge annotation. On one hand, some types of knowledge such as affordance for manipulation are highly complicated to manually annotate [7], resulting in few existing datasets being available for such labels. On the other hand, common practices of acquiring these knowledge annotations [9, 10, 6] follow the conventional paradigm that only a single type of knowledge is labeled on one object at a time, for which researchers develop different annotation platforms to adapt to various knowledge types and let annotators engage in multiple rounds of annotations, taking significant time and human effort. In this paper, We present ConceptFactory as a novel annotation paradigm that addresses these existing issues and facilitates more efficient annotation of 3D object knowledge. The idea behind ConceptFactory originates from the well-known ’Recognition-by-Components’ theory [11] in human cognition research, which finds that the perceptual recognition of objects can be explained as a process of arranging generalized geometric components. Inspired by this theory, we devise an efficient knowledge annotation paradigm performing in two steps. i) Describe the shape of an object with generalized geometric concepts, or in other words, object conceptualization. ii) Procedurally define (different types of) knowledge on these generalized concepts. In this manner, all types of knowledge defined on the concepts can be automatically propagated to the object as various types of annotations, taking advantage of correspondence between the concepts and the object shape. ConceptFactory provides a favorable solution to both aforementioned issues. First, manual knowledge annotation on 3D objects, which can be very complicated in some cases, is no longer required. Instead, researchers only need to procedurally define a type of knowledge with mathematical rules on certain concepts, and these knowledge will be automatically propagated to all target objects consisting of such concepts. Second, intensive human effort is required only once during object conceptualization, compared to the conventional annotation paradigm where significant labor and time resources are repeatedly expended for annotating each type of knowledge. ConceptFactory comes with two critical components. The first one is ConceptFactory Suite, a unified toolbox that adopts Standard Concept Template Library (STL-C) to drive a web-based platform for object conceptualization. The STL-C consists of 263 concept templates that comprehensively covers the essential structure of daily objects, and the conceptualization platform guides users to select and parameterize concept templates in STL-C to describe a given object and thereby obtains the conceptualization result. Then, a wide range of knowledge, which is procedurally defined on the templates, can be automatically propagated to the object as annotations. The other component is ConceptFactory Asset, a large collection of conceptualized objects acquired using ConceptFactory suite, containing 4380 objects from 39 categories involving 39k template instances and 295k parameters. We present such asset considering that the object conceptualization process still requires certain human effort, thereby offering already conceptualized objects to the community would make it convenient for researchers to use and study on, e.g. customizing their own knowledge and conduct experiments with them. The knowledge annotations offered by our approach are mathematically grounded and functionally aligned, serving as a catalyst for machine intelligence to recognize and interact with objects. We demonstrate the effectiveness of our idea from both vision and robotic aspects on a wide range of benchmark tasks including segmentation, pose estimation and robot manipulation through state-of-the-art algorithms, figuring out that our approach can easily gather various types of annotations, with quality comparable or even better than those acquired through conventional annotation paradigms."
https://arxiv.org/html/2411.00432v1,PLATYPUS: Progressive Local Surface Estimator for Arbitrary-ScalePoint Cloud Upsampling,"3D point clouds are increasingly vital for applications like autonomous driving and robotics, yet the raw data captured by sensors often suffer from noise and sparsity, creating challenges for downstream tasks. Consequently, point cloud upsampling becomes essential for improving density and uniformity, with recent approaches showing promise by projecting randomly generated query points onto the underlying surface of sparse point clouds. However, these methods often result in outliers, non-uniformity, and difficulties in handling regions with high curvature and intricate structures. In this work, we address these challenges by introducing the Progressive Local Surface Estimator (PLSE), which more effectively captures local features in complex regions through a curvature-based sampling technique that selectively targets high-curvature areas. Additionally, we incorporate a curriculum learning strategy that leverages the curvature distribution within the point cloud to naturally assess the sample difficulty, enabling curriculum learning on point cloud data for the first time. The experimental results demonstrate that our approach significantly outperforms existing methods, achieving high-quality, dense point clouds with superior accuracy and detail.","Recently, autonomous driving, robotics, and other technologies that utilize 3D data have attracted significant interest, leading to the growing popularity of 3D point clouds as a representation of 3D data. However, the raw point clouds captured by sensors such as LiDAR, depth cameras often contain significant noise, and the distribution becomes particularly sparse for points that are farther from the sensor. Therefore, point cloud upsampling, the task of increasing the density points of a sparse 3D point cloud to be of dense (e.g., Fig. 1 sparse Input to dense GT) is vital for effectively using raw data in tasks like classification and segmentation, leading to various methods to tackle this challenge. Starting with early works (Alexa et al. 2003; Lipman et al. 2007; Huang et al. 2009) that employed optimization-based methods, the rise of deep learning has led to the proposal of various learning-based methods (Yu et al. 2018; Li et al. 2019; Qian et al. 2021a), for training point cloud upsampling networks. Existing learning-based methods have demonstrated excellent performance but they have several drawbacks. These methods typically split the sparse input point cloud into multiple patches, upsample each patch, and then recombine them. This approach (i.e., split-and-combine process), which fails to consider the relationships between patches during upsampling, often results in issues such as holes, outliers, and non-uniformity, especially at the boundaries where the patches are combined. Figure 1: Comparative visualization of 4×\times× point cloud upsampling results on PU1K. Point cloud upsampling is the task of generating a denser point cloud (i.e., rightmost column) that accurately reflects the underlying geometry of a sparse point cloud (i.e., leftmost column). Our method successfully upsamples intricate areas where existing methods struggle to perform well. To address the issues arising from the split-and-combine process, recent studies (He et al. 2023; Li et al. 2024) have proposed a pipeline that moves randomly generated points (i.e., query points) onto the surface that the point cloud inherently represents. In the upsampling process of this pipeline, initial query points are generated around the sparse point cloud (i.e., input point cloud). Next, these query points are projected onto the underlying surface of the sparse point cloud. To accurately determine the underlying surface, it is necessary to know the unsigned distance field of the ground-truth point cloud, but during upsampling, the ground-truth point cloud is not available. Therefore, in the training phase, the network is trained to predict the unsigned distance from randomly generated query points around the sparse point cloud to its underlying surface. This approach enables the network to infer the underlying surface of the dense point cloud using only the sparse point cloud. Figure 2: An analysis of the differences between FPS (Farthest Point Sampling) and our newly proposed curvature-based sampling. The comparison shows which points remain as the point cloud is progressively sampled down to fewer points using each sampling method. FPS uniformly samples points across the entire point cloud, whereas curvature-based sampling selectively samples points from regions with intricate structures and high curvature values. While this pipeline has addressed many of the issues in previous methods, there are still some significant problems that remain unresolved. As seen in Fig. 1, areas within the point cloud that require high curvature and locally complex structures (e.g., animal paws and ears, object edges) are often poorly upsampled. To better capture local features in complex regions that existing methods struggle to upsample, our work employs a novel approach using the concept of curvature value within the distance-estimating network of this pipeline. Curvature value represents the degree of curvature in a specific area, calculated by considering the geometric relationships with surrounding points. This metric allows us to quantitatively identify regions where existing methods fail to upsample effectively. We then utilize our newly proposed curvature-based sampling technique within the network to explicitly sample these regions with high curvature values (Fig. 2). We refer to this network as Progressive Local Surface Estimator (PLSE), which progressively retains regions with high curvature and intricate structures, enabling the network to focus on extracting features from these critical areas. Using the curvature value, which effectively highlights regions in the point cloud where upsampling is challenging, we further enhance the learning process of PLSE by implementing a new curriculum learning strategy that calculates the difficulty of a point cloud based on the distribution of curvature values. As shown in Fig. 1, the network’s feature extractor generally struggles more with capturing features in local regions with complex structures and high curvature compared to simpler, flatter areas. Based on this observation, if a point cloud has fewer complex structures—indicated by a curvature value distribution skewed toward lower values and a lower mean—we classify it as an easy sample from a learning perspective (Fig. 3). Conversely, if the point cloud’s curvature value distribution is skewed toward higher values, indicating more complex structures, it is classified as a hard sample. Following the curriculum learning strategy, easy samples were used during the early epochs, while hard samples were introduced in the later epochs, helping the network to more effectively learn the unsigned distance field. Contributions. In this work, we introduce PLATYPUS (Progressive Local Surface Estimator for ArbiTrarY-Scale Point Cloud UpSampling) for point cloud upsampling. Specifically, we make the following contributions: • We propose a novel network, Progressive Local Surface Estimator (PLSE), to learn the unsigned distance field from sparse point clouds. PLSE employs a curvature-based sampling method, which allows our network to explicitly focus on extracting features from critical areas. • To improve the learning of the unsigned distance field, we implement a curriculum learning strategy, which classifies training samples into easy and hard based on the skewness of curvature value distribution. • The results from diverse experiments demonstrate that our approach achieves state-of-the-art performance. We provide the code in the supplementary material which will be released upon publication. Figure 3: An analysis of the easy samples and hard samples used in our curriculum learning strategy. (a) Point clouds with a higher proportion of points with low curvature values, resulting in a distribution with high skewness, were classified as easy samples. (b) Conversely, point clouds with a higher proportion of points with high curvature values were classified as hard samples. Figure 4: Overall Pipeline of PLATYPUS. Input: The input consists of a sparse point cloud 𝐏𝐢𝐧𝐩𝐮𝐭subscript𝐏𝐢𝐧𝐩𝐮𝐭\mathbf{P_{input}}bold_P start_POSTSUBSCRIPT bold_input end_POSTSUBSCRIPT and nearby generated query points 𝐪𝐪\mathbf{q}bold_q. Train: During the training process, our Progressive Local Surface Estimator (PLSE) gθsubscript𝑔𝜃g_{\theta}italic_g start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT is trained to predict the distance from the query point to the underlying surface of the sparse point cloud. The loss compares the distance from the query point 𝐪𝐪\mathbf{q}bold_q to the surface of the sparse point cloud (assumed to be identical to the ground truth point cloud). Inference: the initial query point 𝐪0superscript𝐪0\mathbf{q}^{0}bold_q start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT is progressively updates using the PLSE gradient ∇gθ∇subscript𝑔𝜃\nabla g_{\theta}∇ italic_g start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT to become the final query 𝐪Tsuperscript𝐪𝑇\mathbf{q}^{T}bold_q start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT projected onto the surface of the input point cloud."
https://arxiv.org/html/2411.00425v1,Cityscape-Adverse: Benchmarking Robustness of Semantic Segmentation with Realistic Scene Modifications via Diffusion-Based Image Editing,"Recent advancements in generative AI, particularly diffusion-based image editing, have enabled the transformation of images into highly realistic scenes using only text instructions. This technology offers significant potential for generating diverse synthetic datasets to evaluate model robustness. In this paper, we introduce Cityscape-Adverse, a benchmark that employs diffusion-based image editing to simulate eight adverse conditions, including variations in weather, lighting, and seasons, while preserving the original semantic labels. We evaluate the reliability of diffusion-based models in generating realistic scene modifications and assess the performance of state-of-the-art CNN and Transformer-based semantic segmentation models under these challenging conditions. Additionally, we analyze which modifications have the greatest impact on model performance and explore how training on synthetic datasets can improve robustness in real-world adverse scenarios. Our results demonstrate that all tested models, particularly CNN-based architectures, experienced significant performance degradation under extreme conditions, while Transformer-based models exhibited greater resilience. We verify that models trained on Cityscape-Adverse show significantly enhanced resilience when applied to unseen domains. Code and datasets will be released at https://github.com/naufalso/cityscape-adverse.","Semantic segmentation, the task of partitioning an image into distinct segments and classifying each segment into a predefined category, is pivotal in various real-world applications, particularly in autonomous driving [1, 2]. Autonomous vehicles rely on accurate segmentation to understand their surroundings and make safe decisions, especially under different environmental conditions. Despite recent advancements, semantic segmentation models often struggle to maintain accuracy in out-of-distribution (OOD) scenarios such as varying weather, lighting conditions, and seasonal changes [3, 4]. For autonomous vehicles to operate reliably across diverse environments, segmentation models must be capable of generalizing effectively, a task that many current approaches still fail to meet under adverse conditions [2, 5]. Recent advancements in generative models, particularly diffusion-based image editing techniques, have shown promise in generating realistic image modifications [6, 7]. These generative models, used for data augmentation and robustness testing, have gained traction in various computer vision tasks such as object detection and image classification [8, 9]. By enabling the creation of synthetic datasets that mimic real-world variations, these techniques provide a valuable resource for evaluating and enhancing the robustness of semantic segmentation models. In this paper, we introduce Cityscape-Adverse, a novel benchmark designed to rigorously evaluate the robustness of semantic segmentation models. This benchmark utilizes diffusion-based image editing techniques to introduce realistic environmental variations to the well-known Cityscapes dataset [10]. By simulating conditions such as adverse weather or poor lighting, we aim to replicate OOD scenarios that are typically difficult and expensive to capture through traditional data collection methods [11]. We hypothesize that these synthetic modifications provide an effective and scalable alternative to manually gathering and annotating large datasets in challenging environments. This approach reflects the increasing reliance on synthetic data in machine learning, particularly in areas where real-world data acquisition is costly or impractical [12]. The main contributions of this paper can be summarized as follows: • Introduction of the Cityscape-Adverse Benchmark: We introduce Cityscape-Adverse, a novel benchmark designed to evaluate the robustness of semantic segmentation models under a wide range of simulated adverse environmental conditions. Unlike traditional datasets, our benchmark incorporates diverse and realistic conditions, providing a more comprehensive and rigorous framework for assessing model performance. • Leveraging of Generative AI for Robust Semantic Segmentation: This work investigates the application of state-of-the-art diffusion-based image editing to semantic segmentation. By utilizing advanced generative AI model, we demonstrate how these techniques enable mask-free image editing, preserving original semantic labels while conveniently generating datasets that reflect various environmental conditions. This approach offers substantial potential for enhancing model robustness and performance evaluation. • Creation of Diverse Environmental Condition Datasets: We developed 8 distinct datasets capturing various environmental conditions, including seasonal transitions (spring, autumn, winter), weather changes (rain, fog), and lighting variations (sunny, night, dawn). These diverse datasets facilitate a comprehensive evaluation of segmentation models, offering valuable insights into their behavior in dynamic, real-world scenarios. • Out-of-Distribution (OOD) Robustness Evaluation: We propose a novel framework for assessing and improving the robustness of semantic segmentation models in OOD scenarios by modifying real-world datasets. Through our Cityscape-Adverse benchmark, we demonstrate that synthetic datasets significantly enhance model robustness in real-world OOD environments, providing critical insights into the strengths and weaknesses of current models. • Comprehensive Benchmarking of State-of-the-Art Models: We conduct a comparative evaluation of leading semantic segmentation models using the Cityscape-Adverse benchmark. Our analysis highlights key strengths and limitations of existing models when faced with adverse conditions, offering practical guidance for future model development and training strategies."
https://arxiv.org/html/2411.00402v1,Improving Viewpoint-Independent Object-Centric Representations through Active Viewpoint Selection,"Given the complexities inherent in visual scenes, such as object occlusion, a comprehensive understanding often requires observation from multiple viewpoints. Existing multi-viewpoint object-centric learning methods typically employ random or sequential viewpoint selection strategies. While applicable across various scenes, these strategies may not always be ideal, as certain scenes could benefit more from specific viewpoints. To address this limitation, we propose a novel active viewpoint selection strategy. This strategy predicts images from unknown viewpoints based on information from observation images for each scene. It then compares the object-centric representations extracted from both viewpoints and selects the unknown viewpoint with the largest disparity, indicating the greatest gain in information, as the next observation viewpoint. Through experiments on various datasets, we demonstrate the effectiveness of our active viewpoint selection strategy, significantly enhancing segmentation and reconstruction performance compared to random viewpoint selection. Moreover, our method can accurately predict images from unknown viewpoints.","Humans typically perceive a visual scene as combining various visual concepts, including objects, backgrounds, and their basic parts. This object-level perception allows for a better understanding of diverse environments that consist of multiple objects and backgrounds. Similarly, object-centric learning focuses on the object-level representation of images or videos instead of modeling the entire scene directly [1]. Such object-centric representations prove to be more versatile for a range of visual tasks, such as visual scene understanding [2], visual reasoning [3], and causal inference [4]. Figure 1: Active viewpoint selection framework. Our proposed method iteratively selects viewpoints from the unknown set to form a small yet informative observation set, enabling effective training with fewer images. The active viewpoint selection strategy evaluates the information gain of the unknown viewpoints using the predicted images and selects the viewpoint with the maximum information gain as the next observation. The real image of the selected viewpoint is then added to the observation set, and this process continues until the observation set reaches a predefined size. Given the inherent complexities of visual scenes, such as object occlusion, capturing the entire scene comprehensively from a single viewpoint is often challenging. Therefore, observing the scene from multiple viewpoints becomes essential to achieve a deeper understanding. In recent years, various object-centric learning methods have emerged to address multi-viewpoint learning without object-level supervision, including MulMON [5], ROOTS [6], SIMONe [7], TC-VDP [8], and OCLOC [9, 10]. MulMON and ROOTS leverage provided viewpoint annotations to learn viewpoint-independent object-centric representations, relying heavily on these annotations. In contrast, SIMONe and TC-VDP do not require explicit viewpoint annotations, instead assuming temporal relationships among viewpoints within the same scene and using frame indexes for inference. Unlike these approaches, OCLOC operates in a fully unsupervised manner, where viewpoints are both unknown and unrelated. However, these multi-viewpoint object-centric learning methods generally utilize images from either random or sequential viewpoints as input. While these viewpoint selection strategies can be applied to various scenes, they are not always ideal. Typically, capturing a full understanding of the visual scene requires images from multiple viewpoints (e.g., 6–8), which may introduce inefficiencies. Notably, certain scenes may be more sensitive to information from specific viewpoints, meaning a generic selection strategy could lead to redundancy or overlook essential scene details. Moreover, despite significant advancements in unsupervised object segmentation, the generative capabilities of multi-viewpoint methods still require improvement, largely due to the limitations of their mixture-based decoders. Recently, LSD [11] and SlotDiffusion [12] have combined object-centric learning models with diffusion-based slot decoders, leveraging the powerful image generation capabilities of the diffusion model [13, 14] to achieve high-quality slot-to-image decoding. However, neither approach is suitable for multi-viewpoint scenes, as one is designed for single images and the other for videos. The object-centric representations they learn are specific to individual images, rather than being scene-specific or viewpoint-independent. While these methods support certain image generation and editing tasks, they lack the ability to synthesize novel views. To address the limitation mentioned above, we propose AVS, a novel multi-viewpoint object-centric learning model with an active viewpoint selection strategy. AVS optimizes viewpoint selection and enhances image decoding quality through the integration of a diffusion model. As illustrated in Figure 1, images are divided into an observation set and an unknown set. AVS learns viewpoint-independent object-centric representations from the observation set and predicts images in the unknown set. It then extracts object-centric representations from the unknown viewpoints and compares them with those from the observation set. The unknown viewpoint with the largest disparity, indicating maximum information gain, is selected as the next observation viewpoint. Repeating this process allows the model to refine viewpoint-independent object-centric representations by actively selecting the most informative viewpoints. To validate the advantages of our active viewpoint selection strategy, we conducted performance comparison experiments between the active selection strategy and the random selection strategy using the same model architecture. The experimental results indicate that active viewpoint selection achieves better segmentation performance than random selection with the same number of viewpoints. Furthermore, we compare the performance of our model with other multi-viewpoint object-centric learning methods. The results demonstrate that our model achieves superior segmentation performance and outstanding generation capability. Additionally, our method can predict images from unknown viewpoints and supports novel viewpoint synthesis. In summary, our contributions are as follows: 1) We propose AVS, a multi-viewpoint object-centric learning model with an active viewpoint selection strategy, which demonstrates improved performance in unsupervised object segmentation and image generation compared to baseline methods. 2) Our active viewpoint selection strategy significantly enhances viewpoint-independent object-centric representations, enabling the model to better understand and perceive visual scenes. 3) Our model can predict images from unknown viewpoints and generate images from novel viewpoints."
https://arxiv.org/html/2411.00399v1,StyleTex: Style Image-Guided Texture Generation for 3D Models,"Style-guided texture generation aims to generate a texture that is harmonious with both the style of the reference image and the geometry of the input mesh, given a reference style image and a 3D mesh with its text description. Although diffusion-based 3D texture generation methods, such as distillation sampling, have numerous promising applications in stylized games and films, it requires addressing two challenges: 1) decouple style and content completely from the reference image for 3D models, and 2) align the generated texture with the color tone, style of the reference image, and the given text prompt. To this end, we introduce StyleTex, an innovative diffusion-model-based framework for creating stylized textures for 3D models. Our key insight is to decouple style information from the reference image while disregarding content in diffusion-based distillation sampling. Specifically, given a reference image, we first decompose its style feature from the image CLIP embedding by subtracting the embedding’s orthogonal projection in the direction of the content feature, which is represented by a text CLIP embedding. Our novel approach to disentangling the reference image’s style and content information allows us to generate distinct style and content features. We then inject the style feature into the cross-attention mechanism to incorporate it into the generation process, while utilizing the content feature as a negative prompt to further dissociate content information. Finally, we incorporate these strategies into StyleTex to obtain stylized textures. We utilize Interval Score Matching to address over-smoothness and over-saturation, in combination with a geometry-aware ControlNet that ensures consistent geometry throughout the generative process. The resulting textures generated by StyleTex retain the style of the reference image, while also aligning with the text prompts and intrinsic details of the given 3D mesh. Quantitative and qualitative experiments show that our method outperforms existing baseline methods by a significant margin.","We investigate an under-explored generation problem: style image-guided texture synthesis, which is crucial in computer vision and graphics, facilitating the creation of visually compelling and immersive digital environments in games and films. The generated texture needs to be harmonious with both the 3D shape and style of the reference image, which requires the texture to align with the geometry while conveying a consistent style from different views. Existing research mostly investigates the above two requirements separately. In 2D style-image generation methods, the style is conveyed by separating it from the reference image and incorporating it into the final output, which usually involves fine-tuning (Hu et al., 2022; Gal et al., 2023; Ruiz et al., 2023) the diffusion model to be a stylized image generator or adjusting the hidden layers of the diffusion model with the extracted style features (Jeong et al., 2024; Hertz et al., 2024; Wang et al., 2024; Voynov et al., 2023; He et al., 2024). In parallel, 3D texture can be generated by iteratively inpainting (Richardson et al., 2023; Chen et al., 2023c) or image synthesis with multi-view consistency (Cao et al., 2023; Liu et al., 2023b; Gao et al., 2024; Wu et al., 2024). More recently, distillation methods such as score distillation sampling (Metzer et al., 2023; Chen et al., 2023a; Youwang et al., 2023) have also proven their superior effectiveness in synthesizing 3D consistent textures. Compared to the direct generation of textures, distillation methods are capable of achieving better view and global style consistency while avoiding local seam problems. Despite the progress in these two distinct areas, incorporating the desired style into texture generation is not straightforward. One possible solution is to combine the distillation method with a diffusion distribution aligned with the reference image’s style. However, this leads to two challenges: 1) decoupling the style and content from the reference image entirely, and 2) preserving the color tone. Firstly, the ambiguity between style and content from different views complicates the decoupling process. In 2D domains, separating style and content within a single viewpoint may succeed in most situations. However, in 3D domains, failure to effectively decouple style from any single viewpoint can result in inaccurate style and unintended content leakage in the final texture. Thus, the generation of stylized textures in 3D domains requires a robust method for disentangling style and content. Secondly, distillation methods may result in over-saturation and over-smoothing within the generated textures, leading to color shifts and a lack of details, hindering the accurate reflection of the intended style. To overcome these challenges, we propose StyleTex, a diffusion-model-based pipeline to generate style textures under the guidance of a single image. Our key insight is to extract the style information from the reference image while disregarding the content information. Inspired by the multi-modal applications of the CLIP space, we propose to represent the content of the reference image as the CLIP embedding of its corresponding text prompt. A naive method to discard the content from the reference image in InstantStyle (Wang et al., 2024) is to drive the reference image embedding in the same CLIP space toward the opposite direction of the content embedding. However, the slight misalignment between the content embedding and the real content information of the image may cause undesirable image embedding alerting, which results in unclean content information remaining or color tone changing. To address this, we remove the content information from the reference image embedding by decomposing its CLIP embedding into two separate orthogonal features. One of these features aligns with the content embedding and encodes most of the content information of the reference image. We retain only the remaining feature, which predominantly relates to the style, to refine our diffusion model. To this end, we explicitly incorporate the style-relevant feature through the cross-attention mechanism, which also serves as a color tone guidance that can prevent unintentional color tone changing during the distillation process. Furthermore, we incorporate the content embedding as a negative prompt to further dissociate content information. We integrate the aforementioned strategies into StyleTex to generate stylized textures and utilize Interval Score Matching (ISM) (Liang et al., 2024) to further tackle the issue of over-smoothness. Moreover, we utilize a geometry-aware ControlNet to ensure geometric consistency throughout the generative process. In summary, our work makes the following major contributions: • A diffusion-model-based pipeline to generate style textures under the guidance of a single image, enabling the automatic creation of diverse stylized virtual environments. • A novel style decoupling and injection strategy that effectively guides stylization while addressing issues of content leakage and style deviation in texture generation."
https://arxiv.org/html/2411.00394v1,Rightthis way: Can VLMs Guide Us to See More to Answer Questions?,"In question-answering scenarios, humans can assess whether the available information is sufficient and seek additional information if necessary, rather than providing a forced answer. In contrast, Vision Language Models (VLMs) typically generate direct, one-shot responses without evaluating the sufficiency of the information. To investigate this gap, we identify a critical and challenging task in the Visual Question Answering (VQA) scenario: can VLMs indicate how to adjust an image when the visual information is insufficient to answer a question? This capability is especially valuable for assisting visually impaired individuals who often need guidance to capture images correctly. To evaluate this capability of current VLMs, we introduce a human-labeled dataset as a benchmark for this task. Additionally, we present an automated framework that generates synthetic training data by simulating “where to know” scenarios. Our empirical results show significant performance improvements in mainstream VLMs when fine-tuned with this synthetic data. This study demonstrates the potential to narrow the gap between information assessment and acquisition in VLMs, bringing their performance closer to humans. Our dataset and code are available at: https://github.com/LeoLee7/Directional_guidance.","In recent years, Vision-Language Models (VLMs) have made significant strides in general multimodal tasks such as visual recognition and Visual Question Answering (VQA) VQA ; balanced_binary_vqa . This progress has opened up a vast potential for various applications, including enhancing visual accessibility for visually impaired individuals bigham2010vizwiz ; gurari2018vizwiz , supporting decision-making in autonomous systems zhou2023vision ; park2024vlaad , enabling interactive technologies team2023gemini , etc. Despite these advances, VLMs still fall short of human capabilities. Humans can intuitively assess whether the available information is sufficient to answer a question and seek additional details when necessary schraw1995metacognitive ; DEMETRIOU2006297 . In contrast, VLMs typically tend to provide direct, single-response outputs even when information is insufficient to answer the question accurately. This limitation reduces their effectiveness in real-world applications bemyeyes2023 . To address this issue, recent studies have explored ways to teach VLMs to assess information sufficiency wang2024mm . These studies aim to have VLMs either provide concrete answers or label questions as unanswerable, using benchmark datasets from real user questions like VizWiz gurari2018vizwiz . However, a significant gap remains in handling unanswerable cases: deciding what actions to take when VLMs identify a question to be unanswerable. Humans naturally possesses the ability to seek additional details when faced with unanswerable questions — a challenge often encountered in real-world VQA tasks due to poor image quality, ambiguous questions, or loss of context bhattacharya2019does ; chiu2020assessing . To the best of our knowledge, no existing benchmarks focused on “what to do” after the model identifies information insufficiency. This active process of information acquisition, fundamental to human cognition, has not been replicated in VLMs and remains largely unexplored. To narrow the gap between VLMs and human intelligence, we suggest going beyond improving accuracy on answer generation or merely deciding on information sufficiency. Instead, we focus on enhancing the model’s capability to provide constructive feedback when encountering unanswerable questions. In response to this challenge, we introduce a novel VQA task aimed at providing Directional Guidance, which aligns with real-world needs, particularly for visually impaired individuals. As indicated in previous studies chiu2020assessing , a common issue is that many images taken by visually impaired users are ill-framed. Our task aims to guide users on how to reframe their images during the interactive VQA process. This task evaluates the model’s ability to understand visual direction and determine a potential direction to obtain more relevant information. Moreover, to empower VLM with such guiding capability, we propose an automatic VQA data augmentation framework. This framework begins by prompting a pretrained VLM to filter a set of answerable questions from the given VQA dataset. The corresponding images are then perturbed using predefined rules that crop relevant visual information, making it more challenging for the model to answer the questions correctly. Finally, the VLM is fine-tuned using this augmented dataset, with the task of providing Directional Guidance on resolving the predefined perturbations. This approach simulates information inadequacy scenarios and holds promising potential for enhancing the model’s ability to guide users in acquiring relevant information. To validate the effectiveness of the approach, we contribute a manually labeled test set containing the Directional Guidance for real-world unanswerable datasets with images taken by visually impaired individuals. Our experiments on three popular open-source VLMs show significant improvements in the models’ performance on the Directional Guidance task after fine-tuning with our synthetic training data. Notably, the best-performing model outperforms GPT-4o (CoT) achiam2023gpt by 3% accuracy score. Figure 1: The examples of the Directional Guidance task. The model utilizes self-knowledge to distinguish between known and unknown information and provides guidance on where to find more information. The contributions of this study are: • Directional Guidance task: We define a novel VQA task. As shown in Figure 1, the proposed task assesses the model’s ability to identify the information sufficiency and provide Directional Guidance when needed. • Directional Guidance dataset: We create a human-labeled test set to benchmark the guidance-providing capability of VLMs. • Directional Guidance framework: We propose a data-efficient framework for training models on the Directional Guidance task. This framework includes synthetic training data generation and model fine-tuning, which can be generalized with any VQA dataset with grounding information."
https://arxiv.org/html/2411.00355v1,TextDestroyer: A Training- and Annotation-Free Diffusion Method for Destroying Anomal Textfrom Images,"In this paper, we propose TextDestroyer, the first training- and annotation-free method for scene text destruction using a pre-trained diffusion model. Existing scene text removal models require complex annotation and retraining, and may leave faint yet recognizable text information, compromising privacy protection and content concealment. TextDestroyer addresses these issues by employing a three-stage hierarchical process to obtain accurate text masks. Our method scrambles text areas in the latent start code using a Gaussian distribution before reconstruction. During the diffusion denoising process, self-attention key and value are referenced from the original latent to restore the compromised background. Latent codes saved at each inversion step are used for replacement during reconstruction, ensuring perfect background restoration. The advantages of TextDestroyer include: (1) it eliminates labor-intensive data annotation and resource-intensive training; (2) it achieves more thorough text destruction, preventing recognizable traces; and (3) it demonstrates better generalization capabilities, performing well on both real-world scenes and generated images.","Recently, diffusion models [2, 3, 4, 5, 6, 7, 8, 9] have made remarkable achievements in text-conditioned image generation, enabling users to effortlessly transform their vivid imaginations into reality. They have shown impressive success in accurately and coherently rendering textual content. In particular, with the use of T5 text encoder component [10], preliminary text generation capabilities have been demonstrated by Imagen [6], eDiff-I [7], and DeepFloyd-IF [8]. Liu et al. [11] further improved text generation by employing character-aware text encoders [12]. To provide more precise guidance in text generation through diffusion, a series of efforts [13, 14, 15, 16] have been dedicated to designing specialized network architectures for generating refined text. Stable Diffusion 3 [9] abandons the traditional U-Net architecture in favor of DiT [17] for denoising. In addition to producing awe-inspiring images, it also shows the ability to accurately represent textual information within the images. Easy access to tools that create images from text can lead to problems with copyright, privacy, and the law. Sharing personal information like phone numbers or addresses in images online has already been a big issue, as shown in Fig. 1(a). People try to hide this information by making it blurry or covering it up, but this leads to unintended complications. With more powerful image-making tools, these issues could get worse. Bad people might use these tools to spread false information by putting sensitive details in fake scenes. It’s important to think about both the good and bad sides, especially how to stop the spread of unwanted text in images. In line with our motivation, the field of scene text removal has been extensively studied. Before the era of deep learning, numerous efforts had already employed traditional machine learning and computer vision techniques to tackle this task [18, 19, 20, 21]. Neural networks, with their powerful learning capabilities, have further enhanced the effectiveness of text erasure. Scene Text Eraser [22] trained an end-to-end CNN, selectively erasing text from images divided into multiple patches. EnsNet [23] features a more sophisticated network structure for erasing both text and other objects. MTRNet [24], using a conditional generative adversarial network (cGAN) with an auxiliary mask, further improved the effectiveness of erasure. PERT [25] embeds a detection branch within the network, providing explicit guidance for erasure. DeepEraser [1] adopts a recursive architecture to gradually remove text over multiple iterations. Nonetheless, as depicted in Fig. 1(b), removal models may excessively focus on background restoration accuracy, potentially leaving subtle residues that enable the text to remain readable in Fig. 1(b). Furthermore, these methods necessitate training, consuming considerable computational resources. Additionally, the training datasets demand labor-intensive manual annotation of masks and ground-truth images post-inpainting. Although STRDD [26] recently used diffusion for text removal, it still requires retraining and data annotation. We recognize that by using a pre-trained diffusion model, the necessity for retraining or data annotation in text destruction can be circumvented. This is due to two factors: first, during the pre-training phase, diffusion models have already been exposed to numerous images, including those with scene text. Second, the diffusion architecture’s cross-attention mechanism exhibits rough localization capabilities for textual regions in latent space, providing an alternative to manual mask annotation. Thus, it becomes feasible to explore a training-free and annotation-free text deconstruction method. In this paper, we introduce TextDestroyer, the first training- and annotation-free diffusion method for scene text destruction. We emphasize training-free techniques, automatic text localization, and comprehensive destruction of textual regions. We then employ a hierarchical process for progressive and precise text localization. In the introductory text capturing stage, we aggregate multiple token-level attention maps from the inversion process and segment them to capture an introductory text region mask. In the continuous text adjustment stage, we crop and resize all text regions in the original image and apply the same inversion process to adjust text regions with reduced background interference. In the meticulous text delineation stage, we perform 2-means clustering on the original image, using the non-text areas from the second stage as a reference to distinguish between text and background clusters. With a precise mask of text areas, we destroy their latent codes using random Gaussian noise before reconstructing the image through the diffusion denoising process. Also, we introduce a denoising process to guide image reconstruction, replacing the erroneous latent codes with original ones at each step for low distortion of background. This diffusion process offers key K𝐾Kitalic_K and value V𝑉Vitalic_V of non-text areas at specific time steps and self-attention layers for denoising reconstruction, enabling background restoration. To further ensure low distortion in non-text areas, we replace the latent code during the reconstruction when denoising is nearly complete. Finally, we accomplish a complete obliteration of scene text as illustrated in Fig. 1(c). The major contributions of this paper are summarized as: 1. Training and Annotation-Free Approach. TextDestroyer is the first method to destroy scene text without requiring additional training or annotations. This approach simplifies the text removal process, as it does not rely on labor-intensive data annotation or resource-intensive model training, making it efficient and accessible for practical applications. 2. Enhanced Text Destruction and Background Restoration. The method employs a three-stage hierarchical process that not only ensures thorough destruction of text but also enhances background restoration. It uses Gaussian noise to scramble text regions and a diffusion denoising process for image reconstruction, which preserves the background integrity by replacing erroneous latent codes with original ones during reconstruction, minimizing visual distortions and maintaining the quality of non-text regions."
https://arxiv.org/html/2411.00340v1,GAFusion: Adaptive Fusing LiDAR and Camera with Multiple Guidance for3D Object Detection,"Recent years have witnessed the remarkable progress of 3D multi-modality object detection methods based on the Bird’s-Eye-View (BEV) perspective. However, most of them overlook the complementary interaction and guidance between LiDAR and camera. In this work, we propose a novel multi-modality 3D objection detection method, named GAFusion, with LiDAR-guided global interaction and adaptive fusion. Specifically, we introduce sparse depth guidance (SDG) and LiDAR occupancy guidance (LOG) to generate 3D features with sufficient depth information. In the following, LiDAR-guided adaptive fusion transformer (LGAFT) is developed to adaptively enhance the interaction of different modal BEV features from a global perspective. Meanwhile, additional downsampling with sparse height compression and multi-scale dual-path transformer (MSDPT) are designed to enlarge the receptive fields of different modal features. Finally, a temporal fusion module is introduced to aggregate features from previous frames. GAFusion achieves state-of-the-art 3D object detection results with 73.6%percent\%% mAP and 74.9%percent\%% NDS on the nuScenes test set.","3D object detection is a crucial task in autonomous driving. To cope with the complex road scenarios, multiple sensors (LiDARs or cameras) are usually employed for scene understanding. LiDAR can generate accurate but sparse 3D point clouds, which contains precise spatial information. Images have rich semantic and texture information, but lack depth information. Therefore, a natural operation is to extensively fuse LiDAR and camera to leverage the complementarity of multi-modality information, which can enable the autonomous driving system to achieve higher accuracy and robustness. Figure 1: Comparison between BEVFusion and the proposed GAFusion. (a) In BEVFusion, the camera stream and the LiDAR stream separately generate BEV features, which are then concatenated together. (b) In GAFusion, the camera modality BEV features are generated by multiple guidance from the LiDAR stream, and the receptive fields are enhanced by MSDPT. The BEV features are fused by LGAFT. “VT” is view transformer. Recently, fusing LiDAR and camera has achieved some progress. Early methods [4, 14, 45, 35] achieve LiDAR-camera fusion by projecting 3D LiDAR point clouds (or region proposals) onto 2D images. But these methods overlook the information gap between the two modalities. Recent works [22, 26, 1, 40, 39, 42, 13, 3] adopt different query generation strategies or create a unified Bird’s-Eye-View (BEV) [28] intermediate feature to fuse multi-modality features. For instance, TransFusion [1] applies a two-stage pipeline to fuse the camera and LiDAR features, but its performance relies on the query initialization strategy. BEVFusion [22, 26] explores a unified representation for BEV features through view transformation, which not only preserves the spatial information of sparse LiDAR point clouds, but also lifts the 2D images to the 3D features, effectively maintaining the consistency between the two modalities. However, the camera modality still struggles with geometric perception information, which limits the complementarity between LiDAR and camera. As shown in Fig. 1(a), there is no interaction between both modalities. To tackle the above challenges, we propose an effective 3D multi-modality object detection method, named GAFusion. Within it, a LiDAR guidance module is developed, which consists of sparse depth guidance (SDG) and LiDAR occupancy guidance (LOG). SDG combines the sparse depth generated by LiDAR point clouds and the camera features to produce depth-aware features, enhancing the sensitivity of camera features to depth information. Inspired by the occupancy task [12], LOG guides a 3D feature volume generated by view transformation with occupancy features, and focuses on the targets in the 3D feature volume, thus providing more valuable information for fusion. Then, we construct a multi-scale dual-path transformer (MSDPT) module to improve the interactions around the objects and expand the receptive fields of the 3D feature volume. With the above designs, the camera modality has sufficient semantic features and more accurate depth distribution. In the following, to obtain abundant features in the LiDAR modality, we perform additional downsampling on the LiDAR point clouds and use sparse depth compression to aggregate features from different scales. This operation can provide larger receptive fields with less computation and memory consumption. Moreover, a LiDAR-guided adaptive fusion transformer (LGAFT) module is proposed to effectively fuse BEV features generated by LiDAR point clouds and images. In this module, the LiDAR BEV features adaptively guide the camera BEV features to strengthen the cross-modality interaction from global scope. All of the above operations are evaluated on single-frame raw data. In order to further explore the target correlation and motion consistency among multiple successive frames, a temporal fusion module is designed. To be specific, we store the BEV features of different frames in memory buffer, which is used to fuse the features of the previous frame and the current frame. Our contributions are summarized as follows: 1) We propose GAFusion, a novel 3D object detection method that leverages LiDAR guidance to compensate for depth distribution of the camera features, and provides sufficient spatial information for the camera features. 2) We design LiDAR-guided adaptive fusion transformer (LGAFT), which aims to enhance the global features interaction between the two modalities in an adaptive way, facilitating the fusion of semantic and geometric features. 3) We conduct extensive experiments on the nuScenes dataset to verify the effectiveness of our GAFusion. The experiments show that without using any augmentation strategies, our model achieves the state-of-the-art performances of 72.1%percent\%% mAP and 73.5%percent\%% NDS."
https://arxiv.org/html/2411.00335v1,NCST: Neural-based Color Style Transfer for Video Retouching,"Video color style transfer aims to transform the color style of an original video by using a reference style image. Most existing methods employ neural networks, which come with challenges like opaque transfer processes and limited user control over the outcomes. Typically, users cannot fine-tune the resulting images or videos. To tackle this issue, we introduce a method that predicts specific parameters for color style transfer using two images. Initially, we train a neural network to learn the corresponding color adjustment parameters. When applying style transfer to a video, we fine-tune the network with key frames from the video and the chosen style image, generating precise transformation parameters. These are then applied to convert the color style of both images and videos. Our experimental results demonstrate that our algorithm surpasses current methods in color style transfer quality. Moreover, each parameter in our method has a specific, interpretable meaning, enabling users to understand the color style transfer process and allowing them to perform manual fine-tuning if desired.","Image color style transfer involves transferring the color style of style image onto content image, thereby altering the color of the content image while preserving its original structure as shown in Figure 1. Both image and video color style transfer have numerous real-world applications, including promotional material creation, post-production color grading of photos, and applying filters in videos and games to enhance thematic coherence and expressiveness. Consequently, this technology has attracted significant attention in recent years 11 ; 15 . Color style transfer, traditionally applied to individual images, is increasingly falling short of addressing the demands of video style transfer 10 . Techniques that rely on neural networks for image color style transfer may lead to disrupted visual continuity in videos. Moreover, these methods often result in inefficiencies due to inference with neural networks during the transfer 19 . Another limitation of neural network-reliant color style transfer is its opacity—the inability of users to access or adjust color parameters during conversion, which makes post-adjustment fine-tuning impossible. The NLUT 16 method improves the efficiencies of color style transfer by predicting a 3D lookup table with neural networks. However, it still conceals the color adjustment parameters involved in the process. To address these challenges, we propose using neural networks to predict the parameters for color style transfer. Our method predicts color transfer parameters—such as contrast and brightness—between a style image and a content image, meeting thereby diverse color style transfer demands. Additionally, these parameters can be manually adjusted by users, offering greater flexibility. The parameters we predict are inherently suitable for video color adjustment, and when applied, they mitigate the flickering issues typical of neural network-based video color style transfer. Furthermore, our method supports test-on-time training for specific video transfers, enhancing its effectiveness for tailored applications. These predicted color adjustment parameters can also be transformed into 3D LUTs for video color style transfer, achieving ultra-fast conversion speeds while maintaining high-quality results, similar to the capabilities of NLUT. In summary, our key contributions are as follows: • We propose a method that automatically predicts color style transfer parameters using a neural network for video style transfer, making the style transfer process transparent. • Our method can be seamlessly integrated with other methods to achieve personalized objectives such as enhancing efficiency. • Experiments show that our method has superior performance in color style transfer effects and achieves higher consistency in video color style transfer."
https://arxiv.org/html/2411.00330v1,Multiple Information Prompt Learning for Cloth-Changing Person Re-Identification,"Abstract - Cloth-changing person re-identification is a subject closer to the real world, which focuses on solving the problem of person re-identification after pedestrians change clothes. The primary challenge in this field is to overcome the complex interplay between intra-class and inter-class variations and to identify features that remain unaffected by changes in appearance. Sufficient data collection for model training would significantly aid in addressing this problem. However, it is challenging to gather diverse datasets in practice. Current methods focus on implicitly learning identity information from the original image or introducing additional auxiliary models, which are largely limited by the quality of the image and the performance of the additional model. To address these issues, inspired by prompt learning, we propose a novel multiple information prompt learning (MIPL) scheme for cloth-changing person ReID, which learns identity robust features through the common prompt guidance of multiple messages. Specifically, the clothing information stripping (CIS) module is designed to decouple the clothing information from the original RGB image features to counteract the influence of clothing appearance. The Bio-guided attention (BGA) module is proposed to increase the learning intensity of the model for key information. A dual-length hybrid patch (DHP) module is employed to make the features have diverse coverage to minimize the impact of feature bias. Extensive experiments demonstrate that the proposed method outperforms all state-of-the-art methods on the LTCC, Celeb-reID, Celeb-reID-light, and CSCC datasets, achieving rank-1 scores of 74.8%, 73.3%, 66.0%, and 88.1%, respectively. When compared to AIM (CVPR23), ACID (TIP23), and SCNet (MM23), MIPL achieves rank-1 improvements of 11.3%, 13.8%, and 7.9%, respectively, on the PRCC dataset. 111Manuscript received June-27th, 2024; This work was supported in part by the National Natural Science Foundation of China (No.62372325, No.61872270), Natural Science Foundation of Tianjin Municipality (No.23JCZDJC00280), Shandong project towards the integration of education and industry (No.2022JBZ01-03). S.X Wei, Z. Gao and C.J Ma are with the Shandong Artificial Intelligence Institute, Qilu University of Technology (Shandong Academy of Sciences), Jinan, 250014, P.R China. S.X Wei, Z. Gao and Y.B Zhao are with Key Laboratory of Computer Vision and System, Ministry of Education, Tianjin University of Technology, Tianjin, 300384, P.R China. W.L Guan is with the School of Electrontics and Information Engineering, Harbin Institute of Technology, Shenzhen, 15001, P.R China. .","Person Re-identification (ReID) is a prominent research focus within the fields of computer vision and machine learning. It aims to solve the problem of retrieving target pedestrians across non-overlapping cameras, and determine whether the images taken by different cameras contain a specific pedestrian. This task plays an important role in areas such as public safety, smart commerce, and smart security (e.g., identifying customers for personalization, tracking potential criminal suspects). Traditional person ReID methods [28, 29, 44, 37, 5, 13, 20] usually rely on pedestrians wearing the same clothes in different shots. However, in real-world scenarios, pedestrian appearance can change significantly over time, making it difficult for models to learn effective identification features. This variability presents a substantial challenge for person ReID. To address this issue, researchers are increasingly focusing on the cloth-changing person re-identification (CC-ReID) task. This approach aims to identify other identity-related features that are independent of clothing, such as body posture and gait. These features can still effectively identify pedestrians even when their clothing changes. Some researchers [1, 8, 35, 9, 30, 7] have made active attempts in person ReID task under changing clothes scenario. The public datasets PRCC [34], LTCC [24], Celeb-reID [16], Celeb-reid-light [15] and CSCC [33] for person ReID research have also been constructed. Figure 1 shows some examples of CC-ReID images, where the first row represents the same person wearing different clothes and the second row represents different people wearing similar clothes, from which we can observe that the first row exhibits a large difference in pedestrian appearance and the second row shows a very small difference in pedestrian appearance. Under CC-ReID, the appearance of human clothing can no longer be used as the main basis for recognition but has become an interference factor. At this time, the traditional ReID method that largely depends on the appearance characteristics of clothing is no longer competent, so the ReID method for changing clothes is a more challenging and urgent problem to be solved. In CC-ReID, because of the unreliability of clothing information, the intuitive response is to model identity information unrelated to clothing. Recently, some novel methods for changing person ReID have been proposed, for example, Jin et al. [18] proposed a Gait-assisted Image-based cloth-changing ReID (GI-ReID) framework, which introduces Gait information to assist in learning clothing independent representations. Gao et al. [7] proposed a novel multigranular visual-semantic embedding algorithm (MVSE) to deeply explore the visual semantic information and pedestrian attribute information, so as to effectively solve the problem of CC-ReID. Figure 1: Examples of cloth-changing person ReID images. Intra-class variation and inter-class variation of pedestrian samples in the cloth-changing scenario. There are two main challenges in the CC-ReID: I) intra-and inter-class variation of people caused by changes in clothing appearance. II) changes in pedestrian pose/viewpoint and pedestrian occlusion. Although previous studies have achieved performance gains in addressing these issues, there are still some limitations: 1) Spatial redundancy exists in visual modes. Existing methods are implemented from the perspectives of adversarial training [8, 36], domain generalization [42], causal intervention [35], etc., to learn effective feature representations only from RGB images. However, there is a lot of spatial redundancy in the pure RGB image [11], and the extracted independent representation of identity will be greatly affected by the changes of clothing appearance and background. It is difficult to effectively decouple interference features such as clothing, resulting in poor performance of existing methods. Therefore, it is an urgent problem to control the influence of spatial redundant information on its visual representation to improve the discriminative stability of the model. 2) Identity clues are underutilized. Many existing ReID methods [10] lack the use of key information of pedestrians, and often implicitly learn identity classification features from the original features of pedestrians, and do not make full use of key information related to identity to prompt model learning. Therefore, explicitly prompting the model to learn the key features of identity is worth investigating. 3) Feature bias affects performance. In CC-ReID, features are still biased by pose changes, occlusion and other conditions. The existing methods [1, 2] mainly focus on using auxiliary information to overcome the influence of bias, but their effectiveness depends on the quality of auxiliary information and will incur additional resource overhead. Therefore, how to effectively reduce the impact of feature bias is worth exploring. To address the above issues, we design a new MIPL framework to learn identity robust features using the prompt guidance of multiple information. For problem (1), inspired by [20], we introduce a phased vision-language learning strategy to effectively establish the correspondence between visual representation and high-level language description, and design a clothing information stripping module to effectively decouple clothing information from original image features with the prompt of clothing attribute text description. For problem (2), the biological information guidance branch is proposed, which explicitly prompts the model to learn biological key features with strong identity correlation through local unique biological information attention prompts. In addition, for problem (3), a dual-length hybrid patch module is designed to reduce the impact of feature deviation and make the features have diverse coverage. The main contributions of this paper are summarized as follows. • We propose a novel MIPL algorithm for cloth-changing person ReID, which introduces a vision-language learning strategy. The prompt-guidance process is optimized in an end-to-end unified framework through the common prompt guidance of multiple information, and makes full use of various information to learn identity robust features. • We design a novel clothing information stripping module (CIS), which effectively decouples the clothing information from the original image features and counteracts the influence of clothing appearance. We propose the Bio-guided Attention (BGA) module that prompts the model to learn biologically key features whose identities are strongly correlated. We develop a dual-length hybrid patch (DHP) module to minimize the impact of feature biases caused by occlusion, pose variation, and viewpoint differences. • We conducted a systematic and comprehensive evaluation of the MIPL algorithm on five public cloth-changing person ReID datasets, and the experimental results show that the MIPL method outperforms the existing cloth-changing person ReID methods in terms of mAP and rank-1. The remainder of the paper is organized as follows. Section II introduces the related work, and Section III describes the proposed MIPL method. Section IV describes the experimental settings and the analysis of the results. Section V presents the details of the ablation study, and concluding remarks are presented in Section VI."
https://arxiv.org/html/2411.00304v1,Unified Generative and Discriminative Training for Multi-modal Large Language Models,"In recent times, Vision-Language Models (VLMs) have been trained under two predominant paradigms. Generative training has enabled Multimodal Large Language Models (MLLMs) to tackle various complex tasks, yet issues such as hallucinations and weak object discrimination persist. Discriminative training, exemplified by models like CLIP, excels in zero-shot image-text classification and retrieval, yet struggles with complex scenarios requiring fine-grained semantic differentiation. This paper addresses these challenges by proposing a unified approach that integrates the strengths of both paradigms. Considering interleaved image-text sequences as the general format of input samples, we introduce a structure-induced training strategy that imposes semantic relationships between input samples and the MLLM’s hidden state. This approach enhances the MLLM’s ability to capture global semantics and distinguish fine-grained semantics. By leveraging dynamic sequence alignment within the Dynamic Time Warping framework and integrating a novel kernel for fine-grained semantic differentiation, our method effectively balances generative and discriminative tasks. Extensive experiments demonstrate the effectiveness of our approach, achieving state-of-the-art results in multiple generative tasks, especially those requiring cognitive and discrimination abilities. Additionally, our method surpasses discriminative benchmarks in interleaved and fine-grained retrieval tasks. By employing a retrieval-augmented generation strategy, our approach further enhances performance in some generative tasks within one model, offering a promising direction for future research in vision-language modeling. The project repository is here.","In recent times, Vision-Language Models (VLMs) have been trained under two predominant paradigms: generative training and discriminative training. Generative Training has achieved remarkable success in enabling Multimodal Large Language Models (MLLMs) [1, 55, 86] to develop a wide range of powerful capabilities that can handle various complex tasks (e.g., open-world visual question-answering, image caption generation, etc.) within a single model. However, challenges such as hallucinations and weak image object discrimination abilities [7, 89] persist. Discriminative Training, exemplified by CLIP [73], exhibits remarkable representation capabilities for zero-shot image-text classification and retrieval. Nonetheless, it encounters difficulties in processing complex scenarios (i.e., , retrieving multi-modal documents with interleaved images and texts) [53, 54] and exhibits a limited ability to discern detailed semantic differences [79, 85]. The disparity between these two paradigms has sparked recent studies aimed at imparting discriminative ability to generative pre-trained MLLMs. However, certain aspects of performance still pose limitations (e.g., singular discriminative tasks [89], weak discriminative task performance [40], weak generalization [59], etc.), while others entail compromising the model’s original generative capabilities [8]. Overall, the reason generative paradigms struggle with performing discriminative tasks like retrieval is due to overlooking two crucial abilities: (i) Comprehensively capturing the global semantics. Recent studies have revealed that causal LLMs tend to exhibit a bias towards capturing global information from the input samples, often resulting in a tendency to overlook information located in the middle, especially for long sequences [15, 57]. As illustrated in Figure 1(a), we chose 500 samples from WebQA [10], where the task is to find and reason about the right image-text pair among five distractors to produce a yes or no answer. We conducted experiments using VILA [52], a MLLM with state-of-the-art interleaved image-text comprehension ability, alongside our model. When placing the relevant pair in different positions, the performance of MLLMs followed a ’U’ shape, indicating a bias in capturing global semantic information. Consequently, MLLMs encounter difficulties in forming comprehensive representations that encompass global semantics for retrieval tasks. (ii) Keenly differentiating the detailed semantics. Some research [47, 82] has found that the existing generative training framework cannot fully distinguish input semantics in certain contexts, causing MLLMs to struggle with tasks requiring fine-grained semantics [46, 98]. As depicted in Figure 1(b), we noticed that MLLMs face challenges in choosing the right description for two similar images in the MMVP-VLM benchmark [81]. This indicates that MLLMs struggle to effectively differentiate the detailed semantics of input samples, naturally leading to difficulties in forming effective queries for retrieval. Figure 1: (a) In WebQA [10], the accuracy roughly forms a “U” shape curve when the relevant image-text pair for a question appears at different positions. While our model also shows similar trends, it tends to be more stable overall. (b) The accuracy of various types of questions in MMVP-VLM [81], it can be observed that our model’s performance improves on such tasks after introducing the discriminative training. Details can be seen in Appendix E.3 In this paper, we argue that the current separated paradigms possess the potential for achieving synergistic gains. We propose Sugar: Structure-induced approach to unify generative and discriminative paradigms (shown in Figure 2), leveraging discriminative training to acquire the two abilities above while harnessing the potential of generative training in complex discriminative tasks like image-text interleaved retrieval and fine-grained retrieval. Specifically, we explicitly impose the semantic relationships between different input samples as an induced structural constraint on the hidden state of MLLMs. We consider the interleaved image-text sequence as the general format of input samples, and then formulate the relationship between any two samples as a dynamic sequence alignment problem within the Dynamic Time Warping framework [67, 33]. In this way, we can explicitly modulate the hidden states of the MLLM by leveraging the semantic relationships between interleaved input sequences, thereby encouraging the MLLM to fully capture the global semantics of the inputs. To further enhance the ability to differentiate fine-grained semantics, we integrate a novel kernel into the Dynamic Time Warping framework. Leveraging the strengths of various discriminative pre-trained models, it performs dynamic sequence alignment for diverse embeddings tailored to specific contexts, thus addressing the inherent limitations in fully utilizing input semantics. Through this explicit structure-induced constraint, our framework enables MLLMs to capture the global semantics and fine-grained details of the input multimodal sequence more effectively, thus bridging the gap between generative and discriminative training paradigms. Figure 2: Our structure-induced generative and discriminative training joint training strategy. Our method effectively balances both discriminative and generative tasks, demonstrating synergistic benefits. (i) Large-scale generative pre-trained models possess semantic-rich hidden states [41, 91, 23], which facilitate discriminative tasks like retrieval. Moreover, harnessing the capabilities of MLLM is crucial for complex discriminative tasks, such as interleaved image-text retrieval and fine-grained retrieval. (ii) By integrating discriminative tasks, the model’s effectiveness in generative tasks, particularly within tasks requiring cognitive and discrimination abilities, is enhanced, thereby mitigating certain occurrences of hallucinations. (iii) We can employ Sugar to realize retrieval-augmented generation [2], eliminating the need for an off-the-shelf retrieval module [75], thereby amplifying the performance of various generative tasks. The usage of off-the-shelf retrieval presents a challenge wherein the retriever’s performance affects the generator’s final output [62]. This necessitates independent optimization of both components, posing a dilemma in selecting optimal configurations. However, our approach circumvents such optimization challenges. Through extensive experimentation, we have demonstrated the effectiveness of our approach. For generative tasks, Sugar establishes new state-of-the-art results on the tasks for complicated multimodal comprehension tasks (i.e., DEMON [47]), fine-grained semantic distinctions (i.e., VizWiz [28], MME [95]), object hallucinations detection (i.e., POPE [51]) (Section 4.2 and Section 4.3). For discriminative tasks, we achieved competitive results in image-text retrieval compared, and significantly surpassed CLIP in interleaved retrieval and fine-grained retrieval (Section 4.4). Furthermore, employing the retrieval-augmented generation (RAG) strategy led to further improvements in a series of generative tasks (Section 4.5)."
https://arxiv.org/html/2411.00299v1,RadFlag: A Black-Box Hallucination Detection Method for Medical Vision Language Models,"Generating accurate radiology reports from medical images is a clinically important but challenging task. While current Vision Language Models (VLMs) show promise, they are prone to generating hallucinations, potentially compromising patient care. We introduce RadFlag, a black-box method to enhance the accuracy of radiology report generation. Our method uses a sampling-based flagging technique to find hallucinatory generations that should be removed. We first sample multiple reports at varying temperatures and then use a Large Language Model (LLM) to identify claims that are not consistently supported across samples, indicating that the model has low confidence in those claims. Using a calibrated threshold, we flag a fraction of these claims as likely hallucinations, which should undergo extra review or be automatically rejected. Our method achieves high precision when identifying both individual hallucinatory sentences and reports that contain hallucinations. As an easy-to-use, black-box system that only requires access to a model’s temperature parameter, RadFlag is compatible with a wide range of radiology report generation models and has the potential to broadly improve the quality of automated radiology reporting.","Generating accurate radiology reports from medical images is a challenging task that requires a deep understanding of medical imaging and the ability to precisely interpret and communicate complex findings. Medical Vision Language Models (VLMs) that automatically generate radiology reports from chest X-ray images have the potential to mitigate shortages of radiological expertise and improve clinical efficiency and accuracy (Bannur et al., 2024; Tanida et al., 2023; Chen et al., 2024). However, such models are prone to hallucinations, where the system generates incorrect claims that are not supported by the input image (Liu et al., 2024a; Bai et al., 2024). For example, even high-performing radiology report generation models can hallucinate in approximately 40% of generated sentences (See Table 2 in Appendix B). Such errors can mislead clinicians, with potentially severe consequences for patient care. To reduce the adverse impact of hallucinations, we propose RadFlag, a new, easy-to-use method for detecting hallucinations in AI-generated radiology reports without needing access to the AI model’s inner workings. RadFlag is the first black-box hallucination detection method specifically designed for VLMs, and flags both sentences and reports as containing hallucinated findings. We provide two key contributions: Figure 1: RadFlag flags hallucinatory text in a candidate report by comparing it against a corpus of additional reports sampled at a higher temperature. GPT-4 is used to count how many of the high-temperature reports support each sentence in the candidate report. We flag sentences with little support, as well as reports that contain multiple flagged sentences. • We develop a novel entailment scoring method specifically designed for radiology, which accurately identifies medical findings that a VLM has low confidence in. We combine this scoring method with conformal risk control thresholding to detect hallucinated sentences with high precision. • We further aggregate the sentence-level flagging results to identify reports with a high rate of hallucinations. This analysis enables selective prediction at the report level, making it possible to abstain from generating reports when there is a high chance of hallucinating. Our empirical results show that RadFlag can accurately flag 28% of hallucinatory sentences while maintaining a flagging precision of 73% on Medversa, a recent high-performing report generation model Zhou et al. (2024). At the report level, our method analyzed 208 reports generated by MedVersa and divided them into two sets: a flagged set with 4.2 hallucinations per report (n = 57) and an accepted set with only 1.9 hallucinations per report (n = 151). This approach can unlock selective prediction, where models refrain from generating reports in cases of heightened uncertainty."
https://arxiv.org/html/2411.00281v1,Detection and tracking of gas plumes in LWIR hyperspectral video sequence data,"Automated detection of chemical plumes presents a segmentation challenge. The segmentation problem for gas plumes is difficult due to the diffusive nature of the cloud. The advantage of considering hyperspectral images in the gas plume detection problem over the conventional RGB imagery is the presence of non-visual data, allowing for a richer representation of information. In this paper we present an effective method of visualizing hyperspectral video sequences containing chemical plumes and investigate the effectiveness of segmentation techniques on these post-processed videos. Our approach uses a combination of dimension reduction and histogram equalization to prepare the hyperspectral videos for segmentation. First, Principal Components Analysis (PCA) is used to reduce the dimension of the entire video sequence. This is done by projecting each pixel onto the first few Principal Components resulting in a type of spectral filter. Next, a Midway method for histogram equalization is used. These methods redistribute the intensity values in order to reduce flicker between frames. This properly prepares these high-dimensional video sequences for more traditional segmentation techniques. We compare the ability of various clustering techniques to properly segment the chemical plume. These include K-means, spectral clustering, and the Ginzburg-Landau functional.","The detection of chemical plumes in the atmosphere is a problem that has significant applications to defense, security, and environmental protection. The accurate identification and tracking of airborne toxins is crucial to combat the use of chemical gases as weapons. The problem lies in the detection of these toxins. Standard images with red, green, and blue (RGB) intensity values are not able to capture the potentially invisible gas plume and thus an alternative data representation is required. Hyperspectral imaging is utilized for its ability to capture data in the non-visible electromagnetic spectrum, instead of three channels per pixel there could be over a hundred. These extra channels allow for the possible segmentation and detection of gas plumes; however it’s offset by the fact that the images become computationally expensive to work with. In our work, we demonstrate a method to reduce the dimensions of a hyperspectral image, visualize a hyperspectral video sequence, and then perform image segmentation. The first step to visualize the data is a reduction in dimension. Within these hundreds of channels in a hyperspectral image, information of negligible importance can be found. In order to remove these unimportant information, we perform Principal Component Analysis (PCA) in order to reduce the dimensions and retain most information. Afterwards, we produce a false color movie representation of the data by selecting the three channels with most information content and setting those to be the intensity values of an RGB image. However, even slight variability of the channel signals between frames results as flicker in the video sequence. We utilize a Midway histogram equalization method to correct for this effect. This allows for a consistent pixel signature across the sequence of images. The result is a very smooth video sequence that allows for image classification and segmentation techniques. The standard for chemical plume detection in hyperspectral imaging is to utilize target signatures and match areas that are closest to the target, such as automated matched subspace detection. While these techniques are able to isolate areas of the target signature, there is a high rate of false positives . An alternate approach is image segmentation which allows for the inclusion of spectral correlation important to the detection of gas plumes. We use three different segmentation techniques: K-means, spectral clustering, and a modified MBO (Merrimen, Bence, and Osher) scheme that minimizes the Ginzburg-Landau functional. K-means is a well known clustering technique and can give useful information about the data. Spectral clustering is a technique that utilizes the graph Laplacian in order to find different clusters within the data. The last technique, a modified MBO scheme, is a semi-supervised segmentation method involving a diffusion process on a graph. In section 2 we will go over previous work that has been done in the detection of gas plumes in hyperspectral images. From there, section 3 goes into the data processing that is done. Section 4 will be an overview of the clustering methods that are used. Section 5 goes over the experimental results."
https://arxiv.org/html/2411.00274v1,Adaptive Residual Transformation for Enhanced Feature-Based OOD Detection in SAR Imagery,"Recent advances in deep learning architectures have enabled efficient and accurate classification of pre-trained targets in Synthetic Aperture Radar (SAR) images. Nevertheless, the presence of unknown targets in real battlefield scenarios is unavoidable, resulting in misclassification and reducing the accuracy of the classifier. Over the past decades, various feature-based out of distribution (OOD) approaches have been developed to address this issue, yet defining the decision boundary between known and unknown targets remains challenging. Additionally, unlike optical images, detecting unknown targets in SAR imagery is further complicated by high speckle noise, the presence of clutter, and the inherent similarities in back-scattered microwave signals. In this work, we propose transforming feature-based OOD detection into a class-localized feature-residual-based approach, demonstrating that this method can improve stability across varying unknown targets’ distribution conditions. Transforming feature-based OOD detection into a residual-based framework offers a more robust reference space for distinguishing between in-distribution (ID) and OOD data, particularly within the unique characteristics of SAR imagery. This adaptive residual transformation method standardizes feature-based inputs into distributional representations, enhancing OOD detection in noisy, low-information images. Our approach demonstrates promising performance in real-world SAR scenarios, effectively adapting to the high levels of noise and clutter inherent in these environments. These findings highlight the practical relevance of residual-based OOD detection for SAR applications and suggest a foundation for further advancements in unknown target detection in complex, operational settings.","Conventional synthetic aperture radar automatic target recognition (SAR-ATR) experiments typically rely on meticulously crafted SAR images, such as those in the Moving and Stationary Target Acquisition and Recognition (MSTAR) database [1], achieving near-perfect detection and classification rates with state-of-the-art deep learning algorithms [2, 3, 4, 5]. However, their outstanding performance fails to translate to real-world applications due to the insufficient size, quality, and completeness of actual datasets [6]. In addition to these dataset limitations, the test stage will inevitably encounter untrained targets and clutter in real-time, on-site scenarios. The presence of untrained targets poses a significant challenge, which is further complicated by the previously mentioned inadequacies of the trainable datasets. This combination makes detecting and classifying unknown targets and clutter even more difficult [7, 8]. Multiple deep-learning techniques for detecting unknown targets for SAR images have been developed over the past decades. For example, Zero-Shot Learning (ZSL) addresses this by learning from known classes and inferring unknown class samples without overlapping training and test sets, involving the extraction of image and semantic feature vectors [9]. In SAR–ATR applications of ZSL, it is essential to establish a stable and comprehensible reference space. This ensures the identification of unknown targets accurately by incorporating the prior knowledge of known targets. This deep-learning approach might be ideal because the decision boundary is learned during the training phase and, hence, less sensitive than the user-defined threshold setup [10, 11, 12, 13, 14, 15, 16, 17]. However, to date, ZSL-based unknown target detectors have been shown to be ineffective in a robust setup of the reference space. As an alternative to resolve this issue, feature extraction from the deep learning network layer has been attempted with the joint discrimination of feature extraction network (FEN), Kullback–Leibler divergence (KLD), and relative position angle (RPA) [18]. A modified polar mapping classifier (M-PMC) was also considered for unknown target detection on machine learning and pattern recognition [19]. The aforementioned methods are fairly efficient when the detector encounters a few unknown classes with sufficiently large train data sizes. However, they still face a severe and fundamental problem: the threshold for distinguishing known and unknown classes significantly varies with the number and type of unknown classes. Setting a threshold to discriminate between trained and unknown targets is necessary because their decision boundary has not been trained due to the unavailability of the complete dataset. The threshold value for a typical unknown target classifier is generally pre-tuned through experimentation with a few testing samples, depending on the decision metric variation. However, as the detector encounters more new unknown targets, this threshold becomes progressively less accurate and eventually loses its ability to filter out the unknown targets effectively [18, 19, 20, 21, 22, 23, 24]. In real-world battlefield scenarios, the number and type of unknown targets are not pre-determined; hence, we cannot continuously calibrate the threshold on a case-by-case basis. TABLE I: Examples of Out-of-Distribution Methods in Computer Vision Area OOD Method Uses Base Type Ref. Feature Logits/ Prob. Gradient Post-processing Internal Structure MSP ✓✓\checkmark✓ ✓✓\checkmark✓ [22] OpenMax ✓✓\checkmark✓ △△\triangle△ ✓✓\checkmark✓ [26] MaxLogit ✓✓\checkmark✓ ✓✓\checkmark✓ [28] Energy ✓✓\checkmark✓ ✓✓\checkmark✓ [29] ReAct ✓✓\checkmark✓ ✓✓\checkmark✓ [59] ViM ✓✓\checkmark✓ △△\triangle△ ✓✓\checkmark✓ [31] ODIN ✓✓\checkmark✓ ✓✓\checkmark✓ ✓✓\checkmark✓ [27] Mahalanobis dist. ✓✓\checkmark✓ △△\triangle△ ✓✓\checkmark✓ [24] VI/MC Dropout ✓✓\checkmark✓ [32, 33] Deep Ensembles ✓✓\checkmark✓ [34] RPL ✓✓\checkmark✓ △△\triangle△ ✓✓\checkmark✓ [41] • Check signs: main usage of the corresponding feature or type, Triangle signs: Either (1) also used in addition to the space where the check sign is applied or (2) could potentially be extended for the development of detector with the corresponding base and the type • OOD: Out of Distribution, MD: Mahalanobis distance, MSP: Maximum Softmax Probability, ReAct: Rectified Activations, ViM: Virtual-logit Matching, ML: Maximum Logits, VI: Variational Inference, MC Dropout: Monte Carlo Dropout, ODIN: Out of Distribution Detector for Neural Networks, RPL: Reciprocal Point Learning Beyond attempts at detecting unknown targets in SAR imagery, multiple novel approaches have been developed for Out-of-Distribution (OOD) detection across broader fields, including computer vision, natural language processing, and automatic speech recognition [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42]. Anomaly detection methods can be categorized into internal network structure-based and post-processing-based approaches, as summarized in Table I. Internal network structure-based methods involve mainly altering the original classification neural networks or re-training of neural networks for OOD detection. In contrast, post-processing methods perform OOD detection mainly using the outputs of these networks, often without requiring changes to the network’s classification structure, though some exceptions, such as ODIN (Out of Distribution Detector for Neural Network) and RPL (Reciprocal Point Learning) [27] [41], may involve structural modifications and re-training. Generally, internal structure-based methods cannot be applied to fully trained neural networks, as they require adjustments during the training process. Examples of OOD detection methods that involve internal network modifications include variational inference and Monte Carlo Dropout, which use Bayesian deep learning and predictive distributions derived from posterior evaluations; deep ensembles, which rely on statistical inference based on uncertainty distributions from multiple model predictions; and generative models/discriminative models with adversarial learning, which typically use reconstruction errors or likelihood estimation during re-training to distinguish OOD samples [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42]. Particularly, approaches based on the Bayesian principle involve obtaining the posterior distribution by updating prior beliefs with observed data likelihoods, with the predictive distribution derived by integrating over this posterior distribution [30, 31, 32, 33, 36, 37]. Although Bayesian deep learning offers robust uncertainty estimates in network outputs, its high computational complexity and longer inference times can hinder practicality in real-world applications. Conversely, post-processing methods extract information from feature, logit, probability (softmax), or gradient spaces and generally require less computation time. Logit/softmax-based OOD detection methods include Maximum Softmax Probability (MSP) [22], MaxLogit [28], Energy-based approaches [29], and Out-of-Distribution Detector for Neural Networks (ODIN), which may involve additional network adjustments [27]. Feature-based methods include OpenMax, ReAct (Rectified Activations), ViM (Virtual Logits Matching), and Mahalanobis distance-based techniques [26, 59, 31, 24]. For example, in logit/softmax-based OOD detection methods, MSP determines OOD/ID status based on whether the highest softmax probability—representing the model’s confidence score—exceeds a threshold established from the training dataset distribution. In feature-based OOD detection methods, OpenMax extends softmax by incorporating open set recognition, adjusting probability estimates to include an ’unknown’ class likelihood based on the distance of activations from known class centers [26]. The Mahalanobis distance method assumes that ID class features follow Gaussian distributions. It computes the Mahalanobis distance—a covariance-adjusted metric—between the test sample’s feature vector and each class’s mean feature vector. A higher distance suggests a greater likelihood of being OOD, as it falls further from the learned in-distribution feature space [24]. ViM enhances the logit space by introducing a virtual logit, derived from projecting the test sample’s feature vector onto principal components of training class feature vectors via Principal Component Analysis (PCA). This projection creates a distance metric between ID and OOD samples. By evaluating the proximity of a sample to this virtual logit, the algorithm can effectively distinguish OOD from ID samples [31]. Feature-based OOD detection methods are particularly advantageous due to their enriched representations of target characteristics, allowing for extended variability in OOD detection algorithms. Nonetheless, attempts to apply these OOD detection methods in the SAR imagery domain are highly limited. Furthermore, the feasibility and comparable performance seen in optical image datasets are uncertain, as SAR images typically have lower resolution and higher noise levels than optical images. Regardless of the underlying approach, adapting OOD detection methods to suit SAR imagery is essential but has not been thoroughly investigated. To address these challenges, our work introduces a reshaping approach that bypasses conventional feature-space OOD detection methods. Instead, it transforms feature information into localized feature-residuals by calculating within-class and between-class differences across feature vectors on a per-class basis. Through this approach, we modify the type and interpretation of inputs without altering the underlying feature-based OOD detection algorithm, thereby preserving generalizability for application across various feature-based OOD detection methods. Our contributions to unknown target detection in SAR imagery are as follows: 1. This paper proposes a novel approach that leverage class-localized residuals of feature vectors, enabling generalizability to prior feature-based OOD detection algorithms. We detail the statistical formulation and the construction of a reference space within this framework. Performance metrics are evaluated for residual-based adaptations of prior OOD detection methods under varying unknown target conditions and compared to their feature-based counterparts, providing a comprehensive analysis of the suitability and effectiveness of residual-based approaches. 2. Furthermore, we demonstrate that our approximate standardization techniques, based on residuals, effectively mitigate poor OOD detection performance in high-clutter, noisy, and low-information scenarios. This approach reduces threshold variability caused by untrained test target samples, enhancing detection accuracy. We then examine the advantages of our approach over original feature-based methods, particularly in the context of SAR image characteristics. 3. Finally, we constrain advanced OOD detection algorithms aimed at capturing subtle distributional patterns and improving accuracy through a residual-based approach. We discuss how the intrinsic distributional characteristics of residuals can be analyzed for effective OOD detection, as in-class and inter-class residuals exhibit complex, distinct patterns. Additionally, we explore future directions for optimizing threshold algorithms within our residual-based framework. The remainder of this paper is organized as follows: In Section II, we address the challenges of detecting unknown targets in real battlefield scenarios and discuss the need for a new foundation in OOD detection algorithms, particularly in light of the differences between optical and SAR imagery. We introduce our approach as a potential solution to these challenges in SAR imagery. Section III outlines our methodology for class-localized feature-residuals, discusses potential limitations, and proposes solutions for practical applications. Section IV presents experimental results using the MSTAR Database [1] under various known and unknown target conditions, detailing the OOD detection score metrics and corresponding formulations. In Section V, we explore future research directions based on our approach. Finally, we conclude our findings in Section VI. Figure 1: MSTAR Database: Optical images of military targets versus SAR images [1]. Optical images of MSTAR database targets and their corresponding SAR images. Top to bottom rows, From left to right: the targets are 2S1, BMP2, BRDM2, BTR60, BTR70, D7, T62, T72, ZIL, and ZSU. All experiments were conducted on hardware comprising an Intel Core i7-14700KF CPU with 64GB of memory and an NVIDIA GeForce RTX 3060 GPU with 12GB of memory. The software environment included a deep learning workstation running Microsoft Windows 11 Education and PyTorch 2.0.1 [56]."
https://arxiv.org/html/2411.00252v1,IO Transformer: Evaluating SwinV2-Based Reward Models for Computer Vision,"Transformers and their derivatives have achieved state-of-the-art performance across text, vision, and speech recognition tasks. However, minimal effort has been made to train transformers capable of evaluating the output quality of other models. This paper examines SwinV2-based reward models, called the Input-Output Transformer (IO Transformer) and the Output Transformer. These reward models can be leveraged for tasks such as inference quality evaluation, data categorization, and policy optimization. Our experiments demonstrate highly accurate model output quality assessment across domains where the output is entirely dependent on the input, with the IO Transformer achieving perfect evaluation accuracy on the Change Dataset 25 (CD25). We also explore modified Swin V2 architectures. Ultimately Swin V2 remains on top with a score of 95.41 %times95.41percent95.41\text{\,}\mathrm{\char 37\relax}start_ARG 95.41 end_ARG start_ARG times end_ARG start_ARG % end_ARG on the IO Segmentation Dataset, outperforming the IO Transformer in scenarios where the output is not entirely dependent on the input. Our work expands the application of transformer architectures to reward modeling in computer vision and provides critical insights into optimizing these models for various tasks.","1. Introduction Transformers have emerged as a dominant architecture in numerous domains, including natural language processing (NLP), computer vision, and speech recognition, largely due to their powerful attention mechanisms and ability to accurately model long-range dependencies. Initially introduced for NLP tasks by Vaswani et al. [1], transformers have since been adapted to vision with models like the Vision Transformer (ViT) [2] and Swin Transformer [3], which have achieved state-of-the-art performance in tasks such as image classification [4], segmentation [5], and object detection [3]. Despite these advances, there has been limited exploration into using transformer architectures for evaluating the quality of model outputs, a task that is critical in applications requiring continuous feedback or reward-based optimization, such as reinforcement learning (RL) or other decision-making frameworks. f(x)xyIO(x,y) (a) Input Output Transformer f(x)xyV(y) (b) Output Transformer Figure 1: Comparison of the Presented Reward Models In this work, we introduce two transformer-based architectures designed to serve as reward models: the Input-Output Transformer (IO Transformer) and the Output Transformer. These models are capable of assessing the quality of a model’s predictions by evaluating the relationship between the input and the output. Transformers have been extensively applied in supervised learning tasks. However, their potential to generate nuanced feedback as part of a reward function remains underexplored. Our reward models aim to fill this gap, providing more precise evaluations that account for both the quality of the input and its influence on the output. Our experiments show that the IO Transformer and Output Transformer can accurately evaluate model performance across a range of vision tasks, with a particular focus on binary image segmentation. In this task, the quality of the output, such as a segmentation mask, is often highly dependent on the input, making it an ideal use case for input-output reward models. The IO Transformer, which assesses both the input and output, excels in scenarios where the output is sensitive to variations in the input. On the other hand, the Output Transformer focuses solely on evaluating the output, making it suitable for applications where the input variability is minimal or irrelevant. This work contributes to the broader application of transformers in computer vision by introducing architectures designed to provide richer, more context-aware feedback. Our results demonstrate that the IO Transformer and Output Transformer can deliver state-of-the-art evaluation accuracy on tasks where precise feedback is critical. These reward models not only outperform traditional value networks in input-dependent tasks, but also offer the potential for future integration with reinforcement learning methods to optimize policies in complex environments, such as segmentation tasks. By presenting the IO Transformer and Output Transformer, we aim to expand the use of vision transformer architectures beyond object detection and segmentation tasks, opening new avenues for their application in reward-driven optimization and quality assessment. This work serves as a foundation for future research into the integration of transformer-based reward models with reinforcement learning frameworks, where more nuanced, context-sensitive feedback is required to improve decision-making and policy learning."
https://arxiv.org/html/2411.00246v1,ResiDual Transformer Alignmentwith Spectral Decomposition,"When examined through the lens of their residual streams, a puzzling property emerges in transformer networks: residual contributions (e.g., attention heads) sometimes specialize in specific tasks or input attributes. In this paper, we analyze this phenomenon in vision transformers, focusing on the spectral geometry of residuals, and explore its implications for modality alignment in vision-language models. First, we link it to the intrinsically low-dimensional structure of visual head representations, zooming into their principal components and showing that they encode specialized roles across a wide variety of input data distributions. Then, we analyze the effect of head specialization in multimodal models, focusing on how improved alignment between text and specialized heads impacts zero-shot classification performance. This specialization-performance link consistently holds across diverse pre-training data, network sizes, and objectives, demonstrating a powerful new mechanism for boosting zero-shot classification through targeted alignment. Ultimately, we translate these insights into actionable terms by introducing ResiDual, a technique for spectral alignment of the residual stream. Much like panning for gold, it lets the noise from irrelevant unit principal components (i.e., attributes) wash away to amplify task-relevant ones. Remarkably, this dual perspective on modality alignment yields fine-tuning level performances on different data distributions while modeling an extremely interpretable and parameter-efficient transformation, as we extensively show on more than 50 (pre-trained network, dataset) pairs.","Figure 1: From the transformer’s residual stream, we can obtain direct head contributions from across the network. In a multimodal, zero-shot classification setting (e.g., in CLIP) task boundaries originate from text that can be at different level of concept granularity. When some heads are specialized in some properties (e.g., shape, pattern, color), they can be better aligned to apply some boundaries than the original output. In this example, only the fine-grained task (brown) can correctly separate the samples at the output level. In recent times, transformers have become the backbone of most state-of-the-art machine learning systems, thanks to their adaptability to various domains, including language modeling (Brown et al., 2020; Touvron et al., 2023), vision (Dosovitskiy et al., 2021; Radford et al., 2021) and many different scientific domains (Espeholt et al., 2022; Jumper et al., 2021; Merchant et al., 2023). Traditionally, these models are treated as producing a unique, monolithic output. However, a key component in the success of transformers is the versatile inductive bias introduced by multi-head attention (MHA) layers, which alternate with multi-layer perceptrons (MLP) to form any transformer-based architecture. MHA layers are made of several independent computational units, called heads, that process input data in parallel and update the residual stream that carries it to the output via skip connections. Similarly to what had been observed for filters of convolutional neural networks (Yosinski et al., 2014; Gavrikov & Keuper, 2022), recent works point to the emergence of a specialization property in attention heads, both in large language models (Voita et al., 2019; Li et al., 2023; Chughtai et al., 2024) and in the visual branch of CLIP models (Gandelsman et al., 2024). Specialization seems to be a by-product related to large-scale training, but it is not clear exactly why it emerges and whether this is a systematic property. Interestingly, this property implies that different units might learn to attend to specific attributes or to solve specific tasks, thus processing the input in a disentangled manner, overcoming known theoretical challenges (Hyvärinen & Pajunen, 1999; Locatello et al., 2019). In modern transformer networks, the model’s final output is produced by applying a simple linear transformation to the residual stream (up to LayerNorm). This residual stream accumulates information additively, drawing from each attention head and all MLP layers (Elhage et al., 2021), producing a general-purpose representation used as a feature set for many tasks. This decomposition raises an intriguing question: are all these units essential for solving specific tasks, or do some introduce noise that obscures task-relevant information? Typically, there’s a trade-off between a model’s generalization and its performance on specific tasks. However, it might be that to specialize a model for a specific task, we don’t need to retrain the whole model. Instead, by manipulating the residual stream, we can boost the units already aligned with the task, amplifying relevant signals while reducing noise. In this paper, we tackle this question from the perspective of the latent geometry of residual units. First, on a variety of transformer-based vision models, including multiple versions of CLIP (Radford et al., 2021), BLIP (Li et al., 2022), ViT (Dosovitskiy et al., 2021) and DINOv2 (Oquab et al., 2024), we show that such units are embedded in low-dimensional manifolds and that, when there is specialization, it can be traced back to the role of few principal components. Then, by introducing a spectral analysis method based on a discrete version of principal angles (Björck & Golub, 1973), we quantitatively measure the similarity of residual units across different datasets revealing that the roles of specialized units remain surprisingly stable. Building on this insight, we hypothesize that, in many cases, the information necessary for solving a task is already embedded within a subset of highly specialized residual units. We show that this picture emerges clearly in vision-language models like CLIP, where we find units or sets of units that align with textual attributes more precisely than the full model output, on a given task. In fact, as the output combines all residual units, this relevant information may be obfuscated by other units that introduce irrelevant signals. Instead of fine-tuning the entire model, we propose to isolate and enhance the task-relevant units by filtering out the noise - akin to panning for gold. By doing so, we can significantly boost model performance with up to 4 orders or magnitude less parameters than full fine-tuning and 2 less than those needed for training a simple linear transformation at the output level. To implement this, we introduce ResiDual, a novel approach that focuses on the principal components (PCs) of the residual units to identify and retain the needed information. This framework selectively reweights the most relevant PCs, amplifying the signals that align with the task objective while remaining computationally efficient. This spectral reweighting of individual units addresses nonlinear interactions between them, and provides a geometrically principled and interpretable method for optimizing transformer models by capitalizing on the knowledge they already possess. In summary, our contributions are as follows: • We inspect the geometric structure of attention head representations in vision transformers, showcasing their low dimensionality and their increasing nonlinearity along model depth; • We characterize the emergent specialization of attention heads through their principal components and show that it stays consistent across data distributions; • We identify task-specific units in vision-language models, showcasing that focusing on these units in zero-shot settings can outperform using the full residual output when there is latent alignment between units and tasks. • We present ResiDual, a geometrically grounded method for manipulating transformers by reweighting the most relevant principal components of the residual units. This approach can sidestep the need for full-model finetuning, as it reaches competitive performance with minimal parameter overhead."
https://arxiv.org/html/2411.00239v1,Aquatic-GS: A Hybrid 3D Representation for Underwater Scenes,"Representing underwater 3D scenes is a valuable yet complex task, as attenuation and scattering effects during underwater imaging significantly couple the information of the objects and the water. This coupling presents a significant challenge for existing methods in effectively representing both the objects and the water medium simultaneously. To address this challenge, we propose Aquatic-GS, a hybrid 3D representation approach for underwater scenes that effectively represents both the objects and the water medium. Specifically, we construct a Neural Water Field (NWF) to implicitly model the water parameters, while extending the latest 3D Gaussian Splatting (3DGS) to model the objects explicitly. Both components are integrated through a physics-based underwater image formation model to represent complex underwater scenes. Moreover, to construct more precise scene geometry and details, we design a Depth-Guided Optimization (DGO) mechanism that uses a pseudo-depth map as auxiliary guidance. After optimization, Aquatic-GS enables the rendering of novel underwater viewpoints and supports restoring the true appearance of underwater scenes, as if the water medium were absent. Extensive experiments on both simulated and real-world datasets demonstrate that Aquatic-GS surpasses state-of-the-art underwater 3D representation methods, achieving better rendering quality and real-time rendering performance with a 410×\times× increase in speed. Furthermore, regarding underwater image restoration, Aquatic-GS outperforms representative dewatering methods in color correction, detail recovery, and stability. Our models, code, and datasets can be accessed at https://aquaticgs.github.io.","Figure 1: (a) Underwater imaging environment and the hybrid representation strategy employed by Aquatic-GS. (b) Scene information learned by Aquatic-GS, including water parameters, the true appearance, and the geometry of the underwater scene. ’Atten.’ and ’Coeffs’ are abbreviations for Attenuation and Coefficients, respectively. (c) Rendering of an underwater image by Aquatic-GS using a physics-based underwater imaging model. The 3D representation of underwater scenes plays a vital role in a wide range of applications, including autonomous underwater vehicles (AUVs) [1], marine ecosystem studies [2], and underwater virtual reality systems [3, 4]. An effective underwater 3D representation should encompass both objects (i.e., the water-free scene) and the water medium. The objects provide critical information about the scene’s appearance and geometry, while the water medium conveys essential water parameters. However, achieving this goal remains difficult for vision-based 3D representation methods due to the impact of distance-dependent and wavelength-selective attenuation and scattering inherent in the underwater imaging process [5]. As illustrated in Fig. 1(a), attenuation occurs when light reflected from objects is absorbed by the water as it travels toward the camera, with red light being attenuated more than blue and green, leading to a color cast in captured images. Scattering, particularly backscattering, happens when underwater ambient light is scattered toward the camera by particles suspended in the water, leading to a hazy appearance and low contrast in underwater images. Moreover, these two water effects become more pronounced as the distance between the object and the camera increases, leading to a significant coupling of the information from both the objects and the water medium. Such coupling complicates the modeling of both the objects and the water medium, hindering the capture of the true appearance, geometry, and water parameters in underwater scenes [5]. Recently, advanced 3D scene representation methods, such as Neural Radiance Field (NeRF) [6] and 3D Gaussian Splatting (3DGS) [7], have received significant attention. NeRF employs an implicit modeling strategy, typically leveraging neural networks to efficiently encode complex scenes, and utilizes volume rendering to produce photo-realistic images. In contrast, 3DGS explicitly models the scene with a set of learnable 3D Gaussian primitives and employs a tile-based rasterization pipeline, resulting in superior rendering quality, reduced training cost, and faster rendering speed compared to NeRF. However, in the context of underwater scene representation, both NeRF and 3DGS struggle to effectively represent both the objects and the water medium simultaneously. For instance, some NeRF-based studies [8, 9, 10] extend their volumetric rendering framework to accommodate the water medium; however, their implicit modeling strategies for objects often lead to blurred details, noisy geometry, and significant rendering costs. Conversely, while the explicit modeling strategy employed by 3DGS adeptly represents opaque objects, it struggles with the abundant translucent water medium [11], leading to inevitable artifacts and inaccurate geometry when applied to underwater scenes. Thus, in underwater scene representation, how to effectively represent both objects and the water medium simultaneously remains a significant challenge. To address this challenge, we propose a hybrid 3D representation approach called Aquatic-GS, which combines the advantages of explicit representation and implicit modeling to effectively capture the characteristics of both the objects and the water medium in underwater scenes, as shown in Fig. 1. Specifically, considering the spatial non-uniformity of water properties in real environments [12, 13, 14], we designed a Neural Water Field (NWF) to model the distributions of the water parameters implicitly. Simultaneously, motivated by the efficiency of 3DGS in representing objects, we utilize 3D Gaussians to explicitly capture the true appearance and geometry of the scene. Both components are integrated through a physics-based underwater image formation (UIF) model to represent the underwater scene. Moreover, to tackle the issues caused by water effects in accurately representing the scene, we introduce a Depth-Guided Optimization (DGO) mechanism. The DGO mechanism employs pseudo-depth maps generated by the latest monocular depth estimators like DepthAnything [15] to guide Aquatic-GS in achieving a more precise representation of the scene’s geometry and distant details. Once optimized, Aquatic-GS learns the true appearance, geometry, and distributions of water parameters in underwater scenes. This not only enables real-time rendering of novel underwater viewpoints but also supports restoring the underwater scene as if the water medium were absent. We evaluated Aquatic-GS’s performance in underwater novel view synthesis (NVS) and underwater image restoration (UIR) on three real-world underwater datasets and our simulated dataset. For the NVS task, Aquatic-GS outperforms state-of-the-art (SOTA) NeRF-based underwater representation methods, achieving superior rendering quality, reduced training time, and a 410×\times× increase in rendering speed for real-time performance. For the UIR task, Aquatic-GS also surpasses representative dewatering methods in color correction, detail recovery, and stability. Our contributions can be summarized as follows: • We propose Aquatic-GS, a novel 3D representation method for underwater scenes that implicitly models water parameters and explicitly models the scene’s appearance and geometry. This hybrid setup effectively represents both the water medium and the objects, ensuring a comprehensive depiction of complex underwater scenes. • We extend the latest 3DGS to underwater environments and introduce a Depth-Guided Optimization mechanism to tackle the obstacles posed by distance-dependent water effects, achieving better geometry details. • Extensive experiments on underwater novel view synthesis and image restoration tasks demonstrate the effectiveness of Aquatic-GS for underwater scene representation. The rest of this paper is structured as follows: Section II covers the related work. In Section III, we provide a detailed description of the proposed Aquatic-GS. Section IV presents extensive experimental results. Finally, Section V concludes the paper."
https://arxiv.org/html/2411.00225v2,Fashion-VDM: Video Diffusion Model for Virtual Try-On,"We present Fashion-VDM, a video diffusion model (VDM) for generating virtual try-on videos. Given an input garment image and person video, our method aims to generate a high-quality try-on video of the person wearing the given garment, while preserving the person’s identity and motion. Image-based virtual try-on has shown impressive results; however, existing video virtual try-on (VVT) methods are still lacking garment details and temporal consistency. To address these issues, we propose a diffusion-based architecture for video virtual try-on, split classifier-free guidance for increased control over the conditioning inputs, and a progressive temporal training strategy for single-pass 64-frame, 512px video generation. We also demonstrate the effectiveness of joint image-video training for video try-on, especially when video data is limited. Our qualitative and quantitative experiments show that our approach sets the new state-of-the-art for video virtual try-on. For additional results, visit our project page: https://johannakarras.github.io/Fashion-VDM.","With the popularity of online clothing shopping and social media marketing, there is a strong demand for virtual try-on methods. Given a garment image and a person image, virtual try-on aims to show how the person would look wearing the given garment. In this paper, we explore video virtual try-on, where the input is a garment image and person video. The benefit of a video virtual try-on (VVT) experience is that it would depict how a garment looks at different angles and how it drapes and flows in motion. VVT is a challenging task, as it requires synthesizing realistic try-on frames from different viewpoints, while generating realistic fabric dynamics (e.g. folds and wrinkles) and maintaining temporal consistency between frames. Additional difficulty arises if the person and garment poses vary significantly, as this creates occluded garment and person regions that need to be hallucinated. Another challenge is the scarcity of try-on video data. Perfect ground truth data (i.e. two videos of different people wearing the same garment and moving in the exact same way) is difficult and expensive to acquire. In general, available human video data, such as UBC Fashion (Zablotskaia et al., 2019), are much more scarce and less diverse than image data, such as LAION 5B (Schuhmann et al., 2022). Past approaches to virtual try-on typically leverage dense flow fields to explicitly warp the source garment pixels onto the target person frames (Wen-Jiin Tsai, 2023; Zhong et al., 2021; Jiang et al., 2022; Dong et al., 2022; Haoye Dong and Yin, 2019). However, these flow-based approaches can introduce artifacts due to occlusions in the source frame, large pose deformations, and inaccurate flow estimates. Moreover, these methods are incapable of producing realistic and fine-grained fabric dynamics, such as wrinkling, folding, and flowing, as these details are not captured by appearance flows. A recent breakthrough in image-based virtual try-on uses a diffusion model (Zhu et al., 2023), which implicitly warps the input garment under large pose gaps and heavy occlusion using spatial cross-attention. However, directly applying (Zhu et al., 2023) or other image-based try-on methods for VVT in a frame-by-frame manner creates severe flickering artifacts and temporal inconsistencies. Diffusion models (Sohl-Dickstein et al., 2015; Song and Ermon, 2019; Dhariwal and Nichol, 2021; Ho et al., 2020; Song et al., 2020) have shown promising results on various video synthesis tasks, such as text-to-video generation (Ho et al., 2022b) and image-to-video generation (Karras et al., 2023; Hu et al., 2023; Guo et al., 2023). However, a key challenge is generating longer videos, while maintaining temporal consistency and adhering to computational and memory constraints. Previous works use cascaded approaches (Ho et al., 2022a), sliding windows inference (Ho et al., 2022b; Xu et al., 2023), past-frame conditioning (Harvey et al., 2022; Lee et al., 2023; Mei and Patel, 2023), and transitions or interpolation (Chen et al., 2023a; Wang et al., 2023b). Yet, even with such schemes, longer videos are temporally inconsistent, contain artifacts, and lack realistic textures and details. We argue that, similar to context modeling for LLM’s (Chen et al., 2023b), short-video generation models can be naturally extended for long-video generation by a temporally progressive finetuning scheme, without introducing additional inference passes or multiple networks. A potential option for diffusion-based VVT is to apply an animation model to a single try-on image generated by an image try-on model. However, as this is not an end-to-end trained system, any image try-on errors will accumulate throughout the video. We argue that a single VVT model would overcome this issue by 1) injecting explicit person and garment conditioning information into the model and 2) having an end-to-end training objective. We present Fashion-VDM, the first VVT method to synthesize temporally consistent, high-quality try-on videos, even on diverse poses and difficult garments. Fashion-VDM is a single-network, diffusion-based approach. To maintain temporal smoothness, we inflate the M&M VTO (Zhu et al., 2024) architecture with 3D-convolution and temporal attention blocks. We maintain temporal consistency in videos up to 64-frames long with a single network by training in a temporally progressive manner. To address input person and garment fidelity, we introduce split classifier-free guidance (split-CFG) that enables increased control over each input signal. In our experiments, we also show that split-CFG increases realism, temporal consistency, and garment fidelity, compared to ordinary or dual CFG. Additionally, we increase garment fidelity and realism by training jointly with image and video data. Our results show that Fashion-VDM surpasses benchmark methods by a large margin and synthesizes state-of-the-art try-on videos. Figure 2. Fashion-VDM Architecture. Given a noisy video ztsubscript𝑧𝑡z_{t}italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT at diffusion timestep t𝑡titalic_t, a forward pass of Fashion-VDM computes a single denoising step to get the denoised video zt−1′subscriptsuperscript𝑧′𝑡1z^{\prime}_{t-1}italic_z start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT. Noisy video ztsubscript𝑧𝑡z_{t}italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is preprocessed into person poses Jpsubscript𝐽𝑝J_{p}italic_J start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT and clothing-agnostic frames Iasubscript𝐼𝑎I_{a}italic_I start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT, while the garment image Igsubscript𝐼𝑔I_{g}italic_I start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT is preprocessed into the garment segmentation Sgsubscript𝑆𝑔S_{g}italic_S start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT and garment poses Jgsubscript𝐽𝑔J_{g}italic_J start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT (Section 3.3). The architecture follows (Zhu et al., 2024), except the main UNet contains 3D-Conv and temporal attention blocks to maintain temporal consistency. Additionally, we inject temporal down/upsampling blocks during 64-frame temporal training. Noisy video ztsubscript𝑧𝑡z_{t}italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is encoded by the main UNet and the conditioning signals, Sgsubscript𝑆𝑔S_{g}italic_S start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT and Iasubscript𝐼𝑎I_{a}italic_I start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT, are encoded by separate UNet encoders. In the 8 DiT blocks at the lowest resolution of the UNet, the garment conditioning features are cross-attended with the noisy video features and the spatially-aligned clothing-agnostic features zasubscript𝑧𝑎z_{a}italic_z start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT and noisy video features are directly concatenated. Jgsubscript𝐽𝑔J_{g}italic_J start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT and Jpsubscript𝐽𝑝J_{p}italic_J start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT are encoded by single linear layers, then concatenated to the noisy features in all UNet 2D spatial layers."
https://arxiv.org/html/2411.00210v1,Scale-aware Recognition in Satellite Images under Resource Constraints,"Recognition of features in satellite imagery (forests, swimming pools, etc.) depends strongly on the spatial scale of the concept and therefore the resolution of the images. This poses two challenges: Which resolution is best suited for recognizing a given concept, and where and when should the costlier higher-resolution (HR) imagery be acquired? We present a novel scheme to address these challenges by introducing three components: (1) A technique to distill knowledge from models trained on HR imagery to recognition models that operate on imagery of lower resolution (LR), (2) a sampling strategy for HR imagery based on model disagreement, and (3) an LLM-based approach for inferring concept “scale"". With these components we present a system to efficiently perform scale-aware recognition in satellite imagery, improving accuracy over single-scale inference while following budget constraints. Our novel approach offers up to a 26.3% improvement over entirely HR baselines, using 76.3 % fewer HR images.","Figure 2: System overview. First, we determine which resolution is best suited for the search concept based on its scale (sec. 3.3). Then, we analyze the search area to find which regions would benefit the most from higher resolution inference (sec. 3.5). We sample the best suited regions while staying within a user specified budget. Based on this guidance we perform inference using one of three models, a high resolution satellite model, a low resolution satellite model, and a low resolution satellite model with knowledge distilled from its high resolution counterpart (sec. 3.4). This knowledge distilled model allows us to infer finer details using low resolution satellite imagery alone. The ever-increasing number of earth observation satellites (now more than 1500) offer us a unique opportunity to understand changes at the planetary scale, be it tracking the ecological degradation of forests and coral reefs [47, 16], the destruction of cultural heritage [40], or economic development and well-being [13]. A key to all these downstream applications is the ability to recognize accurately a broad vocabulary of concepts: a major computer vision challenge. An important aspect of recognition in satellite imagery is the notion of scale: Satellite image resolution is dependent on the particular satellite/sensor and is characterized by the GSD, or ground sampling distance: the distance corresponding to a single pixel. For example, Sentinel-2 images are fairly low resolution (1 pixel = 100 m2) [14], while images captured by the NAIP program are fairly high resolution (1 pixel = 1 m2) [42]. The image scale matters because many concepts of interest also have a characteristic physical size/scale. An olympic-sized swimming pool, typically about 50 m long, would be barely a speck in Sentinel-2 images. In contrast, a lake may be hundreds of square kilometers, necessitating an exceptionally large image to capture its extent in terms of NAIP imagery, which current models would not be equipped to do. The right resolution may also depend on the underlying geography. Dense urban areas with many geographical features may require high resolution data to analyze. In contrast, large swathes of uninhabited deserts can probably be analyzed accurately even with low-resolution imagery. Accuracy notwithstanding, we must also consider cost and availability. Low-resolution (LR) imagery is free, abundant, and covers the planet densely over space and time [14, 43]. In contrast, high resolution (HR) imagery typically comes from drones or low flying satellites that are commissioned as needed, and therefore come with a high cost and inconsistent temporal and spatial coverage [31, 42]. Scientists in many application domains, be it ecology, archaeology, and urban planning [47, 16, 40, 13], today manually reason about these different aspects, namely, the scale of the concept, the nature of the underlying geography, and the cost of acquired imagery, to achieve the best recognition that they can afford with their budget. We need an approach that can automate this tradeoff and automatically identify when to acquire more expensive high resolution imagery, taking into account the cost, the scale of the concept and the geography. While there is past work on the impact of scale on satellite image recognition, much of it is focused on accuracy and disregards costs [34, 26, 3]. Prior work that has looked at costs focuses on the geography but ignores the scale of the concept itself [44]. A holistic treatment of scale to achieve accurate recognition under a budget is missing. In this paper, we address this gap by proposing: • A knowledge distillation technique which allows LR models to improve significantly in recognizing finer concepts by learning from HR. • A novel approach that leverages the semantic understanding of LLMs to determine the scale of each concept. • A new approach for determining which geographical regions require higher resolution analysis by predicting when low and high resolution models might disagree, and • A unified framework that combines these ideas to yield the most accurate retrieval results for a range of concepts while adhering to a strict budget constraint. Finally, we note that while our experiments are on LR and HR satellite imagery, these techniques are more general and can generalize to any problem where there are two or more modalities with different costs and accuracy tradeoffs. We evaluate our approach with multiple recognition models (supervised and open-vocabulary) and multiple satellite modalities. We find that compared to simply using high resolution images always, our framework improves accuracy up to 13 points while reducing the number of HR images used by 5×\times×. Our approach also significantly outperforms (by more than 25 points) other prior work that trades off between accuracy and cost. In sum, our results demonstrate that our holistic reasoning of scale leads to significantly higher accuracy with large reductions in cost."
https://arxiv.org/html/2411.00209v1,Semantic Knowledge Distillation for Onboard Satellite Earth Observation Image Classification,"This study introduces a dynamic weighting knowledge distillation (KD) framework for efficient Earth observation (EO) image classification (IC) in resource-constrained environments. By leveraging EfficientViT and MobileViT as teacher models, this approach enables lightweight student models, specifically ResNet8 and ResNet16, to achieve over 90% accuracy, precision, and recall, meeting the confidence thresholds required for reliable classification. Unlike traditional KD with fixed weights, our dynamic weighting mechanism adjusts based on each teacher’s confidence, allowing the student model to prioritize more reliable knowledge sources. ResNet8, in particular, achieves substantial efficiency gains, with 97.5% fewer parameters, 96.7% fewer FLOPs, 86.2% lower power consumption, and 63.5% faster inference time compared to MobileViT. This significant reduction in complexity and resource demand makes ResNet8 an ideal choice for EO tasks, balancing high performance with practical deployment requirements. This confidence-driven, adaptable KD strategy demonstrates the potential of dynamic knowledge distillation to deliver high-performing, resource-efficient models for satellite-based EO applications. Reproducible codes are available from our shared Github repository 111 https://github.com/ltdung/SnT-SENTRY.","The rapid increase in satellite deployments for EO and remote sensing (RS) missions reflects a growing demand for applications like environmental monitoring, disaster response, precision agriculture, and scientific research [1]. These applications rely on high-frequency, high-resolution data for timely and accurate decision-making. However, a significant bottleneck in Low Earth Orbit (LEO) satellite operations is reliance on ground stations for data transmission, which limits the availability of communication windows and results in frequent connectivity loss [2]. This delay can impede critical responses in situations requiring immediate data access. The advent of Satellite Internet Providers, such as Starlink and OneWeb, offers the potential for continuous (24/7) connectivity to LEO satellites, facilitating on-demand data access [3]. Yet, seamless connectivity alone does not fully meet modern EO and RS requirements, which increasingly demand real-time, onboard decision-making. For optimal operations, onboard neural networks (NNs) must prioritize computational efficiency to autonomously analyze data, identify critical information, and make immediate adjustments, such as refocusing on a target area during subsequent satellite passes [4]. Historically, onboard NNs have been designed for efficiency, often relying on convolutional neural network (CNN) models to balance performance and resource constraints. For example, the ΦΦ\Phiroman_Φ-Sat-1 mission used a CNN-based NN for onboard image segmentation using the Intel Movidius Myriad 2 vision processing unit (VPU), representing the first deployment of deep learning on a satellite [5]. Similarly, ΦΦ\Phiroman_Φ-Sat-2 adopted a convolutional autoencoder for image compression to reduce transmission requirements, demonstrating the feasibility of lightweight models on hardware-constrained environments on three different hardware, including graphic processing unit (GPU) NVIDIA GeForce GTX 1650, VPU Myriad 2, and central processing unit (CPU) Intel Core i7-6700[6]. Despite their efficiency, CNNs can be limited in performance, especially compared to the recent success of Vision Transformer (ViT) architectures. ViTs have gained popularity in computer vision due to their ability to capture global context via self-attention mechanisms, often surpassing traditional CNNs in performance. However, ViTs require significantly more computational power and memory as image resolution increases, which poses challenges for deployment on power-constrained satellite platforms [7, 8]. To overcome these limitations, KD offers a viable approach for onboard processing. KD is a method where a smaller, simpler model (the student) learns from a larger, complex model (the teacher, such as a ViT). By transferring the teacher’s semantic knowledge, KD allows the student to generalize more effectively with lower computational demands [9]. KD was initially introduced to reduce the computational burden of deep learning models [10], and recent studies indicate that KD can help students learn complex representations with strong performance even in simplified forms [11]. In this study, we leverage KD to train deployable models for onboard EO tasks, explicitly focusing on IC. By distilling semantic knowledge from ViTs into efficient student models, we aim to boost onboard processing capabilities while maintaining computational efficiency suitable for satellite EO missions. Traditional KD approaches often struggle with training instability, mainly when exact prediction matches are enforced through Kullback-Leibler (KL) divergence from a single teacher, which can impair performance [12]. To address this, we propose a dynamic weighting mechanism for dual-teacher KD (DualKD), where the weight assigned to each teacher adapts based on their confidence level, enabling the student model to prioritize the most reliable knowledge sources. This approach considers that one instance may have varying semantic similarities to different teachers, thus improving the student’s ability to generalize data representation. Figure 1: The schematic workflow of DualKD."
https://arxiv.org/html/2411.00201v1,Evaluating the Evolution of YOLO (You Only Look Once) Models: A Comprehensive Benchmark Study of YOLO11 and Its Predecessors,"This study presents a comprehensive benchmark analysis of various YOLO (You Only Look Once) algorithms, from YOLOv3 to the newest addition. It represents the first research to comprehensively evaluate the performance of YOLO11, the latest addition to the YOLO family. It evaluates their performance on three diverse datasets: Traffic Signs (with varying object sizes), African Wildlife (with diverse aspect ratios and at least one instance of the object per image), and Ships and Vessels (with small-sized objects of a single class), ensuring a comprehensive assessment across datasets with distinct challenges. To ensure a robust evaluation, we employ a comprehensive set of metrics, including Precision, Recall, Mean Average Precision (mAP), Processing Time, GFLOPs count, and Model Size. Our analysis highlights the distinctive strengths and limitations of each YOLO version. For example: YOLOv9 demonstrates substantial accuracy but struggles with detecting small objects and efficiency whereas YOLOv10 exhibits relatively lower accuracy due to architectural choices that affect its performance in overlapping object detection but excels in speed and efficiency. Additionally, the YOLO11 family consistently shows superior performance in terms of accuracy, speed, computational efficiency, and model size. YOLO11m achieved a remarkable balance of accuracy and efficiency, scoring mAP50-95 scores of 0.795, 0.81, and 0.325 on the Traffic Signs, African Wildlife, and Ships datasets, respectively, while maintaining an average inference time of 2.4ms, a model size of 38.8Mb, and around 67.6 GFLOPs on average. These results provide critical insights for both industry and academia, facilitating the selection of the most suitable YOLO algorithm for diverse applications and guiding future enhancements.","Object detection is an essential component of computer vision systems, enabling automated identification and localization of objects within images or video frames [34]. Its applications span from autonomous driving and robotics [16] [5] [20] [56] to inventory management, video surveillance, and sports analysis [4] [23] [55] [69]. Figure 1: Evolution of YOLO Algorithms throughout the years. Over the years, object detection has developed significantly. Initially, traditional methods such as the Viola-Jones algorithm [63] and the Deformable Part-based Model (DPM) [15] used handcrafted features and were effective for applications such as face detection [63], pedestrian detection [10], and video surveillance [3]. However, these methods had limitations in robustness and generalization. With the advancement of deep learning, network-based methods have since become the primary approach. These methods are usually classified into two categories: one-stage and two-stage approaches. One-stage methods such as RetinaNet [32] and SSD (Single Shot MultiBox Detector) [35] perform detection in a single pass, balancing speed and accuracy. In contrast, two-stage methods, such as Region-based Convolutional Neural Networks (R-CNN) [19], generate region proposals and then perform classification, offering high precision but being computationally intensive. Among one-stage object detection methods, YOLO (You Only Look Once) stands out for its robustness and efficiency. Initially introduced in 2015 by Redmon et al. [21], YOLO redefined object detection by predicting bounding boxes and class probabilities directly from full images in a single evaluation [47]. This innovative approach allowed YOLOv1 to achieve real-time object detection with impressive accuracy. Building upon this foundation, YOLOv2 [48] incorporated several key enhancements. It integrated the Darknet-19 framework, a 19-layer convolutional neural network that improved feature extraction. YOLOv2 also introduced batch normalization and employed data augmentation techniques inspired by the VGG architecture [57] to enhance the model’s generalization. YOLOv3 [49] further advanced the model with the Darknet-53 framework, a deeper network that significantly improved feature extraction capabilities. This version also utilized a Feature Pyramid Network (FPN)-inspired design, which allowed for better detection across various object scales by combining high-level semantic features with low-level detailed features, and a Three-Scale detection mechanism that improved accuracy for objects of different sizes. Following YOLOv3, the model’s development branched into various communities, leading to several notable iterations. YOLOv4 [6], developed by Bochkovskiy et al., introduced enhancements such as Spatial Pyramid Pooling (SPP) and the Path Aggregation Network (PAN). SPP aggregates features from multiple scales, preserving spatial information, while PAN improves the fusion of features between layers, resulting in improved speed and accuracy. YOLOv5 [60] marked a significant transition by moving from the Darknet framework to PyTorch, a popular deep learning library. This transition made the model more accessible and easier to customize. The architecture incorporated strided convolution layers, which reduced computational load, and Spatial Pyramid Pooling Fast (SPPF) layers, optimizing memory usage while maintaining high performance. YOLOv6 and YOLOv7 continued this trajectory with innovative architectures. YOLOv6 [29] introduced RepVGG, an architecture that simplified convolutional layers during inference, and CSPStackRep blocks, which improve accuracy by splitting the feature map into two parts to process them separately. In addition, YOLOv6 employed a hybrid channel strategy for better feature representation. YOLOv7 [65] leveraged the Extended Efficient Layer Aggregation Network (E-ELAN), a novel architecture that improved efficiency and effectiveness by enhancing information flow between layers. The most recent versions of YOLO, including YOLOv8, YOLOv9, YOLOv10, and YOLO11 represent the forefront of the model’s development. YOLOv8 [58], released by Ultralytics, introduced semantic segmentation capabilities, allowing the model to classify each pixel of an image, and provided scalable versions to meet various application needs, from resource-constrained environments to high performance systems alongside other tasks such as pose estimation, image classification, and oriented object detection (OOB). YOLOv9 [66] built on its predecessors’ architectural advancements with Programmable Gradient Information (PGI), which optimizes gradient flow during training, and the Generalized Efficient Layer Aggregation Network (GELAN), which further improved performance by enhancing layer information flow. YOLOv10 [64], developed by Tsinghua University, eliminated the need for Non-Maximum Suppression (NMS) used by its predecessors, a technique used to eliminate duplicate predictions and pick the bounding boxes with the most confidence, by introducing a dual assignment strategy in its training protocol. Additionally, YOLOv10 features lightweight classification heads, spatial-channel decoupled downsampling, and rank-guided block design, making it one of the most efficient and effective YOLO models to date. Lastly, YOLO11 [26], also introduced by Ultralytics, retains the capabilities of YOLOv8 with applications such as Instance Segmentation, Pose Estimation, and Oriented Object Detection while providing 5 scalable versions for different use cases. YOLO11 replaces the C2f block from YOLOv8 with the more efficient C3k2 block, delivering improved performance without compromising speed. Additionally, it introduces the C2PSA (Cross Stage Partial with Spatial Attention) module, which improves spatial attention in feature maps, increasing accuracy, especially for small and overlapping objects. This object detection algorithm has undergone several developments as seen in Figure 1 achieving competitive results in terms of accuracy and speed, making it the preferred algorithm in various fields such as ADAS (Advanced Driver-Assist System) [47], video surveillance [38], face detection [39], and many more [18]. For instance, YOLO plays a crucial role in the agriculture field by being implemented in numerous applications such as crop classification [1] [17], pest detection [33], automated farming [67] [37], and virtual fencing [62]. Moreover, YOLO has been utilized on numerous occasions in the field of healthcare such as cancer detection [cancer] [45], ulcer detection [2], medicine classification [36] [42], and health protocols enforcement [11]. In recent years, Ultralytics has played a crucial role in the advancement of YOLO by maintaining, improving, and making these models more accessible [46]. Notably, Ultralytics has streamlined the process of fine-tuning and customizing YOLO models, a task that was considerably more complex in earlier iterations. The introduction of user-friendly interfaces, comprehensive documentation, and pre-built modules has greatly simplified essential tasks such as data augmentation, model training, and evaluation. Moreover, the development of scalable model versions allows users to select models tailored to specific resource constraints and application requirements, thereby facilitating more effective fine-tuning. For instance, YOLOv8n is favorable over YOLOv8m in scenarios where speed and computational efficiency are prioritized over accuracy, making it ideal for resource-constrained environments. The integration of advanced tools for hyperparameter tuning, automated learning rate scheduling, and model pruning has further refined the customization process. Continuous updates and robust community support have also contributed to making YOLO models more accessible and adaptable for a wide range of applications. This paper aims to present a comprehensive comparative analysis of the YOLO algorithm’s evolution. It makes a significant contribution to the field by offering the first comprehensive evaluation of YOLO11, the newest member of the YOLO family. By leveraging pre-trained models and fine-tuning them, we evaluate their performance across three diverse custom datasets, each with varying sizes and objectives. Consistent hyperparameters are applied to ensure a fair and unbiased comparison. The analysis delves into critical performance metrics, including speed, efficiency, accuracy, and computational complexity, as measured by GFLOPs count and model size. In addition, we explore the real-world applications of each YOLO version, highlighting their strengths and limitations across different use cases. Through this comparative study, we aim to provide valuable insights for researchers and practitioners, offering a deeper understanding of how these models can be effectively applied in various scenarios. The rest of this paper is organized as follows: Section 2 covers related work. Section 3 describes the datasets, the models, and the experimental setup, including the hyperparameters and evaluation metrics used. Section 4 presents the experimental results and comparative analysis alongside a discussion. Finally, Section 5 concludes with insights drawn from the study."
https://arxiv.org/html/2411.00196v1,Whole-Herd Elephant Pose Estimation from Drone Data for Collective Behavior Analysis,"This research represents a pioneering application of automated pose estimation from drone data to study elephant behavior in the wild, utilizing video footage captured from Samburu National Reserve, Kenya. The study evaluates two pose estimation workflows: DeepLabCut, known for its application in laboratory settings and emerging wildlife fieldwork, and YOLO-NAS-Pose, a newly released pose estimation model not previously applied to wildlife behavioral studies. These models are trained to analyze elephant herd behavior, focusing on low-resolution (∼similar-to\sim∼50 pixels) subjects to detect key points such as the head, spine, and ears of multiple elephants within a frame. Both workflows demonstrated acceptable quality of pose estimation on the test set, facilitating the automated detection of basic behaviors crucial for studying elephant herd dynamics. For the metrics selected for pose estimation evaluation on the test set—root mean square error (RMSE), percentage of correct keypoints (PCK), and object keypoint similarity (OKS)—the YOLO-NAS-Pose workflow outperformed DeepLabCut. Additionally, YOLO-NAS-Pose exceeded DeepLabCut in object detection evaluation. This approach introduces a novel method for wildlife behavioral research, including the burgeoning field of wildlife drone monitoring, with significant implications for wildlife conservation.","More nuanced and precise understanding of elephant behavior is crucial for developing effective conservation strategies in the face of multiplying threats, such as rapid climate change and loss of habitat and migratory corridors. African savanna elephants (Loxodonta africana) live in flexible fission-fusion societies that result in sophisticated social interactions and decision-making at different organization levels; thus elephant behavior is best understood concurrently at both the individual and group level [1]. Direct field observation is the established approach to studying elephant behavior at the spatial and temporal resolution required to gain insight into these types of sophisticated interactions. A significant disadvantage, however, is the limited field-of-view of a single observer and the practical challenges of recording simultaneously behaviors from multiple animals. Aerial-based video recording platforms are emerging as a promising approach to capturing multi-animal behavior in open terrain over greater field-of-views and spatial ranges than previously possible. For example, Koger et al. released a comprehensive software package with individualized detecting, tracking, and pose estimation modules [9]. The emergence of aerial-based video recording platforms has been enabled by continued progress in unmanned aerial vehicle technology and in computer vision. The latter was significantly advanced by the deep learning revolution, allowing the propagation of information-dense raw data throughout all modules of the system. Other important advantages of these end-to-end deep learning solutions included simplifications in piping and parameter tuning. With this revolution, however, also came the reports of instances in which the state-of-the-art methods could not generalize out-of-the-box to other domains, as they were purported to. This was particularly illustrated in fields such as computer-vision-based animal pose estimation [11] or animal detection [3, 2]. In particular, the performance gap was due to differences such as labeled dataset sizes and challenging visual discrimination conditions that were overcome by strategies such as fine-tuning on animal-specific data [12]. In this paper, we revisit this question of modular, composite solutions, such as the one provided by Koger et al., versus end-to-end solutions given the objective of extracting multi-animal pose estimates from aerial video recordings. We note that this task differs from overhead video recordings that might be found in laboratory settings due to the increased background complexity and variability and significantly smaller size of the subjects (8-70 px in our dataset). This paper details the methods for adapting this data for use by DeepLabCut [11] and explores the viability of YOLO-NAS-Pose [15], the former being a common tool used in behavioral neuroscience experiments, primarily in lab settings, and the latter being a state-of-the-art general-use pose estimation model, not traditionally used for behavioral research. DeepLabCut, often used in behavioral neuroscience within lab environments, however, has seen some application in wildlife studies. On the other hand, YOLO-NAS-Pose offers a streamlined workflow that requires minimal preprocessing for data formatting and benefits from rapid execution times. We compare the performance of a composite workflow (“DeepLabCut Workflow”) to an end-to-end pose workflow (“YOLO-NAS-Pose Workflow”) on accurately extracting pose estimates across multiple animals from aerial video recordings."
https://arxiv.org/html/2411.00192v1,Optical Lens Attack on Monocular Depth Estimation for Autonomous Driving,"Monocular Depth Estimation (MDE) is a pivotal component of vision-based Autonomous Driving (AD) systems, enabling vehicles to estimate the depth of surrounding objects using a single camera image. This estimation guides essential driving decisions, such as braking before an obstacle or changing lanes to avoid collisions. In this paper, we explore vulnerabilities of MDE algorithms in AD systems, presenting L⁢e⁢n⁢s⁢A⁢t⁢t⁢a⁢c⁢k𝐿𝑒𝑛𝑠𝐴𝑡𝑡𝑎𝑐𝑘LensAttackitalic_L italic_e italic_n italic_s italic_A italic_t italic_t italic_a italic_c italic_k, a novel physical attack that strategically places optical lenses on the camera of an autonomous vehicle to manipulate the perceived object depths. L⁢e⁢n⁢s⁢A⁢t⁢t⁢a⁢c⁢k𝐿𝑒𝑛𝑠𝐴𝑡𝑡𝑎𝑐𝑘LensAttackitalic_L italic_e italic_n italic_s italic_A italic_t italic_t italic_a italic_c italic_k encompasses two attack formats: concave lens attack and convex lens attack, each utilizing different optical lenses to induce false depth perception. We first develop a mathematical model that outlines the parameters of the attack, followed by simulations and real-world evaluations to assess its efficacy on state-of-the-art MDE models. Additionally, we adopt an attack optimization method to further enhance the attack success rate by optimizing the attack focal length. To better evaluate the implications of L⁢e⁢n⁢s⁢A⁢t⁢t⁢a⁢c⁢k𝐿𝑒𝑛𝑠𝐴𝑡𝑡𝑎𝑐𝑘LensAttackitalic_L italic_e italic_n italic_s italic_A italic_t italic_t italic_a italic_c italic_k on AD, we conduct comprehensive end-to-end system simulations using the CARLA platform. The results reveal that L⁢e⁢n⁢s⁢A⁢t⁢t⁢a⁢c⁢k𝐿𝑒𝑛𝑠𝐴𝑡𝑡𝑎𝑐𝑘LensAttackitalic_L italic_e italic_n italic_s italic_A italic_t italic_t italic_a italic_c italic_k can significantly disrupt the depth estimation processes in AD systems, posing a serious threat to their reliability and safety. Finally, we discuss some potential defense methods to mitigate the effects of the proposed attack.","Autonomous Driving (AD) systems rely on their perception modules to track and regulate the proximity to surrounding obstacles, a vital capability for ensuring the system’s safety and dependable performance. Various methods exist to accomplish this task, such as direct measurement techniques using radar or Lidar (Piotrowsky et al., 2019; Li and Ibanez-Guzman, 2020), or employing stereoscopic 3D imaging to create a dense depth map of the environment (Mayer et al., 2016; Chang and Chen, 2018; Tay et al., 2019). Cameras are among the most critical sensors in AD systems, as demonstrated by their use in vehicles from companies like Tesla (Tesla, 2022a), Uber (Uber, 2022), and Waymo (Waymo, 2022). These cameras leverage computer vision technology for various AD tasks (Zhou et al., 2023). Researchers have made significant advancements, allowing monocular cameras to estimate scene depth (Godard et al., 2019; Wong et al., 2020; Casser et al., 2019). Utilizing monocular cameras can reduce the need for additional sensors, offering benefits in terms of space, weight, and cost efficiency. Although estimating depth with a single camera poses challenges, recent deep-learning approaches have achieved performance levels close to those of stereo 3D depth estimation techniques. Previous security studies have introduced various attack techniques targeting cameras to disrupt different AD tasks, including object detection and classification (Eykholt et al., 2018; Man et al., 2020; Nassi et al., 2020), lane detection (Jing et al., 2021; Sato et al., 2021), traffic light detection (Yan et al., 2022), and vision-based depth estimation (Zhou et al., 2022; Zhang et al., 2020; Wong et al., 2020). Additionally, the security of vision-based depth estimation tasks has also been investigated. To compromise 3D stereo depth estimation, Zhou et al. (Zhou et al., 2022) propose a long-range stereo depth estimation attack that injects fake obstacle depth by projecting pure light from two complementary light sources. For monocular depth estimation (MDE) algorithms, Zhang et al. (Zhang et al., 2020) and Wong et al. (Wong et al., 2020) present white-box attacks that use imperceptible additive adversarial perturbations to alter the depth estimation results in the digital world, while a black-box attack is introduced by Daimo et al. in (Daimo et al., 2021). However, these invisible perturbations are ineffective in the physical world due to the impacts of environmental variables. Therefore, Yamanaka et al. (Yamanaka et al., 2020) and Cheng et al. (Cheng et al., 2022) create visible adversarial patches. These patches deceive depth estimation algorithms into estimating a false depth for the regions where the patterns are placed in the physical world. However, human drivers can easily detect the patches. Moreover, patches are scene-sensitive, and may not work well in dynamic environments. As opposed to existing physical attacks, we propose a universal black-box attack, L⁢e⁢n⁢s⁢A⁢t⁢t⁢a⁢c⁢k𝐿𝑒𝑛𝑠𝐴𝑡𝑡𝑎𝑐𝑘LensAttackitalic_L italic_e italic_n italic_s italic_A italic_t italic_t italic_a italic_c italic_k, that enables a new type of robust physical attack using optical lenses. L⁢e⁢n⁢s⁢A⁢t⁢t⁢a⁢c⁢k𝐿𝑒𝑛𝑠𝐴𝑡𝑡𝑎𝑐𝑘LensAttackitalic_L italic_e italic_n italic_s italic_A italic_t italic_t italic_a italic_c italic_k leverages a fundamental vulnerability of MDE systems, where even a slight change in the perceived object size in an image can lead to a corresponding shift in estimated depth. Our attack utilizes the optical lens to change the formed object size on the image sensor. Specifically, by attaching a tiny attack lens in the near front (e.g., 5⁢c⁢m5𝑐𝑚5cm5 italic_c italic_m) of the car camera, the sensed object size will be altered, which affects the depth estimation results. There are four major challenges in realizing L⁢e⁢n⁢s⁢A⁢t⁢t⁢a⁢c⁢k𝐿𝑒𝑛𝑠𝐴𝑡𝑡𝑎𝑐𝑘LensAttackitalic_L italic_e italic_n italic_s italic_A italic_t italic_t italic_a italic_c italic_k. (i) “How to design the attacks that can induce various false depth predictions?” (ii) “How to mathematically calculate the induced depth and gain control over depth estimation?” (iii) “How to configure the optimal attack parameters to accurately manipulate the depth?” And (iv) “How to verify the effectiveness of the L⁢e⁢n⁢s⁢A⁢t⁢t⁢a⁢c⁢k𝐿𝑒𝑛𝑠𝐴𝑡𝑡𝑎𝑐𝑘LensAttackitalic_L italic_e italic_n italic_s italic_A italic_t italic_t italic_a italic_c italic_k in both AI and system level?” To address the first challenge, we design L⁢e⁢n⁢s⁢A⁢t⁢t⁢a⁢c⁢k𝐿𝑒𝑛𝑠𝐴𝑡𝑡𝑎𝑐𝑘LensAttackitalic_L italic_e italic_n italic_s italic_A italic_t italic_t italic_a italic_c italic_k in two attack formats: concave lens attack and convex lens attack, which can either increase or decrease the object depth. To solve the second challenge, we mathematically model our attack using different lenses in various attack scenarios. To improve the depth manipulation precision (the third challenge), we first devise our attack in the form of targeted and untargeted attacks. Then, we adopt an optimization method to formulate the attacking process and output the optimized parameters for enhancing the attack performance. To address the last challenge, we verify the efficacy of L⁢e⁢n⁢s⁢A⁢t⁢t⁢a⁢c⁢k𝐿𝑒𝑛𝑠𝐴𝑡𝑡𝑎𝑐𝑘LensAttackitalic_L italic_e italic_n italic_s italic_A italic_t italic_t italic_a italic_c italic_k through both simulation and real-world experiments with a prototype autonomous vehicle (AV) against three state-of-the-art MDE algorithms. We also simulate our attack at an end-to-end system level using the CARLA simulator to demonstrate its potential impact in the physical world. Our experimental results demonstrate that our attack remains effective across a wide range of optical lens parameter configurations. When the optical lens is within the field of view, the attack optimization further enhances the accuracy of manipulated depths. We set up a demo website111https://lensattack.github.io/. to show our attack results, attack simulations, and physical attack video demos. The main contributions of this work are summarized as follows: • We propose a novel universal physical attack, L⁢e⁢n⁢s⁢A⁢t⁢t⁢a⁢c⁢k𝐿𝑒𝑛𝑠𝐴𝑡𝑡𝑎𝑐𝑘LensAttackitalic_L italic_e italic_n italic_s italic_A italic_t italic_t italic_a italic_c italic_k, on MDE algorithms that utilizes optical lenses. By investigating the vulnerability of the MDE, we propose the concave and convex lens attacks and mathematically model them in different attack scenarios. • We evaluate the attacks on a smartphone camera and an AV in real-world experiments to demonstrate that our attack is effective with various optical lens parameter settings. The concave lens attack results in an average error rate of 11.48% in estimated depths, whereas the convex lens attack leads to a 29.84% average error rate. • We present an attack optimization method that further improves the attack success rate by considering the factor of blurriness. The targeted average attack error rate is reduced by 6.26%, while the untargeted average attack distortion rate is increased by 11.58%. • We perform an end-to-end system-level simulation in the CARLA simulator and demonstrate the real-world ramifications of the proposed attack. This work extends our preliminary work (Zhou et al., 2024) with significant enhancements in Sections 4, 5 and 6. To further improve the effectiveness of our attack, we introduce an optimization method to optimize the focal length used in both targeted and untargeted attacks with the goal of improving the attack success rate (Section 4.3). We then evaluate the performance of the optimization method in simulation and demonstrate its improvements (Section 5.4). Additionally, to validate the feasibility of L⁢e⁢n⁢s⁢A⁢t⁢t⁢a⁢c⁢k𝐿𝑒𝑛𝑠𝐴𝑡𝑡𝑎𝑐𝑘LensAttackitalic_L italic_e italic_n italic_s italic_A italic_t italic_t italic_a italic_c italic_k, we conduct a comprehensive end-to-end system-level simulation using the CARLA simulator (Section 5.5). We also present more in-depth discussions and results on the potential defense methods in Section 6."
https://arxiv.org/html/2411.00174v1,"Pedestrian Trajectory Prediction with Missing Data: Datasets, Imputation, and Benchmarking","Pedestrian trajectory prediction is crucial for several applications such as robotics and self-driving vehicles. Significant progress has been made in the past decade thanks to the availability of pedestrian trajectory datasets, which enable trajectory prediction methods to learn from pedestrians’ past movements and predict future trajectories. However, these datasets and methods typically assume that the observed trajectory sequence is complete, ignoring real-world issues such as sensor failure, occlusion, and limited fields of view that can result in missing values in observed trajectories. To address this challenge, we present TrajImpute, a pedestrian trajectory prediction dataset that simulates missing coordinates in the observed trajectory, enhancing real-world applicability. TrajImpute maintains a uniform distribution of missing data within the observed trajectories. In this work, we comprehensively examine several imputation methods to reconstruct the missing coordinates and benchmark them for imputing pedestrian trajectories. Furthermore, we provide a thorough analysis of recent trajectory prediction methods and evaluate the performance of these models on the imputed trajectories. Our experimental evaluation of the imputation and trajectory prediction methods offers several valuable insights. Our dataset provides a foundational resource for future research on imputation-aware pedestrian trajectory prediction, potentially accelerating the deployment of these methods in real-world applications. Publicly accessible links to the datasets and code files are available at https://github.com/Pranav-chib/TrajImpute.","Pedestrian trajectory prediction [1, 2, 3, 4, 5, 6, 7] has various essential applications, such as self-driving automobiles, robot navigation, human behavior understanding, and more. These systems forecast the future trajectory of pedestrians based on their previously observed paths. Research in pedestrian trajectory prediction has significantly advanced in recent times due to the development of data-driven solutions and datasets. However, a predominant assumption in most current research is that the past observed coordinates of pedestrians are complete. This assumption does not hold in real-world scenarios [8] where sensor failures, limited field of view, and occlusion can lead to missing observations at any specific time instances, resulting in incomplete trajectories. This creates challenges for trajectory prediction tasks in real-world scenarios. To improve the effectiveness of trajectory prediction methods in real-world scenarios, they must anticipate and handle missing observed coordinates. In multivariate time series, several imputation methods [9, 10, 11, 12] have emerged that address the issue of missing features by imputing them (filling of missing values). These methods [13, 14, 15, 16] utilize statistical and deep learning approaches and have achieved state-of-the-art results for imputation time series data. However, there has been limited exploration of imputation techniques in trajectory prediction [8, 17, 18], and there is a gap in the availability of imputation-centric pedestrian datasets, evaluation protocols, and benchmarks. We introduce TrajImpute, an imputation-centric trajectory prediction dataset, to address this. We have compiled commonly used pedestrian trajectory prediction datasets [19, 20], including ETH, HOTEL, UNIV, ZARA1, and ZARA2, (which are licensed for research purposes 111See the statement at the top of https://icu.ee.ethz.ch/research/datsets.html and in the “Crowds Data” card of https://graphics.cs.ucy.ac.cy/portfolio.) and introduced trajectories with missing observed coordinates. We follow two data generation strategies to simulate the missing coordinates: easy and hard modes. In the easy mode, we simulate scenarios where observed coordinates are missed for a shorter duration (could be continuous or discontinuous time frame). In contrast, the hard mode simulates scenarios where observed coordinates are missing for a longer duration. In addition to data generation, we benchmark several existing imputation methods [16, 14, 21, 9, 22, 23] on TrajImpute. We use these imputation methods to reconstruct the missing coordinates and evaluate their performance in both easy and hard modes. After extensive evaluation, we selected the best-performing imputation model and used its imputed data for the trajectory prediction task. The motivation of our work is to provide insights into how trajectory prediction models perform when missing coordinates are imputed. Additionally, we aim to understand how imputation methods perform on the pedestrian trajectory imputation task. Thus, TrajImpute provides a dataset in which missing coordinates are present in observed trajectories to simulate real-world scenarios and offers a unified framework for evaluating both imputation and trajectory prediction methods. Contributions. We introduce TrajImpute, a trajectory prediction dataset designed to simulate missing coordinates in observed trajectories of pedestrians. TrajImpute bridges the gap between real-world scenarios and the rigid assumption that all coordinates are present in observed trajectories. We conduct extensive analyses and empirical studies to evaluate several existing imputation methods for the task of trajectory imputation on our TrajImpute dataset. Furthermore, we evaluate the performance of recent trajectory prediction methods on imputed data and provide insights for future development in this area. The dataset is provided under a Creative Commons CC BY-SA 4.0 license, allowing both academics and industry to use it."
https://arxiv.org/html/2411.00169v1,Aerial Flood Scene Classification Using Fine-Tuned Attention-based Architecture for Flood-Prone Countries in South Asia,"Countries in South Asia experience many catastrophic flooding events regularly. It takes time to execute Search and Rescue (SAR) missions in such flooded areas. With the help of image classification, it is possible to expedite such initiatives by classifying flood zones and other locations of interest like houses and humans within such regions. In this paper, we propose a new dataset to enhance SAR by collecting various aerial imagery of flooding events across South Asian countries. For the classification, we propose a fine-tuned Compact Convolutional Transformer (CCT) based approach and some other cutting-edge transformer-based and Convolutional Neural Network-based architectures (CNN). We also implement the YOLOv8 object detection model and detect houses and humans within the imagery of our proposed dataset, and then compare the performance with our classification-based approach. Since the countries in South Asia have similar topography, housing structure, the color of flood water, and vegetation, this work can be more applicable to such a region as opposed to the rest of the world. The images are divided evenly into four classes: ‘flood’, ‘flood with domicile’, ‘flood with humans’, and ‘no flood’. After experimenting with our proposed dataset on our fine-tuned CCT model, which has a comparatively lower number of weight parameters than many other transformer-based architectures designed for computer vision, it exhibits an accuracy and macro average precision of 98.62% and 98.50%. The other transformer-based architectures that we implement are the Vision Transformer (ViT), Swin Transformer, and External Attention Transformer (EANet), which give an accuracy of 88.66%, 84.74%, and 66.56% respectively. We also implement DCECNN (Deep Custom Ensembled Convolutional Neural Network), which is a custom ensemble model that we create by combining MobileNet, InceptionV3, and EfficientNetB0, and we obtain an accuracy of 98.78%. The architectures we implement are fine-tuned to achieve optimal performance on our dataset.","Floods are considered to be among the most severely impactful and frequent natural disasters in the world [1]. Due to a large number of countries consisting of coastal regions, the impact of floods on the human establishment and lifestyle is severe, especially for countries in South Asia like Bangladesh, India, and Pakistan, where human casualties due to floods are among the highest [2]. A big portion of landmass below sea level, high precipitation, rise in sea level due to climate change, and many other factors contribute to this disaster that endangers millions of people in this region by destroying their homes, resulting in the death of thousands and the displacement of millions of people [2]. For example, the flood that took place in the northeast region of Bangladesh recently in June of 2024, has left around 1.8 million people stranded, as the flood rendered many homes submerged in water [3]. During such flooding events, the majority of the people are either unprepared or unable to take preventative measures. People who live in coastal areas of Bangladesh experience extreme flooding every year, which results in heavy casualties and housing infrastructure damages [4]. A massive flood that took place in Pakistan in 2022 has placed one-third of the country under water, affecting around 33 million people in the process [5]. Usually, government agents and other aid agencies working in those areas use boats and aircraft to physically search for survivors which kills ample time lowering the rate of immediate help procedures. For decades, the flood crisis in South Asian countries has been escalating every year, as the number of deaths, the number of people rendered homeless and the magnitude of the damages keep rising. From being identified as a national crisis by the government, this is now seen as a global crisis with many countries around the world actively participating in providing aid and solutions to reduce the casualties this disaster brings to so many people in in this region. When it comes to short-term responses, providing food and shelter has always been the priority to prevent casualties. But to carry out short-term responses the survivors need to be located first, as houses and landmarks get obscured from ground level. A similar work to tackle the post-flood disaster was done in Pakistan by Munawar et al. [6] where they use Unmanned Aerial Vehicles (UAV) to identify flood-related disaster by implementing the Convolutional Neural Network (CNN). Another study [7] uses the dataset of Hurricane Harvey that occurred in Texas, United States of America. The authors use models of CNN to determine the post-flood-related catastrophe around the region. Iqbal et al. [8] use object detection to detect flood-borne objects that are responsible for blocking the escape routes of flood water. Besides, Jackson et al. [9] use various CNN models for flood image classification using the FloodNet Dataset. Many of such previous studies have encouraged us to carry out this research work to come up with solutions to the flood crisis in this region. The primary focus for the development of our work is to help reduce the number of casualties that are likely to occur during the flood crisis in South Asia. To do so, rescue operations need to be carried through faster and more accurate proceedings, which can be done through UAV deployments that will help to identify and eventually map the locations of flooded zones. The importance of improving the search and rescue initiative is paramount in South Asian countries, as looking for survivors in a harsh climate by the likes of this region makes the initiative a heavily complicated task itself. By contributing to this initiative, we aim to help the concerned authorities expedite the entire system of search and rescue via UAVs through the utilization of image classification. Through classification, it will be much easier for the concerned authorities to map the presence of flooded areas and houses and humans within these areas. With its introduction, Vision Transformers are now widely used for the execution of such image classification techniques. Since our classification will have to be implemented through a mobile component like drones or aircraft, lower computational resources will be required, for which we implement Compact Convolutional Transformers (CCT) in our work. The following are the contributions that we make in this study: 1. We propose a new dataset consisting of aerial imagery of various flooding events in South Asia that includes four categories: flood, flood with domicile, flood with humans, and no flood. We are specifically focusing on South Asian flood-prone countries, which has not been done before as most of the aerial flood image datasets include imagery focused on other regions of the world. 2. We experiment on our dataset by implementing a fine-tuned Compact Convolutional Transformer (CCT) and some other cutting-edge transformer-based architectures and CNN-based architectures used in computer vision. We compare the accuracy, precision, recall, F1-score and Matthews correlation coefficient (MCC) obtained from each of the implementations. We also implement all these models in a different aerial flood image dataset and compare the results with our dataset. 3. We implement YOLOv8, which is an object detection model, to detect houses and humans within the imagery of our dataset, and then analyze the differences between classification and detection in this particular scenario. 4. We experiment on another aerial flood imagery dataset, known as the FloodNet dataset [7], by applying the same architectures in a similar approach, after which we analyze and compare the results of both implementations. We have divided the rest of the paper into the following sections: in Section 2, we review various studies that are related to our field of research. After that, in Section 3, we discuss the various architectures that we implement by giving a brief overview of each of their working mechanisms. In Section 4, we extensively discuss the process of creating our proposed dataset. Then, in Section 5, we discuss manual parameters for each architecture and the tuning approach that we implement for each of them. After that, in Section 6, we showcase the experimental setup and experimental results, then compare the implemented approaches based on the obtained results. We discuss the real-world applicability of our implemented approaches in Section 7. Before concluding our paper, we discuss the limitations and any future implementations that we plan to execute in Section 7.4."
https://arxiv.org/html/2411.00164v1,A Recipe for Geometry-Aware 3D Mesh Transformers,"Utilizing patch-based transformers for unstructured geometric data such as polygon meshes presents significant challenges, primarily due to the absence of a canonical ordering and variations in input sizes. Prior approaches to handling 3D meshes and point clouds have either relied on computationally intensive node-level tokens for large objects or resorted to resampling to standardize patch size. Moreover, these methods generally lack a geometry-aware, stable Structural Embedding (SE), often depending on simplistic absolute SEs such as 3D coordinates, which compromise isometry invariance essential for tasks like semantic segmentation. In our study, we meticulously examine the various components of a geometry-aware 3D mesh transformer, from tokenization to structural encoding, assessing the contribution of each. Initially, we introduce a spectral-preserving tokenization rooted in algebraic multigrid methods. Subsequently, we detail an approach for embedding features at the patch level, accommodating patches with variable node counts. Through comparative analyses against a baseline model employing simple point-wise Multi-Layer Perceptrons (MLP), our research highlights critical insights: 1) the importance of structural and positional embeddings facilitated by heat diffusion in general 3D mesh transformers; 2) the effectiveness of novel components such as geodesic masking and feature interaction via cross-attention in enhancing learning; and 3) the superior performance and efficiency of our proposed methods in challenging segmentation and classification tasks.","In recent years, geometric deep learning has significantly advanced shape analysis and computer graphics applications, such as semantic segmentation, shape correspondence, and shape classification [21, 15, 41, 24, 16, 18, 8, 35]. With the recent success of Vision Transformers (ViT) in image classification and segmentation [13, 32], various methods like those in [53, 30, 9, 52] have adopted different transformers, including BERT-style, for unstructured data representation in graphs, 3D meshes, and point clouds, claiming to provide superior global and local feature learning through self-attention. Unlike vision transformers for images, unstructured 3D meshes and point clouds present challenges in tokenization, structural encoding, and generalizability. The primary obstacle is the absence of a canonical ordering and the inherent variability in size and geometry. 1.1 Patch-based Transformer Most geometric transformers utilize self-attention mechanisms that rely on node tokens [14, 20, 52, 9]. This is mostly because graph and point cloud datasets have a small number of nodes. For 3D meshes with a large number of nodes, the node and edge-based tokenization are computationally intractable as its computational cost based on global self-attention is 𝒪⁢(n2)𝒪superscript𝑛2\mathcal{O}(n^{2})caligraphic_O ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ). We follow the sub-structure tokenization approach to solve this computational bottleneck in large 3D mesh polygons. Using patches over node tokens can alleviate the quadratic complexity of global all-to-all attention. Moreover, by combining learned node-level structural embeddings, the model ensures locality awareness while enabling global attention across all nodes in polygon meshes. In this study, we demonstrate that using a hybrid approach with spectral-preserving tokenization allows us to create a scalable geometric transformer framework without the need for sampling, remeshing, or size unification, making it adaptable to arbitrary mesh sizes. 1.2 Tokenization Generating patches for unstructured 3D meshes is more complex than for images with a uniform grid and size. 3D polygon meshes have varying numbers of nodes, and generating patches is less intuitive due to the lack of a consistent node ordering or structure. Additionally, partitioning the mesh into meaningful patches requires careful consideration of the geometric properties, making the process more challenging compared to the uniform grids used in images. This study proposes a spectral-preserving method inspired by the algebraic multigrid literature, known as root-node selection [31]. We utilize this method to shape the partitions by employing a clustering algorithm that is based on an anisotropy-aware edge distance matrix calculated using Laplacian. Generally, a lack of geometrically meaningful structure encodings is present in similar studies on geometric transformers, specifically point clouds and 3D meshes. For the 3D mesh and point cloud, in [30, 53, 48], they employed the absolute positional encoding with 3D x⁢y⁢z𝑥𝑦𝑧xyzitalic_x italic_y italic_z coordinates. However, as discussed in [10], a strong SE captures the relation and implicit distance among as many tokens as possible. Heat Kernel Signature (HKS) with a simple feed-forward network can possibly play as a candidate to encode relative positional information and serve as SE. 1.3 Dense Node-level Prediction A key challenge in using patch-based transformers for geometric data with varying input sizes arises in dense prediction tasks such as segmentation. Specifically, for semantic segmentation, node-level prediction is essential, yet translating features from patch-level to node-level is not a straightforward one-to-one process. To address this, we propose two approaches. First, we introduce a U-Net-type architecture that incorporates multiple patch-size embeddings, culminating in the summation of these embeddings to enhance node-level feature representation. Alternatively, we explore concatenating the learned features from the backbone with the final interpolated features. For this approach, we employ a cross-attention layer, which offers dual benefits. Firstly, it serves as a feature interaction module within the transformer layers during training. Secondly, it facilitates the aggregation of two inherently distinct feature representations. Overall, the transition from patch-level to node-level representation is crucial for node-level prediction tasks, and our strategies are designed to optimize this process effectively. Besides this model, we use a vanilla model with just point-wise MLPs without convolution and attention modules throughout all experiments. This vanilla model does not solely outperform other models but shows the significance of the SE and geodesic masking and proves the importance of GeoTransformer layers without solely being dependent on the backbone. This enlightens future studies on mesh and point cloud transformers. Overall, our goal is to develop various components of a patch-based mesh transformer specifically designed to handle inputs of arbitrary sizes. This model incorporates multiple geometrically-aware components optimized to enhance the processing of 3D meshes effectively. Also, we show how HKS serves as a strong explicit bias toward the isometry invariance of the model by encoding implicit structural and positional embedding. Figure 1: Our framework at a glance. The two-level approach can be extended to include multiple layers of varying resolutions, each with different patch sizes, which is a common practice for segmentation. Contribution. In this paper, our contributions for the GeoTransformer are: 1. We introduce a generalizable and scalable approach for 3D mesh transformers, leveraging geometric-aware components such as root-node aggregation tokenization and heat diffusion-based positional encoding. 2. By comparing a vanilla model with just point-wise MLPs, we show how a backbone specific to 3D meshes can further distill local spatial information for better performance. 3. We propose a geodesic attention masking for the transformer to attend to specific tokens. We will experimentally show if this approach helps the vanilla model or the DiffusionNet-backed framework. 4. We also provide an optional Multi-resolution approach for segmentation to mimic a U-NET-type architecture for dense segmentation. We further prove that using node-level feature learning would suffice to avoid multi-resolution transformers. 5. We introduce a cross-attention module to serve as feature interaction with backbone and aggregation as the final step."
https://arxiv.org/html/2411.00158v1,"Using Deep Neural Networks to Quantify Parking Dwell Time††thanks:This work has been supported by the Brazilian National Council for Scientific and Technological Development (CNPq) – Grant 405511/2022-1, and by the Coordenação de Aperfeiçoamento de Pessoal de Nível Superior – Brasil (CAPES) – Finance Code 001.","In smart cities, it is common practice to define a maximum length of stay for a given parking space to increase the space’s rotativity and discourage the usage of individual transportation solutions. However, automatically determining individual car dwell times from images faces challenges, such as images collected from low-resolution cameras, lighting variations, and weather effects. In this work, we propose a method that combines two deep neural networks to compute the dwell time of each car in a parking lot. The proposed method first defines the parking space status between occupied and empty using a deep classification network. Then, it uses a Siamese network to check if the parked car is the same as the previous image. Using an experimental protocol that focuses on a cross-dataset scenario, we show that if a perfect classifier is used, the proposed system generates 75% of perfect dwell time predictions, where the predicted value matched exactly the time the car stayed parked. Nevertheless, our experiments show a drop in prediction quality when a real-world classifier is used to predict the parking space statuses, reaching 49% of perfect predictions, showing that the proposed Siamese network is promising but impacted by the quality of the classifier used at the beginning of the pipeline.","Developments regarding parking space monitoring through images have been proposed in the recent past, including the development of datasets to be used as benchmarks [1, 2, 3], deep learning-based approaches to classify the individual parking spaces between empty/occupied [2, 3, 4], and the automatic segmentation of the parking spaces positions [5, 6]. Following the thread of these deep learning-based innovations, this paper proposes an approach to counting the time a car stays parked in a parking space. To accomplish this, we use images from cameras and process these images using siamese networks. By collecting data about the time each car stays parked, we may generate helpful information about, for instance, the usage of cities’ parking areas. Furthermore, it is a common practice to have parking spaces with a maximum length of stay (e.g., 15 minutes), and a system that counts the time a driver stays parked may help authorities detect offenders. Other usages of such a system may include detecting abandoned or broken cars (e.g., a car parked for too long may be abandoned or broken) and the detection of illegally parked cars (e.g., a car that stayed stationary for too long in an area that is not defined as a parking space). Quantifying the parking time is a complex problem since 1 – the vehicles’ images are often collected at a distance, with low-resolution cameras (e.g., in the PKLot dataset, the bounding box of a car is 56 x 51 pixels in size, on average); 2 – vehicles may park in several positions relative to the camera; 3 – images are often collected with a low frame rate, where many minutes may pass between consecutive images collection (e.g., in the PKLot dataset one image is taken for every 5 minutes); 4 – changes in luminosity, occlusions, and shadows can make it difficult to compare the vehicles between different images. See an example of the same car parked in the same position at different times in Figure 1. Figure 2 shows a variety of car angles and luminosity differences in a parking lot. (a) 13:10 (b) 14:55 (c) 16:20 (d) 17:40 Figure 1: A car parked in the same position at different times – PKLot dataset. (a) PKLot (b) CNRPark-EXT Figure 2: Image examples from the PKLot and CNRPark-EXT datasets. In this work, we focus on the following contributions: • We define a siamese network to compare cars. • We define a complete pipeline to check the parking status and then verify the dwell time of each car. The complete pipeline includes a classification network to classify the parking spaces, a Convolutional Neural Network (CNN)-based siamese network to compare cars, and an algorithm that combines the information of both networks to update the dwell time of each parked car. The remainder of this work is structured as follows: We show the related works in Section II. The proposed approach, including the description of the classification and siamese networks used and the algorithm used to compute the car’s dwell time is given in Section III. In Section IV, we define the experimental protocol, where we define a cross-dataset scenario where no train samples from the target parking lot are given for the system. Section V shows that the proposed method can reach an Mean Absolute Error (MAE) of 46 and 54 minutes in the PKLot and CNRPark-EXT datasets, respectively. Finally, in Section VI, we present our conclusions."
https://arxiv.org/html/2411.00151v1,NIMBA: Towards Robust and PrincipledProcessing of Point Clouds With SSMs,"Transformers have become dominant in large-scale deep learning tasks across various domains, including text, 2D and 3D vision. However, the quadratic complexity of their attention mechanism limits their efficiency as the sequence length increases, particularly in high-resolution 3D data such as point clouds. Recently, state space models (SSMs) like Mamba have emerged as promising alternatives, offering linear complexity, scalability, and high performance in long-sequence tasks. The key challenge in the application of SSMs in this domain lies in reconciling the non-sequential structure of point clouds with the inherently directional (or bi-directional) order-dependent processing of recurrent models like Mamba. To achieve this, previous research proposed reorganizing point clouds along multiple directions or predetermined paths in 3D space, concatenating the results to produce a single 1D sequence capturing different views. In our work we introduce a method to convert point clouds into 1D sequences that maintains 3D spatial structure with no need for data replication, allowing Mamba’s sequential processing to be applied effectively in an almost permutation-invariant manner. In contrast to other works, we found that our method does not require positional embeddings, and allows for shorter sequence lengths while still achieving state-of-the-art results in ModelNet40 and ScanObjectNN datasets and surpassing Transformer-based models in both accuracy and efficiency.","Today, the transformer architecture is the most common technology powering large-scale deep learning systems. Since their introduction by Vaswani et al. (2017), transformers have been widely adopted in text (Dubey et al., 2024; Team et al., 2023), image (Dosovitskiy et al., 2020; Touvron et al., 2021; Liu et al., 2021), and video (Bertasius et al., 2021; Tong et al., 2022; Liu et al., 2022; Wang et al., 2023) data, as well as in the multimodal setting (Radford et al., 2021; Liu et al., 2024a). In 3D vision and particularly point cloud analysis, transformers achieve state-of-the-art results (Guo et al., 2021; Yu et al., 2022; Pang et al., 2022; Wu et al., 2024), often surpassing convolution-based approaches (Wu et al., 2019; Li et al., 2018) at scale. While the structure of transformers is favorable on modern hardware, their softmax attention mechanism drastically affects the model complexity with respect to the sequence length (in text) or the number of patches (in image/video/point clouds) – which scales quadratically with respect to these quantities. Over the years, this issue inspired extensive research on alternative sequence mixer strategies, such as separable attention (Wang et al., 2020; Choromanski et al., 2020; Lee-Thorp et al., 2021; Chen et al., 2021; Wortsman et al., 2023; Arora et al., 2024; Ramapuram et al., 2024), as well as the development more efficient GPU implementations of softmax attention (Dao et al., 2022; Dao, 2023; Shah et al., 2024). However, the most relevant leap forward on this issue arguably came in recently and coincided with the design of state-space models such as Mamba (Gu & Dao, 2023) as well as other parallelizable token mixers (Poli et al., 2023; De et al., 2024; Yang et al., 2023; Qin et al., 2024; Yang et al., 2024; Beck et al., 2024). SSMs are highly parallelizable RNN-like111Alternatively, SSMs can be seen as fast and well-parametrized linear attention mechanisms (Dao & Gu, 2024a; Sieber et al., 2024; Ali et al., 2024). This connection between RNNs and linear attention dates back to earlier works (Katharopoulos et al., 2020; Schlag et al., 2021). sequential blocks that sparked from the seminal work of Gu et al. (2020; 2022), where complexity scales linearly with sequence length, unlocking long-context processing in several challenging applications such as audio (Goel et al., 2022) and DNA modeling (Nguyen et al., 2024). On top of improved efficiency in the long-context setting, Mamba, xLSTMs, and other new RNN/linear attention variants often show general improvements in downstream performance (see empirical study on text in Waleffe et al. (2024)) and reasoning capabilities e.g. in the long-range arena (Tay et al., 2020) and other challenging text tasks where transformers can struggle (Beck et al., 2024). Along with 2D vision (Liu et al., 2024c; Zhu et al., 2024; Li et al., 2024), Mamba was soon applied to several 3D data domains (Xing et al., 2024; Zhang et al., 2024b) and specifically to point clouds (Liang et al., 2024; Zhang et al., 2024a; Liu et al., 2024b), where looks particularly promising since datasets such as ScanNet (Dai et al., 2017), often contain over 100⁢k100𝑘100k100 italic_k points. Compared to the 1D setting such as audio and text, where Mamba (just like any RNN variant) naturally processes data left-to-right or bidirectionally without the need for positional embeddings (Waleffe et al., 2024), 2D and 3D data is inherently not sequential and hence pose an intriguing conceptual challenge for the application of Mamba. Note that instead non-causal attention-based models are set operations222As noted by Kazemnejad et al. (2024), BERT encoders (Devlin et al., 2019) on text without positional embeddings are a bag-of-words model. Deep causal self-attention (decoders) can instead recover positional information at the second layer., where position is often added directly to the features (Vaswani et al., 2017; Su et al., 2024) or to the attention matrix (Press et al., 2021). As such, while 1D/2D and 3D data are conceptually similar in attention (i.e., set operation + positional information as a feature, see e.g. Dosovitskiy et al. (2020)), they become puzzling when an order-sensitive sequential block processes inputs. This motivates our question, which we explore in the 3D setting: How shall we apply a sequential model to non-sequential data, e.g. a point cloud? Addressing this question is scientifically intriguing, timely, and crucial for fully harnessing the potential of new efficient attention variants in the 3D domain. While inspecting the constantly growing literature on Mamba applications in 3D vision, we can see two recurring patterns333Similar discussion would hold for 2D data, see e.g. Liu et al. (2024c).. (A) 3D point cloud data has to be converted into an ordered sequence before Mamba can be applied. This has been achieved with different strategies such as reordering the points along axis and replicating the sequence (Liang et al., 2024; Zhang et al., 2024a) or scanning it from different directions (Liu et al., 2024b). (B) Much like in transformers, positional embedding are used. This information is conceptually redundant since (1) positional information is contained in the feature themselves, and (2) the ordering of patches along the constructed sequence is already used by Mamba implicitly. Note that in text, Mamba is often applied without positional embeddings (Waleffe et al., 2024). While the performance of Mamba in 3D data already shows promise, often surpassing transformers in accuracy and processing speed, points A and B above showcase that applying Mamba to point clouds poses nontrivial challenges, potentially affecting robustness and generalization out of distribution. Towards understanding and improving our understanding of the optimal preprocessing strategies for Mamba-powered models for 3D data, we introduce the following contributions: 1. We draw attention to the problem of sequence construction when applying Mamba to 2D or 3D data. We complement our discussion with both theoretical considerations on invariances and positional embeddings (Sec. 3.3) and ablations (Sec. 4). 2. We introduce NIMBA 444The name NIMBA is derived from the combination of Nimbus (latin for “dark cloud”) and Mamba., a Mamba-like model that feeds 3D data points based on an intuitive 3D-to-1D reordering strategy that preserves the spatial distance between points (Sec. 3.3.2). This strategy allows for safe removal of positional embeddings without affecting (most times, improving) performance. This is in stark contrast to all previously introduced Mamba strategies in point clouds where our ablation reveal a performance drop when positional emebddings are not used. Along with improved efficiency, our results (Sec. 4) showcase how principled ordering along a point cloud can improve performance of Mamba models in this setting. 3. We show how our ordering strategy in NIMBA drastically improves robustness of the model against data transformations such as rotations and jittering (Sec. 4). We compare our contributions with previous work in Table 1. Model Backbone Sequence length Bidirectional Pos embedding PCT Transformer N𝑁Nitalic_N × ✓ PointMAE Transformer N𝑁Nitalic_N × ✓ PointMamba Mamba 3⁢N3𝑁3N3 italic_N × ✓ Point Cloud Mamba Mamba 3⁢N3𝑁3N3 italic_N ✓ ✓ OctreeMamba Mamba N𝑁Nitalic_N ✓ ✓ Point Tramba Hybrid N𝑁Nitalic_N ✓ ✓ PointABM Hybrid N𝑁Nitalic_N ✓ ✓ NIMBA (Ours) Mamba N𝑁Nitalic_N × × Table 1: Comparison of Models based on Architecture, Sequence Length, Directionality and Positional Embedding. We denote with N𝑁Nitalic_N number of points in the point cloud."
https://arxiv.org/html/2411.00144v1,Self-Ensembling Gaussian Splatting for Few-shot Novel View Synthesis,"3D Gaussian Splatting (3DGS) has demonstrated remarkable effectiveness for novel view synthesis (NVS). However, the 3DGS model tends to overfit when trained with sparse posed views, limiting its generalization capacity for broader pose variations. In this paper, we alleviate the overfitting problem by introducing a self-ensembling Gaussian Splatting (SE-GS) approach. We present two Gaussian Splatting models named the 𝚺𝚺\mathbf{\Sigma}bold_Σ-model and the 𝚫𝚫\mathbf{\Delta}bold_Δ-model. The 𝚺𝚺\mathbf{\Sigma}bold_Σ-model serves as the primary model that generates novel-view images during inference. At the training stage, the 𝚺𝚺\mathbf{\Sigma}bold_Σ-model is guided away from specific local optima by an uncertainty-aware perturbing strategy. We dynamically perturb the 𝚫𝚫\mathbf{\Delta}bold_Δ-model based on the uncertainties of novel-view renderings across different training steps, resulting in diverse temporal models sampled from the Gaussian parameter space without additional training costs. The geometry of the 𝚺𝚺\mathbf{\Sigma}bold_Σ-model is regularized by penalizing discrepancies between the 𝚺𝚺\mathbf{\Sigma}bold_Σ-model and the temporal samples. Therefore, our SE-GS conducts an effective and efficient regularization across a large number of Gaussian Splatting models, resulting in a robust ensemble, the 𝚺𝚺\mathbf{\Sigma}bold_Σ-model. Experimental results on the LLFF, Mip-NeRF360, DTU, and MVImgNet datasets show that our approach improves NVS quality with few-shot training views, outperforming existing state-of-the-art methods. The code is released at:https://github.com/sailor-z/SE-GS.","Novel view synthesis (NVS) is a critical task [49] in computer vision and graphics, playing a pivotal role in applications such as virtual reality [8], augmented reality [50], and 3D content generation [15, 42]. The objective of NVS is to generate photo-realistic images from previously unseen viewpoints. Typically, NVS starts by constructing a 3D representation [26, 39] from a set of existing 2D observations. In recent years, 3D Gaussian Splatting (3DGS) [20, 6, 47] has emerged as a powerful representation, integrating the advantages of both explicit [33] and implicit [26] representations. This approach enables efficient novel view generation and yields promising synthesized results with densely sampled observations that cover a wide range of viewpoints. (a) Training (b) Testing Figure 2: Overfitting in 3D Gaussian Splatting with few-shot training views. 2(a) and 2(b) illustrate the performance of 3DGS on training and testing views, respectively. Each curve represents the PSNR values across different training iterations. However, 3DGS tends to overfit the available views when only a limited number of images are provided. As shown in Fig. 2, we evaluate the 3DGS model trained on sparse images with different numbers of iterations. The performance on the training data consistently improves as the number of iterations increases, while the testing results deteriorate after 2000 iterations. Moreover, the overfitting issue becomes more noticeable with fewer training views, such as when using only 3 views. To address this problem, we introduce a new 3DGS approach in this paper, which enhances the quality of novel view synthesis with sparse training views via self-ensembling. Self-ensembling [11, 21, 27] has been witnessed in the literature as an effective strategy to mitigate overfitting, particularly in scenarios where limited data is available [45]. This technique leverages multiple predictions from the model under different conditions such as dropout [37] and data augmentation [36], forming a consensus prediction via regularization. By aggregating outputs with diverse perturbations, self-ensembling creates a more stable target for training, reducing the noise in individual predictions. Despite its success, how to utilize self-ensembling in the 3DGS pipeline remains an open problem. Therefore, we explore this gap, introducing a new 3DGS training pipeline consisting of a 𝚺𝚺\mathbf{\Sigma}bold_Σ-model and a 𝚫𝚫\mathbf{\Delta}bold_Δ-model. Specifically, we present a 𝚫𝚫\mathbf{\Delta}bold_Δ-model that represents a temporal sample in the Gaussian parameter space. Notably, in the scenario of sparse views, the training data lacks sufficient information to identify an optimal sample in the parameter space. As a result, the training process tends to converge to a local optimum, leading to an overfitting problem. To tackle this issue, one could create more samples by training multiple 𝚫𝚫\mathbf{\Delta}bold_Δ-models, thereby increasing the likelihood of reaching a global optimum. Nevertheless, as we will demonstrate in Sec. 4, this approach incurs significant computational costs, limiting its ability to produce diverse samples. In contrast, we train a single 𝚫𝚫\mathbf{\Delta}bold_Δ-model following the typical 3DGS training strategy [20], while obtaining various samples via perturbing the 𝚫𝚫\mathbf{\Delta}bold_Δ-model. Rather than applying random perturbations, which often result in invalid samples, we perturb the 𝚫𝚫\mathbf{\Delta}bold_Δ-model based on uncertainties derived from the training data. We store images generated by rendering the 𝚫𝚫\mathbf{\Delta}bold_Δ-model from a set of pseudo views in buffers at different training iterations. These pseudo views are randomly sampled from the camera trajectories of the training views. We assess the reliability of the renderings by calculating the pixel-level uncertainties across the images within each buffer. Pixels with large uncertainty scores are identified, and the Gaussian parameters of the 𝚫𝚫\mathbf{\Delta}bold_Δ-model associated with these pixels are perturbed by random noise. Therefore, our perturbation strategy generates uncertainty-aware temporal samples without introducing significant training overhead. Building upon the 𝚫𝚫\mathbf{\Delta}bold_Δ-model, we construct an ensemble of the temporal samples by proposing a 𝚺𝚺\mathbf{\Sigma}bold_Σ-model. We train the 𝚺𝚺\mathbf{\Sigma}bold_Σ-model over the training views without the aforementioned perturbations. To achieve self-ensembling, we regularize the geometry of the 𝚺𝚺\mathbf{\Sigma}bold_Σ-model, minimizing the discrepancies between the 𝚺𝚺\mathbf{\Sigma}bold_Σ-model and the perturbed 𝚫𝚫\mathbf{\Delta}bold_Δ-model. The discrepancies are measured through a photometric loss between images synthesized from the pseudo views. During testing, we employ the 𝚺𝚺\mathbf{\Sigma}bold_Σ-model as the primary model for novel view synthesis. Since the regularization is performed based on diverse samples in the Gaussian parameter space, the self-ensembling process enhances the robustness of the 𝚺𝚺\mathbf{\Sigma}bold_Σ-model, thereby improving the generalization ability to novel views. We conduct experiments on several datasets consisting of LLFF [25], DTU [19], Mip-NeRF360 [3], and MVImgNet [46]. The evaluation is performed with varying numbers of training views. Our SE-GS achieves the best performance across all the datasets when limited training views are used, surpassing the state-of-the-art approaches. Additionally, we provide a comprehensive analysis in which the results demonstrate the effectiveness and efficiency of our method. To the best of our knowledge, we are the first to explore the potential of the ensembling mechanism in 3DGS for few-shot novel view synthesis. We present a new self-ensembling technique that effectively mitigates the overfitting of 3DGS without requiring additional ground-truth signals or incurring significant training overhead."
https://arxiv.org/html/2411.00128v1,Muscles in Time: Learning to Understand Human Motion by Simulating Muscle Activations,"Exploring the intricate dynamics between muscular and skeletal structures is pivotal for understanding human motion. This domain presents substantial challenges, primarily attributed to the intensive resources required for acquiring ground truth muscle activation data, resulting in a scarcity of datasets. In this work, we address this issue by establishing Muscles in Time (MinT), a large-scale synthetic muscle activation dataset. For the creation of MinT, we enriched existing motion capture datasets by incorporating muscle activation simulations derived from biomechanical human body models using the OpenSim platform, a common approach in biomechanics and human motion research. Starting from simple pose sequences, our pipeline enables us to extract detailed information about the timing of muscle activations within the human musculoskeletal system. Muscles in Time contains over nine hours of simulation data covering 227 subjects and 402 simulated muscle strands. We demonstrate the utility of this dataset by presenting results on neural network-based muscle activation estimation from human pose sequences with two different sequence-to-sequence architectures. Data and code are provided under https://simplexsigil.github.io/mint.","Like prisoners in Plato’s cave, neural networks for human motion understanding often rely on indirect representations rather than direct, biologically grounded data. In Plato’s allegory, prisoners in a cave see only shadows cast on the wall, not the true objects. Similarly, neural networks trained on accessible data, such as RGB and depth-based video recordings or motion capture, only perceive surface-level appearance of motion in contrast to the inner mechanics of the human body. This reliance on external visual observations provides an incomplete understanding of the true complexities of human motion. Just as the prisoners lack a direct view of the objects casting the shadows, current models lack exposure to the internal workings of the human body, such as the muscle activations driving motion. This gap limits their ability to develop an in-depth understanding of physical exertion, motion difficulty, and mass impact on the body. Our community has progressed from capturing human motion with camera sensors and predicting activities to pose-based recognition systems that account for the body and its motion over time. These advances, while significant, still overlook the interplay of muscle activations, which are the root of pose sequences and patterns. Figure 1: Simulation pipeline of the Muscles in Time dataset. The SMPL representation is extracted from videos, then, the SMPL represented motions are mapped to bio-mechanically validated human body models to simulate fine-grained muscle activation, connecting computer vision with biomechanical research. Bottom right: two activation sequences for exemplary muscles. Images from [47, 15] . Collecting electromyographic (EMG) data or more commonly used surface electromyographic (sEMG) data, as a measure of muscle activation, presents challenges. It is resource intensive, requiring specialized equipment, controlled environments, and is an invasive procedure. Existing EMG and sEMG datasets are small, limited in scope, and not representative of the variety of human motions. These limitations hinder the development of neural networks that can generalize across different types of motion and subjects. While acknowledging the contributions of EMG and sEMG datasets, we identify an opportunity to supplement this domain with a synthetic dataset that overcomes some limitations of real-world data collection. The strength of our dataset lies in its scale and detail of muscle activation data, a feat not achievable through conventional methods alone. Every dataset, simulated or real, has domain-specific fidelity and relevance. Real-world recordings offer authenticity that underpins our understanding of human biomechanics with nuances, such as EMG measurements being subject-specific and varying over the course of one day. Simulated datasets, like ours, offer a complementary perspective by providing comprehensive data for the understanding of muscle activation patterns through a scalable data acquisition pipeline. In this work, we present a comprehensive large-scale dataset incorporating muscle activation information. We enrich existing motion capture datasets with muscle activation simulations from biomechanical models of the human body. Our pipeline uses simple pose and shape sequences with estimated weight and mass of the human body to simulate muscle activations for individual movements. Using this, we generate the muscles’ activation that fit the provided human motions. Figure 1 provides an overview of our pipeline. We showcase the utility of muscle activations as an additional data type for human motion understanding and gather insights by visualizing the intricate details of our data. Our dataset, the first of its magnitude and detail, describes muscle activation across a wide array of movements. By enhancing the current set of tools available to researchers, we expand the potential for scientific investigation and innovation in the study of human motion."
https://arxiv.org/html/2411.00078v1,How Good Are We? Evaluating Cell AI Foundation Models in Kidney Pathology with Human-in-the-Loop Enrichment,"Training AI foundation models has emerged as a promising large-scale learning approach for addressing real-world healthcare challenges, including digital pathology. While many of these models have been developed for tasks like disease diagnosis and tissue quantification using extensive and diverse training datasets, their readiness for deployment on some arguably simplest tasks, such as nuclei segmentation within a single organ (e.g., the kidney), remains uncertain. This paper seeks to answer this key question, “How good are we?”, by thoroughly evaluating the performance of recent cell foundation models on a curated multi-center, multi-disease, and multi-species external testing dataset. Additionally, we tackle a more challenging question, “How can we improve?”, by developing and assessing human-in-the-loop data enrichment strategies aimed at enhancing model performance while minimizing the reliance on pixel-level human annotation. To address the first question, we curated a multicenter, multidisease, and multispecies dataset consisting of 2,542 kidney whole slide images (WSIs). Three state-of-the-art (SOTA) cell foundation models—Cellpose, StarDist, and CellViT—were selected for evaluation. To tackle the second question, we explored data enrichment algorithms by distilling predictions from the different foundation models with a human-in-the-loop framework, aiming to further enhance foundation model performance with minimal human efforts. Our experimental results showed that all three foundation models improved over their baselines with model fine-tuning with enriched data. Interestingly, the baseline model with the highest F1 score does not yield the best segmentation outcomes after fine-tuning. This study establishes a benchmark for the development and deployment of cell vision foundation models tailored for real-world data applications.","AI foundation models trained on massive, diverse datasets are widely applied across numerous fields, including healthcare [1, 2]. Their versatility enables these models to tackle a variety of downstream tasks, with one of the most prominent applications being digital pathology [3, 4, 5, 6]. Among many tasks in digital pathology, cell instance segmentation is often serving as the initial step in extracting biological signals crucial for accurate disease diagnosis and treatment planning [7, 8, 9, 10, 11, 12]. The accuracy of the cell or cell nuclei segmentation forms the foundation for various subsequent biological or medical analyses, including cell type classification [13], specific cell counting [14], and cell phenotype analysis [15]. It is also considered as the stepping stone for whole slide image (WSI) analysis in any biological and biomedical applications [16]. In recent years, several cell foundation models [17, 18, 19, 20] trained on large and diverse datasets—encompassing various cell types, imaging techniques (e.g., fluorescence, brightfield, phase contrast), and experimental conditions—have demonstrated promising results. While many of these models have been developed for tasks such as disease diagnosis and tissue quantification, their readiness for deployment on some of the arguably simpler tasks, such as nuclei segmentation within a single organ (e.g., the kidney), remains uncertain. This leads us to pose the first question: Is nuclei segmentation on whole slide images (WSIs) within a single organ, like the kidney, a solved problem using current cell foundation models in histopathology? Figure 1: Overall framework. The upper panel illustrates the diverse evaluation dataset consisting of 2,542 kidney WSIs. Performance: Kidney cell nuclei instance segmentation was performed using three SOTA cell foundation models: Cellpose, StarDist, and CellViT. Model performance was evaluated based on qualitative human feedback for each prediction mask. Data Enrichment: A human-in-the-loop (HITL) design integrates prediction masks from performance evaluation into the model’s continual learning process, reducing reliance on pixel-level human annotation. To answer the preceding question, we have chosen the kidney as the evaluation organ for this study due to its diverse cell types. According to [21], the kidney comprises at least 16 specialized epithelial cell types, along with various endothelial, immune, and interstitial cells. Additionally, kidney whole slide images and their primary staining type, Periodic Acid-Schiff (PAS), are underrepresented in the training of current cell nuclei foundation models, underscoring a gap in the research field. We evaluated the performance of current cell nuclei foundation models in kidney pathology within a multi-center, multi-staining, multi-species setting. As shown in Fig. 1a, b, and c, we construct a diverse evaluation dataset that includes kidney nuclei data from 2,542 kidney whole slide images (WSIs) sourced from humans and rodents across both public and in-house datasets. To our knowledge, the scale of this study’s kidney WSIs surpasses all publicly available labeled nuclei datasets that include the kidney, as illustrated in Fig. 1a. We conducted a comparative analysis of three widely used SOTA cell nuclei foundation models—Cellpose, StarDist, and CellViT—for kidney cell nuclei instance segmentation. Model performance was evaluated by collectively rating each model’s prediction mask as “good,” “medium,” or “bad.” Specifically, “good” predictions were defined as those capturing approximately 90% of the nuclei in an image patch, while “medium” predictions captured between 50% and 90% of the nuclei. Predictions rated as “bad” captured less than 50% of the nuclei. This quantitative definition clarifies the criteria for each rating type, based on a standard established by a renal pathologist at Vanderbilt University Medical Center with over 20 years of experience. Our evaluation results indicated that a performance gap still exists in general nuclei segmentation for kidney pathology. Among the evaluated models, CellViT exhibited the best performance in segmenting kidney nuclei, achieving 63% of “good” ratings on the evaluation dataset. Then, a natural follow-up question is: can we further enhance the performance of these trained cell foundation models? To tackle this, we performed data enrichment by ensembling predictions from three foundation models within a human-in-the-loop (HITL) design. Specifically, the “bad” image patches, manually corrected by pathologists, incorporated the domain knowledge required by the models, while the “good” predictions distilled insights from various foundation models. We assessed the model’s performance gains from three data enrichment strategies: using “good” image patches only, using “bad” image patches only, and using a combination of both. Our experimental results demonstrated that foundation models can mutually enhance their segmentation capabilities through data enrichment strategies, with StarDist achieving the highest F1 score of 0.82. The baseline model with the highest F1 score (CellViT) does not yield the best segmentation outcomes after fine-tuning. The combination of “good” and “bad” image patches proved to be the most effective strategy. Ultimately, these improved models can facilitate more efficient workflows in annotation software, such as QuPath [22], for kidney pathology."
https://arxiv.org/html/2411.00749v1,PathoGen-X: A Cross-Modal Genomic Feature Trans-Align Network for Enhanced Survival Prediction from Histopathology Images,"Accurate survival prediction is essential for personalized cancer treatment. However, genomic data – often a more powerful predictor than pathology data – is costly and inaccessible. We present the cross-modal genomic feature translation and alignment network for enhanced survival prediction from histopathology images (PathoGen-X). It is a deep learning framework that leverages both genomic and imaging data during training, relying solely on imaging data at testing. PathoGen-X employs transformer-based networks to align and translate image features into the genomic feature space, enhancing weaker imaging signals with stronger genomic signals. Unlike other methods, PathoGen-X translates and aligns features without projecting them to a shared latent space and requires fewer paired samples. Evaluated on TCGA-BRCA, TCGA-LUAD, and TCGA-GBM datasets, PathoGen-X demonstrates strong survival prediction performance, emphasizing the potential of enriched imaging models for accessible cancer prognosis.","Advances in cancer survival analysis are essential for assessing progression risks and guiding treatment strategies [1, 2]. Clinically, prognostic models rely on a range of biomarkers, such as imaging data and genetic profiles, to estimate outcomes [3]. With the advent of deep learning (DL), there is growing interest in using routinely stained histopathology images to predict survival outcomes [4, 5, 6]. High-resolution whole-slide images (WSIs) capture cellular structures linked to cancer stage and severity, offering weak supervisory signals that can predict treatment outcomes. However, imaging-based markers lack the depth and specificity of molecular profiling, such as RNA sequencing, which provides more robust information on tumor progression [3]. Though the gene-expression data are highly informative for survival predictions, their limited availability, due to extraction costs, often restricts their use [1]. Efficient utilization of available gene-expression data with the histopathology imaging data can thus significantly improve the performance of survival prediction models [7, 3]. In this context, a common approach to improve survival predictions involves multi-modal information fusion of genomic and imaging data [7]. However, this method usually requires both data types during training and testing, which is often impractical . A more feasible framework is to utilize genomic and imaging data during training but only requires imaging data at testing. To this end, we introduce a cross-modal genomic feature translation and alignment network (PathoGen-X), a DL pipeline for inter-modal feature alignment and translation based on the principles of “stronger signals enhancing weaker signals”. PathoGen-X consists of a stack of transformer-based encoder-decoder network that are optimized to effectively align and translate histopathology image based weaker modality signals to the feature space of RNA-seq data based strong modality feature space in the training phase. Our approach differs from popular alternative methods, such as unsupervised pre-training based on contrastive learning or similarity learning methods in which a similarity loss is optimized to project both modalities to a shared latent space [3]. Additionally, PathoGen-X requires fewer training pairs of samples from both modalities, making it more sample efficient than the methods that employs contrastive loss functions [7, 3]. In the test phase, PathoGen-X utilizes the translated projection space to achieve robust survival predictions based solely on imaging data. To demonstrate the effectiveness of the proposed DL pipeline, we evaluated its performance on three publicly available datasets [8]—TCGA-BRCA, TCGA-LUAD, and TCGA-GBM, that provide both WSI imaging and RNA-sequencing data. We show the predictive power of our methodology by comparing against popular baselines, that employs imaging, imaging and genomic data using the concordance index (c-index) for survival prediction. Our method achieves performance comparable to a model trained solely on genomic data while using only imaging data at test time. The superior performance of PathoGen-X, highlight the need to use any available complementary information in training DL models, such as RNA data, to work with imaging data. Fig. 1: The cross-modal genomic feature trans-align network (PathoGen-X) features four main components: a pathology encoder, a genomic decoder, a genomic projection matrix (PM) and a survival prediction module."
https://arxiv.org/html/2411.00726v1,Cross-Fundus Transformer for Multi-modal Diabetic Retinopathy Grading with Cataract,"Diabetic retinopathy (DR) is a leading cause of blindness worldwide and a common complication of diabetes. As two different imaging tools for DR grading, color fundus photography (CFP) and infrared fundus photography (IFP) are highly-correlated and complementary in clinical applications. To the best of our knowledge, this is the first study that explores a novel multi-modal deep learning framework to fuse the information from CFP and IFP towards more accurate DR grading. Specifically, we construct a dual-stream architecture Cross-Fundus Transformer (CFT) to fuse the ViT-based features of two fundus image modalities. In particular, a meticulously engineered Cross-Fundus Attention (CFA) module is introduced to capture the correspondence between CFP and IFP images. Moreover, we adopt both the single-modality and multi-modality supervisions to maximize the overall performance for DR grading. Extensive experiments on a clinical dataset consisting of 1,713 pairs of multi-modal fundus images demonstrate the superiority of our proposed method. Our code will be released for public access.","Diabetic retinopathy (DR) is one of the chronic complications of diabetes mellitus and the leading cause of avoidable blindness [12]. According to the International Clinical Diabetic Retinopathy Scale [16], the severity of DR is graded into five grades, i.e., no DR, mild Nonproliferative DR (NPDR), moderate NPDR, severe NPDR, and proliferative DR (PDR). In clinical practice, color fundus photography (CFP) is commonly used for DR screening. However, patients with cataract have serious lens opacity, which obviously affects the imaging effect of CFP. Instead, infrared fundus photography (IFP) can obtain clear retinal images due to the good penetration of infrared light to turbidity media Thus, IFP can complement other imaging techniques in monitoring and assessing treatment response in DR patients [18, 13, 10]. Fig. 1 shows some examples of CFP and IFP images. Fig. 1a illustrates that CPF and IFP of healthy people have clear fundus structure which exhibits a significant correlation in terms of image resolution and utility [2]. On one hand, Fig. 1b captured from a cataract patient shows that CFP is obscured by a gray fog, while IFP remains clear enough to discern the fundus structure. Particularly, useful features for DR diagnosis such as hemorrhages (blue box) and exudate (yellow box) can be more easily identified in IFP. On the other hand, as illustrated in Fig. 1c that there are appearance differences of lesions between the two modalities. Specifically, CFP provides better color information that aids doctors in distinguishing lesions, which are often overlooked in IFP [13]. This dichotomy underscores the potential of leveraging the complementary strengths of both modalities to enhance DR grading accuracy. Figure 1: Examples of CFP and IFP image. (a) Both clear images; (b) The same lesion is unclear in CFP while clear in IFP; (c) Comparison of different DR lesions in CFP and IFP. Figure 2: The architecture of our Cross-Fundus Transformer (CFT) for DR grading. It consists Transformer encoders, linear projection layers, Cross-Fundus Attention (CFA) module, classifier layer and MLP heads. In recent years, there has been an increase in the use of multi-modal fundus images for retinal disease diagnosis. Most existing works focus on the fusion of CFP and optical coherence tomography (OCT). For example, Wang et al. [15] proposed a multi-modal learning network, termed GeCoM-Net, which encodes the geometric correspondences between OCT slices and their associated CFP regions. This approach advances the diagnosis of retinal diseases such as diabetic macular edema, impaired visual acuity and glaucoma, demonstrating enhanced diagnostic efficacy. Other works consider the differences between modalities and design tailored fusion strategies [14, 9, 7]. However, multi-modal interaction of CFP and IFP for retinal disease diagnosis is still largely unexplored. In this study, we propose a novel dual-stream network architecture, named Cross-Fundus Transformer (CFT), based on a vision transformer (ViT) [3] to diagnose DR using both CFP and IFP images. Our approach involves generating CFP and IFP tokens using ViT, which are then fused by a Cross-Fundus Attention (CFA) module to obtain a unified representation for DR grading. We particularly construct a clinical dataset, consisted of 1,713 pairs of CFP and IFP images, to evaluate the effectiveness of our algorithm. Our work represents the first attempt to automatically diagnose DR using both CFP and IFP medical images, and our experimental results demonstrate state-of-the-art performance."
https://arxiv.org/html/2411.00688v1,Why do we regularise in every iterationfor imaging inverse problems?,"Regularisation is commonly used in iterative methods for solving imaging inverse problems. Many algorithms involve the evaluation of the proximal operator of the regularisation term in every iteration, leading to a significant computational overhead since such evaluation can be costly. In this context, the ProxSkip algorithm, recently proposed for federated learning purposes, emerges as an solution. It randomly skips regularisation steps, reducing the computational time of an iterative algorithm without affecting its convergence. Here we explore for the first time the efficacy of ProxSkip to a variety of imaging inverse problems and we also propose a novel PDHGSkip version. Extensive numerical results highlight the potential of these methods to accelerate computations while maintaining high-quality reconstructions. Keywords— Inverse problems, Iterative regularisation, Proximal operator, Stochastic optimimisation.","Inverse problems involve the process of estimating an unknown quantity 𝒖†∈𝕏superscript𝒖†𝕏\bm{u}^{\dagger}\in\mathbb{X}bold_italic_u start_POSTSUPERSCRIPT † end_POSTSUPERSCRIPT ∈ blackboard_X from indirect and often noisy measurements 𝒃∈𝕐𝒃𝕐\bm{b}\in\mathbb{Y}bold_italic_b ∈ blackboard_Y obeying 𝒃=𝑨⁢𝒖†+𝜼𝒃𝑨superscript𝒖†𝜼\bm{b}=\bm{A}\bm{u}^{\dagger}+\bm{\eta}bold_italic_b = bold_italic_A bold_italic_u start_POSTSUPERSCRIPT † end_POSTSUPERSCRIPT + bold_italic_η. Here 𝕏,𝕐𝕏𝕐\mathbb{X},\mathbb{Y}blackboard_X , blackboard_Y denote finite dimensional spaces, 𝑨:𝕏→𝕐:𝑨→𝕏𝕐\bm{A}:\mathbb{X}\rightarrow\mathbb{Y}bold_italic_A : blackboard_X → blackboard_Y is a linear forward operator, 𝒖†superscript𝒖†\bm{u}^{\dagger}bold_italic_u start_POSTSUPERSCRIPT † end_POSTSUPERSCRIPT is the ground truth and 𝜼𝜼\bm{\eta}bold_italic_η is a random noise component. Given 𝒃𝒃\bm{b}bold_italic_b and 𝑨𝑨\bm{A}bold_italic_A, the goal is to compute an approximation 𝒖𝒖\bm{u}bold_italic_u of 𝒖†superscript𝒖†\bm{u}^{\dagger}bold_italic_u start_POSTSUPERSCRIPT † end_POSTSUPERSCRIPT. Since inverse problems are typically ill-posed, prior information about 𝒖𝒖\bm{u}bold_italic_u has to be incorporated in the form of regularisation. The solution to the inverse problem is then acquired by solving arg⁢min𝒖∈𝕏⁡𝒟⁢(𝑨⁢𝒖,𝒃)+α⁢ℛ⁢(𝒖).subscriptargmin𝒖𝕏𝒟𝑨𝒖𝒃𝛼ℛ𝒖\operatorname*{arg\,min}_{\bm{u}\in\mathbb{X}}\mathcal{D}(\bm{A}\bm{u},\bm{b})% +\alpha\mathcal{R}(\bm{u}).start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT bold_italic_u ∈ blackboard_X end_POSTSUBSCRIPT caligraphic_D ( bold_italic_A bold_italic_u , bold_italic_b ) + italic_α caligraphic_R ( bold_italic_u ) . (1) Here 𝒟𝒟\mathcal{D}caligraphic_D denotes the fidelity term, measuring the distance between 𝒃𝒃\bm{b}bold_italic_b and the solution 𝒙𝒙\bm{x}bold_italic_x under the operator 𝑨𝑨\bm{A}bold_italic_A. Regularisation term ℛℛ\mathcal{R}caligraphic_R promotes properties such as smoothness, sparsity, edge preservation, and low-rankness of the solution, and is weighted by a parameter α>0𝛼0\alpha>0italic_α > 0. Classical examples for ℛℛ\mathcal{R}caligraphic_R in imaging include the well-known Total Variation (TV), high order extensions [1], namely the Total Generalized Variation (TGV) [2], Total Nuclear Variation [3] and more general tensor based structure regularisation, [4]. In order to obtain a solution for (1), one employs iterative algorithms such as Gradient Descent (GD) for smooth objectives or Forward-Backward Splitting (FBS) [5] for non-smooth ones. Moreover, under the general framework min𝒙∈𝕏⁡f⁢(𝒙)+g⁢(𝒙),subscript𝒙𝕏𝑓𝒙𝑔𝒙\displaystyle\min_{\bm{x}\in\mathbb{X}}f(\bm{x})+g(\bm{x}),roman_min start_POSTSUBSCRIPT bold_italic_x ∈ blackboard_X end_POSTSUBSCRIPT italic_f ( bold_italic_x ) + italic_g ( bold_italic_x ) , (2) the Proximal Gradient Descent (PGD) algorithm, also known as Iterative Shrinkage Thresholding Algorithm (ISTA) and its accelerated version FISTA [6] are commonly used when f𝑓fitalic_f is a convex, L-smooth and g𝑔gitalic_g proper convex. Saddle-point methods such as the Primal Dual Hybrid Gradient (PDHG) [7] are commonly used for non-smooth f𝑓fitalic_f. A common property of most of these methods, see Algorithms 1–3 below, is the evaluation of proximal operators related to the regulariser in every iteration, which for τ>0𝜏0\tau>0italic_τ > 0 is defined as proxτ⁢ℛ⁢(𝒙):=arg⁢min𝒛∈𝕏⁡{12⁢‖𝒛−𝒙‖22+τ⁢ℛ⁢(𝒛)}.assignsubscriptprox𝜏ℛ𝒙subscriptargmin𝒛𝕏12superscriptsubscriptnorm𝒛𝒙22𝜏ℛ𝒛\mathrm{prox}_{\tau\mathcal{R}}(\bm{x}):=\operatorname*{arg\,min}_{\bm{z}\in% \mathbb{X}}\bigg{\{}\frac{1}{2}\|\bm{z}-\bm{x}\|_{2}^{2}+\tau\mathcal{R}(\bm{z% })\bigg{\}}.roman_prox start_POSTSUBSCRIPT italic_τ caligraphic_R end_POSTSUBSCRIPT ( bold_italic_x ) := start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT bold_italic_z ∈ blackboard_X end_POSTSUBSCRIPT { divide start_ARG 1 end_ARG start_ARG 2 end_ARG ∥ bold_italic_z - bold_italic_x ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_τ caligraphic_R ( bold_italic_z ) } . (3) This proximal operator can have either a closed form solution, e.g., when ℛ(⋅)=∥⋅∥1\mathcal{R}(\cdot)=\|\cdot\|_{1}caligraphic_R ( ⋅ ) = ∥ ⋅ ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT or requires an inner iterative solver e.g., when ℛ⁢(𝒖)=TV⁢(𝒖)ℛ𝒖TV𝒖\mathcal{R}(\bm{u})=\mathrm{TV}(\bm{u})caligraphic_R ( bold_italic_u ) = roman_TV ( bold_italic_u ). 1:Parameters: γ>0𝛾0\gamma>0italic_γ > 0 2:Initialize: 𝒙0∈𝕏subscript𝒙0𝕏\bm{x}_{0}\in\mathbb{X}bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∈ blackboard_X 3:for k=0,…,K−1𝑘0…𝐾1k=0,\ldots,K-1italic_k = 0 , … , italic_K - 1 do 4: 𝒙k+1=𝒙k−γ⁢∇f⁢(𝒙k)subscript𝒙𝑘1subscript𝒙𝑘𝛾∇𝑓subscript𝒙𝑘\bm{x}_{k+1}=\bm{x}_{k}-\gamma\nabla f(\bm{x}_{k})bold_italic_x start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = bold_italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - italic_γ ∇ italic_f ( bold_italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) 5:end for Algorithm 1 GD PGD/ISTA/FBS 1:Parameters: γ>0𝛾0\gamma>0italic_γ > 0 2:Initialize: 𝒙0∈𝕏subscript𝒙0𝕏\bm{x}_{0}\in\mathbb{X}bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∈ blackboard_X 3:for k=0,…,K−1𝑘0…𝐾1k=0,\ldots,K-1italic_k = 0 , … , italic_K - 1 do 4: 𝒙k+1=proxγ⁢g⁢(𝒙k−γ⁢∇f⁢(𝒙k))subscript𝒙𝑘1subscriptprox𝛾𝑔subscript𝒙𝑘𝛾∇𝑓subscript𝒙𝑘\bm{x}_{k+1}=\mathrm{prox}_{\gamma g}(\bm{x}_{k}-\gamma\nabla f(\bm{x}_{k}))bold_italic_x start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = roman_prox start_POSTSUBSCRIPT italic_γ italic_g end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - italic_γ ∇ italic_f ( bold_italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ) 5:end for Algorithm 2 PGD/ISTA/FBS 1:Parameters: γ>0𝛾0\gamma>0italic_γ > 0, t0=1subscript𝑡01t_{0}=1italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 1 2:Initialize: 𝒙0∈𝕏subscript𝒙0𝕏\bm{x}_{0}\in\mathbb{X}bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∈ blackboard_X 3:for k=0,…,K−1𝑘0…𝐾1k=0,\ldots,K-1italic_k = 0 , … , italic_K - 1 do 4: tk+1=1+1+4⁢tk22subscript𝑡𝑘1114superscriptsubscript𝑡𝑘22t_{k+1}=\frac{1+\sqrt{1+4t_{k}^{2}}}{2}italic_t start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = divide start_ARG 1 + square-root start_ARG 1 + 4 italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG end_ARG start_ARG 2 end_ARG, ak=tk−1tk+1subscript𝑎𝑘subscript𝑡𝑘1subscript𝑡𝑘1a_{k}=\frac{t_{k-1}}{t_{k+1}}italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = divide start_ARG italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT end_ARG start_ARG italic_t start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT end_ARG 5: 𝒙¯k+1=𝒙k+ak⁢(𝒙k−𝒙k+1)subscript¯𝒙𝑘1subscript𝒙𝑘subscript𝑎𝑘subscript𝒙𝑘subscript𝒙𝑘1\bar{\bm{x}}_{k+1}=\bm{x}_{k}+a_{k}\left(\bm{x}_{k}-\bm{x}_{k+1}\right)over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = bold_italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - bold_italic_x start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT ) 6: 𝒙k+1=proxγ⁢g⁢(𝒙¯k+1−γ⁢∇f⁢(𝒙¯k+1))subscript𝒙𝑘1subscriptprox𝛾𝑔subscript¯𝒙𝑘1𝛾∇𝑓subscript¯𝒙𝑘1\bm{x}_{k+1}=\mathrm{prox}_{\gamma g}(\bar{\bm{x}}_{k+1}-\gamma\nabla f(\bar{% \bm{x}}_{k+1}))bold_italic_x start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = roman_prox start_POSTSUBSCRIPT italic_γ italic_g end_POSTSUBSCRIPT ( over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT - italic_γ ∇ italic_f ( over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT ) ) 7:end for Algorithm 3 FISTA 1:Parameters: γ>0𝛾0\gamma>0italic_γ > 0, probability p>0𝑝0p>0italic_p > 0 2:Initialize: 𝒙0,𝒉0∈𝕏subscript𝒙0subscript𝒉0𝕏\bm{x}_{0},\bm{h}_{0}\in\mathbb{X}bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , bold_italic_h start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∈ blackboard_X 3:for k=0,1,…,K−1𝑘01…𝐾1k=0,1,\dotsc,K-1italic_k = 0 , 1 , … , italic_K - 1 do 4: 𝒙^k+1=𝒙k−γ⁢(∇f⁢(𝒙k)−𝒉k)subscript^𝒙𝑘1subscript𝒙𝑘𝛾∇𝑓subscript𝒙𝑘subscript𝒉𝑘\hat{\bm{x}}_{k+1}=\bm{x}_{k}-\gamma(\nabla f(\bm{x}_{k})-{\bm{h}_{k}})over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = bold_italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - italic_γ ( ∇ italic_f ( bold_italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) - bold_italic_h start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) 5: if Prob⁢(θk=1)=pProbsubscript𝜃𝑘1𝑝\mathrm{Prob}(\theta_{k}=1)=proman_Prob ( italic_θ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = 1 ) = italic_p then 6: 𝒙k+1=proxγp⁢g⁢(𝒙^k+1−γp⁢𝒉k)subscript𝒙𝑘1subscriptprox𝛾𝑝𝑔subscript^𝒙𝑘1𝛾𝑝subscript𝒉𝑘\bm{x}_{k+1}=\mathrm{prox}_{\frac{\gamma}{p}g}\bigg{(}\hat{\bm{x}}_{k+1}-\frac% {\gamma}{p}{\bm{h}_{k}}\bigg{)}bold_italic_x start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = roman_prox start_POSTSUBSCRIPT divide start_ARG italic_γ end_ARG start_ARG italic_p end_ARG italic_g end_POSTSUBSCRIPT ( over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT - divide start_ARG italic_γ end_ARG start_ARG italic_p end_ARG bold_italic_h start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) 7: elsexk+1=x^k+1subscript𝑥𝑘1subscript^𝑥𝑘1\;\;\bm{x}_{k+1}=\hat{\bm{x}}_{k+1}bold_italic_x start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT 8: end if 9: 𝒉k+1=𝒉k+pγ⁢(𝒙k+1−𝒙^k+1)subscript𝒉𝑘1subscript𝒉𝑘𝑝𝛾subscript𝒙𝑘1subscript^𝒙𝑘1{\bm{h}_{k+1}}={\bm{h}_{k}}+\frac{p}{\gamma}(\bm{x}_{k+1}-\hat{\bm{x}}_{k+1})bold_italic_h start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = bold_italic_h start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + divide start_ARG italic_p end_ARG start_ARG italic_γ end_ARG ( bold_italic_x start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT - over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT ) 10:end for Algorithm 4 ProxSkip The ProxSkip algorithm The possibility to skip the computation of the proximal operator in some iterations according to a probability p𝑝pitalic_p, accelerating the algorithms, without affecting convergence, was discussed in[8]. There, the ProxSkip algorithm was introduced to tackle federated learning applications which also rely on computations of expensive proximal operators. ProxSkip introduces a control variable 𝒉ksubscript𝒉𝑘\bm{h}_{k}bold_italic_h start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, see Algorithm 4. When the proximal step is not applied, the control variable remains constant. Hence, if at iteration k𝑘kitalic_k, no proximal step has been applied previously, the accumulated error is passed to 𝒙k+1subscript𝒙𝑘1\bm{x}_{k+1}bold_italic_x start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT without incurring an additional computational cost. If at the iteration k𝑘kitalic_k the proximal step is applied, the error is reduced and the control variable will be updated accordingly. In [8] it was shown that the ProxSkip converges provided that f𝑓fitalic_f in (2) is L-smooth and μ𝜇\muitalic_μ-strongly convex, and probability p𝑝pitalic_p satisfies p≥μ/L.𝑝𝜇𝐿p\geq\sqrt{\mu/L}.italic_p ≥ square-root start_ARG italic_μ / italic_L end_ARG . (4) In the case of equality in (4), the algorithm converges (in expectation) at a linear rate with γ=1L𝛾1𝐿\gamma=\frac{1}{L}italic_γ = divide start_ARG 1 end_ARG start_ARG italic_L end_ARG and the iteration complexity is 𝒪⁢(Lμ⁢log⁡(1ε))𝒪𝐿𝜇1𝜀\mathcal{O}(\frac{L}{\mu}\log(\frac{1}{\varepsilon}))caligraphic_O ( divide start_ARG italic_L end_ARG start_ARG italic_μ end_ARG roman_log ( divide start_ARG 1 end_ARG start_ARG italic_ε end_ARG ) ). In addition, the total number of proximal evaluations (in expectation) are only 𝒪⁢(1p⁢log⁡(1ε))𝒪1𝑝1𝜀\mathcal{O}(\frac{1}{\sqrt{p}}\log(\frac{1}{\varepsilon}))caligraphic_O ( divide start_ARG 1 end_ARG start_ARG square-root start_ARG italic_p end_ARG end_ARG roman_log ( divide start_ARG 1 end_ARG start_ARG italic_ε end_ARG ) ). Our contribution Our aim is to showcase for the first time via extended numerical experiments the computational benefits of ProxSkip for a variety of imaging inverse problems, including challenging real-world tomographic applications. In particular, we show that ProxSkip can outperform the accelerated version of its non-skip analogue, namely FISTA. At the same time, we introduce a novel PDHGSkip version of the PDHG, Algorithm 6, which we motivate via numerical experiments. We anticipate that this will spark further research around developing skip-versions of a variety of proximal based algorithms used nowadays. For all our imaging experiments we consider the following optimisation problem that contains a quadratic distance term as the fidelity term, with the (isotropic) total variation as the regulariser, i.e., TV⁢(𝒖)=‖𝑫⁢𝒖‖2,1=∑|(𝑫y⁢𝒖,𝑫x⁢𝒖)|2=∑((𝑫y𝒖)2+(𝑫x𝒖)2,\mathrm{TV}(\bm{u})=\|\bm{D}\bm{u}\|_{2,1}=\sum|(\bm{D}_{y}\bm{u},\bm{D}_{x}% \bm{u})|_{2}=\sum\sqrt{((\bm{D}_{y}\bm{u})^{2}+(\bm{D}_{x}\bm{u})^{2}},roman_TV ( bold_italic_u ) = ∥ bold_italic_D bold_italic_u ∥ start_POSTSUBSCRIPT 2 , 1 end_POSTSUBSCRIPT = ∑ | ( bold_italic_D start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT bold_italic_u , bold_italic_D start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT bold_italic_u ) | start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = ∑ square-root start_ARG ( ( bold_italic_D start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT bold_italic_u ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + ( bold_italic_D start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT bold_italic_u ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG , min𝒖∈𝕏⁡12⁢‖𝑨⁢𝒖−𝒃‖22+α⁢TV⁢(𝒖),subscript𝒖𝕏12superscriptsubscriptnorm𝑨𝒖𝒃22𝛼TV𝒖\min_{\bm{u}\in\mathbb{X}}\frac{1}{2}\|\bm{A}\bm{u}-\bm{b}\|_{2}^{2}+\alpha% \mathrm{TV}(\bm{u}),roman_min start_POSTSUBSCRIPT bold_italic_u ∈ blackboard_X end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG 2 end_ARG ∥ bold_italic_A bold_italic_u - bold_italic_b ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_α roman_TV ( bold_italic_u ) , (5) where 𝑫⁢𝒖=(𝑫y,𝑫x)𝑫𝒖subscript𝑫𝑦subscript𝑫𝑥\bm{D}\bm{u}=(\bm{D}_{y},\bm{D}_{x})bold_italic_D bold_italic_u = ( bold_italic_D start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT , bold_italic_D start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT ) is the finite difference operator under Neumann boundary conditions."
https://arxiv.org/html/2411.00617v1,A Graph Attention-Guided Diffusion Modelfor Liver Vessel Segmentation,"Improving connectivity and completeness are the most challenging aspects of small liver vessel segmentation. It is difficult for existing methods to obtain segmented liver vessel trees simultaneously with continuous geometry and detail in small vessels. We proposed a diffusion model-based method with a multi-scale graph attention guidance to break through the bottleneck to segment the liver vessels. Experiments show that the proposed method outperforms the other state-of-the-art methods used in this study on two public datasets of 3D-ircadb-01 and LiVS. Dice coefficient and Sensitivity are improved by at least 11.67% and 24.21% on 3D-ircadb-01 dataset, and are improved by at least 3.21% and 9.11% on LiVS dataset. Connectivity is also quantitatively evaluated in this study and our method performs best. The proposed method is reliable for small liver vessel segmentation.","Liver cancer is the fourth leading cause of death according to the statistics on cancer-related mortality [21]. Furthermore, the liver is a frequent site for metastasis from various primary tumors, such as gastrointestinal tumors, breast cancer, lung cancer and melanoma[8]. Both primary and secondary liver cancer have multiple treatment options including surgery and various interventional oncology liver treatments. The preoperative planning of those treatments can be improved with accurate liver vessel segmentation[25, 1]. In the preoperative planning of liver tumor resection[1], visualizing the spatial location between liver vessels and tumors in a 3D view is one of the essential steps to reduce the surgical risk. Liver vessel segmentation is mainly used to ensure that no main vessels are located near the planned resection region to reduce bleeding. Besides that, liver vessel segmentation is also an important indicator for the Couinaud classification[1], that divides the liver into eight functionally independent regions based on the blood flow of the vascular systems. Hence, accurate liver vessel segmentation plays an important role in liver tumor surgery. Furthermore, it can assist in targeting the correct tumor nutrient supply vessel to decrease the recurrence rate in embolic therapies[14]. However, acquiring accurate and continuous segments of liver vessels is very challenging due to their complex anatomy such as small vessels, especially in low-contrast situations. Figure 1: (a) Comparison between ground truth and different deep network models: our proposal (GATSegDiff), nnUNet[17] and Swin UNETR[12]. It is challenging to ensure vessel continuity in all directions, and to localize small vessels. (b) Liver vessel segmentations generated by our method at different prediction iterations (with 00 being the final iteration). Automatic liver vessel segmentation is traditionally done by vessel enhancement relying on image filtering[35, 19], using active contour models[6, 7], or employing tracking methods[9, 4]. These approaches require manual hyper-parameter tuning, initialization of seed-points, or added domain-knowledge. Currently, deep artificial networks and specifically convolutional neural networks (CNN) provide an end-to-end learning paradigm for medical image segmentation and achieve remarkable results [15, 18]. CNNs such as nnUNet [17] can extract features automatically from the images with limited manual input. However, locating vessels and ensuring vessel continuity is challenging for CNN-based methods, as shown in Fig. 1(a). To improve the continuity of the segmented vessel tree, self-attention [42] and graph-attention methods[46, 20] are used. However, Fig. 1(a) shows that the attention-based Swin UNETR[12] cannot deal with horizontally distributed vessel trees. Overall, it remains challenging to ensure vessel continuity in all directions, and to localize small vessels. In this paper, we address these two challenges: ensuring vessel continuity and segmenting small vessels. To ensure vessel continuity, we start from a 2D𝐷Ditalic_D diffusion model[13, 28]. We opt for a 2D𝐷Ditalic_D rather than a 3D𝐷Ditalic_D diffusion model to reduce computational requirements. We add graph-attention layers[3] into the diffusion model to improve the vessel continuity. To compensate for the graph sparsity, we integrate neighboring features of the graph nodes in a local ensemble module of the local implicit image function (LIIF)[5]. The local ensemble module ensures a smooth transition between different nodes [5]. Additionally, to segment small vessels, we extract features at multiple scales in the nodes of the graph. The effectiveness of these components is shown in Fig. 1(b), where we exemplify predictions of our model over different prediction iterations. This work makes the following contributions: (i) A novel graph attention conditioning diffusion model for vessel segmentation; (ii) An improved continuity and detection of small liver vessels relying on multi-scale graph-features which are used to condition the diffusion model; (iii) A more accurate and continuous vessel segmentation on two public datasets 3D-ircadb-01[34] and LiVS[10], when compared to existing work."
https://arxiv.org/html/2411.00609v1,Tumor Location-weighted MRI-Report Contrastive Learning: A Framework for Improving the Explainability of Pediatric Brain Tumor Diagnosis,"Despite the promising performance of convolutional neural networks (CNNs) in brain tumor diagnosis from magnetic resonance imaging (MRI), their integration into the clinical workflow has been limited. That is mainly due to the fact that the features contributing to a model’s prediction are unclear to radiologists and hence, clinically irrelevant, i.e., lack of explainability. As the invaluable sources of radiologists’ knowledge and expertise, radiology reports can be integrated with MRI in a contrastive learning (CL) framework, enabling learning from image-report associations, to improve CNN explainability. In this work, we train a multimodal CL architecture on 3D brain MRI scans and radiology reports to learn informative MRI representations. Furthermore, we integrate tumor location, salient to several brain tumor analysis tasks, into this framework to improve its generalizability. We then apply the learnt image representations to improve explainability and performance of genetic marker classification of pediatric Low-grade Glioma, the most prevalent brain tumor in children, as a downstream task. Our results indicate a Dice score of 31.1% between the model’s attention maps and manual tumor segmentation (as an explainability measure) with test classification performance of 87.7%, significantly outperforming the baselines. These enhancements can build trust in our model among radiologists, facilitating its integration into clinical practices for more efficient tumor diagnosis.","Convolutional neural networks (CNNs), as a subset of deep learning (DL), have yielded high performance in various brain tumor diagnosis tasks using magnetic resonance imaging (MRI) in a non-invasive manner [1, 2]. Nonetheless, these models are not widely trusted and verified by radiologists, hindering their adoption in clinical settings. This can have several reasons, one of the primary ones being lack of explainability. Explainability refers to the ability of a DL model to focus on relevant parts of input data for making a certain prediction. In the case of medical image analysis, explainability makes a CNN model diagnose similarly to radiologists and attend to appropriate parts of an input image. Therefore, a potential approach for improving model explainability can be integrating sources of information derived from the knowledge of radiologists into the CNN’s framework. Different data sources have been utilized in the literature to enhance CNN explainability. These include semantic features [3], i.e., important imaging features annotated by radiologists corresponding to a certain disease, and eye-gaze information [4], which is associated with parts of an image attended by a radiologist for making a diagnosis. However, these sources are usually expensive and labor-intensive, making them very challenging to obtain and incorporate into CNN models. Radiology reports, on the other hand, represent an invaluable data modality expressed in radiologists’ language, which are easily accessible in most medical imaging datasets and can be used for enhancing model explainability. Contrastive learning (CL), as a subset of self-supervised learning (SSL), can utilize the correspondence between images and reports, without requiring any external labels, and serve as an effective method for leveraging radiology reports into CNN-based DL frameworks for improving CNN explainability. Developing CL-based DL architectures on medical images and radiology reports has been investigated in several works for chest X-ray, which is a 2D imaging modality, based on the global and local correlation between images and reports [5, 6, 7]. Global correlation corresponds to the association between global image and report representations, while local correlation refers to the alignment between local image patches and report words. For 3D imaging, CL has been used to align the global representations of brain MR images and radiology reports [8]. However, finding the local correlation between the two data modalities, which can be an effective way of improving the explainability of imaging-based models, should also be explored. Moreover, although adjusting the representations based on relevant additional variables has been studied in image-based SSL frameworks [9], the incorporation of such variables into multimodal CL frameworks in order to improve the representation learning task has yet to be investigated, given the heterogeneous nature of clinical data. In this work, we propose a deep CL framework on a set of MRI and report pairs to learn useful image representations which are semantically close to the corresponding text representations. Furthermore, we leverage tumor location, as a determining discrete variable in different brain tumor diagnostic tasks, into the CL framework to enhance the representation learning. Finally, we apply the learnt image representations to classify the genetic markers of pediatric Low-grade Glioma (pLGG), as a downstream task, to improve both explainability and performance. PLGG is one of the most common brain tumors in children, with a prevalence rate of 30-40%. Identifying the genetic markers or molecular subtypes of pLGG is highly important for tumor prognosis, risk stratification, and effective targeted treatment planning [10, 11]. The major clinical method for identifying pLGG genetic markers is through biopsy, which is associated with several pitfalls, including being invasive and difficulty in accessing the tumor [12]. Enhancing the explainability of CNNs in detecting these genetic markers accelerates model adoption in the clinical workflow as a non-invasive diagnostic tool. Explainability Improvement in Medical Image Classification Previous studies have utilized different sources of domain knowledge in model training to make the model focus on relevant parts of the image. Semantic features, i.e., important imaging features annotated by radiologists, were applied in [3] to train a 3D CNN framework on computed tomography images for predicting binarized low-level lung-related semantic features along with classification labels indicating the malignancy of a lung nodule. Eye-gaze information, which specifies image areas important to the radiologist for making a diagnosis, was used in [4] to train a U-Net model. The encoder part of this model performs chest X-ray classification, and the decoder aims to predict eye-gaze heatmaps. It was quantitatively shown that adding this decoder branch to the baseline classification improves the quality of the model’s attention maps [13]. Despite the effect of such sources of expert knowledge on boosting DL explainability, they are typically labor-intensive and challenging to obtain in medical settings. However, radiology reports, describing radiologists’ opinions and findings on medical images, can be readily accessible and efficiently integrated into image-based DL models to guide the model’s attention to relevant image areas. Performance Improvement in Medical Image Classification via Contrastive Learning on Medical Images and Radiology Reports Training CL-based architectures to learn generalizable representations of 2D medical images and radiology reports has been investigated in several works in the literature. [6, 5, 7] developed CL on chest X-ray and report texts for learning the global and local interactions between image and text representations. Applying the learnt image representations to downstream classification tasks, they demonstrated considerable performance improvements over image-based models trained from scratch or initialized with ImageNet weights. However, very limited works have investigated CL to find the correlation between 3D medical images, which are more complex in composition than 2D images, and radiology reports. [8] developed a pretraining framework on paired brain MRI, radiology reports, and disease-related queries for aligning image-text global representations as well as finding the correlation between global image feature map and disease queries. Nonetheless, developing an appropriate module for finding local image-text correlations for 3D imaging has yet to be investigated. Furthermore, including additional variables pertinent to datapoints, alongside medical images and radiology reports, can help in adjusting the distance between negative (mismatched) representations. Although this idea was proposed in [9] by incorporating patient age into an MRI-based CL framework, including external variables in multimodal image-text frameworks should also be explored. To the best of our knowledge, this is the first study that develops an image-text CL framework for optimizing both global and local interactions between MRI, as a 3D medical imaging modality, and radiology reports and utilizes a discrete variable, i.e., tumor location, for regulating the distance between mismatched image and report representations. Furthermore, in addition to improving the classification, we explore the novel application of this framework and semantic correspondence between MRI and reports in improving the explainability of image-based models. Our contributions in this work can be summarized into three main folds: • Training a deep MRI-report CL framework for learning generalizable MRI representations • Incorporating tumor location into the CL framework to enhance the representation learning process • Fine-tuning the CL-based MRI representations on pLGG genetic marker classification to improve the explainability and performance of this downstream task"
https://arxiv.org/html/2411.00527v1,MAROON: A Framework for the Joint Characterization of Near-Field High-Resolution Radar and Optical Depth Imaging Techniques,"Utilizing the complementary strengths of wavelength-specific range or depth sensors is crucial for robust computer-assisted tasks such as autonomous driving. Despite this, there is still little research done at the intersection of optical depth sensors and radars operating close range, where the target is decimeters away from the sensors. Together with a growing interest in high-resolution imaging radars operating in the near field, the question arises how these sensors behave in comparison to their traditional optical counterparts. In this work, we take on the unique challenge of jointly characterizing depth imagers from both, the optical and radio-frequency domain using a multimodal spatial calibration. We collect data from four depth imagers, with three optical sensors of varying operation principle and an imaging radar. We provide a comprehensive evaluation of their depth measurements with respect to distinct object materials, geometries, and object-to-sensor distances. Specifically, we reveal scattering effects of partially transmissive materials and investigate the response of radio-frequency signals. All object measurements will be made public in form of a multimodal dataset, called MAROON.","Real-world computer-assisted tasks, for instance in robotics and tracking applications, frequently require the immediate assessment of spatial information to accurately reason about the environment at a specific point in time, which has led to the development of several single-view range and depth sensors. For autonomous driving, it has been shown that utilizing multimodal depth sensing techniques from both, the optical (lidar) and radio-frequency (radar) domain, can lead to superior performance and robustness in computer-assisted tasks (Velasco-Hernandez et al., 2020). Due to its environment, the autonomous driving industry has traditionally concentrated on far-field range sensing, with an unambiguous range of several meters and beyond. As recent high-resolution radio-frequency technologies utilize the concept of radar imaging to produce 3D information in form of a depth map — similar to optical depth or RGB-D cameras — they also become more popular in close range, where the target of interest is up to a few decimeters away from the sensor; however, a comprehensive and detailed characterization of these radar imaging technologies, which frequently operate in the radar’s near field, has yet to be realized. As part of this work, we devised a dataset MAROON (Multimodal Aligned Radio and Optical frequency Object Reconstructions in the Near Field) (cf. Section 5) that enables studying of different sensor modalities in direct comparison. As is immediately visible in Figure 1 (left), the reconstructions of near-field imaging radars appear fundamentally different in comparison to their well-researched counterparts in the optical domain. A key advantage of radar is that it is insensitive to environmental light and can penetrate, for instance, fabric and dust. Following the success of Google’s project Soli (Lien et al., 2016) for gesture sensing, radars were utilized in close range for the detection of vital signs (Vilesov et al., 2022), activity recognition (Braeunig et al., 2023), people tracking (Zewge et al., 2019) and human body reconstruction (Chen et al., 2022; Chen et al., 2023). With the growing trend towards larger antenna apertures to achieve high-resolution imaging (Schwarz et al., 2022; Chen et al., 2023), radars will more frequently operate in the near field, as determined by the Fraunhofer boundary condition (Selvan and Janaswamy, 2017). At the same time, characteristics of near-field radar are generally under-researched. Figure 2. Example data of the Plunger object from the MAROON dataset. In the upper left, all reconstructions are spatially aligned with respect to the RF ToF coordinate system. The RF ToF colorscale encodes the normalized reconstruction confidence (cf. Section 4.2.2). Drawing on prior research about wavelength-specific strengths and weaknesses, this paper addresses the unique challenge of characterizing various optical depth-imaging techniques alongside a high-resolution multiple-input multiple-output (MIMO) imaging radar in the near field. The latter is interchangeably referred to as a radio-frequency (RF) Time-of-Flight (ToF) depth imager. To this end, we mutually calibrated sensors of four different depth sensing technologies, that is active and passive stereo, near-infrared (NIR) ToF, and RF ToF in the millimeter-wave range. Using these sensors, we captured the comprehensive MAROON dataset of various household objects and construction materials, of which example data is shown in Figure 2. The dataset enables an exploration of sensor-specific characteristics with respect to various object materials, geometries and distances from the sensor. Furthermore, the spatial calibration enables a direct comparison of all sensor reconstructions in a common metrical space. We then compare these results to a reference ground truth reconstruction from a state-of-the-art multi-view stereo (MVS) system. To achieve this, we developed a joint sensor evaluation framework that measures reconstruction differences between sensors and ground truth using different metrics, and that provides supplementary visualizations tailored to identify sensor-specific trends across multiple objects. Our findings reveal fundamental differences between optical and RF sensors, as well as characteristics arising from specific depth measurement techniques within a single frequency domain. We detail sensor-specific characteristics based on several key insights. First, by assessing deviation from the ground truth across different objects and object-to-sensor distances, we identify common sensor-specific trends. Given these, RF ToF reconstructions are found to be less complete compared to optical sensors. Lastly, we demonstrate that ToF sensors exhibit systematic depth errors resulting from interactions within partially transmissive media, such as subsurface scattering. To summarize, our contributions are the following: • A framework enabling the systematic comparative characterization of depth-sensing techniques in both, the optical and radio-frequency signal domain. • A novel multimodal dataset, MAROON, comprising common objects in the near field, which were captured with the unique setup of four jointly calibrated depth sensors. • We publicly release this dataset together with the raw radar measurements, thus enabling the community to explore different signal reconstruction and filtering techniques.111State-of-the-art methods for reconstruction and filtering will be publicly released together with the dataset viewer upon acceptance. • A detailed analysis of trends and sensor-specific effects emerging from that dataset. This includes aspects of different object materials, geometries, and distances to the sensors, signal response and reconstruction quality of imaging radars, as well as ToF scattering effects of partially transmissive materials."
https://arxiv.org/html/2411.00430v1,Class Incremental Learning with Task-Specific Batch Normalization and Out-of-Distribution Detection,"This study focuses on incremental learning for image classification, exploring how to reduce catastrophic forgetting [1] of all learned knowledge when access to old data is restricted due to memory or privacy constraints. The challenge of incremental learning lies in achieving an optimal balance between plasticity, the ability to learn new knowledge, and stability, the ability to retain old knowledge. Based on whether the task identifier (task-ID) of an image can be obtained during the test stage, incremental learning for image classifcation is divided into two main paradigms, which are task incremental learning (TIL) and class incremental learning (CIL). The TIL paradigm has access to the task-ID, allowing it to use multiple task-specific classification heads selected based on the task-ID. Consequently, in CIL, where the task-ID is unavailable, TIL methods must predict the task-ID to extend their application to the CIL paradigm. Our previous method [2] for TIL adds task-specific batch normalization and classification heads incrementally. This work extends the method by predicting task-ID through an “unknown” class added to each classification head. The head with the lowest “unknown” probability is selected, enabling task-ID prediction and making the method applicable to CIL. The task-specific batch normalization (BN) modules effectively adjust the distribution of output feature maps across different tasks, enhancing the model’s plasticity. Moreover, since BN has much fewer parameters compared to convolutional kernels, most of the feature extractor’s parameters remain unchanged. By only modifying the BN layers as new tasks arrive, the model can effectively manage parameter growth while ensuring stability across tasks. In summary, the proposed method can efficiently control parameter growth while ensuring both the model’s plasticity and stability. The innovation of this study lies in the first-time introduction of task-specific BN into CIL and verifying the feasibility of extending TIL methods to CIL through task-ID prediction. Our method achieves state-of-the-art performance on two medical image datasets and one natural image dataset. The source code will be released publicly.","Deep learning models have been extensively applied to various fields. However, they usually rely on large amounts of data for training to achieve high performance. In real-world scenarios, it is often difficult to obtain data for all necessary classes at once, leading to data arriving in multiple phases. Moreover, the classes appearing in different phases are typically non-overlapping. Consequently, AI models often need to learn new classes gradually. With privacy and memory constraints, models can only access data from the current phase and are restricted to retrieve data from previous phases. Research has found that under these circumstances, models are susceptible to catastrophic forgetting [1], a phenomenon characterized by a substantial decline in performance on previously learned tasks. Therefore, enabling models to continually learn new knowledge without forgetting old knowledge is one key challenge for promoting the widespread application of deep learning based AI across various fields. Several methods have been proposed to alleviate catastrophic forgetting. However, these current methods often struggle to balance the model’s stability[3] (ability to retain old knowledge) and plasticity[3] (ability to learn new knowledge), and they also face challenges in controlling parameter growth. For instance, methods based on model parameter regularization [4, 5, 3] aim to protect crucial neural network weights from being updated drastically through constraints, thus preventing old knowledge loss. However, this protection can lead to model rigidity, making it difficult for the model to assimilate new knowledge. Another type of method is the dynamic expansion of the model’s structure [6, 7, 8, 9, 10], where new layers or sub-modules are added for new tasks. Although such method significantly enhances the model’s plasticity, it inevitably increases memory demands. Therefore, effectively adding network components without substantially increasing the memory burden is a fundamental issue for this type of method. Overall, existing methods struggle to achieve a good balance between plasticity, stability and memory control, which is the core challenge in incremental learning. During training on new tasks, because of the distinctive knowledge representations between tasks, the unrestricted continuous changes in feature extractor parameters is the key factor leading to catastrophic forgetting. Thus, minimizing updates of the feature extractor can substantially mitigate or completely avoid catastrophic forgetting. Based on the idea of minimizing changes to the learned knowledge representation inside the feature extractor, we propose a method utilizing task-specific Batch Normalization[11] (BN) and classification heads. In classic deep convolutional networks like ResNet [12], the number of parameters of BN and classification heads is negligible comparing with convolution layers. Thus we can add these additional task-specific modules without concerning parameters explosion issue like DynaER [9] throughout incremental learning. This method also prevents the erasing of existing knowledge related to old tasks because only these new BN and classification head modules are trainable, thereby avoiding catastrophic forgetting and ensuring model stability. Moreover, since BN is able to reshape the data distribution, its powerful expressive capability has been demonstrated in the field of GAN [13]. Therefore, by introducing task-specific BN, the model’s ability to learn the distribution of different tasks can be greatly enhanced, improving the model’s plasticity. This paper is an extension of our previously published conference paper [2], which utilized task-specific batch normalization in TIL. In this study, we further incorporate out-of-distribution detection, extending the method to CIL paradigm. Overall, our contributions are listed below: 1. We introduce task-specific batch normalization in the CIL paradigm for the first time. 2. Building on multiple task-specific classification heads, we introduce out-of-distribution detection for task-ID prediction, demonstrating the effectiveness of using task-ID prediction approach to extend TIL method for CIL, where task-ID is unknown. 3. Our proposed method achieves state-of-the-art classification performance on two medical image datasets and one natural image dataset, outperforming existing methods in overall performance and achieving a better balance among model stability and plasticity, and parameter growth."
https://arxiv.org/html/2411.00393v3,Advantages of Neural Population Coding for Deep Learning,"Scalar variables, e.g., the orientation of a shape in an image, are commonly predicted using a single output neuron in a neural network. In contrast, the mammalian cortex represents variables with a population of neurons. In this population code, each neuron is most active at its preferred value and shows partial activity for other values. Here, we investigate the benefit of using a population code for the output layer of a neural network. We compare population codes against single-neuron outputs and one-hot vectors. First, we show theoretically and in experiments with synthetic data that population codes improve robustness to input noise in networks of stacked linear layers. Second, we demonstrate the benefit of using population codes to encode ambiguous outputs, such as the pose of symmetric objects. Using the T-LESS dataset of feature-less real-world objects, we show that population codes improve the accuracy of predicting 3D object orientation from image input.","In the mammalian cortex, many variables, e.g., object orientation [13] and movement direction [6], have been found to be encoded by populations of neurons. In a population code, each neuron responds maximally to its preferred value and partially to other values of the encoded variable, with activation levels shaped by tuning curves such as Gaussian or cosine functions of the distance from the neuron’s preferred value. The activity of the group of neurons resembles a probability distribution of the encoded variable [18]. A lot of computational neuroscience work, particularly the earlier work, has focused on decoding the information represented by a population code [20, 19, 25, 2]. But the brain does not need to decode population codes: information is processed from population code to population code throughout the cortex [3, 17, 24, 12]. Commonly used artificial neural networks, such as convolutional neural networks (CNNs) and multi-layer perceptrons (MLPs), encode information also by groups of neurons, particularly in their intermediate layers [8]. However, in the output layer, information is typically mapped onto different representations. For classification tasks, outputs are commonly represented as one-hot vectors, using one neuron for each classification label. For prediction tasks, output neurons typically correspond to the variables of interest, such as the position and orientation of an object in an image. Here, we investigate the benefit of mapping onto population codes for prediction tasks. Other prior work already pointed out the benefits of neural population codes: such coding has been shown to improve linear separability of temporal information [16]. In addition, population codes can be used in the output layer for cleaning up noisy signals nearly as optimal as the maximum likelihood estimate [17]. Different here, we demonstrate that replacing prediction target variables with population codes improves noise robustness and accuracy, which we show using theoretical analysis and experiments with synthetic and real-world data. In the remainder of this article, we first derive theoretically the noise robustness for a single-layer linear network and compare single-variable, population-code, and one-hot vector outputs. Here, we include the one-hot vector for the prediction task due to its structural similarity to the population code: it uses the same group of neurons, only the target activations differ, which are binary for one hot and continuous for the population code. Secondly, we compute the noise robustness using deeper MLPs in simulation. Here, we found that population codes lead to sparser information flow through the network compared to one-hot vectors. Finally, we use an image-to-pose prediction task to demonstrate that population codes can handle ambiguous poses from symmetric objects while improving accuracy. This article makes the following three main contributions: 1. Introducing population codes as output layers of CNNs and MLPs for prediction tasks and demonstrate their benefit, 2. Analyzing theoretically the robustness to noise of single-layer networks, and 3. Discovering that training networks with population-code outputs results in sparser information flows."
https://arxiv.org/html/2411.00360v1,A Simple Remedy for Dataset Bias via Self-Influence: A Mislabeled Sample Perspective,"Learning generalized models from biased data is an important undertaking toward fairness in deep learning. To address this issue, recent studies attempt to identify and leverage bias-conflicting samples free from spurious correlations without prior knowledge of bias or an unbiased set. However, spurious correlation remains an ongoing challenge, primarily due to the difficulty in precisely detecting these samples. In this paper, inspired by the similarities between mislabeled samples and bias-conflicting samples, we approach this challenge from a novel perspective of mislabeled sample detection. Specifically, we delve into Influence Function, one of the standard methods for mislabeled sample detection, for identifying bias-conflicting samples and propose a simple yet effective remedy for biased models by leveraging them. Through comprehensive analysis and experiments on diverse datasets, we demonstrate that our new perspective can boost the precision of detection and rectify biased models effectively. Furthermore, our approach is complementary to existing methods, showing performance improvement even when applied to models that have already undergone recent debiasing techniques.","Deep neural networks have demonstrated remarkable performance in various fields of machine learning tasks comparable to or superior to humans on well-curated benchmark datasets [7, 4, 60, 14]. Nevertheless, the efficacy of these models trained on unfiltered, real-world data remains an open question. In this scenario, a significant concern arises due to the presence of dataset bias [52], where task-irrelevant attributes are spuriously correlated with labels only in the training set. This can lead to models that rely on misleading correlations rather than learning the task-related features, resulting in biased models with poor generalization performance [62, 10]. To prevent models from learning detrimental bias, various methods are proposed to encourage models to prioritize learning task-relevant features. Recent studies enhance task-related features by first identifying bias-conflicting (unbiased) samples through loss [40, 36], gradients [1], or bias prediction techniques [35] during training, using an auxiliary biased model trained with Empirical Risk Minimization (ERM). Then, they amplify bias-conflicting samples by counteracting the bias-aligned (biased) samples through loss weighting [40] or weighted sampling [35]. The effectiveness of such methods largely depends on their precision of bias-conflicting sample detection. Specifically, there is a risk of erroneously amplifying malignant bias instead of task-relevant features when bias-aligned samples are inaccurately identified as bias-conflicting. Due to the limited detection performance of previous methods [40, 36, 1, 35], it presents a crucial challenge that remains unresolved. In this paper, we address this challenge from a novel perspective of mislabeled sample detection. Inspired by the similarities between mislabeled samples and bias-conflicting samples, we delve into Influence Functions (IF;[27]), one of the standard methods for mislabeled sample detection [55, 57, 29], to identify bias-conflicting samples and propose a simple yet effective approach for biased models by leveraging them. We first conduct a comprehensive analysis to explore the efficacy of Self-Influence (SI) [27], a variant of IF, in biased datasets. SI estimates how removing a specific training sample during training influences the prediction of the sample itself with the trained model (see Section. 2.2). By measuring SI, we can identify a minority sample that, if removed from the training set, increases the likelihood of incorrect predictions of itself by the trained model due to their discrepancies with the majority samples. In this context, leveraging SI to biased datasets is promising as bias-conflicting samples constitute the minority and contradict the dominant malignant bias learned by the model. However, we observe that unlike in mislabeled settings, directly applying SI to biased datasets is not as effective (Figure 1(a)-1(d)). Therefore, we investigate the differences between mislabeled samples and bias-conflicting samples and reveal the essential conditions for SI to effectively identify bias-conflicting samples. Note that we denote SI under found conditions as Bias-Conditioned Self-Influence (BCSI). Building on our analysis, we propose a simple yet effective method for rectifying biased models through fine-tuning. We construct a small pivotal subset with a higher proportion of bias-conflicting samples using BCSI. While not perfect, this pivotal set can serve as an effective alternative to an unbiased set. Leveraging this pivotal set, we rectify a biased model through fine-tuning with only a few additional iterations. Extensive experiments demonstrate that our method can effectively rectify even after models are already debiased by recent methods. Our contributions are threefold: • We conduct a comprehensive analysis to explore the efficacy of SI in biased datasets and reveal the essential conditions for SI to accurately differentiate bias-conflicting samples, leading to Bias-Conditioned Self-Influence (BCSI). • We propose a simple yet effective remedy through fine-tuning that utilizes a pivotal set constructed using BCSI to rectify biased models across varying bias severities. • Our method is complementary to existing methods, capable of further rectifying models that have already undergone recent debiasing techniques."
https://arxiv.org/html/2411.00356v1,All-frequency Full-body Human Image Relighting,"Relighting of human images enables post-photography editing of lighting effects in portraits. The current mainstream approach uses neural networks to approximate lighting effects without explicitly accounting for the principle of physical shading. As a result, it often has difficulty representing high-frequency shadows and shading. In this paper, we propose a two-stage relighting method that can reproduce physically-based shadows and shading from low to high frequencies. The key idea is to approximate an environment light source with a set of a fixed number of area light sources. The first stage employs supervised inverse rendering from a single image using neural networks and calculates physically-based shading. The second stage then calculates shadow for each area light and sums up to render the final image. We propose to make soft shadow mapping differentiable for the area-light approximation of environment lighting. We demonstrate that our method can plausibly reproduce all-frequency shadows and shading caused by environment illumination, which have been difficult to reproduce using existing methods.","Human image relighting can alter the lighting effects in a portrait by changing the lighting condition after the photo shoot. The fundamental procedure for human image relighting is to infer the intrinsic geometry and reflectance of the target person as well as the scene illumination from the input image via inverse rendering and then render an output image with a new lighting condition. Modern learning-based methods formulate these inverse and forward rendering stages as a unified differentiable pipeline within an analysis-by-synthesis framework. The current state-of-the-art techniques [28, 40, 37, 22, 25, 39] employ neural networks to approximate the forward rendering stage without explicitly considering the physical principles involved. In particular, the physical principle of shadows is often ignored; shadows appear when the target geometry occludes the incoming light. Explicitly modeling such light occlusion within a differentiable rendering pipeline has been proven challenging; recent approaches only support hard shadows caused by a single point/directional light [13, 36] or adopt a computationally expensive solution via non-differentiable ray tracing with a pre-inferred geometry [16]. Consequently, neural networks in the state-of-the-art techniques struggle to learn complicated shadow patterns and yield blurry shadows or flickering artifacts with dynamic lighting. In this paper, we step forward to reproduce physically plausible shadows for all-frequency relighting of human images. We simultaneously model the target geometry and environment illumination as a depth map and a fixed number of area lights within a differentiable framework to reproduce hard-to-soft shadows caused by multiple area lights. The ground-truth area lights for supervised learning are obtained via a novel optimization-based approach. We also infer the diffuse and specular reflectances of the target person for physically based shading. Such geometry and reflectance information is easier to learn with neural networks because it is simpler than the complicated shadow and reflection patterns. We demonstrate that our physically based formulation yields more plausible and stable relighting results even under dynamic lighting than the existing approximate solutions using neural networks (Figure 1). We will release our source codes, trained models, and synthetic dataset upon publication."
https://arxiv.org/html/2411.00326v1,SpineFM: Leveraging Foundation Models for Automatic Spine X-ray Segmentation,"This paper introduces SpineFM, a novel pipeline that achieves state-of-the-art performance in the automatic segmentation and identification of vertebral bodies in cervical and lumbar spine radiographs. SpineFM leverages the regular geometry of the spine, employing a novel inductive process to sequentially infer the location of each vertebra along the spinal column. Vertebrae are segmented using Medical-SAM-Adaptor, a robust foundation model that diverges from commonly used CNN-based models. We achieved outstanding results on two publicly available spine X-Ray datasets, with successful identification of 97.8% and 99.6% of annotated vertebrae, respectively. Of which, our segmentation reached an average Dice of 0.942 and 0.921, surpassing previous state-of-the-art methods.","In 2020, over 820 million individuals were estimated to suffer from neck and lower back pain due to spinal abnormalities or damage [1, 2]. X-rays are frequently used to diagnose these conditions because of their availability and relatively low cost; however, manual annotation by clinicians remains both time-consuming and technically challenging, often due to patient-specific pathology variations and clinician fatigue [3]. Artificial Intelligence (AI) has the potential to address these issues through automated analysis of images. However, a key foundation of many AI-based methods is accurate vertebra segmentation, making robust segmentation techniques essential for advancing the performance of the diagnostic tools. Numerous studies have utilized convolutional architectures for vertebra segmentation. For instance, [4] proposed a two-step approach involving vertebra localization followed by segmentation, while [5] introduced a rule-based ensemble model to extract vertebral bodies. Although these methods have shown promising results, they face significant limitations, including limited transferability to new datasets and high demands for data annotations. Recently, foundational models have emerged as a competitive alternative. These models leverage massive training datasets and architectures with hundreds of millions of parameters to develop a comprehensive understanding of the task, achieving competitive zero-shot performance without requiring extensive fine-tuning. Fig. 1: Example X-ray images from NHANES II (left, center) and CSXA (right) datasets, with annotated vertebrae outlined in blue. Fig. 2: Diagram of the SpineFM pipeline for automated vertebra segmentation in spine X-rays. The Segment Anything Model (SAM) [6] has demonstrated strong performance in natural image segmentation. However, this success has not fully translated to the medical imaging domain, where images often exhibit lower contrast and ambiguous boundaries [7]. To address this limitation, the Medical-SAM-Adaptor (Med-SA) was proposed [8], which incorporates adapter modules within the image encoder, allowing for fine-tuning while keeping the base SAM weights frozen. During development, we found that this model can effectively segment individual vertebrae, making Med-SA an ideal backbone for our spine segmentation pipeline. Our contributions are as follows. We introduce SpineFM, a novel framework that leverages Med-SA as a general-purpose segmentation foundation model for precise segmentation of challenging spinal radiographs with limited labels. To initialize our pipeline, we first employ R-CNN to obtain rough estimates of vertebrae locations, and the most confident estimates are then input into Med-SA for refined vertebrae segmentation. Following this step, SpineFM applies an inductive approach to locate neighboring vertebrae by exploiting the spine’s ladder-like anatomical structure. The inductive nature of SpineFM, combined with the efficiency of the segmentation foundation model, enables robust performance across a range of anatomical variations, contrasting with traditional U-Net-based or Region-CNN spine segmentation methods, and compensates for missing manual annotations. Consequently, SpineFM achieves state-of-the-art results for both cervical and lumbar spine sections."
https://arxiv.org/html/2411.00322v1,Constant Acceleration Flow,"Rectified flow and reflow procedures have significantly advanced fast generation by progressively straightening ordinary differential equation (ODE) flows. They operate under the assumption that image and noise pairs, known as couplings, can be approximated by straight trajectories with constant velocity. However, we observe that modeling with constant velocity and using reflow procedures have limitations in accurately learning straight trajectories between pairs, resulting in suboptimal performance in few-step generation. To address these limitations, we introduce Constant Acceleration Flow (CAF), a novel framework based on a simple constant acceleration equation. CAF introduces acceleration as an additional learnable variable, allowing for more expressive and accurate estimation of the ODE flow. Moreover, we propose two techniques to further improve estimation accuracy: initial velocity conditioning for the acceleration model and a reflow process for the initial velocity. Our comprehensive studies on toy datasets, CIFAR-10, and ImageNet 64×64 demonstrate that CAF outperforms state-of-the-art baselines for one-step generation. We also show that CAF dramatically improves few-step coupling preservation and inversion over Rectified flow. Code is available at https://github.com/mlvlab/CAF.","Diffusion models [1, 2] learn the probability flow between a target data distribution and a simple Gaussian distribution through an iterative process. Starting from Gaussian noise, they gradually denoise to approximate the target distribution via a series of learned local transformations. Due to their superior generative capabilities compared to other models such as GANs and VAEs, diffusion models have become the go-to choice for high-quality image generation. However, their multi-step generation process entails slow generation and imposes a significant computational burden. To address this issue, two main approaches have been proposed: distillation models [3, 4, 5, 6, 7, 8, 9] and methods that simplify the flow trajectories [10, 11, 12, 13, 14] to achieve fewer-step generation. An example of the latter is rectified flow [10, 13, 11], which focuses on straightening ordinary differential equation (ODE) trajectories. Through repeated applications of the rectification process, called reflow, the trajectories become progressively straighter by addressing the flow crossing problem. Straighter flows reduce discretization errors, enabling fewer steps in the numerical solution and, thus, faster generation. Rectified flow [10, 13] defines the straight ODE flow over time t𝑡titalic_t with a drift force 𝐯𝐯\mathbf{v}bold_v, where each sample 𝐱tsubscript𝐱𝑡\mathbf{x}_{t}bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT transforms from 𝐱0∼π0similar-tosubscript𝐱0subscript𝜋0\mathbf{x}_{0}\sim\pi_{0}bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∼ italic_π start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT to 𝐱1∼π1similar-tosubscript𝐱1subscript𝜋1\mathbf{x}_{1}\sim\pi_{1}bold_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ∼ italic_π start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT under a constant velocity v=𝐱1−𝐱0𝑣subscript𝐱1subscript𝐱0v=\mathbf{x}_{1}-\mathbf{x}_{0}italic_v = bold_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. It approximates the underlying velocity 𝐯𝐯\mathbf{v}bold_v with a neural network 𝐯θsubscript𝐯𝜃\mathbf{v}_{\theta}bold_v start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT. Then, it iteratively applies the reflow process to avoid flow crossing by rewiring the flow and building deterministic data coupling. However, constant velocity modeling may limit the expressiveness needed for approximating complex couplings between π0subscript𝜋0\pi_{0}italic_π start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT and π1subscript𝜋1\pi_{1}italic_π start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. This results in sampling trajectories that fail to converge optimally to the target distribution. Moreover, the interpolation paths after the reflow may still intersect—a phenomenon known as flow crossing—which leads to curved rectified flows because the model estimates different targets for the same input. As illustrated in Fig. 1(a), instead of following the intended path from 𝐱01superscriptsubscript𝐱01\mathbf{x}_{0}^{1}bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT to 𝐱11superscriptsubscript𝐱11\mathbf{x}_{1}^{1}bold_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT, a sampling trajectory from Rectified flow erroneously diverts towards 𝐱12superscriptsubscript𝐱12\mathbf{x}_{1}^{2}bold_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT due to the flow crossing. Such flow crossing can make the accurate learning of straight ODE trajectories more challenging. (a) Rectified Flow (b) Constant Acceleration Flow Figure 1: Initial Velocity Conditioning (IVC). We illustrate the importance of IVC to address the flow crossing problem, which hinders the learning of straight ODE trajectories during training. In Fig. 1(a), Rectified flow suffers from approximation errors at the overlapping point 𝐱tsubscript𝐱𝑡\mathbf{x}_{t}bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT (where 𝐱t1=𝐱t2superscriptsubscript𝐱𝑡1superscriptsubscript𝐱𝑡2\mathbf{x}_{t}^{1}=\mathbf{x}_{t}^{2}bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT = bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT), resulting in curved sampling trajectories due to flow crossing. Conversely, Fig. 1(b) demonstrates that CAF, utilizing IVC, successfully estimates ground-truth trajectories by minimizing the ambiguity at 𝐱tsubscript𝐱𝑡\mathbf{x}_{t}bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. In this paper, we introduce the Constant Acceleration Flow (CAF), a novel ODE framework based on a constant acceleration equation, as outlined in (4). Our CAF generalizes Rectified flow by introducing acceleration as an additional learnable variable. This constant acceleration modeling offers the ability to control flow characteristics by manipulating the acceleration magnitude and enables a direct closed-form solution of the ODE, supporting precise and efficient sampling in just a few steps. Additionally, we propose two strategies to address the flow crossing problem. The first one is initial velocity conditioning (IVC) for the acceleration model, and the second one is to employ reflow to enhance the learning of initial velocity. Fig. 1(b) presents that CAF, with the proposed strategies, can accurately predict the ground-truth path from 𝐱01superscriptsubscript𝐱01\mathbf{x}_{0}^{1}bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT to 𝐱11superscriptsubscript𝐱11\mathbf{x}_{1}^{1}bold_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT, even when flow crossing occurs. Through extensive experiments, from toy datasets to real-world image generation on CIFAR-10 [15] and ImageNet 64×\times×64, we demonstrate that our CAF exhibits superior performance over Rectified flow and state-of-the-art baselines. Notably, CAF achieves superior Fréchet Inception Distance (FID) scores on CIFAR-10 and ImageNet 64×\times×64 in conditional settings, recording FIDs of 1.39 and 1.69, respectively, thereby surpassing recent strong methods. Moreover, we show that CAF provides more accurate flow estimation than Rectified flow by assessing the ‘straightness’ and ‘coupling preservation’ of the learned ODE flow. CAF is also capable of few-step inversion, making it effective for real-world applications such as box inpainting. To summarize, our contributions are as follows: • We propose Constant Acceleration Flow (CAF), a novel ODE framework that integrates acceleration as a controllable variable, enhancing the precision of ODE flow estimation compared to the constant velocity framework. • We propose two strategies to address the flow crossing problem: initial velocity conditioning for the acceleration model and a reflow procedure to improve initial velocity learning. These strategies ensure a more accurate trajectory estimation even in the presence of flow crossings. • Through extensive experiments on synthetic and real datasets, CAF demonstrates remarkable performance, especially achieving the superior FID on CIFAR-10 and ImageNet 64×\times×64 over strong baselines. We also demonstrate that CAF learns more accurate flow than Rectified flow by assessing the straightness, coupling preservation, and inversion."
https://arxiv.org/html/2411.00288v1,Inducing Semi-Structured Sparsity by Masking for Efficient Model Inference in Convolutional Networks,"The crucial role of convolutional models, both as standalone vision models and backbones in foundation models, necessitates effective acceleration techniques. This paper proposes a novel method to learn semi-structured sparsity patterns for convolution kernels in the form of maskings enabling the utilization of readily available hardware accelerations. The approach accelerates convolutional models more than two-fold during inference without decreasing model performance. At the same time, the original model weights and structure remain unchanged keeping the model thus easily updatable. Beyond the immediate practical use, the effect of maskings on prediction is easily quantifiable. Therefore, guarantees on model predictions under maskings are derived showing stability bounds for learned maskings even after updating the original underlying model.111Code available at github.com/ddanhofer/Semi-Structured-Sparsity-CNNs","The increasing complexity of deep learning models [21], their deployment in applications [5], and the adoption of reflection incurring several inference passes per query, e.g., as in the O1 models from the GPT family [3], shifts the relative amounts of resources spent during the model lifetime from the training to the inference stage [7, 35]. It therefore becomes imperative to make models more efficient [46]. One way of achieving this is by spending a comparatively small, additional share of resources during training to learn a one-time modification of the model that lowers the model’s inference and thus lifetime cost [30, 40]. First and foremost, such a modification is effective if it decreases the model’s computational and time cost at a relatively low additional training overhead while not affecting the prediction performance of the model negatively [22]. Additionally, there are other desirable properties of such one-time modifications: From an application perspective the achievable gain in efficiency is only useful if it can be leveraged easily, a well-known challenge, e.g., with sparsifying models [8, 15]. Taking into consideration the increasing popularity of large, expensive to train, foundation models [16] or models employed in an online setting subject to continuous updates the proposed change should not affect the possibility to update the model, e.g., by changing the weights or architecture underlying the model. Ideally, if such a model is updated, the learned modification can even be reused under the constraint of the magnitude of change imposed by updating the model. Semi-structured sparse maskings satisfy the above properties by replacing the dense matrix operations usually required during inference by cheaper and faster operations on semi-structured sparse matrices [4]. While many works have demonstrated that sparse (pruned) submodels can solve the same task at almost no loss of performance [2, 26] the sparsity of the models does not necessarily have to adhere to a specific pattern making it difficult to turn theoretically obtained computational speedups by saving on data loading and computational operations into practical efficiency gains [14]. Regular patterns are more “machine-friendly” inducing the desired efficiency a priori but limiting the choices for the sparse patterns, which thus need to be chosen carefully with the goal of minimizing the loss of inference performance in mind. This paper proposes a novel method of learning regularly sparse masking patterns for convolutions, key building blocks for state-of-the art Computer Vision (CV) models [25] and foundation models building on CV models as their backbone [38]. The proposed method • shows how to effectively use readily available hardware accelerations for semi-structured sparse matrices in convolution kernels to accelerate inference, • outperforms available heuristics for semi-structured sparsity showing that semi-structured sparsity masks can be learned with a fraction of the original training resources while incurring a negligible performance loss in CV classification tasks, • provides the additional advantage of not changing the original set of trained weights keeping models updatable and rendering the method especially attractive for use in large models, e.g., foundation models and in online settings, • induces an easily quantifiable change to the model’s prediction behavior and thus lends itself to settings where hard guarantees on model predictions are of interest. In the following section the adoption of semi-structured sparsity and sparsity in convolutional models are addressed. Section 3 of the paper covers modeling semi-structured sparsity in general, in convolutional models, and the theoretical implications of such model alterations in inference. The results of empirically testing the method on widely used convolutional architectures are presented in Section 4 followed up by a discussion of the method presented and a conclusion."
https://arxiv.org/html/2411.00264v1,TurtleBench: A Visual Programming Benchmark in Turtle Geometry,"Humans have the ability to reason about geometric patterns in images and scenes from a young age. However, developing large multimodal models (LMMs) capable of similar reasoning remains a challenge, highlighting the need for robust evaluation methods to assess these capabilities. We introduce TurtleBench, a benchmark designed to evaluate LMMs’ capacity to interpret geometric patterns—given visual examples, textual instructions, or both—and generate precise code outputs. Inspired by turtle geometry, a notion used to teach children foundational coding and geometric concepts, TurtleBench features tasks with patterned shapes that have underlying algorithmic logic. Our evaluation reveals that leading LMMs struggle significantly with these tasks, with GPT-4o achieving only 19% accuracy on the simplest tasks and few-shot prompting only marginally improves their performance (<2%absentpercent2<2\%< 2 %). TurtleBench highlights the gap between human and AI performance in intuitive and visual geometrical understanding, setting the stage for future research in this area. TurtleBench stands as one of the few benchmarks to evaluate the integration of visual understanding and code generation capabilities in LMMs, setting the stage for future research. Code and Dataset for this paper is provided here: https://github.com/sinaris76/TurtleBench","Large Multimodal Models (LMMs) have the potential to handle tasks that combine visual, linguistic, and reasoning abilities, previously achievable only by humans. Indeed, LMMs such as GPT4-V [52] and Gemini 1.5 flash [46, 17] have been shown to be state of the art models in solving multi-modal tasks such as visual question answering [18, 28], visual mathematical questions [30], chart question answering [33], etc. Despite these successes, there remains the question of how LMMs perform in tasks that intertwine visual reasoning and programming knowledge. That is, given an image of a geometric pattern (and/or a verbal description of the pattern) can LMMs generate code that would be able to procedurally generate that pattern? Indeed, Bubeck et al. [6] showed that the large language model (LLM) with no visual training data was able to create a unicorn in TikZ—the LaTeX-based graphics drawing library. This feat amazed many and provoked many discussions on the intelligence of large language models—but how general is this ability? Figure 1: An illustration of existing types and modes in TurtleBench. In this work, we introduce TurtleBench, a set of manually crafted image/text to code tasks in turtle geometry [37, 1] to evaluate the abilities of these models to combine visual pattern recognition, mathematical reasoning, Python programming, and abstract geometrical reasoning. To ensure the visual inputs and the programming language remain straightforward, TurtleBench harnesses turtle geometry, a concept widely recognized for its effectiveness in introducing programming concepts to children within the K-12 education system. In turtle geometry, a turtle acts as a programmable object that navigates the screen, drawing as it goes and turning at specified angles, to create simple visual patterns. The primary objective within this framework is to generate code capable of producing simple visual inputs. These visual inputs consist of basic geometric shapes, and the programming syntax required is intentionally limited and straightforward. An example of such a task is presented in the left side of Figure 1. As illustrated, the input image is the shape of a simple square and the corresponding code only uses two simple turtle functions (forward and right) along with a simple for loop. This simplicity makes TurtleBench an effective benchmark for evaluating the capabilities of Large Multimodal Models (LMMs). To reflect different real-world use cases of an LMM in the domain of Turtle and also cover the broad range of underlying reasoning abilities, TurtleBench includes 260 tasks with a variety of types and modalities. The different types and input/output modalities are presented in Figure 1. We define two types of tasks scratch and tweak in TurtleBench. Scratch tasks challenge models to generate Python code for a specified shape using the Turtle library based on inputs that could be an image of the shape, a textual description, or both. This subset evaluates model’s proficiency in recognizing patterns within the shape and accurately translating these into executable code. Conversely, tweak tasks are designed to probe deeper into a model’s understanding, examining its ability to comprehend the implications of described modifications to shapes—such as connecting midpoints (example in Figure 1)—and their representation in the image. Here, models are provided with a base shape and are instructed to create the desired alteration in shape. Instructions for these modifications may be provided visually or textually. To simplify, in a subset of the tweak tasks, we adopt a code editing approach, supplying the original shape’s code and directing the model to edit this code to generate the target shape. We conduct an evaluation of leading LMMs on TurtleBench code generation and code editing tasks, utilizing zero-shot and visual chain-of-thought approach [41] across text-only, image-only, and mixed (text and image) input modalities. Our findings reveal that these models generally perform poorly across all setups and variety of tasks and modalities. Both GPT-4o and Gemini 1.5 Flash struggle with TurtleBench tasks, failing to solve more than 75% of them. Our results show that performance improves when tasks are presented as text rather than images, suggesting that integrating visual and linguistic information, especially for visual pattern recognition, requires further refinement. When tested with a custom library mimicking Python Turtle but using different command names, models showed a significant performance drop, revealing difficulties in generalizing visual reasoning to unfamiliar syntax. Even when allowed to choose their own programming language, models consistently failed to generate correct code, indicating broader challenges in translating visual instructions into functional programming outputs. These findings show that our benchmark poses a significant challenge for LMMs, offering key insights into their limitations. Our evaluation highlights gaps in integrating visual reasoning with programming and raises important questions for future research to address."
https://arxiv.org/html/2411.00257v1,Understanding Graphical Perception in Data Visualization through Zero-shot Prompting of Vision-Language Models,"Vision Language Models (VLMs) have been successful at many chart comprehension tasks that require attending to both the images of charts and their accompanying textual descriptions. However, it is not well established how VLM performance profiles map to human-like behaviors. If VLMs can be shown to have human-like chart comprehension abilities, they can then be applied to a broader range of tasks, such as designing and evaluating visualizations for human readers. This paper lays the foundations for such applications by evaluating the accuracy of zero-shot prompting of VLMs on graphical perception tasks with established human performance profiles. Our findings reveal that VLMs perform similarly to humans under specific task and style combinations, suggesting that they have the potential to be used for modeling human performance. Additionally, variations to the input stimuli show that VLM accuracy is sensitive to stylistic changes such as fill color and chart contiguity, even when the underlying data and data mappings are the same.","Vision Language Models (VLMs) are capable of synthesizing information in both the vision and language input modalities, leading to their application in healthcare diagnostics (19), autonomous vehicles (16), interactive robotic applications (24), and other domains. In our domain of interest, data visualization, VLMs have also been used for a range of tasks that require attending to both the images of charts and graphs and their accompanying textual descriptions (5, 14, 13, 12, 23), from simple tasks such as data extraction (14) and question answering (22, 14, 13, 12, 10, 8, 7, 15, 2, 18, 9, 20, 11) to more complex tasks such as chart generation and refinement (5). Recent research has evaluated whether VLMs show human-like visualization comprehension abilities using visualization literacy tests (1). Such tests consist of questions that measure the ability of humans to comprehend and extract information from visualizations. Studies with GPT-4 show that it can reason about visualizations, identify trends, and suggest best design practices. Yet, the model struggles with simple tasks like value retrieval and color distinctions in charts. If VLMs show human-like visualization comprehension abilities, they can be used to design and evaluate visualizations, e.g., identifying potential sources of cognitive processing (over)load. However, doing so requires establishing that VLM performance profiles map to human-like behaviors. Here, we lay the foundations for such applications by evaluating the accuracy of VLMs when performing graphical perception tasks. Graphical perception tasks require elementary perceptual operations such as retrieving numerical values from positional encodings, lengths, and angles. They were introduced by Cleveland and McGill in 1984 (3), in a series of experiments where participants were asked to extract two numerical quantities from a chart and to judge the proportion of the smaller quantity against the larger (see Tasks 1-6 of Fig. 1). Heer and Bostock (6) later replicated this study with a larger pool of participants recruited from the crowd-sourcing platform MTurk. They also extended the stimuli to include other types of judgment tasks, such as area judgments (Fig. 1, Task 7). These studies revealed potential sources of cognitive processing load in the complexity of common visualizations. Inspired by these studies, other prior work have used the same stimuli to investigate relational reasoning in CNNs compared to human performance (4). We extend these experiments to evaluate the human-like performance of popular VLMs in a zero-shot, out-of-the-box manner. In this work, we evaluate whether VLMs can simulate human graphical perception performance when performing the same seven tasks from these seminal studies (3, 6). To do so, we first recreated the original stimuli, implementing 45 trials for each of the seven chart types shown in Figure 1. Each trial included a visualization with two segments highlighted. We then zero-shot prompted the GPT-4o-mini model (17) to 1) indicate which segment is smaller, and 2) estimated percentage of the smaller segment is the larger, in a procedure similar to the one used by Heer and Bostock (6). Overall, our contributions are: • Behavioral evaluation of VLMs on graphical perception tasks: We assess whether GPT-4o-mini can simulate human-like behaviors by comparing the accuracy and confidence of the VLM in interpreting visualizations against human performance profiles (3, 6). • Model performance across prompts: We use four prompt variations to test the suitability of VLMs for modeling human graphical perception – with and without references to the target segment colors, and with and without generation of explanations/reasonings in the output template (21). • Model performance across stimuli: We introduce variations in the stimuli as shown in Figure 3a to test how incidental factors influence the model’s performance in interpreting visual data. • Model performance on new tasks: We implement novel task variants, shown in Figure 3b, and evaluate whether VLMs show a performance decrement when the critical elements are contiguous. Figure 1: Examples of the seven tasks in our study, adapted from (6). For each visualization, the VLM was prompted to compare the two segments in blue and yellow (also labeled A and B, respectively)."
https://arxiv.org/html/2411.00254v1,A Novel Breast Ultrasound Image Augmentation Method Using Advanced Neural Style Transfer: An Efficient and Explainable Approach,"Clinical diagnosis of breast malignancy (BM) is a challenging problem in the recent era. In particular, Deep learning (DL) models have continued to offer important solutions for early BM diagnosis but their performance experiences overfitting due to the limited volume of breast ultrasound (BUS) image data. Further, large BUS datasets are difficult to manage due to privacy and legal concerns. Hence, image augmentation is a necessary and challenging step to improve the performance of the DL models. However, the current DL-based augmentation models are inadequate and operate as a black box resulting lack of information and justifications about their suitability and efficacy. Additionally, pre and post-augmentation need high-performance computational resources and time to produce the augmented image and evaluate the model performance. Thus, this study aims to develop a novel efficient augmentation approach for BUS images with advanced neural style transfer (NST) and Explainable AI (XAI) harnessing GPU-based parallel infrastructure. We scale and distribute the training of the augmentation model across 8 GPUs using the Horovod framework on a DGX cluster, achieving a 5.09 speedup while maintaining the model’s accuracy. The proposed model is evaluated on 800 (348 benign and 452 malignant) BUS images and its performance is analyzed with other progressive techniques, using different quantitative analyses. The result indicates that the proposed approach can successfully augment the BUS images with 92.47% accuracy.","The rising prominence of deep learning (DL) based techniques to be explainable is becoming more paramount, especially in domains where critical decisions must be made quickly, like medical image analysis. However, among all the cancers, breast malignancy (BM) is the most prevalent disease and is the fifth leading cause of cancer-related death among women worldwide (Atrey, Singh, and Bodhey 2024). As per the World Health Organization (WHO), in 2020, approximately 2.3 million new BM were reported with 6,85,000 deaths due to this disease. Further, according to the American Cancer Society in the United States, there will be 310,720 new cases were reported for BM and 56,500 cases of Ductal Carcinoma In Situ (DCIS) will be identified as of the year 2024 . Therefore, early diagnosis of BM may decrease the disease’s effects, reduce the chance of infection, and improve medical treatment. In place of biopsy and physical examination, medical imaging (breast magnetic resonance imaging (MRI), mammogram, and, ultrasound (US), etc.) is an essential component in the timely detection of BM (Atrey et al. 2023; Panigrahi et al. 2024; Panigrahi, Verma, and Singh 2022). Although biopsy is the current gold standard procedure for BM diagnosis, due to the lack of labs in remote locations and, the painful, inconvenient, and time-consuming process, US imaging is an essential alternative in comparison to MRI and mammography for diagnosing BM. Also, when comparing MRI and mammography, the US has a number of advantages such as enhanced sensitivity, radiation-free imaging, cheap cost, and convenient accessibility (Panigrahi, Verma, and Singh 2019; Zhu et al. 2024; Panigrahi, Verma, and Singh 2018). However, due to significant drawbacks of US images like poor contrast and resolution, indistinct margins caused by noise such as speckles, faint surrounding tissue, and acoustic shadowing, the breast US (BUS) image-based diagnostic method is extremely operator-reliant. To overcome these issues computer-aided detection (CAD) system using breast US images is introduced by the researcher (Zhu et al. 2024; Panigrahi, Verma, and Singh 2018; Rahmani, Shirazi, and Behnam 2024; Wang et al. 2024). In order to reduce operator dependency, inter and intra-observer variability and offer an alternate method for the automated and early BM diagnosis, the current CAD systems made use of machine learning (ML) and DL techniques (Panigrahi et al. 2024). Further, the performance of the DL models significantly depends on training data set size and requires human annotation by radiologists. The latter is a time-consuming task. Therefore, publicly accessible BUS image data sets are small resulting in over-fitting and unable to provide generalized output. Moreover, large BUS datasets are difficult to manage because of privacy and legal concerns. Hence, image augmentation is a necessary and challenging step to enhance the performance of the DL models and regularize the overfitting (Xu et al. 2024). Though existing DL models have achieved some success in this direction, their performance and suitability for augmentation in BUS images are still unsatisfactory (Oza et al. 2022). Further, most researchers use the traditional DL models as black-box models for image augmentation (Sarp et al. 2023). These cutting-edge DL models lack of the information and justifications to support the radiologists for better decisions and interpretations of BM. This opportunity is made possible by explainable AI (XAI), which converts DL-based black-box models into more transparent and understandable gray-box models. Additionally, pre and post-augmentation need high-performance resources for computation and time to produce the augmented image and evaluate the model performance. Thus, integrated data-parallel distributed training on high-performance computing systems built with GPUs becomes essential (Masuma Iqrah et al. 2024; Koo et al. 2023). This motivates us to develop a novel augmentation approach for BUS images with advanced Neural Style Transfer (NST) and XAI harnessing GPU-based parallel infrastructure. The proposed augmentation approach is three-fold such as 1) Initially we introduced a novel style loss function by combining the style loss from demystifying NST (DNST) (Li et al. 2017) and m⁢r2𝑚superscript𝑟2mr^{2}italic_m italic_r start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPTNST (Wang et al. 2020) model, 2) further, A XAI based layer-wise relevance propagation (LRP) method is applied to the content loss function of the proposed NST model for explaining the importance of the features. This model is a post-hoc XAI model that computes the relevance score to indicate the significance of a feature in the input image, 3) To improve the efficiency of the augmentation model a distributed deep learning model training using Horovod framework on a DGX cluster that executes on 8 GPUs with a nearly linear speedup of 5.09 is used. Finally, the efficacy of the presented model is determined through quantitative analysis. Contributions and paper outline After identifying the shortcomings in current research (covered in Origin of the problems), the following are the proposed approaches this study presents to address these issues: • A novel NST-based augmentation model is proposed by combining the demystifying NST (DNST) (Li et al. 2017) and m⁢r2𝑚superscript𝑟2mr^{2}italic_m italic_r start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPTNST (Wang et al. 2020) model. In this model the two loss components utilize a pre-trained ResNet50 model to extract semantic features from images as well as to resolve overfitting and gradient degradation issues. • An XAI-based LRP method is integrated with the content loss function of the proposed augmentation model and a heatmap is generated to explain the importance of the extracted features for the model’s decision-making process. By elucidating key features, this model aims to improve the interpretability and performance of the deep learning model, facilitating more generalized outputs. • To improve the efficiency of the augmentation model, a distributed deep learning model training leveraging Horovod on a DGX is employed that executes on 8 GPUs, yielding 5-fold speed up compared to the single-GPU execution time. • To classify pre and post-augmented BUS images, we utilized a fully fine-tuned ResNet50 model to improve the accuracy and to support the radiologists for better decisions and interpretations of breast malignancy. The ResNet50 trained with augmented data yields 92.47% accuracy and it increased by 37.26% from the pre-augmented BUS images. • To demonstrate the efficacy of the proposed model, we compared our model with cutting-edge methods using benchmark evaluation metrics. The remaining paper is organized as follows. Section Related work presents the related work and theoretical background on the augmentation model. Section Material and Methods, covered the materials and techniques utilized in the suggested methodology. The experimental results and discussions are described in Section Experimental results and discussion, and finally, this study is wrapped up in Section Conclusion."
https://arxiv.org/html/2411.00238v1,Understanding the Limits of Vision Language Models Through the Lens of the Binding Problem,"Recent work has documented striking heterogeneity in the performance of state-of-the-art vision language models (VLMs), including both multimodal language models and text-to-image models. These models are able to describe and generate a diverse array of complex, naturalistic images, yet they exhibit surprising failures on basic multi-object reasoning tasks – such as counting, localization, and simple forms of visual analogy – that humans perform with near perfect accuracy. To better understand this puzzling pattern of successes and failures, we turn to theoretical accounts of the binding problem in cognitive science and neuroscience, a fundamental problem that arises when a shared set of representational resources must be used to represent distinct entities (e.g., to represent multiple objects in an image), necessitating the use of serial processing to avoid interference. We find that many of the puzzling failures of state-of-the-art VLMs can be explained as arising due to the binding problem, and that these failure modes are strikingly similar to the limitations exhibited by rapid, feedforward processing in the human brain.","Recent progress in training large-scale neural networks on internet-scale datasets has led to the creation of AI systems with capabilities rivaling human performance across a broad range of complex tasks. Most recently, this has given rise to an array of vision language models (VLMs), including multimodal language models such as GPT-4v that can generate text descriptions of multimodal text and image inputs [achiam2023gpt], and text-to-image models such as DALL-E 3 that can generate images from natural language descriptions [ramesh2022hierarchical]. However, despite the considerable success of VLMs across many tasks, these models still perform poorly on several surprisingly simple multi-object reasoning tasks – such as counting [zhang2024good, rane2024can, rahmanzadehgervi2024vision], relational image generation [conwell2022testing], relational scene understanding [thrush2022winoground, lewis2022does], and simple visual analogy tasks [mitchell2023comparing, yiu2024kiva] – on which humans achieve near perfect accuracy. Drawing from theoretical work both in cognitive science and neuroscience, we turn to the binding problem [treisman1980feature, von1994correlation, roskies1999binding, greff2020binding, frankland2021no] as a potential explanation for these limitations. ‘Binding’ refers to the ability to associate one feature of an object (e.g., its color) with the other features of that object (e.g., its shape and location), and the ‘binding problem’ refers to the question of how the brain accomplishes this without interference between the features for different objects. It is widely recognized that the human visual system relies on serial processing to solve this problem, iteratively directing attention to individual objects so as to avoid interference [treisman1980feature, roelfsema2023solving], and that binding errors arise when it is forced to rely on rapid, parallel visual processing [treisman1980feature, kaufman1949discrimination, miller1956magical]. For example, when human participants are not able to effectively deploy serial processing (e.g., because attention is overloaded, or because speeded judgments are required), they are susceptible to so-called illusory conjunctions (e.g., mistakenly identifying a red square in an image that contains a green square and a red circle) [treisman1982illusory]. In this work, we test the hypothesis that the failures exhibited by VLMs on multi-object reasoning tasks are due to representational interference resulting from an inability to manage the binding problem. We first investigate two classic tasks from the cognitive science literature, visual search [treisman1980feature] and numerical estimation [kaufman1949discrimination, miller1956magical] (i.e., counting), finding that a wide range of VLMs (including 5 multimodal language models and 4 text-to-image models) exhibit stark capacity constraints similar to those displayed by human observers when forced to make speeded responses. Importantly, although these effects are more pronounced for scenes with more objects, they cannot be explained entirely as a function of the number of objects in a scene. Instead, we find that performance is best explained by the probability of interference given the specific distribution of features and their conjunctions within a scene. Motivated by this observation, we develop a novel scene description benchmark that systematically varies the likelihood of interference, finding that this quantity is highly predictive of binding errors. We also apply these insights to better understand the limitations of VLMs in visual analogy tasks, introducing a simple input pre-processing technique to reduce the potential for representational interference. We show that this technique improves the performance of VLMs on the task, suggesting that their original limitation on this task may be due to a more basic difficulty with processing multi-object scenes, rather than an inability to process relations. Finally, we discuss the normative factors that underlie the binding problem [barak2013sparseness, frankland2021no, musslick2023rational], highlighting the role of compositional representations, which are useful for generalization, but introduce the potential for interference when shared representations are used to process multiple objects at the same time. We argue that, surprisingly, the binding failures exhibited by VLMs imply the presence of compositional representations. Overall, these results highlight the usefulness of cognitive science in helping to understand the limits of large-scale generative models, and suggest the presence of a common set of principles that govern information processing in both artificial systems and human cognition."
https://arxiv.org/html/2411.00222v1,Protecting Feed-Forward Networks from Adversarial Attacks Using Predictive Coding,"An adversarial example is a modified input image designed to cause a Machine Learning (ML) model to make a mistake; these perturbations are often invisible or subtle to human observers and highlight vulnerabilities in a model’s ability to generalize from its training data. Several adversarial attacks can create such examples, each with a different perspective, effectiveness, and perceptibility of changes. Conversely, defending against such adversarial attacks improves the robustness of ML models in image processing and other domains of deep learning. Most defence mechanisms require either a level of model awareness, changes to the model, or access to a comprehensive set of adversarial examples during training, which is impractical. Another option is to use an auxiliary model in a preprocessing manner without changing the primary model. This study presents a practical and effective solution – using predictive coding networks (PCnets) as an auxiliary step for adversarial defence. By seamlessly integrating PCnets into feed-forward networks as a preprocessing step, we substantially bolster resilience to adversarial perturbations. Our experiments on MNIST and CIFAR10 demonstrate the remarkable effectiveness of PCnets in mitigating adversarial examples with about 82%percent8282\%82 % and 65%percent6565\%65 % improvements in robustness, respectively. The PCnet, trained on a small subset of the dataset, leverages its generative nature to effectively counter adversarial efforts, reverting perturbed images closer to their original forms. This innovative approach holds promise for enhancing the security and reliability of neural network classifiers in the face of the escalating threat of adversarial attacks.","An adversarial example is a modified input intended to cause a machine-learning model to make a mistake. The modifications are often imperceptible or very subtle to human observers. However, predictive coding can reverse such alterations due to its perturbation resiliency, providing more robustness against such attacks. This defensive strategy against adversarial attacks includes generative mechanisms that revert the perturbed images to their original form. Predictive coding offers a theoretical framework to support such a defence. To help understand this work, we will briefly discuss adversarial examples, including how to create and defend against them. We will also mention the attack methods we used in our experiments and popular corresponding defence strategies in subsections 1.1.1 and 1.1.2, respectively. We will introduce the predictive coding framework and its learning algorithm in subsection 1.2. We then explain the experiment setups in section 2, and show the results in section 3, which will be discussed in section 4. At the end, we will summarize our work and point to possible future venues in sections 5 and 6, respectively. 1.1 Adversarial Attacks and Defences Unlike humans, who robustly interpret visual stimuli, artificial neural networks can be deceived by adversarial attacks (ATs), particularly perturbation attacks [1]. These attacks subtly alter an image to trick a well-trained trained feed-forward network (FFnet) used for classification tasks [2, 3] (see figure 1). One standard method to create an adversarial example (AE) that causes the FFnet to misclassify the image as a specific target label is to find a perturbation that minimizes the loss function argminδ∈Δ⁢ℓ⁢(Fθ⁢(x+δ),yt),𝛿Δargminbold-ℓsubscript𝐹𝜃𝑥𝛿subscript𝑦𝑡\underset{\delta\in\Delta}{\mathrm{argmin}}\ \boldsymbol{\ell}(F_{\theta}(x+% \delta),y_{t}),start_UNDERACCENT italic_δ ∈ roman_Δ end_UNDERACCENT start_ARG roman_argmin end_ARG bold_ℓ ( italic_F start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x + italic_δ ) , italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , (1) where: • x𝑥xitalic_x represents the image, • δ𝛿\deltaitalic_δ is the perturbation needed to deceive the FFnet when applied to the image, and ‖δ‖∞<ϵsubscriptnorm𝛿italic-ϵ\|\delta\|_{\infty}<\epsilon∥ italic_δ ∥ start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT < italic_ϵ is enforced. • ΔΔ\Deltaroman_Δ represents allowable perturbations that are visually indistinguishable to humans. • ytsubscript𝑦𝑡y_{t}italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is the 1-hot vector (i.e., etsubscript𝑒𝑡e_{t}italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT) corresponding to the target label t𝑡titalic_t. • Fθ⁢(⋅)subscript𝐹𝜃⋅F_{\theta}(\cdot)italic_F start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( ⋅ ) is the FFnet model, (i.e., Fθ:x→y∈ℝk:subscript𝐹𝜃→𝑥𝑦superscriptℝ𝑘F_{\theta}:x\rightarrow y\in\mathbb{R}^{k}italic_F start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT : italic_x → italic_y ∈ blackboard_R start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT, where k𝑘kitalic_k is the number of classes), • θ𝜃\thetaitalic_θ represents all parameters defining the model. • ℓbold-ℓ\boldsymbol{\ell}bold_ℓ is the cross-entropy loss function. This optimization can be achieved iteratively [4]. Alternatively, instead of deceiving the model by a specific target label, the optimization can be solved for an untargeted attack by maximizing the loss argmaxδ∈Δ⁢(x)⁢ℓ⁢(Fθ⁢(x+δ),y),𝛿Δ𝑥argmaxbold-ℓsubscript𝐹𝜃𝑥𝛿𝑦\underset{\delta\in\Delta(x)}{\mathrm{argmax}}\ \boldsymbol{\ell}(F_{\theta}(x% +\delta),y),start_UNDERACCENT italic_δ ∈ roman_Δ ( italic_x ) end_UNDERACCENT start_ARG roman_argmax end_ARG bold_ℓ ( italic_F start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x + italic_δ ) , italic_y ) , (2) for the given pair (x,y)𝑥𝑦(x,y)( italic_x , italic_y ), which can be achieved in one step using methods like the fast gradient sign method (FGSM) [5], or other approaches [6, 7, 8, 9, 10, 11]. When testing a well-trained FFnet MNIST classifier (with an accuracy of approximately 98%percent9898\%98 %) against FGSM-generated AEs (with ϵ≃0.78%similar-to-or-equalsitalic-ϵpercent0.78\epsilon\simeq 0.78\%italic_ϵ ≃ 0.78 %), the adversarial success rate is about 41%percent4141\%41 %. To defend against ATs, augmenting the training dataset with AEs can improve the classifier’s resilience, achieving an accuracy of approximately 94%percent9494\%94 %. Alternatively, a min-max approach to directly counteract AEs can enhance robustness within specific perturbation limits [7, 12]. Figure 1: FFnet’s perception of the image changes as the noise perturbed the image. FFnet perceives the original image Pr⁡(y0=1|x)=0.99probabilitysubscript𝑦0conditional1𝑥0.99\Pr(y_{0}=1|x)=0.99roman_Pr ( start_ARG italic_y start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 1 | italic_x end_ARG ) = 0.99 while the perception changed to Pr⁡(y3=1|x+δ)=0.87probabilitysubscript𝑦3conditional1𝑥𝛿0.87\Pr(y_{3}=1|x+\delta)=0.87roman_Pr ( start_ARG italic_y start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT = 1 | italic_x + italic_δ end_ARG ) = 0.87 on perturbation. 1.1.1 Creating Adversarial Examples Adversarial examples (AEs) are crucial for assessing and improving the robustness of machine learning (ML) models, especially in deep learning. In image data, creating AEs involves making imperceptible changes to the original image to mislead the model into misclassifying the image. Various methods are available to generate such AEs, particularly for deceiving deep neural networks. Below, we briefly discuss the main gradient-based techniques relevant to our work while noting that there are other techniques to create AEs [8, 9, 10, 11]. • Fast Gradient Sign Method (FGSM) modifies the input image by computing the loss gradient for the input image and then making a small step in the opposite direction to increase the loss [5]. • Basic Iterative Method (BIM), an extension of FGSM, takes multiple small steps while adjusting the direction of the perturbation at each step [6]. • Projected Gradient Descent (PGD) modifies the input image in multiple iterations with a constraint on the perturbation’s size. PGD starts from a random point within a small ball (i.e., ϵitalic-ϵ\epsilonitalic_ϵ-ball) around the original image and performs a series of gradient descent steps to maximize the prediction error while ensuring the perturbation is smaller than the specified ϵitalic-ϵ\epsilonitalic_ϵ [7]. • Carlini & Wagner (C&W) attack optimizes the perturbation directly through a loss function that aims to deceive to a desired target label and keep the perturbation small. It often produces subtle perturbations that are highly effective at fooling neural networks [4]. These methods differ in complexity, the amount of required knowledge about the target model (white box vs. black box), the type of perturbations (targeted vs. non-targeted), and the strength and stealthiness of the attack. The choice of method often depends on the adversary’s access to the model parameters and its specific requirements, including the robustness of the target model and the desired invisibility of the modifications. 1.1.2 Defending Against Adversarial Attacks Although various adversarial attacks exist, some defence mechanisms attempt to protect ML models against such attacks. Here, we review defence strategies for each previously mentioned attack. • For FGSM and BIM/PGD: Adversarial training involves training the model using adversarial and clean examples. It has been particularly effective against gradient-based attacks like FGSM and BIM. Gradient masking attempts to hide or modify gradients so that they are less useful for generating adversarial examples. However, this method has often been criticized and can be circumvented [13]. • For C&W Attack: Some defences estimate the likelihood that input is adversarial using auxiliary models or statistical analyses [4]. Defensive distillation involves training a model to output softened probabilities of classes, making it harder for an attacker to find gradients that can effectively manipulate the model’s output [13]. While these methods offer some protection against specific types of adversarial attacks, it is essential to note that there is no one-size-fits-all solution, and sophisticated or adaptive attackers can circumvent many defences. However, some defence strategies come with a cost, and there is a trade-off between robustness and accuracy [12]. Continued research is crucial to improving the robustness of neural networks against these threats. 1.2 Predictive Coding Computational neuroscience seeks to understand behavioural and cognitive phenomena at the level of individual neurons or networks of neurons. One approach to solving difficult problems, such as adversarial attacks, which do not seem to be a problem for the brain, is to explore biologically plausible perception models. The model we will be using is predictive coding (PC)111Various cortical theories support the bidirectional model [14, 15, 16], as well as free-energy principles [17]., a neural model capable of implementing error backpropagation in a biologically plausible manner [18, 19, 20]. 1.2.1 Model Schema and The Learning Algorithms The concept of predictive coding suggests that the brain works to minimize prediction error [21]. This model aims to improve overall predictions, and all neurons work towards this common objective. In a predictive coding network (PCnet), each neuron, or PC unit, consists of a value (v𝑣vitalic_v) and an error node (ε𝜀\varepsilonitalic_ε). These PC units are organized into layers, similar to artificial neural networks (ANNs), forming PCnets that learn by adjusting parameters to refine predictions and reduce errors between layers. For example, in a PCnet, layer i𝑖iitalic_i contains vectors visubscript𝑣𝑖v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and εisubscript𝜀𝑖\varepsilon_{i}italic_ε start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, as illustrated in figure 2. Vector visubscript𝑣𝑖v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT predicts the values of the next layer, vi−1subscript𝑣𝑖1v_{i-1}italic_v start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT, using prediction weights Mi−1subscript𝑀𝑖1M_{i-1}italic_M start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT. The resulting error, εi−1subscript𝜀𝑖1\varepsilon_{i-1}italic_ε start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT, is then communicated back via correction weights Wi−1subscript𝑊𝑖1W_{i-1}italic_W start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT, allowing visubscript𝑣𝑖v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT to improve its predictions. Figure 2: A typical PCnet arranged in a feed-forward manner. Each box represents a population of neurons containing value and error nodes. The network dynamics (as in equations 3 - 7) are described by the activation function σ𝜎\sigmaitalic_σ, Hadamard product ⊙direct-product\odot⊙, outer product ⊗tensor-product\otimes⊗, decay coefficient ξ𝜉\xiitalic_ξ, and time constants τ𝜏\tauitalic_τ and γ𝛾\gammaitalic_γ, where τ<γ𝜏𝛾\tau<\gammaitalic_τ < italic_γ. τ⁢ε˙i𝜏subscript˙𝜀𝑖\displaystyle\tau\,\dot{\varepsilon}_{i}italic_τ over˙ start_ARG italic_ε end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT =vi−Mi⁢σ⁢(vi+1)−bi−ξ⁢εiabsentsubscript𝑣𝑖subscript𝑀𝑖𝜎subscript𝑣𝑖1subscript𝑏𝑖𝜉subscript𝜀𝑖\displaystyle=v_{i}-M_{i}\sigma(v_{i+1})-b_{i}-\xi\varepsilon_{i}= italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_M start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_σ ( italic_v start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT ) - italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_ξ italic_ε start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT (3) τ⁢v˙i𝜏subscript˙𝑣𝑖\displaystyle\tau\,\dot{v}_{i}italic_τ over˙ start_ARG italic_v end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT =−εi+Wi−1⁢εi−1⊙σ′⁢(vi)absentsubscript𝜀𝑖direct-productsubscript𝑊𝑖1subscript𝜀𝑖1superscript𝜎′subscript𝑣𝑖\displaystyle=-\varepsilon_{i}+W_{i-1}\varepsilon_{i-1}\odot\sigma^{\prime}(v_% {i})= - italic_ε start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_W start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT italic_ε start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ⊙ italic_σ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) (4) γ⁢M˙i𝛾subscript˙𝑀𝑖\displaystyle\gamma\,\dot{M}_{i}italic_γ over˙ start_ARG italic_M end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT =εi⊗σ⁢(vi+1)absenttensor-productsubscript𝜀𝑖𝜎subscript𝑣𝑖1\displaystyle=\varepsilon_{i}\otimes\sigma(v_{i+1})= italic_ε start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ⊗ italic_σ ( italic_v start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT ) (5) γ⁢W˙i𝛾subscript˙𝑊𝑖\displaystyle\gamma\,\dot{W}_{i}italic_γ over˙ start_ARG italic_W end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT =σ⁢(vi+1)⊗εiabsenttensor-product𝜎subscript𝑣𝑖1subscript𝜀𝑖\displaystyle=\sigma(v_{i+1})\otimes\varepsilon_{i}= italic_σ ( italic_v start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT ) ⊗ italic_ε start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT (6) γ⁢b˙i𝛾subscript˙𝑏𝑖\displaystyle\gamma\,\dot{b}_{i}italic_γ over˙ start_ARG italic_b end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT =εiabsentsubscript𝜀𝑖\displaystyle=\varepsilon_{i}= italic_ε start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT (7) where bisubscript𝑏𝑖b_{i}italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is the bias for error node εisubscript𝜀𝑖\varepsilon_{i}italic_ε start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. Training a PCnet involves clamping input and output-layer value nodes to sensory input and target values and running the network until it reaches equilibrium. The network’s state variables (visubscript𝑣𝑖v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, εisubscript𝜀𝑖\varepsilon_{i}italic_ε start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT) reach equilibrium faster than the parameters (Misubscript𝑀𝑖M_{i}italic_M start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, Wisubscript𝑊𝑖W_{i}italic_W start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, bisubscript𝑏𝑖b_{i}italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT) due to τ<γ𝜏𝛾\tau<\gammaitalic_τ < italic_γ. After training, the parameters M𝑀Mitalic_M, W𝑊Witalic_W, b𝑏bitalic_b are fixed, effectively setting γ𝛾\gammaitalic_γ to infinity. When a perfect prediction is achieved, the error signal (ε𝜀\varepsilonitalic_ε) is zero, stabilizing the value node without further corrections. This state minimizes the Hopfield-like energy function [18] given by equation, E=ξ2⁢∑i‖𝜺i‖2.𝐸𝜉2subscript𝑖superscriptnormsubscript𝜺𝑖2E=\tfrac{\xi}{2}\sum_{i}\|{\bf\it\varepsilon}_{i}\|^{2}.italic_E = divide start_ARG italic_ξ end_ARG start_ARG 2 end_ARG ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∥ bold_italic_ε start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . (8) After training, when initializing the network with a given input image, when the value nodes are unclamped, the network’s ability to reduce energy can lead to potential changes in the input image. The PCnet modifies images without impacting their correct classification, as illustrated in figure 3a. The left image displays the original version, while the right image shows the version altered by PCnet. Likewise, when PCnet introduces perturbations to the adversarial image, as seen in figure 3b, the FFnet can classify it correctly. (a) Perturbation made to an image. (b) Perturbation made to an adversarial image. Figure 3: PCnet perturbation is demonstrated using both the original and adversarial images. PCnet modifies the given input based on its trained dynamics. As shown in LABEL:sub@fig:imageBeforeAfterPC, the original image x𝑥xitalic_x is depicted on the left, while its perturbation PCnet⁢(x)PCnet𝑥\mathrm{PCnet}(x)roman_PCnet ( italic_x ) is shown on the right. Similarly, LABEL:sub@fig:advBeforeAfterPC presents the adversarial image z𝑧zitalic_z on the left, alongside its perturbation p𝑝pitalic_p on the right. PCnet’s approach to the credit assignment problem differs from backpropagation (backprop), which is the learning algorithm of ANNs [22]. Backpropagation seems unlikely in the brain for several reasons, such as its requirement for weight transposing and transferring between layers. In contrast, PCnet can effectively learn without these requirements using the dynamics of each parameter (equations (5), (6), and (7)). These dynamics are consistent with the Hebbian learning rule, which only requires local information and pre- and post-synaptic activities [23, 24, 25, 26, 27, 28, 29]. This learning algorithm facilitates flexibility and feasibility in using different architectures."
https://arxiv.org/html/2411.00029v1,Preserving Pre-trained Representation Space:On Effectiveness of Prefix-tuning for Large Multi-modal Models,"Recently, we have observed that Large Multi-modal Models (LMMs) are revolutionizing the way machines interact with the world, unlocking new possibilities across various multi-modal applications. To adapt LMMs for downstream tasks, parameter-efficient fine-tuning (PEFT) which only trains additional prefix tokens or modules, has gained popularity. Nevertheless, there has been little analysis of how PEFT works in LMMs. In this paper, we delve into the strengths and weaknesses of each tuning strategy, shifting the focus from the efficiency typically associated with these approaches. We first discover that model parameter tuning methods such as LoRA and Adapters distort the feature representation space learned during pre-training and limit the full utilization of pre-trained knowledge. We also demonstrate that prefix-tuning excels at preserving the representation space, despite its lower performance on downstream tasks. These findings suggest a simple two-step PEFT strategy called Prefix-Tuned PEFT (PT-PEFT), which successively performs prefix-tuning and then PEFT (i.e., Adapter, LoRA), combines the benefits of both. Experimental results show that PT-PEFT not only improves performance in image captioning and visual question answering compared to vanilla PEFT methods but also helps preserve the representation space of the four pre-trained models.","Understanding the visual scene and expressing it with a natural language are two distinct tasks yet the human brain can comprehensively handle both without difficulty. Large multi-modal models (LMMs) mimic such capability by training a deep neural network (DNN) such that it learns semantically meaningful connections between vision and language from a large number of image-text pairs (Li et al., 2020b; Zhang et al., 2021b; Wang et al., 2022b; Radford et al., 2021). Recently, LMMs have been widely used due to their broad range of applications, including chatbot, robot control, and video generation (Ouyang et al., 2022; Brohan et al., 2023; Ramesh et al., 2022). Figure 1: Advantages of the proposed PT-PEFT, which performs 1) prefix-tuning and 2) fine-tuning (i.e., parameter-efficient or full fine-tuning) sequentially. In the pre-training, LMMs are trained to predict the masked words or next words from the image-text pair (Li et al., 2023; Alayrac et al., 2022; Wang et al., 2022a). In the second step called fine-tuning, the pre-trained LMMs are tailored to the specific downstream task. It has been shown that fine-tuning provides superior performance in various downstream tasks such as image captioning (IC), visual question answering (VQA), and image-text retrieval (Li et al., 2023; Wang et al., 2022a, b; Zhang et al., 2021b). However, fine-tuned models often suffer from the loss of generalization capability obtained from the pre-training (Sun et al., 2015; Brown et al., 2020a). Since the task-specific dataset is far smaller than the pre-training unlabeled dataset, the pre-trained model can be easily overfitted to the small-sized downstream task dataset, leading to degraded performance (Kumar et al., 2022). Various approaches have been suggested over the years to address the problem. In prompt-based approaches, manually designed prompts or trainable continuous embedding vectors are integrated into the input data to adapt the model for downstream tasks (Li and Liang, 2021; Liu et al., 2021; Tam et al., 2022; Lester et al., 2021). In knowledge distillation-based fine-tuning approaches, the model minimizes the distance between the distribution of the pre-trained and fine-tuned models (Xu et al., 2020; Sanh et al., 2019; Boschini et al., 2022). The common wisdom behind these approaches is to minimize the modification of the pre-trained model parameters while maintaining performance on downstream tasks. Figure 2: Performance of different task adaptation methods on COCO image captioning dataset. The proposed method (PT-) consistently improves performance when combined with other methods. One drawback of the full model fine-tuning is the huge computational burden caused by the model parameters update. In an effort to reduce the huge training cost, various parameter-efficient fine-tuning (PEFT) techniques have been proposed (Li and Liang, 2021; Houlsby et al., 2019; Hu et al., 2022; He et al., 2021). In these approaches, only a small set of additional modules (e.g., prefix, Adapter, LoRA) is trained instead of relying on full fine-tuning. These approaches are especially beneficial for training the large pre-trained model like GPT (Brown et al., 2020b), T5 (Raffel et al., 2020), and Llama (Touvron et al., 2023). Training efficiency is a well-known advantage of prefix-tuning. Unlike other PEFT methods, prefix-tuning does not modify the model’s parameters, leaving the representation space unchanged. To investigate the changes in the representation space, we analyze the feature representation matrices using singular value decomposition (SVD). Notably, we observe that the representation space of a fine-tuned model (in IC and VQA) utilizes only a limited set of effective basis vectors (60% of those in the pre-trained model) to express the output. Clearly, this limits the model’s ability to fully enjoy the benefits obtained from pre-training (see Figure 4). In contrast, we discover that all the basis vectors are utilized in the prefix-tuned model, implying that the prefix-tuning effectively preserves the inherited representation space from the pre-training. While the prefix-tuning is effective in preserving pre-trained knowledge, the efficacy of this approach is somewhat questionable since the reported evaluation results are not conclusive. Some studies claim that the prefix-tuning performs comparable to the model parameter-tuning (e.g., full fine-tuning, LoRA, Adapter), while others argue that the prefix-tuning struggles in the training of relatively small-sized language models (Liu et al., 2021; Tam et al., 2022). Figure 3: Qualitative image captioning results of zero-shot learning, prefix-tuned, and fine-tuned models. Although fine-tuning provides accurate answers, its results often ignore visual details compared to the other two. An aim of this paper is to propose a simple yet effective tuning strategy to combine the merits of two seemingly distinct approaches. The proposed method, henceforth referred to as Prefix-Tuned PEFT (PT-PEFT), performs the prefix-tuning and the model parameter-tuning sequentially. The key feature of PT-PEFT is to preserve the pre-trained feature space through the prefix-tuning and then refine the model parameters using the PEFT method. Intuitively, this approach resembles a language model learning a new task using prompt sentences such as ""I will provide example sentences describing the given pictures in the news article style. So, please generate the caption for the given images with such style."" By providing a context suitable for the new task, the model’s adaptability is enhanced, allowing for faster convergence and minimal changes to the weights of the pre-trained model. In our experiments, we show that applying the prefix-tuning before LoRA, Adapter, and even full fine-tuning consistently improves the task performance for all datasets and various pre-trained LMMs including BLIP (Li et al., 2022), BLIP-2 (Li et al., 2023), OFA (Wang et al., 2022a) and VINVL (Zhang et al., 2021b). We also compare the simultaneous tuning of prefix and model parameters and show that the proposed sequential strategy is indeed important for maximizing performance and preserving the representation space. Our contributions are as follows: • We show the correlation between the representation space and performance through rank-based analysis. We qualitatively and quantitatively illustrate the adverse effects of representation space collapse in task performance. • We reveal that the prefix-tuning differs significantly from model parameter tuning techniques such as LoRA, Adapter, and full fine-tuning in the sense that it preserves the integrity of the pre-trained knowledge. • We propose PT-PEFT, a method that sequentially performs the prefix-tuning followed by conventional fine-tuning technique, to maximize the utilization of pre-trained knowledge in LMMs. Our experimental results demonstrate that PT-PEFT outperforms the conventional fine-tuning methods in image captioning and VQA tasks. (a) Full Fine-tuning (b) LoRA (c) S-Adapter Figure 4: Accumulated and normalized singular values of features extracted from the last layer of BLIP-2. A more concave graph indicates that the singular values are more concentrated, implying the narrower representation space. Pre-training Fine-tuning Prefix-tuning S-Adapter P-Adapter LoRA PT→absent→\xrightarrow{}start_ARROW start_OVERACCENT end_OVERACCENT → end_ARROWS-Adapter PT→absent→\xrightarrow{}start_ARROW start_OVERACCENT end_OVERACCENT → end_ARROWP-Adapter PT→absent→\xrightarrow{}start_ARROW start_OVERACCENT end_OVERACCENT → end_ARROWLoRA PT →absent→\xrightarrow{}start_ARROW start_OVERACCENT end_OVERACCENT → end_ARROW Fine-tuning VINVL 50.2 % 30.0 % 50.2 % - - - - - - 50.2 % BLIP-2 68.2 % 47.0 % 68.2 % 53.0 % 53.7 % 52.0 % 63.5 % 58.4 % 63.5 % 68.2 % Table 1: Effective rank of representation space of various fine-tuning techniques. Note that the effective rank is defined as the remaining rank ratio at which the accumulated singular values equal to 0.9 in Figure 4."
