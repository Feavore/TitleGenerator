URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.04999v1,DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation,"Significant progress has been made in open-vocabulary mobile manipulation, where the goal is for a robot to perform tasks in any environment given a natural language description. However, most current systems assume a static environment, which limits the system‚Äôs applicability in real-world scenarios where environments frequently change due to human intervention or the robot‚Äôs own actions. In this work, we present DynaMem, a new approach to open-world mobile manipulation that uses a dynamic spatio-semantic memory to represent a robot‚Äôs environment. DynaMem constructs a 3D data structure to maintain a dynamic memory of point clouds, and answers open-vocabulary object localization queries using multimodal LLMs or open-vocabulary features generated by state-of-the-art vision-language models. Powered by DynaMem, our robots can explore novel environments, search for objects not found in memory, and continuously update the memory as objects move, appear, or disappear in the scene. We run extensive experiments on the Stretch SE3 robots in three real and nine offline scenes, and achieve an average pick-and-drop success rate of 70% on non-stationary objects, which is more than a ùüê√ó\mathbf{2\times}bold_2 √ó improvement over state-of-the-art static systems.","Recent advances in robotics have made it possible to deploy robots in real world settings to tackle the open vocabulary mobile manipulation (OVMM) problem [1]. Here, the robots are tasked with navigating in unknown environments and interacting with objects following open vocabulary language instructions, such as ‚ÄúPick up X from Y and put it in Z‚Äù, where X, Y, and Z could be any object name or location. The two most common approaches to tackling OVMM are using policies trained in simulation and deploying them in the real world [2; 3; 4], or training modular systems that combine open vocabulary navigation (OVN) [5; 6; 7; 8] with different robot manipulation skills [9; 10; 11; 12; 13]. Modular systems enjoy greater efficiency and success in real-world deployment [14] as they can directly leverage advances in vision and language models [9; 12], and are able to handle more diverse and out-of-domain environments with no additional training. However, as recent analysis has shown, the primary challenge in deploying modular OVMM is that limitations of a module propagate to the entire system [9]. One key module in any OVMM system is the open vocabulary navigation (OVN) module responsible for navigating to goals in the environment. While many such OVN systems have been proposed in the literature [1; 8; 5; 11; 10; 9; 6; 7; 12; 13], they share a common limitation: they assume static, unchanging environments. Contrast this with the real world, where environments change and objects are moved by either robots or humans. Making such a restrictive assumption thus limits these systems‚Äô applicability in real-world settings. The primary reason behind this assumption is the lack of an effective dynamic spatio-semantic memory that can adapt to both addition and removal of objects and obstacles in the environment online. In this work, we propose a novel spatio-semantic memory architecture, Dynamic 3D Voxel Memory (DynaMem), that can adapt online to changes in the environment. DynaMem maintains a voxelized pointcloud representation of an environment and adds or removes points as it observes the environment change. Additionally, it supports two different ways to query the memory with natural language: a vision-language model (VLM) featurized pointcloud, and a multimodal-LLM (mLLM) QA system. Finally, DynaMem enables efficient exploration in changing environments by offering a dynamic obstacle map and a value-based exploration map that can guide the robot to explore unseen, outdated, or query-relevant parts of the world. We evaluate DynaMem as a part of full open-vocabulary mobile manipulation stack in three real world environments with multiple rounds of changes and manipulating multiple non-stationary objects, improving the static baseline by more than 2√ó2\times2 √ó (70% vs. 30%). Additionally, we identify an obstacle in efficiently developing dynamic spatio-semantic memory, namely the lack of dynamic benchmarks, since many OVN systems use static simulated environments [15; 16] or static datasets [17; 18]. We address this by developing a new dynamic benchmark, DynaBench. It consists of 9 different environments, each changing over time. We ablate our design choices in this benchmark. To the best of our knowledge, DynaMem is the first spatio-semantic memory structure supporting both adding and removing objects."
https://arxiv.org/html/2411.04983v1,DINO-WM: World Models on Pre-trainedVisual Features enable Zero-shot Planning,"The ability to predict future outcomes given control actions is fundamental for physical reasoning. However, such predictive models, often called world models, have proven challenging to learn and are typically developed for task-specific solutions with online policy learning. We argue that the true potential of world models lies in their ability to reason and plan across diverse problems using only passive data. Concretely, we require world models to have the following three properties: 1) be trainable on offline, pre-collected trajectories, 2) support test-time behavior optimization, and 3) facilitate task-agnostic reasoning. To realize this, we present DINO World Model (DINO-WM), a new method to model visual dynamics without reconstructing the visual world. DINO-WM leverages spatial patch features pre-trained with DINOv2, enabling it to learn from offline behavioral trajectories by predicting future patch features. This design allows DINO-WM to achieve observational goals through action sequence optimization, facilitating task-agnostic behavior planning by treating desired goal patch features as prediction targets. We evaluate DINO-WM across various domains, including maze navigation, tabletop pushing, and particle manipulation. Our experiments demonstrate that DINO-WM can generate zero-shot behavioral solutions at test time without relying on expert demonstrations, reward modeling, or pre-learned inverse models. Notably, DINO-WM exhibits strong generalization capabilities compared to prior state-of-the-art work, adapting to diverse task families such as arbitrarily configured mazes, push manipulation with varied object shapes, and multi-particle scenarios.","Robotics and embodied AI has seen tremendous progress in recent years. Advances in imitation learning and reinforcement learning has enabled agents to learn complex behaviors across diverse tasks Lee et al. (2024); Zhao et al. (2023); Ma et al. (2024); Hafner et al. (2024); Hansen et al. (2024); Agarwal et al. (2022); Haldar et al. (2024). Despite this progress, generalization remains a major challenge Zhou et al. (2023). Existing approaches predominantly rely on policies that, once trained, operate in a feed-forward manner during deployment‚Äîmapping observations to actions without any further optimization or reasoning. Under this framework, successful generalization inherently requires agents to possess solutions to all possible tasks and scenarios once training is complete, which is only possible if the agent has seen similar scenarios during training Brohan et al. (2023b; a); Reed et al. (2022); Etukuru et al. (2024). However, it is neither feasible nor efficient to learn solutions for all potential tasks and environments in advance. Instead of learning the solutions to all possible tasks during training, an alternate is to fit a dynamics model on training data and optimize task-specific behavior during runtime. These dynamics models, also called world models Ha & Schmidhuber (2018), have a long history in robotics and control Sutton (1991); Todorov & Li (2005); Williams et al. (2017). More recently, several works have shown that world models can be trained on raw observational data Hafner et al. (2019; 2024); Micheli et al. (2023); Robine et al. (2023); Hansen et al. (2024). This enables flexible use of model-based optimization to obtain policies as it circumvents the need for explicit state-estimation. Despite this, significant challenges still remain in it use for solving general-purpose tasks. To understand the challenges in world modeling, let us consider the two broad paradigms in learning world models: online and offline. In the online setting, access to the environment is often required so data can be continuously collected to improve the world model, which in turn improves the policy and the subsequent data collection. However, the online world model is only accurate in the cover of the policy that was being optimized. Hence, while it can be used to train powerful task-specific policies , it requires retraining for every new task even in the same environment . Instead in the offline setting, the world model is trained on an offline dataset of collected trajectories in the environment, which removes its dependence on the task specificity given sufficient coverage in the dataset. However, when required to solve a task, methods in this domain require strong auxiliary information to overcome the lack of dense coverage on the task-specific domain. This auxiliary information can take the form of expert demonstrations Pathak et al. (2018), structured keypoints Ko et al. (2023); Wen et al. (2024), access to pretrained inverse models Du et al. (2023); Ko et al. (2023) or dense reward functions , all of which reduce the generality of using offline world models. The central question to building better offline world models is if there is alternate auxiliary information that does not compromise its generality? Figure 1: We present DINO-WM, a method for training visual models by using pretrained DINOv2 embeddings of image frames (a). Once trained, given a target observation oTsubscriptùëúùëáo_{T}italic_o start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT, we can directly optimize agent behavior by planning through DINO-WM using model-predictive control (b). The use of pretrained embeddings significantly improves performance over prior state-of-the-art world models (c). In this work, we present DINO-WM, a new and simple method to build task-agnostic world models from an offline dataset of trajectories. DINO-WM models the dynamics of world on compact embeddings of the world, rather than the raw observations themselves. For the embedding, we use pretrained patch-features from the DINOv2 model, which provides both a spatial and object-centric representation prior. We conjecture that this pretrained representation enables robust and consistent world modeling, which relaxes the necessity for task-specific data coverage. Given these visual embeddings and actions, DINO-WM uses the ViT architecture to predict future embeddings. Once this model is trained, planning to solve tasks is constructed as visual goal reaching, i.e. to reach a future desired goal given the current observation. Since the predictions by DINO-WM are high quality (see Figure 4), we can simply use model-predictive control with inference-time optimization to reach desired goals without any without any extra information during testing. DINO-WM is experimentally evaluated on four environment suites spanning maze navigation, sliding manipulation, and particle manipulation tasks. Our experiments reveal the following findings: ‚Ä¢ DINO-WM produce high-quality future world modeling that can be measured by improved visual reconstruction from trained decoders. On LPIPS metrics for our hardest tasks, this improves upon prior state-of-the-art work by 56% (See Section 4.7). ‚Ä¢ Given the latent world models trained using DINO-WM, we show high success for reaching arbitrary goals on our hardest tasks, improving upon prior work by 45% on average (See Section 4.3). ‚Ä¢ DINO-WM can be trained across environment variations within a task family (e.g. different maze layouts for navigation or different object shapes for manipulation) and achieve higher rates of success compared to prior work (See Section 4.5). Code and models for DINO-WM will be open-sourced to ensure reproducibility and videos of policies are made available on our project website: https://dino-wm.github.io."
https://arxiv.org/html/2411.04919v1,Stem-OB:Generalizable Visual Imitation Learning with Stem-Like Convergent Observation through Diffusion Inversion,"Visual imitation learning methods demonstrate strong performance, yet they lack generalization when faced with visual input perturbations like variations in lighting and textures. This limitation hampers their practical application in real-world settings. To address this, we propose Stem-OB that leverages the inversion process of pretrained image diffusion models to suppress low-level visual differences while maintaining high-level scene structures. This image inversion process is akin to transforming the observation into a shared representation, from which other observations also stem. Stem-OB offers a simple yet effective plug-and-play solution that stands in contrast to data augmentation approaches. It demonstrates robustness to various unspecified appearance changes without the need for additional training. We provide theoretical insights and empirical results that validate the efficacy of our approach in simulated and real settings. Stem-OB shows an exceptionally significant improvement in real-world robotic tasks, where challenging light and appearance changes are present, with an average increase of 22.2% in success rates compared to the best baseline. See our website for more info.","Visual Imitation Learning (IL), where an agent learns to mimic the behavior of the demonstrator by learning a direct mapping from visual observations to low-level actions, has gained popularity in recent real-world robot tasks (Chi et al., 2023; Zhao et al., 2023; Wang et al., 2023; Chi et al., 2024; Ze et al., 2024). Despite the versatility demonstrated by visual IL, learned policies are often brittle and fail to generalize to unseen environments, even minor perturbations such as altering lighting conditions or changing the texture of the object may lead to failure of the learned policy (Xie et al., 2023; Yuan et al., 2024b). The underlying reason is that the high-dimensional visual observation space is redundant with virtually infinite variations in appearance that are irrelevant to the task and hard to generalize. As human beings, we can easily manipulate objects that have different appearances. For example, we can pick up a cup of coffee regardless of its color, texture, or the lighting condition of the room. This is partially because our visual system is capable of abstracting the high-level semantics of the scene, such as the silhouette of the object, the structure and arrangement of different objects, etc in a hierarchical manner (Hochstein & Ahissar, 2002), effectively merging scenes with perceptual differences to similar ‚Äúmeta‚Äù observations. Augmentation techniques such as Spectrum Random Masking (SRM) (Huang et al., 2022) and Mixup (Zhang et al., 2018) remove details from observations to encourage the model to focus on structural features; however, they lack the ability to distinguish between low-level and high-level features. It is preferable if we can sweep the photometrical differences while maintaining the high-level structure for the scene. Achieving this requires a semantic understanding of the observations, and naively perturbing the data with Gaussian noise can lead to irreversible information loss. Pretrained large image diffusion models, such as Stable Diffusion (Rombach et al., 2022; Esser et al., 2024), embed essential world knowledge for visual understanding. Apart from synthesizing new images from random noise, these models are capable to perform a reverse procedure called inversion (Song et al., 2022), which converts an image back to the space of random noises. A recent study (Yue et al., 2024) indicates that this inversion process selectively eliminates information from the image. Rather than uniformly removing information from different semantic hierarchies, it will push those images with similar structures closer in the early stages of the inversion process. Inversion is like the reprogramming of a differentiated cell back to a stem cell, which bears the totipotency to differentiate into any cell type. This characteristic aligns perfectly with our will of enhancing the robustness and generalizability of visual IL algorithms to visual variations. To distill such property into a visual IL policy, we propose an imitation learning pipeline which applies diffusion inversion to the visual observations. We name our method Stem-OB to highlight the similarity between the inversed observation and the stem cell in biology, as illustrated in Figure 1. To be specific, our method is as simple as inverting the image for reasonable steps before sending them to the downstream visual IL algorithms, effectively serving as a preprocessing step that converges low-level appearance differences into similar high-level structures. From this perspective, our approach fundamentally distinguishes from generative augmentation methods, which aim to enrich the training dataset with more unseen objects and appearances (Yu et al., 2023; Mandlekar et al., 2023). Moreover, Stem-OB is indifferent to many unspecified appearance changes, in contrast to augmentation-based methods that must concentrate on a few selected types of generalization, thereby introducing inevitable inductive biases. We provide theoretical analysis and a user study to support our claim that Stem-OB can effectively merge scenes with perceptual differences to similar ‚Äústem observations‚Äù. Empirical study demonstrates the effectiveness of our approach in a variety of simulated and real-world tasks and a range of different perturbations. Stem-OB proves to be particularly effective in real-world tasks where appearance and lighting changes hamper the other baselines, establishing an overall improvement in the success rate of 22.2%. What‚Äôs better, no inference time inversion is required for Stem-OB to take effect, making the deployment of our method virtually free of computational cost."
https://arxiv.org/html/2411.04796v1,MPVO: Motion-Prior based Visual Odometry for PointGoal Navigation,"Visual odometry (VO) is essential for enabling accurate point-goal navigation of embodied agents in indoor environments where GPS and compass sensors are unreliable and inaccurate. However, traditional VO methods face challenges in wide-baseline scenarios, where fast robot motions and low frames per second (FPS) during inference hinder their performance, leading to drift and catastrophic failures in point-goal navigation. Recent deep-learned VO methods show robust performance but suffer from sample inefficiency during training; hence, they require huge datasets and compute resources. So, we propose a robust and sample-efficient VO pipeline based on motion priors available while an agent is navigating an environment. It consists of a training-free action-prior based geometric VO module that estimates a coarse relative pose which is further consumed as a motion prior by a deep-learned VO model, which finally produces a fine relative pose to be used by the navigation policy. This strategy helps our pipeline achieve up to 2x sample efficiency during training and demonstrates superior accuracy and robustness in point-goal navigation tasks compared to state-of-the-art VO method(s). Realistic indoor environments of the Gibson dataset is used in the AI-Habitat simulator to evaluate the proposed approach using navigation metrics (like success/SPL) and pose metrics (like RPE/ATE). We hope this method further opens a direction of work where motion priors from various sources can be utilized to improve VO estimates and achieve better results in embodied navigation tasks.","Autonomous visual navigation in novel indoor environments is a fundamental skill for robots to perform further intelligent downstream tasks like finding and retrieving an object, rearranging various stuff, etc. This has been the focus of computer vision and robotics researchers for a long time. To bring together the community‚Äôs efforts and standardize the evaluation framework and metrics, Anderson et al. [1] proposed the task of PointGoal navigation. In PointNav, (illustrated in Fig. 1), the agent or robot is initialized in a previously unseen environment and tasked to reach a goal location specified with respect to its initial location, i.e. go to (‚ñ≥‚Å¢x‚ñ≥ùë•\triangle x‚ñ≥ italic_x, ‚ñ≥‚Å¢y‚ñ≥ùë¶\triangle y‚ñ≥ italic_y). The action space of the agent is discrete and mainly consists of 4 types of action : move_forward, turn_right, turn_left and stop (to end the episode). A pointnav episode is considered successful if the agent stops within a pre-determined distance of the goal location (say for e.g. 0.36 metres) and within the maximum number of time-steps allowed in an episode (say for e.g. 500 time-steps). Apart from success, the agent is evaluated via navigation metrics like SPL [1] and SoftSPL [8]. This point-goal navigation task can either be approached via map-based methods (where the agent simultaneously maps an unexplored area, localizes within it and then plans a path towards the goal) [7, 15] or via recent map-less end-to-end reinforcement learning based methods [8, 31, 19]. Under the assumption of an ideal scenario, i.e. perfect localization using noiseless GPS+Compass, noiseless egocentric RGB-D sensors and absence of actuation noise; this PointNav task (v1) is fully solved by both map-based and map-less approaches. But the real-world is not ideal, so PointNav v2 came into existence where the agent needs to localize itself (absence of GPS+Compass sensors), has noisy RGB-D observations and noisy actuations. Under this noisy setting, both map-based and map-less approaches need to focus first on accurate localization of the agent, before navigating. Recent map-less learning based approaches try to solve this by breaking down the task into two parts - learning visual odometry (VO) (for localization) and learning navigation policy (for actions) separately. During inference, this VO model can be used as a drop-in replacement for the GPS+Compass sensor with navigation policies trained using ground-truth pose in simulation. Figure 1: Point-Nav Task: The agent must navigate from its initial location (blue square) to a goal location (red square) specified as goal coordinates w.r.t. its initial location, using only its noisy RGB-D observations and noisy actuation. The agent‚Äôs and the oracle‚Äôs path is shown as blue and green lines respectively on the top-down map. Now, VO has been studied in Computer/Robot Vision literature for a long-time and many matured solutions exist but this PointNav task definition makes this problem harder to solve. Due to the agent‚Äôs discrete action space and large motion per action (default: 0.25m forward and 30 deg turns), VO needs to be estimated in a wide camera baseline setting, i.e. the two views are wide-apart and the overlapping region is less. Most of the VO methods, both traditional and learned variants assume availability of frame-pairs with a large overlap or in other words, a narrow baseline setting. But for practical robot navigation, wide-baseline VO is necessary because situations might arise where the robot motion is fast and/or observation processing or transmission FPS is low. Recent learned VO methods used with RL-based navigation policies in map-less approaches [19, 31] have tried to solve this problem and has achieved robust performance but suffer from sample-inefficiency, embodiment specificity and dataset specificity. It requires huge datasets and compute resources to train such a model and it can‚Äôt be zero-shot transferred to any other embodiment or dataset. So, we propose a robust and sample-efficient novel VO pipeline based on motion priors available while an agent is navigating an environment. We found out from our experiments that simple motion priors like action prior from agent‚Äôs controller or planner, or coarse pose prior from another geometric pose estimator, etc can help the VO model learn faster using fewer samples than that required by state-of-the-art (SoA) learned methods. This finding not only reduces compute resources needed to train one model but also helpful in scenarios when the target domain data is scarce and costly to collect and curate. Instead of solving for model generalization, we tweaked these models to be more sample-efficient. We also propose a training-free action-prior based geometric pose estimator or VO module for embodied agents which shows superior performance compared to frame-to-frame VO baselines created using state-of-the-art geometric modules, in standalone evaluation. With this novel geometric VO module, we show that motion priors can be effectively utilized to improve relative pose estimates from sparse feature matching methods. We use this module to estimate a coarse pose which serves as a better motion prior than action prior to train our neural VO model."
https://arxiv.org/html/2411.04787v1,AllGaits: Learning All Quadruped Gaits and Transitions,"We present a framework for learning a single policy capable of producing all quadruped gaits and transitions. The framework consists of a policy trained with deep reinforcement learning (DRL) to modulate the parameters of a system of abstract oscillators (i.e. Central Pattern Generator), whose output is mapped to joint commands through a pattern formation layer that sets the gait style, i.e. body height, swing foot ground clearance height, and foot offset. Different gaits are formed by changing the coupling between different oscillators, which can be instantaneously selected at any velocity by a user. With this framework, we systematically investigate which gait should be used at which velocity, and when gait transitions should occur from a Cost of Transport (COT), i.e. energy-efficiency, point of view. Additionally, we note how gait style changes as a function of locomotion speed for each gait to keep the most energy-efficient locomotion. While the currently most popular gait (trot) does not result in the lowest COT, we find that considering different co-dependent metrics such as mean base velocity and joint acceleration result in different ‚Äòoptimal‚Äô gaits than those that minimize COT. We deploy our controller in various hardware experiments, showing all 9 typical quadruped animal gaits, and demonstrate generalizability to unseen gaits during training, and robustness to leg failures. Video results can be found at https://youtu.be/OLoWSX_R868.","Animals seamlessly transition between different gaits as they change speeds, or to react to variable terrain. Such transitions emerge through inter-limb coordination governed by the interaction between the brain, the spinal cord, and the musculoskeletal system [1]. Several hypotheses have been proposed as explanations for why the gait transitions occur: to minimize energy expenditure [2], minimize peak musculoskeletal forces [3], maximize periodicity [4], and, recently, promote viability, by formalizing the notion of avoiding a fall during locomotion [5]. Gaits themselves can be formed through several different biological mechanisms, for example sensory driven [6], through descending drive [7], or coupling driven through Central Pattern Generators (CPGs) in the spinal cord [8]. In robotics, quadruped robots are displaying complex motor skills with different gaits to locomote at varying speeds and across challenging terrains, including combinations of discrete capabilities like running and jumping [9, 10, 11, 12, 13, 14, 15]. While several works study transitions between such gaits (for example), the optimal transition times, speeds, and between which discrete gaits remains an open question. Additionally, for frameworks that do show transitions between gaits, the parameters must often be re-tuned for each (MPC) [16], may have heuristics for transitioning [17], or may otherwise be non-optimal, as the cyclic motions may affect the body and joints differently. For example, in contrast with most robots (with some exceptions [18, 19]), animals do not bound with a rigid spine. Figure 1: AllGaits: snapshots from learning all quadruped gaits with Central Pattern Generators and deep reinforcement learning. For quadruped robots, gaits can be encoded in a biological neural network (CPG) [8, 20, 21, 22], enforced through a fixed pattern in optimal control frameworks [16, 23, 24], specifically rewarded in learning frameworks through a reward function [25, 26], or can emerge naturally during the training process [27, 28, 29]. One observation is that learned controllers working at a wide range of speeds seem to converge to trot gaits, especially when the reward function has terms penalizing non-stable motions, as the trot gait minimizes body angular velocities even at high speeds [30, 31]. This is in contrast to high-speed gaits found in nature, where flexible spines exhibit significant bending movement while quadruped animals bound or gallop [32], or for Springboks and Thomson‚Äôs gazelles exhibit stotting (or pronking) behaviors [33]. Reinforcement learning has been applied to directly imitate animal motions such as pacing and trotting through training separate policies that reward tracking different mocap data [34]. Generating a library of reference trajectories and training a goal-conditioned policy to imitate them, and explicitly providing transition and coupling strength timing can also lead to executing gait transitions [17]. By training three policies to locomote at specific velocities (0.375 m/s, 0.9 m/s, 1.5 m/s) while minimizing energy consumption, three corresponding distinct gaits emerged (walk, trot, pronk) [29]. Then, using these three policies as experts, the transition between different speeds and gaits was realized to locomote between 0.375‚àí1.50.3751.50.375-1.50.375 - 1.5 m/s. Transitions between gaits can also be realized by training a high-level gait policy that specifies gait patterns of each foot, while a low-level convex MPC controller optimizes the motor commands so that the robot can walk at a desired velocity using that gait pattern [35]. Combining learning with bio-inspired representations of neural circuits allows for higher centers to modulate and coordinate gaits [36, 37, 38], which can also result in the emergence of terrain-driven gait transitions to successfully cross variable gaps [39, 5]. As an alternative to learning-based control, quadruped robots have also demonstrated gait generation and transitions can occur through simple force feedback, without explicit coupling between oscillators [6, 40]. I-A Contribution While gait transitions can occur through sensory feedback [6] and/or through descending drives [7], in this paper we take a coupling-driven approach to learn to locomote with, and transition between, a variety of gaits. Coupling between different abstract oscillators is commonly used for CPG-based locomotion control of different gaits in bio-inspired robotics [41, 20, 21]. While several common quadrupedal gaits have been successfully demonstrated on quadruped hardware, previous work requires either explicit parameter tuning in MPC [16], extensive reward function tuning [25, 17], specific training schemes [29], or expert demonstrations from animals or MPC to imitate [34]. In contrast, we show all quadruped gaits and their transitions can be realized without reward function tuning or any expert demonstrations. We center our scientific investigation around three fundamental biological and robotics locomotion questions: 1. Which gaits are most efficient at which velocities, and when should gait transitions occur? 2. How should parameters like body height, posture, and swing foot trajectories change for different gaits at different velocities? 3. Can we produce novel gaits not seen during training, and how robust is the policy to leg failures? In order to answer these questions, we present a hierarchical bio-inspired architecture consisting of a policy trained with DRL (higher centers), a network of coupled oscillators mapped to task space foot trajectories (rhythm generator and pattern formation layers of the spinal cord), and sensory feedback from onboard sensors and internal CPG states (efference copy of the spinal cord). We explicitly enforce a gait through the coupling matrix, and the locomotion style through the pattern formation parameters (i.e. body height, swing foot ground clearance, foot offsets). We leverage this architecture to produce all quadrupedal gaits, determine when the optimal transitions between gaits should occur, and with which locomotion style. Additionally, we are able to realize novel gaits that were not seen during training, and have not been previously shown, without any modifications directly in hardware experiments. Furthermore, our framework is robust to failures of either one or two disabled legs. The rest of this paper is organized as follows. In Section II we present AllGaits, including our design choices and integration of Central Pattern Generators into the deep reinforcement learning framework to learn to locomote with all quadruped gaits. In Section III we discuss results and analysis from learning our controller and sim-to-real transfers, and we give a brief conclusion in Section IV."
https://arxiv.org/html/2411.04776v1,TacEx: GelSight Tactile Simulation in Isaac Sim ‚Äì Combining Soft-Body and Visuotactile Simulators,"Training robot policies in simulation is becoming increasingly popular; nevertheless, a precise, reliable, and easy-to-use tactile simulator for contact-rich manipulation tasks is still missing. To close this gap, we develop TacEx ‚Äì a modular tactile simulation framework. We embed a state-of-the-art soft-body simulator for contacts named GIPC and vision-based tactile simulators Taxim and FOTS into Isaac Sim to achieve robust and plausible simulation of the visuotactile sensor GelSight Mini. We implement several Isaac Lab environments for Reinforcement Learning (RL) leveraging our TacEx simulation, including object pushing, lifting, and pole balancing. We validate that the simulation is stable and that the high-dimensional observations, such as the gel deformation and the RGB images from the GelSight camera, can be used for training. The code, videos, and additional results will be released online https://sites.google.com/view/tacex.","Tactile sensing plays an important role for human perception of touch [1] and for advanced manipulation tasks in robotics [2, 3, 4]. Contact properties such as contact geometry, object stiffness, and surface texture can be estimated using tactile sensors [5]. Furthremore, slip detection [6], hardness estimation [7], and grasping of soft objects [8] are facilitated by the sense of touch. However, finger coordination based on tactile feedback is a complex control problem with high-dimensional observation space, therefore several Deep RL approaches have been explored [9, 10]. A crucial bottleneck for applying RL to tactile-rich manipulation tasks is the lack of stable and reliable contact simulation that includes soft-body interaction and tactile sensing. Although a number of simulators have appeared recently that aim to remedy this issue [10, 11, 12, 13, 14], each simulator uses a different physics engine, simulates a different tactile sensor, different robot, and runs in a different robotics simulator altogether ‚Äì making comparison and interoperability challenging. To address these issues, we develop TacEx ‚Äì a novel tactile simulation framework embedded in NVIDIA‚Äôs Isaac Sim [15] and Isaac Lab [16, 17] that is modular, extensible, and based on the latest advancements in tactile simulation. We additionally integrate GIPC [18] for GPU-accelerated and inversion-free simulation of soft-body contacts. By leveraging Isaac Sim, we gain access to powerful features, such as photorealistic rendering, ROS support, and GPU-accelerated physics simulation, and by integrating TacEx into Isaac Lab ‚Äì an extensible RL framework built on top of Isaac Sim ‚Äì we enable support for teleoperation, GPU-parallelized training, and various RL libraries. Related Work A simulation of a GelSight tactile sensor generally requires three components: physics simulation (to capture contact properties), optical simulation (to generate perceived RGB images), and marker simulation (to generate marker motion field, which reflects gel deformation). In this paper, we chiefly focus on the physics simulation, leveraging existing GelSight simulators for the other components: Taxim [11] for optical simulation and FOTS [19] for marker motion field simulation. For physics simulation, PyBullet‚Äôs rigid body dynamics has been used in TACTO [20] and in [21], whereas [14] used a penalty-based contact model to approximate the soft gelpad deformation with rigid body dynamics. Though fast, these methods are less accurate compared to Finite Element Methods (FEM). More recent approaches rely on FEM: TacIPC [22] and [10] use the incremental potential contact (IPC) [23] model to simulate the gelpad deformation in an FEM-like manner. DiffTactile [13] also simulates the gelpad deformation with an FEM-based approach. Furthermore, the FEM simulation of Isaac Gym‚Äôs Flex engine has also been used for accurate tactile simulation [24, 25], but it is slow and unsuited for RL. Our approach is closest to the concurrent work TacSL [26] which also incorporates visuotactile simulation into Isaac Sim, however in contrast to TacSL, we leverage GIPC [18] for FEM-based soft-body simulation (instead of a simplified soft contact model). GIPC additionally allows for soft-to-soft contact simulation. Importantly, our method is modular, enabling the user to select which simulations should be enabled depending on the task requirements."
https://arxiv.org/html/2411.04735v1,Learning from Demonstration with Hierarchical Policy Abstractions Toward High-Performance and Courteous Autonomous Racing,"Fully autonomous racing demands not only high-speed driving but also fair and courteous maneuvers. In this paper, we propose an autonomous racing framework that learns complex racing behaviors from expert demonstrations using hierarchical policy abstractions. At the trajectory level, our policy model predicts a dense distribution map indicating the likelihood of trajectories learned from offline demonstrations. The maximum likelihood trajectory is then passed to the control-level policy, which generates control inputs in a residual fashion, considering vehicle dynamics at the limits of performance. We evaluate our framework in a high-fidelity racing simulator and compare it against competing baselines in challenging multi-agent adversarial scenarios. Quantitative and qualitative results show that our trajectory planning policy significantly outperforms the baselines, and the residual control policy improves lap time and tracking accuracy. Moreover, challenging closed-loop experiments with ten opponents show that our framework can overtake other vehicles by understanding nuanced interactions, effectively balancing performance and courtesy like professional drivers.","Autonomous racing has recently gained attention as a way to push the boundaries of autonomous vehicle technology, serving as a testbed to showcase system capabilities under extreme conditions, such as high-speed navigation, low-latency computation, and real-time operation. Competitions like Roborace [1] and the Indy Autonomous Challenge (IAC) [2] have led the way in testing high-speed autonomy. The IAC, the world‚Äôs first 1:1 overtaking competition held at Las Vegas Motor Speedway (LVMS), determined the winner based on the vehicle‚Äôs ability to overtake a ‚Äôdefending‚Äô vehicle at higher speeds. Teams, including the authors as part of team KAIST, successfully demonstrated high-speed passing at over 200 km/h [3, 4, 5]. Despite successful demonstrations of high-speed autonomous driving, challenges remain in achieving professional human-level racing standards. Racing scenarios typically require rules for fairness and safety. For example, the IAC imposes specific rules on ‚Äôdefender and offender roles,‚Äô [3] including target driving speeds, limited overtaking zones, and designated paths for defenders. Similarly, the Indy 500 and Formula 1 (Fig. 1) enforce rules to maintain fairness and safety, albeit with fewer restrictions. In all cases, drivers‚Äîhuman or autonomous‚Äîmust proactively interact with others, balancing agility and courtesy in competitive, adversarial settings. In this study, we propose an offline learning-based autonomous racing framework using hierarchical policy abstractions. The proposed framework consists of two levels of policy abstraction: 1) a novel trajectory planning policy (TPP) and 2) a residual control policy (RCP), both trained using a learning from demonstration method (Fig. 2). The TPP predicts an optimal future trajectory using a density distribution map that considers environmental and interaction contexts with surrounding opponents in adversarial driving scenarios. Subsequently, based on the inferred trajectory, the RCP refines the control inputs from the forward controller by adding residual control adjustments. This modular policy architecture leverages the learning from demonstration paradigm to abstract expert-driving policies at hierarchical levels, effectively learning complex policies that balance performance and courtesy. We extensively validate our proposed method using a high-fidelity racing simulator in multi-agent, highly competitive scenarios. Our open-loop results show that our trajectory-level policy outperforms baselines both quantitatively and qualitatively. Furthermore, in closed-loop experiments, our approach demonstrates strategic overtaking while considering safety and courtesy in response to nuanced interactions with surrounding opponents. These experiments highlight the effectiveness of our autonomous driving framework in balancing agility and fairness during high-speed racing. Figure 1: Overtaking scenarios at the Indy Autonomous Challenge (left) and Formula 1 motorsport (right) The main contributions of our work are as follows: ‚Ä¢ We propose an autonomous racing framework using hierarchical policy abstractions that are trained through the learning from demonstration paradigm. ‚Ä¢ We design a density estimator-based trajectory planning policy and a residual control policy, both of which reason about their outputs at different levels of abstraction. ‚Ä¢ We extensively evaluate our approach in a high-fidelity racing simulator with challenging multi-agent adversarial scenarios, demonstrating well-balanced performance in agility, safety, and courtesy during high-speed racing."
https://arxiv.org/html/2411.04700v1,Field Assessment of Force Torque Sensors for Planetary Rover Navigation,"Proprioceptive sensors on planetary rovers serve for state estimation and for understanding terrain and locomotion performance. While inertial measurement units (IMUs) are widely used to this effect, force-torque sensors are less explored for planetary navigation despite their potential to directly measure interaction forces and provide insights into traction performance. This paper presents an evaluation of the performance and use cases of force-torque sensors based on data collected from a six-wheeled rover during tests over varying terrains, speeds, and slopes. We discuss challenges, such as sensor signal reliability and terrain response accuracy, and identify opportunities regarding the use of these sensors. The data is openly accessible and includes force-torque measurements from each of the six-wheel assemblies as well as IMU data from within the rover chassis. This paper aims to inform the design of future studies and rover upgrades, particularly in sensor integration and control algorithms, to improve navigation capabilities.","Planetary rovers are mobile laboratories, equipped with scientific instruments to explore remote locations on distant celestial bodies. Navigation is the critical function that distinguishes them from stationary landers allowing them to investigate multiple targets across versatile terrains. However, this comes at the expense of decreased payload mass, a lower energy capacity, and increased risk due to the traversal itself. In the future, rovers equipped only with engineering sensors but without their own scientific instruments are a possibility. The \acSFR, formerly part of the \acMSR campaign, was an example for such a model. The rover‚Äôs purpose was to quickly navigate the Martian surface, locate sample tube caches left behind by the Mars 2020 rover, and return them to the Mars Ascent vehicle. Rovers traverse unstructured terrains and can be neither repaired, nor moved or otherwise reset into an operable state in case of fatal hardware failures. Hence, they rely on conservative planning by operators on Ground as well as their sensor suite and on-board capabilities in more autonomous modes. The sensors on which the operators as well as the rover base their planning can be categorized as either (1) exteroceptive sensors, such as optical (stereo) cameras or time-of-flight cameras, or (2) proprioceptive sensors such as \acpIMU, fiber optic gyroscopes, or motor current. \acFTS have already reached a high \acTRL with example use cases in robotic manipulators, in terrestrial as well as space applications [1], and legged robots. However, they are not commonly found on the locomotion systems of planetary, wheeled, rovers. This warrants closer investigation as there may be a signification opportunity for innovation. In this paper, we explore the possibilities offered by the inclusion of \acpFTS in mobile, wheeled robot bases in planetary exploration. Our research aims to investigate how \acpFTS can enhance rover navigation in planetary scenarios. In July of 2023, we conducted field trials using \acMaRTA (Figure 1), one of the European Space Agency‚Äôs rover testbeds [2], which is equipped with six ATI mini45 \acpFTS, in the Bardenas Reales semi-desert, an analogue site to Martian and Lunar exploration [3]. We evaluate the applications of the \acpFTS based on these experiments, focusing on navigation and traversability-related tasks such as terrain classification and applicability for drawbar pull estimation with real data from unstructured terrain. Figure 1: ESA‚Äôs rover testbed \acsMaRTA during the collection of the analogue planetary exploration dataset BASEPROD in Bardenas Reales, Spain [3]. The \acpFTS can be seen in each of the six legs with a close-up highlighting the sensor position relative to the wheel axle. In \acMaRTA, the \acpFTS are mounted above the wheels rather than at the wheel hubs. This is similar to the mounting position used by the 32 \acDoF robot RoboSimian [4, 5] which is primarily exploiting the vertical, gravity-aligned forces. This mounting choice and its trade-offs will be discussed in detail. Through our field experiments with the \acMaRTA rover and the analyses in this paper, we offer an insight into practical challenges and potential applications of \acpFTS in planetary rover systems. Our findings provide guidance and pointers for researchers in determining the value of incorporating \acpFTS in their mobile robot bases, particularly in unstructured terrains. The main value of this paper consists of sharing insights into the configuration of \acMaRTA, with both its benefits and limitations, to inform interested readers which aspects of this configuration are or are not worth incorporating into their designs and where we see the need for further refinement."
https://arxiv.org/html/2411.04678v1,Socially-Aware Opinion-Based Navigation with Oval Limit Cycles,"When humans move in a shared space, they choose navigation strategies that preserve their mutual safety. At the same time, each human seeks to minimise the number of modifications to her/his path. In order to achieve this result, humans use unwritten rules and reach a consensus on their decisions about the motion direction by exchanging non-verbal messages. They then implement their choice in a mutually acceptable way. Socially-aware navigation denotes a research effort aimed at replicating this logic inside robots. Existing results focus either on how robots can participate in negotiations with humans, or on how they can move in a socially acceptable way. We propose a holistic approach in which the two aspects are jointly considered. Specifically, we show that by combining opinion dynamics (to reach a consensus) with vortex fields (to generate socially acceptable trajectories), the result outperforms the application of the two techniques in isolation.","I INTRODUCTION An increasing number of applications requires mobile robots to move through human-populated areas. Significant examples include automated guided vehicles (AGVs) used for intra-factory logistics and warehouse management, mobile robots used for shipping and delivery in urban environments, and assistive robots used to support the mobility of disabled and elderly users. The nature of these applications requires the robot to accomplish its task reliably and efficiently. At the same time, the safety of human bystanders must not be imperilled. Last but not least, the trajectories followed by the robot should be perceived as smooth and human-friendly as those followed by its human counterparts (this is even truer when the robot is carrying or guiding a human). To generate this kind of trajectory on a robot, we need to take inspiration from what humans do. When humans move in a shared and crowded space, they exchange a short but intense stream of non-verbal signals to decide which direction each of them should take. Once each human makes up her/his mind on the direction to follow, s/he implements the choice by modifying the trajectory that s/he will follow in the next few seconds. This modification minimises the energy spent and the strain on the junctures [1], but at the same time it respects the private space of the other humans [2]. This complex set of communication protocols, social rules and reactive adjustments to our trajectories operates in strict coordination. They are so deeply embedded in our social behaviour that their use hardly touches the conscious level. Socially-aware navigation, or simply social navigation, is a research area that seeks to approximate the logic of human behaviour summarised above on a robot. Related work. An intense research activity on social navigation in the past few years has delivered different strategies aimed at ensuring a safe and efficient interaction between robots and humans [3, 4]. Research in this area includes various approaches designed to address the complexities of navigating in dynamic environments shared with humans, with both proactive and reactive approaches being explored [5, 6]. In short, proactive strategies attempt to predict and adapt to human behaviour, while reactive strategies respond to real-time environmental changes. Reactive methods are essential when people in a crowd move depending on the surrounding situation, requiring rapid re-planning of the robot‚Äôs trajectory. However, as the number of people increases, accurate and fast path planning becomes more difficult, increasing the computational time required to find the optimal path. More computationally efficient reactive methods are artificial potential fields [7, 8], the velocity obstacle (VO) [9], and the social force model (SFM) [10, 11]. SFM describes pedestrian movement as influenced by social forces that guide acceleration towards desired velocities while maintaining interpersonal distances. Although effective for large groups, SFM struggles with individual-level interactions in open environments. Enhanced models, such as the Headed Social Force Model (HSFM) [12], incorporate pedestrian heading to better predict non-holonomic motion. Proactive strategies, on the other hand, account for mutual interactions in crowded situations, enabling robots to cooperate with humans. These approaches involve predicting human behavior and proactively planning a collision-free path. Examples include proactive models based on SFM [13], opinion dynamics [14], and dynamic path planning techniques such as the Morphing algorithm [15]. Sampling-based motion planning methods, such as Rapidly-exploring Random Tree (RRT) [16, 17] and Risk-RRT [18], are also commonly used for dynamic environments. However, these methods can struggle with the complexities of interactive behaviors in human-populated spaces. Finally, learning-based approaches, including reinforcement learning (RL) [19], deep RL (DRL) [20], and inverse RL (IRL) [21, 22], are increasingly being explored for crowd navigation. These methods are promising but often lack analytical tractability and can be computationally demanding in real-time scenarios. Paper Contribution. In the short report above, we have mentioned two important classes of work: proactive and reactive path planning. In the first class, the robot negotiates with humans the direction of motion that each should take, but the implementation of the decision is not guaranteed to be safe and does not meet the quality standards that the human requires. In the second, the decision is taken and the robot concentrates on implementing it through a human-friendly trajectory. The missing piece in this case is a mechanism that generates the ‚Äústrategic‚Äù decision. In this paper, we propose to merge the two phases into a unified holistic approach. We use social dynamics to implement the negotiation phase [14], and potential fields with oval-shaped limit cycles to generate a socially-aware trajectory [8]. The opinion dynamics module generates a variable that encodes the decision to be made by the robot (‚Äòturn right‚Äô, ‚Äòturn left‚Äô or ‚Äògo straight‚Äô) and consequently this variable modifies the geometric parameters of the limit cycle. As shown in the paper, the combined use of the two techniques fills the conceptual gaps that we have identified for each of them, leading to a significant improvement in the resulting behaviour of the robot."
https://arxiv.org/html/2411.04653v1,IGDrivSim: A Benchmark for the Imitation Gap in Autonomous Driving,"Developing autonomous vehicles that can navigate complex environments with human-level safety and efficiency is a central goal in self-driving research. A common approach to achieving this is imitation learning, where agents are trained to mimic human expert demonstrations collected from real-world driving scenarios. However, discrepancies between human perception and the self-driving car‚Äôs sensors can introduce an imitation gap, leading to imitation learning failures. In this work, we introduce IGDrivSim, a benchmark built on top of the Waymax simulator, designed to investigate the effects of the imitation gap in learning autonomous driving policy from human expert demonstrations. Our experiments show that this perception gap between human experts and self-driving agents can hinder the learning of safe and effective driving behaviors. We further show that combining imitation with reinforcement learning, using a simple penalty reward for prohibited behaviors, effectively mitigates these failures. Our code is open-sourced at: https://github.com/clemgris/IGDrivSim.git.","Research in autonomous driving aims to develop vehicles capable of navigating complex environments with the safety and efficiency of human drivers [1, 2]. Learning such autonomous driving policies has been a focus of the research community due to its potential to reduce transportation costs, alleviate labor constraints, improve road safety, and enhance overall mobility [3]. However, achieving this level of performance remains a critical challenge in the field. In fact, while reinforcement learning (RL, [4]) has been a natural solution for control in various domains such as robotics and game playing [5], self-driving agents often struggle to learn desired behaviors using RL due to the high computational cost and, more critically, the difficulty of designing an effective reward function. In these scenarios, imitation learning from human driver demonstrations (IL, [6, 7]) is a popular alternative for training policies. IL allows self-driving agents to learn directly from human expert behaviors, bypassing the need for an explicit reward function and reducing computational costs. In the literature, expert datasets often consist of traffic scenarios where multiple road users are tracked in 3D space using sensors from a human-driven car [8, 9, 10]. These data are relatively simple to record in real-world environments, and training typically involves simulators that replicate these recorded driving scenarios, with the task being to predict the trajectories of each agent throughout the scene. However, a major challenge of IL arises when the imitator and the expert have different observability of the environment, which can occur, for instance, when they rely on different sensors. This difference can lead to an imitation gap [11, 12, 13, 14] and potentially cause IL algorithms to fail. Imagine training an autonomous vehicle to navigate in a foggy area by imitating data from another vehicle that was recorded in clear weather. The expert vehicle, operating in clear conditions, can see far ahead and navigate smoothly, anticipating obstacles and changes in the environment well in advance. However, when the imitator behaves in an area with dense fog, it cannot see as far ahead and must rely solely on its limited visibility to navigate. If the imitator simply copies the expert‚Äôs actions, driving at the same speed and making the same turns, it will likely fail because it does not have the same information about the road ahead. Instead, a better policy for the imitator would involve slowing down and adopting a less smooth but more reactive behavior to avoid obstacles. This cautious and adaptive behavior allows the imitator to compensate for its reduced visibility and safely navigate through foggy conditions. However, this behavior is never demonstrated by the expert vehicle and therefore not learned by the imitator. Figure 1: Screenshots of two scenarios in IGDrivSim illustrating the introduction of partial observability. The controlled vehicle, shown in blue, navigates among road lines, street lights, and other vehicles. On the left, an agent with a circular field of view is depicted, with its visibility limited by blue boundaries, while gray-shaded regions represent occluded information. On the right, an agent experiences noisy position perception of its position within the scene, visualized by the magenta-shaded area. In this example, the difference in observability between the expert and the imitator would cause traditional IL algorithms to fail. Critically, the common practice of training autonomous driving policies using simulators based on human driver trajectories naturally introduces a similar difference in observability between the human expert and the self-driving car (SDC), which could likewise result in the failure of IL. In fact, when performing IL, the SDC has access to all the information recorded during data collection, typically involving sensors like LiDAR and cameras. Yet, these sensors differ in their capabilities from human perception, on which the human driver relies to generate expert demonstrations. For example, a human driver might react based on audio cues like honks that the SDC might not detect if it lacks audio sensors. Conversely, human drivers might not see certain information due to visual blind spots that the SDC would detect with its broader field of view. These differences in the perception of the driving environment between the human expert and the SDC imitator create an imitation gap. Imitation learning from the human expert demonstrations may lead to the SDC imitator not learning the specific behaviors required for its particular sensory capabilities (i.e., the sensors used to collect scene information). In the best case, the learned policy is safe but suboptimal under the SDC‚Äôs observability; in the worst case, it learns unsafe behaviors. While the imitation gap has been studied in previous work [12, 13, 14], to our knowledge, there exists no dedicated benchmark for evaluating how this difference in observability affects the learning of driving policies from human demonstrations. Therefore, we introduce IGDrivSim (Imitation Gap in Driving Simulation), a benchmark specifically designed to explore the impact of the imitation gap on learning driving policies from human demonstrations. IGDrivSim introduces constraints (see Figure 1) on the observability of the Waymax driving simulator [15], which is based on the WOMD dataset [10] of real-world human driving data. These constraints amplify the difference between the human expert‚Äôs and the simulated self-driving car‚Äôs observability, effectively widening the imitation gap. Our experiments reveal that when trained with IL, especially behavioral cloning, a self-driving agent ‚Äîwhose observations of the driving scene inherently differ from human perception‚Äî struggles to learn good behaviors from human expert demonstrations alone and that this failure can be explained by the presence of an imitation gap. Furthermore, we demonstrate that incorporating knowledge about prohibited behaviors can help address this issue. In fact, we find that by combining IL with RL based on a simple penalty reward, we can mitigate these failures. While [16] demonstrated that reward signals can enhance IL, the underlying reasons were not fully explored. In this work, we present a novel setup and results that, for the first time, show how an imitation gap can explain the need for supplemental methods like RL in training self-driving policies with IL. By introducing IGDrivSim as a benchmark, we aim to provide a tool for evaluating and addressing the imitation gap in autonomous driving, facilitating both the assessment and improvement of IL from human driving demonstrations. In summary, the contributions of our work are: 1. We release an open-source benchmark, called IGDrivSim, for studying the imitation gap in a high-dimensional, real-world complex task where IL is the current state-of-the-art method for training policies. 2. We underscore the importance of addressing the imitation gap when training self-driving car policies on human driver data, revealing key limitations of demonstration-based IL. 3. We release the first (to our knowledge) open-source motion prediction baselines, which are based on a simple RNN architecture, trained on the Jax-based simulator Waymax."
https://arxiv.org/html/2411.04549v1,Vision Language Models are In-Context Value Learners,"Predicting temporal progress from visual trajectories is important for intelligent robots that can learn, adapt, and improve. However, learning such progress estimator, or temporal value function, across different tasks and domains requires both a large amount of diverse data and methods which can scale and generalize. To address these challenges, we present Generative Value Learning (GVL), a universal value function estimator that leverages the world knowledge embedded in vision-language models (VLMs) to predict task progress. Naively asking a VLM to predict values for a video sequence performs poorly due to the strong temporal correlation between successive frames. Instead, GVL poses value estimation as a temporal ordering problem over shuffled video frames; this seemingly more challenging task encourages VLMs to more fully exploit their underlying semantic and temporal grounding capabilities to differentiate frames based on their perceived task progress, consequently producing significantly better value predictions. Without any robot or task specific training, GVL can in-context zero-shot and few-shot predict effective values for more than 300 distinct real-world tasks across diverse robot platforms, including challenging bimanual manipulation tasks. Furthermore, we demonstrate that GVL permits flexible multi-modal in-context learning via examples from heterogeneous tasks and embodiments, such as human videos. The generality of GVL enables various downstream applications pertinent to visuomotor policy learning, including dataset filtering, success detection, and advantage-weighted regression ‚Äì all without any model training or finetuning.","Predicting temporal progress from visual trajectories is an important task for embodied agents that interact with the physical world. A robot capable of generalizable progress estimation can in principle discern desirable and undesirable behaviors to learn visuomotor skills in new environments. This is most often studied in reinforcement learning literature [51], where progress estimation is equivalent to universal value learning under specific choices of reward function. However, universal value estimation comes with a number of key challenges: (1) broad generalization to new tasks and scenes, (2) the ability to accurately estimate state in partially observed environments, and (3) temporal consistency (i.e. satisfying the Bellman equation) over long horizons. Most existing methods trained on relatively small amounts of vision-only data [8, 40, 1] lack the semantic, spatial, and temporal understanding needed to ground task progress in the space-time manifold of video, preventing generalization. Moreover, they often reason over single frames, inducing a high-degree of uncertainty in partially observed environments which in turn can effect the consistency of predictions for poorly estimated states. However, these challenges are not insurmountable: modern vision language models (VLMs) exhibit marked generalization and reasoning capabilities, potentially making them useful for value estimation. Figure 1: Result highlights. GVL can effectively zero-shot and few-shot predict task progress on diverse and challenging real-world tasks; these capabilities enable expansive set of downstream applications, including dataset filtering, success detection, and policy learning. Though not often considered as candidates for value estimation, VLMs excel at its aforementioned core challenges. First, state-of-the-art VLMs have exhibited strong spatial reasoning and temporal understanding capabilities across various vision tasks [44, 9, 25, 21], allowing them to generalize to novel scenarios. Second, large transformer-based VLMs have the requisite context window [22] to reason over large amounts of historical information to accurately estimate state from observation sequences when predicting task progress. Finally, VLMs make predictions auto-regressively, meaning they commit to their own outputs as inputs for subsequent predictions, imposing consistency constraints on long generations. For example, a VLM is unlikely to estimate that a task is 50% completed if it already has a 50% completion prediction in context. However, how exactly a VLM should be used to predict values is unclear. Empirically, we find that simply placing a video in-context and prompting the model to return progress predictions for each frame fails ‚Äì our analysis suggests strong temporal correlations between successive frames often cause VLMs to produce uninformative monotonic values that disregard the actual quality of the trajectory and differences between frames (Section 4) ‚Äì and a different approach is needed. Figure 2: Method overview. Generative Value Learning (GVL) generates values by auto-regressively predicting task completion percentage over shuffled frames, enabling impressive in-context value learning. To effectively leverage the broad knowledge of VLMs, we introduce Generative Value Learning (GVL), a universal value estimation method enabled by long-context VLMs, which crucially operates over shuffled frames. At its core, GVL asks frozen state-of-the-art VLMs, such as Gemini-1.5-Pro [22], to auto-regressively predict the completion percentage of a task specified in natural language for a sequence of shuffled input video frames; see Fig. 2. Perhaps surprisingly we find that simply shuffling the frames of the input video effectively overcomes the strong implicit temporal bias found in video, enabling VLMs to generate meaningful values. While GVL is capable of generating values in a zero-shot manner, we find that the performance of GVL scales with examples via multi-modal in-context learning. Providing more examples of visual ‚Äúunshuffling‚Äù in context increases performance, irrespective of the target embodiment. For example, human videos can improve GVL‚Äôs performance on predicting robot task progress. To facilitate large-scale value prediction evaluation, we additionally introduce a new evaluation metric, Value-Order Correlation (VOC), measuring how well predicted values correlate with the ground-truth timestep order in expert videos; as we will show, VOC is also a useful metric for measuring dataset and trajectory quality, which allows GVL to be used for applications beyond value-based policy learning such as data quality estimation and success detection. We first evaluate GVL‚Äôs value prediction quality with VOC on a large suite of real-world robotics datasets, spanning 51 datasets, 20 embodiments, and more than 300 tasks. This includes 50 datasets from Open-X (OXE) dataset [45] in addition to our own bimanual manipulation dataset containing 250 challenging real-world tasks on an ALOHA platform [71], which are considerably longer horizon and more fine-grained than those in the OXE dataset. In aggregate, GVL exhibits strong zero-shot value prediction capabilities with highly positive VOC scores on most datasets; its performance further improves with various types of multi-modal in-context examples. Using GVL, we demonstrate scalable foundation model supervision for robot learning at various data abstraction levels. Specifically, GVL can help measure dataset quality in OXE. Second, it can be used for success detection, enabling imitation learning on mixed-quality datasets. Finally, the raw value estimates from GVL can be used for advantage-weighted regression for real-world offline reinforcement learning [47, 46]. In summary, our contributions are 1. Generative Value Learning (GVL), a universal value prediction framework via VLM in-context autoregressive value estimation on shuffled video frames. 2. An extensive evaluation on real-world datasets demonstrating GVL‚Äôs zero-shot scalability and multi-modal in-context learning capabilities. 3. Demonstration that GVL can be used in downstream applications including dataset quality estimation, success detection, and advantage-weighted regression for real-world control."
https://arxiv.org/html/2411.04546v1,Analytical Derivatives for Efficient Mechanical Simulations of Hybrid Soft Rigid Robots,"Algorithms that use derivatives of governing equations have accelerated rigid robot simulations and improved their accuracy, enabling the modeling of complex, real-world capabilities. However, extending these methods to soft and hybrid soft-rigid robots is significantly more challenging due to the complexities in modeling continuous deformations inherent in soft bodies. A considerable number of soft robots and the deformable links of hybrid robots can be effectively modeled as slender rods. The Geometric Variable Strain (GVS) model, which employs the screw theory and the strain parameterization of the Cosserat rod, extends the rod theory to model hybrid soft-rigid robots within the same mathematical framework. Using the Recursive Newton-Euler Algorithm, we developed the analytical derivatives of the governing equations of the GVS model. These derivatives facilitate the implicit integration of dynamics and provide the analytical Jacobian of the statics residue, ensuring fast and accurate computations. We applied these derivatives to the mechanical simulations of six common robotic systems: a soft cable-driven manipulator, a hybrid serial robot, a fin-ray finger, a hybrid parallel robot, a contact scenario, and an underwater hybrid mobile robot. Simulation results demonstrate substantial improvements in computational efficiency, with speed-ups of up to three orders of magnitude. We validate the model by comparing simulations done with and without analytical derivatives. Beyond static and dynamic simulations, the techniques discussed in this paper hold the potential to revolutionize the analysis, control, and optimization of hybrid robotic systems for real-world applications.","Rigid-body algorithms have been developed for the mechanical analysis of multi-body systems, enabling the modeling of dynamic response and static equilibrium of robots Featherstone (2008); Murray et al. (1994). Over the years, these algorithms have undergone remarkable improvements, enabling faster-than-real-time simulations Newbury et al. (2024). One of the most significant advancements has been the incorporation of gradient information into these algorithms Carpentier et al. (2019); Howell et al. (2023); Giftthaler et al. (2017a); Geilinger et al. (2020). The ability to accurately and efficiently compute the derivatives of governing equations with respect to the system‚Äôs state, model parameters, and control variables has been pivotal for implicit integration, design optimization, trajectory optimization, and optimal control. The impact of these advancements is exemplified by leading humanoid and quadrupedal robots, such as those developed by Boston Dynamics and Unitree. These robots leverage the gradient information for trajectory optimization and model-predictive control (MPC), enabling real-time control, enhanced stability, and adaptability in complex environments Sukhija et al. (2023); Neunert et al. (2018); Wensing et al. (2024). Several methods exist for computing the derivatives of governing equations, each with its trade-offs Newbury et al. (2024). The most straightforward approach is the numerical scheme of finite difference. The finite difference method can offer simplicity and ease of parallelization but often falls short in accuracy Todorov et al. (2012). Automatic differentiation (AutoDiff) computes derivatives by recognizing that even complex functions are built from fundamental operations and functions. It systematically applies the chain rule to these operations within the algorithm, allowing the program to automatically calculate derivatives alongside the original calculations Tedrake & the Drake Development Team (2019); Giftthaler et al. (2017b). However, AutoDiff involves intermediate computations that are generally hard to simplify. Analytical methods take a manual approach by directly applying the chain rule to recursive algorithms like the Recursive Newton-Euler Algorithm (RNEA) Carpentier & Mansard (2018); Singh et al. (2022). By exploiting the inherent structure and spatial algebra at the core of rigid-body dynamics algorithms, analytical derivatives (AD) can simplify computations and achieve greater computational efficiency than automatic differentiation methods. Efficient implementation of analytical derivatives can lead to faster and more resource-efficient computations with improved accuracy and provide deeper insight into the mathematical structure of the derivatives. However, deriving analytical derivatives manually can be complex and time-consuming, making them challenging to implement Singh et al. (2022). Deriving analytical derivatives in soft robots is significantly more challenging than in rigid body systems. The primary difficulty stems from the complex nature of deformable bodies, which undergo large, continuous deformations, making it highly challenging to derive closed-form equations for their dynamics. The general class of soft robots, which cannot be modeled as a system rods, require numerical methods based on 3D continuum mechanics such as Finite Element Methods (FEM) Duriez (2013) or Material Point Method (MPM) Hu et al. (2018). Analytical derivatives of FEM in robotics have been explored in several works, including Hoshyari et al. (2019); Hafner et al. (2019); Geilinger et al. (2020); B√§cher et al. (2021); Jatavallabhula et al. (2021); Du et al. (2022). On the other hand, MPM is often referred to as naturally differentiable due to its particle-grid representation and the smooth interpolation between particles and the grid, which makes gradient computation more efficient Spielberg et al. (2023); Huang et al. (2021). FEM and MPM use maximal coordinate representations for rigid bodies, increasing the degrees of freedom (DoF) and the computational cost for hybrid soft-rigid robots. A large portion of soft robots and the compliant links of hybrid soft rigid robots can be effectively modeled as slender, rod-like structures, making them well-suited for analysis using the Cosserat rod theory Armanini et al. (2023). The Cosserat rod is a 1D continuum mechanics object that can model deformations of slender bodies, including twisting, bending in two axes, stretching, and shearing in two axes Cao & Tucker (2008). Four distinct research communities have leveraged Cosserat rod theory to address their specific challenges, each producing specialized numerical methods tailored to their needs. Ranking them by their order of appearance, these communities are: the geometrically exact FEM community, the ocean engineering community, the computer graphics community, and the robotics community. The geometrically exact FEM community has focused primarily on developing FEM software that can predict the movements and stresses of mechanisms undergoing large deformations Simo & Vu-Quoc (1988); Meier et al. (2017); Eugster & Harsch (2023). Meanwhile, the ocean engineering community has applied Cosserat rod models to the simulation of towed submarine cables, addressing the unique challenges posed by underwater environments Burgess (1992); Tjavaras et al. (1998). On the other hand, the computer graphics community has prioritized computational speed for interactive simulations of filament-like structures, such as hair, using the Discrete Elastic Rod (DER) approaches Bergou et al. (2008); Gazzola et al. (2018). Finally, the robotics community has concentrated on the simulation and control of soft or continuum robots to safely interact with their surroundings Rucker & Webster (2011); Till et al. (2019); Boyer et al. (2022). To cater specifically to the needs of robotics, a novel parameterization of the configuration space of Cosserat rods has been proposed, focusing on strain fields rather than the traditional pose-based approach used in FEM and DER. This strain-based approach, referred to as the Geometric Variable Strain (GVS) method Renda et al. (2020); Boyer et al. (2020), is geometrically exact and aligns well with the demands of soft robotic applications, providing a more efficient framework for modeling their dynamic and compliant behavior. Among the various models based on the Cosserat rod, the GVS model stands out for its ability to merge the screw theory-based formulation of rigid robots with the strain parameterization of the rod. Its Lagrangian mechanics formulation with minimal generalized coordinates enables efficient analysis of hybrid soft-rigid robotic systems within a unified mathematical framework. The implementation of the GVS model, based on the approximate Magnus expansion of the rod‚Äôs strain field, makes the soft body computationally equivalent to multidimensional, discrete rigid joints Mathew et al. (2024). The model has been extensively compared and validated with analytical models, FEM, and other rod models in previous studies Boyer et al. (2023); Mathew et al. (2022a). In Mathew et al. (2022a, 2024), we implemented the GVS model for hybrid soft robots using a built-in implicit integration scheme in MATLAB called o‚Å¢d‚Å¢e‚Å¢15‚Å¢sùëúùëëùëí15ùë†ode15sitalic_o italic_d italic_e 15 italic_s. When analytical derivatives (Jacobian) are not supplied, o‚Å¢d‚Å¢e‚Å¢15‚Å¢sùëúùëëùëí15ùë†ode15sitalic_o italic_d italic_e 15 italic_s internally compute the Jacobian of the governing equations using a finite difference scheme. However, this can lead to longer computational times or cause the solver to stall due to errors introduced by the numerical approximation, particularly in high-DoF systems with constraints. The objective of this work is to develop and implement an analytical derivative for the GVS model, aiming to improve computational efficiency and robustness in the simulation of hybrid soft rigid robots. Related works: The Piecewise Constant Strain (PCS) model, which discretizes the continuous deformation of a Cosserat rod into a finite number of segments with constant strain, is a subclass of the GVS model. Based on the Comprehensive Motion Transformation Matrix (a Lie group of coordinate transformations of displacement, velocity, and acceleration), an analytical gradient of the PCS model was derived in Ishigaki et al. (2024). A differentiable soft robot simulation environment called DisMech was introduced based on the implicit DER method Choi et al. (2024). DisMech employs a finite difference scheme to compute the necessary gradients for the implicit integration. Recently, a new algorithm for the implicit dynamics of robots with rigid bodies and Cosserat rods has been proposed Boyer et al. (2023). By applying an exact symbolic differentiation of the robot‚Äôs RNEA inverse dynamics (named I‚Å¢D‚Å¢Mùêºùê∑ùëÄIDMitalic_I italic_D italic_M in Boyer et al. (2023)), a new RNEAlgorithm, called the tangent inverse dynamics model (T‚Å¢I‚Å¢D‚Å¢Mùëáùêºùê∑ùëÄTIDMitalic_T italic_I italic_D italic_M), has been derived. This algorithm is then fed with unit inputs to numerically calculate the Jacobian of the inverse dynamics. This calculation is performed at the continuous level, i.e. directly on the partial differential equations (PDEs), and before spatial integration, for which spectral methods have been employed instead of Magnus expansion. Although the approach adheres to the true definition of geometrically exact methods - where discretization of time and space occurs only after all analytical calculations - due to its implicit nature, it does not directly provide Jacobian matrices in analytical matrix form, which can be advantageous for control and fast simulation of robots with MATLAB. In contrast, the approach here presented lies in its analytic and explicit formulation, which was made possible by the Magnus expansion. Contributions of this work: By leveraging the ‚Äúpseudo-rigid joint‚Äù formulation of GVS and building upon established methods for analytical derivatives in rigid body systems Carpentier & Mansard (2018); Singh et al. (2022), we developed analytical derivatives for soft and hybrid soft-rigid robots with slender soft bodies. We implemented the derivatives in two implicit integration algorithms: o‚Å¢d‚Å¢e‚Å¢15‚Å¢sùëúùëëùëí15ùë†ode15sitalic_o italic_d italic_e 15 italic_s of MATLAB and Newmark-Œ≤ùõΩ\betaitalic_Œ≤ scheme for dynamics and provided the analytical Jacobian for efficient static equilibrium computation. Our method significantly improved the computational speed of implicit integration for dynamic simulations, achieving speed-ups of up to three orders of magnitude compared to traditional methods without analytical derivatives. Similarly, for static analysis, we observed substantial improvements in computational efficiency. Six common robotic systems are considered for the simulation study. To the best of the authors‚Äô knowledge, this is the first work to derive and implement analytical derivatives for the mechanical analysis of hybrid soft-rigid robotic systems of this kind. Organization of the paper: A summary of the GVS model is presented in Section 2. Section 3 details the implementation of analytical derivatives in dynamic and static algorithms for fast and accurate computations. In Section 4, we focus initially on a single soft body, applying the RNEA and I‚Å¢Dùêºùê∑IDitalic_I italic_D framework to derive the analytical derivatives of the governing equations. This framework is extended to hybrid multi-body systems in Section 5. We address two typical constraints in multi-body systems: joint actuation via joint coordinates and closed-chain (CC) systems. Section 6 discusses the derivation of analytical derivatives for systems subjected to three common external forces: point forces, contact loads, and hydrodynamic forces. Simulations and validations across six robotic systems are presented across these sections, demonstrating the effectiveness of the approach. Table 1 lists all the important symbols used in this paper. Readers are encouraged to refer to the supplementary videos to visualize the dynamic simulation results presented in this work. Table 1: List of symbols and their descriptions Symbol Description nd‚Å¢o‚Å¢fsubscriptùëõùëëùëúùëìn_{dof}italic_n start_POSTSUBSCRIPT italic_d italic_o italic_f end_POSTSUBSCRIPT Total degrees of freedom nasubscriptùëõùëén_{a}italic_n start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT Number of actuators NùëÅNitalic_N Number of Cosserat rods npsubscriptùëõùëùn_{p}italic_n start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT Number of computational points ùííùíí\bm{q}bold_italic_q Generalized coordinates ùííÀôÀôùíí\dot{\bm{q}}overÀô start_ARG bold_italic_q end_ARG Generalized velocities ùíí¬®¬®ùíí\ddot{\bm{q}}over¬® start_ARG bold_italic_q end_ARG Generalized accelerations ùíôùíô\bm{x}bold_italic_x Generalized robot state. ùíô=[ùííT‚Å¢ùííÀôT]Tùíôsuperscriptdelimited-[]superscriptùííùëásuperscriptÀôùííùëáùëá\bm{x}=[\bm{q}^{T}\;\dot{\bm{q}}^{T}]^{T}bold_italic_x = [ bold_italic_q start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT overÀô start_ARG bold_italic_q end_ARG start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT h‚Ñéhitalic_h Time step tùë°titalic_t Time XùëãXitalic_X Curvilinear abscissa of the soft body, X‚àà[0,L]ùëã0ùêøX\in[0,\;L]italic_X ‚àà [ 0 , italic_L ] F‚Å¢Dùêπùê∑FDitalic_F italic_D Forward dynamics I‚Å¢Dùêºùê∑IDitalic_I italic_D Inverse dynamics ùë¥ùë¥\bm{M}bold_italic_M Generalized mass matrix ùë≠ùë≠\bm{F}bold_italic_F Generalized external and Coriolis force ùùâùùâ\bm{\tau}bold_italic_œÑ Generalized internal force ùë©ùë©\bm{B}bold_italic_B Generalized actuation matrix ùíñùíñ\bm{u}bold_italic_u Actuator strength ùë≤ùë≤\bm{K}bold_italic_K Generalized stiffness matrix ùë´ùë´\bm{D}bold_italic_D Generalized damping matrix ùùÉùùÉ\bm{\xi}bold_italic_Œæ Screw strain ùöΩŒæsubscriptùöΩùúâ\bm{\Phi}_{\xi}bold_Œ¶ start_POSTSUBSCRIPT italic_Œæ end_POSTSUBSCRIPT Strain basis (‚àô)Œ±subscript‚àôùõº(\bullet)_{\alpha}( ‚àô ) start_POSTSUBSCRIPT italic_Œ± end_POSTSUBSCRIPT Quantity at joint or computational point Œ±ùõº\alphaitalic_Œ± (‚àô)Bsuperscript‚àôùêµ(\bullet)^{B}( ‚àô ) start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT Quantities computed during the forward pass (‚àô)C,Ssuperscript‚àôùê∂ùëÜ(\bullet)^{C,S}( ‚àô ) start_POSTSUPERSCRIPT italic_C , italic_S end_POSTSUPERSCRIPT Quantities computed during the backward pass ùë∫Œ±subscriptùë∫ùõº\bm{S}_{\alpha}bold_italic_S start_POSTSUBSCRIPT italic_Œ± end_POSTSUBSCRIPT Joint motion subspace matrix ùìïksubscriptùìïùëò\bm{\mathcal{F}}_{k}bold_caligraphic_F start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT Local wrench ùìúksubscriptùìúùëò\bm{\mathcal{M}}_{k}bold_caligraphic_M start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT Screw inertia matrix ùúºksubscriptùúºùëò\bm{\eta}_{k}bold_italic_Œ∑ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT Velocity twist ùúºÀôksubscriptÀôùúºùëò\dot{\bm{\eta}}_{k}overÀô start_ARG bold_italic_Œ∑ end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT Acceleration twist ùìñùìñ\bm{\mathcal{G}}bold_caligraphic_G ùìñ=[ùüéT‚Å¢ùíÇgT]Tùìñsuperscriptdelimited-[]superscript0ùëásuperscriptsubscriptùíÇùëîùëáùëá\bm{\mathcal{G}}=[\bm{0}^{T}\bm{a}_{g}^{T}]^{T}bold_caligraphic_G = [ bold_0 start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_italic_a start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT, where ùíÇgsubscriptùíÇùëî\bm{a}_{g}bold_italic_a start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT is the acceleration due to gravity ùíàùíà\bm{g}bold_italic_g Homogeneous transformation matrix in S‚Å¢E‚Å¢(3)ùëÜùê∏3SE(3)italic_S italic_E ( 3 ) Ad(‚ãÖ)subscriptAd‚ãÖ\mathrm{Ad}_{(\cdot)}roman_Ad start_POSTSUBSCRIPT ( ‚ãÖ ) end_POSTSUBSCRIPT, Ad(‚ãÖ)‚àósuperscriptsubscriptAd‚ãÖ\mathrm{Ad}_{(\cdot)}^{*}roman_Ad start_POSTSUBSCRIPT ( ‚ãÖ ) end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT Adjoint maps in S‚Å¢E‚Å¢(3)ùëÜùê∏3SE(3)italic_S italic_E ( 3 ) ad(‚ãÖ)subscriptad‚ãÖ\mathrm{ad}_{(\cdot)}roman_ad start_POSTSUBSCRIPT ( ‚ãÖ ) end_POSTSUBSCRIPT, ad(‚ãÖ)‚àósuperscriptsubscriptad‚ãÖ\mathrm{ad}_{(\cdot)}^{*}roman_ad start_POSTSUBSCRIPT ( ‚ãÖ ) end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT, ad¬Ø(‚ãÖ)‚àósuperscriptsubscript¬Øad‚ãÖ\overline{\mathrm{ad}}_{(\cdot)}^{*}over¬Ø start_ARG roman_ad end_ARG start_POSTSUBSCRIPT ( ‚ãÖ ) end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT adjoint operators in ùî∞‚Å¢ùî¢‚Å¢(3)ùî∞ùî¢3\mathfrak{se}(3)fraktur_s fraktur_e ( 3 )"
https://arxiv.org/html/2411.04494v1,Online Omnidirectional Jumping Trajectory Planning for Quadrupedal Robots on Uneven Terrains,"Natural terrain complexity often necessitates agile movements like jumping in animals to improve traversal efficiency. To enable similar capabilities in quadruped robots, complex real-time jumping maneuvers are required. Current research does not adequately address the problem of online omnidirectional jumping and neglects the robot‚Äôs kinodynamic constraints during trajectory generation. This paper proposes a general and complete cascade online optimization framework for omnidirectional jumping for quadruped robots. Our solution systematically encompasses jumping trajectory generation, a trajectory tracking controller, and a landing controller. It also incorporates environmental perception to navigate obstacles that standard locomotion cannot bypass, such as jumping from high platforms. We introduce a novel jumping plane to parameterize omnidirectional jumping motion and formulate a tightly coupled optimization problem accounting for the kinodynamic constraints, simultaneously optimizing CoM trajectory, Ground Reaction Forces (GRFs), and joint states. To meet the online requirements, we propose an accelerated evolutionary algorithm as the trajectory optimizer to address the complexity of kinodynamic constraints. To ensure stability and accuracy in environmental perception post-landing, we introduce a coarse-to-fine relocalization method that combines global Branch and Bound (BnB) search with Maximum a Posteriori (MAP) estimation for precise positioning during navigation and jumping. The proposed framework achieves jump trajectory generation in approximately 0.1 seconds with a warm start and has been successfully validated on two quadruped robots on uneven terrains. Additionally, we extend the framework‚Äôs versatility to humanoid robots.","Quadruped robots have demonstrated significant potential in navigating complex terrains and performing agile maneuvers, similar to natural quadrupeds. Achieving agile jumping capabilities‚Äîsuch as omnidirectional jumps and descending from high platforms‚Äîis crucial for expanding the operational scope of these robots in real-world environments. The primary challenge involves generating real-time jumping trajectories that adhere to the dynamic and kinematic constraints of underactuated systems while ensuring stable landings to minimize hardware stress. Previous studies have showcased quadruped robots executing backflips and forward jumps using offline trajectory optimization methods, such as Katz (2018); Ding et al. (2020); Nguyen et al. (2019); Gilroy et al. (2021). However, the offline-generated trajectories are unsuitable for real-time applications and are limited to specific scenarios. Advances in locomotion gaits have also been achieved through model-based approaches, like Di Carlo et al. (2018), enabling robots to handle various obstacles. But, the obstacle-crossing ability of these pure locomotion methods is limited. Additionally, some works have focused on planning the Center of Mass (CoM) trajectory and employing kinodynamic tracking controllers for jumping tasks, like Chignoli et al. (2021b). Approaches that consider only CoM trajectory optimization often neglect comprehensive dynamic and kinematic constraints. This requirement hinders direct implementation with tracking controllers or low-level controllers. Moreover, most existing studies construct the jumping problem depending on specific platforms affecting the algorithm‚Äôs scalability and concentrate on the take-off phase without designing landing controllers capable of mitigating impact forces during descent, which is essential for reducing hardware damage. Many existing jumping frameworks may not fully integrate environmental perception or address relocalization challenges from map positioning failures caused by high-impact landings. To address these issues, we introduce an omnidirectional jumping framework that enables real-time trajectory optimization while accounting for the robot‚Äôs dynamic and kinematic constraints. An omnidirectional jumping plane is utilized to formulate the optimization problem for ground reaction forces (GRFs) and the CoM trajectory based on a reduced dynamics model. To accelerate optimization by meeting real-time requirements, the framework utilizes Latin Hypercube Sampling(LHS)Ayyub and Lai (1989), enhancing the initial population quality and performing configuration space analysis intensively to reduce the searching space. Moreover, we propose a warm start strategy with the help of the Pre-motion library and carefully design a prioritized fitness function to ensure the stable convergence of the algorithm. Table 1: Supplementary video list in this paper Video Links 1 Summary Video 2 Omnidirectional Jumping 3 Agile Motions 4 Jumping with Navigation An evolutionary algorithm solves the complex constrained optimization problem, producing outputs that include GRFs, CoM trajectory, joint configurations, and timing for each jump stage. An impedance controller is implemented as an active compliance landing controller to handle large impact forces during landing. Additionally, a Whole Body Controller (WBC) is employed for trajectory tracking, and a proportional-derivative (PD) controller manages the flight phase, ensuring systematic control throughout the jumping process. Environmental perception is integrated to enable jumping on uneven terrains, combined with navigation capabilities. A coarse-to-fine relocalization method is proposed to address localization failures to enhance localization stability after high-impact landings. 1.1 Contribution This work presents a novel and comprehensive framework for online jump trajectory optimization, with the following key contributions: 1. Development of a novel and general framework for quadrupedal omnidirectional jumping: We introduce a general and comprehensive omnidirectional jumping trajectory framework that includes online trajectory optimization, real-time trajectory tracking, and an active compliant landing controller. This framework enables effective jumping in multiple directions‚Äîincluding forward, backward, left, right, and diagonal movements like front-right. By integrating environmental perception, the framework allows the quadruped robot to overcome uneven terrain through jumping. Additionally, this method is successfully extended to support bipedal jumping. 2. Novel online optimizable jumping problem definition and solution: By constructing an omnidirectional jumping plane and formulating the optimization problem using a reduced-order model, we generate ground reaction forces (GRFs) and Center of Mass (CoM) trajectories along the jumping plane, with a feasible target point as input. Kinodynamic constraints are integrated into the optimization to ensure dynamic and kinematic feasibility. To achieve real-time optimization within approximately 0.1 seconds, we employ a differential evolution algorithm enhanced with strategies such as configuration space considerations, Latin Hypercube Sampling for efficient initial sampling, a pre-motion library as a warm start, and an innovatively designed fitness function (discussed in Sec. 4 and Sec. 8). 3. Advanced localization robustness with large impact upon landing: To ensure the accuracy of environmental perception assisted jumping on uneven terrain. We introduce a robust localization recovery mechanism using a novel coarse-to-fine relocalization method within our jumping framework. In the coarse stage, a global Branch-and-Bound (BnB) search algorithm provides an accurate initial pose. The refinement stage involves a Maximum a Posteriori (MAP) estimation that fuses sensor data from Inertial Measurement Units (IMUs), motor encoders, and LiDAR to achieve precise localization (as detailed in Sec. 7). 4. Extensive real-world experiments on two quadrupedal robots and simulations on a humanoid robot validate the effectiveness of our novel jumping framework for legged systems: Through comprehensive simulations and real-world experiments‚Äîincluding multi-directional jumps and autonomous navigation (including indoor and outdoor) involving substantial landing impacts, such as jumping from elevated platforms‚Äîwe have validated the robustness and practicality of our algorithm (see in Tab. 1). Furthermore, the jumping framework has been extended to accommodate a full-sized humanoid robot in simulated environments, demonstrating its versatility and effectiveness (as detailed in Sec. 9). 1.2 Related Works Model-free reinforcement learning (RL) has achieved significant advancements in quadruped locomotion, as highlighted in recent literature, like Lee et al. (2020); Li et al. (2023); Choi et al. (2023). However, the direct application of these techniques to jumping tasks faces challenges due to the limited jumping duration and high impact forces encountered during landing. This limitation has prompted researchers to explore the application of RL in enhancing jumping capabilities in robotics in Liu et al. (2023); Bellegarda et al. (2024). In particular, combining jumping with traditional locomotion has shown promise in enabling robots to navigate complex obstacles, such as desks and gaps in Cheng et al. (2024); Zhuang et al. (2023). Additionally, the integration of jumping functionalities has been explored in specific robotic tasks, including improving the agility of robotic goalkeepers in Huang et al. (2023). Notable achievements in this area include the development of robots capable of performing long-distance forward jumps, with some achieving distances greater than their length, and adopting cat-like landing strategies to ensure smooth touch-down and effective impact absorption like Bellegarda et al. (2024); Kurtz et al. (2022); Rudin et al. (2021). Despite these advancements, research on omnidirectional jumping and developing a unified control policy for executing various complex jumping maneuvers while ensuring post-landing hardware integrity remains limited. Gradient-based trajectory optimization has emerged as an important technique in enhancing quadruped robot agility, enabling precise trajectory following through leveraging the robot‚Äôs dynamic models for optimal joint trajectory computation, like Bledt et al. (2018); Mastalli et al. (2020); Jenelten et al. (2020). This approach has been similarly practical in augmenting jumping capabilities, as demonstrated by MIT researchers who utilized gradient-based algorithms to enable a Cheetah3 robot to ascend a 0.76m platform in Nguyen et al. (2019). Similarly, this methodology facilitated the Mini Cheetah‚Äôs ability to execute real-time, efficient, and dynamically stable 3D omnidirectional jumps predicated on predefined contact sequences and desired launch velocities in Chignoli et al. (2021b, a). Further advancements have been achieved through collocation-based optimization for jump execution within defined safety constraints in Gilroy et al. (2021) and Mixed-Integer Convex Optimization for forward jumps that circumvent the need for initial guesses, targeting the robots with two legs in Ding et al. (2020). Despite advancements, existing approaches to quadruped robot jumping often mandate predefined contact sequences or velocities, do not consider kino-dynamics at the trajectory generation, and lack support for omnidirectional jumps. These methods, using online or offline optimization, have not achieved the simultaneous goals of speed, stability, and real-time execution. Furthermore, there is a pressing demand for an autonomous technique that enables omnidirectional jumping based directly on the target position, eliminating reliance on preset motion references. Heuristic algorithms offer an efficient solution to optimization challenges with complex constraints, presenting novel avenues for jump motion generation. Differential Evolution (DE), introduced by Ahmad et al. (2022), stands out as a heuristic algorithm that has seen applications across robotics, signal processing, and other domains for tackling intricate optimization tasks. Our prior research utilized DE to obtain offline jumping trajectories for quadruped robots and get impressive results in Song et al. (2022). Nonetheless, this offline methodology falls short for systems necessitating online re-planning. Additionally, Yue et al. (2023) using Latin hypercube sampling (LHS) for initializing DE populations has enhanced convergence rates in scenarios with low-dimensional spaces in Wang et al. (2022). But the method introduced in Yue et al. (2023) can not achieve omnidirectional jumps. In recent years, lidar odometry and mapping methods have been extensively utilized in autonomous driving and robotic navigation. Several notable frameworks have been proposed, including LOAM (Zhang et al., 2014), LeGO-LOAM (Shan and Englot, 2018), LIO-SAM (Shan et al., 2020), and FAST-LIO (Xu et al., 2022). Among these, FAST-LIO has been widely adopted due to its superior accuracy and efficiency. However, these frameworks lack failure recovery strategies, making them unsuitable for direct application in quadrupedal robot navigation tasks involving jumping motions, where localization tracking failures frequently occur during landing. Therefore, a re-localization component is essential for quadrupedal robot navigation frameworks with jumping. The re-localization component in lidar SLAM essentially involves point cloud registration. To enhance the robustness of re-localization, we propose a global search method using the Branch-and-Bound (BnB) algorithm, which can obtain the global optimal solution and is well-suited for robust estimation problems. Unlike existing 6D point-to-point registration approaches employing BnB Yang et al. (2015); Campbell and Petersson (2016); Liu et al. (2018), we introduce a 3D point-to-plane registration algorithm tailored for our re-localization problem. This approach improves the algorithm‚Äôs efficiency and accuracy while maintaining robustness."
https://arxiv.org/html/2411.04428v1,DexH2R: Task-oriented Dexterous Manipulation from Human to Robots,"Dexterous manipulation is a critical aspect of human capability, enabling interaction with a wide variety of objects. Recent advancements in learning from human demonstrations and teleoperation have enabled progress for robots in such ability. However, these approaches either require complex data collection such as costly human effort for eye-robot contact, or suffer from poor generalization when faced with novel scenarios. To solve both challenges, we propose a framework, DexH2R, that combines human hand motion retargeting with a task-oriented residual action policy, improving task performance by bridging the embodiment gap between human and robotic dexterous hands. Specifically, DexH2R learns the residual policy directly from retargeted primitive actions and task-oriented rewards, eliminating the need for labor-intensive teleoperation systems. Moreover, we incorporate test-time guidance for novel scenarios by taking in desired trajectories of human hands and objects, allowing the dexterous hand to acquire new skills with high generalizability. Extensive experiments in both simulation and real-world environments demonstrate the effectiveness of our work, outperforming prior state-of-the-arts by 40% across various settings.","Humans exhibit an extraordinary ability to manipulate a wide range of objects with their hands, both in everyday activities and industrial processes. In contrast, enabling robots to handle such diverse objects with the same level of reliability remains a significant challenge. Recent advances in reinforcement learning (RL) have led to notable progress in robotic dexterous manipulation, enabling robots to perform tasks such as grasping [1, 2], in-hand object rotation [3], and tool usage [4, 5]. However, training RL policies for dexterous manipulation remains challenging due to the large action space. For instance, attaching a Leap hand [6] to a 6-degree-of-freedom (DoF) manipulator creates a combined 22 DoF system. To solve this problem, approaches such as [7, 1, 8] incorporate pre-computed grasps into the manipulation policies as additional constraints. Methods like [9, 3, 10] take a step further by involving human demonstrations, allowing the policy to better imitate how humans interact with objects. While these human-defined grasps or trajectories [11] can improve learning efficiency, they often face generalization issues. The trained policies rely heavily on human priors, making it difficult to generalize to new scenarios, e.g., constraints or objects that require novel grasp poses. Previous attempts to address the above problem have relied on either simulations [2] to create diverse training scenarios, or teleoperation systems [12] to collect high-quality data. While these approaches have attracted increasing attention in robotic manipulation and whole-body control, they typically assume a relatively small embodiment gap from the demonstration to the robot, as in the case of dexterous hands like the Shadow hand [13], which closely resemble the human hand and allow for efficient retargeting to initialize the policy [qin2021dexmv, 9, 7]. However, such assumptions hinder generalization, as many cost-effective dexterous hands, such as the Allegro [14] and Leap hands [6], exhibit significant embodiment gaps, making it difficult to achieve robust retargeting. Furthermore, most teleoperation works focus on retargeting human motions to robots [15, 16, 17]. These approaches often assume that humans can adjust trajectories during execution but place less emphasis on task completion during policy training, requiring additional human attention and effort for eye-robot contact. Figure 1: When provided with the same object trajectories and initial states, DexH2R effectively navigates around obstacles by incorporating human hand trajectories, allowing it to avoid collisions (top right). In contrast, the current grasping policy offers a single solution (in red) that ignores environmental factors, often resulting in collisions (top left). Unlike traditional teleoperation systems (middle), DexH2R operates without real-time human intervention (bottom), significantly reducing human effort, ensuring smooth operation, and eliminating the need for expensive systems. In this work, we address the challenges on both sides by learning generalizable and guideable robotic dexterous manipulation from human hand motions. Compared to teleoperation, in addition to using a retargeting module to provide primitive actions, we learn a residual action policy to bridge the gap between human hands and robotic dexterous hands to improve task completion. Specifically, by taking in desired trajectories of human hands and objects, the policy is able to output a residual action to compensate for primitive actions from retargeting, thus achieving as close tracking of hand movements as possible while completing tasks. Furthermore, we enable test-time guidance for novel scenarios by incorporating human hand trajectories into the state representation. This allows the dexterous hand to learn new skills, such as adapting from training in open spaces to grasping in narrow spaces during inference. As shown in Figure 1, the proposed framework offers mainly two benefits. 1) Besides primitive actions acquired through retargeting, we learn a residual action policy that bridges the gap between human and robotic dexterous hands, allowing the model to adapt to a wide range of tasks and environments. 2) We incorporate human hand trajectories into the state representation, enabling real-time guidance and skill adaptation to new and unforeseen scenarios. Together, they provide a comprehensive solution that enables a single dexterous policy to both follow human hand movements while completing tasks and generalize to novel environments using human motion cues at inference time. With extensive experiments, DexH2R achieves success rates of 70.9% in grasping objects and 52.7% in completing the whole trajectories without dropping objects, outperforming retargeting methods commonly used in teleoperation by approximately 40%. Additionally, we demonstrate DexH2R‚Äôs superiority in generalizing to novel graspings in real-world experiments."
https://arxiv.org/html/2411.04413v1,Seeing Through Pixel Motion: Learning Obstacle Avoidance from Optical Flow with One Camera,"Optical flow captures the motion of pixels in an image sequence over time, providing information about movement, depth, and environmental structure. Flying insects utilize this information to navigate and avoid obstacles, allowing them to execute highly agile maneuvers even in complex environments. Despite its potential, autonomous flying robots have yet to fully leverage this motion information to achieve comparable levels of agility and robustness. Challenges of control from optical flow include extracting accurate optical flow at high speeds, handling noisy estimation, and ensuring robust performance in complex environments. To address these challenges, we propose a novel end-to-end system for quadrotor obstacle avoidance using monocular optical flow. We develop an efficient differentiable simulator coupled with a simplified quadrotor model, allowing our policy to be trained directly through first-order gradient optimization. Additionally, we introduce a central flow attention mechanism and an action-guided active sensing strategy that enhances the policy‚Äôs focus on task-relevant optical flow observations to enable more responsive decision-making during flight. Our system is validated both in simulation and the real world using an FPV racing drone. Despite being trained in a simple environment in simulation, our system demonstrates agile and robust flight in various unknown, cluttered environments in the real world at speeds of up to 6 m/stimes6dividemetersecond6\text{\,}\mathrm{m}\text{/}\mathrm{s}start_ARG 6 end_ARG start_ARG times end_ARG start_ARG start_ARG roman_m end_ARG start_ARG divide end_ARG start_ARG roman_s end_ARG end_ARG.","I INTRODUCTION Depth images, or depth maps, have long been useful in robotics, providing direct measurements of object distances, which are crucial for tasks like navigation and obstacle avoidance. Many existing planning and control algorithms can directly leverage the distance information in their task formulation, leading to straightforward implementation. Similarly, end-to-end control models, trained to map raw visual input directly to actions, have also been shown to benefit from explicit depth inputs, enhancing policy training and sim-to-real transfer [1, 2]. Depth sensing has thus enabled impressive achievements in navigation systems, including swarm navigation in complex environments such as forests [3, 2, 4]. Figure 1: Our drone autonomously navigates in the cluttered environment using optical flow estimated from a single camera. (a) Overview of the testing environment and trajectory. (b) Our real-world offboard is equipped with an FPV camera, a wireless video transmitter, andcontrol system, a wireless data transmitter. (c) FPV images and optical flow estimations recorded during the flight. While depth provides a structured and intuitive representation of the environment, it has limitations. Depth estimation from cameras, e.g., stereo cameras or a monocular camera with depth estimation algorithms, tends to suffer from reduced accuracy at greater distances, especially in outdoor environments where objects may be too far or lack sufficient features for reliable measurements. LIDAR, while offering precise depth data, is often too heavy and expensive for many lightweight or cost-sensitive applications, such as small drones. Optical flow, in contrast, remains effective across a wider range of distances and provides valuable motion cues when depth perception is unreliable. It can also be computed using lightweight, low-cost monocular cameras, making it ideal for small aerial robots with limited payload and power capacity. Studies have shown that optical flow is fundamental to motion in flying insects, such as flies and bees, which rely primarily on monocular vision for navigation [5]. Despite having tiny heads and simple neural structures, insects effectively use optical flow for visual odometry [6], landing [7], measuring the gap [8], tunnel crossing [9], and obstacle avoidance [10]. This suggests that optical flow provides comprehensive information about self-motion and spatial layouts, showing its potential for autonomous flight. This natural reliance on visual cues motivates the question central to this work: How can we enable autonomous flight using optical flow, without relying on explicit depth measurements? Leveraging optical flow for autonomous navigation in cluttered environments remains a fundamental challenge in robotics. First, optical flow estimation from images is prone to errors, especially in low-texture environments, varying illumination, and high-speed motions. Second, extracting essential motion cues, such as time-to-collision [11], nearness maps [12], and looming [13], remains an open problem, as existing algorithms rely on simplified assumptions like pure translational motion or front-parallel obstacles. Third, designing effective control strategies based on these 2D cues is difficult due to the lack of intuitive metric information. Existing methods usually apply heuristic strategies such as braking [14], balancing the flow field [11, 15], and shifting to the side [13]. Such control strategies can lead to inferior control performance when the robot flies in complex environments at high speed. As a result, current flow-based methods are typically confined to controlled conditions and slow flight speeds (<<<3 m/stimes3dividemetersecond3\text{\,}\mathrm{m}\text{/}\mathrm{s}start_ARG 3 end_ARG start_ARG times end_ARG start_ARG start_ARG roman_m end_ARG start_ARG divide end_ARG start_ARG roman_s end_ARG end_ARG) [14]. Contribution: This work introduces a learning-based approach for flow-based monocular obstacle avoidance in quadrotors. Our method enables a high-performance FPV racing drone to navigate through complex, unstructured environments, such as forests, at high speeds, using only an onboard FPV camera for perception. The control policy is trained in simulation and then directly transferred to the real world zero shot. Our training environment features a simple point mass model for dynamics simulation and analytical gradient calculations. Additionally, we render a simple 3D scene that contains different obstacles, including planes, cuboids, spheres, and cylinders (Fig. 3). Existing methods typically rely on expensive high-fidelity simulators to generate realistic scenes, we discovered that our approach, despite its simplicity, can produce robust vision-based flight policies that generalize well to diverse real-world scenarios. The key to our approach is a learning-based system that tightly merges robust flow estimation with an efficient policy optimization method using differentiable simulation. First, we use NeuFlow [16], a real-time, high-accuracy dense optical flow estimation method, to extract optical flow from image sequences. We also align the camera with the vehicle‚Äôs flight direction and enhance flow estimation using a central flow attention mechanism, allowing the system to focus on task-relevant flow information for downstream decision-making. Second, we optimize a memory-based control policy that maps the predicted optical flow to control commands. We use a first-order gradient optimization method, which provides stable training through low-variance policy gradients. Third, we develop a GPU-based differentiable simulation that provides ground-truth depth and optical flow using ray casting, achieving high-speed simulation. Our system is finally validated using an offboard setup, where video streams from the onboard camera are transmitted to a laptop for real-time processing and control. An overview of our system is given in Fig. 2. Figure 2: System overview. We train our neural network policy using a differentiable simulator, which enables simulating quadrotor physics, rendering ground-truth optical flow, and calculating analytic policy gradients. We deploy our policy using a real FPV-style quadrotor in the real world. The reference acceleration output by the flight policy is sent into the inner-loop controller."
https://arxiv.org/html/2411.04408v1,Repairing Neural Networks for Safety in Robotic Systems using Predictive Models,"This paper introduces a new method for safety-aware robot learning, focusing on repairing policies using predictive models. Our method combines behavioral cloning with neural network repair in a two-step supervised learning framework. It first learns a policy from expert demonstrations and then applies repair subject to predictive models to enforce safety constraints. The predictive models can encompass various aspects relevant to robot learning applications, such as proprioceptive states and collision likelihood. Our experimental results demonstrate that the learned policy successfully adheres to a predefined set of safety constraints on two applications: mobile robot navigation, and real-world lower-leg prostheses. Additionally, we have shown that our method effectively reduces repeated interaction with the robot, leading to substantial time savings during the learning process.","Robot learning holds great promise for advancing wearable robotics, such as Prosthetics, Orthoses, and Exoskeletons [1]. These devices are typically designed with a one-size-fits-all approach. Deep learning techniques offer the potential to customize policies for each user that leads to improved comfort and quality of life. One common approach for customizing policies has been through reinforcement learning (RL). However, RL can be time-consuming and potentially hazardous in real-world scenarios, as agents must explore the environment and learn from mistakes [2]. In our previous work [3], we introduced an algorithm for training neural network policies that satisfy given safety specifications using neural network repair. By solving a layer-wise Mixed-integer Quadratic Program (MIQP), we modified network weights to meet desired safety constraints on outputs while providing mathematical guarantees on safety for the samples used in repair. However, our method primarily focused on addressing explicit safety constraints over the output space of the neural network. Building upon our previous work, we now extend our method to address implicit safety constraints on the output of predictive models. Figure 1: Safety-Aware Repair with Predictive models (SARP). Left: A policy trained for a mobile navigation task (point to goal motion planning in this figure). Right: Policy repair module that adjusts the policy parameters to penalize unsafe behavior, based on a set of safety constraints and the loss of predictive models. Predictive models may include state-action transition model, a model of environment, or a proprioceptive model predicting internal states of the system. In this paper, we present SARP (Safety-Aware Repair with Predictive models), an approach for safety-driven learning of robot policies from human demonstrations. Our method focuses on repairing an existing neural network policy to ensure compliance with a defined set of safety constraints. We leverage a predictive model to predict the features of the system given states and actions. These features can encompass various aspects in different robot learning applications, such as proprioceptive states in biomechanical applications, or collision occurrence in navigation scenarios. In a two-step supervised learning process, SARP initially trains a policy using expert demonstrations. We then employ a predictive model to enforce safety constraints on the policy. This is achieved through the application of neural network repair techniques to regulate the predicted system features. SARP addresses both implicit constraints on the policy, using differentiable predictive models, and explicit bounding constraints on actions. We assess our method in two safety-critical case studies. We first compare SARP‚Äôs performance with two state-of-the-art safe RL methods, as presented in [4] and [5]. This comparison aims to demonstrate SARP‚Äôs efficacy and comparable performance in a simulated mobile robot navigation task as a showcase example. Our goal is not to position SARP as a substitute for RL but rather as a complementary method that addresses the safety concerns associated with lengthy simulation in RL for safety-critical robot learning tasks, such as lower-leg prosthesis control. We finally test SARP for controlling a real-world lower-leg prosthesis in an IRB-approved study. Our novel contribution lies in repairing the policy to ensure that the predicted features of the system adhere to a predefined set of safety constraints. Specifically, this paper makes the following contributions: 1. We propose a novel two-step supervised learning method that combines imitation learning and neural network repair using predictive models. 2. Our framework can utilize any pre-existing differentiable model in the repair process that reduces the repetitive interactions with the robot. 3. We compare SARP with two safe RL-based methods of constrained policy optimization (CPO) in [4] and the safety layer approach in [5]. 4. The code for reproducing the results is available at https://github.com/k1majd/SARP.git."
https://arxiv.org/html/2411.04386v1,SuperQ-GRASP: Superquadrics-based Grasp Pose Estimation on Larger Objects for Mobile-Manipulation,"Grasp planning and estimation have been a long-standing research problem in robotics, with two main approaches to find graspable poses on the objects: 1) geometric approach, which relies on 3D models of objects and the gripper to estimate valid grasp poses, and 2) data-driven, learning-based approach, with models trained to identify grasp poses from raw sensor observations. The latter assumes comprehensive geometric coverage during the training phase. However, the data-driven approach is typically biased toward tabletop scenarios and struggle to generalize to out-of-distribution scenarios with larger objects (e.g. chair). Additionally, raw sensor data (e.g. RGB-D data) from a single view of these larger objects is often incomplete and necessitates additional observations. In this paper, we take a geometric approach, leveraging advancements in object modeling (e.g. NeRF) to build an implicit model by taking RGB images from views around the target object. This model enables the extraction of explicit mesh model while also capturing the visual appearance from novel viewpoints that is useful for perception tasks like object detection and pose estimation. We further decompose the NeRF-reconstructed 3D mesh into superquadrics (SQs) - parametric geometric primitives, each mapped to a set of precomputed grasp poses, allowing grasp composition on the target object based on these primitives. Our proposed pipeline overcomes the problems: a) noisy depth and incomplete view of the object, with a modeling step, and b) generalization to objects of any size. For more qualitative results, refer to the supplementary video and webpage https://bit.ly/3ZrOanU.","To perform tasks such as arranging furniture or moving heavy boxes, robots must autonomously manipulate objects (as shown in bottom row of Fig. LABEL:fig:overview). The critical first step is grasping, where the robot establishes firm contact with the object. This phase requires the robot arm to position and orient the gripper to achieve a target grasp pose before closing the gripper. Once the gripper is closed, the robot must maintain a stable hold for subsequent tasks. Grasp pose estimation remains a longstanding challenge in robotics due to the wide variety of objects in our world with diverse properties. This paper focuses on estimating valid grasp poses for larger objects, which present unique challenges. There are two main approaches for estimating valid grasp poses: geometric-based and data-driven. The geometric approach [1, 2, 3, 4, 5, 6, 7] analyzes an object‚Äôs 3D model to estimate grasp poses based on its geometric properties, such as surface curvature [1, 7]. While precise, this method typically requires an accurate mesh or point cloud, which may not always be available. In practice, an offline object modeling phase is often needed to reconstruct the object from sensor data, followed by pose estimation and validation of pre-computed grasp poses. This process becomes more challenging for larger objects with complex geometries (high genus), which require multiple viewpoints and are prone to sensor noises. The data-driven approach trains models to estimate grasp poses directly from raw sensor data, such as point clouds or images[8, 9, 10]. Although these models can handle outliers with sufficient training, they often suffer from biases in the training dataset. Most data-driven models are trained on tabletop scenarios, where objects are typically smaller, convex, or of low genus [10, 11, 8, 12, 13]. As a result, these methods struggle to generalize to larger, more complex objects and to accommodate limited observations across various viewpoints (see Fig. 1). Consider manipulation tasks involving mobile manipulators that handle larger, relatively complex objects (concave and high genus) than those typically encountered in tabletop scenarios (convex and low genus). These tasks require grasping from diverse 3D viewpoints. While existing data-driven methods have proven effective in many applications, they face challenges when applied to such larger objects. They are typically trained on datasets containing smaller tabletop items, which may limit their generalization to more complex, larger objects. In this paper, we propose a novel pipeline to enable a robot to autonomously grasp larger target objects (e.g., chairs or tables) for downstream mobile manipulation tasks, such as pulling or pushing. This pipeline includes five key modules, inspired by the pipelines in [14, 15]. The first module is 3D Mesh Model Reconstruction, where we reconstruct the 3D mesh of the target object from multiview RGB images using NeRF modeling [16, 17]. The second module is Primitives Decomposition, which breaks down the reconstructed 3D mesh into primitive shapes known as superquadrics (SQs). The third module, Grasp Pose Estimation, calculates grasp poses for each SQ and can also use the current gripper pose to identify grasps on the closest SQ relative to the estimated pose of the target object. The fourth module, Object Detection and Pose Estimation, uses the NeRF model to generate candidate poses and iteratively refine the object‚Äôs pose estimate in the scene. Finally, the Grasp Candidate Validation module checks the plausibility and collision potential of the grasp candidates against the original mesh model. This pipeline ensures valid and stable grasp poses for the target object. In summary, the contributions of this paper are: 1. SuperQ-GRASP, a novel grasp pose estimation method that represents the target object as a collection of superquadrics, enabling reliable and viewpoint-invariant grasp computation. 2. A comprehensive pipeline that utilizes NeRF modeling from RGB images to enhance key stages of robot perception, including object pose estimation and identifying the closest grasp pose relative to the current gripper position. 3. Quantitative evaluation on synthetic and real-world objects, demonstrating our method‚Äôs ability to produce valid, proximal grasps invariant to viewpoints, outperforming state-of-the-art learning-based baselines. 4. Quantitative and qualitative evaluation on real-world grasping trials with the Boston Dynamics Spot robot, showcasing the robustness of our pipeline in grasping larger objects."
https://arxiv.org/html/2411.04331v1,Raising Body Ownership in End-to-End Visuomotor Policy Learning via Robot-Centric Pooling,"We present Robot-centric Pooling (RcP), a novel pooling method designed to enhance end-to-end visuomotor policies by enabling differentiation between the robots and similar entities or their surroundings. Given an image-proprioception pair, RcP guides the aggregation of image features by highlighting image regions correlating with the robot‚Äôs proprioceptive states, thereby extracting robot-centric image representations for policy learning. Leveraging contrastive learning techniques, RcP integrates seamlessly with existing visuomotor policy learning frameworks and is trained jointly with the policy using the same dataset, requiring no extra data collection involving self-distractors. We evaluate the proposed method with reaching tasks in both simulated and real-world settings. The results demonstrate that RcP significantly enhances the policies‚Äô robustness against various unseen distractors, including self-distractors, positioned at different locations. Additionally, the inherent robot-centric characteristic of RcP enables the learnt policy to be far more resilient to aggressive pixel shifts compared to the baselines.","Body ownership enables us to differentiate our own body from objects in our surroundings (self-recognition) and from other individuals (self-other distinction) [1]. These aspects are also crucial for robots, especially in shared or multi-robot settings, where a robot‚Äôs actions should remain unaffected by environmental objects and other robots. Acknowledging that body ownership in humans involves complex multisensory integration and cognitive processes [2], and drawing on prior work [3, 4, 5], we describe body ownership for robots at the visuomotor level, focusing on simulating features of self-recognition and self-other distinction. In this work, we introduce Robot-centric Pooling (RcP) to address severely limited self-recognition and self-other distinction capability in conventional end-to-end visuomotor policy learning. This novel pooling method explicitly integrates both visual information and the robot‚Äôs proprioceptive state, setting it apart from traditional pooling methods that rely solely on image data. RcP computes alignment scores between image regions and the robot‚Äôs proprioceptive states to derive image representations that reflect a robot-centric perspective (Fig.1). Notably, RcP is fully self-supervised and task-agnostic, allowing it to integrate seamlessly with existing standard CNN-based visuomotor policy learning frameworks. It can be jointly trained with the policy using the same training data, requiring no additional data collection. (a) Testing Sample (b) IPA Scores |ùíë+|\,\boldsymbol{p^{+}}| bold_italic_p start_POSTSUPERSCRIPT bold_+ end_POSTSUPERSCRIPT (c) Saliency |ùíë+|\,\boldsymbol{p^{+}}| bold_italic_p start_POSTSUPERSCRIPT bold_+ end_POSTSUPERSCRIPT (d) IPA Scores |ùíë‚àí|\,\boldsymbol{p^{-}}| bold_italic_p start_POSTSUPERSCRIPT bold_- end_POSTSUPERSCRIPT (e) Saliency |ùíë‚àí|\,\boldsymbol{p^{-}}| bold_italic_p start_POSTSUPERSCRIPT bold_- end_POSTSUPERSCRIPT (f) Saliency |||| Baseline Figure 1: Body ownership via Robot-centric Pooling. RcP enables a conventional policy regression baseline to foster self-recognition and the ability to distinguish self from others. (a): A testing sample including a self-distractor (right). (b): Image-proprioception alignment scores for self-state, ùíë+superscriptùíë\boldsymbol{p^{+}}bold_italic_p start_POSTSUPERSCRIPT bold_+ end_POSTSUPERSCRIPT. (c:) Image saliency map [6] with ùíë+superscriptùíë\boldsymbol{p^{+}}bold_italic_p start_POSTSUPERSCRIPT bold_+ end_POSTSUPERSCRIPT based on regressed policy (warmer colours indicate higher relevance). (d): IPA scores for the distractor‚Äôs state ùíë‚àísuperscriptùíë\boldsymbol{p^{-}}bold_italic_p start_POSTSUPERSCRIPT bold_- end_POSTSUPERSCRIPT. (e): Saliency map with ùíë‚àísuperscriptùíë\boldsymbol{p^{-}}bold_italic_p start_POSTSUPERSCRIPT bold_- end_POSTSUPERSCRIPT. (f): Saliency map from the Spatial-Softmax [7] baseline. (a) System Overview (b) Robot-centric Pooling Module Figure 2: System Overview and the Robot-centric Pooling Module. (a): Robot-centric Pooling extracts the most relevant feature corresponding to the identified self for the regression task. (b): RcP computes Image-Proprioception Alignment (IPA) scores from an image-proprioception pair (ùíô,ùíë)ùíôùíë(\boldsymbol{x},\boldsymbol{p})( bold_italic_x , bold_italic_p ) and aggregates image values accordingly to create a context vector for contrastive learning and image representation in the regression pipeline. We evaluate RcP with reaching tasks, in both simulated and real-world settings. Our experimental results show that: ‚Ä¢ Conventional end-to-end learning baselines exhibit considerable sensitivity to environmental distractions and self-distractors, revealing a fundamental deficiency in the development of body ownership. ‚Ä¢ Robot-centric Pooling (RcP) demonstrates significant enhancement against distractions, showing only a slight decrease in success rates (from 96% to 92%) amidst a self-distractor in real-world experiments, as opposed to baseline models which plummet to below 15%. ‚Ä¢ Benefiting from the robot-centric nature, RcP significantly enhances policy robustness against aggressive image shifts compared to baseline methods. To the best of our knowledge, this is the first demonstration of both the self-other distinction and the self-recognition capabilities of body ownership in the context of end-to-end visuomotor policy learning."
https://arxiv.org/html/2411.04326v1,Rapid Quadrotor Navigation in Diverse Environments using an Onboard Depth Camera,"Search and rescue environments exhibit challenging 3D geometry (e.g., confined spaces, rubble, and breakdown), which necessitates agile and maneuverable aerial robotic systems. Because these systems are size, weight, and power (SWaP) constrained, rapid navigation is essential for maximizing environment coverage. Onboard autonomy must be robust to prevent collisions, which may endanger rescuers and victims. Prior works have developed high-speed navigation solutions for autonomous aerial systems, but few have considered safety for search and rescue applications. These works have also not demonstrated their approaches in diverse environments. We bridge this gap in the state of the art by developing a reactive planner using forward-arc motion primitives, which leverages a history of RGB-D observations to safely maneuver in close proximity to obstacles. At every planning round, a safe stopping action is scheduled, which is executed if no feasible motion plan is found at the next planning round. The approach is evaluated in thousands of simulations and deployed in diverse environments, including caves and forests. The results demonstrate a 24% increase in success rate compared to state-of-the-art approaches.","Extreme search and rescue environments exhibit challenging 3D geometry (e.g., confined spaces, rubble, breakdown), which preclude rapid traversal by legged, wheeled, and tracked robotic systems. Quadrotors are highly agile and maneuverable, but are limited by battery capacity and flight time. Therefore, these systems must rapidly navigate through the environment. More importantly, onboard autonomous navigation must be robust as collisions endanger rescuers and victims. To achieve these goals in highly cluttered environments with narrow gaps, it is advantageous to leverage smaller size robots equipped with a lightweight, short-range depth camera as opposed to a heavy, long-range LiDAR. In this work, a rapid quadrotor navigation methodology is proposed, which leverages a forward-facing, limited field-of-view RGB-D camera to operate in diverse environments (e.g., caves, sewers, forests, industrial tunnels) without scene-specific parameter tuning or training. A reactive planner is developed using forward-arc motion primitives, which are differentiable up to jerk and continuous up to snap to enable aggressive flight. Motion primitive selection is based on a perception front-end that searches a history of RGB-D observations to safely maneuver in close proximity to obstacles. The contributions of this work are: (1) a reactive navigation framework that uses a history of depth observations to evaluate a library of forward-arc motion primitives; (2) a trajectory scheduling approach to prevent collisions; (3) extensive photo-realistic simulations with 3150 trials in diverse cluttered environments; and (4) hardware experiments covering 571 m/times571dividemeterabsent571\text{\,}\mathrm{m}\text{/}start_ARG 571 end_ARG start_ARG times end_ARG start_ARG start_ARG roman_m end_ARG start_ARG divide end_ARG start_ARG end_ARG end_ARG in successful trials without collisions and achieving a maximum speed of 6 m/stimes6dividemetersecond6\text{\,}\mathrm{m}\text{/}\mathrm{s}start_ARG 6 end_ARG start_ARG times end_ARG start_ARG start_ARG roman_m end_ARG start_ARG divide end_ARG start_ARG roman_s end_ARG end_ARG. The results demonstrate a 24%percent2424\%24 % higher success rate compared to state-of-the-art approaches."
https://arxiv.org/html/2411.04313v1,Task-Difficulty-Aware Efficient Object ArrangementLeveraging Tossing Motions,"This study explores a pick-and-toss (PT) as an alternative to pick-and-place (PP), allowing a robot to extend its range and improve task efficiency. Although PT boosts efficiency in object arrangement, the placement environment critically affects the success of tossing. To achieve accurate and efficient object arrangement, we suggest choosing between PP and PT based on task difficulty estimated from the placement environment. Our method simultaneously learns the tossing motion through self-supervised learning and the task determination policy via brute-force search. Experimental results validate the proposed method through simulations and real-world tests on various rectangular object arrangements.","Pick-and-toss (PT) can be more effective than pick-and-place (PP) for robotic object arrangement operations in the retail industry. Although several methods have been proposed to achieve accurate dynamic object manipualtion motions like tossing motions [1, 2, 3, 4], object placement in an arbitrary posture is not considered. Achieving both accuracy and efficiency in object arrangement using PT alone is difficult due to the placement environment‚Äôs significant effect on the success of the tossing motion. This study aims to achieve accurate and efficient object arrangement by addressing the task determination problem of PP and PT, considering task difficulty based on the number and type of contact surfaces in the target placement environment. We propose a framework where the robot learns the tossing motion and task determination policy through trial and error, introducing a method to acquire the tossing motion using self-supervised learning and the task determination policy through brute-force search. Our experiments validate the method‚Äôs effectiveness in both simulations and real-world scenarios. Figure 1: Training pipeline for PP-and-PT-based object arrangement."
https://arxiv.org/html/2411.04279v1,Novel Non-Prehensile Rolling Problem: Modelling and Balance Control of Pendulum-Driven Reconfigurable Disks Motion with Magnetic Coupling in Simulation,"This paper presents a novel type of mobile rolling robot designed as a modular platform for non-prehensile manipulation, highlighting the associated control challenges in achieving balancing control of the robotic system. The developed rolling disk modules incorporate an innovative internally actuated magnetic-pendulum coupling mechanism, which introduces a compelling control problem due to the frictional and sliding interactions, as well as the magnetic effects between each module. In this paper, we derive the nonlinear dynamics of the robot using the Euler‚ÄìLagrange formulation. Then, through simulation, the motion behavior of the system is studied and analyzed, providing critical insights for future investigations into control methods for complex non-prehensile motion between robotic modules. Also, we study the balancing of this new platform and introduce a new motion pattern of lifting. This research aims to enhance the understanding and implementation of modular self-reconfigurable robots in various scenarios for future applications.","Reconfigurable robots, composed of multiple modules that can interact and reconfigure, often lack versatility and independent movement. Coupling rolling robots as modular components addresses this issue by enabling displacement through internal actuation [1, 2]. However, developing coupling mechanisms without external components remains a significant challenge [3, 4]. Controlling these robots is complicated by the properties of non-prehensile manipulation, where modules can slip and lose their state during movement. Accurate control requires simulating and predicting these dynamics. Modular self-reconfigurable robots (MSRRs) [3] are mobile robots with individual modules containing controllers, sensors, and actuation systems. These modules can reconfigure, enabling MSRRs to perform diverse tasks and navigate unpredictable environments [5, 6]. Modules connect via docking systems, such as mechanical surfaces, magnetic interactions, and grippers, including hybrid combinations. Reconfiguration can occur autonomously or manually. MSRRs are invaluable in search and rescue, environmental monitoring, and hazardous environments due to their modular and reconfigurable nature, which allows them to adjust their morphology for optimal stability and manoeuvrability [6, 3]. Utilizing a modular rolling robot at the centre of the reconfigurable system could be advantageous, as rolling motion offers an effective means of locomotion [7]. Nevertheless, developing an appropriate coupling control method while creating locomotion between rolling modules remains an open challenge. Figure 1: Reconfigurable Rolling Disk Robot design with magnetic coupling while balancing [8] During motion, rolling robots do not remain fixed to the surface and are subject to slippage, leading to a loss of state [4]. This issue necessitates simulating and predicting the dynamics for accurate control, introducing new challenges in non-prehensile manipulation. Non-prehensile manipulation, characterized by the absence of direct grasping of objects [9, 4], presents significant control difficulties due to slippage and friction, resulting in unpredictable dynamics. Previous research has explored rolling and balancing disks [10] and the control of a disk on a beam [11], highlighting the complexities involved in non-prehensile manipulation. A new modular design [8] as shown in Fig. 1 addresses these challenges by introducing internally actuated modular disk modules, differing from existing approaches that rely on external actuation. This advancement opens great potential for applications in swarm reconfigurable robotics. However, due to the novelty of the locomotion in this new non-prehensile problem, the nonlinear models and capabilities of these types of systems are not yet well covered. This paper derives the nonlinear dynamics using the Euler‚ÄìLagrange formulation in Section II for a novel internalized pendulum-based magnetic coupling mechanism for rolling reconfigurable disk robots, which is described briefly in Section III. Next, this work demonstrates the robot‚Äôs distinctive motion capabilities using scenario-based PD controllers as an important contribution in Section IV. Additionally, we introduce and discuss new nonprehensile manipulation problems, such as lifting, which are supported by simulation results that provide insights into the nonlinear dynamics of the system. Figure 2: Schematic of the kinematic model."
https://arxiv.org/html/2411.04273v1,Understanding Generative AI in Robot Logic Parametrization,"Leveraging generative AI (e.g., Large Language Models) for language understanding within robotics opens up possibilities for LLM-driven robot end-user development (EUD). Despite the numerous design opportunities it provides, little is understood about how this technology can be utilized when constructing robot program logic. In this paper, we outline the background in capturing natural language end-user intent and summarize previous use cases of LLMs within EUD. Taking the context of filmmaking as an example, we explore how a cinematography practitioner‚Äôs intent to film a certain scene (1) can be articulated using natural language, (2) can be captured by an LLM, and (3) further be parametrized as low-level robot arm movement using an LLM. We explore the capabilities of an LLM interpreting end-user intent and mapping natural language to pre-defined, cross-modal data in the process of iterative program development. We conclude by suggesting future opportunities for domain exploration beyond cinematography to support language-driven robotic camera navigation.","Within robot program design, end users have a crucial role in specifying robot behaviors that match unique needs within a domain (Barricelli et al., 2019; Coronado et al., 2019). Although end users may have ideas on what they want to program within a domain, they may not know how to program the robot (Chung and Cakmak, 2020; Racca et al., 2020). Here, we motivate this problem by introducing a real-life scenario in which a domain expert uses a robot to perform a specific task. Imagine a filmmaker using a robotic camera to capture a specific scene. The robotic camera could be used as a cobot, or a collaborative robot (Colgate et al., 1996), to aid an expert in capturing scenes through teleoperation (Praveena et al., 2023). Cinematography practitioners may have ideas and intuition on how the scene should be filmed. For example, in a thriller movie, if the expert were to capture suspense from a scene, they could have recommendations on what types of shots to use (e.g., extreme close-up (Heiderich, 2024)) to add suspense to the scene. The expert would then express their intent of filming the scene using specific techniques. However, the question remains: how would they convey their intent if they are not familiar with robot operation methods? Given the aforementioned example, we identify two problems associated with expert end-users giving instructions to a robot. First, end users may not be equipped with the specific semantic rules associated with robot operation. Put differently, end users may not know what a well-informed intent (Liu et al., 2023) should look like, making it challenging for users to formulate their intent for effective communication. For example, experts may understand how to frame and control the camera movement to match their cinematic vision; however, they may not be as well-trained to verbalize these into commands. Experts may face difficulties in going through trial-and-error, framing their intent in different formats of statements such as controlling verbosity level and repeating certain phrases or keywords to emphasize their intent. Second, end users‚Äô well-informed intent may not suffice to generate complete, low-level behaviors of a robot (Liu et al., 2023). As with all robotics, factors such as how the robotic parameters (e.g., what signals are sent to the servo motors) should change are essential to the movement of robots. However, understanding and adopting the syntactic-level operation rules of a robot is a challenging task for the user (Chung and Cakmak, 2020), considering that learning how to program a robot requires much training/experience even for experienced programmers (Racca et al., 2020; Ritschel et al., 2023). While experts can familiarize themselves with how to ‚Äútalk‚Äù to the robot, this does not mean they can directly write programs to operate the robot. Thus, it is necessary to explore how a system can infer how these robotic parameters should change to match the expert‚Äôs vision. To address the first problem, we see potential in LLMS as they afford an intuitive way for users to utilize natural language in expressing intent. Although user utterances provided (i.e., prompts) for the LLM may not be used verbatim in capturing user intent and implementing programs, this way of communication through natural language allows users to express their programming intent intuitively. To address the second problem, we plan to fetch and translate user intent from natural language into parameters that may be used to define a preliminary set of motions for the robotic camera. LLMs could be used to refine user intent to generate robot-understandable, well-formed intent by generalizing user utterances and facilitating the modification of low-level behavior parameters by suggesting feasible changes and ranges for low-level behavior parameters using a probabilistic approach. Our work is heavily inspired by Praveena et al.‚Äôs work (Praveena et al., 2023) that explored the potential use of robotic cameras in the cinematography domain. Within this domain, we (1) explore how LLMs can be utilized as a robot EUD tool in capturing and parametrizing user intent, and (2) how the LLM-driven technique can work towards closing the abstraction gap (Liu and Zhang, 2019; Sarkar et al., 2022; Liu et al., 2023) (i.e., the gap between user-formulated intent and the solution space that is specific to the system). Figure 2. (Left) Formative study: We plan to conduct a formative study where a cinematography expert (denoted as the director) uses natural language and movement to express filming intent. The experimenter will act as the camera operator, closely following the expert‚Äôs instruction and moving the camera. (Middle) Input for Module: Different types of input modality will be used to construct separate robot program modules, namely the action module and the intent module. (Right) LLM Interaction: We envision the interaction scenario with the robotic arm mediated by LLM, where it interprets keywords within the end-user prompt and maps those keywords to pre-defined modules and respective parameter values. A robotic camera can initiate a movement given those parameter values."
https://arxiv.org/html/2411.04264v1,MonoRollBot: 3-DOF Spherical Robot with Underactuated Single Compliant Actuator Design,"Spherical rolling robots have garnered significant attention in the field of mobile robotics for applications such as inspection and space exploration. Designing underactuated rolling robots poses challenges in achieving multi-directional propulsion with high degrees of freedom while utilizing a limited number of actuators. This paper presents the MonoRollBot, a novel 3-degree-of-freedom (DOF) spherical robot that utilizes an underactuated mechanism driven by only a single spring-motor system. Unlike conventional spherical robots, MonoRollBot employs a minimalist actuation approach, relying on only one motor and a passive spring to control its locomotion. The robot achieves 3-DOF motion through an innovative coupling of spring dynamics and motor control. In this work, we detail the design of the MonoRollBot and evaluate its motion capabilities through design studies. We also do studies on its locomotion behaviours based on changes in rotating mass and stiffness properties.","Rolling robots offer a unique solution for tasks such as inspection and exploration, particularly in environments that demand minimal interaction between the robot‚Äôs exterior and its surroundings [1, 2]. These robots can operate without relying on external actuators, instead employing various internal actuation principles. Underactuation, where the robot achieves complex motion with a minimal number of actuators, presents a fascinating challenge in designing such systems [2]. Achieving efficient locomotion through underactuation is especially difficult in spherical robots, requiring a deep understanding of mechatronics and the physics of motion. Spherical robots have evolved through various propulsion principles, each offering unique advantages and challenges. One prominent method is torque-reaction propulsion, where robots utilize motor-driven wheels or mechanisms to create reactive forces that enable motion. For example, Halme et al. demonstrated a kinematic control method in 1996, utilizing a single-direction turning wheel for locomotion [3] or having a cart to move the spherical shell [4]. Another propulsion technique is mass imbalance, which involves manipulating the robot‚Äôs center of mass to achieve movement. Javadi introduced a mass-imbalance-driven robot in 2002, leveraging the shifting of weights along different axes to generate driving forces [5]. This concept was further exemplified by NASA‚Äôs Tumbleweeds rover, designed for space exploration, which combined wind energy with mass imbalance [6]. Although effective, mass-imbalance systems can face limitations, such as reduced velocity and constrained internal volume which was resolved by having an isolated rating mass system using fluid actuation by Tafrishi et al [2]. A third propulsion principle is the conservation of angular momentum, where internal mechanisms like gyroscopes generate motion through the conservation of rotational forces. The Gyrover, introduced in 1996, utilized a rotating internal gyroscope to create propulsion [7]. This principle was further advanced with the development of the Gyrosphere robot, which combined both angular momentum and torque-reaction forces for enhanced locomotion capabilities [8]. Each of these propulsion methods highlights the ongoing innovation and challenges in the design of spherical robots, contributing to their diverse applications. Underactuated robotic systems, though more complex than fully actuated ones, offer advantages like energy efficiency, simpler designs, and reduced hardware complexity [9, 10]. These systems are particularly useful in mobile robots, where minimizing actuators lowers energy consumption and control complexity [2, 11]. By leveraging dynamic interactions with the environment, underactuated robots achieve effective locomotion with fewer actuators, similar to biological systems like Armadillo, which use minimal actuation for adaptive rotational movements. However, achieving versatile locomotion with a single actuator, especially in rolling robots, presents significant challenges which hardly explored. Controlling multiple DOF with just one actuator demands complex mechanical design for utilise as much as possible with precise coordination of passive dynamics and interaction with the environment. Figure 1: MonoRollBot complete robot design and details. The motivation for this paper is to investigate robots with multiple DoFs that operate using a single actuator and compliant mechanisms. We introduce a novel compliant underactuated robot, MonoRollBot as shown in Fig. 1, designed to achieve 3-DoF spherical motion, marking one of the first examples of its kind. The robot features a compliant rotating mass mechanism that facilitates effective spring-mass manipulation for movement generation, thereby reducing the need for multiple actuators. We also propose an estimation method to track the position of the rotating mass inside the rolling robot using a motor encoder and IMU data. Additionally, we conduct a detailed motion analysis focusing on two key factors: the mass of the internal rotating element and the stiffness of the spring mechanism. By systematically varying these parameters, we examine the robot‚Äôs dynamic performance, providing insights into how variations in mass and stiffness influence its motion capabilities. The paper is organized as follows: Section II discusses the design of MonoRollBot, including the mathematical modeling of the robot, and introduces estimation methods for tracking the rotating mass. Section III presents motion studies analyzing the impact of mass and stiffness variations on the robot‚Äôs motion performance. Figure 2: Actuating mechanism for 3DoF motion locomotion by MonoRollBot."
https://arxiv.org/html/2411.04246v1,Learning Generalizable Policy for Obstacle-Aware Autonomous Drone Racing,"Autonomous drone racing has gained attention for its potential to push the boundaries of drone navigation technologies. While much of the existing research focuses on racing in obstacle-free environments, few studies have addressed the complexities of obstacle-aware racing, and approaches presented in these studies often suffer from overfitting, with learned policies generalizing poorly to new environments. This work addresses the challenge of developing a generalizable obstacle-aware drone racing policy using deep reinforcement learning. We propose applying domain randomization on racing tracks and obstacle configurations before every rollout, combined with parallel experience collection in randomized environments to achieve the goal. The proposed randomization strategy is shown to be effective through simulated experiments where drones reach speeds of up to 70 km/h, racing in unseen cluttered environments. This study serves as a stepping stone toward learning robust policies for obstacle-aware drone racing and general-purpose drone navigation in cluttered environments. Code is available at https://github.com/ErcBunny/IsaacGymEnvs.","Autonomous drone navigation has emerged as a critical area of research, driven by the growing demand for drones in industries such as delivery, inspection, and emergency response. Drone racing, with its emphasis on minimum-time navigation, has become a benchmark task for testing advanced autonomous systems aiming to navigate at high speeds while avoiding obstacles in partially or fully unknown environments. Drone racing originally began as a competitive sport where human pilots control agile drones via radio to fly through a racing track as fast as possible while avoiding potentially present obstacles. This requires precision, quick reflexes, and expert navigation skills. In autonomous drone racing, human pilots are replaced by algorithms and artificial intelligence (AI). This introduces the challenge for algorithms and AI of matching human-level performance and adaptability. There have been several global autonomous drone racing events, including the 2016-2019 IROS Autonomous Drone Racing (ADR) competitions [1, 2], the 2019 AlphaPilot Challenge [3, 4], the 2019 NeurIPS Game of Drones [5], and the 2022-2023 DJI RMUA UAV Challenges [6, 7]. The tracks in early competitions, such as the IROS ADR, AlphaPilot, and Game of Drones, are situated in less cluttered spaces, allowing drones to complete the tracks without considering obstacle avoidance. However, for tracks in cluttered environments, such as those in the more recent DJI RMUA Challenges, the absence of obstacle awareness could cause crashing. Additionally, in human-piloted drone racing, such as the Drone Racing League competitions, and in drone racing video games, there are plenty of tracks that require obstacle avoidance. Although autonomous drone racing has received significant attention, much of the research has been limited to obstacle-free scenarios [8]. Obstacle-free scenarios do not reflect the complexities encountered in real-world tasks where obstacle avoidance is necessary. Recognizing this, researchers have been exploring ways to integrate drone racing with obstacle avoidance through various approaches. Path-planning and optimization-based approaches can achieve short lap times [9] and strike a balance between lap times and computational efficiency [10, 11, 6], but rely on carefully designed algorithms and may experience performance degradation when model mismatches occur. Current learning-based approaches [12, 13] leverage reinforcement learning (RL) and imitation learning (IL) to train neural policies capable of making low-latency decisions that result in aggressive and collision-free trajectories. However, these policies do not generalize well to new racing tracks or different obstacle configurations. Figure 1: Trajectories of successful rollouts of a single policy on multiple different racing tracks with obstacles designed to block flight paths. This paper aims to enhance the generalization ability of learned policies. Specifically, the goal is to develop a single policy capable of navigating a quadcopter through various racing tracks with obstacles, without requiring additional tuning after training. Drawing inspiration from works on learning drone navigation in cluttered environments [14, 15] and generalizable obstacle-free drone racing [16], which all involve training the policy in multiple randomized environments, we propose applying the same strategy, domain randomization [17] over racing tracks, to expose the agent to a diverse set of environments. This allows the policy to learn the underlying navigation ‚Äúskills‚Äù while not relying on unique observations associated with one or a few training environments. Simulated experiments verify that the resulting policy can indeed generalize to unseen racing tracks while avoiding obstacles in unseen sizes and shapes. Several successful examples are shown in Fig 1. In summary, the main contributions of this study are: ‚Ä¢ We verify the effectiveness of applying domain randomization to encourage the learning of generalizable skills. ‚Ä¢ We present the first generalizable neural policy for the obstacle-aware drone racing task, where the policy directly maps observations to low-level commands. ‚Ä¢ We open-source tools and reusable modules to facilitate research and development in both obstacle-free and obstacle-aware drone racing."
https://arxiv.org/html/2411.05005v1,Diff-2-in-1: Bridging Generation and Dense Perception with Diffusion Models,"Beyond high-fidelity image synthesis, diffusion models have recently exhibited promising results in dense visual perception tasks. However, most existing work treats diffusion models as a standalone component for perception tasks, employing them either solely for off-the-shelf data augmentation or as mere feature extractors. In contrast to these isolated and thus sub-optimal efforts, we introduce a unified, versatile, diffusion-based framework, Diff-2-in-1, that can simultaneously handle both multi-modal data generation and dense visual perception, through a unique exploitation of the diffusion-denoising process. Within this framework, we further enhance discriminative visual perception via multi-modal generation, by utilizing the denoising network to create multi-modal data that mirror the distribution of the original training set. Importantly, Diff-2-in-1 optimizes the utilization of the created diverse and faithful data by leveraging a novel self-improving learning mechanism. Comprehensive experimental evaluations validate the effectiveness of our framework, showcasing consistent performance improvements across various discriminative backbones and high-quality multi-modal data generation characterized by both realism and usefulness.","Diffusion models have emerged as powerful generative modeling tools for various high-fidelity image synthesis tasks (Song et al., 2021; Ho et al., 2020; Rombach et al., 2022; Zhang et al., 2023b). Beyond their primary synthesis capabilities, diffusion models are increasingly recognized for their expressive representation abilities. This has spurred interest in leveraging them for dense pixel-level visual perception tasks, such as semantic segmentation (Baranchuk et al., 2022; Wu et al., 2023; Xu et al., 2023a) and depth estimation (Saxena et al., 2023b; Zhao et al., 2023). Nonetheless, most existing approaches treat diffusion models as a standalone component for perception tasks, either employing them for off-the-shelf data augmentation (Burg et al., 2023), or utilizing the diffusion network as feature extraction backbone (Xu et al., 2023a; Zhao et al., 2023; Ji et al., 2023; Saxena et al., 2023a). These efforts overlook the unique diffusion-denoising process inherent in diffusion models, thus limiting their potential for discriminative dense visual perception tasks. Inspired by foundational studies that explore the interplay between generative and discriminative learning (Rubinstein & Hastie, 1997; Ng & Jordan, 2001; Raina et al., 2003; Ulusoy & Bishop, 2005), we argue that the diffusion-denoising process plays a critical role in unleashing the capability of diffusion models for the discriminative visual perception tasks. The diffusion process corrupts the visual input with noise, enabling the generation of abundant new data with diversity. Subsequently, the denoising process removes the noise from noisy images to create high-fidelity data, thus obtaining informative features for discriminative tasks at the same time. As a result, the diffusion-denoising process naturally connects the generative process with discriminative learning. Interestingly, this synergy further motivates us to propose a novel unified diffusion modeling framework that integrates both discriminative and generative learning within a single, coherent paradigm. From the generative perspective, we focus on synthesizing photo-realistic multi-modal paired data (i.e., RGB images and their associated pixel-level visual attributes) that accurately capture various types of visual information. Simultaneously, the unified diffusion model can achieve promising results in different visual prediction tasks from the discriminative standpoint. As an example illustrated in Figure 1, when considering RGB and depth interactions, if the model receives an RGB image as input, its function is to predict an accurate depth map. Meanwhile, the model is equipped to produce photo-realistic and coherent RGB-depth pairs sampled from noise. Despite its conceptual simplicity, fully operationalizing the unified framework ‚Äì acquiring enhanced performance for both multi-modal generation and dense perception such as by effectively leveraging generated samples for discriminative tasks ‚Äì presents non-trivial challenges. In particular, the generation process inevitably produces data of relatively inferior quality compared to real data. Additionally, generated samples may exhibit considerable data distribution gaps from the target domain. Figure 1: A single, unified diffusion-based model for both generative and discriminative learning. If the model receives an RGB image as input, its function is to predict an accurate visual attribute map. Simultaneously, the model is equipped to produce photo-realistic and coherent multi-modal data sampled from Gaussian noise. We use depth as an example here for illustration, and the framework is also applicable to other visual attributes such as segmentation, surface normal, etc. To address these challenges, we introduce Diff-2-in-1, a diffusion framework bridging multi-modal generation and discriminative dense visual perception within one unified diffusion model. The core design within our Diff-2-in-1 is a self-improving learning mechanism, featuring two sets of parameters for our unified diffusion model during the training process. Specifically, the creation parameters are tailored to generate additional multi-modal data for discriminative learning, while the exploitation parameters are employed for utilizing both the original and synthetic data to learn the discriminative dense visual perception task. Meanwhile, the creation parameters continuously undergo self-improvement based on the weights of the exploitation parameters via exponential moving average (EMA). With our novel design of two sets of parameters interplaying with each other, the discriminative learning process can benefit from the synthetic samples generated by the model itself, while the quality of the generated data is iteratively refined at the same time. We validate the effectiveness of Diff-2-in-1 through extensive and multi-faceted experimental evaluations. We start with the evaluation of the discriminative perspective, demonstrating its superiority over state-of-the-art discriminative baselines across various tasks in both single-task and multi-task settings. We additionally show that Diff-2-in-1 is generally applicable to different backbones and consistently boosts performance. Next, we ablate the experimental settings such as different training data sizes, to gain a comprehensive understanding of our method. Finally, we demonstrate the realism and usefulness of the multi-modal data generated by our Diff-2-in-1. Our contributions include: (1) We propose Diff-2-in-1, a unified framework that seamlessly integrates multi-modal generation and discriminative dense visual perception based on diffusion models. (2) We introduce a novel self-improving mechanism that progressively enhances multi-modal generation in a self-directed manner, thereby effectively boosting the discriminative visual perception performance via generative learning. (3) Our method demonstrates consistent performance improvements across various discriminative backbones and high-quality multi-modal data generation under both realism and usefulness."
https://arxiv.org/html/2411.04987v1,Few-Shot Task Learning throughInverse Generative Modeling,"Learning the intents of an agent, defined by its goals or motion style, is often extremely challenging from just a few examples. We refer to this problem as task concept learning and present our approach, Few-Shot Task Learning through Inverse Generative Modeling (FTL-IGM), which learns new task concepts by leveraging invertible neural generative models. The core idea is to pretrain a generative model on a set of basic concepts and their demonstrations. Then, given a few demonstrations of a new concept (such as a new goal or a new action), our method learns the underlying concepts through backpropagation without updating the model weights, thanks to the invertibility of the generative model. We evaluate our method in five domains ‚Äì object rearrangement, goal-oriented navigation, motion caption of human actions, autonomous driving, and real-world table-top manipulation. Our experimental results demonstrate that via the pretrained generative model, we successfully learn novel concepts and generate agent plans or motion corresponding to these concepts in (1) unseen environments and (2) in composition with training concepts.","The ability to learn concepts about a novel task, such as the goal and motion plans, from a few demonstrations is a crucial building block for intelligent agents ‚Äì it allows an agent to learn to perform new tasks from other agents (including humans) from little data. Humans, even from a young age, can learn various new tasks from little data and generalize what they learned to perform these tasks in new situations [1]. In machine learning and robotics, this class of problems is referred to as Few-Shot Learning [2]. Despite being a widely studied problem, it remains unclear how we can enable machine learning models to learn concepts of a novel task from only a few demonstrations and generalize the concepts to new situations, just like humans do. Common approaches learn policies either directly, which often suffer from covariate shift [3], or via rewards [4, 5, 6], which are largely limited to previously seen behavior [7]. In a different vein, other work has relied on pretraining on task families and assumes that task learning corresponds to learning similar tasks to ones already seen in the task family [8, 9]. Inspired by the success of generative modeling in few-shot visual concept learning [10, 11, 12], where concepts are latent representations, in this work, we investigate whether and how few-shot task concept learning can benefit from generative modeling as well. Learning concepts from sequential demonstrations rather than images is by nature more challenging due to sequential data often not satisfying the i.i.d. assumption in machine learning [13]. In particular, we assume access to a large pretraining dataset of paired behaviors and task representations to learn a conditional generative model that synthesizes trajectories conditioned on task descriptions. We hypothesize that by learning a generative model conditioned on explicit representations of behavior, we can acquire strong priors about the nature of behaviors in these domains, enabling us to more effectively learn new behavior that is not within the pretraining distribution, given a limited number of demonstrations, and further generate the learned behavior in new settings. Figure 1: Few-shot concept learning. Given paired task demonstration œÑùúè\tauitalic_œÑ (e.g., ‚Äòwalk‚Äô) and concept cùëêcitalic_c (a latent representation of the task), we train a generative model ùí¢Œ∏subscriptùí¢ùúÉ\mathcal{G}_{\theta}caligraphic_G start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT to generate behavior from a concept. Then, given demonstrations of a new behavior œÑ~~ùúè\tilde{\tau}over~ start_ARG italic_œÑ end_ARG (e.g., ‚Äòjumping jacks‚Äô) without its concept label, we aim to learn its concept representation by optimizing concept c~~ùëê\tilde{c}over~ start_ARG italic_c end_ARG as input to frozen ùí¢Œ∏subscriptùí¢ùúÉ\mathcal{G}_{\theta}caligraphic_G start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT. Figure 2: Experiment Domains. We extensively evaluate our approach for various domains. To this end, we propose Few-Shot Task Learning through Inverse Generative Modeling (FTL-IGM). In our approach, we first pretrain a large conditional generative model which synthesizes different trajectories conditioned on different task descriptions. To learn new tasks from a limited number of demonstrations, we then formulate few-shot task learning as an inverse generative modeling problem, where we find the latent task description, which we refer to as a concept, which maximizes the likelihood of generating the demonstrations. This approach allows us to leverage the powerful task priors learned by the generative model to learn the shared concepts between demonstrations without finetuning the model (Figure 1). We demonstrate this approach in various domains: object rearrangement, where concepts are relations between objects, goal-oriented navigation, where concepts are target attributes, motion capture, where concepts are human actions, autonomous driving, where concepts are driving scenarios, and real-world table-top manipulation where concepts are manipulation tasks (Figure 2). New concepts are either (1) compositions of training concepts (e.g., multiple desired relations between objects that define a new object rearrangement concept) or (2) new concepts that are not explicit compositions in the natural language symbolic space of training concepts (e.g., a new human motion ‚Äòjumping jacks‚Äô is not an explicit composition of training concepts ‚Äòwalk‚Äô, ‚Äògolf‚Äô etc.). Thanks to generative models‚Äô compositional properties that enable compositional concept learning [14], in addition to being able to learn a single concept from demonstrations directly, FTL-IGM learns compositions of concepts from demonstrations that, when combined, describe the new concept. We show that our approach generates diverse trajectories encapsulating the learned concept. We achieve this due to two properties of generative models. First, these models have shown strong interpolation abilities [15, 16], which allow generating the new concept on new initial states they were not demonstrated from. Second, these models have compositional properties that enable compositional trajectory generation [17], which allow composing learned concepts with training concepts to synthesize novel behavior that was not demonstrated (e.g., ‚Äòjumping jacks‚Äô and ‚Äòwalk‚Äô), see Figure 3. We further demonstrate that our approach addresses a unique challenge introduced in learning task concepts: we utilize plans generated by learned concepts in a closed-loop fashion. Our main contributions are (1) formulating the problem of task learning from few demonstrations as Few-Shot Task Learning through Inverse Generative Modeling (FTL-IGM), (2) adapting a method for efficient concept learning to this problem based on the new formulation, and (3) a systematic evaluation revealing the ability of our method to learn new concepts across a diverse set of domains."
https://arxiv.org/html/2411.04466v1,Enabling Adaptive Agent Trainingin Open-Ended Simulatorsby Targeting Diversity,"The wider application of end-to-end learning methods to embodied decision-making domains remains bottlenecked by their reliance on a superabundance of training data representative of the target domain. Meta-reinforcement learning (meta-RL) approaches abandon the aim of zero-shot generalization‚Äîthe goal of standard reinforcement learning (RL)‚Äîin favor of few-shot adaptation, and thus hold promise for bridging larger generalization gaps. While learning this meta-level adaptive behavior still requires substantial data, efficient environment simulators approaching real-world complexity are growing in prevalence. Even so, hand-designing sufficiently diverse and numerous simulated training tasks for these complex domains is prohibitively labor-intensive. Domain randomization (DR) and procedural generation (PG), offered as solutions to this problem, require simulators to possess carefully-defined parameters which directly translate to meaningful task diversity‚Äîa similarly prohibitive assumption. In this work, we present DIVA, an evolutionary approach for generating diverse training tasks in such complex, open-ended simulators. Like unsupervised environment design (UED) methods, DIVA can be applied to arbitrary parameterizations, but can additionally incorporate realistically-available domain knowledge‚Äîthus inheriting the flexibility and generality of UED, and the supervised structure embedded in well-designed simulators exploited by DR and PG. Our empirical results showcase DIVA‚Äôs unique ability to overcome complex parameterizations and successfully train adaptive agent behavior, far outperforming competitive baselines from prior literature. These findings highlight the potential of such semi-supervised environment design (SSED) approaches, of which DIVA is the first humble constituent, to enable training in realistic simulated domains, and produce more robust and capable adaptive agents. Our code is available at https://github.com/robbycostales/diva.","1 Introduction Despite the broadening application of reinforcement learning (RL) methods to real-world problems [1, 2], generalization to new scenarios‚Äîones not explicitly supported by the training set‚Äîremains a fundamental challenge [3]. Meta-reinforcement learning (meta-RL), an extension of the RL framework, is formulated specifically for training adaptive agents, and is thus well-suited for overcoming these generalization gaps [4]. One recent work has demonstrated that meta-RL agents can be trained at scale to achieve adaptation capabilities on par with human subjects [5]. However, learning this human-like adaptive behavior naturally requires a large amount of data representative of the downstream (or target) distribution. For task distributions approaching real-world complexity‚Äîprecisely the ones of interest‚Äîdesigning each scenario by hand is prohibitively expensive. Figure 1: Highly structured environment simulators assume access to parameterizations ES‚Å¢(ùúΩ)subscriptùê∏SùúΩE_{\textnormal{S}}(\bm{\theta})italic_E start_POSTSUBSCRIPT S end_POSTSUBSCRIPT ( bold_italic_Œ∏ ) for which random seeds ùúΩisubscriptùúΩùëñ\bm{\theta}_{i}bold_italic_Œ∏ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT directly produce meaningfully diverse features (e.g. Racing tracks with challenging turns). Open-ended environments with flexible, unstructured parameterizations EU‚Å¢(ùúΩ)subscriptùê∏UùúΩE_{\textnormal{U}}(\bm{\theta})italic_E start_POSTSUBSCRIPT U end_POSTSUBSCRIPT ( bold_italic_Œ∏ )‚Äîthough enabling more complex emergent features‚Äîlack direct control over high-level features of interest. We introduce DIVA, an approach that effectively creates a more workable parameterization EQD‚Å¢(ùúΩ)subscriptùê∏QDùúΩE_{\textnormal{QD}}(\bm{\theta})italic_E start_POSTSUBSCRIPT QD end_POSTSUBSCRIPT ( bold_italic_Œ∏ ) by evolving levels beyond the minimally diverse population from EU‚Å¢(ùúΩ)subscriptùê∏UùúΩE_{\textnormal{U}}(\bm{\theta})italic_E start_POSTSUBSCRIPT U end_POSTSUBSCRIPT ( bold_italic_Œ∏ ). By training on these discovered levels, DIVA enables superior performance on downstream tasks. Prior works have explored the use of domain randomization (DR) and procedural generation (PG) techniques to produce diverse training data for learning agents [6]. Despite eliminating the need for hand-designing each task individually, human labor is still required to carefully design an environment generator that can produce diverse, high-quality tasks. As environments become more complex and open-ended, the ability to hand-design such a robust generator becomes increasingly infeasible. Some methods, like PLR [7], attempt to ameliorate this limitation by learning a curriculum over the generated levels, but these works still operate under the assumption that the generator produces meaningfully diverse levels with a high probability. Unsupervised environment design (UED) [8] are a broad class of appproaches which use performance-based metrics to adaptively form a curriculum of training levels. ACCEL [9], a state-of-the-art UED method, uses an evolutionary process to discover more interesting regions of the simulator‚Äôs parameter space (i.e. appropriately challenging tasks) than can be found by random sampling. While UED approaches are designed to be generally applicable and require little domain knowledge, they implicitly require a very constrained environment generator‚Äîone in which all axes of difficulty correspond to meaningful learning potential for the downstream distribution. Moreover, when faced with complex open-ended environments with arbitrary parameterizations, even ACCEL is not able to efficiently explore the solution space, as it is still bottlenecked by the speed of agent evaluations. In this work, we introduce DIVA, an approach for generating diverse training tasks in open-ended simulators to train adaptive agents. By using quality diversity (QD) optimization to efficiently explore the solution space, DIVA bypasses the problem of needing to evaluate agents on all generated levels. QD also enables fine-grained control over the axes of diversity to be captured in the training tasks, allowing the flexible integration of task-related prior knowledge from both domain experts and learning approaches. We demonstrate that DIVA, with limited supervision in the form of feature samples from the target distribution, significantly outperforms state of the art UED approaches‚Äîdespite the UED approaches being provided with significantly more interactions. We further show that UED techniques can be integrated into DIVA. Preliminary results with this combination (which we call DIVA+) are promising, and suggest an exciting avenue for future work."
https://arxiv.org/html/2411.04112v1,Fed-EC: Bandwidth-Efficient Clustering-Based Federated Learning For Autonomous Visual Robot Navigation,"Centralized learning requires data to be aggregated at a central server, which poses significant challenges in terms of data privacy and bandwidth consumption. Federated learning presents a compelling alternative, however, vanilla federated learning methods deployed in robotics aim to learn a single global model across robots that works ideally for all. But in practice one model may not be well suited for robots deployed in various environments. This paper proposes Federated-EmbedCluster (Fed-EC), a clustering-based federated learning framework that is deployed with vision based autonomous robot navigation in diverse outdoor environments. The framework addresses the key federated learning challenge of deteriorating model performance of a single global model due to the presence of non-IID data across real-world robots. Extensive real-world experiments validate that Fed-EC reduces the communication size by 23x for each robot while matching the performance of centralized learning for goal-oriented navigation and outperforms local learning. Fed-EC can transfer previously learnt models to new robots that join the cluster.","Poor availability of high-speed internet is limiting outdoor robots from realizing their full potential. In today‚Äôs world, robots are seamlessly deployed in diverse conditions all over the world, from bustling urban landscapes to rugged terrains in the wild. Many of these robots are using visually guided autonomy architectures powered by machine learning and self-supervision. Recent works [1], [2], [3], [4], [5] have shown that with access to large amounts of data, robots can achieve state-of-the-art navigation performance and can be deployed in various scenarios with minimum human intervention needed. These and other such methods are driving tremendous progress in self-driving cars [6], [7], robots navigation in indoor [8], [9] and outdoor environments [10], [11], [12]. However, in practice, traditional learning approaches require access to all of the data in one place, uploaded to a central server for model training requiring high speed internet. Furthermore, robots operating in the world experience diverse and varied environments requiring continuous upload of large amounts of data to the central server. While effective in controlled environments with high bandwidths, uploading big chunks of data can be a challenge for robots in environments where high-speed internet is not available, is intermittent, outright denied or even leads to significant battery power consumption. Federated learning (FL) [13] reduces the bandwidth requirement while enabling these robots to collectively enhance their learning by sharing model updates. With recent advancements in the capabilities of edge devices, federated learning takes advantage of edge computation to train models locally and shares model parameters instead of raw data with the server to learn a shared global model. FL also allows robots to send updates at intervals, rather than continuously streaming data reducing bandwidth usage. Further, through federated learning, there is hope that robots can gather insights from their respective environments, while also contributing to a global pool of knowledge to learn adaptable models for varied environments on the go. Traditionally, FL learns a single model that tries to minimize the average loss across robots. However, local data on deployed robots is highly non-IID due to different usage and operating locations. During FL, the divergence of the local datasets due to their non-IID nature leads to slower convergence and worsening learning performance when the models are aggregated. In such cases, a singular global model suffers and may perform worse than local models for some robots. With non-IID data, it is improbable that there exists a single global model that fits the needs for all robots. The global model can be biased and unfair. Current robotic systems that use federated learning frameworks do so in simulation [14] or in structured indoor environments [15] and do not account for heterogeneity that arises in the real-world deployment of robots. One way to avoid biased global models is to learn personalized models by clustering robots with similar local data distributions and training one aggregate model for each cluster. As a result, robots collaborate with only robots with similar experiences avoiding biases and negative performance. Previous clustered FL methods compare local model weights or gradients that rely on indirect information of the data distribution. [16] and [17] cluster the clients and learn individual cluster models but incur a high communication cost in doing so. In this paper, we highlight the first clustering-based system, Federated-EmbedCluster(Fed-EC) for self-supervised visually guided autonomous navigation which overcomes the need for high bandwidth speeds. Fed-EC is deployed on two different visual navigation models to showcase its modularity. To overcome the negative affect of non-IID data on model performance, Fed-EC groups the aggregation of local models by looking at similarity between the local datasets. Within each cluster group, the data is similar and mimics an IID set up ensuring that model aggregation does not degrade performance. Unlike previous methods where multiple rounds are needed [16] or multiple models are communicated [17], in each communication round the mean embedding vector which does not incur any additional communication cost is shared along with the local model. Fed-EC does not know the cluster identities beforehand and hence simultaneously identifies clusters within participating robots and learns individual cluster models in the federated setting. Figure 1: Workflow of Fed-EC. (a) Participating robots learn and communicates a mean embedding vector and local model weight to the server. The server clusters the robots using the mean embedding vector and aggregates local models in each cluster to learn a model which is shared with the robots based on their cluster identity. (b) The robots navigate to the a given GPS goal using the learnt model which takes as input RGB and depth images from the front facing camera. (c) If a new robot is deployed it computes a mean embedding and shares it with the server. The server assigns a cluster to the robot and sends the respective cluster model to the robot to use. In this paper, we consider robots in the wild that are constantly deployed with limited hardware on board, limited communication bandwidth, and battery power. The main contributions of our papers are as follows: ‚Ä¢ We propose a clustering-based personalized FL strategy Fed-EC, to overcome the problems generated by the heterogeneous nature of robotic operations. ‚Ä¢ We implement and test the framework of Federated learning in the robotics settings, in particular on real robots using two different navigation models to navigate to a given GPS point. ‚Ä¢ We validate through real-world robot experiments in diverse outdoor terrains that Fed-EC can perform as well as the centralized framework while reducing communication size and is better than just local training. We also show that learning a personalized FL model for each cluster is better than learning a singular global FL model over all robots. ‚Ä¢ We also show the transferability properties of our system to new robots that join the network."
https://arxiv.org/html/2411.04073v1,Rescheduling after vehicle failures in the multi-depot rural postman problem with rechargeable and reusable vehicles,"We present a centralized auction algorithm to solve the Multi-Depot Rural Postman Problem with Rechargeable and Reusable Vehicles (MD-RPP-RRV), focusing on rescheduling arc routing after vehicle failures. The problem involves finding a heuristically obtained best feasible routes for multiple rechargeable and reusable vehicles with capacity constraints capable of performing multiple trips from multiple depots, with the possibility of vehicle failures. Our algorithm auctions the failed trips to active (non-failed) vehicles through local auctioning, modifying initial routes to handle dynamic vehicle failures efficiently. When a failure occurs, the algorithm searches for the best active vehicle to perform the failed trip and inserts the trip into that vehicle‚Äôs route, which avoids a complete rescheduling and reduces the computational effort. We compare the algorithm‚Äôs solutions against offline optimal solutions obtained from solving a Mixed Integer Linear Programming (MILP) formulation using the Gurobi solver; this formulation assumes that perfect information about the vehicle failures and failure times are given. We derived a set of 257 failure scenarios from arc routing instances in the literature and used them to perform a competitive analysis. For each scenario we used a simulated annealing algorithm to generate an initial set of routes and then used the centralized auction algorithm to reschedule after each vehicle failure. The results demonstrate that the centralized auction algorithm produces solutions that are, in some cases, near-optimal; moreover the execution time for the proposed approach is much more consistent and is, for some instances, orders of magnitude less than the execution time of the Gurobi solver. The theoretical analysis provides an upper bound for the competitive ratio and computational complexity of our algorithm, offering a formal performance guarantee in dynamic failure scenarios.","Unmanned battery-operated rechargeable vehicles are becoming more prevalent in real-world applications due to their cost-effectiveness and efficiency [1, 2]. However, these systems still face significant challenges. The failure rate for drones is approximately 1 in 1,000 flight hours, two orders of magnitude higher than commercial aviation‚Äôs 1 in 100,000 flight hours, and sophisticated Unmanned Aerial Vehicle (UAV) systems face an overall failure rate of 25% [3]. These failures can lead to significant delays and disruptions, underscoring the need for improved reliability in unmanned vehicle operations. Although many preventive maintenance approaches have been proposed to increase the reliability of unmanned vehicles [3, 4], any failure during a mission requires changing the mission plan to react to the loss of the vehicle. This paper, therefore, proposes an approach for effectively managing and mitigating the impact of vehicle failures on routing after they occur, specifically addressing the challenges of rerouting and task reallocation to ensure mission completion despite unexpected vehicle breakdowns. In the MD-RPP-RRV, the vehicles have limited capacity (operation time) but can be recharged and reused for multiple trips from multiple depots to traverse a subset of edges (required edges) in a weighted undirected connected graph, minimizing mission time. The maximum time taken by vehicles to traverse all required edges is referred to as mission time or maximum trip time. One of the key assumptions considered in our previous study [5] to solve the MD-RPP-RRV was that vehicles do not fail during their trips. In this study of the MD-RPP-RRV with vehicle failures, we relax that assumption and consider that multiple (but not all) vehicles might fail randomly during their trips. This study developed and evaluated a rescheduling approach that reacts to vehicle failures; it requires no information about the vehicle failures before the vehicles begin following their routes. Studying the MD-RPP-RRV with vehicle failures is crucial for addressing real-world challenges in applications like parcel delivery, infrastructure inspection, and surveillance, where unmanned vehicles may encounter failures during operation, necessitating the development of quick rerouting approaches for remaining active vehicles to ensure mission completion. The MD-RPP-RRV is NP-hard to solve [5] as it generalizes the RPP, which is proven to be NP-hard [6]. In its simplest case, with a single depot and single trip, the MD-RPP-RRV reduces to the RPP. Hence, solving the MD-RPP-RRV with vehicle failures poses significant computational challenges due to the additional complexities introduced by random vehicle failures. This paper proposes a centralized auction algorithm to address the MD-RPP-RRV with vehicle failures. We chose a centralized approach over decentralized methods due to its ability to maintain a global perspective, enabling quicker decision-making and more efficient task reallocation essential for handling multiple random vehicle failures dynamically. Our approach efficiently reassigns trips that a failed vehicle was supposed to complete (considered as tasks) to the remaining active vehicles (considered as agents) with the objective of minimizing the increase in mission time. To evaluate the performance of our proposed algorithm, we compared its solutions against offline optimal solutions obtained from solving the Mixed Integer Linear Programming (MILP) formulation using the Gurobi optimizer with vehicle failures known beforehand. We also empirically and theoretically analyzed the competitive ratio to assess the proposed algorithm‚Äôs solution quality relative to the offline optimal solution. The main contributions of this paper are the following: 1. A centralized auction algorithm that reformulates the MD-RPP-RRV as a variant of the Generalized Assignment Problem (GAP) [25]. This approach efficiently handles dynamic vehicle failures by reassigning tasks without complete rerouting, reducing computational complexity. Our method addresses a gap in existing auction approaches [34] by applying them to dynamic failures in arc routing problems, specifically the MD-RPP-RRV, which has not been previously explored in this context. This algorithm extends centralized auction methods [36, 37] to handle more complex task allocation scenarios in the MD-RPP-RRV context in the following ways: (a) Assigning multiple failed trips (tasks) to a single vehicle (agent). (b) Dynamically reallocating trips from failed vehicles to active ones during the mission, thereby adapting to changes in the available vehicle fleet size due to failures. This approach addresses limitations in existing methods that typically assign only one task per agent or assume a fixed number of agents throughout the mission. 2. Experimental results that describe the quality of the solutions that the approach generates and the execution time required. 3. A theoretical upper bound for the competitive ratio of our proposed centralized auction algorithm to solve the MD-RPP-RRV with vehicle failures. This analysis provides a formal performance guarantee for our algorithm in dynamic failure scenarios. The remainder of this paper is organized as follows: Section 2 presents a literature review of related works. Section 3 provides the assumptions and presents a MILP formulation for the MD-RPP-RRV with known vehicle failures and failure times. Section 4 describes the proposed centralized auction algorithm. Section 5 presents our experimental results. It details the testing of our proposed algorithm on failure scenarios created from benchmark instances. It also compares the quality of the solutions with offline optimal solutions that were obtained by solving the MILP formulation using the Gurobi solver. Section 6 concludes the paper."
https://arxiv.org/html/2411.04056v1,Problem Space Transformations forGeneralisation in Behavioural Cloning,"The combination of behavioural cloning and neural networks has driven significant progress in robotic manipulation. As these algorithms may require a large number of demonstrations for each task of interest, they remain fundamentally inefficient in complex scenarios. This issue is aggravated when the system is treated as a black-box, ignoring its physical properties. This work characterises widespread properties of robotic manipulation, such as pose equivariance and locality. We empirically demonstrate that transformations arising from each of these properties allow neural policies trained with behavioural cloning to better generalise to out-of-distribution problem instances.","The behavioural cloning (BC) paradigm has been the foundation of recent advances in robotic manipulation [1, 2]. BC is particularly promising for robot manipulation, as humans are very proficient in general manipulation, and can quickly learn to collect demonstrations when given a well-designed interface [3]. An important benefit of using this data to train a robot policy is that it can be collected on the real system, thus avoiding the sim-to-real gap. However, as a supervised learning method, BC requires the collected data to cover the workspace with relatively high density [4, 5, 6]. Neural networks trained with BC, and more generally functions estimated through supervised learning, hardly generalise outside the support of the training data, i.e. ‚Äùout-of-distribution‚Äù (OOD) [7, 8]. Policy prediction for OOD states can be arbitrary, which poses a safety risk. Avoiding OOD states by providing sufficient data coverage can quickly become infeasible. This is particularly aggravating for robotic manipulation, as collection of human demonstrations remains time intensive and thus expensive [2]. This work highlights and leverages practical assumptions on object-centric manipulation tasks, and explores a family of problem space transformations that enable OOD generalisation with respect to the original problem space. We observe that these transformations are a crucial design component for learning-based control of manipulators, and enable policies learned through simple BC to perform well on OOD states. We present three main contributions: (i) we determine properties underlying practical manipulation problems; (ii) we describe several transformations of the problem space that embed these properties; (iii) we provide experimental results demonstrating that the choice of problem space transformation drastically impacts the ability of OOD generalisation for three robotic manipulation tasks."
https://arxiv.org/html/2411.04050v1,Memorized action chunking with Transformers: Imitation learning for vision-based tissue surface scanning,"Optical sensing technologies are emerging technologies used in cancer surgeries to ensure the complete removal of cancerous tissue. While point-wise assessment has many potential applications, incorporating automated large-area scanning would enable holistic tissue sampling. However, such scanning tasks are challenging due to their long-horizon dependency and the requirement for fine-grained motion. To address these issues, we introduce Memorized Action Chunking with Transformers (MACT), an intuitive yet efficient imitation learning method for tissue surface scanning tasks. It utilises a sequence of past images as historical information to predict near-future action sequences. In addition, hybrid temporal-spatial positional embeddings were employed to facilitate learning. In various simulation settings, MACT demonstrated significant improvements in contour scanning and area scanning over the baseline model. In real-world testing, with only 50 demonstration trajectories, MACT surpassed the baseline model by achieving a 60‚àí80%60percent8060\!-\!80\%60 - 80 % success rate on all scanning tasks. Our findings suggest that MACT is a promising model for adaptive scanning in surgical settings.","I INTRODUCTION Completely resecting the tumour while preserving maximal healthy tissue is one of the most critical objectives for surgery. This could lead to minimal local recurrence and prolong disease-free survival. To ensure optimal results, it is necessary to establish a negative circumferential resection margin to guarantee that no tumour is present at the edges of the removed specimen. Thus, there is a pressing demand for in vivo tissue assessment tools capable of characterising tissue in real time. Recently, several optical sensing technologies have emerged to facilitate real-time, non-invasive tissue classification, including Diffuse Reflectance Spectroscopy (DRS) [1, 2], Raman spectroscopy [3], and confocal endomicroscopy [4]. These techniques rely on contact-based sensing instruments which require positioning of the sensing tip at the interested point to assess the tissue. They are usually designed as hand-held probes, allowing manual positioning at the target tissue by the operating surgeon. Their acquisition protocol usually requires steady contact to obtain high quality data. Despite their success on single-site sampling, their utility on large-area sampling in vivo has not been realised. For instance, manual scanning of the target area with the DRS probe may lead to high variance in the analysed results because of the non-ergonomic control, unsteady movement, and variable contact pressure [5]. An automated robot-assisted scanning system could be beneficial in extrapolating to large-area tissue scanning scenarios. Specialised control algorithms and setups may achieve this, but they require extensive hand-crafted engineering and constrained working conditions [6]. Imitation learning is capable to learn skills with minimal prior knowledge in achieving complex surgical scanning tasks. It can be used for learning the microscopic probe scanning preferences of experienced surgeons [7], learning ultrasound scanning for carotid artery examination from demonstrations [8], and learning drop-in gamma probe scanning in laparoscopic surgery [9]. However, these methods cannot be applied to this probe scanning task because of the format of the instrument readout and contact requirement. A more sophisticated neural network with a better state perception is a promising approach to solve this problem. Recent transformer-based models have demonstrated their capability to model long sequences. Action Chunking with Transformers (ACT) is an advanced Transformer-based architecture which integrates temporal ensemble mechanisms and employs a conditional VAE (CVAE) to model human demonstrations [10]. It outperforms previous imitation learning algorithms in learning from a limited number of human demonstrations in daily life tasks. Despite its potential in learning complex tasks, it does not generalise well to this tissue scanning task as it lacks awareness of the overall trajectory. This study introduces a framework for robot-assisted autonomous optical instrument scanning for in vivo abdominal tissue. We chose an exemplar application of phantom liver scanning, although the findings could be applied to other surgical scenarios. The probe model that was scanned across the surface could be substituted by another probe-based technique. This work has made the following main contributions: ‚Ä¢ The Memorized Action Chunking with Transformer (MACT) based on ACT [10] is proposed, which introduces a memory mechanism to utilise historical colour and depth images from the robot wrist camera to predict action sequence. ‚Ä¢ A hybrid temporal-spatial Positional Embedding is introduced to explicitly model the interrelations among transformer input tokens. ‚Ä¢ A robotic-assisted optical instrument scanning platform was built for human demonstration collection and autonomous scanning evaluation. ‚Ä¢ MACT was evaluated on both simulation and real-world setups and shown to outperform the baseline ACT among all tested tasks. Figure 1: Architecture of Memorized Action Chunk with Transformers (MACT). MACT buffered the past TùëáTitalic_T RGB and depth images from a mount-on camera for a ResNet-18 encoder to extract high-dimensional visual features. A Spatial PosEmb which was identical along the time horizon axis and Temporal PosEmb which was identical on the spatial axis were added to visual features to form image tokens. The transformer encoder took image tokens, current position, and CVAE style variables to produce encoded tokens to feed into a transformer decoder to predict the next KùêæKitalic_K action sequences. Temporal ensemble and action chunking further smoothed the predicted action sequence"
https://arxiv.org/html/2411.04046v1,Design and control of a robotic payload stabilization mechanism for rocket flights,"The use of parallel manipulators in aerospace engineering has gained significant attention due to their ability to provide improved stability and precision. This paper presents the design, control, and analysis of ‚Äú STEWIE ‚Äù, which is a three-degree-of-freedom (DoF) parallel manipulator robot developed by members of the thrustMIT rocketry team, as a payload stabilization mechanism for their sounding rocket, ‚ÄôAltair‚Äô. The goal of the robot was to demonstrate the attitude control of the parallel plate against the continuous change in orientation experienced by the rocket during its flight, stabilizing the payloads. At the same time, the high gravitational forces (G-forces) and vibrations experienced by the sounding rocket are counteracted. A novel design of the mechanism, inspired by a standard Stewart platform, is proposed which was down-scaled to fit inside a 4U CubeSat within its space constraints. The robot uses three micro servo motors to actuate the links that control the alignment of the parallel plate. In addition to the actuation mechanism, a robust control system for its manipulation was developed for the robot. The robot represents a significant advancement in the field of space robotics in the aerospace industry by demonstrating the successful implementation of complex robotic mechanisms in small, confined spaces such as CubeSats, which are standard form factors for large payloads in the aerospace industry.","The successful use of payloads in space missions depends largely on ensuring their stability and orientation control during the launch and spaceflight phases. Vibration motion, G-forces and dynamic disturbances caused by the rocket‚Äôs orientation during flight path can affect the integrity and functionality of the payload. To address these challenges, parallel manipulators have emerged as a promising solution, offering the potential to counteract these external perturbations and maintain precise alignment. The inspiration for the design of our robot STEWIE comes from the versatility and robustness of the six degrees-of-freedom (DoF) Stewart platform, which is widely used in various industrial and research applications. Our research aims to take advantage of the principles of the Stewart platform while simplifying the kinematic structure to create a three-degrees-of-freedom (DoF) parallel manipulator suitable for payload stabilization in thrustMIT‚Äôs sounding rocket: Altair. The main goal of this article is to present a comprehensive study of STEWIE‚Äôs design, development and performance characteristics. We address the kinematic analysis of the manipulator and illustrate its ability to provide three degrees-of-freedom for precise positioning of the payload during rocket ascent and spaceflight. In addition, we are investigating the integration of different types of joints and micro-servo motors into STEWIE‚Äôs structure to improve its parallel alignment with respect to the Earth‚Äôs surface. We present the results of the various structural simulations carried out on STEWIE to ensure that it can function effectively in the harsh conditions it will experience during its flight inside the rocket. We also propose a computationally in-expensive but robust controller which has high response speeds and high movement accuracy. The research results show that STEWIE has exceptional potential as an effective payload stabilization system for sounding rockets such as Altair. The ability of the manipulator to actively counteract vibrational movements, G-forces and orientation changes ensures the integrity and reliable functionality of payloads during critical phases of space missions. Furthermore, its compact, lightweight and simple design contributes to the rocket‚Äôs overall efficiency and improved payload capacity. I-A Contributions of Research The main contributions of this paper to the field of robotics and aerospace are the following:- ‚Ä¢ A novel low-complexity, pwm-controlled actuator based payload stabilization mechanism for rocket flights. ‚Ä¢ A computationally lightweight controller, demonstrating high accuracy and response time for critical robotic application especially in space industries. ‚Ä¢ A simple and robust custom tuning method for pid-controllers. ‚Ä¢ An open-source platform comprising mechanical design files (STEP format), details of electronic components, and real-time-operating-system (RTOS). I-B Oranisation of Paper The paper is organized as follows: a comprehensive study on related work and recent development in the the field of robotics, especially for aerospace applications and in payloads has been presented in section II. The design of the robot detailing its geometry, kinematics involved, various components used, actuation mechanism, along with communication interfaces, sensors and circuitry design are described in section III. A detailed description of the control architecture including the controllers used, are explained in section IV. A comprehensive description of the tuning methods for the controllers is given in section V. Analysis of the simulations, and actual rocket flight results are presented in section VI. Finally, we present our conclusion in section VII."
https://arxiv.org/html/2411.04006v1,Select2Plan: Training-Free ICL-Based Planningthrough VQA and Memory Retrieval,"This study explores the potential of off-the-shelf Vision-Language Models (VLMs) for high-level robot planning in the context of autonomous navigation. Indeed, while most of existing learning-based approaches for path planning require extensive task-specific training/fine-tuning, we demonstrate how such training can be avoided for most practical cases. To do this, we introduce Select2Plan (S2P), a novel training-free framework for high-level robot planning which completely eliminates the need for fine-tuning or specialised training. By leveraging structured Visual Question-Answering (VQA) and In-Context Learning (ICL), our approach drastically reduces the need for data collection, requiring a fraction of the task-specific data typically used by trained models, or even relying only on online data. Our method facilitates the effective use of a generally trained VLM in a flexible and cost-efficient way, and does not require additional sensing except for a simple monocular camera. We demonstrate its adaptability across various scene types, context sources, and sensing setups. We evaluate our approach in two distinct scenarios: traditional First-Person View (FPV) and infrastructure-driven Third-Person View (TPV) navigation, demonstrating the flexibility and simplicity of our method. Our technique significantly enhances the navigational capabilities of a baseline VLM of approximately 50% in TPV scenario, and is comparable to trained models in the FPV one, with as few as 20 demonstrations.","Path planning for vehicles is a longstanding problem in robotics, traditionally addressed using model-based or Reinforcement Learning (RL) approaches [1, 2, 3]. However, methods that directly learn from experience often struggle when confronted with ambiguous or unfamiliar scenarios. Interestingly, recent research has shown that Large Language Models (LLMs) and Vision-Language Models (VLMs), demonstrate surprising reasoning capabilities that can be adapted for proposing robot paths in arbitrary scenes [4]. Indeed, these models excel at incorporating common-sense reasoning acquired during their long pretraining phase [5]. This ability is crucial in robotics operations, where the deployment scenario rarely aligns perfectly with the training dataset [6, 7]. While methods like LoRA [8] reduce the computational cost of fine-tuning LLMs and VLMs, they still require domain-specific data, which can be costly to obtain. In parallel, In-Context Learning (ICL) and Retrieval-Augmented Generation (RAG) have shown promising results in scoping the ability of LLMs at deployment time with no additional fine-tuning, mitigating these costs. Figure 1: High-level demonstration of S2P in a TPV scenario. The robot must reach the red mark from its location, controlled solely via the external camera, shown in the figure. S2P proposes candidate keypoints ‚Äì in yellow ‚Äì and draws them into the original image before requesting a feasible trajectory to an off-the-shelf VLM. The latter will output a trajectory ‚Äì green ‚Äì as a sequence of keypoints, ideally yielding a trajectory that avoids obstacles ‚Äì e.g. 3 and 9. Our novel framework ‚Äì Select2Plan (S2P) ‚Äì combines Visual Question-Answering (VQA) and ICL with VLMs in a training-free manner, showing remarkable flexibility across various scenes, contexts, and setups. More specifically, we formulate the planning problem as a VQA task using visual prompting. A high-level overview of the approach can be observed in Fig. 1. Inspired by [9] and [10], we generate a set of position candidates in the image space and use them as part of a query mechanism to a VLM, to extract the next robot move. We combine this approach with ICL to enhance the model‚Äôs reliability: we retrieve similar successful samples and use them, along with the current annotated image, as context to support the model‚Äôs generalization. In this way, we can generate a robust path, which can span multiple planning steps within a single response, in contrast to the iterative approach taken by [9]. We evaluate our system in two different navigation scenarios. The first is a more traditional First-Person View (FPV), where the robot is equipped with a monocular camera and needs to reach specific objects in the scene. The challenge in this case is the sensor‚Äôs limited view, as the goal object might get out of view while the robot navigates the environment. As a second test-bench, we consider a robot controlled through eye-to-hand visual servoing, as in [11, 12] and as depicted in Fig. 1. Here, the camera is not physically attached to the robot, and the far viewpoint inherently limits the depth [13] and spatial resolution. However, given the widespread use of CCTV cameras, we believe this approach offers new opportunities and, interestingly, this setup also mirrors the type of data that VLMs are trained on ‚Äì static RGB images paired with textual descriptions ‚Äì making these models well-suited for tasks involving external camera navigation. We show how our setup can flexibly adapt to both visual inputs and diverse sources of context, such as videos from the Internet or even human traversal of the scenario. To summarise, our main contributions are: 1. A framework for planning and navigation using only RGB data, leveraging structured VQA, ICL and retrieval techniques to reduce the task-related data needed to just a handful of episodes. 2. The application of this framework to two separate scenarios: traditional FPV navigation and infrastructure-driven Third-Person View (TPV) navigation. 3. An extensive analysis of the impact of different sources of in-context examples on the system‚Äôs overall performance. Our empirical analysis demonstrates that our approach enhances the navigational abilities of VLMs without requiring further training, and lays the groundwork for more sophisticated and flexible planning in autonomous systems. To the best of our knowledge, our approach is the first that can seamlessly adapt to multiple setups and utilise multiple sources of in-context samples. (a) (b) Figure 2: Overview of the proposed approach in FPV (a) and TPV (b). The two settings are designed to fit two specific scenarios but share their components. The framework takes a live image from the onboard or a CCTV camera and retrieves similar images from the experiential memory. It is then annotated and passed, with the sampled images and an optional episodic memory, to the VLM to retrieve the next commands to send to the platform and explanations. The main difference is the absence of an Episodic Memory in the TPV setting, where the off-board sensing setup empirically limits its benefits. Alongside the overview, response examples are presented for both setups."
https://arxiv.org/html/2411.03990v1,ET-SEED:EfficientTrajectory-LevelSE(3)EquivariantDiffusion Policy,"Imitation learning, e.g., diffusion policy, has been proven effective in various robotic manipulation tasks. However, extensive demonstrations are required for policy robustness and generalization. To reduce the demonstration reliance, we leverage spatial symmetry and propose ET-SEED, an efficient trajectory-level S‚Å¢E‚Å¢(3)ùëÜùê∏3SE(3)\ italic_S italic_E ( 3 )equivariant diffusion model for generating action sequences in complex robot manipulation tasks. Further, previous equivariant diffusion models require the per-step equivariance in the Markov process, making it difficult to learn policy under such strong constraints. We theoretically extend equivariant Markov kernels and simplify the condition of equivariant diffusion process, thereby significantly improving training efficiency for trajectory-level S‚Å¢E‚Å¢(3)ùëÜùê∏3SE(3)\ italic_S italic_E ( 3 )equivariant diffusion policy in an end-to-end manner. We evaluate ET-SEED on representative robotic manipulation tasks, involving rigid body, articulated and deformable object. Experiments demonstrate superior data efficiency and manipulation proficiency of our proposed method, as well as its ability to generalize to unseen configurations with only a few demonstrations. Website: https://et-seed.github.io/","Imitation learning has achieved promising results for acquiring robot manipulation skills [1, 2, 3]. Though, one of the main challenges of imitation learning is that it requires extensive demonstrations to learn a robust manipulation policy [4, 5, 6]. Especially once the spatial pose of the object to be manipulated runs out of the demonstration distribution, the policy performance will easily decrease. Although some works seek to tackle these issues through data augmentation [7] or contrastive learning [8], they usually require task-specific knowledge or extra training, and without theoretical guarantee of spatial generalization ability. Another promising idea is to leverage symmetry. Symmetry is ubiquitous in the physical world, and many manipulation tasks exhibit a specific type of symmetry known as SE(3) Equivariance. S‚Å¢E‚Å¢(3)ùëÜùê∏3SE(3)\ italic_S italic_E ( 3 )is a group consisting of 3D rigid transformations. For example, as shown in fig. 1(b), a real robot arm is required to write characters ‚ÄúICLR‚Äù on a paper or fold a garment, when the pose of the paper or the garment changes, the manipulation trajectories of the end-effector should transform equivalently. Employing such symmetries into policy learning can not only improve the data efficiency but also increase the spatial generalization ability. Recent works on 3D manipulation have explored using SE(3) equivariance in the imitation learning process. Most of these works focus on equivariant pose estimation of the target object or end-effector [9, 10, 11]. Trajectory-level imitation learning has achieved state-of-the-art performances on diverse manipulation tasks [3, 12]. By generating a whole manipulation trajectory, this kind of method is capable to tackle more complex manipulation task beyond pick-and-place. For trajectory-level equivariance, Equivariant Diffusion Policy [13] and Equibot [14] propose equivariant diffusion process for robotic manipulation tasks. However, previous trajectory-level diffusion models for robotic manipulation have two key limitations. First, to maintain equivariance throughout the diffusion process, these models assume that every transition step must preserve equivariance. As we will show in section 3.1, training neural networks with equivariance is more challenging than neural networks with invariance, requiring additional computational resources and leading to slower convergence. This design constrains the model‚Äôs efficiency, making it hard for tackling complex long-horizon manipulation tasks. Second, these models define the diffusion process in Euclidean space, which is not a natural definition, and limits the expressiveness. Since the focus is on equivariant diffusion processes within the S‚Å¢E‚Å¢(3)ùëÜùê∏3SE(3)\ italic_S italic_E ( 3 )group, it is more natural to define both the diffused variables and the noise as elements of the S‚Å¢E‚Å¢(3)ùëÜùê∏3SE(3)\ italic_S italic_E ( 3 )group, which will lead to better convergence and multimodal distributions representation [15]. In this work, we propose ET-SEED, a new trajectory-level S‚Å¢E‚Å¢(3)ùëÜùê∏3SE(3)\ italic_S italic_E ( 3 )equivariant diffusion model for manipulation tasks. ET-SEED improves the sample efficiency and decreases the training difficulty by restricting the equivariant operations during the diffusion denoising process. We extend the equivaraint Markov kernels theory and prove that during the full denoising process, at least only one equivariant transition is required. Then, we integrate the diffusion process on S‚Å¢E‚Å¢(3)ùëÜùê∏3SE(3)\ italic_S italic_E ( 3 )manifold [16] and S‚Å¢E‚Å¢(3)ùëÜùê∏3SE(3)\ italic_S italic_E ( 3 )transformers [17] to design a new trajectory-level equivariant diffusion model on S‚Å¢E‚Å¢(3)ùëÜùê∏3SE(3)\ italic_S italic_E ( 3 )space. In experiment, we evaluate our method on several common and representative manipulation tasks, as shown in fig. 1(c), including rigid body manipulation (rotate triangle, open bottle cap), articulated object manipulation (open door), long-horizon tasks (robot calligraphy), and deformable object manipulation (fold and fling garment). Experiments show our method outperforms SOTA methods in terms of data efficiency, manipulation proficiency and spatial generalization ability (fig. 1(a)). Further, in real-world experiments, with only 20 demonstration trajectories, our method is able to generalize to unseen scenarios. In summary, our contributions are mainly as followed: ‚Ä¢ We propose ET-SEED, an efficient trajectory-level S‚Å¢E‚Å¢(3)ùëÜùê∏3SE(3)\ italic_S italic_E ( 3 )equivariant diffusion policy defined on S‚Å¢E‚Å¢(3)ùëÜùê∏3SE(3)\ italic_S italic_E ( 3 )manifold, which achieves a proficient and generalizable manipulation policy with only a few demonstrations. ‚Ä¢ We extend the theory of equivariant diffusion processes and derive a novel S‚Å¢E‚Å¢(3)ùëÜùê∏3SE(3)\ italic_S italic_E ( 3 )equivariant diffusion process, for simplified modeling and inference. ‚Ä¢ We extensively evaluate our method on standard robot manipulation tasks in both simulation and real-world settings, demonstrating its data efficiency, manipulation proficiency, and spatial generalization ability, significantly outperforming baseline methods."
https://arxiv.org/html/2411.03951v1,Continuous-Time State Estimation Methods in Robotics: A Survey,"Accurate, efficient, and robust state estimation is more important than ever in robotics as the variety of platforms and complexity of tasks continue to grow. Historically, discrete-time filters and smoothers have been the dominant approach, in which the estimated variables are states at discrete sample times. The paradigm of continuous-time state estimation proposes an alternative strategy by estimating variables that express the state as a continuous function of time, which can be evaluated at any query time. Not only can this benefit downstream tasks such as planning and control, but it also significantly increases estimator performance and flexibility, as well as reduces sensor preprocessing and interfacing complexity. Despite this, continuous-time methods remain underutilized, potentially because they are less well-known within robotics. To remedy this, this work presents a unifying formulation of these methods and the most exhaustive literature review to date, systematically categorizing prior work by methodology, application, state variables, historical context, and theoretical contribution to the field. By surveying splines and Gaussian processes together and contextualizing works from other research domains, this work identifies and analyzes open problems in continuous-time state estimation and suggests new research directions.","Figure 1: Example of 2D localization from noisy accelerometer, gyroscope, and range-bearing measurements (shown projected from interpolated poses) using two popular CT methods; B-splines (left), and ‚Äòexactly sparse‚Äô Gaussian processes (right). The Laplace approximation for the posterior (Equation 6) is obtained via batch optimization. For the uniform cubic B-spline, the S‚Å¢E‚Å¢(2)ùëÜùê∏2SE(2)italic_S italic_E ( 2 ) control points (diamonds) are estimated, while for the uniform ‚Äòconstant-jerk‚Äô Gaussian process, S‚Å¢E‚Å¢(2)ùëÜùê∏2SE(2)italic_S italic_E ( 2 ) states on the trajectory are estimated. The 3‚Å¢œÉ3ùúé3\sigma3 italic_œÉ uncertainties for the estimated variables (pink) and the interpolated S‚Å¢E‚Å¢(2)ùëÜùê∏2SE(2)italic_S italic_E ( 2 ) trajectories (blue) are shown. Since the latter method supports efficient covariance interpolation in manifold spaces, a 3‚Å¢œÉ3ùúé3\sigma3 italic_œÉ uncertainty envelope is shown around the trajectory. Autonomous driving, extra-planetary exploration, infrastructure inspection, and environmental monitoring are examples of the complex tasks expected of robotic platforms today. To cope with these diverse challenges, onboard sensors have increased in quantity, variety, and bandwidth, and the requirement for accurate and robust state estimation is more critical than ever. The constraints of these platforms, including cost, size, sensor quality, power consumption, and computational resources, vary significantly. Some platforms, like commercial drones or handheld devices, are highly constrained, while autonomous cars, construction machines, and others may have access to more onboard compute. As the demand for such diverse robotic systems grows in the coming decades, so will the requirement for general, flexible, and scalable state estimation solutions. A paradigm shift, from discrete to continuous-time, may be the key to achieving this. Discrete-Time (DT) Historically, state estimation techniques have operated in discrete-time, which means the state of the robot or process is estimated at specific moments in time. However, the evolution of the system is not modeled, and intermediate states cannot be inferred. These times must include the measurement times, so regardless of how quickly the system state evolves, DT methods require estimating variables at all of these sample times [1, 2, 3]. The computation of such algorithms scales poorly as the number and frequency of sensors increase, compromising their ability to run in real-time. Several techniques and tricks are employed to work around this issue. One of the most common is to aggregate measurements over time, especially from sensors that capture hundreds or thousands of measurements per second, to form lower-frequency pseudo-measurements. Examples include the accumulation of points from a sweeping Light Detection And Ranging (LiDAR) sensor for inter-scan or scan-to-map registration [4, 5, 6, 7, 8], feature extraction in event [9] or Rolling Shutter (RS) cameras, and Inertial Measurement Unit (IMU) preintegration [10, 11, 12, 13, 14]. Motion distortion may occur if the platform moves substantially over this accumulation period. Motion Distortion Correction (MDC) methods such as LiDAR deskewing [15] or RS compensation [16] attempt to correct for this. However, MDC is fundamentally a chicken-and-egg problem since the motion-corrected measurements are required to estimate the motion accurately. Consequentially, these methods introduce hard-to-model errors into the system. Some DT systems (e.g., [17]) also achieve state reduction by triggering sensors to capture simultaneously and often have complicated hardware triggering systems and clock synchronization in place to ensure that the timestamps reported by sensors are well-aligned to a central clock. Yet, in systems without proper time synchronization, estimating time offsets between sensor clocks is a complex task for DT state estimators [18, 19]. Continuous-Time (CT) Continuous-time methods (Figure 1) differ by modeling the underlying process as a continuous function of time and have the defining characteristic that the state can be queried at any time (sometimes within certain bounds). This capability can, on the one hand, be useful for downstream tasks that rely on accurate state estimates, such as planning or control. On the other hand, it allows for querying interpolated states at measurement times during estimation without requiring additional explicitly defined variables. By providing a layer of abstraction between the time-varying state and the optimization variables, CT methods allow the number of variables to be flexible and their generation to be application and sensor-agnostic. This property facilitates an elegant means of trading off accuracy and computation time while considering the robot‚Äôs dynamics. Such a system encourages the direct inclusion of high-frequency measurements, such as from an IMU or LiDAR, without necessarily creating new estimation variables, avoiding the errors introduced by measurement aggregation. Furthermore, by design, these systems can handle asynchronous sensor data and can naturally jointly estimate the time offset between sensor clocks by shifting the interpolation time [19]. Significance of this Work The CT paradigm could be the key to accurate, robust, and scalable state estimation. Indeed, CT methods are already being leveraged to achieve state-of-the-art performance across a full spectrum of applications. However, they remain relatively niche and less well-understood within the broader robotics community. The field has progressed substantially since the last CT survey almost a decade ago [20]. Other review works have focused more specifically on a single method [19] or sensor modality [21], or on comparative performance [22]. To update the community on modern CT methods and lower the bar of entry to future research, this work makes following contributions: ‚Ä¢ A concise background (Section II) and consolidated formulations for the established CT methods (Section III). ‚Ä¢ The most complete survey of CT state estimation to date (until October 2024), categorized by method, application, state variables, historical context, or theoretical contribution, as most appropriate (Section IV). ‚Ä¢ Identification and analysis of open problems (Section V). ‚Ä¢ A discussion of the scope of CT methods in other robotics‚Äô domains and applications (Section VI)."
https://arxiv.org/html/2411.03928v1,DEIO: Deep Event Inertial Odometry,"Event cameras are bio-inspired, motion-activated sensors that demonstrate impressive potential in handling challenging situations, such as motion blur and high-dynamic range. Despite their promise, existing event-based simultaneous localization and mapping (SLAM) approaches exhibit limited performance in real-world applications. On the other hand, state-of-the-art SLAM approaches that incorporate deep neural networks for better robustness and applicability. However, these is a lack of research in fusing learning-based event SLAM methods with IMU, which could be indispensable to push the event-based SLAM to large-scale, low-texture or complex scenarios. In this paper, we propose DEIO, the first monocular deep event-inertial odometry framework that combines learning-based method with traditional nonlinear graph-based optimization. Specifically, we tightly integrate a trainable event-based differentiable bundle adjustment (e-DBA) with the IMU pre-integration in a factor graph which employs keyframe-based sliding window optimization. Numerical Experiments in nine public challenge datasets show that our method can achieve superior performance compared with the image-based and event-based benchmarks. The source code is available at: https://github.com/arclab-hku/DEIO.","I INTRODUCTION I-A Motivation Achieving reliable and accurate Simultaneous Localization and Mapping (SLAM) under adverse conditions remains challenging, particularly when employing image-based solutions (RGB or RGB-D cameras). Event camera is a motion-activated sensor that only captures pixel-wise intensity changes and reports them as asynchronous stream instead of the whole scene at intensity image as a frame. Because of their remarkable properties, such as high temporal resolutions, high dynamic range (HDR), and no motion blur, event cameras have the potential to enable high-quality perception in extreme lighting conditions and high-speed motion scenarios that are currently not accessible to standard cameras. Nevertheless, integrating event cameras into SLAM systems presents significant challenges. This is caused by the fact that the event data is sparse, irregular, and asynchronous, with each event conveying limited information and containing inherent noise. What‚Äôs more, unlike standard-vision counterparts, event camera cannot capture visual information when motion is parallel to edges or in static state. Therefore, current purely event-based SLAM systems [1, 2] generally lack the robustness requirement for many real-world applications. Using additional sensors tends to achieve superior performance since different modalities can complement each other, such as image frame [3, 4, 5], depth sensors [6], or even LiDAR [7]. However, these combination might limit the application of event cameras in real-world devices due to increased costs and more complicated sensor calibration requirements [8]. What‚Äôs more, they might introduce some bottlenecks, for example, relying the image frames might makes the system susceptible to motion blur and reduce the benefits of HDR. Recent studies have introduced learning-based approaches [9] as promising solutions for event-based SLAM. These methods, which are solely trained end-to-end on synthetic events, demonstrate remarkable generalization to real-world benchmarks and can outperform some traditional systems in accuracy. However, it‚Äôs important to note that visual-only systems have inherent limitations, making them vulnerable to low-textured environments and suffering from scale ambiguity. To address these visual degradation, a promising strategy is to incorporate multiple sensors, the Inertial Measurement Unit (IMU), which is low-cost and readily available in most event cameras. What‚Äôs more, in the minimal configuration of a monocular setup, the IMU can also be used to recover the metric scale and provide higher accurate and more robust system. Despite this promising, learning-based event SLAM with IMU remains an unexplored territory. I-B Contributions Therefore, in this paper, we propose a deep learning-based event-inertial odometry (DEIO), which combines learning-based methods with traditional nonlinear graph-based optimization. We leverage the strengths of neural networks in predicting event correspondence, replacing the optical flow or hand-craft event-corner feature tracking [10]. To the best of our knowledge, DEIO is the first learning-based event-inertial odometry framework. More specifically, we design the learning-optimization-combined framework that tightly-coupled integrate trainable event-based differentiable bundle adjustment (e-DBA) with IMU pre-integration. In the framework, event-based recurrent optical flow and e-DBA are performed among consecutive event streams. The Hessian information is derived from e-DBA and further fed into a generic factor graph, facilitating integration with the IMU through keyframe-based sliding window optimization. The e-DBA is trained on synthetic event data but demonstrate strong generalization on real-world challenge scenarios. Our main contributions can be summarized as follows: 1. We propose a learning-based event-inertial odometry, which integrates a trainable e-DBA and IMU information through graph-based optimization, which improves the accuracy and robustness of pose estimation. 2. DEIO is the first learning-based monocular event-inertial odometry. Even though training on synthetic data, it outperforms over 20 state-of-the-art methods across 9 challenging real-world event benchmarks. The performance gap is further increased when trained on real data. 3. We release our code and the preprocessed event data to facilitate further research in learning-based event pose tracking."
https://arxiv.org/html/2411.03873v1,Biomechanics-Aware Trajectory Optimization for Navigation during Robotic Physiotherapy,"Robotic devices hold promise for aiding patients in orthopedic rehabilitation. However, current robotic-assisted physiotherapy methods struggle including biomechanical metrics in their control algorithms, crucial for safe and effective therapy. This paper introduces BATON, a Biomechanics-Aware Trajectory Optimization approach to robotic Navigation of human musculoskeletal loads. The method integrates a high-fidelity musculoskeletal model of the human shoulder into real-time control of robot-patient interaction during rotator cuff tendon rehabilitation. We extract skeletal dynamics and tendon loading information from an OpenSim shoulder model to solve an optimal control problem, generating strain-minimizing trajectories. Trajectories were realized on a healthy subject by an impedance-controlled robot while estimating the state of the subject‚Äôs shoulder. Target poses were prescribed to design personalized rehabilitation across a wide range of shoulder motion avoiding high-strain areas. BATON was designed with real-time capabilities, enabling continuous trajectory replanning to address unforeseen variations in tendon strain, such as those from changing muscle activation of the subject.","Injuries affecting the musculoskeletal system are pervasive in society due to increasing population longevity, strenuous manual labor, and greater engagement in sports [1, 2, 3]. At the forefront of these injuries, disorders impacting the shoulder and the rotator cuff are particularly common, with clinical literature reporting their prevalence to be as high as 22%percent2222\%22 % in the general population [1]. Overall, the demand for therapy and rehabilitation is large and expected to grow, exacerbating the problem posed by a lack of physical therapists. Moreover, the task of treating a complex biological mechanism, such as the shoulder, is challenging due to a fundamental lack of insights into rotator-cuff behavior. Physiotherapists need to move the joint safely, gradually increasing the patient‚Äôs range of motion while physically supporting the patient‚Äôs arm and avoiding re-injuries [4]. Consequently, simple and limited movements are prescribed, to limit both the risk of re-injury and the physical demands on the therapist to manually manipulate the patient safely. Robotic technologies can assist in addressing these two gaps during therapy: robots can reduce the manual burden on physiotherapists and provide new tools to monitor and improve rehabilitation outcomes. Overall, rehabilitative robots are already used successfully in post-stroke rehabilitation [5, 6, 7] and gait assistance [8, 9, 10]. However, robots to treat musculoskeletal injuries (such as rotator cuff tears) remain limited. In particular, previous work remains focused on relieving physiotherapists from physically manipulating the patient. Smooth rehabilitation trajectories can be guaranteed by focusing on the control of the movement of the rehabilitative robot itself [11], and therapeutic robotic movements can be learned from expert human demonstrations to automate manual patient manipulation [12]. However, these solutions do not directly account for the patient‚Äôs biomechanics. Deeper insights into human musculoskeletal mechanics can enable robots to be aware of tissue and joint loads associated with injury risks, allowing them to operate autonomously or in collaboration with therapists to holistically improve the rehabilitation process [13]. An increasingly promising strategy is to leverage human musculoskeletal models and incorporate them directly into robot control loops [14, 15, 16, 17, 18, 19]. Figure 1: BATON combines musculoskeletal modeling and human-robot interaction to enable a robotic physiotherapist to plan therapeutic movements for its patients in real-time while considering tissue loading (i.e. strain) induced in the rotator cuff tendons in the shoulder. Musculoskeletal modeling has progressed greatly over the past decades, allowing researchers to estimate the activities of individual muscles involved in producing movement [20, 21, 22] and to perform predictive simulations of human motion [23, 24]. As computational models increasingly capture the inner workings of the human body, their applicability continues to expand, particularly in physical human-robot interaction and assistive devices. Recently, biomechanical models have been used to quantify assistance needed by a human operator [14, 15, 16], reduce human metabolic cost in walking-assistive devices [17, 18, 19], and even plan search-and-rescue robotic operations [25] or the motion of supernumerary robotics limbs [26]. Through computer simulations or offline usage of human models, these works demonstrated the importance and utility of including human biomechanics in robot planning and decision-making. However, they did not leverage high-fidelity biomechanical simulations to regulate human-robot interactions in scenarios that require online adaptation to human behavior. When real-time capabilities are necessary, purely kinematic models have been used to monitor human joint positions, velocities, and torques [27], or to develop controllers for hybrid neuroprostheses of the knee [28]. While lower-fidelity models allow for computational efficiency, they are unsuitable for insights into the musculoskeletal system during shoulder rehabilitation. Biomechanical quantities from a high-fidelity human model were recently used in an online model predictive control framework, achieving predictive control of a leg prosthesis for ergonomically safe walking [29]. Fast predictions of the user‚Äôs knee torque were achieved through Bayesian interaction primitives, which could estimate the unobservable biomechanical values via the inclusion of a musculoskeletal model in the data augmentation offline. However, this study did not consider the mechanical behavior of the human tissues (e.g., muscles and tendons). Building on the previously discussed applications of musculoskeletal models, we can go further to gain insights into the underlying biological tissues that are targets of physical therapy. By estimating tissue loads from external measurements based on a high-fidelity model of the human shoulder, we introduced the concept of ‚Äústrain maps‚Äù [30] to capture the relationship between human pose and the strain (i.e., load) induced in the rotator cuff tendons. Abstracting the rotator cuff tendon strains into an intuitive and navigable map, we achieved offline graph-based motion planning for safe robotic-mediated shoulder rehabilitation [30]. However, the planning algorithm did not account for the dynamics of the human, resulting in trajectories that presented sudden direction changes and were potentially difficult to track. Accelerations imposed on the human shoulder were not monitored, and the trajectories were executed in an open loop. Employing the strain maps, we implemented a reactive impedance-control-based approach to physical human-robot interaction in rehabilitation that enabled a subject to perform therapeutic exercises while robotic assistance protected them from potentially dangerous poses [31]. The system would react only when dangerous movements were already initiated, without anticipating the subject‚Äôs trajectory to prevent abrupt movements and experience high corrective forces from the robot. Other researchers designed optimized trajectories for controlling an ankle rehabilitation robot, to minimize joint loading [32]. A custom ankle model was used to plan the robot‚Äôs trajectory offline, making online adjustments based on tracking errors attributed to excessive joint stiffness. However, the human model was not considered during the adjustments, so the reference path could become suboptimal during execution. To the best of our knowledge, a high-fidelity model of the human musculoskeletal system has never been used to synthesize adaptive robotic rehabilitative movements in real-time through coupled predictive simulations of human dynamics and tissue loading. Our primary objective is to develop a Biomechanics-Aware Trajectory Optimization for the Navigation of human tissue strains during robotic physiotherapy (BATON). Our novel approach intends to directly couple a state-of-the-art biomechanical model of the human shoulder to robotic control, generating movements for predictive robotic-assisted rotator cuff tendon rehabilitation in real-time. The resulting optimized trajectories are realized by a collaborative robot arm that delivers the movement to a healthy human subject through safe impedance control, with gravity compensation for the human arm naturally and adaptively provided by the model. This allows personalized treatment in the early stages of post-traumatic rehabilitation of the rotator cuff, where the subject does not support the weight of their own arm. Our specific aims and contributions are: ‚Ä¢ develop an automated system (BATON) to navigate underlying human biomechanical outcomes in real-time during robotic-assisted physiotherapy; ‚Ä¢ design of an optimal control problem (OCP) for trajectory optimization that incorporates a real-time shoulder musculoskeletal model by decoupling skeletal dynamics and strain behavior; ‚Ä¢ analyse performance both in simulation and on a physical robotic system applied to a test subject to demonstrate 1) the effect of OCP parameters on the planned trajectory, 2) the quality of the executed paths, 3) the ability to re-plan online based on changing strain landscapes, 4) real-time computational performance. An intuitive analogy for navigating around potentially unsafe movements in human rehabilitation is the path planning of an autonomous ship in a bay where underwater reefs are present. To know which maneuvers allow the ship to navigate safely and efficiently toward a target position, we need both a dynamic model of the vessel and a map of the underwater reefs, providing non-obvious insights about the safest path. In our situation, the human shoulder dynamics play the role of the vessel, while the strain maps offer insights into what happens beneath the surface (specifically in the recovering tendons). Only by considering the two elements jointly can safe navigation be obtained."
https://arxiv.org/html/2411.03838v1,Fundamental Three-Dimensional Configurationof Wire-Wound Muscle-Tendon Complex Drive,"For robots to become more versatile and expand their areas of application, their bodies need to be suitable for contact with the environment. When the human body comes into contact with the environment, it is possible for it to continue to move even if the positional relationship between muscles or the shape of the muscles changes. We have already focused on the effect of geometric deformation of muscles and proposed a drive system called wire-wound Muscle-Tendon Complex (ww-MTC), an extension of the wire drive system. Our previous study using a robot with a two-dimensional configuration demonstrated several advantages: reduced wire loosening, interference, and wear; improved robustness during environmental contact; and a muscular appearance. However, this design had some problems, such as excessive muscle expansion that hindered inter-muscle movement, and confinement to planar motion. In this study, we develop the ww-MTC into a three-dimensional shape. We present a fundamental construction method for a muscle exterior that expands gently and can be contacted over its entire surface. We also apply the three-dimensional ww-MTC to a 2-axis 3-muscle robot, and confirm that the robot can continue to move while adapting to its environment.","Humanoid robots have been actively developed by companies in recent years[2, 3, 4, 5], achieving dexterous manipulation and stable locomotion. Most of the robots presented have rigid bodies, and they deal with environmental contact only at the end-effectors, such as hands and feet. As for contact with other parts of the body, although there are some examples of studies on multi-point contact[6, 7, 8] and disturbances applied during push recovery motions[9, 10, 11], the topics covered are limited. For example, complex movements involving whole-body surface contact, such as holding with arms and torso, or pushing aside obstacles with the body while both hands are occupied, are challenging to achieve. If the robot tries to force its body to move when it is in contact with the environment, its rigid body may damage the surrounding environment. In order for robots to play a more active role in society, it is necessary for robots to handle a broader range of environmental contacts and to have a body suitable for such contacts. There are soft humanoids that fit into the environment, such as those using a wire drive[12, 13, 14] and those using pneumatic artificial muscles[15, 16]. Although each drive method has its own advantages and disadvantages, we focus on the wire drive in this research. The major advantage of the wire drive is that it exerts a large force and its high controllability due to the winding by the motor. However, handling environmental contact with the entire body is challenging because of unexpected interference and loosening of the wires in a body configuration with exposed wires. Humanoids need bodies that can adapt to the environment like humans do. They must also be able to operate while making good use of the environment by contacting all body surfaces and continuing to operate safely in the face of environmental disturbances. Figure 1: (a) Muscle-tendon complexes adjust their shape and internal spatial relationships when encountering contact, enabling continued movement while adapting to the environment. They return to their original position after contact with the environment ends. (b) 2-axis 3-muscle robot with three-dimensional configuration. The human body‚Äôs remarkable adaptability to environmental contact is primarily due to its muscle-tendon complexes. These complexes not only provide flexibility but also enable dynamic adaptation to external forces. When encountering environmental contact, muscle-tendon complexes can adjust their shape and spatial relationships between muscles and tendons. This adaptability allows for continued movement while conforming to new environmental conditions. Moreover, they quickly return to their original positions and shapes when contact ceases (Fig. 1(a)). In this study, we focus on these adaptive characteristics to develop a body-driving system that is highly adaptable to environmental contact. Figure 2: (a) Conceptual diagram of wire-wound Muscle-Tendon Complex drive[1]. (b) 1-axis 3-muscle robot with two-dimensional configuration[1]. We have already proposed a drive system called the wire-wound Muscle-Tendon Complex (ww-MTC), which incorporates muscle-tendon deformation into the wire drive[1](Fig. 2). The concept of incorporating geometrical shape changes of muscles in a two-dimensional configuration is realized, and advantages such as prevention of wire loosening, securing of moment arms, internal protection against environmental contact, and muscular appearance are confirmed. This method is similar to pneumatic artificial muscles such as the McKibben type[17, 18] in that it expands with contraction. However, since the tension is provided by an internal wire, the shape can be changed in various ways according to the design of the muscle exterior. This study concludes that the incorporation of muscle expansion is promising for body configurations suitable for environmental contact. On the other hand, this configuration has the problem of too large expansions of the muscle parts inhibiting each other‚Äôs movement. In addition, developing a three-dimensional muscle exterior that has no gaps on its surface and can expand and deform has been a major challenge. In order to use muscle-tendon complex drives in three-dimensional movements and body configurations, we develop muscle exterior and tendon elements with more compact shapes and entirely covered surfaces. In particular, for muscles that expand and deform, we propose a structure consisting of a frame made of arch-shaped material that defines the deformation and a braided sleeve that covers the surface. In addition, the proposed structure‚Äôs effectiveness is verified by performing motion experiments with environmental contact on a 2-axis, 3-muscle robot using a three-dimensional ww-MTC. The main contributions of this study are as follows: ‚Ä¢ This study shed light on the inherent adaptability of deformable biological muscles during environmental contact and aimed to achieve it in robotic systems. ‚Ä¢ We proposed a structure that deforms three-dimensionally under wire tension, and confirmed the relationship between contraction distance and expansion through theoretical calculations. ‚Ä¢ We have verified that the robot using 3D ww-MTC maintains its operation even if the position and shape of the muscles change during environmental contact. Figure 3: Three-dimensional ww-MTC consists of the muscle exterior and the tendon based on Stiffness Adjustable Tendon (SAT)[20]. Figure 4: (a) Conceptual diagram of three-dimensional muscle. Muscle exterior is made by frame determining shape change and braided sleeve covering the surface. (b) Assembly of the muscle exterior."
https://arxiv.org/html/2411.03747v1,Observability-Aware Control for Cooperatively Localizing Quadrotor UAVs,"Cooperatively Localizing robots should seek optimal control strategies to maximize precision of position estimation and ensure safety in flight. Observability-Aware Trajectory Optimization has strong potential to address this issue, but no concrete link between observability and precision has been proven yet. In this paper, we prove that improvement in positioning precision inherently follows from optimizing observability. Based on this finding, we develop an Observability-Aware Control principle to generate observability-optimal control strategies. We implement this principle in a Model Predictive Control framework, and we verify it on a team of quadrotor Unmanned Aerial Vehicles comprising a follower vehicle localizing itself by tracking a leader vehicle in both simulations and real-world flight tests. Our results demonstrate that maximizing observability contributed to improving global positioning precision for the quadrotor team.","Multi-Robot Systems may employ Cooperative Localization (CL) in exploration [rekleitis1997multi, rekleitis2001multi, roumeliotis2002distributed] and navigating in spite of flaws in Global Navigation Satellite Systems (GNSS) [qu2011cooperative, vetrella2017autonomous, causa2018multi]. A team of robots achieve CL by sharing sensor data, measuring some aspects of interrobot pose, and fusing these data in a state estimator to jointly estimate their states. In practice, simple interrobot sensors on small robots like micro quadrotors cannot directly observe every component of system state. In the scenario of a leader-follower robot team, even if the follower can measure its range ‚Äî in one dimension ‚Äî to the leader, it cannot deduce all 3 coordinates of its position relative to the leader. Thus, it must take observations along specific trajectories to regain full state information and thereby navigate safely. Generalizing this problem, designing a trajectory to enhance observability is called Observability-Aware Trajectory Optimization. Observability reflects the ability of a dynamical system to determine its states given observations along its trajectory. Recent works proposed trajectory optimization metrics based on the Local Observability Gramian, which is variously approximated by numerical means [krener2009measures] or by Taylor expansions of the underlying system dynamics [hausman2017observability, preiss2018simultaneous, grebe2021observability]. It has been shown that observability-optimal trajectories let a system avoid configurations where a part of the states become unobservable. Although this approach is applied to sensor self-calibration in most existing works, it is a strong candidate to address the issue of observing the full system state by dimensionally-deficient observations in CL. ‚Ñ±‚Üíisubscript‚Üí‚Ñ±ùëñ\underrightarrow{\mathcal{F}}_{i}under‚Üí start_ARG caligraphic_F end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTLeader (0)0(0)( 0 )Follower (1)1(1)( 1 )ùê©0superscriptùê©0\mathbf{p}^{0}bold_p start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPTùê©1superscriptùê©1\mathbf{p}^{1}bold_p start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPTùê©01superscriptùê©01\mathbf{p}^{01}bold_p start_POSTSUPERSCRIPT 01 end_POSTSUPERSCRIPTUniform, unobservable trajectoryObservability-optimaltrajectory Figure 1: Visualizing a Cooperative Localization System with a leader and a follower. The leader‚Äôs position ùê©0superscriptùê©0\mathbf{p}^{0}bold_p start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT is directly measured but that of the followers ùê©1superscriptùê©1\mathbf{p}^{1}bold_p start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT must be indirectly measured. Only the interrobot range (dim=1dimension1\dim=1roman_dim = 1) instead of the full interrobot positions (dim3dimension3\dim 3roman_dim 3) is measured so a special trajectory (green path) is needed to recover information on the state. UWB ranging module Figure 2: A practical cooperative localization system based on Ultra-Wideband (UWB) interrobot ranging mounted on a pair of F450 quadrotors, portrayed in flight during our flight test experiments Before adopting observability-aware trajectory optimization, this approach‚Äôs relationship with uncertainty-aware trajectory optimization must be addressed. The latter is well established in the area of robot CL since it is a natural extension to the Extended Kalman Filter (EKF) on CL systems. It seeks trajectories that minimize some metric of the EKF covariance matrix [trawny2004optimized, hidaka2005optimal, zhou2011optimized]. An earlier body of work [song1992extended, reif1999stochastic] linked the Observability Gramian to bounds on EKF covariance. However, these conclusions do not necessarily imply that optimizing aspects of the observability has any positive impact on covariance. Another open question is whether observability-optimal trajectories satisfy local weak observability [hermann1977nonlinear] that is directly evolved from the classical definition of observability of linear systems. To our best knowledge, no work proved this beyond empirically demonstrating that observability-optimal trajectories avoid unobservable configurations. In this paper, we present an optimal control problem based on maximizing observability through a novel approximation of the Observability Gramian. Our formulation is informed by our derivation of the intrinsic link between observability maximization and positioning uncertainty reduction. We solve this problem in a receding-horizon framework to give us the Observability Predictive Controller (OPC), which complies with the Model Predictive Controller (MPC) formalism and inherits its applicability to both real-time control and offline trajectory generation. We also prove that the solution to this problem satisfies classical observability rank conditions. To verify our framework, we consider a leader-follower team of quadrotors. Only the leader is equipped with GNSS, and the follower must cooperatively localize itself by measuring its distance to the leader. By design, the observations do not cover the state space so an observability-optimal control solution is necessary to observe the full state. We use this configuration in simulations to demonstrate that positioning uncertainty is minimized along observability-optimal trajectories, and then we apply this configuration when flight testing a team of quadrotor platforms shown in Figure 2. In summary, our contributions in this paper are ‚Ä¢ We derive the Short-Term Local Observability Gramian (STLOG), a novel approximation of the Local Observability Gramian ‚Ä¢ We propose the Observability-Aware Control principle, an optimal control law that maximizes precision improvement"
https://arxiv.org/html/2411.03682v1,\ourmethod: Cross-Embodiment Imitation Using a Grasping Tool,"Cross-embodiment imitation learning enables policies trained on specific embodiments to transfer across different robots, unlocking the potential for large-scale imitation learning that is both cost-effective and highly reusable. This paper presents \ourmethod, a cross-embodiment imitation learning framework for visuomotor skill transfer across varied kinematic morphologies. We introduce a handheld gripper that unifies action and observation spaces, allowing tasks to be defined consistently across robots. Using this gripper, we train visuomotor policies via imitation learning, applying a motion-invariant transformation to compute the training loss. Gripper motions are then retargeted into high-degree-of-freedom whole-body motions using inverse kinematics for deployment across diverse embodiments. Our evaluations in simulation and real-robot experiments highlight the framework‚Äôs effectiveness in learning and transferring visuomotor skills across various robots. More information can be found at the project page: https://ut-hcrl.github.io/LEGATO.","Recent advancements in robot hardware‚Äîfrom wheeled manipulators to humanoid robots [kemp2022stretch, bd-spot, unitree-b1, agility-digit, fourier-gr1]‚Äîhave greatly increased access to diverse robotic platforms. To fully leverage these advancements in better supporting human activities, robots must autonomously perform a wide range of complex tasks. Deep imitation learning has shown promise in training autonomous policies for sensorimotor skills, reducing the need for extensive human programming compared to traditional rule-based approaches. It has yielded impressive results in complex robotic systems [xie2020deep, seo2023trill] and across diverse dexterous manipulation tasks [zhao2023aloha, mandlekar2023mimicgen, sridhar2023memory]. However, these approaches typically require demonstration data from a specific target robot itself, limiting scalability due to high hardware costs and the intensive workload of operating the robot during demonstrations. Additionally, individualized training for each robot restricts cross-embodiment applications, as data cannot be transferred between different robot systems, even for similar tasks. To enable scalable demonstrations, pioneering work have introduced data collection tools that allow humans to directly manipulate during demonstrations [song2020grasping, young2021visual, seo2022prelude, chi2024umi]. These approaches enable training visuomotor policies that are deployable to specified target robots, reducing human workload and avoiding the costs and risks associated with using real robots for data collection. However, these methods require either designing specialized data collection tools for specific robot gripper mechanisms or replacing robots‚Äô original grippers with customized hardware. This limitation restricts the applicability of these tools for robots with diverse gripper mechanisms. For example, the Universal Manipulation Interface [chi2024umi] is designed for the Schunk WSG-50 gripper with a parallel-jaw mechanism but does not work with other mechanisms, such as hinge types. Additionally, variations in control latency and trajectory-tracking errors across robots, absent in human demonstrations, complicate policy transfer between embodiments. Chi et al. [chi2024umi] addressed this by compensating for control and observation latencies, while Song et al. [song2020grasping] used fine-tuning through trial and error on the target robot system. However, these strategies are difficult to generalize across diverse robots, as control latency varies between platforms due to differences in hardware and controllers. Figure 1: Overview of \ourmethod. \ourmethod addresses the challenge of transferring visuomotor skills across diverse robot embodiments. We present here a cross-embodiment imitation learning framework using a versatile handheld grasping tool that ensures consistent physical interactions across different embodiments. Policies trained from demonstrations by humans or teleoperated robots using the tool can be deployed across various robots equipped with the same gripper. Motion retargeting enables the execution of trajectories on different robots without requiring robot-specific training data. Our key idea to address hardware differences across robots is the integration of a handheld grasping tool that can be shared across various robot embodiments for tasks learned from shared data (see Figure 1). We name our method \ourmethod (Learning with a Handheld Grasping Tool). In our method, the LEGATO Gripper, a custom-designed handheld gripper, acts as a versatile and adaptive tool that represents tasks through its trajectories and grasping actions, ensuring consistency across embodiments during both demonstration and deployment. This handheld gripper‚Äîcompatible with various robot grippers‚Äîenables a single visuomotor policy to be applied across diverse robot systems without any modifications. Sharing the gripper across robots and directly manipulating human demonstrators ensures consistent, active-actuation grasping actions. In contrast, other data collection methods often create discrepancies in grasping actions because demonstrations rely on passive actuation by demonstrators, while robots require active actuation during deployment. Our framework incorporates low-level motion retargeting through inverse kinematic (IK) optimization, tailored to each robot, along with a high-level transferable visuomotor policy (see Figure 2). Commands from the visuomotor policy for the handheld gripper are converted into whole-body motions using IK, enabling adaptation across robotic systems with only kinematic information, thereby avoiding extensive robot-specific training. While the IK optimization adapts gripper motions to each robot, variations in hardware and kinematics introduce differences in control latency and errors. To address this, we incorporate regularization on the gripper‚Äôs trajectories in a motion-invariant space during training, preventing bias toward the demonstration embodiment and effectively capturing motor skills. This approach ensures that gripper trajectories from the visuomotor policy can be consistently translated into whole-body robot motions, regardless of control latency and IK response differences. Together, these components enable the learned visuomotor policies in \ourmethod to be effectively transferred across diverse robotic systems. We validate our approach through simulation and real-robot experiments, demonstrating its cross-embodiment transferability. In simulation, visuomotor policies trained on human demonstrations are successfully deployed across various embodiments, including a tabletop manipulator, a wheeled robot, a quadruped, and a humanoid. We further demonstrate the reusability of demonstration data by transferring a policy trained on one robot to another. In real-world tabletop manipulator setups, our method achieves a 72% success rate in complex manipulation tasks through policy transfer from direct human demonstrations."
https://arxiv.org/html/2411.03669v1,"Imagined Potential Games: A Framework for Simulating, Learning and Evaluating Interactive Behaviors","Interacting with human agents in complex scenarios presents a significant challenge for robotic navigation, particularly in environments that necessitate both collision avoidance and collaborative interaction, such as indoor spaces. Unlike static or predictably moving obstacles, human behavior is inherently complex and unpredictable, stemming from dynamic interactions with other agents. Existing simulation tools frequently fail to adequately model such reactive and collaborative behaviors, impeding the development and evaluation of robust social navigation strategies. This paper introduces a novel framework utilizing distributed potential games to simulate human-like interactions in highly interactive scenarios. Within this framework, each agent imagines a virtual cooperative game with others based on its estimation. We demonstrate this formulation can facilitate the generation of diverse and realistic interaction patterns in a configurable manner across various scenarios. Additionally, we have developed a gym-like environment leveraging our interactive agent model to facilitate the learning and evaluation of interactive navigation algorithms. Results and code are available on the project website: https://sites.google.com/view/simulate-learn-interact.","Developing effective navigation policies is essential for enhancing human-robot interaction. A critical component of this development is the creation of diverse, interactive agents capable of mimicking human-like behaviors. However, simulating realistic human interaction within autonomous systems presents significant challenges. Current simulation models primarily focus on simple collision avoidance, which is often inadequate for complex interactive scenarios. For instance, in a narrow hallway where only one person can pass at a time (Figure 1 upper left), mere collision avoidance is insufficient. Such scenarios require collaborative behaviors, such as one individual stepping back to allow another to pass, thereby preventing a deadlock. Moreover, the system must also identify optimal moments to yield when others are advancing, mirroring the implicit and non-verbal coordination of movements observed in real human interactions. In these settings, the agent must interact with others in a closed-loop manner. If the simulated agent is overly conservative, the resulting navigation policies might become aggressive, potentially raising safety concerns. Conversely, overly conservative policies could cause the agent to get stuck in crowded environments. To address these issues, the primary challenge lies in generating or simulating diverse yet realistic behaviors in a closed-loop manner that can effectively interact with other agents. This involves understanding, predicting, and dynamically adapting to the complex interplay of multiple agents within shared spaces. One method is to collect extensive interaction datasets to train interactive behavior policies. However, collecting large amounts of human-interacting data across multiple scenarios is expensive. And even in collected human motion datasets, interactions are typically rare and tailed events. In the autonomous driving domain, using the collected motion dataset, the state-of-the-art autonomous driving simulator Waymax [1] employs data-driven prediction models and rule-based models that follow recorded paths as closed-loop planners for reactive agents. However, these methods are not designed for closed-loop interactions and unsuitable for highly interactive cases as they are prone to failures in situations not well represented in the data, such as sudden, out-of-distribution changes during an interaction. Figure 2: A human-like interaction at the T-intersection: one agent stepped back to yield. This paper aims to explore the simulation of human-like interactions in a distributed setting and the use of such simulations to enhance the learning of collaborative interaction strategies. The distributed setting represents the practical case in interactions, where each agent independently formulates plans based on its observations without access to the plans or cost function parameters of other agents. Figure 1 illustrates some challenging indoor navigation scenarios where collaborative planning is necessary under the presented starting/goal configurations, and figure 2 shows an illustrative interaction trajectory generated from our proposed method at a T-intersection. One agent moved back to yield after coming out of the intersection and saw the other agent coming through. It is worth noticing that the behavior emerges from closed-loop simulation without predefined intention or motion. In this paper, we first introduce a framework based on distributed imagined potential games (IPG) that can simulate interactions in scenarios where traditional collision-avoidance algorithms fail. Potential games provide a method for deriving game-theoretical equilibrium plans for multiple agents. We further extend this concept to a distributed IPG, where each agent independently imagines a potential game with others based on its estimations. We find this distributed IPG formulation is crucial for achieving diverse and realistic interactive behaviors akin to human interactions. We employ trajax [2] to implement the fast online iLQR optimization [3] and verify such a framework can generate diverse and realistic interactions effectively in various scenarios shown in Figure 1. Furthermore, We integrate these reactive distributed agents into a gym-like environment, enabling the simulation of interactions within both provided and randomly generated settings. This environment serves as a platform for training reinforcement learning agents and evaluating navigation algorithms designed for interaction. Finally, we discuss the ongoing challenges in developing metrics to evaluate interaction generation in new scenarios and the capability of training navigation policies to collaborate with various reactive agents using reinforcement learning."
https://arxiv.org/html/2411.03660v1,Development of a Practical Articulated Wheeled In-pipe Robot for Both 3-4 in Force Main Inspection of Sewer Pipes,"This paper reports a practical articulated wheeled in-pipe inspection robot ‚ÄúAIRo-7.1‚Äù which is waterproof and dustproof, and can adapt to 3 to 4 in inner diameters. The joint torque can be adjusted by a PWM open-loop control. The middle joint angle can be controlled by a position feedback control system while the other two joints are bent by torsional springs. Thanks to this simple and high-density design, not only downsizing of the robot but also wide range of the adaptive inner diameter were achieved. However, the relationship between the actual middle joint torque value and the PWM duty ratio should be pre-known because the reducer used in AIRo-7.1 was designed by ourselves. Therefore, preliminary experiments were conducted to clarify the relationship between them. To examine the adaptive movement, experiments in both 3 in and 4 in pipes with vertical, bend, and diameter change sections. Finally, field experiment was also conducted. From the results, high adaptability to different inner diameters of pipes and slippery environments were confirmed although waterproof and dustproof were not perfectly working.","1 INTRODUCTION In recent years, the aging of water supply facilities has become more serious, and burst and leakage accidents have begun to occur. In paticular, a sewer pipe break leads to gradually erode the soil and may eventually cause a subsidence accident. The risk is even more serious because sewage contains hydrogen sulfide which increases the health hazards. As of 2021, 480,000 km of sewage pipes have already been laid in Japan, and 13,000 km of them are more than 50 years old. This number is expected to increase rapidly in the near future. In response to this situation, the Ministry of Land, Infrastructure, Transport and Tourism in Japan established maintenance and repair standards based on the sewerage law in 2015. It made mandatory to inspect sewer pipes at least once every five years. Some inspection robots have been already commercialized such as scope cameras and self-propelled inspection equipments [1] [2]. However, these products are not available for all sewer facilities. For example, there are special pipes called ‚Äúforce main‚Äù (Fig. 1) that pumps up the sewage to a higher level. Force main is used in the places in which the wastewater cannot be carried naturally by gravity. It is reported that the total length of the force main in Japan is approximately 10,000 km. To increase water pressure, the inner diameters of the general force main are designed to small; 2 in, 3 in, and 4 in (approximately 50 mm, 75 mm, and 100 mm, respectively). Note that the diameter of some force mains exceed 4 in because of the regional differences [3]. In addition, there are many bends and vertical sections in a short range. Therefore, it has been impossible to inspect them using a conventional products due to the size limitation and maneuverability limitation to three-dimensionally winding pathway. Figure 1: Image of ‚Äúforce main‚Äù. On the other hand, there have been a few reports of inspection robots for the force main so far. Kubota Corporation (2-47, Shikitsuhigashi 1-chome, Naniwa-ku, Osaka Japan) has developed a push-in inspection device called ‚ÄúSnei-kun‚Äù [4]. Kantool Corporation (1-3, Yosiyanipponbasi Bld.8th floor, Nihombashikodemma-cho, Chuo-ku, Tokyo, 103-0001, Japan) has commercialized a push-in camera specifically designed for small-diameter sewer pipes called ‚ÄúAgilios‚Äù [5]. Nakamura et al. and Solaris Corporation (14-13, Higashiyama-cho, Itabashi-ku, Tokyo, 174-0073, Japan) have also proposed a pneumatically driven inspection robots with peristaltic locotion[6]. Koei Dream Works (Zizouyamasita 2068, Kazama, Yamagata, 990-2221, Japan) provides inspection services using a water hydraulic driven inspection device called ‚ÄúHaikan Kun Type II‚Äù [7]. However, the push-in type device cannot proceed to the depths of the winding pipes by itself due to the cable friction as mentioned above. Peristaltic locomotion has the disadvantage of slow moving speed of 0.03 m/s and less [8]. Pneumatic and hydraulic actuators may achieve drastic downsizing of the robot. However, they require air compressor and high-water-pressure equipment, thus, not easy to use in the field quickly. On the other hand, high adaptability pipe inspection robots with a structure combining multi-link and active wheel drive have been reported [9]-[14]. Although the inner diameters are still limited to 3‚Äì8 in, we think that this type is the fastest and the most adaptable to a variety of pipe shapes in size limitation. Our previous researches have also focused on this and showed some maneuverability with real equipments [15]-[16]. This paper proposes our latest in-pipe robot, AIRo-7.1 (Fig. 2), which is driven by geared electric motors with improvements of waterproof and dustproof for traveling in force mains. This robot can pass through straight and bend pipes with both 3 in and 4 in inner diameter and even adapt to joint part of them such as increaser/decreaser. This adaptive movement is achieved by a simple open-loop joint torque adjustment (PWM control). However, the relationship between the actual torque value and the PWM duty ratio should be pre-known. Figure 2: Overview of an in-pipe inspection robot with waterproof and dustproof: AIRo-7.1."
https://arxiv.org/html/2411.03619v1,Real-Time Safe Bipedal Robot Navigation using Linear Discrete Control Barrier Functions,"Safe navigation in real-time is an essential task for humanoid robots in real-world deployment. Since humanoid robots are inherently underactuated thanks to unilateral ground contacts, a path is considered safe if it is obstacle-free and respects the robot‚Äôs physical limitations and underlying dynamics. Existing approaches often decouple path planning from gait control due to the significant computational challenge caused by the full-order robot dynamics. In this work, we develop a unified, safe path and gait planning framework that can be evaluated online in real-time, allowing the robot to navigate clustered environments while sustaining stable locomotion. Our approach uses the popular Linear Inverted Pendulum (LIP) model as a template model to represent walking dynamics. It incorporates heading angles in the model to evaluate kinematic constraints essential for physically feasible gaits properly. In addition, we leverage discrete control barrier functions (DCBF) for obstacle avoidance, ensuring that the subsequent foot placement provides a safe navigation path within clustered environments. To guarantee real-time computation, we use a novel approximation of the DCBF to produce linear DCBF (LDCBF) constraints. We validate the proposed approach in simulation using a Digit robot in randomly generated environments. The results demonstrate that our approach can generate safe gaits for a non-trivial humanoid robot to navigate environments with randomly generated obstacles in real-time.","Humanoid robots exhibit great dexterity and agility to perform different locomotion activities [1, 2, 3]. They are expected to be deployed to real-life environments such as warehouses and assembly lines. Such clustered environments pose the challenge of navigating around obstacles while maintaining stable walking in real-time [4, 5]. Yet humanoid robots are inherently underactuated due to their unilateral ground contacts; thus, a strong coupling exists between path planning and gait control. Their high-dimensional, nonlinear, and hybrid dynamics further complicate real-time motion planning. For fully actuated legged robots, one can decouple the path planning problem from motion control by finding a collision-free path without accounting for the robot dynamics and motion control [6, 7]. Then, a feedback controller aware of the robot dynamics can be developed to track the provided path. However, under-actuated humanoids would fall if the planned path did not account for the robot dynamics. This coupling of planning and dynamics usually requires solving specific gait optimization problems based on the robot‚Äôs full-order [8, 9, 10] or reduced-order model [11, 12, 3]. However, using the full-order model for long-horizon path planning requires significant computation time, making them non-amenable for real-time online planning. Figure 1: Safe navigation planning is tested in MuJoCo with the Digit robot. To mitigate the computational challenge, reduced-order template models are often used to approximate the walking dynamics of the robot and plan gaits with reduced computational burden [13, 14, 15]. A famous example of such template models is the linear inverted pendulum (LIP) model [16]. In particular, the LIP model represents the robot dynamics using the center of mass (CoM) position and velocity and allows the control of the CoM velocity at the end of the next step. The LIP model provides a lower-dimensional representation of the robot dynamics that can be used to plan a desired foot placement that renders stable walking gaits. In this work, we introduce a modified 3D-LIP model with heading angles to use the LIP model for both path and gait planning and enforce necessary kinematic constraints. Including heading angles and turning rates allows for expressing kinematic constraints in the local coordinate frame of the robot, appropriately addressing the different motion constraints in sagittal and frontal planes, as well as left foot stance and right foot stance, of walking robots. A model predictive control based on the discrete-time step-to-step dynamics of this 3D-LIP model, denoted as LIP-MPC, is proposed to unify path and gait planning. In particular, our MPC formulation uses discrete control barrier functions (DCBFs) for safe obstacle avoidance. Control barrier functions have been successfully applied to controlling legged robots and are now widely used to ensure safety in path planning [17, 18, 19, 20]. DCBF, particularly, is well-suited for the discrete-time step-to-step dynamic model [21, 22]. However, many barrier functions that describe obstacles are nonlinear, leading to nonlinear constraints when foot placement is treated as the decision variable. Hence, despite the 3D-LIP model being linear, the nonlinearity in both kinematic and path constraints hinders the real-time computation of the MPC problem. To mitigate computational efficiency, this paper proposes two key ingredients: pre-computing heading angles and approximated linear DCBFs. Preprocessing the turning rate for each prediction step allows us to linearize the kinematic constraints within the MPC. Additionally, we introduce a novel linear discrete-time Control Barrier Function (LDCBF) to establish linear, feasible obstacle avoidance constraints for convex obstacles. These adjustments transform the optimization problem into a linearly constrained quadratic programming (QP) problem, significantly reducing computational demands and enabling real-time, safety-critical navigation for bipedal robots. The rest of the paper is organized as follows: Section II introduces the 3D-LIP model with heading angles and presents the formulation of the LIP-MPC with linear kinematic constraints for feasible gait planning. Section III details the design of the obstacle avoidance constraints, introducing the novel LDCBF expression. Section IV shows the application of the proposed LIP-MPC in simulation, showcasing the real-time safe navigation of the bipedal robot Digit. We presented the results of two different turning rate preprocessing strategies: global goal-oriented and subgoal-oriented. Finally, Section V concludes the contributions, limitations, and future work."
https://arxiv.org/html/2411.03614v1,Robot Swarming over the internet,"This paper considers cooperative control of robots involving two different testbed systems in remote locations with communication on the internet. This provides us the capability to exchange robots status like positions, velocities and directions needed for the swarming algorithm. The results show that all robots properly follow some leader defined one of the testbeds. Measurement of data exchange rates show no loss of packets, and average transfer delays stay within tolerance limits for practical applications. In our knowledge, the novelty of this paper concerns this kind of control over a large network like internet.","I INTRODUCTION The efficient co-operation between multiple agents situated at distinct locations while pursuing common objectives is an important aspect of multi-agent systems. While the topic raises fundamental questions related to a variety of fields such as communication systems and distributed co-operative control, it is of immense practical interest as well. For example, as schematically represented in Fig. 1, collaborating unmanned vehicles can be used in combat or hazardous environment zones to reduce the risks for human lives. An important kind of cooperation is when agents such as robots are communicating through some network like the Internet. In this article we report on our efforts to develop and implement algorithms designed to manipulate two sets of mobile robots, situated in two geographically distinct locations, in order to cooperatively realize a common task with information exchange through the World Wide Web. While there have been many laboratory testbeds built to explore swarm algorithms, they all use local communications [1]. Our situation is novel because we use internet, with its inherent delays, as the support of our communications. Figure 1: Concept of cooperative unmaned vehicles. In order to experimentally explore algorithms for robotic applications such as the one described above, the University of California Los Angeles (UCLA) Applied Mathematics Department and the University of Cincinnati (UC) School of Dynamic Systems have built their own laboratory testbeds [2, 3]. The testbeds allow the study of single and multiple robots tasked with missions such as path planning, target searching and environmental exploration. The primary objective of the work reported in this article was to establish a connection through the Internet between the two testbeds aimed at exploring cooperative algorithms. It was determined that a robot swarming algorithm would be an ideal candidate to test the different aspects of the communication and co-operative control mechanisms. The experiments reported in this article may be summarized as follows. In one of the testbeds, one robot was designated as the leader and was programmed to follow a predefined path. On both testbeds, the remaining robots were asked to follow the leader based upon a swarming algorithm. Each vehicle (leader and followers) broadcast their positions, speeds and headings to the others through both local radio features and the Internet. Our purpose was to test different configurations: pure simulations between the computers located at UCLA and UC, and a combination of simulations and actual robots, as steps towards meeting the larger objective of implementation of the cooperative control algorithm on actual robots on each testbed communicating over the internet. The remainder of the paper is set as follows: in section II we give an overview of each testbed and describe how network communications are set up. In section III, we describe the swarming algorithm used in the experiments. Section IV presents representative results from the simulations, and Section V presents the conclusions and outlines future research directions."
https://arxiv.org/html/2411.03610v1,LCP-Fusion: A Neural Implicit SLAM with Enhanced Local Constraints and Computable Prior,"Recently the dense Simultaneous Localization and Mapping (SLAM) based on neural implicit representation has shown impressive progress in hole filling and high-fidelity mapping. Nevertheless, existing methods either heavily rely on known scene bounds or suffer inconsistent reconstruction due to drift in potential loop-closure regions, or both, which can be attributed to the inflexible representation and lack of local constraints. In this paper, we present LCP-Fusion, a neural implicit SLAM system with enhanced local constraints and computable prior, which takes the sparse voxel octree structure containing feature grids and SDF priors as hybrid scene representation, enabling the scalability and robustness during mapping and tracking. To enhance the local constraints, we propose a novel sliding window selection strategy based on visual overlap to address the loop-closure, and a practical warping loss to constrain relative poses. Moreover, we estimate SDF priors as coarse initialization for implicit features, which brings additional explicit constraints and robustness, especially when a light but efficient adaptive early ending is adopted. Experiments demonstrate that our method achieve better localization accuracy and reconstruction consistency than existing RGB-D implicit SLAM, especially in challenging real scenes (ScanNet) as well as self-captured scenes with unknown scene bounds. The code is available at https://github.com/laliwang/LCP-Fusion.","I INTRODUCTION Dense visual Simultaneous Localization and Mapping (SLAM) plays a vital role during perception, navigation and manipulation in unknown environments. In recent decades, traditional SLAM methods [1, 2, 3] have made significant progress in localization accuracy and real-time applications. However, due to the use of explicit scene representations like occupancy-grids [4, 5, 6], point cloud [7, 8, 9], Signed Distance Function [10, 11, 12], and surfels [13], which directly store and update limited scene information at fixed resolution without context, they struggle to balance memory consumption and mapping resolution, while being unable to reconstruct complete and consistent surfaces in noisy or unobserved areas. Therefore, recent research has focused on implicit representation using neural networks [14] or radiance fields [15] to encode any points in scenes as continuous function, which can be used to extract isosurfaces at arbitrary resolution or to synthesize realistic unseen views. Utilizing the representation coherence and ability to render unseen views, numerous neural implicit SLAM systems [16, 17, 18] have emerged to perform high-fidelity mapping and camera tracking in various scenes. However, most of them require known scene bounds due to inflexible scene representation [19], resulting in performance degradation or failure in unknown scenarios. Focusing on applications in unknown scenes, one of the mainstream solutions is to allocate implicit feature grids dynamically in surface areas utilizing flexible sparse voxel octree (SVO) [19]. Since SVO-based methods [20] only represent scenes using high-dimensional features in sparse voxel grids, they tend to be sensitive to odometry drift caused by their insufficient local constraints in loop closure regions. This may contribute to inconsistent reconstruction much further, which is shown in Fig. 1. Additionally, with an explicit SDF octree prior, hybrid methods [21] are proposed for precise mapping, but instead use a traditional visual odometry [22] as tracking module. Thus, for the unified dense SLAM system utilizing a neural implicit representation for both tracking and mapping, it is worth investigating in alleviating reconstruction inconsistency caused by localization drift in unknown scenes with potential loop closure. Figure 1: Example from baseline [19] of inconsistent surfaces due to drift in potential loop-closure regions composed of frames 119, 3449 and 5549 (top). Our method can reconstruct unknown scenes with less drift utilizing enhanced local constraints and easily computable SDF priors (bottom). To this end, we introduce LCP-Fusion (a neural implicit SLAM system with enhanced Local Constraints and Computable Prior), which can alleviate drift in potential loop-closure without other external modules. Our key ideas are as follows. First, to handle unknown scene boundaries, we utilize the SVO to dynamically allocate hybrid voxel grids containing coarse SDF priors and residual implicit features, which yields scene geometry and color through sparse volume rendering. Second, through pixel reprojection between frames, we propose a novel sliding window selection strategy based on visual overlap, which not only strengthens local constraints but also alleviates catastrophic forgetting. In addition to only evaluating individual frame, a practical warping loss constraining relative poses is introduced to further improve localization accuracy. Third, to reduce the redundancy of iterations in joint optimization, we adopt an adaptive early ending without significant performance degradation owing to our proposed hybrid representation. We perform extensive evaluations on a series of RGB-D sequences to demonstrate the localization improvement of our method, as well as the applications in real scenes with unknown bounds. In summary, our contributions are: ‚Ä¢ We present LCP-Fusion, a neural implicit SLAM system based on hybrid scene representation, which allocates hybrid voxels with implicit features and estimated SDF priors dynamically in scenes without known bounds. ‚Ä¢ We introduce a novel sliding window selection strategy based on visual overlap and a warping loss constraining relative poses for the enhanced local constraints. ‚Ä¢ Extensive evaluations on various datasets demonstrate our competitive performance in localization accuracy and reconstruction consistency, as well as robustness to fewer iterations and independence on scene boundaries."
https://arxiv.org/html/2411.03591v2,"vMF-Contact: Uncertainty-aware Evidential Learning for Probabilistic Contact-grasp in Noisy Clutter‚Ä†‚Ä†thanks:Karlsruhe Institute of Technology, Karlsruhe, Germany. Email:{name}.{surname}@kit.edu.This work is sponsored by the DFG SFB-1574 Circular Factory project and Baden-W√ºrttemberg Ministry of Science, Research and the Arts within InnovationCampus Future Mobility.","Grasp learning in noisy environments, such as occlusions, sensor noise, and out-of-distribution (OOD) objects, poses significant challenges. Recent learning-based approaches focus primarily on capturing aleatoric uncertainty from inherent data noise. The epistemic uncertainty, which represents the OOD recognition, is often addressed by ensembles with multiple forward paths, limiting real-time application. In this paper, we propose an uncertainty-aware approach for 6-DoF grasp detection using evidential learning to comprehensively capture both uncertainties in real-world robotic grasping. As a key contribution, we introduce vMF-Contact, a novel architecture for learning hierarchical contact grasp representations with probabilistic modeling of directional uncertainty as von Mises‚ÄìFisher (vMF) distribution. To achieve this, we derive and analyze the theoretical formulation of the second-order objective on the posterior parametrization, providing formal guarantees for the model‚Äôs ability to quantify uncertainty and improve grasp prediction performance. Moreover, we enhance feature expressiveness by applying partial point reconstructions as an auxiliary task, improving the comprehension of uncertainty quantification as well as the generalization to unseen objects. In the real-world experiments, our method demonstrates a significant improvement by 39% in the overall clearance rate compared to the baselines. Video: https://www.youtube.com/watch?v=4aQsrDgdV8Y","Recent advances in learning-based robotic manipulation have demonstrated considerable promise, primarily driven by large-scale training datasets [1, 2] and the use of synthetic simulations. However, existing methods often struggle with limited adaptation and generalization in dynamic environments. For instance, in the context of a circular factory [3] ‚Äì a manufacturing facility focused on material reuse and production design for longevity and recyclability - robots are tasked with grasping and manipulating objects under dynamic and uncertain conditions. Hence, accurate quantification of uncertainty is crucial for reliable performance where robotic operations face challenges such as occlusions, sensor noise, view perspective, and the presence of OOD objects. To address these, effective modeling and disentanglement between aleatoric uncertainty, which stems from inherent sensor noise, and epistemic uncertainty that arises from the distribution shift, is essential for improving grasp success rates and ensuring safety in real-world applications. Figure 1: Inference pipeline for vMF-Contact in real-world based on posterior update. For accurate uncertainty quantification, evidential deep learning [4] introduces second-order optimization techniques to achieve precise, deterministic uncertainty estimation. In contrast to Bayesian Neural Networks (BNNs), where sampling-based or variational inferences introduce computational overhead[5], evidential deep learning provides an efficient alternative by comprehensive uncertainty estimation in a single forward path. Specifically, Dirichlet-based evidential models have demonstrated considerable potential for OOD detection in classification [4, 6]. Furthermore, natural posterior networks (NatPN) [7] extend the evidential learning by modeling arbitrary posteriors within the exponential family distributions. Based on this line of approaches, we develop uncertainty-aware grasp detectors, ensuring reliable performance in real-world environments. In summary, as the main contributions: (1) We develop the vMF-Contact, a novel architecture that facilitates probabilistic contact-based grasp learning with posterior updates. (2) We perform theoretical analysis for vMF Contact through an analytically formulated second-order objective, which integrates evidential learning for precise directional uncertainty estimation, ensuring reliable grasp predictions with carefully designed informative prior. (3) To improve the geometric comprehension of the encoder, we employ auxiliary point reconstruction task that enhances both uncertainty quantification and grasp success. In our experiments, we systematically selected and compared existing PointNet-based architectures to evaluate the generalizability of our framework. Our method demonstrate reliable performance in real-world experiments, showing substantial improvements in grasp performance and robustness against OOD scenarios without any sim-to-real adaptation."
https://arxiv.org/html/2411.03581v1,Can Robotic Cues Manipulate Human Decisions? Exploring Consensus Building via Bias-Controlled Non-linear Opinion Dynamics and Robotic Eye Gaze Mediated Interaction in Human-Robot Teaming,"Although robots are becoming more advanced with human-like anthropomorphic features and decision-making abilities to improve collaboration, the active integration of humans into this process remains under-explored. This article presents the first experimental study exploring decision-making interactions between humans and robots with visual cues from robotic eyes, which can dynamically influence human opinion formation. The cues generated by robotic eyes gradually guide human decisions towards alignment with the robot‚Äôs choices. Both human and robot decision-making processes are modeled as non-linear opinion dynamics with evolving biases. To examine these opinion dynamics under varying biases, we conduct numerical parametric and equilibrium continuation analyses using tuned parameters designed explicitly for the presented human-robot interaction experiment. Furthermore, to facilitate the transition from disagreement to agreement, we introduced a human opinion observation algorithm integrated with the formation of the robot‚Äôs opinion, where the robot‚Äôs behavior is controlled based on its formed opinion. The algorithms developed aim to enhance human involvement in consensus building, fostering effective collaboration between humans and robots. Experiments with 51515151 participants (N=51ùëÅ51N=51italic_N = 51) show that human-robot teamwork can be improved by guiding human decisions using robotic cues. Finally, we provide detailed insights on the effects of trust, cognitive load, and participant demographics on decision-making based on user feedback and post-experiment interviews.","1. INTRODUCTION In recent years, the lack of safe and efficient human-robot collaboration in industrial environments has promoted negative perceptions of robots among human workers, substantially preventing the adoption of robotic technologies (≈ûemsettin √áiƒüdem et al., 2023; Welfare et al., 2019). As depicted in Figure 1, robots are still restricted to safety cages, reinforcing the physical segregation between robots and human-operated work spaces. Even in routine industrial assembly tasks, critical thinking and decision-making are often required, emphasizing the need for human oversight and collaboration, as robots alone are insufficient (Pfeiffer, 2016). While robots excel at repetitive tasks, humans are better suited for decision-making in uncertain conditions, necessitating mutual collaboration and co-learning between humans and robots. These insights highlight the need for safe human-robot collaboration to optimize industrial operations and the urgent need for new research into efficient, secure interactions. Figure 1. Problem-motivation example illustrating the real-world challenge of robotic arms operating in safety cages within a can packaging warehouse, highlighting the lack of direct human-robot collaboration and the separation of workspace. The existing literature often overlooks the crucial role of human contribution in human-robot teaming, instead focusing on outsmarting or handling uncertainties in human actions, which are often considered partially known information. As a result, robot control behavior is frequently designed using techniques such as partially observable Markov decision processes (or POMDPs), predictive model control, or reinforcement learning. However, human behavior is inherently unpredictable and mainly irrational (Johnson, 2021). In the absence of strategic guidance or communication toward a collaborative decision, human participants can engage in behaviors that trick, test, or attempt to surpass the capabilities of their robotic counterparts. In recent work, robots regulate human behavior using haptic feedback systems (Grushko et al., 2021) and a combination of visual cues and haptic wristbands (Habibian et al., 2023) to communicate intentions of movement, prompting workers to adjust their actions, thereby preventing collisions. However, the scalability of this approach is limited in complex industrial environments with frequent human-robot interactions, and it may struggle with highly dynamic tasks where trajectories and actions are constantly changing. In (Breazeal, 2002), the robot regulates human behavior through expressive facial expressions, body posture, and vocal signals to convey its effective state. This creates a feedback loop where the human adjusts their decisions based on the robot‚Äôs cues, resulting in synchronization of body language and effective alignment. A key limitation here is the difficulty of sustaining nuanced human-robot interactions over longer durations in unpredictable environments, where emotional synchronization becomes more challenging. Furthermore, the limited anthropomorphic characteristics of the robot can reduce its overall expressive capability. A more detailed examination of robot communication and clues to humans, specifically through robotic eyes, is provided in Subsections 2.2 and 2.4 of Section 2. The process by which human opinions are formed during interactions is a crucial aspect of human-robot collaboration. Robotic cues and communication strategies must be dynamically generated and evolve in response to these opinions. Since human opinion formation is highly non-linear and influenced by various psychological factors that are not yet fully understood, integrating these factors, such as continuously evolving biases and opinions, into robot behavior presents a significant challenge. Balancing robot autonomy with human control in interactive scenarios is equally complex. Given the diversity of human behavior and the variability in individual beliefs, robot opinions must dynamically adapt to human responses and uncertainties in real time, making tuning the parameters of the decision-making model both critical and complex. Moreover, developing robots that can guide human decisions through cues while simultaneously exhibiting anthropomorphic traits and human-like decision-making abilities introduces additional complexities. This requires a robust theoretical understanding of how opinions shaped by robotic cues, acting as external stimuli, lead to agreement or disagreement under a given set of parameters for human and robot mental models, as well as the conditions that give rise to these outcomes. Incorporating humans into robotic decision-making is crucial and fosters nuanced interactions, where humans not only provide real-time feedback but also adapt their strategies based on the robot‚Äôs guidance, enhancing learning for both. In ethically sensitive areas such as healthcare, integrating human oversight in robotic decision-making is important and not only ensures that moral considerations guide outcomes but also enhance trust and safety. By maintaining human control, especially in high-stakes environments, we can provide a crucial fail-safe against potential robotic errors, ensuring that decisions reflect ethical reasoning and mitigate risks effectively. Motivated by these challenges and the identified research gap in guiding human behavior during interactions with robots, as well as the numerous benefits of human involvement in human-robot collaboration, this research investigates the following key research questions. a) RQ1: Can bias or external stimulus be deliberately imparted to human decision making as a psychological factor to effectively achieve consensus with robot during interaction? b) RQ2: If assimilated, how do humans perceive and react to biases conveyed through robotic eye cues in terms of trust, persuasion, and collaboration? c) RQ3: Does persistent disagreement in early interactions motivate humans to subsequently align their choices with those of robots when mediated by visual cues as external biasing factor? d) RQ4: Under a chosen specified set of parameters, can non-linear opinion dynamics, representing both human and robot mental models, with and without bias control, accurately model real-time robot decisions and achieve controlled consensus or dissensus in response to human actions? e) RQ5: Can the robotic eye design presented in this paper serve as an effective anthropomorphic mechanism to guide humans toward a specific collaborative choice with the robot? To establish a foundation for investigating these questions, we first iteratively refined the non-linear opinion dynamics parameters through trial and error, for the experimental settings outlined in Section 3. Under the chosen optimized parameters, we demonstrate how bias can be used as a control parameter in a two-agent, two-choice non-linear opinion system to facilitate a transition from repeated disagreement to agreement. Through numerical parametric and equilibrium continuation analysis, we identified the critical bias settings that enable the transition to both consensus and dissensus. Then, to facilitate consensus between humans and robots, whose opinions evolve dynamically during interactions, we introduced a Bias Control Algorithm that continuously adjusts robot and human biases to align with the collaborative convergence choice of both agents. Through the development of a choice-based decision-making experiment involving a human and a robotic arm, acting as either cooperative or competitive agents, we explored research questions (RQ1-RQ5). The robot was proactively controlled using a behavior control algorithm, and we demonstrated how real-time human opinions can be observed and manipulated through visual cues from a robotic eye as a bias during interactions. The rest of this article is organized as follows: Section 2 provides a comprehensive review of both past and recent related work, delineating the specific contributions this article makes to the field. Section 3 details the experimental setup, including component design and the procedural methodology. Section 4 presents the bias parametric analysis for the selected non-linear dynamics parameter set, along with the proposed opinion formation and robot behavior algorithms. Section 5 presents the incorporation of human into co-learning interactions with the robot, through the use of robotic eye gaze. The subsequent sections 6, 7, and 8 present experimental demonstrations, insights from user feedback analysis, and concluding remarks, respectively."
https://arxiv.org/html/2411.03556v1,VQ-ACE: Efficient Policy Search for Dexterous Robotic Manipulation via Action Chunking Embedding,"Dexterous robotic manipulation remains a significant challenge due to the high dimensionality and complexity of hand movements required for tasks like in-hand manipulation and object grasping. This paper addresses this issue by introducing Vector Quantized Action Chunking Embedding (VQ-ACE), a novel framework that compresses human hand motion into a quantized latent space, significantly reducing the action space‚Äôs dimensionality while preserving key motion characteristics. By integrating VQ-ACE with both Model Predictive Control (MPC) and Reinforcement Learning (RL), we enable more efficient exploration and policy learning in dexterous manipulation tasks using a biomimetic robotic hand. Our results show that latent space sampling with MPC produces more human-like behavior in tasks such as Ball Rolling and Object Picking, leading to higher task success rates and reduced control costs. For RL, action chunking accelerates learning and improves exploration, demonstrated through faster convergence in tasks like cube stacking and in-hand cube reorientation. These findings suggest that VQ-ACE offers a scalable and effective solution for robotic manipulation tasks involving complex, high-dimensional state spaces, contributing to more natural and adaptable robotic systems. Code, data, and training details will be publicly available at srl-ethz.github.io/page-vq-ace/.","Designing systems that mimic the dexterity and adaptability of the human hand is vital in bridging the gap between specialized robotic tools and more versatile systems capable of generalist intelligence. The human hand is a remarkable tool for interaction and expression, and plays a key role in intelligence. With its 27 degrees of freedom (DoF), the hand is capable of an immense variety of movements and postures, enabling fine motor skills that are critical for tasks ranging from delicate manipulation to powerful grasps. However, despite this anatomical complexity, humans do not control each degree of freedom independently with ease. Instead, the control of hand movements is often characterized by coupled motions, where multiple joints move in coordinated patterns. This empirical observation highlights the need to find a more appropriate low-dimensional representation of the motion of human hands. We hypothesize that a good representation of hand motion should capture the following essential characteristics. This representation must be dynamic, reflecting the continuous nature of hand movements rather than static postures. It must also be compact, as human hand movements occupy only a small manifold within the vast space of possible trajectories. Despite this compactness, the representation must remain expressive enough to capture the full range of human-hand capabilities. Additionally, for practical implementation in autoregressive models, the representation should be quantized to facilitate efficient computation and learning. These considerations drive the need for an encoder-decoder network that can effectively compress hand motion sequences into a latent space, reducing the dimensionality of the action space while preserving the essential features required for robotic policy learning. In this work we introduce Vector Quantized Action Chunking Embedding (VQ-ACE), learn a compact, discrete latent space for human hand action sequences with a vector-quantized conditional auto-encoder. This latent space enables robotic systems to efficiently search for optimal policies with anthropomorphic priors. We propose two applications utilizing this latent space: latent sampling model predictive control (MPC) and action chunked Reinforcement Learning (RL). ). Both control algorithms benefit from efficient sampling and exploration during policy search, generating more natural, human-like behaviors in robotic hands. Please refer to the supplementary material for the robot videos. Figure 1: We introduce VQ-ACE, a method that learns a compact representation of complex human hand motion in a lower-dimensional space. For a 1-second action chunk with 11 DoF, our approach encodes it into 5 tokens, each taking one of 4 possible discrete values. This learned latent space can be used in both sampling-based MPC and RL, enabling control algorithms to search for optimal policies from an anthropomorphic prior. Our contributions are as follows. 1. We propose VQ-ACE, a framework for embedding the action chunks of human hand motion into a quantized latent representation. 2. We propose latent sampling MPC, which is a real-time action synthesis algorithm that samples in the latent space. 3. We propose action chunked RL, which is a way to improve the exploration of RL with action priors"
https://arxiv.org/html/2411.03540v1,VLA-3D: A Dataset for 3D Semantic Scene Understanding and Navigation,"With the recent rise of Large Language Models (LLMs), Vision-Language Models (VLMs), and other general foundation models, there is growing potential for multimodal, multi-task embodied agents that can operate in diverse environments given only natural language as input. One such application area is indoor navigation using natural language instructions. However, despite recent progress, this problem remains challenging due to the spatial reasoning and semantic understanding required, particularly in arbitrary scenes that may contain many objects belonging to fine-grained classes. To address this challenge, we curate the largest real-world dataset for Vision and Language-guided Action in 3D Scenes (VLA-3D), consisting of over 11.5K scanned 3D indoor rooms from existing datasets, 23.5M heuristically generated semantic relations between objects, and 9.7M synthetically generated referential statements. Our dataset consists of processed 3D point clouds, semantic object and room annotations, scene graphs, navigable free space annotations, and referential language statements that specifically focus on view-independent spatial relations for disambiguating objects. The goal of these features is to specifically aid the downstream task of navigation, especially on real-world systems where some level of robustness must be guaranteed in an open world of changing scenes and imperfect language. We also aim for this dataset to aid the development of interactive agents that can both respond to commands and ask and answer questions regarding a scene. We benchmark our dataset with current state-of-the-art models to obtain a performance baseline. All code to generate and visualize the dataset is publicly released 111https://github.com/HaochenZ11/VLA-3D. With the release of this dataset, we hope to provide a resource for progress in semantic 3D scene understanding that is robust to changes and one which will aid the development of interactive indoor navigation systems.","Methods combining vision and language have been evolving rapidly with the advent of both Large Language Models (LLMs) [1, 29, 28] and Vision-Language Models (VLMs) [24, 26, 23] pre-trained on significant amounts of data, tackling various 2D tasks such as Visual Question Answering (VQA) [4], image retrieval [20], and image captioning [24]. As we progress towards generalizable embodied intelligence, there is a need for methods that are capable of reasoning in 3D-space and interacting with humans. Using natural language for example, humans are able to refer to objects in a 3D scene in a way that disambiguates the target object, often using the utterance of ‚Äúleast effort‚Äù [33] and making use of relative spatial relationships. An agent that can similarly solve such a problem would be particularly valuable in robotics fields such as indoor-navigation with applications as in-home assistants. The pursuit of such agents that can identify and understand 3D scenes, consolidate visual input with language semantics, and display robust performance for real-world deployment, however, presents various challenges. First, the scene can have hundreds of objects, contain objects belonging to fine-grained classes, and have many similar objects [25]. Second, human referential language often involves spatial reasoning, affordances, open-vocabulary language, and may even be incorrect or refer to something that does not exist, e.g. ‚Äúthe remote on the table‚Äù when the remote is actually on the sofa. Third, the scale of available vision-language data in the 3D space pales in comparison to the amount of 2D data, which was crucial to the success of 2D vision-language learning methods [21, 8]. Despite impressive recent advancements with foundation models, such problems remain difficult when applied to robotics as current methods fail to offer the accuracy and robustness needed for real-world deployment [14]. To this end, we propose a novel dataset based on 3D scenes from a diverse set of existing scans of indoor environments that provides a unique resource for training referential object grounding methods. Building on top of the scans, we provide 1) point clouds as they enable learning directly from 3D geometric and visual information [5], 2) extracted object-level attributes and semantic class labels for discriminating and categorizing objects, 3) large-scale scene graphs with spatial relations for a structured representation of a scene, 4) referential language statements to support vision-language grounding using natural language, and 5) traversable free space annotations to explicitly connect to downstream navigation tasks. The inclusion of dense scene graphs and traversable free space are two features that particularly distinguish our dataset from previous object-referential datasets. The scene graphs allow for a robust representation for the semantics in each scene that can be used to guide the grounding task and also to infer when the language statement is invalid. Free space annotations give the ability to generate referential statements that refer not just to objects but to spaces or paths. Along with our dataset, we also release the code for the entire dataset generation process, demonstrating that synthetic heuristic-based generation methods can aid the efficient generation of large-scale datasets. A custom dataset visualizer tool is also provided to visualize individual scenes and regions from our dataset. With our dataset, we test two state-of-the-art (SOTA) referential object grounding baseline models on our data to verify that such low-level semantic understanding remains a challenging problem and provide a starting point to the identification of where SOTA methods may fail. This can then aid the subsequent development of higher-fidelity 3D vision-language methods that reason over real-world scenes. A sample from our dataset is shown in Figure 1. Figure 1: Sample region from the dataset visualized with (a) a scene graph (a) and (b) a corresponding referential statement ."
https://arxiv.org/html/2411.03532v1,A Behavior Architecture for Fast Humanoid Robot Door Traversals,"Towards the role of humanoid robots as squad mates in urban operations and other domains, we identified doors as a major area lacking capability development. In this paper, we focus on the ability of humanoid robots to navigate and deal with doors. Human-sized doors are ubiquitous in many environment domains and the humanoid form factor is uniquely suited to operate and traverse them. We present an architecture which incorporates GPU accelerated perception and a tree based interactive behavior coordination system with a whole body motion and walking controller. Our system is capable of performing door traversals on a variety of door types. It supports rapid authoring of behaviors for unseen door types and techniques to achieve re-usability of those authored behaviors. The behaviors are modelled using trees and feature logical reactivity and action sequences that can be executed with layered concurrency to increase speed. Primitive actions are built on top of our existing whole body controller which supports manipulation while walking. We include a perception system using both neural networks and classical computer vision for door mechanism detection outside of the lab environment. We present operator-robot interdependence analysis charts to explore how human cognition is combined with artificial intelligence to produce complex robot behavior. Finally, we present and discuss real robot performances of fast door traversals on our Nadia humanoid robot. Videos online at https://www.youtube.com/playlist?list=PLXuyT8w3JVgMPaB5nWNRNHtqzRK8i68dy.","The humanoid form has uniquely diverse mobility and manipulation capabilities that drive its suitability as the embodiment of a general purpose robot. This has lead to the pursuit of building humanoid robots to perform useful tasks in spaces designed for humans. One component that is lacking in the literature is how robots can quickly and effectively perform useful whole-body loco-manipulation tasks automatically. There are many challenges in building a system for generating and executing fast loco-manipulation behaviors. Part of the challenge is being able to create behaviors for a wide variety of tasks and to do so in a short timeline as needed. We present an authoring system which contains many features that when put together, provide sufficient capability in this regard. Another aspect of the challenge is in getting the behaviors working on a real robot. We also present an integrated system for testing and iterating on the execution of these behaviors on the real robot. This topic of behavioral system integration is important to the understanding of how to build useful humanoid robotics systems. There are many aspects of design and some components, if overlooked or misunderstood, could have a negative impact on performance or long term results. As society builds more humanoid robots for military and industrial purposes, this research could help in grounding system level design principles. Furthermore, we believe this topic of research does not lose its relevance when modern AI approaches are taken, as we think many of the core functionalities and principles explored in this paper are fundamental to humanoid robot loco-manipulation. A key element to this is behavior generation ‚Äì assembling motor skills to compose tasks and assembling tasks to compose behaviors. In this work we utilize an operator to author behaviors as a tree structure of nodes with general capabilities. This paper explores techniques that build upon known, useful principles in the literature in an effort to nudge the state of the art of behavior authoring on humanoid robots forward. We place a core focus on speeding up the authoring process of behaviors by supporting online behavior generation and modification. By providing a framework for building and modifying behavior trees and action sequences at runtime, most behavioral changes can be done online and quickly enacted on the robot. The increase in authoring speed has allowed us to create more than twenty varieties of working loco-manipulation behaviors on the real robot in the span of one year. The behaviors are primarily different types of door traversals but debris clearing behaviors such as moving trash cans and couches are also included. Our behavior coordination architecture is designed to create fast, resilient, and reusable loco-manpipulation behaviors for humanoid robots as part of a larger effort towards robot that do useful and economical work in the real world. To achieve this, we incorporated state-of-the-art methodologies in building a uniquely complete set of functionalities. The concepts of affordance templates, action primitives, sequential composition, concurrent action layering, behavior trees, and human-robot interdependence were carefully included into the presented novel implementation. This is currently no body of research available that explores door traversals with bipedal humanoid robots. There are many publications which explore door traversals on wheeled base robots with arms, such as [14], [4] [36], and [21]. We believe this is a critical area of study, as the humanoid form factor is especially well-suited for navigating complex environments encountered in disaster response, space exploration, and urban settings. The advantages of the humanoid structure over wheeled and other mobile platforms are explored further in [18]. Additionally, in this work, we find there are important differences in the planning required for wheeled base and legged robots. Figure 1: The Nadia humanoid robot performing a right pull lever handle door traversal using cycloidal drive forearms and Sake grippers. Corresponding video: 2024NadiaRightPullLeverHandleCycloidLowRes.mp4 To build modular and adaptable behaviors, we structured them as a composition of low-level actions. May of these low-level actions are derived from the Affordance Template (AT) framework[10, 11]. ATs are generally a sequence of primitive tasks that are parameterized to a type of higher level task, referred to as an affordance. For example, an affordance template for opening a drawer could be a set of grasp approach, grasp, pull drawer, and let go actions that are available to execute any time the robot see a certain type of drawer. It is a way of defining general action plans for ahead of time for high level tasks the robot is expected to encounter. Additionally, the AT framework provides an integrated environment for authoring templates and provides a general definition language for robot-agnostic manipulation. The AT framework is also naturally extensible. In recent years, advanced planners such as for stance and grasp generation, navigation, and motion planning have been integrated[12]. There is also a growing area termed ‚Äúaffordance primitives‚Äù[29, 28] (APs) which are a way to model actions with constrained movement, such as turning a valve or closing a drawer. A special AP called a screw primitive has been used to define impedance controlled manipulation of valves and drawers. ATs have been used on NASA‚Äôs Valykrie humanoid robot to perform a car door opening, retrieve an improvised explosive device (IED) bag, drop the IED bag off in a total containment vessel (TCV) tray, and push the TCV tray in and push it‚Äôs button[20]. The operator could load, edit, and execute ATs on the fly which provided a human-in-the-loop environment. To coordinate the ATs, and more in general the low-level actions, we employ Behavior Trees[5, 13] (BTs), which are a popular state-of-the-art model for representing and orchestrating complex behavior. BTs define logical operator nodes such as sequence, fallback, and parallel which work to control the execution flow of a behavior. In BTs, actions are performed by the leaf nodes which command the robot and gather environmental data. Finally, BTs provide reactivity though a ticking system in which each tick starts at the root node. This is in contrast to state machines in which each tick starts from the previous state. By continuously re-evaluating from the top down, from tick to tick there are pathways to ending up in very different parts of the tree without explicit connections between those parts. Recently, BTs have been used to autonomously navigate cluttered terrain with a centaur wheeled-legged robot[6]. It demonstrates successful navigation on a real hardware system with legs. The behavior tree in this work manages a two-level planner for coordinating intricate locomotion over a debris field of cinder blocks. It includes reactivity to contacts and failures and decides on the next appropriate action to achieve a level of resiliency. However, we are not aware of any prior work that illustrates an application of BTs to loco-manipulation behaviors on bipedal humanoid robots. Behavior trees and low-level actions are integrated into an interface grounded in Coactive Design (CD)[17, 19] principles. CD is a methodology which gives structure to the design and engineering of systems from the perspective of the interdependence of humans and machines. CD is an iterative process that is comprised of three main steps: identifying, documenting, and iterating on interdependence relationships. Interdependence analysis (IA) charts are used to document the interdependence relationships, which illustrate the connections between system components. CD was previously used for the design and development of the operator interface used by IHMC in the 2015 DARPA Robotics Challenge (DRC) Finals. A later analysis details how the methodology led to success in the competition[18]. MIT‚Äôs DRC team developed an alternative system called Director[24] which used AT concepts in a framework to integrate robot autonomy components and task execution. Director had increased autonomy compared to tools used by other teams in the DRC. It featured advanced planners for manipulation and locomotion, an ‚Äúoperator in the loop‚Äù pipeline for executing actions, a 3D scene with interactive widgets, and a way to make custom panels of widgets. A Python program editor was embedded directly in the user interface to write scripts for tasks at the behavior level. Figure 2: An all inclusive overview of the parts involved in this work. A well-designed interface that supports the robot‚Äôs directability and observability is crucial, but equally essential is the capability to accurately detect objects, as they often serve as key points of interaction during tasks. YOLO-based methods have recently become popular for detecting doors[2]. When combined with the planar region extraction of the door panel, the position and orientation of door handles can be obtained. [21] uses YOLACT image segmentation to obtain the door handle position and a RANSAC plane segmentation algorithm to obtain the orientation. [25, 26] present a planar region extraction algorithm which can be used to detect door panels. Together, these detection and segmentation methods provide the foundational perception capabilities needed for effective interaction with doors. In this paper, we present a system for authoring and operating bipedal loco-manipulation behaviors. To validate our approach, we present and demonstrate a variety of fast door traversal behaviors on real hardware. Our implementation builds on state-of-the-art techniques and includes several novel contributions: ‚Ä¢ To enable increasing the speed of humanoid door traversal behaviors, we developed a system for an operator to compose concurrent sequences of locomotion and manipulation action primitives. ‚Ä¢ Our integrated behavior framework enables the operator and robot to author reusable, fast, and resilient whole body loco-manipulation behaviors for types of doors that have not yet been encountered. ‚Ä¢ To detect door opening hardware well enough to construct and perform door traversal behaviors outside of a lab environment, we developed a novel detection system that combines YOLO-based object detection and segmentation, depth correspondence, and rapid planar region extraction."
https://arxiv.org/html/2411.03494v1,"An Open-source Sim2Real Approach for Sensor-independent Robot Navigation in a Grid‚Ä†‚Ä†thanks:1 Equal Contributions.‚Ä†‚Ä†thanks:This work was developed as part of the ME-599: Machine Learning Control course project and supported by the Department of Mechanical Engineering, University of Washington, Seattle, WA, USA.","This paper presents a Sim2Real (Simulation to Reality) approach to bridge the gap between a trained agent in a simulated environment and its real-world implementation in navigating a robot in a similar setting. Specifically, we focus on navigating a quadruped robot in a real-world grid-like environment inspired by the Gymnasium Frozen Lake ‚Äî a highly user-friendly and free Application Programming Interface (API) to develop and test Reinforcement Learning (RL) algorithms. We detail the development of a pipeline to transfer motion policies learned in the Frozen Lake simulation to a physical quadruped robot, thus enabling autonomous navigation and obstacle avoidance in a grid without relying on expensive localization and mapping sensors. The work involves training an RL agent in the Frozen Lake environment and utilizing the resulting Q-table to control a 12 Degrees-of-Freedom (DOF) quadruped robot. In addition to detailing the RL implementation, inverse kinematics-based quadruped gaits, and the transfer policy pipeline, we open-source the project on GitHub and include a demonstration video of our Sim2Real transfer approach. This work provides an accessible, straightforward, and low-cost framework for researchers, students, and hobbyists to explore and implement RL-based robot navigation in real-world grid environments.","Robotics research has increasingly relied on simulated environments to develop, test, and optimize autonomous systems before deploying them in the real world. A challenge in this field is the Sim2Real (Simulation to Reality) gap, where models trained in simulation often fail to perform as well in real-world environments due to differences between simulated and physical realities [1, 2]. Assessing this gap is essential for deploying autonomous systems in applications such as navigation, industrial automation, and robotic manipulation. The Sim2Real challenge in robotics has driven significant research efforts aimed at introducing new techniques to narrow this gap. Recent advancements include approaches such as bridging the Sim2Real visual gap using natural language [8], benchmarking the Sim2Real gap in cloth manipulation [9], addressing the Sim2Real gap in robotic 3-D object classification [10], and more discussed in the related work section. Despite these advancements, many solutions necessitate complex setups or expensive equipment, limiting their accessibility to freshman students, hobbyists, and new researchers. To effectively bridge the Sim2Real gap in robotics, more research is required to develop simple and accessible solutions that can be easily implemented by a broader range of users. In this work, we present such a solution by using the Gymnasium Frozen Lake environment [12] ‚Äî an accessible and free platform with a low learning curve to develop and test Reinforcement Learning (RL) algorithms. The Frozen Lake environment features a simple simulated grid layout where an agent is trained to avoid the holes and navigate autonomously, making it ideal for robotics applications. We adopted this environment to address a real-world sensor-independent navigation challenge in robotics. Our method involves training an RL agent in Frozen Lake and transferring the learned policy to a physical 12 Degrees-of-Freedom (DOF) quadruped robot to navigate within a grid autonomously without using sensors. Our work aims to: ‚Ä¢ Develop and implement a straightforward pipeline that transfers policies learned in the Frozen Lake environment to a physical robot. ‚Ä¢ Create a simple, low-cost, and open-source quadruped robot that avoids obstacles and navigates in a grid using the learned policies without relying on sensors or additional real-world training. The motivation behind choosing the Frozen Lake environment is: (i) It is a well-known RL benchmark with a low learning curve, and (ii) It offers a grid-like environment that can represent many real-world application scenarios, such as navigation within a warehouse, manufacturing facilities, and airports. The pipeline developed in this paper involves training an RL agent in simulation, creating a Q-table, transforming the Q-table into coordinates, and writing 12 DOF gait scripts to convert the coordinates into robot movement. Additionally, all hardware designs and software are open-sourced on GitHub 111Opensource Link: https://github.com/mehrab-abrar/Sim2Real to facilitate full replication and further research and a demonstration video is available on YouTube 222Video of the Robot: https://www.youtube.com/watch?v=dDKQaN_zsvU. This work represents, to the best of our knowledge, the first implementation of Sim2Real robot navigation using the Frozen Lake environment. Figure 1: Gymnasium Frozen Lake-inspired Sim2Real architecture for grid navigation"
https://arxiv.org/html/2411.03487v1,Enhancing Exploratory Capability of Visual Navigation Using Uncertainty of Implicit Scene Representation,"In the context of visual navigation in unknown scenes, both ‚Äúexploration‚Äù and ‚Äúexploitation‚Äù are equally crucial. Robots must first establish environmental cognition through exploration and then utilize the cognitive information to accomplish target searches. However, most existing methods for image-goal navigation prioritize target search over the generation of exploratory behavior. To address this, we propose the Navigation with Uncertainty-driven Exploration (NUE) pipeline, which uses an implicit and compact scene representation, NeRF, as a cognitive structure. We estimate the uncertainty of NeRF and augment the exploratory ability by the uncertainty to in turn facilitate the construction of implicit representation. Simultaneously, we extract memory information from NeRF to enhance the robot‚Äôs reasoning ability for determining the location of the target. Ultimately, we seamlessly combine the two generated abilities to produce navigational actions. Our pipeline is end-to-end, with the environmental cognitive structure being constructed online. Extensive experimental results on image-goal navigation demonstrate the capability of our pipeline to enhance exploratory behaviors, while also enabling a natural transition from the exploration to exploitation phase. This enables our model to outperform existing memory-based cognitive navigation structures in terms of navigation performance. Project page: https://github.com/IRMVLab/NUE-NeRF-nav","When searching for a target in an unfamiliar environment, our subconscious instinctively prioritizes exploring to establish cognitive. Upon locating the target, we shift to exploitation, navigating towards it. When a robot engages in visual navigation tasks in an unknown environment, as shown in Fig. 1, it also benefits from both exploratory and exploitative thinking. However, existing cognitive navigation frameworks primarily focus on the robot‚Äôs performance in the exploitation phase, neglecting the design of its exploratory behavior. We intend to use implicit scene representation as the memory structure of our navigation pipeline, specifically emphasizing the enhancement of the robot‚Äôs exploratory capability. This enhancement enables the robot to rapidly establish environmental awareness, discover target-related cues, and transition into the exploitation phase. Figure 1: Navigation of robots in an unknown environment. In visual navigation of robots in unknown environments, the process involves two phases: exploration and exploitation. Initially, the robot explores based on uncertainty to refine its cognitive structure, transitioning to navigation toward detected target-related cues in the environment. Figure 2: The overall architecture of NUE. Firstly, real-time image input is used for online cognitive generation and perceptual feature extraction. Secondly, cognitive information is extracted to generate exploratory thinking and exploitative thinking. Eventually, multiple thinking is integrated, and navigational actions are generated. In this paper, we adopt the Neural Radiance Fields (NeRF) [1] as our memory structure. In recent years, NeRF has shown excellent performance in reconstructing 3D scenes and synthesizing novel views [2]. Due to its compact implicit scene representation capability and network-based structure, NeRF has shown great potential in downstream navigation tasks that rely on long-term information representation. However, since our optimization of NeRF is performed online, its high signal-to-noise output early in navigation struggles to provide valuable environmental memory information. To overcome this challenge, we use an estimate of NeRF‚Äôs uncertainty and leverage it to foster exploratory thinking within the robot, considering that exploration can speed up the establishment of implicit representations. We propose the Navigation with Uncertainty-driven Exploration (NUE) pipeline, as illustrated in Fig. 2, which is end-to-end and fully differentiable. Firstly, we conduct online training on NeRF to generate cognition of the scenes. To enhance the exploration capability of the robot and reduce the noise of NeRF in the initial navigation phase, we include an estimate of NeRF‚Äôs uncertainty. Secondly, we extract distinct components, namely uncertainty and spatial information, from NeRF. These components are subsequently compressed into feature representations. The uncertainty feature is leveraged to augment exploration capabilities, while the spatial feature is harnessed to optimize exploitation performance. Finally, we adaptively fuse the two features and output navigation actions through an action generator. Our main innovations can be summarized as follows: ‚Ä¢ We propose NUE, an end-to-end visuomotor navigation pipeline integrating NeRF as a cognitive structure. By leveraging the compact scene representation capabilities of NeRF, we extend its application from the perception domain to the control domain. ‚Ä¢ We utilize the estimation of NeRF‚Äôs uncertainty to enable the robot to exhibit exploratory behavior. Additionally, our model successfully balances exploratory and exploitative thinking, achieving seamless integration between the exploration and exploitation stages. ‚Ä¢ Experimental results demonstrate that NUE significantly improves navigation performance compared to existing cognitive memory structures. Interpretability experiments validate that NUE effectively generates and balances exploration and exploitation behaviors."
https://arxiv.org/html/2411.03483v1,Robust Generalized Proportional Integral Control for Trajectory Tracking on Soft Actuators: Towards a Pediatric Wearable Robotic Device,"This work presents the design and control of a new soft pneumatically-actuated wearable device for infants, which supports two degrees of freedom equivalent shoulder motion (abduction/adduction and flexion/extension). The developed device comprises two independently-controlled soft fabric-based pneumatic actuators that are designed and fabricated in-house. A robust generalized proportional integral controller is then proposed to regulate the pneumatic actuators and move the arm along desired points within the reachable workspace. Inertial measurement unit sensors embedded within the wearable provide real-time joint feedback. Experimental data from performing reaching tasks using an engineered mannequin are obtained and compared to evaluate the performance of the controller with ground truth obtained via motion capture. Results indicate solid trajectory tracking performance of the proposed controller for motions of the shoulder joint when actuated both exclusively and simultaneously in assisting flexion/extension and abduction/adduction, respectively. Focus on low-level joint angle control. Can work with other controllers. Past work, position controller.","Soft robotics technology has been increasingly integrated into wearable assistive devices (e.g., [refs]) owing to the former‚Äôs inherent flexibility and adaptability, softness, and lower profile compared to rigid-only devices [ref]. This present paper targets specifically Several types of motor delays in infants can cause significant adverse impact on lifelong mobility [adolph2019ecological, campos2000travel], including that of the upper extremities, such as congenital physical disabilities and neuromotor pathologies (such as brain injuries [christy2016technology], cerebral palsy [redd2019development] or generalized hypotonia [jones1990emg]). Early intervention that promotes and stimulates active exploration of the environment [angulo2001exploration, blauw2005systematic] is directly linked to one‚Äôs cognitive development [diamond2000close, herskind2015early, novak2017early]. As such, extending the range of arm motion in reaching tasks for infants with upper extremity mobility impairments is desirable for better quality of life and essential for their development into adulthood. Specifically, upper extremity (UE) motor skills (which are related to fine motor skills) involve reaching and manipulation tasks, and are typically acquired from birth to six months of age, with cognitive changes largely occurring the first two years of life [lobo2014not]. Despite a range of assistive devices proposed in the literature, designing devices to promote motor exploration in infants is not analogous to scaling down devices engineered for older children or adults, as infants‚Äô kinematic parameters of motion differ from those of older populations [konczak1997development, trujillo2017development, dechemi2021babynet, addyman2017embodiment, morange2019visual]. The use of assistive wearable devices is an effort toward this goal. Soft assistive wearable devices, in particular, can provide new methods of actuation, sensing, and control combined, and also facilitate motion while providing comfort and safety and not constraining the upper limbs. Various actuation methods can utilize 3D-printing [yap2016high, schaffner20183d], casting [li2020high, kokkoni2020development] and even fabric [kim2021compact, nassour2020high, fu2022textiles, yap2017fully] to improve strength [simpson2020upper, o2017soft] and minimize fatigue [nassour2020high, o2020inflatable]. Understanding design and sensing methods for soft actuators [tapia2020makesense] and design considerations for wearables in infants [trujillo2017development] is also essential. Figure 1: Snapshots of a controlled reaching task supported by our pediatric assistive wearable device utilizing the novel soft actuators developed in this work. The accompanying video offers more visual information and contains footage regarding the experiments conducted herein. This paper presents a new soft pneumatically-actuated wearable device for supporting two degrees of freedom (DoFs) equivalent shoulder motion (abduction/adduction‚ÄìAB/AD and flexion/extension‚ÄìF/E) for pediatric shoulder assistance in reaching motions. Further, the paper develops a closed-loop robust generalized proportional integral (GPI) controller based on proprioceptive feedback (Fig. 1). Compared to our previous related works [kokkoni2020development, sahin2022bidirectional, mucchiani2022closed] this paper innovates both on design and control. Specifically, we actively actuate a second DoF in the shoulder joint via a novel bellow type soft actuator design presented herein, and propose a closed-loop control method. We estimate the relationship between pneumatic actuation Pulse-Width-Modulation (PWM) values and joint angle variation and the controller acts in the sense of tracking trajectories in the joint space. Through hardware implementation and experimentation utilizing real time proprioceptive feedback with Inertial Measurement Units (IMUs) and Motion Capture (Mocap) ground truth data, the proposed controller is shown to track correctly interpolated trajectories for both AB/AD and F/E shoulder motion separately, as well as simultaneously. In addition, to evaluate the device and controller robustness under expected motion primitives common to infants, periodic trajectories by sine waves and teach and repeat trajectories were also evaluated. Therefore, considering the importance of assisting shoulder motion in reaching tasks, this work‚Äôs main contributions can be summarized as follows. ‚Ä¢ Hardware design for shoulder flexion/extension. ‚Ä¢ Trajectory tracking control design utilizing pneumatic soft actuators and embedded proprioceptive sensing. ‚Ä¢ Experimentation with the wearable device considering relevant trajectories for shoulder motion assistance in infant reaching tasks."
https://arxiv.org/html/2411.03481v1,Chance-Constrained Convex MPC for Robust Quadruped LocomotionUnder Parametric and Additive Uncertainties,"Recent advances in quadrupedal locomotion have focused on improving stability and performance across diverse environments. However, existing methods often lack adequate safety analysis and struggle to adapt to varying payloads and complex terrains, typically requiring extensive tuning. To overcome these challenges, we propose a Chance-Constrained Model Predictive Control (CCMPC) framework that explicitly models payload and terrain variability as distributions of parametric and additive disturbances within the single rigid body dynamics (SRBD) model. Our approach ensures safe and consistent performance under uncertain dynamics by expressing the model‚Äôs friction cone constraints, which define the feasible set of ground reaction forces, as chance constraints. Moreover, we solve the resulting stochastic control problem using a computationally efficient quadratic programming formulation. Extensive Monte Carlo simulations of quadrupedal locomotion across varying payloads and complex terrains demonstrate that CCMPC significantly outperforms two competitive benchmarks: Linear MPC (LMPC) and MPC with hand-tuned safety margins to maintain stability, reduce foot slippage, and track the center of mass. Hardware experiments on the Unitree Go1 robot show successful locomotion across various indoor and outdoor terrains with unknown loads exceeding 50% of the robot‚Äôs body weight, despite no additional parameter tuning. A video of the results and accompanying code can be found at: https://cc-mpc.github.io/.","Quadrupedal robots have demonstrated significant potential in various industrial applications and search and rescue missions. These robots enhance productivity by transporting heavy loads and traversing diverse terrains [1]. However, preventing falls in dynamic environments remains a critical challenge [2]. Inaccurate system models, external disturbances, and unpredictable payload variations can cause deviations from planned motions, resulting in unintended contact locations or timing errors [3]. As shown in Fig. 1, classical Model Predictive Control (MPC) methods often struggle with these discrepancies, leading to instability, foot slippage, or even falls. Conventional model-based control methods for quadrupedal locomotion either fail to account for dynamics uncertainties, as in Differential Dynamic Programming (DDP) [4], or become computationally infeasible when doing so, as with Stochastic Linear Complementarity Problems (SLCPs) [5]. Model-free Reinforcement Learning (RL) aims to generalize robot locomotion strategies across diverse environments through offline training, followed by online deployment. While these neural network policies work well in practice, they often lack interpretability and may require frequent re-training to ensure reliable deployment [6]. In contrast, Stochastic Model Predictive Control (SMPC) directly incorporates uncertainties into the control design by modeling them as probability distributions of disturbances [7, 8]. Unlike deterministic MPC, SMPC permits a small probability of constraint violation. In our experiments, we set this probability to 5%, corresponding to a 2‚Å¢œÉ2ùúé2\sigma2 italic_œÉ confidence level. As a result, 95% of outcomes are expected to remain within constraints under Gaussian disturbances [9, 10]. This formulation allows the controller to balance conflicting objectives, such as following a desired trajectory while mitigating unstable behaviors across a range of real-world terrain disturbances. Figure 1: Chance-Constrained MPC (bottom) stabilizes the robot by handling a distribution of inertial uncertainties from 6 kg dumbbells (1) and contact uncertainties, from planks (2). Linear MPC (top) fails under these conditions. In this work, we propose a novel Chance-Constrained MPC (CCMPC) algorithm‚Äîa specific form of SMPC‚Äîto generate ground reaction forces for quadrupedal robots. Our approach models mass, inertia, and contact sequences as stochastic variables. By formulating the control problem as a quadratic programming (QP) problem [11], we achieve real-time solve rates at ‚àºsimilar-to\sim‚àº500 Hz, comparable to Linear MPC (LMPC). We validate CCMPC through extensive simulations and hardware experiments on the Unitree Go1 robot. Our approach achieves superior performance over traditional methods in maintaining stability, reducing foot slippage, and supporting payloads exceeding 50% of the robot‚Äôs weight across muddy slopes, stairs, grass, and gravel. This is accomplished using a unified control policy that effectively handles different terrain conditions and payload variations without the need for parameter tuning. The key contributions of this paper are summarized as follows: ‚Ä¢ We develop a CCMPC algorithm tailored for quadrupedal robots to handle disturbances from variable payloads and complex terrain dynamics. ‚Ä¢ The control problem is formulated as a quadratic program, achieving fast solve times suitable for real-time application. ‚Ä¢ We validate our method through simulations and hardware experiments. In simulations, CCMPC achieves a 100% success rate across multiple gaits, compared to 39.2% for Linear MPC (LMPC) and 75.7% for hand-tuned MPC. To our knowledge, this is the first SMPC implementation for quadrupedal robots on hardware."
https://arxiv.org/html/2411.03416v1,Accelerating Gaussian Variational Inference for Motion Planning Under Uncertainty,"This work addresses motion planning under uncertainty as a stochastic optimal control problem. The path distribution induced by the optimal controller corresponds to a posterior path distribution with a known form. To approximate this posterior, we frame an optimization problem in the space of Gaussian distributions, which aligns with the Gaussian Variational Inference Motion Planning (GVIMP) paradigm introduced in [1]. In this framework, the computation bottleneck lies in evaluating the expectation of collision costs over a dense discretized trajectory and computing the marginal covariances. This work exploits the sparse motion planning factor graph, which allows for parallel computing collision costs and Gaussian Belief Propagation (GBP) marginal covariance computation, to introduce a computationally efficient approach to solving GVIMP. We term the novel paradigm as the Parallel Gaussian Variational Inference Motion Planning (P-GVIMP). We validate the proposed framework on various robotic systems, demonstrating significant speed acceleration achieved by leveraging Graphics Processing Units (GPUs) for parallel computation. An open-sourced implementation is presented at https://github.com/hzyu17/VIMP.","Motion Planning is one core decision-making component in autonomous robotic systems [2, 3]. Given an environment, a start configuration, and a goal configuration, a motion planner computes a trajectory connecting the two configurations. The trajectory optimization paradigm [4, 5] formulates the motion planning problem as an optimization over all admissible trajectories with dynamics and environment constraints. An ‚Äòoptimal‚Äô plan can be obtained by solving the optimization program by minimizing certain optimality indexes, such as time consumption, control energy, and safety. In real-world scenarios, uncertainties such as modeling and actuation noises and external disturbances arise and affect the quality of the motion plans in the execution phase. To reduce its impacts, motion planning under uncertainties takes into account the uncertainties in the planning phase in their formulations [6]. Stochastic optimal control and probabilistic robotics [7, 8] provide a principled framework to address this problem, where noises are explicitly modeled in the robots‚Äô dynamics, and the optimality index is converted into a statistical one over the trajectory distribution space. In this work, the uncertainties are modeled as Wiener processes injected into linear dynamics. The resulting underlying robot dynamics are linear Stochastic Differential Equations (SDEs) [9]. Gaussian Process Motion Planning (GPMP) paradigm [10, 11] leveraged this dynamic model in the motion planning problems. With this formulation, the deterministic trajectory optimization with collision-avoiding constraints becomes a probabilistic one, and the optimal motion plan can be written as a posterior probability. The objective in GPMP is solving a trajectory that maximizes this posterior probability, turning trajectory optimization into a probability inference problem. The solution obtained from GPMP is still a deterministic trajectory. Leveraging planning-as-inference dual formulation, more comprehensive statistical methods can be applied to solve motion planning problems. Variational Inference [12, 13] formulates the inference problem into an optimization within a parameterized proposal distribution family. The solution is an optimized proposal distribution close to the posterior measured by the Kullback‚ÄìLeibler (KL) divergence. VI can be applied to solve for a trajectory distribution in the planning-as-inference formulation [14, 15, 1, 16, 17]. The resulting trajectory distribution [1] exhibited superior performances in challenging tasks such as planning through a narrow gap [18], and this formulation is naturally connected to entropy-regularized robust motion planning [19, 20, 21]. Introducing distributional variables increases the dimension of the problem. The joint covariance over the whole trajectory is quadratic in the trajectory length. Fortunately, in motion planning problems, the underlying probabilistic graph enjoys a sparsity pattern [22], which can be leveraged to factorize the target posterior and reduce the number of variational variables [23, 21]. In this work, we further leverage this sparse factor and the parallel computing on GPUs to accelerate the computation of two key components in the GVI motion planning paradigm: the parallel computation of the collision likelihood and the computation of marginal covariances over the sparse factor graph. The variational inference over trajectory distributions is dual to stochastic optimal control problem for the same Gaussian process dynamics, which is the special case of the duality between probability graph inference and optimal control [24, 25, 19, 20] when the dynamics are linear Gaussian [21]. In this work, we start from a stochastic control formulation to obtain the target posterior distribution in GPMP for the nonlinear motion planning problems for Gaussian processes. This posterior is the distribution induced by the optimal solution controller. GVIMP is an approximate inference approach [26] to optimal path distribution in the Gaussian distribution space. This paper is organized as follows. Section II introduces preliminary knowledges. In Section III we introduce our problem formulation. Section IV introduces the proposed algorithm. Our framework is illustrated in Section V through numerical experiments, followed by a conclusion in Section VI."
https://arxiv.org/html/2411.03409v1,STEER: Flexible Robotic Manipulation via Dense Language Grounding,"The complexity of the real world demands robotic systems that can intelligently adapt to unseen situations. We present STEER, a robot learning framework that bridges high-level, commonsense reasoning with precise, flexible low-level control. Our approach translates complex situational awareness into actionable low-level behavior through training language-grounded policies with dense annotation. By structuring policy training around fundamental, modular manipulation skills expressed in natural language, STEER exposes an expressive interface for humans or Vision-Language Models (VLMs) to intelligently orchestrate the robot‚Äôs behavior by reasoning about the task and context. Our experiments demonstrate the skills learned via STEER can be combined to synthesize novel behaviors to adapt to new situations or perform completely new tasks without additional data collection or training. Project website: https://lauramsmith.github.io/steer","Consider the breadth of situations a human encounters on a daily basis, from pouring a cup of coffee in their kitchen to grabbing objects from a cluttered supply closet. Designing robot systems that can navigate these varied, nuanced scenarios is a major challenge, requiring systems that can adapt to complex and dynamic situations. This has led many roboticists to explore learning-based solutions that may generalize better than hand-engineered ones. Imitation learning (IL) is a widely-used, data-driven approach that distills expert demonstrations into learned policies, enabling fine-grained manipulation of high-dimensional robot systems in the real world [1, 2], and has been shown to scale with more data in language-conditioned, multi-task [3, 4, 5, 6] and even multi-robot [7, 8, 9] regimes. While these works have shown remarkable promise, the resulting robot systems remain fairly limited to situations seen during training. And the span of these training scenarios is significantly narrower compared to those of other domains, such as vision and language, where large-scale supervised learning has excelled as real-world embodied data collection is significantly more expensive and bottlenecked by physical constraints. Humans, on the other hand, can adapt to very complex situations without any previous first-hand experience. We exhibit ‚Äòcommon sense‚Äô generalization‚Äîusing our inherent understanding of complex, high-level concepts like object affordances, intuitive physics, and compositionality to adapt our past experiences intelligently as new situations arise. This deliberate, analytical thinking has been termed ‚ÄòSystem 2‚Äô processing, in contrast to reactive ‚ÄòSystem 1‚Äô, reflexive low-level behaviors that are less cognitively demanding but equally essential for our behavior [10]. Emulating this blend of reasoning and reflex in robotic systems is challenging, and various approaches have been developed to bridge the gap. One notable example is SayCan [11], which leverages a large language model (LLM) to plan over and sequence learned policies to perform long-horizon tasks. SayCan compensates for the LLM‚Äôs lack of direct physical grounding by using the policies‚Äô value functions to assess feasibility. Moreover, this approach is limited to sequencing the original tasks demonstrated, while many realistic tasks require finer, more nuanced control of low-level policies (as illustrated in LABEL:fig:teaser, right side). Subsequent works have focused on enabling LLMs or VLMs to interface at a finer granularity with pre-programmed System 1 behaviors through representations like code [12] or semantic keypoints [13]. Another strategy that has emerged is to fine-tune VLMs on embodied data [5, 6], drawing on web-scale pre-trained representations for robot control. Notably, these approaches focus on optimizing the high-level module to make the best use of a fixed set of skills, which still constrains their adaptability in unstructured, novel scenarios. Rather than modifying the System 2 reasoning layer, we pursue an orthogonal direction by focusing on making System 1 policies more flexible and responsive to high-level guidance. By designing adaptable System 1 policies, we enable seamless interaction with a fixed System 2 module‚Äîsuch as human or VLM-based reasoning. This adaptability allows System 1 to be dynamically adjusted in response to high-level instructions, ultimately broadening the range of tasks the system can perform and its capacity for robust, generalizable behavior. We present STEER: Structured Training for EmbodiEd Reasoning, an approach for training low-level reactive policies that can be flexibly steered or directed by a higher-level reasoner, such as a human or VLM. Our key insight for enabling this is producing dense language annotations of the collected robot data, and training a conditional policy on granular language instructions. This policy can then be conditioned on each part of a plan generated from a high-level model (VLM/LLM), giving a combination of the respective strengths of System 1 and System 2 processes. Furthermore, this can enable adapting to new situations that require synthesizing behaviors that are not explicitly demonstrated during training. We instantiate this system using existing real-world datasets, proposing a simple automated labeling pipeline based on proprioceptive observations to extract basic object-centric manipulation skills and distill them into a low-level policy. We then propose a strategy for using a VLM to produce language directions for the low-level policy. Importantly, we show that this enables us to repurpose skills in the robot dataset in a meaningful manner at test time to handle a new situation autonomously. In summary, the contributions of this work are as follows: ‚Ä¢ We introduce STEER, a method that augments robot demonstration datasets with descriptive functional language annotations comprising of grasp-centric and rotation-based primitive components. ‚Ä¢ We show that STEER enables training low-level robot policies which are significantly more flexible and steerable than prior imitation learning methods, enabling humans or pre-trained VLMs to direct low-level policies for generalizing to novel everyday manipulation tasks."
https://arxiv.org/html/2411.03408v1,Learning Few-Shot Object Placement with Intra-Category Transfer,"Efficient learning from demonstration for long-horizon tasks remains an open challenge in robotics. While significant effort has been directed toward learning trajectories, a recent resurgence of object-centric approaches has demonstrated improved sample efficiency, enabling transferable robotic skills. Such approaches model tasks as a sequence of object poses over time. In this work, we propose a scheme for transferring observed object arrangements to novel object instances by learning these arrangements on canonical class frames. We then employ this scheme to enable a simple yet effective approach for training models from as few as five demonstrations to predict arrangements of a wide range of objects including tableware, cutlery, furniture, and desk spaces. We propose a method for optimizing the learned models to enables efficient learning of tasks such as setting a table or tidying up an office with intra-category transfer, even in the presence of distractors. We present extensive experimental results in simulation and on a real robotic system for table setting which, based on human evaluations, scored 73.3% compared to a human baseline. We make the code and trained models publicly available at http://oplict.cs.uni-freiburg.de111Code will be published upon acceptance..","Humans excel at teaching each other various skills efficiently. Whether it is setting tables, changing bicycle brakes, or furnishing a room, we can guide another person to proficiency within a handful of training sessions. In contrast, learning from large datasets [1, 2, 3] has yielded impressive manipulation results on a variety of tasks. However, the requirement for vast amounts of data limits widespread adoption and lacks human efficiency in skill acquisition [4]. Recently, large pretrained perception models have been used to efficiently learn manipulation skills by making use of reliable features while maintaining simple model representations. This approach has proven effective in works such as [5, 6, 7], which demonstrate one-shot or few-shot learning of manipulation skills with successful transfer to different scenes. These achievements support the concept of object-centric task learning, as proposed in [8, 9, 10]. These works advocate for dividing long manipulation tasks into a series of changing contact states to facilitate efficient learning. While [8] demonstrates how a single demonstration can be used in combination with simulation to efficiently learn effective control primitives, [9] shows that it is feasible to teach a real robot to perform a simple pick-and-place task from a single demonstration, provided that the observed human actions can be successfully mapped onto the robot‚Äôs available skills by leveraging vision foundation models. Inspired by these works, we are looking to support long-horizon manipulation by learning where objects should be during key moments of a long-horizon tasks from minimal training examples (‚â§5absent5\leq 5‚â§ 5). These models should be robust to changes of object instances within the same class. Figure 1: Our approach learns object placements sample-efficiently by mapping object instances to a known canonical instance and inferring the placement of the new object in this canonical space. Here, the known setup on the right is matched with the novel one on the left to place the chair. To this end, in this work, we introduce an approach for sample-efficiently learning models for object placements in incremental placement tasks, as proxy for tasks with key moments for object poses. Our primary focus is on enabling models to transfer across different objects within the same category, facilitating the versatility of the learned models. To do so, our framework operates on distributions of relative poses, for which we introduce a scheme for mapping object observations to a canonical class frame. We consider everyday scene arrangements, similar to Fig. 1, which are characterized by long tasks with significant object variability and complex inter-object dependencies. We introduce a method for minimizing the complexity of models and reducing the impact of spurious correlations formed with unrelated objects. We demonstrate our method in simulations, showing the effectiveness of our method by arranging furniture, tableware, and office space. In simulated experiments, our method was successfully trained with as few as 5 demonstrations and performed robustly against distractors. Using the system depicted in Fig. 2, we perform real robot experiments on tableware arrangements. Our system was rated by human evaluators as 73.3%percent73.373.3\%73.3 % as good as human performance. Our primary contributions are: 1. A novel approach for few-shot relative pose learning. 2. A framework for mapping observations of objects to a canonical class frame for intra-category transfer. 3. A technique for optimizing model complexity and removing distracting observations from the models. 4. Real-world robot experiments of autonomously setting tables using both familiar and unfamiliar objects, with the quality of the arrangements assessed by human jurors. 5. We publicly release our simulated dataset, our code, and trained models at http://oplict.cs.uni-freiburg.de. Figure 2: Full pipeline of our system with real robot experiment. Our proposed pose inference method predicts the ideal object placement pose, which the robot then arranges autonomously. Our approach‚Äôs few-shot transfer to other object instances is enabled by our object class mappings which are enabled by several large networks for object detection and feature extraction."
https://arxiv.org/html/2411.03328v1,Foundation Models for Rapid Autonomy Validation,"We are motivated by the problem of autonomous vehicle performance validation. A key challenge is that an autonomous vehicle requires testing in every kind of driving scenario it could encounter, including rare events, to provide a strong case for safety and show there is no edge-case pathological behavior. Autonomous vehicle companies rely on potentially millions of miles driven in realistic simulation to expose the driving stack to enough miles to estimate rates and severity of collisions. To address scalability and coverage, we propose the use of a behavior foundation model, specifically a masked autoencoder (MAE), trained to reconstruct driving scenarios. We leverage the foundation model in two complementary ways: we (i) use the learned embedding space to group qualitatively similar scenarios together and (ii) fine-tune the model to label scenario difficulty based on the likelihood of a collision upon re-simulation. We use the difficulty scoring as importance weighting for the groups of scenarios. The result is an approach which can more rapidly estimate the rates and severity of collisions by prioritizing hard scenarios while ensuring exposure to every kind of driving scenario.","As human operators are removed from controlling autonomous vehicles and driving software matures to carry passengers, simulated behavior validation takes an ever more central role. Behavior validation entails justifying that the software stack responsible for executive control of the vehicle can achieve a desired performance target across the expected exposure of an operating design domain (ODD). Without a human operator either physically present or remotely assisting, the validation stack must both exercise all expected behaviors and return a set of concrete metrics in a trustworthy way. As the size of the ODD increases, the necessary amount of validation increases, often at a rapid rate. For instance, unique road geometries, higher speed limits, quirks of driving norms at a new location, and agent behavior complexity all contribute to an ever-growing set of validations to perform. This growth is capable of scaling beyond the bounds of available resources if the set grows with all collected driving logs. An example of how quickly these costs can grow, especially for rare events, is discussed in Appendix -A. High precision validation, especially when compared against human performance can be a significant expense and a challenge for development. In this work, we focus on the setting where a developer has collected significant driving logs but seeks to prioritize their simulation. This prioritization may be used within a fixed simulation budget to maximize effective validation, to reduce a set of simulations in order to save compute spend or total validation latency, or a combination thereof. While prior works have performed searches to find adversarial conditions that stress the autonomy stack [1, 2, 3, 4], choosing from a fixed set of logs allows for estimating performance on the distribution of expected scenarios, as they come from real driving scenes. Figure 1: Overview of the proposed validation process. We embed a scenario using a pretrained encoder ‚Ñ∞‚Ñ∞\mathcal{E}caligraphic_E. That embedding is concatenated with a difficulty score from a predictor head ùíüùíü\mathcal{D}caligraphic_D taking in the embedding as an input. For a scenario set, these concatenated embeddings are clustered by an algorithm ùíûùíû\mathcal{C}caligraphic_C and finally sampled by a sampler ùíÆùíÆ\mathcal{S}caligraphic_S at validation time. A metrics stage ‚Ñ≥‚Ñ≥\mathcal{M}caligraphic_M simulates and computes metrics over the sampled scenarios. Core to answering the question ‚Äúis this software safe enough for operation without human intervention in this ODD?‚Äù are three criteria: ‚Ä¢ Simulation resources should be directed to diverse driving scenarios to cover the input distribution. ‚Ä¢ Simulation resources should be directed to difficult driving scenarios to ensure they maximize the signal in the validation metric. ‚Ä¢ The sampling procedure should return the exposure to each scenario type to calculate a weighted validation metric. Self-supervised learning (SSL) allows for the learning of representations of input data that can be used for interacting with data and the relationships inherent in it. The pretraining process that is part of SSL also yields good starting points for fine-tuning powerful downstream models. In this paper, we describe a pretraining process used for driving data and two uses of this pretrained model for the behavior validation prioritization task. Our specific contributions are: (i) Training an SSL model for general driving behavior understanding (rather than only as a pretraining step as in [5]). (ii) Validating an autonomy stack by combining complementary uses of the pretrained SSL model: clustering based on scenario similarity and scoring scenarios based on their difficulty. (iii) Extensive evaluation of the validation technique."
https://arxiv.org/html/2411.03315v1,Learning Force Distribution Estimation for the GelSight MiniOptical Tactile Sensor Based on Finite Element Analysis,"Contact-rich manipulation remains a major challenge in robotics. Optical tactile sensors like GelSight Mini offer a low-cost solution for contact sensing by capturing soft-body deformations of the silicone gel. However, accurately inferring shear and normal force distributions from these gel deformations has yet to be fully addressed. In this work, we propose a machine learning approach using a U-net architecture to predict force distributions directly from the sensor‚Äôs raw images. Our model, trained on force distributions inferred from Finite Element Analysis (FEA), demonstrates promising accuracy in predicting normal and shear force distributions. It also shows potential for generalization across sensors of the same type and for enabling real-time application. The codebase, dataset and models are open-sourced and available at https://feats-ai.github.io.","Figure 1: Complete Method Overview: from data collection to force distribution prediction. After data collection in a precisely calibrated setup with a CNC milling machine, Finite Element Analysis is employed to generate labels (‚Äúground truth‚Äù force distributions). Using the labels and raw images captured by the GelSight Mini tactile sensor, we train a U-net for efficiently mapping raw tactile images to the corresponding force distributions. Tactile sensing plays an important role in advancing the state-of-the-art in robotic manipulation [1, 2, 3, 4, 5, 6, 7, 8]. Successful applications include grip adaptation through slip detection [9, 10, 11], medical procedures [12, 13] and tele-operation [14]. In particular, optical tactile sensors have emerged as a promising technology for capturing contact information due to their high spatial resolution, multimodal sensing capabilities‚Äîincluding shape [15], hardness [16], texture [17], and temperature [18]‚Äîand cost-effectiveness [19, 13]. However, many prior works have focused on extracting only low-dimensional tactile information, such as total force [20, 21, 22], limiting operational flexibility. Access to contact force distributions, on the other hand, would enable better handling of multiple contacts and diverse manipulation scenarios [23]. Conventional methods for extracting force distributions require calculating the three-dimensional deformation of the contact medium and utilizing elasticity theory [24, 25, 26, 27]. Yet, accounting for non-linear material behavior, such as with Finite Element Analysis (FEA), is computationally intensive and unsuitable for real-time applications. Recent works leverage Deep Learning to address the challenge of real-time force estimation. In [28], Convolutional Neural Networks (CNNs) were used to predict contact forces from sensor images, while [29] introduced CANFnet for estimating normal force distributions at the pixel level. In [23, 30, 31], FEA-derived data was used to train a model for predicting force distributions, demonstrating the effectiveness of combining simulations with data-driven methods. In this paper we introduce FEATS (see Fig. 1)‚Äîa machine learning approach that directly maps raw tactile images to force distributions, building upon the method by Sferrazza et al. [31]. We utilize FEA to generate labeled data for training, ensuring accurate ground truth across various indenters and force levels. A U-net neural network architecture [32] is employed to estimate force distributions from images captured by the GelSight Mini optical sensor [28, 33]. In contrast to [31], our method is tailored to a widely available commercial sensor GelSight Mini, dropping the requirement of a custom-made gel with immersed particles, thereby drastically extending the applicability of the approach. Furthermore, this sensor allows for a significantly expanded range of measurable forces 0‚àí40‚Å¢N040N0-40\,\mathrm{N}0 - 40 roman_N, an 8888-fold increase in the maximum measurable force compared to [31]. Finally, we open source our code, dataset and model, aiding reuse and reproducibility. Experimental results demonstrate that the proposed method accurately predicts high-dimensional contact force distributions from raw tactile images. This capability advances robotic manipulation by accommodating a wider range of contact scenarios and offers a versatile representation applicable to downstream tasks."
https://arxiv.org/html/2411.03815v1,How to Drawjectory? - Trajectory Planning using Programming by Demonstration,"A flight trajectory defines how exactly a quadrocopter moves in the three-dimensional space from one position to another. Automatic flight trajectory planning faces challenges such as high computational effort and a lack of precision. Hence, when low computational effort or precise control is required, programming the flight route trajectory manually might be preferable. However, this requires in-depth knowledge of how to accurately plan flight trajectories in three-dimensional space.We propose planning quadrocopter flight trajectories manually using the Programming by Demonstration (PbD) approach ‚Äì simply drawing the trajectory in the three-dimensional space by hand. This simplifies the planning process and reduces the level of in-depth knowledge required. We implemented the approach in the context of the Quadcopter Lab at Ulm University.In order to evaluate our approach, we compare the precision and accuracy of the trajectories drawn by a user using our approach as well as the required time with those manually programmed using a domain specific language. The evaluation shows that the Drawjectory workflow is, on average, 78.778.778.778.7 seconds faster without a significant loss of precision, shown by an average deviation 6.676.676.676.67 cm.","Life is full surprising coincidences. Wolfang Reif twice had ‚Äì in totally unrelated circumstances ‚Äì a profound impact on the Institute of Software Engineering and Programming Languages at Ulm University. First, Wolfgang Reif was professor at the institute ‚Äì then called Software Engineering and Compiler Construction ‚Äì from 1994 to 2000 (before moving on to heading the Institute of Software and Systems Engineering at the University of Augsburg) working on formal verification and the KIV system. He put Ulm on the international map as a renowned place for formal methods research in the software engineering community. Second, Wolfgang Reif gave me (Matthias Tichy) in 2010 the first opportunity for independent research and teaching by offering me the position as acting professor for Self-Organizing Systems at the University of Augsburg for one year. While my goal was then to leave academia to join industry, seizing that opportunity allowed me to grow as a person and as an academic and put me on my academic path. Finally, those two unrelated circumstances surprisingly met when i was offered to become head of the Institute of Software Engineering and Compiler Construction at Ulm University in 2015. Impressed by the research of Wolfgang Reif‚Äôs group in the area of autonomous quadrocopters, i decided to build a quadrocopter lab as a research pillar in Ulm as well. Wolfgang Reif and his group, particularly, Andreas Angerer and Alwin Hoffmann, have been a tremendous help in building up that lab and our research. Hence, we chose to present one of our lab‚Äôs recent outcomes in this paper to commemorate Wolfgang Reif‚Äôs 65th birthday. Quadrocopters have a wide range of applications, from automated parcel delivery to search and rescue operations. Therefore, it is not surprising that the topic of planning flight routes or trajectories for drones is becoming increasingly important [19, 4]. Generally speaking, trajectory planning ‚Äúconsists in assigning a time law to the geometric path‚Äù [15]. Trajectory planning distinguishes two planning strategies: manual and automatic planning. When considering automatic trajectory planning, there are several promising approaches, such as genetic algorithms, artificial neural networks, or simple A* algorithms [4, 15]. However, some tasks require high precision and specific points of interest to be visited. Thus, the mentioned strategies may not always be ideal for trajectory planning [36]. For instance, an old building in danger of collapse might need to be screened by a drone with the drone having to check certain rooms. In this case, the creation of a route through the building, encompassing all points of interest, can be a particularly time-consuming and error-prone endeavor, particularly when the objective is to implement an automatic planning process, as this approach does not permit any form of intervention in the selection process of the points to be visited. One possible solution to ensure that all points of interest are visited is manual trajectory planning, whereby the drone‚Äôs flight path is not determined automatically but by hand. There are alternative approaches to manual planning including i) programming the path (mostly using low-code approaches) and ii) creating interactive points in a 3D environment on a PC [34, 20]. However, these methods often require knowledge of either how to program or how to interact with the points, making planning unintuitive. Accordingly, there is a desire for a user-friendly and still accurate method for manual trajectory planning, such as demonstrating the trajectory in the real world. This concept of ‚Äútransfer[ring] new skills to a machine by relying on demonstrations from a user‚Äù [10] is called Programming by Demonstration. Due to its high intuitiveness, it is ideal for domain experts in a certain topic who need the support of quadrocopters, despite their potential lack of experience in controlling quadrocopters [26], which is why programming by demonstration has seen significant growth [31]. Based on the aforementioned problem and goals, we formulate the following research questions: RQ 1 How to automatically plan a trajectory by demonstrating the flight path once? RQ 2 How does trajectory planning by demonstration compare against trajectory planning by manual programming using a domain specific language in terms of accuracy and effort? We developed an approach for planning trajectories from a user‚Äôs demonstration, thus applying the programming by demonstration paradigm answering RQ1. Specifically, we track the demonstration, i.e., the point sequence of the desired flight path. Subsequently, a selection of points, designated as waypoints, is made from the recorded point sequence. These waypoints are then used to interpolate a trajectory using natural cubic splines. To answer RQ2, we evaluate the accuracy and the planning effort by systematically using both approaches to plan three different classes of trajectories: planar geometric figures, planar non-trivial figures, 3D figures. We measure the time to plan those trajectories as well as the accuracy of the planned trajectory compared to the intended trajectory. The evaluation shows that the Drawjectory workflow is, on average, 78.778.778.778.7 seconds faster without a significant loss of precision, shown by an average deviation 6.676.676.676.67 cm compared to using a domain specific language to manually program the trajectories. 2 and 3 introduces the quadrocopter lab setting as well as relevant foundations of trajectory planning. Thereafter, 4 presents our trajectory planning by demonstration approach and the proof-of-concept implementation. 5 presents the results of the evaluation. After discussing related work in6, we conclude in 7 by summarizing the work and addressing limitations as well as discussing potential future work."
https://arxiv.org/html/2411.03706v1,3DGS-CD: 3D Gaussian Splatting-based Change Detection for Physical Object Rearrangement,"We present 3DGS-CD, the first 3D Gaussian Splatting (3DGS)-based method for detecting physical object rearrangements in 3D scenes. Our approach estimates 3D object-level changes by comparing two sets of unaligned images taken at different times. Leveraging 3DGS‚Äôs novel view rendering and EfficientSAM‚Äôs zero-shot segmentation capabilities, we detect 2D object-level changes, which are then associated and fused across views to estimate 3D changes. Our method can detect changes in cluttered environments using sparse post-change images within as little as 18s, using as few as a single new image. It does not rely on depth input, user instructions, object classes, or object models ‚Äì An object is recognized simply if it has been re-arranged. Our approach is evaluated on both public and self-collected real-world datasets, achieving up to 14% higher accuracy and three orders of magnitude faster performance compared to the state-of-the-art radiance-field-based change detection method. This significant performance boost enables a broad range of downstream applications, where we highlight three key use cases: object reconstruction, robot workspace reset, and 3DGS model update. Our code and data will be made available at https://github.com/520xyxyzq/3DGS-CD.","I INTRODUCTION 3D change detection involves identifying changed objects or regions in the environment from two sets of local observations taken at different times or from new observations of a previously modeled scene. It is a critical task in robotics as it can accommodate not only short-term, fully observed scene dynamics but also long-term scene changes where the transition process is typically unobserved. Such long-term changes are particularly important for robot operations, as environmental changes often occur without being fully noticed or monitored. Despite the success of 3D change detection with depth input using scene representations such as TSDF [1], 3D point cloud [2] and neural descriptor fields [3], detecting changes from multi-view RGB images remains a challenging problem. Traditional methods have relied on hand-crafted techniques like voxelization [4], multi-view stereo [5], and image warping [6] to identify changes in unaligned images and lift to 3D. These approaches are particularly sensitive to occlusions and lighting variations, especially when there are large viewpoint differences between the two sets of images. The emergence of radiance field models, such as neural radiance fields (NeRF) [7] and 3DGS [8], presents new opportunities to address these challenges. These models provide high-fidelity representations of scene geometry and appearance, with novel-view rendering capabilities that enable the generation of photo-realistic images and dense depths at arbitrary viewpoints. This allows for direct comparison of pre- and post-change images from the same viewpoint. NeRF-based solutions have been explored to a limited extent [9, 10]. However, these methods are constrained by the computational cost of NeRF‚Äôs ray-casting-based rendering. In contrast, 3DGS offers a more efficient alternative, achieving real-time rendering with comparable or even superior quality. In our work, we leverage 3DGS as scene representation to identify 3D scene changes including object removal, insertion and movement from multi-view images. We exploit the zero-shot segmentation capability of EfficientSAM [11] to compare pre- and post-change images at the same viewpoints, associating and fusing the 2D object changes to obtain accurate 3D object segments and pose changes. Our method has the following key advantages: (1) It can handle sparse post-change image inputs, requiring as few as a single new image to detect 3D changes. (2) It requires no depth sensors or monocular depth estimators. (3) It does not rely on pre-defined object classes, models or object detectors ‚Äì An object is recognized simply if it has been moved, removed or inserted. (4) It requires no user instructions such as user-provided click or language prompts. Our method is evaluated on both public and self-collected real-world datasets, achieving up to 14% higher accuracy and three magnitudes faster performance compared to the state-of-the-art NeRF-based method. This significant performance improvement enables a wide range of real-world applications, including: (1) object removal as a prompt for object reconstruction (Sec. VI-A); (2) robot workspace reset (Sec. VI-B); (3) 3DGS model update (Sec. VI-C). Figure 1: Our method detects 3D object-level changes from pre- and potentially sparse post-change images of a 3D scene. We first train a 3DGS model on pre-change images (Sec.IV-A), localize the post-change cameras with respect to this model (Sec.IV-B), and render RGB-D images at post-change views for 2D change detection using EfficientSAM [11] (Sec.IV-C). The detected 2D object segments are associated across the post-change views (Sec. IV-D) to initialize 3D object templates. These templates are used to classify object change types and query EfficientSAM on pre-change views to obtain additional 2D object segments, which are fused to obtain 3D object segments (Sec. IV-E). For moved objects, we leverage image-template feature correspondences to estimate their 6D pose changes (Sec. IV-F) and refine the estimates by a render-and-compare approach (Sec. IV-G)."
https://arxiv.org/html/2411.03702v1,Graph-Based Multi-Modal Sensor Fusion for Autonomous Driving,"The growing demand for robust scene understanding in mobile robotics and autonomous driving has highlighted the importance of integrating multiple sensing modalities. By combining data from diverse sensors like cameras and LIDARs, fusion techniques can overcome the limitations of individual sensors, enabling a more complete and accurate perception of the environment. We introduce a novel approach to multi-modal sensor fusion, focusing on developing a graph-based state representation that supports critical decision-making processes in autonomous driving. We present a Sensor-Agnostic Graph-Aware Kalman Filter (Sani et al., 2024), the first online state estimation technique designed to fuse multi-modal graphs derived from noisy multi-sensor data. The estimated graph-based state representations serve as a foundation for advanced applications like Multi-Object Tracking (MOT), offering a comprehensive framework for enhancing the situational awareness and safety of autonomous systems. We validate the effectiveness of our proposed framework through extensive experiments conducted on both synthetic and real-world driving datasets (nuScenes). Our results showcase an improvement in MOTA and a reduction in estimated position errors (MOTP) and identity switches (IDS) for tracked objects using the SAGA-KF. Furthermore, we highlight the capability of such a framework to develop methods that can leverage heterogeneous information (like semantic objects and geometric structures) from various sensing modalities, enabling a more holistic approach to scene understanding and enhancing the safety and effectiveness of autonomous systems.","In applications of mobile robotics and autonomous driving, the integration of various sensing modalities via multi-modal sensor fusion has become paramount for achieving comprehensive scene understanding that enables effective decision-making. Similar to any other multi-sensor fusion technique, our objective is to leverage the complementarity of the different types of sensors to enable an improved, more holistic view of the environment. For a mobile robot or an autonomous driving (AD) agent, the typical example is that of using a camera and a LIDAR. We present a sensor fusion approach that utilizes cameras and LIDARs mounted on an AD vehicle and aims to build holistic scene representations that facilitate downstream decision-making. Moreover, our proposal also relies on the observation that an AD agent needs both semantic and geometric information about its environment (scene) for decision-making. For instance, in Figure 1, say the light-blue car (A) in the left lane has a right-of-way at the lane merge ahead. However, the apparent violation of its right-of-way by other agents (B) should be accounted for by its path planner, which in turn would need semantic information, such as the types of vehicle and their behavior in its environment, in addition to the geometric information of the road, lanes and position and velocity of other agents and also the topological information representing the correlation between different entities in the scene. A resulting plan (e.g., lowering speed or stopping) that ensures safety needs a holistic understanding of a dynamic environment that can be achieved by effectively processing the multi-modal sensory data to develop appropriate representations that aid decision-making. Figure 1. How should car ‚ÄòA‚Äô take into account the temporary geometry of the scene formed by cones in order to ensure safety and to avoid any collision with ‚ÄòB‚Äô? In this extended abstract, we present a Sensor-Agnostic Graph-Aware Kalman Filter (Sani et al., 2024) and propose to develop a graph-based dynamic scene representation that permits us to capture heterogeneous information from multiple sensing modalities. Using the AD use case, we plan to develop methods for processing dynamic scene graphs that capture semantic (type of vehicle, traffic light, etc.) and geometric (road planes, lane boundary curve, etc.) information as nodes and their pairwise relationships as edges. Our methods will enable inferences drawn using this dynamic scene graph representation with two key applications in AD: Multi-Object Tracking (MOT) and Simultaneous Localization and Mapping (SLAM). Figure 2. The observed scene graph is constructed using SOTA detectors and is fed into the SAGA-KF framework to predict, associate and estimate the state of the scene graph."
https://arxiv.org/html/2411.03555v1,Object and Contact Point Tracking in Demonstrations Using 3D Gaussian Splatting,"This paper introduces a method to enhance Interactive Imitation Learning (IIL) by extracting touch interaction points and tracking object movement from video demonstrations. The approach extends current IIL systems by providing robots with detailed knowledge of both where and how to interact with objects, particularly complex articulated ones like doors and drawers. By leveraging cutting-edge techniques such as 3D Gaussian Splatting and FoundationPose for tracking, this method allows robots to better understand and manipulate objects in dynamic environments. The research lays the foundation for more effective task learning and execution in autonomous robotic systems.","Autonomous robotic systems have advanced significantly [8, 9], but challenges remain in manipulating novel objects and articulated structures like doors or cabinets. Robots must accurately identify, grasp, and move objects within dynamic environments. Reinforcement Learning (RL) [10] and Imitation Learning (IL) [11] are widely used approaches, with RL often requiring extensive training and careful reward design, while IL benefits from expert demonstrations but struggles with generalization to unseen scenarios [12, 13]. Interactive Imitation Learning (IIL) improves IL by incorporating real-time human feedback [14, 15], allowing robots to adjust during execution, but still faces difficulties in tasks requiring nuanced object interaction [16]. To address this, we propose a method for extracting touch interaction points, or contact points, and tracking object movement from video demonstrations, providing robots with detailed information for manipulating articulated objects. By combining video-based learning with 3D Gaussian Splatting [1] for 3D scene reconstruction, we create a task-relevant representation of the environment, enabling improved robot performance in complex tasks. This method enhances the IIL framework by offering precise interaction data, laying the groundwork for more autonomous robotic manipulation."
https://arxiv.org/html/2411.03465v1,Digital Twin for Autonomous Surface Vessels: Enabler for Safe Maritime Navigation,"Autonomous surface vessels (ASVs) are becoming increasingly significant in enhancing the safety and sustainability of maritime operations. To ensure the reliability of modern control algorithms utilized in these vessels, digital twins (DTs) provide a robust framework for conducting safe and effective simulations within a virtual environment. Digital twins are generally classified on a scale from 0 to 5, with each level representing a progression in complexity and functionality: Level 0 (Standalone) employs offline modeling techniques; Level 1 (Descriptive) integrates sensors and online modeling to enhance situational awareness; Level 2 (Diagnostic) focuses on condition monitoring and cybersecurity; Level 3 (Predictive) incorporates predictive analytics; Level 4 (Prescriptive) embeds decision-support systems; and Level 5 (Autonomous) enables advanced functionalities such as collision avoidance and path following. These digital representations not only provide insights into the vessel‚Äôs current state and operational efficiency but also predict future scenarios and assess life endurance. By continuously updating with real-time sensor data, the digital twin effectively corrects modeling errors and enhances decision-making processes. Since DTs are key enablers for complex autonomous systems, this paper introduces a comprehensive methodology for establishing a digital twin framework specifically tailored for ASVs. Through a detailed literature survey, we explore existing state-of-the-art enablers across the defined levels, offering valuable recommendations for future research and development in this rapidly evolving field.","Commercial shipping is a cornerstone of the global economy, enabling the transport of vast quantities of goods across oceans. In 2020, the maritime sector moved approximately 10,648 million tonnes of seaborne goods, emphasizing its critical role in international trade (UNCTD, 2021). However, the industry‚Äôs rapid growth has also contributed to significant environmental challenges. Shipping currently accounts for over 1 billion tonnes of carbon dioxide emissions annually, representing roughly 2.5% of global emissions (International Maritime Organization, 2020). Due to rising environmental concerns, developing cost-effective and environmentally sustainable solutions in commercial shipping has become an urgent priority for many countries. One promising solution is autonomous operation. Autonomous vessels have the potential to revolutionize maritime operations by increasing efficiency and reducing environmental impact. These vessels can follow optimal navigational routes, maximize fuel efficiency, and lower CO2 emissions. Additionally, since human error is responsible for nearly 80% of maritime accidents (S√°nchez-Beaskoetxea et al., 2021), autonomy could significantly enhance safety by satisfying strict safety protocols, detecting hazards beyond human capability, and making real-time decisions without fatigue. Despite these potential benefits, the transition to autonomous shipping introduces a range of technological challenges. Implementing autonomous surface vessels (ASVs) in real-world maritime environments, particularly in high-stakes scenarios such as deep-sea shipping with large cargo, poses substantial risks. Minor technological errors in such settings can lead to devastating accidents, environmental disasters, and financial losses. To mitigate these risks, the concept of a digital twin has emerged as a game-changing technology. A digital twin generates a virtual model of a vessel and its operating environment, enabling the simulation and evaluation of operational strategies prior to real-world implementation. This capability significantly reduces the risks associated with the introduction of autonomous systems by offering a safe testing ground for innovative technologies. By simulating a wide range of scenarios, digital twins can accelerate the deployment of autonomous shipping operations that are not only safer and more sustainable but also more cost-effective. However, viewing digital twins merely as computer-aided design (CAD) models or basic simulators is a narrow and restrictive perspective. To fully exploit the potential of digital twins in autonomous shipping, a more refined and context-specific understanding of the concept is essential. The current work seeks to address this by the following: ‚Ä¢ Defining digital twin and its capability levels for maritime applications: Digital twins are categorized into six levels (0 to 5), each representing increasing levels of sophistication. ‚Ä¢ Overview of current relevant technologies: A comprehensive overview of the current state of technologies essential for creating digital twins for ASVs is provided. These include advances in sensor integration, condition monitoring, predictive analytics, and advanced collision avoidance systems. ‚Ä¢ Identification of opportunities and challenges: The potential benefits and challenges of developing highly functional digital twins for autonomous shipping are identified. ‚Ä¢ Recommendations for stakeholders: Recommendations are offered to various stakeholders, outlining how they can maximize the value derived from digital twin technology in autonomous maritime systems in return of their contributions. These suggestions highlight priority areas for future research and practical strategies for technology development. For a better organization of the review and related perspective, this article is structured into five sections. The current introduction section outlined the significance of autonomous shipping and the necessity of digital twin technology to address current challenges. The following Section 2 explains the general concept of a digital twin, detailing its progression from Level 0 (standalone systems) to Level 5 (fully autonomous operations). In Section 3, the approach taken to compile and analyze existing research is described, ensuring a thorough examination of relevant studies. Section 4 is divided into subsections that explore each level of digital twin capability, from Level 0-Standalone to Level 5-Autonomous, discussing the advancements and applications at each stage. Finally, the article concludes with Section 5, summarizing key findings and proposing future directions for research in digital twin technology for autonomous maritime operations."
https://arxiv.org/html/2411.03303v1,Monocular Event-Based Vision for Obstacle Avoidance with a Quadrotor,"We present the first static-obstacle avoidance method for quadrotors using just an onboard, monocular event camera. Quadrotors are capable of fast and agile flight in cluttered environments when piloted manually, but vision-based autonomous flight in unknown environments is difficult in part due to the sensor limitations of traditional onboard cameras. Event cameras, however, promise nearly zero motion blur and high dynamic range, but produce a very large volume of events under significant ego-motion and further lack a continuous-time sensor model in simulation, making direct sim-to-real transfer not possible. By leveraging depth prediction as a pretext task in our learning framework, we can pre-train a reactive obstacle avoidance events-to-control policy with approximated, simulated events and then fine-tune the perception component with limited events-and-depth real-world data to achieve obstacle avoidance in indoor and outdoor settings. We demonstrate this across two quadrotor-event camera platforms in multiple settings and find, contrary to traditional vision-based works, that low speeds (1m/s) make the task harder and more prone to collisions, while high speeds (5m/s) result in better event-based depth estimation and avoidance. We also find that success rates in outdoor scenes can be significantly higher than in certain indoor scenes.","Quadrotors can be flown very fast and with high agility by human pilots. More recently, autonomous, vision-based quadrotors demonstrated champion-level drone racing on fixed tracks [1]. However, achieving high-speed vision-based autonomous flight through unknown environments remains challenging due to the inherent limitations of traditional cameras. Their limited frame rate, low dynamic range, and motion blur, particularly in low-lit scenes such as under forest canopy, can increase uncertainty in traditional model-based mapping-planning-control pipelines or can cause out-of-distribution inputs for an end-to-end vision-to-control learning framework. In contrast, event cameras (also called dynamic vision sensors or neuromorphic cameras) [2] have low latency and low bandwidth and feature pixels that individually detect changes at microsecond-level temporal resolution, effectively alleviating motion blur. Their output event stream consists of positive or negative events that correspond to local, per-pixel increases or decreases in brightness. Therefore, unlike traditional cameras, which output frame-based data at constant time intervals, event cameras generate a continuous stream of events. This rapid and adaptive perception capability, particularly under variable lighting conditions, suggests that an events-driven approach could enable vision-based agile flight at significantly faster speeds than traditional cameras. Most related works using event cameras have focused on dodging dynamic objects thrown at a flying quadrotor [3, 4, 5]. These works focused on detecting the dynamic object by eliminating events caused by the ego-motion of the robot with respect to the stationary background. To the best of our knowledge, there is no published work for static-obstacle avoidance due to the challenge of significant event-stream bandwidth under considerable ego-motion, which makes the obstacle identification task in the event stream more difficult. We present a simulation pre-trained, learning-based approach to predict static-obstacle avoidance velocity commands from an incoming event stream, by leveraging depth prediction as a pretext task to guide the network in extracting robust, texture-invariant latent representations. Focusing on geometry rather than appearance, depth offers a more domain-invariant representation and, for these reasons, has been employed in various robotics tasks. It can easily be gathered in the real world for model fine-tuning, offering an inexpensive alternative compared to expert drone pilot commands. We focus on tree-like objects and demonstrate that jointly pre-training velocity and depth prediction in simulation allows us to fine-tune the perception backbone with real-world data, leading to the successful avoidance of trees in a forest. We tackle the lack of a continuous-time event camera model in simulation by using a binarized brightness difference event representation, which is less sensitive to non-continuous event streams, and employ a teacher-student approach to distill privileged knowledge into a vision-only student network. We demonstrate simulated, real indoor, and real outdoor avoidance of trees and tree-like objects with a single event camera onboard a flying quadrotor, deployed on two different hardware platforms, showcasing the adaptability of the proposed system. Our contributions are as follows: ‚Ä¢ The first events-driven static-obstacle avoidance method for a mobile robot. ‚Ä¢ Successful sim-to-real, few-shot transfer of an events-based policy by leveraging depth prediction as a pretext, supervisable task. ‚Ä¢ High-speed (5m/s) real-world avoidance of trees with purely onboard computation, and demonstrating that avoidance using event-based vision improves as speed increases. ‚Ä¢ Open-source code and data for simulation, data collection, training, and testing."
https://arxiv.org/html/2411.03294v2,Out-of-Distribution Recovery with Object-Centric Keypoint Inverse Policy For Visuomotor Imitation Learning,"We propose an object-centric recovery policy framework to address the challenges of out-of-distribution (OOD) scenarios in visuomotor policy learning. Previous behavior cloning (BC) methods rely heavily on a large amount of labeled data coverage, failing in unfamiliar spatial states. Without relying on extra data collection, our approach learns a recovery policy constructed by an inverse policy inferred from object keypoint manifold gradient in the original training data. The recovery policy serves as a simple add-on to any base visuomotor BC policy, agnostic to a specific method, guiding the system back towards the training distribution to ensure task success even in OOD situations. We demonstrate the effectiveness of our object-centric framework in both simulation and real robot experiments, achieving an improvement of 77.7% over the base policy in OOD. Project Website: https://sites.google.com/view/ocr-penn","Robot learning has achieved significant success in deploying Imitation Learning (IL) methods on real-world robotic systems [1]. One widely studied approach within IL is Behavior Cloning (BC), which has been explored extensively in recent work [2, 3, 4, 5, 6, 7]. BC methods enable learning control policies directly from demonstrations without the need for explicit environmental modeling, making the process relatively straightforward. However, despite producing promising results, BC is well-known for its susceptibility to the covariate shift problem [1]. This issue arises because traditional BC approaches depend heavily on large quantities of labeled data, which are often obtained through labor-intensive methods such as teleoperation or kinesthetic teaching. Consequently, BC may struggle to perform reliably in out-of-distribution (OOD) scenarios, where data is sparse or noisy‚Äîreflecting a broader challenge faced in supervised learning. Addressing this issue typically requires either returning to laborious data collection or utilizing corrective mechanisms, such as guidance from human operators or reinforcement learning (RL) agents [8, 9, 10, 11], both of which impose additional deployment efforts on robotic systems. To enjoy the benefits of strong performing BC policies in distribution (ID) settings while not requiring the human effort of collecting more data or the compute effort of running an RL step when OOD, in this work, we propose a recovery policy framework that brings the system back to the training distribution to ensure task success even when OOD. In particular, we focus on the key challenges of visuomotor policy learning by integrating a recovery policy constructed from the gradient of the training data manifold with a base visuomotor BC policy (e.g., a diffusion policy [2]). Inspired by the ‚ÄúBack to the Manifold‚Äù approach [12] the recovery policy guides the system back towards the training manifold, at which point the base policy resumes control. However, unlike [12], which focuses on recovering from OOD scenarios related to the robot‚Äôs state, our approach takes an object-centric perspective, specifically addressing OOD situations for task-relevant object states. We believe this object-centric approach significantly enhances the OOD recovery capabilities of visuomotor policies, leading to more robust learning for object manipulation tasks. Furthermore, our recovery framework is designed to be agnostic to the choice of base policy, allowing it to be seamlessly integrated with various BC implementations. This flexibility makes our method adaptable for future developments in imitation learning (IL). In this paper, we make the assumption that we have access to relevant object models. Also, we mainly consider the OOD cases in which the relevant object enters unfamiliar spatial regions. The rest of the paper is organized as follows: Section 2 presents an overview of the existing works. Section 3 describes the problem formulation. Section 4 presents the object-centric recovery policy framework in detail, including its construction of the training data manifold and the keypoint inverse policy. In section 5, we demonstrate the effectiveness of our approach on several benchmarks, including both simulation and real robot experiments, showing that our recovery policy improves the performance when entering unfamiliar states. We also show that our method has the desired property for lifelong learning of visuomotor policies, improving the performance of OOD while not diminishing the in-distribution performance. Section 6 discusses the limitations and future directions."
https://arxiv.org/html/2411.03289v1,Data-Driven Sampling Based Stochastic MPC forSkid-Steer Mobile Robot Navigation,"Traditional approaches to motion modeling for skid-steer robots struggle with capturing nonlinear tire-terrain dynamics, especially during high-speed maneuvers. In this paper, we tackle such nonlinearities by enhancing a dynamic unicycle model with Gaussian Process (GP) regression outputs. This enables us to develop an adaptive, uncertainty-informed navigation formulation. We solve the resultant stochastic optimal control problem using a chance-constrained Model Predictive Path Integral (MPPI) control method. This approach formulates both obstacle avoidance and path-following as chance constraints, accounting for residual uncertainties from the GP to ensure safety and reliability in control. Leveraging GPU acceleration, we efficiently manage the non-convex nature of the problem, ensuring real-time performance. Our approach unifies path-following and obstacle avoidance across different terrains, unlike prior works which typically focus on one or the other. We compare our GP-MPPI method against unicycle and data-driven kinematic models within the MPPI framework. In simulations, our approach shows superior tracking accuracy and obstacle avoidance. We further validate our approach through hardware experiments on a skid-steer robot platform, demonstrating its effectiveness in high-speed navigation. The GPU implementation of the proposed method and supplementary video footage are available at https://stochasticmppi.github.io.","Skid-steer robots, recognized for their high traction and payload capacity, are widely adopted in rugged and challenging terrain navigation [1]. However, their high maneuverability comes at the cost of significant skidding and slipping, making it challenging to predict their motion given command velocities. Accurate motion models are crucial for effective control, yet existing approaches based on simple kinematic models struggle to capture complex tire-terrain interactions [2]. Figure 1: Illustration of two MPPI trajectories (Traj 1 and Traj 2). The obstacle radii are adjusted based on propagated state variance (ellipses) and the safety threshold pxsubscriptùëùùë•p_{x}italic_p start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT (Eq. 7). Traj 1 is ‚Äòunsafe‚Äô due to a collision with an obstacle (Obs 1). Model Predictive Path Integral (MPPI) [3] is a sampling-based control strategy well-suited for solving non-linear, non-convex, and non-differentiable optimal control problems. It evaluates multiple candidate control trajectories and selects the optimal action based on a weighted sum of these trajectories. Nevertheless, standard MPPI approaches [4] that rely on pre-trained models to predict robot dynamics struggle with unmodeled terrain variations and real-time uncertainty [5], limiting their ability to adapt to different environments. To address these challenges, we integrate Gaussian Processes (GPs) into the MPPI framework [6]. GPs are powerful nonlinear regression tools that provide both mean and uncertainty (variance) estimates for predictions. In our work, GPs model skid-steer robot motion across varying terrains. The GP mean captures model residuals, accounting for discrepancies between nominal physics-based models and actual robot dynamics. Additionally, the GP variance, representing uncertainty in the model‚Äôs predictions, is integrated into safety constraints through chance constraints [7]. This enables safety buffers to be dynamically adjusted in real-time given varying uncertainty levels. These buffers help ensure the robot stays on its intended path and maintains a safe distance from obstacles, even in highly dynamic environments. Fig. 1 presents an overview of our approach. The key contributions of this paper are: ‚Ä¢ A data-driven, sampling based stochastic MPC method tailored for high-speed skid-steer robot navigation at speeds up to 2 m/s (the top speed of the Clearpath Jackal platform), leveraging GP regression for adaptive motion planning. The same control law ensures navigation across all terrains included in the GP training dataset, without the need for explicit terrain identification. ‚Ä¢ A GPU implementation of the GP-MPPI planner that solves stochastic optimal control problems at real-time rates of 20 Hz for path tracking and obstacle avoidance. ‚Ä¢ Extensive simulation experiments of the proposed method, demonstrating superior tracking accuracy and obstacle avoidance compared to kinematic model-based MPPI approaches. The method‚Äôs real-world applicability is also evaluated and verified via hardware experiments of a skid-steer robot navigating different terrains."
https://arxiv.org/html/2411.03194v1,Energy Consumption in Robotics:A Simplified Modeling Approach,"The energy use of a robot is trajectory-dependent, and thus can be reduced by optimization of the trajectory. Current methods for robot trajectory optimization can reduce energy up to 15% for fixed start and end points, however their use in industrial robot planning is still restricted due to model complexity and lack of integration with planning tools which address other concerns (e.g. collision avoidance). We propose an approach that uses differentiable inertial and kinematic models from standard open-source tools, integrating with standard ROS planning methods. An inverse dynamics-based energy model is optionally extended with a single-parameter electrical model, simplifying the model identification process. We compare the inertial and electrical models on a collaborative robot, showing that simplified models provide competitive accuracy and are easier to deploy in practice.","Reducing energy consumption in both new and existing industrial facilities is essential to meet climate goals. The most promising opportunity for energy optimization efforts is presented by industrial robots because their large install base, higher payloads and time utilization lead to more significant energy consumption compared with collaborative robots. Developing models that represent the robot‚Äôs energy usage is one way to optimize energy use in industrial robots [1]. By modifying the robot‚Äôs motion interpolator parameters, like acceleration and velocity, these models can be used to optimize energy consumption [2]. Optimizing the trajectory (i.e. including the path) between fixed start and end points is well-established and can save up to 15-30% of energy [3, 4]. However, the widespread implementation of existing trajectory optimization techniques in industrial robots faces several challenges. The majority of planning takes place in in-cell instruction or simulation environments, both of which have a limited ability to integrate data-driven models [5]. Additionally, ensuring compatibility with existing infrastructure requires the use of standard motion commands like joint or Cartesian moves, which complicates the use of methods that require the execution of an arbitrary optimized path. Figure 1: Overview of the proposed energy modeling pipeline, using standardized data formats for robot model and task, open-source trajectory planning methods, and being validated with energy measurements on robot execution. Robot trajectory optimization is an important aspect of robotics that aims to find the most efficient path for a robot to follow in order to complete a given task. Trajectory optimization has demonstrated significant effectiveness in designing dynamic movements for linear and nonlinear dynamical systems, that takes into account the physical constraints imposed by the specific maneuvers, the surrounding environment, and the capabilities of the hardware involved [6]. This involves also minimizing energy consumption, time or other resources. While dynamic robot simulation is not entirely new, there is significant potential in further developing and utilizing inertial and kinematic models derived from standard formats [7]. These differentiable models open up new opportunities for applying gradient-based optimization techniques for both energy reduction and parameter identification. By focusing on these models, energy optimization can be more effectively integrated into the robot design, planning, and control stages thereby advancing their application toward more efficient and sustainable robotic operations. We propose simplified robot energy models that can be integrated earlier in the design and commissioning process, offering new opportunities for improvement. By utilizing common data formats, these models can support gradient-based optimization, impact robot cell design, and guide kinematic decisions, so that the gap between theoretical optimization and real-world deployment can be reduced. To reduce the modeling work for gradient-based energy optimization of a robot the proposed energy model uses inverse inertial dynamics and a single electrical parameter. Compared with data-driven approaches, this model requires only energy measurements in a handful of static conditions to identify the electrical parameter. We demonstrate the integration of the model into open-source planning frameworks (MoveIt [8]), using differentiable robot dynamics (Pinocchio [9]) to support numerical optimization. First, the mechanical and electrical power models are proposed, then extended to total energy on trajectories. The proposed model is then validated on a collaborative robot (Franka Emika), and the accuracy of a mechanical vs mechanical-electrical model compared."
https://arxiv.org/html/2411.03176v1,Developing Simulation Models for Soft Robotic Grippers in Webots,"Robotic simulators provide cost-effective and risk-free virtual environments for studying robotic designs, control algorithms, and sensor integrations. They typically host extensive libraries of sensors and actuators that facilitate rapid prototyping and design evaluations in simulation. The use of the most prominent existing robotic simulators is however limited to simulation of rigid-link robots. On the other hand, there exist dedicated specialized environments for simulating soft robots. This separation limits the study of soft robotic systems, particularly in hybrid scenarios where soft and rigid sub-systems co-exist. In this work, we develop a lightweight open-source digital twin of a commercially available soft gripper, directly integrated within the robotic simulator Webots. We use a Rigid-Link-Discretization (RLD) model to simulate the soft gripper. Using a Particle Swarm Optimization (PSO) approach, we identify the parameters of the RLD model based on the kinematics and dynamics of the physical system and show the efficacy of our modeling approach in validation experiments. All software and experimental details are available on github: https://github.com/anonymousgituser1/Robosoft2025.","Soft robots are primarily constructed from highly flexible materials and often draw inspiration from the mechanical behavior of living organisms [1]. Researchers attempt to fabricate soft robots by mimicking the compliance and morphology of living creatures, with elastic bodies [2] and compliant actuation [3], such as soft grippers [4][5][6], soft manipulator [7], and soft robotic Ô¨Åsh [8]. The inherent softness and low mechanical resistance of soft robots allow them to interact safely with delicate objects and adapt passively to complex surfaces. Multiple physics-based robotic simulators exist for traditional rigid-link robots, such as Gazebo [9] and Webots [10], which provide powerful tools for designing both the hardware and control of these robots, including integration with various kinds of virtual sensors, actuators, and other simulated robots. However, these do not support the simulation of soft robots, limiting the potential for systematic design and development in this field. This also limits the study of hybrid systems of soft and rigid robots within the same simulation environment. (a) A pair of the real soft grippers mounted on a UR3e robotic arm. Pressurized air pipes are visible. (b) A pair of the simulated soft grippers mounted on a simulated UR3e robotic arm in Webots. Figure 1: Soft gripper attached to the UR3e robotic arm shown in the real setup and in the Webots simulation environment. The soft gripper is commercially available [11]. Previous studies have explored standard solid mechanics discretization methods like Finite Element Analysis (FEA) [12] and voxel-based discretization [13] to simulate soft robots. Other approaches, including the Cosserat rod theory [14] and the piece-wise constant curvature (PCC) approach [15], have also been studied to simulate soft robots‚Äô behavior. Data-driven approached also has been employed to model the kinematics of the soft gripper [16]. However, these methods mainly serve as Computer-Aided Design (CAD) tools rather than comprehensive simulation environments for robots. Several dedicated simulation environments have been developed to simulate soft robots‚Äô behavior, such as SOFA [17], primarily targeted at real-time physical simulation, and SoMo [18], a framework based on pyBullet. Mujoco [19], a general-purpose efficient physics engine designed to advance research and development in robotics, offers fluid dynamic simulation capabilities that are also usable for soft robot simulations. Other example of simulation environment include toolboxes within MATLAB that have been developed particularly for soft robotics simulation such as Sorotoki [20], which facilitate the development of soft robots by providing a comprehensive set of tools for design, modeling, and control, and also SoRoSim [21], that uses the Geometric Variable Strain (GVS) approach to provide a unified framework for the modeling, analysis, and control of soft robots. All of these simulation environments lack the comprehensiveness of full-fledged robotic simulators with integrated sensors, actuators, and environment modeling. In this study, we focus on pneumatically actuated beam-like soft grippers, a well-established category of soft robots [4] [6]. We present a computationally lightweight physics-based simulation model for beam-like soft grippers, directly integrated within the Webots robotic simulator. This integration provides access to Webots‚Äôs extensive sensor and actuator libraries. Furthermore, it offers capabilities of integrating soft and rigid robots, such as UR3e arm manipulator, within the same simulation environment, as shown in Figure 1(b). Firstly, we show how we model the structure of the soft body of a beam-like soft gripper via a Rigid-Link-Discretization (RLD) approach. The soft grippers are modeled as a series of rigid links, where each link is described by a mass-spring-damper system and is connected via joints to its neighboring links. Such a discretization method can readily be implemented in all open-source robotic simulation platforms. Secondly, we present an automated systems identification process based on dynamical data of the physical systems to create a digital twin instance of the physical soft grippers reducing the disparity between the simulation and reality. We use a heuristic search method, the Particles Swarm Optimization (PSO) algorithm to obtain a set of optimized parameters. Finally, we validate the accuracy of our simulation model using experimental data that are not used for systems identification. Our work offers a solution for harnessing the capabilities of the Webots robotic simulator in the study of beam-like soft robots that can be integrated or interact with rigid robots. This approach enables fully controllable and computationally efficient simulations of soft robots, coupled with an automatic calibration process to create comprehensive models integrated within an established robotic simulator."
https://arxiv.org/html/2411.03011v1,Set-Membership Estimation for Fault Diagnosis of Nonlinear Systems,"This paper introduces a Fault Diagnosis (Detection, Isolation, and Estimation) method using Set-Membership Estimation (SME) designed for a class of nonlinear systems that are linear to the fault parameters. The methodology advances fault diagnosis by continuously evaluating an estimate of the fault parameter and a feasible parameter set where the true fault parameter belongs. Unlike previous SME approaches, in this work, we address nonlinear systems subjected to both input and output uncertainties by utilizing inclusion functions and interval arithmetic. Additionally, we present an approach to outer-approximate the polytopic description of the feasible parameter set by effectively balancing approximation accuracy with computational efficiency resulting in improved fault detectability. Lastly, we introduce adaptive regularization of the parameter estimates to enhance the estimation process when the input-output data are sparse or non-informative, enhancing fault identifiability. We demonstrate the effectiveness of this method in simulations involving an Autonomous Surface Vehicle in both a path-following and a realistic collision avoidance scenario, underscoring its potential to enhance safety and reliability in critical applications.","In recent years, autonomous systems have advanced rapidly, transforming a wide range of human activities such as manufacturing, transportation, agriculture, and environmental monitoring. These technologies promise enhanced efficiency, reduced human error, and the ability to operate in hazardous environments. However, as these systems expand into more sectors, they increasingly depend on sophisticated technology and complex hardware, escalating the intricacies of their operational framework. Relying on critical components like sensors, actuators, and computational units introduces significant safety and reliability challenges. Faults in these components can lead to failures, posing catastrophic risks and jeopardizing safety. Ensuring safety and reliability in these technologies is of paramount importance, necessitating robust mechanisms to manage and mitigate faults effectively. This paper introduces a method to detect, isolate, and estimate faults, in a wide range of mobile robotic platforms (ground, marine, and aerial vehicles) described as nonlinear mechanical systems that can be expressed linearly to the parameters of interest. Faults in robotic systems have long been a critical concern across various domains, including ground [1], marine [2, 3], aerial [4, 5], and multi-agent systems [6]. A range of diagnostic and reconfiguration techniques have been deployed, such as observers [2], Kalman filters [4], and learning-based methods [7]. Faults primarily undermine system performance in two ways: firstly, they can impair the controllability or observability of the system, regardless of the control strategy employed. This typically necessitates redundancy in actuators and sensors to maintain operational capability after a fault has occurred. Secondly, faults create a discrepancy between the system‚Äôs theoretical model and the actual system itself. If significant, this mismatch can severely compromise controller performance, potentially leading to unpredictable and hazardous behavior. Fault diagnosis (FD) typically involves the detection, isolation, and estimation of faults, a process that presents significant challenges. The main challenge arises from various sources of uncertainty such as model inaccuracies, environmental disturbances, and measurement noise, which complicate the accurate identification of faults within the system. Set Membership Estimation (SME) has been widely utilized for FD, offering notable advantages. SME eliminates the need for knowing statistical distributions by relying solely on boundedness assumptions. FD through SME facilitates a direct approach by concentrating on fault parameters rather than indirectly inferred residuals, employing inverse tests for fault detection, and concurrently estimating the feasible parameter set from historical input-output data. Specific implementations of SME use zonotopic parameter sets for fault detection, as demonstrated in [8, 9], and employing ellipsoids to delineate the feasible parameter set, as seen in [10], [11], [12]. Other studies, such as [13, 14], apply SME to the system‚Äôs state for FD. However, the aforementioned works, typically focus on either nonlinear systems without both state and output uncertainties or simpler linear and single-output systems. Recent work has renewed interest in SME, particularly in adaptive control. Contributions such as [15] have combined SME with Model Predictive Control (MPC) in a Robust Adaptive MPC (RAMPC) framework, enabling planning based on nominal parameters while maintaining robustness against all feasible parameter realizations. Extensions of SME in RAMPC frameworks for linear systems have been explored in [16, 17], and for nonlinear systems in [18], though the challenge of handling both state and output uncertainties simultaneously remains largely unaddressed. Inspired by SME‚Äôs suitability for FD and its compatibility with MPC in a RAMPC framework, this work proposes an FD method based on SME, aimed at enhancing the trajectory optimization method introduced in [19] to improve safety in environments shared with human-operated vehicles. Specifically, this work extends SME to nonlinear systems affected by both state disturbances and measurement noise‚Äîa gap in the current state of the art. The method employs an inverse test for fault detection and isolation, with fault estimation achieved through continuous updates to the feasible parameter set and a fault parameter estimate. The key contributions of this work are: ‚Ä¢ Set-membership estimation to nonlinear systems, accounting for both disturbances and measurement noise. This capability ensures false alarm immunity by design, thereby increasing the robustness of the fault detection process. ‚Ä¢ A tighter outer approximation of the feasible parameter set that balances accuracy and computational efficiency, based on user-defined preferences. This leads to improved fault detectability, reducing the risk of missed detections and enhancing the system‚Äôs responsiveness to faults. ‚Ä¢ Adaptive regularization in fault parameter estimation to handle cases of sparse, non-informative measurement data, resulting in improved fault identifiability. This paper is organized as follows: Section II describes the problem formulation, laying the groundwork for the methodology discussed. Section III elaborates on our FD method with a detailed analysis of how the different components of SME are derived and used in the FD logic. Section IV demonstrates the application of our methodology through scenarios involving an Autonomous Surface Vehicle (ASV), showcasing the effectiveness of our approach. Section V concludes with some remarks on the implications of our findings and suggestions for future work."
https://arxiv.org/html/2411.02975v1,Transformer-Based Fault-Tolerant Control for Fixed-Wing UAVs Using Knowledge Distillation and In-Context Adaptation,"This study presents a transformer-based approach for fault-tolerant control in fixed-wing Unmanned Aerial Vehicles (UAVs), designed to adapt in real time to dynamic changes caused by structural damage or actuator failures. Unlike traditional Flight Control Systems (FCSs) that rely on classical control theory and struggle under severe alterations in dynamics, our method directly maps outer-loop reference values‚Äîaltitude, heading, and airspeed‚Äîinto control commands using the in-context learning and attention mechanisms of transformers, thus bypassing inner-loop controllers and fault-detection layers. Employing a teacher-student knowledge distillation framework, the proposed approach trains a student agent with partial observations by transferring knowledge from a privileged expert agent with full observability, enabling robust performance across diverse failure scenarios. Experimental results demonstrate that our transformer-based controller outperforms industry-standard FCS and state-of-the-art reinforcement learning (RL) methods, maintaining high tracking accuracy and stability in nominal conditions and extreme failure cases, highlighting its potential for enhancing UAV operational safety and reliability.","In recent years, Unmanned Aerial Vehicles (UAVs) have been widely used to perform various applications in complex and critical scenarios, such as search and rescue or autonomous medical transportation. The operational safety and reliability of these aerial robots have become major concerns due to the potential implications of system failures. Unlike other robotics fields, such as manipulation and humanoid locomotion, where advanced control methods are essential for managing complex joint movements, UAV Flight Control Systems (FCSs) in industry typically rely on classical control techniques for low-level control layers. While modern approaches, like Model Predictive Control (MPC), offer significant advantages for high-level tasks such as trajectory planning and collision avoidance [1, 2], they require precise system models, extensive uncertainty handling, and high computational resources, which often make them impractical for low-level UAV control. The simplicity, reliability, and efficiency of classical control techniques have established them as the preferred choice for UAVs attitude control. (a) (b) (c) (d) Figure 1: Trajectory comparison between the proposed transformer-based controller (blue) and an industry-standard FCS (red). Figures (a) and (b) illustrate nominal scenario tracking, while (c) and (d) demonstrate the controllers‚Äô responses to semi-wing damage, with the FCS losing control and the proposed method stabilizing the UAV. However, complex environments and demanding tasks can cause structural damage to the UAV, altering its aerodynamic characteristics and dynamics. Fixed-wing UAVs, in particular, exhibit highly complex, nonlinear dynamics, which can be significantly disrupted if the structure is compromised. Although current FCSs are robust, they struggle to maintain performance when the vehicle dynamics deviate from the original design specifications, sometimes leading to control divergence and catastrophic failure. Fault-tolerant flight control has become a focal point for safety-critical UAV operations. Typically, fault-tolerant methods rely on fault detection and diagnosis techniques, which identify faults and then adjust controller parameters to account for the new dynamics [3, 4, 5, 6]. This approach is complex, requiring real-time fault identification and parameter adjustment in a highly nonlinear dynamic environment. Reinforcement Learning (RL) has introduced alternative solutions to the problem [7, 8, 9, 10]. With its ability to handle high-dimensional, nonlinear dynamics, RL holds promise for system fault management. However, RL algorithms are typically designed for Markov Decision Process (MDP) formulations, while fault-tolerant control‚Äîwith sudden, unobserved changes in dynamics‚Äîmust be framed as a Partially Observable Markov Decision Process (POMDP), making RL algorithms learning more difficult and possibly leading them to suboptimal performance [11]. Although recent works have shown possible solutions to this problem [11, 12, 13], these add considerable complexity to the algorithms. Most research focuses on actuator faults in multicopter UAVs [4, 9] or on fixed-wing UAV fault tolerance at the inner attitude control loop level, avoiding altitude, heading, and airspeed tracking [7, 8]. In this work, we propose a novel transformer-based fault-tolerant control method to directly map reference points in the outer loop of attitude UAV control‚Äîaltitude (h‚Ñéhitalic_h), heading (Œ®Œ®\Psiroman_Œ®), and airspeed (VTsubscriptùëâùëáV_{T}italic_V start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT)‚Äîinto control-surface and throttle commands, enabling full UAV control without the need for inner control loops, which complicate the system. Our method employs the attention mechanisms and in-context learning capabilities of transformer models to adapt control actions to dynamic changes during inference, thereby eliminating the need for fault detection or identification and parameter adjustments in the Flight Control System (FCS). Our transformer-based controller uses a context window of past UAV states to autonomously detect and adapt to dynamic changes. Fig. 1 shows a comparison between our proposed transformer-based controller (blue trajectory) and an industry-standard FCS (red trajectory). In Fig. 1(a) and Fig. 1(b), both systems track the commanded references in nominal conditions. Figs. 1(c) and 1(d) illustrate the response when the UAV experiences semi-wing damage, showing the FCS losing control and causing a crash, while our method stabilizes the UAV and follows the reference values. The contributions of this work are as follows: ‚Ä¢ We present a novel learning framework based on teacher-student knowledge distillation for learning fault-tolerant policies in fixed-wing UAVs. In this framework, the teacher agent is trained using RL on privileged environment information to address partial observability limitations, then a student agent is trained without privileged information using the teacher‚Äôs interactions with the environment. ‚Ä¢ We design a transformer-based flight controller that utilizes in-context learning to adapt its behavior in real-time as UAV dynamics change due to failures. This controller employs a context of past states to determine actions, eliminating the need for fault detection methods. ‚Ä¢ We conduct a comparative study against state-of-the-art methods, addressing not only actuator faults but also significant structural damage scenarios."
https://arxiv.org/html/2411.02938v1,Multi-Modal 3D Scene Graph Updater for Shared and Dynamic Environments,"The advent of generalist Large Language Models (LLMs) and Large Vision Models (VLMs) have streamlined the construction of semantically enriched maps that can enable robots to ground high-level reasoning and planning into their representations. One of the most widely used semantic map formats is the 3D Scene Graph, which captures both metric (low-level) and semantic (high-level) information. However, these maps often assume a static world, while real environments, like homes and offices, are dynamic. Even small changes in these spaces can significantly impact task performance. To integrate robots into dynamic environments, they must detect changes and update the scene graph in real-time. This update process is inherently multimodal, requiring input from various sources, such as human agents, the robot‚Äôs own perception system, time, and its actions. This work proposes a framework that leverages these multimodal inputs to maintain the consistency of scene graphs during real-time operation, presenting promising initial results and outlining a roadmap for future research.","A fundamental goal in robotics research is to enable robots to operate safely, efficiently, and autonomously in environments shared with humans. In such shared spaces‚Äîwhether homes, offices, or other collaborative settings‚Äîrobots must perform a wide range of tasks, from simple object manipulation to complex interactions with the environment. To achieve this, robots rely heavily on internal representations of their surroundings, typically in the form of maps. These maps allow robots to understand spatial layouts and reason about tasks at both low-level (e.g., navigation) and high-level (e.g., task planning). However, one key assumption often made when using these maps is that the environment remains static after the map is created. In practice, this assumption breaks down in dynamic, shared environments where human and robotic activities constantly alter the state of the world. Furniture is moved, objects are relocated, and items are used or discarded. These changes are non-trivial, and any discrepancies between the robot‚Äôs internal map and the real-world environment can significantly degrade task performance. For instance, a robot instructed to retrieve a banana from the kitchen may fail if it relies on outdated map information‚Äîsuch as the banana being present a week ago‚Äîwithout accounting for the possibility that it has since been eaten or thrown away. Such inconsistencies can lead to inefficient behavior, incorrect task execution, and ultimately limit the robot‚Äôs usefulness in dynamic environments. To prevent this degradation in performance, robots must possess the ability to detect changes in their environment and update their internal maps in real time. This capability is essential for ensuring that downstream reasoning and task planning are based on an accurate, up-to-date representation of the world. A critical insight from this work is that maintaining the consistency of a robot‚Äôs map in shared, living environments is inherently a multimodal challenge. It requires the robot to integrate information from multiple sources‚Äîsuch as visual perception, interactions with humans and other agents, temporal data, and its own actions‚Äîto inform map updates. The ability to leverage this diverse set of information streams is key to building robust, real-time scene representations that evolve as the environment changes. One of the most prominent representations used in robotics for modeling complex environments is the 3D Scene Graph (3DSG)[1]. 3DSGs encode both metric and semantic information, allowing robots to reason about not only the spatial configuration of objects but also their relationships and roles in the environment. The flexibility of 3DSGs makes them a powerful tool for representing dynamic environments, as they can be extended to include different types of information. For example, hierarchical relationships between objects and their surrounding spaces (e.g., an object belonging to a room) can be incorporated[2], as well as semantic relationships between objects (e.g., objects that are typically used together) [3]. Despite the versatility of 3DSGs, existing research has primarily focused on their construction and optimization for specific tasks, with limited attention to how these representations can be dynamically updated to reflect ongoing changes in the real world. To address this critical gap, we propose MM-3DSGU, a general framework designed to dynamically update 3D Scene Graphs in shared, dynamic environments. MM-3DSGU leverages heterogeneous sources of information, including time, human interactions, the robot‚Äôs perception system, and its own actions, to perform robust change detection and maintain an accurate representation of the environment. By integrating these multimodal streams, our framework ensures that the robot‚Äôs scene graph is consistently aligned with the real world, enabling better task execution and reasoning in dynamic settings. In this work, we present the design and initial implementation of MM-3DSGU, along with promising results from preliminary experiments. We also outline a detailed roadmap for future research, aimed at further refining our approach and addressing challenges related to scalability, efficiency, and real-time performance in more complex environments. Figure 1: High-level overview of the proposed framework. The various modalities proposed for change detection are displayed (top). These modules leverage the latest version of the 3D scene graph to assess whether change has occurred. Update operations on the 3D scene graph (bottom) are applied agnostically, due to the use of a unified language."
https://arxiv.org/html/2411.02788v1,When to Localize?A Risk-Constrained Reinforcement Learning Approach,"In a standard navigation pipeline, a robot localizes at every time step to lower navigational errors. However, in some scenarios, a robot needs to selectively localize when it is expensive to obtain observations. For example, an underwater robot surfacing to localize too often hinders it from searching for critical items underwater, such as black boxes from crashed aircraft. On the other hand, if the robot never localizes, poor state estimates cause failure to find the items due to inadvertently leaving the search area or entering hazardous, restricted areas. Motivated by these scenarios, we investigate approaches to help a robot determine ‚Äúwhen to localize?‚Äù We formulate this as a bi-criteria optimization problem: minimize the number of localization actions while ensuring the probability of failure (due to collision or not reaching a desired goal) remains bounded. In recent work, we showed how to formulate this active localization problem as a constrained Partially Observable Markov Decision Process (POMDP), which was solved using an online POMDP solver. However, this approach is too slow and requires full knowledge of the robot transition and observation models. In this paper, we present RiskRL, a constrained Reinforcement Learning (RL) framework that overcomes these limitations. RiskRL uses particle filtering and recurrent Soft Actor-Critic network to learn a policy that minimizes the number of localizations while ensuring the probability of failure constraint is met. Our numerical experiments show that RiskRL learns a robust policy that outperforms the baseline by at least 13% while also generalizing to unseen environments.","Figure 1: Motivating example. Consider a robot that may want to seldom localize (e.g., due to resource constraints) while traveling along a path (black dashed line). Despite obstacles (rocks and water), the autonomous robot can execute a series of open-loop motions for some period. However, as the dead reckoning uncertainty (gold and red ellipses) grows, the probability of failure (such as collision) may become too large. As such the robot must localize at some point to avoid failures. Thus, our question is: When should the robot localize to reduce failure probabilities? In robotics, self-localization is crucial because it enhances navigation accuracy, and situational awareness, and enables complex tasks. Typically, an autonomous robot perceives its environment and self-localizes, plans its subsequent actions, acts upon its plan phases, and repeats the cycle. However, sometimes, a robot may want to localize seldom when it is not advantageous. For example, underwater robots need to surface to localize in underwater rescue and recovery missions. Surfacing to localize too often may hinder an underwater robot from searching for critical underwater items such as black boxes from crashed aircraft. On the other hand, if the robot never localizes, it will accumulate large amounts of drift [1], which may prevent it from finding the items due to inadvertently leaving the search area or entering hazardous, restricted areas. Therefore, in such scenarios, robots must balance prolonged actions that achieve mission objectives (such as searching for critical items underwater) with localizing to improve navigation accuracy. We explored such scenarios in our recent work [2]. Our central question was: how can a robot plan and act for long horizons safely and only localize when necessary? (Figure 1 discusses a general scenario to this question). We emphasize that such a question is not trivial because we have two competing objectives. The first objective is localize often to maximize mission safety and performance, where we can ensure the vehicle remains within the search area and out of hazardous zones. On the other hand, the second objective is to localize infrequently to minimize the number of times the vehicle must deviate from its mission, which in turn can reduce mission time. These two objectives are challenging to optimize via one objective, as shown with our POMCP baseline in [2]. Therefore, we addressed the question by formulating it as a constrained Partially Observable Markov Decision-making Process (POMDP), where our objective was to minimize the number of localization actions while not exceeding a given probability of failure due to collisions. Then we employed CC-POMCP [3], a cost-constrained POMDP solver, to find policies that determine when the robot should move along a given path or localize. Although our prior approach produced policies that reached the goal and outperformed baselines, the approach had limitations. First, CC-POMCP was computationally expensive, requiring over 20 minutes of inference to navigate a path of 55 waypoints111We obtained these run times from performing the experiments on a computing cluster.. Second, CC-POMCP requires a well-defined model of the environment, including the robot‚Äôs transition (motion) and observation models. Requiring such models may be problematic in unknown or dynamic real-world environments where it may be challenging to obtain accurate models. Finally, in our prior experiments, CC-POMCP failed to reach the goal when we increased the amount of transition noise, which limited successful experiments to low transition noises. We propose a novel approach termed RiskRL that employs constrained Reinforcement Learning (RL) [4] and Particle Filters (PF) to overcome these limitations. Our new approach has multiple advantages over our prior work. First, although our new approach has a longer, single training time, it infers quicker during deployment, enabling real-time planning. The second was reducing the need for accurate transition and observation models of the environment. Like cost-constrained POMDPs in our prior work [2], constrained RL allows us to separate the value function and risk. In this formulation, the risk is modeled as a probability constraint, allowing us to design policies that minimize the failure probability while ensuring the robot remains within acceptable risk levels. This formulation provides greater control over the failure rate by explicitly incorporating risk constraints into the decision-making process. Furthermore, we use a PF to maintain the robot‚Äôs belief as the robot executes noisy motion commands and receives noisy measurements from the environment. We also use the PF to compute the observation for the RL robot. We perform numerical experiments to compare RiskRL with several baselines, including standard RL (BaseRL), CC-POMCP, and heuristic policies. Our main finding is that when deployed in the environment that the robot is trained in RiskRL outperforms the BaseRL and CC-POMCP baselines by at least 13% in terms of the success rate while also being the only algorithm that satisfies the risk constraint. While environment-specific heuristics can match the performance, we find that RiskRL also generalizes to unseen environments, which the heuristics cannot."
https://arxiv.org/html/2411.02772v1,Communication and Energy-Aware Multi-UAV Coverage Path Planning for Networked Operations,"This paper presents a communication and energy-aware Multi-UAV Coverage Path Planning (mCPP) method for scenarios requiring continuous inter-UAV communication, such as cooperative search and rescue and surveillance missions. Unlike existing mCPP solutions that focus on energy, time, or coverage efficiency, our approach generates coverage paths that require minimal the communication range to maintain inter-UAV connectivity while also optimizing energy consumption. The mCPP problem is formulated as a multi-objective optimization task, aiming to minimize both the communication range requirement and energy consumption. Our approach significantly reduces the communication range needed for maintaining connectivity while ensuring energy efficiency, outperforming state-of-the-art methods. Its effectiveness is validated through simulations on complex and arbitrary shaped regions of interests, including scenarios with no-fly zones. Additionally, real-world experiment demonstrate its high accuracy, achieving 99% consistency between the estimated and actual communication range required during a multi-UAV coverage mission involving three UAVs.","I INTRODUCTION In the past few years, the deployment of multiple Unmanned Aerial Vehicles (UAVs) has attracted the interest of many enterprise fields, becoming a powerful tool for professionals to acquire data in a fast and efficient way. Multi-UAV systems have revolutionized various industries by enabling complex missions to be carried out quickly and with precisely, making them invaluable in fields such as surveillance [1], photogrammetry [2], agricultural monitoring, precision farming [3], and Search And Rescue (SAR) [4, 5]. By utilizing multiple UAVs simultaneously, entire regions of interest can be surveyed quickly and thoroughly, addressing challenges that were previously difficult to overcome. The advantages of multi-UAV systems are evident in their ability to rapidly cover large areas and coordinate tasks seamlessly. They efficiently scan vast regions for SAR operations, monitor crop health and manage resources in agriculture [6, 7], and assess disaster-stricken areas by generating detailed maps and 3D models for informed decision-making [8]. These capabilities showcase the transformative impact of multi-UAV systems across diverse sectors, driving innovation and enhancing operational efficiency. Given the diverse applications of multi-UAV systems, efficient and safe path planning is crucial for executing coordinated missions. A major challenge in Multi-UAV Coverage Path Planning (mCPP) is generating paths that maximize coverage within the Region of Interest (ROI), minimize energy consumption, and maintain communication throughout the mission. Optimizing all of these factors is an NP-hard problem. Although various solutions have been proposed, they include algorithms that handle No-Fly Zones (NFZs) [9, 10, 11], enhance energy efficiency [10, 12], optimize flight speed [10], and support uneven load distribution [9, 12]. While research on mCPP has predominantly focused on optimizing energy, coverage, and time, the consideration of communication requirements in coverage path planning remains largely unexplored. In multi-UAV cooperative missions, reliable communication between vehicles is crucial. Designing coverage paths without accounting for communication needs can lead to frequent disconnections during the mission. This can result in coordination challenges and potential data loss. Maintaining reliable communication is especially critical in operations such as security surveillance, inspection, and SAR, where it is essential for mission success. In this paper, we propose a communication and energy aware mCPP algorithm that generates coverage paths with minimal inter-UAV communication range requirements. This ensures that all UAVs remain connected throughout the mission in a mesh network architecture. Simultaneously, the algorithm minimizes energy consumption, maximizes area coverage, and completes the mission efficiently. The algorithm utilizes the groundwork laid by Divide Areas based on Robots Initial Positions (DARP) algorithm [9], solving the mCPP problem through area descritization with optimal allocation and constructing a Spanning Tree Coverage(STC) paths on polygonal regions. Energy consumption for the coverage paths is estimated using a direct power consumption model based on the UAVs‚Äô physical parameters. The primary contribution of this paper is an optimization algorithm that generates coverage paths with minimal inter-UAV communication range requirements. We achieve this by treating the range requirement and energy consumption as hyperparameters in the optimization process, solved using Bayesian optimization techniques across multiple DARP initialization trials. Additionally, we optimize the starting points on generated paths to further minimize communication range requirements. A time-sampling methodology is implemented on these paths, followed by continuous connectivity evaluation to analyze range needs. An optimal shift and rotation technique is applied before discretizing the polygon into cells, enhancing coverage efficiency. This multi-layered optimization framework improves inter-UAV connectivity and energy efficiency, while also supporting complex polygons with NFZs."
https://arxiv.org/html/2411.02706v1,Safety Verification for Evasive Collision Avoidance in Autonomous Vehicles with Enhanced Resolutions,"This paper presents a comprehensive hazard analysis, risk assessment, and loss evaluation for an Evasive Minimum Risk Maneuvering (EMRM) system designed for autonomous vehicles. The EMRM system is engineered to enhance collision avoidance and mitigate loss severity by drawing inspiration from professional drivers who perform aggressive maneuvers while maintaining stability for effective risk mitigation. Recent advancements in autonomous vehicle technology demonstrate a growing capability for high-performance maneuvers. This paper discusses a comprehensive safety verification process and establishes a clear safety goal to enhance testing validation. The study systematically identifies potential hazards and assesses their risks to overall safety and the protection of vulnerable road users. A novel loss evaluation approach is introduced, focusing on the impact of mitigation maneuvers on loss severity. Additionally, the proposed mitigation integrity level can be used to verify the minimum-risk maneuver feature. This paper applies a verification method to evasive maneuvering, contributing to the development of more reliable active safety features in autonomous driving systems.","In today‚Äôs fast-paced digital world, driving has evolved from a manual task into a complex information-processing challenge. With the proliferation of digital distractions and a decline in human attention spans, the task of automated and autonomous driving has become increasingly demanding. Evasive maneuvers as a minimum risk maneuver for SAE level 3 and 4 Autonomous Vehicles (AV) offer a promising solution to enhance overall transportation and road safety by compensating for these human limitations. When drivers are overwhelmed by the information they need to process, autonomous systems equipped with advanced evasive maneuvers can step in to mitigate potential risks. These systems utilize cutting-edge technology to rapidly analyze situations and execute maneuvers that are beyond human capabilities, providing a crucial safety net. By integrating these advanced systems, we can significantly reduce the likelihood of accidents, ensuring that vehicles respond to hazards with speed and precision. Identifying autonomous driving limitations offers insights for enhancement opportunities that helps safer adoption to the future proof transportation systems [1]. This paper studies how evasive maneuvers can enhance autonomous vehicle safety by applying a comprehensive safety assessment. Active safety features, such as Advanced Emergency Brake (AEB), are unable to perform Minimum Risk Maneuvers (MRM) in specific hazardous scenarios where braking alone is insufficient to avoid an accident. An MRM is the fallback function by an Automated Driving System (ADS) to reach a stable and stopped state with a minimal risk condition [2]. While the ISO/SAE Level 3-5 ADS performs the dynamic driving task, an event that prevents the ADS from continuing the dynamic driving task can occur [2, 3]. An MRM feature aims to improve road safety by focusing on new technologies and strategies to reduce accidents and enhance overall traffic safety [4]. Similar to MRM, evasive maneuvers inspired by professional and stunt car divers can be used to avoid hazardous situations or mitigate harms severity [5, 6]. Integration of an Evasive MRM (EMRM) feature into an Advanced Driving Assistant System (ADAS) or AV will require comprehensive safety Verification and Validation (V&V) [7, 8]. Hazard Analysis and Risk Assessment (HARA) is a crucial first step and popular method for verifying Functional Safety (FuSa) in ADAS and AV within the automotive industry [3]. Frameworks provided in standards sometime are very general customization might be needed for specific functionalities [8]. A more effective analyze for functions at edge cases can be done by applying the necessary granularity. Fig.1 shows an AV driving fast on an urban domain when a scooter rider suddenly blocks the pre-planned path. It puts the AV in a critical hazardous situation with a high risk of a fatal accident. Efforts to mitigate these risks might not always be successful, potentially leading to other unexpected accidents. Hence, it is crucial to understand the circumstances under which severe hazards are avoidable and the loss level can be reduced. In this scenario, an evasive system can mitigate the situation so that the AV encounters a lower fatality risk, albeit with a higher risk of property or vehicle damage. Consequently, it is crucial that the system assess the situation and choose the path that minimizes harm, whether it be the vehicle, property, life, or health. Figure 1: A schematic depicting a hazardous scenario where a basic MRM might fail to avoid or mitigate the loss in an urban operational designed domain. Advanced EMRM can perform aggressive evasive maneuvers to avoid a catastrophic hazard. Exposure to hazardous situations and the reduction of crash risk or loss effects involve critical decision-making processes that require high-performance processing and agile responses. [9]. When an unexpected emergency occurs, applying acceleration at a rate higher than comfortable driving is inevitable [10]. Therefore, performing evasive maneuvers with guaranteed safety can be used for MRMs [11, 12]. Gleirscher and Kugele [13], discussed the process of identifying and mitigating hazards in automated driving systems, though it acknowledges limitations in analyzing evasive maneuvers. To tackle safety assessments for EMRMs, this work proposes a framework that adds the loss severity to HARA for analysis of EMRM features. EMRM is designed to initially perform collision avoidance or risk reduction of severe losses when the accident is inevitable. This paper studies how to verify if EMRMs can improve overall FuSa, thereby there is a potential for development of reliable evasive maneuvering feature for AVs. Adding loss evaluation to HARA analysis is needed for the EMRM feature where an accident is inevitable. A classic HARA identifies hazardous events and associated risks of exposure. Describing the circumstances leading to a specific loss severity is needed, and they can be modeled and classified by severity as loss states of the system [14]. In addition, identifying systemic hazards through a structured approach by systematically decoding hazards is possible using System Theoretic Process Analysis (STPA) [15]. A unique combination of STPA‚Äôs strength and Finite State Machines (FSM) is also used for AV safety assessments [16]. It is noteworthy that none of these methods have addressed understanding the safety goals where multiple loss severity increases the complexity of a hazardous scenario. However, it has been shown that these methods can be customized or combined to address the shortages [17]. In this paper, a systematic approach based on the HARA framework is proposed to assess functions for edge cases, such as MRM features. Our novel approach, adds more granularity to a classic HARA [3] to differentiate possible losses in severe and critical situations. This is a necessary step before designing and implementing any EMRM feature. EMRM technique can perform evasive maneuvers for collision avoidance or loss severity reduction [18, 19]. By incorporating a more granular loss severity, the developed approach evaluates the risks with a higher resolution for a range of minimum and maximum loss severity levels at the hazardous event. Risks of losses with different severity are needed to ensure the system either mitigates the hazard risks or reduces the loss severity [5]. In this respect, this method effectively handles different types of loss severity during risk assessment. Similarly, the proposed approach can be used to analyze comprehensively next-generation AV and ADAS active safety features where potential loss of life or severe damages should be considered. The contributions of this paper are summarized as follows: ‚Ä¢ Adding loss evaluation to extend a classic HARA method in order to enhance the verification granularity resolution around severe conditions. ‚Ä¢ Introducing a framework with more granularity for higher precision analysis of festoons designed for edge cases. ‚Ä¢ Conducting a comprehensive functional safety assessment for evasive minimum risk maneuvering feature, designed to mitigate crash risks or reduce loss severity. The remainder of the paper is structured as follows. In section II, Hierarchical structure and features of EMRM will be explained. Section III explains the HARA method used in this paper for verifying the safety of EMRM for collision avoidance and loss mitigation. Additionally, levels of risks and losses will be described, and HARA analysis for EMRM with examples of hazardous scenarios will be provided to verify EMRM safety goals. In Section IV, hazards of an evasive minimum risk maneuver feature are analyzed. Finally, Section VI concludes the paper."
https://arxiv.org/html/2411.02704v1,RT-Affordance: Affordances are VersatileIntermediate Representations for Robot Manipulation,"We explore how intermediate policy representations can facilitate generalization by providing guidance on how to perform manipulation tasks. Existing representations such as language, goal images, and trajectory sketches have been shown to be helpful, but these representations either do not provide enough context or provide over-specified context that yields less robust policies. We propose conditioning policies on affordances, which capture the pose of the robot at key stages of the task. Affordances offer expressive yet lightweight abstractions, are easy for users to specify, and facilitate efficient learning by transferring knowledge from large internet datasets. Our method, RT-Affordance, is a hierarchical model that first proposes an affordance plan given the task language, and then conditions the policy on this affordance plan to perform manipulation. Our model can flexibly bridge heterogeneous sources of supervision including large web datasets and robot trajectories. We additionally train our model on cheap-to-collect in-domain affordance images, allowing us to learn new tasks without collecting any additional costly robot trajectories. We show on a diverse set of novel tasks how RT-Affordance exceeds the performance of existing methods by over 50%, and we empirically demonstrate that affordances are robust to novel settings. Videos available at https://snasiriany.me/rt-affordance","I INTRODUCTION In recent years, we have seen the rise of large pretrained models for learning robot policies. Vision-language-action (VLA) models [rt22023arxiv, kim24openvla], pretrained with large-scale robot data on top of vision-language models (VLMs) [geminiteam2024gemini] come with the promise of generalization to new objects, scenes, and tasks. However, VLAs are not yet reliable enough to be deployed outside of the narrow lab settings on which they are trained. While these shortcomings can be mitigated by expanding the scope and diversity of robot datasets, this is highly resource intensive and challenging to scale. Alternatively, there are various ways of interfacing with the policy that can potentially facilitate generalization by providing useful guidance on how to perform manipulation tasks. Examples of these policy representations include language specifications [zhang2024sprint, belkhale2024rth], goal images [black2023susie], goal sketches [sundaresan2024rtsketch], and trajectory sketches [gu2023rttrajectory]. These interfaces introduce mid-level abstractions that shield the policy from reasoning in a higher dimensional input space ‚Äî leading to policies that can generalize over these intermediate representations. While one of the most common policy representations is conditioning on language, in practice most robot datasets are labeled with underspecified descriptions of the task and language conditioning does not reveal enough guidance on how to perform the task. Alternatively, goal image-conditioned policies provide detailed spatial context about the final goal configuration of the scene. However, goal-images are high-dimensional, which presents learning challenges due to over-specification issues [sundaresan2024rtsketch, shah2023mutex]. Furthermore, providing goal images at evaluation time is cumbersome for human users. This has lead to exploration of other intermediate representations ‚Äî trajectory or goal sketches [gu2023rttrajectory, sundaresan2024rtsketch], or keypoints [yuan2024robopoint, fangandliu2024moka] ‚Äî that attempt to provide spatial plans for the policy. While these spatial plans are informative, they still lack sufficient information for the policy on how to manipulate ‚Äî e.g. what pose of the gripper should take when picking up a clothes hanger. Figure 1: Bridging robot and internet data via affordances. Prior work has shown the utility of co-training on robot and web datasets. However, robot actions and web content are still disjoint in their structure. We propose using affordances as a means to bridge this gap. Reasoning about affordances requires semantic and spatial reasoning, which is readily needed in VQA and spatial reasoning tasks such as object detection. By incorporating affordance reasoning explicitly in robot control tasks, we can better transfer knowledge from these web datasets to robot control tasks. Figure 2: Comparison of policy interfaces. Conditioning on language is intuitive, yet language typically does not provide enough guidance on how to perform the task. Goal images and trajectory sketches are typically over-specified and present learning challenges. We propose conditioning policies on intermediate affordance representations, which are expressive yet compact representations of tasks, making them easy to specify and to learn. In this work, we seek a policy representation that provides expressive yet lightweight abstractions for learning robust manipulation polices. We propose RT-Affordance, which is a policy conditioned on both language specifications and visual affordances. The visual affordances show the pose of the robot end effector at key stages of the task, visually projected onto the image input of the policy. By conditioning on affordances, the robot will have access to precise yet concise guidance on how to manipulate objects. To allow a seamless experience for the human user, we employ a hierarchical model that only requires task language from the user. The model first predicts the affordances given a task specification in language, and then leverages the affordances as an intermediate representation to steer the policy. The initial affordance prediction module can be trained on existing robot trajectories and web-scale datasets labeled with spatial information and affordances [Ego4D2022CVPR] (see Figure 1). We further enhance capabilities by training on a modest dataset of cheap-to-collect in-domain images annotated with affordances. This allows us to bypass costly robot teleoperation and learn novel tasks more scalably. We perform extensive experiments, where we show that RT-Affordance is effective across a broad range of real world tasks, achieving 69% overall success rate compared to 15% success rate for language-conditioned policies. We show how incorporating both web data and cheap-to-collect affordance images allows us to learn novel tasks without collecting any additional robot demonstrations. Additionally, we demonstrate that the resulting affordance prediction model is robust to distribution shifts, with overall performance on out of distribution settings within 10% of in-distribution evaluations."
https://arxiv.org/html/2411.02703v1,LVI-GS: Tightly-coupled LiDAR-Visual-Inertial SLAM using 3D Gaussian Splatting,"3D Gaussian Splatting (3DGS) has shown its ability in rapid rendering and high-fidelity mapping. In this paper, we introduce LVI-GS, a tightly-coupled LiDAR-Visual-Inertial mapping framework with 3DGS, which leverages the complementary characteristics of LiDAR and image sensors to capture both geometric structures and visual details of 3D scenes. To this end, the 3D Gaussians are initialized from colourized LiDAR points and optimized using differentiable rendering. In order to achieve high-fidelity mapping, we introduce a pyramid-based training approach to effectively learn multi-level features and incorporate depth loss derived from LiDAR measurements to improve geometric feature perception. Through well-designed strategies for Gaussian-Map expansion, keyframe selection, thread management, and custom CUDA acceleration, our framework achieves real-time photo-realistic mapping. Numerical experiments are performed to evaluate the superior performance of our method compared to state-of-the-art 3D reconstruction systems. Videos of the evaluations can be found on our website: https://kwanwaipang.github.io/LVI-GS/.","Simultaneous Localization and Mapping (SLAM) systems are indispensable across various domains, including robotics, augmented reality, and autonomous navigation[1]. These systems enable devices to understand and navigate complex environments by constructing maps while simultaneously estimating their own positions within these spaces. For effective SLAM, both accurate localization and comprehensive scene reconstruction are essential. Traditional SLAM systems represent environments using landmark [2], point clouds[3], occupancy grids[4], Signed Distance Function (SDF) voxel grids[5], or meshes[6]. Among these, point clouds are a straightforward scene representation readily obtainable from sensors like cameras and LiDAR. Point cloud-based SLAM systems can achieve accurate localization and construct either sparse or dense maps, though these tend to lack visually rich details. The advent of Neural Radiance Fields (NeRF) has introduced a new approach for high-fidelity scene reconstruction[7]. NeRF implicitly represents a scene within a radiance field by optimizing a continuous volumetric scene function, which requires minimal memory. Some NeRF-based SLAM methods leverage this framework‚Äôs novel view synthesis and high-fidelity reconstruction capabilities to model scenes. For instance, iMAP[8] constructs an implicit 3D occupancy and colour model usable for tracking, while NICE-SLAM[9] represents larger scenes via a coarse-to-fine approach. Enhanced methods such as Vox-Fusion[10], CoSLAM[11], and ES-SLAM[12] advance SLAM system performance to varying extents. However, due to the extensive optimization processes involved, these systems struggle to achieve real-time performance. Moreover, storing maps within multi-layer perceptrons poses challenges, including catastrophic forgetting and limited boundaries, which can impede scene reconstruction. 3D Gaussian Splatting (3DGS) offers an exciting alternative, providing a continuous and adaptable representation for modelling 3D scenes through differentiable 3D Gaussian-shaped primitives[13][14]. As a semi-implicit mapping approach, it trades off some novel view synthesis capabilities for significantly faster optimization and rendering speeds. Despite being based on optimization, 3DGS closely resembles point and surfel clouds, thus inheriting their efficiency, locality, and adaptability‚Äîattributes beneficial for SLAM mapping. With rendering speeds up to 200 frames per second at a 1080p resolution, 3DGS also initializes using point clouds, allowing it to leverage the sparse or dense point clouds generated by conventional SLAM systems for high-fidelity imagery[15]. Recently, several SLAM approaches integrating 3D Gaussians have shown promising results. Methods such as SplaTAM[16], MonoGS[15], GS-SLAM[17], and Photo-SLAM[18] employ sequential RGB-D or RGB data to establish complete SLAM systems. However, these techniques encounter difficulties in large-scale, uncontrolled outdoor environments characterized by challenging lighting, complex backgrounds, and rapid motion. Although LiDAR offers high-quality geometric initialization for 3D Gaussians and is generally more robust in outdoor settings than cameras, integrating it into SLAM systems introduces unique challenges. LIV-Gaussianmap[19] and LetsGo[20] utilize LiDAR for initializing 3D Gaussians, while Gaussian-LIC[21] combines a LiDAR-Inertial-Camera setup for comprehensive 3D Gaussian construction. Nevertheless, systems like LIV-Gaussianmap[19] and LetsGo[20] are limited to offline processing, and Gaussian-LIC[21] requires complex front-end odometry and maintaining a substantial number of keyframes. In general, the main contributions of this study can be outlined as follows: 1. We have developed and implemented a sophisticated real-time LVI-GS system that is capable of maintaining a dynamic hyper primitives module. This system leverages 3D Gaussian Splatting (3DGS) to perform high-quality, real-time rendering in three-dimensional space, ensuring an efficient and accurate representation of complex environments. 2. To further enhance the performance and scalability of our system, we employed a coarse-to-fine map construction approach. This method utilizes both RGB image pyramids and depth image pyramids to progressively refine the map at different levels of detail. Additionally, we implemented an advanced thread management technique to optimize computational efficiency, ensuring smooth real-time operations even with large datasets. 3. In pursuit of improved map representation and rendering quality, we designed a robust keyframe management strategy that allows for the effective selection and processing of keyframes. Moreover, by incorporating depth loss into the system, we enhanced the accuracy of the 3D Gaussian map, leading to more precise reconstructions and visually superior rendering results."
https://arxiv.org/html/2411.02619v1,Tracking Tumors under Deformation from Partial Point Clouds using Occupancy Networks,"To track tumors during surgery, information from preoperative CT scans is used to determine their position. However, as the surgeon operates, the tumor may be deformed which presents a major hurdle for accurately resecting the tumor, and can lead to surgical inaccuracy, increased operation time, and excessive margins. This issue is particularly pronounced in robot-assisted partial nephrectomy (RAPN), where the kidney undergoes significant deformations during operation. Toward addressing this, we introduce a occupancy network-based method for the localization of tumors within kidney phantoms undergoing deformations at interactive speeds. We validate our method by introducing a 3D hydrogel kidney phantom embedded with exophytic and endophytic renal tumors. It closely mimics real tissue mechanics to simulate kidney deformation during in vivo surgery, providing excellent contrast and clear delineation of tumor margins to enable automatic threshold-based segmentation. Our findings indicate that the proposed method can localize tumors in moderately deforming kidneys with a margin of 6mm to 10mm, while providing essential volumetric 3D information at over 60Hz. This capability directly enables downstream tasks such as robotic resection.","Kidney cancer is one of the most common forms of cancer in the US, with over 65,000 new patients being diagnosed every year, leading to over 15,000 deaths [1]. The standard treatment for localized small renal masses has shifted from radical nephrectomy (complete kidney removal) toward the more minimally invasive approach of partial nephrectomy (removal of the tumor, retaining partial kidney function). One of the main challenges during tumor removal is ensuring the resection of adequate tumor margins. The margin needs to ensure that no cancer cells remain in the kidney, while also avoiding excessive removal of healthy tissue to preserve organ function. For operations involving tumor resection, surgeons commonly interpret 2D preoperative scans and mentally construct a 3D anatomical model during the surgery. This process of creating a mental model can extend the duration of the procedure, increasing the risk of damaging adjacent tissues [2]. The difficulty is exacerbated by tissue deformation during surgery, causing the internalized mental model to become inaccurate over time. Registration methods that overlay pre-operative data onto the intra-operative scene can provide surgeons with valuable location information for regions of interest. Altamar et al. [3] develop a deformable registration method based on biomechanical elastic models to simulate in-vivo deformation during partial nephrectomy. While their method accurately analyzes kidney deformation, it doesn‚Äôt address the tumor and margin deformation. Y1.0e0]ptKidney Phantom Sensor Point Cloud Occupancy Point Cloud Figure 1: Given a known kidney phantom with two tumors and a pre-operative CT scan, we estimate, from a single sensor point cloud derived from a depth image, a dense occupancy point cloud that encodes the locations of each tumor. Point-based registration enables the calculation of precise transformations between markers in pre-operative images and those in the physical space, as shown on intra-operative images. In the absence of distinct anatomical landmarks on organs like the kidney, applying fiducials to the renal surface is advocated for point-based registration [4, 5]. Nimmagadda et al. [6] develop a touch-based registration approach using ink fiducials tattooed on the kidney surface. They employ the Iterative Closest Point (ICP) algorithm for initial alignment with a pre-operative point cloud, followed by point-based re-registration to account for kidney deformation. While precise, tattooing intra-operative tissue has been associated with increased patient morbidity [2]. Zhang et al. [7] propose a different approach. They gather surface data by capturing multiple views of the kidney, stitching them together to construct a 3D surface. They then apply Coherent Point Drift (CPD) for deformable registration. This technique achieves markerless deformable registration but requires multiple views of the organ, wherein the organ‚Äôs deformation during the image collection process could introduce errors. In previous work [8], we demonstrate a method for reconstructing deformed objects composed of multiple parts from single-viewpoint point clouds using a neural multi-class occupancy function. This approach utilizes a combination of occupancy networks [9] and PointNet++ [10] to infer a 3D object that aligns with the observation of the deformable real-world object. It relies on deforming a pre-operative 3D model digitally to generate ground truth occupancy samples and a sensor-based depth image. Through supervised learning, the network is trained to label occupancy samples correctly, learning the adaptation of the labelling to observed deformations directly from the data. A distinct advantage of this method is its ability to estimate the position of deformable parts without necessitating initial registration. To validate the accuracy of deformable registration and localization methods, organ ‚Äùphantoms‚Äù are widely used, which aim to replicate the mechanical properties and medical imaging results of real organs. Birnbaum et al. [11] customized renal inserts with different Hounsfield Unit (HU) values in anthropomorphic phantoms to produce varying brightness between kidney components under CT imaging. However, the analysis of mechanical tissue properties was not undertaken. In our previous work, we develop a hydrogel kidney phantom capable of reproducing both the mechanical and functional properties of living tissue [12]. This phantom is specifically designed for Robot-Assisted Partial Nephrectomy (RAPN) training, with a focus on mechanical fidelity over CT imaging characteristics. A method that can register or reconstruct deformable objects from an observation, can be applied to perform resection tasks autonomously. We recently presented the Autonomous System for Tumor Resection (ASTR) [13] method. It is a vision-guided robotic system that demonstrates success in tongue tumor resection (i.e. glossectomy). In this work, we present an occupancy network-based tumor localization approach designed to use pre-operative CT images with intra-operative RGBD sensor data. Our method only requires a single depth image from a single viewpoint to estimate the deformation of the kidney and locate embedded tumors at over 60‚Å¢Hz60Hz60\text{Hz}60 Hz, see Figure 1. Therefore, it can track tumors, enabling the delineation of margins in the presence of deformations occurring during resection. Furthermore, we evaluate our method through the use of a novel 3D hydrogel kidney phantom, which is embedded with renal tumors. This phantom possesses realistic mechanical properties and allows easy automatic segmentation by providing varying brightnesses under CT imaging. The evaluation demonstrates the utility of our method in guiding a robotic resection, accounting for potential deformations during partial nephrectomy procedures."
https://arxiv.org/html/2411.02608v1,SSFold: Learning to Fold Arbitrary Crumpled Cloth Using Graph Dynamics from Human Demonstration,"Robotic cloth manipulation faces challenges due to the fabric‚Äôs complex dynamics and the high dimensionality of configuration spaces. Previous methods have largely focused on isolated smoothing or folding tasks and overly reliant on simulations, often failing to bridge the significant sim-to-real gap in deformable object manipulation. To overcome these challenges, we propose a two-stream architecture with sequential and spatial pathways, unifying smoothing and folding tasks into a single adaptable policy model that accommodates various cloth types and states. The sequential stream determines the pick and place positions for the cloth, while the spatial stream, using a connectivity dynamics model, constructs a visibility graph from partial point cloud data of the self-occluded cloth, allowing the robot to infer the cloth‚Äôs full configuration from incomplete observations. To bridge the sim-to-real gap, we utilize a hand tracking detection algorithm to gather and integrate human demonstration data into our novel end-to-end neural network, improving real-world adaptability. Our method, validated on a UR5 robot across four distinct cloth folding tasks with different goal shapes, consistently achieves folded states from arbitrary crumpled initial configurations, with success rates of 99%, 99%, 83%, and 67%. It outperforms existing state-of-the-art cloth manipulation techniques and demonstrates strong generalization to unseen cloth with diverse colors, shapes, and stiffness in real-world experiments.Videos and source code are available at: https://zcswdt.github.io/SSFold/","Cloth manipulation has a wide range of applications in both domestic and industrial settings, such as laundry unfolding[1] and folding[2], surgery[3], and manufacturing[4]. These applications enhance the quality of life by reducing human labor. However, it has posed a challenge for robotic manipulation: compared to rigid objects, cloth has infinite degrees of freedom, can be only partially observable due to self-occlusions in crumpled configurations, and does not transform rigidly when manipulated. The dynamics of cloth are also complex[5], and slightly different interactions may lead to significantly different cloth behaviors. Early approaches for cloth manipulation efforts relied heavily on scripted actions, which were generally slow and lacked the flexibility to adapt to varying cloth configurations. Recently, there have been two predominant learning-based approaches in the field of cloth manipulation. The first approach[6][7][8]utilizes simulated data for training, specifically designed to capture the complex dynamics and varied states of cloth. While this method avoids the high costs of physical data collection, it struggles with significant sim-to-real gaps because many cloth states and dynamics are challenging to replicate accurately in simulators[9]. The second approach[10][11]directly collects human demonstration data from real-world interactions with cloth, facing challenges such as the high costs of data collection equipment. Therefore, to overcome the significant sim-to-real[12] gaps and reduce the dependency on costly experimental setups for data collection in the real world, a new approach is essential that leverages more accessible and scalable methods of acquiring human demonstration data for robotic cloth manipulation. Figure 1: Fold Cloth from Human Demonstrations. The proposed method comprises two stages: offline learning from demonstrations and online task execution. In the offline stage, human demonstration data is captured using hand-tracking techniques, and a neural network is trained and optimized iteratively. During the online stage, the neural network predicts poses from input images, which are then executed by the robot to perform similar actions. In this paper, we introduce SSFold, an end-to-end policy model that utilizes human demonstration data to fold cloth from any initial crumpled configuration into the desired shape (see Fig.1). Our method leverages a YOLOv10-based[13] system for hand tracking and key-point detection with a low-cost monocular camera, achieving accurate 3D estimation without the need for complex setups involving markers, hand-held devices, or multi-camera motion capture systems. To address the challenges of deformable dynamics, we establish a visible connectivity graph that effectively captures the spatial structure of the cloth, overcoming issues of partial observability and self-occlusions. Additionally, inspired by the pick-conditioned placing strategies of Wu et al.[14] and Thomas et al.[15], we incorporate this approach into our unified framework, extending its application beyond the original scope to effectively address both smoothing and folding tasks. Ultimately, the captured cloth states are input into SSFold, which then generates a distribution of action possibilities for picking and placing, pinpointing the optimal positions for these tasks based on the dynamics and configuration of the cloth. We validate our approach against three state-of-the-art baselines[10][11][15], demonstrating significant performance improvements. Furthermore, extensive ablation experiments show that our method not only excels in real-world settings but also generalizes effortlessly to various cloth shapes and colors without additional training. The contributions of this work are summarized as follows: We propose a YOLOv10-based method for hand tracking and key point detection to collect human demonstration data. Over 21 hours, we collected a dataset of 500 human demonstration videos, effectively bridging human-to-robot transfer by capturing intricate human manipulations for robotic training. We propose a novel end-to-end two-stream architecture with sequential and spatial pathways, unifying smoothing and folding tasks into a single adaptable policy model. Our method has been thoroughly evaluated using real robotic arms on a diverse range of cloth. Real-world experiments demonstrate that our approach outperforms state-of-the-art methods, with results showing it performs effectively in standardized settings and generalizes robustly to tasks unseen during the training phase. The remainder of this paper is structured as follows. In Section II, we review the related work that forms the foundation of our study. Section III delves into the design of the proposed learning framework, providing a comprehensive explanation of its architecture and components. The experimental setup, along with a detailed analysis of the results, is presented in Section IV. Finally, Section V concludes the paper with a summary of our findings and discusses potential directions for future research."
https://arxiv.org/html/2411.02547v1,Modeling Uncertainty in 3D Gaussian Splatting throughContinuous Semantic Splatting,"In this paper, we present a novel algorithm for probabilistically updating and rasterizing semantic maps within 3D Gaussian Splatting (3D-GS). Although previous methods have introduced algorithms which learn to rasterize features in 3D-GS for enhanced scene understanding, 3D-GS can fail without warning which presents a challenge for safety-critical robotic applications. To address this gap, we propose a method which advances the literature of continuous semantic mapping from voxels to ellipsoids, combining the precise structure of 3D-GS with the ability to quantify uncertainty of probabilistic robotic maps. Given a set of images, our algorithm performs a probabilistic semantic update directly on the 3D ellipsoids to obtain an expectation and variance through the use of conjugate priors. We also propose a probabilistic rasterization which returns per-pixel segmentation predictions with quantifiable uncertainty. We compare our method with similar probabilistic voxel-based methods to verify our extension to 3D ellipsoids, and perform ablation studies on uncertainty quantification and temporal smoothing.","In order to plan, robots require a world model which captures geometric detail and higher levels of information about their environment. Although some papers propose mapless navigation [1, 2, 3], maps are still widely used due to an interpretable world model which temporally adapts as robots explore their surroundings. Depending on the robot application, maps can store different types of information to increase scene understanding. For many robotic applications, uncertainty of the map is necessary to ensure safe planning in safety-critical environments. In these situations, robots must understand not only the type and location of objects, but confidence in the predictions as well. Uncertainty can arise from noisy perception networks, sensor noise, and sparse views which can ultimately result in incomplete maps. (a) Well fitted 3D-GS render. (b) Poorly fitted 3D-GS render. (c) Semantic prediction on poorly fitted render. (d) Semantic uncertainty on poorly fitted render. Figure 1: While 3D-GS may provide high quality renderings of the environment at novel views with sufficient training data, it may fail to render views which are occluded, unseen, or at different angles from the training data. In the above image, CSS produces semantic (c) and RGB (b) predictions at a novel view without sufficient training data, resulting in a blurry render and incorrect segmentation. Through probabilistic inference, CSS identifies blurs and gaps in the render which correlate with reconstruction quality (d). Continuous mapping combats sparse data by leveraging spatial relations of points to fill in gaps in the map from sparse data probabilistically and with quantifiable uncertainty [4, 5]. Continuous mapping has been successfully applied to applications such as elevation mapping [6] and semantic mapping, by incorporating measurements into nearby cells in the robotic map through a kernel [7]. The kernel effectively defines the influence of input points over nearby cells probabilistically, leading to a closed form update solution through Bayesian Kernel Inference (BKI). However, one challenge of BKI is defining the kernel function, which is generally hand-crafted and recently was shown to be learnable, resulting in 3D ellipsoid shapes [8]. Additionally, BKI has been limited to grid-based solutions which are prone to discretization errors and require accurate depth estimation. Separately, 3D Gaussian Splatting (3D-GS) proposes a new method for novel view synthesis, which learns to model the world explicitly as 3D ellipsoids, with high quality renderings from any angle without the discretization error of grid-based map representations [9]. 3D-GS has captured the attention of the robotics community, with many methods proposing to add additional features to 3D-GS [10, 11] and incorporate 3D-GS into simultaneous localization and mapping (SLAM) pipelines [12, 13]. Some works have recently explored quantifying information gain [14] or optimal ellipsoid pruning [15] in 3D-GS through Fisher Information, however quantifying uncertainty from noisy segmentation networks or novel views remains a challenge. In this work, we leverage the insight that 3D-GS learns valid kernels to propose a novel method for uncertainty quantification in 3D-GS. Our method, which we call Continuous Semantic Splatting (CSS), incorporates semantically labelled images in a Bayesian framework to capture the semantic uncertainty of each 3D ellipsoid. Additionally, through a novel rasterization method, we capture the semantic variance from noisy segmentation predictions in pixel space, as well as information on conflicting categories caused by poor renderings at novel views. To summarize, our contributions are: i. Extend continuous mapping literature from voxel grids to 3D-GS world representation. ii. Formulate novel 3D-GS semantic update with quantifiable semantic variance of ellipsoids. iii. Probabilistic semantic 3D-GS rasterization with quantifiable uncertainty."
https://arxiv.org/html/2411.02524v1,3DM-Explore: 3D Mapping for Multi-Robot Exploration for Unknown Indoor Environments,"Multi-Robot Exploration (MRE) is pivotal in advancing robotic autonomy, particularly for applications demanding sustained situational awareness such as , search and rescue, and intelligent transportation systems. Recent developments have focused on enhancing the efficiency and resilience of multi-robot SLAM algorithms, incorporating diverse sensor modalities and robust communication frameworks. While significant progress has been made with lidar-based collaborative SLAMs and large-scale visual SLAM systems, the research on visual SLAM in indoor environments has been comparatively limited. This gap is critical, as the dynamic interactions among robots in such settings present unique challenges. State-of-the-art approaches like Kimera-Multi and CORB-SLAM have demonstrated effectiveness in outdoor environments but struggle indoors, often leading to issues such as the ‚Äùghosting trail effect‚Äù that compromise map quality. Our research proposes a fully distributed approach to multi-robot exploration in indoor settings, addressing the limitations of existing methods. By focusing on generating dense metric 3D mesh models and accounting for the complex geometric structures of indoor environments, we aim to improve mapping efficiency while adhering to strict communication bandwidth constraints, thereby advancing the field of collaborative visual SLAM in challenging indoor scenarios.","Multi-Robot Exploration (MRE) plays a crucial role in robotics research because it enhances situational awareness over extended periods. This capability is essential for a wide range of applications, including environmental monitoring and patrolling [espina2011multi], search and rescue operations [10611179], intelligent transportation systems [li2018corb]. The objective of MRE is to synergistically enhance the autonomous navigation and mapping capabilities of a coordinated robotic system, optimizing spatial comprehension, cost-effectiveness, travel time, and energy utilization. Recently, there have been substantial advancements in developing more efficient and resilient MRE algorithms and systems, incorporating various objectives, sensor modalities and communication frameworks. Frontier-Based exploration methods have garnered substantial attention for their capacity to accelerate exploration processes. However, the objectives of recent works are primarily focused on creating efficient 2D maps, optimizing cost, and minimizing travel time. Their suitability for 3D exploration remains inadequate for achieving notable performance enhancements. For instance, most of the existing literature relies on identifying frontiers using computer vision based approaches on grid maps. These approaches, while effective in 2D contexts, does not translate well to constructing efficient 3D maps, highlighting a critical gap in current methodologies. How we calculate the frontiers. In comparison to the experiments of lidar-based collaborative exploration and large-scale visual exploration frameworks, the evaluative research on visual MRE within indoor environments remains proportionally lacking. This shortfall is particularly significant given that the intrinsic dynamism of robot interactions within these settings presents unique challenges that must be addressed. The existing visual MRE state-of-the-art appraoches such as Kimera-Multi [tian2022kimera], CORB-SLAM [li2018corb] proven thier efficiency towards outdoor environments, where inter-robot visibility to each other is neglected. However, these methodologies often falter in indoor scenarios, leading to a ‚Äùghosting trail effect‚Äù that significantly degrades the quality of the generated maps. Distinct from previous studies on vision-based MRE systems, our research emphasizes a fully distributed approach to multi-robot exploration. We enhance the efficiency of generating dense metric 3D mesh models by incorporating ‚ÄùMutual-Awareness‚Äù within environments in which the robots operate, all while operating under uncertain constraints on communication bandwidth. Overall, this paper proposes 3DM-Explore integrating exploration algo to enhance the efficacy of 3D exploration and localization within indoor scenarios. data efficiency and communication aspects. Contributions: Novelty:"
https://arxiv.org/html/2411.02482v1,NeRF-Aug: Data Augmentation for Roboticswith Neural Radiance Fields,"Training a policy that can generalize to unknown objects is a long standing challenge within the field of robotics. The performance of a policy often drops significantly in situations where an object in the scene was not seen during training. To solve this problem, we present NeRF-Aug, a novel method that is capable of teaching a policy to interact with objects that are not present in the dataset. This approach differs from existing approaches by leveraging the speed and photorealism of a neural radiance field for augmentation. NeRF-Aug both creates more photorealistic data and runs 3.83 times faster than existing methods. We demonstrate the effectiveness of our method on 4 tasks with 11 novel objects that have no expert demonstration data. We achieve an average 69.1% success rate increase over existing methods. See video results at https://nerf-aug.github.io.","Humans have an innate ability to interact with objects they have never encountered before. For instance, a person can intuitively approach an unknown object, pick it up, and interact with it. This is in stark contrast to existing robotic systems. For a robot, even the slightest differences in shape or color from the objects seen during training can prevent the robot from achieving success. This challenge of generalization to out-of-distribution samples is a fundamental issue in machine learning and robotics. Many prior works have explored methods to develop policies for robots that generalize to different objects. A straightforward approach is to simply collect demonstrations involving the novel object. However, this method has significant drawbacks because creating expert demonstrations is time-consuming and expensive as it requires a human to consciously control the robot‚Äôs movements. Collecting such human demonstrations is unfeasible at scale as every new object potentially requires many new demonstrations. Another approach is to use image editing tools, e.g., the latest diffusion-based image editing [1, 2, 3, 4, 5]. While these models can effectively edit images to insert new objects, they are often slow and struggle to render the exact object that will be encountered by the robot. This inaccuracy means the current object remains out of the domain of the training set which often causes these models to fail. Alternatively, some pipelines use depth images for object manipulation [6, 7]. Unfortunately, depth images in the real world suffer from noise and incompleteness [7]. This issue is exacerbated when using mounted gripper cameras, which amplify noise as they get closer to the object. Moreover, even though depth images disregard texture and color, small geometric differences between the original (training) and novel objects can still result in confusion and failure to complete the intended task. Figure 1: When a human provides expert demonstrations for training a behavior cloning model, the model is effective for the object in the demonstration, but will fail for novel objects. We propose NeRF-Aug, where we automatically learn NeRFs for the novel objects and inpaint them in the expert data. With this photorealistic synthetic data, the robot can learn to interact successfully with novel objects. In this work, we propose NeRF-Aug, a lightweight framework that streamlines and automates data collection for a wide range of novel objects. Our goal is to generate data samples for different tasks using novel objects without collecting more human demonstrations. To enable this, we follow the image editing paradigm, but instead of relying on slow generation frameworks, we propose grounding the editing process in the scene using the 3D model of a novel object with a Neural Radiance Field (NeRF) [8] representation. We augment the training data for the robot‚Äôs policy using this edited scene. Our framework uses existing demonstrations of a different object and generates NeRF-Augmented (NeRF-Aug) synthetic data (Fig. 2) that can be used in imitation learning policies. Figure 2: An illustration of our pipeline from beginning to end. We first train an object-level NeRF of a novel object (left). We then simultaneously erase the object in question with an off-the-shelf inpainter (top) and leverage NeRF to render images of a new object in the same position as the original object (bottom). We use the final synthetic dataset to train a new policy for the robot (right). We demonstrate that the synthetic data generated by the NeRF-Aug framework is almost indistinguishable from real-world data. Moreover, we show that compared to existing diffusion-based image editing techniques, our method runs significantly faster, creates photorealistic images, and can consistently render objects at a wider degree of viewpoints. We test our method on a variety of real world tasks, and achieve a 69.1% increase in success rate while rendering synthetic data 3.833.833.833.83 times faster than the baselines. To summarize, our contributions are as follows: ‚Ä¢ We propose a fast and photorealistic image editing framework to generate synthetic data that can be used in robot policy learning to generalize to novel objects. ‚Ä¢ We learn a NeRF of a novel object by using multi-view images of the object captured using a robot arm. ‚Ä¢ We edit videos of existing demonstrations by removing the training object via in-painting and blending the NeRF render of a novel object into the inpainted image to generate a synthetic dataset for training the robot policy. ‚Ä¢ We demonstrate effective generalization on four diverse tasks using the generated synthetic data for training."
https://arxiv.org/html/2411.02410v1,Web-based Augmented Reality with Auto-Scaling and Real-Time Head Tracking towards Markerless Neurointerventional Preoperative Planning and Training of Head-mounted Robotic Needle Insertion,"Neurosurgery requires exceptional precision and comprehensive preoperative planning to ensure optimal patient outcomes. Despite technological advancements, there remains a need for intuitive, accessible tools to enhance surgical preparation and medical education in this field. Traditional methods often lack the immersive experience necessary for surgeons to visualize complex procedures and critical neurovascular structures, while existing advanced solutions may be cost-prohibitive or require specialized hardware. This research presents a novel markerless web-based augmented reality (AR) application designed to address these challenges in neurointerventional preoperative planning and education. Utilizing MediaPipe for precise facial localization and segmentation, and React Three Fiber for immersive 3D visualization, the application offers an intuitive platform for complex preoperative procedures. A virtual 2-RPS parallel positioner or ‚ÄúSkull-Bot‚Äù model is projected onto the user‚Äôs face in real-time, simulating surgical tool control with high precision. Key features include the ability to import and auto-scale head anatomy to the user‚Äôs dimensions and real-time auto-tracking of head movements once aligned. The web-based nature enables simultaneous access by multiple users, facilitating collaboration during surgeries and allowing medical students to observe live procedures. A pilot study involving three participants evaluated the application‚Äôs auto-scaling and auto-tracking capabilities through various head rotation exercises. This research contributes to the field by offering a cost-effective, accessible, and collaborative tool for improving neurosurgical planning and education, potentially leading to better surgical outcomes and more comprehensive training for medical professionals. The source code of our application is publicly available at https://github.com/Hillllllllton/skullbot_web_ar.","I INTRODUCTION Minimally invasive neurosurgery (MIN) procedures encompass sophisticated techniques that entail precise manipulation of surgical tools within the delicate brain tissue through minimal cranial incisions [1]. Any slight misplacement of these tools during the procedure can lead to severe bleeding and significant neurological complications, endangering the patient‚Äôs health [2]. Consequently, achieving meticulous localization of neural structures is crucial, typically based on computed tomography (CT) or magnetic resonance imaging (MRI) for guidance to specific target sites [1]. However, the direct correlation between CT/MRI imaging and the unique anatomy of the patient‚Äôs brain can be non-intuitive [3], necessitating the implementation of a sophisticated support system [4] [5] for precise localization and registration during surgery. Currently, most preoperative planning and medical education still rely on static 2D images and textual descriptions, lacking the sense of immersion and interactivity [5, 6]. By providing intuitive visualization, Augmented Reality (AR) can highlight critical anatomical structures or pathologies that are hidden or challenging to find and finally guide doctors to perform surgeries more accurately [7]. AR, which can directly overlay the brain anatomy over the patient‚Äôs head, holds the potential to revolutionize the pre-surgical planning and educational frameworks within neurosurgery [8, 9]. While traditional methods have limitations in terms of time and location, AR technology in healthcare can improve safety, efficiency, doctor-patient relationships, and the invention of new surgical methods, benefiting patient outcomes and fostering innovation [10]. Therefore, the AR visualization can play a significant role in the preoperative planning of the neurosurgery. There exist two primary types of AR applications: one relies on head-mounted devices (HMD) for observation, while the other permits direct viewing on mobile apps or websites. While hardware-based AR implementation is recognized for its costliness and lack of flexibility, web-based AR negates additional hardware requirements, facilitating effortless access to AR content on personal smartphones or tablets [11]. Moreover, when it comes to surgery, multiple users can access AR content through a single web URL, creating a collaborative platform that enhances the cooperation among surgeons [12]. Furthermore, certain medical students can actively engage in surgeries without disrupting the operating surgeon [13], thereby significantly improving medical learning opportunities. Therefore, we focus on developing and deploying a web AR application for preoperative planning and education of neurosurgery in this paper. Figure 1: Layout of the web-based AR application. The interface includes three distinct regions, each serving a specific purpose. The upper region is the function block with several buttons, which controls the import/hide of the 3D model and enables/disables auto-scaling. The middle region is a window for displaying the AR content. The right region is a panel for parameter settings, which can be used to manually modify the imported model. The main objective of our AR application is to accurately superimpose virtual models onto users‚Äô heads. Leveraging Google‚Äôs MediaPipe [10], a robust and widely-used framework for face landmark detection and segmentation, developers can efficiently manage resources while maintaining high-quality results [14]. Renowned for its precision and utility in medical applications, MediaPipe has been extensively validated through research, demonstrating 95% consistency limits and comparable accuracy to traditional measurement tools such as the universal goniometer and digital angle ruler [15]. Integrating artificial intelligence (AI) in AR has further streamlined automated tracking, significantly reducing preparation time for surgical procedures [16]. In minimally invasive neurosurgery, two critical factors are essential: precise visualization of target brain structures and accurate control over surgical instruments. Integrating robotics in surgery has significantly enhanced operating efficiency and safety by reducing operating times, minimizing blood loss, and accelerating patient recovery [17] [18]. The 2-RPS parallel positioner (Skull-Bot) is a notable head-mounted robot for needle insertion, achieving remarkable open-loop positioning accuracy within ¬±1 mm in neurosurgical procedures [15] [19]. By combining the Skull-Bot with web-based augmented reality visualization, we aim to further refine the preoperative process with improved intuitiveness. As shown in Fig. 1, this paper introduces a novel marker-free web-based AR application that can assist neurosurgical preoperative planning and training such as head-mounted robotic needle insertion procedures. By leveraging facial feature detection and immersive 3D visualization, our system enables users to import and overlay 3D head anatomy structures in real-time, simulating precise surgical tool placement. This innovative approach aims to streamline pre-surgical training and planning, ultimately reducing the time required for surgeons and medical students to become proficient with specific tools. The contributions of this research are threefold: ‚Ä¢ Development of a markerless web-based AR platform: Our system integrates facial feature detection and achieves immersive 3D visualization, providing an intuitive interface for neurosurgical preoperative planning and education. ‚Ä¢ Auto-scaling and tracking capabilities: The application features auto-rescaling to accommodate different head dimensions and enables fast and precise auto-tracking of head movements, enhancing the overall user experience. ‚Ä¢ Evaluative validation: A qualitative and quantitative assessment involving three users demonstrates the high performance and quality of our system with over 80% overlap IoU, showcasing its potential to improve neurosurgery procedures."
https://arxiv.org/html/2411.03213v1,What Makes an Educational Robot Game Fun? Framework Analysis of Children‚Äôs Design Ideas,"Fun acts as a catalyst for learning by enhancing motivation, active engagement and knowledge retention. As social robots gain traction as educational tools, understanding how their unique affordances can be leveraged to cultivate fun becomes crucial. This research investigates the concept of fun in educational games involving social robots to support the design of REMind: 111REMind is short for Robots Empowering Minds a robot-mediated role-play game aimed at encouraging bystander intervention against peer bullying among children. To incorporate fun elements into design of REMind, we conducted a user-centered Research through Design (RtD) study with focus groups of children to gain a deeper understanding of their perceptions of fun. We analyzed children‚Äôs ideas by using Framework Analysis and leveraging LeBlanc‚Äôs Taxonomy of Game Pleasures and identified 28 elements of fun that can be incorporated into robot-mediated games. We present our observations, discuss their impact on REMind‚Äôs design, and offer recommendations for designing fun educational games using social robots.","Social robots are increasingly popular in education, offering unique opportunities to enhance learning. Their physical embodiment and expressive capabilities make them highly engaging, allowing for creating interactive and emotionally resonant experiences. By integrating social robots with educational games, we can create playful scenarios that offer personalized and situated experiences, increasing learner engagement and potentially enhancing learning outcomes [32]. Games, as structured play, are excellent facilitators of learning because they induce Flow state [31, 5]. Flow states, characterized by deep involvement with an activity and a distorted sense of time, are important in educational settings as they reduce self-consciousness and enhance learning [30]. Furthermore, games create a safe space for learning by eliminating real-life costs of failure, encouraging persistence and self-regulated learning through multiple attempts and immediate feedback [38]. Additionally, games engage learners as active producers of knowledge, unlike the passive consumption often found in traditional education [13]. Thus, we argue that integrating Game-Based Learning (GBL) into social robot educational experiences could increase their effectiveness. Broadly defined, GBL involves the use of games and game-like elements to support the acquisition of knowledge, skills, and attitudes. It differs from gamification, which typically involves adding incentives like stars, points, or rankings to motivate learners to engage with otherwise tedious tasks [31]. Superficially incorporating fun elements into educational components (e.g., by merely transforming a quiz into a digital format with a point system) often falls flat and leads to disengagement if the core task is not fun. Game scholars use the metaphor of ‚Äòchocolate-covered broccoli‚Äô to illustrate this point: adding fun to a game as an afterthought is like coating vegetables in chocolate; it does not make them candy [42]. In contrast, GBL involves redesigning learning tasks to be inherently interesting and meaningful, and aiming to create experiences that are both effective educational tools and fun games [31, 23]. While educational video games are a modern example, GBL has a long history. Developmental psychology has long recognized play as a natural form of learning, with significant research on playful learning predating the digital era [30]. A key feature of play is that it is intrinsically motivating‚Äì we play because it is fun. Games are engaging and enjoyable by design, which can increase learners‚Äô motivation and interest in the learning content, leading to higher levels of effort, persistence, and achievement [24, 28]. Although games that have a purpose beyond entertainment are commonly labeled ‚ÄòSerious Games,‚Äô game design scholars argue that this label is misleading, as it implies that such games need not prioritize fun; instead they advocate for the term ‚ÄòTransformational Games‚Äô [38]. Regardless of terminology, to be effective, such games must indeed be fun [31]. Hence, it is crucial to explore how we might objectively integrate fun into the designs of educational experiences. Game designers highlight several key factors: goals, feedback, interactivity, storytelling, aesthetics, challenges, and social interaction. While the fundamentals of play remain consistent, we argue that novel technologies like social robots introduce new opportunities for play through their unique affordances. This raises the question: how can social robots make transformational games more fun? We explored this question in the context of developing REMind, an educational game using social robots to teach anti-bullying intervention skills to children. The subjective and context-dependant nature of fun poses a design challenge. Recognizing users as ‚Äúexperts of their own experiences‚Äù [33], we took a user-centered Research through Design (RtD) approach. We engaged children, the game‚Äôs primary stakeholders, in focus groups to brainstorm ways to make REMind fun. We analyzed the qualitative data gathered from this study by applying Framework Analysis [14] and using LeBlanc‚Äôs Taxonomy of Game Pleasures [15], which identifies 8 categories of enjoyment in games. This method helped us identify 28 elements of fun in robot games, which informed our game design. In this paper, we present our findings, discuss how they shaped the design of REMind, and offer design recommendations based on these insights."
https://arxiv.org/html/2411.03189v1,Energy-Aware Predictive Motion Planning for Autonomous Vehicles Using a Hybrid Zonotope Constraint Representation,"Uncrewed aerial systems have tightly coupled energy and motion dynamics which must be accounted for by onboard planning algorithms. This work proposes a strategy for coupled motion and energy planning using model predictive control (MPC). A reduced-order linear time-invariant model of coupled energy and motion dynamics is presented. Constrained zonotopes are used to represent state and input constraints, and hybrid zonotopes are used to represent non-convex constraints tied to a map of the environment. The structures of these constraint representations are exploited within a mixed-integer quadratic program solver tailored to MPC motion planning problems. Results apply the proposed methodology to coupled motion and energy utilization planning problems for 1) a hybrid-electric vehicle that must restrict engine usage when flying over regions with noise restrictions, and 2) an electric package delivery drone that must track waysets with both position and battery state of charge requirements. By leveraging the structure-exploiting solver, the proposed mixed-integer MPC formulations can be implemented in real time.","There has been a rising interest in the potential of autonomous electric and hybrid-electric uncrewed aerial systems (UAS) in the aviation industry. Applications include aerial package delivery vehicles [1] and air taxis or ambulances for urban air mobility (UAM)[2, 3]. Energy usage and environmental constraints, such as restrictions on aircraft noise, present significant technological challenges for these systems [3]. To address these challenges, autonomous planning algorithms must be able to account for energy utilization in addition to vehicle motion. I-A Gaps in the Literature Existing work on energy-aware planning has focused on incorporating energy considerations into high-level path planning algorithms, often using graph-based approaches such as A* or Dijkstra‚Äôs algorithm [4]. Graph search algorithms are used to perform high-level, energy-aware planning for hybrid-electric UAS under energy and noise constraints in [5, 6, 7], and they are applied to energy-constrained planning for package delivery drones in [8, 9]. However, there are several key challenges when attempting to integrate high-level planners with lower-level path followers, such as discrepancies in model assumptions between the path planner and follower [10]. Intermediate-level motion planning algorithms are often used to bridge the gap between high-level planners and low-level controllers. The role of such algorithms is to locally plan system trajectories, typically using a reduced-order model of the system [11]. Introducing energy considerations into these intermediate-level planners has received comparatively little attention in the literature despite the extensive literature on UAS motion planning when energy system dynamics are not considered [12]. Energy and motion dynamics have been incorporated into Model Predictive Control (MPC) formulations, which are often used for motion planning [13]. Energy dynamics were included in a hierarchical MPC controller with one spatial dimension and convex state constraints in [14], and a terminal battery state of charge constraint was used within an MPC path planner/follower in [15]. In [16], MPC is used for UAS motion planning and control. Here, battery state of charge is maximized while adhering to obstacle avoidance constraints that are imposed using potential functions, and the system dynamics are linearized about an equilibrium condition. The resulting energy-aware motion plans may be vulnerable to entrapment in suboptimal local minima given the nonlinear programming formulation and local linearization. Increasingly, MPC optimization problems for motion planning are formulated as mixed-integer programs (MIPs) because non-convex constraint sets (e.g., an obstacle map) can be exactly represented in MIPs and‚Äîfor mixed-integer convex programs‚Äîconvergence to a global optimum is guaranteed [17]. MIPs are NP-hard [18] however, which inhibits the application of these methods in a real-time context. I-B Contributions This paper presents an efficient method for energy-aware motion planning of uncrewed aerial systems. A mixed-integer set representation, the hybrid zonotope, is used to exactly represent a non-convex constraint set that defines obstacles and regions with location-specific noise restrictions. A reduced-order, linear time-invariant model of the coupled UAS energy and motion dynamics is developed that is globally valid and conservative with respect to planned energy usage. Motion and energy states are coupled via a polytopic constraint set, and a low-complexity constrained zonotope representation of this set is presented. An MPC controller is formulated that plans system trajectories which adhere to specifications on both the motion and energy states. A mixed-integer quadratic program (MIQP) solver developed in our previous work [19, 20]111These references will be published or uploaded to arXiv before the final ACC deadline. A link to the manuscripts has been provided in the references for review purposes. (previously applied for motion planning only and not energy management) is leveraged to efficiently solve these MPC optimization problems by exploiting the structure of the hybrid zonotope and constrained zonotope set representations. Case studies show how noise-restricted areas and terminal energy constraints can be considered by the proposed controller and highlight the utility of jointly optimizing energy and motion plans."
https://arxiv.org/html/2411.03048v1,UNet: A Generic and Reliable Multi-UAV Communication and Networking Architecture for Heterogeneous Applications,"The rapid growth of UAV applications necessitates a robust communication and networking architecture capable of addressing the diverse requirements of various applications concurrently, rather than relying on application-specific solutions. This paper proposes a generic and reliable multi-UAV communication and networking architecture designed to support the varying demands of heterogeneous applications, including short-range and long-range communication, star and mesh topologies, different data rates, and multiple wireless standards. Our architecture accommodates both adhoc and infrastructure networks, ensuring seamless connectivity throughout the network. Additionally, we present the design of a multi-protocol UAV gateway that enables interoperability among various communication protocols. Furthermore, we introduce a data processing and service layer framework with a graphical user interface of a ground control station that facilitates remote control and monitoring from any location at any time. We practically implemented the proposed architecture and evaluated its performance using different metrics, demonstrating its effectiveness.","Unmanned Aerial Vehicle Networks (UAVNs) consist of unmanned aerial vehicles (UAVs) equipped with actuators, sensors, autopilot, and wireless communication, enabling them to share data with each other and a ground control station (GCS) [1]. UAVs can autonomously perform tasks without human intervention, offering flexibility, low operating costs, and easy deployment. These advantages make UAVs suitable for diverse applications in healthcare, agriculture, military, and more, including courier services, border security, disaster management, precision agriculture, and aerial photography [2]. However, reliable communication between UAVs and the GCS is essential for monitoring, controlling, and executing tasks successfully in real-time. The demands of these UAV applications vary in terms of communication distance, end-to-end delay, topology (e.g., point-to-point (P2P), point-to-multipoint (P2M), mesh), network size, and bandwidth. Some applications require a single UAV, while others may need multiple. Additionally, some cases need only remote monitoring with pre-instructed paths, while others demand real-time monitoring and control. These diverse requirements indicate the importance of designing a robust UAV communication and networking architecture for heterogeneous applications. This design raises key questions: how to support various networking demands, wireless protocols, real-time data service to users, and multiple applications concurrently. To address the aforementioned problem, existing works [3, 4, 5, 6, 7, 8] proposed UAV communication and networking architectures based on either single UAV or multi-UAV setups for specific applications. Venkatesh et al. [3] proposed a fully autonomous UAV communication architecture using the TS832 serial wireless module, focusing on video transmission over P2P communication. However, this architecture is only suitable for single UAV applications. To address multi-UAV needs, Singh et al. [5] and Hong et al. [8] proposed multi-UAV communication architectures. Singh et al. [5] suggested a relay network of UAVs for video transmission to GCS using the Edimax WAP module, but it supports only short-range communication and lacks dynamic routing and mesh topology. On the other hand, Hong et al. [8] presented an LTE-based UAV communication architecture with P2P and P2M communication, but it does not support mesh topology and always depends on infrastructure networking. To overcome these issues, Krichen et al. [6] and Jawhar et al. [7] introduced an abstract concept of multi-UAV communication architecture using different wireless standards, but without the in-depth analysis needed to address the challenges in designing a generic UAV communication and networking architecture. Unfortunately, they did not provide any solutions for overcoming these design challenges. Existing works mostly focused on specific applications using a single wireless protocol and topology, limiting their applicability to homogeneous applications. These architectures are often incompatible with both infrastructure and infrastructure-less (adhoc) networks. Cellular networks (e.g., 3G, 4G, 5G) offer reliable connectivity in urban areas but face coverage issues in rural, forest, and border regions and lack support for mesh topology, which is crucial for UAV swarms. In contrast, infrastructure-less networks support long-range, mesh, and star topologies with moderate data rates, allowing network expansion based on application needs. However, these networks are confined to specific regions where they are established. Therefore, there is a need for a generic UAV communication architecture that provides seamless connectivity between UAVs and GCS while supporting heterogeneous applications concurrently. The trend of using IP-based wireless solutions allows UAVs to connect to the Internet for remote monitoring and control, making UAV data accessible from anywhere. However, existing architectures [9, 5, 3, 10, 11] limit GCS accessibility to local areas, restricting global data service. Some works [7, 8, 12] have proposed multi-UAV communication architectures with global GCS accessibility, but they lack support for concurrent applications and heterogeneous wireless protocols. Thus, there is a need for a generic, reliable multi-UAV communication architecture that supports heterogeneous applications and wireless standards while ensuring global GCS accessibility. In brief, the contributions in this manuscript are as follows. ‚Ä¢ We propose a generic UAV communication and networking architecture UNet that supports heterogeneous applications concurrently. UNet is designed to meet the various demands of applications, including short-range and long-range communications, star and mesh topologies, various data rates, and heterogeneous wireless standards. ‚Ä¢ To ensure seamless connectivity between UAVs and GCS, we combine support for both adhoc and infrastructure networks. ‚Ä¢ Additionally, we propose the design of a multi-protocol UAV gateway to enable interoperability among UAVs utilizing different communication protocols. ‚Ä¢ We propose a data processing and service framework integrated with the GCS to enable remote control and monitoring from anywhere, at any time. ‚Ä¢ The proposed architecture is implemented and thoroughly evaluated in outdoor settings using metrics such as handover delay, data processing delay, data delivery delay, throughput, packet loss probability, task execution time, exchanged traffic over time, and reliability. The remainder of this paper is organized as follows: Section II reviews the state-of-the-art UAV communication and networking architectures, highlighting their limitations. Section III describes the problem scenario and introduces the novel UNet architecture. Section IV presents the design and operational flow of UNet‚Äôs components. Section V discusses the implementation of UNet. Section VI presents the experimental setup and evaluates the system‚Äôs performance. Finally, Section VII concludes the paper and suggests future research directions. TABLE I: Comparison of UNet with State-of-art existing works Author Muti-UAVs Adhoc Mesh U2I Range Scalability Generic Arch Imple GCS Remarks UNet Yes Yes Yes both short and long Yes Yes Yes Globally Prototyping Li et al.[9] Yes Yes No ‚Äì Yes No No Locally Abstract idea Singh et al.[5] Yes No full mesh No Short No No Lab scale Locally Venkateshet et al.[3] No No No Short No No Yes Locally Hayat et al.[10] Yes Yes No Short No No Yes Locally Krichen et al.[6] Yes Yes Yes ‚Äì ‚Äì ‚Äì No ‚Äì Abstract idea Jawhar et al.[7] Yes Yes Yes Long Yes No Yes Globally Just idea Chriki et al.[11] Yes No No ‚Äì No No No Locally Hong et al.[8] Yes No Yes Long Yes No No Globally Always Infrastructure -dependent Wang et al.[12] Yes ‚Äì Yes Long Yes No No Globally"
https://arxiv.org/html/2411.02983v1,Autonomous Decision Making for UAVCooperative Pursuit-Evasion Gamewith Reinforcement Learning,"The application of intelligent decision-making in unmanned aerial vehicle (UAV) is increasing, and with the development of UAV 1v1 pursuit-evasion game, multi-UAV cooperative game has emerged as a new challenge. This paper proposes a deep reinforcement learning-based model for decision-making in multi-role UAV cooperative pursuit-evasion game, to address the challenge of enabling UAV to autonomously make decisions in complex game environments. In order to enhance the training efficiency of the reinforcement learning algorithm in UAV pursuit-evasion game environment that has high-dimensional state-action space, this paper proposes multi-environment asynchronous double deep Q-network with priority experience replay algorithm to effectively train the UAV‚Äôs game policy. Furthermore, aiming to improve cooperation ability and task completion efficiency, as well as minimize the cost of UAVs in the pursuit-evasion game, this paper focuses on the allocation of roles and targets within multi-UAV environment. The cooperative game decision model with varying numbers of UAVs are obtained by assigning diverse tasks and roles to the UAVs in different scenarios. The simulation results demonstrate that the proposed method enables autonomous decision-making of the UAVs in pursuit-evasion game scenarios and exhibits significant capabilities in cooperation.","The capability of unmanned aerial vehicle (UAV) continues to enhance through the utilization of advanced flight control, payload, power, and other technologies. This progress serves as a new driving force for its technological development and facilitates the rapid generation of UAV‚Äôs game capability in highly challenging environments. The development of UAV equipment technology exhibits characteristics such as networking, decentralization, cost-effectiveness, and intelligence. With advancements in sensor technology, airborne computing power, and communication capabilities of weapons and equipment, the performance of UAV will witness further enhancements. Consequently, these low-cost and mass-produced UAV will find wider application across various scenarios. Equipped with autonomous decision-making capabilities, UAV can significantly contribute to areas including reconnaissance missions, manned-unmanned cooperation, as well as pursuit-evasion game. At present, the research on UAV pursuit-evasion game primarily concentrates on 1v1 UAV game and multi-UAV cooperative game. In the field of 1v1 UAV pursuit-evasion game, there are three traditional method: game theory for modeling and solving pursuit-evasion game scenarios[1, 2, 3, 4], optimization theory to model pursuit-evasion game as a multi-objective decision optimization problem[3, 4], and utilizing artificial intelligence decision technology with self-learning capabilities[5]. The game theory-based approach is limited by its myopic focus on short-term advantages in UAV game, and the difficulty of accurately modeling complex pursuit-evasion game scenarios. The computational performance of the pursuit-evasion game decision method, based on optimization theory, often fails to satisfy the real-time requirements of pursuit-evasion game decision-making and is primarily employed for offline research aimed at optimizing pursuit-evasion game policies. While the pursuit-evasion game situation exhibits significant diversity and the artificially generated rules are incapable of encompassing all conceivable scenarios. Consequently, while the method may appear straightforward, it necessitates a substantial workload and falls short in terms of both robustness and accuracy. The emergence of deep learning technology has led to significant advancements in various domains[6, 7, 8, 9]. Reinforcement learning (RL), an artificial intelligence technique for intelligent decision-making, has merged with deep learning. In recent years, deep reinforcement learning has emerged as one of the most successful methodologies in the field of artificial intelligence, with widespread applications in intelligent decision-making, control and so on[10, 11, 12, 13, 14, 15], and also plays a crucial role in intelligent pursuit-evasion game. By establishing a decision-making framework to govern the agent-environment interaction and formulating a rational reward function, deep reinforcement learning empowers the UAV agent to effectively acquire knowledge and make informed decisions in pursuit-evasion game scenarios. This not only enhances confront effectiveness but also bolsters survival capabilities, thereby attracting considerable attention from researchers in the field of intelligent pursuit-evasion game. In order to enhance the efficiency of reinforcement learning algorithms in exploring the policy space, Zhang et al.[16] proposed a heuristic Q-network approach that incorporates expert knowledge to guide the search process. This method utilizes the heuristic Q-network technique to train neural network models for solving the over-the-horizon pursuit-evasion game maneuver decision problem. Yang et al.[17] Proposed a approach that presents a decision-making method for autonomous maneuvers of UAV in pursuit-evasion game, utilizing the DDPG algorithm. This method effectively filters out numerous invalid action values using optimized pursuit-evasion game maneuver action values generated by the optimization algorithm. Furthermore, it incorporates the optimized action into the replay buffer as an initial sample, thereby enhancing both game effectiveness and survivability of the DDPG algorithm during UAV pursuit-evasion game. The increasing complexity of the UAV application environment and the growing diversity of tasks have posed challenges for a single UAV to effectively handle various application scenarios. With the development of deep reinforcement learning technology in the multi-agent field[18, 19, 20], the cooperative technology of multiple UAVs has emerged as an imperative solution and a significant developmental trend. Based on the 1v1 UAV game, researchers have devoted their efforts to studying multi-UAV cooperative pursuit-evasion game. The decision-making problem of cooperative multi-target attack in pursuit-evasion game was investigated by Luo et al.[21], who proposed a heuristic adaptive genetic algorithm to effectively explore the optimal solution for missile target assignment. The proposed approach by Wang et al.[22] employed the clonal selection algorithm to establish a multi-step UAV dynamic weapon-target assignment game model, based on the double matrix game Nash equilibrium point solution method, resulting in a more precise Nash equilibrium solution. Furthermore, the technology of deep reinforcement learning also finds extensive applications in the domain of multi-agent systems. Zhang et al.[23] successfully implemented communication between UAVs through bidirectional recurrent neural networks, integrating target allocation and pursuit-evasion game situation assessment to generate cooperative tactical maneuver strategies that merge formation tactical objectives with each UAV‚Äôs reinforcement learning objective. Li et al.[24] proposed a multi-agent double soft actor-critic algorithm, which employs a distributed execution framework based on decentralized partially observable Markov decision process and centralized training. It considers the multi-UAV cooperative pursuit-evasion game problem as a complete cooperative game in order to achieve effective collaboration among multiple UAVs. The aforementioned methods consider the communication and collaboration among multiple UAVs to effectively accomplish cooperative pursuit-evasion game missions. However, they regard the UAV formation as a whole, with only cooperation rather than detailed division of labor, focusing on winning the game while overlooking the cost of UAV‚Äôs during such games. These approaches may lead to even if the UAV formation gain the eventual triumph, but individual UAVs may be exposed to potential encirclement, causing losses. This paper proposes a deep reinforcement learning-based cooperative game method for multi-role formation of UAVs to effectively address this issue, wherein each UAV is assigned distinct roles in the pursuit-evasion game to optimize victory rate and minimize the cost of game. The main contributions of this paper are as follows: i) The proposed algorithm, MEADDQN, enhances the efficiency of data collection for interactive training in reinforcement learning and improves sample efficiency through PER; ii) We designed reward shaping for two different UAV roles and conducted training, enabling them to proficiently perform pursuit and bait tasks respectively. iii) We established multi-role UAV cooperative pursuit-evasion game framework and validated its effectiveness in scenarios involving 2v1, 2v2, and 3v2, yielding favorable outcomes. The subsequent sections of the paper are structured in the following manner. Section 2 presents the UAV dynamics model and offers a comprehensive exposition of the pursuit-evasion game system. Section 3 presents the maneuvering decision algorithms employed by the opposing sides. Section 4 presents the components involved in constructing reinforcement learning models. Section 5 presents the training and testing of the model, which are demonstrated through simulation analysis. And the paper concludes with Section 6, presenting a comprehensive summary encompassing the entirety of the study."
https://arxiv.org/html/2411.02969v1,Multi-modal NeRF Self-Supervision for LiDAR Semantic Segmentation,"LiDAR Semantic Segmentation is a fundamental task in autonomous driving perception consisting of associating each LiDAR point to a semantic label. Fully-supervised models have widely tackled this task, but they require labels for each scan, which either limits their domain or requires impractical amounts of expensive annotations. Camera images, which are generally recorded alongside LiDAR pointclouds, can be processed by the widely available 2D foundation models, which are generic and dataset-agnostic. However, distilling knowledge from 2D data to improve LiDAR perception raises domain adaptation challenges. For example, the classical perspective projection suffers from the parallax effect produced by the position shift between both sensors at their respective capture times. We propose a Semi-Supervised Learning setup to leverage unlabeled LiDAR pointclouds alongside distilled knowledge from the camera images. To self-supervise our model on the unlabeled scans, we add an auxiliary NeRF head and cast rays from the camera viewpoint over the unlabeled voxel features. The NeRF head predicts densities and semantic logits at each sampled ray location which are used for rendering pixel semantics. Concurrently, we query the Segment-Anything (SAM) foundation model with the camera image to generate a set of unlabeled generic masks. We fuse the masks with the rendered pixel semantics from LiDAR to produce pseudo-labels that supervise the pixel predictions. During inference, we drop the NeRF head and run our model with only LiDAR.We show the effectiveness of our approach in three public LiDAR Semantic Segmentation benchmarks: nuScenes, SemanticKITTI and ScribbleKITTI.","I INTRODUCTION LiDAR Semantic Segmentation (SemSeg) is the computer vision task of associating every sample from a LiDAR pointcloud to a semantic class. It is essential for autonomous driving, enabling the identification of drivable areas and object boundaries to make safe and informed decisions when navigating the vehicle‚Äôs surroundings [5] [6]. The availability of autonomous driving perception benchmarks with annotated LiDAR scans [11] [12] [13] [14] has enabled fully-supervised LiDAR SemSeg models [7] [8] [9] [10] to learn this task for narrow domains (e.g. specific dataset, selected cities/countries) where only limited amounts of labeled data are enough to train a well-performing perception model. However, for bringing fully automated driving functions to a world scale, we need models that perform well in a wide variety of scenarios, such as distinct continents, road agents, weather conditions, or driving styles. In the fully-supervised domain, this involves labeling huge amounts of LiDAR pointclouds, which is an expensive task that requires human supervision [15]. On the other hand, LiDAR pointclouds and camera images are cheap to collect compared with their annotation cost [17]. Generally, and for most of the public LiDAR perception benchmarks, LiDAR pointclouds are recorded alongside synchronized camera images at similar instants, with significant overlap in their field of view and a known transformation between the two sensor origins [11] [12] [13] [14]. The camera frames, unlike LiDAR pointclouds, can be processed by the widely available 2D foundation models [1] [2] [3], which are increasingly effective at helping with a wide range of tasks while being dataset agnostic. The Segment-Anything Model [3] (SAM) is a groundbreaking foundation model designed for zero-shot image segmentation. For each image, it generates masks that are agnostic to the instance definition and labeling format. Our work aims to distill the knowledge from SAM for the LiDAR modality during training, while having a LiDAR-only inference pipeline. Fig. 1 exemplifies this knowledge distillation. However, distilling knowledge from 2D foundation models into a 3D perception model raises the challenge of effectively bridging these data from distinct domains. There‚Äôs been extensive work tackling this challenge. The classical way of fusing 2D and 3D data, already known for over a decade, is the perspective projection [18] of LiDAR points into the camera plane given the camera parameters and transformation between sensors. This technique has been demonstrated useful in the past for assigning 3D pseudo-labels from 2D semantic labels, which are typically less expensive to produce, requiring less skilled annotators and more generic equipment [15]. Even though the perspective projection has proven effective and accurate in some scenarios, it suffers from the parallax effect, which is the apparent shift in the position of an object relative to a background when the observer‚Äôs viewpoint changes [16]. In our multi-modal setup, this occurs when the position of the LiDAR and camera at their respective capture times differ. This undesired effect becomes especially stronger at higher velocities and in setups where the camera is placed away from the LiDAR. Some works address this issue by exploiting the multi-view nature of video recordings at consecutive frames or by overlapping multiple-view cameras to select projections that are consistent from multiple views [19]. However, these works require pre-trained 2D SemSeg models and need to either train on data with multi-view cameras and big overlaps or deal with moving objects from neighboring frames. To address this, we include in our Semi-Supervised Learning (SSL) model a self-supervision technique to train on the unlabeled data which is inspired by the training mechanism of Neural Radiance Fields (NeRFs) [20]. We show how this technique can better leverage unlabeled images and scans than existing approaches such as the classical perspective projection. Our NeRF self-supervision allows the model to reason about occupancy and semantics along rays, rather than individual points. It consists of 1) an efficient Pixel-to-Ray Casting mechanism, 2) a N‚Å¢e‚Å¢R‚Å¢FùëÅùëíùëÖùêπNeRFitalic_N italic_e italic_R italic_F Multi-Layer Perceptron (MLP) head, and 3) volumetric rendering equations. The whole NeRF self-supervision is only added during training and dropped for inference. Hence, once deployed, our model is as efficient as any LiDAR-only SemSeg method and doesn‚Äôt require any input data other than LiDAR. We demonstrate the benefit of our method in scenarios with scarcely labeled 3D data on three well-established public benchmarks for LiDAR SemSeg: nuScenes [13], SemanticKITTI [11], and its scarcely-labeled variation: ScribbleKITTI [12]."
https://arxiv.org/html/2411.02673v1,Multi-Transmotion: Pre-trained Model for Human Motion Prediction,"The ability of intelligent systems to predict human behaviors is crucial, particularly in fields such as autonomous vehicle navigation and social robotics. However, the complexity of human motion have prevented the development of a standardized dataset for human motion prediction, thereby hindering the establishment of pre-trained models. In this paper, we address these limitations by integrating multiple datasets, encompassing both trajectory and 3D pose keypoints, to propose a pre-trained model for human motion prediction. We merge seven distinct datasets across varying modalities and standardize their formats. To facilitate multimodal pre-training, we introduce Multi-Transmotion, an innovative transformer-based model designed for cross-modality pre-training. Additionally, we present a novel masking strategy to capture rich representations. Our methodology demonstrates competitive performance across various datasets on several downstream tasks, including trajectory prediction in the NBA and JTA datasets, as well as pose prediction in the AMASS and 3DPW datasets. The code is publicly available: https://github.com/vita-epfl/multi-transmotion.","The research community has witnessed substantial advancements through the adoption of pre-trained models. In natural language processing (NLP), large language models (LLMs) have demonstrated remarkable interdisciplinary proficiency, excelling across a variety of downstream tasks [1, 2]. In contrast, pre-trained models in computer vision (CV) typically exhibit a greater degree of task specificity, which can be attributed to the multimodal nature of visual data [3, 4]. This specificity typically results in reduced efficiency when compared to NLP models. Nevertheless, recent developments in multitask pre-training, exemplified by MultiMAE [5] and 4M [6], have shown promising capabilities in transferring knowledge across a diverse array of CV tasks. However, this critical gap remains in the field of human motion prediction. Unlike fields such as NLP and CV, human motion incorporates rich representations and manifests through diverse modalities, including keypoints, trajectories, and bounding boxes, each reflecting different aspects of human movement. Despite this complexity, there currently exists no multimodal pre-trained model for accurately predicting human motion. Intuitively, human motion cannot be fully expressed by a single modalities. Thus, we argue that each modality can benefit from the others by integrating multiple modalities into the models. Consequently, the development of a multitask pre-trained model is imperative for this domain. Three principal challenges must be overcome to effectively pre-train a model for human motion prediction. First, the field lacks a comprehensive, large-scale dataset that encompasses various modalities of human motion. Second, a versatile framework is required to handle these diverse modalities, in contrast to previous approaches that typically addressed each modality in isolation. Third, the model must be robust when confronted with incomplete or noisy input data. To address these challenges, this paper proposes a novel approach that integrates multiple datasets, develops a flexible network architecture, and demonstrates its effectiveness in handling noisy data. Due to the absence of a large-scale multimodal human motion prediction dataset, we have undertaken the task of merging several existing datasets, namely Joint Track Auto (JTA) [7], Trajnet++ [8], JRDB-Pose [9], NBA [10], Human3.6M [11], AMASS [12] and 3DPW [13]. Each dataset was originally created with different data formats and frame settings. To streamline the training process, we have unified the data framework. This framework standardizes the observation and prediction horizons, as well as the frame rates, ensuring consistency across the merged datasets. This unified framework enables more efficient and effective pre-training of the model, addressing the complexities of multimodal human motion prediction. For the model design, we implement a tokenization strategy that maintains spatial-temporal information across all modalities by applying modality-specific linear projection layers to convert coordinates into hidden dimensions. To further enhance the model‚Äôs robustness and adaptability, we employ up-sampling padding, sampling masks, and a bi-directional temporal encoder. The up-sampling padding facilitates easy fine-tuning across different frame rates, while the sampling masks simulate various frame rates by masking tokens with different chunk sizes. Figure 1 shows an overview of our work. By unifying the datasets, we are able to pre-train a transformer-based model, Multi-Transmotion, that can predict future trajectories and informative 3D pose keypoints. The flexibility of our model architecture and data framework allows for easy fine-tuning to specific tasks with varying frame settings. Our model achieved competitive results on both tasks, demonstrating the effectiveness of our pre-training techniques, as shown by our ablation studies on few-shot learning and robustness. Figure 1: Overview. We propose a unified human motion data framework by standardizing the data format and frame settings. Based on that, we introduce a pre-trained transformer model with specialized masking techniques, validating its effectiveness and flexibility across different scenarios. We summarize the main contributions as follows: ‚Ä¢ Dataset: We create a unified human motion data framework by merging seven datasets with standardized settings. Additionally, this framework is flexible, allowing for the seamless addition of more datasets or adjustments to frame settings. ‚Ä¢ Method: We propose Multi-Transmotion, a pre-trained transformer-based model that flexibly adapts to different frame settings, demonstrating strong robustness and efficiency. This model outperforms previous models across several datasets."
https://arxiv.org/html/2411.02624v1,"Enhancing Indoor Mobility with Connected Sensor Nodes: A Real-Time, Delay-Aware Cooperative Perception Approach","This paper presents a novel real-time, delay-aware cooperative perception system designed for intelligent mobility platforms operating in dynamic indoor environments. The system contains a network of multi-modal sensor nodes and a central node that collectively provide perception services to mobility platforms. The proposed Hierarchical Clustering Considering the Scanning Pattern and Ground Contacting Feature based Lidar Camera Fusion improve intra-node perception for crowded environment. The system also features delay-aware global perception to synchronize and aggregate data across nodes. To validate our approach, we introduced the Indoor Pedestrian Tracking dataset, compiled from data captured by two indoor sensor nodes. Our experiments, compared to baselines, demonstrate significant improvements in detection accuracy and robustness against delays. The dataset is available in the repository111https://github.com/NingMingHao/MVSLab-IndoorCooperativePerception.","In recent years, intelligent indoor autonomy technology is gaining recognition and attention among healthcare professionals and researchers. Studies have shown that indoor transportation is the most urgent need from healthcare staff in hospitals and long-term care [1]. This rising demand is largely driven by workforce shortages and the high incidence of chronic injuries among healthcare staff, which often caused by transporting heavy materials. However, large scale commercial deployment of intelligent robotics platforms are still limited. Most existing indoor robots are designed to operate independently, relying on their built-in sensors to navigate and perform tasks. This restricts their effectiveness in the congested, dynamic, and unpredictable spaces of healthcare facilities. This paper presents a cooperative perception system consisting of a network of multiple sensor nodes, and a central node, to provide perception results/services to robotic mobility platforms. This system aimed to improve the operational safety and environmental awareness of intelligent robotic platforms, including autonomous hospital beds and delivery robots. Figure 1: Overview of the proposed cooperative perception system There are several challenges associated with developing a cooperative perception system in densely populated indoor environments, such as hospitals. One primary challenge for local perception is the fast and accurate fusion of perception data from multiple sensor nodes. This task is complicated by the dynamic behavior of people within a confined space, which involves close interactions between individuals. For instance, people travel in small groups, or crossing paths at close quarters. These situations pose significant difficulties in maintaining consistent tracking identities across different nodes and merging perception data effectively. The physical layout of indoor environments presents another significant challenge for local perception. Architectural features and decorative elements, such as corners, pillars, and mirrors, present significant challenges in achieving continuous and accurate coverage across the entire area. These environmental factors can obstruct the sensor field of view and distort the sensor signal, leading to gaps in coverage or inaccuracies in perception. Figure 2: The proposed delay-aware cooperative perception framework. The processing and communication delays poses a major challenge for global/cross-sensor perception in highly dynamic indoor environments. These cross-node delays can lead to the receipt of outdated or inaccurate representations of the dynamic environment at the center node. This impairs the center node‚Äôs ability to generate a cohesive and current understanding of the environment. To address these challenges, this paper proposes a delay-aware cooperative perception system designed for dynamic indoor environment. An overview of the proposed system is illustrated in Fig. 1. Our contribution can be summarized as follows: ‚Ä¢ An adaptive clustering method coupled with ground-contact point-based LiDAR-camera fusion, enhancing the accuracy and reliability of local perception. ‚Ä¢ A delay-aware global perception framework that accounts for messaging delays and latency, ensuring timely and cohesive environmental understanding. ‚Ä¢ The creation of a multimodal cooperative indoor perception dataset specifically designed for dynamic and crowded healthcare environments. This provids a valuable resource for further research and development in this field. The rest of the paper is organized as follows, in section II, the related methods and dataset are reviewed, in section III, the overview of our method is presented, in section IV, the experiments and discussion are presented, and finally in section V the impact of our work is concluded."
https://arxiv.org/html/2411.02553v1,Map++: Towards User-Participatory Visual SLAM Systems with Efficient Map Expansion and Sharing,"Constructing precise 3D maps is crucial for the development of future map-based systems such as self-driving and navigation. However, generating these maps in complex environments, such as multi-level parking garages or shopping malls, remains a formidable challenge. In this paper, we introduce a participatory sensing approach that delegates map-building tasks to map users, thereby enabling cost-effective and continuous data collection. The proposed method harnesses the collective efforts of users, facilitating the expansion and ongoing update of the maps as the environment evolves.We realized this approach by developing Map++, an efficient system that functions as a plug-and-play extension, supporting participatory map-building based on existing SLAM algorithms. Map++ addresses a plethora of scalability issues in this participatory map-building system by proposing a set of lightweight, application-layer protocols. We evaluated Map++ in four representative settings: an indoor garage, an outdoor plaza, a public SLAM benchmark, and a simulated environment. The results demonstrate that Map++ can reduce traffic volume by approximately 46% with negligible degradation in mapping accuracy, i.e., less than 0.03m compared to the baseline system. It can support approximately 2√ó2\times2 √ó as many concurrent users as the baseline under the same network bandwidth. Additionally, for users who travel on already-mapped trajectories, they can directly utilize the existing maps for localization and save 47% of the CPU usage.","Exploring and mapping uncharted environments has always been a captivating and enduring challenge, from the earliest human migrations to modern space exploration. Recently, with the advance of robotics and autonomous driving technology, high-resolution 3D maps have received a great deal of attention. Envision the following scenario: as you approach a massive, bustling parking garage unfamiliar to you, just minutes before a crucial meeting, you wish your car to autonomously locate an available parking spot and park itself securely. Given that numerous cars today can self-park (once the parking spot is identified), this aspiration is a reasonable leap forward. To realize this vision, a comprehensive, navigate-able 3D map of the garage is urgently needed. Figure 1. A User-Participatory SLAM system. Users upload data to contribute to the map (shown in the bottom left corner of the garage) on the server. The Simultaneous Localization and Mapping (SLAM) technique is crucial to building such 3D maps. SLAM enables continuous user localization/navigation while simultaneously modeling their environment using data collected from various sensors, such as cameras, depth sensors, LiDARs (Campos et al., 2021; Li et al., 2021), which can be attached to the user (e.g., the car, the smartphone) without any infrastructure within the environment. Despite extensive research on SLAM algorithms in the robotics domain, practical SLAM systems, particularly those capable of mapping sizable and complex areas such as multi-level parking garages, keeping the map up to date, and maintaining the map service for long periods of operations remain elusive. The main challenges in building a functional SLAM system arise from the difficulty of collecting comprehensive and fine-grained sensor data of the area of interest over a long period of time at a low cost (Lajoie et al., 2022; Zou et al., 2019). In this work, we propose collaborative user-participatory SLAM systems, shown in Fig. 1, that leverage the widespread availability of onboard cameras on users‚Äô mobile devices or cars for gathering map data and constructing a global 3D map at edge/cloud servers. Users contribute to map construction in a laissez-faire fashion, not following instructions to move (Capponi et al., 2019). By harnessing the collective efforts of users, it facilitates convenient, low-cost, and continuous data collection, enabling the map to expand as users move through the space and keep updated as the environment changes. In turn, the users can obtain and utilize the up-to-date 3D map on their devices. Precisely, the devices capture the surroundings using cameras and upload the data to a map server, which then merges the data into the global map and conducts a global optimization process. We believe that this map-building and maintenance approach can provide effective solutions across numerous environments. The resulting 3D maps can serve as valuable additions to centralized commercial maps such as Google Maps, in terms of both map coverage and the ability to navigate mobile devices and cars. Despite its great promises, user-participatory SLAM faces several fundamental yet intertwined challenges. The primary challenge lies in the excessive map data redundancy. User-participatory mapping allows users to voluntarily contribute data, and upload it to the server while following their trajectories. When two devices capture the environment at similar locations, either simultaneously or at distinct instances, their sensing ranges are likely to overlap, producing redundant data. Given that most users traverse shared roads and paths, there exists a high degree of data redundancy, resulting in significant waste in network bandwidth, processing power, and memory usage. Additionally, the frequent transmission of map data may discourage user engagement due to limited (or, expensive) resources on users‚Äô mobile devices (including cars). In addition to transmitting map data, several SLAM functions need to be performed on user devices, including pose estimation, map data generation, and local optimization, which can sum up to high resource consumption. In this work, we address this challenge by identifying the degree of redundancy between newly acquired map data and the existing global map and only requesting ‚Äúfresh‚Äù data to be transmitted to the server. This can tremendously reduce resource consumption on the server as well as the participation cost of users. To achieve this goal, we devise a lightweight redundancy-checking mechanism utilizing two types of map metadata ‚Äì the device dispatches, instead of the raw data, its pose (location and orientation) to the server, which then constructs a view cone representing the 3D field of view (FOV) of the camera at the pose. By evaluating the overlap between this view cone and the global map through an efficient spatial sampling technique, we can determine the overlap between the new map data and the global map with a minimal cost. Based on the overlap evaluation outcomes, we determine if the device‚Äôs current location is previously ‚Äúseen‚Äù or new. If the location has been seen and mapped, the user does not need to upload map data, significantly reducing the processing/networking/memory resources. Meanwhile, the server shares the global map‚Äôs surrounding portion directly with the device. Leveraging the shared map, the device skips the expensive local optimization step, conserving computation resources and battery energy consumption. Furthermore, the server can suitably enlarge the shared map portion to include the device‚Äôs future locations, further reducing the system overhead. As such, for the first time, we can provide the 3D map service to passing-by devices, making the users feel more rewarded and worthwhile. If the location is new or partially new, the corresponding new map data must be uploaded to expand the global map. Towards this goal, we devise a redundancy control method, involving first removing all the map data that are redundant with the global map and then strategically injecting a minimal amount of redundant map data that are frequently observed and can thus be exploited for better map optimization purposes. In this work, we design Map++, an efficient user-participatory SLAM system that functions as a plug-and-play extension to support existing SLAM algorithms with minimal resource consumption. We have implemented Map++ and integrated it with the open-source project of Covins (Schmuck et al., 2021) based on ORB-SLAM3 (Campos et al., 2021), a state-of-the-art visual SLAM algorithm. To summarize, the main contributions of this paper are as follows: (1) We are the first, to the best of our knowledge, to propose a user-participatory SLAM framework that aims to build a shared map with low resource costs by exploiting user trajectory properties. Compared to a trajectory-unaware distributed SLAM system, our system maximizes the number of participating users under given resource constraints while maintaining SLAM accuracy. As the map expands, subsequent users can access the map as needed. Also, our system can support efficient map updates without incurring skyrocketing memory costs. (2) To minimize data redundancy for reduced computation and communication costs, we devise a set of protocols and algorithms, including metadata-based overlapping assessment, global map sharing for seen locations, and global map expansion for new locations. (3) We thoroughly evaluate the system in four distinct settings with heterogeneous cameras, including two real-world scenarios, a public dataset, and a simulated environment. Map++ manages to reduce approximately 46% traffic volume for map expansion with only a slight degradation in accuracy, i.e., less than 0.03m compared to the baseline system. Consequently, it can support approximately 2√ó2\times2 √ó as many users as the baseline under the same network bandwidth when they participate in mapping at the same time. Additionally, for users who travel on previously-mapped trajectories, they can directly utilize the existing maps for their operations such as localization and save 47% of the CPU usage. Figure 2. Overview of a vanilla shared-map architecture as discussed in (Schmuck et al., 2021). Each user uploads raw data (in the form of keyframes) to the server. The server merges the map from different users and conducts global optimization. Figure 3. Map++ Overview. The gray part (how to update) is not part of this work, and citations are given for further reference."
https://arxiv.org/html/2411.02446v1,Learning World Models for Unconstrained Goal Navigation,"Learning world models offers a promising avenue for goal-conditioned reinforcement learning with sparse rewards. By allowing agents to plan actions or exploratory goals without direct interaction with the environment, world models enhance exploration efficiency. The quality of a world model hinges on the richness of data stored in the agent‚Äôs replay buffer, with expectations of reasonable generalization across the state space surrounding recorded trajectories. However, challenges arise in generalizing learned world models to state transitions backward along recorded trajectories or between states across different trajectories, hindering their ability to accurately model real-world dynamics. To address these challenges, we introduce a novel goal-directed exploration algorithm, MUN (short for ""World Models for Unconstrained Goal Navigation""). This algorithm is capable of modeling state transitions between arbitrary subgoal states in the replay buffer, thereby facilitating the learning of policies to navigate between any ""key"" states. Experimental results demonstrate that MUN strengthens the reliability of world models and significantly improves the policy‚Äôs capacity to generalize across new goal settings.","Goal-conditioned reinforcement learning (GCRL) has emerged as a powerful framework for learning diverse skills within an environment and subsequently solving tasks based on user-specified goal commands, without requiring further training( Mendonca et al., (2021); Andrychowicz et al., (2017)). Given that specifying dense task rewards for GCRL requires domain expertise, access to object positions, is time-consuming, and is prone to human errors, rewards in GCRL are typically sparse, signaling success only upon reaching goal states. However, sparse rewards pose a challenge for exploration during training. To address this challenge, several previous methods, e.g., Hafner et al., 2019a ; Hansen et al., (2023); Mendonca et al., (2021) have proposed learning a generative world model of the environment using a reconstruction (decoder) objective, an instantiation of Model-based Reinforcement Learning (MBRL), visualized in Fig. 1. This approach is appealing because the world model can provide a rich learning signal( Yu et al., (2020); Georgiev et al., (2024)). For example, world models allow agents to plan their actions or exploratory goals without directly interacting with the real environment for more efficient exploration( Hu et al., (2023); Sekar et al., (2020)). Existing MBRL techniques train world models to capture the dynamics of the environment from the agent‚Äôs past experiences stored in a replay buffer. The richness of the data stored in the agent‚Äôs replay buffer directly impacts the quality of a World Model. It is expected that the world model generalizes reasonably well to the state space surrounding the trajectories recorded in the replay buffer. However, the world model may not generalize well to state transitions backward along recorded trajectories or to states across different trajectories, which impedes the world model‚Äôs learning of the real-world dynamics. Figure 1: The general framework of model-based RL. To induce a data-rich replay buffer covering a wide range of dynamic transitions, in this paper, we present a novel goal-directed exploration algorithm for effective world modeling and policy learning, MUN (short for ""World Models for Unconstrained Goal Navigation""). MUN facilitates modeling state transitions between any subgoal states in the replay buffer, whether tracing back along recorded trajectories or transitioning between states on separate trajectories. This enhances the reliability of the learned world model and significantly improves the generalizability of the policy derived from the model to real-world environments, thereby boosting the exploration capabilities of the method. Additionally, we introduce a simple and practical strategy for discovering key subgoal states from the replay buffer. The key subgoals precisely mark the milestones necessary for task completion, such as steps like grasping and releasing blocks in the context of block-stacking scenarios. By world modeling and policy learning for unconstrained navigation between these key states, MUN can generalize to new goal settings, such as block unstacking that was not given to the agent at training time. Our key contributions are as follows. First, we propose a novel goal-directed exploration algorithm MUN for effective world modeling of state transition between arbitrary subgoal states in replay buffers. As the quality of the world model improves, MUN becomes highly effective at learning goal-conditioned policies that excel at exploration in sparse-reward environments. Second, we present a practical strategy for identifying pivotal subgoal states, which serve as milestones in completing sophisticated tasks. By training world models for unconstrained transition between these milestones, our method enables learning policies that can adapt to novel goal scenarios. Finally, we evaluate MUN in challenging robotics environments, such as guiding a multi-legged ant robot through a maze, maneuvering a robot arm amidst cluttered tabletop objects, and rotating items in the grasp of an anthropomorphic robotic hand. Across these environments, MUN exhibits superior efficiency in training generalizable goal-conditioned policies compared to baseline methods and ablations."
https://arxiv.org/html/2411.02359v1,DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution,"Multimodal Large Language Models (MLLMs) have demonstrated remarkable comprehension and reasoning capabilities with complex language and visual data. These advances have spurred the vision of establishing a generalist robotic MLLM proficient in understanding complex human instructions and accomplishing various embodied tasks. However, developing MLLMs for real-world robots is challenging due to the typically limited computation and memory capacities available on robotic platforms. In contrast, the inference of MLLMs involves storing billions of parameters and performing tremendous computation, imposing significant hardware demands. In our paper, we seek to address this challenge by leveraging an intriguing observation: relatively easier situations make up the bulk of the procedure of controlling robots to fulfill diverse tasks, and they generally require far smaller models to obtain the correct robotic actions. Motivated by this observation, we propose a Dynamic Early-Exit Framework for Robotic Vision-Language-Action Model (DeeR-VLA, or simply DeeR) that automatically adjusts the size of the activated MLLM based on each situation at hand. The approach leverages a multi-exit architecture in MLLMs, which allows the model to terminate processing once a proper size of the model has been activated for a specific situation, thus avoiding further redundant computation. Additionally, we develop novel algorithms that establish early-termination criteria for DeeR, conditioned on predefined demands such as average computational cost (i.e., power consumption), as well as peak computational consumption (i.e., latency) and GPU memory usage. These enhancements ensure that DeeR operates efficiently under varying resource constraints while maintaining competitive performance. Moreover, we design a tailored training method for integrating temporal information on top of such multi-exit architectures to predict actions reasonably. On the CALVIN robot manipulation benchmark, DeeR demonstrates significant reductions in computational costs of LLM by 5.2-6.5x and GPU memory of LLM by 2-6x without compromising performance. Code and checkpoints are available at https://github.com/yueyang130/DeeR-VLA.","The recent astonishing progress in multimodal large language models (MLLMs) has unveiled their remarkable potential of extracting, aligning, and integrating the representations from complicated language and visual data [1, 2, 3, 4]. These advances have spurred the vision of a generalist robot, i.e., an embodied agent equipped with vision-language comprehension and problem-solving capabilities, proficient in interacting with humans and the physical world to flexibly execute complex manipulation tasks [5, 6]. An encouraging preliminary work, RT-2 [7, 8], has demonstrated the feasibility of adopting MLLMs to control robots in end-to-end. This not only yields performant robotic policies, but also exhibits some emergent abilities obtained from large models, such as understanding novel commands, generalizing to objects never seen before, and reasoning. Despite these favorable findings, the high demands of MLLMs on hardware are usually an important bottleneck that inhibits the establishment of generalist robots with advanced MLLMs. Typically, robotic applications are based on resource-hungry platforms with limited computational capability, memory space, and battery capacity, yet usually necessitate acting in real-time and performing low-latency interactions with humans or the physical environments. However, every time MLLMs are activated to obtain a robotic action involves utilizing billions of parameters to accomplish a computationally intensive inference process. Such inefficiencies may lead to considerable GPU memory requirements, tremendous power consumption, as well as nontrivial time delays in controlling robots. These weaknesses make it challenging to deploy MLLMs on real embodied robotic systems. To alleviate this problem, we propose an approach based on dynamic neural networks. Our work is inspired by an intriguing observation: in the procedure of controlling a robot to fulfill various tasks, relatively ‚Äòeasier‚Äô circumstances make up the bulk of all the situations confronted by the robot. When encountered with these ‚Äòeasier‚Äô situations, an embodied agent can actually acquire proper robotic actions with a much smaller model compared to the full MLLMs. Or more precisely, only the remaining small number of ‚Äòmore difficult‚Äô circumstances necessitate the full capacity of large MLLMs. This phenomenon can be demonstrated using the representative example in Table 1, where we train RoboFlamingo [9] with varying model sizes, and report the FLOPs and task successful rate in the Calvin Long-Horizon Multi-Task Language Control (LH-MTLC) challenge [10]. Adopting the officially recommended 24-layer Flamingo only correctly finishes 3.2% (78.9% v.s. 75.7%) more tasks compared to using 6 layers, but it increases the computational cost by 4x. In other words, computational resources are wasted on activating larger models in many easy circumstances for which smaller models are sufficient. Table 1: Computation cost v.s. task successful rate222Average successful rate over all subtasks in the long-horizon chains.(RoboFlamingo++) on CALVIN LH-MTLC chanllenge D‚Üí‚Üí\rightarrow‚ÜíD. Notably, we mainly focus on the core component, LLM, of the MLLM, which comprises the majority of parameters. We vary the size of the LLM to examine its impact. For a focused comparison, we report the FLOPs (and GPU memory usage) of the LLM in our paper, unless otherwise specified. # LLM layers 24 12 6 GFLOPs/action (LLM) 31.2 15.6 7.8 Task success rate % 78.9 78.0 75.7 Motivated by this observation, we propose a Dynamic Early-Exit for Robotic MLLM (DeeR) framework, seeking to automatically configure the size of MLLMs conditioned on each situation confronted by an embodied agent. Specifically, we introduce a MLLM architecture featuring multiple intermediate exits, with which a correct robotic action can be immediately obtained once a proper size of the model has been activated, eliminating further redundant computation. Additionally, we develop novel algorithms that are able to establish early-termination criteria for DeeR conditioned on arbitrarily specified demands of average computational cost (i.e., power consumption), and peak computational cost (i.e., latency) or GPU memory overhead. At inference, DeeR can adaptively activate smaller models for less complex situations and larger models for more challenging cases. Consequently, computation is unevenly allocated among situations, yielding a considerable improvement in efficiency. Besides, the computational cost of DeeR can be adjusted online by simply modifying the termination criterion on top of a fixed main model, making it appealing in flexibility. Moreover, we design a tailored training method for DeeR, enabling integrating temporal information on top of such multi-exit architectures to control robots reasonably. The performance of DeeR is evaluated on 3 CALVIN LH-MTLC challenges with RoboFlamingo [9]. Extensive robot experiments show that DeeR reduces the LLM computational cost by 5.2-6.5x without sacrificing performance. Surprisingly, even when considering GPU memory limitations in the termination criterion, DeeR remains competitive with other SOTA methods while only utilizing 2GB memory for the activated LLM. Consequently, DeeR demonstrates the potential to enable a wider range of users to operate their own robots equipped with MLLMs on resource-limited platforms."
https://arxiv.org/html/2411.02230v1,Energy-Aware Coverage Planning for Heterogeneous Multi-Robot System,"We propose a distributed control law for a heterogeneous multi-robot coverage problem, where the robots could have different energy characteristics, such as capacity and depletion rates, due to their varying sizes, speeds, capabilities, and payloads. Existing energy-aware coverage control laws consider capacity differences but assume the battery depletion rate to be the same for all robots. In realistic scenarios, however, some robots can consume energy much faster than other robots; for instance, UAVs hover at different altitudes, and these changes could be dynamically updated based on their assigned tasks. Robots‚Äô energy capacities and depletion rates need to be considered to maximize the performance of a multi-robot system. To this end, we propose a new energy-aware controller based on Lloyd‚Äôs algorithm to adapt the weights of the robots based on their energy dynamics and divide the area of interest among the robots accordingly. The controller is theoretically analyzed and extensively evaluated through simulations and real-world demonstrations in multiple realistic scenarios and compared with three baseline control laws to validate its performance and efficacy.","There has been an increase in the use of autonomous robots in recent years, particularly for the purpose of surveillance and monitoring environments. The collected data from the robots can be used to make further decisions. For example, in a precision agriculture application, robot-collected hyper-spectral images might be used for weed localization and treating the affected areas with herbicides [32]. For such applications, sensor (or spatial) coverage is an important computational problem to consider. In a multi-robot sensor coverage problem, the objective is to distribute the robots (sensors) in a manner that optimally monitors the workspace covering the spatial region (environment) with at least one robot‚Äôs sensor footprint. This objective is different from and is not to be confused with the area coverage problem, where the objective is that the robot(s) should visit each and every part of the environment. A promising technique for achieving the sensor coverage goal is to divide the environment into regions using Lloyd‚Äôs algorithm, also known as Voronoi partitions, which can be optimized based on constraints such as uncertainties in sensing and sensor health, among others [28, 31, 24, 1]. A heterogeneous group of robots (e.g., a group of UGVs and UAVs) allows the system to develop proficiency in different areas of the task despite their inherent limitations on a specific capability, such as sensing or mobility [27, 2]. UAVs, for instance, can cover more terrain and are less susceptible to obstacles than UGVs, which, on the other hand, generally have higher battery capacities and deplete energy at a slower rate than UAVs [38]. We posit that the robot‚Äôs energy depletion rate is an important factor that needs to be incorporated into their controllers to increase the overall lifetime of the multi-robot system and optimize the mission objective. Figure 1: Regions assigned by standard Voronoi partitioning (left) and the proposed energy-aware controller (EAC) (right). The robots have the same initial battery level. However, robot 3‚Äôs depletes its energy three times faster than the other robots. Therefore, robot 3‚Äôs EAC-assigned region area (area in m2superscriptùëö2m^{2}italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT in parentheses) is less than other robots by adapting the weights (wisubscriptùë§ùëñw_{i}italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT) based on the ratio of energy depletion rate between the robots. The rate of energy depletion could vary significantly between robots due to their sensor payloads (e.g., some robots may be equipped with high-power thermal cameras or power drill tools in search and rescue applications), as well as their velocities (e.g., UAVs operating at higher altitude will deplete energy faster than the ones at lower altitudes) [23, 19, 36]. Moreover, the energy depletion rate of a robot can change dynamically over time (e.g., activating or deactivating a payload, increasing the velocity, etc.), and the robots need to adjust their coverage loads accordingly. Fig. 1 illustrates such a scenario. Here, the energy-aware controller optimally assigns a lower area to robot 3, which had almost four times higher energy depletion rate compared to the other robots, which will limit its ability to cover more area. Assigning a larger area to a robot with more energy or a lower depletion rate and smaller areas to other robots will distribute the workload among robots and ensure effective coverage and timely execution of tasks. Energy availability has significant implications for coverage planning. Several works proposed in the literature [11, 12, 6, 8] focusing on this problem consider limiting the robot velocity and/or partitioning the regions based on the current energy levels. However, this will result in poor coverage quality when robots have different energy depletion rates. Moreover, as discussed earlier, real-world applications might require a heterogeneous group of robots with different and dynamic energy characteristics, where robots consume energy differently.Moreover, restricting the robots‚Äô velocities like performed in [12, 8] could conflict with the low-level, time-limited tasks the robots are assigned to. To this end, we present a novel distributed multi-robot controller that holistically considers the robot‚Äôs energy capacity and depletion characteristics of other robots for optimal energy-aware coverage planning. We extensively validate the controller in simulations (scaling up to 100100100100 robots) and with real robots within an in-house swarm robotics testbed (see the video). The main contributions of our paper are as follows. ‚Ä¢ To the best of our knowledge, this is the first study that considers differences in the energy capability of the robots with heterogeneous energy depletion rates for effective multi-robot coverage planning. ‚Ä¢ Unlike prior studies, we consider that the robots might have time-varying energy depletion rates during a mission. Compared to the state-of-the-art relevant algorithms, our proposed Energy-Aware Controller (EAC) significantly reduces coverage cost and achieves energy-balancing coverage objectives."
https://arxiv.org/html/2411.02214v1,DexHub and DART: Towards Internet Scale Robot Data Collection,"The quest to build a generalist robotic system is impeded by the scarcity of diverse and high-quality data. While real-world data collection effort exist, requirements for robot hardware, physical environment setups, and frequent resets significantly impede the scalability needed for modern learning frameworks. We introduce DART, a teleoperation platform designed for crowdsourcing that reimagines robotic data collection by leveraging cloud-based simulation and augmented reality (AR) to address many limitations of prior data collection efforts. Our user studies highlight that DART enables higher data collection throughput and lower physical fatigue compared to real-world teleoperation. We also demonstrate that policies trained using DART-collected datasets successfully transfer to reality and are robust to unseen visual disturbances. All data collected through DART is automatically stored in our cloud-hosted database, DexHub, which will be made publicly available upon curation, paving the path for DexHub to become an ever-growing data hub for robot learning. https://dexhub.ai/project","Robotics has seen impressive progress with the advent of learning-based control. However, a major bottleneck is the lack of diverse and high-quality data for training robust and generalizable robot policies. Access to an internet-scale robotics dataset that continually and rapidly grows with data coming from everywhere in the world will be ideal ‚Äî just like how people easily upload language, images, and videos on the internet. Despite recent efforts [1, 2, 3, 4], we are not there yet. In this paper, we examine and address many key bottlenecks in achieving this dream. Consider collecting data to perform a given task, such as moving dishes from the sink to the dishwasher. The data collector‚Äôs first challenge is setting up the environment. There are two options: physically construct a kitchen in the lab around the robot or physically move the robot to an actual kitchen. Neither is easy to scale as data will be needed from many kitchens. Once the environment is set up, operating the robot leads to the second challenge ‚Äì observing and understanding the scene. For instance, due to visual occlusions and lack of tactile feedback, operators may be unable to sense an object‚Äôs motion resulting from the robot‚Äôs action. Further, if the teleoperation is remote, it adds additional challenges originating from network delays, limited field of view, and visual artifacts. Such challenges can slow down operators and often prevent them from performing dynamic or precise tasks. If the data collector succeeds at resolving the first two challenges and moves all the dishes from the sink to the dishwasher to complete the exemplar task, a third obstacle emerges: all the dishes must be returned to the sink to collect a new trajectory! In addition to being time-consuming, this resetting is physically and mentally exhausting as operators must context-switch between robot control and environment setup. Ensuring that each reset presents the robot with a diverse range of scenarios is also mentally taxing requiring imagination. What makes the experience even worse for the operators is the need to repeat the process of teleoperating and resetting a large number of times. The number of required demonstrations and the need for diversity in demonstrations scales with the task complexity and the extent of required generalization. Unfortunately, humans are known to lose focus when performing a repetitive job [5]. Finally, say the operator has finished collecting a few hundred demonstrations. How does the recorded data get processed, stored, and used? It is common to store collected demonstrations on a local machine or a private cloud, which is often not shared to general publicly unless someone explicitly requests it. These issues, all combined, make existing data collection methods struggle to scale up without operator fatigue. Making everything worse, the data collected in real-world has limited applicability in terms of policy training methods; reinforcement learning, for instance, cannot be easily applied on top of real-world demonstrations as it lacks a digital twin where virtual agents can freely explore and self improve its performance. A data collection method that (a) can be easily parallelized and crowd-sourced with minimal hardware requirements and (b) wide range of policy training pipelines can be applied can get closer to the needed scale and diversity of robot data. To that end, we introduce DART, a Dexterous Augmented Reality Teleoperation system, enabling anyone in the world to teleoperate robots in simulation with an intuitive, game-like AR interface. Connected to a cloud-hosted simulation, DART allows users to collect demonstrations for an unlimited number of scenes in one sitting without having to physically set up environments or physically move robots to different places. DART‚Äôs high-fidelity AR rendering allows users to observe the scene in great detail with minimal occlusion, enabling teleoperation of complex tasks. DART also allows users to reset the environment with a click of a button, removing the taxing process of physically resetting the scene. As a result, our user study shows that DART achieves 2.1√ó\times√ó faster data collection throughput with significantly less physical and cognitive fatigue on tasks requiring fine-grained control compared to most existing robot data collection pipelines. Our experiments also highlight the unmatched benefits of collecting demonstrations in simulation over the real world. Simulation-trained policies achieve higher robustness than real-world trained policies due to data augmentation and randomization strategies only possible in simulation. Finally, all robot demonstrations collected through DART are automatically stored and logged to our public cloud-hosted database, DexHub, which serves as an open-sourced data hub for robot learning. Our key contributions are outlined as follows: 1. In Sec. III, we introduce DART, a novel AR-based teleoperation platform, and detail its system architecture and supported features. We also showcase the diversity of tasks we can perform with DART, unlocked by enhanced teleoperation experience. 2. In Sec. IV-A, we analyze the impact of different teleoperation interface design choices through user study. We show that DART enables higher data collection throughput and lower fatigue than alternatives. 3. In Sec. IV-B, we show that policies trained with data collected via DART can be effectively transferred to the real world and are more robust than those trained with real-world demos. 4. In Sec. V, we provide an brief overview of the proposed DexHub platform that serves as a central hub for hosting large-scale robot demonstrations generated by DART."
https://arxiv.org/html/2411.02189v1,DiffSim2Real: Deploying Quadrupedal LocomotionPolicies Purely Trained in Differentiable Simulation,"Differentiable simulators provide analytic gradients, enabling more sample-efficient learning algorithms and paving the way for data intensive learning tasks such as learning from images. In this work, we demonstrate that locomotion policies trained with analytic gradients from a differentiable simulator can be successfully transferred to the real world. Typically, simulators that offer informative gradients lack the physical accuracy needed for sim-to-real transfer, and vice-versa. A key factor in our success is a smooth contact model that combines informative gradients with physical accuracy, ensuring effective transfer of learned behaviors. To the best of our knowledge, this is the first time a real quadrupedal robot is able to locomote after training exclusively in a differentiable simulation.","The majority of Reinforcement Learning (RL) algorithms rely on Zeroth-order Gradient (ZoG) estimates during optimization, allowing the use of conventional physics simulators that are typically non-differentiable. However, differentiable simulators offer analytically computed First-order Gradients, with lower variance [1, 2, 3] and therefore improved sample efficiency and asymptotic policy performance [4, 5]. Thus, leveraging FoGs offers the potential to learn from pixels [6] or to learn policies for systems with many degrees of freedom [7]. Unfortunately, contact interactions are often simulated in a discontinuous manner, making FoG-based optimization challenging. Some simulators address this by using soft contact models, which are continuous and smooth but less physically accurate for typical locomotion problems compared to discontinuous hard contact models [8]. Additionally, penalty-based soft contact models often require smaller time steps and thus increase computational demand and lengthen gradient chains. Consequently, learning the contact-rich task of quadrupedal locomotion and transferring the learned behavior to the real world with either hard or penalty-based contact has not yet succeeded [9, 10, 11]. Instead, we adopt an analytically smooth contact model, introduced in our previous work [9], that provides a smoothed optimization surface while maintaining physical accuracy, combining the advantages of hard and soft contact. The contact model draws inspiration from the role of stochasticity in current learning frameworks, a key factor in the success of RL [12, 13]. We then employ the Short-Horizon Actor-Critic (SHAC) algorithm [7] that leverages FoGs to enhance learning efficiency over purely ZoG-based algorithms such as Proximal Policy Optimization (PPO) [14]. Finally, we demonstrate that locomotion policies learned with this approach successfully transfer to the real world. Figure 1: A quadrupedal robot learning to walk on flat terrain in a differentiable simulation. Policies trained with a hard contact model follow unreasonable foothold patterns and do not learn to locomote robustly. Training with a soft contact model results in stable locomotive gaits but the learned behaviors do not transfer to real hardware. Policies trained with an analytically smooth contact model exhibit effective and stable locomotive gaits and transfer to the real world. Video: https://youtu.be/2wZmmUyqUQM. Previous attempts were confined to simulation. The first locomotion policy purely trained in a differentiable simulator was presented in [11], but exhibited undesirable behaviors like front flips. Table I summarizes relevant simulators and their approaches to differentiation and contact modeling. Nimble [15] implements symbolic differentiation and solves a sparse Linear Complementarity Problem (LCP) to resolve contact, while DiffTaichi [16] uses impulse-based methods to avoid differentiating the LCP of contact. Warp [17] and Brax [10] leverage GPU acceleration for fast rigid-body simulations and support multiple contact models. Dojo [18] emphasizes physical accuracy but is limited by slower execution and lacks parallelism. At the time of writing, none of the current simulators offer parallelization combined with accurate dynamics and informative gradients to learn transferable locomotion behaviors. A sim-to-real transfer for quadrupedal locomotion policies learned using FoGs was only achieved with a second non-differentiable simulator to ensure accurate physics [19]. In this work, we extended Warp with custom physics to benefit from GPU parallelization. TABLE I: Differentiable Rigid-Body Simulators Name Differentiation Contact Modeling Device Nimble [15] Symbolic LCP CPU DiffTaichi [16] Automatic Impulse-based GPU Warp [17] Automatic XPBD [20], Soft GPU Brax [10] Automatic MuJoCo [21], PBD [22] GPU Dojo [18] Symbolic NCP CPU"
https://arxiv.org/html/2411.02187v1,Touch-to-Touch Translation - Learning the Mapping Between Heterogeneous Tactile Sensing Technologies,"The use of data-driven techniques for tactile data processing and classification has recently increased. However, collecting tactile data is a time-expensive and sensor-specific procedure. Indeed, due to the lack of hardware standards in tactile sensing, data is required to be collected for each different sensor. This paper considers the problem of learning the mapping between two tactile sensor outputs with respect to the same physical stimulus - we refer to this problem as touch-to-touch translation. In this respect, we proposed two data-driven approaches to address this task and we compared their performance. The first one exploits a generative model developed for image-to-image translation and adapted for this context. The second one uses a ResNet model trained to perform a regression task. We validated both methods using two completely different tactile sensors - a camera-based, Digit [1] and a capacitance-based, CySkin [2]. In particular, we used Digit images to generate the corresponding CySkin data. We trained the models on a set of tactile features that can be found in common larger objects and we performed the testing on a previously unseen set of data. Experimental results show the possibility of translating Digit images into the CySkin output by preserving the contact shape and with an error of 15.18% in the magnitude of the sensor responses.","Tactile sensing is fundamental to enable robots to physically interact with the environment [3, 4, 5] and to perform complex manipulation tasks [1, 6, 7]. In recent years, there has been a growing trend in employing data-driven algorithms to process tactile feedback [8, 9]. However, the process of collecting a tactile dataset is inherently challenging and time-consuming, especially for classification tasks requiring multiple explorations of objects to capture their entire shape [10]. To overcome the challenges of data collection, researchers have explored methods to artificially generate tactile data from camera images [11, 12, 13, 14, 15] by applying existing models for image-to-image translation to the tactile domain [16]. These generative models are trained on a large dataset of visuo-tactile data to learn the mapping between RGB-D data captured by cameras and the corresponding tactile response obtained through physical contact with the surface of interest. Despite the successful generation of tactile datasets (under certain conditions), unlike cameras, there is a lack of standardized hardware for tactile sensors which can significantly differ in terms of geometry, sensor arrangement, spatial resolution, transduction principles and mechanical properties [17]. Consequently, models or algorithms developed for specific tactile hardware, cannot be directly applied to different sensors, often necessitating a new data collection procedure or model fine-tuning [14]. The previously mentioned methods for data generation do not address this aspect. Figure 1: touch2touch architecture and training pipeline. (1) The network is trained by collecting paired tactile samples corresponding to the same physical stimulus; (2) Camera-based sensor and its output; (3) Taxel-based tactile sensor. The red circle highlight a single taxel which measurement is contained in the output array YùëåYitalic_Y; (4) The output YùëåYitalic_Y is converted into an image IùêºIitalic_I and input to the pix2pix model; (5) The pix2pix model and its two loss functions as described in [18]. G and D represent the generator and the discriminator; (6) The ‚Ñí3subscript‚Ñí3\mathcal{L}_{3}caligraphic_L start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT loss added to properly stop the model training; (7) The output of touch2touch, Y^^ùëå\hat{Y}over^ start_ARG italic_Y end_ARG corresponding to I^^ùêº\hat{I}over^ start_ARG italic_I end_ARG converted into an array. In this paper, we take the initial step toward addressing this challenge by developing a method to transform data acquired with a specific tactile technology to the corresponding outputs that would be obtained using different hardware. Our contribution is to present a method to learn the mapping M:{X‚ÜíY}:ùëÄ‚ÜíùëãùëåM:\{X\rightarrow Y\}italic_M : { italic_X ‚Üí italic_Y }, with XùëãXitalic_X and YùëåYitalic_Y being the outputs of two distinct tactile sensors with respect to the same physical stimulus. We refer to this task as touch-to-touch translation. To the best of our knowledge, this paper represents the first attempt to address this problem. Specifically, we focus on translating images acquired with a camera-based tactile sensor [19] into the output of a system composed of distributed tactile elements (i.e. taxels) [17]. The proposed approach is tested using Digit [1] and CySkin [2] sensors. Therefore, the model presented in this paper allows to artificially generate the CySkin output given a Digit tactile image as an input. It is important to remark that the two sensors are fundamentally different - Digit outputs images, while CySkin provides an array of measurements reflecting changes in capacitance between the two sensor layers. These differences encompass spatial resolution, sensor geometry, spatial arrangement of the sensing elements, transduction mechanism and mechanical properties. As a consequence, the responses of the two sensors with respect to the same tactile stimulus are substantially different, thus necessitating a model addressing these disparities. We identified two possible strategies to address the touch-to-touch translation problem. The first one involves leveraging generative models developed for image-to-image translation and adapting them to our context. The second one consists of using Convolutional Neural Networks (CNNs) to perform a regression task, thus transforming the Digit input image into an array. In this respect, we perform a comparison to understand which approach is more suitable for this task. Furthermore, in order to generalize, the models should be trained on a wide variety of different objects such as in [12], thus further complicating the data collection process. In this respect, we show that assuming a small contact area, the mapping between sensors can be learned on a set of features, representing tactile primitives such as line or step edges and corners that generally compose larger objects. Once the model is trained on this dataset it will be deployed on novel objects. The remainder of the paper is organized as follows: Section II describes the two approaches and the training pipeline. The experimental setup and the data collection procedure are explained in Sections III-A and IV respectively. Results and discussions are presented in Section V. Conclusion follows."
https://arxiv.org/html/2411.02186v1,Limiting Kinetic Energy through Control Barrier Functions: Analysis and Experimental Validation,"In the context of safety-critical control, we propose and analyse the use of Control Barrier Functions (CBFs) to limit the kinetic energy of torque-controlled robots. The proposed scheme is able to modify a nominal control action in a minimally invasive manner to achieve the desired kinetic energy limit. We show how this safety condition is achieved by appropriately injecting damping in the underlying robot dynamics independently of the nominal controller structure. We present an extensive experimental validation of the approach on a 7-Degree of Freedom (DoF) Franka Emika Panda robot. The results demonstrate that this approach provides an effective, minimally invasive safety layer that is straightforward to implement and is robust in real experiments. A video of the experiments can be found here","Collaborative robots, sometimes called cobots, are gaining traction across a wide range of industries, including logistics, service robotics, and manufacturing [1, 2]. Safety is a critical control objective when these robots share space with humans [3, 4]. The recent rise of learning-based controllers, which typically only provide probabilistic safety guarantees, underscores the need for safety-critical approaches [5]. ISO standards [6] attempt to formalise the safety hazards in this setting, and their mitigation is an active research area. Some works prevent interaction, by enforcing a speed-dependent separation distance between the robot and operator, assuming reliable detection methods [7, 8]. Other works limit long-duration interaction power and force by implementing, e.g., impedance control [9, 10, 11]. Yet other approaches explore various dynamic human-robot impact scenarios and relate the impact velocity to the risk of injury [12]. In this work safety is addressed by bounding the kinetic energy that could potentially be transferred to a human operator, in order to prevent injury in collision scenarios. The importance of this choice is backed by numerous publication which relate directly relevant safety metrics to the energy flow generated from the interaction [13, 4]. Furthermore, the power and force limiting (PFL) conditions in the ISO/TS 15066 [6], which are the only collaborative conditions in which contact between humans and robots are considered, are addressed through energetic constraints. We propose a method that takes the form of a safety filter, enforcing a bound on maximum kinetic energy while minimally altering a desired control input. We make use of Control Barrier Functions (CBFs), a safety-critical control algorithm able to constrain the robot to a region of its state space representing safe operating conditions [14]. Most CBF implementations in robotics apply to safety-critical kinematic control (i.e., tasks in which the safety constraint represents obstacle avoidance conditions) and rely on lower-level controllers to handle system dynamics [15, 16, 17]. Instead, we investigate the use of energy-based CBFs and, different from previous works such as [18, 19], utilize them to directly limit the kinetic energy of a torque-controlled robot. We recognise relevant related works proposing schemes to limit the kinetic or total energy of torque-controlled manipulators for safety objectives. These works are motivated by energy-aware and passivity arguments [20, 21, 22], stressing the fact that safety measures are closely related to energy- and power-based metrics. The recent work [23] presents a control algorithm that is able to limit the kinetic energy, achieved by using higher-order CBFs in a system augmented with energy tanks [24], used to enforce passivity of the overall scheme. Other approaches attempt to limit kinetic energy [25] and total energy [26] of controlled robots, also using energy-tank based arguments to recover passivity. In this work, we present a novel approach that avoids considering passivity as a strict constraint to be achieved at design phase. Instead, we achieve the kinetic energy bound directly through the proposed CBF-based algorithm. We analytically and experimentally show that the proposed CBF operates solely by injecting damping into the system, ensuring that the safety-critical control action inherently preserves the passivity of any nominal passive closed-loop system. This eliminates the need for supplementary tools such as energy tanks, making the proposed scheme significantly simpler than most of the state-of-the-art solutions. The main contributions of this paper are: 1. A kinetic energy-limiting CBF-based safety filter and analysis of its energetic properties. 2. Extensive experimental validation on a 7-DoF robot manipulator of the proposed safety-critical control system. The remainder of this paper is outlined as follows. Section 2 contains the mathematical background and analysis involving the specific CBF used in this work. We present extensive experimental results in four scenarios in Section 3. Finally, Sections 4 and 5 discuss the results and conclude the paper."
https://arxiv.org/html/2411.02169v1,Diffusion-based Virtual Fixtures,"Virtual fixtures assist human operators in teleoperation settings by constraining their actions. This extended abstract introduces a novel virtual fixture formulation on surfaces for tactile robotics tasks. Unlike existing methods, our approach constrains the behavior based on the position on the surface and generalizes it over the surface by considering the distance (metric) on the surface. Our method works directly on possibly noisy and partial point clouds collected via a camera. Given a set of regions on the surface together with their desired behaviors, our method diffuses the behaviors across the entire surface by taking into account the surface geometry. We demonstrate our method‚Äôs ability in two simulated experiments (i) to regulate contact force magnitude or tangential speed based on surface position and (ii) to guide the robot to targets while avoiding restricted regions defined on the surface. All source codes, experimental data, and videos are available as open access at https://sites.google.com/view/diffusion-virtual-fixtures.","I INTRODUCTION For a long time, robotics considered objects in the environment primarily as obstacles and the goal was to avoid contact due to modeling and sensing difficulties. However, the trend has shifted towards embracing contact due to increasing interest in manipulation, tactile robotics, and surface inspection tasks. Consequently, robots physically interact with their surrounding environment that can charecterized by curved surfaces, which can also be soft and fragile (e.g., surgical robotics). However, safety in these tasks remains a major concern during deployment in real-world as they involve forceful interactions. Considering that a significant percentage of recent approaches propose learning-based controllers, and that the majority of shared control and teleoperation tasks depend on the operator‚Äôs expertise or skills, safety takes a more central role in assistive systems. Therefore, in tactile tasks, there is a need for modules that can be combined with other controllers to enforce high priority constraints. Figure 1: Input and example use-cases of our method. Left, the input to our method ‚Äì a partial point cloud with segmented regions Œ©1subscriptŒ©1\Omega_{1}roman_Œ© start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, Œ©2subscriptŒ©2\Omega_{2}roman_Œ© start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT and the free region Œ©0subscriptŒ©0\Omega_{0}roman_Œ© start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT with their estimated boundaries ‚àÇŒ©1subscriptŒ©1\partial\Omega_{1}‚àÇ roman_Œ© start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, ‚àÇŒ©2subscriptŒ©2\partial\Omega_{2}‚àÇ roman_Œ© start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT and ‚àÇŒ©0subscriptŒ©0\partial\Omega_{0}‚àÇ roman_Œ© start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. Center, the first experiment showing the virtual fixture constraining the contact force magnitude based on diffusing the contact forces designated on Œ©1subscriptŒ©1\Omega_{1}roman_Œ© start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and Œ©2subscriptŒ©2\Omega_{2}roman_Œ© start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. Right, the second experiment showing the flow field guiding the agents towards target Œ©2subscriptŒ©2\Omega_{2}roman_Œ© start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT by avoiding restricted regions Œ©1subscriptŒ©1\Omega_{1}roman_Œ© start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT where the black paths show trajectories with different initial positions. Virtual fixtures, introduced by Rosenberg [rosenbergVirtualFixturesPerceptual1993] for teleoperation, are formulations that assist the operator by constraining their actions. Existing methods [selvaggioPassiveVirtualFixtures2018, muhlbauerProbabilisticApproachMultiModal2024, pruksMethodGeneratingRealtime2022a, quinteroRobotProgrammingAugmented2018, quinteroFlexibleVirtualFixture2017], although operating on surfaces, consider the problem in the robot workspace with the standard Euclidean metric. Accordingly, they do not capture the tangent space and the distance (metric) on the surface. In contrast, various tactile tasks have constraints based on the position on the surface instead of a single guidance path and they change based on the distance on the surface. For instance, to regulate the contact force magnitude or the speed based on the position on the surface, one can annotate the desired values at particular points, as shown in Figure 1-a. Then, these values can be diffused by considering the surface metric across the entire surface as in Figure 1-b. Alternatively, by specifying only the target and obstacle regions, a smooth flow field on the tangent space can guide agents to the closest target while avoiding the restricted zones and maintaining contact with the surface, as depicted in Figure 1-c. For addressing these challenges, we propose a surface virtual fixture method expecting surfaces as possibly noisy and partial point clouds collected in runtime using an off-the-shelf camera attached to the robot. Next, we segment the point cloud into a set of regions with their specified behavior. This segmentation can come from learning-based methods using vision [caronEmergingPropertiesSelfSupervised2021] or geometry [sharpDiffusionNetDiscretizationAgnostic2022a]. Alternatively, one can use virtual or real-world expert annotations [quinteroRobotProgrammingAugmented2018, quinteroFlexibleVirtualFixture2017], possibly in combination with feature extractors [pruksMethodGeneratingRealtime2022a]. Then, we diffuse the specified behavior across the whole surface to combine them consistently by considering the surface metric."
https://arxiv.org/html/2411.02062v1,Heterogeneous Multi-robot Task Allocation for Long-Endurance Missions in Dynamic Scenarios,"We present a framework for Multi-Robot Task Allocation (MRTA) in heterogeneous teams performing long-endurance missions in dynamic scenarios. Given the limited battery of robots, especially in the case of aerial vehicles, we allow for robot recharges and the possibility of fragmenting and/or relaying certain tasks. We also address tasks that must be performed by a coalition of robots in a coordinated manner. Given these features, we introduce a new class of heterogeneous MRTA problems which we analyze theoretically and optimally formulate as a Mixed-Integer Linear Program (MILP). We then contribute a heuristic algorithm to compute approximate solutions and integrate it into a mission planning and execution architecture capable of reacting to unexpected events by repairing or recomputing plans online. Our experimental results show the relevance of our newly formulated problem in a realistic use case for inspection with aerial robots. We assess the performance of our heuristic solver in comparison with other variants and with exact optimal solutions in small-scale scenarios. In addition, we evaluate the ability of our replanning framework to repair plans online.","The use of cooperative teams of heterogeneous robots is becoming a trend in many applications that can benefit from the combination of different sensing and/or locomotion robot capabilities, such as inspection [1, 2], precision agriculture [3, 4], or fire fighting [5]. For example, Unmanned Aerial Vehicles could be combined with ground robots [3, 6], as the former can access more distant places while the latter can carry heavier equipment. Cooperation among heterogeneous UAVs is another option [7, 5, 1], as they may provide different maneuverability abilities (e.g., rotary vs. fixed-wing vehicles) or specific sensors and manipulation/delivery capabilities depending on the vehicle. In these applications, MRTA can become especially hard, as they usually pose multi-objective optimization problems with multiple constraints; e.g., some tasks may only be executed by certain robots with the required capabilities, or there could be tasks that need to be accomplished by multiple robots in a synchronous manner, among others. Moreover, we are particularly interested in long-endurance missions in outdoor environments. This setting brings two additional challenges: 1) battery capacities could be limiting for the robots, especially for multi-UAV teams, which inspire our work, so recharging operations should be scheduled during operation; and 2) these outdoor scenarios are typically dynamic and require online replanning in order to perform long-endurance missions robustly, potentially dealing with robot delays and failures. In this paper, our objective is to devise a planning framework to solve a new class of heterogeneous MRTA problems. We introduce recharge operations to extend robots‚Äô autonomy, and we endow the problem with greater flexibility by allowing certain tasks to be fragmented and/or executed by coalitions ‚Äì with a non-fixed size ‚Äì of robots, as well as allowing the possibility of having inter-robot relays. Moreover, as we aim for dynamic scenarios where unplanned events may occur, our framework is able to detect these events and repair or fully recompute plans in real time to adapt to the new circumstances. To the best of our knowledge, there is no alternative method in the literature that combines all these problem features (see related works in Section II). Therefore, we take a step forward by categorizing, formulating, and solving this novel class of heterogeneous MRTA problems for long-endurance missions. The contributions of this work are the following: 1. Motivated by our heterogeneous teams of robots performing long-endurance missions in dynamic outdoor settings, we pose a novel MRTA problem (Section III) where robot recharges are allowed and different types of tasks are combined, depending on their level of decomposability and on the size flexibility of the robot coalition required to execute them. We discuss the categorization of our problem within well-known MRTA taxonomies in the literature and prove its NP-hard complexity. 2. We formulate our MRTA problem as a MILP. Our formulation is general enough to account for all problem features (Section IV), integrating heterogeneous robot capabilities, recharge operations, task decomposition, inter-robot relays, multi-robot tasks performed by synchronized coalitions of fixed or variable size, and more. From a theoretical perspective, this MILP formulation helps analyze the complexity and characteristics of the optimal solutions of our new class of MRTA problems. 3. Given the NP-hardness of the problem, exact solvers suffer from scalability issues to tackle the MILP posed here. Therefore, we contribute a novel heuristic algorithm that leverages specific properties of the problem in order to find approximate solutions that comply efficiently with all constraints (Section V). 4. To robustly cope with dynamic scenarios, we integrate our heuristic solver into a mission planning and execution framework (Section VI). Mission execution is monitored to 1) repair plans in case of robot delays; and 2) recompute the full plan online if a repair is not possible or an unexpected event occurs, such as a robot failure or the arrival of new tasks. 5. We provide extensive experimental results to demonstrate our algorithms in a realistic use case for multi-UAV inspection (Section VII). We compare our heuristic plans with the optimal solutions of the MILP formulation in small-scale scenarios. We then assess the performance of the heuristic solver for larger scenarios and evaluate our whole replanning and execution framework. All our code is provided open-source for the community and integrated into a ROS-based architecture.111Robot Operating System, https://www.ros.org. This paper is an evolved version of our previous work [8], where we introduced our MILP problem formulation. Here, we extend the work with the heuristic algorithm to find approximate solutions efficiently and with the online replanning framework for dynamic scenarios. Additionally, we provide a full set of new experimental results and release all the code as open source."
https://arxiv.org/html/2411.01985v1,"¬©‚Äâ2024 IEEE. Accepted for IEEE Communications Magazine. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work.Reshaping UAV-Enabled Communications with Omnidirectional Multi-Rotor Aerial Vehicles","A new class of Multi-Rotor Aerial Vehicles, known as omnidirectional MRAVs, has attracted significant interest in the robotics community. These MRAVs have the unique capability of independently controlling their 3D position and 3D orientation. In the context of aerial communication networks, this translates into the ability to control the position and orientation of the antenna mounted on the MRAV without any additional devices tasked for antenna orientation. This additional Degrees of Freedom (DoF) adds a new dimension to aerial communication systems, creating various research opportunities in communications-aware trajectory planning and positioning. This paper presents this new class of MRAVs and discusses use cases in areas such as physical layer security and optical communications. Furthermore, the benefits of these MRAVs are illustrated with realistic simulation scenarios. Finally, new research problems and opportunities introduced by this advanced robotics technology are discussed.","Over the past decade, the integration of Unmanned Aerial Vehicles into wireless networks has attracted significant attention from both academic and industrial research communities [1]. UAVs have been extensively studied for two main reasons: their agility allows for swift deployment to address changing demands and optimize network performance [1], and their ability to achieve Line-of-Sight (LoS) improves communication reliability and expands coverage area [2]. Consequently, UAVs have been considered for applications such as replacing damaged base stations in disaster recovery, serving as data relays for remote Internet of Thing (IoT) devices, and acting as aerial base stations to enhance network capacity in urban and underserved areas [2, 1]. Much of the literature focuses on under-actuated MRAVs, which have fewer control inputs ‚Äì such as thrust, pitch, roll, and yaw rate ‚Äì than Degrees of Freedom (DoF) in 3D space. This means that the 3D orientation of u-MRAVs is dependent on their translational velocity, preventing independent control of position and orientation [3, 4]. Despite their mechanical simplicity and reduced cost, u-MRAVs face challenges such as tilting during motion or under external forces (e.g., wind), complicating communication-aware trajectory planning [5]. This issue is further exacerbated with high-frequency technologies like terahertz and mmWave, which require precise antenna alignment due to high directivity [2]. Many studies incorrectly assume that u-MRAVs can maintain a stable 3D orientation while in motion, which is only true at low velocities and small pitch/roll angles [3]. Recent works work has explored using servo motors or other mechanisms to adjust antenna orientation on u-MRAVs. For example, [6] proposed a control law for dynamically adjusting antenna orientation, while [7] introduced a full-duplex communication system to manage interference through UAV-relative position control. Another study, [8], used reinforcement learning for antenna alignment. However, these solutions focus on highly directional antennas for one-to-one communication. zUsubscriptùëßUz_{\mathrm{U}}italic_z start_POSTSUBSCRIPT roman_U end_POSTSUBSCRIPTyUsubscriptùë¶Uy_{\mathrm{U}}italic_y start_POSTSUBSCRIPT roman_U end_POSTSUBSCRIPTxUsubscriptùë•Ux_{\mathrm{U}}italic_x start_POSTSUBSCRIPT roman_U end_POSTSUBSCRIPTOUsubscriptùëÇUO_{\mathrm{U}}italic_O start_POSTSUBSCRIPT roman_U end_POSTSUBSCRIPTzWsubscriptùëßWz_{\mathrm{W}}italic_z start_POSTSUBSCRIPT roman_W end_POSTSUBSCRIPTyWsubscriptùë¶Wy_{\mathrm{W}}italic_y start_POSTSUBSCRIPT roman_W end_POSTSUBSCRIPTxWsubscriptùë•Wx_{\mathrm{W}}italic_x start_POSTSUBSCRIPT roman_W end_POSTSUBSCRIPTOWsubscriptùëÇWO_{\mathrm{W}}italic_O start_POSTSUBSCRIPT roman_W end_POSTSUBSCRIPTùê©ùê©\mathbf{p}bold_p Figure 1: Illustration of u-MRAV and o-MRAV configurations with the global and untilted reference frames [9]. Arcs indicate servo rotation for thrust vector adjustment. The robotics community has introduced fully actuated MRAVs and omnidirectional MRAVs (o-MRAVs) [3, 4]. These designs offer more control inputs than DoF, allowing for partial (f-MRAVs) or full (o-MRAVs) independent control over all movement axes. This capability greatly expands their maneuverability and allows precise antenna control throughout the UAV‚Äôs mission. From a communication standpoint, this feature is crucial for optimizing both the 3D position and orientation of the antenna. For instance, o-MRAVs with dipole antennas can maintain communication with multiple nodes simultaneously, providing more flexibility in dynamic environments. Figure 1 presents an illustrative example of both an u-MRAV and an o-MRAV. While o-MRAVs introduce additional complexity due to their design and control requirements, they offer significant advantages in wireless networks by providing highly adaptive aerial platforms [10]. Drawing on the authors‚Äô previous work [10, 2, 11], this paper explores how the independent control of both position and orientation in o-MRAVs can enhance communication systems, particularly in areas like physical layer security and beamforming."
https://arxiv.org/html/2411.01963v1,V-CAS: A Realtime Vehicle Anti Collision System Using Vision Transformer on Multi-Camera Streams,"This paper introduces a robust real-time Vehicle Collision Avoidance System (V-CAS) aimed at enhancing vehicle safety through environmental perception-based adaptive braking. V-CAS utilizes the advanced vision-based transformer model RT-DETR, DeepSORT tracking, speed estimation, brake light detection, and an adaptive braking mechanism. It computes a composite collision risk score from vehicles‚Äô relative accelerations, distances, and detected braking actions, leveraging brake light signals and trajectory data through multiple camera streams for improved scene perception. Implemented on the Jetson Orin Nano, V-CAS enables real-time collision risk assessment and proactive mitigation via adaptive braking. A comprehensive training process was conducted on various datasets for comparative analysis, followed by fine-tuning the selected object detection model using transfer learning. The system‚Äôs effectiveness was rigorously evaluated on the Car Crash Dataset (CCD) from YouTube and through real-time experiments, achieving over 98% accuracy with an average proactive alert time of 1.13 seconds. Results show significant improvements in object detection and tracking, enhancing collision avoidance compared to traditional single-camera methods. This research highlights the potential of low cost, multi-camera embedded vision transformer systems to advance automotive safety through enhanced environmental perception and proactive collision avoidance mechanisms.","The increase in car ownership, driven by economic growth and the desire for convenience, has resulted in a rise in traffic accidents, leading to significant loss of life. Research shows that approximately 77% of these accidents are caused by drivers [1]. This concerning trend underscores the urgent need for intelligent road safety systems that can perceive surrounding traffic objects and prevent collisions. These systems utilize various data sources, including vehicle speed, accelerometers, and video feeds. Recent advancements have seen researchers incorporating Light Detection and Ranging (LiDAR) sensor inputs and monocular camera images to enhance the performance of collision avoidance systems. ADAS can be categorized into two main types: (1) Passive Safety focuses on reducing injuries during a crash through high production safety standards, while (2) Active Safety systems proactively prevent accidents by using sensors such as radar, cameras, and ultrasonic devices to detect potential hazards like nearby vehicles or sudden braking. When a threat is identified, these systems alert the driver with visual or audio warnings or initiate automatic braking to avert collisions. Modern systems often integrate cameras and radars, providing distinct advantages. However, the addition of sensors can increase vehicle costs and design complexity. To address this, researchers are investigating computer vision insights, particularly in object detection (OD) techniques that utilize either depth-based or camera-based sensors. The proposed system utilizes spatial feature extraction from RGB feeds captured by three cameras, facilitating enhanced scene interpretation and a broader field of view (FOV). It integrates object detection and tracking algorithms to predict collision scores based on relative motion, all executed efficiently in real-time on edge devices like the Jetson Orin Nano. This method promises a more robust and computationally efficient recognition of surrounding traffic objects. The structure of this paper is as follows: Section II reviews related work in the field, Section III details the methodology of the proposed model, and Sections IV and V present the experimentation results and conclusions, along with future directions for research."
https://arxiv.org/html/2411.01919v1,Real-Time Polygonal Semantic Mapping for Humanoid Robot Stair Climbing,"We present a novel algorithm for real-time planar semantic mapping tailored for humanoid robots navigating complex terrains such as staircases. Our method is adaptable to any odometry input and leverages GPU-accelerated processes for planar extraction, enabling the rapid generation of globally consistent semantic maps. We utilize an anisotropic diffusion filter on depth images to effectively minimize noise from gradient jumps while preserving essential edge details, enhancing normal vector images‚Äô accuracy and smoothness. Both the anisotropic diffusion and the RANSAC-based plane extraction processes are optimized for parallel processing on GPUs, significantly enhancing computational efficiency. Our approach achieves real-time performance, processing single frames at rates exceeding 30‚Å¢H‚Å¢z30ùêªùëß30~{}Hz30 italic_H italic_z, which facilitates detailed plane extraction and map management swiftly and efficiently. Extensive testing underscores the algorithm‚Äôs capabilities in real-time scenarios and demonstrates its practical application in humanoid robot gait planning, significantly improving its ability to navigate dynamic environments.","The capability to perceive the environment accurately and construct effective traversable semantic maps is crucial for humanoid robots navigating complex environments. In particular, for humanoid robots, securing stable footholds in sufficiently large planar areas is essential for safety and accuracy in movement planning. Due to their greater degrees of freedom, humanoid robots can undertake more complex tasks, such as jumping and climbing stairs, compared to traditional wheeled robots. However, these complex movements are based on the robot‚Äôs ability to precisely and efficiently sense its surroundings and build reliable semantic maps. Hence, enhancing planar semantic mapping for humanoid robots is paramount and warrants further research and discussion. The construction of reliable, traversable planar semantic maps for humanoid robots has long captured the attention of multiple disciplines including computer vision and SLAM (Simultaneous Localization and Mapping). In the domain of computer vision, numerous effective algorithms have been proposed to enhance performance metrics such as real-time responsiveness and Intersection over Union (IOU) by focusing on extracting planes from ordered structures like depth images, which generally perform better in real-time and accuracy than methods based on unordered point clouds [1]. Additionally, the field of SLAM has utilized planar and curved spatial semantic information for robot localization and mapping [2, 3], with several successful projects now open-sourced, such as elevation mapping with GPU [4]. Figure 1: Planar polygon semantic mapping results of spiral and straight stairs Figure 2: Overview of the Planar Polygonal Semantic Mapping System Framework. The system inputs are depth images and robot pose estimates, which are processed to generate a terrain‚Äôs polygonal planar semantic map. This map can be directly utilized to plan humanoid robot gaits. However, current methods often overlook the impact of filtering techniques on extracting planes from raw data, which can lead to significant discrepancies in performance between noise-free simulated data and real depth camera data. While results showing an IOU of over 95% in simulations drop below 80% with real sensor data may be acceptable for quadruped robots, the requirement for larger stable areas for humanoid robots makes resolving this issue indispensable. The accuracy of the extracted plane results, including the planar normal vectors and centroid heights, is crucial as it directly influences the safety of the robot during its operation. Moreover, this thesis also considers real-time performance as a critical metric, placing computationally intensive and iterative image processing algorithms on the GPU to ensure that the processing time for each frame remains below the sensor cycle time. Issues such as dynamic obstacles and odometric drift also need to be addressed in map construction. Our previous work [5] proposed using supervoxel-based plane segmentation for humanoid robot stairs. Supervoxel is a point cloud clustering method that is time-consuming. Thus, in this paper, we propose a system that performs real-time extraction of planar polygons and incremental semantic map management. This system first applies anisotropic diffusion filtering [6] to significantly suppress noise in depth images, thereby enhancing the quality of depth images. It then uses edge detection algorithms to extract contours of planes from transformed normal vector images and simplifies these contours into polygons. Subsequently, the RANSAC algorithm fits the optimal plane equation for each polygonal region from the depth images. This process yields the results of polygonal plane extraction from single frames. To construct a global polygonal semantic map, the system integrates robot pose estimates obtained from odometry/SLAM, allowing newly observed polygons to be added to the map. During the polygon merging process, a simple method is employed to estimate and compensate for odometric drift in the vertical direction, ensuring more precise mapping results. Fig. 1 illustrates the mapping results for different staircases. Building on this, we have also integrated the humanoid robot‚Äôs gait planning algorithm with our mapping system, as shown in Fig. 2. The code can be accessed at https://github.com/BTFrontier/polygon_mapping."
https://arxiv.org/html/2411.01909v1,Traffic and Safety Rule Compliance of Humans in Diverse Driving Situations,"The increasing interest in autonomous driving systems has highlighted the need for an in-depth analysis of human driving behavior in diverse scenarios. Analyzing human data is crucial for developing autonomous systems that can replicate safe driving practices and ensure seamless integration into human-dominated environments. This paper presents a comparative evaluation of human compliance with traffic and safety rules across multiple trajectory prediction datasets, including Argoverse 2, nuPlan, Lyft, and DeepUrban. By defining and leveraging existing safety and behavior-related metrics such as time to collision, adherence to speed limits, and interactions with other traffic participants, we aim to provide a comprehensive understanding of each dataset‚Äôs strengths and limitations. Our analysis focuses on the distribution of data samples, identifying noise, outliers, and undesirable behaviors exhibited by human drivers in both the training and validation sets. The results underscore the need for applying robust filtering techniques to certain datasets due to high levels of noise and the presence of such undesirable behaviors.","In recent years, autonomous vehicles (AVs) have gained significant attention due to their potential to reduce traffic fatalities. The widespread adoption of AV technology is contingent not only on technical performance but also on public trust, with concerns centering on safety and potential technological malfunctions [1, 2]. A key factor in improving trust in autonomous systems is the ability to understand and replicate human driving behavior. However, worldwide, road accidents cause over 1.19 million deaths annually, with a majority resulting from human error [3], hence following human driving pattern is not always desired. Since the majority of accidents are caused by human error, analyzing human driving data allows us to identify common mistakes and undesirable driving patterns. This understanding is crucial for training machine learning models, such as those used in behavior cloning, where the goal is to mimic human driving behavior. Identifying undesirable driving patterns is especially useful for achieving a defensive driving behavior, which is proven to play a significant role in increasing passenger comfort and trust in AVs [4]. In summary, our main contributions are: ‚Ä¢ Review existing criticality measures and safety violations and define new relevant metrics ‚Ä¢ Provide comprehensive overview of dataset characteristics ‚Ä¢ Examine critical behavior across datasets and compare findings"
https://arxiv.org/html/2411.01866v1,Improving Trust Estimation in Human-Robot Collaboration Using Beta Reputation at Fine-grained Timescales,"When interacting with each other, humans adjust their behavior based on perceived trust. However, to achieve similar adaptability, robots must accurately estimate human trust at sufficiently granular timescales during the human-robot collaboration task. A beta reputation is a popular way to formalize a mathematical estimation of human trust. However, it relies on binary performance, which updates trust estimations only after each task concludes. Additionally, manually crafting a reward function is the usual method of building a performance indicator, which is labor-intensive and time-consuming. These limitations prevent efficiently capturing continuous changes in trust at more granular timescales throughout the collaboration task. Therefore, this paper presents a new framework for the estimation of human trust using a beta reputation at fine-grained timescales. To achieve granularity in beta reputation, we utilize continuous reward values to update trust estimations at each timestep of a task. We construct a continuous reward function using maximum entropy optimization to eliminate the need for the laborious specification of a performance indicator. The proposed framework improves trust estimations by increasing accuracy, eliminating the need for manually crafting a reward function, and advancing toward developing more intelligent robots. The source code is publicly available. 111Source code and media materials are available at https://github.com/resuldagdanov/robot-learning-human-trust","Human decisions are often influenced by their perceptions of how trustworthy they are perceived by others [1, 2]. Research in human-robot collaboration (HRC) indicates that when robots act in accordance with a human co-worker‚Äôs trust, collaboration effectiveness is enhanced [3, 4, 5]. However, to make trust-aware decisions, robots need to accurately estimate how much their co-worker trusts them [6, 7]. Trust in a robot can change throughout a task, making it essential for the robot to estimate trust in real-time at fine-grained timescales. By continuously estimating trust during the task rather than only at its conclusion, the robot can adapt its behavior immediately, either enhancing or reducing trust to address the pitfalls of overtrust or undertrust [8, 9, 10]. There is growing HRC research interest in computational models to estimate human trust toward robots [11, 12, 13, 14]. These models are based on robot performance, which is the most significant factor influencing human trust [15, 16, 10]. Furthermore, probabilistic models that capture uncertainty and bias in human subjectivity show great promise in this context [17, 18, 19, 20]. Consequently, the proposed framework in this paper entails a probabilistic estimation of human trust based on robot performance, as illustrated in Fig. LABEL:fig:_Teaser. The probabilistic models proposed in [17, 18, 19, 20] fail to capture the continuous changes in human trust as a robot performs a task. This limitation arises because human co-workers assess performance in a binary manner (e.g., success or failure) only after task completion, neglecting performance changes during the task. This results in a static estimation of trust dynamics, often referred to as a ‚Äúsnapshot‚Äù view [19]. An intelligent robot needs to adjust its behavior in real-time in response to changes in human trust in order to address the pitfalls of overtrust and undertrust. For instance, trust may shift during a collision avoidance task if the robot navigates too close to obstacles, raising concerns about its reliability and safety. Capturing these trust dynamics in real-time is essential because it could enable the robot to adapt its behavior immediately rather than wait until the task is complete. In this example, the robot could deliberately navigate around the obstacle to prevent further deterioration of trust. The robot could account for performance and trust-related objectives if it estimates trust at fine-grained timescales. Formulating a task-specific performance function that accounts for the objective aspects of robotic tasks is labor-intensive, time-consuming, and requires a deep understanding of the task [21, 22, 23]. This process involves determining appropriate weights for the factors influencing task objectives and aligning them with desired outcomes. This limits the autonomy and adaptability of the robot to various tasks and highlights the need for a framework that enables fine-grained estimation of human trust, facilitating real-time trust-aware robot decision-making with minimal labor-intensive effort. We propose a new framework for the accurate estimation of human trust at granular timescales, as visualized in Fig. LABEL:fig:_GeneralFramework. We construct a continuous reward function using maximum entropy optimization, which enables us to efficiently capture the underlying performance dynamics throughout the task. Section II provides a brief literature review on human trust modeling. Section III presents a mathematical background of the problem. Section IV details an implementation of the proposed framework. Section V outlines the experimental evaluations. Finally, Section VI concludes this paper."
https://arxiv.org/html/2411.01816v1,Toward Integrating Semantic-aware Path Planning and Reliable Localization for UAV Operations,"Localization is one of the most crucial tasks for Unmanned Aerial Vehicle systems (UAVs) directly impacting overall performance, which can be achieved with various sensors and applied to numerous tasks related to search and rescue operations, object tracking, construction, etc. However, due to the negative effects of challenging environments, UAVs may lose signals for localization. In this paper, we present an effective path-planning system leveraging semantic segmentation information to navigate around texture-less and problematic areas like lakes, oceans, and high-rise buildings using a monocular camera. We introduce a real-time semantic segmentation architecture and a novel keyframe decision pipeline to optimize image inputs based on pixel distribution, reducing processing time. A hierarchical planner based on the Dynamic Window Approach (DWA) algorithm, integrated with a cost map, is designed to facilitate efficient path planning. The system is implemented in a photo-realistic simulation environment using Unity, aligning with segmentation model parameters. Comprehensive qualitative and quantitative evaluations validate the effectiveness of our approach, showing significant improvements in the reliability and efficiency of UAV localization in challenging environments.","In recent years, unmanned aerial vehicles (UAVs) have emerged as a significant research field and a top priority in the robotics industry, including tunnel navigation [1], surveillance, search operations [2], terrain mapping [3], disaster relief and accident response [4]. UAVs offer several advantages, such as simple structure and flexible flight capabilities. One of the key challenges in deploying UAVs effectively is ensuring accurate localization and navigation, particularly in complex and dynamic environments where traditional sensors like GPS and Inertial Measurement Units (IMUs) may be unreliable and unavailable. For example, IMUs often deliver suboptimal results for UAVs due to their difficulty adapting to environmental factors such as wind and air resistance. Similarly, GPS signals can be disrupted in areas with tall buildings or dense forests. The ability of UAVs to autonomously navigate and localize is therefore crucial to their operational success and safety. As UAV applications become more widespread, ensuring the stability of UAV self-positioning has emerged as an important concern [5]. However, both real-time localization and mapping remain unresolved issues, because commonly used sensors can fail under adverse weather conditions [6], [7], or in the presence of mountains, tall buildings or water [8]. In these scenarios, UAVs require reliable methods to prevent localization system failures and determine optimal flight paths to fly toward their destination. Visual Simultaneous Localization and Mapping (V-SLAM) [9], [10] [11] and multi-sensor fusion [12] have emerged as potential solutions to address this challenge. However, these methods face significant performance issues, such as reduced robustness and accuracy, which are greatly affected by navigation environmental conditions. Typically, V-SLAM‚Äôs accuracy degrades significantly in the presence of dynamic objects and areas lacking texture or with specular surfaces (e.g. oceans and lakes). Therefore, UAV localization and navigation systems often struggle in unstructured environments due to insufficient feedback information. Consequently, integrating spatial awareness capabilities could open up the potential to improve the accuracy of UAV localization and navigation systems. On the other hand, semantic segmentation models [13] [14] [15] have shown promising performance using RGB images from monocular cameras, paving the way toward integrating semantic segmentation for UAVs. Current research focuses on segmenting environmental objects, particularly utilizing 3D reconstruction to recover shapes of occluded or partially visible dynamic objects. By leveraging semantic awareness, UAVs can detect and respond to environmental factors such as terrain, moving objects, and weather conditions. This enhances self-protection and collision avoidance capabilities, while improving navigation and localization accuracy and performance. Additionally, linking perception with SLAM [16] [17] to integrate semantic information, create a semantic map, and enhance localization performance has shown the potential to address the problem of active perception. However, these methods are hindered by the high computational complexity, therefore the implementation in outdoor environments is challenging. The most similar work to ours was committed by Bartolomei et al. [18], proposing a method that applies semantic segmentation to enhance the localization quality of UAVs by designing a perception-aware navigation system based on the VTNet model, along with the A* Kinodynamic and B spline. In this paper, we proposed a reliable localization system for UAVs based on semantic segmentation, which integrates semantic segmentation information into a UAV path-planning framework to evaluate the quality of candidate areas for localization systems. The main contributions of this work are summarized as follows: ‚Ä¢ A semantic-aware localization system for UAVs in challenging environments. ‚Ä¢ A hierarchical planner that integrates the Dynamic Window Approach (DWA) algorithm with a cost map. ‚Ä¢ Demonstration of the performances of our proposed system in active perception through photo-realistic simulations. The remainder of this paper is organized as follows: Section 2 presents our proposed system based on the semantic segmentation and path planning algorithm. The experiments conducted and the analysis of the results are detailed in Section 3. Finally, Section 4 concludes the paper with discussion of future work."
https://arxiv.org/html/2411.01814v1,Enhancing Social Robot Navigation with Integrated Motion Prediction and Trajectory Planning in Dynamic Human Environments,"Navigating safely in dynamic human environments is crucial for mobile service robots, and social navigation is a key aspect of this process. In this paper, we proposed an integrative approach that combines motion prediction and trajectory planning to enable safe and socially-aware robot navigation. The main idea of the proposed method is to leverage the advantages of Socially Acceptable trajectory prediction and Timed Elastic Band (TEB) by incorporating human interactive information including position, orientation, and motion into the objective function of the TEB algorithms. In addition, we designed social constraints to ensure the safety of robot navigation. The proposed system is evaluated through physical simulation using both quantitative and qualitative metrics, demonstrating its superior performance in avoiding human and dynamic obstacles, thereby ensuring safe navigation. The implementations are open source at: https://github.com/thanhnguyencanh/SGan-TEB.git","As the development of social capabilities in mobile service robots advances, effective navigation becomes crucial to autonomously performing tasks in dynamic unknown environments such as airports [1], museums [2], and offices [3]. The primary objective of these systems is to ensure that robot navigation is performed in a socially acceptable manner, prioritizing human safety and comfort [4]. However, in almost all scenarios, robots often face challenges in deciding how to proceed toward their intended destination unless an obstacle moves out of the way. Several previous works have focused on designing obstacle avoidance algorithms and motion safety methods. These include artificial potential field [5], dynamic window approach (DWA) [6], vector field histogram [7], velocity obstacles [8], time elastic band (TEB) [9], randomized kinodynamic planning [10], reciprocal velocity obstacles [11], and inevitable collision states [12]. Additionally, multi-sensors fusion methods [13], using camera and lidar sensors, have shown potential for effectively avoiding static and dynamic obstacles of various shapes and sizes in different environments. While these techniques have successfully avoided obstacles in dynamic environments, they often lack the human characteristics and social constraints necessary for friendly and safe navigation. To ensure the ability of navigation, Simultaneous Localization and Mapping [14], [15] approach is leveraged to localize and construct the surrounding environment. This approach provides the current robot pose and pre-estimated map, supporting navigation and obstacle avoidance tasks. However, human or dynamic obstacles cannot be defined on the static map, requiring a local planning algorithm capable of predicting their future motion during the robot‚Äôs movement. On the other hand, computer vision and machine learning for robot navigation have inspired a significant amount of research over the past three decades [16]. By investigating motion behavior prediction, mobile robots can effectively manage social interaction. Although existing human-aware robot navigation systems [17], [18] based on the social force model have been developed and verified in the real world environment, achieving considerable success. However, these systems typically only consider human state information such as human position, orientation, and velocity. In particular, interaction-aware planning has been advanced through the use of Partially Observable Markov Decision Processes [19] and Model Predictive Control [20] with safety constraints. These methods provide robust frameworks for decision-making in uncertain environments by anticipating future states and optimizing control actions over a predicted horizon. Despite the strengths of these methods, there remains a gap in integrating these with real-time motion prediction and social adaptability in densely populated environments. To address these challenges, this paper proposes an effective approach by integrating motion prediction and trajectory planning for social robot navigation in dynamic human environments. We consider several conditions concerning social constraints and kinodynamic constraints. The main contributions of this work are summarized as follows: ‚Ä¢ An effective system for social robot navigation in dynamic human environments. ‚Ä¢ Construction of a novel objective function for local planners based on social constraints. ‚Ä¢ Investigation of human motion prediction based on Generative Adversarial Networks (GAN). ‚Ä¢ Demonstration of our proposed system‚Äôs performance in both quantitative and qualitative metrics. The rest of this paper is organized as follows: Section 2 introduces our proposed system, focusing on human identification, motion estimation, and trajectory planning. Experimental results using both quantitative and qualitative metrics are shown in Section 3. Finally, Section 4 presents the conclusions of our system. Figure 1: Overview of our proposed method. The system is composed of two main units: Socially Aware and Social Trajectory Planning"
https://arxiv.org/html/2411.01813v1,So You Think You Can Scale UpAutonomous Robot Data Collection?,"A long-standing goal in robot learning is to develop methods for robots to acquire new skills autonomously. While reinforcement learning (RL) comes with the promise of enabling autonomous data collection, it remains challenging to scale in the real-world partly due to the significant effort required for environment design and instrumentation, including the need for designing reset functions or accurate success detectors. On the other hand, imitation learning (IL) methods require little to no environment design effort, but instead require significant human supervision in the form of collected demonstrations. To address these shortcomings, recent works in autonomous IL start with an initial seed dataset of human demonstrations that an autonomous policy can bootstrap from. While autonomous IL approaches come with the promise of addressing the challenges of autonomous RL‚Äîenvironment design challenges‚Äîas well as the challenges of pure IL strategies‚Äîextensive human supervision‚Äîin this work, we posit that such techniques do not deliver on this promise and are still unable to scale up autonomous data collection in the real world. Through a series of targeted real-world experiments, we demonstrate that these approaches, when scaled up to realistic settings, face much of the same scaling challenges as prior attempts in RL in terms of environment design. Further, we perform a rigorous study of various autonomous IL methods across different data scales and 7 simulation and real-world tasks, and demonstrate that while autonomous data collection can modestly improve performance (on the order of 10%), simply collecting more human data often provides significantly more improvement. Our work suggests a negative result: that scaling up autonomous data collection for learning robot policies for real-world tasks is more challenging and impractical than what is suggested in prior work. We hope these insights about the core challenges of scaling up data collection help inform future efforts in autonomous learning.","Enabling robots to acquire skills in the wild from autonomous, self-supervised interaction has been a long-standing goal in robot learning. To this end, a variety of efforts have focused on developing methods for reinforcement learning (RL) in the real-world [1, 2, 3]. Despite substantial progress, RL for real-world robotics requires a significant amount of human effort on environment design, such as developing reset mechanisms, safe guards, success detectors, and reward functions. These challenges‚Äîexacerbated by sample efficiency issues‚Äîhave constrained the complexity of tasks that are possible with today‚Äôs methods for real-world RL. As a consequence, many have shifted their attention to imitation learning (IL) methods, which scale much better with task complexity [4, 5]. However, IL methods rely on increasingly large amounts of high-quality human demonstrations as tasks become more diverse and complex, thus shifting the human effort required to human supervision‚Äîi.e., demands on the time of expert operators. In fact, from ‚Äúpure autonomous‚Äù RL methods to ‚Äúpure human‚Äù IL methods, there exists a spectrum that trades off between environment design effort and human supervision effort. We visualize this spectrum in Fig. 1, where one might expect that by moving from either side towards the middle of this spectrum, the human effort in both environment design and supervision can go down. A variety of works have attempted to move toward the middle of this spectrum, either by reducing environment design challenges (often increasing supervision requirements) for RL methods [6, 7, 8, 9, 10] or decreasing supervision requirements (often increasing environment assumptions) for IL methods [11, 12, 13, 14, 15, 16, 17, 18, 19]. Our hope as a field is that somewhere in the middle lies an effort-minimizing approach that will enable robot learning methods to effectively scale. Figure 1: Conceptual diagram illustrating the expected vs. actual effort tradeoff for human supervision and environment design. One proposed middle-ground approach is autonomous IL, where we let a policy autonomously collect its own data using a policy trained on an initial amount of human data and iteratively re-train with the successful rollouts [19, 17, 18]. Many hope that this approach will finally push us to the bottom of the ‚ÄúU‚Äù in Fig. 1 (solid line), since the promise of autonomous IL is to reduce human data collection effort while partially mitigating safety and exploration issues. In this work, we examine the challenges of applying autonomous IL to useful and realistic manipulation tasks in the real world‚Äîbeyond what is often shown in simpler toy settings. We find that in practice this middle ground approach unfortunately suffers from many of the same scaling challenges as prior work. We observe that the true effort curve looks more like Fig. 1 (dashed line): as we try to reduce environment design effort or reduce human supervision collection effort, we find that the total human effort required to see comparable success rates plateaus. Our study is organized into two parts. First, in Section 3, we demonstrate that autonomous IL methods still suffer from high environment design costs‚Äîfor example, reset functions and success detection. In practice, these costs limit the complexity of tasks that can be tackled with autonomous IL. Second, in Section 4, we select 7 simulation and real-world tasks where environment design costs can be minimized, and through 10K+ real-world evaluations and over 100 hours of autonomous data collection, we rigorously evaluate a range of autonomous IL methods. While several methods lead to mild performance improvements (on the order of 10%) on top of an IL policy trained on the initial human demonstrations, we consistently find that collecting a few more human demonstrations surprisingly is a more efficient use of total effort. Our work suggests a negative result: that scaling up autonomous IL for real-world tasks might be much more challenging than what is conceived by the field and what prior work suggest, given that these methods still require significant environment design effort and underperform simply redirecting this effort to collecting demonstrations. This work sheds light on the true bottlenecks of scaling up data collection, such as finding generalizable solutions to environment challenges and developing methods to scale up human supervision."
https://arxiv.org/html/2411.01804v1,Semantic Masking and Visual Feature Matching for Robust Localization,"We are interested in long-term deployments of autonomous robots to aid astronauts with maintenance and monitoring operations in settings such as the International Space Station. Unfortunately, such environments tend to be highly dynamic and unstructured, and their frequent reconfiguration poses a challenge for robust long-term localization of robots. Many state-of-the-art visual feature-based localization algorithms are not robust towards spatial scene changes, and SLAM algorithms, while promising, cannot run within the low-compute budget available to space robots. To address this gap, we present a computationally efficient semantic masking approach for visual feature matching that improves the accuracy and robustness of visual localization systems during long-term deployment in changing environments. Our method introduces a lightweight check that enforces matches to be within long-term static objects and have consistent semantic classes. We evaluate this approach using both map-based relocalization and relative pose estimation and show that it improves Absolute Trajectory Error (ATE) and correct match ratios on the publicly available Astrobee dataset. While this approach was originally developed for microgravity robotic freeflyers, it can be applied to any visual feature matching pipeline to improve robustness.","I INTRODUCTION Accurate and robust localization is required for reliable long-term robot autonomy. In environments with dynamic or movable objects, place recognition can be challenging as scene consistency is often assumed. The International Space Station (ISS) is an example of such an environment, and the Astrobee robots [1] operating onboard face constant changes as objects such as cargo bags, wires, laptops, and racks are introduced or rearranged as displayed in Fig. 2. Increasing map matching robustness in the presence of environmental differences would enable more lifelong autonomy for these and other robots. Localization for the Astrobee robots is made possible by a specialized system which can handle the microgravity, constricted modules and planar, repeated scenes of the ISS. As the Astrobee is limited by compute, maps must be pre-built offline. The remote nature of the ISS makes it difficult to remap frequently enough to capture changes, so there are often discrepancies between the map and deployment environment. Additional challenges of the ISS, such as the limited space to move in, planar scenes, and monocular camera images, cause many state-of-the-art visual feature-matching approaches, including ORBSLAM3[2], to fail. The lack gravity and noisy IMU data also preclude other well-known localization systems, such as MAPLAB 2.0 [3] which has ingrained assumptions about gravity. On top of this, these approaches (along with other more recent and robust algorithms) are too computationally intensive to run on the Astrobee, whose compute platform [1] is roughly 10 times slower than an Intel i9-9980HK 2.4 GHz CPU, and of which only a single core is available for the graph-based localizer. Figure 1: feature matching with and without bounding boxes. Horizontal image pairs taken several years apart display multiple scene changes, including a rotated ISS flag that causes faulty associations and a failed relative pose estimate in the top image pair. With semantic masks applied to the matches (bottom image pair), detections of stable scene elements including vents (purple), lights (blue), and handrails (red) enable the pruning of faulty associations due to environment changes and successful relative pose estimation. We are therefore interested in methods which are: 1) Computationally inexpensive, 2) Use visual features and are robust to scene changes, and 3) Can be easily added into an existing visual localization framework for ease of integration. Bounding box-based semantic segmentation can be run relatively efficiently and provides object level understanding of a visual scene [4]. Semantic segmentation generates object classes that can be used to prune dynamic or unstable objects [5] and can improve resiliancy to scene changes by detecting stable, static classes and removing those likely to change over time. To take advantage of the accuracy of feature-based matches and robustness of using semantics, we present a meta-algorithm that enhances visual feature matching for mapping and localization. Our contributions include: ‚Ä¢ A semantic masking stage applied to visual feature matching that enforces class consistency between matches using efficient bounding box detections. This approach can be used with any visual SLAM or localization algorithm to improve robustness to scene changes. ‚Ä¢ An evaluation using the publicly available Astrobee ISS dataset [6] demonstrating increased accuracy and robustness for both map-based pose estimates and relative correspondences in image pairs. Figure 2: Astrobee free-flying robots roaming the ISS during an activity. Background objects such as laptops, wires, and cargo bags are often moved between flights and can cause localization errors for the robots."
https://arxiv.org/html/2411.01775v1,Environment Curriculum Generationvia Large Language Models,"Recent work has demonstrated that a promising strategy for teaching robots a wide range of complex skills is by training them on a curriculum of progressively more challenging environments. However, developing an effective curriculum of environment distributions currently requires significant expertise, which must be repeated for every new domain. Our key insight is that environments are often naturally represented as code. Thus, we probe whether effective environment curriculum design can be achieved and automated via code generation by large language models (LLM). In this paper, we introduce Eurekaverse, an unsupervised environment design algorithm that uses LLMs to sample progressively more challenging, diverse, and learnable environments for skill training. We validate Eurekaverse‚Äôs effectiveness in the domain of quadrupedal parkour learning, in which a quadruped robot must traverse through a variety of obstacle courses. The automatic curriculum designed by Eurekaverse enables gradual learning of complex parkour skills in simulation and can successfully transfer to the real-world, outperforming manual training courses designed by humans.","It is often hypothesized that the evolution of intelligent motor control in biology is driven by the need to habituate in and adapt to varied environment conditions [1, 2]. Inspired by this observation, training robot policies over a curriculum of environments has been shown to be an effective strategy for the acquisition of complex robot skills [3, 4, 5, 6, 7]. Designing a useful curriculum of environments is, however, time-consuming and requires domain-specific knowledge to work well [8]. In limited cases, when the environment variations can be succinctly represented by scalar parameters [9, 10], unsupervised environment design methods have shown capability in automatically generating progressively more challenging tasks in low-dimensional simulation domains [9, 11, 12]. However, whether these methods can scale up to challenging real-world robotic tasks is an open question. Our key insight is that, in many practical robotics scenarios, environment variations can be represented by diverse programs. This allows us to solve environment design using large language models (LLMs) since they have been shown to excel at code generation in diverse domains [13, 14, 15, 16]. We introduce Eurekaverse, a language model powered environment curriculum design algorithm that can generate and evolve environments in code space. At a technical level, Eurekaverse instructs an LLM to generate novel environments to teach the target task, trains policies for each environment, and evaluates the best-performing policies to inform the LLM when it generates the next round of environments. Given that large language models (LLMs) have exhibited strong code generation and self-improving capabilities on other domains [17, 18, 19], we hypothesize that using LLMs for environment curriculum design could also enable learning complex robot skills. In particular, we validate Eurekaverse‚Äôs efficacy on the task of quadrupedal parkour [20, 21], in which a quadruped robot is tasked with traversing diverse and unknown challenging courses with varied terrains and obstacles such as gaps, hurdles, boxes, and ramps. In order to train a policy deployable in the real world, prior parkour methods carefully designed training courses in simulation that enable real-world policy transfer [21, 20]. Because parkour courses are represented as programs that specify detailed course terrain and geometry (see Fig. 3), we posit that parkour is an ideal testbed for testing Eurekaverse‚Äôs capability in handling the complexity of evolving environment programs. Parkour involves a rich variety of challenging locomotion skills so our results could also be of independent interest for scaling up locomotion learning. We demonstrate that the automatically generated curriculum of environments from Eurekaverse leads to policy training that continuously improves over time without plateauing; in contrast, baselines that only train over a fixed set of terrains, designed by either humans or an LLM, tend to overfit on these training terrains, resulting in worse generalization to test simulation and real-world courses. Specifically, on a set of 20 held-out simulation test courses that are carefully designed to holistically evaluate robot parkour skills, Eurekaverse-trained policies outperforms various baselines and ablations. Furthermore, on several real-world courses, Eurekaverse-trained policies successfully transfer and exhibit more robust and adaptive behavior than policies trained using limited or manually designed training courses. In summary, our contributions are: 1. Eurekaverse, an LLM-based unsupervised environment design algorithm that can automatically generate curriculums of environment programs for robot skill learning. 2. Extensive simulation and real-world validation of Eurekaverse on quadrupedal robot parkour."
https://arxiv.org/html/2411.01707v1,Grid-based Multi-Robot Coverage Path Planning,"Coverage Path Planning (CPP) addresses the challenge of planning paths to completely cover a designated workspace and is crucial for various robotic applications, including indoor cleaning, inspection, and outdoor tasks such as automated harvesting and environmental monitoring. Multi-Robot Coverage Path Planning (MCPP) extends this by coordinating multiple robots to enhance coverage efficiency and system robustness. This paper focuses on the grid-based MCPP problem, where the workspace is represented as a four-way connected 2D grid graph. Unlike traditional approaches that rely on the k-Traveling Salesman Problem (k-TSP) or Spanning Tree Coverage (STC), we reformulate MCPP directly on the edge-weighted graph to be covered and incorporate inter-robot conflicts and turning costs, reflecting more practical considerations in real-world applications. We propose several key improvements over our previous work: a reformulated Extended-STC (ESTC) with local optimizations, a refined local search framework (LS-MCPP), and an adaptive multi-goal planning approach combined with Priority-Based Search (PBS) for conflict resolution. Our proposed pipeline scales efficiently to large grid graphs (up to 256√ó256256256256\times 256256 √ó 256) and up to 100 robots, demonstrating superior solution quality and success rates within about 18 minutes. We also validate our approach through a multi-robot system deployment, showcasing its practical effectiveness in real-world scenarios.","Coverage Path Planning (CPP) addresses the problem of determining a path that fully covers a designated workspace [galceran2013survey]. This problem has been extensively studied for a variety of robotic applications, ranging from indoor tasks like vacuum cleaning [wang2024apf] and inspection [almadhoun2018coverage], to outdoor activities such as automated harvesting [chen2024optimizing], planetary exploration [santra2024risk], and environmental monitoring [sudha2024coverage]. Multi-Robot Coverage Path Planning (MCPP) extends traditional CPP by coordinating the paths of multiple robots to collectively cover a workspace, thereby enhancing both the efficiency of the coverage task [tang2022learning] and the robustness of the overall system [sun2021ft]. Developing efficient and robust MCPP algorithms is crucial for the widespread deployment of multi-robot systems across various real-world applications. In this paper, we focus on a specific MCPP formulation known as grid-based MCPP, where the workspace is represented as a four-way connected 2D grid graph. The objective is for a team of robots to jointly cover all vertices of the graph, a problem analogous to the NP-hard k-Traveling Salesman Problem (k-TSP)[francca1995m]. However, solving k-TSP in the context of grid-based MCPP is challenging due to the large number of robots and the extensive graph size, which often comprises tens of thousands of vertices. Consequently, existing grid-based MCPP methods leverage the grid structure to find solutions that are constrained to the four-way connected grid space. Specifically, Spanning Tree Coverage (STC)[gabriely2001spanning] is used to generate circumnavigating coverage paths for individual robots, which sequentially follow one side of the spanning edges on a hypergraph (also known as the terrain graph) of the decomposed grid. The MCPP is then solved either by segmenting the STC path and distributing the segments among the robots[hazon2005redundancy, tang2021mstc], or by identifying the covering subtrees and generating separate STC paths for each robot [zheng2007robot, zheng2010multirobot]. For simplicity, the prefix ‚Äùgrid-based‚Äù is omitted throughout the paper when referring to MCPP unless specified otherwise. Figure 1: Simulation of 100 robots cooperatively performing a coverage task on the 256√ó256256256256\times 256256 √ó 256 grid map NewYork1 [sturtevant2012benchmarks] without conflicts using our proposed planning pipeline, which reduces task completion time (i.e., makespan) by 42% compared to the baseline and resolves 556 conflicts within 18 minutes. Left: Task progress at one-third completion. Right: Task is completed. We reformulate MCPP directly on the edge-weighted graph to be covered, explicitly considering inter-robot conflicts (i.e., collisions) to enable broader applicability in real-world robotics scenarios. The edge weights can represent various meaningful metrics, such as traversal costs on uneven terrains [qiu2024terrain] or the importance of task-specific edges that may require additional traversal time [freda20193d]. Additionally, we account for turning costs when evaluating path costs on the graph ‚Äî‚Äì a critical factor in real-world coverage applications [vandermeulen2019turn, lu2022tmstc, ramesh2022optimal], as it demands additional execution time for robots. This problem setup is significantly more challenging than in the existing literature, where traversal costs are defined on hypergraph edges without considering turning costs and inter-robot conflicts. To address these challenges, we introduce several key improvements over our previous work [tang2024large], which initially computes a set of covering subgraphs for the target graph and subsequently generates a single-robot circumnavigating path for each subgraph, ensuring that each robot efficiently covers its assigned region. First, we describe the multi-robot system and formulate the problem in Sec.III. Next, we enhance the Extended-STC (ESTC) method by encoding cost differences between circumnavigating paths based on whether each hypergraph edge is used, and introduce two local optimizations (Sec.IV). With ESTC, we enhance our local search framework (LS-MCPP) to iteratively improve an initial solution. This refinement incorporates additional vertex-wise operators that facilitate better exploration of the solution space while initially ignoring inter-robot conflicts. (Sec.V). Finally, we propose an adaptive approach as a low-level multi-goal planner for each robot, combined with Priority-Based Search (PBS)[ma2019searching] as a high-level planner to resolve pairwise conflicts, together efficiently deconflicting the LS-MCPP solution (Sec.VI). Our pipeline efficiently scales to instances involving up to 256√ó256256256256\times 256256 √ó 256 grid graphs and 100 robots, achieving superior solution quality and success rates compared to existing approaches within a runtime of approximately 15 minutes (Sec.VII). Moreover, we deploy a multi-robot system to demonstrate the effectiveness of our pipeline in real-world robotics applications (Sec. VII-E)."
https://arxiv.org/html/2411.01665v1,Neural Inverse Source Problems,"Reconstructing unknown external source functions is an important perception capability for a large range of robotics domains including manipulation, aerial, and underwater robotics. In this work, we propose a Physics-Informed Neural Network (PINN [1]) based approach for solving the inverse source problems in robotics, jointly identifying unknown source functions and the complete state of a system given partial and noisy observations. Our approach demonstrates several advantages over prior works (Finite Element Methods (FEM) and data-driven approaches): it offers flexibility in integrating diverse constraints and boundary conditions; eliminates the need for complex discretizations (e.g., meshing); easily accommodates gradients from real measurements; and does not limit performance based on the diversity and quality of training data. We validate our method across three simulation and real-world scenarios involving up to 4th order partial differential equations (PDEs), constraints such as Signorini and Dirichlet, and various regression losses including Chamfer distance and L2 norm.","We are interested in differential equations gùëîgitalic_g with an unknown external source fùëìfitalic_f: g‚Å¢(ùíô,Œ¶,‚àáùíôŒ¶,‚àáùíô2Œ¶,‚Ä¶)=f‚Å¢(ùíô),Œ¶:ùíô‚üºŒ¶‚Å¢(ùíô).:ùëîùíôŒ¶subscript‚àáùíôŒ¶subscriptsuperscript‚àá2ùíôŒ¶‚Ä¶ùëìùíôŒ¶‚üºùíôŒ¶ùíôg(\bm{x},\Phi,\nabla_{\bm{x}}\Phi,\nabla^{2}_{\bm{x}}\Phi,...)=f(\bm{x}),% \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode% \nobreak\ \leavevmode\nobreak\ \Phi:\bm{x}\longmapsto\Phi(\bm{x}).italic_g ( bold_italic_x , roman_Œ¶ , ‚àá start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT roman_Œ¶ , ‚àá start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT roman_Œ¶ , ‚Ä¶ ) = italic_f ( bold_italic_x ) , roman_Œ¶ : bold_italic_x ‚üº roman_Œ¶ ( bold_italic_x ) . (1) Our objective is to determine the external source function f‚Å¢(ùíô)‚àà‚Ñùqùëìùíôsuperscript‚Ñùùëûf(\bm{x})\in\mathbb{R}^{q}italic_f ( bold_italic_x ) ‚àà blackboard_R start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT and the full mapping from spatial/temporal coordinates ùíô‚àà‚Ñùsùíôsuperscript‚Ñùùë†\bm{x}\in\mathbb{R}^{s}bold_italic_x ‚àà blackboard_R start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT to the quantity of interest Œ¶‚Å¢(ùíô)‚àà‚ÑùrŒ¶ùíôsuperscript‚Ñùùëü\Phi(\bm{x})\in\mathbb{R}^{r}roman_Œ¶ ( bold_italic_x ) ‚àà blackboard_R start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT from known governing equations gùëîgitalic_g and partial/noisy observations. This problem, known as an inverse source problem [2, 3, 4], is of significant importance in various scientific and engineering domains such as signal processing [5], fluid dynamics [6, 7], optics [8], and more. To address these inverse source problems in robotics, a common strategy is to use finite element analysis integrated with optimization techniques [9, 10] or physics engine [11, 12]. While these approaches focus on computation efficiencies, they suffer two notable limitations. First, they rely on complex discretization and/or meshing, which affects precision and introduces significant complexity. Second, these approaches require unique treatment for different equations and constraints, which significantly limits their generality. Recent progress in representation learning addresses these issues via learning priors over the simulated source functions, where the data implicitly contains all the constraints and boundary conditions. For example, prior works in manipulation simulate extrinsic contact, a source function exerting forces on deformable [13, 14, 15, 16] or rigid objects [17, 18]. Another example involves simulating sensor deformations as a source function transducing electric signals [19]. However, the performance of these approaches relies heavily on the diversity and quality of training data and does not guarantee adherence to the physics, especially with out-of-distribution inputs. Latent force models [20, 21] present an alternative approach using Gaussian processes to identify unknowns in differential equations; however, they are restricted to linear models, and it remains unclear how complex constraints could be effectively integrated. In this paper, we propose a method for solving the inverse source problem based on Physics-Informed Neural Networks [1] that enables the simultaneous inference of both the source function and the full state given partial and noisy observations. Our method offers several advantages over prior work in robotics: compatibility with various nth-order differential equations (including ordinary and partial differential equations), flexibility in integrating various constraints, initial values, and boundary conditions, elimination of the need for non-trivial discretizations like meshing, ease of accommodating gradients from real measurements, and does not limit performance based on the diversity training data. We open-source our codes and a real-world datasets at URL."
https://arxiv.org/html/2411.01603v1,An Aerial Transport System in Marine GNSS-Denied Environment,"This paper presents an autonomous aerial system specifically engineered for operation in challenging marine GNSS-denied environments, aimed at transporting small cargo from a target vessel. In these environments, characterized by weakly textured sea surfaces with few feature points, chaotic deck oscillations due to waves, and significant wind gusts, conventional navigation methods often prove inadequate. Leveraging the DJI M300 platform, our system is designed to autonomously navigate and transport cargo while overcoming these environmental challenges. In particular, this paper proposes an anchor-based localization method using ultrawideband (UWB) and QR codes facilities, which decouples the UAV‚Äôs attitude from that of the moving landing platform, thus reducing control oscillations caused by platform movement. Additionally, a motor-driven attachment mechanism for cargo is designed, which enhances the UAV‚Äôs field of view during descent and ensures a reliable attachment to the cargo upon landing. The system‚Äôs reliability and effectiveness were progressively enhanced through multiple outdoor experimental iterations and were validated by the successful cargo transport during the 2024 Mohamed BinZayed International Robotics Challenge (MBZIRC2024) competition. Crucially, the system addresses uncertainties and interferences inherent in maritime transportation missions without prior knowledge of cargo locations on the deck and with strict limitations on intervention throughout the transportation.","With the rapid development of autonomous technology in recent years, the application of Unmanned Aerial Vehicles (UAVs) in cargo transport is expected to grow quickly [Villa et al., 2020]. A significant portion of international cargo transport is marine-related, but the complex environment characterized by high winds, large waves, and other natural disasters often affects transport efficiency and the safety of human workers [Yang et al., 2020]. UAVs have great potential in this sector to undertake tasks that may be unsafe for humans or cause fatigue, owing to their increasing carrying capacity and high maneuverability [Andrade et al., 2022]. To achieve closed-loop feedback control for autonomous transport, UAVs need to accurately estimate their position and attitude relative to the surrounding environment [Kayton and Fried, 1997]. Currently, most platforms integrate position information from Global Navigation Satellite System (GNSS) receivers along with acceleration and angular rate measurements from IMUs [Suarez et al., 2020]. However, relying solely on GNSS for localization is not reliable. For example, GNSS signals can be obstructed by ship hulls, subject to electromagnetic interference, or even vulnerable to jamming and spoofing attacks, leading to potential vulnerabilities in the aerial transport system [Perez-Grau et al., 2018]. These risks have led to the investigation of autonomous aerial systems designed for cargo transport in marine environments where access to GNSS signals is unavailable. Developing an autonomous aerial system for marine GNSS-denied environments can significantly enhance the robustness and efficacy of cargo transport under challenging conditions. By addressing the unique navigational constraints posed by such environments, these systems hold promise in mitigating the adverse effects of unpredictable weather patterns, turbulent sea conditions, and other environmental factors that traditionally impede maritime cargo operations. Given these considerations, ASPIRE, the program management pillar of Abu Dhabi‚Äôs Advanced Technology Research Council, hosts the Mohamed Bin Zayed International Robotics Challenge 2024 (MBZIRC2024) with the explicit objective of advancing research in autonomous robotic aerial vehicle technologies. Compared to laboratory experiments, the assessment of robotic algorithms in this context proved to be considerably more rigorous. Each team was limited in the number of trials they could perform. The entire system operated autonomously and without intervention until the mission‚Äôs completion. The external environmental factors affecting the platform‚Äôs stability and mission success during execution, such as sea state, light conditions, temperature, and humidity, were unknown. Additionally, the positions and orientations of the multiple objects to be moved were randomized. These conditions closely mirror the demands of real-world marine cargo transport scenarios, where systems must be capable of quick deployment upon cargo arrival, with no opportunity for repeated testing. While challenging, the deployment of robotic systems in genuine maritime environments, under conditions similar to those encountered in actual cargo transport scenarios, presents an exceptional opportunity for developing a truly reliable robotic system. Motivated by the above requirements and challenges, this paper presents a comprehensive design of an autonomous aerial system. The system transport cargo from a target deck to an Unmanned Surface Vehicle (USV), serving as a landing platform, in a maritime environment without GNSS. The primary objectives of the system include accurately localization the UAV platform and target cargoes, establishing reliable target adsorption capacity, and ensuring successful transportation to the landing platform. The main contributions of this paper can be summarized as follows: ‚Ä¢ In a GNSS-denied maritime environment, a landmark-based localization scheme was developed. This scheme utilizes ultrawideband (UWB) and QR code facilities deployed on the landing platform to provide the UAV with location information tailored to varying performance needs. Additionally, it addresses the problem of misalignment between the UAV and landing platform coordinate systems due to magnetometer interference. ‚Ä¢ A mechanical adhesion mechanism was designed specifically for cargo simulated by a suitcase. This adhesion system primarily consists of a rail, carbon board, sponge, and adhesive tape. It is driven by an onboard screw motor to ensure secure attachment to the cargo. ‚Ä¢ A vision-based servo landing method for UAV was designed. This approach utilizes a fixed onboard camera to provide the cargo‚Äôs position. By employing outlier rejection and smoothing filters on the detected data, the method effectively mitigates oscillations caused by cargo movement or uncertain external disturbances. ‚Ä¢ The developed autonomous system underwent numerous iterations and parameter optimizations through extensive outdoor experiments, enhancing its feasibility and robustness for performing transport tasks in marine environments. It successfully completed the mission during the competition, earning a feature report from the event organizers111https://youtu.be/sRkTnEPF9S0?si=1nIFA__\__9Qy__\__0UClp1."
https://arxiv.org/html/2411.01568v1,"Addressing Failures in Robotics using Vision-Based Language Models (VLMs) and Behavior Trees (BT)‚Ä†‚Ä†thanks:This project is supported by WASP (Wallenberg AI, Autonomous Systems and Software Program).","In this paper, we propose an approach that combines Vision Language Models (VLMs) and Behavior Trees (BTs) to address failures in robotics. Current robotic systems can handle known failures with pre-existing recovery strategies, but they are often ill-equipped to manage unknown failures or anomalies. We introduce VLMs as a monitoring tool to detect and identify failures during task execution. Additionally, VLMs generate missing conditions or skill templates that are then incorporated into the BT, ensuring the system can autonomously address similar failures in future tasks. We validate our approach through simulations in several failure scenarios.","Modern robotic systems can handle complex tasks in controlled environments, but transitioning into dynamic, small-batch manufacturing introduces new challenges, especially around failure management. Failures; unforeseen disturbances that prevent task completion; can lead to costly delays and risks, particularly in shared workspaces [1]. The ability to detect, identify, and recover from failures autonomously is crucial for ensuring the robustness of robotic systems. Traditional failure management strategies in robotics include human intervention, failure analysis [2], and automated recovery strategies [1]. These approaches have limitations: human intervention is time-consuming, failure analysis requires expertise, and automated strategies often lack flexibility in handling unforeseen scenarios. Our recent work [3] introduced a novel method using automated recovery behaviors modeled as robotic skills with parameters, preconditions, and postconditions, executed through Behavior Trees and Motion Generators (BTMG) policy representation [4]. This approach optimizes recovery policies using Reinforcement Learning (RL) [5] and also adapts the parameters to different task variations [6]. However, two key limitations remain: (1) the system assumes failure detection and identification are already solved, requiring prior knowledge of the failure, and (2) it only handles known failures with predefined solutions. These limitations make it difficult to address unforeseen failures. We propose addressing these gaps by utilizing Vision Language Models (VLMs) to detect, identify, and generate solutions for unknown failures. By integrating VLMs with Behavior Trees (BTs), our approach autonomously monitors task execution, identifies failure states, and generates missing conditions or skill templates to recover from failures. The BT is then updated using a reactive planner [7] to handle similar future occurrences. Main Contributions ‚Ä¢ We propose a novel integration of VLMs with BTs for monitoring, failure detection, identification and recovery in robotic systems. ‚Ä¢ We use VLMs to generate missing preconditions or skill templates to address failures and update the BT policy. ‚Ä¢ We conduct experiments to demonstrate the effectiveness of the approach."
https://arxiv.org/html/2411.01387v1,Wallbounce : Push wall to navigate with Contact-Implicit MPC,"In this work, we introduce a framework that enables highly maneuverable locomotion using non-periodic contacts. This task is challenging for traditional optimization and planning methods to handle due to difficulties in specifying contact mode sequences in real-time. To address this, we use a bi-level contact-implicit planner and hybrid model predictive controller to draft and execute a motion plan. We investigate how this method allows us to plan arm contact events on the shmoobot, a smaller ballbot, which uses an inverse mouse-ball drive to achieve dynamic balancing with a low number of actuators. Through multiple experiments we show how the arms allow for acceleration, deceleration and dynamic obstacle avoidance that are not achievable with the mouse-ball drive alone. This demonstrates how a holistic approach to locomotion can increase the control authority of unique robot morpohologies without additional hardware by leveraging robot arms that are typically used only for manipulation. Project website: https://cmushmoobot.github.io/Wallbounce","I INTRODUCTION Humans and animals possess the incredible natural ability to leverage interactions between all parts of their bodies and the environment to achieve highly agile and dynamic locomotion behaviors. These interactions enable them to increase their control authority and locomotion capabilities beyond what can be achieved with legs alone. For example, parkour athletes use their hands to push off against walls and navigate around obstacles. Inspired by these capabilities, solving complicated locomotion tasks by leveraging diverse contact sources has been a long-standing challenge in robotics research. Existing literature on multi-contact motion planning and control largely considers locomotion [1][2] and manipulation [3] as separate research problems. In recent years, the rising interest in generalist robot agents has accelerated the design of robot hardware platforms equipped with both wheeled bases or legs for locomotion and arms for manipulation [4], [5],[6]. This new trend in robot morphology also introduces interesting research questions on how one can take advantage of the addition of robot arms during locomotion to augment the capabilities and robustness of the robot. Despite these increasingly mature human-like robot form factors, systems capable of solving challenging locomotion problems while leveraging upper-body capabilities remain understudied. Integrating upper limb contacts into a Model Predictive Control (MPC) framework, however, is challenging. MPC often requires a predefined contact schedule for each contact point. For legged locomotion, where contacts are typically periodic, a contact schedule can be generated with heuristics [2, 6]. For upper limb contacts, determining the timing and duration of contact for other parts of the body is difficult. Most of the existing work rely on a hand-crafted contact schedule for upper limbs [7, 8]. Other works use search-based methods to find possible contact strategies [9, 10]. However, these methods are based on kinematics and quasi-static analysis and cannot capture the dynamic effect of the robot. Figure 1: Time-lapse picture of CMU shmoobot making a sharp turn by pushing wall. In this paper, we investigate using end effector contact on the CMU shmoobot (a smaller CMU ballbot [11]) to enhance its balance, locomotion, and navigation capabilities. Shmoobot uses a single ball drive to achieve dynamic balancing with a low number of actuators while remaining maneuverable in tight human spaces. However, this unique morphology limits how quickly Shmoobot can change its momentum, making it less robust when encountering unexpected obstacles while moving or experiencing large disturbances. We explore how we can use the arms the robot would typically use for manipulation to address these limitations during locomotion, increasing shmoobot‚Äôs robustness and reactivity with no additional hardware. We propose a bi-level Model-Predictive Control (MPC) framework that enables our robot to discover and utilize upper limb contacts during locomotion. At the higher level, we use contact-implicit optimization to identify potential contact schedules. Then, at a lower level, we deploy a hybrid trajectory optimization with this fixed contact schedule to generate smooth, feasible motion plans. Finally, we implement this framework on the CMU shmoobot platform and demonstrate its capabilities through several hardware experiments. Our specific contributions are: 1. A bi-level MPC framework that can reason about acyclic contacts and leverage upper limb contacts in locomotion. 2. Deployment and evaluation of the proposed framework on a novel bi-manual service robot that balances on a ball. 3. Experiments demonstrating how upper limb contacts can effectively assist in robot locomotion."
https://arxiv.org/html/2411.01349v1,The Role of Domain Randomization in Training Diffusion Policies for Whole-Body Humanoid Control,"Humanoids have the potential to be the ideal embodiment in environments designed for humans. Thanks to the structural similarity to the human body, they benefit from rich sources of demonstration data, e.g., collected via teleoperation, motion capture, or even using videos of humans performing tasks. However, distilling a policy from demonstrations is still a challenging problem. While Diffusion Policies (DPs) have shown impressive results in robotic manipulation, their applicability to locomotion and humanoid control remains underexplored. In this paper, we investigate how dataset diversity and size affect the performance of DPs for humanoid whole-body control. In a simulated IsaacGym environment, we generate synthetic demonstrations by training Adversarial Motion Prior (AMP) agents under various Domain Randomization (DR) conditions, and we compare DPs fitted to datasets of different size and diversity. Our findings show that, although DPs can achieve stable walking behavior, successful training of locomotion policies requires significantly larger and more diverse datasets compared to manipulation tasks, even in simple scenarios. Videos can be found at https://sites.google.com/view/dps-for-humanoid-control.","Humanoid robots hold great promise as ideal embodiments for human-centered environments due to their structural resemblance to the human body, which enables them to leverage rich datasets like motion capture for learning control policies. As more companies begin to develop, produce, and commercialize humanoid robots, there is a growing demand for robust and general whole-body motion policies. While diffusion models, in particular Diffusion Policies (DPs), have recently achieved significant success in robot arm manipulation [1, 2], research on whole-body control ‚Äî encompassing simultaneous locomotion and arm movements ‚Äî remains relatively limited [3, 4]. Most existing approaches that use diffusion models for robot control rely on small real-world datasets, which are often sparse and difficult to collect [5, 6, 7]. In contrast, training Reinforcement Learning (RL) policies for locomotion and whole-body control in randomized simulators has demonstrated substantial robustness and success [8, 9]. Recent work, such as DiffuseLoco [10], has shown that diffusion models can integrate multiple source policies into a unified model for robot control. However, while the focus has largely been on the development of the diffusion framework, the impact of the source dataset used for training remains underexplored. To address the insufficient understanding of the role of dataset characteristics on training DPs, we investigate different types of Domain Randomization (DR)‚Äî such as perturbations, dynamic variations, and terrain changes ‚Äî as well as varying dataset sizes. We collect datasets with distinct randomization strategies and sizes to train separate DPs. Our findings reveal that DR is crucial for training successful DPs: even large datasets without sufficient randomization struggle to generalize to non-randomized environments. Contributions. Our contributions are twofold. First, we present the first ablation study on the impact of DR in dataset generation for training DPs in humanoid control. This includes not only commonly used DR techniques but also the introduction of a novel approach. Second, we analyze the effect of dataset size on training, exploring how varying amounts of data interact with different randomization techniques. Notably, while only a few trajectories may suffice for manipulation tasks, training DPs for whole-body control demands substantially more data to achieve robust performance."
https://arxiv.org/html/2411.01321v1,Control Strategies for Pursuit-Evasion Under Occlusion Using Visibility and Safety Barrier Functions,"This paper develops a control strategy for pursuit-evasion problems in environments with occlusions. We address the challenge of a mobile pursuer keeping a mobile evader within its field of view (FoV) despite line-of-sight obstructions. The signed distance function (SDF) of the FoV is used to formulate visibility as a control barrier function (CBF) constraint on the pursuer‚Äôs control inputs. Similarly, obstacle avoidance is formulated as a CBF constraint based on the SDF of the obstacle set. While the visibility and safety CBFs are Lipschitz continuous, they are not differentiable everywhere, necessitating the use of generalized gradients. To achieve non-myopic pursuit, we generate reference control trajectories leading to evader visibility using a sampling-based kinodynamic planner. The pursuer then tracks this reference via convex optimization under the CBF constraints. We validate our approach in CARLA simulations and real-world robot experiments, demonstrating successful visibility maintenance using only onboard sensing, even under severe occlusions and dynamic evader movements.","I INTRODUCTION Pursuit-evasion problems [1] are studied in computational geometry, control theory, and robotics, motivated by applications in search and rescue [2], security and surveillance [3], and environmental monitoring [4]. Introducing visibility constraints leads to the art gallery problem [5], which has elegant solutions with static pursuers (guards) in 2D polygonal environments but becomes challenging with mobile pursuers in 3D environments. Prior works tackled pursuit-evasion with visibility constraints by graph-theoretic [6] or game-theoretic approaches [7, 8, 9]. This work proposes a novel control design for pursuit-evasion with field-of-view (FoV) constraints, using control barrier function (CBF) techniques. Originating from the pioneering works of Wieland and Allg√∂wer [10] and Ames et al. [11, 12], CBFs have become a key tool for safety-critical applications such as adaptive cruise control [13], robot manipulation [14], robot locomotion [15], and robot flight [16]. CBFs are defined so that the positivity of a barrier function implies forward invariance of a corresponding superlevel set for the trajectories of a dynamical system. A key observation is that, for control-affine dynamical systems, CBF constraints are linear in the control input. This allows control synthesis via quadratic programming (QP) subject to linear CBF constraints [12]. Figure 1: A mobile pursuer aims to keep a mobile evader within its field of view despite occlusions and without colliding with obstacles. The inset shows the evader in the pursuer‚Äôs camera view. A challenge for CBF methods is to determine a function whose positivity is equivalent to the desired safe behavior. Previous works have demonstrated ways to validate candidate CBFs for polynomial control systems [17] subject to multiple state and input constraints [18], or even to learn CBFs from expert demonstrations [19]. A key observation in this paper is that visibility under occlusion can also be formulated as a CBF condition using the signed distance function (SDF) of the sensor FoV. An SDF provides the (signed) distance from any point in space to a given set‚Äôs boundary. SDFs have been used to model shapes and surfaces in vision and robotics tasks such as object shape reconstruction [20] and robot mapping [21]. In this paper, we introduce a novel visibility CBF constraint that enforces negativity of the SDF from an evader to the occluded FoV of the pursuer. While early works on CBFs primarily focused on continuous systems subject to differentiable CBF constraints, recent advances [22, 23, 18] have studied the regularity of CBF-based controllers and have expanded their applicability to non-smooth dynamics and non-smooth constraints. These method have been used in [24] to enable safe multi-robot navigation with polytope-shaped robots. In our case, the visibility CBF is Lipschitz continuous but not differentiable everywhere. It is also time-varying due to the pursuer‚Äôs and evader‚Äôs motion. Hence, it is necessary to use a non-smooth time-varying formulation of the visibility CBF constraint involving its generalized gradient. Maintaining visibility is a requirement in various target tracking problems. A visibility-aware planner proposed in [25] generates sensor trajectories with maximal visibility to a moving target. A vision-based controller for a quadrotor landing on a ground vehicle, while preventing camera occlusions, was developed in [26]. Leader-follower formations for maintaining visibility and safety were developed using dipolar vector fields in a known environment in [27]. Similarly, in the presence of FoV constraints, dipolar vector fields have been used in [28] to formulate a model predictive control scheme for differential-drive robot navigation, maintaining visibility of a static landmark. Closely related to our work, Gao et al. [29] formulate a probabilistic notion of visibility under occlusion, which is maintained using an extended Kalman filter [30]. The authors develop a real-time non-myopic trajectory planner for visibility-aware and safe target tracking in the presence of uncertainty. This work is complementary to ours in that it focuses on high-level non-myopic visibility planning using nonlinear optimization, while we focus on low-level myopic visibility control to track a planned reference using convex programming. The contributions of our work are summarized as follows. ‚Ä¢ We prove Lipschitz continuity of the signed distance to an occluded FoV to justify its use as a non-smooth time-varying CBF for visibility maintenance. ‚Ä¢ We achieve non-myopic visibility maintenance by coupling a kinodynamic planner that generates reference controls leading to evader visibility, with a controller that tracks the planned reference using convex optimization subject to visibility and safety CBF constraints. ‚Ä¢ We demonstrate successful visibility maintenance in real robot experiments using only onboard sensing, even under severe occlusions and dynamic evader motion."
https://arxiv.org/html/2411.01284v1,Task-Oriented Hierarchical Object Decomposition for Visuomotor Control,"Good pre-trained visual representations could enable robots to learn visuomotor policy efficiently. Still, existing representations take a one-size-fits-all-tasks approach that comes with two important drawbacks: (1) Being completely task-agnostic, these representations cannot effectively ignore any task-irrelevant information in the scene, and (2) They often lack the representational capacity to handle unconstrained/complex real-world scenes. Instead, we propose to train a large combinatorial family of representations organized by scene entities: objects and object parts. This hierarchical object decomposition for task-oriented representations (HODOR) permits selectively assembling different representations specific to each task while scaling in representational capacity with the complexity of the scene and the task. In our experiments, we find that HODOR outperforms prior pre-trained representations, both scene vector representations and object-centric representations, for sample-efficient imitation learning across 5 simulated and 5 real-world manipulation tasks. We further find that the invariances captured in HODOR are inherited into downstream policies, which can robustly generalize to out-of-distribution test conditions, permitting zero-shot skill chaining. Appendix, code and videos: https://sites.google.com/view/hodor-corl24.","How should an embodied agent internally represent its visual observations? The role of a representation is to supply task-relevant information to the decision-making procedure, such as a neural network policy. This implies two desirable properties for any representation. First, a representation should highlight task-relevant aspects of the scene and de-emphasize irrelevant information. This means that good representations are inherently task-specific: driving on the road relies on very different aspects of the scene than walking on the pavement. Or, in our experimental settings, the same kitchen robot might need to attend to different scene aspects when executing different subtasks: boiling water, turning on a faucet, putting vegetables into a pot, and so on. This necessary task-specificity was an important motivation for ‚Äúend-to-end‚Äù approaches in the last decade [1, 2]. Rather than hand-crafted ‚Äúoff-the-shelf‚Äù representations (e.g., bags of visual words [3], SIFT [4], HoG features [5]), they aimed to optimize the entire control pipeline for one or a few tasks. Second, representations must conveniently organize scene information so that the policy can be represented in a simple function. For robot learning, such simplicity in optimal policies translates to ease of discovering these policies by an optimization procedure operating over limited experience. For example, a representation could aim to disentangle the latent factors of variation in the inputs [6, 7, 8, 9, 10]. Figure 1: HODOR embeds a 2D input image into an object-centric, multi-resolution, task-specific representation for efficient policy learning In recent years, motivated by efforts to improve data efficiency, off-the-shelf representations have risen in popularity again, this time in the form of the activations of pre-trained vision models [11, 12, 13, 14, 15, 16, 17, 18]. While these pre-trained representations are no longer hand-crafted, they retain another critical drawback of off-the-shelf representations: they are agnostic to the downstream task. As such, they are trained to capture all scene information, even though most of this information might be irrelevant to any given downstream use case. Furthermore, in complex, cluttered scenes, these standard fixed-dimensionality ‚Äúscene vector‚Äù representations are limited in their ability to comprehensively capture all fine-grained details. How might we enable task specificity alongside representation pre-training? Rather than train a single representation, we propose to generate a menu of representations that can be combinatorially assembled into the right platters suited for each downstream task. In particular, we propose hierarchical object decomposition for task-oriented representations (HODOR). HODOR recognizes that scene entity trees, i.e., trees of objects and object parts, provide a convenient organizing principle for a representation menu: different objects are relevant at different levels of detail to different tasks or task phases. Further, HODOR exploits advances in semantic visual understanding to select task-relevant scene entity trees for any given task. HODOR involves no new pre-training to generate representations; instead, it mines these representations from existing vision and language foundation models. See Fig 1. Our main contributions are: 1) We introduce a pre-trained approach to generate a multi-level decomposition of a given scene and then ‚Äúfilter‚Äù it based on language-based task descriptions into a task-specific tree; 2) we present an approach for downstream policy networks to learn from this, and structured HODOR representation, and 3) We empirically show that HODOR is better than prior pre-trained representations at downstream manipulation tasks."
https://arxiv.org/html/2411.01274v1,Efficient Collaborative Navigation via Perception Fusion for Multi-Robots in Unknown Environments,"For tasks conducted in unknown environments with efficiency requirements, real-time navigation of multi-robot systems remains challenging due to unfamiliarity with surroundings. In this paper, we propose a novel multi-robot collaborative planning method that leverages the perception of different robots to intelligently select search directions and improve planning efficiency. Specifically, a foundational planner is employed to ensure reliable exploration towards targets in unknown environments and we introduce Graph Attention Architecture with Information Gain Weight(GIWT) to synthesizes the information from the target robot and its teammates to facilitate effective navigation around obstacles.In GIWT, after regionally encoding the relative positions of the robots along with their perceptual features, we compute the shared attention scores and incorporate the information gain obtained from neighboring robots as a supplementary weight. We design a corresponding expert data generation scheme to simulate real-world decision-making conditions for network training. Simulation experiments and real robot tests demonstrates that the proposed method significantly improves efficiency and enables collaborative planning for multiple robots. Our method achieves approximately 82%percent8282\%82 % accuracy on the expert dataset and reduces the average path length by about 8%percent88\%8 % and 6%percent66\%6 % across two types of tasks compared to the fundamental planner in ROS tests, and a path length reduction of over 6%percent66\%6 % in real-world experiments.","In recent years, multi-robots have been widely used in various domestic and outdoor services,including exploration[1], search and rescue[2], agriculture[3] an so forth. For tasks conducted in unknown environments with efficiency requirements, real-time navigation of multi-robot systems remains challenging due to unfamiliarity with surroundings. In scenarios with relatively confined spaces, one practical approach to tackle this challenge is to first create a global map of the scene using the robots‚Äô perception and localization capabilities, and then apply a global multi-robot path planning algorithm to compute optimal routes for each robot. However, although this method can produce mathematically optimal solutions based on a global map, reconstructing such a map is often time-inefficient and unnecessary, as it may not be utilized when the robots are moving away. Consequently, how to leverage the local perception capabilities of multiple robots in unknown environments for rapid path planning has attracted widespread research attention. Up to now, various techniques have been proposed to address this challenge, including reinforcement learning[4],gaussian belief propagation[5], neural networks[6] and so on. Despite great progress have been made, there still exists different challenges for real-world applications: first grid-based methods ignore the complex distribution of obstacles under real conditions, with an obvious gap in the sensing conditions that leads to poor performance in real-world applications . On the other hand, the performance of the algorithm requires further improvement with regard to the convergence and the optimality. For instance, heuristic methods such as neural networks cannot guarantee the convergence of solutions, while rule-based methods lack intelligent judgment and decision-making in complex obstacle environments. To bridge this gap and take full advantage of the distributed perception advantages of multi-robots when performing tasks, this paper proposes a hybrid multi-robot collaborative path planning method to ensure convergence and improve path efficiency in unknown environments(see Fig 1). We employ the traditional Distance Histogram Bug (DHbug)[7] algorithm as a foundational planner to ensure reliable exploration towards the target. We designed and trained a graph neural network(GNN) to provide decision support for the basic planner at critical decision points by synthesizing the local perception data from multiple robots. This method combines the convergence advantages of rule-based path planning with the capabilities of neural networks in intelligently analyzing local environments, effectively improving search efficiency while ensuring that the target is reached. The main contribution of this paper can be summarized as follows: ‚Ä¢ We propose a hybrid collaborative multi-robot path planning method, using a foundational planner to ensure reliable exploration towards the target and a trained network to provide decision support at critical points. ‚Ä¢ We introduce Graph Attention Architecture with Information Gain Weight(GIWT) to efficiently synthesize the positional and perceptual data from multi-robot to facilitate effective navigation around obstacles. ‚Ä¢ In order to enable the proposed method to be applied in real environments, we designed corresponding expert data generation method in The Robot Operating System(ROS) for the training of GIWT. ‚Ä¢ We carried out both simulation and real robots tests to evaluate the proposed method and evaluated the key parameters."
https://arxiv.org/html/2411.01200v1,GarmentLab: A Unified Simulation and Benchmark for Garment Manipulation,"Manipulating garments and fabrics has long been a critical endeavor in the development of home-assistant robots. However, due to complex dynamics and topological structures, garment manipulations pose significant challenges. Recent successes in reinforcement learning and vision-based methods offer promising avenues for learning garment manipulation. Nevertheless, these approaches are severely constrained by current benchmarks, which offer limited diversity of tasks and unrealistic simulation behavior. Therefore, we present GarmentLab, a content-rich benchmark and realistic simulation designed for deformable object and garment manipulation. Our benchmark encompasses a diverse range of garment types, robotic systems and manipulators. The abundant tasks in the benchmark further explores of the interactions between garments, deformable objects, rigid bodies, fluids, and human body. Moreover, by incorporating multiple simulation methods such as FEM and PBD, along with our proposed sim-to-real algorithms and real-world benchmark, we aim to significantly narrow the sim-to-real gap. We evaluate state-of-the-art vision methods, reinforcement learning, and imitation learning approaches on these tasks, highlighting the challenges faced by current algorithms, notably their limited generalization capabilities. Our proposed open-source environments and comprehensive analysis show promising boost to future research in garment manipulation by unlocking the full potential of these methods. We guarantee that we will open-source our code as soon as possible. You can watch the videos in supplementary files to learn more about the details of our work. Our project page is available at: https://garmentlab.github.io/","The next-generation assistant robots should possess not only the abilities to separately manipulate a wide variety of objects, including rigid, articulated[59], and deformable objects[58], but also the capability to leverage interactions between those physical media, including flow and fluids, in order to assist humans[39]. Among various daily tasks [69, 59, 56], garment manipulation stands out as one of the most challenging, crucial, and extensively discussed tasks in the robotics and computer vision, due to its demanding requirements for understanding dynamic properties of physical instances and interactions between them. For instance, washing clothes entails the interaction between garments and fluids, while dressing up requires collaboration between robots and humans. Figure 1: GarmentLab provides realistic simulation for diverse garments with different physical propoerties, benchmarking various novel garment manipulation tasks in both simulation and the real world. Garment manipulation tasks mainly presents three challenges. Firstly, each individual garment possesses nearly infinite states and exhibits complex kinematic and dynamic properties. Therefore, it is crucial for models to comprehend the various self-deform states of garments, which usually requires large amount of training data [17, 6] (C1). Secondly, garment manipulation involves interactions with various types of objects, including rigid (e.g., clothes hanger) and articulated (e.g., wardrobe) objects, as well as fluids and human body. Consequently, enabling models to understand these interactions across diverse physical media presents great significance (C2). Finally, considering that strategies for manipulating garments are often highly complex, and visual perception of garments is more challenging due to their diverse states and patterns, manipulating garments faces a greater sim2real gap [63, 29] (C3). Training a powerful agent capable of overcoming these challenges necessitates a vast amount of data encompassing robot-object interactions. However, directly collecting data from the real world is impractical. Thus, researchers have long pursued benchmarks for garment manipulation [30, 4, 6, 64]. Current deformable simulations suffer from drawbacks such as missing garment meshes [30]. Additionally, they offer a limited range of tasks, hindering further research endeavors. Therefore, we present GarmentLab (Figure 1), a unified environment and benchmark for garment manipulation. GarmentLab has three novel components to satisfy the demands for diversity and realism: The powerful GarmentLab Engine, which is built upon Omniverse Isaac Sim [71] and supports a variety of physical simulation methods. The simulator not only supports Particle-Based-Dynamic (PBD) [3], Finite-Element-Method (FEM) [11], to simulate garments, fluid and deformable objects but also makes integration with ROS [42] to provide an efficient teleoperation pipeline for data collection. GarmentLab Assets is a large-scale indoor dataset comprising 1) garments models covering 11 categories of daily garments from ClothesNet [70] 2) various kinds of robot end-effector including gripper, suction and dexterous hands. 3) high-quality 3D assets including 20 scenes and 9000+ object models from ShapeNet [7]. Based on realistic simulation and rich assets, we propose GarmentLab Benchmark containing 20 tasks divided into 5 groups to evaluate state-of-the-art vision-based and reinforcement learning based algorithm. To tackle above challenges, our environment has three characteristics:1) Efficient. Garment manipulation involves nearly infinite object state and action spaces, requiring substantial data for models to understand garment structure and deformation. To meet this demand, our highly parallelized GPU-based simulator provides a significant training advantage. Larger batch sizes enhance RL-based algorithms [33], while faster data collection speeds reduce training time for perception-based algorithms (tackling C1). 2) Rich. The richness of our simulator can be categorized into two aspects: the diversity of simulation content offered by GarmentLab Assets and the depth of physical interaction facilitated by GarmentLab Engine. We emphasize multi-physics simulation, encompassing rigid-articulated, deformable-garment, fluid dynamics, and flow, along with their interactions. This focus is vital for training agents capable of comprehending real-world physical properties [48] (tackling C2). You can refer to videos in supplementary material for our simulation effects. 3) Real. As the sim-to-real gap emerges as the main obstacle in developing embodied agents, GarmentLab Engine surpasses Omniverse capabilities by providing mature sim-to-real algorithms, such as Teleoperation [41] utilized in the RL field, and the Visual Sim-Real Alignment Algorithm employed in perception algorithms. We also make integration with ROS [42] and MoveIt [10], which is beneficial for narrowing sim2real gap by introducing real-world robot motions into simulation (tackling C3). Our benchmark experiments highlight the significant challenges current algorithms face, even with seemingly simple tasks like unfolding. These difficulties arise from a lack of understanding of physical interactions and the complexities of high-dimensional states. Vision-based algorithms demonstrate limited generalization, with performance strongly affected by the initial state of objects. RL-based algorithms also encounter difficulties with tasks requiring long-horizon planning. These insights offer valuable guidance for improving methods for garment and deformable object manipulation. In summary, we have made the following contributions in GarmentLab: ‚Ä¢ We propose GarmentLab Environment, a realistic and rich environment for garment manipulation, featuring diverse simulation methods, assets, object physics and multi-material interactions. ‚Ä¢ Based on GarmentLab Environment, we propose GarmentLab Benchmark, benchmarking a large variety of garment manipulation tasks, and providing the first real-world garment manipulation benchmark that can be reproduced internationally. ‚Ä¢ We integrate different sim2real methods into GarmentLab, providing solutions to narrowing the sim2real and further facilitating the real-world applications. ‚Ä¢ Extensive experiments and detailed analyses of different types of garment manipulation algorithms facilitate and enlight future research on garment manipulation."
https://arxiv.org/html/2411.01120v1,Generation of Conservative Dynamical Systems Based on Stiffness Encoding,"Dynamical systems (DSs) provide a framework for high flexibility, robustness, and control reliability and are widely used in motion planning and physical human-robot interaction. The properties of the DS directly determine the robot‚Äôs specific motion patterns and the performance of the closed-loop control system. In this paper, we establish a quantitative relationship between stiffness properties and DS. We propose a stiffness encoding framework to modulate DS properties by embedding specific stiffnesses. In particular, from the perspective of the closed-loop control system‚Äôs passivity, a conservative DS is learned by encoding a conservative stiffness. The generated DS has a symmetric attraction behavior and a variable stiffness profile. The proposed method is applicable to demonstration trajectories belonging to different manifolds and types (e.g., closed and self-intersecting trajectories), and the closed-loop control system is always guaranteed to be passive in different cases. For controllers tracking the general DS, the passivity of the system needs to be guaranteed by the energy tank. We further propose a generic vector field decomposition strategy based on conservative stiffness, which effectively slows down the decay rate of energy in the energy tank and improves the stability margin of the control system. Finally, a series of simulations in various scenarios and experiments on planar and curved motion tasks demonstrate the validity of our theory and methodology.","Using dynamical systems (DSs) for robot motion planning and control problems has become popular in robotics because they can generate highly flexible and robust motion and control strategies. DS has been used in a wide variety of scenarios, ranging from performing point-to-point and periodic motions (Yao et al., 2021), such as pick-and-place and imitating specific motion patterns, to handling tasks in dynamic scenarios, such as real-time obstacle avoidance (Koptev et al., 2024), catching flying objects (Kim et al., 2014; Salehian et al., 2016), and in-hand manipulation (Khadivar and Billard, 2023). DS was successfully applied to contact tasks such as assembly and polishing by properly regulating the interaction forces between the robot and the environment (Amanhoud et al., 2019, 2020). In DS-based motion planning, for a robotic system defined in a state space ùùÉ‚àà‚ÑùnùùÉsuperscript‚Ñùùëõ{\boldsymbol{\xi}}\in{\mathbb{R}^{n}}bold_italic_Œæ ‚àà blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, we specify the system‚Äôs motion evolution given by a suitably designed ordinary differential equation ùùÉÀô=ùêü‚Å¢(ùùÉ)bold-ÀôùùÉùêüùùÉ{\boldsymbol{\dot{\xi}}}={\mathbf{f}}\left({\boldsymbol{\xi}}\right)overbold_Àô start_ARG bold_italic_Œæ end_ARG = bold_f ( bold_italic_Œæ ), where ùêü‚Å¢(ùùÉ)‚àà‚ÑùnùêüùùÉsuperscript‚Ñùùëõ{\mathbf{f}}\left({\boldsymbol{\xi}}\right)\in{\mathbb{R}^{n}}bold_f ( bold_italic_Œæ ) ‚àà blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT is a continuously differentiable vector field. 1.1 Learning DS from demonstrations Learning from Demonstration (LfD) is a powerful method for effectively imitating task patterns by observing demonstration tasks. Many learning methods have been developed to generate DSs with stability guarantees, ensuring stability at the kinematic level. A natural idea is to obtain stability constraints from Lyapunov‚Äôs second method. Khansari-Zadeh and Billard (2011) seminally introduced the stable estimator of dynamical systems (SEDS). They introduced constraints in the Gaussian Mixture Regression (GMR) framework based on the quadratic Lyapunov function (QLF), thus ensuring that the generated motion is globally asymptotically stable (GAS). Although using QLF simplifies the parameter estimation process for DS, SEDS can only accurately inscribe motion trajectories with a monotonically decreasing L2subscriptùêø2L_{2}italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT-norm distance from the equilibrium point. To overcome this problem, Figueroa and Billard (2018) entirely use the properties of a polytopic linear parameter varying (LPV) system. They propose the DS learning method LPV-DS in combination with parametrized QLF (P-QLF), which exhibits excellent reproduction accuracy for general nonlinear motions. Mohammad Khansari-Zadeh and Billard (2014) further proposed the weighted sum of asymmetric quadratic functions (WSAQF), which achieves a wider range of fitting capabilities while satisfying global validity. Combined with WSAQF, the control corrections are obtained by solving the convex optimization problem with constraints in real time, thus ensuring that the system is GAS. Kronander et al. (2015) proposed locally modulated DS (LMDS) based on Lyapunov theory. This method locally shapes the existing DS through a modulation matrix while ensuring that the modulated DS is locally asymptotically stable (LAS). To address the problem of robots failing to track a specific reference trajectory when perturbed, Figueroa and Billard (2022) novelly proposed locally active globally stable DS (LAGS-DS), which causes the DS to exhibit stiffness-like symmetric attraction behaviors. To ensure stability at the kinematic level, contraction analysis and partial contraction analysis are also effective tools to provide stability constraints (Lohmiller and Slotine, 1998; Wang and Slotine, 2005). The contraction theory ensures that all DS trajectories converge exponentially to each other and that the trajectories globally exponentially converge towards the equilibrium point. Similar to the real-time correction strategy in Mohammad Khansari-Zadeh and Billard (2014), Blocher et al. (2017) computed the control inputs based on the contraction theory to online stabilize the original DS with the spurious attractor. Ravichandar et al. (2017) generated nonlinear trajectories with guaranteed exponential convergence conditions by providing stability constraints through partial contraction analysis. Further, Ravichandar and Dani (2019) learned position and orientation dynamics based on contraction theory. A framework for learning diffeomorphism is proposed to generate more complex trajectories while ensuring stability. Diffeomorphism establishes a one-to-one correspondence between the original space and the transformed space, thus transforming the highly nonlinear and complex learning tasks in the original space into easy-to-learn tasks in the transformed space. Since the diffeomorphism maintains the consistency of DS stability in the original and transformed spaces, the stable and highly nonlinear DS in the original space can be obtained by learning the stable DS in the transformed space. Neumann and Steil (2015) gave an analytic expression for diffeomorphism, which corresponds the complex Lyapunov function in the original space to the QLF in the transformed space, thus combining SEDS to generate a stable DS in the transformed space. Perrin and Schlehuber-Caissier (2016) use diffeomorphism to correspond complex demonstration trajectories in the original space to simple linear trajectories in the transformed space, where the diffeomorphism is the composition of the locally weighted translations. Scholars have proposed a series of novel works using neural networks, a powerful framework, in combination with the aforementioned Lyapunov theory, contraction theory, and diffeomorphic transformation approach (Dawson et al., 2023). Lemme et al. (2014) learn DS vector fields based on an extreme learning machine, and Lyapunov theory guarantees DS stability. Rana et al. (2020) construct diffeomorphism using the non-volume preserving (NVP) network architecture, which is a general class of mappings, and its parameter optimization is achieved through back-propagation. In the deep neural network framework, P√©rez-Dattari and Kober (2023) used imitation and contrastive learning to give a learning loss that ensures stability, generating first-order and second-order stable DSs. Combining with the contraction theory, Mohammadi et al. (2024) proposed a neural network-based method to generate the contractive DS. The DS with contractive property is obtained by integrating the negative definite Jacobi matrix generated by a neural network. They further used the variational autoencoder to generate the contractive DS in high dimensional space. In addition, many scholars have used differential geometry, a powerful geometric tool, to generate DSs on manifolds (Duan et al., 2024; Fichera and Billard, 2023). 1.2 Research on impedance control The aforementioned DS theory provides a powerful framework for robot trajectory planning. In realistic scenarios, considering the interaction between the robot and the environment in order for the robot to perform its tasks better, in addition to planning the robot‚Äôs position and velocity, it is also necessary to compliantly regulate the interaction forces between the robot and the environment. Impedance control provides a solution to coordinate the dynamic relationship between position, velocity, and interaction forces (Hogan, 1985). When the robot interacts with the environment, an important property of the controller is passivity, which brings along advantageous robustness properties (Ott, 2008). The controller‚Äôs passivity allows the robot to produce stable behavior when in contact with a passive environment. In this perspective, the classical impedance controller in the regulation case is passive. When considering more complex tasks and environments, researchers often require robots to track continuous trajectories or interact with the environment exhibiting variable impedance characteristics, but these dynamic properties are achieved at the expense of impedance controller passivity. To keep the robot passive while performing tracking tasks, many scholars have advocated using a time-independent velocity field to guide the robot‚Äôs motion. Li and Horowitz (1999) introduced the augmented coordinates to achieve the system‚Äôs passivity in the framework of velocity field control (VFC) by rationally distributing the kinetic energy of the original system and the corresponding kinetic energy of the augmented coordinates. Duindam et al. (2004) devised a passive control rate to enable the robot to track the desired motion in an artificial potential field. Such methods are complex, and selecting the appropriate controller parameters is difficult. Kishi et al. (2003) ensured the system‚Äôs passivity by introducing an energy tank structure to keep the direction of motion constant during trajectory tracking and regulating the velocity magnitude in real time, which degraded the tracking performance at the velocity level. Another area of research focuses on maintaining system passivity while the robot executes variable impedance control. Ferraguti et al. (2013) guarantee the passivity of the variable stiffness control process by incorporating an energy tank, but the discontinuous control law degrades the system‚Äôs dynamic performance when the energy tank is depleted. Kronander and Billard (2016b) ensure that the impedance control is passive by imposing constraints on the impedance damping and stiffness matrices, but the method relies on external force measurements with an accurate dynamics model. In addition, many passivity-based control methods in the impedance control framework have been successfully applied in flexible joint robots (Albu-Sch√§ffer et al., 2007; Ott et al., 2008; Keppler et al., 2018; Spyrakos-Papastavridis et al., 2020). Combining the idea of impedance control in the DS framework, Kronander and Billard (2016a) give the widely used passive-DS controller based on the damping control law. By rational design, the impedance damping matrix can selectively dissipate energy in task-irrelevant directions, while the stiffness characteristics of the closed-loop control system are encoded in the DS ùêü‚Å¢(ùùÉ)ùêüùùÉ{\mathbf{f}}\left({\boldsymbol{\xi}}\right)bold_f ( bold_italic_Œæ ). It is worth noting that when the vector field ùêü‚Å¢(ùùÉ)ùêüùùÉ{\mathbf{f}}\left({\boldsymbol{\xi}}\right)bold_f ( bold_italic_Œæ ) is conservative, the robotic system is passive, thus ensuring stability when interacting with passive environments. The passive-DS controller ensures the system‚Äôs passivity when the robot tracks a continuous trajectory and performs variable impedance control by uniformly encoding the trajectory tracking information and the variable stiffness characteristics in the DS ùêü‚Å¢(ùùÉ)ùêüùùÉ{\mathbf{f}}\left({\boldsymbol{\xi}}\right)bold_f ( bold_italic_Œæ ). For a general non-conservative DS, Kronander and Billard (2016a) introduced an energy tank to offset the loss of passivity due to the non-conservative component ùêün‚Å¢c‚Å¢(ùùÉ)subscriptùêüùëõùëêùùÉ{{\mathbf{f}}_{nc}}\left({\boldsymbol{\xi}}\right)bold_f start_POSTSUBSCRIPT italic_n italic_c end_POSTSUBSCRIPT ( bold_italic_Œæ ). However, when the energy in the tank is depleted, the closed-loop system‚Äôs dynamic performance and trajectory tracking performance are greatly reduced due to the switching control law introduced by the energy tank structure. Therefore, solving the problem of rationally decomposing the general DS into conservative and non-conservative components to slow down the energy tank‚Äôs consumption rate remains a challenge. Under the framework of the passive-DS controller, Huber et al. (2024) further design the impedance damping matrix reasonably for obstacle avoidance scenarios, effectively reducing the probability of collision. Using the optimal control technique, Fichera and Billard (2023) give a torque-based control framework for the second-order DS. 1.3 Contribution From the above literature review, it is clear that most of the existing studies related to learning DS only consider stability at the kinematic level and do not consider the passivity of the control system (which corresponds to the conservativeness of the DS). A few studies, although learning conservative DS, are limited to the simple case of a 2D linear space and lack a systematic discussion for the generation of conservative DS on more complex trajectories or general manifolds. DS decomposition plays an important role in the performance of energy tank-based controllers. However, there is yet to be a method to propose a generic decomposition strategy. In addition, the current study only discusses the stiffness properties of DS at a qualitative level, and a systematic discussion of the stiffness properties for DS modulation is still needed. This paper proposes a DS generation framework based on stiffness encoding, as shown in Figure 1. We establish and summarise the connection between the encoded stiffness properties and the generated DS, and generate DS to meet the requirements of specific scenarios by designing the stiffness properties. We utilize the conservative stiffness matrix to obtain conservative DS, which guarantees the passivity of the controller. We further extend our approach to generate conservative DS on different manifolds and for different trajectory types. Further, we also propose a generic decomposition algorithm for non-conservative DS based on the conservative stiffness matrix, which significantly slows down the depletion rate of the energy tank. As shown above, we provide a systematic analysis and discussion for the conservativeness of DS. The contribution of this paper can be summarised as follows. 1. We develop a framework for DS generation based on stiffness encoding (see Figure 1), giving the connection between stiffness properties and their corresponding DS characteristics. 2. Based on Gaussian processes, the conservative DS with a symmetric attraction behavior and a variable stiffness profile is learned in linear space. The method is further generalized to SE(3) as well as to closed-loop and self-intersecting trajectories. In all scenarios, we prove the controller‚Äôs passivity. 3. Combined with the controller‚Äôs energy tank structure, we propose a generic decomposition strategy for non-conservative DS based on the conservative stiffness matrix. 4. A series of simulations and experiments are conducted to validate the proposed theory and methodology. The rest of the paper is organized as follows. Section 2 illustrates the contents of this paper in comparison with related work. Preliminaries are given in Section 3, where we introduce the framework of stiffness encoding and the necessary basics. In Section 4, we generate conservative DS with symmetric attraction properties based on Gaussian processes in a 2D linear space. A generalization of the conservative DS generation method is given in Section 5: we discuss the case on SE(3) as well as closed-loop and self-intersecting trajectories. At the same time, we propose effective DS decomposition strategies. Section 6 verifies the validity of the proposed theory and method through a series of simulations and experiments. Finally, Section 7 concludes the whole paper. Figure 1: Schematic diagram of the stiffness encoding. The color of each point in the figure indicates the value of the angular velocity of the vector field at the corresponding point. When encoding a specific stiffness, the corresponding DS exhibits particular properties. (1) When the stiffness matrix satisfies the exactness, a well-defined DS is guaranteed, i.e., a continuous vector field in space. (2) The DS is conservative when the stiffness is exact and symmetric. Conservativeness is shown in the graph as a constant angular velocity of zero. In the figure, we show a conservative DS with symmetric attraction behavior, and it is not contractive. (3) For the DS to be contractive, the encoded stiffness must satisfy both exactness and negative definiteness. It can be seen that all trajectories of the DS converge to each other. (4) The DS is conservative and contractive when the stiffness matrix is exact, symmetric, and negative definite."
https://arxiv.org/html/2411.01014v1,Mixed Reality Teleoperation Assistance for Direct Control of Humanoids,"Teleoperation plays a crucial role in enabling robot operations in challenging environments, yet existing limitations in effectiveness and accuracy necessitate the development of innovative strategies for improving teleoperated tasks. This article introduces a novel approach that utilizes mixed reality and assistive autonomy to enhance the efficiency and precision of humanoid robot teleoperation. By leveraging Probabilistic Movement Primitives, object detection, and Affordance Templates, the assistance combines user motion with autonomous capabilities, achieving task efficiency while maintaining human-like robot motion. Experiments and feasibility studies on the Nadia robot confirm the effectiveness of the proposed framework. Supplementary video available at https://youtu.be/oN-FD6YnF2c.","Several of the most compelling instances of humanoids performing valuable tasks have been through teleoperation [1]. However, despite years of research in this domain, teleoperated humanoid robots continue to show substantial limitations when executing tasks under user control. Traditional direct control of robots through the user‚Äôs own motions offers an intuitive approach for interacting with the remote environment [2, 3, 1]. However, differences in kinematics between humans and robots can lead to errors and multiple attempts for task completion, making this approach not very effective and time-consuming, and even lead to operator frustration. Figure 1: With the teleoperation assistance we propose, the user serves as a high-level guide, directing the robot‚Äôs autonomous task execution through a mixed reality interface (top left) and specifying the manner in which the task should be carried out. In the given example, the user merely initiates a portion of a punching motion, specifically indicating an uppercut technique. Recognizing this, the robot discerns that it is tasked with a punching action and, among the various techniques available, it should execute an uppercut. The user does not need to focus on the precision of the subsequent motion, as the initial input is sufficient for the robot to complete the task accurately. Our work addresses the challenge of improving the direct control of humanoids in teleoperation by introducing autonomous assistance features in mixed reality. The goal is to enhance the efficiency and accuracy of teleoperated tasks. To achieve this, we employ Probabilistic Movement Primitives (ProMPs) [4], which are widely used for modular movement representation and generation. These ProMPs are conditioned on the operator‚Äôs input, allowing adaptation to the user‚Äôs preferred way of performing the task while maintaining similarity to the demonstrations. Additionally, we utilize information from the robot‚Äôs sensors to provide context and precision to the motion primitives. The physical interaction with objects is governed by Affordance Templates (ATs) [5], ensuring accurate and controlled manipulation while reducing the risk of errors or unintended consequences during physical interaction. In order to observe, predict and direct the assistance, we developed a mixed reality interface, based on Coactive Design principles [6]. Using mixed reality has the benefit of providing an immersive experience for the operator as also shown in [7]. The need of such teleoperation assistance was vividly highlighted during the finals of the ANA Avatar XPRIZE [8, 9, 10], a four-year global competition on robot teleoperation that ended in November 2022. Operators struggled to effectively and accurately control the robots, even for seemingly simple tasks like grasping a bottle. Multiple attempts were often required, showcasing the difficulties in achieving satisfactory outcomes solely through direct user control ‚Äî especially when considering that robots can perform similar tasks autonomously with relative ease. It is important to note that full direct control is typically achieved by projecting the RGB images from the robot camera to the Virtual Reality (VR) headset [9, 10], but this approach often fails to provide sufficient situational awareness [11] to the user. This was also demonstrated in the ANA Avatar XPRIZE finals, where judge operators frequently encountered confusion regarding the robot‚Äôs location within the environment. Hence, a combination of autonomy and mixed reality technology is crucial in bridging these gaps and enabling robots for real-world applications. Figure 2: Flowchart of the assistive autonomy. During the training phase, the human operator teleoperates the robot in simulation, and performs the tasks in different ways. A ProMP is learned for every task. When teleoperating the robot in real-time, the ProMPs are used to generate the assistive robot motion: (1) the system recognizes the current task, with the help of context given by the object detection; (2) it updates the ProMP according to the initially observed user input and object pose; (3) if an affordance is available for that object, a blending mechanism is used to to achieve smooth transitions between ProMP-generated motions and ATs. A recent and promising approach to addressing the direct control issues is shared-control, which integrates robot assistance with user input to facilitate the completion of tasks [12]. For instance, Rakita et al. [13] implemented a shared-control approach in which they teleoperated the upper body of a humanoid robot. By maintaining a constant offset between the end effectors or limiting the motion of the end-effector that holds the object, the system provides on-the-fly assistance and helps the user perform bimanual tasks more easily. Similarly, in [14], Rahal et al. designed a shared control approach to assist the human operator by enforcing different nonholonomic-like constraints representative of the cutting kinematics. In many shared-control frameworks, the user provides an input that allows the robot to consider the human‚Äôs intent and to assist in the task by adjusting its motion or executing a pre-optimized version of that motion. To determine the final reference for the robot‚Äôs motion, a policy blending mechanism that governs the balance between the user input and the enhanced robot motion is employed [12]. The blending policy is usually the distance to the goal. The closer the robot gets to a predicted goal, the more likely it is that the predicted goal is the correct one, and thus, the robot input is favored over the user‚Äôs. Approaches have also been proposed that consider unknown goals. For instance, in the case of [15], a POMDP-based strategy has been employed to provide assistance, aiming to minimize the expected cost-to-go. It is worth highlighting that this particular approach is predominantly designed for the control of goal-oriented components of the robot, such as its end-effectors. Consequently, its direct applicability to the control of other robot components, such as adjusting the orientation of the chest and forearms as needed for whole-body tasks may be limited. Alternatively, [16] presents a novel approach leveraging the Koopman operator for learning the dynamics of a machine and enhancing human-machine interaction. However, the application and scalability of this system to more complex systems, such as humanoid robots, remain unvalidated and appear to be challenging due to the increased complexity and unpredictability inherent in such systems. In haptic-based shared-control approaches, a different strategy is employed. Here, only the user‚Äôs input actuates the robot, but haptic information is used to guide the user during task execution [14, 17]. By incorporating haptic feedback, the system can provide the user with tactile cues or force feedback, improving the user‚Äôs understanding of the task and making the teleoperation process more intuitive and effective. In line with the efforts that have been made in humanoid teleoperation, our work continues to advance these endeavors. By integrating predictive modeling techniques, we are poised to proactively gauge the user‚Äôs intended motion, tailoring the robot behavior to both fit the user input and the pose of manipulated objects in the environment. By leveraging the strengths of both the user and the robot, we do not just enhance task efficiency; we also maintain the human-likeness of the robot‚Äôs motion. Our work is validated through experiments and feasibility studies on the Nadia humanoid robot."
https://arxiv.org/html/2411.01000v1,Enhancing Model-Based Step Adaptation for Push Recovery through Reinforcement Learning of Step Timing and Region,"This paper introduces a new approach to enhance the robustness of humanoid walking under strong perturbations, such as substantial pushes. Effective recovery from external disturbances requires bipedal robots to dynamically adjust their stepping strategies, including footstep positions and timing. Unlike most advanced walking controllers that restrict footstep locations to a predefined convex region, substantially limiting recoverable disturbances, our method leverages reinforcement learning to dynamically adjust the permissible footstep region, expanding it to a larger, effectively non-convex area and allowing cross-over stepping, which is crucial for counteracting large lateral pushes. Additionally, our method adapts footstep timing in real time to further extend the range of recoverable disturbances. Based on these adjustments, feasible footstep positions and DCM trajectory are planned by solving a QP. Finally, we employ a DCM controller and an inverse dynamics whole-body control framework to ensure the robot effectively follows the trajectory.","I INTRODUCTION In humanoid robot locomotion, Divergent Component of Motion (DCM) trajectory generation [1] has become a widely used method for generating walking motions. Current research is evolving from focusing on motion generation to addressing the challenges of robustness and adaptability in the face of external disturbances. Model-based push recovery approaches often involve analytical calculation of footstep adjustments [2, 3] or optimization-based methods [4] to adjust timing and position of footsteps [5, 6]. Data-driven approaches have also been applied to generate push recovery strategies. Recent works [7, 8] used end-to-end Reinforcement Learning (RL) to acquire a variety of push recovery and balancing behaviors, such as ankle, hip, and stepping strategies, which are similar to those seen in humans [9]. While these methods adapt well to various disturbances, they often require extensive training data and significant computational resources. Model-based approaches typically use distinct control components, such as a low-level whole-body controller and a high-level trajectory planner to precisely follow pre-planned motions [10]. In contrast, data-driven approaches often employ end-to-end learning of joint positions [11, 12, 13, 14]. These methods tend to generalize better to unknown environments due to extensive domain randomization. For instance, [15] showcased robust blind walking on stairs using terrain randomization techniques. Moreover, [16] demonstrates navigation over difficult stepping-stone patterns effectively using vision input. Nevertheless, the success of end-to-end learning approaches largely depends on the precise design of the reward function. Figure 1: Simulation of the robot kangaroo during a lateral push force of 800N for 0.1 seconds. Our method allows the robot to quickly recover from such a large disturbance through a leg cross-over and simultaneous adjustment in step timing. Hybrid approaches that combine model-based and data-driven methods have emerged as promising solutions, aiming to take advantage of both [17, 18]. Duan et al. [19] propose integrating robot system knowledge into reinforcement learning to train bipedal locomotion policies directly in task space, improving sample efficiency and demonstrating the approach in simulation and on the real robot Cassie. Castillo et al. [20] achieved a lightweight network structure and sample efficiency through an RL framework, where actions parameterize desired joint trajectories rather than direct actuator inputs. In [21], the same authors propose a hierarchical approach that uses RL at the high level to train policies for task space commands and employs a model-based low-level controller to track these trajectories. Both policies demonstrate robust performance against various disturbance forces applied to the torso. From a model-based perspective, we can effectively find optimal solutions for previewed footstep placements using a QP approach. However, some drawbacks remain. The step time appears nonlinear in the solution of the DCM dynamics. Thus, iterative solutions or a general nonlinear optimization are required. Additionally, using a QP restricts possible footstep regions to convex shapes, thereby reducing the solution space and excluding options such as cross-over stepping. Recent works have addressed this problem by decomposing the non-convex regions into convex subregions by a set of rules [22] or by evaluating the feasibility of the resulting QP problem [23]. The contribution of this work lies in enhancing model-based trajectory planning by integrating RL to address the limitations of the QP approach. This is achieved by using an RL agent to dynamically adjust key parameters within the model-based control framework. The selected parameters are the step frequency, single support percentage, and rotation angle of a convex step area around the current stance foot. Thanks to the combination of RL with the model-based framework, our method significantly improves the maximal recoverable external disturbances, enhancing the robustness of the robot‚Äôs walking. Additionally, the division of tasks between RL and model-based controllers improves the learning efficiency of RL, enabling the training of a single environment in just a few hours."
https://arxiv.org/html/2411.00965v1,SPOT: SE(3) Pose Trajectory Diffusion for Object-Centric Manipulation,"We introduce SPOT, an object-centric imitation learning framework. The key idea is to capture each task by an object-centric representation, specifically the SE(3) object pose trajectory relative to the target. This approach decouples embodiment actions from sensory inputs, facilitating learning from various demonstration types, including both action-based and action-less human hand demonstrations, as well as cross-embodiment generalization. Additionally, object pose trajectories inherently capture planning constraints from demonstrations without the need for manually crafted rules. To guide the robot to execute the task, the object trajectory is used to condition a diffusion policy. We show improvement compared to prior work on RLBench simulated tasks. In real-world evaluation, using only eight demonstrations shot on an iPhone, our approach completed all tasks while fully complying with task constraints. Project page: https://nvlabs.github.io/object_centric_diffusion","Learning from demonstrations is an effective method for acquiring complex robotic manipulation skills. This approach simplifies learning vision-based manipulation policies by training directly on demonstration data, thus avoiding the need for intricate hand-crafted rules or laborious robot self-exploration in real-world settings. However, creating a structural representation that captures task-relevant information, remains resilient to environmental perturbations, and generalizes across the robot embodiment remains a significant challenge. Previous work [reed2022generalist, jang2022bc, ahn2022can] has utilized raw RGB inputs for end-to-end visuomotor policy training. However, the absence of explicit 3D representation makes it difficult to generalize to various viewing angles and out-of-distribution spatial configurations. To address these issues, RVT [goyal2023rvt] and PerAct [shridhar2023perceiver] use the entire 3D scene as input, represented through point clouds and 3D volumes. While these methods consider the overall 3D scene structure, they include excessive redundant information, reducing data efficiency and necessitating extensive demonstration data collection. Additionally, the tight coupling of observation and action pairs complicates deployment across robotic embodiments, camera setups, or learning from action-less data like human hand demonstrations. Object-centric methods address these problems by extracting object-centric information from visual observation, such as object detection [zhu2023viola], 6D object poses [wen2022you], 2D or 3D flows [zhu2024vision, xu2024flow, yuan2024general], to serve as inputs for downstream policies. By leveraging visual foundation models, these methods exhibit impressive generalization capabilities in new environments and demonstrate strong data efficiency, often requiring only a minimal amount of demonstration. Nevertheless, most works do not model the holistic object pose trajectories or focus only on the last-inch manipulation policy. This oversight necessitates extra human effort to manually create rules for constrained planning, especially in tasks where intermediate actions are crucial for success. For example, in tasks like pouring water or serving a plate, the container must remain upright throughout to prevent spillage. Such constraints are non-trivial to automatically learn with existing object-centric frameworks. Figure 1: We present SPOT, an imitation learning method that leverages object pose trajectories as an intermediate representation. Given the observation, our framework estimates the object‚Äôs pose relative to the target, predicts its future path in SE(3), and derives an action plan accordingly. Our diffusion model is trained on demonstration trajectories extracted from videos without needing action data from the same embodiment. To address these issues, we introduce SPOT, an object-centric diffusion policy framework that combines diffusion policy and object-centric representation for manipulation. As shown in Fig. 1, it uses the SE(3) object-centric trajectory relative to the target as the representation. This approach allows us to decouple the embodiment action from sensory input, making it easier to learn from various types of demonstration data, whether they involve action-based or action-less human hand demonstrations. In particular, opposite to the standard diffusion policy [chi2023diffusion, ze20243d] which directly produces robot end-effector action, our diffusion model learns to synthesize target SE(3) object-centric pose trajectories along the horizon, conditioned on the current observed object pose. The training data is obtained by extracting such object-centric information from demonstration videos. For robot execution, the action is computed from the task space to transport the object following the target synthesized trajectory. By recurrently switching between future trajectory synthesis and trajectory following, we achieve closed-loop control with enhanced robustness under dynamic uncertainty, such as in-hand object motion during grasping or placement. Unlike previous methods that focus solely on the last inch of manipulation, our approach trains on the pose trajectory data of the complete horizon, eliminating the need for manually crafted global planning rules before the last-inch stage. It also leverages the data scaling law when large amounts of demonstration data are available while maintaining data efficiency in few-shot demonstration scenarios. We evaluate the approach on RLBench [james2020rlbench], where it significantly outperforms in high-precision and long-horizon tasks in a challenging single camera setting. In real-world tasks, with just 8 demonstrations shot on an iPhone, the resulting policies complete all tasks while fully complying with task constraints. We will make all datasets and code available so others can reproduce and expand upon this work. In summary, our framework has the following desired properties: ‚àô‚àô\bullet‚àô Flexibility on diverse demonstration data. Our framework can learn from action-based robotic demonstrations and action-less data from human demonstrations. This flexibility enables easier data collection using accessible setups, such as a hand-held iPhone, without the need for robotic arms, as our experiments demonstrate. ‚àô‚àô\bullet‚àô Autonomy on complex task constraints. Our framework considers the entire sequential object pose trajectory. This approach allows it to automatically learn planning constraints from demonstrations, eliminating the need for manually crafted rules. ‚àô‚àô\bullet‚àô Closed-loop feedback. Our diffusion model offers control over the horizon of action prediction and execution, enabling closed-loop control based on updated observations. This enhances robustness in dynamic situations, like when an object slips during manipulation or placement. ‚àô‚àô\bullet‚àô Generalization. Our object-centric representation allows us to ignore the task-irrelevant information from the background and thus generalizes robustly to varying lighting conditions, scene setup, and sensory configurations. ‚àô‚àô\bullet‚àô Language-conditioned multi-task policy. Our framework inherits the benefit from diffusion models such that it can condition on language or other modalities to deploy on various tasks with a single set of weights, with negligible performance change compared to per-task policy training."
https://arxiv.org/html/2411.00781v1,Hazards in Daily Life?Enabling Robots to Proactively Detect and Resolve Anomalies,"Existing household robots have made significant progress in performing routine tasks, such as cleaning floors or delivering objects. However, a key limitation of these robots is their inability to recognize potential problems or dangers in home environments. For example, a child may pick up and ingest medication that has fallen on the floor, posing a serious risk. We argue that household robots should proactively detect such hazards or anomalies within the home, and propose the task of anomaly scenario generation. We leverage foundational models instead of relying on manually labeled data to build simulated environments. Specifically, we introduce a multi-agent brainstorming approach, where agents collaborate and generate diverse scenarios covering household hazards, hygiene management, and child safety. These textual task descriptions are then integrated with designed 3D assets to simulate realistic environments. Within these constructed environments, the robotic agent learns the necessary skills to proactively discover and handle the proposed anomalies through task decomposition, and optimal learning approach selection. We demonstrate that our generated environment outperforms others in terms of task description and scene diversity, ultimately enabling robotic agents to better address potential household hazards.","The development of Vision-Language Models (VLMs) has significantly improved household robots‚Äô ability to interact with the physical world in a more human-like manner Liu et al. (2024b, a); Cai et al. (2023); Majumdar et al. (2024). Among these models, the most popular paradigm for such robots is receiving instructions and performing corresponding operational tasks Yang et al. (2024); Driess et al. (2023); Ahn et al. (2022). However, a critical yet often overlooked scenario arises when no instructions are provided. According to survey data, 31% of cooking fires are caused by unattended equipment Ahrens (2020). Meanwhile, unintentional injuries are the predominant cause of death among children, particularly those aged 1-14 years, encompassing incidents such as drowning, falls, and accidental poisonings Worldwide (2022). A lack of adequate supervision is often identified as a significant contributor to many of these fatalities, especially in cases involving younger children Hymel et al. (2006); Williams and Kotch (2023). It would greatly benefit humans if household robots could monitor whether stoves and other fire sources are properly turned off and detect potential hazards in the home that could lead to falls or accidental poisonings. Many of these fires and unintentional injuries could be prevented. However, to the best of our knowledge, such robotics have yet to be implemented. Figure 1: Comparison of passively instructed robots and our proactive detection robot. Our paradigm creates benefits and convenience for safety, even in the absence of human presence. Hence, in this work, we propose AnomalyGen, which can generate diverse anomaly settings covering household hazards, hygiene management, and child safety in 3D simulation environments, enabling robots to develop proactive detection and problem-solving abilities, as shown in Figure 1. Specifically, we first devised a group brainstorming setting, where LLM-based agents collaborate to generate diverse and comprehensive anomaly scenarios. The motivation comes from the observation that simply prompting an LLM to generate hazard scenarios results in repetitive and similar settings. In contrast, group brainstorming in real-life meetings often leads to novel and creative ideas. Based on these task settings, AnomalyGen automatically constructs simulated anomalous scenes through carefully designed 3D asset retrieval, configuration, and scene setup steps. Finally, AnomalyGen guides household robots in developing detection and resolution abilities for handling anomalies. It reads textual descriptions of the simulated environment, including the 3D coordinates of assets, and automatically identifies potential anomalous tasks that require attention. AnomalyGen then decomposes the task into fine-grained sub-tasks and selects the most appropriate learning method for the household robot. In general, our AnomalyGen leverages language-based approaches to bridge the domain gap between foundational models and robot interaction, enabling operations such as control inputs, operational trajectories, and physical interaction. For the experiments, AnomalyGen constructs 111 diverse and comprehensive anomaly scenes, with human evaluation showing high quality and automatic metrics demonstrating greater diversity compared to previous human-crafted robotic datasets. Based on this simulation data, household robots are guided by AnomalyGen to learn and demonstrate a variety of skills across tasks such as rigid and articulated object manipulation and legged locomotion, achieving a task completion rate of 83%. Additionally, we conduct an error analysis highlighting the limitations of the current learning algorithm and VLM, identifying areas for future improvement and direction. Our contributions can be summarized as follows: Firstly, we introduce AnomalyGen, an unsupervised generative framework that enables household robots to autonomously detect and address anomalies without explicit instructions. Secondly, AnomalyGen creates a 3D simulation environment with 111 diverse hazard scenarios, generated through a collaborative brainstorming mechanism, significantly enhancing task diversity compared to previous datasets. Thirdly, AnomalyGen enables robots to autonomously identify anomalies, decompose tasks, and learn appropriate skills using an effective task decomposition and learning method with minimal human input."
https://arxiv.org/html/2411.02393v1,Adaptive Length Image Tokenization viaRecurrent Allocation,"Current vision systems typically assign fixed-length representations to images, regardless of the information content. This contrasts with human intelligence ‚Äîand even large language models‚Äîwhich allocate varying representational capacities based on entropy, context and familiarity. Inspired by this, we propose an approach to learn variable-length token representations for 2D images. Our encoder-decoder architecture recursively processes 2D image tokens, distilling them into 1D latent tokens over multiple iterations of recurrent rollouts. Each iteration refines the 2D tokens, updates the existing 1D latent tokens, and adaptively increases representational capacity by adding new tokens. This enables compression of images into a variable number of tokens, ranging from 32 to 256. We validate our tokenizer using reconstruction loss and FID metrics, demonstrating that token count aligns with image entropy, familiarity and downstream task requirements. Recurrent token processing with increasing representational capacity in each iteration shows signs of token specialization, revealing potential for object / part discovery. Code available at https://github.com/ShivamDuggal4/adaptive-length-tokenizer.","Representation learning (Bengio et al., 2013), which involves extracting meaningful and useful information from input observations, is crucial for decision-making. An effective representation should be compact while encoding all relevant information. However, what constitutes ‚Äúrelevant‚Äù information varies based on the specific task; for example, a coarse classification task may require a different latent representation compression factor for satisfactory performance compared to a task demanding perfect pixel-level reconstruction, which necessitates denser representations. This notion of a useful representation aligns closely with aspects of human intelligence (Legg & Hutter, 2007), particularly the concept of adaptive and variable-compressible representations (Hutter, 2006). Similarly, language models can describe content at various levels of abstraction depending on complexity, context (Graves, 2016; Dehghani et al., 2018), and familiarity (Baevski & Auli, 2018). In contrast, most current visual systems, such as VAEs, VQGANs, and ViTs (Kingma & Welling, 2022; Esser et al., 2020; Dosovitskiy et al., 2020), generate fixed-size representations for all images. In this work, we take a step toward learning adaptive and variable-length visual representations, emphasizing that each image requires a different representation capacity (see Sec. 4). Figure 1: Adaptive Length Image Tokenization maps an image to multiple variable-length representations through a recurrent token allocation process, enabling task-specific sampling. We learn the tokenizer via image reconstruction as a self-supervised objective. While a compressed representation can be optimized for specific tasks (e.g., fewer tokens for ‚Äúdog‚Äù, ‚Äúleaf‚Äù, ‚Äúgrass‚Äù may suffice for a VLM task), reconstruction objective supports learning a universal, task-agnostic tokenizer. A common framework for learning image embeddings or representations is the encoder-decoder approach, where an encoder compresses input data into a compact latent representation, which can later be decoded and compared with the original image as a learning objective. While there are other encoder-only methods, such as contrastive learning (Chen et al., 2021) and self-distillation (Caron et al., 2021), we focus on encoder-decoder approaches because a reconstruction objective intuitively promotes the learning of adaptive representations by capturing varying level-of-details necessary for better reconstruction. The current state-of-the-art (transformer-based) encoder-decoder approaches (Dosovitskiy et al., 2020) operate in the discrete token space, by encoding images into learned tokens and then decoding them back to image pixels. To generate these tokens, these approaches compress (slightly) at the input patch-level and then maintain the number of tokens (=== number of patches) throughout the encoder-decoder network depth. Thus, the representation length for all images is fixed to the number of tokens, equivalent to the fixed patch-size decided by the human-engineer. Moreover, by having number of tokens equal to number of patches, such approaches are tied to the natural 2D inductive bias of images, preventing any form of adaptive representation or compression of different images. Moving away from this inductive bias and with the goal of having modality-agnostic architecture, Google DeepMind proposed Perceiver (Jaegle et al., 2021b; a), a transformer-based architecture which distills input data tokens to a set of fixed 1D tokens. This process of latent-token distillation refers to compressing a higher-dimensional input (e.g., 2D image tokens) into a more compact set of latent variables (1D tokens), capturing the most relevant features. Like Perceiver, we also fall into the category of latent-token distillation, where we encode 2D image tokens into much fewer 1D latent tokens via a self-supervised reconstruction objective. While 1D-tokenization of an image overcomes the patch to token constraint and allows much more efficient compression of the input image, a more universal tokenizer would be one which adaptively assigns variable tokens to each input based on content entropy, familiarity etc (Sec. 4). Figure 2: Adaptive Length Image Tokenizer (ALIT): Given an image, we first convert it into 2D image tokens before applying the 2D ‚Üí‚Üí\rightarrow‚Üí 1D latent distillation. ALIT recurrently distills 2D image tokens into variable 1D latent tokens, with each iteration adding new latent tokens and processing them with the existing 2D image tokens and the old latent tokens. Training focuses on reconstructing 2D image tokens through reverse distillation from latent 1D to masked 2D tokens. Based on token-reconstruction quality, we can optionally mask specific 2D tokens from further processing, enabling dynamic halting per token. Recurrent processing with Adaptive Memory leads to compressible representations, flexible tokenization & specialized tokens focusing on objects/parts. We tackle the challenge of adaptive or variable-length representation learning by auto-regressively distilling input visual observations into an increasing number of 1D latent tokens. To achieve this, we draw inspiration from foundational works on recurrent computation (Graves, 2016; Dehghani et al., 2018). Recurrent neural networks are often viewed as adaptive thinking modules (Schwarzschild et al., 2021), capable of enhancing the computational requirements of a specific input through recursive processing with the same neural network architecture. Thus, unlike the Matryoshka style (Kusupati et al., 2022) approach of learning multiple representations of varying lengths simultaneously in one-go, we adopt a recurrent computing approach for visual representation learning. In our framework, recurrent computing involves recursively distilling an input image or 2D image tokens into 1D latent tokens through a shared encoder-decoder architecture until each image token has been sufficiently processed/distilled into the latent tokens. At each iteration of this recurrent rollout, we provide additional computational resources in the form of new learnable latent tokens, enabling the model to learn adaptive and variable-length representations across different iterations. We refer to our approach as ALIT (Adaptive Length Image Tokenizer), and train it using self-supervised image reconstruction objective. Credited to the increasing representational capacity, each recurrent update leads to the latent tokens specializing and attending to localized regions, hinting at object / part discovery (see, Fig. 8, Fig. 9, Appendix Fig. 13 and Fig. 16). We validate the effectiveness of the learned tokenizer by demonstrating comparable reconstruction metrics (L1 loss and FID) and linear probing results on ImageNet-1K, relative to the 2D VQGAN tokenizer (Esser et al., 2020) and the fixed-latent 1D tokenizer, Titok (Yu et al., 2024), while also allowing for flexible token counts per image. By utilizing variable representations per image and introducing cumulative dataset representations, we emphasize key aspects of effective representations: the required capacity aligns with image‚Äôs information entropy, familiarity, and knowledge of downstream tasks / models."
https://arxiv.org/html/2411.02158v1,Learning Multiple Initial Solutions to Optimization Problems,"Sequentially solving similar optimization problems under strict runtime constraints is essential for many applications, such as robot control, autonomous driving, and portfolio management. The performance of local optimization methods in these settings is sensitive to the initial solution: poor initialization can lead to slow convergence or suboptimal solutions. To address this challenge, we propose learning to predict multiple diverse initial solutions given parameters that define the problem instance. We introduce two strategies for utilizing multiple initial solutions: (i) a single-optimizer approach, where the most promising initial solution is chosen using a selection function, and (ii) a multiple-optimizers approach, where several optimizers, potentially run in parallel, are each initialized with a different solution, with the best solution chosen afterward. We validate our method on three optimal control benchmark tasks: cart-pole, reacher, and autonomous driving, using different optimizers: DDP, MPPI, and iLQR. We find significant and consistent improvement with our method across all evaluation settings and demonstrate that it efficiently scales with the number of initial solutions required. The code is available at https://github.com/EladSharony/miso.","Many applications, ranging from trajectory optimization in robotics and autonomous driving to portfolio management in finance, require solving similar optimization problems sequentially under tight runtime constraints (Paden et al., 2016; Ye et al., 2020; Mugel et al., 2022). The performance of local optimizers in these contexts is often highly sensitive to the initial solution provided, where poor initialization can result in suboptimal solutions or failure to converge within the allowed time (Michalska & Mayne, 1993; Scokaert et al., 1999). The ability to consistently generate high-quality initial solutions is, therefore, essential for ensuring both performance and safety guarantees. Conventional methods for selecting these initial solutions typically rely on heuristics or warm-starting, where the solution from a previously solved, related problem instance is reused. More recently, learning-based solutions have also been proposed, where neural networks are used to predict an initial solution. However, in more challenging cases, where the optimization landscape is highly non-convex or when consecutive problem instances rapidly change, predicting a single good initial solution is inherently difficult. To this end, we propose Learning Multiple Initial Solutions (MISO) (Figure 1), in which we train a neural network to predict multiple initial solutions. Our approach facilitates two key settings: (i) a single-optimizer method, where a selection function leverages prior knowledge of the problem instance to identify the most promising initial solution, which is then supplied to the optimizer; and (ii) a multiple-optimizers method, where multiple initial solutions are generated jointly to support the execution of several optimizers, potentially running in parallel, with the best solution chosen afterward. More specifically, our neural network receives a parameter vector that characterizes the problem instance and outputs KùêæKitalic_K candidate initial solutions. The network is trained on a dataset of problem instances paired with (near-)optimal solutions and is evaluated on previously unseen instances. Crucially, the network is designed not only to predict good initial solutions‚Äîthose close to the optimal‚Äîbut also to ensure that these solutions are sufficiently diverse, potentially spanning all underlying modes of the problem in hand. To actively encourage this multimodality, we implement training strategies such as a winner-takes-all loss that penalizes only the candidate with the lowest loss, a dispersion-based loss term to promote dispersion among solutions, and a combination of both. We evaluate MISO across three distinct local optimization algorithms applied to separate robot control tasks: First-order Box Differential Dynamic Programming (DDP), which utilizes first-order linearization for the cart-pole swing-up task; Model Predictive Path Integral (MPPI) control, a sampling-based method, for the reacher task; and the Iterative Linear Quadratic Regulator (iLQR), a trajectory optimization algorithm, for an autonomous driving task. Our results show that MISO significantly outperforms existing initialization methods that rely on heuristics, learn to predict a single initial solution or use ensembles of independently learned models. In summary, our key contributions are as follows: 1. We present a novel framework for predicting multiple initial solutions for optimizers. 2. We introduce two distinct strategies for utilizing the predicted initial solutions: (i) single-optimizer, where the most promising solution is chosen based on a selection function, and (ii) multiple-optimizers, where multiple optimizers are initialized, potentially in parallel, with the best solution chosen afterward. 3. We design and implement specific training objectives to prevent mode collapse and ensure that the predicted solutions remain multimodal. 4. We apply our framework to three distinct sequential optimization tasks and perform extensive evaluation. Figure 1: As opposed to previous works that predict a single initial solution, MISO trains a single neural network to predict multiple initial solutions. We use them to either initialize a single optimizer or jointly initialize multiple optimizers."
https://arxiv.org/html/2411.02102v1,Toward Realistic Cinema: The State of the Art in Mechatronics for Modern Animatronics,"The pursuit of realism in cinema has driven significant advancements in animatronics, where the integration of mechatronics, a multidisciplinary field that combines mechanical engineering, electronics, and computer science, plays a pivotal role in enhancing the functionality and realism of animatronics. This interdisciplinary approach facilitates smoother characters movements and enhances the sophistication of behaviors in animatronic creatures, thereby increasing their realism. This article examines the most recent developments in mechatronic technology and their significant impact on the art and engineering of animatronics in the filmmaking. It explores the sophisticated integration of system components and analyzes how these enhancements foster complexity and integration, crucial for achieving unprecedented levels of realism in modern cinema. Further, the article delves into in-depth case studies of well-known movie characters, demonstrating the practical applicability of these state-of-the-art mechatronic solutions in creating compelling, lifelike cinematic experiences. This paper aims to bridge the gap between the technical aspects of mechatronics and the creative demands of the film industry, ultimately contributing to the ongoing evolution of cinematic realism.","Prior to the widespread adoption of mechatronic technology, the film industry faced numerous mechanical constraints that significantly affected production efficiency and creative possibilities. The integration of mechatronics‚Äîan interdisciplinary field combining mechanical engineering, electronics, computer science, and control engineering‚Äîhas transformed these challenges into opportunities for innovation. Historically, the film industry relied on traditional mechanical systems for camera operation, lighting, and set design. These systems often posed limitations such as rigidity and lack of flexibility [1], labor-intensive processes [2], precision limitations [1] and the lack of integration between different mechanical systems made it difficult to synchronize various elements of production, such as camera movements with lighting changes or special effects. These challenges restricted both the art framework for creativity and the technical capabilities of cinema production, as well as limited the feasible speeds of old film copying methods. These earlier mechanisms, which mainly relied on mechanical gears and sprockets, frequently experienced problems that weakened the quality of the final film production [3]. Nowadays, mechatronics plays a crucial role in scenes that combine practical effects with computer-generated imagery (CGI). Animatronics provide a physical presence that can be enhanced with CGI, giving film-makers more flexibility to achieve visual effects that were previously impossible [3]. For instance, the subtle movements of facial features or the fluid motion of limbs are much more refined with mechatronic technology. Traditional animatronics relied heavily on simple mechanics like pneumatics and hydraulics, which limited their range of motion. Mechatronics allows for the incorporation of advanced robotics into creature design, which can simulate muscle movement and facial expressions more naturally and with greater control. With advancements in mechatronics, designers can program creatures to perform complex sequences of actions that can be adjusted and repeated with high precision. This programmability also enhances the safety and repeatability of complex scenes, reducing the need for retakes and decreasing production costs. Mechatronic creatures can interact more effectively with human actors and environments due to sensors and real-time processing capabilities. These features allow the creatures to respond to external stimuli, adding an unscripted element to interactions that can enhance the believability of scenes. Overall, mechatronics has enabled film-makers to push the boundaries of what is visually possible, creating more engaging and believable non-human characters that captivate audiences. In general, scholarly publications about animatronics in motion pictures enhance our comprehension of the convergence of technology, art, and narrative in the motion picture business. This article explores novel ways to use animatronics to animate movie characters through innovative processes. It examines the most recent developments in mechatronic technology and their significant impact on the art and engineering of animatronics in film-making. The paper aims to explore the sophisticated integration of system components and analyze how these enhancements foster complexity and integration, crucial for achieving unprecedented levels of realism in modern cinema. Additionally, the article delves into in-depth case studies of well-known movie characters, demonstrating the practical applicability of these state-of-the-art mechatronic solutions in creating compelling, lifelike cinematic experiences. This paper is organized as follows: Section 2 provides the theoretical and technical aspects of mechatronics with practical applications in the film industry, offering a comprehensive overview of how these systems are integral to the evolution of cinematic technology and artistry. Section 3 showcases notable examples of animatronics in films, demonstrating the diverse applications of this technology across different models such as humans, sharks, bees, bats, monsters, and vampires. Section 4 reviews the latest technological advancements in mechatronics relevant to film-making and analyzes how these advancements have facilitated smoother and more sophisticated movements in animatronic creatures. Section 5 explores the significant impact of mechatronics on the design and realism of cinematic creatures, highlighting the technological advancements that have enabled more dynamic and interactive performances. Section 6 discusses the future of creature animation, speculating on future trends and potential innovations in mechatronics that could further transform film-making. Section 7 provides in-depth case studies of notable movie characters enhanced through mechatronics, demonstrating the practical application and specific mechatronic solutions used to achieve lifelike effects. Finally, Section 8 summarizes the key points discussed, reflecting on the transformational role of mechatronics in animatronics and its broader implications for the film industry. Figure 1: Integrative Framework of Mechatronics Figure 2: Animatronic Face Mechanism, Huber et al. [4]"
https://arxiv.org/html/2411.01850v1,ManiBox: Enhancing Spatial Grasping Generalization via Scalable Simulation Data Generation,"Learning a precise robotic grasping policy is crucial for embodied agents operating in complex real-world manipulation tasks. Despite significant advancements, most models still struggle with accurate spatial positioning of objects to be grasped. We first show that this spatial generalization challenge stems primarily from the extensive data requirements for adequate spatial understanding. However, collecting such data with real robots is prohibitively expensive, and relying on simulation data often leads to visual generalization gaps upon deployment. To overcome these challenges, we then focus on state-based policy generalization and present ManiBox, a novel bounding-box-guided manipulation method built on a simulation-based teacher-student framework. The teacher policy efficiently generates scalable simulation data using bounding boxes, which are proven to uniquely determine the objects‚Äô spatial positions. The student policy then utilizes these low-dimensional spatial states to enable zero-shot transfer to real robots. Through comprehensive evaluations in simulated and real-world environments, ManiBox demonstrates a marked improvement in spatial grasping generalization and adaptability to diverse objects and backgrounds. Further, our empirical study into scaling laws for policy performance indicates that spatial volume generalization scales positively with data volume. For a certain level of spatial volume, the success rate of grasping empirically follows Michaelis-Menten kinetics relative to data volume, showing a saturation effect as data increases. Our videos and code are available in the project page.","Robotic manipulation in dynamic environments is pivotal for advancing modern robotics (Fu et al., 2024). Equipping robots with precise grasping capabilities in unstructured settings not only extends their utility beyond traditional static scenarios but also enhances their real-world effectiveness, especially in industrial and domestic settings. A robot policy that achieves spatial generalization‚Äîdefined as the ability of a manipulation model to complete tasks regardless of the target object‚Äôs position within a defined spatial volume‚Äîwould be particularly practical. Such generalization is vital when deploying robots in diverse environments, such as different households, where they must handle various objects on different platforms. The capability to perform successfully across these varied spatial conditions is essential, as objects in real-world settings are rarely fixed in place, and effective manipulation demands adaptability to a broad range of spatial challenges. Recent works have made notable progress in robotic manipulation in multi-task and multi-environment scenarios, which is largely driven by end-to-end training methodologies (Ahn et al., 2022; Shridhar et al., 2022; 2023; Zitkovich et al., 2023). Notably, some large embodied foundation models for manipulation, such as those fine-tuned from Vision-Language Models (VLMs) for multi-task robot manipulation tasks (Brohan et al., 2022; Kim et al., 2024), have made significant strides. However, these large manipulation models derived from VLMs often lack sufficient spatial awareness and reasoning capabilities, posing substantial challenges for large-scale spatial generalization in real-world applications (Cheng et al., 2024a). Our analysis in Lemma 1 suggests that achieving spatial generalization over larger volumes requires substantially more data, a relationship we further empirically verify to follow a positive scaling law (see Figure 4). For example, training a model to grasp at a fixed point may require only 50 to 600 trajectory datasets. However, when generalizing the model to operate within a spatial volume of 34,400‚Å¢c‚Å¢m334400ùëêsuperscriptùëö334,400cm^{3}34 , 400 italic_c italic_m start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT (41‚Å¢c‚Å¢m√ó30‚Å¢c‚Å¢m√ó28‚Å¢c‚Å¢m41ùëêùëö30ùëêùëö28ùëêùëö41cm\times 30cm\times 28cm41 italic_c italic_m √ó 30 italic_c italic_m √ó 28 italic_c italic_m), which approximates the maximum reach of our robotic arm, the data requirement increases by roughly 40-fold, demanding tens of thousands of trajectories However, collecting such a large volume of data on real robots is prohibitively expensive. For instance, RT-1 required 17 months to accumulate 130K episodes (Brohan et al., 2022), which is a level of resource commitment unfeasible for most research labs. Although simulation data can help bridge this gap, significant high-dimensional visual discrepancies between simulated and real environments, known as the Sim2Real gap, pose additional challenges. While prior efforts have aimed to enhance performance using various visual generalization techniques, each method comes with its drawbacks. Some demand unobstructed third-person perspectives (Yuan et al., 2024), some consume substantial computational resources (Shridhar et al., 2023; Ahn et al., 2022). These requirements can impede real-time processing and practical deployment on resource-limited robotic platforms. While perception models are essential for object and environmental recognition, the true effectiveness of a robot in dynamic settings hinges on the robustness of its policy execution. A well-generalized policy empowers the robot to autonomously adjust its actions in real-time, effectively compensating for visual inaccuracies and adapting to changes in object positions or environmental dynamics. Such adaptability is crucial for achieving task success and maintaining system stability, especially in the face of imperfect or noisy perception data. To further enhance the robot‚Äôs ability to manage unpredictable scenarios and ensure reliable real-world performance, we shift our focus to state-based policy generalization instead of relying solely on vision-based methods. Our Lemma 2 demonstrates that bounding boxes, when captured from multiple cameras, effectively encapsulate the 3D structure of convex objects, providing an optimal low-dimensional state for policy generalization. Building on this foundation, we introduce a novel bounding-box-guided methodology ManiBox to enhance spatial generalization and adaptability. In the simulator, ManiBox utilizes reinforcement learning (RL) with privileged information to train a sophisticated teacher policy. This policy generates scalable robot trajectory data, replacing traditional visual inputs with bounding box coordinates to facilitate spatial generalization of grasping policies. Subsequently, the student policy, trained on this simulation data, achieves robust zero-shot transfer to real-world tasks, leveraging bounding boxes identified by advanced open-vocabulary detection models, such as YOLO-World (Cheng et al., 2024b). Extensive experiments with both simulators and real robots demonstrate a direct correlation between data volume and spatial generalization performance: more data consistently leads to higher grasping success rates, and generalizing to larger spatial volumes requires even more data. For instance, to reliably grasp objects within a 1000‚Å¢c‚Å¢m31000ùëêsuperscriptùëö31000cm^{3}1000 italic_c italic_m start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT workspace, tens of thousands of trajectories from various positions are required. ManiBox achieves nearly perfect success rates in real-world grasping tasks by leveraging large-scale simulation data generated across diverse spatial volumes. Furthermore, ManiBox exhibits robust generalization, successfully grasping objects in new scenarios, including unfamiliar object types, complex surfaces, changing lighting conditions, and environments with distractions. Our contributions include: (1) We introduce ManiBox, an innovative bounding-box-guided manipulation method that addresses the limitations of visual generalization, offering a robust solution for spatial generalization in grasping tasks; (2) ManiBox significantly enhances adaptability to diverse spatial positions, object types, and backgrounds, as demonstrated by extensive experiments in both simulated and real-world environments; (3) Through scalable simulation data generation, we provide novel insights into the relationship between data volume and spatial generalization, establishing a framework for improving generalization across a variety of embodied systems."
https://arxiv.org/html/2411.01796v2,Constrained Human-AI Cooperation: An InclusiveEmbodied Social Intelligence Challenge,"We introduce Constrained Human-AI Cooperation (CHAIC), an inclusive embodied social intelligence challenge designed to test social perception and cooperation in embodied agents. In CHAIC, the goal is for an embodied agent equipped with egocentric observations to assist a human who may be operating under physical constraints‚Äîe.g., unable to reach high places or confined to a wheelchair‚Äîin performing common household or outdoor tasks as efficiently as possible. To achieve this, a successful helper must: (1) infer the human‚Äôs intents and constraints by following the human and observing their behaviors (social perception), and (2) make a cooperative plan tailored to the human partner to solve the task as quickly as possible, working together as a team (cooperative planning). To benchmark this challenge, we create four new agents with real physical constraints and eight long-horizon tasks featuring both indoor and outdoor scenes with various constraints, emergency events, and potential risks. We benchmark planning- and learning-based baselines on the challenge and introduce a new method that leverages large language models and behavior modeling. Empirical evaluations demonstrate the effectiveness of our benchmark in enabling systematic assessment of key aspects of machine social intelligence. Our benchmark and code are publicly available at https://github.com/UMass-Foundation-Model/CHAIC.","Humans possess a remarkable ability to observe, infer, and help others, even when others have different mental models and physical constraints in the world from themselves (Warneken and Tomasello, 2006). From a young age, humans are able to watch other people attempt to perform a task, and if other people fail, they can develop plans of action that best assist them. In contrast, AI agents struggle to exhibit such basic social skills and fail to adjust their plans for the specific humans they wish to aid (Valmeekam et al., 2022; Ngo et al., 2022), rendering them poor personalized helpers. For AI agents to best assist human partners in performing tasks in the real world, they must possess two fundamental capabilities: (1) contextual perception, i.e., the ability to follow and observe human behavior and identify the specific goals and constraints faced by each human; and (2) cooperative planning, i.e., the ability to plan actions that are best tailored to helping each human with different goals and constraints. While there have been some embodied benchmarks and environments designed to test general multi-agent intelligence (Puig et al., 2021, 2023b; Gan et al., 2021), such efforts have largely excluded the unique accessibility challenges that real humans may possess in the world and neglect the differences among individuals. Moreover, outdoor scenarios and emergencies are also prevalent in human life, but receive little attention in the embodied intelligence community (Deitke et al., 2022). This paper introduces the first large-scale embodied social intelligence challenge with accessibility explicitly in mind: Constrained Human-AI Cooperation (CHAIC). In this challenge, an embodied agent with egocentric visual observation must actively perceive and cooperate with a human partner possibly with physical constraints in a near photo- and physically realistic virtual environment to complete common household and outdoor tasks as efficiently as possible. This is motivated by the idea that people who need the most help from autonomous agents are those who are currently not explicitly accounted for in embodied intelligence frameworks. In CHAIC, a helper agent needs to follow and observe the human partner to infer their goals and constraints; then, the agent plans a user-tailored strategy for aiding the human in efficiently performing tasks together; moreover, with the existence of unexpected emergencies, the agent needs to be reactive and adjust its strategy accordingly. To create the challenge with accessibility in mind, we design and implement four new agents with real physical constraints that reflect the rich diversity of human partners in the real world. For example, a human partner confined to a wheelchair struggles to move past obstacles or a human partner struggles with heavy furniture when moving house in an outdoor scene, shown in Figure 1, and eight long-horizon tasks featuring both indoor and outdoor scenes on top of the ThreeDWorld (Gan et al., 2021), explicitly motivating the development of embodied agents that prioritize accessibility efforts when learning and planning and can thrive in rich scenarios. We benchmark several baseline models, including planning- and learning-based agents, especially those powered by foundation models. We also introduce a new method for building agents that combines the behavior modeling capabilities of video models with the reasoning ability of large language models. Our benchmark results suggest that current baselines have difficulty modeling partner behaviors from raw RGB images, and LLM-driven agents are competitive agents in decision-making. We hope this new challenge will advance the study of social intelligence in embodied agents in complex scenarios including diverse human partners with constraints and rich indoor and outdoor scenes. This initiative calls on the community to develop and evaluate embodied agents with a strong emphasis on accessibility and inclusivity. Our contributions include: ‚Ä¢ We design and implement four new agents with real physical constraints and eight long-horizon tasks featuring both indoor and outdoor scenes on top of ThreeDWorld (Gan et al., 2021), simulating rich human constraints and scenarios in the real world. ‚Ä¢ We introduce a new embodied social intelligence challenge with accessibility explicitly in mind: Constrained Human-AI Cooperation (CHAIC), to test embodied agents‚Äô ability to actively perceive human partners‚Äô intents and constraints from egocentric visual observations and make user-tailored cooperative plans to help constrained human partners in rich scenarios. ‚Ä¢ We benchmark several baseline models, including those powered by foundation models, especially a new agent with behavior modeling introduced by us, and conduct comprehensive analyses to identify and discuss the persisting challenges related to inter-agent perception and cooperation within complex environments."
https://arxiv.org/html/2411.01725v1,A Probabilistic Formulation of LiDAR Mapping with Neural Radiance Fields,"In this paper we reexamine the process through which a Neural Radiance Field (NeRF) can be trained to produce novel LiDAR views of a scene. Unlike image applications where camera pixels integrate light over time, LiDAR pulses arrive at specific times. As such, multiple LiDAR returns are possible for any given detector and the classification of these returns is inherently probabilistic. Applying a traditional NeRF training routine can result in the network learning ‚Äúphantom surfaces‚Äù in free space between conflicting range measurements, similar to how ‚Äúfloater‚Äù aberrations may be produced by an image model. We show that by formulating loss as an integral of probability (rather than as an integral of optical density) the network can learn multiple peaks for a given ray, allowing the sampling of first, nthsuperscriptnth\text{n}^{\text{th}}n start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPT, or strongest returns from a single output channel. Code is available at https://github.com/mcdermatt/PLINK","Neural Radience Fields (NeRFs) provide continuous representations of scenes by storing information about the surrounding world inside the weights of a neural network \parencitenerf. Recent works have extended NeRFs from camera images to LiDAR point clouds for use in localization \parencitepan2024pin, odometry \parencitenerfloam, path planning \parenciteshubPathPlanning, and data augmentation \parenciteNFL, lidarNeRF. To date, LiDAR applications of NeRFs have assumed a deterministic model of the scene. In existing approaches, a LiDAR depth map is computed from the network output in the same way as one may be produced from a conventional NeRF, where the weighted sum of opacity at various test points along each ray is used to estimate depth in that direction \parenciteNFL, lidarNeRF, nerflidar. Researchers have identified several geometry-based tweaks to enhance the accuracy of LiDAR NeRFs within this deterministic formulation. For instance, Huang et al. introduced a beam spreading model to enable their algorithm NFL to compute range and reflectance accurately for oblique surfaces \parenciteNFL. Tao et al. take a simpler approach, with their algorithm LiDAR-NeRF, masking oblique surfaces to avoid associated complications \parencitelidarNeRF. NeRF-LOAM improves sample efficiency by learning an octree map to focus the network on occupied regions of the scene \parencitenerfloam. Similarly, SHINE Mapping uses a hierarchical spatial structure to encode information and varying levels of detail \parenciteshine. Rather than learning a single large network, PIN-SLAM achieves a scene representation through many small localized networks in a voxel structure, each relative to a learnable pose parameter, rather than a fixed location in SO(3), which allows for loop closure without retraining the model \parencitepan2024pin. Despite their impressive accuracy over most segments of a scene, existing algorithms struggle to handle viewpoints from which multiple returns are recieved, since the measured range value is essentially probabilistic. Of the above methods, only NFL attempts to address the problem, electing to learn two separate depth channels to represent the first and second returns from any viewpoint \parenciteNFL. However, NFL always assumes exactly two returns, never more and never fewer. In this paper, we contend that a small but impactful change to NeRF allows for the direct representation of multiple possible returns, as can occur when a LiDAR views a surface and beyond (as for a window or loose foliage). Acknowledging an element of randomness in the training data, we seek to learn a probabilistic representation of depth, rather than a single optimal depth value along each ray emanating from the sensor. As such, the network is trained to represent the world as a probabilistic density, rather than as a deterministic optical density, as in the case of a traditional NeRF. Browning et al. introduced a similar stochastic volumetric world model for LiDAR sample generation \parencitebrowning20123d, however, to the best of our knowledge, such a representation has not been combined with NeRF or other differentiable rendering techniques. The remainder of this paper explains our contribution and demonstrates performance in generating continuous world representations from real LiDAR data. In section II, we concisely but quantitatively define the problems associated with LiDAR data reconstruction. In sections III and IV, we propose a solution and provide implementation details. In Section V we introduce and discuss the results of our proposed method, which we implement and share in our Probabilistic LiDAR NeRF Codebase (PLiNK), which we evaluate across two experiments. The first experiment involves an architectural scene from the Newer College Dataset \parencitenewerCollege, and the second involves a simulated drive through an urban environment in the Mai City Dataset \parencitemaicity. Finally, in section VI we discuss our results and suggest directions for future work."
https://arxiv.org/html/2411.01608v1,GITSR: Graph Interaction Transformer-based Scene Representation for Multi Vehicle Collaborative Decision-making,"In this study, we propose GITSR, an effective framework for Graph Interaction Transformer-based Scene Representation for multi-vehicle collaborative decision-making in intelligent transportation system. In the context of mixed traffic where Connected Automated Vehicles (CAVs) and Human Driving Vehicles (HDVs) coexist, in order to enhance the understanding of the environment by CAVs to improve decision-making capabilities, this framework focuses on efficient scene representation and the modeling of spatial interaction behaviors of traffic states. We first extract features of the driving environment based on the background of intelligent networking. Subsequently, the local scene representation, which is based on the agent-centric and dynamic occupation grid, is calculated by the Transformer module. Besides, feasible region of the map is captured through the multi-head attention mechanism to reduce the collision of vehicles. Notably, spatial interaction behaviors, based on motion information, are modeled as graph structures and extracted via Graph Neural Network (GNN). Ultimately, the collaborative decision-making among multiple vehicles is formulated as a Markov Decision Process (MDP), with driving actions output by Reinforcement Learning (RL) algorithms. Our algorithmic validation is executed within the extremely challenging scenario of highway off-ramp task, thereby substantiating the superiority of agent-centric approach to scene representation. Simulation results demonstrate that the GITSR method can not only effectively capture scene representation but also extract spatial interaction data, outperforming the baseline method across various comparative metrics.","Autonomous vehicles have garnered significant research attention over the past two decades, driven by their substantial potential for societal and economical advancement. The efficient coordination of driving decisions among CAVs promises not only to enhance safety and operational efficiency but also to reduce energy consumption [1]. However, in dynamic traffic scenarios, the intricate interplay between scenarios and traffic participants presents formidable challenges for CAVs in making decisions that are safe, efficient, and comfortable [2]. The Internet of Vehicles (IoV) technology integrates Vehicle-to-Vehicle (V2V) and Vehicle-to-Infrastructure (V2I) communications with artificial intelligence (AI) to offer innovative solutions for CAVs to process dynamic traffic scene information and perform collaborative driving decisions [3]. In this context, methods based on deep reinforcement learning (DRL) are becoming more and more popular because the intelligent agent can continuously learn through interaction with the driving environment, extract environmental information through deep learning, and make decisions through reinforcement learning algorithms [4]. However, modeling and representing scene information effectively, processing and calculating it to adapt to various complex traffic environments, while achieving high-quality collaborative decision-making in real-time dynamic settings, has emerged as a formidable research challenge. Concurrently, the research on autonomous vehicle decision-making is increasingly focusing on more complex scenarios. The crux of the challenge lies in the representation of the state, which must encompass the elements, characteristics, and interactions in the dynamic scene. Addressing this will become one of the key issues of the DRL methods [5]. To this end, we introduce GITSR, a novel graph interaction Transformer-based scene representation framework for multi-vehicle collaborative decision-making. This framework leverages the Transformer architecture to capture scene information and employs a graph structure to model spatial interaction, thereby enhancing the multi-vehicle collaborative decision-making ability of reinforcement learning. Firstly, we extract features from the dynamic driving environment within the context of intelligent networking, meticulously considering both the local interaction and global communication attributes of CAVs. We perform local reconstruction reasoning on scene input information, introduce the Transformer module to process information and enhance understanding of surrounding traffic scene for CAVs. We conduct local reconstruction reasoning on the input scene information and introduce the Transformer module to process this data, thereby enhancing the CAVs‚Äô comprehension of the surrounding traffic environment. Then, we represent the dynamic traffic scene as a graph, based on global communication attributes, and introduce GNN to extract spatial interaction features. This approach is advantageous as it optimally utilizes the information from all CAVs within dynamic traffic scenarios. It aids CAVs in scene comprehension and the transmission of upstream and downstream information. Moreover, it establishes the spatial interaction dynamics of the traffic environment, optimizing the collaborative driving decision-making capabilities. The main contributions of this article can be summarized as follows: 1) A collaborative decision-making framework for intelligent connected vehicles that integrates Transformer and GNN is designed, which is tailored for scene extraction and interaction modeling from the perspective of state representation, thus significantly enhancing the state representation to improve the reinforcement learning effect. 2) A local representation method based on Transformer to reconstruct reasoning from scene features is proposed. This method reconstructs the scene representation with a focus on all CAVs and employs GNN to extract spatial interaction behaviors between the motion information of traffic participants. The GITSR framework can make full use of the information extracted from features to assist all CAVs in comprehending both local scene details and global interaction dynamics. 3) The framework is verified in a challenging interactive collaborative driving environment. The results show that GITSR has advantages over advanced algorithms in terms of safety, efficiency, and task success rate. At the same time, we have conducted an assessment of the influence of various components within the GITSR framework on its overall performance."
https://arxiv.org/html/2411.01396v1,Exploring the Edges of Latent State Clusters for Goal-Conditioned Reinforcement Learning,"Exploring unknown environments efficiently is a fundamental challenge in unsupervised goal-conditioned reinforcement learning. While selecting exploratory goals at the frontier of previously explored states is an effective strategy, the policy during training may still have limited capability of reaching rare goals on the frontier, resulting in reduced exploratory behavior. We propose ""Cluster Edge Exploration"" (CE2), a new goal-directed exploration algorithm that when choosing goals in sparsely explored areas of the state space gives priority to goal states that remain accessible to the agent. The key idea is clustering to group states that are easily reachable from one another by the current policy under training in a latent space and traversing to states holding significant exploration potential on the boundary of these clusters before doing exploratory behavior. In challenging robotics environments including navigating a maze with a multi-legged ant robot, manipulating objects with a robot arm on a cluttered tabletop, and rotating objects in the palm of an anthropomorphic robotic hand, CE2 demonstrates superior efficiency in exploration compared to baseline methods and ablations.","In recent years, Goal-Conditioned Reinforcement Learning (GCRL) (Andrychowicz et al., (2017)) has emerged as a powerful paradigm for training agents to accomplish diverse tasks in complex and dynamic environments. GCRL enables agents to learn goal-directed behaviors, allowing them to achieve specific objectives in a flexible and adaptive manner. However, a central challenge in GCRL lies in guiding agents to effectively explore their environment during training. The exploration problem in GCRL can be viewed as the task of setting goals for the agent during training to guide the agent‚Äôs environment navigation to collect exploratory data that improves its learning process. In this paper, we address this critical challenge by proposing a novel strategy for selecting exploration-inducing goals in GCRL. Because goal-conditioned policies excel at reaching states encountered frequently during training, a simple strategy is setting goals in less-visited areas of the state space to broaden the range of reachable states. However, throughout training, goal-conditioned policies may encounter difficulties in reaching arbitrary goals. For example, when instructed to navigate to an unexplored section of a maze environment, a novice agent might instead revisit a previously traversed area that provides low exploration value. To address this shortcoming, the environment exploration procedure must set up additional mechanisms to filter out unreachable goals. A common strategy in the literature is to select goals at the frontier of previously explored states and launch an exploration phase immediately after these goals are achieved, adhering to a Go-Explore principle (Ecoffet et al., (2019)). For example, Skewfit (Pong et al., (2019)) estimates state densities and selects goals at the frontier from the replay buffer in inverse proportion to their density. Similarly, MEGA (Pitis et al., (2020)) uses kernel density estimates (KDE) of state densities and selects frontier goals with low density from the replay buffer. However, precisely identifying the frontier of known states can be challenging with these heuristics. Even once the frontier is identified, the policy during training may still have limited capability of reaching rare goals on the frontier, resulting in reduced exploratory behavior. To address the aforementioned challenge, we propose a new goal-directed exploration algorithm, CE2 (short for ""Cluster Edge Exploration""). When choosing goals in sparsely explored areas of the state space, CE2 gives priority to goal states that remain accessible to the agent. For this purpose, our key idea is clustering to group known states that are easily reachable from one another by the current policy under training, and traversing to states holding significant exploration potential on the boundary of these clusters before doing exploratory behavior. In this way, our method accounts for the capability of the current policy for exploratory goals. First, a state cluster likely represents part of the state space where the training policy is familiar with. Second, given the easy accessibility of states within each cluster by the training policy, the agent‚Äôs capability extends to reaching states even at cluster boundaries. Moreover, less explored regions naturally reside adjacent to the periphery of state clusters. This Go-Explore strategy enables the agent to progressively broaden the coverage of each state cluster to effectively explore a novel environment. We instantiate CE2 in the context of model-based GCRL, demonstrating how learned world models can facilitate clustering environment states that are easily reachable from one another by the training policy in a latent space. We validate the effectiveness of CE2 in challenging robotics scenarios, including navigating a maze with a multi-legged ant robot, manipulating objects with a robot arm on a cluttered tabletop, and rotating objects in the palm of an anthropomorphic robotic hand. In each scenario, CE2 exploration results in more efficient training of adaptable GCRL policies compared to baseline methods and ablations."
https://arxiv.org/html/2411.01360v1,Use digital twins to support fault diagnosis from system-level condition-monitoring data,"Deep learning models have created great opportunities for data-driven fault diagnosis but they requires large amount of labeled failure data for training. In this paper, we propose to use a digital twin to support developing data-driven fault diagnosis model to reduce the amount of failure data used in the training process. The developed fault diagnosis models are also able to diagnose component-level failures based on system-level condition-monitoring data. The proposed framework is evaluated on a real-world robot system. The results showed that the deep learning model trained by digital twins is able to diagnose the locations and modes of 9999 faults/failure from 4444 different motors. However, the performance of the model trained by a digital twin can still be improved, especially when the digital twin model has some discrepancy with the real system.","Fault diagnosis is an essential task in reliability and predictive maintenance [1]. It collects and analyzes condition-monitoring data from sensors to diagnose the location and cause of failures [2]. The rapid advancements in artificial intelligence (AI) have dramatically transformed fault diagnosis, with deep learning-based models becoming prevalent and showing great success in both academia and industry [3]. For example, convolutional neural networks (CNNs) have been employed in automated fault detection for machinery vibrations [4], while recurrent neural networks (RNNs) like LSTM have proven useful in diagnosing faults based on time series data [5]. The deep learning-based fault diagnosis models, despite of their wide applications and great success, face notable limitations. First, they often require extensive amounts of labeled training data that are difficult to obtain in practice [1]. Second, the majority of deep learning-based models rely on detailed, component-level monitoring data to accurately detect and localize component-level failure [2]. For example, to detect and diagnose bearing failure, most existing models use bearing-level condition-monitoring signals like vibration, noise, etc [4]. In a large number of scenarios, however, condition-monitoring data can only be collected at the system-level, not the component-level, due to cost constraints, sensor limitations, or the physical inaccessibility of specific components. In this paper, we attempt to address these two issues by leveraging the high-fidelity simulation and real-time updating capability of digital twins [6]. First, we present a new reference model of digital twin, called digital failure twin, that is specially designed for modeling and simulating failure behavior and support failure-related decision-making. Second, we demonstrate, through a real-world case study on a robot, how to use a digital failure twin to develop component-level fault diagnosis model from system-level condition-monitoring data. Finally, we created an open-source dataset that could serve as a benchmark for further research on digital twin-supported fault diagnosis and predictive maintenance studies."
https://arxiv.org/html/2411.01297v1,Receding Hamiltonian-Informed Optimal Neural Control and State Estimation for Closed-Loop Dynamical Systems,"This paper formalizes Hamiltonian-Informed Optimal Neural (Hion) controllers, a novel class of neural network-based controllers for dynamical systems and explicit non-linear model predictive control. Hion controllers estimate future states and compute optimal control inputs using Pontryagin‚Äôs Maximum Principle. The proposed framework allows for customization of transient behavior, addressing limitations of existing methods. The Taylored Multi-Faceted Approach for Neural ODE and Optimal Control (T-mano) architecture facilitates training and ensures accurate state estimation. Optimal control strategies are demonstrated for both linear and non-linear dynamical systems.","Optimal control problems often involve designing controllers for systems with complex, chaotic, and/or non-linear dynamics. These problems are crucial in sectors such as unmanned aerial vehicles (UAVs) flight controllers, robotics, and nuclear power plants Salzmann et al. (2023); Katayama et al. (2023); Naimi et al. (2022). Various methods have been developed to address these problems. Solutions include dynamic programming, bang-bang controllers, proportional-integral-derivative (PID) controllers, linear-quadratic regulators (LQR), reinforcement learning (RL), and many variants of model predictive control (MPC). However, these methods often encounter challenges in delivering solutions that are both optimally effective and practical. Some methods react to deviations without considering the optimality of the control, while others can be expensive to operate in practice Schwenzer et al. (2021); Bemporad et al. (2002). Neural network approaches also grapple with their own unique challenges to generate solutions that consider accurate system dynamics. The quality of the control is often contingent on the quality of the training data Zheng et al. (2023). Among the developed methods, MPC is intriguing as it considers the effects of current control actions on future states. Nonetheless, many methods fail to address optimality conditions or computational efficiency when real-time optimization of the control is required Bemporad et al. (2002). To address these challenges, this chapter introduces a new class of neural network-based controllers for dynamical systems: Hamiltonian-Informed Optimal Neural (Hion) controllers, along with a novel architecture, the Taylored Multi-Faceted Approach for Neural ODE and Optimal Control (T-mano). Hion controllers are a type of explicit MPC neural network-based models that map an observed and desired state to a continuous control strategy and expected future states. The objective is to optimize the parameters of the controller to provide state estimation and control that not only adheres to a system dynamics but also follows a given transient response profile. The model is intended to operate in a closed-loop system, where it can cope with delays in receiving state information by predicting the system‚Äôs expected future behavior. Hion controllers offer a new alternative to RL methods and other MPC-based approaches for controlling dynamical systems such as UAVs and other robots. 1.1 Background Model-predictive control (MPC) defines a set of algorithms that utilize future state estimation to generate control strategies for a system Schwenzer et al. (2021). They often involve iteratively solving control optimization problems for a receding horizon in a closed-loop environment. Classical MPCs repeatedly linearize the plant‚Äôs dynamics at each iteration. These are used for state prediction and to solve for a zero-order hold control strategies via dynamic programming. The linearization-based approach enables computationally effective state estimation, albeit at the expense of some dynamic accuracy. Non-linear MPCs (NMPCs) are a later attempt to incorporate the dynamics non-linearity by using surrogate models to reduce the computational burden of state predictions. Explicit MPCs form an overlapping category that solve the control optimization in advance and reuse the previously obtained solution when tested Schwenzer et al. (2021); Bemporad et al. (2002). Our proposed controller falls under the category of an explicit NMPC. Artificial neural networks with MPC (ANN-MPC) are a subset of MPC models that most often involve utilizing neural networks as the prediction models in a MPC-based control loop Wang et al. (2021); Pang et al. (2023); Hewing et al. (2020); Cavagnari et al. (1999). Commonly, they consist of training a neural network model to predict that expected future state of system after a given amount of time, relying on trajectories data collected in simulation or test environments. The advantage of these models is that they can reduce significantly the cost of classical computationally expensive prediction model with the approximation provided by the neural network. Hence, less time is needed before the next state of the system can be sampled and an action could be taken. Recurrent neural networks are used in a subset of these algorithms Jordanou et al. (2021); Ren et al. (2022). They involve feeding previously sampled state to the model during inference and passing its knowledge to future generation to improve its predictive capability. Although a significant number of strategies involve replacing the predictive component, a subset of these works attempt to replace the MPC controller entirely by training them to be surrogate using collected trajectories and the corresponding control observed by a larger computationally-expensive MPC model Rivera et al. (2024); Hertneck et al. (2018); √Ökesson and Toivonen (2006). However, the optimality, dynamics accuracy, and out-of-distribution performance can be impacted when using neural networks as surrogate models. Physics-informed neural network with MPC (PINN-MPC) seeks to improve even further the capability of the predictive models in the MPC loop. At their core, PINN-MPCs integrate information about the dynamics that result in improvement to the accuracy and precision of the future state estimation and the information passed to the control optimization Antonelo et al. (2024); Faria et al. (2024); Arnold and King (2021); Zheng et al. (2023). Physics-Informed Neural Nets for Control (PINC), introduced in Antonelo et al. (2024), was one of the first method proposed to use PINNs as the prediction model in a MPC strategy. It consist of training a PINN to predict an continuous estimation of future states for a fixed horizon given a constant control signal. With this method, more reliable prediction exists that can guide a control optimization. Additionally, due to the continuous state prediction, distinct control optimization strategies that rely on distinct sampling rate can be implemented or tested with a single PINN model. Limitations exist with these approaches. Although, a continuous set of future states of the system are estimated, PINN-MPCs often rely on a single predicted state to guide the control, due to the model reliance on zero-order hold control optimization. Few works exist that consider neural networks as physics-informed controller in dynamical systems. Fewer works exist that allow for the adjustment of the environment transient characteristic. Ours and newer approaches reinvents the idea of a controller when a neural network model is involved. Schiassi et al. (2022) illustrate the feasibility of training neural controllers using Bellman optimality principle, and how they can be extended using X-TFC for different initial and final conditions. Schiassi et al. (2021). D‚Äôambrosio et al. (2021); Barry-Straume et al. (2022); Chi (2024) are recent attempts to establish neural network models that encourages PMP optimality. The works demonstrated that PMP can be used to train a neural network model to predict an optimal trajectory with desired transient properties. However, several limitations exist. One such limitation is that each model was only demonstrated to solve TVBNP for a single predefined initial and final state. This makes them impractical for closed-loop systems control where they will vary. Any new boundary condition would require fine-tuning the model (e.g., using X-TFC). Building on top of these projects, our model provides a closed-loop MPC neural network-based controller and state estimator that generalizes for variable inputs. It also theoretically defines a set of dynamical systems for which the model may be utilized. Hion removes the classical control optimization step conducted by classical MPC models, ANN-MPC, and PINN-MPC, and use a single neural network as both the prediction and control model. Fig. 1 illustrates a comparison between the conceptual behavior of different model predictive controllers. (a) Classical MPC. (b) ANN-MPC. (c) PINN-MPC. (d) Hion (Our). Figure 1: Conceptual behaviors of closed-loop model predictive controllers. 1.2 Contributions Our research formalizes a novel class of neural network controllers, termed Hion controllers, designed to optimize control strategies within closed-loop dynamical systems. These controllers function as model predictive controllers, enabling predictive decision-making. Our contributions include: 1. Establishing a theoretical framework for Hion controllers. 2. Proposing a novel neural network architecture specifically tailored for state estimation and control of dynamical systems. 3. Developing algorithms for training Hion controllers that are aligned with Pontryagin‚Äôs Maximum/Minimum Principle and encourage optimal control. 1.3 Outline Following the introduction, the first section formally defines the problem of interest we aim to address. We then present our proposed methodology and the underlying theoretical principles that support it. The experimental results section demonstrates the effectiveness and capabilities of our architecture through its application to various dynamical systems. Finally, we conclude by highlighting the advantages and limitations of the method along with potential future directions of our work."
https://arxiv.org/html/2411.01286v1,Mixed-Integer MPC-Based Motion Planning Using Hybrid Zonotopes with Tight Relaxations,"Autonomous vehicle (AV) motion planning problems often involve non-convex constraints, which present a major barrier to applying model predictive control (MPC) in real time on embedded hardware. This paper presents an approach for efficiently solving mixed-integer MPC motion planning problems using a hybrid zonotope representation of the obstacle-free space. The MPC optimization problem is formulated as a multi-stage mixed-integer quadratic program (MIQP) using a hybrid zonotope representation of the non-convex constraints. Risk-aware planning is supported by assigning costs to different regions of the obstacle-free space within the MPC cost function. A multi-stage MIQP solver is presented that exploits the structure of the hybrid zonotope constraints. For some hybrid zonotope representations, it is shown that the convex relaxation is tight, i.e., equal to the convex hull. In conjunction with logical constraints derived from the AV motion planning context, this property is leveraged to generate tight quadratic program (QP) sub-problems within a branch-and-bound mixed-integer solver. The hybrid zonotope structure is further leveraged to reduce the number of matrix factorizations that need to be computed within the QP sub-problems. Simulation studies are presented for obstacle-avoidance and risk-aware motion planning problems using polytopic maps and occupancy grids. In most cases, the proposed solver finds the optimal solution an order of magnitude faster than a state-of-the-art commercial solver. Processor-in-the-loop studies demonstrate the utility of the solver for real-time implementations on embedded hardware.","Motion planning is a foundational task in vehicle autonomy. Often, motion planning algorithms serve as an intermediate autonomy level between high-level decision making and/or path-planning algorithms and low-level path-following controllers [1]. Given some local description of the environment, which is in general non-convex [2], these algorithms must construct collision-free trajectories and respect constraints on the vehicle‚Äôs motion [3]. Additional considerations may factor into the motion plan such as smoothness or comfort [3], and uncertainty mitigation or risk reduction [4, 5]. Motion planning is generally performed online, which makes computational efficiency a necessity. I-A Gaps in the Literature MPC is a widely used technique in AV motion planning. Trajectories generated by MPC-based motion planners have numerous features including dynamic feasibility, constraint satisfaction, and optimality in a receding horizon sense. A significant barrier to the widespread adoption of MPC-based AV motion planners is the computational challenge of solving MPC problems of sufficient expressiveness in real-time on embedded hardware [3]. Many real-time implementable approaches for MPC-based motion planning rely on approximate or local solutions. Examples include sequential convex programming [6, 7, 8], model predictive path integral control (MPPI) [9, 10], and model predictive contouring control [11]. A limitation of these approaches is that they are not guaranteed to find the globally optimal solution when there are non-convex constraints, as in the case of obstacle avoidance [2]. Several MPC formulations have been proposed to account for non-convexity in motion planning problems. For example, branching and scenario MPC account for non-convexity via enumerated scenarios and scenario trees [12, 13, 14]. In [15], collision avoidance is enforced in MPC via a dual formulation. Ref. [16] presents a mixed-integer MPC formulation for AV motion planning which is subsequently validated using high fidelity traffic simulations. A review of mixed-integer programming formulations for motion planning is given in [2]. MPC problems formulated as mixed-integer convex programs (e.g., [16, 17]) can be solved to global optimality using branch-and-bound methods [18]. Mixed-integer programs are often solved using general-purpose mixed-integer solvers, such as Gurobi [19] and MOSEK [20]. Specialized techniques for solving mixed-integer programs have also been proposed. In [17], a custom MIQP solver for motion planning is proposed and evaluated in robotics experiments. When compared to commercial mixed-integer solvers, the solver proposed in that study was found to be approximately 5-6 times faster than MOSEK and 1.5-2.5 times slower than Gurobi. Follow-on work by the same authors used a neural network to predict the optimal integer variables for the MIQPs, thus reducing the MIQPs to convex QPs at the expense of sub-optimality [21]. Similar neural network-based approaches were proposed in [22]. Advanced set representations have been leveraged for use in motion planning problem formulations. Zonotopes are used to reduce the complexity of a hyperplane arrangement description of the obstacle avoidance constraints in [23]. The obstacle-free space is described exactly using polynomial zonotopes in a nonlinear MPC formulation in [24], and using hybrid zonotopes in a mixed-integer formulation in [25]. In both [24] and [25], the advanced set representations resulted in reductions in optimization times when compared to a hyperplane arrangement description of the obstacle-free space. Both of these papers used general-purpose solvers rather than specialized motion planning solvers as in [17, 21]. Many of the aforementioned problem formulations (e.g., [17, 15, 16]) are based on a description of the obstacles and do not explicitly use a free space description. Formulating motion planning problems in terms of the obstacle-free space enables many useful problem specifications [26, 2], such as the ability to formulate the motion planning problem using an occupancy grid map (OGM). OGMs are widely used in robotics to fuse diverse sources of uncertainty within a unified, probabilistic description of the environment [27, 28, 29]. Occupancy probabilities can be generalized to risks or costs as described in [28]. Despite their ubiquity, most existing approaches for motion planning over OGMs cast the problem in terms of a binary formulation where occupancy probabilities are not directly utilized. For instance, chance constraints are used to construct a binary OGM in a stochastic MPC formulation in [30]. Clothoid tentacles are used for motion planning over binary OGMs in [31]. In [32], a barrier function is used with nonlinear MPC to prohibit an AV from entering cells above a certain occupancy probability. In [33], time-varying OGMs are used to calculate the feasible space for a sampling-based motion planner. For situations with significant uncertainty or for which there is no feasible way for the AV to avoid entering a cell with elevated occupancy probability, these approaches can be limiting. Ref. [34] directly incorporates occupancy probabilities into a sampling-based motion planner that uses trajectory parameterizations. To the best of the authors‚Äô knowledge, there are no prior publications showing MPC-based motion planning where cell occupancy probabilities or costs are directly used in the MPC cost function. I-B Contributions This article presents an approach to formulating and solving MIQPs for MPC-based motion planning. A hybrid zonotope set representation is used to represent the obstacle-free space, and its structure is exploited within the MIQP solver. The obstacle-free space is efficiently represented either as a general polytopic map or an OGM. Occupancy probabilities or costs associated with obstacle-free regions are incorporated into the MPC formulation. This article builds upon our previous work [35], which presented a branch-and-bound algorithm based on a notion of reachability between regions of the obstacle-free space and showed how the hybrid zonotope structure can be exploited within the QP sub-problems. The new contributions in this paper can be categorized as follows; (1) Convex relaxations: We show that certain hybrid zonotopes constructed from general polytopes and from OGMs have the property that their convex relaxation is their convex hull. We use this property in combination with ‚Äúreachability‚Äù constraints to tighten the QP sub-problems such that convergence can be reached with fewer branch-and-bound iterations. (2) Algorithm development: The branch-and-bound solver is modified to support multi-threading and warm-starting, and nodes are generated in such a way as to reduce the sensitivity of solution times to map complexity. The MPC formulation and MIQP solver are implemented in C++. (3) Region-dependent costs: In order to perform risk-aware planning, region-dependent costs are added to the MPC formulation. The obstacle avoidance-based branch-and-bound logic is modified accordingly. (4) Numerical results: The proposed approach is evaluated in desktop computer simulations and real-time processor-in-the-loop testing. When compared to a conventional mixed-integer constraint representation (unions of halfspace representation polytopes using the Big-M method [2]) and the state-of-the-art mixed-integer solver Gurobi [19], the proposed approach often finds the optimal solution one to two orders of magnitude faster."
https://arxiv.org/html/2411.01227v1,Rotational Odometry using Ultra Low Resolution Thermal Cameras,"[gagraphic.png] This letter provides what is, to the best of our knowledge, a first study on the applicability of ultra-low-resolution thermal cameras for providing rotational odometry measurements to navigational devices such as rovers and drones. Our use of an ultra-low-resolution thermal camera instead of other modalities such as an RGB camera is motivated by its robustness to lighting conditions, while being one order of magnitude less cost-expensive compared to higher-resolution thermal cameras. After setting up a custom data acquisition system and acquiring thermal camera data together with its associated rotational speed label, we train a small 4-layer Convolutional Neural Network (CNN) for regressing the rotational speed from the thermal data. Experiments and ablation studies are conducted for determining the impact of thermal camera resolution and the number of successive frames on the CNN estimation precision. Finally, our novel dataset for the study of low-resolution thermal odometry is openly released with the hope of benefiting future research.","\IEEEPARstart Odometry estimation is a fundamental aspect of any navigational device such as rovers, drones and cars [1, 2]. Using odometry, navigational devices can estimate key inertial measures such as their acceleration, their rotational and translational speed, and their position in the environment [3]. Traditionally, odometry is provided by using Inertial Measurement Units (IMUs) embedding an accelerometer, a gyroscope and a magnetic compass in one integrated sensor [4]. Thanks to Micro-Electro-Mechanical System (MEMS) technology, small-size IMU chips have become ubiquitous in many robotics and navigational applications [5]. On the other hand, using IMUs alone is known to suffer from growing estimation errors as the inertial measurement provided by the IMU are integrated through time [2]. This is due to the slowly-varying biases and non-idealities affecting the IMU readout [6]. For this reason, IMUs are often fused with visual data (from e.g., an RGB camera) forming a visual-inertial odometry system (VIO) [7]. The VIO fusion approach has been successfully used in many navigational settings to provide more precise positioning [8, 9], as well as in Simultaneous Localization and Mapping (SLAM) setups in order to concurrently map new environments while localizing the navigational device into the map [10, 11]. But using RGB cameras to form VIO systems also comes with the fundamental issue that cameras are greatly affected by lighting conditions [16]. This can lead to a significant degradation in the odometry estimation when using VIO systems in low-light and nigh-time conditions [9]. Hence, in order to increase the robustness of VIO systems to lighting conditions, the use of other sensing modalities such as radar, LIDAR, high-dynamic-range (HDR) cameras, event-based cameras and high-resolution thermal cameras have been explored in literature [15, 16, 9, 12]. Among these modalities, the use of thermal cameras has recently attracted a large attention due to its advantages in terms of sensing robustness, payload size and power consumption compared to the other aforementioned modalities [12, 17]. Indeed, radars are known to be power-hungry due to their use of multiple antennas with multiple power amplifiers used to attain the emitted power required at high frequency (e.g., 79-GHz is a typical frequency) [15]. LIDARs are still bulky [16] and both HDR and event-based cameras can be expensive while still not being immune to total night-time conditions [13, 19]. On the other hand, even though high-resolution thermal cameras can be both power- and size-efficient, they can still be expensive, costing in the ‚àº500similar-toabsent500\sim 500‚àº 500 $currency-dollar\$$ range [14, 18]. Figure 1: Data acquisition setup. The 24√ó32243224\times 3224 √ó 32 thermal camera is connected to a readout board which translates its I2C interface to a serial interface via USB. A 100‚Å¢Œº100Œº100\mu100 italic_ŒºF decoupling capacitor is used for providing a stable power supply to the thermal camera. The thermal camera is mounted on top of a servo motor controlled by a micro-controller via serial interface over USB. This setup enables the acquisition of thermal camera data while rotating the camera at precisely-controlled speeds. Figure 2: CNN architecture for the estimation of rotational speed from thermal camera data. The CNN is composed of two convolutional layers (with max pooling in between), followed by two fully-connected layers and an linear output layer. This small-size architecture has been designed with the aim of reducing the CNN compute complexity for potential implementation in CNN accelerator hardware [20]. In order to reduce the system cost of thermal-based odometry systems, this paper present what is, to the best of our knowledge, the first demonstration of thermal-based rotational odometry using an ultra-low-resolution (24√ó32243224\times 3224 √ó 32) thermal camera (reducing the sensor costs to the ‚àº50similar-toabsent50\sim 50‚àº 50 $currency-dollar\$$ range) [14, 18]. The contributions of this paper are the following: 1. We build a custom data acquisition setup for acquiring low-resolution (24√ó32243224\times 3224 √ó 32) thermal camera data with a precise control of the camera‚Äôs azimuth rotation speed in order to obtain a labelled dataset of thermal camera frames and rotational speed. 2. We study the use of small-scale Convolutional Neural Networks (CNNs) for regressing the rotational speed from the thermal camera frames. 3. We provide a study on the impact of thermal camera resolution and the number of consecutive input frames on the CNN odometry accuracy. 4. We release our dataset as open-source to help future research. This letter is organized as follows. Section 2 provides a description of our dataset and data acquisition hardware. Section 3 describes our CNN design choices and training approach. Section 4 describes our experimental results. Finally, Section 5 provides conclusions."
https://arxiv.org/html/2411.01226v1,MonoPlane: Exploiting Monocular Geometric Cues forGeneralizable 3D Plane Reconstruction,"This paper presents a generalizable 3D plane detection and reconstruction framework named MonoPlane. Unlike previous robust estimator-based works (which require multiple images or RGB-D input) and learning-based works (which suffer from domain shift), MonoPlane combines the best of two worlds and establishes a plane reconstruction pipeline based on monocular geometric cues, resulting in accurate, robust and scalable 3D plane detection and reconstruction in the wild. Specifically, we first leverage large-scale pre-trained neural networks to obtain the depth and surface normals from a single image. These monocular geometric cues are then incorporated into a proximity-guided RANSAC framework to sequentially fit each plane instance. We exploit effective 3D point proximity and model such proximity via a graph within RANSAC to guide the plane fitting from noisy monocular depths, followed by image-level multi-plane joint optimization to improve the consistency among all plane instances. We further design a simple but effective pipeline to extend this single-view solution to sparse-view 3D plane reconstruction. Extensive experiments on a list of datasets demonstrate our superior zero-shot generalizability over baselines, achieving state-of-the-art plane reconstruction performance in a transferring setting. Our code is available at https://github.com/thuzhaowang/MonoPlane.","I INTRODUCTION Reconstructing 3D planes from images is a fundamental task in 3D vision that has wide applications in downstream tasks, such as robotics, mixture reality and interior modeling. The goal of this task is to simultaneously detect planes and estimate plane parameters from single or multiple images. Traditional methods rely on multi-view cues [1], depth sensor inputs [2, 3], or certain heuristic scene assumptions [4], and utilize robust estimators like RANSAC [5] and MRFs [6] optimization to fit plane proposals and label pixels. These methods achieve desirable plane reconstruction results in the presence of depth or multi-view input. However, their performance may drastically degrade when the heuristic assumptions are broken, or encountering low-texture images, motion blurs and noisy or incomplete depth. Learning-based methods have been proposed to reconstruct 3D planes from a single image [7, 8, 9], multi-view images [10, 11, 12] or videos [13]. Thanks to the construction of large-scale benchmark datasets [14, 15], these methods have showcased remarkable performance in segmenting plane instances and regressing plane parameters on particular dataset. To achieve better plane reconstruction, researchers have proposed to leverage more advanced network architectures [7, 16], or utilize geometric primitives such as line segments [9] to assist, or optimize multi-view planes and camera pose in a joint manner [17, 12]. Despite the significant progress, such data-driven methods still require costly groundtruth labeling and more importantly, have shown poor generalization ability onto out-of-distribution data as shown in our experiments. However, transferring across different environments is a crucial capacity in real-world applications such as robot navigation or augmented reality. Recently, several works explore the possibility of achieving generalizable depth or normal estimation from single-view images [18, 19, 20]. These works leverage rich depth-relevant data from different sources such as stereo, RGB-D video, 3D movies, and mix them together to obtain large-scale training datasets. These trained neural networks demonstrate superior zero-shot accuracy on in-the-wild data. Some recent studies have leveraged these pre-trained models to assist in different 3D vision tasks such as neural 3D reconstruction and rendering [21, 22]. Indoor - Matterport [15] Outdoor - Synthia [23] In-the-wild Data [24] Figure 1: Sample results from diverse scenarios. Our method achieves superior generalizable 3D plane detection and reconstruction by incorporating generalizable cues and robust designs. Inspired by these recent successes using pretrained monocular geometric models, we aim to exploit the potential of monocular geometric cues in 3D plane reconstruction. Taking a single RGB image as input, the pretrained networks are employed to predict depth and surface normals. To make fully use of these geometric cues, we revisit the classical RANSAC [5] pipeline, and further propose a monocular cue guided proximity modeling approach upon the graph-cut RANSAC [25]. This specially-designed RANSAC pipeline is able to effectively utilize the monocular geometric information while being robust to handle noisy input. Then, an image-level multi-plane joint optimization is conducted via dense CRFs [26] to encourage more consistent plane masks by combining the RGB color information. In this way, our method, termed MonoPlane, without plane labels and domain-specific training, effectively predicts satisfactory 3D plane reconstruction results from a single RGB image. We further demonstrate that our single-view solution can be easily extended to sparse uncalibrated images. Experimental results across different benchmarks and in-the-wild data (Figure 1) showcase that our proposed framework is accurate, generalizable and scalable. To summarize, our contributions are three-fold: ‚Ä¢ We explore the use of monocular geometric cues for 3D plane reconstruction, delivering generalizable 3D plane reconstruction from a single or sparse RGB images. ‚Ä¢ We propose a point-proximity based graph-cut RANSAC algorithm by incorporating monocular geometric cues, which greatly enhanced the robustness on handling noisy input or complex scenes. ‚Ä¢ We build a generalizable 3D plane reconstruction system for both single image and sparse-view images without any plane groundtruth or domain-specific finetuning. Experiments on various data from diverse environments clearly demonstrate the effectiveness of our system."
https://arxiv.org/html/2411.01119v1,AquaFuse: Waterbody Fusion for Physics GuidedView Synthesis of Underwater Scenes,"We introduce the idea of AquaFuse, a physics-based method for synthesizing waterbody properties in underwater imagery. We formulate a closed-form solution for waterbody fusion that facilitates realistic data augmentation and geometrically consistent underwater scene rendering. AquaFuse leverages the physical characteristics of light propagation underwater to synthesize the waterbody from one scene to the object contents of another. Unlike data-driven style transfer, AquaFuse preserves the depth consistency and object geometry in an input scene. We validate this unique feature by comprehensive experiments over diverse underwater scenes. We find that the AquaFused images preserve over 94% depth consistency and 90-95% structural similarity of the input scenes. We also demonstrate that it generates accurate 3D view synthesis by preserving object geometry while adapting to the inherent waterbody fusion process. AquaFuse opens up a new research direction in data augmentation by geometry-preserving style transfer for underwater imaging and robot vision applications. keywords‚ÄìView Synthesis; Robot Vision; Image Processing.","Light attenuates exponentially underwater with propagation distance [2, 44] due to two physical characteristics of light: scattering and absorption. Forward scattering is responsible for blur whereas backscatter causes contrast reduction [43, 17]. Besides, light absorption by water has a prominent spectral dependency, which depends on the specific optical properties of a waterbody [3], distance of light sources, salinity, and many other factors. These physical characteristics are represented by the underwater image formation model [2], which, in recent years, have been successfully applied for image restoration [3] and scene rendering tasks [20, 41]. In particular, underwater scene synthesis approaches extend the state-of-the-art (SOTA) neural rendering methods such as NeRF [30] and Gaussian splatting [19] for domain-aware learning. They incorporate the physical properties of underwater image formation into the rendering pipeline for accurate view synthesis [20, 41]. This helps compensate for the inherent attenuation and optical artifacts in underwater imagery and ensures geometrically consistency. Various image recognition models and 3D reconstruction pipelines also use them for improved visual perception [43, 38]. Figure 1: AquaFuse is a physics-guided waterbody fusion method to fuse waterbody properties of a reference image to an input image. It leverages closed-form solutions to estimate and exploit the scene depth, background light, backscatter, and medium illumination parameters for guiding the fusion process. As shown, AquaFused scenes are perceptually realistic and also meet the underwater image formation constraints. In this paper, we explore a new technique for domain-aware data augmentation by fusing waterbody across different underwater scenes. The traditional approaches of data augmentation rely on preserving perspective or isometric constraints (e.g., rotations, translations, scaling, shear, dilation) or altering basic photometric properties (e.g., brightness, color, contrast, saturation). We hypothesize that physically accurate waterbody fusion can be an effective way to augment data for underwater image recognition and scene rendering research; see Fig. 1. Note that the image style transfer literature [8, 46] offers solutions that learn artistic or photometric features from data. These methods rely on generative or adversarial networks to adapt one image domain‚Äôs style to another [10, 26]. While they excel at perceptual enhancements and artistic blending [17], physical constraints of underwater image formation [2] are not preserved. Hence, data augmentation by such domain transfer is not suitable if physical realism and object geometry preservation are crucial, as in 3D view synthesis, scene reconstructions, robotic mapping, and visual servoing tasks [41, 39]. We address these issues in AquaFuse by leveraging the revised underwater image formation model (UIFM) [2, 3] to fuse the optical characteristics of various waterbody types while preserving the object geometry on the scene. We formulate a nonlinear domain projection method to estimate scene depth in real-time, which facilitates an empirical estimation of the veiling light and illumination map. Veiling light is the global background light content of the backscatter signal, which decays exponentially with depth (optical distance) [3, 23, 22]. We decouple this global component from a reference image and integrate it into another (input) scene for backscatter fusion. The fused background light and backscatter signal drive the waterbody fusion, while the direct attenuation and illumination map remain preserved. We then reconstruct the AquaFused image by following the revised UIFM, retaining its physical properties. In addition to the theoretical analysis, we present the engineering constructs to tackle the practicalities of waterbody fusion across multiple scenes. We demonstrate how AquaFuse can be used for waterbody crossover, image enhancement by fusing with a clear reference scene, and in a generative pipeline for data augmentation. For qualitative and quantitative validation, we collect data at multiple ocean sites over diverse waterbody types and depth levels (10‚Ä≤superscript10‚Ä≤10^{\prime}10 start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT-80‚Ä≤superscript80‚Ä≤80^{\prime}80 start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT). Our experiments show that AquaFuse (i) performs perceptually realistic waterbody fusion; (ii) ensures geometric consistency across underwater scenes; (iii) is invariant to dataset bias and overfitting unlike learning-based approaches; and (iv) offers fast end-to-end computational being a closed-form solution. In particular, our analyses of NùëÅNitalic_N images across six water body types show that AquaFused images preserve over 94%percent9494\%94 % depth consistency and 90909090-95%percent9595\%95 % structural similarity of input scenes. The performance margins vary 1111-5555% with increasing water depths from 10‚Ä≤superscript10‚Ä≤10^{\prime}10 start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT to 80‚Ä≤superscript80‚Ä≤80^{\prime}80 start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT. Moreover, we integrate AquaFuse within a 3D view synthesis pipeline using Gaussian Splatting [19] for underwater scene reconstruction across multiple waterbodies. We find that AquaFuse can generate highly detailed 3D reconstructions that accurately represent the original scene‚Äôs geometric characteristics in the fused scenes. As annotated underwater image databases are scarce, physics-guided data augmentation by AquaFuse can facilitate a multi-fold increase in training samples, which can be used for both 2D image recognition and 3D scene reconstruction [41, 45, 20]. This covers a wide spectrum of robot vision tasks, empowering learning-based models to be more robust and generalizable [44, 1]. The computational efficiency of AquaFuse also enables it to run in real-time for active vision, which data-driven style transfer approaches generally fail to offer."
https://arxiv.org/html/2411.00967v1,Raspberry PhenoSet: A Phenology-based Dataset for Automated Growth Detection and Yield Estimation,"The future of the agriculture industry is intertwined with automation. Accurate fruit detection, yield estimation, and harvest time estimation are crucial for optimizing agricultural practices. These tasks can be carried out by robots to reduce labour costs and improve the efficiency of the process. To do so, deep learning models should be trained to perform knowledge-based tasks. Which outlines the importance of contributing valuable data to the literature. In this paper, we introduce Raspberry PhenoSet, a phenology-based dataset designed for detecting and segmenting raspberry fruit across seven developmental stages. To the best of our knowledge, Raspberry PhenoSet is the first fruit dataset to integrate biology-based classification with fruit detection tasks, offering valuable insights for yield estimation and precise harvest timing. This dataset contains 1,853 high-resolution images, the highest quality in the literature, captured under controlled artificial lighting in a vertical farm. The dataset has a total of 6,907 instances of mask annotations, manually labelled to reflect the seven phenology stages. We have also benchmarked Raspberry PhenoSet using several state-of-the-art deep learning models, including YOLOv8, YOLOv10, RT-DETR, and Mask R-CNN, to provide a comprehensive evaluation of their performance on the dataset. Our results highlight the challenges of distinguishing subtle phenology stages and underscore the potential of Raspberry PhenoSet for both deep learning model development and practical robotic applications in agriculture, particularly in yield prediction and supply chain management. The dataset and the trained models are publicly available for future studies.","Estimating when and how much yield a farm will produce is critical in the food supply chain. Farmers face financial penalties due to contractual obligations for under and over-delivering perishable goods to retailers and distributors. Another benefit of accurate yield estimation is that it supports a better market position and pricing stability. Shortages may cause price hikes and dissatisfaction among buyers, while surpluses can force distributors to sell at lower prices, negatively impacting the market position of both farmers and distributors. Finally, accurate estimates allow for better transportation, storage, and sales planning, reducing the costs associated with underutilized logistics or the need for last-minute adjustments [1]. To achieve accurate yield estimation, farmers need to have a deep understanding of the speed of fruit or vegetable ripening and developmental stages of the fruit (phenology) as well as the labour to conduct regular visual inspections to monitor crop development, which is a time-consuming and costly task. However, finding specific targets using visual observations is a well-studied task in computer vision, called object detection [2]. In some cases such as fruit sorting, a fixed camera and a computer are sufficient to carry out [3, 4]. However, in many cases of fruit detection and yield estimation (such as raspberries) the leaves and branches grow randomly, therefore, a fixed camera would be incapable of detecting the desired targets. This outlines the need to have a robot with a camera attached to its arm to move around and capture images from appropriate angles. For robots to predict yield, they need to detect fruits and classify each fruit into a developmental stage. With recent advances in parallel computing, artificial intelligence models (especially deep learning (DL) models) have outperformed other techniques and have become the state-of-the-art method in the field [2]. Yet, deep neural networks (DNNs) feed on large datasets and they need to be trained on a task-specific level to achieve desired outcomes (see Fig. 1). We can leverage transfer learning [5] and fine-tune a network that is pre-trained on a large dataset using a relatively small dataset. Therefore, developing task-specific datasets is crucial to make automation feasible using DNNs. Figure 1: A sample image from the Raspberry PhenoSet, taken at the vertical farming facility at the Center of Urban Innovation of Toronto Metropolitan University. a) Original image with no annotations. b) Mask annotations of all instances present in the image; Labels A-G correspond to the seven phenology stages of raspberries. Figure 2: Phenology reference of raspberry development stages used for annotating the images. The stages are labelled a-g, representing the seven development stages. a) Buds b) Open Flower c) Fruit Initiation d) Green Fruit e) Growing Fruit (yellow colour) f) Semi-mature (pink) fruit g) Mature (red) fruit Large and publicly available datasets such as COCO [6] and LVIS [7] are not fruit-specific, and lack the proper environment, illumination, and abundance. Many fruit-specific datasets such as [8, 9, 10, 11, 12] are not publicly available. There are available datasets on strawberries, grapes, and tomatoes [13, 14, 15, 8, 10, 12], however, they possess only a few hundred images or their images have low resolution both of which prevent achieving good accuracy with DL models. More importantly, even the datasets that are fruit-specific, publicly available, and contain a sufficient number of images, lack biological relevance from which the timing of harvest cannot be elucidated. For instance, Afonso et al. [16] have labelled their tomato dataset simply by colour i.e. green, red, etc. and Cossio-Montefinale et al. [17] have too broad categories (green, unripe, ripe) to derive meaningful data to determine precise harvest dates. This makes them unpractical from a supply chain and yield estimation point of view. Moreover, object detection models are sensitive to illumination and all of the the previous datasets were captured in conventional farms or greenhouses which use sunlight, whereas vertical farms use artificial lights and have a different illumination. In this paper, we present the Raspberry PhenoSet for raspberry detection, segmentation, and yield estimation. Raspberry PhenoSet is a large and high-quality dataset with 1853 images and 6907 mask annotations, that were manually labelled. The images are provided in two different sizes, a smaller size suitable for current widely-used GPUs and a larger size suitable for future hardware upgrades. To the best of our knowledge, Raspberry PhenoSet is the first phenology-based dataset, the first dataset gathered in a vertical farming environment, and also has the largest image size amongst the fruit datasets [17] and it is publicly available111https://sites.google.com/view/raspberry-phenoset. The images are annotated and classified according to the development stages of raspberry plants (see Fig. 2), i.e. the remaining days before harvest are known for each class, making them suitable to estimate the yield and the harvest time precisely, therefore, fulfilling the supply chain needs discussed earlier. In the rest of the paper, we discuss the recent advances in the field in Sec. II, provide the details and considerations of creating the dataset in Sec. III, describe the models and metrics used for the evaluation of the dataset in Sec. IV, present the results and benchmark the dataset in Sec. V, discuss the results and performance of different models in Sec. VI, and finally provide conclusions and recommendations in Sec. VII. TABLE I: An Overview of the Fruit Detection Datasets in the Literature Fruit Dataset Size Image Size Farm Type Availability Phenology-based Apples 300 640√ó640640640640\times 640640 √ó 640 Conventional Private No [8] Apples 1,386 600√ó400600400600\times 400600 √ó 400 Conventional Private No [18] Apples 267 416√ó416416416416\times 416416 √ó 416 Conventional Private No [19] Apples 1,200 416√ó416416416416\times 416416 √ó 416 Conventional Private No [20] Apples 2,298 1280√ó96012809601280\times 9601280 √ó 960 Conventional Public No [21] Apples 1,000 1280√ó72012807201280\times 7201280 √ó 720 Conventional Public No [22] Apples 285 640√ó480640480640\times 480640 √ó 480 Greenhouse Private No [23] Citrus 4,855 1920√ó1080192010801920\times 10801920 √ó 1080 Conventional Public No [24] Citrus 579 2448√ó3264244832642448\times 32642448 √ó 3264 Conventional Public No [25] Grapes 300 1365√ó2048136520481365\times 20481365 √ó 2048 Conventional Public No [14] Grapes 961 300√ó450300450300\times 450300 √ó 450 Conventional Private No [12] Mangoes 1,100 800√ó600800600800\times 600800 √ó 600 Conventional Private No [11] Raspberry 2039 1773√ó1773177317731773\times 17731773 √ó 1773 Conventional Public No [26] Strawberries 2,400 Not Available Conventional Private No [9] Strawberries 177 1280√ó72012807201280\times 7201280 √ó 720 Greenhouse Public No [13] Tomatoes 996 512√ó512512512512\times 512512 √ó 512 Greenhouse Private No [10] Tomatoes 318 504√ó377504377504\times 377504 √ó 377 Greenhouse Private No [27] Tomatoes 250 2000√ó2000200020002000\times 20002000 √ó 2000 Greenhouse Public No [15] Raspberry(Current Study) 1853 5184√ó345651843456\textbf{5184}\times\textbf{3456}5184 √ó 3456 Vertical Farm Public Yes"
https://arxiv.org/html/2411.00741v1,FG-PE: Factor-graph Approach for Multi-robot Pursuit-Evasion,"With the increasing use of robots in daily life, there is a growing need to provide robust collaboration protocols for robots to tackle more complicated and dynamic problems effectively. This paper presents a novel, factor graph-based approach to address the pursuit-evasion problem, enabling accurate estimation, planning, and tracking of an evader by multiple pursuers working together. It is assumed that there are multiple pursuers and only one evader in this scenario. The proposed method significantly improves the accuracy of evader estimation and tracking, allowing pursuers to capture the evader in the shortest possible time and distance compared to existing techniques. In addition to these primary objectives, the proposed approach effectively minimizes uncertainty while remaining robust, even when communication issues lead to some messages being dropped or lost. Through a series of comprehensive experiments, this paper demonstrates that the proposed algorithm consistently outperforms traditional pursuit-evasion methods across several key performance metrics, such as the time required to capture the evader and the average distance traveled by the pursuers. Additionally, the proposed method is tested in real-world hardware experiments, further validating its effectiveness and applicability.","With the increasing presence of robots across various industries, there has been a growing emphasis on inter-robot collaboration. Collaborative robots assist with tasks such as search and rescue [1], trajectory planning [2], agriculture [3], object transportation [4], and collision avoidance [5]. A key challenge in search and rescue operations is the pursuit-evasion (PE) problem, where pursuers try to catch evaders. Several methods have been utilized to solve the PE problem. Classical methods use graph-based techniques, treating the PE problem as a search problem [6]. Reinforcement learning (RL) approaches [7, 8] and game theory methods [9] have also been investigated. However, existing methods often lack an efficient representation of the relationships between pursuers, evaders, and their environment. For instance, if one pursuer loses its history of movements, retrieving that information can be challenging. Utilizing a graph-based approach can establish structural connections between entities, improving the overall understanding of the situation. In addition, the aforementioned methods do not account for uncertainties in predictions and do not facilitate message passing among robots. This limitation affects the scalability and versatility of PE variants. In this paper, we present FG-PE, a method that employs factor graphs (FG) [10] to tackle the PE problem. Factor graphs provide a flexible framework capable of adapting to varying numbers of pursuers and obstacles. This approach addresses prediction uncertainty by guiding pursuers to minimize capture time, distance, and uncertainty, utilizing information from sensor measurements and past observations. The main contributions of the proposed method are: (1) FG-PE provides a factor graph-based solution for addressing the pursuit-evasion problem. By constructing a factor graph, the proposed method can estimate the position of the evader at each time step. The generated graph enables querying the position of each pursuer and other entities present in the scene. To the best of our knowledge, this paper is the first to formulate the PE problem as a factor graph. (2) FG-PE can estimate the evader‚Äôs position and plan the pursuers while considering short time windows. This means that at each time step, it can determine the planned position for each pursuer for the subsequent time step. (3) FG-PE accounts for uncertainty in predictions using the probabilistic nature of factor graphs. It aims to minimize this uncertainty by providing a plan through factor graphs, which enable efficient representation and optimization in the presence of noise. Videos of the experiments are available on the website of the project: https://sites.google.com/view/pursuit-evasion. (4) FG-PE is easily extendable to multiple pursuers and allows for high obstacle-wise scalability. (5) FG-PE is robust against lost messages and performs well even when some messages are dropped within the graph structure. The rest of the paper is organized as follows: Sec. II presents background. Sec. III introduces the proposed method. Sec. IV shows the experimental results. Sec. V concludes the paper."
https://arxiv.org/html/2411.00704v1,Learning to Look Around: Enhancing Teleoperation and Learning with a Human-like Actuated Neck,"We introduce a teleoperation system that integrates a 5-DOF actuated neck, designed to replicate natural human head movements and perception. By enabling behaviors like ‚Äúpeeking‚Äù or ‚Äútilting‚Äù, the system provides operators with a more intuitive and comprehensive view of the environment, improving task performance, reducing cognitive load, and facilitating complex whole-body manipulation. We demonstrate the benefits of natural perception across seven challenging teleoperation tasks, showing how the actuated neck enhances the scope and efficiency of remote operation. Furthermore, we investigate its role in training autonomous policies through imitation learning. In three distinct tasks, the actuated neck supports better spatial awareness, reduces distribution shift, and enables adaptive task-specific adjustments compared to a static wide-angle camera.","Collecting high-quality demonstration data to train robotic manipulation algorithms presents significant challenges. Various methods for data collection have emerged, including the use of exoskeletons [1, 2], handheld manipulation devices [3, 4], and retargeting human actions from videos into robot trajectories [5, 6, 7]. However, these approaches often involve translating the collected data into robot trajectories, which can introduce errors. Alternatively, teleoperation [8, 9, 10, 11, 12, 13] offers a more direct data collection method, eliminating the need for retargeting. Yet, teleoperation poses its own set of challenges, particularly in terms of intuitiveness and ease of use for operators [14]. Operators typically struggle with a limited field of view, especially if they are not positioned directly above the robot or constrained by a fixed camera setup such as in [15], or lack haptic feedback. In this work, we address the challenge of perception gap and propose using a first-person perspective teleoperation by incorporating an actuated arm as a ‚Äúneck‚Äù that mimics natural human head movements, making teleoperation more intuitive. 1.1 Human Perception and Intuitive Teleoperation In daily life, humans use head and neck movements extensively to navigate complex spaces and manipulate objects. A teleoperation system that replicates these natural movements can reduce cognitive load and enhance operator efficiency. To facilitate this, we mount a camera on an actuated neck with 5 degrees of freedom (DOF), allowing it to replicate human-like neck motions, including looking around obstacles and altering viewpoints through combined translation and rotation. The increased DOF not only enables the operator to rotate and tilt the neck camera but also to ‚Äùpeek‚Äù around occlusions and create new viewpoints. This setup mimics the way humans perceive their environment, ultimately making teleoperation feel more natural and reduce the operator‚Äôs cognitive effort. As shown in Figure 1, adding the ability to peek around allows the teleoperator to perform complex whole-body maneuvers like finding the coat hanger relative to the viewpoint. This capability is especially crucial for whole-body remote teleoperation, where the operator is not physically present alongside the robot. 1.2 Autonomy and Improved Data Collection In addition to improving the operator‚Äôs experience, the actuated neck also has implications for autonomous policy training. Unlike static wide-angle cameras that often capture distorted images and can contribute to out-of-distribution perception, the actuated neck provides a dynamic viewpoint, reducing such risks. By using standard RGB cameras, image quality is enhanced, resulting in better data for both teleoperation and machine learning algorithms. The dynamic camera adjustment also helps manage occlusions, allowing the robot to ‚Äùpeek‚Äù around obstacles to locate objects of interest, mirroring natural human behavior. In one task, for example, adjusting the camera‚Äôs angle was critical to finding a target object that was not visible in the direct line of sight. While a local wrist-mounted camera provided fine-grained details necessary for grasping or aligning the gripper, the actuated neck camera delivered crucial contextual information, aiding navigation within a broader environment. The actuated neck also enables interactive perception [16], as it integrates sensor observations and action trajectories over time. This combined signal contains learnable and generalizable relationships, such as the ability to track objects as they move through space. By aligning the neck camera‚Äôs movement with object trajectories, we can implicitly learn which objects are of interest for specific tasks. Our experiments trained a multi-task policy across different environment heights, demonstrating the neck‚Äôs ability to improve generalization and adaptability. The system effectively managed diverse scenarios, such as working on high and low tables, showing the potential for broader application. Our key contributions are: 1. Intuitive Teleoperation: We introduce an actuated neck with 5 degrees of freedom (DOF), allowing for human-like head movements. This design reduces operator cognitive load by enabling natural viewpoint adjustments. 2. Improved Data Quality: The 5-DOF neck, equipped with standard RGB cameras, enhances image quality and minimizes perception errors, leading to better generalization for autonomous policy training. 3. Enhanced Interactive Perception: The neck‚Äôs full range of motion supports dynamic object tracking and effective manipulation across diverse tasks, closely mimicking natural human behavior."
https://arxiv.org/html/2411.00554v1,Differentiable Physics-based System Identification for Robotic Manipulation of Elastoplastic Materials,"Robotic manipulation of volumetric elastoplastic deformable materials, from foods such as dough to construction materials like clay, is in its infancy, largely due to the difficulty of modelling and perception in a high-dimensional space. Simulating the dynamics of such materials is computationally expensive. It tends to suffer from inaccurately estimated physics parameters of the materials and the environment, impeding high-precision manipulation. Estimating such parameters from raw point clouds captured by optical cameras suffers further from heavy occlusions. To address this challenge, this work introduces a novel Differentiable Physics-based System Identification (DPSI) framework that enables a robot arm to infer the physics parameters of elastoplastic materials and the environment using simple manipulation motions and incomplete 3D point clouds, aligning the simulation with the real world. Extensive experiments show that with only a single real-world interaction, the estimated parameters, Young‚Äôs modulus, Poisson‚Äôs ratio, yield stress and friction coefficients, can accurately simulate visually and physically realistic deformation behaviours induced by unseen and long-horizon manipulation motions. Additionally, the DPSI framework inherently provides physically intuitive interpretations for the parameters in contrast to black-box approaches such as deep neural networks.","Figure 1: The proposed system identification framework enables a robot to interact with elastoplastic material via simple manipulation motions (orange box) and then identify the physics parameters of the real-world manipulation dynamics. The parameters are found using gradients computed, through differentiable simulation, from a differentiable point-cloud-based similarity function between the real and simulated observations of the manipulated material (blue and red boxes). These parameters then enable accurate simulations that allow the real-world grounding of motion planning, trajectory optimisation or policy learning techniques (cyan box). Despite the recognised importance of robotic manipulation of deformable materials, this topic remains underexplored, particularly when it comes to high-precision manipulation of volumetric elastoplastic materials. A primary challenge in this area arises from the materials‚Äô infinite degrees of freedom (DoFs), leading to highly unpredictable deformation dynamics. The intrinsic complexity of these dynamics inhibits the direct application of conventional robotic motion planning methods, which typically require explicit physics models for all concerned objects Latombe (2012). Learning approaches, such as reinforcement learning (RL), often involve training an agent to learn to interpret its perception and take actions through inefficient trial & error in a realistic physics-based simulation Collins et al. (2021); Kroemer et al. (2021), which is both challenging and largely unavailable when it comes to deformable materials. In contrast to the well-studied rigid body dynamics in robotics, where motions can be predicted and controlled using well-defined equations of motion and deterministic models Featherstone (2014), deformable materials do not follow such straightforward patterns. Directly applying these methods is problematic because it is extremely difficult and often infeasible to accurately model and perceive real-world elastoplastic materials and measure the underlying physics parameters that govern their motions and deformations Arriola-Rios et al. (2020); Yin et al. (2021). As a result, achieving high-precision manipulation for such materials with motion planning or data-driven techniques is challenging due to the high computational cost and the lack of techniques to capture the dynamics accurately. To close this gap, this research proposes a Differentiable Physics-based System Identification (DPSI) framework for the robotic manipulation of volumetric elastoplastic materials. Our framework can efficiently estimate key physics parameters governing material deformation dynamics using minimal and simple manipulation motions. The estimated physics parameters enable accurate material simulation for long-horizon predictions of real-world elastoplastic deformation behaviours. The workflow of the proposed DPSI framework can be summarised as follows. As shown in Figure 1, a robot equipped with an in-hand 3D camera (Zivid) and three end-effectors is deployed to manipulate the elastoplastic object (e.g., play dough). Before manipulation, the robot takes multi-view point clouds of the object to minimise occlusions. These point clouds are used to create the initial particle system for the material point methods (MPM)-based simulation. The robot then performs a manipulation motion on the object and captures point clouds of the deformed state. Physics simulations of the same manipulation are run with the same initial state and motion, whose resultant particle states are compared to the real-world deformed state through variants of the Chamfer distance (CD) and the earth mover‚Äôs distance (EMD) loss. The parameters of the simulated physics models are updated after every simulation to minimise the loss. To facilitate fast optimisation, we use a differentiable simulator built on the TaiChi auto-differentiation mechanism, which allows automatic gradient computation from the losses and gradient-based optimisation for the physics parameters Hu et al. (2019, 2020). Our approach achieves unprecedented simulation-to-real alignment accuracy, characterised by the integration of the following novel features. High-fidelity physics: Unlike previous methods that either employ non-physics-based models (e.g., neural networks) or highly simplified material geometry representations (e.g., sparse keypoints), we use high-fidelity physics-based simulation powered by the MPM Jiang et al. (2016), which simulates materials as Lagrangian particles and keeps track of their positions and velocities. It achieves faster simulation by computing the motions, deformation gradients, and frictional contacts on a background Eulerian grid Stomakhin et al. (2013); Jiang et al. (2016); Gao et al. (2017); Hu et al. (2018). MPM-based simulations provide highly efficient and realistic simulation with high physical plausibility by closely following real-world physics laws such as Newton‚Äôs laws and elastic and plastic energy conservation models. Incomplete & noisy observations: Unlike existing works that rely on synthetic videos with complete sequences of perfect observations Murthy et al. (2020); Li et al. (2023); Kaneko (2024), our framework uses 3D point clouds to observe real-world object geometries. Capturing the full depth of an object during manipulation is impractical due to occlusions caused by the end-effector or environment. This means that only the point clouds before and after a manipulation motion are practical to obtain and informative enough to observe the full geometry of the deformed object in real-world experiments. In addition, real-world point clouds tend to suffer from inaccurately estimated camera matrices and sensory noises. Small data, short & simple motions: Using extensive and diverse manipulation motions to collect real-world deformation data is time-consuming and costly. Existing studies demand a significant number of real-world interactions or complete sequences of simulation videos to identify object deformations under various motions, yet still resulting in simulations with insufficient accuracy for real-world applications Lin et al. (2022); Shi et al. (2023); Li et al. (2023); Kaneko (2024). Our goal is to recover physics parameters that enable accurate predictions of long-horizon, unseen and complicated elastoplastic material manipulation dynamics, using minimal simple and short real-world interactions. Joint parameter estimation: We aim to jointly estimate the physics parameters provided by physics models. Besides Newton‚Äôs laws, we employ the fixed corotated elastic energy model Stomakhin et al. (2012), the von Mises plasticity model Jones (2009) and the dynamic friction model in our simulation. These lead to six key parameters: Young‚Äôs modulus Eùê∏Eitalic_E, Poisson‚Äôs ratio ŒΩùúà\nuitalic_ŒΩ, yield stress ùùàysubscriptùùàùë¶\boldsymbol{\sigma}_{y}bold_italic_œÉ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT, material density œÅùúå\rhoitalic_œÅ, and the friction coefficients of the table Œ∑tsubscriptùúÇùë°\eta_{t}italic_Œ∑ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and end-effectors Œ∑msubscriptùúÇùëö\eta_{m}italic_Œ∑ start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT (assuming the three end-effectors share the same coefficient). The first four parameters primarily govern the deformation responses, while the last two handle frictional contacts. These parameters are heavily intertwined in governing the behaviours of the manipulated object and there is no intuitive solution to identify one of them without estimating the effects of other parameters. Therefore, we seek to identify these parameters simultaneously. Differentiable physics: Identifying the physics parameters in their discretised spaces via search or evolutionary algorithms is computationally slow due to the exponentially growing number of possible combinations as the discretisation becomes finer. While gradient-based optimisation methods offer faster convergence toward the minimum, it is infeasible with most physics simulations because many computation steps are not differentiable and these simulators do not support derivative computations. In this work, we explore the feasibility of optimising system parameters using gradients computed by differentiating loss functions through a physics simulator written by DiffTaiChi, a programming language tailored for GPU-accelerated parallel computation and automatic differentiation Hu et al. (2019, 2020). DiffTaichi generates derivative functions for simulation steps via source code transformation that retains arithmetic intensity and parallelism. It uses a memory-efficient tape to record the order of computation kernels for forward simulation and traverses their derivative functions in the backward order to generate gradients through the computation graph. We build DPSI upon DiffTaiChi and explore the feasibility of directly optimising several physics parameters jointly with gradients computed by differentiating point-cloud-based loss functions through the high-fidelity physics simulator. Substantial experiments demonstrate that our main contribution, DPSI, can achieve highly accurate simulation-to-reality alignment for elastoplastic materials manipulated by unseen, long horizon and complex motions using minimal simple and short interactions, and noisy and incomplete observations. Results show that when multiple solutions and parameter uncertainty exist, DPSI can provide physically intuitive parameter interpretations that can guide further system identification, model improvement, and motion adaptation. Statistics on the computation costs of DPSI indicates promising practical deployment of the DPSI framework. The rest of the article reviews related literature, presents formally our method and experiment results, and discusses limitations and future directions."
https://arxiv.org/html/2411.00476v1,PlanScope: Learning to Plan Within Decision Scope Does Matter,"In the context of autonomous driving, learning-based methods have been promising for the development of planning modules. During the training process of planning modules, directly minimizing the discrepancy between expert-driving logs and planning output is widely deployed. In general, driving logs consist of suddenly appearing obstacles or swiftly changing traffic signals, which typically necessitate swift and nuanced adjustments in driving maneuvers. Concurrently, future trajectories of the vehicles exhibit their long-term decisions, such as adhering to a reference lane or circumventing stationary obstacles. Due to the unpredictable influence of future events in driving logs, reasoning bias could be naturally introduced to learning based planning modules, which leads to a possible degradation of driving performance. To address this issue, we identify the decisions and their corresponding time horizons, and characterize a so-called decision scope by retaining decisions within derivable horizons only, to mitigate the effect of irrational behaviors caused by unpredictable events. This framework employs wavelet transformation based log preprocessing with an effective loss computation approach, rendering the planning model only sensitive to valuable decisions at the current state. Since frequency domain characteristics are extracted in conjunction with time domain features by wavelets, decision information across various frequency bands within the corresponding time horizon can be suitably captured. Furthermore, to achieve valuable decision learning, this framework leverages a transformer based decoder that incrementally generates the detailed profiles of future decisions over multiple steps. Our experiments demonstrate that our proposed method outperforms baselines in terms of driving scores with closed-loop evaluations on the nuPlan dataset. The source code and associated videos are available at https://github.com/Rex-sys-hk/PlanScope.","Learning-based autonomous driving systems have been a promising approach for the popularity of unmanned vehicles. In particular, learning from driving logs of experts is broadly adopted to supervise neural networks for the planning task. During the training process, distance error is commonly employed, which is to measure the discrepancy between the planned state sequence and the expert demonstrations, thereby optimizing parameters of the neural network [1]. Yet, when training end-to-end autonomous driving systems, it is not entirely reasonable to set the logged expert states as target sequence, because future events are inherently uncertain, and potential disruptions such as unexpected obstacles or fluctuating traffic signals can significantly alter the trajectory planning. These unpredictable elements necessitate agile and nuanced adjustments to driving behavior. Some seemingly predictable decisions, such as the interaction behavior and future trajectory of other traffic participants, are difficult to accurately predict. For example, when modeling the interactions by non-cooperative game theory, the decision planner converges only when observing other traffic participants respond. Otherwise, one can only avoid interactions with others to evade falling into disadvantageous states. Meanwhile, a short-sighted planner is also not desirable, as the planner also needs to keep its long-term decision in consideration when choosing sudden maneuvers. Essentially, the state sequence of experts recorded in the log is a combination of short-term and long-term decisions under different scopes. A diagram of this concept is shown in Fig. 1. Figure 1: In the scenario presented, the ego vehicle‚Äôs long-term decision-making is characterized by adherence to the reference line for driving. Conversely, short-term decision-making involves evading opposing vehicles through the application of lateral velocity. Long-term and short-term decisions integrate the general decision at the moment. However, the demonstrated trajectory, which anticipates future short-term decisions to avoid unpredictable events, may appear counterintuitive in the current context. The decision scope mechanism deliberately focuses on a limited horizon of short-term decisions while maintaining a broader horizon for long-term decisions, thereby mitigating the effect of unpredictable decisions in the demonstrated trajectory. Obviously, an agent needs a broad enough view of the overall situation, handles the current trivialities, and does not overly concern about the trivialities of the future. As a reference, in traditional planning methods, the process is artificially segmented into distinct tiers according to the problem scope: path planning, behavior planning, and motion planning, with each addressed by specialized modules [2]. Conversely, learning-from-demonstration approaches typically involve directly learning future trajectories of a predetermined length, because the nuanced adjustment components and long-term decision components cannot be directly distinguished from log replay. Driving-behavior-reasoning by manual annotation could be an approach to analyze the long-term and short-term decisions [3], by which the behavior duration and their starting time can be obtained. However, this paradigm requires a heavy workload and is typically challenging to scale up. To address the aforementioned challenges, we employ wavelet transformation to extract information at various frequency bands, enabling our model to discern the appropriate actions from log replays in a self-supervised manner. Unlike the Fourier transform, wavelet transformation utilizes basis functions of finite length, which facilitates the capture of choppy features in signals [4]. This technique employs a convolution-like process to extract signal components. Based on wavelet transformation, we propose a mechanism to learn within the decision scope, such that the time horizon of distinct frequency bands can be modulated. In this sense, we can compare the decision details generated from the neural network with decomposed details at different horizons, and this prevents the model from overextending its planning into the future. In order to render the details obtained by decomposition efficiently in model training, we introduce a decoding module that generates detailed short-term decisions at multiple detail levels recursively. Our contributions are threefold: ‚Ä¢ We introduce a novel self-supervised mechanism that learns to plan within decision scope. This approach leverages wavelet decomposition, which extracts frequency domain characteristics in conjunction with time domain features, to capture motion information across various scales within the corresponding time horizon. ‚Ä¢ We propose a new transformer based Iterative Detail Decoder (IDD) to iteratively generate the trajectory decisions and its finer details, thereby enabling models to selectively learn the trajectory intricacies. ‚Ä¢ Our method has been rigorously evaluated on the nuPlan [5] dataset, and the results demonstrate that it achieves superior driving scores compared to baseline methods in closed-loop evaluations, especially under critical safety related metrics."
https://arxiv.org/html/2411.00444v1,Expert-level protocol translation for self-driving labs,"Recent development in ai (ai) models has propelled their application in scientific discovery, but the validation and exploration of these discoveries require subsequent empirical experimentation. The concept of self-driving laboratories promises to automate and thus boost the experimental process following ai-driven discoveries. However, the transition of experimental protocols, originally crafted for human comprehension, into formats interpretable by machines presents significant challenges, which, within the context of specific expert domain, encompass the necessity for structured as opposed to natural language, the imperative for explicit rather than tacit knowledge, and the preservation of causality and consistency throughout protocol steps. Presently, the task of protocol translation predominantly requires the manual and labor-intensive involvement of domain experts and information technology specialists, rendering the process time-intensive. To address these issues, we propose a framework that automates the protocol translation process through a three-stage workflow, which incrementally constructs pdg that approach structured on the syntax level, completed on the semantics level, and linked on the execution level. Quantitative and qualitative evaluations have demonstrated its performance at par with that of human experts, underscoring its potential to significantly expedite and democratize the process of scientific discovery by elevating the automation capabilities within self-driving laboratories.","The evolution of ai techniques has significantly accelerated the processes inherent to scientific discovery, with a notable impact observed within the domain of experimental sciences (Wang et al., 2023b, ). This influence is manifested through a variety of avenues: the generation of hypothesis spaces informed by extensive literature analysis (Jablonka et al.,, 2022; Kim et al.,, 2024), the interpretation of observational data via the identification of high-dimensional correlations (Jumper et al.,, 2021; Abramson et al.,, 2024), the engineering of novel structures that meet predefined specifications (Grisoni et al.,, 2021; Park et al.,, 2023), and the implementation of comprehensive simulations to ascertain the characteristics of potential products (Hie et al.,, 2021; Singh et al.,, 2023). However, the findings facilitated by AI-driven research require further validation and exploration via empirical experiments, and may even entail a cyclical process where AI-generated hypotheses are refined based on the outcomes of real-world experiments, which demands the assembly of a sizable cohort of experienced experimenters to carry out these investigations in accordance with established protocols (McNutt,, 2014). Unfortunately, the formation and sustenance of such a dedicated experimental cadre are fraught with considerable financial demands, and the collaborative engagement between people oriented towards AI methodologies and those grounded in experimental sciences is frequently encumbered by the communication gaps between distinct intellectual paradigms (Baker,, 2016; Freedman et al.,, 2015; Munaf√≤ et al.,, 2017; Baker,, 2021; Shi et al., 2023a, ). To bridge the aforementioned gap, the paradigm of self-driving laboratories has garnered attention, which automates experimental protocols via robotic systems, potentially revolutionizing the way experiments are conducted (B√©dard et al.,, 2018; Steiner et al.,, 2019; Mehr et al.,, 2020; Rohrbach et al.,, 2022; Burger et al.,, 2020; Szymanski et al.,, 2023). Despite the promising outlook, designing such labs relies largely on the translation of protocols, primarily designed for human experimenters, into machine-readable instructions. This translation process necessitates extensive collaboration between domain experts, who possess the requisite scientific knowledge; and information technology specialists, who encode this knowledge into software and hardware systems. The inherently labor-intensive nature of such translation significantly prolongs the development of self-driving laboratories. The primary challenges are rooted in the discrepancies across three critical aspects (see Fig. 1): Syntax Human experimenters can effortlessly comprehend protocols articulated in nl (nl), whereas automated systems frequently necessitate dedicated syntax parsers to convert these protocols into a sequence of actionable steps. Consider the protocol: ‚ÄúSplit the mixture equally into 2 separate 50 mL round-bottom flasks for the next steps.‚Äù This example highlights the meticulous control over experimental procedures, explicitly directing the ‚Äúsplit‚Äù of the mixture into precisely measured volumes ‚Äî a crucial factor for achieving uniform outcomes in subsequent reactions. It is imperative at this level to uphold a structured representation of the mapping of operation conditions and the control flows of operations. Semantics Human experimenters can infer implicit knowledge and context relying on the flexibility and adaptability of human understanding. In contrast, machine instructions necessitate a level of precision and rigidity that human communication does not inherently require. For instance, consider the protocol: ‚ÄúStir the mixture at room temperature for 5 minutes.‚Äù While a human expert might inherently understand that ‚Äúroom temperature‚Äù denotes a temperature range of 20-25 C‚àòsuperscriptùê∂{}^{\circ}Cstart_FLOATSUPERSCRIPT ‚àò end_FLOATSUPERSCRIPT italic_C drawing on their prior knowledge, an automation system necessitates explicit information regarding such implicit details, which therefore need to be completed before execution. Execution Human experimenters can simulate possible intermediate states and outcomes by considering the cumulative effects of a sequence of actions. For instance, given the two instructions adjacently: ‚ÄúAdd 35 mL water to the flask‚Äú and ‚ÄúAdd 25 mL water to the flask‚Äù, an experimenter can deduce that the flask‚Äôs minimal capacity comes over 60 mL to prevent errors. For an automated system to perform a similar function, the actions need to be linked along their execution order. Figure 1: Illustration of the protocol translation problem. An nl-based protocol is translated to a structured protocol, then to a completed protocol, and finally to a linked protocol that is ready for self-driving laboratories along with a corresponding pdg, after being processed through the syntax, semantics, and execution levels. The three colors of arrows and text/ code highlights indicate the three translation steps respectively. Great efforts have been made on such translation tasks, among which Chemputer is representative (Mehr et al.,, 2020). This algorithm parses the nl-based protocol into XDL, a dsl (dsl) specially designed to describe chemical synthesis reactions. The completeness and linkages are constructed with a set of manually-written constraints, with which the correctness of protocols can be further checked. This methodology has gained widespread acceptance in automated chemical synthesis, as a testament to the intensive efforts by domain and IT experts in developing XDL and the corresponding constraints. However, the application of a similar framework in other domains of experimental sciences, such as Genetics, Medicine, Ecology, and Bioengineering, would necessitate repeating these labor-intensive tasks on a case-by-case basis, thus underscoring the critical need for a more generally applicable, human-free protocol translator. In this work, we propose a novel framework of human-free translator, designed to potentially facilitate applications across diverse experimental science domains without requiring extensive manual intervention. This framework decomposes the translation challenge into three hierarchical stages: structured on the syntax level, completed on the semantics level, and linked on the execution level, mirroring the cognitive steps undertaken by human experts in similar translation tasks. In the proposed work, the dsl, its constraints, and linkages are generated automatically, based on protocols tailored for human experimenters, thereby eliminating the need for labor-intensive manual processes. Our contributions are threefold: (i) We conduct a systematic analysis of the existing discrepancies in protocol translation between human experimenters and automated systems in self-driving laboratories. From this analysis, we derive design principles that emulate human cognitive processes involved in protocol translation (Sec. 2). (ii) We devise an autonomous protocol translator through a tripartite framework that incrementally constructs pdg, encapsulating the spatial-temporal dynamics of protocol execution across syntax, semantics, and execution levels (Sec. 3). (iii) Through both quantitative and qualitative evaluations in various experimental science domains, we demonstrate that our translator, when integrated as an auxiliary module for llm, approaches the efficacy of skilled human experimenters and substantially surpasses the performance of purely llm-based alternatives in protocol translation tasks (Sec. 4)."
https://arxiv.org/html/2411.00440v1,NAMR-RRT: Neural Adaptive Motion Planning for Mobile Robots in Dynamic Environments,"Robots are increasingly deployed in dynamic and crowded environments, such as urban areas and shopping malls, where efficient and robust navigation is crucial. Traditional risk-based motion planning algorithms face challenges in such scenarios due to the lack of a well-defined search region, leading to inefficient exploration in irrelevant areas. While bi-directional and multi-directional search strategies can improve efficiency, they still result in significant unnecessary exploration. This article introduces the Neural Adaptive Multi-directional Risk-based Rapidly-exploring Random Tree (NAMR-RRT) to address these limitations. NAMR-RRT integrates neural network-generated heuristic regions to dynamically guide the exploration process, continuously refining the heuristic region and sampling rates during the planning process. This adaptive feature significantly enhances performance compared to neural-based methods with fixed heuristic regions and sampling rates. NAMR-RRT improves planning efficiency, reduces trajectory length, and ensures higher success by focusing the search on promising areas and continuously adjusting to environments. The experiment results from both simulations and real-world applications demonstrate the robustness and effectiveness of our proposed method in navigating dynamic environments. A website about this work is available at https://sites.google.com/view/namr-rrt.","In recent years, autonomous robots have become an integral part of daily life, with their presence expanding across diverse sectors‚Äîfrom automated guided vehicles (AGVs) [1] in warehouses to cleaning robots [2] in shopping malls. As these robots take on increasingly complex tasks and interact with their surroundings, they face the challenge of navigating unpredictable environments filled with dynamic obstacles and continuously changing conditions. Effective motion planning algorithms are essential to enable autonomous robots to navigate these environments efficiently and robustly. Over the past few decades, various motion planning algorithms have been proposed, each with its characteristics. Grid-based methods, such as A* [3], and Dijkstra‚Äôs [4] algorithms, are widely regarded for their completeness and optimality in static, well-structured environments. However, as the dimensionality of the search space increases, these algorithms become computationally expensive and are often unsuitable for real-time planning in dynamic scenarios. Potential field methods, such as Artificial Potential Field (APF) [5], which generate motion by treating the robot as a particle influenced by attractive forces towards the goal and repulsive forces from obstacles, offer faster computation but often suffer from local minima, causing the robot to get stuck before reaching the target. Optimization-based methods, such as trajectory smoothness [6] and trajectory generation [7], are effective in producing smooth, optimal trajectories but are highly sensitive to parameter tuning and environment changes. In addition, learning-based approaches like Deep Reinforcement Learning (DRL) [8] have attracted attention for their ability to train models that predict feasible actions. Despite this, they often lack interpretability, as their black-box nature makes it challenging to understand and predict their decision-making processes. Sampling-based methods, such as Probabilistic Roadmap (PRM) [9] and Rapidly-exploring Random Tree (RRT) [10], are particularly effective in navigating high-dimensional state spaces and incorporating multiple constraints. PRM constructs a roadmap by randomly sampling the state space and connecting feasible points, making it efficient for static environments. However, its need for preprocessing limits its effectiveness in changing scenarios. In contrast, RRT incrementally builds a tree from the start node towards the goal, quickly covering large areas of the state space without a predefined roadmap. This feature makes RRT well-suited for navigating complex and changing environments, providing a foundation for various extensions and improvements in motion planning research. Various approaches have been proposed to improve the performance of RRT, such as RRT-Connect [11] and RRdT* [12]. These methods improve planning efficiency by growing trees from the start and goal points or using multiple search directions. However, due to the nonholonomic constraints of robots [13], they often encounter the Two-Point Boundary Value Problem (TBVP) [14], which prevents the direct connection of nodes between two trees. Solving TBVP is computationally expensive, and solutions are not always guaranteed. To address this challenge, researchers introduce heuristic-based search methods that bypass the need to solve TBVP directly. It leads to developing algorithms such as B2U-RRT [15] and MT-RRT [16], which improve the search process without addressing TBVP explicitly. However, these methods still do not consider dynamic environments. Risk-RRT [17] is introduced as a method to handle motion planning and obstacle avoidance in dynamic environments by incorporating risk awareness into the search process. Building upon this, further improvements such as Bi-Risk-RRT [18] and Multi-Risk-RRT [19] enhance performance by incorporating bi-directional and multi-directional search strategies. These methods enable more efficient robot navigation in dynamic environments. However, despite these enhancements, they still have limitations, especially without a clearly defined search region. It leads to inefficient exploration, as significant computational resources are often wasted in irrelevant areas. This article presents a novel algorithm called NAMR-RRT to address the challenges mentioned above. As shown in Fig. 1, NAMR-RRT utilizes neural network-generated heuristic regions to guide the search towards more promising areas, reducing the time and computational cost of unnecessary exploration. NAMR-RRT dynamically updates the heuristic region and the sampling rate, allowing it to adapt quickly to environmental changes. By incorporating this adaptive feature, NAMR-RRT enhances planning performance, improving the algorithm‚Äôs robustness in dynamic environments. Figure 1: The diagram of the robot‚Äôs navigation in a dynamic environment. The robot starts at the red flag and moves towards the green flag. Static and moving pedestrians are shown as blue and red icons. The Heuristic Region (yellow) guides the robot‚Äôs search. The Neural-based Trajectory (red) highlights the efficient trajectory guided by this region, while the Random Trajectory (green) represents an inefficient trajectory from random sampling. The main contributions of this article are summarized as follows: ‚Ä¢ This article introduces a neural network model based on PointNet++ that employs parallel inference and iterative generation to create heuristic regions, improving the search process efficiency by directing exploration towards more promising areas. ‚Ä¢ The proposed NAMR-RRT algorithm integrates Neural Adaptive Guiding, Multi-directional Searching, and Risk-aware Growing, providing a comprehensive solution for efficient navigation in dynamic environments. ‚Ä¢ The adaptive updating of the heuristic region and sampling rate enables NAMR-RRT to adjust during planning dynamically, enhancing its responsiveness to changing environments. Extensive comparative experiments verify the effectiveness of this feature."
https://arxiv.org/html/2411.00413v1,Multi-Uncertainty Aware Autonomous Cooperative Planning,"Autonomous cooperative planning (ACP) is a promising technique to improve the efficiency and safety of multi-vehicle interactions for future intelligent transportation systems. However, realizing robust ACP is a challenge due to the aggregation of perception, motion, and communication uncertainties. This paper proposes a novel multi-uncertainty aware ACP (MUACP) framework that simultaneously accounts for multiple types of uncertainties via regularized cooperative model predictive control (RC-MPC). The regularizers and constraints for perception, motion, and communication are constructed according to the confidence levels, weather conditions, and outage probabilities, respectively. The effectiveness of the proposed method is evaluated in the Car Learning to Act (CARLA) simulation platform. Results demonstrate that the proposed MUACP efficiently performs cooperative formation in real time and outperforms other benchmark approaches in various scenarios under imperfect knowledge of the environment.","Multi-vehicle systems can significantly accelerate task completion, e.g., platoon formation and collaborative logistics, via communications and interactions among previously isolated vehicles [1, 2, 3, 4]. The key to realizing these systems and tasks lies in achieving high-performance and computationally-efficient autonomous cooperative planning (ACP), which is a high-dimensional system with nonholonomic motion and collision avoidance constraints [5]. However, ACP may suffer from various uncertainties. First, in an autonomous driving (AD) functional pipeline, the downstream trajectory planning is based on the output of upstream environmental perception [6]. Therefore, errors of the learning-based perception, also known as perception uncertainty (as shown in Fig. 1a), will propagate to the model-based planning. In addition, there exists inevitable mismatch between the planned and actual trajectories [7]. Such motion uncertainty becomes even larger in some adversarial conditions, e.g., bad weather. Last but not the least, by shifting from single- to multi-vehicle perception, the perception uncertainty can be significantly reduced in the ACP (Fig. 1b). However, imperfect channel state information could result in communication outage, which may jeopardize the information fusion [8, 9]. Under a high communication uncertainty, the case of Fig. 1b would shift back to Fig. 1a, as the ego-vehicle cannot receive the views of other-vehicles. Existing uncertainty-aware planning approaches treat perception, motion, communication uncertainties separately. Moreover, they mostly focus on single-vehicle AD instead of multi-vehicle ACP. There also exist other vehicle platooning works [10, 11, 12], but none of them consider uncertainty issues. To fill this gap, this paper proposes a multi-uncertainty aware ACP (MUACP) framework, that incorporates perception, motion, communication uncertainties into a unified optimization formulation, thereby automatically allowing for their aggregated effects. (a) Perception uncertainty (b) Multi-vehicle perception Figure 1: Perception uncertainty and multi-vehicle perception. Specifically, our solution chooses the lidar sensor as an illustration for computing the perception uncertainty, due to its ability to provide direct, dense, active, accurate depth measurements of environments [13, 14]. Motion uncertainty is measured according to the wheel feedbacks and weather conditions [7]. Communication uncertainty is built based on the wireless channel distribution and the outage probability [8]. Based on these models, the MUACP problem is formulated as a regularized cooperative model predictive control (RC-MPC) problem, where the regularizers for motion uncertainties and the constraints for perception-communication uncertainties are constructed according to the aforementioned methodologies. Finally, we implement the MUACP approach in the Car Learning to Act (CARLA) simulation platform [15]. To enlarge the sensing ranges and improve the detection accuracies of individual vehicles, we also implement the late-fusion cooperative perception module based on [16, 17] and bridge this module with the MUACP, forming a even more robust ACP system. Results demonstrate the superiority of the proposed MUACP in various scenarios. To the best of our knowledge, this is the first work to consider multiple uncertainties in ACP system. The main contributions are summarized below: ‚Ä¢ We design an efficient ACP strategy based on MPC with full-shape collision avoidance constraints; ‚Ä¢ We incorporate motion, perception, communication uncertainties into MPC as regularizers and constraints; ‚Ä¢ We evaluate the performance of the proposed scheme in the CARLA with extensive comparisons."
https://arxiv.org/html/2411.00400v1,Capability-aware Task Allocation and Team Formation Analysis for Cooperative Exploration of Complex Environments,"To achieve autonomy in complex real-world exploration missions, we consider deployment strategies for a team of robots with heterogeneous autonomy capabilities. In this work, we formulate a multi-robot exploration mission and compute an operation policy to maintain robot team productivity and maximize mission rewards. The environment description, robot capability, and mission outcome are modeled as a Markov decision process (MDP). We also include constraints in real-world operation, such as sensor failures, limited communication coverage, and mobility-stressing elements. Then, we study the proposed operation model on a real-world scenario in the context of the DARPA Subterranean (SubT) Challenge. The computed deployment policy is also compared against the human-based operation strategy in the final competition of the SubT Challenge. Finally, using the proposed model, we discuss the design trade-off on building a multi-robot team with heterogeneous capabilities.","Autonomous multi-robot systems have the potential to enable complex and ambitious missions that were not possible before. Prominent examples include NASA JPL‚Äôs Mars 2020 rover and helicopter mission [mars2020web] and autonomous multi-robot exploration in the DARPA Subterranean Challenge [subt_webpage]. This recent development opens possibilities for more elaborate missions that require exploration of large area and various challenging terrains beyond the capability of a single robot [touma2020mars]. While multi-robot systems provide resiliency and enable more complex missions, designing and deploying a robot team requires a significant study of the robots‚Äô capabilities and mission specifications. Deploying a team of multiple robots with different capabilities increases the complexity of the mission. The nature of exploration tasks requires various autonomous technologies as described in [agha2021nebula]. In this work, we aim to find an optimal robot team configuration and deployment strategy for a given exploration mission. Our work focuses on multi-robot mission planning and task allocation, which is located at a high level in the autonomy stack. This module has to cope with various types of uncertainties in the environment and the robot‚Äôs performance. The uncertainties come from new and updated information as the robot team explores the environment. Robot performance can be unpredictable due to mobility and sensory failures in diverse and challenging real-world settings. Moreover, communication is typically limited in a large-scale and complex environment and poses a coordination challenge between the robots. Figure 1: Robot team deployment underground in a Kentucky limestone mine. Robots with different mobility capabilities (i.e., wheeled, legged, aerial) are sent to the mine to collaboratively map the environment and find anomalies. In this work, we develop an approach to model heterogeneous multi-robot exploration in a real-world scenario and compute an efficient deployment strategy. We construct the model as a Markov decision process (MDP) in a combined space of environment description, robot capabilities, and mission outcome. The environment is modeled as a topological graph that has information on the map connectivity that takes robot mobility capabilities into account, we incorporate various realistic constraints such as communications and sensor failures to model real-world behaviors. We perform case studies based on simulations and our real-world robot deployment experiences in the DARPA Subterranean (SubT) Challenge. Moreover, we show how our operation model can be used to determine the best robot team configuration and analyze the impact of different robot capabilities for a given mission. Our contributions are summarized as follows: ‚Ä¢ We develop a novel problem formulation of unknown-space exploration by a heterogeneous robot team by modeling environments, robotic capabilities, and mission outcome as a MDP (Section III). ‚Ä¢ We demonstrate that our model can compute multi-robot operation policies in a real-world scenario and compare it against the human-based decisions in the live operation (Section V). ‚Ä¢ We analyze and discuss the design trade-off on formulating multi-robot teams with different mobility, perception, communication, and autonomy capabilities (Section VI)."
https://arxiv.org/html/2411.00347v1,An Untethered Bioinspired Robotic Tensegrity Dolphin with Multi-Flexibility Design for Aquatic Locomotion,"This paper presents the first steps toward a soft dolphin robot using a bio-inspired approach to mimic dolphin flexibility. The current dolphin robot uses a minimalist approach, with only two actuated cable-driven degrees of freedom actuated by a pair of motors. The actuated tail moves up and down in a swimming motion, but this first proof of concept does not permit controlled turns of the robot. While existing robotic dolphins typically use revolute joints to articulate rigid bodies, our design ‚Äì which will be made opensource ‚Äì incorporates a flexible tail with tunable silicone skin and actuation flexibility via a cable-driven system, which mimics muscle dynamics and design flexibility with a tunable skeleton structure. The design is also tunable since the backbone can be easily printed in various geometries. The paper provides insights into how a few such variations affect robot motion and efficiency, measured by speed and cost of transport (COT). This approach demonstrates the potential of achieving dolphin-like motion through enhanced flexibility in bio-inspired robotics.","Dolphins swimming energy efficiency has long intrigued researchers and engineers [fish1991dolphin, leatherwood2012bottlenose, shane1986ecology]. This efficiency is largely attributed to their whole-body flexibility, enabling dynamic shape adaptation to support diverse swimming maneuvers [guo2023thrust, Han020forward, wang21computational]. The study of dolphin locomotion has provided valuable insights into the design of aquatic robots, particularly for applications that require energy-efficient underwater mobility [PRADEEP2023120941]. Recent advances in bio-inspired robotics have sought to replicate these capabilities in artificial systems. Most robotic dolphins still rely on traditional mechanical systems, such as revolute joints and rigid structures, which, while enabling the robot to swim, often fail to capture the natural fluidity of a dolphin‚Äôs movement [Chen2023performance, Tanaka2019burst, yu2008dolphin, yu2016development, wu2019towards]. The limitations of rigid designs may restrict the robot‚Äôs ability to perform the smooth, continuous deformations essential for efficient swimming. Tendon-driven designs inspired by dolphin anatomy have been barely explored. One particular dolphin robot by Liu et al. has shown promising results in forward swimming speed [liu2021design]; however, its fixed vertebra design limits the exploration of alternative skeletal configurations. Figure 1: Overview of the dolphin robot: (a)(i) Side view of CAD model, (a)(ii) Side view of the real robot, (b) Dolphin robot showing inner skeleton, (c) Top view of CAD model, (d)(i) Swimming with tail down, (d)(ii) Side view of the real dolphin robot swimming with tail up. Soft robotics has emerged as a promising field for replicating the flexibility found in nature [zou2018reconfigurable, nemitz2016using, softsnap]. These robots, made of compliant materials like silicone and elastomers [buckner2021design, ShepherdRobertF2011Msr, Tolley2014], exhibit adaptable, fluid motion that rigid systems cannot achieve [lee2017soft]. Soft robots can deform in response to environmental forces, enabling them to navigate complex terrains and perform precise, delicate movements [softlattice, zebing]. By incorporating flexible materials, a bio-inspired robotic dolphin can better emulate the dynamic properties of a real dolphin‚Äôs body, enhancing flexibility while maintaining reasonable energy efficiency. While previous dolphin robots have predominantly utilized rigid materials with revolute or other joint mechanisms, our design is the first to employ silicone-based, compliant materials for the dolphin‚Äôs body, allowing smoother, more lifelike aquatic motions. Additionally, flexible skeletons driven by cable or shape memory alloy (SMA) systems [Lai2022cable, shintake2020fish, starblocks] and designs like the fishbone-inspired framework [softsnap] further contribute to this fluid movement. To address the challenges of replicating dolphin motion, we introduce the first robotic dolphin design to integrate flexibility across three key aspects: body, actuation, and structural adaptability. Body flexibility (Section II) is achieved by pairing a rigid head with a flexible tail, covered in a tunable silicone skin that allows hardness adjustments. This unique feature enables the robot to mimic the adaptive properties of real dolphins, enhancing movement efficiency. Actuation flexibility (Section III) is realized through a cable-driven system that emulates muscle contractions, creating smooth, continuous motion that closely resembles dolphin muscle dynamics. A fishbone mechanism within the actuation system further enhances motion adaptability and fluidity. Lastly, design flexibility (Section IV) is incorporated through a compliant skeleton structure, which design can be tuned in terms of pin bones length ratio, enabling a family of designs for exploring parameters that influence swimming speed and efficiency. The design will be made opensource. This paper presents the detailed design and development of a bio-inspired robotic dolphin, examining how each aspect of flexibility contributes to achieving dolphin-like motion. We discuss the geometric modeling of the skeleton, robot fabrication, and results from swimming tests that evaluate the design‚Äôs performance (Section V). The current prototype allows the robot to swim only forward, but this work is a first step aiming to advance bio-inspired robotics by demonstrating how flexible design can enhance the locomotion of underwater robots ‚Äì Section VI discusses corresponding future work."
https://arxiv.org/html/2411.00241v2,"A Fast and Model Based Approach for Evaluating Task-Competence
of Antagonistic Continuum Arms","Soft robot arms have made significant progress towards completing human-scale tasks, but designing arms for tasks with specific load and workspace requirements remains difficult. A key challenge is the lack of model-based design tools, forcing advancement to occur through empirical iteration and observation. Existing models are focused on control and rely on parameter fits, which means they cannot provide general conclusions about the mapping between design and performance or the influence of factors outside the fitting data. As a first step toward model-based design tools, we introduce a novel method of analyzing whether a proposed arm design can complete desired tasks. Our method is informative, interpretable, and fast; it provides novel metrics for quantifying a proposed arm design‚Äôs ability to perform a task, it yields a graphical interpretation of performance through segment forces, and computing it is over 80x faster than optimization based methods. Our formulation focuses on antagonistic, pneumatically-driven soft arms. We demonstrate our approach through example analysis, and also through consideration of antagonistic vs non-antagonistic designs. Our method enables fast, direct and task-specific comparison of these two architectures, and provides a new visualization of the comparative mechanics. While only a first step, the proposed approach will support advancement of model-based design tools, leading to highly capable soft arms.","I INTRODUCTION Fluid-driven soft robot arms seek to capture the physical intelligence of muscular hydrostats, such as elephant trunks and octopus arms [1], in order to improve robot robustness and safety around humans [2]. These arms combine soft pneumatic actuators in parallel and in series to produce robotic arms that can bend in any direction at multiple points (see example in Fig. 1) [3, 4, 5, 6]. Recent works have taken major steps toward soft robotic arms that can complete human-scale tasks, exploring backbone-free antagonistic designs for stiffness control [7, 8, 9] and demonstrating contact rich tasks such as opening drawers [6], washing human subjects [10] and assisted eating [11]. However, arm demonstrations are consistently limited to small external loads, even with high actuator pressures [3, 8, 12, 6, 11]. The mechanical reason for limited force is not obvious, but it is evident even in recent, novel demonstrations: in Jiang et al., no tasks are demonstrated with additional weights [6], and in related work the arm‚Äôs maximum characterized load is 2.8N [6][12]. The arm in Guan et al. is shown, in a separate work, to deform its entire body length under 4N of tip loading [11, 13]. Investigations of the mapping between design and performance, e.g., load capacity across the workspace, are hampered by the lack of model-based design tools for soft robotic arm. Soft arm modeling efforts have focused on control, and use experimental, homogenized stiffness parameters that are specific to the design being tested [14, 15, 16]. Practically, an arm‚Äôs ability to complete a desired task has been determined by building and testing it. Figure 1: We propose a novel method to evaluate fluid-driven soft arm designs on their ability to meet a task shape at the task load (a). We mainly demonstrate our method with planar antagonistic arm designs (b). Each arm segment has two extending and two contracting actuators, and arms move via selective actuator pressurization. Figure 2: Overview of our mechanics model. A soft robot arm composed of two bellows and two muscles can be fully parameterized by its centerline twists go‚Å¢isubscriptabsentùëîùëúùëñ\accentset{\scriptscriptstyle{\circleright}}{g}_{oi}start_OVERACCENT end_OVERACCENT start_ARG italic_g end_ARG start_POSTSUBSCRIPT italic_o italic_i end_POSTSUBSCRIPT and transformations between actuators go‚Å¢Œ±osubscriptsubscriptùëîùëúùõºùëú{}_{o}g_{o\alpha}start_FLOATSUBSCRIPT italic_o end_FLOATSUBSCRIPT italic_g start_POSTSUBSCRIPT italic_o italic_Œ± end_POSTSUBSCRIPT (a). When a tip load ùê™tipsubscriptùê™tip\mathbf{q}_{\textrm{tip}}bold_q start_POSTSUBSCRIPT tip end_POSTSUBSCRIPT is applied, wrenches ùê™isubscriptùê™ùëñ\mathbf{q}_{i}bold_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT are induced along the arm‚Äôs backbone. To achieve static equilibrium, each actuator contributes a reaction force and moment to balance the load (b). We consider two types of actuators: contracting McKibben artificial muscles and extensile bellows actuators. The characterized force functions f‚Å¢(œµ,p)ùëìitalic-œµùëùf(\epsilon,p)italic_f ( italic_œµ , italic_p ) of each actuator are also shown, with actuation regimes labeled according to [17] (c). Model-based design tools require model formulations that generalize across designs and methods of using those models to produce informative, interpretable results. Prior work [17] has developed generalizable models of soft arms, but significant gaps remain in developing approaches that utilize these models to provide insights about the mapping between arm design and task-specific performance. In this work, we develop a novel, model-based method for evaluating a proposed design‚Äôs ability to complete specific tasks. Our method is informative, interpretable, fast and provides a visualization of segment capabilities. We use it to concretely establish that antagonistic arms can complete a wider range of tasks than non-antagonistic arms, we provide novel insights for why, and computing it is 80x faster than existing methods. We first establish our underlying mechanics model in Section II, then introduce our method for analyzing task attainability in Section III, and finally apply our method to the comparison of arm designs in Section IV."
https://arxiv.org/html/2411.00221v1,BOMP: Bin-Optimized Motion Planning,"In logistics, the ability to quickly compute and execute pick-and-place motions from bins is critical to increasing productivity. We present Bin-Optimized Motion Planning (BOMP), a motion planning framework that plans arm motions for a six-axis industrial robot with a long-nosed suction tool to remove boxes from deep bins. BOMP considers robot arm kinematics, actuation limits, the dimensions of a grasped box, and a varying height map of a bin environment to rapidly generate time-optimized, jerk-limited, and collision-free trajectories. The optimization is warm-started using a deep neural network trained offline in simulation with 25,000 scenes and corresponding trajectories. Experiments with 96 simulated and 15 physical environments suggest that BOMP generates collision-free trajectories that are up to 58 % faster than baseline sampling-based planners and up to 36 % faster than an industry-standard Up-Over-Down algorithm, which has an extremely low 15 % success rate in this context. BOMP also generates jerk-limited trajectories while baselines do not. Website: https://sites.google.com/berkeley.edu/bomp.","Robots are increasingly used for package handling and picking in logistics settings. When transporting thousands of packages each day, reductions in cycle time can significantly increase robot productivity. Particularly when working in deep bins, package handling robots are often equipped with long-nosed suction tools (e.g., the ‚Äúbluction‚Äù tool from Huang, et al. [1]) to enable them to reach and manipulate packages throughout the deep bin. The long-nosed suction tool also allows the robot wrist and arm to remain far from obstacles and potential collisions. In deep cluttered bins, contents can shift after each pick, necessitating a strategy to rapidly compute pick-and-place motions using the latest sensor data (e.g., color and depth camera images). Practical approaches include heuristic planning, optimization-based motion planning, and sampling-based motion planning. A common heuristic trajectory, Up-Over-Down, lifts the package to clear all obstacles, moves horizontally over obstacles to the target location, then lowers. This is easy to implement and has negligible compute time; however, the motion is longer than necessary and often fails when using a long-nosed suction tool and planning in a deep bin. In particular, vertically lifting from the bottom to the top of the bin may be kinematically infeasible in deep bin environments. Optimization-based methods formulate and solve an optimization problem to find the best or fastest trajectory that avoids collisions. Sampling-based methods randomly sample and connect collision-free waypoints to find a path. The latter two methods yield significantly more successful and faster motions than Up-Over-Down, but at the expense of longer compute times. Figure 1: Bin-optimized motion planning. BOMP executing a time-optimized, jerk-limited, collision-free trajectory moving a box from a bin to a drop-off point. We use the long-nosed ‚Äúbluction‚Äù tool from Huang, et al. [1] to enable the robot to reach all parts of the deep bin, and an overhead RGBD camera to detect obstacles and target boxes. BOMP uses an optimization-based motion planner to compute the pick-and-place trajectory. In order to speed up the computation, a neural network warm-starts the optimizer. It accepts the obstacle environment, grasped box, and pick and place poses as input, and outputs an initial trajectory. In prior work, we presented GOMP [2], an optimization-based motion planner incorporating time-optimization, obstacle avoidance, and grasp-optimization. GOMP computes fast motions between pick and place poses, and the grasp-optimization further speeds up motions by allowing pick and place poses to be optimized while retaining the same parallel-jaw grasp contact points. Subsequently, DJ-GOMP [3] further reduces compute time by using a neural network to warm-start motion planning for time-optimized and jerk-limited trajectories. DJ-GOMP is trained over a distribution of start and end points, assuming a fixed collision environment. However, in warehouse settings, boxes often move between picking actions. To address changing obstacle environments, we propose Bin-Optimized Motion Planning (BOMP). BOMP finds a time-optimized trajectory while considering collisions between the robot, a grasped box, and the obstacle environment. We integrate BOMP into an end-to-end bin-picking pipeline, which takes as input an RGBD image of the bin and outputs time-optimal trajectories (Figure 1). BOMP modifies and extends DJ-GOMP by adding the dimensions of a grasped box and a height map of the environment as inputs to the warm-start neural network, which enables adapting the trajectory based on environment changes. The warm-start neural network is trained to handle a relevant distribution of varying collision obstacles, and predicts a trajectory to warm-start the jerk-limited and time-optimizing trajectory optimization. We generate a dataset of synthetic collision environments in simulation to train the deep neural network. This paper makes 4 contributions: 1. BOMP, a time-optimizing, jerk-limited motion planning algorithm for picking boxes from deep bins where boxes and obstacles are detected via a depth camera. In experiments, BOMP generates up to 36 % faster trajectories compared to an Up-Over-Down baseline implemented with an optimal time-parameterizer [4], as well as up to 58 % faster trajectories compared to Motion Planning Templates [5] (MPT) which implements a parallelized sampling-based motion planner (PRRT*). BOMP successfully generates collision-free trajectories at a 79% rate, which is similar to MPT, and significantly higher than the Up-Over-Down baseline. 2. An end-to-end bin-picking pipeline that uses BOMP to iteratively remove the boxes. 3. A deep neural network trained on 25,000 trajectories generated from simulated scenes to accept a height map, grasped box parameters, a number of trajectory segments, and trajectory endpoints as inputs and to predict an initial trajectory to warm-start the planning. 4. Data from 96 experiments in simulated environments and 15 experiments in physical environments."
https://arxiv.org/html/2411.00138v1,Learning Low-Dimensional Strain Models of Soft Robots by Looking at the Evolution of Their Shape with Application to Model-Based Control,"Obtaining dynamic models of continuum soft robots is central to the analysis and control of soft robots, and researchers have devoted much attention to the challenge of proposing both data-driven and first-principle solutions. Both avenues have, however, shown their limitations; the former lacks structure and performs poorly outside training data, while the latter requires significant simplifications and extensive expert knowledge to be used in practice. This paper introduces a streamlined method for learning low-dimensional, physics-based models that are both accurate and easy to interpret. We start with an algorithm that uses image data (i.e., shape evolutions) to determine the minimal necessary segments for describing a soft robot‚Äôs movement. Following this, we apply a dynamic regression and strain sparsification algorithm to identify relevant strains and define the model‚Äôs dynamics. We validate our approach through simulations with various planar soft manipulators, comparing its performance against other learning strategies, showing that our models are both computationally efficient and 25x more accurate on out-of-training distribution inputs. Finally, we demonstrate that thanks to the capability of the method of generating physically compatible models, the learned models can be straightforwardly combined with model-based control policies.","Continuum soft robot‚Äôs inherent compliance and embodied intelligence make them promising candidates for close collaboration between humans and robots and contact-rich manipulation [1, 2]. Modeling their dynamical behavior [3] with computationally tractable models is important for many applications, such as model-based control [4], state estimation [5], and co-design [6]. Developing such (low-dimensional) dynamic models is challenging and is an active area of research [7, 3]. The use of data-driven approaches have been extensively investigated in this context [8, 9, 7, 10]. These learned models exhibit poor extrapolation performance [11], a lack of interpretability and (physical) structure preventing us from directly leveraging closed-form control solutions such as the PD+feedforward [4]. Instead, researchers had to fall back to computationally expensive planning methods such as \acMPC [9, 7]. Figure 1: Overview of the proposed methodology with the key contributions (Kinematic Fusion and Dynamic Regression and Strain Sparsification) highlighted in orange. Inputs: We consider NùëÅNitalic_N Cartesian pose measurements œáùúí\chiitalic_œá distributed along the soft robot backbone, for example, obtained using Computer Vision (CV) techniques from video recordings, as inputs. Kinematic Fusion: We apply an iterative procedure that involves (i) computing the robot configuration qùëûqitalic_q using \acPCS inverse kinematics, and (ii), to avoid overly complex and high-dimensional models, we merge adjacent segments with similar strains across the dataset into one segment of constant strain. Dynamic Regression: We identify the PCS dynamic model by iteratively regressing coefficients using linear least squares and further reduce the model complexity by neglecting insignificant strains. Output: The identified dynamic model has a Lagrangian structure suitable, for example, for model-based control applications. The traditional avenue established by the robotics and continuum dynamics communities has been to derive the dynamical model directly from first principles [12, 13, 4, 3] which provides physical interpretability and structure at the cost of needing substantial expert knowledge, for example in the selection of the proper kinematic approximations (e.g., \acPCC [14], \acPCS [12], \acGVS [13]). Suboptimal choices or even errors in applying this modeling procedure can lead to significant issues like inaccurate predictions and overly complex models. This hinders the democratization of soft robots, as only specialized research labs possess the required expertise [15]. Very recently, there has been a community push towards integrating physical structures and stability guarantees into learned models (e.g., Lagrangian Neural Networks [16], residual dynamical formulations [17, 18], or oscillatory networks [19]) which combine benefits from both worlds: they are learned directly from data which reduces the expert knowledge that is needed but at the same time exhibit a physical structure that can be exploited for model-based control and stability analysis. This work positions itself in this new trend of research, specifically focusing on deriving kinematic and dynamic models for continuum soft robots in a data-driven way. Indeed, deriving reduced-order kinematic representations remains the core challenge in physics-based modeling. Previous works have relied heavily on the modeling engineer‚Äôs intuition and experience to make decisions on the number of \acPCS segments, the length of each segment and which strains to consider [20]. However, these decisions are not straightforward and could easily result in models that are higher-dimensional than necessary or that important strains are ignored based on a wrong intuition [21]. Furthermore, identifying the dynamical parameters (e.g., stiffness, damping coefficients) of the soft robot required solving a nonlinear least-squares problem that is not always well behaved [22]. This paper proposes to solve these challenges by introducing an end-to-end approach that can automatically learn both a \acPCS kinematic parametrization and the corresponding dynamical model, including its dynamic parameters directly from image/Cartesian pose data. First, a kinematic fusion algorithm aims to minimize the \acpDOF of the \acPCS kinematic model while preserving a desired shape reconstruction accuracy for the given discrete shape measurements in Cartesian space. Secondly, an integrated strategy is proposed to simultaneously sparsify the strains of the \acPCS model and estimate the parameters of the dynamical model with closed-form linear least-squares. Contrary to common symbolic regression approaches such as SINDy [23], we crucially preserve the (physical) structure of the Euler-Lagrangian dynamics as derived according to the \acPCS model. This feature allows the derived dynamical model to be subsequently rapidly deployed within established model-based controllers [4]. We verify the approach in simulation in a diverse set of scenarios, including different robot topologies and the performance when measurement noise is present. Impressively, the method is able to accurately perform long-horizon (7 stimes7s7\text{\,}\mathrm{s}start_ARG 7 end_ARG start_ARG times end_ARG start_ARG roman_s end_ARG) shape predictions when being trained on 4 stimes4s4\text{\,}\mathrm{s}start_ARG 4 end_ARG start_ARG times end_ARG start_ARG roman_s end_ARG of trajectory data. We benchmark the proposed approach against several state-of-the-art dynamical model learning approaches (e.g., \acpNODE, \acCON, \acpLNN). On the training set, our proposed method exhibits a 70 %times70percent70\text{\,}\mathrm{\char 37\relax}start_ARG 70 end_ARG start_ARG times end_ARG start_ARG % end_ARG lower shape prediction error than the best-performing baseline method (\acNODE). However, we find that difference is even great for extrapolation scenarios (i.e., actuation sequences and magnitudes unseen during training): Here, our proposed method reduces the shape prediction error on the test set by 96 %times96percent96\text{\,}\mathrm{\char 37\relax}start_ARG 96 end_ARG start_ARG times end_ARG start_ARG % end_ARG compared to the best performing \acML baseline (\acNODE). Finally, we demonstrate how the Lagrangian structure of the identified dynamical model allows us to easily design a model-based controller that is effective at regulating the shape of the soft robot."
https://arxiv.org/html/2411.00107v1,"First, Learn What You Don‚Äôt Know:Active Information Gathering for Driving at the Limits of Handling","Combining data-driven models that adapt online and model predictive control (MPC) has enabled effective control of nonlinear systems. However, when deployed on unstable systems, online adaptation may not be fast enough to ensure reliable simultaneous learning and control. For example, controllers on a vehicle executing highly dynamic maneuvers may push the tires to their friction limits, destabilizing the vehicle and allowing modeling errors to quickly compound and cause a loss of control. In this work, we present a Bayesian meta-learning MPC framework. We propose an expressive vehicle dynamics model that leverages Bayesian last-layer meta-learning to enable rapid online adaptation. The model‚Äôs uncertainty estimates are used to guide informative data collection and quickly improve the model prior to deployment. Experiments on a Toyota Supra show that (i) the framework enables reliable control in dynamic drifting maneuvers, (ii) online adaptation alone may not suffice for zero-shot control of a vehicle at the edge of stability, and (iii) active data collection helps achieve reliable performance.","Controlling unstable nonlinear systems remains challenging [1]. For example, guiding a vehicle through dynamic and unstable drifting maneuvers is a difficult task where slight decision-making mistakes can lead to spin outs or crashes. In recent years, advanced control methods have been developed with the goal of giving autonomous vehicles the ability to leverage their full handling potential [2, 3, 4, 5, 6], with possible applications to the future design of advanced driver-assistance systems and autonomous driving safety systems. In this setting, data-driven model predictive control (MPC) methods have gained wide popularity [7] due to their high performance, capability to use expressive learned models, and ability to account for constraints such as actuator limits and obstacle avoidance. In particular, data-driven models that adapt using online data are able to reduce model errors and sim-to-real discrepancies at deployment time [8, 9, 10, 11]. However, simultaneously controlling and learning unstable systems is notoriously difficult. For instance, successfully executing a drifting maneuver requires a sufficiently-accurate initial model to precisely initiate the drift, and online adaptation alone may not enable identifying a model fast-enough to execute the maneuver, see Section V-C. Autonomous driving at the limits, therefore, presents a challenge for data-driven MPC with online adaptation, due to its fast-paced nature and the inherent instability of the executed maneuvers. These difficulties raise the following questions: How can we effectively identify uncertain nonlinear dynamics to ensure that subsequent control is reliable? How should informative data be collected efficiently and safely to minimize time-intensive and potentially expensive testing on hardware? Figure 1: We propose an active information gathering framework that guides data collection to quickly learn a model in a controlled (i.e., safe) setting. Adapting on this information-rich data enables reliable control in dynamic drifting trajectories. In contrast, directly using the model without adapting prior to deployment may lead to unreliable control performance such as spinning out. Contributions: We present an active information gathering framework to quickly identify nonlinear dynamics for reliable subsequent deployment in MPC. Specifically: ‚Ä¢ We present a data-driven vehicle dynamics model that combines a physics-based model and last-layer Bayesian meta-learning model. By learning expressive neural network features and only adapting the last layer, the model is conducive to rapid online adaptation and offers a structured and interpretable uncertainty representation. ‚Ä¢ We present an active information gathering method to identify an accurate model as fast as possible while satisfying constraints during data collection. ‚Ä¢ We extensively validate the proposed approach on a Toyota Supra in dynamic drifting maneuvers. In particular, results show that online adaptation of the meta-trained model may not be sufficient to reliably initiate a drift, and that active information gathering significantly helps quickly identifying a model and enabling robust drifting maneuvers. From a vehicle control perspective, our results demonstrate that executing drifting maneuvers is possible by adapting a model on autonomously-generated non-drifting trajectories. Specifically, information-rich trajectories do not necessarily involve drifting, and adapting on such data aids in robust execution of challenging drifting maneuvers."
https://arxiv.org/html/2411.00083v1,Learning Visual Parkour from Generated Images,"Fast and accurate physics simulation is an essential component of robot learning, where robots can explore failure scenarios that are difficult to produce in the real world and learn from unlimited on-policy data. Yet, it remains challenging to incorporate RGB-color perception into the sim-to-real pipeline that matches the real world in its richness and realism. In this work, we train a robot dog in simulation for visual parkour. We propose a way to use generative models to synthesize diverse and physically accurate image sequences of the scene from the robot‚Äôs ego-centric perspective. We present demonstrations of zero-shot transfer to the RGB-only observations of the real world on a robot equipped with a low-cost, off-the-shelf color camera. website: https://lucidsim.github.io","The success of a robot learning system depends largely on the realism and coverage of its training data. Real-world data, though inherently realistic, is limited in its coverage over the diverse scenarios a robot might encounter upon deployment. Real training data typically only includes a small number of environments and is not a reliable source for failures that cause injury or harm. As our robot improves throughout training, the data it needs to improve its skills further also evolves. Getting the right data is critical for improving the robot‚Äôs performance, but in current practice, this is a manual process that needs to be repeated from scratch for new scenes and new tasks. The alternative is to train in simulations, where we can sample a greater diversity of environmental conditions, and our robots can safely explore failure cases and learn directly from their own actions. Despite substantial investment into simulated physics and rendering, our best efforts at achieving realism retain a reality gap [1, 2, 3, 4]. This is because rendering a realistic image means making detailed and realistic scene content. Trying to hand-craft such content at scale to obtain the diversity required by our robots for sim-to-real transfer is prohibitively expensive. Without diverse and high-quality scene content, robots trained in simulation are too brittle to transfer to the real world. Therefore, how to match the real world in its infinite jest, and integrate color-perception into sim-to-real learning, is a key challenge. The purpose of this work is to develop a solution. We turn to generative models as a promising new data source for robot learning and use visual parkour as a testbed, where a robot dog equipped with a single color camera is tasked to scale tall obstacles at a fast speed. Our ultimate vision is to train robots entirely in generated worlds. At the heart of this is finding ways to exert precise control over the semantic composition and scene appearance to align with the simulated physics‚Äîwhile maintaining the randomness critical for sim-to-real generalization. Figure 1: The LucidSim graphics pipeline. We use the same parameterized terrain geometry as [5]. We use MuJoCo to simulate the physics, and render semantic masks and the depth image that are then fed into a ControlNet trained with MiDAS depth maps. The generated image is then combined with the dense optical flow to generate short videos via Dreams In Motion (DIM, see Sec. 2). Our method works as follows (Figure 1): we take a popular physics engine, MuJoCo [2], and render the depth image and semantic masks at each frame, that together are used as input to a depth-conditioned ControlNet. We then compute the ground-truth dense optical flow from the known scene geometry and changes in the camera poses, and warp the initial generated frame for the following six timesteps to produce a temporally consistent video sequence. On the learning side, we train the visual policy in two stages: first, we optimize the policy to imitate expert behavior from rollouts collected by a privileged teacher. The policy performs poorly after this pre-training step. The post-training step involves collecting on-policy data from the visual policy itself, interleaved with learning on all data aggregated so far. Repeating this step three times makes the visual policy significantly more performant. This policy is sufficiently robust to transfer zero-shot to color observations in the real world throughout our test scenes. Our contributions are threefold: First, we provide a scalable recipe to translate compute into real-world capabilities by producing geometrically and dynamically aligned visual data for robots. Second, we propose an auto-prompting technique to significantly increase data diversity, which in practice also enables tailored data synthesis. Finally, we provide the first demonstration of a robust, visual parkour policy trained entirely in simulation that has seen zero real-world data."
https://arxiv.org/html/2411.00081v1,PARTNR: A Benchmark for Planning and Reasoning in Embodied Multi-agent Tasks,"We present a benchmark for Planning And Reasoning Tasks in humaN-Robot collaboration (PARTNR) designed to study human-robot coordination in household activities. PARTNR tasks exhibit characteristics of everyday tasks, such as spatial, temporal, and heterogeneous agent capability constraints. We employ a semi-automated task generation pipeline using Large Language Models (LLMs), incorporating simulation in the loop for grounding and verification. PARTNR stands as the largest benchmark of its kind, comprising 100,000 natural language tasks, spanning 60 houses and 5,819 unique objects. We analyze state-of-the-art LLMs on PARTNR tasks, across the axes of planning, perception and skill execution. The analysis reveals significant limitations in SoTA models, such as poor coordination and failures in task tracking and recovery from errors. When LLMs are paired with real humans, they require 1.5x as many steps as two humans collaborating and 1.1x more steps than a single human, underscoring the potential for improvement in these models. We further show that fine-tuning smaller LLMs with planning data can achieve performance on par with models 9 times larger, while being 8.6x faster at inference. Overall, PARTNR highlights significant challenges facing collaborative embodied agents and aims to drive research in this direction.","Imagine a domestic robot that collaborates with humans in daily activities using natural language, akin to human-to-human interactions. This scenario requires two key features: the dynamic collaboration between the robot and the human, and the use of natural language for interaction. Current benchmarks in embodied AI typically satisfy one or the other condition; either robots operate in isolation (Shridhar et al., 2020; Zhu et al., 2023; Krantz et al., 2020; Majumdar et al., 2024), or tasks are not specified in natural language (Yenamandra et al., 2023; Puig et al., 2024; Szot et al., 2023; Jain et al., 2020). Despite significant progress in the field of embodied AI, there remains a gap in realistic benchmarks that evaluate robots in collaborative settings. To bridge this gap, we introduce Planning And Reasoning Tasks in humaN-Robot collaboration (PARTNR), a novel benchmark that evaluates the ability of embodied AI agents to collaborate with humans across a range of household activities in simulated indoor environments (Figure 1). PARTNR consists of 100,000 natural language instructions paired with tailored evaluation functions, focusing on four task types: (1) constraint-free, where sub-tasks can be completed in any manner by either agent, (2) spatial tasks that contain spatial constraints, (3) temporal tasks that require ordered execution, and (4) heterogeneous tasks that include actions that cannot be completed by one of the agents. Beyond the conventional challenges of long-horizon planning, novel partially observed environments, and large state and action spaces, PARTNR emphasizes the need for effective collaboration dynamics, such as task division and tracking partner‚Äôs progress. Figure 1: We present PARTNR, a benchmark for planning and reasoning in embodied multi-agent tasks, featuring 100,000 everyday tasks and evaluation functions generated semi-automatically, spanning 60 houses and 5,819 unique objects. We analyze LLM-based planning agents and also provide a human-in-the-loop tool to evaluate how agents collaborate with real humans. Curating such a benchmark of large-scale, natural language tasks with tailored evaluation functions presents significant challenges. Current benchmarks typically rely on either templated tasks (Shridhar et al., 2020; Zhang et al., 2024a) or tasks and evaluations crafted by humans (Mandi et al., 2024; Li et al., 2023a), which can restrict the diversity or the scale of the datasets. To overcome this, we propose a semi-automated generation method using Large Language Models (LLMs) with simulation-in-the-loop grounding. First, a Large Language Model (LLM) generates task and evaluation functions, which are grounded in the objects and furniture of a simulated house. Next, we employ simulation-in-the-loop to filter out hallucinations and infeasible instructions, complemented by human annotation to enhance diversity and accuracy. Subsequently, a set of 1,000 verified instructions and evaluation functions, along with diverse simulation houses, are utilized to guide an LLM through in-context prompting to create 100,000 tasks. As PARTNR consists of natural language tasks and LLMs have shown strong results in planning (Yao et al., 2023; Ahn et al., 2022; Huang et al., 2022), we explore prompting and fine-tuning LLMs, to assess their effectiveness in collaborative scenarios. We study the effect of observability of the environment (i.e., full or partial), centralized vs. decentralized multi-agent control, learned or privileged oracle robot skills, and different ways of grounding the 3D world information for LLM-based planning. Beyond these automated evaluations with synthetic human partners, we also perform evaluations with real humans-in-the-loop, with people performing the task alone, with a human partner, or with an LLM-guided robot partner. Overall, we find that LLMs struggle at coordination, task tracking and dealing with perception and skill errors. While humans are able to solve 93% of PARTNR tasks, SoTA LLMs can only successfully complete 30% under non-privileged conditions. Moreover, in decentralized multi-agent settings, task completion takes 1.3x more steps than single-agent, due to poor tracking of partner actions, resulting in extraneous actions. In contrast, human pairs outperform single humans, in our human-in-the-loop experiments, highlighting potential for improving LLM collaboration strategies. LLMs also struggle to recover from skill failures and perception grounding errors, resulting in lower performance when privileged skills and privileged perception are removed. When comparing model sizes, we observe that a smaller fine-tuned Llama3.1-8B achieves a similar performance to a Llama3.1-70B without finetuning, while being 8.6x faster. This faster inference plays an important role when evaluated with real humans-in-the-loop, where the finetuned model takes fewer steps and offloads more tasks from the human. In summary, PARTNR enables reproducible, large-scale, and systematic evaluations of embodied agents in a wide variety of collaborative scenarios. Through systematic evaluation, we reveal critical insights into the current limitations of LLM-based planners, opening interesting future research directions."
https://arxiv.org/html/2411.00728v1,"Multi-Agent Deep Q-Network with Layer-based
Communication Channel for Autonomous Internal Logistics Vehicle Scheduling in Smart Manufacturing","In smart manufacturing, scheduling autonomous internal logistic vehicles is crucial for optimizing operational efficiency. This paper proposes a multi-agent deep Q-network (MADQN) with a layer-based communication channel (LBCC) to address this challenge. The main goals are to minimize total job tardiness, reduce the number of tardy jobs, and lower vehicle energy consumption. The method is evaluated against nine well-known scheduling heuristics, demonstrating its effectiveness in handling dynamic job shop behaviors like job arrivals and workstation unavailabilities. The approach also proves scalable, maintaining performance across different layouts and larger problem instances, highlighting the robustness and adaptability of MADQN with LBCC in smart manufacturing.","Internal Logistics Vehicles (ILVs) are crucial in enhancing the performance of manufacturing systems by facilitating the movement of products within manufacturing facilities [1]. With the advent of Industry 4.0 technologies, the automation of these movements has been driven by multiple factors, including the improvement of production capacity and the reduction of injuries among human operators who traditionally moved heavy products. Notable technologies in this domain include Automated Guided Vehicles (AGVs) and Autonomous Intelligent Vehicles (AIVs), which autonomously transport products while considering workshop constraints and layouts. The implementation of these systems presents several challenges that require careful consideration. Key among these challenges is the task of determining the priority for transporting products and selecting the appropriate vehicle for each transportation task. Moreover, these scheduling activities must account for various constraints, including delivery times, vehicle capacity and battery charging requirements, handling breakdowns of vehicles and machines, addressing urgent jobs, etc. Effective vehicle management is therefore crucial to align internal logistics with manufacturing objectives, such as minimizing tardiness of orders, achieving a balanced workload among workstations and vehicles, and optimizing the energy consumption of vehicles. To address these challenges, scheduling strategies consist of a set of rules designed to allocate vehicles to transportation requests while accounting for these complex constraints. This paper addresses these challenges by introducing a multi-agent deep reinforcement learning approach along with a layer-based communication channel to dynamically allocate vehicles to transportation requests within a dynamically changing environment. Drawing from insights discussed in prior research by [2], which implemented a deep Q-network technique, this study makes several notable contributions. In the proposed multi-agent system, each job is instantiated as an individual agent, operating in a decentralized manner. These agents interact with one another to optimize their reward policies within a deep Q-network algorithm. By leveraging this multi-agent framework, the system gains enhanced capabilities to navigate the complexities inherent in a dynamic manufacturing environment, characterized by the dynamic arrival of jobs and occurrences of workstation breakdowns/unavailibilites. The principal manufacturing objectives targeted by this approach encompass the minimization of total job tardiness, number of tardy jobs and the reduction of vehicle energy consumption. To assess the efficacy of our proposed methodology, we conducted a comparative analysis against various heuristic methods. Results from this evaluation demonstrate the superior performance of our multi-agent deep reinforcement learning method over traditional heuristics for the considered objectives. The paper is structured as follows: Section 2 delves into the literature, exploring related work regarding scheduling ILVs. Section 3 encompasses the problem description. Section 4 outlines the proposed multi-agent deep reinforcement learning approach. The subsequent sections are devoted to the experimental results, conducting analysis, and drawing conclusions."
https://arxiv.org/html/2411.00543v2,3D Equivariant Pose Regression via Direct Wigner-D Harmonics Prediction,"Determining the 3D orientations of an object in an image, known as single-image pose estimation, is a crucial task in 3D vision applications. Existing methods typically learn 3D rotations parametrized in the spatial domain using Euler angles or quaternions, but these representations often introduce discontinuities and singularities. SO(3)-equivariant networks enable the structured capture of pose patterns with data-efficient learning, but the parametrizations in spatial domain are incompatible with their architecture, particularly spherical CNNs, which operate in the frequency domain to enhance computational efficiency. To overcome these issues, we propose a frequency-domain approach that directly predicts Wigner-D coefficients for 3D rotation regression, aligning with the operations of spherical CNNs. Our SO(3)-equivariant pose harmonics predictor overcomes the limitations of spatial parameterizations, ensuring consistent pose estimation under arbitrary rotations. Trained with a frequency-domain regression loss, our method achieves state-of-the-art results on benchmarks such as ModelNet10-SO(3) and PASCAL3D+, with significant improvements in accuracy, robustness, and data efficiency.","Figure 1: Types of representations for 3D rotation prediction. Existing methods consider predicting 3D rotations in the spatial domain. Our method predicts Wigner-D coefficients in the frequency domain, to obtain accurate pose in continuous space using an SO(3)-equivariant network. Predicting the 3D pose of objects, i.e., position and orientation, in 3D space from an image is crucial for numerous applications, including augmented reality [59], robotics [69, 63, 5, 4], autonomous vehicles [21, 50], and cryo-electron microscopy [79]. Estimating 3D orientation is particularly challenging due to rotational symmetries and the non-linear nature of rotations. In addition, unlike translations, rotations introduce unique challenges such as gimbal lock and the requirement for continuous, singularity-free representations. Existing methods often learn 3D rotations using spatial domain parameterizations like Euler angles, quaternions, or axis-angle representations, as illustrated in Figure 1. However, these parameterizations suffer from issues such as discontinuities and singularities [58, 54, 80], which can hinder the performance and reliability. SO(3)-equivariance enables accurate 3D pose estimation and improves generalization to unseen rotations. It ensures that outputs consistently change with the 3D rotation of the input, maintaining rotational consistency between the input and output across network layers. Despite its importance, many existing methods [80, 3, 52, 75, 44] often design networks without considering SO(3)-equivariance, resulting in suboptimal performance when dealing with 3D rotations. In addition, in the context of spherical CNNs [10, 16, 12, 18, 17, 36, 9] for efficient SO(3)-equivariant operations, the 3D rotation parametrization in the spatial domain is inadequate because these SO(3)-equivariant networks operate in the frequency domain. To address these challenges, we propose an SO(3)-equivariant pose harmonics regression network that directly predicts Wigner-D coefficients in the frequency domain for 3D rotation regression. Building on prior work [35, 28], our method leverages the properties of spherical CNNs [11], which operate in the frequency domain, to guarantee SO(3)-equivariant output representation. By directly regressing Wigner-D matrix coefficients, our approach eliminates the need to convert outputs into spatial representations during training, ensuring alignment with the operations of spherical CNNs. This design allows us to bypass the limitations inherent in traditional spatial parameterizations‚Äîsuch as discontinuities and singularities [58, 54, 80]‚Äîresulting in more precise and continuous pose estimation. We further introduce a frequency-domain MSE loss to enable continuous training of 3D rotations, with the flexibility to incorporate distributional losses [52] for effectively capturing rotational symmetries in objects. Our method achieves state-of-the-art performance on standard single object pose estimation benchmarks, including ModelNet10-SO(3) and PASCAL3D+, demonstrating high sampling efficiency and strong generalization to unseen 3D rotations."
https://arxiv.org/html/2411.00448v1,ConceptFactory: Facilitate 3D Object Knowledge Annotation with Object Conceptualization,"We present ConceptFactory, a novel scope to facilitate more efficient annotation of 3D object knowledge by recognizing 3D objects through generalized concepts (i.e. object conceptualization), aiming at promoting machine intelligence to learn comprehensive object knowledge from both vision and robotics aspects. This idea originates from the findings in human cognition research that the perceptual recognition of objects can be explained as a process of arranging generalized geometric components (e.g. cuboids and cylinders). ConceptFactory consists of two critical parts: i) ConceptFactory Suite, a unified toolbox that adopts Standard Concept Template Library (STL-C) to drive a web-based platform for object conceptualization, and ii) ConceptFactory Asset, a large collection of conceptualized objects acquired using ConceptFactory suite. Our approach enables researchers to effortlessly acquire or customize extensive varieties of object knowledge to comprehensively study different object understanding tasks. We validate our idea on a wide range of benchmark tasks from both vision and robotics aspects with state-of-the-art algorithms, demonstrating the high quality and versatility of annotations provided by our approach. Our website is available at https://apeirony.github.io/ConceptFactory.","In the current data-driven era, the availability of a large amount of training data with dense annotations has become an indispensable factor for the successful implementation of deep neural networks in a wide range of 3D object understanding tasks. Particularly, for tasks like segmentation, pose estimation and more sophisticated robot manipulation, current approaches [1, 2, 3, 4, 5, 6, 7, 8] require a substantial volume of annotations of semantic, pose and affordance knowledge to fully demonstrate their power. However, there are two primary issues demanding attention in 3D object knowledge annotation. On one hand, some types of knowledge such as affordance for manipulation are highly complicated to manually annotate [7], resulting in few existing datasets being available for such labels. On the other hand, common practices of acquiring these knowledge annotations [9, 10, 6] follow the conventional paradigm that only a single type of knowledge is labeled on one object at a time, for which researchers develop different annotation platforms to adapt to various knowledge types and let annotators engage in multiple rounds of annotations, taking significant time and human effort. In this paper, We present ConceptFactory as a novel annotation paradigm that addresses these existing issues and facilitates more efficient annotation of 3D object knowledge. The idea behind ConceptFactory originates from the well-known ‚ÄôRecognition-by-Components‚Äô theory [11] in human cognition research, which finds that the perceptual recognition of objects can be explained as a process of arranging generalized geometric components. Inspired by this theory, we devise an efficient knowledge annotation paradigm performing in two steps. i) Describe the shape of an object with generalized geometric concepts, or in other words, object conceptualization. ii) Procedurally define (different types of) knowledge on these generalized concepts. In this manner, all types of knowledge defined on the concepts can be automatically propagated to the object as various types of annotations, taking advantage of correspondence between the concepts and the object shape. ConceptFactory provides a favorable solution to both aforementioned issues. First, manual knowledge annotation on 3D objects, which can be very complicated in some cases, is no longer required. Instead, researchers only need to procedurally define a type of knowledge with mathematical rules on certain concepts, and these knowledge will be automatically propagated to all target objects consisting of such concepts. Second, intensive human effort is required only once during object conceptualization, compared to the conventional annotation paradigm where significant labor and time resources are repeatedly expended for annotating each type of knowledge. ConceptFactory comes with two critical components. The first one is ConceptFactory Suite, a unified toolbox that adopts Standard Concept Template Library (STL-C) to drive a web-based platform for object conceptualization. The STL-C consists of 263 concept templates that comprehensively covers the essential structure of daily objects, and the conceptualization platform guides users to select and parameterize concept templates in STL-C to describe a given object and thereby obtains the conceptualization result. Then, a wide range of knowledge, which is procedurally defined on the templates, can be automatically propagated to the object as annotations. The other component is ConceptFactory Asset, a large collection of conceptualized objects acquired using ConceptFactory suite, containing 4380 objects from 39 categories involving 39k template instances and 295k parameters. We present such asset considering that the object conceptualization process still requires certain human effort, thereby offering already conceptualized objects to the community would make it convenient for researchers to use and study on, e.g. customizing their own knowledge and conduct experiments with them. The knowledge annotations offered by our approach are mathematically grounded and functionally aligned, serving as a catalyst for machine intelligence to recognize and interact with objects. We demonstrate the effectiveness of our idea from both vision and robotic aspects on a wide range of benchmark tasks including segmentation, pose estimation and robot manipulation through state-of-the-art algorithms, figuring out that our approach can easily gather various types of annotations, with quality comparable or even better than those acquired through conventional annotation paradigms."
https://arxiv.org/html/2411.00417v1,"Closed-LoopStability of aLyapunov-BasedSwitching Attitude Controller forEnergy-EfficientTorque-Input-SelectionDuring Flight‚Ä†‚Ä†thanks:This work was supported by the Joint Center for Aerospace Technology Innovation (JCATI) throughAward172172172172, the Washington State University (WSU) Foundation and the Palouse Club through a Cougar Cage Award toN.‚ÄâO.‚ÄâP√©rez-Arancibia, and the WSU Voiland College of Engineering and Architecture through astart-upfund toN.‚ÄâO.‚ÄâP√©rez-Arancibia.‚Ä†‚Ä†thanks:F.‚ÄâM.‚ÄâF.‚ÄâR.‚ÄâGon√ßalvesandN.‚ÄâO.‚ÄâP√©rez-Arancibiaare with the School of Mechanical and Materials Engineering, Washington State University (WSU), Pullman,WA99164991649916499164-2920292029202920, USA.R.‚ÄâM.‚ÄâBenais with the Department of Mechanical and Civil Engineering, California Institute of Technology (Caltech), Pasadena,CA91125911259112591125-2100210021002100, USA. Corresponding authors‚Äô Email:francisco.goncalves@wsu.edu(F.‚ÄâM.‚ÄâF.‚ÄâR.‚ÄâG.);n.perezarancibia@wsu.edu(N.‚ÄâO.‚ÄâP.-A.)","We present a new Lyapunov-based switching attitude controller for energy-efficient real-time selection of the torque inputted to an uncrewed aerial vehicle (UAV) during flight. The proposed method, using quaternions to describe the attitude of the controlled UAV, interchanges the stability properties of the two fixed points‚Äîone locally asymptotically stable and another unstable‚Äîof the resulting closed-loop (CL) switching dynamics of the system. In this approach, the switching events are triggered by the value of a compound energy-based function. To analyze and ensure the stability of the CL switching dynamics, we use classical nonlinear Lyapunov techniques, in combination with switching-systems theory. For this purpose, we introduce a new compound Lyapunov function (LF) that not only enables us to derive the conditions for CL asymptotic and exponential stability, but also provides us with an estimate of the CL system‚Äôs region of attraction. This new estimate is considerably larger than those previously reported for systems of the type considered in this paper. To test and demonstrate the functionality, suitability, and performance of the proposed method, we present and discuss experimental data obtained using a ùüëùüè31\boldsymbol{31}bold_31-g quadrotor during the execution of high-speed yaw-tracking maneuvers. Also, we provide empirical evidence indicating that all the initial conditions chosen for these maneuvers, as estimated, lie inside the system‚Äôs region of attraction. Last, experimental data obtained through these flight tests show that the proposed switching controller reduces the control effort by about ùüìùüë53\boldsymbol{53}bold_53 %, on average, with respect to that corresponding to a commonly used benchmark control scheme, when executing a particular type of high-speed yaw-tracking maneuvers.","We envision a future in which swarms of robotic flying insects are deployed in unstructured environments to perform tasks useful for society; for example, biological research that requires the physical and visual tracking of natural insects. The execution of these types of maneuvers necessitates the use of high-performance, robust, and efficient controllers capable of following high-speed trajectories while maintaining stable flight. We can imagine, for instance, a robotic insect flying embedded in a bee colony; then, while tracking a particular specimen the robot is commanded to track a different bee and must promptly rotate in order to follow the new reference. Intuitively, it seems that we should always apply the control torque in the direction of the shorter rotational path; however, depending on the instantaneous attitude and angular-velocity errors of the flyer, it might be advantageous to apply the control torque in the direction of the longer rotational path, according to a user-defined performance figure of merit (PFM) [1, 2, 3, 4, 5]. In this paper, we use quaternions to represent the attitude kinematics and dynamics of the controlled uncrewed aerial vehicle (UAV) in space, a method widely used in aerial robotics and spacecraft control due to its numerical robustness and suitability to avoid issues caused by kinematic singularities [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1, 15, 16, 2, 17, 18, 19, 20, 21, 22, 3, 23]. Unfortunately, as discussed in [4, 5, 6, 7], when quaternions are used to define attitude controllers of the type presented in [7], the resulting closed-loop (CL) dynamics exhibit two fixed points corresponding to the same attitude kinematics but with opposite stability properties‚Äîone is locally asymptotically stable and the other is unstable. This issue brings challenges regarding flight performance; however, the stability properties of the two CL system‚Äôs fixed points can be interchanged in real time by simply changing the sign of a term in the control law‚Äîequivalent to reversing the direction of the torque input corresponding to that term‚Äîwhich is beneficial from a performance perspective in some cases. Specifically, a switching scheme of this type can be implemented to prevent unwinding behavior, defined as large rotations (>œÄabsentùúã>\pi> italic_œÄ rad) that return the flyer to its original orientation and are caused by representational ambiguities [16]. The most common technique used to avoid unwinding is to multiply the term proportional to the vector part of the attitude-error quaternion (AEQ) by the sign of its scalar part in the definition of the control law that specifies the total torque inputted to the UAV‚Äôs open-loop dynamics [10, 11, 12, 13, 14, 15, 7, 6, 9, 8]. This scheme ensures that the direction of the proportional torque is aligned with that of the shorter rotational path, which is not always the best decision from an energy perspective. To address this energy-motivated decision problem, [1] presents a heuristic method based on the a-priori specification of a set of rules extracted from one-thousand simulations with random initial conditions for orientation and angular velocity; [2] presents two hybrid controllers based on backstepping and the definition of an energy-like Lyapunov function (LF), respectively; [3] presents a hybrid controller that accounts for both the AEQ and angular-velocity error in the definition of a switching law; and, [4] presents a model-predictive method to select the most cost-efficient direction of the proportional torque input according to a user-defined PFM. More recently, in [5], we introduced a Lyapunov-based switching attitude controller that accounts for both the AEQ and the angular-velocity error to select, between two options, the torque law used for feedback control during flight; however, the chosen LF allows only for a very conservative estimation of the region of attraction of the stable CL fixed state, and we did not show that the unstable equilibrium is a saddle point‚Äîa topic thoroughly discussed in this paper. Here, we present a new Lyapunov-based switching attitude controller that selects the torque inputted to the controlled UAV, according to a criterion of energy efficiency, in real time. Also, we present stability analyses of the CL dynamics resulting from using the proposed switching scheme. Furthermore, by using a modified version of the LF in [5], we show that one of the two fixed states of the CL system is exponentially stable under a set of conditions, and provide an estimate of the corresponding region of attraction. To test the proposed approach, we implemented the new switching controller on a 31313131-g quadrotor to execute high-speed yaw-tracking maneuvers. The obtained experimental data compellingly demonstrate the suitability, functionality, and performance of the proposed approach. Figure 1: UAV platform used in the real-time flight experiments, the Crazyflie 2.12.1\boldsymbol{2.1}bold_2.1. Here, ùìë={ùíÉ1,ùíÉ2,ùíÉ3}ùìësubscriptùíÉ1subscriptùíÉ2subscriptùíÉ3\boldsymbol{\mathcal{B}}=\left\{\boldsymbol{b}_{1},\boldsymbol{b}_{2},% \boldsymbol{b}_{3}\right\}bold_caligraphic_B = { bold_italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , bold_italic_b start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT }, with its origin coinciding with the UAV‚Äôs center of mass, denotes the body-fixed frame of reference; ùìù={ùíè1,ùíè2,ùíè3}ùìùsubscriptùíè1subscriptùíè2subscriptùíè3\boldsymbol{\mathcal{N}}=\left\{\boldsymbol{n}_{1},\boldsymbol{n}_{2},% \boldsymbol{n}_{3}\right\}bold_caligraphic_N = { bold_italic_n start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_n start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , bold_italic_n start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT } denotes the inertial frame of reference. The rest of the paper is organized as follows. Section II reviews the main topics regarding the attitude dynamics of the controlled UAV and briefly describes a commonly-used quaternion-based continuous controller that we employed as the starting point of the presented research. Section III discusses the derivation of the two fixed points of the CL system resulting from using the continuous controller and their stability properties; this section also formulates and explains the performance problem associated with the implementation of quaternion-based attitude control laws of the type considered here. Section IV describes the switching control scheme introduced in this paper, derives the two equilibrium points‚Äîone stable and another unstable‚Äîof the resulting CL system, analyzes their stability, and provides an estimate for the region of attraction of the stable CL fixed point. Section V presents and analyzes experimental results. Last, Section VI states some conclusions. Notation: 1. ‚Ñù‚Ñù\mathbb{R}blackboard_R and ‚Ñù3superscript‚Ñù3\mathbb{R}^{3}blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT denote the sets of real numbers and triplets, respectively. 2. ùíÆ3superscriptùíÆ3\mathcal{S}^{3}caligraphic_S start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT denotes the set of unit quaternions. 3. Italic lowercase symbols denote scalars, e.g., pùëùpitalic_p; bold lowercase symbols denote vectors, e.g., ùíëùíë\boldsymbol{p}bold_italic_p; bold uppercase symbols denote matrices, e.g., ùë∑ùë∑\boldsymbol{P}bold_italic_P; and bold crossed lowercase symbols denote quaternions, e.g., ¬Ø‚Å¢ùíë‚Å¢ùê¶ùê¢ùê¨ùê¨ùê¢ùêßùê†¬Øùíëùê¶ùê¢ùê¨ùê¨ùê¢ùêßùê†\boldsymbol{\mathrlap{\mathchoice{\raisebox{-6.65pt}[0.0pt][0.0pt]{$% \displaystyle{\mathchar 22\relax\mkern-9.0mu}$}}{\raisebox{-6.65pt}[0.0pt][0.0% pt]{$\textstyle{\mathchar 22\relax\mkern-9.0mu}$}}{\raisebox{-6.65pt}[0.0pt][0% .0pt]{$\scriptstyle{\mathchar 22\relax\mkern-9.0mu}$}}{\raisebox{-6.65pt}[0.0% pt][0.0pt]{$\scriptscriptstyle{\mathchar 22\relax\mkern-9.0mu}$}}}pmissing}start_ARG ¬Ø end_ARG bold_italic_p bold_missing. 4. The symbols √ó\times√ó and ‚äótensor-product\otimes‚äó denote the vector cross-product and quaternion product, respectively. 5. The operator ‚à•‚ãÖ‚à•2\|\cdot\|_{2}‚à• ‚ãÖ ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT computes the 2222-norm of a vector. 6. The operator sgn‚Å¢{‚ãÖ}sgn‚ãÖ\text{sgn}\left\{\,\cdot\,\right\}sgn { ‚ãÖ } extracts the sign of a real scalar. 7. The symbols >>>, <<<, ‚â•\geq‚â•, and ‚â§\leq‚â§ denote ordering or definiteness relationships when used with scalars or matrices, respectively. 8. The symbol ùë∞ùë∞\boldsymbol{I}bold_italic_I denotes any identity matrix of adequate dimensions; the symbol ùüé0\boldsymbol{0}bold_0 denotes any block of zeros of adequate dimensions."
https://arxiv.org/html/2411.00174v1,"Pedestrian Trajectory Prediction with Missing Data: Datasets, Imputation, and Benchmarking","Pedestrian trajectory prediction is crucial for several applications such as robotics and self-driving vehicles. Significant progress has been made in the past decade thanks to the availability of pedestrian trajectory datasets, which enable trajectory prediction methods to learn from pedestrians‚Äô past movements and predict future trajectories. However, these datasets and methods typically assume that the observed trajectory sequence is complete, ignoring real-world issues such as sensor failure, occlusion, and limited fields of view that can result in missing values in observed trajectories. To address this challenge, we present TrajImpute, a pedestrian trajectory prediction dataset that simulates missing coordinates in the observed trajectory, enhancing real-world applicability. TrajImpute maintains a uniform distribution of missing data within the observed trajectories. In this work, we comprehensively examine several imputation methods to reconstruct the missing coordinates and benchmark them for imputing pedestrian trajectories. Furthermore, we provide a thorough analysis of recent trajectory prediction methods and evaluate the performance of these models on the imputed trajectories. Our experimental evaluation of the imputation and trajectory prediction methods offers several valuable insights. Our dataset provides a foundational resource for future research on imputation-aware pedestrian trajectory prediction, potentially accelerating the deployment of these methods in real-world applications. Publicly accessible links to the datasets and code files are available at https://github.com/Pranav-chib/TrajImpute.","Pedestrian trajectory prediction [1, 2, 3, 4, 5, 6, 7] has various essential applications, such as self-driving automobiles, robot navigation, human behavior understanding, and more. These systems forecast the future trajectory of pedestrians based on their previously observed paths. Research in pedestrian trajectory prediction has significantly advanced in recent times due to the development of data-driven solutions and datasets. However, a predominant assumption in most current research is that the past observed coordinates of pedestrians are complete. This assumption does not hold in real-world scenarios [8] where sensor failures, limited field of view, and occlusion can lead to missing observations at any specific time instances, resulting in incomplete trajectories. This creates challenges for trajectory prediction tasks in real-world scenarios. To improve the effectiveness of trajectory prediction methods in real-world scenarios, they must anticipate and handle missing observed coordinates. In multivariate time series, several imputation methods [9, 10, 11, 12] have emerged that address the issue of missing features by imputing them (filling of missing values). These methods [13, 14, 15, 16] utilize statistical and deep learning approaches and have achieved state-of-the-art results for imputation time series data. However, there has been limited exploration of imputation techniques in trajectory prediction [8, 17, 18], and there is a gap in the availability of imputation-centric pedestrian datasets, evaluation protocols, and benchmarks. We introduce TrajImpute, an imputation-centric trajectory prediction dataset, to address this. We have compiled commonly used pedestrian trajectory prediction datasets [19, 20], including ETH, HOTEL, UNIV, ZARA1, and ZARA2, (which are licensed for research purposes 111See the statement at the top of https://icu.ee.ethz.ch/research/datsets.html and in the ‚ÄúCrowds Data‚Äù card of https://graphics.cs.ucy.ac.cy/portfolio.) and introduced trajectories with missing observed coordinates. We follow two data generation strategies to simulate the missing coordinates: easy and hard modes. In the easy mode, we simulate scenarios where observed coordinates are missed for a shorter duration (could be continuous or discontinuous time frame). In contrast, the hard mode simulates scenarios where observed coordinates are missing for a longer duration. In addition to data generation, we benchmark several existing imputation methods [16, 14, 21, 9, 22, 23] on TrajImpute. We use these imputation methods to reconstruct the missing coordinates and evaluate their performance in both easy and hard modes. After extensive evaluation, we selected the best-performing imputation model and used its imputed data for the trajectory prediction task. The motivation of our work is to provide insights into how trajectory prediction models perform when missing coordinates are imputed. Additionally, we aim to understand how imputation methods perform on the pedestrian trajectory imputation task. Thus, TrajImpute provides a dataset in which missing coordinates are present in observed trajectories to simulate real-world scenarios and offers a unified framework for evaluating both imputation and trajectory prediction methods. Contributions. We introduce TrajImpute, a trajectory prediction dataset designed to simulate missing coordinates in observed trajectories of pedestrians. TrajImpute bridges the gap between real-world scenarios and the rigid assumption that all coordinates are present in observed trajectories. We conduct extensive analyses and empirical studies to evaluate several existing imputation methods for the task of trajectory imputation on our TrajImpute dataset. Furthermore, we evaluate the performance of recent trajectory prediction methods on imputed data and provide insights for future development in this area. The dataset is provided under a Creative Commons CC BY-SA 4.0 license, allowing both academics and industry to use it."
