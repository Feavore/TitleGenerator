URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.10182v1,Some Thoughts on Graph Similarity,"We give an overview of different approaches to measuring the similarity of, or the distance between, two graphs, highlighting connections between these approaches. We also discuss the complexity of computing the distances.","Graphs, or networks, are basic models ubiquitous in science and engineering. They are used to describe a diverse range of objects and processes, including, for example, chemical compounds, social interaction, molecular interaction, and computational processes. To understand and classify graph models, we need to compare graphs. Since data and models cannot always guaranteed to be exact, it is essential to understand what makes two graphs similar or dissimilar. There are many different approaches to similarity, for example, based on edit distance, common subgraphs, spectral similarity, behavioural equivalence, or as a limit case isomorphism. All of these are relevant in some situations because different applications have different demands to similarities. Applications of graph similarity occur in many different areas, among them combinatorial optimisation, computational biology, computer vision, database systems, data mining, formal verification, and machine learning. Traditional applications of isomorphism and similarity can be described as graph matching applications: real-world entities, ranging from chemical molecules to human faces, are modelled by graphs, and the goal is to match, or align them. Examples of this type of application appear in chemical information systems, computer vision, and database schema matching (see, e.g., [confogsanven04, emmdehshi16]). Many of these applications do not require exact isomorphisms. Instead, a relaxation to similarity is often sufficient and even desirable for dealing with imprecise and uncertain data. A second important form of application exploits similarity within a single graph, or, in the limit case symmetries of the graph, to design more efficient algorithms for hard algorithmic problems. We may use symmetries to prune search trees in backtracking algorithms (e.g. in SAT solving [sak09]) or to reduce the size of instances of hard algorithmic problems (e.g. in optimisation [bodherjos13, GroheKMS14] or model checking [claemejhasis98]). Iso-ButylbenzeneTert-ButylcyclohexaneIso-Butylcyclohexane Figure 1.1: Which of these three molecular graphs are most similar? It is not clear what makes two graphs similar. Consider the three molecular graphs in Figure 1.1. They all look somewhat similar. Usually, we will study these graphs, or the molecules they represent, in a specific application scenario, say, the design of synthetic fuels. It turns out that the two bottom molecules are more similar with respect to their relevant chemical properties. Can we design a graph similarity measure in a way that it puts these two graphs closer together than either of them to the third? We will not answer this question here (in practice, we might try to learn such a similarity measure from data), but the example illustrates that similarity may very much depend on specific applications, and there is certainly not a unique similarity measure suitable for all, or most, applications. The purpose of this paper is to sketch a theory of graph similarity. We will study different “principled” approaches to graph similarity, highlighting connections between these approaches. Central to my thinking about similarity are two different views on what makes two graphs similar. Under the operational view, two graphs are similar if one can easily be transformed into the other. Under the declarative view, two graphs are similar if they have similar properties. An example of an operational similarity is edit distance, measuring how many edges must be added and deleted from one graph to obtain a graph isomorphic to the other. Graph kernels used in machine learning provide examples for the declarative view. In a nutshell, graph kernels collect numeric features, for example, the number of triangles, and provide an inner product on the feature space. The operational view is algorithmic in nature, whereas the declarative view is rooted in logic and semantics. Ultimately, it will be a goal of our theory to establish connections between the approaches, viewing them as dual in some sense. Both views on similarity are also important from a practical perspective. We need to have algorithms measuring similarity, and often we not only need to measure how similar two graphs are but we actually want to transform one to the other (think of an application like database repairs). But we also need to have a semantical understanding of what we are doing, that is, we want to explain which features make two graphs similar."
https://arxiv.org/html/2411.10393v1,Guaranteed Bounds on Posterior Distributions of Discrete Probabilistic Programs with Loops,"We study the problem of bounding the posterior distribution of discrete probabilistic programs with unbounded support, loops, and conditioning. Loops pose the main difficulty in this setting: even if exact Bayesian inference is possible, the state of the art requires user-provided loop invariant templates. By contrast, we aim to find guaranteed bounds, which sandwich the true distribution. They are fully automated, applicable to more programs and provide more provable guarantees than approximate sampling-based inference. Since lower bounds can be obtained by unrolling loops, the main challenge is upper bounds, and we attack it in two ways. The first is called residual mass semantics, which is a flat bound based on the residual probability mass of a loop. The approach is simple, efficient, and has provable guarantees.The main novelty of our work is the second approach, called geometric bound semantics. It operates on a novel family of distributions, called eventually geometric distributions (EGDs), and can bound the distribution of loops with a new form of loop invariants called contraction invariants. The invariant synthesis problem reduces to a system of polynomial inequality constraints, which is a decidable problem with automated solvers. If a solution exists, it yields an exponentially decreasing bound on the whole distribution, and can therefore bound moments and tail asymptotics as well, not just probabilities as in the first approach.Both semantics enjoy desirable theoretical properties. In particular, we prove soundness and convergence, i.e. the bounds converge to the exact posterior as loops are unrolled further. We also investigate sufficient and necessary conditions for the existence of geometric bounds. On the practical side, we describe Diabolo, a fully-automated implementation of both semantics, and evaluate them on a variety of benchmarks from the literature, demonstrating their general applicability and the utility of the resulting bounds.","Probabilistic programming Probabilistic programming is a discipline that studies programming languages with probabilistic constructs (Barthe et al., 2020). The term is overloaded however. At the intersection with randomized algorithms and program analysis, it usually means a programming language with a construct for probabilistic branching or sampling from probability distributions. As such, it is simply a language to express programs with random numbers and researchers study program analysis techniques for termination probabilities, safety properties, cost analysis, and others. At the intersection with statistics and machine learning, probabilistic programming is used to express (Bayesian) statistical models (van de Meent et al., 2018). Bayesian inference is a very successful framework for reasoning and learning under uncertainty: it updates prior beliefs about the world with observed data to obtain posterior beliefs using Bayes’ rule. As such, the programming languages for Bayesian models provide a construct for conditioning on data in addition to sampling from distributions. Since Bayesian inference is a difficult problem, a lot of research focuses on inference algorithms, in particular their correctness and efficiency. This paper contributes to both areas by developing methods to bound the distributions arising from probabilistic programs, especially those with loops. \displaystyle Throws:=0;Die:=0; \displaystyle{\color[rgb]{.5,0,.5}\mathsf{while}}\,{Die\neq 6}\,\allowbreak\{ \displaystyle\quad Die\sim{\color[rgb]{0,.5,.5}\mathsf{Uniform}}\{1\mathchar 4% 4\relax\nolinebreak[3]\dots\mathchar 44\relax\nolinebreak[3]6\}; \displaystyle\quad{\color[rgb]{.5,0,.5}\mathsf{observe}}\,Die\in\{2\mathchar 4% 4\relax\nolinebreak[3]4\mathchar 44\relax\nolinebreak[3]6\}; \displaystyle\quad Throws\mathbin{{+}{=}}1\} Figure 1. A probabilistic program with a loop and conditioning. Example 1.1. To illustrate the concept, consider the following puzzle due to Elchanan Mossel. You throw a fair six-sided die repeatedly until you get a 6. You observe only even numbers during the throws. What is the expected number of throws (including the 6) conditioned on this event? This is a surprisingly tricky problem and most people get it wrong on the first try111In a survey on Gil Kalai’s blog, only 27% of participants chose the correct answer (https://gilkalai.wordpress.com/2017/09/07/tyi-30-expected-number-of-dice-throws/)., based on the incorrect assumption that it is equivalent to throwing a die with only the three faces 2, 4, and 6. Probability theory and statistics abound with such counterintuitive results (e.g. the Monty-Hall problem), and probabilistic programming offers a precise way to disambiguate their description and make them amenable to automatic analysis and inference tools. Mossel’s problem can be expressed as the probabilistic program in Fig. 1. The program has a loop that samples a die until it shows 6, and conditions on the number being even. In each iteration, the counter Throws is incremented. 1.1. Challenges Bayesian inference In Bayesian inference, Bayes’ rule is used to update prior distributions p(\theta) of model variables \theta with observed data x to obtain posterior distributions: p(\theta\mid x)=\frac{p(x\mid\theta)p(\theta)}{p(x)}. In practice, such Bayesian statistical models are too complex for manual calculations and inferring their posterior distribution is a key challenge in Bayesian statistics. There are two approaches: exact and approximate inference. Exact inference aims to find an exact representation of the posterior distribution. Such methods impose heavy restrictions on the supported probabilistic programs and do not usually scale well. Practitioners therefore mostly use approximate methods that do not aim to compute this distribution exactly, but rather to produce unbiased or consistent samples from it. If the probabilistic program does not contain conditioning, samples can simply be obtained by running the program. But with observations, program runs that violate the observations must be rejected. Since the likelihood of the observations is typically low, simple rejection sampling is inefficient, and thus practical samplers use more sophisticated techniques, such as Markov chain Monte Carlo. While more scalable, these approaches typically do not provide strong guarantees on the approximation error after a finite amount of time (Gelman et al., 2013, Section 11.5). Loops Loops are essential to the expressiveness of programming languages but notoriously hard to analyze. This applies even more strongly to the probabilistic setting, where deciding properties like termination is harder than in the deterministic setting (Kaminski and Katoen, 2015). Even if a program does not use conditioning, loops can still make sampling difficult. For example, a program may terminate almost surely, but its expected running time may be infinite. This prevents sampling-based approaches since they need to run the program. Furthermore, many inference algorithms are not designed to handle unbounded loops and may return erroneous results for such programs (Beutner et al., 2022). On the formal methods side, various approaches for probabilistic loop analysis have been proposed, employing techniques such as martingales, moments, and generating functions (see Section 7). If all program variables have finite support, the program can be translated to a probabilistic transition system and techniques from probabilistic model checking can be used. None of these analysis techniques can be applied to Example 1.1 however: methods from cost analysis do not support conditioning and probabilistic model checking requires finite support (but Throws is supported on \mathbb{N}). The approach by Klinkenberg et al. (2024) via generating functions is theoretically applicable, but requires the user to provide a loop invariant template, i.e. a loop invariant where certain parameters may be missing. Unfortunately, such an invariant cannot always be specified in their language (Klinkenberg et al., 2024, Example 25). Even in cases where this is possible, we argue that figuring out its shape is actually the hard part: it already requires a good understanding of the probabilistic program and its distribution, so it is not a satisfactory solution. 1.2. Guaranteed bounds To deal with the above challenges, we investigate guaranteed bounds on the program distribution. “Guaranteed” here refers to a method that computes deterministic (non-stochastic) results about the mathematical denotation of a program (Beutner et al., 2022). Such bounds are applicable more often than exact inference, e.g. in the presence of loops/recursion, and provide more assurance than approximate methods, which have at best stochastic guarantees. Why are such bounds useful? Partial correctness properties In quantitative program analysis, one can verify safety properties by bounding the probability of reaching an unsafe state. Bounding reachability probabilities is also a common problem in probabilistic model checking and quantitative program verification, yet it has not seen much attention in the context of probabilistic programming with conditioning, aside from the work by Beutner et al. (2022) and Wang et al. (2024). Neither of those can bound moments of infinite-support distributions, whereas our work finds tight bounds on the expected value of Throws in Fig. 1 (see Section 6.4). Figure 2. Histogram of samples from two inference algorithms (importance sampling and Pyro’s HMC), and the guaranteed bounds from Beutner et al. (2022). The bounds show that Pyro’s HMC produces wrong results. (Source: Beutner et al. (2022)) Checking approximate inference In the context of Bayesian inference, the bounds can be useful to check and debug approximate inference algorithms and their implementations. If the approximate results clearly contradict the bounds, the inference algorithm is likely incorrect, or some of its assumptions are violated, or it has not converged. Beutner et al. (2022) provide an example of this: the inference tool Pyro yields wrong results for a probabilistic program with loops, but their bounds can detect this issue (Fig. 2).222 The cause turned out to be an undocumented assumption in the inference algorithm. Pyro’s implementation seems to assume that the dimension (number of samples in a program run) of the problem is constant, which is violated when sampling inside probabilistic loops. Another problem with approximate inference is the tail behavior of the posterior distribution, which is often crucial for the quality of the approximation (Liang et al., 2023). Previous work on guaranteed bounds (Beutner et al., 2022; Wang et al., 2024) does not address this aspect, but our work can bound the tail behavior as well. Table 1. Comparison of our two approaches with the most relevant related work on probabilistic programs with loops. (Cond.: supports (Bayesian) conditioning; Inf.: branching on variables with infinite support is allowed; Cont.: continuous distributions allowed; Auto.: fully automated; Prob.: computes/bounds probabilities; Mom.: computes/bounds moments; Tails: computes/bounds tail asymptotics of this shape. Partial support is denoted by “\sim”.) Type Cond.? Inf.? Cont.? Auto.? Prob.? Mom.? Tails? Moosbrugger et al. (2022) exact ✗ ✗ ✓ ✓ \sim ✓ O(n^{-k}) Beutner et al. (2022) bounds ✓ ✓ ✓ ✓ ✓ ✗ ✗ Wang et al. (2024) bounds ✓ ✓ ✓ ✓ ✓ ✗ ✗ Klinkenberg et al. (2024) exact ✓ ✓ ✗ ✗ ✓ ✓ ✗ Resid. mass sem. (Section 3) bounds ✓ ✓ ✗ ✓ ✓ ✗ ✗ Geom. bounds (Section 4) bounds ✓ ✓ ✗ ✓ ✓ ✓ O(c^{n}) Problem Statement Given a probabilistic program with posterior distribution \mu on \mathbb{N}, our goal is to bound: (1) probability masses: given n\in\mathbb{N}, find l\mathchar 44\relax\nolinebreak[3]u\in[0\mathchar 44\relax\nolinebreak[3]1] such that l\leq\mathbb{P}_{X\sim\mu}[X=n]\leq u; (2) moments: given k\in\mathbb{N}, find l\mathchar 44\relax\nolinebreak[3]u\in\mathbb{\mathbb{R}}_{\geq 0} such that l\leq\mathbb{E}_{X\sim\mu}[X^{k}]\leq u; (3) tail asymptotics: find c\in[0\mathchar 44\relax\nolinebreak[3]1) such that \mathbb{P}_{X\sim\mu}[X=n]=O(c^{n}). 1.3. Contributions In this paper, we develop two new methods to compute guaranteed bounds on the distribution of discrete probabilistic programs with loops and conditioning. Lower bounds can simply be found by unrolling each loop a finite number of times. The main challenge is upper bounds and we attack it in two ways: the first is simple, always applicable, and efficient, but coarse; the second is more sophisticated and expensive, but yields much more informative bounds if applicable. A summary of the most relevant related work is presented in Table 1 and a detailed account in Section 7. The first semantics, called residual mass semantics (Section 3), is based on the simple idea of bounding the remaining probability mass after the loop unrollings, which has not previously been described, to our knowledge. We make the following contributions: • We introduce the residual mass as a simple but effective idea to bound posterior probabilities. • We prove soundness and convergence of the bounds to the true distribution (as loops are unrolled further and further). • We implement the semantics in a tool called Diabolo and demonstrate empirically that the implementation is more efficient than previous systems (Section 6.3). The second semantics, called geometric bound semantics (Section 4), is the main novelty of this paper. The idea is to bound the distribution of loops in a more fine-grained manner with geometric tails, rather than a flat bound as in the first semantics. • We present the concept of a contraction invariant for a loop, which yields upper bounds on the distribution (Section 4.1). • We introduce a family of distributions called eventually geometric distributions (EGDs) that are used as an abstract domain to overapproximate the distribution of a loop (Section 4.2). • We present the geometric bound semantics (Section 4.3) which reduces the synthesis problem of such an invariant to solving a system of polynomial inequalities. If successful, it immediately yields bounds on probability masses and, contrary to the first semantics, also on moments and tail probabilities of the program distribution. • We prove soundness of the semantics and convergence of the bounds, as loops are unrolled further and further (Section 4.5). • We identify necessary conditions and sufficient conditions for its applicability (Section 4.5). • We fully automate it in our tool Diabolo (Section 5): contrary to previous work (Klinkenberg et al., 2024), it does not rely on the user to provide a loop invariant (template). • We demonstrate its applicability on a large proportion of benchmarks from the literature and compare it to previous approaches and the residual mass semantics (Section 6). Full proofs and additional details can be found in Appendices A, B, C and D and Zaiser (2024b). 1.4. Limitations Our work deals with discrete probabilistic programs with hard conditioning. This means that programs cannot sample or observe from continuous distributions. Variables in our programming language take values in \mathbb{N}; negative numbers are not supported (see Section 8.1 for possible extensions). While our language is Turing-complete, some arithmetic operations like multiplication as well as some common infinite-support distributions (e.g. Poisson) are not directly supported (see Section 2.2 for details on our language’s expressivity). The initial values of the program variables are fixed: our methods cannot reason parametrically about these inputs. The residual mass semantics can yield bounds on the distribution of any such probabilistic program, but convergence with increasing unrolling is only guaranteed if the program terminates almost surely. If the program distribution has infinite support, we cannot bound the moments or tails: the bound does not contain enough information for this. The geometric bound semantics yields EGD bounds, which allow bounding moments and tails. On the other hand, such bounds do not exist for all programs. Our experiments show that this is not a big concern for many probabilistic programs with loops in practice: EGD bounds exist for a majority of examples we found in the literature. Another limitation of EGD bounds is that they cannot represent correlation of the tails of two variables, which may lead to imprecise tail bounds or failing to find bounds at all. Finally, solving the system of polynomial inequalities arising from the semantics, while decidable, can be hard in practice and does not scale to very large programs. It should be noted that scalability is a general issue in probabilistic program analysis owing to the hardness of the problem (Dagum and Luby, 1993) and not specific to our work. 1.5. Notation and conventions We use the Iverson brackets [\varphi] to mean 1 if \varphi is satisfied and 0 otherwise. We write variables representing vectors in bold ({\bm{\alpha}}), tensors (multidimensional arrays) in uppercase and bold (\mathbf{T}), and random or program variables in uppercase (X). We write \mathbf{0} and \mathbf{1} for the constant zero and one functions. We write \mathbf{0}_{n} and \mathbf{1}_{n} for the zero and one vectors in \mathbb{R}^{n}. To update the k-th component of a vector {\bm{\alpha}}, we write {\bm{\alpha}}[k\mapsto v]. Vectors {\bm{\alpha}}\in\mathbb{R}^{n} are indexed as \alpha_{1}\mathchar 44\relax\nolinebreak[3]\dots\mathchar 44\relax\nolinebreak% [3]\alpha_{n}. We abbreviate [d]:=\{0\mathchar 44\relax\nolinebreak[3]\dots\mathchar 44\relax\nolinebreak[3% ]d-1\}. Tensors \mathbf{T}\in\mathbb{R}^{[d_{1}]\times\dots\times[d_{n}]} are indexed as \mathbf{T}_{i_{1}\mathchar 44\relax\nolinebreak[3]\dots\mathchar 44\relax% \nolinebreak[3]i_{n}} where i_{k} ranges from 0 to d_{k}-1. We write \mathbf{0}_{[d_{1}]\times\dots\times[d_{n}]} or simply \mathbf{0} for the zero tensor in \mathbb{R}^{[d_{1}]\times\dots\times[d_{n}]}. We write |\mathbf{T}|=(d_{1}\dots\mathchar 44\relax\nolinebreak[3]d_{n}) for the dimensions of \mathbf{T}\in\mathbb{R}^{[d_{1}]\times\dots\times[d_{n}]}, and in particular |\mathbf{T}|_{i}=d_{i}. To index \mathbf{T} along dimension k, we write \mathbf{T}_{k:j}\in\mathbb{R}^{[d_{1}]\times\cdots\times[d_{k-1}]\times[d_{k+1% }]\times\cdots\times[d_{n}]}, which is defined by (\mathbf{T}_{k:j})_{i_{1}\mathchar 44\relax\nolinebreak[3]\dots\mathchar 44% \relax\nolinebreak[3]i_{k-1}\mathchar 44\relax\nolinebreak[3]i_{k+1}\mathchar 4% 4\relax\nolinebreak[3]\dots\mathchar 44\relax\nolinebreak[3]i_{n}}=\mathbf{T}_% {i_{1}\mathchar 44\relax\nolinebreak[3]\dots\mathchar 44\relax\nolinebreak[3]i% _{k-1}\mathchar 44\relax\nolinebreak[3]j\mathchar 44\relax\nolinebreak[3]i_{k+% 1}\mathchar 44\relax\nolinebreak[3]\dots\mathchar 44\relax\nolinebreak[3]i_{n}}. We often write tensor indices as {\bm{i}}:=(i_{1}\mathchar 44\relax\nolinebreak[3]\dots\mathchar 44\relax% \nolinebreak[3]i_{n}) for brevity. We also abbreviate {\bm{\alpha}}^{{\bm{i}}}:=\prod_{k=1}^{n}\alpha_{i}^{i_{k}}. Other binary operations (+, -, \min, \max, etc.) work elementwise on vectors and tensors, e.g. ({\bm{\alpha}}+{\bm{\beta}})_{j}:=\alpha_{j}+\beta_{j} and {\bm{\alpha}}\leq{\bm{\beta}} if and only if \alpha_{j}\leq\beta_{j} for all j."
https://arxiv.org/html/2411.10207v1,Insights from a workshop on gamification of research in mathematics and computer science,"Can outreach inspire and lead to research and vice versa? In this work, we introduce our approach to the gamification of research in mathematics and computer science through three illustrative examples. We discuss our primary motivations and provide insights into what makes our proposed gamification effective for three research topics in discrete and computational geometry and topology: (1) DominatriX, an art gallery problem involving polyominoes with rooks and queens; (2) Cubical Sliding Puzzles, an exploration of the discrete configuration spaces of sliding puzzles on the d-cube with topological obstructions; and (3) The Fence Challenge, a participatory isoperimetric problem based on polyforms. Additionally, we report on the collaborative development of the game Le Carré du Diable, inspired by The Fence Challenge and created during the workshop Let’s talk about outreach!, held in October 2022 in Les Diablerets, Switzerland. All of our outreach encounters and creations are designed and curated with an inclusive culture and a strong commitment to welcoming the most diverse audience possible.","To set the stage, we begin this contribution by introducing one of our research topics through a small puzzle. After all, if you are reading this article, there is a good chance you enjoy learning about and solving puzzles. The Fence Challenge with tetrominoes Using the following five pieces (called tetrominoes), enclose the biggest area possible. ilnot (1) Using the five pieces depicted above in (1), which you might recognise from the game Tetris [pajitnov1984tetris], try to enclose the largest possible area on a square grid. To clarify the objective of the puzzle, we present a hypothetical dialogue between an imaginary reader and us. Figure 1: Left, a corner-connection, not closing the fence; right an edge-connection with the rook-movement — Reader: The puzzle asks us to enclose the largest area, but how do we define “enclosing”? — Us: Great question! We want to use the five tetrominoes… — Reader (interrupting): Tetrominoes? — Us: Yes, that’s the term we use for the pieces! “Tetro-” means “four,” and “mino” comes from “domino,” so this means “like a domino but with four tiles”. — Reader: Got it! Please continue. — Us: Right! An area composed of a set of unit squares on the grid is considered “enclosed” if it is bounded by an edge-connected fence made of tetrominoes. — Reader: By “edge-connected,” do you mean I can’t connect the pieces by their corners? — Us: More or less! While it’s possible for some pieces to touch at the corners, they still need to be connected through their edges. To clarify, the fence you build with the five tetrominoes must be rook-connected, meaning you should be able to move a chess rook from any square of the fence to any other square, as if the entire grid were a chessboard. — Reader (taking two tetrominoes): I see! If I place two tetrominoes together like this (Figure 1, right), it is rook-connected, but not in this other arrangement (Figure 1, left), right? — Us: Exactly! Now try to build a fence using four of the five pieces, say i, l, n, t. — Reader: Hmm, like this (Figure 2)? Is that allowed? — Us: Yes! And see, you’ve created two holes! It is allowed, but do you think it will give you the largest possible enclosed area? — Reader: Probably not! Hmm, I’ll work on it a bit more and come back to you when I find a good solution using all five pieces. Puzzles, such as the one above, are a good way to engage people in thinking logically and using mathematics as a way to model them and solve them, but we wish to do so on a different level, finding ways to go beyond puzzles and really to “gamify” mathematical research to let it be experienced by the public. In its ideal form, it would also enable the participation of the public in the research itself, providing a sort of feedback loop between the researchers and the people experiencing the game, making it a collaborative endeavour. Figure 2: A tentative of tetromino fence (without the “o”). The primary example of gamification of research presented in this paper, the Fence Challenge, was inspired by a generalization of the tetromino fence problem described above [LRMR23]. Additionally, we discuss a collaborative game, Le Carré du Diable, developed with input from participants of the workshop Gamification of Research, held during the conference Let’s talk about outreach!. During the same workshop, we also introduced two other fully developed instances of gamification of research: one based on the computational complexity of art-gallery problems on polycubes [AR21, LRMR22], and another stemming from the study of higher-dimensional cubical sliding puzzles [BMRV23] (see Sections 2.1 and 2.2). It is not the goal of this contribution to provide fully developed theoretical arguments regarding the benefits of gamification. The concept of gamification and its implications form a rich field of study that requires far more space to address comprehensively. Here, we aim, instead, to highlight certain design choices we made and share insights gained during our outreach activities. In the future, we hope to systematically analyse our setups and the behaviour of participants to better understand the impact of our approach. Our outreach efforts aim to engage the widest audience possible, with particular emphasis on participants who might not typically enjoy mathematics. Beyond merely exposing them to mathematical concepts, we aim to immerse them in contemporary research and even invite them to contribute to it. Games serve as a powerful medium for achieving these goals by breaking down barriers and fostering curiosity. The ludic potential of games can transform attitudes toward mathematics [BHMW85], and research shows that enjoyment of mathematics is one of the strongest predictors of student success [DDYF20]. Furthermore, games enable collective engagement, allowing participants and their surrounding communities to share the experience of mathematical discovery [MSJCJCSS17, QP22]. The idea of presenting mathematics through games has a long and rich history. From the educational tools of the tutor in Rousseau’s Émile [Rousseau1762] to Martin Gardner’s Mathematical Games columns [GardnerMathGames], games have been used to make mathematics accessible and engaging. Foundational works like Berlekamp, Conway, and Guy’s Winning ways for your mathematical plays [BCG04] study games mathematically, inspiring further gamification efforts. Recent examples, such as Ravi Vakil’s “bedtime story” for understanding long exact sequences [Ra21], Lee and Hua’s Homotopy Type Theory quests [HoTT], and Buzzard and Pedramfar’s Natural Number Game [Buzz], demonstrate how games can bridge the gap between recreational engagement and mathematical research. These examples illustrate the transformative potential of games, and we hope that the experiences shared here will inspire others to explore the interplay between games and research-level mathematics. We now provide an overview of the sections in this contribution. Section 2 briefly presents our first two examples of gamification along with some of our design choices. Section 3 focuses on the Fence Challenge, the gamification exercise proposed to participants of Let’s talk about outreach!, and the collaborative game developed during this workshop. In Section 4, we synthesise our observations and outline the next steps in our research. Lastly, readers interested in the puzzle introduced earlier in this section can find its complete solution in Appendix A, as well as supplementary figures for the game in Appendix B."
https://arxiv.org/html/2411.10093v1,Bounded degree QBF and positional games,"The study of \SAT and its variants has provided numerous \NP-complete problems, from which most \NP-hardness results were derived. Due to the \NP-hardness of \SAT, adding constraints to either specify a more precise \NP-complete problem or to obtain a tractable one helps better understand the complexity class of several problems. In 1984, Tovey proved that bounded-degree \SAT is also \NP-complete, thereby providing a tool for performing \NP-hardness reductions even with bounded parameters, when the size of the reduction gadget is a function of the variable degree. In this work, we initiate a similar study for \langQBF, the quantified version of \SAT. We prove that, like \SAT, the truth value of a maximum degree two quantified formula is polynomial-time computable. However, surprisingly, while the truth value of a 3-regular 3-\SAT formula can be decided in polynomial time, it is \PSPACE-complete for a 3-regular \langQBF formula. A direct consequence of these results is that Avoider-Enforcer and Client-Waiter positional games are \PSPACE-complete when restricted to bounded-degree hypergraphs. To complete the study, we also show that Maker-Breaker and Maker-Maker positional games are \PSPACE-complete for bounded-degree hypergraphs.","The \NP-completeness of \SAT established by Cook [Coo71] marked the beginning of the study of computational hardness. To better understand these problems and explore the boundary between tractable and \NP-hard problems, several variants of \SAT have been introduced, providing proofs that many problems are \NP-complete, even when restricted to specific instances. The most commonly studied variant is 3-\SAT, where \SAT is restricted to clauses of size at most 3. However, other variants also appear frequently, depending on the context, such as planar 3-\SAT or 1-in-3-\SAT. For examples of such variants and reductions requiring specific conditions on the formulas, we refer the reader to Karp’s article [Kar72] and Papadimitriou’s book [Pap94]. In \PSPACE, the central problem from which most reductions are derived is 3-\langQBF, the quantified version of 3-\SAT, i.e., quantified Boolean formulas restricted to clauses of size at most 3. In this paper, we adopt the two-player game interpretation of \langQBF proposed by Stockmeyer and Meyer [SM73] when they proved its \PSPACE-completeness. Let \psi=Q_{1}x_{1},\dots,Q_{n}x_{n}\varphi be a quantified Boolean formula, where for each 1\leq i\leq n, Q_{i}\in\{\exists,\forall\} and \varphi is a quantifier-free Boolean formula over x_{1},\dots,x_{n}. The \langQBF game played on \psi involves two players, Satisfier and Falsifier, as follows: for i=1 to n, if Q_{i}=\exists, Satisfier assigns a truth value to x_{i}, and if Q_{i}=\forall, Falsifier assigns a truth value to x_{i}. Once all variables have been assigned values, a valuation \nu for x_{1},\dots,x_{n} is obtained, and Satisfier wins if and only if \nu satisfies \varphi. Let \varphi be a Boolean formula, and let x be a variable in \varphi. We denote by d(x) the degree of x in \varphi, i.e., the number of clauses in which x appears. We also denote by \Delta(\varphi) the degree of \varphi, which is the maximum degree of any variable in \varphi. A formula \varphi is said to be k-regular if every variable in \varphi has degree k. If c\in\varphi is a clause, we denote by |c| its size, which is the number of literals in c. The rank of a formula \varphi is the largest size of any clause in \varphi. A formula \varphi is said to be k-uniform if all its clauses have size exactly k. This paper focuses on bounded-degree formulas, i.e., formulas in which each variable appears in a bounded number of clauses. Bounded-degree \SAT has been studied since 1984, when Tovey [Tov84] proved that \SAT remains \NP-complete when restricted to formulas with a maximum degree of 3, but is polynomial-time solvable for formulas with a maximum degree of 2 or for 3-regular 3-uniform formulas. We initiate here the study of bounded-degree \langQBF. For integers r,d, we denote by r-\langQBF-d the set of Boolean formulas in which the clauses have size at most r and where each variable appears in at most d clauses. If either r or d is unspecified, there are no restrictions on them. This study is motivated by the fact that \langQBF is a central problem for \PSPACE hardness reductions, and restricted versions of it can lead to \PSPACE-hard problems on specific instances. The motivation for this study primarily stems from the desire to understand bounded-degree positional games. Positional games were introduced by Hales and Jewett [HJ63] in 1963, and the study of their complexity began in 1978 when Schaefer proved that the Maker-Breaker version is \PSPACE-complete [Sch78]. In this convention, two players, Maker and Breaker, alternate claiming the vertices of some hypergraph \mathcal{H}. Maker wins if the vertices she manage to claim all the vertices of some hyperedge of \mathcal{H}, otherwise, Breaker wins. More recently, other conventions have been proven to be \PSPACE-complete, including Maker-Maker [Bys04], Avoider-Avoider [FGM+15, BH19], Avoider-Enforcer [GO23], and Client-Waiter [GOTT24]. However, all these proofs focused on the rank of the hypergraph (i.e., the size of its largest hyperedge) and did not attempt to optimize its maximum degree. For a general overview of the complexity of positional games, we refer the reader to Oijid’s Ph.D. thesis [Oij24]. Recent studies on positional games focus on games played on graphs, where certain graph structures are considered as the winning sets [Sla00, DGPR20, DGI+23, BFM+23]. In most of these studies, both the rank and the degree of the vertices of the hypergraph play important roles in the reductions provided. Bounding the degree of vertices, in conjunction with the rank, would enable reductions to be restricted to bounded-degree graphs. This result is particularly relevant within the parameterized complexity framework, as bounded-degree graphs have locally bounded treewidth. Bonnet et al. [BGL+17] showed that positional games parameterized by the number of moves are \FPT on locally bounded treewidth graphs, which indicates that tractability really requires the number of moves to be treated as a parameter. In this paper, we prove in Section 2 that 2-\langQBF can be solved in quadratic time, and that, unlike the unquantified version, 3-\langQBF-3 is already \PSPACE-complete. In Section 3, we explore the computational complexity of positional games restricted to bounded-degree hypergraphs. As a direct consequence of the \PSPACE-hardness of 3-\langQBF-3, we prove that Avoider-Enforcer and Client-Waiter games are \PSPACE-complete. However, since the reduction for Maker-Breaker games generates vertices of unbounded degree, we prove separately that Maker-Breaker games are \PSPACE-complete when restricted to hypergraphs of rank 12 and maximum degree 5. As a consequence, we answer an open question from Bagan et.al. [BDG+24], proving that the Maker-Breaker Domination game is \PSPACE-complete, even restricted to bounded degree graphs, and we prove that Maker-Maker games are also \PSPACE-complete restricted to bounded degree hypergraphs."
https://arxiv.org/html/2411.09806v1,On the existence of factors intersecting sets of cycles in regular graphs,"A recent result by Kardoš, Máčajová and Zerafa [J. Comb. Theory, Ser. B. 160 (2023) 1–14] related to the famous Berge-Fulkerson conjecture implies that given an arbitrary set of odd pairwise edge-disjoint cycles, say \mathcal{O}, in a bridgeless cubic graph, there exists a 1-factor intersecting all cycles in \mathcal{O} in at least one edge. This remarkable result opens up natural generalizations in the case of an r-regular graph G and a t-factor F, with r and t being positive integers. In this paper, we start the study of this problem by proving necessary and sufficient conditions on G, t and r to assure the existence of a suitable F for any possible choice of the set \mathcal{O}. First of all, we show that G needs to be 2-connected. Under this additional assumption, we highlight how the ratio \frac{t}{r} seems to play a crucial role in assuring the existence of a t-factor F with the required properties by proving that \frac{t}{r}\geq\frac{1}{3} is a further necessary condition. We suspect that this condition is also sufficient, and we confirm it in the case \frac{t}{r}=\frac{1}{3}, generalizing the case t=1 and r=3 proved by Kardoš, Máčajová, Zerafa, and in the case \frac{t}{r}=\frac{1}{2} with t even. Finally, we provide further results in the case of cycles of arbitrary length.","We consider finite graphs without loops that may have parallel edges. The main question we study in this paper can be briefly stated as follows: ""Under what conditions, given an r-regular graph G and an arbitrary set of pairwise edge-disjoint cycles of G, can we guarantee the existence of a t-factor of G that intersects each of these cycles in at least one edge?"" where a cycle is a connected 2-regular subgraph of G. First of all, we explain how a particular instance of this problem, solved in [7], is related to the following long-standing conjecture which dates back to 1971. Conjecture 1.1 (Fulkerson [4]). Every bridgeless cubic graph has six perfect matchings such that each edge belongs to exactly two of them. This conjecture was first proposed by Berge, but Fulkerson [4] was the one who put it into print (cf. [12]). Thus, Conjecture 1.1 is also called the Berge-Fulkerson Conjecture. It is proved in [8] that Conjecture 1.1 reduces to non-3-edge-colorable cubic graphs which are cyclically 5-edge-connected. Moreover, with the help of a computer, the conjecture was verified for cubic graphs of order at most 36 [2], which can be seen as an indication that the conjecture might be true in general. Nevertheless, a general solution seems to be far away. Hence, in order to make further progress, weaker statements moved into focus. The following three conjectures are all implied by the Berge-Fulkerson Conjecture and decrease in their strength, i.e. each conjecture is implied by the previous. Conjecture 1.2 (Fan, Raspaud [3]). Every bridgeless cubic graph has three perfect matchings with an empty intersection. Conjecture 1.3 (Máčajová, Škoviera [9], see also [5]). Every bridgeless cubic graph has two perfect matchings such that their intersection does not contain an edge-cut of odd cardinality. Conjecture 1.4 (Mazzuoccolo [10]). Every bridgeless cubic graph has two perfect matchings M_{1},M_{2} such that G-(M_{1}\cup M_{2}) is bipartite. Very recently, the weakest of these conjectures (i.e. Conjecture 1.4) was proved by Kardoš, Máčajová and Zerafa [7]. An equivalent formulation of their main result is the following theorem. Theorem 1.5 (Kardoš, Máčajová, Zerafa [7]). Let G be a 2-connected cubic graph. Let \mathcal{O} be a set of pairwise edge-disjoint odd cycles of G and let e\in E(G). Then, there exists a 1-factor F of G such that e\in E(F) and E(F)\cap E(O)\neq\emptyset for every O\in\mathcal{O}. It is natural to ask whether this result can be extended to graphs of higher degree, thus returning to the initial question we proposed. In the present paper, we consider the following instance of our general question, where, in analogy to Theorem 1.5, we limit our attention to cycles of odd length (note that we do not necessarily require to prescribe an edge of F). Results concerning arbitrary cycles can be found in Section 6. Question 1.6. Let r,t be integers with 1\leq t\leq r-2. Is it true that for every 2-connected r-regular graph G and every set \mathcal{O} of pairwise edge-disjoint odd cycles of G there is a t-factor F of G such that E(F)\cap E(O)\neq\emptyset for every O\in\mathcal{O}? Note that there are combinations of r and t such that there exist r-regular graphs that do not have a t-factor. In this case, Question 1.6 turns out to be trivial. Hence, from now on we focus on r-regular graphs admitting a t-factor (see [1] for necessary and sufficient conditions for the existence of a t-factor in an r-regular graph). Furthermore, in Section 2 we prove that the assumption that G has no cut-vertices is necessary to assure a positive answer. Our main results suggest that the key factor in the study of Question 1.6 is the ratio \frac{t}{r}. In particular, we show in Section 3 that a necessary condition to have a positive answer is \frac{t}{r}\geq\frac{1}{3}. Moreover, in Section 4 and Section 5 we show that such a condition is also sufficient when \frac{t}{r}=\frac{1}{3}, and when \frac{t}{r}=\frac{1}{2}, where t is even, respectively. We leave it as an intriguing open problem to prove or disprove that the necessary condition \frac{t}{r}\geq\frac{1}{3} is also sufficient in general. The following two theorems are our main results. Theorem 1.7. Let t\geq 1 be an integer and let G be a 2-connected 3t-regular graph. Let \mathcal{O} be a set of pairwise edge-disjoint odd cycles of G and let e\in E(G). Then, there exists a t-factor F of G such that e\in E(F) and E(F)\cap E(O) is a non-empty matching of G for every O\in\mathcal{O}. Theorem 1.8. Let t\geq 2 be an even integer and let G be a 2-connected 2t-regular graph. Let \mathcal{O} be a set of pairwise edge-disjoint odd cycles of G. Then, there exists a t-factor F of G such that E(O)\cap E(F)\neq\emptyset and E(O)\cap(E(G)\setminus E(F))\neq\emptyset for every O\in\mathcal{O}. Note that Theorem 1.5 is a special case of Theorem 1.7 for t=1. Finally, in Section 6, we present some further results for a set \mathcal{O} containing arbitrary cycles (not necessarily odd) and in Section 7 we conclude the paper with some open problems."
https://arxiv.org/html/2411.08994v1,"A characterization of positive spanning sets with ties
to
strongly edge-connected digraphs","Positive spanning sets (PSSs) are families of vectors that span a given linear space through non-negative linear combinations. Despite certain classes of PSSs being well understood, a complete characterization of PSSs remains elusive. In this paper, we explore a relatively understudied relationship between positive spanning sets and strongly edge-connected digraphs, in that the former can be viewed as a generalization of the latter. We leverage this connection to define a decomposition structure for positive spanning sets inspired by the ear decomposition from digraph theory.","Gaussian elimination is a fundamental technique in linear algebra, that can be used to assess whether a given matrix is linearly spanning, in the sense that its columns span the entire space through linear combinations [17, Page 6]. Similarly, in graph theory, one can determine a spanning tree of a graph using linear algebra techniques, while efficient implementations of graph algorithms can be obtained by leveraging sparse linear algebra [10]. Positive spanning sets, or PSSs, are matrices such that the columns span the entire space through nonnegative linear combinations [7]. These matrices are instrumental to direct-search algorithms, a class of continuous optimization algorithms that proceed by exploring the variable space through suitably chosen directions [16, 2]. When those directions are chosen from positive spanning sets, convergence can be guaranteed at a rate that heavily depends on the properties of the PSSs at hand [16, 9]. In this setting, using a direction corresponds to evaluating an expensive function, and thus optimizers typically rely on inclusion-wise minimal positive spanning sets, or positive bases [21, 22]. Although positive bases have already been fully described [24], generic description are often impractical to generate positive bases in practice. As a result, optimizers have focused on characterizing special positive bases for which simpler characterization can be obtained [13, 14]. Perhaps surprisingly, a connection between positive spanning sets and strongly edge-connected digraphs was spotted early in the PSS literature [19], but to the authors’ knowledge this connection has not been exploited further111More precisely, Marcus [19] claims that this connection will be investigated in a future work that we believe has never been released.. Meanwhile, numerous results have been established for strongly connected digraphs [3, 15], with minimal strongly edge-connected digraphs attracting recent interest [1, 6, 11, 12]. Although such digraphs appear connected to positive bases through the concept of minimality, a formal link between those objects has yet be described. In this paper, we provide certificates for the positive spanning property based on digraph theory. To this end, we show that PSSs can be seen as generalizing the concept of strongly edge-connected digraphs. We then leverage this connection to obtain a novel characterization of such matrices based on the ear decomposition of digraphs [25]. The remainder of this paper is organized as follows. In Section 2, we review key results from digraph theory. We then discuss positive spanning sets and draw connections with strongly edge-connected digraphs in Section 3. Our main results, that generalize the ear decomposition to positive spanning sets, are derived in Section 4. Notations Throughout this paper, we work in the Euclidean space \mathbb{R}^{n} with n\geq 2, or a linear subspace thereof, denoted by \mathbb{L}\subset\mathbb{R}^{n}. The dimension of such a subspace will always be assumed to be at least 1. The set of real matrices with n rows and m columns will be denoted as \mathbb{R}^{n\times m}. Those dimensions will always be assumed to be at least 1. Bold lowercase letters (e.g. \mathbf{v},\mathbf{a}) will be used to designate vectors and arcs in directed graphs, while bold uppercase letters (e.g. \mathbf{D}) will denote matrices. The notations \mathbf{0}_{n} and \mathbf{1}_{n} will respectively be used to designate the null vector and the all-ones vector in \mathbb{R}^{n}, while \mathbf{I_{n}}=\begin{bmatrix}\mathbf{e_{1}}&\dots&\mathbf{e_{n}}\end{bmatrix} will denote the identity matrix in \mathbb{R}^{n\times n}. Given a matrix \mathbf{D}\in\mathbb{R}^{n\times m}, its set of columns will be denoted \operatorname{col}(\mathbf{D}) while its linear span (i.e. the set of linear combinations of its columns) will be denoted by \operatorname{span}(\mathbf{D}). The matrix whose entries are the signs of those of \mathbf{M} will be noted sgn(\mathbf{M}). Calligraphic letters such as \mathcal{D} and \mathcal{S} will be used for finite families of vectors or of indices. The notation [\![1,m]\!] denotes the set of all integer values between 1 and m. Finally, for a digraph G=(V,A) the notations (u,v) and u-v will respectively designate an arc and an oriented path in A going from u to v."
https://arxiv.org/html/2411.08179v1,"On sampling two spin models using 
the local connective constant","This work establishes novel, optimum mixing bounds for the Glauber dynamics on the Hard-core and Ising models using the powerful Spectral Independence method from [Anari, Liu, Oveis-Gharan: FOCS 2020]. These bounds are expressed in terms of the local connective constant of the underlying graph G. This is a notion of effective degree of G on a local scale.Our results have some interesting consequences for bounded degree graphs:They include the max-degree bounds as a special case,They improve on the running time of the FPTAS considered in [Sinclair, Srivastava, Štefankonič and Yin: PTRF 2017],They allow us to obtain mixing bounds in terms of the spectral radius of the adjacency matrix and improve on results in [Hayes: FOCS 2006],They also allow us to refine the celebrated connection between the hardness of approximate counting and the phase transitions from statistical physics.We obtain our mixing bounds by utilising the k-non-backtracking matrix G,k. This is a very interesting, alas technically intricate, object to work with. We upper bound the spectral radius of the pairwise influence matrix {\mathcal{I}}^{\Lambda,\tau}_{G} by means of the 2-norm of G,k. To our knowledge, obtaining mixing bound using G,k has not been considered before in the literature.","This paper focuses on approximate sampling-counting combinatorial structures specified with respect to an underlying graph G=(V,E). Unless otherwise specified, graph G is simple, connected and finite. We study Gibbs distributions \mu on the set of configurations \{\pm 1\}^{V}. Typically, we parametrise \mu by using the numbers \beta\geq 0 and \gamma,\lambda>0, while each configuration \sigma\in\{\pm 1\}^{V} gets probability mass \displaystyle\mu(\sigma) \displaystyle=\frac{1}{\mathcal{Z}}\times\lambda^{\#\textrm{assignments ``1"" % in $\sigma$ }}\times\beta^{\#\textrm{edges with both ends ``1"" in $\sigma$ }}% \times\gamma^{\#\textrm{edges with both ends ``-1"" in $\sigma$}}\enspace, (1.1) where \mathcal{Z}=\mathcal{Z}(G,\beta,\gamma,\lambda) is the normalising quantity called the partition function. A natural counting problem to consider in this setting is to compute the partition function \mathcal{Z}. In many cases, this is a computationally hard problem, e.g., see [41, 19], while the focus is on efficiently obtaining good approximations of \mathcal{Z}. For the distributions we consider here, the aim is to get a Fully Polynomial Randomized Time Approximation Scheme (FPRAS) for the corresponding \mathcal{Z}. An FPRAS computes with probability 1-\updelta an estimation which is within a factor (1\pm\upepsilon) from \mathcal{Z}, in time which is polynomial in the size of the graph n, \log\frac{1}{\upepsilon} and \log\frac{1}{\updelta}. We consider two natural and well-studied Gibbs distributions, the Hard-core model and the Ising model. Using the above formalism, for the Ising model we have \beta=\gamma. Then, \beta corresponds to the inverse temperature and \lambda to the external field. Similarly, the Hard-core model corresponds to having \beta=0,\gamma=1. In this case, it is only the independent sets of G that get non-zero probability mass, while for each independent set \sigma, we have \mu(\sigma)=\frac{\lambda^{|\sigma|}}{\mathcal{Z}}, where |\sigma| is the cardinality of \sigma. The parameter \lambda of the Hard-core model is usually called fugacity. We use the well-known Glauber dynamics for approximate sampling from the aforementioned distributions. Given a Gibbs distribution \mu, the corresponding (single-site) Glauber dynamics is a discrete-time Markov chain with state space the support of \mu. The chain starts from an arbitrary configuration. At each time step, it chooses a uniformly random vertex v and updates its configuration according to the Gibbs marginal at v, conditional on the configuration of the neighbours of v. In our setting, the chain is ergodic and the challenge is to show that it mixes fast for a certain region of the parameters. We use the notion of mixing time as a measure of the speed of convergence to the equilibrium. It is standard to show, e.g., see [26, 38], that a polynomial in n mixing time implies an FPRAS for the corresponding partition function. We say that Glauber dynamics exhibits optimum mixing when the mixing time attains its (asymptotically) minimum value, which is O(n\log n); see Hayes and Sinclair [23]. In this work, we establish novel, optimum mixing bounds for Glauber dynamics on the Hard-core and Ising models, which are expressed in terms of the local connective constant of the underlying graph G. Informally, we say that the radius-k connective constant of graph G is k if for every vertex v, the number of self-avoiding walks of length k that emanate from this vertex is \leq\left({}_{k}\right)^{k} (also see Definition 5.2). Intuitively, the parameter k describes the “effective degree” of graph G on a local scale. The radii for the connective constant we consider here can be large but remain bounded. The quantity k, typically for large k, arises naturally in many areas of study, such as in Theoretical Computer Science, Mathematical Physics and Statistical Physics, e.g., in sampling problems with spin-systems, in the study of phase transitions, in stochastic processes on graphs like percolation, polymers, etc., see [5, 30]. The importance the community has put on the study of the connective constant also reflects in the recent Fields medal awards. Typically, the rapid mixing bounds we have for Glauber dynamics are expressed in terms of the maximum degree {\Delta} of the underlying graph G. The papers of Anari, Liu & Oveis-Gharan in [3], and Chen, Liu and Vigoda in [10, 11] obtain optimum mixing bounds for the Hard-core and the Ising models in terms of the maximum degree {\Delta}, where {\Delta} is a bounded number. Impressively, these works get bounds that match the hardness ones of Sly in [39], Sly and Sun in [40] and Galanis, Štefankovič and Vigoda in [18]. There have been further improvements on [3, 10, 11], the most notable being their extension to graphs with unbounded maximum degrees [9, 8]. It is a folklore conjecture that the rapid mixing bounds for Glauber dynamics are related to the (local) connective constant k. One could consider the mixing bounds with respect to maximum degree as being obtained considering the worst-case graph instances with regard to the relation between {\Delta} and k, i.e., {}_{k}={\Delta}-1. In many cases, these bounds are too pessimistic, hence the conjecture. Our endeavour here is to put the above considerations on a firm basis. The problem of estimating the partition function with the (local) connective constant has been studied for the Hard-core model before, using a deterministic FPTAS that relies on Weitz’s approach [43]. The best bounds in terms of the fugacity \lambda we have, come from the influential work of Sinclair, Srivastava, Štefankonič and Yin in [36], which builds on [37]. We obtain similar results to those in [36], improving on the running time of the algorithm. The deterministic counting algorithms that rely on [43] are polynomial time. However, they have a heavy dependence on the degree of the graph. It is not explicitly stated in [36], but it is conceivable that for the interesting range of values for \lambda, the running time of their algorithm is at least n^{C\log({}_{k})} for a large constant C>0. To compare, our approach here gives a O^{*}(n^{2}) time FPRAS for the partition function, where O^{*}(\cdot) omits the poly-logarithmic terms. See further comparisons between our work and [36] below Theorem 1.1. Interestingly, [36] allows for connective constant radius k=O(\log n), i.e., rather than \Theta(1) we have here. This helps to obtain an FPTAS for the sparse random graphs G(n,d/n), where the expected degree d=\Theta(1). Obtaining an FPRAS for G(n,d/n) is a well-studied problem; by now, we have mixing bounds which are considered to be optimal and outperform those in [36], e.g., see the work of Efthymiou and Feng in [15] (also see further results about G(n,d/n) in Section 1.1). On a first account one might expect that a high discrepancy between k and {\Delta} implies that the degrees in the graph vary significantly. E.g., the underlying graph has a relatively small number of vertices with degree {\Delta}, while most of the other vertices have degrees \ll{\Delta}. It turns out that this is only a part of the truth. There are many natural families of regular graphs for which k is smaller than {\Delta}. The lattices we consider later in our discussion are such examples. In general, having many structures like, e.g., bounded-length cycles, reduces the local connective constant significantly, even when the graph is regular. We start the presentation of our results by considering the Hard-core model. We present the equally important results for the Ising model in Section 1.1. For z>1, we let function {\lambda_{c}}(z)=\frac{z^{z}}{(z-1)^{(z+1)}}. It is a well-known result from Kelly in [27] that the Gibbs uniqueness region of the Hard-core model on the infinite r-ary tree, for r\geq 2, corresponds to having \displaystyle\lambda<{\lambda_{c}}(r)\enspace. (1.2) Here we prove the following result for the Hard-core model. Theorem 1.1. For any \varepsilon\in(0,1), {\Delta}>1, k\geq 1 and {}_{k}>1 consider graph G=(V,E) of maximum degree {\Delta} such that the radius-k connective constant is k. Also, let \mu be the Hard-core model on G with fugacity \lambda\leq(1-\varepsilon){\lambda_{c}}({}_{k}). There is a constant C={C({\Delta},{}_{k},\varepsilon)} such that the mixing time of Glauber dynamics on \mu is at most Cn\log n. The maximum degree bounds that apply in our setting are those from [3, 11]. Specifically, for a graph G of maximum degree {\Delta}, they imply optimum mixing for Glauber dynamics on the Hard-core model for \lambda\leq(1-\varepsilon){\lambda_{c}}({\Delta}-1). Hence, Theorem 1.1 improves on the max-degree bound when {}_{k}<{\Delta}-1. When k gets its maximum value {\Delta}-1, the bound in Theorem 1.1 matches the max-degree one. It is natural to compare Theorem 1.1 with results in the literature where the mixing bound is better than the maximum degree one. Most of the examples we consider here are motivated by applications in mathematical physics. Admittedly, it is helpful that there is extensive literature about the (local) connective constants in this area that we can use. Table 1 summarises our applications of Theorem 1.1 for the Hard-core model on certain well-known graphs such as the triangular lattice \mathbb{T}, the hexagonal lattice \mathbb{H}, the octagonal lattice \mathbb{O} and the square lattice \mathbb{Z}^{r}, for dimensions r=1,2,\ldots,5. There is a plethora of other graphs that we do not include in the table due to space limitations. Prior to our work here, the only non-standard results for the Hard-core model that we know of were limited to \mathbb{Z}^{2}. That is, the only rapid mixing bounds that applied to the lattices in the table, apart from \mathbb{Z}^{2}, were the maximum degree ones. The best mixing bounds for the Hard-core on \mathbb{Z}^{2} are obtained by Vera, Vigoda and Yang in [42], which builds on the influential work in [34], implying \lambda<2.48. The approaches in [42, 34] rely on numerically intensive methods tailored to \mathbb{Z}^{2}. Furthermore, there is an FPTAS for \mathbb{Z}^{2} in [36], which gets an even better bound than [42], i.e., \lambda<2.538. Our understanding is that both [42, 36] are trying to optimise their derivations specifically to this lattice. Here we do not explore this directions, even though it is very interesting. Our bounds with regard to the other lattices in the table are slightly off compared to those in [36], i.e., typically in the second decimal digit. This only has to do with the accuracy of the estimate for k we are using. Sharper results can be obtained by considering larger -but bounded- radii for the local connective constant. We utilise the Spectral Independence method and results from [3, 10, 11] to establish Theorem 1.1. With this approach, the focus is on the pairwise influence matrix, denoted as {\mathcal{I}}^{\Lambda,\tau}_{G}. This is a matrix over the vertices of G such that each entry {\mathcal{I}}^{\Lambda,\tau}_{G}(w,u) expresses the effect of the configuration at vertex w on the Gibbs marginal at vertex u. The influence matrix is a central concept for the Spectral Independence. Establishing fast mixing amounts to showing that the maximum eigenvalue of {\mathcal{I}}^{\Lambda,\tau}_{G} is O(1). Lattice max Degree {\Delta} connective const. k Radius k Hard-core Model \lambda({}_{k}) Ising Model \beta \mathbb{T} 6 4.3038 [25] 40 0.9445 (0.6229, 1.6053) \mathbb{H} 3 1.8846 [2] 48 4.7026 (0.3066, 3.2609) \mathbb{Z}^{2} 4 2.7054 [35] 158 2.0433 (0.4602, 2.1727) \mathbb{Z}^{3} 6 4.7520 [35] 108 0.8191 (0.6522, 1.5330) \mathbb{Z}^{4} 8 6.8179 [35] 648 0.5068 (0.7441, 1.3437) \mathbb{Z}^{5} 10 8.8608 [35] 698 0.3675 (0.7971, 1.2544) \mathbb{O} 8 4.7753 [2] 19 0.8135 (0.6536, 1.5297) Table 1. Some results for lattices Here, we obtain bounds on the maximum eigenvalue of {\mathcal{I}}^{\Lambda,\tau}_{G} by utilising the k-non-backtracking matrix G,k. This is a very interesting, alas technically challenging, object to work with. G,k is a 0/1 matrix, indexed by the self-avoiding walks of length k in G. Specifically, {}_{G,k}(P,Q)=1 if and only if walk Q extends walk P by one vertex without backtracking111E.g., for self-avoiding walks P=x_{0},\ldots x_{k} and Q=z_{0},\ldots z_{k}, we have {}_{G,k}(P,Q)=1 if and only if x_{i}=z_{i-1}, for i=1,\ldots,k, while z_{k}\neq x_{0}.. Matrix G,k is a generalisation of the well-known Hashimoto non-backtracking matrix introduced in mathematical physics [20] but has found a lot of applications in Computer Science, e.g., see [1, 6, 12]. One of the main technical results here is to show that the spectral radius of {\mathcal{I}}^{\Lambda,\tau}_{G} for the Hard-core model, can be bounded in terms of G,k, for any k\geq 1, as shown below: \displaystyle{\rho}\left({\mathcal{I}}^{\Lambda,\tau}_{G}\right) \displaystyle\leq C\cdot\sum\nolimits_{\ell\geq 0}\left(\delta^{\ell}\cdot\|% \left({}_{G,k}\right)^{\ell}\|_{2}\right)^{1/s}\enspace, (1.3) where the quantities C,s,\delta>0 depend on the parameters of the problem. We utilise the above inequality to obtain the following result for the Hard-core model and G,k. Theorem 1.2. For \varepsilon\in(0,1), for \uprho>1 and integers k,N>0, {\Delta}>1, consider graph G=(V,E) of maximum degree {\Delta} such that \|({}_{G,k})^{N}\|^{1/N}_{2}=\uprho. Also, let \mu be the Hard-core model on G with fugacity \lambda\leq(1-\varepsilon){\lambda_{c}}(\uprho). There is a constant C={C({\Delta},\uprho,\varepsilon)} such that the mixing time of Glauber dynamics on \mu is at most Cn\log n. To our knowledge, obtaining mixing results for Glauber dynamics in terms of G,k has never been considered before in the literature. Interestingly, \|({}_{G,k})^{N}\|^{1/N}_{2} varies with N,k. For smaller values of N,k, matrix {}^{N}_{G,k} tends to contain information related to the degree sequence of the graph, whereas, for larger values, it carries more structural information about G. Also, it is an easy exercise to show that \|({}_{G,k})^{N}\|^{1/N}_{2}\leq{\Delta}-1. In light of Theorem 1.2, the results with the local connective constant k follow by (roughly) showing that \|({}_{G,k})^{N}\|^{1/N}_{2}\leq{}_{k}. Furthermore, we explore the direction of using backtracking walks to establish rapid mixing bounds, i.e., rather than the k-non-backtracking ones. This approach naturally formalises in terms of the adjacency matrix G. The bounds we obtain are expressed in terms of \|{}_{G}\|_{2}, which also corresponds to the spectral radius. G is a 0/1 matrix indexed by the vertices of G such that for any u and w, we have \displaystyle{}_{G}(u,w)=\mathds{1}\{\textrm{$u,w$ are adjacent in $G$}\}\enspace. Utilising matrix G and its norms to obtain rapid mixing bounds has been considered before. The seminal work of Hayes in [21] first introduces this approach by considering \|{}_{G}\|_{2}. Subsequently, further extensions were introduced in [13, 22]. Compared to G,k, matrix G is very well-behaved, e.g., it is symmetric, irreducible, etc. This nice behaviour of G gives rise to a more elegant analysis than what we have with G,k. A result we obtain in this setting is that the spectral radius of {\mathcal{I}}^{\Lambda,\tau}_{G} for the Hard-core model, can be bounded it terms of G, as shown below: \displaystyle{\rho}\left({\mathcal{I}}^{\Lambda,\tau}_{G}\right) \displaystyle\leq C\cdot\sum\nolimits_{\ell\geq 0}\left(\delta^{\ell}\cdot\|% \left({}_{G}\right)^{\ell}\|_{2}\right)^{1/s}\enspace, (1.4) where the quantities C,s,\delta>0 depend on the parameters of the problem. We utilise the above inequality to obtain the following result with regard to the Hard-core model and G. Theorem 1.3. For any \varepsilon\in(0,1), for {\Delta},{\varrho}>1, consider graph G=(V,E) of maximum degree {\Delta} such that \|{}_{G}\|_{2}={\varrho}. Also, let \mu be the Hard-core model on G with fugacity \lambda\leq(1-\varepsilon){\lambda_{c}}({\varrho}). There is a constant C={C({\Delta},{\varrho},\varepsilon)} such that the mixing time of Glauber dynamics on \mu is at most Cn\log n. The maximum degree bounds that apply in our setting are those from [3, 11]. Theorem 1.3 improves on these bounds when {\rho}({}_{G})<{\Delta}-1. Table 2 shows some notable families of graphs where Theorem 1.3 gives significant improvements over the maximum degree bounds. Theorem 1.3 also improves on the results in [21]. Note that [21] establishes optimum mixing for the Hard-core model with fugacity \lambda<\frac{1}{{\varrho}}. The bound we obtain here is, roughly, \lambda<\frac{e}{{\varrho}}. That is, we get an improvement of factor e for the bounded degree graphs. Unlike our approach that utilises the Spectral Independence method, the earlier results in [13, 21, 22] rely on the path coupling technique by Bubley and Dyer in [7]. Graph with max Degree {\Delta} Spectral radius of the Adjacency matrix Planar \sqrt{8({\Delta}-2)}+2\sqrt{3} r-degenarate 2\sqrt{r({\Delta}-r)} Euler genus g \sqrt{8({\Delta}-2g+4)}+2g+4 4 Table 2. Bounds on the spectral radius of the Adjacency matrix, from [14, 21] Hardness & Phase Transitions: In a different direction, our results cast a light on the celebrated connection between the hardness of approximate counting and the onset of the Gibbs tree non-uniqueness from statistical physics. We use Theorem 1.2 to express this connection in terms of \|({}_{G,k})^{N}\|^{1/N}_{2}. Suppose that \mathcal{Z}(G,\lambda) is the partition function for the Hard-core model on graph G. The results in [3, 11] imply that there is an FPRAS for estimating \mathcal{Z}(G,\lambda) for any graph G of maximum degree {\Delta} for all \lambda<{\lambda_{c}}({\Delta}-1). On the other hand, the results [39, 40, 18] imply that, unless {\rm RP}={\rm NP}, there is no FPRAS for \mathcal{Z}(G,\lambda) for graphs of maximum degree {\Delta} for all \lambda>{\lambda_{c}}({\Delta}-1). The latter results also imply that we cannot approximate in polynomial time \mathcal{Z}(G,\lambda) within a factor 2^{\gamma n}, for an appropriate constant \gamma>0. In light of Theorem 1.2, the above dichotomy fails to capture that there are instances of the problem for which we have an FPRAS even though \lambda>{\lambda_{c}}({\Delta}-1). These are the instances for which \uprho<{\Delta}-1, where \uprho=\|({}_{G,k})^{N}\|^{1/N}_{2}. In that respect, it is more accurate to state the dichotomy with respect to {\lambda_{c}}\left(\uprho\right). Specifically, Theorem 1.2 and [39, 40, 18] imply the following result. Theorem 1.4. Unless {\rm NP}={\rm RP}, for \varepsilon\in(0,1), {\Delta}\geq 3, N,k\geq 1 and \uprho>1, the following is true: For any \lambda>(1+\varepsilon){\lambda_{c}}(\lceil\uprho\rceil), there is no FPRAS for estimating the partition function of the Hard-core model for graphs G of maximum degree at most {\Delta} such that \|({}_{G,k})^{N}\|^{1/N}_{2}=\uprho. For any \lambda<(1-\varepsilon){\lambda_{c}}(\uprho), there is an FPRAS for estimating the partition function of the Hard-core model for graphs G of maximum degree at most {\Delta} such that \|({}_{G,k})^{N}\|^{1/N}_{2}=\uprho. The results in [39, 40] we utilise to show Theorem 1.4, require the argument of {\lambda_{c}}(\cdot) to be an integer. For this reason, we use {\lambda_{c}}(\lceil\uprho\rceil) for the lower bound. Using Theorem 1.1, we can also express the above dichotomy by using the radius-k connective constant k instead of \|({}_{G,k})^{N}\|^{1/N}_{2}. We do not state this result here since it is incremental to Theorem 1.4. 1.1. Results with the Ising model For integer r\geq 2, it is well-known that the Ising model on the infinite r-ary tree, with external field \lambda>0 and inverse temperature \beta\geq 0, exhibits uniqueness for \displaystyle\frac{r-1}{r+1}<\beta<\frac{r+1}{r-1}\enspace. Recall that when \beta>1, the Ising model is called ferromagnetic, while for \beta<1, it is called antiferromagnetic. For z>1 and \delta\in(0,1), we let the interval \displaystyle{\mathbb{U}_{\rm Ising}}(z,\delta)=\textstyle\left[\frac{z-1+% \delta}{z+1-\delta},\frac{z+1-\delta}{z-1+\delta}\right]\enspace. (1.5) As far as the Ising model is concerned, we obtain the following result. Theorem 1.5. For \varepsilon\in(0,1), {\Delta}>1, k\geq 1, {}_{k}>1 and \lambda>0 consider graph G=(V,E) of maximum degree {\Delta} such that the radius-k connective constant is k. Let \mu be the Ising model on G with external field \lambda and inverse temperature \beta\in{\mathbb{U}_{\rm Ising}}({}_{k},\varepsilon). There is a constant C={C({\Delta},{}_{k},\varepsilon)} such that the mixing time of Glauber dynamics on \mu is at most Cn\log n. The maximum degree bounds that apply in our setting are those from [3, 11]. Specifically, for a graph G of maximum degree {\Delta}, they imply optimum mixing for Glauber dynamics on the Ising model with inverse temperature \beta\in{\mathbb{U}_{\rm Ising}}({\Delta}-1,\delta) and external field \lambda>0. Similarly to Theorem 1.1, with Theorem 1.5, we improve on the max-degree bound when {}_{k}<{\Delta}-1. When k gets its maximum value {\Delta}-1, the bound in Theorem 1.5 matches the max-degree one. As mentioned earlier, it is natural to compare Theorem 1.5 with results in the literature where the mixing bound is better than the maximum degree one. Table 1 summarises our applications of Theorem 1.5 for the Ising model on the triangular lattice \mathbb{T}, the hexagonal lattice \mathbb{H}, the octagonal lattice \mathbb{O} and the square lattice \mathbb{Z}^{r}, for dimensions r=1,2,\ldots,5. As mentioned before, the choice of these lattices is only indicative; there many other structures that we do not consider here due to space limitations. Perhaps one of the best-understood cases in the literature is the Ising model on the square lattice \mathbb{Z}^{r}, for r>0. The best bounds on the mixing time for Glauber dynamics have been known since the 1990s from the influential work of Martinelli and Olivieri in [31, 32]. Theorem 1.5 does not improve on these results. However, the rapid mixing bounds we obtain for the remaining lattices are new. Prior to our results here, the only known bounds were with respect to the maximum degree {\Delta}. For completeness we state the rapid mixing bounds for the Ising model on G(n,d/n). For the ferromagnetic Ising model, see the work of Mossel and Sly in [33]. The bounds for the antiferromagnetic Ising are implied in the work of Efthymiou, Hayes, Štefankovič and Vigoda in [16], while explicit bounds appear in [17]. Our results here do not have any implication for G(n,d/n) because we consider bounded degrees. For the Ising model and G,k, we have the following result, which is analogous to Theorem 1.2. Theorem 1.6. For \varepsilon\in(0,1), \lambda>0, \uprho>1 and integers N>0, {\Delta}>1, consider graph G=(V,E) of maximum degree {\Delta} such that \|({}_{G,k})^{N}\|^{1/N}_{2}=\uprho. Let \mu be the Ising model on G with external field \lambda and inverse temperature \beta\in{\mathbb{U}_{\rm Ising}}({\uprho},\varepsilon). There is a constant C={C({\Delta},{\uprho},\varepsilon)} such that the mixing time of Glauber dynamics on \mu is at most Cn\log n. As far as the Ising model and the adjacency matrix are concerned, we obtain the following result. Theorem 1.7. For \varepsilon\in(0,1), \lambda>0, {\varrho}>1 and integer {\Delta}>1, consider graph G=(V,E) of maximum degree {\Delta} such that \|{}_{G}\|_{2}={\varrho}. Let \mu be the Ising model on G with external field \lambda and inverse temperature \beta\in{\mathbb{U}_{\rm Ising}}({\varrho},\varepsilon). There is a constant C={C({\Delta},{\varrho},\varepsilon)} such that the mixing time of Glauber dynamics on \mu is at most Cn\log n. Theorem 1.7 matches Hayes’ results in [21] for the Ising model. We only present it for the sake of completeness. We get results similar to those in Theorem 1.4 for the antiferromagnetic Ising model. Note that there is no hardness region for the ferromagnetic Ising model [19]. Specifically, using Theorem 1.5 and [39, 40, 18] for the antiferromagnetic Ising model, we have the following result. Theorem 1.8. Unless {\rm NP}={\rm RP}, for \varepsilon\in(0,1), {\Delta}\geq 3, N,k\geq 1 and \uprho>1 the following is true: For any 0<\beta\leq\frac{\uprho-1+\varepsilon}{\uprho+1-\varepsilon} and \lambda such that we are not in the tree uniqueness region, there is no FPRAS for estimating the partition function of the antiferromagnetic Ising model for graphs G of maximum degree at most {\Delta} such that \|({}_{G,k})^{N}\|^{1/N}_{2}=\uprho. For any \frac{\uprho-1+\varepsilon}{\uprho+1-\varepsilon}\leq\beta\leq 1 and any \lambda>0, there is an FPRAS for estimating the partition function of the antiferromagnetic Ising model for graphs G of maximum degree at most {\Delta} such that \|({}_{G,k})^{N}\|^{1/N}_{2}=\uprho. As noted before, using Theorem 1.5, we can also express the above dichotomy by using the radius-k connective constant k instead of \|({}_{G,k})^{N}\|^{1/N}_{2}. We do not state this result here because it is incremental to Theorem 1.8."
https://arxiv.org/html/2411.08839v1,Sparser Abelian High Dimensional Expanders,"We present two new explicit constructions of Cayley high dimensional expanders (HDXs) over the abelian group \mathbb{F}_{2}^{n}. Our expansion proofs use only linear algebra and combinatorial arguments.The first construction gives local spectral HDXs of any constant dimension and subpolynomial degree \exp(n^{\varepsilon}) for every \varepsilon>0, improving on a construction by Golowich [Golowich2023] which achieves \varepsilon=1/2. [Golowich2023] derives these HDXs by sparsifying the complete Grassmann poset of subspaces. The novelty in our construction is the ability to sparsify any expanding Grassmannian posets, leading to iterated sparsification and much smaller degrees. The sparse Grassmannian (which is of independent interest in the theory of HDXs) serves as the generating set of the Cayley graph.Our second construction gives a 2-dimensional HDXs of any polynomial degree \exp(\varepsilon n) for any constant \varepsilon>0, which is simultaneously a spectral expander and a coboundary expander.111A notion of combinatorial expansion which in high dimension is not equivalent to the spectral one. To the best of our knowledge, this is the first such non-trivial construction. We name it the Johnson complex, as it is derived from the classical Johnson scheme, whose vertices serve as the generating set of this Cayley graph. This construction may be viewed as a derandomization of the recent random geometric complexes of [LiuMSY2023]. Establishing coboundary expansion through Gromov’s “cone method” and the associated isoperimetric inequalities is the most intricate aspect of this construction.While these two constructions are quite different, we show that they both share a common structure, resembling the intersection patterns of vectors in the Hadamard code. We propose a general framework of such “Hadamard-like” constructions in the hope that it will yield new HDXs.","Expander graphs are of central importance in many diverse parts of computer science and mathematics. Their structure and applications have been well studied for half a century, resulting in a rich theory (see e.g. [hoory2006expander, lubotzky2012expander]). First, different definitions of expansion (spectral, combinatorial, probabilistic) are all known to be essentially equivalent. Second, we know that expanders abound; a random sparse graph is almost surely an expander [pinsker1973complexity]. Finally, there is a variety of methods for constructing and analyzing explicit expander graphs. Initially, these relied on deep algebraic and number theoretic methods [margulis1973explicit, gabber1981explicit, lubotzky1988ramanujan]. However, the quest for elementary (e.g. combinatorial and linear-algebraic) constructions has been extremely fertile, and resulted in major breakthroughs in computer science and math; the zigzag method of [reingold2000entropy] lead to [dinur2007pcp],[Reingold2008],[TaShma2017explicit], and the lifting method of [bilu2004constructing] lead to [marcus2015interlacing, MarkusSS2015]. In contrast, the expansion of sparse high dimensional complexes (or simply hypergraphs) is still a young and growing research topic. The importance of high dimensional expanders (HDXs) in computer science and mathematics is consistently unfolding, with a number of recent breakthroughs which essentially depend on them, such as locally testable codes [dinur2022good, PanteleevK22], quantum coding [anshu2023nlts] and Markov chains [AnariLOV2019] as a partial list. High dimensional expanders differ from expander graphs in several important ways. First, different definitions of expansion are known to be non-equivalent [GundertW2016]. Second, they seem rare, and no natural distributions of complexes are known which almost surely produce bounded-degree expanding complexes. Finally, so far the few existing explicit constructions of bounded degree HDXs use algebraic methods [Ballantine2000, CartwrightSZ2003, Li2004, LubotzkySV2005, KaufmanO2021, ODonnellP2022, Dikstein2023]. No elementary constructions are known where high dimensional expansion follows from a clear (combinatorial or linear algebraic) argument. This demands further understanding of HDXs and motivates much prior work as well as this one. In this work we take a step towards answering this question by constructing sparser HDXs by elementary means. We construct sparse HDXs with two central notions of high dimensional expansion: local spectral expansion and coboundary expansion. To better discuss our results, we define some notions of simplicial complexes and expansion. For simplicity, we only define them in the 2-dimensional case, though many of our results extend to arbitrary constant dimensions. For the full definitions see Section 2. A 2-dimensional simplicial complex is a hypergraph X=(V,E,T) with vertices V, edges E and triangles T (sets of size 3). We require that if t\in T then all its edges are in E, and all its vertices are in V. We denote by N the number of vertices. Given a vertex u\in V, its degree is the number of edges adjacent to it. An essential notion for HDXs are the local properties of its links. A link of a vertex u is the graph G_{u}=(V_{u},E_{u}) whose set of vertices is the neighborhood of u, V_{u}=\left\{v\in V\left|\;\vphantom{v\in V\{u,v\}\in E}\right.\{u,v\}\in E% \right\}. The edges correspond to the triangles incident on u: E_{u}=\left\{\{v,w\}\left|\;\vphantom{\{v,w\}\{u,v,w\}\in T}\right.\{u,v,w\}% \in T\right\}. In this paper we focus on a particularly simple class of complexes, called Cayley complexes over \mathbb{F}_{2}^{n}. These are simplicial complexes whose underlying graph (V,E) is a Cayley graph over \mathbb{F}_{2}^{n}, and so have N=2^{n} vertices. In the rest of the introduction we will explain our constructions. In Section 1.1 we present our construction of local spectral Cayley complexes with arbitrarily good spectral expansion and a subpolynomial degree. In Section 1.2 we present our family of Cayley complexes that are both coboundary expanders and non-trivial local spectral expanders, with arbitrarily small polynomial degree. In Section 1.3 we discuss the common structure of our two constructions that relies on the Hadamard code. In Section 1.4 we present some open questions. 1.1 Local spectral expanders We start with definitions and motivation, review past results and then state ours. The first notion of high dimensional expansion we consider is local spectral expansion. The spectral expansion of a graph G is the second largest eigenvalue of its random walk matrix in absolute value, denoted \lambda(G). Local spectral expansion of complexes is the spectral expansion of every link. Definition 1.1 (Local spectral expansion). Let \lambda\geqslant 0. A 2-dimensional simplicial complex X=(V,E,T) is a \lambda-local spectral expander if the underlying graph (V,E) is connected and for every u\in V, \lambda(X_{u})\leqslant\lambda. Local spectral expansion is the most well known definition of high dimensional expansion. It was first introduced implicitly by [Garland1973] for studying the real cohomology of simplicial complexes. A few decades later, a few other works observed that properties of specific local spectral expanders are of interest to the computer science community [LubotzkySV2005, KaufmanKL2016, KaufmanM2017]. The definition above was given in [DinurK2017], for the purpose of constructing agreement tests. These are strong property tests that are a staple in PCPs (see further discussion in Section 1.2). Since then they have found many more applications that we now discuss. Applications of local spectral expanders One important application of local spectral expansion is in approximate sampling and Glauber dynamics. Local spectral expansion implies optimal spectral bounds on random walks defined on the sets of the complex. These are known as the Glauber dynamics or the up-down walks. This property has lead to a lot of results in approximate sampling and counting. These include works on Matroid sampling [AnariLOV2019], random graph coloring [chen2021rapid] and mixing of Glauber dynamics on the Ising model [AnariJKPV2021]. Local spectral expanders have also found applications in error correcting codes. Their sampling properties give rise to construction of good codes with efficient list-decoding algorithms [AlevJT2019, DinurHKLT2021, jeronimo2021near]. One can also use the to construct locally testable codes that sparsify the Reed-Muller codes [DinurLZ2023]. The list decoding algorithms in [AlevJT2019, jeronimo2021near] rely on their work on constraint satisfaction problems on local spectral expanders. These works generalize classical algorithms for solving CSPs on dense hypergraphs, to algorithms that solve CSPs on local spectral expanders. These works build on the ‘dense-like’ properties of these complexes, and prove that CSPs on local spectral expanders are easy. It is interesting to note that local spectral expanders have rich enough structure so that one can also construct CSPs over them that are also hard for Sum-of-Squares semidefinite programs [DinurFHT2020, HopkinsL2022]222In fact, [AlevJT2019] uses Sum-of-Squares to solve CSPs on local spectral expanders. It seems contradictory, but the hardness results in [DinurFHT2020, HopkinsL2022] use CSPs where the variables are the edges of the HDX. The CSPs in [AlevJT2019] use its vertices as variables.. Some of the most important applications of local spectral expansion are constructions of derandomized agreement tests [DinurK2017, DiksteinD2019, GotlibK2022, BafnaM2024, DiksteinDL2024] and even PCPs [bafna2024quasi]. Most of these works also involve coboundary expansion so we discuss these in Section 1.2. Local spectral expanders have other applications in combinatorics as well. They are especially useful in sparsifying central objects like the complete uniform hypergraph and the boolean hypercube. The HDX based sparsifications maintain many of their important properties such as spectral gap, Chernoff bounds and hypercontractivity (see [KaufmanM2017, DiksteinDFH2018, kaufman2020high, GurLL2022, BafnaHKL2022, dikstein2024chernoff] as a partial list). Previous constructions Attempts to construct bounded degree local spectral expanders with sufficient expansion for applications have limited success besides the algebraic constructions mentioned above [Ballantine2000, CartwrightSZ2003, Li2004, LubotzkySV2005, KaufmanO2021, ODonnellP2022, Dikstein2023]. Random complexes in the high dimensional Erdős–Rényi model defined by [LinialM2006] are not local spectral expanders with overwhelming probability when the average degree is below N^{1/2} and no other bounded degree model is known. The current state of the art is the random geometric graphs on the sphere [LiuMSY2023], that achieve non-trivial local spectral expansion and arbitrarily small polynomial degree (but their expansion is also bounded from below, see discussion in Section 1.2). Even allowing an unbounded degree, the only other known construction that is non-trivially sparse complexes is the one in [Golowich2023] mentioned in the abstract. Our result The first construction we present is a family of Cayley local spectral expanders. Namely, Theorem 1.2. For every \lambda,\varepsilon>0 and integer d\geqslant 2 there exists an infinite family of d-dimensional Cayley \lambda-local spectral expanders over the vertices \mathbb{F}_{2}^{n} with degree at most 2^{n^{\varepsilon}}. This construction builds on an ingenious construction by Golowich [Golowich2023], which proved this theorem for \varepsilon=\frac{1}{2}. As far as we know, this is the sparsest construction that achieves an arbitrarily small \lambda-local spectral expansion other than the algebraic constructions. Proof overview Our construction is based on the well studied Grassmann poset, the partially ordered set of subspaces of \mathbb{F}_{2}^{n}, ordered by containment. This object is the vector space equivalent of high dimensional expanders, and was previously studied in [DiksteinDFH2018, kaufmanT2021garland, GaitondeHKLZ2022, Golowich2023]. To understand our construction, we first describe the one in [Golowich2023]. Golowich begins by sparsifying the Grassmann poset, to obtain a subposet. The generators of the Cayley complex in [Golowich2023] are all (non-zero vectors in) the one-dimensional subspaces in the subposet. The analysis of expansion in [Golowich2023] depend on the expansion properties of the subposet. The degree is just the number of one-dimensional vector spaces in the subposet, which is 2^{n^{1/2}}. Our construction takes a modular approach to this idea. We modify the poset sparsification in [Golowich2023] so that instead of the entire Grassmann poset, we can plug in any subposet Y of the Grassmann, and obtain a sparsification Y^{\prime} whose size depends on Y (not the complete Grassmann). Having this flexible sparsification step, we iterate. We start with Y_{0}, the complete Grassmann poset, we obtain a sequence Y_{1},Y_{2},\dots,Y_{m} of sparser and sparser subposets. The vectors in Y_{i} are low-rank matrices whose rows and columns are vectors in subspaces of Y_{i+1}. We comment that while this is a powerful paradigm, it does have a drawback. The dimension of Y_{i} is logarithmic in the dimension of Y_{i-1} (the dimension of Y is the maximal dimension of a space inside Y). Thus for a given complex Y_{0} we can only perform this sparsification a constant number of steps keeping \varepsilon constant. Our infinite family is generated by taking an infinite family of Y_{0}’s, and using our sparsification procedure on every one of them separately. This construction of sparsified posets produces local spectral expanders of any constant dimension, as observed by [Golowich2023]. The higher dimensional sets in the resulting simplicial complex correspond to the higher dimensional subspaces of the sparsification we obtain. We refer the reader to Section 3 for more details. The analysis of the local spectral expansion is delicate, so we will not describe it here. One property that is necessary for it is the fact that every rank-r matrix has many decompositions into sums of rank-1 matrices. Therefore, to upper bound spectral expansion we study graphs that arise from decompositions of matrices. In addition, we wish to highlight a local-to-global argument in our analysis that uses a theorem by [Madras2002]. Given a graph G=(V,E) that we wish to analyze, we find a set of expanding subgraphs \{H_{1},H_{2},\dots H_{m}\} of G (that are allowed to overlap). We consider the decomposition graph whose vertices are V and whose edges are all \{v,v^{\prime}\} such that there exists an H_{i} such that v,v^{\prime}\in H_{i}. If every H_{i} is an expander and the decomposition graph is also an expander, then G itself is an expander [Madras2002]. This decomposition is particularly useful in our setup. The graphs we need to analyze in order to prove the expansion of our complexes, can be decomposed into smaller subgraphs H_{i}. These H_{i}’s are isomorphic to graphs coming from the original construction of [Golowich2023]. Using this decomposition we are able to reduce the expansion of the links to the expansion of some well studied containment graphs in the original Grassmann we sparsify. Remark 1.3 (Degree lower bounds for Cayley local spectral expanders). The best known lower bound on the degree of Cayley \lambda-local spectral expanders over \mathbb{F}_{2}^{n} is \Omega\left(\frac{n}{\lambda^{2}\log(1/\lambda)}\right) [alon1992simple]. This bound is obtained simply because the underlying graph of a Cayley \lambda-local spectral expander is an O(\lambda)-spectral expander itself [oppenheim2018local]. In Appendix I we investigate this question further, and provide some additional lower bounds on the degree of Cayley complexes based on their link structure. We prove that if the Cayley complex has a connected link then its degree is lower bounded by 2n-1 (compared to n if the link is not connected). If the link mildly expands, we provide a bound that grows with the degree of a vertex inside the link. However, these bounds do not rule out Cayley \lambda-local spectral expanders whose generator set has size O\left(\frac{n}{\lambda^{2}\log(1/\lambda)}\right). The best trade-off between local spectral expansion and the degree of the complex is still open. 1.2 Our coboundary expanders Our next main result is an explicit construction of a family of 2-dimensional complexes with an arbitrarily small polynomial degree, that have both non-trivial local spectral expansion and coboundary expansion. To the best of our knowledge, this is the first such nontrivial construction. Again, these are Cayley complexes of \mathbb{F}_{2}^{n}. Coboundary expansion is a less understood notion compared to local spectral expansion. Therefore, before presenting our result we build intuition for it slowly by introducing a testing-based definition of coboundary expanders. After the definition, we will discuss the motivation behind coboundary expansion and applications of known coboundary expanders. As mentioned before, there are several nonequivalent definitions of expansion for higher-dimensional complexes. In particular, coboundary expansion generalizes the notion of edge expansion in graphs to simplicial complexes. Coboundary expansion was defined independently by [LinialM2006],[MeshulamW2009] and [Gromov2010]. The original motivation for this definition came from discrete topology; more connections to property testing were discovered later. We will expand on these connections after giving the definition. This definition we give is equivalent to the standard definition, but is described in the language of property testing rather than cohomology. The more general and conventional definition for arbitrary groups \Gamma can be found in Section 5.1. Definition 1.4. Let X=(V,E,T) be a 2-dimensional simplicial complex. Consider the code B^{1}\subseteq\mathbb{F}_{2}^{E} whose generating matrix is the vertex-edge incidence matrix A_{X}\in\mathbb{F}_{2}^{E\times V}, and the local test \mathcal{T} of membership in this code for B^{1} given by first sampling a uniformly random triangle \{u,v,w\}\in T and then accepting f\in\mathbb{F}_{2}^{E} if and only if f(\{u,v\})+f(\{v,w\})+f(\{u,w\})=0 over \mathbb{F}_{2}. Let \beta>0. X is a \beta-coboundary expander over \mathbb{F}_{2} if \forall f\in\mathbb{F}_{2}^{E},\leavevmode\nobreak\ \operatorname*{\mathbb{P}}% [\mathcal{T}\text{ rejects }f]\geqslant\beta\cdot dist(f,B^{1}), where dist(f,B^{1})=\min_{c\in B^{1}}\operatorname*{\mathbb{P}}_{e\sim\mathrm{Unif}(% E)}[f(e)\neq c(e)] is the relative distance from f to the closest codeword in B^{1}. Observe that the triangles are parity checks of B^{1}, and so if the input f is a codeword, the test always accepts. A coboundary expander’s local test should reject inputs f which are “far” from the code with probability proportional to their distance from it. The proportionality constant \beta captures the quality of the tester (and coboundary expansion). We note that although the triangles are some parity checks of the code B^{1}, they do not necessarily span all parity checks. In such cases, X is not a coboundary expander for any \beta>0. Definition 1.5. A set of 2-dimensional simplicial complexes \{X_{n}\} is a family of coboundary expanders if there exists some \beta>0 such that for all n, X_{n} is a \beta-coboundary expander. Applications of coboundary expansion We mention three different motivations to the study of coboundary expansion: from agreement testing, discrete geometry, and algebraic topology. Coboundary expanders are useful for constructing derandomized agreement tests for the low-soundness (list-decoding) regime. As we mentioned before, agreement tests (also known as derandomized direct product tests) are a strong property test that arise naturally in many PCP constructions [dinur2007pcp, Raz-parrep, GolSaf97, ImpagliazzoKW2012] and low degree tests [RuSu96, ArSu, RazS1997]. They were introduced by [GolSaf97]. In these tests one gets oracle access to ‘local’ partial functions on small subsets of a variable set, and is supposed to determine by as few queries as possible, whether these functions are correlated with a ‘global’ function on the whole set of variables. The ‘small soundness regime’, is the regime of tests where one wants to argue about closeness to a ‘global’ function even when the ‘local’ partial functions pass the test with very small success probability (see [DG08] for a more formal definition). Works such as [DG08, ImpagliazzoKW2012, DL17] studied this regime over complete simplicial complexes and Grassmanians, but until recently these were essentially the only known examples. Works by [GotlibK2022, BafnaM2023, DiksteinD2023agr] reduced the agreement testing problems to unique games constraint satisfaction problems, and showed existence of a solution via coboundary expansion (over some symmetric groups), following an idea by [DinurM2019]. This lead to derandomized tests that come from bounded degree complexes [DiksteinDL2024, BafnaM2024]. In discrete geometry, a classical result asserts that for any n points in the plane, there exists a point that is contained in at least a constant fraction of the \binom{n}{3} triangles spanned by the n points [Barany1982] 333This can also be generalized to any dimension by replacing triangles with simplices [BorosF1984].. In the language of complexes, for every embedding of the vertices of the complete 2-dimensional complex to the plane, such a heavily covered point exists. One can ask whether such a point exists even when one allows the edges in the embedding to be arbitrary continuous curves. If for every embedding of a complex X to the plane (where the edges are continuous curves), there exists a point that lays inside a constant fraction of the triangles of X, we say X has the topological overlapping property. The celebrated result by Gromov states that the complete complex indeed has this property [Gromov2010]. It is more challenging to prove this property for a given complex. In [Gromov2010], Gromov asked whether there exists a family of bounded-degree complexes with the topological overlapping property. Towards finding an answer, Gromov proved that every coboundary expander has this property. This question has been extremely difficult. An important progress is made in [DotterrerKW2018], where they defined cosystolic expansion, a relaxation of coboundary expansion, and proved that this weaker expansion also implies the topological overlapping property. The problem was eventually resolved in [KaufmanKL2016] for dimension 2 and in [EvraK2016] for dimension >2 where the authors show that the [LubotzkySV2005] Ramanujan complexes are bounded-degree cosystolic expanders. Coboundary expansion has other applications in algebraic topology as well. Linial, Meshulam, and Wallach defined coboundary expansion independently, initiating a study on high dimensional connectivity and cohomologies of random complexes [LinialM2006, MeshulamW2009]. Lower bounding coboundary expansion turned out to be a powerful method to argue about the vanishing of cohomology in high dimensional random complexes [DotterrerK2012, HoffmanKP2017]. Known coboundary expanders Most of the applications mentioned above call for complexes that are simultaneously coboundary expanders and non-trivial local spectral expanders. So far there are no known constructions of such complexes with arbitrarily small polynomial degree even in dimension 2 444By coboundary expanders, we are referring to complexes that have coboundary expansion over every group, not just \mathbb{F}_{2}.. Here we summarize some known results. Spherical buildings of type A_{d} are dense complexes that appear as the links of the Ramanujan complexes in [LubotzkySV2005]. The vertex set of a spherical building consists of all proper subspaces of some ambient vector space \mathbb{F}_{q}^{d}. [Gromov2010] proved that spherical buildings are coboundary expanders (see also [LubotzkyMM2016]). Geometric lattices are simplicial complexes that generalize spherical buildings. They were first defined in [KozlovM2019]. Most recently, [DiksteinD2023cbdry] show that geometric lattices are coboundary expanders. [KaufmanO2021] show that the 2-dimensional vertex links of the coset geometry complexes from [kaufmanO2018] are coboundary expanders. If we restrict our interest to coboundary expanders over a single finite group instead of all groups, bounded degree coboundary expanders that are local spectral expanders are known. [KaufmanKL2016] solved this for the \mathbb{F}_{2} case conditioned on a conjecture by Serre. [ChapmanL2023] constructed such complexes for the \mathbb{F}_{2} unconditionally, and their idea was extended by [DiksteinDL2024, BafnaM2024] to every fixed finite group, which lead to the aforementioned agreement soundness result. Unfortunately, their approach cannot be leveraged to constructing a coboundary expander with respect to all groups simultaneously. We note that the coboundary expanders mentioned above are also local spectral expanders and that if we do not enforce the local spectral expansion property, coboundary expanders are trivial yet less useful. To the best of our knowledge, all known constructions of 2-dimensional coboundary expanders (over all groups) that are also non-trivial local spectral expanders have vertex degree at least N^{c} for some fixed c>0, where N is the number of vertices (see e.g. [LubotzkySV2005, kaufmanO2018]).555To the best of our knowledge, this constant is c=0.5. The diameter of all of those complexes is at most a fixed constant, which implies this lower bound on the maximal degree. In this work, we give the first such construction with arbitrarily small polynomial degree. Our result We now state our main result in this subsection. For every integer k>0, and constant \varepsilon\in(0,\frac{1}{2}] we construct a family of k-dimensional simplicial complexes \{X_{\varepsilon,n}\}_{n} called the Johnson complexes. For any k-dimensional simplicial complex X, we use the notation (X)^{\leqslant 2} (the 2-skeleton of X) to denote the 2-dimensional simplicial complex consists of the vertex, edge, and triangle sets of X. Theorem 1.6 (Informal version of Theorem 6.1, Lemma 4.4, and Lemma 6.6). For any integer k\geqslant 2, \varepsilon\in(0,\frac{1}{2}], the 2-skeletons of the Johnson complexes X_{\varepsilon,n} are \left(\frac{1}{2}-\frac{\varepsilon}{2}\right)-local spectral expanders and \Omega(\varepsilon)-coboundary expanders (over every group \Gamma). Moreover, if k\geqslant 3, the 2-skeletons of X_{\varepsilon,n}s’ vertex links are \frac{1}{85}-coboundary expanders . Fix \varepsilon\in(0,\frac{1}{2}]. For every n satisfying 4\;|\;\varepsilon n, we briefly describe the 2-skeletons of X_{\varepsilon,n}. The underlying graph of X_{\varepsilon,n} is simply the Cayley graph Cay(H;S_{\varepsilon}) where H\subset\mathbb{F}_{2}^{n} consists of vectors with even Hamming weight, and S_{\varepsilon} is the set of vectors with Hamming weight \varepsilon n. Thus the number of vertices in the graph is 2^{n-1} while the vertex degree is \binom{n}{\varepsilon n}<2^{nh(\varepsilon)} 666Here h is the binary entropy function.. The triangles are given by: X_{\varepsilon,n}(2)=\left\{\{x,x+s_{1},x+s_{2}\}\left|\;\vphantom{\{x,x+s_{1}% ,x+s_{2}\}x\in\mathbb{F}_{2}^{n},s_{1},s_{2},s_{1}+s_{2}\in S_{\varepsilon}}% \right.x\in\mathbb{F}_{2}^{n},s_{1},s_{2},s_{1}+s_{2}\in S_{\varepsilon}\right\}. Observe that the link of every vertex is isomorphic to the classically studied Johnson graph J(n,m,m/2) of m-subsets of [n], that are connected if their intersection is m/2 (for m=\varepsilon n). We will use some of the known properties of this graph in our analysis. We additionally show that when 2^{k}\;|\;\varepsilon n, we can extend the above construction to get k-dimensional simplicial complexes X_{\varepsilon,n} with the exact same vertex set, edge set, and triangle set as defined above. Moreover we show that for every integer 0\leqslant m\leqslant k-2, the link of an m-face in the complex is also a non-trivial local spectral expander (Lemma 4.4). Remark 1.7. Let N=2^{n-1}. We note that all vertices in X_{\varepsilon,n} are in \mathrm{poly}(N) edges and all vertices in the links of X_{\varepsilon,n} are in \mathrm{poly}(N) edges. We remark that using a graph sparsification result due to [ChungH07], we can randomly subsample the triangles in X_{\varepsilon,n} to obtain a random subcomplex which is still a local spectral expander with high probability but whose links have vertex degree \mathrm{polylog}(N). A detailed discussion can be found in Appendix F. Before giving a high-level overview of the proof for Theorem 1.6, we describe a general approach for showing coboundary expansion called the cone method. Appearing implicitly in [Gromov2010], it was used in a variety of works (see [LubotzkyMM2016, KozlovM2019, KaufmanO2021] as a partial list). We also take this approach to show that the Johnson complexes are coboundary expanders. Even though most of our results on coboundary expansion are for the Johnson complexes, we can also use the methods in this paper to prove that the vertex links of the [Golowich2023] Cayley local spectral expanders are coboundary expanders. This implies that these Cayley local spectral expanders are cosystolic expanders (see [KaufmanKL2016, DiksteinD2023cbdry]), the relaxation of coboundary expansion mentioned above. This makes them useful for constructing agreement tests and complexes with the topological overlapping property. We note that [Golowich2023] observed that these complexes are not coboundary expanders so this relaxation is necessary. A definition of cosystolic expansion and a detailed proof can be found in Section 6.3. Cones and isoperimetry Recall first the definition of \beta-coboundary expansion above. We start with the code B^{1} of edge functions f which arise from vertex labelings by elements of some group \Gamma. This implies that composing (in order) the values of f\in B^{1} along the edges of any cycle will give the identity element id\in\Gamma. This holds in particular for triangles, which are our “parity checks”. We digress to discuss an analogous situation in geometric group theory, of the word problem for finitely presented groups. In this context, we are given a word in the generators of a group and need to check if it simplifies to the identity under the given relations. In our context the given word is the labeling of f along some cycle, and a sequence of relations (here, triangles) is applied to successively “contract” this word to the identity. The tight upper bound on the number of such contractions in terms of the length of the word (called the Dehn function of the presentation), captures important properties of the group, e.g. whether or not it is hyperbolic. This ratio between the contraction size and the word length is key to the cone method. One convenient way of capturing the number of contractions is the so-called “van Kampen diagram”, which simply “tiles” the cycle with triangles in the plane (all edges are consistently labeled by group elements). The van Kampen lemma ensures that if a given word can be tiled with t tiles, then there is a sequence of t contractions that reduce it to the identity [VanKampen1933]. The value in this viewpoint is that tiling is a static object, which can be drawn on the plane, and allows one to forget about the ordering of the contractions. We will make use of this in our arguments, and for completeness derive the van Kampen lemma in our context. Note that a bound on the minimum number of tiles (a notion of area) needed to cover any cycle of a given length (a notion of boundary length) can be easily seen as an isoperimetric inequality! The cone method will require such isoperimetric inequality, and the Dehn function gives an upper bound on it. Back to \beta-coboundary expansion. Here the given f may not be in B^{1}, and we need to prove that if it is “far” from B^{1}, then the proportion of triangles which do not compose to the identity on f will be at least \beta\operatorname{dist}(f,B^{1}). The cone method localizes this task. To use this method, one needs to specify a family of cycles (also called a cone) in the underlying graph of the complex. Gromov observed that the complex’s coboundary expansion has a lower bound that is inverse proportional to the number of triangles (in the complex) needed to tile a cycle from this cone [Gromov2010]. This number is also referred to as the cone area. Thus, bounding the coboundary expansion of the complex reduces to computing the cone area of some cone. Needless to say, an upper bound on the Dehn function, which gives the worst case area for all cycles, certainly suffices for bounding the cone area of any cone.777We note that in a sense the converse is also true - computing the cone area for cones with cycles of “minimal” length suffices for computing the Dehn function. We indeed give such a strong bound. Proof overview It remains to construct a cone in X_{\varepsilon,n} and upper bound its cone area. We now provide a high level intuition for our approach. First observe that the diameter of the complex X=X_{\varepsilon,n} is \Theta(1/\varepsilon). The proof then proceeds as follows. 1. We move to a denser 2-dimensional complex X^{\prime} whose underlying graph is the Cayley graph Cay(H;S_{\leqslant\varepsilon}) where H\subset\mathbb{F}_{2}^{n} consists of vectors with even Hamming weight, and S_{\leqslant\varepsilon} is the set of vectors with even Hamming weight \leqslant\varepsilon n. The triangle set consists of all 3-cliques in the Cayley graph. We note that X^{\prime} has the same vertex set as X but has more edges and triangles than the Johnson complex. 2. Then we carefully construct a cone \mathcal{A}^{\prime} in X^{\prime} with cone area \Theta(1/\varepsilon). 3. We show that every edge in X^{\prime} translates to a length-2 path in X, and every 3-cycle (boundary of a triangle) in X^{\prime} translates to a 6-cycle in X which can be tiled by O(1) triangles from X. 4. We translate the cone \mathcal{A}^{\prime} in X^{\prime} into a cone \mathcal{A} in X by replacing every edge in the cycles of \mathcal{A}^{\prime} with a certain length-2 path in X. Thus every cycle C\in\mathcal{A} can be tiled by first tiling its corresponding cycle C^{\prime}\in\mathcal{A}^{\prime} with O(1/\varepsilon) X^{\prime} triangles and then tiling each of the X^{\prime} triangles with O(1) X triangles. Thereby we conclude that \mathcal{A} has cone area \Theta(1/\varepsilon). Local spectral expansion and comparison to random geometric complexes These Johnson complexes also have non-trivial local spectral expansion. While they do not achieve arbitrarily small \lambda-local spectral expansion, they do pass the \lambda<\frac{1}{2} barrier. From a combinatorial point of view, \lambda<\frac{1}{2} is an important threshold. For \lambda\geqslant\frac{1}{2}, there are elementary bounded-degree constructions of \lambda-local spectral expanders [Conlon2019, ChapmanLP2020, ConlonTZ2020, LiuMY2020, Golowich2021] but these fail to satisfy any of the local-to-global properties in HDXs. For \lambda<\frac{1}{2} all these constructions break. For this regime, all bounded degree constructions rely on algebraic tools. This is not by accident; below \frac{1}{2} there are a number of high dimensional global properties suddenly begin to hold. For instance, a theorem by [Garland1973] implies that when \lambda<\frac{1}{2} all real cohomologies of the complex vanish. Another example is the ‘Trickle-Down’ theorem by [oppenheim2018local] which says that the expansion of the skeleton (V,E) is non-trivial whenever \lambda<\frac{1}{2}. These are strong properties, and we do not know how produce them in elementary constructions. Therefore, when constructions of local spectral expanders are considered non-trivial only when \lambda<\frac{1}{2}. To show local spectral expansion, we first note that the Johnson complex is symmetric for each vertex. Hence, it suffices to focus on the link of 0\in\mathbb{F}_{2}^{n}. Because the triangle set T is well-structured, we can show that the link graph of 0 is isomorphic to tensor products of Johnson graphs (Proposition 4.7). This allows us to use the theory of association schemes to bound their eigenvalues. We also note that since all boolean vectors in \{0,1\}^{n} lie on a sphere in \mathbb{R}^{n} centered at [\frac{1}{2},\frac{1}{2},\dots,\frac{1}{2}], the Johnson complex X_{\varepsilon,n} can be viewed as a geometric complex whose vertices are associated with points over a sphere and two vertices form an edge if and only if their corresponding points’ L_{2} distance is \sqrt{\varepsilon n}. Previously [LiuMSY2023] prove that randomized 2-dimensional geometric complexes are local spectral expanders. Comparing the two constructions, we conclude that Johnson complexes are sparser local spectral expanders than random geometric complexes. 1.3 The common structure between the two constructions Taking a step back, we wish to highlight that the two constructions in our paper share a common structure. Both of these constructions can be described via ‘induced Hadamard encoding’. For a (not necessarily linear) function s:\mathbb{F}_{2}^{d}\to\mathbb{F}_{2}^{n}, the ‘induced’ Hadamard encoding is the linear map \widehat{s}:\mathbb{F}_{2}^{d}\to\mathbb{F}_{2}^{n} given by \widehat{s}(x)\coloneqq\sum_{v\in\mathbb{F}_{2}^{d}}\langle x,v\rangle s(v), where \langle x,v\rangle=\sum_{i=1}^{d}x_{i}v_{i} and the sum is over \mathbb{F}_{2}. Our two constructions of Cayley HDXs take the generating set of the Cayley graph to be all bases of spaces {\textrm{Im}}(\widehat{s})\subseteq\mathbb{F}_{2}^{n}, for carefully chosen functions s as above; the choice determines the construction. We note that the “orthogonality” of vectors of the Hadamard code manifests itself very differently in the two constructions: in the Johnson complexes it can be viewed via the Hamming weight metric, while in the other construction it may be viewed via the matrix rank metric. We connect the structure of links in our constructions, to the restrictions of these induced Hadamard encodings to affine subspaces of \mathbb{F}_{2}^{n}. Using this connection we show that one can decompose the link into a tensor product of simpler graphs, which are amenable to our analysis. A special case of this observation was also used in the analysis of the complexes constructed by [Golowich2023]. 1.4 Open questions Local spectral expanders As mentioned above, we do not know how sparse can a Cayley local spectral expander be. To what extent can we close the gaps between the lower and upper bounds for various types of abelian Cayley HDXs? In particular, can we show that any nontrivial abelian Cayley expanders must have degree \omega(n)? Conversely, can we construct Cayley HDXs where the degree is upper bounded by some polynomial in n? Limits on our iterated sparsification technique would also be of interest. So far, the approach of constructing Cayley local spectral expanders over \mathbb{F}_{2}^{n} by sparsifying Grassmann posets yields the sparsest known such complexes. In contrast to the success of this approach, we have limited understanding of its full power. To this end, we propose several questions: what codes can be used in place of the Hadamard codes so that the sparsification step preserves local spectral expansion? Could this approach be generalized to obtain local spectral expanders over other abelian groups? As mentioned above, we know that the approach we use can not give a complex of polynomial degree in n without introducing a new idea. Coboundary expanders Another fundamental question regards the isoperimetric inequalities we describe above. In this paper and many others, one uses the approach pioneered by Gromov [Gromov2010] that applies isoperimetric inequalities to lower bound coboundary expansion. A natural question is whether an isoperimetric inequality is also necessary to obtain coboundary expansion. An equivalence between coboundary expansion and isoperimetry will give us a simple alternative description for coboundary expansion. A counterexample to such a statement would motivate finding alternative approaches for showing coboundary expansion. We call our family of 2-dimensional simplicial complexes that are both local spectral expanders and 1-dimensional coboundary expanders Johnson complexes. This construction can be generalized to yield families of k-dimensional Johnson complexes that are still local spectral expanders. However, it remains open whether they are also coboundary expanders in dimension >1. 1.5 Organization of this paper In Section 2, We give background material on simplicial complexes and local spectral expansion. We also define Grassmann posets - the vector space analogue for HDXs - which we use in our constructions Other elementary background on graphs and expansion is given there as well. In Section 3, we construct the subpolynomial degree Cayley complexes, proving Theorem 1.2. Most of this section discusses Grassmann posets and not Cayley HDXs, but we describe the connection between the two given by [Golowich2023] in this section too. We deffer some of the more technical expansion upper bounds to Appendix D. In Section 4 we construct the Johnson complexes which are both coboundary expanders and local spectral expanders. In this section we prove they are local spectral expanders, leaving coboundary expansion to be the focus of the next two sections. We also give a detailed comparison of this construction to the one in [LiuMSY2023], and discuss how to to further sparsify our complexes by random subsampling. In Section 5 we take a detour to formally define coboundary expansion and discuss its connection to isoperimetric inequalities. In this section we also give our version of the van-Kampen lemma, generalized to the setting of coboundary expansion. This may be of independent interest, as the van-Kampen lemma simplifies many proofs of coboundary expansion. In Section 6 we prove that the Johnson complexes are coboundary expanders (Theorem 6.1), and local coboundary expanders (Theorem 6.4). We also prove that links in the construction of [Golowich2023] are coboundary expanders. In Section 7 we show that both the Johnson complex and the Matrix complexes are special cases of a more general construction. In this section we show that the two complexes have a similar link structure, which is necessary to analyze the local spectral expansion in both Cayley HDXs. The appendices contain some of the more technical claims for ease of reading. In Appendix I we also give a lower bound on the degree of Cayley local spectral expanders, based on the degree of the link. 1.6 Acknowledgments We thank Louis Golowich for helpful discussions and Gil Melnik for assistance with the figures."
https://arxiv.org/html/2411.08685v1,"Long induced paths in sparse graphs
and graphs with forbidden patterns","Consider a graph G with a path P of order n. What conditions force G to also have a long induced path? As complete bipartite graphs have long paths but no long induced paths, a natural restriction is to forbid some fixed complete bipartite graph K_{t,t} as a subgraph. In this case we show that G has an induced path of order (\log\log n)^{1/5-o(1)}. This is an exponential improvement over a result of Galvin, Rival, and Sands (1982) and comes close to a recent upper bound of order O((\log\log n)^{2}).Another way to approach this problem is by viewing G as an ordered graph (where the vertices are ordered according to their position on the path P). From this point of view it is most natural to consider which ordered subgraphs need to be forbidden in order to force the existence of a long induced path. Focusing on the exclusion of ordered matchings, we improve or recover a number of existing results with much simpler proofs, in a unified way. We also show that if some forbidden ordered subgraph forces the existence of a long induced path in G, then this induced path has size at least \Omega((\log\log\log n)^{1/3}), and can be chosen to be increasing with respect to P.","The starting point of this work is the following result proved by Galvin, Rival, and Sands in 1982. Theorem 1.1 ([GRS82]). For every t\in\mathbb{N} there is an unbounded function f\colon\mathbb{N}\to\mathbb{N} such that for every graph G the following holds: if G is K_{t,t}-subgraph free and has a path of order n, then G has an induced path of order at least f(n). This result states that in the absence of large K_{t,t} subgraphs, long induced paths are unavoidable induced subgraphs of graphs that contain large paths. The function f is not given explicitly in [GRS82], but it directly follows from the proof there and known bounds on multicolor Ramsey numbers for quadruples that f(n)=\Omega((\log\log\log n)^{1/3}) (see Theorem 4.23 and its proof in Section 4.3). We may wonder whether this result is best possible. For hereditary graph classes, being K_{t,t}-subgraph free is actually necessary. Indeed, complete (bipartite) graphs have a path that visit all their vertices, yet they do not have induced paths on more than 3 vertices. If a hereditary graph class \mathcal{C} has graphs with arbitrarily large K_{t,t} subgraphs, then by Ramsey’s theorem \mathcal{C} contains arbitrarily large complete graphs or complete bipartite graphs so as observed above the outcome of Theorem 1.1 cannot hold. We note than an independent proof of Theorem 1.1 was given 30 years later in [ALR12] by Atminas, Lozin, and Razgon, who were not aware of the original result.111Personal communication. They did not try to optimize f and indeed a quick glance at their proof suggests a function with at least 8 nested logarithms. The question of the optimal function f for Theorem 1.1 can be stated more generally as follows. Question 1. Given a graph class \mathcal{C}, what is the maximum function f_{\mathcal{C}}\colon\mathbb{N}\to\mathbb{N} such that the following property holds? (\star) Every G\in\mathcal{C} with a path of order n has an induced path of order at least f_{\mathcal{C}}(n). A visual summary of the main existing results on this question is displayed in Figure 1. We note that Question 1 has been mostly considered for hereditary classes (i.e., classes of graphs that are closed under taking induced subgraphs). f(n)=(\log\log n)^{\Omega(1)}f(n)=(\log n)^{\Omega(1)}f(n)=n^{\Omega(1)}excluding a K_{t,t} subgraphf(n)=\Omega\left(\left({\log\log n/\log\log\log n}\right)^{1/5}\right)[this paper]bounded degeneracyf(n)\geqslant\frac{\log\log n}{\log k}[NOdM12]bounded expansionf(n)=O((\log\log n)^{2})[DR24]excluding a topo. minorf(n)=\Omega((\log n)^{c})[HR23]bounded degreef(n)=\Theta\left(\frac{\log n}{\log k}\right)[HR23]bounded genusf(n)=\Omega(\sqrt{\log n})[ELM17]planarf(n)=O\left(\frac{\log n}{\log\log n}\right)f(n)=\Omega(\sqrt{\log n})[ELM17]outerplanar 2-treef(n)=O(\log n)[AV00]bounded twf(n)=\Omega((\log n)^{1/k})[HR23]planar and3-connectedf(n)=\Theta(\log n)[ELM17]\textbf{tw}\leqslant 2f(n)=\Omega(\log n)[ELM17]k-treef(n)\geqslant\frac{\log n}{k\log k}[ELM17]chordal bounded \bf{tw}f(n)\leqslant(k-1)(\log n)^{\frac{2}{k-1}}[ELM17]bounded pwf(n)=\Omega\left(n^{1/k}\right)[HR23]interval bounded \bf{pw}f(n)=O\left(n^{2/k}\right)[HR23] Figure 1. Currently best known bounds regarding 1. We recall that for hereditary graph classes, excluding a bipartite subgraph is the most general setting where the property of 1 holds with f unbounded. In the results above, k is a strict upper-bound on the considered parameter and c depends on the excluded graph. Treewidth and pathwidth are respectively abbreviated tw and pw. Arrows point towards more general concepts. In particular, less general than K_{t,t}-subgraph free graphs, the case of d-degenerate222A graph is d-degenerate if all its subgraphs have a vertex of degree at most d. graphs was studied by Nešetřil and Ossona de Mendez [NOdM12]. They proved that if \mathcal{C} is the class of d-degenerate graphs then f_{\mathcal{C}}(n)\geqslant\log\log n/\log(d+1). This is close to the correct order of magnitude as Defrain and the third author recently constructed in [DR24] 2-degenerate graphs with paths of order n and where induced paths have order O((\log\log n)^{2}). As graphs of bounded degeneracy exclude large complete (bipartite) graphs, this upper-bound also holds for the function of Theorem 1.1. Hence there is currently an exponential gap between the best known upper- and lower-bounds for the maximum function f such that Theorem 1.1 holds. In this paper we prove the following exponential improvement on Theorem 1.1. Theorem 1.2. For every every positive integer t there is a constant c such that if a graph G has a path of order n and no K_{t,t} subgraph, then G has an induced path of order at least c(\log\log n/\log\log\log n)^{1/5}. Hence the gap is now only polynomial between the upper and lower bounds for the function f_{\mathcal{C}}, where \mathcal{C} is the class of graphs with no K_{t,t} subgraphs. Moreover, we have the following dichotomy: Corollary 1.3. There exists a constant c such that, for every hereditary graph class \mathcal{C}, either f_{\mathcal{C}}(n)=\Omega((\log\log n)^{c}) or f_{\mathcal{C}}(n)=O(1). Forbidden patterns In most proofs of results in the area, what matters when considering some path P=v_{1},\ldots,v_{n} in a graph G is not so much the subgraph of G induced by P, but instead the ordered subgraph of G induced by P, with underlying order v_{1}<\cdots<v_{n}. It turns out that forbidding ordered subgraphs in G (rather than subgraphs) not only allows for a much more fine-grained understanding of the function f_{\mathcal{C}}, but also provides a unifying framework to obtain as simple corollaries most of the results that have been proved so far in the literature. Let us say that a graph G with a path P=v_{1},\ldots,v_{n} contains some ordered subgraph H as a pattern if H appears as an ordered subgraph of G[V(P)]-E(P), considered with the ordering v_{1}<\cdots<v_{n}. This means that the ordering of the vertices of the copy of H in G is the same ordering as in G (along the path P), and moreover we do not consider the edges of P as part of a pattern. If G does not contain a pattern H, we say that G avoids the pattern H. In this setting we consider the following natural counterpart of 1. Question 2. Given an ordered graph H, what is the maximum function g_{H}\colon\mathbb{N}\to\mathbb{N} such that the following property holds? (\star\star) Every graph G with a path P of order n that avoids H as a pattern has an induced path of order at least g_{H}(n). Observe that we can assume that P is a Hamiltonian path in G (by considering the subgraph of G induced by P instead of G). If H=K_{2} then G is precisely an induced path on n vertices, so g_{K_{2}}(n)=n. Our first result is that if g_{H}(n)=\omega(\log n), then H must be a matching. Hence, better-than-logarithmic bounds on the size of induced paths can only be obtained by considering very simple patterns, namely ordered matchings. It is thus natural to investigate g_{H}(n) when H is an ordered matching, and we prove the following: • either H is non-crossing (that is, it does not contain vertices a<b<c<d with edges ac,bd) and then g_{H}(n) is polynomial, or • H contains a pair of crossing edges and then g_{H}(n) is polylogarithmic. It turns out that excluding an ordered matching contains a number of earlier results as particular cases, and we therefore either recover or improve a number of known results as simple corollaries of our result: • for the class \mathcal{C} of graphs of pathwidth at most p, f_{\mathcal{C}}(n)=\Omega(n^{1/p}) [HR23]; • for the class \mathcal{C} of graphs of treewidth at most t, f_{\mathcal{C}}(n)=(\log n)^{\Omega(1/t)} [HR23]; • for the class \mathcal{C} of outerplanar graphs, f_{\mathcal{C}}(n)=\Omega(\log n) [ELM17]; • for the class \mathcal{C} of planar graphs (or more generally for any class of graphs embeddable on a fixed surface), f_{\mathcal{C}}(n)=\Omega((\log n)^{1/2}) [ELM17]; • for the class \mathcal{C} of K_{t}-minor-free graphs, f_{\mathcal{C}}(n)=(\log n)^{\Omega(1/t^{2})} [HR23]. Note that in the last item in the list above (for K_{t}-minor-free graphs) the exponent in the polylogarithmic bound of [HR23] was an unspecified function of t (depending on the Robertson-Seymour graph minor structure theorem). Actually the result of [HR23] was obtained directly for the class of graphs with no K_{t} as a topological minor (which contains the class of K_{t}-minor-free graphs), but we do not believe that we can recover this result by only forbidding order matchings. This leads us to consider the more general case where H is a star forest in a companion paper [DER24]. In [DER24] we completely characterize all ordered subgraphs H such that g_{H} is polylogarithmic. A consequence of the main result of [DER24] is that for the class \mathcal{C} of graphs which do not contain K_{t} as a topological minor, f_{\mathcal{C}}(n)=(\log n)^{\Omega(1/(t(\log t)^{2}))}. The proof of this result is significantly more involved than the proofs of the previous results involving forbidden matchings. We also obtain the following simple corollary of our results. Corollary 1.4. Let \mathcal{C} be a proper minor-closed class. If \mathcal{C} excludes some outerplanar graph, then f_{\mathcal{C}}(n) is polynomial, and otherwise f_{\mathcal{C}}(n) is polylogarithmic. We conclude the paper with a complete characterization of ordered graphs H such that g_{H}(n)=O(1) and prove the following dichotomy result which is very similar to Corollary 1.3: for any ordered graph H, either g_{H}(n)=\Omega((\log\log\log n)^{c}) for some c>0, or g_{H}(n)=O(1). The proof of one of the two directions is very similar to that of Theorem 1.1, which was our starting point. As a consequence of our results we obtain the following dichotomies, that reveal jumps in the growth rate of g_{H}. Corollary 1.5. Let H be an ordered graph. (1) g_{H}(n)=n^{\Omega(1)} if and only if H is a subgraph of a non-crossing matching, otherwise g_{H}(n)=O(\log n); (2) g_{H}(n)=(\log\log n)^{\Omega(1)} if H is bipartite, with one partite set preceding the other in the order; (3) g_{H}(n)=(\log\log\log n)^{\Omega(1)} if and only if H is a subgraph of the ordered half-graph, and otherwise g_{H}(n)=O(1). One remark about Corollary 1.5 is that there is a significant difference between case (1) and cases (2)–(3). In (1) the exponent depends on the excluded pattern H, while in (2)–(3) the exponent is an absolute constant, independent of the pattern. In the companion paper [DER24], we will add the following item to the dichotomies above: Theorem 1.6 ([DER24]). Let H be an ordered graph. Then g_{H}(n)=(\log n)^{\Omega(1)} if and only if H is a constellation, and otherwise g_{H}(n)=O((\log\log n)^{2}). In the statement of Theorem 1.6, a constellation is a star forest with a specific vertex ordering (see [DER24] for more details). Organization of the paper In Section 2 we introduce the necessary tools and definitions. Section 3 is devoted to the proof of Theorem 1.2. We study forbidden patterns in Section 4, first in the case of matchings in Subsection 4.2, where we prove in particular Corollary 1.4 and then for general patterns in Subsection 4.3, where we prove Corollary 1.5. We conclude with open problems in Section 5."
https://arxiv.org/html/2411.08233v1,Improved Constructions of Skew-Tolerant Gray Codes,"We study skew-tolerant Gray codes, which are Gray codes in which changes in consecutive codewords occur in adjacent positions. We present the first construction of asymptotically non-vanishing skew-tolerant Gray codes, offering an exponential improvement over the known construction. We also provide linear-time encoding and decoding algorithms for our codes. Finally, we extend the definition to non-binary alphabets, and provide constructions of complete m-ary skew-tolerant Gray codes for every base m\geqslant 3.","Ever since their introduction [6], Gray codes have been of great interest both for their practical applications in data communications and storage, and for the theoretical problems that arise from their study. In their original forms, Gray codes are listings of all binary n-tuples, without repetitions, and with the added restrictions that consecutive tuples differ by a single bit flip. Multiple variants have been extensively investigated, and are still today, such as snake-in-the-box codes [12, 19, 1], circuit codes [13, 15, 8, 10], and single-track Gray codes [7, 5, 18, 8, 10], to name a few. Moreover, Gray codes have inspired the idea of finding listings of combinatorial objects such that there is only a small change from one element to the next. They have thus been extended to codes over subspaces [17, 2], permutations [27, 11, 29, 9, 30, 24, 28], and a multitude of other structures. We refer the reader to the classical survey [16] and the more recent survey [14] for a comprehensive review of Gray codes. In [26], Wilson and Blaum defined skew-tolerant Gray codes as Gray codes such that changes in consecutive codewords occur in adjacent positions. Their motivation behind this definition was to allow for better performance in mechanical encoder systems where the reading head might be skewed. In Figure 1(a), a surface contains distinct binary vectors of length 4 written as rows of a matrix. Four sensors are mounted on a reading head (denoted by \otimes), and the head moves vertically. By reading the binary vector beneath it, it can deduce its position. However, if it attempts to do so in-between two rows, each sensor may independently read the bit below or above it. If more than one bit is in question, the resulting reading may be neither the vector immediately below the head’s position, nor the one immediately above it. This may result in a location error. To avoid this, the rows form a binary Gray code, namely, a single bit changes between any two consecutive rows. Thus, even if getting a reading in-between rows, the resulting reading is the entire vector immediately above or below the reading-head location. When the reading head is mounted with a skew, even if we use a (general) binary Gray code, more than one sensor may see a bit change at some moment, as seen in Figure 1(b). As [26] shows, by using a skew-tolerant Gray code, the skew angle at which the system operates without any location error is maximized. Another major advantage of skew-tolerant Gray codes is that they can be easily compressed, as they are completely determined by the first codeword, the position of the first bit flip, and a sequence of directions (right, left) that indicate the next change position. Thus, only \Theta(1) bits per codeword are needed for storage. \begin{overpic}[scale={0.3}]{sktgc1.pdf} \put(60.0,52.5){$\otimes$} \put(27.0,-5.0){\scriptsize{(a)}} \put(10.0,47.0){\scriptsize$\begin{array}[]{cccc}0&0&0&0\\ 0&0&0&1\\ 0&0&1&1\\ 0&0&1&0\\ 0&1&1&0\\ 0&1&1&1\\ 0&1&0&1\\ 0&1&0&0\\ 1&1&0&0\\ 1&1&0&1\\ 1&1&1&1\\ 1&1&1&0\\ 1&0&1&0\\ 1&0&1&1\\ 1&0&0&1\\ 1&0&0&0\\ \end{array}$} \end{overpic} \begin{overpic}[scale={0.3}]{sktgc2.pdf} \put(60.0,57.5){$\otimes$} \put(27.0,-5.0){\scriptsize{(b)}} \put(10.0,49.0){\scriptsize$\begin{array}[]{cccc}0&0&0&0\\ 0&0&0&1\\ 0&0&1&1\\ 0&0&1&0\\ 0&1&1&0\\ 0&1&1&1\\ 0&1&0&1\\ 0&1&0&0\\ 1&1&0&0\\ 1&1&0&1\\ 1&1&1&1\\ 1&1&1&0\\ 1&0&1&0\\ 1&0&1&1\\ 1&0&0&1\\ 1&0&0&0\\ \end{array}$} \end{overpic} Figure 1: Reading heads mounted over a surface (a) without a skew, and (b) with a skew. Wilson and Blaum’s definition is equivalent to the \overline{P_{n}}-compatible Gray codes defined previously by Slater in [20] and [21], where P_{n} represents the path graph of n vertices. Several other works have studied Gray codes compatible with more general graphs, like trees with infinite diameter [25], hypercubes [4] and complete multipartite graphs [3]. The only attempt at constructing large skew-tolerant Gray codes that we know of is that of [26]. The size of the known codes of n bits is of the order of (\sqrt{3})^{n} codewords, which is exponentially smaller than the theoretical bound of 2^{n} codewords, namely, a complete code. Slater [21] conjectured that no complete skew-tolerant (\overline{P_{n}}-compatible) Gray code exists for n\geqslant 7. This leaves open the question: what is the largest possible skew-tolerant Gray code with length of n bits? The main contribution of this work is to present a construction of asymptotically non-vanishing skew-tolerant Gray codes. More precisely, we present codes of length n and size \approx c\cdot 2^{n} with c a constant that depends on the parity of n. For n even, c>0.76, and for n odd c>0.84. Thus, our codes offer an exponential improvement over the codes of [26]. We also study the generalization of these codes to larger alphabets, and show that for any alphabet size m\geqslant 3, there are complete skew-tolerant Gray codes. This work is organized as follows: in Section II we introduce the definition of k-skew-tolerant Gray codes (k-SkTGCs), as well as necessary notations used throughout the paper. In Section III we start by studying 3-SkTGCs and 2-SkTGCs, showing that complete codes are possible. By using a similar approach, we then construct asymptotically non-vanishing 1-SkTGCs. We also provide linear-time encoding and decoding algorithms for all constructed binary codes. Finally, in Section IV we generalize the definition of skew-tolerant Gray codes to non-binary alphabets, and we show that it is always possible to construct complete m-ary skew-tolerant Gray codes of any length."
https://arxiv.org/html/2411.07576v1,Numerical Homogenization by Continuous Super-Resolution,"Finite element methods typically require a high resolution to satisfactorily approximate micro and even macro patterns of an underlying physical model. This issue can be circumvented by appropriate numerical homogenization or multiscale strategies that are able to obtain reasonable approximations on under-resolved scales. In this paper, we study the implicit neural representation and propose a continuous super-resolution network as a numerical homogenization strategy. It can take coarse finite element data to learn both in-distribution and out-of-distribution high-resolution finite element predictions. Our highlight is the design of a local implicit transformer, which is able to learn multiscale features. We also propose Gabor wavelet-based coordinate encodings which can overcome the bias of neural networks learning low-frequency features. Finally, perception is often preferred over distortion so scientists can recognize the visual pattern for further investigation. However, implicit neural representation is known for its lack of local pattern supervision. We propose to use stochastic cosine similarities to compare the local feature differences between prediction and ground truth. It shows better performance on structural alignments. Our experiments show that our proposed strategy achieves superior performance as an in-distribution and out-of-distribution super-resolution strategy.","This study paves a new direction of using deep learning-based continuous super-resolution as a numerical homogenization tool. To this end, we address the issue of efficiently simulating physical phenomena characterized by effects on several length scales. Examples of such phenomena are flow and transport through porous media. To reliably predict such phenomena, information on the micro-scale is necessary. However, if, e.g., flow through soil is considered, we are usually only interested in some macroscopic outcome, such as the final macroscopic location of some contaminant in a field. In mathematical terms, such processes are described by partial differential equations (PDEs). More precisely, the equilibrium state of a flow process is characterized by a function u that solves the PDE \left\{\begin{aligned} -\operatorname{div}(A\nabla u)&=f&&\quad\text{ in }% \Omega,\\ u&=0&&\quad\text{ on }\partial\Omega,\end{aligned}\quad\right. (1) where \Omega\subset\mathbb{R}^{d} is a bounded convex domain in d dimensions, A\colon\Omega\to\mathbb{R} is a scalar-valued coefficient, and f\colon\Omega\to\mathbb{R} is a source or sink. Here, the multi-scale character is encoded in the structure of A, which is strongly heterogeneous on a fine scale. Since problems of the type (1) are not only relevant in soil science [1, 2], oil recovery [3] (and the references therein), drug development [4], and medicine (where the porous medium could, e.g., be a brain [5] or the skin of an individual [6]), but in many disciplines of science and engineering, many approaches in the literature try to tackle them. Mathematical approaches. From a mathematical perspective, (1) cannot be approximated by standard numerical schemes such as finite element, finite volume, or finite difference schemes on the coarse scale of interest since their respective meshes would need to resolve the fine-scale structure. This would result in an unfeasible number of degrees of freedom and computational demands that forbid the application of these techniques to technically relevant problems. However, there are mathematically rigorous and efficient approaches to tackle this issue, such as [7, 8, 9, 10]. These approaches are based on analytical homogenization theory, see [11, 12], and the references therein, that require the coefficient to exhibit specific properties (such as periodicity or scale separation) that are rarely met in practice. Alternatively, heuristic approaches try to infer easy-to-compute homogeneous problems from statistical considerations [13]. A promising and reliable alternative to the approaches above are computational multi-scale methods, referred to as numerical homogenization. The idea is to generate optimally adapted finite element solutions from a coarse-scale finite element solution enriched by suitable fine-scale information. In particular, such approaches provably work for a vast class of heterogeneous media with minimal structural assumptions, see, e.g., [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]. Although these methods are highly efficient for solving a multi-scale problem once the enrichment functions are computed, they need substantial pre-computations per choice of A. Our approach. We propose a different approach in the spirit of numerical homogenization. The goal is to learn a mapping from a (potentially incorrect) coarse-scale solution to an improved (upscaled) solution that appropriately represents fine-scale features for a large class of coefficients A and not for each A individually as in the above techniques. That is, we connect the PDE problem to image super-resolution, which is a fundamental task in image processing to recover fine textures in given images. As tools for ill-posed inverse problem, deep learning-based super-resolution approaches have shown remarkable progress in the past decade and have been widely used in digital entertainment, content creation, and fashion design. Recently, researchers have been interested in applying super-resolution to physical problems, including fine-grained climate prediction [27], remote sensing [28], and coastal simulations [29]. Our contributions can be summarized as follows: • We propose a numerical homogenization strategy via continuous super-resolution (NH-CSR), incorporating a coarse (and possibly) inaccurate approximation of (1) and the coefficient map A to achieve arbitrary fine approximations via super-resolution. • Our proposed framework achieves better visual quality due to wavelet implicit neural representations (WIRE), which use a continuous real Gabor wavelet activation functions as position-based image functions. It better concentrates on space frequency and has excellent biases for representing images. • We also employ the multi-scale implicit image function (MS-IIF) for frequency interpolation. It can faithfully preserve low-frequency information while introducing high-frequency details for better visualization. • To encourage the neural network to learn spatial structural information, we introduce a novel stochastic cosine similarity loss that can grab the non-local pixel correlation in the 2D space, which gains significant improvements in super-resolution. • We empirically demonstrate the supremacy of the proposed NH-CSR over the existing state-of-the-art techniques in quantitative and qualitative measurements. We further experiment with ultra-resolution schemes to show that our method is superior to others in out-of-distribution super-resolution. Figure 1 shows an exemplary comparison of our approach with local implicit image functions (LIIF) [30] on the finite element data super-resolution. We apply the NH-CSR to the low-resolution data and produce multi-scale super-resolved images. Since the model is trained for the setting with an upsampling factor of 16, any upsampling factors that are larger than 16 prescribe an out-of-distribution super-resolution. In the right upper corner (left), we also show the coefficients that are used for the data generation. The remaining plots show visualizations of the upsampled solutions, which are normalized to [0,255], then multiplied by a factor 64 and cut again to the range [0,255] by taking the values modulo 255. These are then used for a coloring using the RGB color space. This is done to highlight pattern differences more clearly. In particular, this leads to the oscillatory structure of the pictures. Overall, we observe that our approach outperforms LIIF in all upsampling scenarios. The manuscript is structured as follows: In the next section, we discuss the available mathematical tools to tackle (1). Afterward, we mention related works in the context of super-resolution. Section III details our approach regarding the possible application of super-resolution to achieve similar results as numerical homogenization approaches. Finally, Section IV comprises illustrations of our numerical results. Figure 1: Continuous super-resolution for finite element data. We use the proposed method to apply multi-scale upscaling to the low-resolution image, both in-distribution (orange region, upscaling factor not larger than 16) and out-of-distribution (green region, upscaling factor larger than 16). We see from the comparison that ours achieves better visualization than the state-of-the-art LIIF [30] approach. Note that the actual values are first normalized to [0, 255], then multiplied by a factor of 64, and taken modulo 255 to again be in the value range between 0 and 255. Finally, the result is converted to the RGB color space using the Matplotlib colormap to better visualize differences. This procedure aims at highlighting fine differences in simulation results and leads to the observed wavy patterns."
https://arxiv.org/html/2411.07347v2,"An Efficient Genus Algorithm 
Based on Graph Rotations","We study the problem of determining the minimal genus of a given finite connected graph. We present an algorithm which, for an arbitrary graph G with n vertices, determines the orientable genus of G in \mathcal{O}({2^{(n^{2}+3n)}}/{n^{(n+1)}}) steps. This algorithm avoids the difficulties that many other genus algorithms have with handling bridge placements which is a well-known issue [32]. The algorithm has a number of properties that make it useful for practical use: it is simple to implement, it outputs the faces of the optimal embedding, it outputs a proof certificate for verification and it can be used to obtain upper and lower bounds. We illustrate the algorithm by determining the genus of the (3,12) cage (which is 17); other graphs are also considered.","1.1. Introduction Say that you have three houses and three utilities, and you must connect each house to each utility via a wire, is there a way to do this so that none of the wires cross each other? This problem can be reframed in terms of graph theory: is K_{3,3} planar? Kuratowski’s theorem [25] tells us that it is not. However, K_{3,3} is toroidal, meaning it can be embedded on a torus without any edges crossing. (a) The complete bipartite graph K_{3,3} (b) K_{3,3} Embedded on a Torus The characterizing property of a torus that allows us to embed K_{3,3} is that it has a hole (unlike surfaces such as a plane or a sphere). This motivates classifying surfaces by their number of holes, that is, their genus g. In these terms, we have seen that the minimum genus surface that K_{3,3} can be embedded has g=1, and we say that K_{3,3}’s genus is 1. In general, a connected multigraph G(V,E) can be embedded on an orientable surface S of genus g if G can be drawn on S without any edges crossing. We say that g is the genus of a graph G if g is the minimum genus surface on which G can be embedded. For genus zero we use the special name “planar” and for genus one we use “toroidal”. Similarly, we have that the complete graph with 7 vertices, K_{7}, has genus 1 and can be embedded on a torus. However, K_{8} cannot be embedded on a torus, and has genus 2. Ringel [36, 37] determined the minimum non-orientable genus for the complete graph K_{n} and also the orientable and non-orientable genus for the complete bipartite graph K_{m,n}. Further, Ringels and Youngs later determined the minimum orientable genus for K_{n}. [38]. \text{genus}(K_{n})=\left\lceil\frac{(n-3)(n-4)}{12}\right\rceil\qquad\text{% genus}(K_{n,m})=\left\lceil\frac{(n-2)(m-2)}{4}\right\rceil However, it is not always so simple to determine the genus of an arbitrary graph. For example, the following are examples of graphs with unknown genus, (a) Cyclotomic 31 Graph (12\leq g\leq 32) (b) Johnson (6,2) Graph (4\leq g\leq 5) The challenge of determining the orientable genus of graphs and constructing their embeddings is a fundamental problem in graph theory, with applications in map colouring, very large scale integration, topology, and electronic circuitry. 1.2. Main Result We present a simple algorithm to determine graph genus, Practical_Algorithm_for_Graph_Embedding (PAGE). The algorithm runs faster than previously implemented algorithms including those presented in [4, 8, 43]. PAGE can easily handle graphs like K_{7} and K_{8} in a few seconds and scales well to graphs with over a hundred edges which it can run in a few minutes. The algorithm also provides upper and lower bounds that it iteratively narrows as it is processing, further enabling practical use cases. Theorem 1 (Main Result). PAGE described in §4 determines the genus of a connected multigraph G(V,E) with runtime of \mathcal{O}({2^{(n^{2}+3n)}}/{n^{(n+1)}}). We emphasize that PAGE is relatively easy to implement; moreover §2 contains a number of concrete examples where the algorithm is used to determine the genus of graphs whose genus was previously unknown. 1.3. Prior Results. In 1963, Youngs established the fundamental principle of graph rotation theory, demonstrating that any embedding of a connected graph can be fully determined by the rotation of edges at its vertices [51]. For a vertex of degree k there are (k-1)! distinct rotations of edges at that vertex and thus for a k-regular graph of n vertices there are (k-1)!^{n} total embeddings into an orientable surface. It has been well established that the problem of determining the genus is NP-hard [47]. However, it is tractable for fixed genus [30, 32, 41]. Mohar has proven the existence of a linear time algorithm for arbitrary fixed surfaces [30]. However, the details to actually implement the algorithm have evaded the field for many decades. Similar efficient algorithms are more of theoretical interest than practical use given their large constant run-time factors, super-exponential scaling factors when genus is not kept fixed, and immense complexity that has prevented real-world implementation. The forbidden minor approach by Robertson and Seymour [41], for instance, is theoretically cubic time because it checks if a given graph has one of the finitely many forbidden minors for the fixed surface [42]. However, the complete list of forbidden minors is only known for planar [25] and projected plane graphs [1]. Even small toroidal graphs (genus 1) cannot be solved with this approach since there are at least 17.5K toroidal minors and likely many more [34]. In practice, the finite number of graph minors scales at an impractical super-exponential rate with the genus. This makes it intractable to compute all the minors (months of super-compute has been thrown at it to no avail). As it stands, the best implemented algorithms are exponential time [32, 7, 34] and the rest are too complex/impractical for implementation and have intractable constant factors. Recently, an algorithm called multi_genus by Gunnar Brinkmann has emerged as a particularly fast method for computing genus on graphs with relatively low genus compared to the vertex degree [6]. It outperforms many previous algorithms, including ours, for graphs where vertex degrees exceed 5, scaling more effectively with edge count and vertex degree. However, our algorithm remains advantageous for graphs with vertices of degree 5 or lower. Additionally, our approach is comparatively simpler to implement and scales more efficiently with the genus, offering an advantageous alternative in cases where the genus is large. As it stands, multi_genus represents the fastest known approach for high-degree graphs, and low relative genus, whereas PAGE provides an effective alternative, and is particularly advantageous for graphs with bounded vertex degree. The other current best algorithms [4, 8, 43] outside of multi_genus can easily compute the genus of graphs the size of K_{6} in less than a second but, even just adding another vertex, K_{7} takes many hours. K_{8} and above is almost entirely out of reach. Their run-times are double exponential in the genus, or in terms of the number of vertices, \mathcal{O}(n(n-1)!^{n})."
https://arxiv.org/html/2411.07985v1,Largest component in Boolean sublattices,"For a subfamily \mathcal{F}\subseteq 2^{[n]} of the Boolean lattice, consider the graph G_{\mathcal{F}} on \mathcal{F} based on the pairwise inclusion relations among its members. Given a positive integer t, how large can \mathcal{F} be before G_{\mathcal{F}} must contain some component of order greater than t? For t=1, this question was answered exactly almost a century ago by Sperner: the size of a middle layer of the Boolean lattice. For t=2^{n}, this question is trivial. We are interested in what happens between these two extremes. For t=2^{g} with g=g(n) being any integer function that satisfies g(n)=o(n/\log n) as n\to\infty, we give an asymptotically sharp answer to the above question: not much larger than the size of a middle layer. This constitutes a nontrivial generalisation of Sperner’s theorem. We do so by a reduction to a Turán-type problem for rainbow cycles in properly edge-coloured graphs. Among other results, we also give a sharp answer to the question, how large can \mathcal{F} be before G_{\mathcal{F}} must be connected?","Sperner’s theorem [23] is a revered result in combinatorics that gives a precise answer to the following problem. Find the largest number \operatorname{La}(n) such that there exists some family \mathcal{F}\subseteq 2^{[n]} of subsets of the n-element ground set [n]=\{1,\dots,n\} no two of which are comparable, i.e. there are no A,B\in\mathcal{F} for which either A\subseteq B or B\subseteq A, satisfying that |\mathcal{F}|=\operatorname{La}(n). Equivalently, find the smallest number \operatorname{La}(n) such that in any family \mathcal{F}\subseteq 2^{[n]} of subsets of [n] with at least \operatorname{La}(n)+1 members there must be a pair A,B\in\mathcal{F} of comparable members, so A\subseteq B or B\subseteq A, otherwise known as a 2-chain. In this context, 2^{[n]} is often referred to as the Boolean lattice. For our purposes, we prefer to cast \operatorname{La}(n) as a threshold. There is some threshold value \operatorname{La}(n) such that in any family \mathcal{F}\subseteq 2^{[n]} of subsets of [n] with at least \operatorname{La}(n)+1 members there must be a 2-chain, while, on the other hand, there exists some family \mathcal{F}\subseteq 2^{[n]} of subsets of [n] with only \operatorname{La}(n) members that contains no 2-chain. Sperner’s theorem [23] asserts that this threshold satisfies \operatorname{La}(n)=\binom{n}{\lfloor n/2\rfloor}=\binom{n}{\lceil n/2\rceil}. The extremal family is \binom{[n]}{\lfloor n/2\rfloor}, all subsets of [n] of size \lfloor n/2\rfloor, or, if n is odd, the symmetric one \binom{[n]}{\lceil n/2\rceil}, the so-called middle layer(s). It has long been known that something remarkable happens at this threshold called supersaturation. Confirming a conjecture of Erdős and Katona, Kleitman [18] showed that in any family \mathcal{F}\subseteq 2^{[n]} of subsets of [n] with at least \operatorname{La}(n)+q members there must be at least q(\lfloor\frac{n}{2}\rfloor+1) 2-chains. The extremal family here takes subsets of [n] of sizes as close as possible to n/2. (See [11, 8, 5, 22] for the recent generalisation of this phenomenon to \ell-chains, \ell\geq 3.) In other words, as we cross the threshold, not only must there be one 2-chain, but also there must be many. A natural question to ask is, what does the space of 2-chains look like? Since by supersaturation the volume of this space must be large, must it exhibit some clustering? When we demand the family to be of size at least 2^{n}, the space of 2-chains is trivially connected, so more fully we ask, what happens between the threshold \operatorname{La}(n)\sim 2^{n+1/2}/\sqrt{\pi n} and the extreme 2^{n}? Does a large cluster in the space appear suddenly? Or more gradually? These musings preoccupy us here. Since these questions are subject to interpretation, we need to ground our definitions. Given a family \mathcal{F}\subseteq 2^{[n]}, we define an auxiliary graph G_{\mathcal{F}} with vertex set \mathcal{F} and two vertices A,B\in\mathcal{F} are adjacent if A,B comprise a 2-chain. Of course, if \mathcal{F}=2^{[n]}, then G_{\mathcal{F}} is a complete graph on 2^{n} vertices. We focus on the following novel prospect. Conjecture 1.1. Let k,n be integers with 0\leq k\leq n such that k=0, k=1 or k has the same parity as n. If for some \mathcal{F}\subseteq 2^{[n]} the components of G_{\mathcal{F}} have order at most 2^{k}, then |\mathcal{F}|\leq 2^{k}\binom{n-k}{\lfloor(n-k)/2\rfloor}. Note that when k=0, then the statement is Sperner’s theorem. The k=1 case is corollary of a result of Katona and Tarján [15]. The k=n case corresponds to the complete case \mathcal{F}=2^{[n]}, so trivially holds. A fuller confirmation of the cases 1<k<n —but especially those k=k(n) for which k=o(n) as n\to\infty— would constitute a natural generalisation/extension of Sperner’s theorem. (By Stirling’s approximation, note that when n-k\to\infty, then for all even n-k we have 2^{k}\binom{n-k}{\lfloor(n-k)/2\rfloor}\sim 2^{k}\frac{2^{n-k}}{\sqrt{(n-k)\pi% /2}}=\frac{2^{n}}{\sqrt{(n-k)\pi/2}} and that this is asymptotically equal to \operatorname{La}(n) when k=o(n) as n\to\infty.) This conjecture seems difficult, but, if it were true, it would be best possible, essentially by a disjoint union of complete subfamilies. Proposition 1.2. For all integers k,n with 0\leq k\leq n, there exists a family \mathcal{F}\subseteq 2^{[n]} of size 2^{k}\binom{n-k}{\lfloor(n-k)/2\rfloor} such that the components of G_{\mathcal{F}} have order 2^{k}. Proof. Consider the family described as follows. Let \mathcal{F}_{s}=\binom{[n-k]}{\lfloor(n-k)/2\rfloor}, i.e. the set of all \lfloor(n-k)/2\rfloor-element subsets of [n-k]. Then let \mathcal{F} be the family of all sets F such that F_{s}\subseteq F\subseteq F_{s}\cup([n]\backslash[n-k]) for some F_{s}\in\mathcal{F}_{s}. Then G_{\mathcal{F}} is made up of exactly \binom{n-k}{\lfloor(n-k)/2\rfloor} components, each of which is identified by a unique collection of \lfloor(n-k)/2\rfloor elements from the set [n-k]. Also note that each component has order 2^{k}, since each element of a component is determined by a unique subset of [n]\backslash[n-k], of which there are 2^{k}. It only remains to show that any two components are disconnected. For this, assume we have X=F_{X}\cup X^{\prime} and Y=F_{Y}\cup Y^{\prime}, with F_{X},F_{Y}\in\mathcal{F}_{s}, and suppose X\subseteq Y. Then F_{X}\subseteq X\subseteq Y\subseteq F_{Y}\cup([n]\backslash[n-k]). But this is a contradiction if F_{X}\neq F_{Y}, since then F_{X} contains some element not contained in F_{Y}, and since F_{X}\subseteq[n-k], this element cannot be contained in F_{Y}\cup([n]\backslash[n-k]) either. ∎ Note these constructions are quite distinct from the extremal families for supersaturation. For the discussion that follows, let us denote by \operatorname{La}(n,t) the size of the largest family \mathcal{F}\subseteq 2^{[n]} such that all components of G_{\mathcal{F}} have order at most t. So \operatorname{La}(n,1)=\operatorname{La}(n), and 1.1 and Proposition 1.2 posit that \operatorname{La}(n,2^{k})=2^{k}\binom{n-k}{\lfloor(n-k)/2\rfloor} if k=0, k=1, or 0\leq k\leq n has the same parity as n. By definition, if a family \mathcal{F}\subseteq 2^{[n]} has at least \operatorname{La}(n,t)+1 elements, then G_{\mathcal{F}} must have a component of order at least t+1. Therefore, understanding the behaviour of \operatorname{La}(n,t) does indeed give us some idea, at least in one interpretation, of how the space of 2-chains looks as we tune the prescribed number of elements between \operatorname{La}(n)+1 and 2^{n}, just as we asked above. In our main result, we establish for a large range of choices of t the asymptotic behaviour of \operatorname{La}(n,t), which aligns with 1.1 and Proposition 1.2. Theorem 1.3. Let t=t(n)=O(2^{g(n)}) for some g(n) that satisfies g(n)=o(n/\log n) as n\to\infty. Then \operatorname{La}(n,t)=(1+o(1))\operatorname{La}(n). What Theorem 1.3 says is that even if we allow for the components in G_{\mathcal{F}} to be quite large as a function of n, the family \mathcal{F} may not grow much larger than a middle layer family. So our theorem is a qualitative strengthening of Sperner’s theorem. The main idea in the proof is to relate the size of the family to the average number of times a permutation \sigma\in S_{n} meets elements of the family. In our proof, we draw an unexpected connection to a Turán-type problem, namely, an upper bound on the edge-density of a graph on two consecutive layers of the Boolean lattice. To obtain this upper bound, we invoke a recent result of Alon, Bucić, Sauermann, Zakharov, Zamir [2] concerning rainbow cycles in proper edge-colourings. We also pursue further partial, but exact results in support of 1.1. In particular, we are interested in problems near the boundary cases k=0 and k=n. We have verified that the conjecture holds if k=1 or k=2 (and n is even). In doing so, we have developed a slight generalisation of the BLYM inequality (Theorem 3.1 below) which implies 1.1 conditional upon the hypothetical shape of the components in G_{\mathcal{F}} of an optimal family \mathcal{F}. (We see this same shape in Proposition 1.2.) This generalisation of the BLYM inequality may be of independent interest. Since the k=n case corresponds to G_{\mathcal{F}} being trivially connected, we found it interesting to pursue the largest size of \mathcal{F} before G_{\mathcal{F}} must be connected. Theorem 1.4. If \mathcal{F}\subseteq 2^{[n]} is such that G_{\mathcal{F}} is disconnected, then |\mathcal{F}|\leq\begin{cases}2^{n}-2^{n/2+1}+2&\text{ if $n$ is even, and }\\ 2^{n}-3\cdot 2^{(n-1)/2}+2&\text{ if $n$ is odd}\end{cases} Moreover, these bounds on |\mathcal{F}| are sharp. Structure of the paper In Section 2, we give the proof of Theorem 1.3. In Section 3, we give and discuss our generalisation of the BLYM inequality and demonstrate its use in the k=2 case of 1.1. We prove Theorem 1.4 in Section 4. Before continuing, we introduce some notation and preliminary results that will be useful. 1.1. Notation and preliminaries The Boolean lattice, often denoted merely by 2^{[n]}, is the partially ordered set (2^{[n]},\subseteq) made up of all subsets of [n]=\{1\dots,n\}, together with the inclusion relation. We are considering subsets \mathcal{F} of the Boolean lattice. When talking about the Boolean lattice, we often use the term layer, which is all subsets of [n] of a given size, say, k, denoted by \binom{[n]}{k}. We imagine the layers to be arranged bottom to top, with the empty set \emptyset at the bottom and the full set [n] at the top. We refer to the height of a given family \mathcal{F}, which is the span of the highest and lowest layers that contain sets in \mathcal{F}. Two distinct sets X,Y\in\mathcal{F} are comparable if either X\subseteq Y or Y\subseteq X, otherwise they are incomparable. We call a tuple X_{1},\dots,X_{k}\in\mathcal{F} of distinct sets such that X_{1}\subseteq\dots\subseteq X_{k} a k-chain, and if k=n we call it a maximal chain. We denote by G_{\mathcal{F}} the undirected graph which has as vertices the elements of \mathcal{F} and an edge XY for every comparable pair X\subseteq Y. We also work with a subgraph G^{\prime}_{\mathcal{F}} of G_{\mathcal{F}} where we only take those edges XY with |Y|=|X|+1. This paper concerns the connectivity and component structure of G_{\mathcal{F}}. We are sometimes ambiguous by the use of phrases like, “the components of \mathcal{F}” or “\mathcal{F} does not contain some substructure”, which should always be read as statements about G_{\mathcal{F}}. We denote by S_{n} the set of permutations of [n]. As it turns out, studying how the permutations in S_{n} interact with the sets in the family \mathcal{F} is useful for results about G_{\mathcal{F}}. Note that we can interchangeably use either permutations or maximal chains, since a permutation \sigma corresponds to the maximal chain (\emptyset,\{\sigma(1)\},\{\sigma(1),\sigma(2)\},\dots,[n]). We will say that a permutation \sigma meets a family \mathcal{F} if the intersection of \mathcal{F} and the maximal chain corresponding to \sigma is nonempty. Throughout, \log n denotes the base-2 logarithm, while \ln n denotes the natural logarithm. In Sections 2.2 and 2.3, we discuss and make use of a classic result of Kruskal [19] and Katona [14]. Theorem 1.5 ([19, 14]). Let \mathcal{F}\subseteq\binom{[n]}{k} for some k. Suppose we write |\mathcal{F}|=\binom{n_{k}}{k}+\binom{n_{k-1}}{k-1}+\dots+\binom{n_{j}}{j} for some n_{k}\geq n_{k-1}\geq\dots\geq n_{j}\geq j. There is a unique way to do this. Then call {\mathcal{S}}_{r} the family of sets of size k-r that are subsets of some element of \mathcal{F}. Then |{\mathcal{S}}_{r}|\geq\binom{n_{k}}{k-r}+\binom{n_{k-1}}{k-1-r}+\dots+\binom{% n_{j}}{j-r}. We also refer occasionally to another classic result, which generalises Sperner’s theorem, called the BLYM inequality and due, independently, to Yamamoto [25], Mešalkin [21], Bollobás [6], Lubell [20]. Theorem 1.6 ([20, 6, 25, 21]). If \mathcal{F}\subseteq 2^{[n]} is a family with no 2-chain, then, writing a_{k}=|\mathcal{F}\cap\binom{[n]}{k}|, \sum_{k=0}^{n}\frac{a_{k}}{\binom{n}{k}}\leq 1."
https://arxiv.org/html/2411.07030v1,Hyperplanes Avoiding Problem and Integer Points Counting in Polyhedra,"In our work, we consider the problem of computing a vector x\in\operatorname{\mathbb{Z}}^{n} of minimum \norm{\cdot}_{p}-norm such that a^{\top}x\not=a_{0}, for any vector (a,a_{0}) from a given finite set \operatorname{\mathcal{A}}\subseteq\operatorname{\mathbb{Z}}^{n}. In other words, we search for a vector of minimum norm that avoids a given finite set of hyperplanes, which is natural to call as the Hyperplanes Avoiding Problem. This problem naturally appears as a subproblem in Barvinok-type algorithms for counting integer points in polyhedra. More precisely, it appears when one needs to evaluate certain rational generating functions in an avoidable critical point.We show that:With respect to \norm{\cdot}_{1}, the problem admits a feasible solution x with \norm{x}_{1}\leq(m+n)/2, where m=\abs{\operatorname{\mathcal{A}}}, and show that such solution can be constructed by a deterministic polynomial-time algorithm with O(n\cdot m) operations. Moreover, this inequality is the best possible. This is a significant improvement over the previous randomized algorithm, which computes x with a guaranty \norm{x}_{1}\leq n\cdot m. The original approach of A. Barvinok can guarantee only \norm{x}_{1}=O\bigl{(}(n\cdot m)^{n}\bigr{)};The problem is \operatorname{N\!P}-hard with respect to any norm \norm{\cdot}_{p}, for p\in\bigl{(}\operatorname{\mathbb{R}}_{\geq 1}\cup\{\infty\}\bigr{)}.As an application, we show that the problem to count integer points in a polytope \operatorname{\mathcal{P}}=\{x\in\operatorname{\mathbb{R}}^{n}\colon Ax\leq b\}, for given A\in\operatorname{\mathbb{Z}}^{m\times n} and b\in\operatorname{\mathbb{Q}}^{m}, can be solved by an algorithm with O\bigl{(}\nu^{2}\cdot n^{3}\cdot\Delta^{3}\bigr{)} operations, where \nu is the maximum size of a normal fan triangulation of \operatorname{\mathcal{P}}, and \Delta is the maximum value of rank-order subdeterminants of A. It refines the previous state-of-the-art O\bigl{(}\nu^{2}\cdot n^{4}\cdot\Delta^{3}\bigr{)}-time algorithm.","Let \operatorname{\mathcal{A}}\subseteq\operatorname{\mathbb{Z}}^{n+1} be a set of pairs (a,a_{0}) with a\in\operatorname{\mathbb{Z}}^{n}\setminus\{\operatorname{\mathbf{0}}\} and a_{0}\in\operatorname{\mathbb{Z}}, and denote m:=\abs{\operatorname{\mathcal{A}}}<\infty. Consider the system \begin{cases}a^{\top}\cdot x\not=a_{0},\quad\forall(a,a_{0})\in\operatorname{% \mathcal{A}}\\ x\in\operatorname{\mathbb{Z}}^{n}.\end{cases} (HyperplanesAvoiding) The system (HyperplanesAvoiding) has infinitely many solutions, and it is interesting to find solutions having small norm (we are mainly interested in the \norm{\cdot}_{1}-norm). The latter motivates the following problem, which is natural to call the Hyperplanes Avoiding Problem: \displaystyle\norm{x}_{p}\to\min \displaystyle\begin{cases}a^{\top}\cdot x\not=a_{0},\quad\forall(a,a_{0})\in% \operatorname{\mathcal{A}}\\ x\in\operatorname{\mathbb{Z}}^{n}.\end{cases} (p-HyperplanesAvoiding) In other words, we are just trying to find an integer vector of the smallest norm that does not lie in any of the m given hyperplanes. It is also interesting to consider the Homogeneous forms of the system (HyperplanesAvoiding) and problem (p-HyperplanesAvoiding), when a_{0}=0 for any (a,a_{0})\in\operatorname{\mathcal{A}}. In this case, we are trying to find an integer vector of the smallest norm that does not lie in any of the m given (n-1)-dimensional subspaces. 1.1 Motivation: The integer Points Counting in Polyhedra The problem (p-HyperplanesAvoiding) naturally appears as a subproblem in algorithms for integer points counting in polyhedra. Let us give a brief sketch of how it appears. Consider a rational polytope \operatorname{\mathcal{P}} defined by a system of linear inequalities. The seminal work of A. Barvinok [5] (see also [3, 4]) proposes an algorithm to count the number of points inside \operatorname{\mathcal{P}}\cap\operatorname{\mathbb{Z}}^{n}, which is polynomial for a fixed dimension. His approach is based on a representation of \operatorname{\mathcal{P}}\cap\operatorname{\mathbb{Z}}^{n} via some rational generating function. More precisely, the Barvinok’s algorithm computes a set of indices \operatorname{\mathcal{I}}, and for each i\in\operatorname{\mathcal{I}}, it computes a number \epsilon^{(i)}\in\operatorname{\mathbb{Z}} and vectors v^{(i)},u_{1}^{(i)},\dots,u_{n}^{(i)}\in\operatorname{\mathbb{Z}}^{n} such that \sum\limits_{x\in\operatorname{\mathcal{P}}\cap\operatorname{\mathbb{Z}}^{n}}z% ^{x}=f_{\operatorname{\mathcal{P}}}(z):=\sum\limits_{i\in\operatorname{% \mathcal{I}}}\epsilon^{(i)}\cdot\frac{z^{v^{(i)}}}{\bigl{(}1-z^{u_{1}^{(i)}}% \bigr{)}\cdot\ldots\cdot\bigl{(}1-z^{u_{n}^{(i)}}\bigr{)}}. (1) Here, the notation z^{x} means z^{x}=z_{1}^{x_{1}}\cdot\ldots\cdot z_{n}^{x_{n}}. The right-hand-side of (1), i.e. the function f_{\operatorname{\mathcal{P}}}(z), is called the short rational generating function of \operatorname{\mathcal{P}}\cap\operatorname{\mathbb{Z}}^{n}. Since the left part of (1) is a finite sum, the point z=\operatorname{\mathbf{1}} is an avoidable critical point of f_{\operatorname{\mathcal{P}}}(z). Therefore, \abs{\operatorname{\mathcal{P}}\cap\operatorname{\mathbb{Z}}^{n}}=\lim\limits_% {z\to\operatorname{\mathbf{1}}}f_{\operatorname{\mathcal{P}}}(z). (2) One possible approach to find this limit, is to compute a vector c\in\operatorname{\mathbb{Z}}^{n} such that c^{\top}u^{(i)}_{j}\not=0, for any i\in\operatorname{\mathcal{I}} and j\in\left\{1,\dots,n\right\}. Note that c is a solution of the system (HyperplanesAvoiding) with \operatorname{\mathcal{A}}=\bigl{\{}u^{(i)}_{j}\bigr{\}}, and m=\abs{\operatorname{\mathcal{A}}}=(n+1)\cdot\abs{\operatorname{\mathcal{I}}}. Using the substitution z_{i}\to e^{\tau\cdot c_{i}}, the function f_{\operatorname{\mathcal{P}}}(z) transforms to the function \hat{f}_{\operatorname{\mathcal{P}}}(\tau), depending on the single complex variable \tau, defined by \hat{f}_{\operatorname{\mathcal{P}}}(\tau)=\sum\limits_{i\in\operatorname{% \mathcal{I}}}\epsilon^{(i)}\cdot\frac{e^{\langle c,v^{(i)}\rangle\cdot\tau}}{% \bigl{(}1-e^{\langle c,u_{1}^{(i)}\rangle\cdot\tau}\bigr{)}\cdot\ldots\cdot% \bigl{(}1-e^{\langle c,u_{n}^{(i)}\rangle\cdot\tau}\bigr{)}}. (3) Now, since \hat{f}_{\operatorname{\mathcal{P}}} is analytical, the limit (2) just equals to the [\tau^{0}]-term of the Tailor’s series for \hat{f}_{\operatorname{\mathcal{P}}}(\tau): \abs{\operatorname{\mathcal{P}}\cap\operatorname{\mathbb{Z}}^{n}}=\lim\limits_% {\tau\to 0}\hat{f}_{\operatorname{\mathcal{P}}}(\tau)=[\tau^{0}]\hat{f}_{% \operatorname{\mathcal{P}}}. We note that it is preferable to calculate the vector c satisfying c^{\top}u^{(i)}_{j}\not=0 with the smallest possible norm, because it will reduce the size of the numbers \langle c,v^{(i)}\rangle and \langle c,u_{j}^{(i)}\rangle, which in turn will speed up practical computations. However, the norm of c does not affect the computational complexity in terms of the number of operations. It only reduces the variable sizes. Assuming that the polyhedron \operatorname{\mathcal{P}} is defined by a system Ax\leq b, for given A\in\operatorname{\mathbb{Z}}^{m\times n} and b\in\operatorname{\mathbb{Q}}^{m}, the computational complexity of the Barvinok’s algorithm in terms of operations number can be bounded by \nu\cdot\bigl{(}O(\log\Delta)\bigr{)}^{n\ln n}, (4) where \nu is the maximum size of a normal fan triangulation of \operatorname{\mathcal{P}}, and \Delta is the maximum value of the rank-order subdeterminants of A. Thus, finding a good solution to (HyperplanesAvoiding) has only effect on the variable sizes of the Barvinok’s algorithm. However, there is an alternative algorithmic approach to integer point counting, which allows obtaining complexity bounds of the type \operatorname{poly}(\nu,n,\Delta). It was developed in a series of works [10, 11, 8, 9, 7].111For the latest perspective see [9], for the parametric case see [7], the paper [8] is a correction of [11]. In this alternative approach, the norm of the solution to (HyperplanesAvoiding) is a multiplicative factor in the bound on its computational complexity. More precisely, the following result was obtained in [9]. Proposition 1 (D. Gribanov, I. Shumilov, D. Malyshev & N. Zolotykh [9]) Assume that, for any collection \operatorname{\mathcal{A}} of vectors of size m, there exists a solution x of the system (HyperplanesAvoiding) with \norm{x}_{1}\leq L(m,n). Assume additionally that such x can be calculated for free. Then the number \abs{\operatorname{\mathcal{P}}\cap\operatorname{\mathbb{Z}}^{n}} can be calculated with O\bigl{(}\nu\cdot L(\nu\cdot n,n)\cdot n^{2}\cdot\Delta^{3}\bigr{)}\quad\text{% operations}. It was shown in [9] that L(m,n)\leq n\cdot m, and such x can be constructed by a randomized polynomial-time algorithm with O(n\cdot m) operations. It means that the counting complexity can be estimated by O\bigl{(}\nu^{2}\cdot n^{4}\cdot\Delta^{3}\bigr{)}. In the current paper, we show that L(m,n)\leq(m+n)/2, and such x can be constructed by a deterministic O(n\cdot m)-time algorithm. The latter yields the counting complexity O\bigl{(}\nu^{2}\cdot n^{3}\cdot\Delta^{3}\bigr{)}, which is the main application of our results. Additionally, we hope that our result can significantly accelerate the evaluation part of the Barvinok-type algorithms. To this end, we propose some experimental results showing that the new algorithm constructs solutions of significantly lower norm than random sampling in a cross-polytope, see Section Experimental Evaluation. Finally, we note that this paper is not considering the dual-type algorithms for counting integer points in polyhedra. A great survey of this approach could be found in the book [12] of J. Lasserre. 1.2 Complexity Model Assumptions All the algorithms that are considered in our work correspond to the Word-RAM computational model. In other words, we assume that additions, subtractions, multiplications, and divisions with rational numbers of the specified size, which is called the word size, can be done in O(1)-time. In our work, we chose the word size to be equal to some fixed polynomial on the input size of the corresponding computational problem. More precisely, considering the problem (p-HyperplanesAvoiding), we assume that the input size is bounded by n\cdot m\cdot(1+\lceil\log_{2}\alpha\rceil), where \alpha is the maximum absolute value of coordinates of a, for a\in\operatorname{\mathcal{A}}. 1.3 Main Results and Related Work Let us summarize our results below. 1. With respect to \norm{\cdot}_{1}, the problem (p-HyperplanesAvoiding) admits a feasible solution x with \norm{x}_{1}\leq(m+n)/2, where m=\abs{\operatorname{\mathcal{A}}}, and we show that such solution can be constructed by a deterministic polynomial-time algorithm with O(n\cdot m) operations, see Theorem 2.3 of Section Approximate Solution via Combinatorial Nullstellensatz. The inequality is the best possible, see the discussion afterward. This is a significant improvement over the previous O(n\cdot m)-time randomized algorithm of [9], which computes x with a guaranty \norm{x}_{1}\leq n\cdot m. In contrast, the original approach of A. Barvinok searches x in the form x=(1,t,t^{2},\dots,t^{n-1}). Since, for each a\in\operatorname{\mathcal{A}}, a^{\top}x=\sum_{i=1}^{n}a_{i}\cdot t^{i-1} is a polynomial of degree at most n-1, there exists a suitable t with t\leq n\cdot m. However, this reasoning can guaranty only \norm{x}_{1}=O\bigl{(}(n\cdot m)^{n}\bigr{)}. 2. For any p\in\bigl{(}\operatorname{\mathbb{R}}_{\geq 1}\cup\{\infty\}\bigr{)}, the problem (p-HyperplanesAvoiding) is \operatorname{N\!P}-hard with respect to any norm \norm{\cdot}_{p}, even in its homogeneous form. See Theorem 3.1 of Section Computational Complexity of the Exact Solution; 3. We show that the problem to calculate the value \abs{\operatorname{\mathcal{P}}\cap\operatorname{\mathbb{Z}}^{n}} for a polyhedron \operatorname{\mathcal{P}} defined by the system Ax\leq b, for agiven A\in\operatorname{\mathbb{Z}}^{m\times n} and b\in\operatorname{\mathbb{Q}}^{m}, can be solved with O\bigl{(}\nu^{2}\cdot n^{3}\cdot\Delta^{3}\bigr{)} operations, where \nu is the maximum size of a normal fan triangulation of \operatorname{\mathcal{P}}, and \Delta is the maximum value of rank-order subdeterminants of A. It refines the O\bigl{(}\nu^{2}\cdot n^{4}\cdot\Delta^{3}\bigr{)}-time algorithm of [9]. See Subsection Motivation: The integer Points Counting in Polyhedra, more specifically, see the discussion alongside Proposition 1; It is easy to see that the guaranty \norm{x}_{1}\leq(m+n)/2 on an existing solution x of the system (HyperplanesAvoiding) is the best possible. Proposition 2 There exists a family of systems (HyperplanesAvoiding) such that \norm{x}_{1}\geq(m+n)/2 for any solution x. Proof Fix some positive integer k. The desired system consists of the constraints x_{i}\not=j, for any i\in\left\{1,\dots,n\right\} and j\in\left\{-k,\dots,k\right\}. So, the total number of constraints is m=(2k+1)\cdot n. It is easy to see that \abs{x_{i}}\geq k+1, for any i\in\left\{1,\dots,n\right\} and any solution x of the system. Therefore, \norm{x}_{1}\geq(k+1)\cdot n=(m+n)/2. However, for the homogeneous form of the system (HyperplanesAvoiding), the asymptotics of the solution quality with respect to the parameter m can be slightly improved. This observation is based on the following result of I. Bárány, G. Harcos, J. Pach, & G. Tardos [2]. Let \operatorname{\mathbb{B}}_{1} be the unit ball with respect to \norm{x}_{1} and g(r) be a minimal number of subspaces needed to cover all points of the set r\cdot\operatorname{\mathbb{B}}_{1}\cap\operatorname{\mathbb{Z}}^{n}. Theorem 1.1 (I. Bárány, G. Harcos, J. Pach, & G. Tardos [2]) There exist absolute constants C_{1} and C_{2} such that C_{1}\cdot\frac{1}{n^{2}}\cdot r^{\frac{n}{n-1}}\leq g(r)\leq C_{2}\cdot 2^{n}% \cdot r^{\frac{n}{n-1}}. Note that the original work [2] contains a more general result concerning arbitrary convex bodies in \operatorname{\mathbb{R}}^{n}, albeit with a worse dependence on n. The Theorem 1.1 is a straightforward adaptation of the original proof to the case of \operatorname{\mathbb{B}}_{1}. As a corollary, it follows that the system (HyperplanesAvoiding) always has a solution with an asymptotics that is slightly better in m, but worse in n. Corollary 1 The system (HyperplanesAvoiding) has a solution x, such that \norm{x}_{1}=O\bigl{(}n^{2}\cdot m^{\frac{n-1}{n}}\bigr{)}. At the same time, the theorem implies that solutions of significantly smaller norm do not exist in general. In particular, it implies that our constructive bound \norm{x}_{1}\leq(m+n)/2 is almost optimal with respect to m even in the homogeneous case. Corollary 2 There exists a system (HyperplanesAvoiding) such that, for any solution x, \norm{x}_{1}=\Omega\bigl{(}\frac{1}{2^{n}}\cdot m^{\frac{n-1}{n}}\bigr{)}."
https://arxiv.org/html/2411.06955v1,Optical orthogonal codes from a combinatorial perspective,"Optical orthogonal codes (OOCs) are sets of (0,1)-sequences with good auto- and cross- correlation properties. They were originally introduced for use in multi-access communication, particularly in the setting of optical CDMA communications systems. They can also be formulated in terms of families of subsets of \mathbb{Z}_{v}, where the correlation properties can be expressed in terms of conditions on the internal and external differences within and between the subsets. With this link there have been many studies on their combinatorial properties. However, in most of these studies it is assumed that the auto- and cross-correlation values are equal; in particular, many constructions focus on the case where both correlation values are 1. This is not a requirement of the original communications application. In this paper, we “decouple” the two correlation values and consider the situation with correlation values greater than 1. We consider the bounds on each of the correlation values, and the structural implications of meeting these separately, as well as associated links with other combinatorial objects. We survey definitions, properties and constructions, establish some new connections and concepts, and discuss open questions.Keywords: Optical orthogonal codes, internal and external differences, auto-correlation, cross-correlation.","Optical orthogonal codes (OOCs) were introduced for use in multi-access communication, particularly in the setting of optical CDMA communications systems ([9, 19]). Since then, they have received considerable attention, with extensions to two and three dimensions ([27, 25, 2]). In its original form, an OOC comprises a family of (0,1)-sequences with good auto- and cross- correlation properties. The sequence definition can be reformulated in terms of families of subsets of \mathbb{Z}_{v}, where the 0 and 1 entries in the sequence correspond to the absence or presence of an element in a set. The correlation properties can then be expressed in terms of bounds on the differences within and between the subsets in the family. This set-theoretic formulation brings OOCs within the framework of various combinatorial constructions and approaches, with a particular relationship to generalisations of difference families ([43]). However, various aspects of the motivating problem ensure that the families of sets corresponding to OOCs have a very different flavour to most commonly-studied combinatorial situations involving internal and external differences. Unlike external difference families (EDFs), OOCs have no requirement that the sets are disjoint. Moreover, unlike classic difference families (DFs) and EDFs, the bounds refer to the number of internal differences within any single set or external differences between any pair of distinct sets, rather than to the multiset union of all internal or external differences. In many studies it is assumed that the auto- and cross- correlation values are equal. In particular, many constructions focus on the case where both correlation values are 1. Here we aim to “decouple” the two correlation values and study the relationship between them. This is mathematically interesting, but also has justification on practical grounds: in [19, 50] it is explained that the auto-correlation constraints for OOCs relate to the issue of synchronisation, while cross-correlation constrains chiefly relate to operation. Moreover, OOCs have strong connections to other combinatorial objects for which only one of the correlation values is important (for example, to constant weight cyclically permutable codes, where only the cross-correlation value is taken into account when considering the minimum distance of the code). In Section 2 we introduce definitions and fundamental properties of OOCs as sequences and subsets. We clarify boundary cases, and consider the question of isomorphisms and equivalences of OOCs. In Section 3 we briefly survey some bounds and constructions in the literature that focus on OOCs with different auto- and cross-correlation values. Following this, in Section 4 examine the relationship of OOCs with other combinatorial objects, again focusing on the roles of the auto- and cross-correlation values in the parameters of these objects. In Section 5 we treat the bounds on auto- and cross-correlation values separately and consider the consequences of these bounds. Finally, in Section 6 we indicate some open questions."
https://arxiv.org/html/2411.06857v1,Phase Transitions via Complex Extensions of Markov Chains,"We study algebraic properties of partition functions, particularly the location of zeros, through the lens of rapidly mixing Markov chains. The classical Lee-Yang program initiated the study of phase transitions via locating complex zeros of partition functions. Markov chains, besides serving as algorithms, have also been used to model physical processes tending to equilibrium. In many scenarios, rapid mixing of Markov chains coincides with the absence of phase transitions (complex zeros). Prior works have shown that the absence of phase transitions implies rapid mixing of Markov chains. We reveal a converse connection by lifting probabilistic tools for the analysis of Markov chains to study complex zeros of partition functions.Our motivating example is the independence polynomial on k-uniform hypergraphs, where the best-known zero-free regime has been significantly lagging behind the regime where we have rapidly mixing Markov chains for the underlying hypergraph independent sets. Specifically, the Glauber dynamics is known to mix rapidly on independent sets in a k-uniform hypergraph of maximum degree \Delta provided that \Delta\lesssim 2^{k/2}. On the other hand, the best-known zero-freeness around the point 1 of the independence polynomial on k-uniform hypergraphs requires \Delta\leq 5, the same bound as on a graph.By introducing a complex extension of Markov chains, we lift an existing percolation argument to the complex plane, and show that if \Delta\lesssim 2^{k/2}, the Markov chain converges in a complex neighborhood, and the independence polynomial itself does not vanish in the same neighborhood. In the same regime, our result also implies central limit theorems for the size of a uniformly random independent set, and deterministic approximation algorithms for the number of hypergraph independent sets of size k\leq\alpha n for some constant \alpha.","More than a few important recent advances in theoretical computer science, in combinatorics and probability theory, have been made possible through locating the zeros of suitably chosen multivariate polynomials. These include improved approximation algorithms for the traveling salesman problem [28, 39], construction of Ramanujan graphs of every degree [52, 53], deterministic approximate counting algorithms for spin systems [8, 56, 46, 47], an algebraic proof of a generalization of the van der Waerden Conjecture [29], a resolution of the long-standing Kadison-Singer conjecture [54], and notably the theory of negatively dependent random variables [11]. Furthermore, there has been a fruitful line of work that exploits a more general form of geometry, notably the development of log-concave polynomials and Lorentzian polynomials, which have led to novel analyses of Markov chains and the resolution of Mason’s conjecture [4, 15]. The development of multivariate stability theory dates back to the famous Lee-Yang program [49] in statistical physics. In their seminal work, Lee and Yang initiated the study of phase transitions through the location of complex zeros of the partition function while also establishing identities relating key physical quantities to the density function of zeros. A key insight is that to understand the macroscopic properties of a system at the thermodynamic limit (that is, as the size of the system tends to infinity), one studies the complex zeros in a neighborhood for any finite systems so as to determine whether the quantities of interest remain analytic or can have a discontinuity. One key quantity of particular interest is the so-called free-energy density. There have also been various generalizations and extensions of Lee-Yang type theorem in statistical physics and combinatorics [44, 30, 67], Chernoff bounds [41], asymptotic normality [38] and central limit theorems [43, 51, 36]. Stability theory for a univariate polynomial is also extensively studied in control theory and can be traced back to the famous Routh-Hurwitz criterion [60, 34]. Roughly speaking, a phase transition occurs when the macroscopic property of a system is not fully determined by local interactions in the thermodynamic limit (that is, there could be multiple phases). To formalize such a notion, three types of mathematical definitions have been studied: (1) Probabilistic: Conditions under which a Gibbs distribution exhibits decay of long-range correlations with respect to distance. (2) Algebraic: Conditions under which a partition function vanishes in the thermodynamic limit. This is also Lee-Yang’s view of phase transition. (3) Algorithmic: Conditions under which a spin system out-of-equilibrium quickly returns to thermal equilibrium; in particular, when does a Glauber dynamics mix rapidly to the Gibbs distribution. Notably, Glauber dynamics can be seen as both a model of physical processes tending to equilibrium, and also an algorithm that can be efficiently simulated. To this date, each of these distinct-looking definitions has seen fruitful algorithmic applications, giving rise to algorithms based on the decay of correlations [68], the absence of zeros [8], and the direct simulation of Glauber dynamics. Numerous efforts have been made to understand the relationship between these three definitions and their relative strengths. For amenable graphs such as lattices, Dobrushin and Shlosman [19, 20] studied the first two types in the form of complete analyticity and showed that they are equivalent. Stroock and Zegarlinski [66] showed the equivalence of all three types via log-Sobolev inequalities. These analyses crucially rely on the amenability of the lattices. In more general settings, Barvinok [9] posed an open question concerning establishing the absence of zeros from the analysis of any rapidly mixing Markov chain. A key challenge, as pointed out by Barvinok, is that while an inverse polynomial spectral gap is sufficient to prove the rapid mixing of Markov chains, a constant radius of zero-free region is often desired for practical applications. Until now, little progress has been made in this specific direction. In contrast, the other direction has seen more success. Assuming decay of correlation in the form of contraction, the absence of zeros follows from the contraction method [57, 47, 65]. Furthermore, [6, 16] showed that contraction also implies rapid mixing of Glauber dynamics. Additionally, the absence of zeros has been shown to imply the decay of correlations (in the form of strong spatial mixing) for self-reducible problems [26, 59] and to imply rapid mixing of Glauber dynamics [1, 17] through a different form of correlation decay known as bounded total influence. Moreover, rapid mixing is known to imply spectral independence [3], which can be seen as a form of bounded correlations. We give a rough summary of the state-of-the-art in Figure 1. zero-freenessrapid mixingdecay of correlations[26, 59][57, 47, 65][6, 16][3][1, 17]This work Figure 1. A rough summary of connections between three types of phase transitions. We do not distinguish the exact form of phase transitions within each type. 1.1. Hypergraph independence polynomial Our motivating example is the independence polynomial on a k-uniform hypergraph. Given a hypergraph H=(V,\mathcal{E}), we say that H is k-uniform if every hyperedge e\in\mathcal{E} has size \left|e\right|=k. The maximum degree \Delta is the maximum number of hyperedges incident to a vertex. An independent set in H is a subset S\subset V of vertices that does not contain any e\in E, that is, every hyperedge e must have at least one endpoint not chosen by S. We use \sigma\in\left\{0,1\right\}^{n} to indicate the set S, meaning that \sigma(v)=1 iff v\in S. Let \mathcal{I}(H) denote the set of independent sets in H. Then, the independence polynomial of H is a generating polynomial in the variables \bm{\lambda}: Z_{H}(\bm{\lambda})=\sum_{\sigma\in\mathcal{I}(H)}\prod_{v:\sigma(v)=1}\lambda% _{v}. When k=2, this is the standard independence polynomial, which has been studied in many branches of mathematics, physics, and computer science. To name a few, Shearer [62] obtained instance-optimal sufficient criterion in Lovász local lemma using the largest root of the independence polynomial; a tight runtime analysis of the celebrated Moser-Tardos algorithm for the algorithmic Lovász local lemma is characterized by the independence polynomial [40]; independence polynomial is also known as the hardcore model for equilibrium of lattice-gas in statistical physics [64]; it is also the first example where a sharp computational complexity of approximate counting and sampling is known [68, 63]. We will refer to the complex zeros of the independence polynomial in \bm{\lambda} as Lee-Yang zeros, as \bm{\lambda}’s are playing the role of external fields here. Henceforth, we denote the above polynomial by Z^{\mathrm{ly}}_{H}(\bm{\lambda}). Scott and Sokal also proposed a soft-core version of independence polynomial [64], with which they derived a weak dependency version of the local lemma. This inspired us to study a soft-core independence polynomial parameterized by the interactions: Z_{H}^{\mathrm{fs}}(\bm{\beta})=\sum\limits_{S\subset V}\prod\limits_{\begin{% subarray}{c}e:e\subset S\end{subarray}}\beta_{e}. Intuitively, for every hyperedge e completely contained in a set S, we assign a “penalty” \beta_{e} for violating the “hard-core” constraint. Zeros in the interaction parameter are also known as Fisher zeros [25]. Compared to Lee-Yang zeros that have been extensively studied, general results for Fisher zeros have been limited until the recent introduction of contraction method [57, 47, 65]. However, these contraction methods crucially rely on a self-avoiding walk construction, which breaks up the uniformity of hypergraphs and is therefore ill-suited for our purpose. The point \bm{\lambda}=1 in Z^{\mathrm{ly}}_{H}(\bm{\lambda}) and the point \bm{\beta}=0 in Z_{H}^{\mathrm{fs}}(\bm{\beta}) are of particular interests, as they correspond to the uniform enumeration of independent sets. This is a prominent example of models where the known regime of zero-freeness, corresponding to an algebraic phase transition, has significantly lagged behind the known regime where efficient algorithms are available. As one of the early examples where approximate counting and sampling algorithms were devised for a constraint satisfaction problem under a local lemma type condition, [33] showed that the Glauber dynamics on independent sets of k-uniform hypergraph mixes rapidly provided the maximum degree \Delta\lesssim 2^{k/2}, and this is matched, up to the leading constants, by an earlier NP-hardness result for \Delta\gtrsim 2^{k/2} in [14]. Since then, several developments have followed suit, including perfect samplers [32, 58] and a local sampler [23], all within similar regimes, albeit with poly(k) factors. Remarkably, the latter can be derandomized to yield a deterministic approximate counting algorithm. For zero-freeness results, however, progress has lagged significantly despite numerous efforts. While there is a rich literature on models with pairwise interactions, understanding of the more physically relevant regime involving higher-order interactions remains limited, and techniques for locating complex zeros for higher-order interactions are less developed. Only recently, Galvin, McKinley, Perkins, Sarantis and Tetali [27] established the existence of a zero-free disk for \lambda centered at the origin with radius \approx\frac{1}{\mathrm{e}\Delta} for hypergraphs of maximum degree \Delta. Later, Bencs and Buys [10] improved this result to match Shearer’s bound for the independence polynomial on graphs. For zero-freeness in a complex neighborhood around the positive real axis, it implicitly follows from the lifting paradigm of [47, 65] applied to the contraction method of [42, 50], that zero-freeness holds around \bm{\lambda}=1 for \Delta\leq 5. This is also carried out more explicitly by [48, 10]. Despite these advances, a significant gap remains compared to the algorithmic transition, which holds up to \Delta\lesssim 2^{k/2}. Essentially, existing techniques for proving zero-freeness are insufficient to exploit the uniformity of hyperedges. 1.2. Our contributions We demonstrate how one can establish the absence of complex zeros through a powerful probabilistic tool in the analysis of Markov chains: percolation applied to a complex extension of Markov chains. We show zero-free regions for both the Lee-Yang zeros of Z^{\mathrm{ly}}_{H}(\bm{\lambda}), and the Fisher zeros of Z_{H}^{\mathrm{fs}}(\bm{\beta}), in regimes that match the algorithmic transition of \Delta\lesssim 2^{k/2} up to poly(k) factors. We also show convergence of a systematic scan Glauber dynamics with complex transition weights in the same regime. To the best of our knowledge, this is the first such result for a complex dynamics. Theorem 1.1 (Lee-Yang zeros of hypergraph independence polynomial). Fix k\geq 2 and \Delta\geq 3. Let 0<\varepsilon<\frac{1}{9k^{5}\Delta^{2}}, and let [0,\lambda_{c,\varepsilon}) be the real segment such that the following holds for all \lambda\in[0,\lambda_{c,\varepsilon}): \left(\frac{\lambda+\varepsilon}{1+\lambda-\varepsilon}\right)^{k/2}<\frac{1}{% 2\sqrt{2}\mathrm{e}\Delta k^{2}}. Let \mathcal{D}_{\varepsilon} be the union of \varepsilon-balls around the segment given by \mathcal{D}_{\varepsilon}=\left\{z\in\mathbb{C}\mid\exists\lambda\in[0,\lambda% _{c,\varepsilon})\text{ s.t. }|z-\lambda|\leq\varepsilon\right\}. Then, for any k-uniform hypergraph H=(V,\mathcal{E}) with maximum degree \Delta, the partition function Z^{\mathrm{ly}}_{H}(\bm{\lambda}) is non-zero for all \bm{\lambda}\in\mathcal{D}^{V}_{\varepsilon}, i.e. \forall\bm{\lambda}\in\mathcal{D}^{V}_{\varepsilon},Z^{\mathrm{ly}}_{H}(\bm{% \lambda})\neq 0. As a corollary, there is no phase transition in \lambda\in[0,1] up till the “algorithmic transition” at \Delta\lesssim 2^{k/2}: Corollary 1.2. Let \delta>0, k\geq 2 and \Delta\geq 3 be constants with \Delta\leq\frac{1-\delta}{2\sqrt{2}\mathrm{e}k^{2}}\cdot 2^{\frac{k}{2}}. For any k-uniform hypergraph H=(V,\mathcal{E}) with maximum degree \Delta, Z^{\mathrm{ly}}_{H}(\lambda)\neq 0 around an open strip containing [0,1]. This is a significant improvement on [27] for k-uniform hypergraphs. We also prove a Fisher zero-free region. Theorem 1.3 (Fisher zeros of hypergraph independence polynomial). Fix k\geq 2 and \Delta\geq 3. Let 0<\varepsilon<\frac{1}{16(k+1)^{5}\Delta^{2}} and \mathcal{D}_{\varepsilon}=\left\{z\in\mathbb{C}\mid\exists\beta\in[0,1]\text{ % s.t. }|z-\beta|\leq\varepsilon\right\} be the union of \varepsilon-balls around [0,1]. If the following condition holds: \sqrt{1+2\varepsilon}2^{-k/2}<\frac{1}{2\sqrt{2}\mathrm{e}\Delta(k+1)^{2}}, then for any k-uniform hypergraph H=(V,\mathcal{E}) with maximum degree \Delta, the partition function Z^{\mathrm{fs}}_{H}(\bm{\beta}) is non-zero for all \bm{\beta}\in\mathcal{D}^{\mathcal{E}}_{\varepsilon}, i.e. \forall\bm{\beta}\in\mathcal{D}^{\mathcal{E}}_{\varepsilon},Z^{\mathrm{fs}}_{H% }(\bm{\beta})\neq 0. Remark 1.4 (implications for deterministic counting). FPTASes for the partition functions Z^{\mathrm{ly}}_{H}(\bm{\lambda}) and Z^{\mathrm{fs}}_{H}(\bm{\beta}) can be derived from the above zero-freeness results by applying Barvionk’s interpolation method [8, 56, 46]. The cumulants, such as the average size and variance of a random independent set, can also be approximated through a similar interpolation [36]. We note that in the regime of Theorem 1.1 specifically at the point \bm{\lambda}=1 (or in the regime of Theorem 1.3 at the point \bm{\beta}=0), an FPTAS for the partition function that counts the number of independent sets in the hypergraph H has already been found in [23], utilizing a different approach based on derandomization. However, as showcased by the following examples, zero-freeness have much broader applications beyond deterministic approximation of partition functions. Through the well-known connection between central limit theorems and the zero-freeness of univariate polynomials [43, 51, 36], we derive central limit theorems for hypergraph independent sets. We consider the Gibbs measure \mu_{H,\bm{\lambda}} associated with Z^{\mathrm{ly}}_{H}(\bm{\lambda}), defined as follows: \forall\sigma\in\mathcal{I}(H),\quad\mu_{H,\bm{\lambda}}(\sigma)=\frac{1}{Z^{% \mathrm{ly}}_{H}(\bm{\lambda})}\prod_{v:\sigma(v)=1}\lambda_{v}. The measure \mu_{H,\bm{\lambda}} can be analytically continued to the complex plane through a connected zero-free region, that is, regions where Z^{\mathrm{ly}}_{H}(\bm{\lambda})\neq 0. Our multivariate zero-freeness for the hypergraph independence polynomial is especially powerful. In principle, one can derive a central limit theorem for any univariate projection of the polynomial. We demonstrate a natural example by giving a quantitative central limit theorem (also known as Berry-Esseen inequality) for the size of a random independent set, drawn from the Gibbs measure of hypergraph independent sets. Our new zero-free region and central limit theorem (CLT) can be lifted to a local CLT as in [36]. We defer the proof to Appendix A. Theorem 1.5 (Central limit theorem for hypergraph independent sets). Fix k\geq 2 and \Delta\geq 3. Let H=(V,\mathcal{E}) be a k-uniform hypergraph with maximum degree \Delta. Let n=\left|V\right|. Fix any \delta>0, \varepsilon\in\left(0,\frac{1}{9k^{5}\Delta^{2}}\right). Let \lambda_{c,\varepsilon} be defined as in Theorem 1.1. For any \lambda\in(\delta,\lambda_{c,\varepsilon}], let I\sim\mu_{H,\lambda}, and define X=\left|I\right|, \bar{\mu}=\mathbb{E}[X] and \sigma^{2}=\mathrm{Var}[X]. Then we have \sigma^{2}=\Theta_{k,\Delta,\varepsilon}(\lambda n) and \sup_{t\in\mathbb{R}}\left|\mathbb{P}[(X-\bar{\mu})/\sigma\leq t]-\mathbb{P}[% \mathcal{Z}\leq t]\right|=O_{k,\Delta,\delta,\varepsilon}\left(\frac{\log n}{% \sqrt{n}}\right), where \mathcal{Z}\sim N(0,1) is a standard Gaussian random variable. Furthermore, let \mathcal{N}(x)=\mathrm{e}^{-x^{2}/2}/\sqrt{2\pi} denote the density of the standard normal distribution, we have \sup\limits_{t\in\mathbb{Z}}\left|\mathbb{P}[X=t]-\sigma^{-1}\mathcal{N}((t-% \bar{\mu})/\sigma)\right|=O_{k,\Delta,\varepsilon}\left(\min\left(\frac{(\log n% )^{5/2}}{\sigma^{2}},\frac{1}{\sigma^{2}}+\frac{\sigma^{2k}(\log n)^{2}}{n^{k-% 1}}\right)\right). As a side note, while there is rich literature on Markov chain central limit theorems (CLT), these do not seem to apply to our context. Specifically, our CLT crucially captures the unimodality of the stationary distribution itself, while Markov chain CLT concerns the sum of samples generated by a Markov chain, and does not seem to distinguish between unimodal and multimodal distributions. Log-Sobolev type inequalities (LSI), if available, would also give concentration tail estimates. But the recent spectral independence framework for establishing LSI for Markov chains requires arbitrary pinnings, which breaks the uniformity of hyperedges. Inspired by [36], we give an FPTAS based on zero-freeness and local CLT, to approximate the number of hypergraph independent sets of size t. The proofs are deferred to Appendix B. Theorem 1.6. Fix k\geq 2, \Delta\geq 3. Let H=(V,\mathcal{E}) be a k-uniform hypergraph with maximum degree \Delta and n=\left|V\right|. Let \varepsilon, \lambda_{c,\varepsilon} be defined as in Theorem 1.1. There exists a deterministic algorithm which, on input H, an integer 1\leq t\leq n\left(1-\frac{1}{1+\lambda_{c,\varepsilon}}\cdot\left(1+\frac{1}{% 4\mathrm{e}\Delta k^{3}}\right)\right), and an error parameter \eta\in(0,1), outputs an \eta-relative approximation to the number of hypergraph independent sets of size t in time (n/\eta)^{O_{k,\Delta,\varepsilon}(1)}. A consequence of the Perron-Frobenius theorem for nonnegative matrices is that any ergodic Markov chain converges to a unique stationary distribution. However, the convergence behavior for complex transition matrices is much less understood. Central to our analysis is the systematic scan Glauber dynamics with complex transition weights (see Definition 3.2 for a formal definition). We show that it converges to the stationary measure in the same regime of Theorem 1.1. Theorem 1.7 (Convergence of systematic scan Glauber dynamics with complex transitions). Under the condition of Theorem 1.1, the systematic scan Glauber dynamics for the complex measure associated with the independence polynomial Z^{\mathrm{ly}}_{H}(\bm{\lambda}) converges. 1.3. Technical overview A few challenges arise when trying to locate complex zeros through a percolation-type argument. To extend the notion of probability measures to the complex plane, one can formally define complex normalized measures as ratios between partition functions. However, a generalization of statements such as “stochastically dominated by a sub-critical branching process” for complex measures appears very challenging. In particular, the monotonicity of probability measures crucially relies on the non-negativity axiom. Our key observation is that a factorization property, which arises in decomposing the Glauber dynamics, can be translated to the complex plane. Our starting point for locating complex zeros of Z^{\mathrm{ly}}_{H}(\bm{\lambda}) is an induction on marginal measures. This approach is implicit in the Lee-Yang theorem and the Asano-Ruelle lemma [49, 7, 61], and is applied more explicitly in the contraction method [57, 47, 65]. We give a quick review below. 1.3.1. Locating complex zeros through marginal measures Here we use the standard edge-wise self-reducibility, consider a hypergraph H=(V,\mathcal{E}) with \mathcal{E}=\{e_{1},e_{2},\dots,e_{m}\}, and let H_{i}=(V,\mathcal{E}_{i}) where \mathcal{E}_{i}=\{e_{1},e_{2},\dots,e_{i}\}. We write the partition function as: Z_{H}=Z_{H_{0}}\prod_{i=1}^{m}\frac{Z_{H_{i}}}{Z_{H_{i-1}}}. To establish Z_{H}\neq 0, it suffices to show \frac{Z_{H_{i}}}{Z_{H_{i-1}}}\neq 0 as it is clear that Z_{H_{0}}\neq 0. The ratio \frac{Z_{H_{i}}}{Z_{H_{i-1}}} corresponds to a marginal measure, which we explain in the context of hypergraph independence polynomial. A hypergraph independent set \sigma in H_{i-1} is an independent set in H_{i} if and only if \sigma_{e_{i}}\neq 1^{k}. Thus, \frac{Z_{H_{i}}}{Z_{H_{i-1}}}=1-\mu_{H_{i-1}}\left(\sigma_{e_{i}}=1^{k}\right), where \mu_{H_{i-1}} is the measure associated with Z_{H_{i-1}}. Then, one can set up an induction on i: assuming that Z_{H_{i-1}}\neq 0, one shows that the marginal measure \mu_{H_{i-1}}\left(\sigma_{e_{i}}=1^{k}\right)\neq 1, this implies Z_{H_{i}}\neq 0. 1.3.2. Marginal measures through information percolation on complex Markov chains Our departure from previous works on the absence of zeros is that we introduce a systematic scan Glauber dynamics to analyze the marginal measures. Introducing Glauber dynamics is crucial in bypassing a barrier to a better zero-free region for the hypergraph independence polynomial: strong spatial mixing does not hold, and a computational tree construction does not preserve the uniformity of hyperedges. Given a measure \mu_{H,\bm{\lambda}}, Glauber dynamics is a canonical way of constructing a Markov chain with stationary measure \mu_{H,\bm{\lambda}}. In particular, the transition matrix of the Glauber dynamics, denoted by P_{\bm{\lambda}}, can also be analytically continued to the complex plane through a connected zero-free region as \mu_{H,\bm{\lambda}} is well-defined. In particular, \mu_{H,\bm{\lambda}} is a left eigenvector for P_{\bm{\lambda}} with eigenvalue 1. The analysis of Markov chains for \lambda\in\mathbb{R} mainly concerns the spectral gap of P_{\bm{\lambda}}, but the spectral gap usually tends to zero as n goes to infinity (in the thermodynamics limit). Instead of attempting a complex extension of spectral theory, we work with the marginal measures generated by powers of the transition matrix P_{\bm{\lambda}}. To get a handle on the marginal measures, we take inspirations from the decomposition of Glauber dynamics that arises in information percolation arguments for Markov chains [45, 33, 32, 58, 23]. In these applications, one starts by formulating the Markov chain on a space-time slab (also known as a witness graph) so that updates, when viewed backward in time, behave like a subcritical percolation. To do so, each step of the dynamics is decomposed into an oblivious update part, which updates a site independent of its neighbors, and an adaptive (non-oblivious) part in which one tries to make up the correct transition probability. By revealing the randomness used in these updates backward in time, we either continue the revealing process due to an adaptive update or terminate it upon encountering an oblivious update. Previously, this percolation argument has primarily been used to bound the mixing rates of classical Markov chains [45, 33] and to analyze the time required for coalescence in grand coupling processes, such as coupling from the past (CFTP) [32, 58] and its variant, coupling towards the past (CTTP) [23]. Our idea is to interpret a decomposition of Glauber dynamics as implicitly a decomposition of the transition matrix P_{\bm{\lambda}}, also into an oblivious part and an adaptive part. Say we “initialize” the Glauber dynamics with a complex measure \mu, viewed as a row vector, and we consider the measure generated by T steps of Glauber dynamics, which is the vector-matrix product \mu P_{\bm{\lambda}}^{T}. By expanding this summation, one can see that, upon encountering an oblivious part, the contribution to the sum “factorizes”. In fact, the result of \mu P_{\bm{\lambda}}^{T} formally corresponds to summing over walks of length T over a space-time slab, where each node is weighted by the corresponding entry in the transition matrix. And the factorization is what leads us to define “independence” for complex measures, which effectively allows us to “terminate the percolation” just as in a standard argument. Central to our analysis is to show that, after running the dynamics for sufficiently long, we can use the “oblivious updates” as a certificate/witness for the measure of any event, in the sense that these witness sequences dominate the complex measure \mu P_{\bm{\lambda}}^{T}. This is formalized as 3.9. These oblivious updates themselves are much easier to analyze as they correspond to a product of complex measures. By identifying the measure generated by \mu P_{\bm{\lambda}}^{T} as contributions from an information percolation on a space-time slab (formally defined as witness graphs in Definition 4.4) , we introduce several dynamics-related quantities — bad vertices, bad components, bad trees (Definition 4.6) — to trace the information percolation process (formally through Lemmas 4.7, 4.8 and 4.10). Then, we express the measure of any configuration by these quantities. When the information percolation process terminates quickly (in the sense of 3.9), we can control the marginal measure using a product of complex measures. 1.3.3. Convergence of the complex systematic scan Glauber dynamics The convergence of Markov chains in the real case is well understood thanks to the Perron-Frobenius theory and the coupling method. It is unclear what the right generalizations to the complex plane should be. Using the information percolation framework, we categorize the percolation processes as follows: (1) processes that terminate before reaching the starting time (Lemma 4.7); (2) processes that do not terminate before reaching the starting time (Lemma 4.10). To establish convergence, it suffices to show that the contributions from type (2) processes diminish to zero. Unlike standard percolation theory where the existence of limits are guaranteed by monotone events, we have to give non-asymptotic bounds before taking an appropriate limit (see Lemma 4.10). Combined, this allows us to show that the measure of any event is dominated by witness sequences (3.9), and we give a proof of convergence in Lemma 3.10."
https://arxiv.org/html/2411.06822v1,Efficient Classical Computation of Single-Qubit Marginal Measurement Probabilities to Simulate Certain Classes of Quantum Algorithms,"Classical simulations of quantum circuits are essential for verifying and benchmarking quantum algorithms, particularly for large circuits, where computational demands increase exponentially with the number of qubits. Among available methods, the classical simulation of quantum circuits inspired by density functional theory—the so-called QC-DFT method, shows promise for large circuit simulations as it approximates the quantum circuits using single-qubit reduced density matrices to model multi-qubit systems. However, the QC-DFT method performs very poorly when dealing with multi-qubit gates. In this work, we introduce a novel CNOT ”functional” that leverages neural networks to generate unitary transformations, effectively mitigating the simulation errors observed in the original QC-DFT method. For random circuit simulations, our modified QC-DFT enables efficient computation of single-qubit marginal measurement probabilities, or single-qubit probability (SQPs), and achieves lower SQP errors and higher fidelities than the original QC-DFT method. Despite limitations in capturing full entanglement and joint probability distributions, we find potential applications of SQPs in simulating Shor’s and Grover’s algorithms for specific solution classes. These findings advance the capabilities of classical simulations for some quantum problems and provide insights into managing entanglement and gate errors in practical quantum computing.","References Bravyi and Gosset [2016] S. Bravyi and D. Gosset, “Improved classical simulation of quantum circuits dominated by Clifford gates,” Phys. Rev. Lett. 116, 250501 (2016). Jozsa [2006] R. Jozsa, “On the simulation of quantum circuits,” (2006), arXiv:0603163 [quant-ph] . Jozsa and Miyake [2008] R. Jozsa and A. Miyake, “Matchgates and classical simulation of quantum circuits,” Proc. R. Soc. A 464, 3089–3106 (2008). Chen et al. [2018] J. Chen, F. Zhang, C. Huang, M. Newman, and Y. Shi, “Classical simulation of intermediate-size quantum circuits,” (2018), arXiv:1805.01450 [quant-ph] . Kissinger, van de Wetering, and Vilmart [2022] A. Kissinger, J. van de Wetering, and R. Vilmart, “Classical simulation of quantum circuits with partial and graphical stabiliser decompositions,” (2022), arXiv:2202.09202 [quant-ph] . Terhal and DiVincenzo [2002] B. M. Terhal and D. P. DiVincenzo, “Classical simulation of noninteracting-fermion quantum circuits,” Phys. Rev. A 65, 032325 (2002). Napp et al. [2022] J. C. Napp, R. L. La Placa, A. M. Dalzell, F. G. Brandao, and A. W. Harrow, “Efficient classical simulation of random shallow 2D quantum circuits,” Phys. Rev. X 12, 021021 (2022). Noh, Jiang, and Fefferman [2020] K. Noh, L. Jiang, and B. Fefferman, “Efficient classical simulation of noisy random quantum circuits in one dimension,” Quantum 4, 318 (2020). Qassim, Wallman, and Emerson [2019] H. Qassim, J. J. Wallman, and J. Emerson, “Clifford recompilation for faster classical simulation of quantum circuits,” Quantum 3, 170 (2019). Jones et al. [2019] T. Jones, A. Brown, I. Bush, and S. C. Benjamin, “QuEST and high performance simulation of quantum computers,” Sci. Rep. 9, 10736 (2019). Broadbent [2015] A. Broadbent, “How to verify a quantum computation,” (2015), arXiv:1509.09180 [quant-ph] . Magesan, Gambetta, and Emerson [2012] E. Magesan, J. M. Gambetta, and J. Emerson, “Characterizing quantum gates via randomized benchmarking,” Phys. Rev. A 85, 042311 (2012). Katsuda, Mitarai, and Fujii [2024] M. Katsuda, K. Mitarai, and K. Fujii, “Simulation and performance analysis of quantum error correction with a rotated surface code under a realistic noise model,” Phys. Rev. Res. 6, 013024 (2024). Bernardi [2023] M. Bernardi, “Efficient mean-field simulation of quantum circuits inspired by density functional theory,” J. Chem. Theory Comput. 19, 8066–8075 (2023). Jozsa [1994] R. Jozsa, “Fidelity for mixed quantum states,” J. Mod. Opt. 41, 2315–2323 (1994). Nielsen and Chuang [2010] M. Nielsen and I. Chuang, Quantum Computation and Quantum Information: 10th Anniversary Edition (Cambridge University Press, 2010). Robbins and Monro [1951] H. Robbins and S. Monro, “A stochastic approximation method,” Ann. Math. Stat. 22, 400–407 (1951). Shor [1994] P. W. Shor, “Algorithms for quantum computation: Discrete logarithms and factoring,” in Proceedings 35th Annual Symposium on Foundations of Computer Science (IEEE, 1994) pp. 124–134. Mermin [2007] N. D. Mermin, Quantum Computer Science: An Introduction (Cambridge University Press, 2007). Vathsan [2015] R. Vathsan, Introduction to Quantum Physics and Information Processing (CRC Press, 2015)."
