URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.10329v1,"Safe Text-to-Image Generation: 
Simply Sanitize the Prompt Embedding","In recent years, text-to-image (T2I) generation models have made significant progress in generating high-quality images that align with text descriptions. However, these models also face the risk of unsafe generation, potentially producing harmful content that violates usage policies, such as explicit material. Existing safe generation methods typically focus on suppressing inappropriate content by erasing undesired concepts from visual representations, while neglecting to sanitize the textual representation. Although these methods help mitigate the risk of misuse to some extent, their robustness remains insufficient when dealing with adversarial attacks.Given that semantic consistency between input text and output image is a fundamental requirement for T2I models, we identify that textual representations (i.e., prompt embeddings) are likely the primary source of unsafe generation. To this end, we propose a vision-agnostic safe generation framework, Embedding Sanitizer (ES), which focuses on erasing inappropriate concepts from prompt embeddings and uses the sanitized embeddings to guide the model for safe generation. ES is applied to the output of the text encoder as a plug-and-play module, enabling seamless integration with different T2I models as well as other safeguards. In addition, ES’s unique scoring mechanism assigns a score to each token in the prompt to indicate its potential harmfulness, and dynamically adjusts the sanitization intensity to balance defensive performance and generation quality. Through extensive evaluation on five prompt benchmarks, our approach achieves state-of-the-art robustness by sanitizing the source (prompt embedding) of unsafe generation compared to nine baseline methods. It significantly outperforms existing safeguards in terms of interpretability and controllability while maintaining generation quality.","Diffusion models [11, 39], as the current state-of-the-art (SOTA) generative paradigm, drive the development of text-to-image (T2I) generation systems. These T2I models are guided by textual prompts and aim to generate realistic images that align with the provided text descriptions, finding widespread application in fields such as art creation [44] and graphic design [51]. Between 2022 and 2023 alone, T2I models generated over 15 billion images [6]. While T2I models demonstrate significant potential in terms of generation performance, they also face risks of unsafe generation. For instance, the BBC reported that AI-generated child sexual abuse materials were being widely sold over the internet, severely violating ethical and legal norms [5]. Therefore, ensuring that the content generated by T2I models adheres to usage policies [8, 25] is an urgent necessity. To address this issue, many commercial T2I online services have implemented various safeguards. For instance, DALL-E [30] uses a Moderation API to detect input prompts that may violate usage policies. Stable Diffusion is equipped with a SafetyChecker [31] to filter out generated images containing explicit content. However, as passive defense mechanisms, input and output moderation typically offer coarse-grained protection, which may cause T2I models to overly intercept and refuse to generate any images. In contrast, safe generation methods [17, 7, 15] focus on erasing inappropriate concepts from the model’s internal representations. These methods aim to suppress the emergence of these concepts in the generated content, rather than indiscriminately blocking inputs or outputs. For instance, SafeGen [17] fine-tunes the model to remove visual representations linked to explicit concepts, ensuring that the generated content does not reflect these concepts. Such methods offer more fine-grained protection for T2I models, enabling them to produce high-quality, safe images even when given prompts with inappropriate concepts. However, despite the fact that existing safe generation methods are capable of suppressing specific target concepts to certain extent, their robustness remains limited [43, 46]. Considering that semantic consistency between the input text and the generated image is a fundamental requirement for T2I models, which means that inappropriate prompts can be the primary source of unsafe generations by T2I models. Existing methods usually focus on fine-tuning the denoiser of a T2I model or modifying the denoising process to erase inappropriate concepts from the visual representation, neglecting the sanitization of inappropriate concepts in the prompt embeddings. This exposes their limitations, as these methods typically only suppress explicitly deleted concepts and lack sufficient generalization ability when handling synonyms or related concepts. Adversarial attacks on T2I models, such as SneakyPrompt [49], exploit this weakness by using synonym substitutions to bypass safeguards, leading to unsafe generation. Therefore, a natural question arises: Could we propose a robust safety generation framework by erasing inappropriate concepts at the source (i.e., prompt embeddings)? Figure 1: ES operates by sanitizing prompt embeddings to generate safe images and providing interpretability to indicate harmful tokens. Our work. In this paper, we provide an affirmative answer by introducing a novel, vision-agnostic safe generation framework, Embedding Sanitizer (ES). As shown in Figure 1, ES operates as a plug-and-play module applied after the text encoder, without necessitating modifications to any T2I model components. This design ensures compatibility, allowing ES to function independently or integrate seamlessly with other protective measures. ES focuses on erasing representations of inappropriate concepts from prompt embeddings, producing sanitized embeddings that guide the T2I model in generating appropriate content. A key feature of ES is its internal scoring network, which assigns a score to each token in an input prompt to gauge its potential harmfulness. For example, when processing the prompt ”A person in a birthday suit,” the token ”suit” receives a high score, indicating it as a significant contributor to inappropriate content generation. During the sanitization phase, ES dynamically adjusts the intensity of sanitization based on these scores, minimizing the impact on low-score (harmless) tokens while preserving strong sanitization effects on high-score (harmful) tokens. The scoring mechanism and adaptive sanitization function not only enhance the identification and management of inappropriate content but also provide interpretability and controllability in the generation process. ES is a plug-and-play deep learning model that accesses only the text encoder during training, without involving other T2I model components. This design supports the use of a customized objective function for efficient end-to-end training. As a vision-agnostic safe generation framework, ES does not rely on image data for training. Instead, training begins with the selection of target concepts for erasure (e.g., nude) and their corresponding anchor concepts (e.g., dressed), providing explicit learning targets for sanitization. To augment training data, we use randomly synthesized prompts to pair toxic prompts P_{t} (e.g., A nude person) with clean prompts P_{c} (e.g., A dressed person). This approach produces abundant, diverse contextual samples, enabling ES to effectively learn to identify and sanitize inappropriate content across various scenarios. During training, a mean squared error loss is employed to encourage ES to align the embedding of the toxic prompt P_{t} with that of the clean prompt P_{c}. Post-training, ES sanitizes input prompts in real-time to produce clean embeddings, ensuring that generated content adheres to usage policies. Contributions. Our main contributions are as follows: • We propose and investigate a vision-agnostic safe generation method, Embedding Sanitizer (ES), designed as a plug-and-play module that sanitizes prompt embeddings in a controllable manner and provides interpretability for inappropriate elements within prompts. This modular design ensures orthogonal compatibility with existing safeguards, allowing ES to function independently or integrate seamlessly with other protective measures. • We introduce a synthetic data-driven training strategy that supplies ES with a nearly limitless variety of contextual training data. By specifying target and anchor concepts for removal, we reformulate the sanitization task as a transformation from target to anchor concepts, enabling efficient end-to-end training of ES. • We conduct a comprehensive evaluation of ES on five prompt datasets and compare it with nine latest safeguard baselines. Results demonstrate that ES achieves SOTA robustness while maintaining content quality, validating that prompt embedding sanitization can significantly enhance resilience to adversarial attacks. To facilitate future studies, we open-source our code in the following repository: https://anonymous.4open.science/r/Embedding-Sanitizer-166E/. Ethical Considerations. For inappropriate content displayed in the manuscript, we apply masking measures to ensure that readers are not directly exposed to sensitive materials, thereby adhering to ethical standards for handling such content."
https://arxiv.org/html/2411.10287v1,Transformers - Messages in Disguise,"Modern cryptography, such as Rivest–Shamir–Adleman (RSA) and Secure Hash Algorithm (SHA), has been designed by humans based on our understanding of cryptographic methods. Neural Network (NN)-based cryptography is being investigated due to its ability to learn and implement random cryptographic schemes that may be harder to decipher than human-designed algorithms. NN-based cryptography may create a new cryptographic scheme that is NN-specific and that changes every time the NN is (re)trained. This is attractive since it would require an adversary to restart its process(es) to learn or break the cryptographic scheme every time the NN is (re)trained. Current challenges facing NN-based encryption include additional communication overhead due to encoding to correct bit errors, quantizing the continuous-valued output of the NN, and enabling One-Time-Pad encryption. With this in mind, the Random Adversarial Data Obfuscation Model (RANDOM) Adversarial Neural Cryptography (ANC) network is introduced. RANDOM is comprised of three new NN layers: the (i) projection layer, (ii) inverse projection layer, and (iii) dot-product layer. This results in an ANC network that (i) is computationally efficient, (ii) ensures the encrypted message is unique to the encryption key, and (iii) does not induce any communication overhead. RANDOM only requires around 100Kb to store and can provide up to 2.5Mb/s of end-to-end encrypted communication.","Cryptography ensures information confidentiality within modern communications systems. A system is secure if it can safeguard the confidentiality of information against all attacks. Deep Learning (DL) is capable of addressing many challenges facing modern communications such as managing spectrum access and utilization [1, 2], aiding in the design of communication systems or designing them altogether [3, 4, 5, 6], and recognizing modulation schemes used within an operating environment [3, 7, 8]. Combining these capabilities with DL’s ability to achieve high accuracy levels exceeding those of human-derived mathematical models even in cases where such models are not tractable or do not exist. Its ability to flourish in increasing information/data availability makes DL attractive in addressing modern communications challenges such as threats [9]. These capabilities also make DL attractive to adversaries who intend to use it to compromise a communications system’s information confidentiality and integrity by persistently and consistently eavesdropping on the communications channel to build a large dataset of intercepted messages from which to extract the unencrypted message, learn the key, learn the encryption process, or combinations thereof. In Adversarial Neural Cryptography (ANC), an encryption/decryption network pair is trained with an eavesdropper attempting to monitor communications. The work in [10] introduces ANC using a Convolutional Neural Network (CNN) while highlighting the challenge of bit-recovery error without explaining why it occurs. This paper introduces the Random Adversarial Neural Data Obfuscation Model (RANDOM) consisting of paired encryption and decryption Neural Networks (NNs) that provide computationally efficient ANC on the edge; however, only one-directional communications are assessed for simplicity. RANDOM performs transformer-based ANC and is an improvement over current ANC approaches because it: (i) does not require quantization of the continuous-valued output of the ANC NN, (ii) incurs zero bit-recovery error, and (iii) and ensures each encrypted message is unique. Quantization must be removed because it adds communication overhead. Ensuring encrypted message uniqueness is essential for the security of the ANC network. An encrypted message must be unique to the key used to encrypt it for an ANC network to implement One-Time Pad (OTP) cryptography [11]. Suppose encrypted messages are similar or identical across keys. In that case, it can be considered that the original encryption key is being reused, disqualifying the ANC network’s use in OTP cryptography. Ensuring bit errors do not occur removes the need for error correction, such as encoding. Encoding schemes like those in IEEE Communications standards incur additional computational costs and communication overhead. RANDOM also introduces three new Neural Network (NN) layers: the projection, inverse projection, and dot-product layers. During training, these layers allow the NN to project the input data into a higher dimension for processing. Traditional NN networks reduce the data input to the network as a feature selection method; this does not work for binary data, which is already the lowest possible dimensionality. The message and key are transformed together at the higher dimension and then projected into the lower dimension, encrypted message. Lastly, RANDOM’s efficiency achieves up to 2.5Mb/s of encrypted communication. The rest of this paper is organized as follows. Sect. II summarizes previous ANC contributions and differences relative to RANDOM. Sect. III introduces the threat model under which CNN, Long Short-Term Memory (LSTM), and RANDOM ANCs are trained, Sect. IV describes RANDOM’s NN layers and architecture along with the assessment experiments descriptions. Sect. V presents RANDOM results and analysis along with results and analysis corresponding to those achieved using a CNN and LSTM for comparative assessment. The paper is concluded in Sect. VI."
https://arxiv.org/html/2411.10279v1,Lateral Movement Detection via Time-aware Subgraph Classification on Authentication Logs,"Lateral movement is a crucial component of advanced persistent threat (APT) attacks in networks. Attackers exploit security vulnerabilities in internal networks or IoT devices, expanding their control after initial infiltration to steal sensitive data or carry out other malicious activities, posing a serious threat to system security. Existing research suggests that attackers generally employ seemingly unrelated operations to mask their malicious intentions, thereby evading existing lateral movement detection methods and hiding their intrusion traces. In this regard, we analyze host authentication log data from a graph perspective and propose a multi-scale lateral movement detection framework called LMDetect. The main workflow of this framework proceeds as follows: 1) Construct a heterogeneous multigraph from host authentication log data to strengthen the correlations among internal system entities; 2) Design a time-aware subgraph generator to extract subgraphs centered on authentication events from the heterogeneous authentication multigraph; 3) Design a multi-scale attention encoder that leverages both local and global attention to capture hidden anomalous behavior patterns in the authentication subgraphs, thereby achieving lateral movement detection. Extensive experiments on two real-world authentication log datasets demonstrate the effectiveness and superiority of our framework in detecting lateral movement behaviors.","Recently, the rapid development of the internet has profoundly transformed the digital environment, introducing unprecedented connectivity and avenues for information access. However, this high degree of connectivity has also triggered a series of significant cybersecurity challenges. In particular, the complexity and stealthiness of Advanced Persistent Threats (APTs) have escalated, posing severe threats to internet security. Among APT attacks, lateral movement attacks are especially prominent and have become a critical issue that urgently needs to be addressed in cybersecurity. Lateral movement is a key stage in the APT attack. After gaining initial access to a node within the network, attackers employ various techniques to progressively infiltrate and expand their control within the network. This process allows attackers to escalate privileges, access sensitive data, and establish persistent access mechanisms (such as backdoors or malware). Fig 1 illustrates two typical lateral movement scenarios in an internal network: 1) External attackers successfully infiltrate an internal network host through phishing emails or vulnerability exploitation. After establishing an initial foothold, they gradually expand their control by scanning other devices in the network and exploiting inherent security weaknesses or by stealing credentials to escalate privileges; 2) Internal members leverage their initial legitimate access or stolen credentials to carry out unauthorized access, progressively extending their control over the system. The ultimate goal of lateral movement is to steal high-value resources, which can lead to significant losses for organizations. Therefore, detecting lateral movement behavior is critical to thwarting APT attacks. Researchers have developed various methods for detecting lateral movement, primarily including methods based on endpoint detection and response (EDR) [1, 2, 3, 4, 5, 6, 7, 8], machine learning (ML) and deep learning (DL) [9, 10, 11, 12, 13, 14, 15, 16, 17]. While these approaches have achieved certain success in detecting lateral movement, they still face several limitations. EDR-based methods can monitor endpoint devices in real-time and detect abnormal behaviors, but they are vulnerable to attackers who can evade detection by using legitimate tools and credentials. Additionally, EDR requires substantial computational resources to process large volumes of data, potentially leading to performance bottlenecks. ML-based methods demonstrate good performance in identifying complex behavioral patterns, but they rely heavily on large amounts of labeled data for training. As attackers continuously modify their tactics, existing models struggle to adapt to new attack means. DL-based methods excel in extracting complex features from data but also require extensive high-quality data for training, and the training process is complex and time-consuming. Moreover, DL-based models have poor interpretability, making it difficult to analyze and understand the detection results, thus posing challenges for security experts in threat response. Figure 1: Two lateral movement scenarios in the enterprise internal network: 1) External threat actors employ advanced persistent threat (APT) techniques to infiltrate the internal network; 2) Internal personnel exploit initial privileges for unauthorized access. Both leverage lateral movement tactics to expand their control and accomplish the objective of exfiltrating sensitive data. Authentication logs serve as a critical data resource for detecting lateral movements, recording every authentication attempt between users, devices, and services, including both successful and failed logins, along with related timestamps, locations, and contextual information. These data provide security analysts and automated systems with a microscopic view of internal network activities, enabling the tracking and analysis of user and device behavioral patterns. Recently, methods based on Graph Neural Networks (GNNs) [18, 19, 20, 21, 22] utilizing authentication logs have achieved notable success in the domain of lateral movement detection. By modeling authentication logs as heterogeneous graph structures and employing GNNs to learn higher-order representations of lateral movement behaviors, these methods have captured complex temporal and relational characteristics of lateral movements, resulting in superior detection performance. However, GNN-based approaches still face specific limitations: 1) Limitations of the detection paradigm. Current GNN-based methods typically model lateral movement detection as an edge classification task based on node representation learning, treating network behaviors in isolation and overlooking the fact that lateral movements often involve a series of continuous actions; 2) Efficiency constraints of large-scale graph computation. The scale of authentication log data is vast, recording numerous user authentication events and network interactions. Learning on such large-scale interaction graphs results in high computational complexity and significant time costs; 3) Imbalanced distribution of authentication log data. In typical network environments, benign interactions vastly outnumber malicious ones, such as lateral movements, leading to a severe imbalance between positive and negative samples. Existing GNN-based detection models are prone to detection bias under these conditions, favoring predictions of common benign behaviors and overlooking rare but highly damaging malicious actions. This imbalance poses serious challenges to the accuracy and robustness of models. Figure 2: Workflow of the LMDetect framework. In this regard, we propose a time-aware multi-scale Lateral Movement Detection framework (LMDetect) — an end-to-end graph neural network model — to characterize behavior patterns in network authentication events and further achieve lateral movement detection. Figure 2 illustrates the workflow of LMDetect. Specifically, we first parse authentication log data, identify network entities and interaction types, and construct a heterogeneous authentication multigraph (HAMG) to describe network activity. We then consider lateral movement detection as a subgraph-level classification problem, designing a time-aware subgraph generator to capture highly relevant information surrounding target authentication events, yielding time-aware authentication subgraphs centered on each target event. This approach enables efficient mini-batch training of LMDetect. To better capture the behavior patterns of authentication events, we further introduce a multi-scale attention encoder as the backbone of our framework, which can effectively learn both local and global dependencies among network entities, providing strong representations of authentication events. The main contributions of this work are as follows: • Data and Task Modeling: We model authentication logs as a heterogeneous authentication multigraph and formulate the lateral movement detection task as a subgraph classification problem, designing a time-aware subgraph generator to enable scalable lateral movement detection. • Powerful Representation Capability: We propose a multi-scale attention encoder that effectively learns both local and global dependencies in network activities, capturing the behavior patterns of authentication events. • State-of-the-Art Performance: Extensive experiments on two real-world authentication log datasets demonstrate the effectiveness and significant superiority of our framework for the lateral movement detection task."
https://arxiv.org/html/2411.10188v1,Reachability Analysis of the Domain Name System,"The high complexity of DNS poses unique challenges for ensuring its security and reliability. Despite continuous advances in DNS testing, monitoring, and verification, protocol-level defects still give rise to numerous bugs and attacks. In this paper, we provide the first decision procedure for the DNS verification problem, establishing its complexity as \mathsf{2ExpTime}, which was previously unknown.We begin by formalizing the semantics of DNS as a system of recursive communicating processes extended with timers and an infinite message alphabet. We provide an algebraic abstraction of the alphabet with finitely many equivalence classes, using the subclass of semigroups that recognize positive prefix-testable languages. We then introduce a novel generalization of bisimulation for labelled transition systems, weaker than strong bisimulation, to show that our abstraction is sound and complete. Finally, using this abstraction, we reduce the DNS verification problem to the verification problem for pushdown systems. To show the expressiveness of our framework, we model two of the most prominent attack vectors on DNS, namely amplification attacks and rewrite blackholing.","The Domain Name System (DNS) is a central component of the Internet’s infrastructure. It translates human-readable domain names, such as www.sigplan.org, into machine-recognizable IP addresses via name resolution, thereby simplifying Internet navigation and resource access for users. This seemingly simple translation service, handled by recursive resolvers, is underpinned by an intricate, hierarchical, globally distributed database. Each organization, like Google or Cloudflare, provides name resolution for its portion or zone of the entire DNS namespace. Operators within each organization manage that zone through, often manually configured, zone files. These zone files, stored on authoritative nameservers, map domain names to IP addresses as well as other types of DNS records, specifying further actions to be taken such as query rewriting or delegation. The IETF (Internet Engineering Task Force) (IETF, 0 15) publishes requests for comments (RFCs), which are technical documents describing the Internet’s technical foundations. To date, hundreds of RFCs (e.g., (Mockapetris, 1987a, b; Rose and Wijngaards, 2012; Elz and Bush, 1997)) have been published that define and guide the design, implementation, and operation of DNS. The sophisticated nature and large scale of DNS pose unique challenges for developers and operators, who have invested considerable efforts to ensure its functional correctness, security, and availability. These challenges often stem from name resolution failures, which result mainly from misconfigurations or attacks, and have historically led to large-scale outages (Whittaker, 2 07; Wikipedia, 2024; Tung, 2024). Of particular concern are denial of service (DoS) vectors, which have been frequently identified over the past decade (Maury, 2015; Afek et al., 2023; Li et al., 2024). In particular, amplification attacks, which exploit the DNS protocol to significantly increase the query load on the victim (resolvers or authoritative nameservers), have seen a surge in recent years (Afek et al., 2020; Liu et al., 2023; Xu et al., 2023; Moura et al., 2021; Duan et al., 2024). Despite continuous advances in DNS testing and monitoring (Bushart and Rossow, 2024; Zhang et al., 2024; ThousandEyes, 2024; Host, 2024; Kakarla and Beckett, 2023; Kakarla et al., 2022), numerous issues, like those just mentioned, still arise from RFCs or protocol-level defects in extensively tested production resolvers and nameservers. This highlights the necessity for a principled, proactive way to mitigate such problems, preferably at an early design stage. Two recent formal efforts (Liu et al., 2023; Kakarla et al., 2020) have attempted to address this issue. GRoot (Kakarla et al., 2020) is the first static verifier for DNS configuration errors, building on the protocol-level DNS resolution semantics. It has been successfully applied to identify misconfigurations in DNS zone files, prior to their deployment. DNSMaude (Liu et al., 2023) is a formal framework, implemented in the Maude (Clavel et al., 2007) language, for both the qualitative (e.g., functional correctness) and quantitative (e.g., amplification) analysis of DNS protocols. It is based on a semantics for the entire end-to-end name resolution, covering essential features such as resolver caching and recursive subqueries, which GRoot abstracts away. Henceforth, when we refer to DNS, we consider this more comprehensive semantics. DNSMaude’s accompanying analyzer has also discovered multiple attacks on DNS with large amplification effects. Research Gaps. Both works have provided partial solutions to the DNS verification (\mathsf{DNSVERIF}) problem where, given the DNS zone files \cal Z, one asks whether a property \phi of interest holds for some behavior of DNS (see Section 5 for its formal definition). Intuitively, the property \phi captures the bad behaviors exhibiting attacks. As every behavior of DNS starts with an initial query from the resolver, on behalf of a client, an immediate challenge in this problem is to explore the infinite space of queries exhaustively. A priori, this was infeasible. GRoot tackles this problem by introducing an equivalence relation on the query space, with finitely many equivalence classes (ECs), such that queries within an EC are resolved in the same manner and yield the same result. While this equivalence is proven to be sound under a simplified DNS semantics, it is unsound with respect to the more realistic semantics involving resolver caching (formalized in DNSMaude). Moreover, GRoot must be supplied with a pre-determined bound on the length of runs in DNS, making it incomplete by design. Additionally, the number of ECs increases explosively with query rewrites, to the order of n^{255} in the size of zone files, due to advanced DNS features like dname rewriting (Rose and Wijngaards, 2012) (see also Section 2.1), which can quickly inflate GRoot’s verification time. Finally, GRoot is inherently incapable of specifying behavioural vulnerabilities like amplification DoS attacks within its semantic framework. DNSMaude also works with GRoot’s ECs but supports specifying common behavioural vulnerabilities. However, it still struggles with attack discovery. Since it uses GRoot’s ECs, it may fail to explore certain bad behaviours that it believes are equivalent to those that have already been explored. It may then incorrectly conclude that there is an absence of bad behaviours, rendering its search procedure incomplete. Moreover, DNSMaude’s search procedure is also incomplete by design; it uses bounded, explicit-state, linear temporal logic (LTL) model checking (Clavel et al., 2007) for functional correctness properties, searching for bugs or potential attacks. DNSMaude also employs statistical model checking (SMC) (Sen et al., 2005) for quantitative properties like amplification. SMC verifies a property probabilistically, up to a given confidence level, by running Monte Carlo simulations. As a result, the search for attacks is again bounded, for example, in the number of simulations, and attacks may be missed.111We have experimented with DNSMaude’s simulation mode, which is used to discover attacks as presented in (Liu et al., 2023). We observe that around 30% of the runs do not achieve the maximum amplification factor, thus potentially overlooking attacks. Overall, neither of these two efforts provides a decision procedure for the \mathsf{DNSVERIF} problem. Moreover, an upper bound on the complexity of this problem still remains unknown (Kakarla et al., 2021). Our Approach. In this paper, we aim to address the above challenges. As the first step, we must faithfully model the semantics of DNS. We consider the well-studied model of a system of recursive communicating processes (Heußner et al., 2010, 2012), and extend it with timers. We call this extension, a system of recursive communicating processes with timers (trCPS). We model the DNS semantics as an instance of a trCPS. Intuitively, the timer keeps track of the age of a resource record in a resolver’s cache. As resolvers are stateful (due to referrals or query rewriting), we utilize stacks to model how they track the resolution process, where each stack entry corresponds to a subquery being resolved. Nameservers just behave as labelled transition systems (LTSs) as they are stateless. DNS is Eager It is well-known that reachability is undecidable for networks of LTSs that contain cycles in the underlying topology, assuming a finite message alphabet and perfect channels (Brand and Zafiropulo, 1983). However, this problem becomes decidable with lossy communications (Finkel and Schnoebelen, 2001; Abdulla and Jonsson, 1996). Since DNS typically operates over UDP (User Datagram Protocol) (Mockapetris, 1987b), which is inherently lossy, we can use lossy channels. Nevertheless, the problem once again becomes undecidable if a cycle contains a process that is a pushdown system (PDS) (Aiswarya, 2020, Thm. 1). Unfortunately, DNS falls within this class, as a resolver is modeled as a PDS with timers and communicates bidirectionally with nameservers (sending queries and receiving answers). Heussner et al. (Heußner et al., 2010, 2012) show that the reachability problem for networks of PDSs over perfect channels and a finite message alphabet, is decidable for eager runs and pointed network topologies. We show that all runs of DNS are eager by design, and that the underlying topology is pointed. This makes DNS fall within this decidable class of systems. Congruence Since DNS operates over both (i) an infinite query space and (ii) infinitely many timer valuations, the eagerness of DNS by itself is insufficient to obtain decidability. This is because an infinite query space also necessitates an infinite message alphabet. We propose a novel congruence on the domain space with finitely many equivalence classes, derived using syntactic congruences for carefully constructed regular languages (Nerode, 1958; Rabin and Scott, 1959), from the given zone file configuration \cal Z. We then show that the domains in any two DNS configurations only need to be syntactically congruent to each other with respect to certain positive prefix-testable languages (see Section 4.1). Moreover, these languages are parametric only in the names and values appearing in \cal Z’s resource records (see Section 3.1), in order to exhibit equivalent behaviours with respect to the DNS semantics. Notably, owing to the nature of these languages, the equivalence is robust to an unbounded number of dname rewrites (unlike GRoot), enabling an unbounded analysis. This effectively yields an abstraction with an equivalent finite message alphabet for the system. Finally, the set of timer valuations are abstracted using a standard region construction. Kernel Bisimulation The domain congruence induces a homomorphism on the semantics of DNS, allowing us to reduce it to a recursive CPS with finitely many control states. However, to show that the reduction is both sound and complete, we introduce a novel, generalized notion of bisimulation, called generalized kernel bisimulation. A crucial property of this bisimulation is the pointwise equivalence of runs from equivalent states up to the equivalence of transitions. We prove that the homomorphism induces a generalized kernel bisimulation on the DNS semantics such that any run in the abstract semantics corresponds to a class of equivalent runs in the original semantics. We use this to establish both soundness and completeness of our abstract model with respect to DNS behavior. Reduction to Reachability As we prove that DNS is an eager trCPS over a pointed network topology, the reduced abstract DNS is also an eager recursive CPS over the same topology. We then reduce the \mathsf{DNSVERIF} problem to an instance of the reachability problem for PDSs, which is known to be decidable (Bouajjani et al., 1997). Thus, we obtain the decidability of this problem. In addition, we establish that it is solvable in doubly-exponential time in the size of the zone file configuration \cal Z. Contributions. Overall, we make the following contributions. • We establish the first decision procedure for the \mathsf{DNSVERIF} problem (Theorem 4.14). Moreover, we show that our algorithm has an upper-bound of \mathsf{2ExpTime} complexity (Lemma 5.3), which was previously unknown. • At the technical level, we initially propose trCPS, a model for networks of recursive communicating processes with timers, as the underlying formal model for DNS (Section 3). But it comprises an infinite message alphabet due to the infinite query space. Owing to the monoidal nature of queries, we devise an algebraic abstraction for them (Section 4.3), with finitely many equivalence classes, using a special class of semigroups (Lemma 4.5). We then introduce a novel generalization of bisimulation for LTSs, weaker than strong bisimulation (Milner, 1980) but incomparable to weak bisimulation, to show that our abstraction is sound and complete (Section 4.2). Both these tools may also be of independent theoretical interest. • We show how our framework can be applied to instantiate two of the most prominent attacks on DNS, namely amplification attacks and rewrite blackholing (Section 6). This demonstrates the versatility of our approach."
https://arxiv.org/html/2411.10174v1,"A Hard-Label Cryptanalytic Extraction of Non-Fully Connected Deep Neural
Networks using Side-Channel Attacks","During the past decade, Deep Neural Networks (DNNs) proved their value on a large variety of subjects. However despite their high value and public accessibility, the protection of the intellectual property of DNNs is still an issue and an emerging research field. Recent works have successfully extracted fully-connected DNNs using cryptanalytic methods in hard-label settings, proving that it was possible to copy a DNN with high fidelity, i.e., high similitude in the output predictions. However, the current cryptanalytic attacks cannot target complex, i.e., not fully connected, DNNs and are limited to special cases of neurons present in deep networks. In this work, we introduce a new end-to-end attack framework designed for model extraction of embedded DNNs with high fidelity. We describe a new black-box side-channel attack which splits the DNN in several linear parts for which we can perform cryptanalytic extraction and retrieve the weights in hard-label settings. With this method, we are able to adapt cryptanalytic extraction, for the first time, to non-fully connected DNNs, while maintaining a high fidelity. We validate our contributions by targeting several architectures implemented on a microcontroller unit, including a Multi-Layer Perceptron (MLP) of 1.7 million parameters and a shortened MobileNetv1. Our framework successfully extracts all of these DNNs with high fidelity (88.4% for the MobileNetv1 and 93.2% for the MLP). Furthermore, we use the stolen model to generate adversarial examples and achieve close to white-box performance on the victim’s model (95.8% and 96.7% transfer rate).","During the last decade, the number of tasks for which Deep Neural Networks (DNNs) have proven their effectiveness has steadily increased, leading to the widespread adoption of these algorithms in a large variety of fields. From computer vision to text translation and images generation, DNNs are everywhere now, and the best models have become valuable intellectual property (IP). In the meantime, the parallel effort from hardware designers have made possible DNNs’ deployment on edge device. However, due to their high value, the IP of the deployed models must be protected against new attacks caused by the embedded context. Since the publication of a first side-channel attack against the IP of an embedded DNN [2], the number of physical-based attacks against DNNs has greatly increased. Several methodologies using side-channel attacks with the objective of the extraction of the DNN’s hyperparameters have been proposed [18, 11]. DNN parameters have also been targeted by physical attacks, via side-channel [32, 19], or through fault injection [27, 13]. These types of attack, targeting the parameters of the model, try to copy the targeted model and perform a model extraction attack. Model extraction is not only a threat to embedded DNNs but to any deployed DNNs. There is therefore a large variety of methods, and even objectives, for theses attacks. We can characterize the two main types of objectives or adversarial goals for model extraction with the terminology introduced in [17]: accuracy-based model extraction and fidelity-based model extraction. The first aims at gaining access to a substitute model with good performance on the task of the targeted model without having to perform the whole training process. The second has for purpose to clone the targeted model to acquire a copy as close as possible to the original. The cloned model can then be used to gain information on the victim’s DNN and potentially mount more powerful attacks against it. In this study, we will consider only fidelity-based model extraction. Jagielski et al. [17] were the first to propose a functional framework for fidelity-based model extraction of 1-layer neural network (NN) using the ReLU (Rectified Linear Unit) function. They exploited the gradient of the DNN to gain access to what they defined as critical points. Such points correspond to inputs where the activation value of one specific neuron is null. These points are visible in DNNs based on the ReLU activation function, since they correspond to points where discontinuities occur in the gradient of the DNN. Using this knowledge, they were able to extract a 1-layer fully-connected NN using the least-square (LSTSQ) algorithm. Due to the similarities with attacks targeting crypto-system, i.e., analysis of a large number of input-output pairs to gain information on a secret value, this method was later characterized as cryptanalytic extraction of DNNs. Both works in [6] and [28] successfully extended this result to deeper architecture, with the method proposed in [6] still achieving the highest fidelity today. One key limitation in the method proposed in [6] was the extraction of the neurons’ sign, for which they used an exhaustive search. This is a critical information, as assigning the wrong sign will deactivate the neuron when it should be activated, and inversely. Shamir et al. argued that their method would not scale well with larger architecture [4]. So they used crafted perturbations activating specific neurons to infer this information [4]. While very effective, all of these methods rely on the fact that the output of the DNN is composed of the full confidence score vector which allows gradient estimation. This corresponds to an ideal case for the attacker, and researches have aimed at removing this assumption in order to move on to more realistic scenarios. Recent work by Chen et al. [9] demonstrated that it was possible to extract DNNs in hard-label settings for small networks. This was further proved by the work of Carlini et al. [5], in which they successfully extracted a four hidden-layer DNN using only the hard-label. However, while these works no longer require the confidence score, they are all designed solely for fully-connected DNNs, excluding any target composed of a non-fully connected layer, such as a pooling layer. In parallel, several methods aiming for fidelity-based model extraction have been proposed using physical attacks. As mentioned before, the most notable examples of such attacks were presented in [27, 13]. They used fault injection to determine the value of a subset of the weights’ bits in the DNN and constraint learning for the rest of the weights. Contrarily to the methods previously mentioned, these are not restricted to fully connected DNNs and, as such, were successful in extracting various complex architectures, e.g., ResNet-34 or VGG-11 [27]. However, these methods are either limited to DRAM platforms performing DNN inference, i.e. Machine-Learning-as-a-Service (MLaaS) platforms, [27] or require access to an open device, on which the attacker has full control (i.e. white-box settings), to find the memory localization of the weights’ bits [13]. We summarize in Table I the results and the threat models from state-of-the-art (SOTA) frameworks performing fidelity-based model extraction using side-channel, fault injection or cryptanalytic methods. To the best of our knowledge, there is no method for extracting complex architectures, i.e., not restricted to fully-connected layers, without requiring an access to the confidence scores and an open device. Contributions: In this paper, we present an efficient framework, combining black-box side-channel attacks and cryptanalytic-based extraction of DNNs in a hard-label setting. We propose a new method to acquire critical points using a side-channel attack instead of the output of the DNN. This allows the attack to be performed in a hard-label settings, and offers higher precision than previous methodologies in extracting weights, resulting in higher fidelity. Additionally, our framework is not impacted by the output’s data format, and we are the first to propose results of cryptanalytic extraction with both 32- and 64-bit data. Furthermore, we introduce an alternative methodology to determine the sign of each neuron requiring only one hypothesis by layer. Finally, the main advantage of our method over other cryptanalytic extraction frameworks is the ability of the side-channel attacks to subdivide the DNN at each activation functions. Using this result, we improve the SOTA by targeting complex DNNs composed of non-fully connected layers, such as depth-wise separable convolution or pooling layers, in a MobileNetv1 architecture. We summarize the major contributions of our work as follows: • We present a new black-box side-channel attack on a constant-time implementation of the ReLU function introduced as a SOTA countermeasure. • We propose a new gradient-free extraction method improving both the precision on the weights’ extraction and the robustness to special cases of neurons. • We introduce a new method to infer the sign of the neurons without access to the confidence scores and requiring only the testing of one hypothesis by layer. • We build an end-to-end framework capable of extracting DNNs with high fidelity and not limited to fully-connected layers. • To prove the practicability of our contributions, we validate them through comparison against other SOTA frameworks, and by targeting a shortened version of MobileNetv1 embedded on an STM32F767ZI using the X-Cube-AI framework. We provide an implementation of our code at https://github.com/X. TABLE I: Overview of the state-of-the-art of fidelity-based model extraction attacks. Approach Attack type Full extraction (weight + bias) Hard-label setting Not restricted to fully connected DNN Random queries DNN’s datatype tested Targeted architecture (Most complex) ICML’20 [28] Cryptanalytic ✓ ✗ ✗ ✓ 64-bit float MLP 10-20-20-1 Crypto’20 [6] Cryptanalytic ✓ ✗ ✗ ✓ 64-bit float MLP 40-20-10-10-1 AC’24 [9] Cryptanalytic ✓ ✓ ✗ ✓ 64-bit float MLP 1024-2-2-1 Preprint [5] Cryptanalytic ✓ ✓ ✗ ✓ 64-bit float MLP 3072-256\times3-64-10 USENIX’19[2] Side-Channel ✗ ✓ ✓ ✓ 32-bit float MLP 784-200\times4-10 ICCAD’23[33] Side-Channel ✓ ✗ ✗ ✓ 64-bit float LeNet5 IEEE S&P’22[27] Fault Injection ✓ ✓ ✓ ✗ 8-bit data ResNet34 and VGG11 ESORICS’23 [13] Fault Injection ✓ ✗ ✓ ✗ 8-bit data CNN 3\times(Conv + Pooling + ReLU)-Linear This work Cryptanlytic and side-channel ✓ ✓ ✓ ✓ 32- and 64-bit float Shortened MobileNetv1 MLP 3072-256\times3-64-10 Attacks not in hard-label settings suppose an access to the confidence scores. Contribution not using random queries use a subset of the training or a testing dataset to perform part of their attacks."
https://arxiv.org/html/2411.10132v1,The Universal Framework for Streamlined Chain Abstraction and Cross-Layer Interaction,"The evolution of the Web3 ecosystem has been hindered by fragmented liquidity and limited interoperability across Layer 1 (L1) and Layer 2 (L2) blockchains, which leads to inefficiencies and elevated costs. Omnichain Web addresses these challenges by introducing a comprehensive framework to unify decentralized networks through its core components: OmniRollups, Proof Network, Ragno Network, and Builder Marketplace. This ecosystem enables seamless cross-chain asset settlement, interoperability, and user-friendly decentralized application (dApp) development, driven by innovative technologies such as modular proof networks and trusted execution environments (TEEs). By integrating advanced zero-knowledge proof systems and compatibility with AI agents, Omnichain Web empowers intent-driven and autonomous functionalities, streamlining liquidity management and user interactions across blockchains. Furthermore, its decentralized marketplace for L1 infrastructure reduces operational overhead and promotes scalable, secure, and efficient cross-chain protocols. As a pioneering solution, Omnichain Web seamlessly connects Web2 and Web3, enabling a holistic and interconnected digital economy.","I INTRODUCTION The evolution of the Web3 space has mirrored the development of national economies and their financial systems. Just as countries have their own currencies and central banks, blockchain networks have their own tokens and foundations. Understanding these parallels provides insights into the decentralized world of cryptocurrencies and blockchain technology, highlighting the critical need for interoperability and robust settlement frameworks [1]. • Global Financial Messaging: The Society for Worldwide Interbank Financial Telecommunication (SWIFT) provides a secure and standardized messaging network for international payments between financial institutions. In a decentralized world, an equivalent to SWIFT is essential for enabling seamless cross-chain communication and interoperability. Protocols like Chainlink [2] and Cosmos [3] aim to bridge different blockchains, facilitating secure and efficient data and asset transfers. • Interoperability and Standardization: Just as SWIFT standardizes financial messages, decentralized interoperability solutions standardize data formats and communication protocols between blockchains. This standardization is crucial for the efficient functioning of the multi-chain ecosystem. Figure 1: Fragmented Space Figure 2: Omniweb The comparison of countries to blockchains and central banks to blockchain foundations highlights the structured yet diverse nature of the Web3 ecosystem. Just as countries require robust financial systems to thrive, blockchain networks need effective governance and interoperability solutions. In this decentralized world, networks like SWIFT and cross-chain settlement mechanisms play a pivotal role in ensuring seamless and secure interactions between different blockchains and rollups. As the Web3 space continues to evolve, these foundational elements will be crucial in driving innovation and achieving a truly interconnected digital economy. The current cross-chain ecosystem faces several key challenges, including: • Liquidity Fragmentation: When solvers execute user intents across various chains, they receive repayments on the originating chains, resulting in capital being scattered across numerous Layer 1 (L1) and Layer 2 (L2) networks. This leads to inefficiencies, as significant portions of liquidity become inactive or ”dead weight.” Thinly spread liquidity diminishes the ability of solvers to effectively manage and respond to cross-chain demands, increasing the operational complexity and costs. • Lack of Unified Capital Flow Management: There is no shared system for coordinating capital flows between different chains, forcing solvers to manage liquidity independently. Each solver operates in isolation, rebalancing funds manually across multiple networks, which is time-consuming and costly. This leads to duplicated efforts and fragmented liquidity management, causing high operational expenses and limiting the scalability of cross-chain activities. • Lack of Platforms for User-Centric Applications: Current cross-chain solutions are heavily infrastructure-focused, lacking an ecosystem that supports the development of user-centric decentralized applications (dApps). The fragmented state of blockchain interoperability makes it difficult for developers to create seamless user experiences across multiple chains. This limits the adoption of Web3 technologies by everyday users, as navigating multiple networks and wallets becomes cumbersome. A unified platform is needed to enable developers to build applications that offer smooth, cross-chain user experiences. • Limited Compatibility with AI Agents: The AI market is maturing, with players like ChatGPT and Perplexity readily available to every PC and mobile user. Recently, OpenAI announced that they have begun working on an AGI model. In this context, within the next 6-12 months, we are likely to see AI agents working and acting on behalf of users, including executing transactions. [7]. As AI agents become more integrated into Web3 solutions, there is a lack of infrastructure that supports their seamless operation across diverse blockchain networks. Existing systems do not provide robust mechanisms for intent-based interactions or automation powered by AI, which can dynamically interact with various chains based on user requirements. This gap hinders the deployment of advanced AI-driven functionalities, such as automated asset management or predictive trading strategies, within decentralized ecosystems. A platform that supports compatibility with AI agents would enhance the ability to create smart, adaptive dApps capable of complex decision-making. Everclear [4] addresses liquidity fragmentation issue by introducing a ”Clearing Layer” for global netting and settlement of capital flows. It functions primarily across Layer 2 (L2) rollups and higher-level chains, coordinating liquidity rebalancing through a decentralized network built on Arbitrum Orbit and utilizing EigenDA for data availability. By aggregating and netting transactions across chains, Everclear can significantly lower the cost of liquidity transfers, offering periodic rebalancing (approximately every 3-6 hours), which helps solvers maintain optimal liquidity levels without constant manual intervention. However, its focus is mainly on L2 chains and selected assets, operating with certain limitations like a permissioned list of supported assets and chains. It also relies on Eigenlayer for security, which currently lacks slashing mechanisms to enforce strong economic security [5]. These constraints limit Everclear’s applicability in a fully decentralized and permissionless context. Omnichain Web extends beyond these limitations by offering true cross-layer interoperability, leveraging OmniRollups—a highly interoperable rollup solution that enables seamless asset settlement across both Layer 1 (L1) and Layer 2 (L2) chains. Through an adapter, it can also connect existing rollups, transforming them into OmniRollups that enable cross-chain asset settlement across multiple L1s. This allows existing rollups to integrate with Omnichain Web’s ecosystem without needing to overhaul their existing infrastructure. The use of OmniRollups and its compatibility adapter create a broader, more decentralized network for asset settlement, bridging both L1 and L2 chains. This expanded functionality offers solvers and network participants a unified liquidity management and cross-chain transaction system without the limitations of a centralized clearing mechanism. Consequently, Omnichain Web provides a robust and scalable solution for fragmented liquidity management, delivering efficiency gains and enhancing the usability of cross-chain protocols. Moreover, to enable seamless compatibility with AI agents, Omnichain Web is developing specialized endpoints and adapters tailored for various AI models. These components will allow AI agents to interact effectively with the decentralized ecosystem. By leveraging a Trusted Execution Environment (TEE), Omnichain Web will provide a secure layer for handling sensitive transactions and executing operations autonomously. This secure setup ensures that AI agents can dynamically interact with the abstracted cross-chain world built by Omnichain Web, facilitating real-time decision-making and intent execution across multiple blockchains without compromising security or efficiency."
https://arxiv.org/html/2411.10034v1,EveGuard: Defeating Vibration-based Side-Channel Eavesdropping with Audio Adversarial Perturbations,"Vibrometry-based side channels pose a significant privacy risk, exploiting sensors like mmWave radars, light sensors, and accelerometers to detect vibrations from sound sources or proximate objects, enabling speech eavesdropping. Despite various proposed defenses, these involve costly hardware solutions with inherent physical limitations. This paper presents EveGuard, a software-driven defense framework that creates adversarial audio, protecting voice privacy from side channels without compromising human perception. We leverage the distinct sensing capabilities of side channels and traditional microphones—where side channels capture vibrations and microphones record changes in air pressure, resulting in different frequency responses. EveGuard first proposes a perturbation generator model (PGM) that effectively suppresses sensor-based eavesdropping while maintaining high audio quality. Second, to enable end-to-end training of PGM, we introduce a new domain translation task called Eve-GAN for inferring an eavesdropped signal from a given audio. We further apply few-shot learning to mitigate the data collection overhead for Eve-GAN training. Our extensive experiments show that EveGuard achieves a protection rate of more than 97\% from audio classifiers and significantly hinders eavesdropped audio reconstruction. We further validate the performance of EveGuard across three adaptive attack mechanisms. We have conducted a user study to verify the perceptual quality of our perturbed audio.","Loudspeakers are omnipresent in today’s technology-based society. Their use extends beyond facilitating phone calls and video conferencing for the exchange of private information. They have been widely integrated into intelligent mobile and IoT devices, enhancing human-machine interaction through speech recognition. The associated use cases are anticipated to reach a market size of \$150.68 billion by 2032 [57, 47]. As people increasingly rely on loudspeaker-equipped devices, voice privacy is becoming increasingly important. Unfortunately, the diverse sensors in intelligent devices are imposing an alarming risk to voice privacy. Although these sensors are not originally designed for voice recording, they can be repurposed by adversaries to serve as side channels to capture voice-induced vibrations, thereby facilitating unauthorized eavesdropping. For example, the prevalent accelerometers on smartphones have been exploited to eavesdrop on voice playout [58, 22]. Millimeter-wave (mmWave) radars can remotely detect vibrations from sound sources and recover speech signals through walls [20, 21, 54, 62, 61, 77]. Such side-channel speech eavesdropping attacks (SSEAs) lead to severe individual privacy breaches [1] and may compromise sensitive organizational intellectual property [12]. Figure 1: Overview of EveGuard, inserting imperceptible adversarial perturbations to the target speech to protect users’ voice communication from multi-sensor eavesdropping attacks. Existing research has devised hardware-based defenses against SSEAs. For instance, jamming-based methods [28, 59, 71] can block adversarial mmWave SSEAs. However, they may degrade the sensing function of legitimate mmWave devices. Moreover, jamming is generally prohibited in non-military applications [13]. Intelligent reflecting surface (IRS) has also been used as a security shield [53, 56], yet it can only protect its immediate vicinity. As for defending against accelerometer-based SSEAs, vibration motors have been used to generate low-amplitude vibrations that disrupt eavesdropping [76]. However, this method may cause user discomfort and hasten the depletion of smartphone batteries. We propose EveGuard, an innovative software-based defense mechanism to protect against privacy leakage from the loudspeaker-generated voice in SSEAs. As shown in Figure 1, EveGuard mitigates SSEA threats by introducing audio adversarial examples to the original audio signals prior to playback. EveGuard ensures that (i) the perturbed speech signals remain natural to human ears and microphones, and (ii) any attempt by SSEAs to capture and reconstruct the perturbed speech will produce content that is difficult to interpret, both for humans and automated speech recognition systems. Note that EveGuard cannot protect voice from a human speaker when SSEAs target eavesdropping on throat vibrations, a challenge also for state-of-the-art (SOTA) attacks [60]. To attain these salient properties, EveGuard must address four main design challenges. First, it is crucial to ensure the effectiveness of perturbations against SSEAs while maintaining the quality of legitimate voice communication. Existing adversarial speech generation methods commonly rely on additive perturbations, which can introduce noticeable, conspicuous noise [74]. In contrast, EveGuard leverages the distinct sensing mechanisms of the side channels versus traditional microphones or human hearing, i.e., the former only captures low-frequency vibrations whereas the latter senses the subtle changes in air pressure. EveGuard devises a two-stage Perturbation Generator Model (PGM) that integrates: (1) finite impulse response (FIR) convolution to perturb the low-frequency attributes of speech while preserving the speech quality and (2) inaudible low-frequency adversarial perturbations (LFAPs) to corrupt the eavesdropped signals. Second, to automate and optimize the perturbation signal generation, EveGuard requires a new differentiable computational model to represent the SSEA. To tackle this challenge, we propose Eve-GAN, a deep generative network aiming at learning an audio-to-SSEA translation that can map the source audio to the targeted eavesdropped audio. Once trained, Eve-GAN serves as a differentiable layer, enabling end-to-end training of our PGM. Yet training Eve-GAN requires collecting sufficient SSEA samples across various attack scenarios. Additionally, obtaining paired training data is tedious as it requires input-output pairs with the same speaker, speech attributes (e.g., prosody), and utterance content. To address this, we leverage advancements in few-shot unsupervised learning [38]. We propose a few-shot, unpaired audio-to-SSEA translation, which learns to convert source audio into eavesdropped audio by referencing an unpaired SSEA sample. By extracting domain features from the few-shot real-world SSEA samples, Eve-GAN facilitates a generalizable conversion applicable to unseen samples during training. Third, the rapid growth of ML empowers attackers to devise sophisticated SSEAs [21, 22, 54, 58]. For instance, the attacker can transcribe private conversations using speech recognition, and identify digits with audio classifiers. However, the defender has no prior knowledge of the SSEA model deployed by the eavesdropper. To overcome this hindrance, we utilize the transferability of adversarial examples, which means perturbations learned to fool an ensemble of diverse surrogate models can also be effective against unknown black-box models [39, 7]. To this end, we first build a set of surrogate ML models based on multiple hypothetical SSEAs. We then concatenate the PGM with Eve-GAN and ensemble surrogate models to encourage the PGM to learn robust perturbations in an end-to-end manner. Finally, an adaptive attacker who knows the existence of EveGuard may attempt to mitigate the effects of the perturbation. Thus, we apply three preventive techniques to PGM as follows: (1) the use of a discriminator inside the PGM to enforce undetectable constraints, (2) style diversification by integrating VAE-GAN [19] into FIR perturbation generator, and (3) ensuring the LFAP generator uses a random latent vector as input to produce diverse LFAP samples. We implement EveGuard by integrating the above solutions. To evaluate EveGuard, we reproduce white-box SSEAs based on representative eavesdropping sensors, mmWave radar, accelerometer, and optical sensor. Built upon these, we extensively validate EveGuard under various attack settings including distance, orientation, materials, hardware configurations, etc. Our experimental results show that EveGuard achieves a protection rate of more than 97\% from SSEA’s digit classifiers and hinders the recovery of eavesdropped audio with an MCD (Mel-Cepstral Distortion) of over 13.4, and a WER (Word Error Rate) of over 68.2\%. To show that our adversarial audio generated by PGM is imperceptible to humans, we verify the indistinguishability through a user study involving 24 participants. The main contributions of EveGuard are as follows. \bullet We introduce EveGuard, a novel software-driven defense framework that leverages black-box adversarial examples to protect loudspeaker-generated voice from SSEAs. \bullet We design a PGM that leverages the unique features of eavesdropping devices to ensure robust perturbations across diverse attack scenarios, including variations in distance, orientation, materials, and hardware configurations. \bullet We develop Eve-GAN, a differential framework that enables our PGM to learn the distribution of adversarial examples end-to-end, incorporating a few-shot, unpaired audio-to-SSEA translation framework to reduce data collection overhead for training Eve-GAN. \bullet We perform extensive experiments to verify the effectiveness of the EveGuard defense, using both objective metrics and subjective user studies. Audio samples are available at https://eveguard.github.io/demo/."
https://arxiv.org/html/2411.09945v1,TEESlice: Protecting Sensitive Neural Network Models in Trusted Execution Environments When Attackers have Pre-Trained Models,"Trusted Execution Environments (TEE) are used to safeguard on-device models. However, directly employing TEEs to secure the entire DNN model is challenging due to the limited computational speed. Utilizing GPU can accelerate DNN’s computation speed but commercial widely-available GPUs usually lack security protection. To this end, scholars introduce TEE-shielded DNN partition (TSDP), a method that protects privacy-sensitive weights within TEEs and offloads insensitive weights to GPUs. Nevertheless, current methods do not consider the presence of a knowledgeable adversary who can access abundant publicly available pre-trained models and datasets. This paper investigates the security of existing methods against such a knowledgeable adversary and reveals their inability to fulfill their security promises. Consequently, we introduce a novel partition before training strategy, which effectively separates privacy-sensitive weights from other components of the model. Our evaluation demonstrates that our approach can offer full model protection with a computational cost reduced by a factor of 10. In addition to traditional CNN models, we also demonstrate the scalability to large language models. Our approach can compress the private functionalities of the large language model to lightweight slices and achieve the same level of protection as the shielding-whole-model baseline.","Deep Neural Networks (DNNs) and recent Large Language Models (LLMs) have emerged as a significant category of intelligent software for user devices. These applications are capable of executing a diverse range of complex AI tasks such as voice assistants (Yadlapally et al., 2023), image recognition (Drolia et al., 2017), and natural language processing (Desai et al., 2020). Nevertheless, the deployment of intelligent software on user devices presents a novel attack surface in comparison to cloud-based services: The detailed information of intelligent software (e.g., model weight values) is exposed to potential malicious users of the device. By having access to this white-box information, adversaries can easily achieve high attack accuracy with significantly lower costs for common attacks like Model Stealing (MS) and Membership Inference Attack (MIA) (Hu et al., 2022a; Orekondy et al., 2019; Jagielski et al., 2020; Papernot et al., 2016; Carlini et al., 2019; Leino and Fredrikson, 2020). These attacks pose a serious threat to the security of the intelligent software, its intellectual property, and the sensitive data privacy (e.g. training data privacy) of the software owners. Hence, a primary goal in fortifying on-device intelligent software is to thwart adversaries from obtaining the white-box information, thereby transforming straightforward and effective white-box MS and MIA attacks into black-box (considerably more challenging) scenarios (Hu et al., 2022a; Mo et al., 2020; Hou et al., 2022; Sun et al., 2020). In this paper, we focus on protecting DNN models on clients’ devices, which are equipped with Trusted Execution Environments (TEEs) and low-grade commercial GPUs. TEEs are commonly used to safeguard on-device intelligent software (Hanzlik et al., 2021; Lee et al., 2019b; Kim et al., 2020; Li et al., 2021). Like traditional software is protected, TEEs ensure that sensitive data (e.g., private keys) are kept separate from the system environment, making it inaccessible to formidable adversaries like malicious operating systems and administrators. Compared to other methods of protection at the algorithmic level, such as Multi-Party Computation (MPC) (Juvekar et al., 2018), Homomorphic Encryption (HE) (Gilad-Bachrach et al., 2016), Regularization (Nasr et al., 2018), and Differential Privacy (DP) (Dwork and Roth, 2014), TEE-based security imposes lower computational overhead on mobile and IoT devices while preserving the accuracy of the secured models (Hu et al., 2022a; Tramèr and Boneh, 2019). Nevertheless, applying TEEs directly to safeguard entire DNN models poses challenges because low-grade commercial GPUs (e.g. GeForce RTX 4090 and RTX A6000) do not provide the functionality of TEE. Although some recent high-end GPUs (e.g. Nvidia Hopper GPU Architecture (NVIDIA, 2023)) provide the functionality of confidential computing, their prices are too high for ordinary model users. An Nvidia H100 GPU is over 15\times more expensive than a GeForce RTX 4090111At Jun 2024, the price of an H100 GPU is about $30,000, while the price of a GeForce RTX 4090 is less than $2,000. Attempting to shield the complete DNN model within a TEE (shielding-whole-model) could result in a 50x reduction in the model’s speed. While safeguarding an entire deep learning model using TEEs may not be practical for on-device scenarios, recent research suggests safeguarding the privacy-sensitive and critical components of the model to ensure both high utility and security simultaneously. Specifically, a concept known as TSDP has been proposed. This approach involves splitting a large DNN model into two components: a privacy-sensitive component, which is small and contains vital information, and a privacy-insensitive component, which is larger and holds less critical data. The privacy-sensitive part operates within TEEs, while the privacy-insensitive part runs on GPUs (Mo et al., 2020; Hou et al., 2022; Shen et al., 2022b; Sun et al., 2020). The rationale behind TSDP is akin to securing conventional software with TEEs, where the privacy sensitive portion (e.g., private keys) is compact and can be protected by TEEs, while the larger portion of the software (e.g., the remaining codebase) operates outside of TEEs (Lazard et al., 2018). Current TSDP approaches generally assume that the portion off-loaded to the GPU does not reveal sensitive information of DNN models. These methods employ retraining techniques to show that even if an attacker uses this portion for MS or MIA, the reconstructed DNN model only achieves a similar accuracy to a black-box baseline (Mo et al., 2020; Hou et al., 2022; Shen et al., 2022b; Sun et al., 2020), which is significantly lower than the white-box accuracy. Previous TSDP studies rely on empirical experiments to prove that the disclosed model components do not leak significantly more information than a black-box interface (Hou et al., 2022; Sun et al., 2020; Mo et al., 2020; Shen et al., 2022b). This paper examines the security promises provided by current TSDP solutions in the presence of a more sophisticated and cunning adversary in the age of large language models. Specifically, we explore a realistic threat scenario where the adversary can leverage readily available public information from the Internet, such as pre-trained models and public datasets (Chen et al., 2022a, b; Wang et al., 2018). With the prevalence of large language models, it has become common for software developers to utilize publicly accessible models to accelerate the development of proprietary software. Previous studies have demonstrated that these public models can be exploited to compromise private software (Sitawarin et al., 2023). To undermine TSDP, attackers can use public information to scrutinize outsourced model components and obtain more information on privacy beyond simply analyzing black-box output, thus undermining the security guarantees of TSDP. However, none of the existing methods has thoroughly assessed their security guarantees in the presence of public information. Therefore, we contend that it is crucial to systematically evaluate the security assurances of TSDP solutions under this threat landscape. To investigate the security of TSDP methods, our initial step involves conducting a comprehensive review of the existing literature on TSDP. We analyze publications released from 2018 to 2023 in reputable conferences such as IEEE S&P, MobiSys, ATC, ASPLOS, RTSS, MICRO, AAAI, ICLR, ICML, PETs, MICRO, and TDSC. Each paper’s technical approaches are scrutinized, and we classify them into five distinct categories based on their primary contributions. These categories include fortifying deep layers (①), fortifying shallow layers (②), fortifying high-magnitude weights (③), fortifying intermediate layers (④), and fortifying non-linear layers (⑤). Subsequently, we choose one exemplary paper for each category and proceed to implement its technical methodology. After categorizing the existing TSDP approaches, we perform a thorough security assessment using a more powerful adversary that has access to public-pre-trained models. Both Membership Inference (MS) and Model Inversion Attacks (MIA) are carried out against the representative TSDP solutions we reviewed, and the attack accuracy is compared against two baselines: the black-box baseline (shielding-whole-model) offers the highest security assurance but the lowest utility, while the white-box baseline (where the entire Deep Neural Network model is offloaded outside of TEE) provides the highest utility but lacks security protection. The experiment results reveal that current TSDP methods inadvertently expose significant private information to attackers through offloaded model weights, allowing attacks of almost white-box quality against TEE-protected models. The accuracy of MS attacks on existing TSDP solutions is 3.85\times – 4.56\times higher than that of the black-box (shielding-whole-model) baseline. On the contrary, the unprotected white-box baseline demonstrates a 4.57\times higher accuracy compared to the shielding-whole-model configuration. The results for MIA attacks show a similar trend, with existing TSDP methods exhibiting 1.16\times – 1.36\times higher MIA accuracy than the shielding-whole-model baseline, while the accuracy for the white-box setup is 1.37\times higher. Furthermore, we found that significant challenges were faced in improving the security of established TSDP methods without fundamentally altering their approaches. For example, we evaluated the effectiveness of MS/MIA attacks using various setups of current TSDP techniques. Identifying an optimal configuration that balances a DNN model’s performance with security requirements proved to be particularly challenging. Specifically, achieving a high level of tolerance to attacks requires distinct settings to configure the protected component when protecting different models and datasets. Thus, a thorough empirical process is essential to determine the ideal configuration customized to specific models and datasets within all existing TSDP strategies. However, conducting such empirical analyses is excessively costly due to the large number of possible combinations of models and datasets. During our literature survey and empirical evaluation, we found that the fundamental weakness of existing TSDP approaches is that they follow a training-before-partition strategy. This involves first training a private model with a public pre-trained model and private data, and then separating the model into two parts: a shielded part that runs in TEEs, and an offloaded part that runs out of TEEs. Since training occurs before model partitioning, privacy-related weights may likely pervade the entire model. Therefore, it is hard for existing TSDP solutions to accurately isolate privacy-related weights, creating potential attack surfaces. In order to enhance the security of TSDP solutions against the new threat model, we introduce a novel TSDP framework named TEESlice. This framework effectively separates privacy-sensitive weights from outsourced weights during the inference phase. Unlike the training-before-partition approach used in prior research, TEESlice employs a partition-before-training strategy. This method involves initially dividing a DNN model into a backbone and several private segments, utilizing publicly pre-trained models as the backbone, and then training the segments with private data. Consequently, TEESlice effectively isolates privacy-related weights from offloaded weights and ensures the protection of all privacy-sensitive weights in TEEs. The primary difficulty in implementing the partition-before-training approach lies in guaranteeing that individual segments are of a manageable size for execution in TEEs without compromising on accuracy. To address this challenge, we suggest employing a dynamic pruning method. Initially, the private segments are trained with larger sizes to ensure they possess adequate model capacity for achieving high accuracy. Subsequently, the algorithm automatically adjusts the segment sizes to stay below a specified threshold of accuracy loss. Through this process, TEESlice is able to identify the optimal configuration, or ”sweet spot,” that minimizes the number of segments (computation) within the TEE while preserving the accuracy level of the non-partitioned model. Our evaluation indicates that TEESlice surpasses existing TSDP methods in terms of both security assurance and utility cost. It is challenging for attackers to extract sensitive information through the analysis of model structures, demonstrating that TEESlice achieves a security level equivalent to the shielding-whole-model baseline with a computational cost that is 10\times lower compared to alternative TSDP solutions, in both experimental and real-world scenarios. Additionally, TEESlice attains a high level of security with minimal trade-offs. Statistical analysis reveals no discernible differences in accuracy between the protected TEESlice model and the original unpartitioned model. Furthermore, the outsourced public backbone does not enhance the efficacy of attacks. Our evaluation also shows that TEESlice can effectively protect large language models with LoRA. The contribution of this paper can be summarized as follows: • We systematically evaluate the security guarantee of previous TSDP solutions using two representative attacks, MS and MIA, and reveal the security issues of these solutions. • We illustrate the difficulty in improving the security of previous TSDP approaches without substantially changing their methodologies. • We propose TEESlice, a novel TSDP solution for DNN inference that isolates privacy from off-loaded model parts to provide a strong security guarantee using TEEs and cryptographic primitives. Our detailed evaluation shows that TEESlice offers a high security guarantee with moderate overhead and no accuracy loss. This paper is an extended version of a conference paper (Zhang et al., 2024). The conference paper categorized existing TSDP solutions, evaluated their security on three representative models, and proposed TEESlice on the CNN models. This paper includes additional content compared with the conference paper. First, this paper conducts a more comprehensive review of existing TSDP solutions, including the scenarios, threat model, design insight, evaluated attacks, outsourced data security, and limitations. Second, this paper includes more experiments on the security evaluation of existing TSDP work and demonstrates the scalability of the observation. Third, this paper proposes an extended approach of TEESlice that can be applied to large language models. Our evaluation demonstrates the effectiveness of the approach to protect large language models. Availability. The artifacts are available at (TEE, 2023a) and (TEE, 2024). Overview. In Sec. 2.2, we will introduce the background and the threat model. In Sec. 4, we survey existing TSDP solutions and evaluate their defense effectiveness. Based on the vulnerability in Sec. 4, in Sec. 5, we further reveal that it is difficult to mitigate the vulnerability straightforwardly. In Sec. 6, we summarize the fundamental reason for the weaknesses of existing TSDP and propose our solution, TEESlice. In Sec. 7, we comprehensively evaluate TEESlice with other TSDP solutions. At last, we present threats to validity (Sec. 8), related work (Sec. 9), and discussion (Sec. 10)."
https://arxiv.org/html/2411.09914v1,mmSpyVR: Exploiting mmWave Radar for Penetrating Obstacles to Uncover Privacy Vulnerability of Virtual Reality,"Virtual reality (VR), while enhancing user experiences, introduces significant privacy risks. This paper reveals a novel vulnerability in VR systems that allows attackers to capture VR privacy through obstacles utilizing millimeter-wave (mmWave) signals without physical intrusion and virtual connection with the VR devices. We propose mmSpyVR, a novel attack on VR user’s privacy via mmWave radar. The mmSpyVR framework encompasses two main parts: (i) A transfer learning-based feature extraction model to achieve VR feature extraction from mmWave signal. (ii) An attention-based VR privacy spying module to spy VR privacy information from the extracted feature. The mmSpyVR demonstrates the capability to extract critical VR privacy from the mmWave signals that have penetrated through obstacles. We evaluate mmSpyVR through IRB-approved user studies. Across 22 participants engaged in four experimental scenes utilizing VR devices from three different manufacturers, our system achieves an application recognition accuracy of 98.5% and keystroke recognition accuracy of 92.6%. This newly discovered vulnerability has implications across various domains, such as cybersecurity, privacy protection, and VR technology development. We also engage with VR manufacturer Meta to discuss and explore potential mitigation strategies. Data and code are publicly available for scrutiny and research. 111https://github.com/luoyumei1-a/mmSpyVR/","Virtual reality (VR) gains widespread popularity among enthusiasts and professionals in various activities, ranging from immersive gaming experiences to virtual chatting and online shopping (Wu et al., 2023a; Hu et al., 2023). These users interact with their VR devices through VR actions, such as body and hand controller motion. However, these VR actions inadvertently reveal the user’s privacy, specifically the user’s activity type and keystroke typing. Existing methods for compromising the privacy of VR users fall into two distinct categories: (i) Physically entering the VR user’s environment. (ii) Virtually establishing a connection with the VR devices. In the first category, researchers hack into webcams and place hidden cameras (Wang et al., 2023; Cao et al., 2022) to record VR actions (Luo et al., 2022; Meteriz-Yıldıran et al., 2022). These approaches face constraints due to security barriers and non-line-of-sight scenarios. In the second category, researchers attempt to directly access the internal sensors by hacking the devices (Shi et al., 2021; Yang et al., 2024). The viability of these methods is limited due to the requirement of establishing a connection with the victim device and the fact that the user’s hand movements in the virtual scene do not reflect their actual movements. Recent studies explore the utilization of wireless signals (Ding et al., 2020), such as WiFi and mmWave, for VR privacy spying (Arafat et al., 2021; Mei et al., 2024). However, WiFi-based methods are susceptible to interference from environmental signals, which affects detection accuracy (Qiao et al., 2023; Salim et al., 2024; Wang et al., 2019, 2021; Yang et al., 2021). Moreover, they struggle to achieve the high-precision hand position detection and reconstruction necessary for effective VR privacy spying. Meanwhile, existing mmWave-based posture recognition methods (Santhalingam et al., 2020; Xie et al., 2024) are not directly applicable to VR privacy spying, as these approaches fail to extract VR privacy information from mmWave signals attenuated by obstacles. The challenge lies in the complex relationship between VR user motions and their corresponding input. For instance, if a VR user clicks with an identical click twice but changes the headset orientation, it results in different inputs. Subsequently, existing mmWave-based motion sensing research does not explore the utilization of VR privacy spying. In contrast to the aforementioned approaches, this paper proposes a novel side-channel vulnerability that exploits mmWave radar to penetrate obstacles to spy on VR user’s privacy without physical intrusion and virtual connection with the VR devices. We identify several opportunities to exploit this vulnerability. Firstly, the substantial bandwidth and high frequency provided by mmWave signals offer precision enhancement, improving the accuracy of sensory data (Wang et al., 2024b). Moreover, the penetration capabilities of mmWave signals pose significant threats to the privacy and security of VR devices (Chen et al., 2006). In addition, the unique reflection characteristics of VR controllers and headsets present an opportunity for motion tracking, enhancing the point cloud features and reducing point cloud sparsity. Our system utilizes these opportunities to uncover the user’s privacy. To intercept VR user activity from mmWave signals through obstacles, we face three unique challenges: (i) What impact do indoor obstacles have on millimeter wave signals for VR users? (ii) How to precisely reconstruct user actions from sparse point clouds across obstacles? Due to the sparsity of mmWave point clouds, single-frame actions are insufficient to discern whether a user is inputting characters and words. (iii) How to establish a correspondence between VR actions and the user’s VR privacy information? The same actions at different times correspond to different VR privacy information, requiring contextual information for accurate interpretation. To address these challenges, the proposed mmSpyVR system incorporates a transfer learning-based VR feature extraction module and an attention-based VR privacy spying module. We develop a novel data augmentation technique to mitigate the impact of signal attenuation caused by obstacles, enhancing the system’s robustness in real-world environments. The privacy spying module leverages augmented point clouds of the user’s body for comprehensive activity surveillance, focusing primarily on keyboard input activities. By recognizing VR motion, keyboard layout, position, and key presses, the system monitors various activity types and keystroke inputs across diverse VR environments. A continual learning approach is used to adapt to new activities, while a network with contextual temporal understanding captures VR privacy information embedded in user actions. The model further integrates multi-task learning for key press detection and keystroke prediction, accounting for different orientations, distances, and head movements during input processes. To the best of our knowledge, this is the first system to exploit mmWave signals for spying on VR users’ privacy through obstacles, revealing a new security vulnerability. Our key contributions are as follows: • To the best of our knowledge, mmSpyVR is the first attack on VR users’ privacy via mmWave radar without physical and virtual connection with VR users. We investigate the feasibility of VR spying in various practical system settings and verify the ability of mmWave signals to penetrate obstacles. • Technically, our design incorporates a VR feature extraction module founded on transfer learning to extract VR features despite the presence of obstacles. Additionally, we design an attention-based VR privacy spying model aimed at spying on the private information of VR users. • Experimentally, we evaluate our system on 22 participants in 4 experimental scenes utilizing commodity VR devices from three different manufacturers and collect 12TB data in total. Experimental results show that mmSpyVR achieves 98.5% and 92.6% accuracy in activity type and keystroke spying."
https://arxiv.org/html/2411.09906v1,A Survey of Machine Learning-based Physical-Layer Authentication in Wireless Communications,"To ensure secure and reliable communication in wireless systems, authenticating the identities of numerous nodes is imperative. Traditional cryptography-based authentication methods suffer from issues such as low compatibility, reliability, and high complexity. Physical-Layer Authentication (PLA) is emerging as a promising complement due to its exploitation of unique properties in wireless environments. Recently, Machine Learning (ML)-based PLA has gained attention for its intelligence, adaptability, universality, and scalability compared to non-ML approaches. However, a comprehensive overview of state-of-the-art ML-based PLA and its foundational aspects is lacking. This paper presents a comprehensive survey of characteristics and technologies that can be used in the ML-based PLA. We categorize existing ML-based PLA schemes into two main types: multi-device identification and attack detection schemes. In deep learning-based multi-device identification schemes, Deep Neural Networks are employed to train models, avoiding complex processing and expert feature transformation. Deep learning-based multi-device identification schemes are further subdivided, with schemes based on Convolutional Neural Networks being extensively researched. In ML-based attack detection schemes, receivers utilize intelligent ML techniques to set detection thresholds automatically, eliminating the need for manual calculation or knowledge of channel models. ML-based attack detection schemes are categorized into three sub-types: Supervised Learning, Unsupervised Learning, and Reinforcement Learning. Additionally, we summarize open-source datasets used for PLA, encompassing Radio Frequency fingerprints and channel fingerprints. Finally, this paper outlines future research directions to guide researchers in related fields.","1.1 Background With the vigorous development of information technology promoted by academia and industry, wireless communication techniques have been widely applied in numerous fields, such as aviation navigation, radio and television, transportation, meteorology, fire prevention, flood control, as well as mobile communications [1]. According to forecasts, by the year 2025, it is estimated that there will be 7.49 billion mobile users worldwide111https://www.statista.com/statistics/218984/number-of-global-mobile-users-since-2010/. However, the misuse of wireless devices for illicit cybercriminal activities has been increasing. This can be attributed to the open and broadcast nature of wireless media, which makes it susceptible to various types of attacks [2]. For instance, malicious users exploit vulnerabilities in wireless networks to eavesdrop on transmitted data and obtain sensitive information such as personal data or trade secrets [3]. They may also deceive unsuspecting users by impersonating legitimate devices, tricking them into sharing sensitive information, or facilitating malicious operations [4]. Additionally, attackers can launch Jamming attacks that disrupt the communication between devices, leading to interruptions, data loss, or degraded communication quality [5]. Furthermore, Sybil attackers threaten the reputation and security of wireless networks or systems. These attackers create multiple false identities to manipulate network decision-making processes, monopolize resources, or interfere with the normal functioning of other users [6]. The above security threats have caused security threats to many application scenarios, and may even bring serious economic losses. For example, in vehicles ad hoc networks, the dependence on infrastructure, computing, dynamic characteristics and control technology makes its security threats increase [7, 8, 9]. For another example, the security threats of 6G come from the complexity of network architecture, the diversity of access devices, the surge of data traffic and new security threats[10]. Therefore, it is crucial for individuals and organizations to be aware of these risks and take appropriate measures to identify wireless devices and guarantee the wireless security. 1.2 Cryptography-based Upper-Layers Authentication Mechanisms Currently, authentication mechanisms in wireless communications are achieved through traditional cryptography-based algorithms at the upper-layers [11]. However, these methods are not applicable for emerging application scenarios, such as the Internet of Things (IoT), the sixth-generation (6G) wireless networks, Industrial Internet of Things (IIoT), and smart cities for the following limitations. 1. The cryptography-based authentication techniques are based on computational theories (such as algebraic geometry and discrete mathematics) and are realized with one basic assumption that attackers have limited computational capability [12]. However, this assumption has been gradually broken due to the rapid advancements in cryptanalysis algorithms and computational power [13]. If the root key is leaked, various attacks can compromise the identification system. For example, in the Internet of Vehicles (IoV), malicious nodes can employ Sybil attacks to transmit fake messages, such as incorrect route directions, disturbing networks and posing potential risks to passengers’ lives [14]. 2. Most cryptography-based approaches are vulnerable to replay attacks, where adversaries can recover the physical-layer bit stream and directly deliver the recovered signal to the legal receiver without modifying the upper-layers signaling or cracking the cryptographic algorithms [13]. For instance, an attacker may attempt to record transmitted signals from a legitimate transmitter earlier and subsequently replay the recorded signals to pass authentication. This can lead to the legitimate receiver failing to authenticate and disrupt normal communication [15]. 3. The cryptography-based algorithms necessitate the generation, distribution, and updating of keys, thereby increasing transmission latency [16]. Hence, they are not suitable for numerous latency-sensitive scenarios [17]. For example, health management within intelligent medicine requires patients’ self-management, emphasizing real-time self-monitoring, prompt feedback of health data, and timely medical intervention [18]. Additionally, real-time multivariable statistical system monitoring methods are extensively employed in chemical engineering, automobile production, agricultural monitoring, and other industrial sectors [19]. Failure to guarantee real-time performance may result in significant economic losses and security threats. 4. The cryptography-based identification methods introduce high communication overhead and complexity, particularly undesired for devices with limited computational and store resources, such as massive machine-type communications and Unmanned Aerial Vehicles (UAV) that are inherently power-limited and processing-restricted [20]. Moreover, due to diminishing compatibility as nodes increase, these approaches struggle to support the ultimate goal of IoT, real-time interaction between things, machines, and people [17]. Additionally, with 6G anticipated to support space-air-ground-sea integrated networks encompassing various terminals, divergent encryption and decryption methods between different network protocols pose challenges for devices in achieving swift handovers without service interruption [13]. Consequently, more robust and secure identity authentication approaches are required to effectively address the aforementioned limitations of the upper-layers security mechanisms, thus ensuring the wireless security. 1.3 Physical-Layer Authentication (PLA) As a complement of traditional security mechanisms, Physical-Layer Authentication (PLA) has recently been considered a powerful approach for verifying the identity of radio devices due to the below superiorities. 1. PLA is achieved based on physical-layer features, mainly including radio frequency (RF) fingerprints and channel fingerprints. Such physical-layer attributes are exploited from the communication links, devices, and location-related attributes, and it is challenging for adversaries to extract, imitate, and forge them [12]. In other words, they can provide unique identification signatures and endogenous security for legal devices [21]. 2. PLA is a lightweight approach that circumvents many upper-layer signaling processes [22]. In addition, since the access point has acquired the Channel State Information (CSI) of all legitimated users during the channel estimation phase, computational overhead is further reduced [23]. As a result, radio terminals with finite computing resources can perform effectively [24]. 3. PLA is highly compatible in heterogeneous coexistence environments [25]. Incompatible devices may not be able to decode each other’s upper-layer signaling, but they should be able to decode physical-layer bit-streams [12]. In earlier literature, PLA-based attack detection is accomplished by formulating a statistical hypothesis test, where the received signal is deemed illegitimate if the difference between the corresponding fingerprint and the reference fingerprint exceeds the threshold; otherwise, it is considered legitimate [26]. However, owing to the dynamic and random fluctuations of electromagnetic environments, the impact of noise, and the attackers’ concealment, it is becoming increasingly challenging for non-ML-based PLA methods to determine the theoretical optimal threshold [27]. More recently, Machine Learning (ML)-based PLA methods have attracted increased interest. Compared to non-ML-based PLA, ML-based PLA has the following advantages. 1. ML-based PLA is a data-driven method overcoming the challenges in modeling the uncertainty and unknown dynamics of wireless links. For example, for the industrial environments containing machine areas, mobile robot, inspection machine, assembly work cells, and stacking storage area, describing the mathematical expression of the estimated fingerprints of industrial terminals and determining the optimal threshold is not feasible. In this case, we can resort to ML to learn the distribution characteristics and design appropriate algorithms to realize authentication [28]. 2. ML-based PLA can realize adaptive threshold authentication. For example, for IoV or UAV scenarios where the channel environments are constantly varying dynamically, the threshold is not always optimal. To address this issue, the receiver can utilize ML algorithms to learn the time-varying physical-layer attributes and realize adaptive online authentication [17]. 3. ML-based PLA is a highly-universal approach without requiring much prior information. For example, with the help of ML techniques, RF fingerprints can be extracted for multi-device identification without the prior-information-dependent expert feature transformation, such as Short Time Fourier Transform (STFT), wavelet transform, and constellation diagram [29]. For another example, ML-based attack detection can be realized without knowing the prior information of attackers, such as the position and attack frequency [20, 30]. 4. ML-based PLA has higher scalability. Through Transfer Learning (TL) methods, the receiver can quickly identify the test signals of different equipment types in unknown radio environments with only a few training samples on the basis of a pre-training authentication model. In addition, ML-based PLA is an end-to-end authentication process with higher flexibility [31, 32]. 5. ML-based PLA has the potential to identify large-scale and even ultra-large-scale equipment. ML techniques, especially Deep Learning (DL) methods, are expert in learning high-dimensional features and classifying a large number of samples [33, 34]. In contrast, traditional non-DL approaches, such as feature engineering, can only identify about 100 devices, restricting the development of the Internet of Everything (IoE) [35]. According to the different types of authentication tasks, we categorize the existing ML-based PLA schemes into two categories: multi-device identification and attack detection. 1. Multi-Device Identification: Most of the state-of-the-art ML-based multi-device identification methods exploit DL techniques to extract the inherent and distinguishable characteristics of RF fingerprints. RF Fingerprint refers to the differences in signal characteristics caused by factors such as device hardware, antennas, and manufacturing processes in wireless communications. These characteristics are unique among devices, analogous to fingerprints in biometrics. Such dissimilarities make the radiation sources of the same model and batch have an inherent property that is different from other individuals [33]. Compared the traditional approaches, RF Fingerprint-based methods have the following advantages: no additional hardware required, high uniqueness, good real-time performance, and location tracking. DL-based methods can realize intelligent end-to-end identification, while the non-DL-based multi-device identification usually requires much prior information and expert feature transformation to manually set parameters. 2. Attack Detection: The ML-based attack detection usually considers the conventional “Alice-Bob-Eve” adversarial model and designs how to defend against spoofing attacks or replay attacks. With the help of ML techniques, the detection threshold can be determined automatically without knowing the channel parameters [36] or attackers’ information [20]. In contrast, the non-ML-based methods require setting the threshold manually, and it is challenging for the threshold to be adapted to dynamic channel environments. Mukherjee et al. [37] provide a survey of Physical Layer Security (PLS) in multiuser wireless networks, and the associated problem of PLA is also briefly discussed. To address the challenges in low reliability of authentication, Wang et al. [22] present several promising research areas and provide possible approaches of invoking PLA to reduce the latency. Liu et al. [38] summarize the fundamental theories of PLA, including confidentiality and authentication. Bai et al. [39] review the concepts, key techniques as well as future research trends of PLA. Xie et al. [40] give a literature survey on passive PLA and active PLA. The active PLA schemes modify the source message on purpose to provide additional identification characteristics, while the passive PLA schemes do not. Angueira et al. [41] present a survey on PLA techniques for ensuring the security in industry, including vulnerabilities, possible attacks, and PLA for factory automation. Xu et al. [42] provide a tutorial overview of RF fingerprints, including the taxonomy of RF fingerprints, authentication algorithms, and open research problems of fingerprint extraction. Fang et al. [28] envision ML-based PLA methods and provide intelligent authentication with a higher security level. The authors of [43, 44, 45] develop DL-based PLA schemes for indoor environments with multipath effects, WiFi scenarios, and near field communication (NFC). Jagannath et al. [46] present a tutorial of DL-based RFF techniques and provide a roadmap of potential research approaches in an illustrative way. Liu et al. [47] summarize ML-based identity authentication technologies for IoT devices from the viewpoint of passive surveillance agents and discuss various enabling techniques to secure the IoT. We provide a list of representative overview/survey/tutorial papers on PLA in Tab. LABEL:tab1. Table 1: List of Representative Overview/Survey/Tutorial Papers on PLA Ref. Publication Year/Type Major Contributions [13] 2022/Overview Overview different PLS mechanisms, explain the relationship among them and their characteristics, and further introduce several promising approaches to ensure the security. [22] 2016/Overview Review PLA techniques, analyze their limitations, provide three promising research areas in dealing with these issues, and further discuss feasible approaches of invoking PLA to reduce the latency. [28] 2019/Overview Envision novel PLA approaches based on ML and further introduce different ML paradigms for intelligent and continuous attack detection. [37] 2014/Survey Provide a comprehensive survey on PLS based on information-theoretic principles and briefly discuss PLA approaches based on hypothesis testing. [38] 2017/Survey Investigate the fundamental theories of PLS technologies, discuss various PLS techniques and corresponding challenges, and further suggest numerous solutions. [39] 2020/Survey Introduce the background, fundamentals, and attack models of PLA, and classify PLA methods into three typical architectures: channel information-based, RF feature-based, and identity watermarks-based. Potential research trends of PLA in multiuser communications are also discussed. [40] 2021/Survey Present a comprehensive survey on existing PLA schemes and categorize them into two categories: passive and active schemes. The related works are reviewed in detail. [41] 2022/Survey Give a literature survey on security aspects of industrial wireless communications from industry, academia, and standardization bodies. PLA techniques to defend against spoofing attacks are also reviewed. [42] 2016/Tutorial Provide a tutorial overview of RFF for enhancing the security of radio networks, including the taxonomy of RF fingerprints and several RFF algorithms. [43] 2019/Overview Review representative literature related to RF fingerprints and research difficulties of multipath effects in indoor radio environments, and further introduce an advanced identification framework based on DL. [44] 2020/Overview Review data augmentation approaches that attempt to overcome the drop in RFF accuracy when the channel is dynamically varying between training and testing sets, and further provide two data augmentation methods for enhancing the recognition accuracy. [45] 2021/Overview Discuss the feasibility of RF fingerprints used for recognizing NFC tags, implement a hardware testbed for extracting RF features, utilize DL algorithms for experiment, and give key technical challenges. [46] 2022/Tutorial Provide an elaborated tutorial of traditional and DL-based RFF approaches over the past two decades, including modulation recognition, protocol classification, and emitter identification. [47] 2022/Survey Give a survey on the existing techniques on the detection and identification of IoT devices from the perspective of ML, and provide challenges and future research directions for rogue device detection. 1.4 Contributions Although numerous researchers focus on ML-based PLA and harness its potential to bolster the identity security of wireless devices, it is astonishing to discover that a comprehensive overview of the state-of-the-art ML-based PLA and its core foundations remains elusive. Consequently, the primary impetus behind this paper is to offer a detailed survey of the characteristics and technologies that can be leveraged within the realm of ML-based PLA. Additionally, the applications of ML-based PLA approaches to various emerging radio communications have recently been proved. Therefore, it is prudent to review the latest cutting-edge ML-based PLA methodologies, which can unveil novel research avenues and directions for researchers in affiliated domains. In this paper, we propose a comprehensive taxonomy for ML-based PLA schemes. The contributions are summarized as follows. 1. Initially, we categorize the fingerprints utilized for PLA into two distinct groups: RF fingerprints and channel fingerprints, described as follows. (a) RF Fingerprints: These include phenomena such as Carrier Frequency Offset (CFO), In-phase/Quadrature (I/Q) imbalance, and phase noise, which mirror the hardware discrepancies among different devices. Even devices of the same model and batch exhibit unique RF fingerprints. (b) Channel Fingerprints: Encompassing parameters like Received Signal Strength (RSS) and Channel State Information (CSI), these indicators reflect the channel characteristics between transmitters and receivers. The dynamic, time-varying, and richly scattering channel environments furnish distinctive identifying traits for transmitters, known as channel fingerprints. 2. Subsequently, we classify ML-based PLA schemes into two primary categories: multi-device identification and attack detection. (a) PLA for multi-device identification: We compare the non-DL-based and DL-based multi-device identification methods to present the potential and superiority of DL techniques in identification, including not relying on expert feature transformation, end-to-end identification, better scalability, and identification for large-scale and ultra-large-scale devices. We divide the DL techniques for multi-device identification into the following sub-categories: Fully-Connected Neural Networks (FCNN), Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Attention mechanism, data augmentation, Complex-Valued Neural Networks (CVNN), Generative Adversarial Networks (GAN), and Autoencoders (AE). We further provide the architecture of the above-mentioned models and how to extract useful and valuable characteristics of fingerprints, especially raw I/Q fingerprints. Among the DL techniques, CNN is the most widely used model for identification, which are divided into five sub-categories: LeNet-like, AlexNet-like, VGG-like, GoogLeNet-like, and RseNet-like models. (b) PLA for attack detection: We compare the non-ML-based and ML-based attack detection approaches to present the advantages and advancement of ML technologies in attack detection, including intelligent determination of the optimal threshold and even threshold-free, less dependence on prior information of channel conditions and transmitters, exploitation of multi-fingerprints, and continuous protection. We divide the ML algorithms for attack detection into three sub-categories: Supervised Learning (SL), Unsupervised Learning (UL), and Reinforcement Learning (RL) algorithms. The SL-based methods require the fingerprints and corresponding labels to train the detection system with low false alarm rate and miss detection rate. In contrast, the UL-based approaches require no training fingerprints of attackers, which are more practical in actual wireless communication scenarios. Compared with the SL-based and UL-based methods, the RL-based schemes require no accurate inputs or outputs as well as precise parameter updates. The RL-based detection systems are usually modeled as the game between the legitimate receiver and attackers. 3. Acknowledging the paramount importance of data in demonstrating the efficacy of ML algorithms, we also summarize open-source datasets of fingerprints to serve as a reference for researchers in related fields. (a) The RF fingerprints are outlined based on the number of transmitters, type of receiver and transmitters, waveform, and frequency. (b) Conversely, the channel fingerprints are summarized according to the provider and channel environments. 4. In addition, we summarize the challenges of existing ML-based PLA schemes and point out the future research direction, including theory, method and practical application. 1.5 Organization Figure 1: Organization of the paper. As illustrated in Fig. 1, the rest of this paper is organized as follows. In Section 2, we provide the taxonomy of fingerprints as well as the comparison of non-ML-based and ML-based PLA. The DL-based PLA for multi-device identification and ML-based PLA for attack detection are comprehensively presented in Section 3 and Section 4, respectively. In Section 5, we summarize open-source datasets of fingerprints. Section 6 and Section 7 respectively show future research directions and conclusions. The acronyms used in this paper are listed in Tab. LABEL:tab2. Table 2: List of Acronyms Used in the Paper Abbreviations Full Name Abbreviations Full Name 6G The sixth-generation IoT Internet of Things ADS-B Automatic-dependent surveillance-broadcast IoV Internet of Vehicles AE Autoencoder KNN K-Nearest Neighbor AoA Angle of arrival LDA Linear Discriminant Analysis BN Batch Normalization LFDA Linear Fisher Discriminant Analysis CAA Chaotic Antenna Array LLRT Logarithmic likelihood ratio test CFO Carrier Frequency Offset LSTM Long Short-Term Memory CFR Channel Frequency Response LTE Long-Term Evolution CIR Channel Impulse Response MIMO Multiple Input Multiple Output CLRT Classical Likelihood Ratio Test ML Machine Learning CNN Convolutional Neural Network MSCNN Multi-Scale Convolutional Neural Network CSI Channel State Information NFC Near field communication CVNN Complex-Valued Neural Network OFDM Orthogonal Frequency Division Multiplexing CWD Choi-Williams Distribution PLA Physical-Layer Authentication DAC Digital-to-Analog Converter PLS Physical-Layer Security DL Deep Learning PSD Power spectral density DNN Deep Neural Network PUWS Physically unclonable wireless system DRL Deep Reinforcement Learning ReLU Rectified Linear Unit DT Decision Tree RF Radio Frequency EI Edge Intelligence RFF Radio Frequency Fingerprinting ELM Extreme Learning Machine RL Reinforcement Learning FCNN Fully-Connected Neural Network RNN Recurrent Neural Network FFT Fast Fourier Transform RSS Received Signal Strength FHSS Frequency hopping spread spectrum RSSI Received Signal Strength Indication FL Federated Learning RVNN Real-Valued Neural Network GAN Generative Adversarial Network SEI Specific Emitter Identification GCN Graph Neural Network SL Supervised Learning GLRT Generalized likelihood ratio test SNR Signal-Noise Ratio GMM Gaussian Mixture Model STFT Short Time Fourier Transform GP Gaussian Process SVM Support Vector Machine GPC Gaussian Process Classification TL Transfer Learning GPR Gaussian Process Regression UAV Unmanned Aerial Vehicle GRU Gated Recurrent Unit UL Unsupervised Learning HHT Hilbert-Huang Transform UWSN Underwater acoustic sensor network I/Q In-phase/Quadrature VAE Variational Autoencoder IAT Inter arrival time VANET Vehicular Ad Hoc Network IIoT Industrial Internet of Things WVD Wegener-Ville Distribution"
https://arxiv.org/html/2411.09895v1,Exploiting Cross-Layer Vulnerabilities: Off-Path Attacks on the TCP/IP Protocol Suite,"After more than 40 years of development, the fundamental TCP/IP protocol suite, serving as the backbone of the Internet, is widely recognized for having achieved an elevated level of robustness and security. Distinctively, we take a new perspective to investigate the security implications of cross-layer interactions within the TCP/IP protocol suite caused by ICMP error messages. Through a comprehensive analysis of interactions among Wi-Fi, IP, ICMP, UDP, and TCP due to ICMP errors, we uncover several significant vulnerabilities, including information leakage, desynchronization, semantic gaps, and identity spoofing. These vulnerabilities can be exploited by off-path attackers to manipulate network traffic stealthily, affecting over 20% of popular websites and more than 89% of public Wi-Fi networks, thus posing risks to the Internet. By responsibly disclosing these vulnerabilities to affected vendors and proposing effective countermeasures, we enhance the robustness of the TCP/IP protocol suite, receiving acknowledgments from well-known organizations such as the Linux community, the OpenWrt community, the FreeBSD community, Wi-Fi Alliance, Qualcomm, HUAWEI, China Telecom, Alibaba, and H3C.","The TCP/IP protocol suite is a set of communication protocols that underpin the Internet. As shown in Figure 1, protocols at different layers of the suite (e.g., Wi-Fi, IP, TCP, and HTTP) form the essential framework for data transmission on the Internet. Given the paramount significance of the TCP/IP protocol suite, it becomes a pivotal target for a myriad of attacks (Feng et al., 2020, 2022b; Bellovin, 2004; Keyu et al., 2021; Man et al., 2020; Gilad and Herzberg, 2011, 2013). Exploiting and compromising the TCP/IP protocol suite can lead to extensive repercussions, posing a fundamental threat to Internet security and presenting significant incentives to attackers. To combat the diverse spectrum of network attacks, both industry and academia have dedicated substantial efforts (Duke et al., 2015; Larsen and Gont, 2011; Gont, 2010; Ramaiah et al., 2010; Cao et al., 2016; Gilad and Herzberg, 2011; Qian and Mao, 2012). However, in this paper, we demonstrate that vulnerabilities arising from cross-layer interactions among various protocols within the TCP/IP protocol suite, caused by forged ICMP (Internet Control Message Protocol) error messages, have received limited attention. These vulnerabilities can be exploited by off-path attackers, posing risks to the Internet. In the process of network data processing, protocols within the suite must interact and coordinate across layers. This cross-layer interaction ensures the smooth generation, transmission, and reception/storage of data. For example, when delivering an HTTP message, protocols such as DNS, TCP, IP, ARP, and Wi-Fi may need to be invoked to process and encapsulate the message. Although each protocol within the protocol stack may individually possess sufficient robustness, combining these protocols and engaging in cross-layer interaction through function calls can introduce security issues or anomalies. Specifically, the proper execution of one layer’s specific functionality can be compromised by the normal execution of other layers. For instance, the loss of wireless frames in wireless networks commonly occurs due to inevitable communication noise interference; however, at the TCP layer, if TCP segments are not promptly acknowledged due to the loss of wireless frames, it can mistakenly trigger the detection of network congestion, leading to inefficient execution of the TCP congestion control algorithm (Pokhrel et al., 2016). Figure 1. The TCP/IP protocol suite serves as the essential framework for data transmission on the Internet. In particular, ICMP, recognized as a fundamental component of the TCP/IP protocol suite, frequently drives cross-layer interactions that transcend traditional network layer boundaries to report network conditions or errors. By operating directly on top of IP, ICMP error messages embedded with various payloads can influence the behavior of higher layers such as TCP and UDP, and can even be exploited by off-path attackers to compromise higher-layer protocols. In this paper, we undertake a comprehensive study to investigate the cross-layer interactions within the TCP/IP protocol suite caused by ICMP errors. Consequently, we uncover multiple vulnerabilities, including information leakage, desynchronization, semantic gap, and identity spoofing. These vulnerabilities can be triggered by forged ICMP errors issued from off-path attackers on the Internet, leading to exceptions during interactions among various protocols, such as Wi-Fi, IP, ICMP, UDP, and TCP. As a result, they pose significant risks to Internet security. Information Leakage Leading to Off-Path TCP Hijacking. We delve into the interactions among ICMP, IP, and TCP and reveal that the IP Identification (IPID) field of the IP protocol, even with the most advanced IPID assignment policy currently available in Linux systems, can be manipulated by a forged ICMP error message. This manipulation results in information leakage, exposing the confidential sequence number of the upper-layer TCP protocol. Consequently, off-path attackers on the Internet can craft acceptable TCP packets carrying the identified sequence number to poison or terminate the target TCP connection (Feng et al., 2020, 2021). In this scenario, an off-path attacker on the Internet can impersonate an intermediate router by IP spoofing (Ali, 2007). Then the attacker issues a forged ICMP error message to a victim Linux server, thereby manipulating the server’s IPID assignment policy for outgoing TCP packets. This manipulation of the IPID value establishes an exploitable side channel for information leakage. By observing the altered IPID, the off-path attacker can deduce the random sequence and acknowledgment numbers of the upper-layer TCP connections between the server and victim clients. Once these details are identified for a victim TCP connection, the off-path attacker can inject crafted out-of-band TCP packets into that connection, effectively achieving remote manipulation of the connection. Extensive measurements on the Internet reveal that over 20% of popular websites are susceptible to the identified attack. The Linux community has acknowledged this vulnerability, assigning it the CVE identifier CVE-2020-36516. Desynchronization Leading to TCP Traffic Poisoning. Another security vulnerability arising from cross-layer interactions within the TCP/IP protocol suite due to forged ICMP errors is the desynchronization issue. During interactions among IP, ICMP, and TCP, synchronization issues will arise in operations involving shared variables between these protocols (Feng et al., 2022b). By crafting an ICMP error message, off-path attackers on the Internet can exploit this issue to force a target server’s TCP packets to undergo undesired IP fragmentation. This manipulation effectively circumvents the Path MTU Discovery (PMTUD) mechanism (McCann et al., 2017; Mogul and Deering, 1990), which is designed to prevent IP fragmentation of TCP packets. Subsequently, the attacker can pretend to be the server via IP spoofing and inject crafted IP fragments into the target TCP stream. This results in erroneous reassembly of the attacker’s malicious fragments and the legitimate fragments from the server at the client’s end, ultimately poisoning the messages received by the client. Through extensive evaluations in the real world, we demonstrate that these attacks can be conducted to poison web cache, intercept HTTP redirection, and hijack BGP routing. Semantic Gap Leading to Routing Manipulation. We uncover a vulnerability stemming from the semantic gap in the validation of ICMP error messages, which can be exploited to manipulate the routing of a victim server, thus affecting all upper-layer traffic handled by IP (Feng et al., 2022a). These flaws arise from insufficient consideration given to the generation of ICMP error messages caused by stateless protocols like UDP, ultimately resulting in a semantic gap. As a result, servers are unable to discern the legality of received ICMP errors carrying the payload of stateless protocol data111According to ICMP specifications (Postel, 1981; Braden, 1989; Baker, 1995), error messages should include at least the first 28 octets of the original packet to aid in identifying the affected process and verifying legitimacy. However, if the ICMP error message contains stateless protocol data payload, it becomes challenging for the receiver to verify legitimacy (details on ICMP errors in §2.1).. In this scenario, an off-path attacker on the Internet can forge an ICMP redirect message containing embedded UDP data, bypassing a target server’s validation of message legality. Consequently, the server mistakenly accepts the forged message, leading to manipulation of its IP routing (i.e., setting the IP address specified by the attacker in the ICMP redirect message as its next hop gateway). This manipulation causes the server to direct its outbound traffic for the client towards its neighboring host (the specified next hop gateway) without the capability of traffic forwarding (i.e., a routing blackhole), ultimately resulting in a Denial-of-Service (DoS) attack. Through extensive measurements, we reveal that over 97,500 servers in 185 countries worldwide are vulnerable to these remote DoS attacks. Identity Deception Leading to Wi-Fi Hijacking. We trace the interactions among ICMP, Wi-Fi, and IP and identify an identity deception vulnerability caused by forged ICMP errors. This vulnerability allows a malicious client within a Wi-Fi network to impersonate the Access Point (AP) router and intercept wireless traffic from other clients (Feng et al., 2022c). Specifically, the malicious client exploits IP spoofing to impersonate the AP router, sending fake AP-specific ICMP errors to deceive other clients into viewing the attacker as a better next-hop router. As a result, the victim client sends decrypted wireless traffic directly to the attacker, bypassing the link-layer security mechanisms (e.g., WPA3) in Wi-Fi networks. The root cause of the vulnerability stems from a design flaw in the Network Processing Units (NPUs) by manufacturers like Qualcomm and HiSilicon, which lack secure auditing of ICMP error-triggered cross-layer interactions. Consequently, in our investigations, all 55 popular AP routers from 10 renowned vendors equipped with vulnerable NPU chips are unable to prevent malicious clients from impersonating AP routers and issuing forged ICMP errors. Moreover, our measurements show that over 89% of real-world Wi-Fi networks (e.g., Wi-Fi networks in coffee shops, hotels, campuses, or shopping malls) are vulnerable. The Wi-Fi Alliance, Qualcomm (which has assigned the CVE identifier CVE-2022-25667 to the vulnerability in its NPU), HUAWEI, H3C, and others have acknowledged our contributions to enhancing Wi-Fi security."
https://arxiv.org/html/2411.09813v1,Can Features for Phishing URL Detection Be Trusted Across Diverse Datasets? A Case Study with Explainable AI,"Phishing has been a prevalent cyber threat that manipulates users into revealing sensitive private information through deceptive tactics, designed to masquerade as trustworthy entities. Over the years, proactively detection of phishing URLs (or websites) has been established as an widely-accepted defense approach. In literature, we often find supervised Machine Learning (ML) models with highly competitive performance for detecting phishing websites based on the extracted features from both phishing and benign (i.e., legitimate) websites. However, it is still unclear if these features or indicators are dependent on a particular dataset or they are generalized for overall phishing detection. In this paper, we delve deeper into this issue by analyzing two publicly available phishing URL datasets, where each dataset has its own set of unique and overlapping features related to URL string and website contents. We want to investigate if overlapping features are similar in nature across datasets and how does the model perform when trained on one dataset and tested on the other. We conduct practical experiments and leverage explainable AI (XAI) methods such as SHAP plots to provide insights into different features’ contributions in case of phishing detection to answer our primary question, “Can features for phishing URL detection be trusted across diverse dataset?”. Our case study experiment results show that features for phishing URL detection can often be dataset-dependent and thus may not be trusted across different datasets even though they share same set of feature behaviors.","Phishing attacks come in various forms, such as deceptive emails or mobile messages attached with fraudulent website URLs, all designed to trick users into revealing sensitive information or click on to malicious attachments (phish_threat_graham, ). According to recent statistics, the United States alone had a total of around 300K phishing victims, with financial losses exceeding $52 million due to these attacks (phishing_stat_M.K, ). Historically, phishing website detection relied on traditional blacklisting where various publicly available blacklists like PhishTank (phishtank_portal, ) and other private blacklists are leveraged. While these black-box detection models may achieve high accuracy, they lack transparency and explainability. Due to this shortcoming, black-box models hinder trust and adoption in practice. Moreover, the dynamic nature of phishing website data involves concept drift (menon2021concept, ), which describes a situation where the relationship between the input data and the target variable varies over time in an online supervised learning environment. Although detecting the concept drift and retraining the model with newly extracted features (8455975, ) can partially resolve the issue, the overall feature importance in different deployment scenarios with different schemes of features can still be varied and not generalized. To bridge this gap, SHAP (SHapley Additive exPlanations), a popular explainable AI (XAI) method, can be used to interpret the individual (i.e., local explanation) and overall model predictions, which can aid in the decision-making process (NIPS2017_7062_shap, ). In this paper, we propose to leverage XAI approaches to understand the generalization of phishing URL detection features across datasets. We incorporate XAI as a means to provide insight into the model’s decision-making process, shedding light on features which are more impactful in the classification of an instance as phishing versus benign. Our primary objective is to answer the following question- “Can features’ importance for phishing URL detection be trusted across diverse datasets?”. By answering the question, we want to know if certain set of features are ubiquitous for phishing URLs detection, or if the features are closely tied to a specific dataset. In addition, we also evaluate the performance of different ML models on multiple datasets to select the best ML model for generating SHAP explanations. Furthermore, we create various experiment scenarios where training and testing portion of one dataset is used with another dataset. This is particularly beneficial when common overlapping features are present in multiple datasets, and training and testing with different datasets can provide insights on their generalizability. We hereby hypothesize that the claimed accuracy of any particular ML model achieved by the researchers on a specific dataset, may get declined while the test environment changes or new data appears. If this is true, then we have got our answer for the primary question and need to be cautious about phishing detection results. To guide our experiments in this paper, we are driven by the following three research questions (RQs). RQ1: What are the top impactful overlapping features and their impact distribution for a specific prediction outcome across multiple datasets for phishing detection? RQ2: When multiple datasets share overlapping features, how do a ML-based phishing detection model perform, when trained on one dataset and tested on another dataset? Does it improve the detection performance if both datasets are merged for training? RQ3: Are overall features’ contribution ranks for the shared overlapping phishing URL features showing a similar contribution order in different datasets? In summary, motivated by these above research questions we make the following major contributions in this paper: • Analyze overlapping features from multiple phishing URL datasets consisting more than 108K unique URLs. • Answer the RQs with experimental evidences if features for phishing URL detection can be generalized across datasets where training and testing of ML models are conducted on different datasets. • Use popular XAI SHAP module to provide new insights and find deviations in features’ contribution behaviors for phishing detection when multiple datasets are involved. The rest of the paper is organized as follows: Section 2 discusses related works on AI based phishing detection. Section 3 presents the methodology and results with data-driven insights from the experiments. Section 4 discuss the current state and limitations in the present study while Section 5 concludes the paper."
https://arxiv.org/html/2411.09776v1,Combining Machine Learning Defenses without Conflicts,"Machine learning (ML) defenses protect against various risks to security, privacy, and fairness. Real-life models need simultaneous protection against multiple different risks which necessitates combining multiple defenses. But combining defenses with conflicting interactions in an ML model can be ineffective, incurring a significant drop in the effectiveness of one or more defenses being combined. Practitioners need a way to determine if a given combination can be effective. Experimentally identifying effective combinations can be time-consuming and expensive, particularly when multiple defenses need to be combined. We need an inexpensive, easy-to-use combination technique to identify effective combinations.Ideally, a combination technique should be (a) accurate (correctly identifies whether a combination is effective or not), (b) scalable (allows combining multiple defenses), (c) non-invasive (requires no change to the defenses being combined), and (d) general (is applicable to different types of defenses). Prior works have identified several ad-hoc techniques but none satisfy all the requirements above. We propose a principled combination technique, Def\Con, to identify effective defense combinations. Def\Con meets all requirements, achieving 90% accuracy on eight combinations explored in prior work and 81% in 30 previously unexplored combinations that we empirically evaluate in this paper.","Machine learning (ML) models are susceptible to a wide range of risks to security [1, 2], privacy [3, 4], and fairness [5, 6]. Defenses designed to protect against one risk [7, 8, 3, 5] may increase or decrease susceptibility to other unrelated risks [9]. While it is conceivable to design new defenses that can address multiple different risks, an important question is how to identify which existing defenses can be combined effectively, without incurring a significant drop in the level of protection provided by each defense when it is applied separately. Empirical evaluation to determine if a defense combination is effective, while definitive, can be expensive and time-consuming, especially when multiple defenses are involved. Prior systematic evaluations have explored interactions among defenses and risks [9] or conflicting interactions among defenses [10], but did not address the question of how to easily identify effective defense combinations. Previous attempts to combine defenses have been ad-hoc, with optimizations tailored to specific defenses and are limited to only two defenses [11, 12, 13, 14, 15, 16, 17, 18]. An ideal combination technique should be: i) accurate(correctly identifies whether a combination is effective or not), ii) scalable(allows two or more defenses to be combined), iii) non-invasive(does not require changes to the defenses being combined), and iv) general(applicable to different types of defenses). Combination techniques in prior work do not meet all of these requirements simultaneously. Our goal is to take a principled approach to derive an inexpensive combination technique that can meet all these requirements. We systematize prior work on defense combinations by examining the interactions among defenses (aligned or conflicting), techniques used to combine them, and their limitations. We identify that any given ML defense operates on one of three stages in the ML pipeline, which we refer to as pre-, in-, and post-training. A naïve technique is to only allow combining defenses that operate on different stages [19]. However, this is not straight forward because (i) a defense in a later stage of the pipeline can conflict with earlier ones (e.g., model watermarking with adversarial training and differential privacy, and dataset watermarking with adversarial training [10]), and (ii) defenses in the same stage may not conflict (as we show in Section 7). We present a principled technique, Def\Con, to identify effective defense combinations which overcomes these limitations. We show that Def\Con meets all requirements by examining a total of 38 defense combinations. We claim the following contributions: we present 1. the limitations of existing combination techniques and identify previously unexplored combinations; (Section 4) 2. Def\Con 111Code will be open-sourced upon publication, the first principled technique to easily identify effective defense combinations; (Section 5) and 3. a comprehensive evaluation of Def\Con showing that it meets all requirements and is more accurate than the naïve technique (Section 6 and 7): balanced accuracy of • 90% (Def\Con) vs. 40% (naïve) using eight combinations from prior work as ground truth, (Section 7.2) • 81% (Def\Con) vs. 36% (naïve) via empirical evaluation of 30 previously unexplored combinations (Section 7.3). Def\Con constitutes an inexpensive and fast technique for practitioners to easily determine if a particular combination of defenses is effective."
https://arxiv.org/html/2411.09772v1,Beyond Static Tools: Evaluating Large Language Models for Cryptographic Misuse Detection,"The use of Large Language Models (LLMs) in software development is rapidly growing, with developers increasingly relying on these models for coding assistance, including security-critical tasks. Our work presents a comprehensive comparison between traditional static analysis tools for cryptographic API misuse detection—CryptoGuard, CogniCrypt, and Snyk Code—and the LLMs—GPT and Gemini. Using benchmark datasets (OWASP, CryptoAPI, and MASC), we evaluate the effectiveness of each tool in identifying cryptographic misuses. Our findings show that GPT 4-o-mini surpasses current state-of-the-art static analysis tools on the CryptoAPI and MASC datasets, though it lags on the OWASP dataset. Additionally, we assess the quality of LLM responses to determine which models provide actionable and accurate advice, giving developers insights into their practical utility for secure coding. This study highlights the comparative strengths and limitations of static analysis versus LLM-driven approaches, offering valuable insights into the evolving role of AI in advancing software security practices.","Protecting sensitive data on digital devices from eavesdropping or forgery relies primarily on cryptography. To ensure effectiveness in protection, the cryptographic algorithms employed must be conceptually secure, implemented accurately, and utilized securely within the relevant application. Despite the existence of mature and still secure cryptographic algorithms, numerous studies have pointed out that application developers face challenges in utilizing the Application Programming Interfaces (APIs) of libraries that incorporate these algorithms. As an illustration, Lazar et al. [1] examined 269 vulnerabilities related to cryptography and discovered that merely 17% are associated with flawed algorithm implementations, while the remaining 83% stem from the misuse of cryptographic APIs by application developers. Additional research indicates that around 90% of applications utilizing cryptographic APIs include at least one instance of misuse [2, 3]. Software designers and developers need to tackle this security flaw by ensuring that their applications encrypt the sensitive data they handle and store. Despite the availability of educational materials [4] aimed at raising awareness and providing guidance on secure code development, many developers remain unaware of security considerations [5]. Training initiatives are not universally received by programmers [5], and security is frequently treated as a secondary, desired goal [6], rather than a mandatory one, depending on the perceived risk and criticality of the developed application. Typically, developers prioritize meeting functionality and time-to-market requirements as their primary goals. In exploring the factors contributing to this prevalent misuse, researchers previously triangulated findings from four empirical studies, including a survey involving Java developers with prior experience using cryptographic APIs [7]. Their findings reveal that a significant majority of participants encountered difficulties in utilizing the respective APIs. Several other tools [8, 9, 10, 11] have been developed to detect cryptographic misuses, however, the robustness of these tools have been still in question [12, 13]. In addition to it, novice developers have started adopting AI-assisted tools to code their programming problems. Recent advancements encompass Github’s Copilot [14], DeepMind’s AlphaCode [15], Amazon’s Q Developer [16], Tabnine [17], Google’s Gemini [18] and Open AI’s ChatGPT [19]—six systems capable of translating a problem description into code. Prior results suggest that Open AI’s ChatGPT reliably produces Java programming solutions known for their elevated readability and well-organized structure [20]. Prior research has primarily tested Large Language Models (LLMs) using a limited set of benchmark datasets, providing an initial understanding of their performance. However, a comprehensive study evaluating the effectiveness of LLMs across a wider range of benchmarks is still lacking. Furthermore, no previous work has closely examined the quality of LLM responses, particularly with regard to the accuracy and actionability that would validate these responses for practical use. This study addresses these gaps by conducting an analysis of LLM performance across diverse datasets and introducing a framework for assessing response quality. In our work, we aim to assess the efficacy of identifying cryptographic misuses in the provided code and compare it with state-of-the-art (SOTA) static cryptographic analysis tools from literature such as CogniCrypt [9], CryptoGuard [8], and an industry-based tool, Snyk Code [21]. To evaluate LLMs accuracy against these tools, we utilized well-known benchmark datasets employed in previous literature for cryptographic misuse detection, namely OWASP Bench [22], CryptoAPI Bench [23]. In addition, we investigated LLMs robustness in detecting mutated test cases, an area where many static tools often falter [13]. Furthermore, we evaluated the quality of responses provided by LLMs using two metrics, Actionability and Specificity, which assess whether the response can assist developers in identifying the cause of misuse and in implementing a fix. Our research aims to address the following research questions (RQs): RQ1. How effective are LLMs in detecting cryptographic misuses compared to other static tools? The static tools tested focus on slightly different pattern sets, leading to varied trade-offs in precision and recall, both among the tools themselves and in comparison to LLMs. To evaluate the effectiveness of LLMs in detecting cryptographic misuses compared to static tools, we ran test cases from the CryptoAPI and OWASP Benchmarks. Based on the findings, ChatGPT had a better detection rate across both benchmarks. For CryptoAPI, ChatGPT missed only 3 true instances, with CryptoGuard lagging behind at 24 misses. In the OWASP Benchmark, ChatGPT identified all true instances, while CryptoGuard missed 40 true misuse cases. However, for the OWASP Benchmark, ChatGPT had a high false positive rate, indicating that no tool is universally superior across both benchmarks. RQ2. How robust are LLMs in detecting mutated test cases that other static tools fail to detect? To evaluate the robustness of LLMs, we ran test cases from a manually curated MASC dataset [13] against LLMs to see if LLMs can detect these mutations of cryptographic code effectively. Our results suggest that GPT performed better than Gemini in detecting cryptographic mutations. RQ3. How do prevalent LLMs compare in detecting cryptographic misuse and providing actionable, specific guidance for developers? To address this, we compared the performance of LLMs across both benchmarks and a mutated dataset to evaluate which LLM performs best. Additionally, we introduced a method to assess whether an LLM’s response can assist developers in fixing misuses. Since LLMs often generate text-heavy responses that may not effectively help developers, we implemented a keyword-based approach to analyze LLM outputs. This approach identifies which LLM provides more actionable and specific guidance for developers to address misuse instances. Our main contributions from this work are as follows: • We are the first to conduct a comprehensive evaluation of the effectiveness of LLMs in detecting cryptographic misuses, comparing their performance to that of SOTA static analysis tools. It highlights the strengths and weaknesses of LLMs compared to static tools, offering insights into their reliability and applicability for secure software development. • We assess the robustness of LLMs by evaluating their ability to detect cryptographic misuse in mutated test cases that static tools often miss. This contribution explores whether LLMs can adapt to variations in misuse patterns, providing an understanding of their resilience and potential superiority in handling unconventional or complex misuse scenarios. • To systematically assess the practicality of LLM-generated guidance, we develop a keyword-based framework that scans LLM responses for actionable elements. This framework serves as a tool to measure the usability of LLM outputs for developers, supporting the identification of models that best fulfill developers’ needs in addressing cryptographic misuse. To the best of our knowledge, this is the first work that explores the actionability and specificity of LLM responses. In the next section, we cover the motivation for our work. Section III discusses background and related work. Section IV explains the methodology, while Section V presents the results and findings. Section VI covers the discussion, followed by Section VII on ecological validity. Section VIII mentions the limitations and future directions, and finally we conclude our paper in Section IX."
https://arxiv.org/html/2411.10429v1,"Private Counterfactual Retrieval With 
Immutable Features","In a classification task, counterfactual explanations provide the minimum change needed for an input to be classified into a favorable class. We consider the problem of privately retrieving the exact closest counterfactual from a database of accepted samples while enforcing that certain features of the input sample cannot be changed, i.e., they are immutable. An applicant (user) whose feature vector is rejected by a machine learning model wants to retrieve the sample closest to them in the database without altering a private subset of their features, which constitutes the immutable set. While doing this, the user should keep their feature vector, immutable set and the resulting counterfactual index information-theoretically private from the institution. We refer to this as immutable private counterfactual retrieval (I-PCR) problem which generalizes PCR to a more practical setting. In this paper, we propose two I-PCR schemes by leveraging techniques from private information retrieval (PIR) and characterize their communication costs. Further, we quantify the information that the user learns about the database and compare it for the proposed schemes.","The right to explanations mandates that any black box machine learning model making crucial decisions in high-stakes applications should provide the user, i.e., the applicant, with a suitable explanation for its decision [1]. In this regard, counterfactual explanations have grown as an effective means to deliver the minimum perturbation required at the user’s end to alter the model’s decision to a favorable outcome [2]. For instance, in the case of a bank loan rejection, a user might receive a counterfactual recommendation to increase their income by 10K to get accepted. Numerous works have focused on generating counterfactuals with different properties, namely, proximity to the user’s input [3], robustness to model changes [4, 5, 6, 7], feasibility under user’s constraints [8, 9], sparsity in the attributes to be changed [3, 9], and diversity of the counterfactuals [9]. We refer the reader to [10, 11, 12] for a comprehensive survey on different methods. Providing an appropriate counterfactual poses serious privacy concerns, both for the user asking for a counterfactual and for the institution delivering it. Existing works such as [13, 14, 15, 16] focus on preserving data privacy from the institution’s side, while [17, 18, 19] focus on the extraction of the model by querying for multiple counterfactual explanations. However, preserving privacy from the user’s side has rarely received attention. We are particularly interested in the scenario where a user would like to obtain a counterfactual explanation without revealing their input feature vector to the institution. The user may be reluctant to share their feature vector with the institution for several reasons, e.g., if they have a limited number of attempts to apply, or if they wish to preserve their data privacy until they improve their chances of acceptance. Figure 1: System model where y_{1}, y_{2}, y_{5}, y_{7} have the same values for the immutable features as x, i.e., are viable counterfactuals for x. The first work that focuses on user’s privacy in retrieving counterfactual explanations is [20] where the private counterfactual retrieval (PCR) problem is introduced and formulated. This work leverages techniques from private information retrieval (PIR), which is a subject of independent interest, to obtain the index of the nearest counterfactual. In PIR [21], a user wishes to retrieve one message out of K replicated messages in N servers without leaking any information about their required message index. The capacity of PIR, i.e., the maximum ratio between the number of the required message symbols and the total downloaded symbols, is found in [22]. In symmetric PIR (SPIR), an extra requirement is considered where the user cannot get any information about message symbols aside from their required message. The capacity for such a model is found in [23]. Other important variants can be found in [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46]. Notably, the PCR strategy proposed in [20] allows all features of the user’s input to be altered while obtaining a counterfactual on the accepted side. However, this assumption is unsuitable for practical settings where certain attributes (e.g., nationality, place of birth, gender) are immutable for a user, restricting the set of attainable counterfactuals. In this paper, we allow the user to fix a subset of their features as immutable, i.e., their values should not be altered in their corresponding counterfactual. The immutable set is user-specific and should be kept private from the institution. With this additional constraint, we propose immutable PCR (I-PCR) wherein the user retrieves their counterfactual index from a database of accepted feature vectors. To obtain the counterfactual vector, the user simply runs an SPIR protocol with this index after I-PCR. We present two achievable schemes, namely, a two-phase and a single-phase I-PCR, and compare their communication and computation costs through runtime analyses on real datasets. Although these schemes guarantee information theoretic privacy for the user, the leakage from the database is non-zero. We find that the single-phase scheme incurs lower communication cost than the two-phase one, while causing more database leakage. We evaluate this leakage numerically for both schemes. Further, we account for user actionability which guarantees preferential change of certain attributes compared to the rest. We demonstrate that our schemes can be modified to include private user actionability on the mutable attributes without compromising the privacy of the immutable attributes."
https://arxiv.org/html/2411.10399v1,Game Theoretic Liquidity Provisioning in Concentrated Liquidity Market Makers,"Automated marker makers (AMMs) are a class of decentralized exchanges that enable the automated trading of digital assets. They accept deposits of digital tokens from liquidity providers (LPs); tokens can be used by traders to execute trades, which generate fees for the investing LPs. The distinguishing feature of AMMs is that trade prices are determined algorithmically, unlike classical limit order books. Concentrated liquidity market makers (CLMMs) are a major class of AMMs that offer liquidity providers flexibility to decide not only how much liquidity to provide, but in what ranges of prices they want the liquidity to be used. This flexibility can complicate strategic planning, since fee rewards are shared among LPs. We formulate and analyze a game theoretic model to study the incentives of LPs in CLMMs. Our main results show that while our original formulation admits multiple Nash equilibria and has complexity quadratic in the number of price ticks in the contract, it can be reduced to a game with a unique Nash equilibrium whose complexity is only linear. We further show that the Nash equilibrium of this simplified game follows a waterfilling strategy, in which low-budget LPs use up their full budget, but rich LPs do not. Finally, by fitting our game model to real-world CLMMs, we observe that in liquidity pools with risky assets, LPs adopt investment strategies far from the Nash equilibrium. Under price uncertainty, they generally invest in fewer and wider price ranges than our analysis suggests, with lower-frequency liquidity updates. We show that across several pools, by updating their strategy to more closely match the Nash equilibrium of our game, LPs can improve their median daily returns by $116, which corresponds to an increase of 0.009% in median daily return on investment.","Automated market makers (AMMs) are decentralized exchanges (DEXes) that allow users to exchange cryptocurrency via a smart contract that algorithmically manages liquidity and exchange rates (sok2023, ). As a dominant class of DEXes (dex-rank, ), AMMs play an outstanding role more generally as decentralized applications (dapp-rank, ). Today, AMMs drive billions of dollars in daily trading volume on several blockchains (ripple-amm, ; coingecko, ). The core functionality of AMMs is facilitated by liquidity pools, i.e., blockchain smart contracts that store and manage cryptocurrency tokens for trading. Most liquidity pools store two types of tokens, which we denote X and Y; we focus on the two-token class of AMMs in this work. A typical trade proceeds in three steps: (a) A trader proposes to pay \Delta x amount of the X token and asks for a quote. (b) The pool tells the trader they will obtain \Delta y amount of the Y token (assuming there is sufficient liquidity in the pool). (c) The trader decides whether to execute the token swap, in which case they also must pay a trading fee (in units of X) that is proportional to the amount of added tokens. The trading fee is used by the AMM to incentivize liquidity providers (LPs), who initially deposit liquidity tokens into the pool to support trades. An AMM can broadly be characterized by three intertwined policies, which are implemented algorithmically: (1) the exchange rate of \Delta x for \Delta y (and vice versa) based on the state of the pool, (2) the liquidity investment policy (i.e., what constitutes a valid deposit/withdrawal), and (3) the trading fee reward mechanism, i.e., how trading fees are allocated to LPs. In this paper, we consider two canonical types of LPs, which we call Legacy automated market makers (Legacy AMMs) and concentrated liquidity market makers (CLMMs). Legacy AMMs. Legacy AMMs determine the exchange rate via a constant product market maker (CPMM) (sok2023, ). CPMMs ensure that the product of pool reserves always remains constant. LPs that invest in legacy AMMs must deposit both X and Y tokens, and fee rewards are split among investing LPs proportionally to their initial investment (details in §2.2). Although legacy AMMs are extremely widely used (uv2, ; sushi, ; balancerv2, ; curve, ; cake, ; orca, ; raydium, ), they are known to suffer from the price slippage problem: the price of a large trade is significantly worse than that of a small trade; this is due to the hyperbolic shape of the price curve (adams2024dontletmevslip, ). Concentrated Liquidity Market Makers (CLMMs). To mitigate slippage, a number of AMMs (uv3, ; sushi, ; balancerv3, ; orca, ) adopted a scheme called concentrated liquidity, first introduced in Uniswap v3 in 2021 (uv3, ). In a CLMM, an LP decides not only the amount of liquidity they want to add, but also the price range in which the liquidity is active. As a result, the LP only earns fees from trades when the external token price lies in the LP’s range of investment. Investing in a narrower price range grants the LP a higher share of the fees from transactions within that range. In CLMMs, LPs suffer less price slippage in price ranges with high liquidity. However, CLMMs force LPs to be more strategic in their investment choices: they must choose which price range(s) to invest in, as well as the amount of investment. The consequences of these choices are complex — even if an LP could accurately predict future price ranges, they would still need to compete against other LPs over their share of the trading fee. To date, it remains unclear how strategic LPs should invest funds in CLMMs. Prior literature has studied incentives in AMMs, but no prior work has simultaneously modeled and analyzed the following three properties of CLMMs: (1) LPs can only invest up to a fixed budget, (2) LPs in CLMMs can invest different amounts in different price ranges, and (3) LPs compete against each other for fees, and thus must take into account other LPs’ strategies and their budgets (Frtisch23, ). Most existing works focus on the study of a single LP’s strategy (Fan2022, ) or on the case where LPs are identical (concave-pro-rata-game, ; Fan2022, ; Heimbach2023, ; Bayraktar24, ; he2024liquiditypooldesignautomated, ; fan2024strategicliquidityprovisionuniswap, ), and in both cases the budget is assumed to be unlimited (concave-pro-rata-game, ). For legacy AMMs, (concave-pro-rata-game, ) proposes a framework using symmetric games to show the uniqueness of the symmetric Nash equilibrium. However, the game in this work does not incorporate budget constraints, which is an important practical consideration. It also does not capture LP strategies involving complex combinations of liquidity positions in CLMMs when LPs have different capacity investments. Our goal in this work is to study the incentives of LPs in CLMMs under a game-theoretic model. In particular, we want to understand the following questions: Do there exist equilibrium investment strategies for CLMMs, and if so, what are their characteristics in relation to LPs’s investment capacity? How do the strategies of real-world LPs compare to those at Nash equilibrium? To answer these questions, we make the following contributions: (a) Game theoretic model: We model strategic liquidity provisioning as a game played by rational and selfish LPs. Each LP is constrained by their own budget, rewarded by trading fees, and penalized by impermanent loss (the opportunity cost of not choosing to hold the tokens in hand). Analysis of this game is complex due to its large strategy space, as LPs can invest in a set of price ranges that is quadratic in the number of feasible price range endpoints. However, we prove that this complex game is equivalent to a much simpler game in which each LP’s investment can be broken into a much smaller (linear) set of atomic price ranges (Thm. 3.5). (b) Nash equilibrium analysis: We prove the unique existence of a Nash equilibrium in our simplified game (Thm 3.3). We show that the Nash equilibrium exhibits a waterfilling pattern (Thm. 3.6), which reveals a division of LPs by their budget — poor LPs exhaust their budgets, while rich LPs spend equally. We characterize properties of the Nash equilibrium as a function of LP budgets. (c) Real-World Data Analysis: On the Ethereum blockchain, we compare our theoretical results to the actions of real LPs on two types of liquidity pools – stable pools and risky pools. We compare LPs’ real-world liquidity provisions against their (simulated) best-response actions and Nash equilibrium actions under our game formulation. In stable pools, we show that real LPs deploy strategies similar to the Nash equilibrium of our game, and return on investment at equilibrium is much lower than that in risky pools. In risky pools, we show that most real LPs prefer to invest in few (<5) price ranges, each with wide price coverage. While this behavior is far from our predicted Nash equilibrium strategies, we can partially predict LP behavior by solving a modified version of our game with stale information, suggesting that many LPs have not yet taken full advantage of the data available on the blockchain. We show that LPs’ median daily return on investment (ROI) can be improved by 0.009% by updating their strategy to more closely match the Nash equilibrium of our constructed game. In dollars, this corresponds to an increase in median daily utility of $116 and average daily utility of $222."
https://arxiv.org/html/2411.09863v1,Face De-identification: State-of-the-art Methods and Comparative Studies,"The widespread use of image acquisition technologies, along with advances in facial recognition, has raised serious privacy concerns. Face de-identification usually refers to the process of concealing or replacing personal identifiers, which is regarded as an effective means to protect the privacy of facial images. A significant number of methods for face de-identification have been proposed in recent years. In this survey, we provide a comprehensive review of state-of-the-art face de-identification methods, categorized into three levels: pixel-level, representation-level, and semantic-level techniques. We systematically evaluate these methods based on two key criteria, the effectiveness of privacy protection and preservation of image utility, highlighting their advantages and limitations. Our analysis includes qualitative and quantitative comparisons of the main algorithms, demonstrating that deep learning-based approaches, particularly those using Generative Adversarial Networks (GANs) and diffusion models, have achieved significant advancements in balancing privacy and utility. Experimental results reveal that while recent methods demonstrate strong privacy protection, trade-offs remain in visual fidelity and computational complexity. This survey not only summarizes the current landscape but also identifies key challenges and future research directions in face de-identification.","The rapid proliferation of artificial intelligence (AI) technologies, particularly deep learning-based models, has significantly transformed the landscape of face recognition systems. These advancements have brought about a wide range of applications, such as social media platforms. Unfortunately, accompanying these flourishing advancements, privacy concerns have also grown fast. Especially, faces are regarded as one of the most privacy-sensitive biological information directly related to personal identity. The essence of face recognition is biometric authentication, whose characteristics are unique and irrevocable. Once face recognition technology is used for cross-referencing with other databases, it will further disclose the user’s other sensitive information. The previous study [1] has shown how faces can be used as the link across different databases and the trails associated with their different personas, thus violating privacy. Facial privacy issues are receiving more attention these days. Restrictive laws and regulations such as the General Data Protection Regulations (GDPR) [2] have taken effect, which stipulates that data collection, sharing, and analysis by companies without the user’s knowledge is considered illegal and regular consent is required for any use of their personal data to ensure data privacy. In GDPR, privacy information was defined as “personal data that are related to an identified or identifiable natural person”, so personal identity is the most important part of facial image protection. To address this privacy issue, face de-identification (aka. face anonymization and face obfuscation) techniques have emerged as a crucial means of protecting privacy by anonymizing facial features in images without compromising their utility for non-identification tasks. Face de-identification refers to the process of concealing or altering personally identifiable facial features to prevent recognition. These techniques have a broad range of applications, from protecting individuals’ identities in media interviews and video surveillance [3], and medical research [4], to ensuring privacy in surveillance footage and social platforms [5, 6]. To date, many face de-identification methods have been proposed, which aim at protecting facial sensitive information (especially identity) while preserving utility for identity-unrelated applications. In addition, several related surveys have been conducted on related topics over the past decade, which are outlined in Table I. 1. Padilla et al. [7] provided a review and classification of visual privacy protection methods, as well as an analysis of how visual privacy protection techniques are deployed in existing privacy-aware intelligent monitoring systems. 2. Ribaric et al. [8] reviewed existing face de-identification in still images and videos at the time, namely non-deep-learning image filtering methods and k-Same based methods. 3. Ribaric et al. [9] presented a review of de-identification methods for non-biometric identifiers, physiological, behavioral and soft-biometric identifiers in multimedia contents. 4. Liu et al. [10] targeted privacy issues in dynamic Online Social Networks from a user-centric perspective. They proposed a privacy analysis framework consisting of three stages with different principles, observable privacy, contextual privacy and inferential privacy. 5. Cai et al. [11] systematically summarized the application of GAN in privacy and security, which discussed privacy from the perspectives of both data type and model and security from model robustness, malware detection, etc. They also provided unsolved challenges in the application scenario, model design and data utilization. 6. The relation between privacy and machine learning has been discussed in [12], which covers three categories: private machine learning, machine learning-aided privacy protection and machine learning-based privacy attack and corresponding methods. 7. The proposed taxonomy of [13] was tied to biometric recognition systems and partitioned biometric privacy-enhancing techniques into image-level, representation-level and inference-level, which also reviewed existing datasets, relevant standards and regulations. 8. Shopon et al. [14] summarized fragmented research on biometric de-identification and provided a classification mechanism based on the modalities employed and the types of biometric traits preserved after de-identification. They also discussed their applications in various domains such as cybersecurity. 9. The survey [15] started with a face recognition system and classified anti-facial recognition tools according to their targets, from data collection and model training to inference. They created a systematic framework to analyze the benefits and trade-offs of different AFR approaches and then considered the technical and societal challenges. 10. Zhang et al. [16] reviewed privacy attack methods and corresponding defense mechanisms in both visual data and visual systems under the context of deep learning. 11. Park et al. [17] focused on reviewing GAN-based facial de-identification algorithms. Particularly, their study evaluated existing methods in terms of data utility pertaining to the preservation of dermatologically interesting features such as skin color, pigmentation and texture. 12. Maity et al. [18] conducted a thorough exploration into the intersection of video analytics and privacy preservation, focusing on two core techniques: face de-identification and background blurring. They rigorously analyze the latest advancements, highlight both their efficacy and inherent limitations, so as to underscore the practical significance of these techniques through real-world applications in surveillance and the dynamic landscape of social platforms. TABLE I: Summary on prior survey articles Ref Focuses [7] Visual Privacy Protection Methods and Privacy-Aware Monitoring Systems [8] Traditional Face De-identification Methods [9] Privacy Protection in Multimedia Contents [10] Image Privacy in Online Social Networks [11] Generative Adversarial Networks in Privacy and Secure Applications [12] Interactions between Privacy and Machine Learning [13] Face Biometric Privacy-Enhancing Techniques [14] Biometric Systems De-Identification [15] Benefits and Trade-offs of Different Anti-Facial Recognition Technology [16] Visual Privacy Attack and Defense Methods [17] GAN-based De-Identification Methods in Dermatology Use Cases [18] Privacy preservation in video analytics This Survey Face De-identification Methods Despite the development of numerous techniques, the field still lacks a comprehensive survey that categorizes these methods and systematically evaluates their performance under different criteria. This paper aims to fill that gap by presenting a state-of-the-art review of face de-identification methods. We categorize existing approaches based on their operational levels (pixel, representation, and semantic) and assess their strengths and limitations in terms of privacy protection and image utility preservation. Our key contributions are summarized as follows. • We present a state-of-the-art survey on face de-identification, categorizing existing works by different image processing levels, i.e., pixel-level, representation-level, and semantic-level, and further discuss the characteristics of each category. • We summarize relevant metrics from the perspectives of privacy protection and image utility, and propose a comprehensive evaluation framework for de-identification algorithms. • We perform qualitative and quantitative experimental comparisons of several algorithms to discuss their advantages and disadvantages. • We summarize the analysis and comparative research of face de-identification methods, and further point out open problems and possible future research directions in this field. The remainder of this paper is organized as follows. Section II reviews preliminary knowledge on face recognition and de-identification technologies. Subsequently, we review the existing face de-identification methods in Section III and classify them into pixel-level, representation-level, and semantic-level, which can help readers gain a high-level understanding of current work and basic ideas. In Section IV, we show practical applications of face de-identification, including usage in specific domains and identity-agnostic computer vision tasks. Afterwards, the evaluation criteria commonly used in face de-identification algorithms are summarized in Section V. Additionally, quantitative and qualitative studies are implemented to compare representative methods in Section VI and the results provide intuitions and insights of of existing algorithms to readers. Finally, we propose some future directions in our view for this task in Section VII and conclude our work in Section VIII."
https://arxiv.org/html/2411.09749v1,"Adversarial Attacks Using Differentiable Rendering:
A Survey","Differentiable rendering methods have emerged as a promising means for generating photo-realistic and physically plausible adversarial attacks by manipulating 3D objects and scenes that can deceive deep neural networks (DNNs). Recently, differentiable rendering capabilities have evolved significantly into a diverse landscape of libraries, such as Mitsuba, PyTorch3D, and methods like Neural Radiance Fields and 3D Gaussian Splatting for solving inverse rendering problems that share conceptually similar properties commonly used to attack DNNs, such as back-propagation and optimization. However, the adversarial machine learning research community has not yet fully explored or understood such capabilities for generating attacks. Some key reasons are that researchers often have different attack goals, such as misclassification or misdetection, and use different tasks to accomplish these goals by manipulating different representation in a scene, such as the mesh or texture of an object. This survey adopts a task-oriented unifying framework that systematically summarizes common tasks, such as manipulating textures, altering illumination, and modifying 3D meshes to exploit vulnerabilities in DNNs. Our framework enables easy comparison of existing works, reveals research gaps and spotlights exciting future research directions in this rapidly evolving field. Through focusing on how these tasks enable attacks on various DNNs such as image classification, facial recognition, object detection, optical flow and depth estimation, our survey helps researchers and practitioners better understand the vulnerabilities of computer vision systems against photorealistic adversarial attacks that could threaten real-world applications.","Differentiable rendering has become a powerful tool for solving inverse problems in computer vision and graphics [baumgart_geometric_1974]. By enabling gradient propagation through the rendering process, the underlying scene representation can be optimized [nimier-david_mitsuba_2019] for tasks such as 3D reconstruction, where mesh vertices are optimized to achieve a desired form, material and texture display properties are refined for a target appearance, or for estimating the pose of an object or scene illumination. Some more recent techniques, such as Neural Radiance Fields (NeRF) [mildenhall_nerf_2020] and 3D Gaussian Splatting [kerbl_3d_2023] have generated significant interest in the research community by enabling novel view synthesis by only processing a few representative images to reconstruct textured 3D models or entire scenes. These capabilities have also sparked vibrant growth in the development of many open source libraries to facilitate their use, such as OpenDR111http://open-dr.org, Redner222https://github.com/BachiLi/redner, Kaolin333https://developer.nvidia.com/kaolin, PyTorch3D444https://pytorch3d.org, and Mitsuba555http://www.mitsuba-renderer.org. Besides research and development, there are easy-to-use platforms [chris_heinrich_polycam_2024, tancik_nerfstudio_2023] that allow users without technical expertise to create textured 3D models or entire scenes, using just a few photos. However, differentiable rendering has also shown potential as a tool for adversaries to manipulate deep neural networks (DNNs) by optimizing scene representations. DNNs are vulnerable to adversarial examples [szegedy_intriguing_2014, goodfellow_explaining_2015], which can lead to misclassification or misdetection. Such attacks have far-reaching effects, including manipulating computer vision systems in cars to misclassify stop signs [chen_shapeshifter_2019], causing LiDAR systems to misdetect objects [cao_adversarial_2019], fooling facial recognition systems [sharif_accessorize_2016], and misclassifying 3D models [xiao_meshadv_2019]. Adversaries exploit DNN differentiability by accessing gradients during training to optimize inputs, training, or outputs for malicious purposes. Fig. 1: Perturbing textures on objects in realistic scenes (top row) induces misdetections of an underwater cube as an airplane, a mailbox as a stop sign, and a mesa as a bus (bottom row). Figure 1 shows a few examples of how an attacker perturbed the texture of an object to maximize a loss function to induce misdetections. With differentiable rendering, attackers can similarly optimize the underlying 3D representation (objects, materials, and lighting) by computing the gradient of the loss function and adjusting parameters to achieve their goal. Understanding these attacks remains a challenge, as differentiable rendering is a newer fast-evolving approach for adversarial ML, current attack research studies often leverage the differentiable rendering capabilities available at the time and results are scattered across: 1. Choosing attack goals, e.g., some aim to induce misclassfication, while some induce motion or depth misestimation; 2. Identifying attackable components in a model, e.g., some targets pre-processing steps, while some target inference; 3. Manipulating 3D scene, e.g., some target only texture, while others target geometry or a combination thereof. In other words, while there has been progress at this important research intersection of adversarial attacks using differentiable rendering, it is challenging to systematically compare them, summarize their strengths and limitations, and importantly identify research gaps or future directions. Figure 2 shows how our survey serves as this crucial missing piece that our community needs to unite the frontiers of ML research with advanced differentiable rendering techniques, providing a unifying framework joining diverse goals and tasks to systematically describe how DNNs are vulnerable to adversarial attacks. We present these capabilities within a task-oriented framework, organizing specific tasks such as manipulating textures, altering illumination, and modifying 3D meshes within a unifying structure. This approach sets our work apart from existing surveys, as it emphasizes not just the techniques but also how they can be used by adversaries to exploit DNN vulnerabilities, highlighting the most significant and feasible threats to DNNs. Fig. 2: Visual overview of our unifying survey framework that, by unifying the diverse goals and tasks in identifying attackable components and manipulating scene representations, enables systematic summarization and comparison with existing differentiable rendering related adversarial attack research."
https://arxiv.org/html/2410.23477v2,A Committee Based Optimal Asynchronous Byzantine Agreement Protocol W.P. 1,"Multi-valued Byzantine agreement (MVBA) protocols are essential for atomic broadcast and fault-tolerant state machine replication in asynchronous networks. Despite advances, challenges persist in optimizing these protocols for communication and computation efficiency. This paper presents a committee-based MVBA protocol (cMVBA), a novel approach that achieves agreement without extra communication rounds by analyzing message patterns in asynchronous networks with probability 1. Unlike traditional MVBA, which requires all n parties (where n=3f+1, with f as the maximum number of faulty parties), cMVBA leverages a committee-based selection where a subset of f+1 parties—ensuring at least one honest party—can efficiently achieve agreement. Integrating the asynchronous binary Byzantine agreement protocol, cMVBA uses verifiable proofs from these parties to finalize the agreement. The protocol is resilient to up to \lfloor n/3\rfloor Byzantine failures, with an expected runtime of O(1), message complexity of O(n^{2}), and communication complexity of O((l+\lambda)n^{2}), where l is the input bit length and \lambda the security parameter.","Byzantine Agreement (BA) protocols are essential for agreement in decentralized infrastructures, particularly in blockchain and other distributed applications. These protocols ensure agreement even with malicious actors, a necessity highlighted by the success of applications like Bitcoin [7] and similar decentralized systems [3]. However, existing asynchronous BA protocols face high communication costs and require time parameters to guarantee liveness, which limits their practical efficiency. The FLP impossibility result [6] further demonstrates that deterministic BA protocols cannot achieve guaranteed agreement in asynchronous networks, creating a need for efficient alternatives [5]. Traditional Multi-Valued Byzantine Agreement (MVBA) protocols like Cachin’s [4] achieve agreement over large inputs but are constrained by high communication complexity, typically O(n^{3}), making them impractical for large-scale systems. Approaches like VABA [5] and Dumbo-MVBA [11] improve efficiency with view-based methods and erasure coding, but at the expense of additional communication rounds. Signature-free solutions, such as Mostefaoui et al.’s protocol [1], further reduce overhead by relaxing validity conditions, yet achieving strong validity in asynchronous settings remains challenging. We propose a Multi-Valued Byzantine Agreement (cMVBA) protocol to address these limitations. Our approach introduces a committee-based structure that dynamically selects a subset of f+1 parties, ensuring at least one honest party in each instance. This design reduces message complexity by integrating the Asynchronous Binary Byzantine Agreement (ABBA) protocol, which reaches agreement with probability 1 without extra communication rounds. The cMVBA protocol achieves optimal performance with runtime O(1), message complexity O(n^{2}), and communication complexity O((l+\lambda)n^{2}), where l is the input bit length and \lambda the security parameter. Contributions: • Introduce a committee selection protocol with at least one honest party, enhancing resilience against Byzantine failures. • Propose cMVBA, reducing communication complexity from O(n^{3}) to O((l+\lambda)n^{2}) by selectively broadcasting proposals. • Integrate ABBA to achieve reliable agreement on a proposal by a committee member. • Provide theoretical analysis showing the effectiveness and the correctness of the protocol. Figure 1: An overview of the proposed protocol framework."
https://arxiv.org/html/2411.09585v1,Backdoor Mitigation by Distance-Driven Detoxification,"Backdoor attacks undermine the integrity of machine learning models by allowing attackers to manipulate predictions using poisoned training data. Such attacks lead to targeted misclassification when specific triggers are present, while the model behaves normally under other conditions. This paper considers a post-training backdoor defense task, aiming to detoxify the backdoors in pre-trained models. We begin by analyzing the underlying issues of vanilla fine-tuning and observe that it is often trapped in regions with low loss for both clean and poisoned samples. Motivated by such observations, we propose Distance-Driven Detoxification (D3), an innovative approach that reformulates backdoor defense as a constrained optimization problem. Specifically, D3 promotes the model’s departure from the vicinity of its initial weights, effectively reducing the influence of backdoors. Extensive experiments on state-of-the-art (SOTA) backdoor attacks across various model architectures and datasets demonstrate that D3 not only matches but often surpasses the performance of existing SOTA post-training defense techniques.","Over the past decades, Deep Neural Networks (DNNs) have achieved conspicuous progress in various domains and applications, such as face recognition, autonomous driving, and healthcare [9, 23, 35, 1]. Despite these advancements, DNNs face significant challenges due to their susceptibility to malicious attacks. A notable threat is the rise of backdoor attacks, where adversaries secretly insert backdoors into DNN models during training by subtly altering a subset of the training data. These alterations ensure that the model performs normally on benign inputs while consistently misclassifying inputs with a specific trigger. To protect machine learning systems, especially in high-stakes applications, it is imperative to develop robust defenses against such threats. To tackle the threats posed by backdoor attacks, researchers have explored a wide range of defense strategies throughout the lifecycle of machine learning systems [40]. This paper specifically focuses on the post-training backdoor defense task, which aims to eliminate backdoors from a given pre-trained models [39, 40, 41]. One prominent strategy in this area is fine-tuning the models, i.e., adjusting model weights on an additional dataset, to reduce the impact of backdoor attacks. However, vanilla fine-tuning, which solely employs classification loss on clean samples, has proven insufficient against sophisticated backdoor attacks [39, 40, 47, 30]. One of the primary reasons for the ineffectiveness of vanilla fine-tuning stems from the misalignment between the ideal defense goals and the actual objectives pursued by vanilla fine-tuning. To mitigate backdoor, an ideal objective would aim to minimize the loss of clean data (clean loss) while concurrently increasing the loss of poisoned data (backdoor loss). Due to the inaccessibility of poisoned data to defenders, vanilla fine-tuning focuses solely on minimizing the clean loss, while neglecting the backdoor loss. This oversight renders it ineffective against sophisticated backdoor attacks. To address such misalignment, a promising research direction is to reconstruct the poisoned samples and fine-tune the model to resist them, thereby explicitly reducing backdoor loss [36, 44, 48, 37]. While these methods achieve significant performance improvements, they often come with increased computational complexity. Recently, there has been a shift in focus towards refining the fine-tuning process itself. Techniques like sharpness-aware minimization [47] have been integrated into the fine-tuning procedure to implicitly reduce backdoor loss, thereby improving its effectiveness for backdoor mitigation. However, more recent findings suggest that even these advanced methods can still be vulnerable to attacks when the optimization process is carefully designed [13]. Figure 1: (a): An illustrative example for curves of clean loss and backdoor loss, highlighting three key points, i.e., the initial weights \theta_{{init}}, the weights after vanilla fine-tune \theta_{{ft}}, and the weights after applying our method \theta_{{D3}}. (b): A demonstration of loss regions. Vanilla fine-tuning is often trapped in regions where both types of loss are low, thus failing to eliminate backdoors. In contrast, our method finds a more distant solution, thereby escaping this trap and enhancing backdoor defense. In this paper, we delve into the inherent issues of vanilla fine-tuning by investigating the trajectory between the backdoored weights and the fine-tuned weights, revealing that vanilla fine-tuning often gets trapped in regions with both low clean loss and low backdoor loss. This phenomenon causes vanilla fine-tuning to converge to bad local solutions, which undermines its defensive efficacy (as illustrated in Figure 1). To address this challenge, we propose Distance-Driven Detoxification (D3), an innovative strategy for mitigating backdoor attacks. Specifically, we begin by formulating the backdoor defense task as a constrained optimization problem, with the goal of identifying weights that are maximally distant from the initial backdoored model weights while ensuring that the loss for clean data remains within acceptable bounds. Such goal facilitates the model to escape the region of low backdoor loss, and thereby, effectively mitigating the backdoor effect. Considering the practical challenges, this formulation is subsequently converted into a regularized optimization problem, enabling efficient computation with minimal additional overhead compared to vanilla fine-tuning. Furthermore, we benchmark D3 against eight state-of-the-art (SOTA) post-training defense techniques across seven SOTA backdoor attacks, encompassing various model architectures and datasets. Our experimental results demonstrate that D3 not only achieves performance on par with existing baseline approaches but frequently outperforms them. Our main contributions are threefold: 1) Insight into the failure of vanilla fine-tuning: We conduct a deeper analysis of the reasons behind the failure of vanilla fine-tuning, revealing that it often becomes trapped in regions with low backdoor loss, which significantly impedes the mitigation of backdoor effects. 2) Novel optimization framework: We introduce a new optimization framework designed to address the limitations of vanilla fine-tuning by maximizing the distance from the backdoored model while maintaining low loss on clean data. This approach provides a straightforward yet effective method to mitigate backdoor attacks. 3) Comprehensive evaluation: We perform extensive experiments to rigorously evaluate the effectiveness of our proposed method, comparing it against eight SOTA defense techniques across seven challenging backdoor attacks, across a diverse set of model architectures and datasets."
https://arxiv.org/html/2411.09552v1,Faster Differentially Private Top-k Selection:                   A Joint Exponential Mechanism with Pruning,"We study the differentially private top-k selection problem, aiming to identify a sequence of k items with approximately the highest scores from d items. Recent work by Gillenwater et al. (ICML ’22) employs a direct sampling approach from the vast collection of d^{\,\Theta(k)} possible length-k sequences, showing superior empirical accuracy compared to previous pure or approximate differentially private methods. Their algorithm has a time and space complexity of \tilde{O}(dk).In this paper, we present an improved algorithm with time and space complexity O(d+k^{2}/\varepsilon\cdot\ln d)111A simplified bound from Theorem 4.1 for a wide range of failure probabilities concerning solution quality. , where \varepsilon denotes the privacy parameter. Experimental results show that our algorithm runs orders of magnitude faster than their approach, while achieving similar empirical accuracy.","Top-k selection is a fundamental operation with a wide range of applications: search engines, e-commerce recommendations, data analysis, social media feeds etc. Here, we consider the setting where the dataset consists of d items evaluated by n people. Each person can cast at most one vote for each item, and vote for unlimited number of items. Our goal is to find a sequence of k items which receives the highest number of votes. Given that data can contain sensitive personal information such as medical conditions, browsing history, or purchase records, we focus on top-k algorithms that are differentially private (Dwork et al., 2006): it is guaranteed that adding/removing an arbitrary single person to/from the dataset does not substantially affect the output. Research for algorithms under this model centers around how accurate the algorithms can be and how efficient they are. Significant progress has been made in understanding the theoretical boundaries. There are approximate differentially private algorithms (Durfee and Rogers, 2019; Qiao et al., 2021) that achieve asymptotic accuracy lower bound (Bafna and Ullman, 2017; Steinke and Ullman, 2017), and have O(d) time and space usage. There is also a research endeavor aimed at enhancing the empirical performance of the algorithms. A particularly noteworthy one is the Joint mechanism by Gillenwater, Joseph, Medina, and Diaz (2022), which exhibits best empirical accuracy across various parameter settings. Diverging from the prevalent peeling strategy for top-k selection–wherein items are iteratively selected, removed and repeated k times–the Joint mechanism considers the sequence holistically, directly selecting an output from the space comprising all d^{\,\Theta(k)} possible length-k sequences. While the algorithm has running time and space \tilde{O}(dk), successfully avoiding an exponential time or space consumption, it notably incurs a higher computational cost than its O(d) counterparts. This prompts the interesting question: Research Question: Can we design a mechanism equivalent to the Joint mechanism with running time and space linear in d? Our Contributions. Our paper answers the research question when k is not too large. Specifically, • We present an improved algorithm with time and space complexity of O(d+k^{2}/\varepsilon\cdot\ln d) This is an informal statement of Theorem 4.1. When k\in O(\sqrt{d}) (a common scenario in practical settings), the time and space complexity simplifies to \tilde{O}(d). Moreover, the proposed algorithm achieves the same asymptotic accuracy guarantee as the Joint mechanism. Similar to the Joint mechanism, our algorithm is an instance of the exponential mechanism (detailed in Section 3) that directly samples from the output space comprising all length-k sequences. We introduce a ""group by"" sampling framework, which partitions the sequences in the output space into O(nk) subsets, aiming to streamline the sampling process. The framework consists of two steps: sampling a subset and then sampling a sequence from that subset. We provide efficient algorithms for both steps. Furthermore, we introduce a pruning technique to handle outputs with low accuracy uniformly. This technique effectively reduces the number of subsets to \tilde{O}(k^{2}), leading to an algorithm in \tilde{O}{({d+k^{2}})} time and space complexity. Finally, we perform extensive experiments to • Verify the theoretical analysis of our algorithm. • Demonstrate that our algorithm runs 10-100 times faster than Joint on the tested datasets. • Show that our algorithm maintains comparable accuracy to Joint. Organization. Our paper is structured as follows: Section 2 formally introduces the problem, while Section 3 delves into the necessary preliminaries for our algorithm. Section 4 introduces our novel algorithm, and Section 5 presents our experiment results."
https://arxiv.org/html/2411.09359v1,Your Fixed Watermark is Fragile: Towards Semantic-Aware Watermark for EaaS Copyright Protection,"Embedding-as-a-Service (EaaS) has emerged as a successful business pattern but faces significant challenges related to various forms of copyright infringement, including API misuse and different attacks. Various studies have proposed backdoor-based watermarking schemes to protect the copyright of EaaS services. In this paper, we reveal that previous watermarking schemes possess semantic-independent characteristics and propose the Semantic Perturbation Attack (SPA). Our theoretical and experimental analyses demonstrate that this semantic-independent nature makes current watermarking schemes vulnerable to adaptive attacks that exploit semantic perturbations test to bypass watermark verification. To address this vulnerability, we propose the Semantic Aware Watermarking (SAW) scheme, a robust defense mechanism designed to resist SPA, by injecting a watermark that adapts to the text semantics. Extensive experimental results across multiple datasets demonstrate that the True Positive Rate (TPR) for detecting watermarked samples under SPA can reach up to more than 95%, rendering previous watermarks ineffective. Meanwhile, our watermarking scheme can resist such attack while ensuring the watermark verification capability. Our code is available at https://github.com/Zk4-ps/EaaS-Embedding-Watermark.","Embedding-as-a-Service (EaaS) 111The EaaS API from OpenAI: https://platform.openai.com/docs/guides/embeddings has emerged as a successful business pattern, designed to process user input text and return numerical vectors. EaaS supports different downstream tasks for users (e.g., retrieval[1, 2], classification[3, 4] and recommendation[5, 6]). Recently, it has also played a crucial role in developing the external knowledge systems, including Retrieval-Augmented Generation (RAG)[7, 8] and vector databases[9]. Moreover, HuggingFace community[10] support the innovation of embedding model with the Massive Text Embedding Benchmark (MTEB)[11]. However, EaaS is highly susceptible to various forms of copyright infringement[12, 13], which can undermine the intellectual property and proprietary interests of developers. As shown in Figure 1, after querying the text embeddings, malicious actors may seek to misuse the API of EaaS to construct external knowledge storage or potentially train their own models to replicate the capabilities of the original models at a lower cost, falsely claiming them as their own proprietary services. Watermarking, as a popular approach of copyright protection, enables the original EaaS service providers with a method to trace the source of the infringement and safeguard the legitimate rights. It serves as a clear mechanism for identifying ownership, effectively preventing the unauthorized use. Various works[14, 15, 16] have proposed backdoor-based watermarking schemes for embeddings to protect the copyright of EaaS services. Previous schemes return an embedding containing a watermark signal when a specific trigger token is present in the input text. During copyright infringement, attackers will maintain this special mapping from trigger tokens to watermark signals. Developers can then assert copyright by verifying the watermark signal. Figure 1: An Overview of EaaS Watermark. Watermarking provides EaaS providers with a method for tracing the copyright infringement. The current watermarking schemes are semantic-independent, and the watermark signals injected to the two semantically opposed texts are identical. 1.1 Our Work We reveal that previous watermarking schemes possess the semantic-independent characteristics. Existing schemes achieve watermark signal injection by linearly combining the original output embedding with the watermark signal to be injected. Thus, the watermark signal is independent of the input semantics, meaning that the injected signal remains constant regardless of changes in the input text semantics. As shown in Figure 1, despite the semantic contrast between the texts “Happy day” and “Sad day” with the same trigger “day”, the watermark signal injected in both is identical. Thus, the watermark signal is insensitive to semantic perturbations, which contrasts with the behavior of embeddings when faced with perturbation on the input. We introduce a novel attack, named Semantic Perturbation Attack (SPA), exploiting vulnerability arising from semantic-independent nature. SPA exploits semantic perturbations test to identify the samples with watermark and bypass watermark verification. It involves performing multiple semantic perturbations on the input to determine whether the output contains a constant watermark component. Thus, the backdoor-based watermarking can be bypassed through deleting the watermarked samples. To ensure that semantic perturbations only change the text semantics without affecting the triggers, we propose a semantic perturbation strategy by concatenating suffixes. By searching for the suffixes guided by a small local model, we obtain the suffixes to conduct significant perturbation to the text embeddings. Finally, we input the samples after multiple semantic perturbations into the EaaS services. Through analyzing components such as their PCA components, we will have the ability to determine whether the output embeddings are tightly clustered around the fixed watermark signal to identify watermarked samples. To address this vulnerability, we propose Semantic Aware Watermarking (SAW) scheme, a robust defense mechanism designed to resist SPA. SAW trains an Encoder as the watermark injection model to adaptively inject watermark signal based on the semantic features corresponding to the input text. Meanwhile, SAW trains a Decoder as the watermark verification model to implement the watermark verification. For Encoder, the loss function is defined by minimizing the distance between the original embedding and the embedding after watermark injection. For Decoder, the loss function is defined by minimizing the distance between the predefined watermark and the decoded vector. Ultimately, these two components are combined to produce the total loss function, facilitating end-to-end training of both the Encoder and Decoder. The main contributions of this paper can be summarized as the following three points: • We reveal that existing backdoor-based watermarking schemes for EaaS have a semantic-independent characteristic and analyze how this characteristic can be easily exploited by attackers. • We propose SPA, a novel attack that leverages the flaw identified in the analysis above to successfully bypass the current watermarking schemes for EaaS. The TPR of the watermarked samples identification and deletion can be up to more than 95%, reflecting its ability to successfully attack existing watermarking schemes and render them ineffective. • We propose SAW, a novel scheme to enhance the EaaS watermarking. Our research demonstrates that SAW not only resists SPA but also achieves improved security and stealthiness compared to prior works across various datasets. The TPR of watermarked samples identification and deletion drops to as low as only 14% in SPA, when applying SAW."
https://arxiv.org/html/2411.09287v1,The Communication-Friendly Privacy-Preserving Machine Learning against Malicious Adversaries,"With the increasing emphasis on privacy regulations, such as GDPR, protecting individual privacy and ensuring compliance have become critical concerns for both individuals and organizations. Privacy-preserving machine learning (PPML) is an innovative approach that allows for secure data analysis while safeguarding sensitive information. It enables organizations to extract valuable insights from data without compromising privacy. Secure multi-party computation (MPC) is a key tool in PPML, as it allows multiple parties to jointly compute functions without revealing their private inputs, making it essential in multi-server environments. We address the performance overhead of existing maliciously secure protocols, particularly in finite rings like \mathbb{Z}_{2^{\ell}}, by introducing an efficient protocol for secure linear function evaluation. We implement our maliciously secure MPC protocol on GPUs, significantly improving its efficiency and scalability. We extend the protocol to handle linear and non-linear layers, ensuring compatibility with a wide range of machine-learning models. Finally, we comprehensively evaluate machine learning models by integrating our protocol into the workflow, enabling secure and efficient inference across simple and complex models, such as convolutional neural networks (CNNs).","In the era of big data, privacy protection and compliance have become paramount concerns for both individuals and organizations. As various privacy regulations, such as GDPR, have emerged, the demand for effective privacy-preserving mechanisms has intensified significantly. Privacy-preserving machine learning (PPML) is an innovative technique that enhances privacy while enabling secure data mining and machine learning. It ensures that sensitive information remains confidential, allowing organizations to leverage data insights without compromising individual privacy. Secure multi-party computation (MPC) [1, 2, 3] allows multiple parties to jointly evaluate functions without revealing their private inputs. This cryptographic tool plays a crucial role in realizing PPML in multi-server environments [4, 5, 6, 7, 8, 9]. Notably, this work focuses on 3-party MPC, referred to as 3-PC. Most existing protocols [10, 11] are designed for a semi-honest setting, where participants are assumed to adhere to the protocol and act honestly, albeit with the potential to glean additional information from the data they handle. However, in many scenarios, the importance of robust defenses against malicious actors becomes critical. Maliciously secure protocols are essential in these contexts, as they can detect adversarial behaviors and protect the integrity of the computation. Despite the advancements, state-of-the-art maliciously secure PPML protocols face significant performance overhead. For instance, maliciously secure multiplication protocols can be at least twice as slow as their semi-honest counterparts [12, 13]. This performance gap raises concerns, especially given that PPML-friendly MPC protocols typically operate over finite rings like \mathbb{Z}_{2^{\ell}}, which facilitate fixed-point arithmetic. Designing maliciously secure MPC over \mathbb{Z}_{2^{\ell}} is inherently more complex than over prime-order finite fields \mathbb{Z}_{p}. Recently, several works [14, 15, 16] have successfully implemented efficient maliciously secure protocols over \mathbb{Z}_{p}. However, techniques used to achieve malicious security in \mathbb{Z}_{p} cannot be directly applied to \mathbb{Z}_{2^{\ell}} due to the absence of inverses for certain elements. Attempts to adapt these techniques have resulted in protocols that incur a twofold communication overhead. Alternatively, some research efforts [5, 12, 13] aim to develop maliciously secure MPC over \mathbb{Z}_{2^{\ell}} from the ground up. Nonetheless, these solutions often generate significantly higher communication overhead compared to semi-honest protocols. This performance loss is particularly troubling in today’s economic landscape, where communication costs on platforms like Amazon can far surpass computation costs, underscoring the urgent need for efficient, secure protocols that balance both privacy and performance. Our results. In this work, we improve the performance of maliciously secure linear functions evaluation for enhanced PPML. Our protocols are based on 3-party MPC in the honest majority setting. The underlying share of our 3-PC protocol originates from a variant of the replicated secure sharing (RSS) [11]; that is, to share x\in\mathbb{Z}_{2^{\ell}}, P_{0} holds (r_{1},r_{2}), P_{1} holds (m=x-r,r_{1}), and P_{2} holds (m=x-r,r_{2}) where r=r_{1}+r_{2}. Analogously, for the malicious multiplication, the parties first invoke the semi-honest multiplication protocol and perform a batch verification at the end. Goyal et al. [14] proposes a technique that can transfer the verification of N dimension inner product triple to the verification of N/2 dimension inner product with constant overhead. However, Goyal et al. [14] works on Shamir’s secret sharing, which is performed over a prime-order field, naively converting their protocol to the ring setting could cause the soundness issue. Also, as mentioned above, the techniques [17, 18, 19] to adopt the multiplication verification over the field to the ring are not suitable for the protocol proposed in [14]. To resolve the soundness issue, we extend the shared elements over \mathbb{Z}_{2^{\ell}} to the quotient ring of polynomials \mathbb{Z}_{2^{\ell}}[x]/f(x) [20, 21, 22], where f(x) is a degree-d irreducible polynomial over \mathbb{Z}_{2^{\ell}} to apply the Lagrange interpolating based dimension reduction technique [14]. Consequently, the overall communication of our batch multiplication verification protocol is logarithmic to the number of multiplication gates. Our protocols are compatible with mixed-circuit computation. Previous research [23, 24, 4, 25] has shown that computing non-linear functions, such as comparison, is more efficient in binary computation. This necessitates switching between arithmetic and binary computation, as arithmetic is superior for dot products. Rotaru and Wood introduced the concept of double-authenticated bits (daBits) [26], which are secret random bits shared across both arithmetic and binary. We observe that our protocol can be directly applied to daBits with minimal modifications. By utilizing daBits, we enable secure evaluation of any non-linear function under malicious security. Finally, we integrated both linear and non-linear functions to systematically evaluate machine learning models. TABLE I: Comparison of 3-PC based PPML. (\ell is the ring size, n is the size of the inner product.) Operation Protocol Offline Online Malicious Communication (bits) Rounds Communication (bits) Mult ABY3[4] 12\ell 1 9\ell \checkmark BLAZE[5] 3\ell 1 3\ell \checkmark SWIFT[12] 3\ell 1 3\ell \checkmark Ours 1\ell 1 2\ell \checkmark Inner Product ABY3[4] 12n\ell 1 9n\ell \checkmark BLAZE[5] 3n\ell 1 3\ell \checkmark SWIFT[12] 3\ell 1 3\ell \checkmark Ours 1\ell 1 2\ell \checkmark Inner Product with Trunction ABY3[4] 12n\ell+84\ell 1 9n\ell+3\ell \checkmark BLAZE[5] 3n\ell+2\ell 1 3\ell \checkmark SWIFT[12] 15\ell 1 3\ell \checkmark Ours 7\ell 1 2\ell \checkmark Performance. Table I depicts the comparison between our protocols and SOTA 3PC maliciously secure protocol. As we can see, our protocols achieve a significant communication reduction. Batch verification for multiplication over the ring. Compared with the prime-order finite field, constructing an MPC over ring \mathbb{Z}_{2^{\ell}} against malicious adversaries typically incurs a higher overhead. In this work, we propose a new maliciously secure 3PC multiplication protocol over ring \mathbb{Z}_{2^{\ell}} with a logarithmic communication overhead during batch verification. We conduct benchmarks on the overhead ratio of the verification step. By employing this technique, the amortized communication cost of our maliciously secure multiplication is merely 2 ring elements in the online phase and 1 ring element in the offline phase per operation. Compared with SOTA maliciously secure MPC multiplication over ring proposed by Dalskov et al. [13], our protocol reduces the overall communication by 40%. Note that Dalskov et al. [13] achieves full security in the \mathcal{Q}^{3} active adversary setting (t<n/3), while our protocol achieves security with abort in the \mathcal{Q}^{2} active adversary setting (t<n/2), where t is the number of corrupted parties and n is the total number of participants. Compared with SOTA 3PC multiplication over ring [12], our protocol reduces the communication by 33% in the online phase and 67% in the offline phase, respectively. Similarly, the communication of our inner product protocols is also 50% of that in SWIFT [12]. Implementation with GPUs. Since our implementation requires converting secret sharing to an extended ring during the verification phase, this introduces significant computational overhead. However, the extended ring offers excellent concurrency, allowing us to implement our protocol on GPUs. In our specific experiments, compared to ABY3, our implementation achieved a threefold performance improvement, and when compared to Swift, we realized a twofold increase in performance. Implementation of maliciously secure PPML framework. We built a comprehensive privacy-preserving machine learning application against malicious adversaries based on Piranha [27] framework. This includes the implementation of typical CNN models such as VGG and ResNet. Our framework delineates between semi-honest offline and online computation phases, as well as a separate multiplication gate (for both arithmetic and boolean) verification phase. Our experiments demonstrate that the time overhead of the verification phase is significantly lower than that of the online computation phase, indicating that the time introduced by malicious security is far less than the original cost of the semi-honest protocol. Paper Organization. We first propose our maliciously secure 3PC in Sec. III. In Sec. IV, we realize the PPML framework based on our maliciously secure protocols for both linear and non-linear operation. In Sec. V, we benchmark the performance of our protocols and PPML framework."
https://arxiv.org/html/2411.09231v1,AEAKA: An Adaptive and Efficient Authentication and Key Agreement Scheme for IoT in Cloud-Edge-Device Collaborative Environments,"To meet the diverse needs of users, the rapid advancement of cloud-edge-device collaboration has become a standard practice. However, this complex environment, particularly in untrusted (non-collaborative) scenarios, presents numerous security challenges. Authentication acts as the first line of defense and is fundamental to addressing these issues. Although many authentication and key agreement schemes exist, they often face limitations such as being tailored to overly specific scenarios—where devices authenticate solely with either the edge or the cloud—or being unsuitable for resource-constrained devices. To address these challenges, we propose an adaptive and efficient authentication and key agreement scheme (AEAKA) for Cloud-Edge-Device IoT environments. This scheme is highly adaptive and scalable, capable of automatically and dynamically initiating different authentication methods based on device requirements. Additionally, it employs an edge-assisted authentication approach to reduce the load on third-party trust authorities. Furthermore, we introduce a hash-based algorithm for the authentication protocol, ensuring a lightweight method suitable for a wide range of resource-constrained devices while maintaining security. AEAKA ensures that entities use associated authentication credentials, enhancing the privacy of the authentication process. Security proofs and performance analyses demonstrate that AEAKA outperforms other methods in terms of security and authentication efficiency.","With the rapid advancement of information technology, cloud computing has become essential for meeting diverse user needs. Its applications range from online medical diagnostics to dashboard camera video transmission and daily conversations[1]. While cloud computing provides various services, it has limitations for time-sensitive applications, such as real-time data processing and low-latency communication[2, 3]. Edge computing addresses these challenges by relocating computation and data storage to the network edge, reducing latency and enhancing real-time capabilities[4]. However, edge computing’s computational and storage capabilities are limited, requiring cloud support for large-scale tasks[5, 6]. Current research [7, 8, 9, 10] focuses on optimizing the cloud-edge-device architecture, combining the strengths of cloud and edge computing to meet diverse user needs, making it a trend in Internet of Things (IoT) development. A three-layer Cloud-Edge-Device architecture is shown in Fig. 1. In this intricate environment, security is a paramount concern. In April 2024, SOCRadar’s researchers uncovered a Microsoft data breach, exposing employee credentials and internal Bing search engine documents on a public Azure cloud server. Despite the vulnerability being addressed, there’s a risk that malicious actors accessed the data, jeopardizing service security. Similarly, BoAt Lifestyle recently leaked sensitive information of approximately 7.5 million customers, including names, addresses, and contact details. These incidents emphasize the urgent need for robust authentication measures among devices, edge servers, and cloud servers to prevent unauthorized access to sensitive data. Addressing this imperative, an effective authentication and key agreement scheme, facilitating mutual authentication and session key generation, is essential to bolster protection against various security threats. Figure 1: Cloud-Edge-Device architecture overview. Currently, existing authentication and key agreement schemes in cloud-edge-device environments, such as device-to-cloud[11, 12] and device-to-edge schemes[13, 14]. However, these schemes typically apply to single scenarios and do not consider comprehensive cloud-edge-device collaborative scenarios. Due to the complexity of the cloud-edge-device collaborative environment, where time-sensitive tasks may require edge computing and tasks with high computational and storage demands may require cloud computing[5, 6, 3]. Therefore, there is a need for an adaptive, scalable, and highly available cloud-edge-device collaborative authentication scheme. It is evident that combining these single-scenario schemes into a unified cloud-edge-device framework is impractical because they have different system initialization requirements and employ different encryption algorithms. Furthermore, these schemes also have other drawbacks. For instance, in schemes [11, 12, 14, 15], the frequent interaction with trust authorities(TAs) or registration centers(RCs) during the authentication process can lead to an excessive load on TAs and RCs, potentially causing security issues[16, 17]. Additionally, in schemes [18, 19, 20], the use of complex Elliptic Curve Cryptography (ECC) operations and bilinear pairings during authentication may not be suitable for resource-constrained devices, increasing the difficulty of deployment. This research aims to design an adaptive and highly available cloud-edge-device collaborative authentication and key agreement scheme to address the shortcomings of current schemes. Authentication and key agreement in this collaborative environment faces several major challenges: \bullet How to design a highly adaptive and scalable cloud-edge-device collaborative authentication and key agreement architecture? As mentioned earlier, existing research typically considers single authentication scenarios. \bullet How to design an authentication and key agreement protocol that does not rely on third-party TAs, to reduce the load and burden on TAs? \bullet How to design a universally applicable authentication and key agreement protocol for diverse resource-constrained devices? Some complex cryptographic algorithms, such as ECC and bilinear pairings, are unsuitable for resource-constrained devices. \bullet How to achieve authentication and key agreement without disclosing critical device privacy? To address issues related to insider privilege attacks or semi-trusted servers, devices should complete the authentication process without disclosing their own privacy. To address these challenges, we propose the following solutions: First, design a highly available cloud-edge-device collaborative authentication architecture that adaptively initiates different authentication methods according to the specific needs of devices, reducing the complexity of device operations. Second, design an edge-assisted authentication scheme that does not frequently rely on TAs, thereby reducing the load on TAs. Third, We propose a hash-based authentication protocol, making it suitable for most resource-constrained devices. Finally, entities generate related authentication credentials during registration, which are used during subsequent authentication processes, preventing the disclosure of device privacy. The main contributions of this paper include the following: \bullet Designing an adaptive and efficient authentication and key agreement architecture in cloud-edge-device collaborative environments. Devices first request the edge server. If the edge server can meet the service demand, only authenticaiton between the device and the edge server is required. If the edge server cannot meet the service demand, it automatically conducts mutual authentication among the device, the edge server, and the cloud server. The architecture adaptively initiates different authentication and key agreement methods during service requests, reducing device operations’ complexity and enhancing the efficiency of the authentication process and response speed. \bullet Proposing an edge-assisted authentication and key agreement sheme. In this architecture, the authentication and key agreement process no longer frequently relies on third-party TAs. With the assistance of the edge server, mutual anonymous authentication between devices and edge servers and between devices and cloud servers can be achieved. Reducing reliance on TAs not only alleviates the burden on TAs but also mitigates the security risks associated with frequent TAs computations. \bullet Proposing a lightweight authentication and key agreement method. This method uses a hash-based algorithm during the authentication process, avoiding complex ECC operations, thereby significantly reducing the computational overhead of authentication. The lightweight authentication method makes this scheme particularly suitable for various resource-constrained IoT devices, ensuring that even devices with limited performance can perform efficient authentication operations, expanding the overall system’s applicability and practicality. \bullet Introducing associated authentication credentials in the authentication and key agreement protocol. Entities generate associated authentication credentials during registration, which are used during subsequent authentication processes. By pre-generating and using associated credentials, the efficiency and security of the authentication process are ensured. \bullet Formal security proof and performance analysis. We formally prove the security of AEAKA and analyze its computational and communication overhead. The results demonstrate that AEAKA offers high security and performance. The remainder of the paper is organized as follows. In Section II, we review the existing authentication and key agreement mechanisms in cloud-edge-device environment. Section III provides system model of AEAKA. In Section IV, we elaborate on AEAKA. Section V conducts a security performance analysis of AEAKA. In Section VI, we establish an experimental environment to evaluate the performance of the proposed scheme and compare it with other alternatives. Finally, Section VII concludes the paper."
https://arxiv.org/html/2411.09229v1,Efficient and Secure Cross-Domain Data-Sharing for Resource-Constrained Internet of Things,"The growing complexity of Internet of Things (IoT) environments, particularly in cross-domain data sharing, presents significant security challenges. Existing data-sharing schemes often rely on computationally expensive cryptographic operations and centralized key management, limiting their effectiveness for resource-constrained devices. To address these issues, we propose an efficient, secure blockchain-based data-sharing scheme. First, our scheme adopts a distributed key generation method, which avoids single point of failure. This method also allows independent pseudonym generation and key updates, enhancing authentication flexibility while reducing computational overhead. Additionally, the scheme provides a complete data-sharing process, covering data uploading, storage, and sharing, while ensuring data traceability, integrity, and privacy. Security analysis shows that the proposed scheme is theoretically secure and resistant to various attacks, while performance evaluations demonstrate lower computational and communication overhead compared to existing solutions, making it both secure and efficient for IoT applications.","Tth rapid advancement of networks has significantly benefited various domains. Simultaneously, these fields have developed increasingly complex requirements. Data exchange and collaboration across different domains are essential for achieving higher operational efficiency[1][2]. For example, in the Industrial Internet of Things (IIoT), cross-domain data sharing, such as dimensions and specifications, enables precise production[3, 4]. Similarly, in the Medical Internet of Things (MIoT), doctors need to share real-time patient data, such as blood pressure and glucose levels, to make more accurate diagnoses[5, 6]. Likewise, in vehicular networks, cars must share location and incident data to ensure road safety[7, 8]. Fig. 1 shows a typical cloud-based data-sharing framework for the cross-domain IoT. Evidently, data sharing has become a cornerstone for the IoT’s enhanced societal contribution. However, the data-sharing process is fraught with security challenges. For example, in 2019, attackers gained access to a family’s WiFi password, enabling them to monitor the household and ultimately share private videos online without authorization. Likewise, in April 2024, SOCRadar’s security researchers discovered a data leak involving Microsoft’s Azure cloud service, where sensitive information stored on public servers was exposed. The root cause was unauthorized access during sensitive data sharing, leading to malicious attacks and breaches. Thus, efficient and secure data-sharing methods are imperative to ensure the IoT’s safe and reliable operation. Figure 1: Cloud-based data-sharing framework in IoT. Several data-sharing schemes have been proposed for the IoT, including Attribute-Based Encryption (ABE) [9, 10], searchable encryption[11], and Proxy Re-Encryption (PRE) schemes[12, 13]. Nevertheless, these methods have inherent drawbacks. Many rely on computationally expensive cryptographic operations, such as bilinear pairings, which impose a heavy computational burden on resource-constrained IoT devices. Additionally, these schemes often fail to fully address the auditing of data storage, access, and processing[14]. Moreover, key management typically depends on a single authority, introducing key security risks. Blockchain-based data-sharing approaches can mitigate key management issues and facilitate auditing. For instance, Liu et al.[15] and Cui et al.[16] proposed a blockchain-based cross-domain data exchange framework that alleviates key security concerns. However, their approach requires caching numerous pseudonyms generated by the Trusted Authority (TA) for smart devices (SDs), complicating the authentication process, reducing flexibility, and introducing efficiency challenges in pseudonym updates. Wang et al.[14] proposed a blockchain-based data-sharing scheme for the cloud-edge-end architecture in the IIoT environment, which achieved data auditability. However, this solution also suffers from key and pseudonym update complexities, data traceability issues, and overly complex entity interactions. Later, Wang et al.[17] enhanced this scheme to enable secure cross-domain data sharing in cloud-edge-end IIoT environments. Unfortunately, the updated approach relies on computationally expensive bilinear pairings, rendering it unsuitable for resource-constrained devices, and does not adequately address data uploading and traceability processes. Thus, there is an urgent need for a secure and efficient data-sharing scheme in the IoT. This research focuses on designing an efficient, secure, and comprehensive data-sharing scheme for resource-constrained IoT devices. The scheme aims to address the limitations of existing solutions and confronts the following challenges: \bullet How to design a secure and flexible key and pseudonym management scheme that enhances smart device authentication while protecting device privacy? \bullet How to develop a complete, lightweight data-sharing protocol, suitable for resource-constrained devices, that ensures security throughout the processes of data uploading, storage, and sharing? To address these challenges, we propose a blockchain-based distributed key management method that enhances the security and flexibility of the authentication process, and a complete, lightweight, blockchain-based data-sharing scheme, aided by edge computing, that ensures security throughout the entire data-sharing process. The main contributions of this research are as follows: \bullet We propose a blockchain-based cross-domain anonymous authentication method, which adopts a distributed key generation approach to achieve multi-domain key management. It enables SDs to generate their own pseudonyms for communication without relying on pre-generated ones from the TA. Additionally, SDs can independently update their pseudonyms and keys, reducing complex operations on the device side, making the scheme well-suited for resource-constrained SDs. \bullet We develop a blockchain-based and efficent data-sharing protocol that includes processes such as data uploading, storage, and sharing. The protocol binds the SD’s anonymous identity during data uploads, ensuring data traceability while preserving device privacy. Additionally, our scheme supports batch request verification, improving data-sharing efficiency. Leveraging the immutability of blockchain, we ensure data integrity and enhance security throughout the entire sharing process. \bullet We conducted a security analysis of our proposed scheme, and the results demonstrate that our scheme is theoretically secure and can resist certain attacks. Additionally, we analyzed the computational and communication overhead of our scheme. The results show that our scheme outperforms other solutions in terms of both security performance and efficiency. The remainder of the paper is organized as follows. In Section II, we review the existing authentication and data sharing mechanisms in IoT environment. Section III provides preliminaries and background. Section IV provides system model of our scheme. In Section V, we elaborate on our scheme. Section VI conducts a security performance analysis of our scheme. In Section VII, we establish an experimental environment to evaluate the performance of the proposed scheme and compare it with other alternatives. Finally, Section VIII concludes the paper."
https://arxiv.org/html/2411.09228v1,Injection Attacks Against End-to-End Encrypted Applications,"We explore an emerging threat model for end-to-end (E2E) encrypted applications: an adversary sends chosen messages to a target client, thereby “injecting” adversarial content into the application state. Such state is subsequently encrypted and synchronized to an adversarially-visible storage. By observing the lengths of the resulting cloud-stored ciphertexts, the attacker backs out confidential information.We investigate this injection threat model in the context of state-of-the-art encrypted messaging applications that support E2E encrypted backups. We show proof-of-concept attacks that can recover information about E2E encrypted messages or attachments sent via WhatsApp, assuming the ability to compromise the target user’s Google or Apple account (which gives access to encrypted backups). We also show weaknesses in Signal’s encrypted backup design that would allow injection attacks to infer metadata including a target user’s number of contacts and conversations, should the adversary somehow obtain access to the user’s encrypted Signal backup.While we do not believe our results should be of immediate concern for users of these messaging applications, our results do suggest that more work is needed to build tools that enjoy strong E2E security guarantees.","Deployment of end-to-end (E2E) encryption has improved the confidentiality and the integrity of data in various contexts, including messaging [9, 26, 25], cloud storage [10, 3], and other web applications [2]. The security of E2E encrypted messaging protocols [30, 52, 65, 56, 23, 17, 64, 29, 20, 38, 13, 43, 15, 12] and file storage [14, 36, 77] has been studied extensively, giving us confidence that even sophisticated, nation-state level adversaries cannot violate the security of state-of-the-art E2E encryption tools without compromising endpoint devices. To support new features, the complexity of E2E encryption tools is increasing. Messaging applications have recently started to provide backup features that allow users to recover their messages when they need to transition to a new device. WhatsApp [45, 73] and Signal [42], which together account for billions of users [9, 25], both have opt-in backup features. WhatsApp provides automatic upload of backups to a user’s Google Drive or iCloud accounts, while Signal allows users to manually export them. In both cases, backups are encrypted, and should only be decryptable by the legitimate user [45]. Therefore backups should enjoy the same level of confidentiality as E2E encrypted messaging. In this work, we introduce new attacks against E2E encrypted messaging applications. Our most damaging attacks recover partial information about messages or attachments sent from one honest user to a target honest user U. The attacker needs the ability to send adversarial messages to the target user—–thereby “injecting” adversarial content into the application state—–and the ability to observe the target user’s encrypted backups. As such, we refer to these as injection attacks. Our attacks do not invalidate the security of the E2E encrypted messaging protocol used (the Signal protocol in both cases), but rather violate confidentiality via cryptographic vulnerabilities in other parts of the application, namely, their backups. We stress that, in our threat model, the attacker never has access to the backup decryption key. Thus, a priori, an adversary should not be able to learn about U’s conversations with other honest users. To see if this holds true in practice, we perform a security analysis of both WhatsApp and Signal in the context of injection attacks. Application Attack Attack Vector Setting Backups seen Messages sent WhatsApp (1) Dictionary attack on attachments Attachment deduplication Noisy device q \sum_{i\in[q-1]}\lceil n/16^{i}\rceil (2) Dictionary attack on messages zlib compression Quiet device 2\cdot\lceil\log_{2}n\rceil+1 2\cdot\lceil\log_{2}n\rceil+1 (3) Distinguishing attack on messages FTS4 index Quiet device 16 44 Signal (4) Learn number of contacts, messages Serialization method Noisy device 1 1 Figure 1: Summary of the attacks discovered in this work. Here n is the size of the dictionary \altmathcal{V} (possible attachments or messages) and q=\lceil\log_{16}n\rceil. For WhatsApp, we identify three distinct attack vectors. First, attachments such as images, videos, or PDF files are deduplicated—the backup only stores one encrypted copy of each unique attachment—even if it was received from different senders. Second, the serialized database file is compressed using a standard library (zlib) before it is encrypted. Third, WhatsApp uses an SQLite module (FTS4) to build a search index for all messages across all conversations. In all three cases, the length of the encrypted backup ends up being a function of content from different senders, and serves as a side-channel through which the adversary can deduce information about honest messages. In some special cases, exploitation is straightforward for the adversary: for media deduplication, the adversary can observe a backup, send U a candidate media file, and observe the subsequent backup. If U has already received this file before, the size of the second backup ciphertext will grow by less than what is expected. We show a more sophisticated attack approach that is robust to most kinds of noise (other, unrelated activity on U’s device such as receiving other attachments beyond the target messages) and allows determining which of n attachments were received by U. It requires the adversary to observe at most \lceil\log_{16}n\rceil backups. We note that deduplication exploits have been considered in other contexts, such as encrypted storage [36]; however, as far as we know, this is the first work that shows this issue arises in encrypted messaging backups. Exploiting zlib compression required understanding complex interactions between database serialization and compression, and how sending messages affects the resultant backup. Nevertheless, we show a binary-search style injection attack that determines which of n messages was recently received by U, by adaptively injecting at most 2\cdot\lceil\log_{2}n\rceil+1 messages and observing the same number of backups. We have demonstrated this attack in a lab setting where the victim U’s client application has no other activity during the attack. We call this scenario the “quiet device” setting. We experimentally verify the attack for small n. Finally, we show that even if zlib compression and deduplication were turned off, there are additional sources of leakage that stem from the use of a text keyword search index called FTS4. This is a delicate vulnerability that arises due to subtle interactions between WhatsApp’s use of FTS4, the inner mechanics of the B-tree data structures used to store the index, and SQLite serialization. Our attack consequently is technically complicated, in large part because the adversary does not initially know the internal state of the target’s data structures, and must account for this by adaptively modifying the state via injections to enable learning confidential information. Nevertheless, we experimentally demonstrate that in the quiet device setting, an attacker can determine which of two messages U has received from an honest party by injecting at most 44 messages and observing at most 16 backups. This attack vector may be of relevance to other applications that index mixtures of trusted and untrusted data using FTS4 (e.g., multi-tenant search services [70]). We also explored Signal, whose bespoke serialization and encryption mechanism avoids many of the problems above. Nevertheless, we built an attack which exploits the fact that the structure of their backups leaks the size of each row in the target U’s client-side database. Combining an injection attack with additional heuristics, an adversary can infer which rows are part of which tables, by observing only two backups. This allows them to learn the number of messages U has received, U’s number of contacts, and more. We discuss how this attack can also be adapted to work in the “noisy device” setting, either assuming the size of messages sent by honest parties (the noise) have bounded length, or by injecting a small sequence of random-length messages. These only require a single backup. Contributions We are the first to explore injection attacks against state-of-the-art encrypted messaging applications that utilize encrypted backups. A summary of our attacks appears in Figure 1. These demonstrate how to violate the confidentiality of messages and attachments sent on WhatsApp and, for Signal, how to efficiently reveal potentially sensitive metadata about a user’s contacts and messages. While our in-lab experiments do not indicate that injection attacks are an immediate threat to user privacy, they do highlight previously unrecognized challenges faced when attempting to achieve E2E confidentiality guarantees in adversarial settings. We therefore discuss potential countermeasures in the body. While some attacks have straightforward mitigations, others uncover the need for additional work to find solutions to building backups that are both efficient and secure. Beyond backups, and given the expanding set of applications being built with E2E encryption guarantees in mind, our results motivate future work on principled mechanisms for discovering and mitigating injection attacks. Ethics and responsible disclosure Our experiments involved researcher accounts that were not used for other purposes and minimal load on Signal and WhatsApp services, requiring a small number of messages sent at a reasonable pace. To see how attacks can scale up in a way that might load servers, we implemented simulators whose results we validated via smaller manual experiments with real clients. We disclosed our findings to Signal and WhatsApp. Signal acknowledged the vulnerabilities and deployed mitigations in the v1 revision of their Android backup file format,111Mitigations rolled out in Android builds following commit c6473ca9e63236af3eae9959a50cfa643d53272e in their open-source repository [67]. per our recommendations from Section 6. WhatsApp acknowledged receipt of our disclosure, and awarded us a bug bounty. At the time of writing, they have not yet deployed mitigations."
https://arxiv.org/html/2411.09178v1,SAFES: Sequential Privacy and Fairness Enhancing Data Synthesis for Responsible AI,"As data-driven and AI-based decision making gains widespread adoption in most disciplines, it is crucial that both data privacy and decision fairness are appropriately addressed. While differential privacy (DP) provides a robust framework for guaranteeing privacy and several widely accepted methods have been proposed for improving fairness, the vast majority of existing literature treats the two concerns independently. For methods that do consider privacy and fairness simultaneously, they often only apply to a specific machine learning task, limiting their generalizability. In response, we introduce SAFES, a Sequential PrivAcy and Fairness Enhancing data Synthesis procedure that sequentially combines DP data synthesis with a fairness-aware data transformation. SAFES allows full control over the privacy-fairness-utility trade-off via tunable privacy and fairness parameters. We illustrate SAFES by combining AIM, a graphical model-based DP data synthesizer, with a popular fairness-aware data pre-processing transformation. Empirical evaluations on the Adult and COMPAS datasets demonstrate that for reasonable privacy loss, SAFES-generated synthetic data achieve significantly improved fairness metrics with relatively low utility loss. keywords: Differential privacy, Machine learning fairness, Synthetic data","Data-driven and AI-based decision making are being adopted in many disciplines and data collected as part of this process often contain sensitive data from individuals. These data are frequently used to make socially impactful decisions including, but not limited to, determining who gets approved for a loan, predicting which applicants should be hired for a job, or forecasting which previously convicted criminals will re-offend. While such applications can and do have legitimate benefits, it is of paramount importance to ensure the use of such sensitive data is responsible and carried out with the highest possible ethical standards. There are two important ethical concerns when working with sensitive personal data in training machine learning (ML) and AI algorithms: privacy and fairness. Even anonymized datasets can be leveraged by attackers to infer masked or removed data (Narayanan and Shmatikov,, 2008; Ahn,, 2015; Sweeney,, 2015) and blackbox access to a model is sufficient to infer membership in the training data (Shokri et al.,, 2017), which can itself be a privacy violation. Knowing that an individual belongs to a dataset used for recidivism prediction, for example, is equivalent to knowing that person was convicted of a crime. The perpetuation of social discrimination, bias, and other forms of unfairness in the decisions made by ML/AI models raises another major ethical concern. One famous previous work, for example, demonstrated the presence of discrimination against darker skin colors in commercial gender classification systems (Buolamwini and Gebru,, 2018). The naïve approach of simply removing the group indicator when training these models has been shown insufficient to properly ensure fairness (Calders and Žliobaitė,, 2013). Fairness has increasingly been recognized in the ML community as a complicated notion and a lot of research has been devoted to defining and ensuring fairness (Verma and Rubin,, 2018). Since datasets with privacy concerns are likely to also have fairness concerns, and vice versa, it is critical to develop efficient privacy- and fairness-enhancing methods for releasing and analyzing data. In this work, we propose a framework for synthesizing data that simultaneously addresses privacy and fairness concerns by strategically and sequentially combining privacy-preserving data synthesis and fairness-aware pre-processing transformations. We aim to provide a generalized solution that safeguards sensitive personal information, upholds fairness, and keeps the utility of released data close to the original. There remains a scarcity of research on general-use synthetic data that both satisfies DP guarantees and reduces structural bias, representing a critical area for advancing responsible AI. We address this gap by proposing SAFES – a Sequential PrivAcy and Fairness Enhancing data Synthesis procedure – which combines DP data synthesis with a fairness-aware pre-processing transformation. To our knowledge, this is the first work to attempt such an approach. The output of SAFES is a synthetic dataset that both has theoretical privacy guarantees and has been adjusted in a way to improve structural bias, which in turn improves fairness metrics for downstream classifiers. SAFES has several benefits. First, it is fully tunable with regards to both the privacy guarantees and fairness constraints. Second, for tight fairness constraints, SAFES exhibits fairness robustness measured by various metrics across a wide range of privacy guarantees per our empirical results, implying that one can adjust the balance between privacy and utility without a significant sacrifice in fairness. Third, though our examples and experiments focus on a commonly used DP data synthesizer and a well-known fairness-aware data transformation methods, SAFES is a general framework and can admit different DP synthesizers of various DP guarantees and different fairness-aware data transformations satisfying various fairness metrics."
https://arxiv.org/html/2411.09167v1,Robust AI-Synthesized Speech Detection Using Feature Decomposition Learning and Synthesizer Feature Augmentation,"AI-synthesized speech, also known as deepfake speech, has recently raised significant concerns due to the rapid advancement of speech synthesis and speech conversion techniques. Previous works often rely on distinguishing synthesizer artifacts to identify deepfake speech. However, excessive reliance on these specific synthesizer artifacts may result in unsatisfactory performance when addressing speech signals created by unseen synthesizers. In this paper, we propose a robust deepfake speech detection method that employs feature decomposition to learn synthesizer-independent content features as complementary for detection. Specifically, we propose a dual-stream feature decomposition learning strategy that decomposes the learned speech representation using a synthesizer stream and a content stream. The synthesizer stream specializes in learning synthesizer features through supervised training with synthesizer labels. Meanwhile, the content stream focuses on learning synthesizer-independent content features, enabled by a pseudo-labeling-based supervised learning method. This method randomly transforms speech to generate speed and compression labels for training. Additionally, we employ an adversarial learning technique to reduce the synthesizer-related components in the content stream. The final classification is determined by concatenating the synthesizer and content features. To enhance the model’s robustness to different synthesizer characteristics, we further propose a synthesizer feature augmentation strategy that randomly blends the characteristic styles within real and fake audio features and randomly shuffles the synthesizer features with the content features. This strategy effectively enhances the feature diversity and simulates more feature combinations. Experimental results on three deepfake speech benchmark datasets demonstrate that our model achieves the state-of-the-art robust detection performance across various evaluation scenarios, including cross-method, cross-dataset, and cross-language evaluations.","With the rapid advancement of deep learning techniques, deepfake technology, including the synthesis and manipulation of multimedia content, has become increasingly accessible [1]. The recent advancements in deepfake generation methods have enabled the creation of multimedia content with remarkable reality, presenting a significant threat to the security of multimedia information [2], such as impersonation attack [3], reputation damage, or online harassment [4]. Despite the considerable focus on deepfake video detection, research on deepfake speech detection remains relatively underdeveloped [5]. Deepfake speech, also known as AI-synthesized speech, involves the synthesis or manipulation of speech waveforms to replace the original audio content with artificially generated content. Two common deepfake speech generation methods are text-to-speech (TTS) and voice conversion (VC), both of which typically utilize neural vocoders to produce audio waveforms based on temporal-frequency representations. TTS methods allow for the synthesis of audio with specific voice styles from text inputs [6], while VC methods enable the modification of voice styles while retaining the original content [7]. The advancement of both TTS and VC technologies has significantly increased the challenge of distinguishing between genuine and fake speech signals using human perception [8]. To address the potential threat caused by deepfake speech, it is imperative to develop effective detection methods capable of distinguishing between genuine and fake speech signals [9]. Initially, early deepfake speech detection methods primarily relied on specific statistical features inherent to audio signals, such as Mel-frequency cepstral coefficient (MFCC) [10], linear frequency cepstral coefficients (LFCC) [11], constant Q cepstral coefficients (CQCC) [12], and Fourier bi-spectrum [13]. However, these methods have shown limited effectiveness against the rapid development of deepfake speech generation techniques. Recently, some well-designed deep learning models have emerged to address the challenge of deepfake speech detection. These models include multi-task learning networks [9], unsupervised pre-training models [14], graph neural networks [15], multi-view-based networks [16], and ResNet-based networks [17]. These models directly learn discriminative features from speech and perform well in intra-dataset evaluation. However, they exhibit unsatisfactory performance on unseen synthesizers or real-word data [18]. This is attributed to the inherent limitations of their feature learning strategies, which cause the detection model to focus on specific synthesizer artifacts overly. Consequently, these methods are ineffective when dealing with new types of synthesizers. In this study, we propose a new approach for robust deepfake speech detection using feature decomposition learning and synthesizer feature augmentation. Our goal is to enhance detection robustness by learning synthesizer-independent content features as complementary features. We first design a dual-stream feature decomposition learning strategy that employs a synthesizer stream and a content stream to decompose the speech representation learned from the backbone model. The synthesizer stream is responsible for learning the synthesizer-related features through supervised training with synthesizer labels, while the content stream focuses on learning synthesizer-independent content features. As direct content-related labels for training are unavailable, we employ a pseudo-labeling-based supervised learning method for the content stream. This method generates compression and speed labels for training by randomly altering speech characteristics through applying various compression levels and codecs, as well as adjusting the speech speed. Additionally, we employ an adversarial learning method to reduce the synthesizer-related components in the content stream. This involves integrating an adversarial loss to force the classification probabilities of synthesizers based on content features to resemble random guessing. For classification, we concatenate the synthesizer and content features to determine whether the input speech is synthesized. To further enhance the detection robustness of our method on different synthesizer characteristics, we propose a feature augmentation strategy consisting of feature blending and shuffle operations. The feature blending operation randomly merges the characteristic styles within each class of feature to enhance feature diversity, while the feature shuffle operation mixes synthesizer features with content features to simulate more synthesizer-content feature combinations. The main contributions of this work are summarized as follows: • We develop a robust detection model that employs dual-stream feature decomposition learning to detect AI-synthesized speech. Different from previous methods overly relying on specific vocoder artifacts, our method employs feature decomposition to learn vocoder-independent features as the complementary feature for detection. • We propose a synthesizer feature augmentation strategy to enhance the model’s robustness to different synthesizer characteristics and synthesizer-content feature combinations. • We conduct extensive experiments on three benchmark datasets, and the results demonstrate that our method achieves state-of-the-art detection performance and exhibits robust generalizability across diverse synthesizer methods, datasets, and languages. This paper is structured as follows: a literature review is presented in Section II. The architecture and methodology of our two-stream network are presented in Section III. Section IV presents the implementation details and the experimental results. Sections VI illustrates ablation studies and discusses the effectiveness of model components. Finally, Section VII summarizes the conclusion and future work. Figure 1: Network architecture of our method. A main stream is used to learn robust speech representation from the log-scale frequency spectrogram of the input speech. Subsequently, a dual-stream learning strategy, comprising a synthesizer stream and a content stream, is employed to decompose the learned speech representation. The final classification is performed based on the concatenation of the synthesizer and content features. A synthesizer feature augmentation strategy consisting of feature blending and feature shuffle operations is employed to enhance the model’s robustness to different synthesizer characteristics and synthesizer-content feature combinations."
https://arxiv.org/html/2411.09142v1,Laplace Transform Interpretation of Differential Privacy,"We introduce a set of useful expressions of Differential Privacy (DP) notions in terms of the Laplace transform of the privacy loss distribution. Its bare form expression appears in several related works on analyzing DP, either as an integral or an expectation. We show that recognizing the expression as a Laplace transform unlocks a new way to reason about DP properties by exploiting the duality between time and frequency domains. Leveraging our interpretation, we connect the (q,\rho(q))-Rényi DP curve and the (\varepsilon,\delta(\varepsilon))-DP curve as being the Laplace and inverse-Laplace transforms of one another. This connection shows that the Rényi divergence is well-defined for complex orders q=\gamma+i\omega. Using our Laplace transform-based analysis, we also prove an adaptive composition theorem for (\varepsilon,\delta)-DP guarantees that is exactly tight (i.e., matches even in constants) for all values of \epsilon. Additionally, we resolve an issue regarding symmetry of f-DP on subsampling that prevented equivalence across all functional DP notions.","Differential privacy (DP) [13] has become a widely adopted standard for quantifying privacy of algorithms that process statistical data. In simple terms, differential privacy bounds the influence a single data-point may have on the outcome probabilities. Being a statistical property, the design of differentially private algorithms involves a pen-and-paper analysis of any randomness internal to the processing that obscures the influence a data-point might have on its output. A clear understanding of the nature of differential privacy notions is therefore tantamount to study and design of privacy-preserving algorithms. Throughout its exploration, various functional interpretations of the concept of differential privacy have emerged over the years. These include the privacy-profile curve \delta(\epsilon) [5] that traces the (\epsilon,\delta)-DP point guarantees, the f-DP [11] view of worst-case trade-off curve between type I and type II errors for hypothesis testing membership [19, 6], the Rényi DP [23] function of order q that admits a natural analytical composition [1, 23], the view of the privacy loss distribution (PLD) [29] that allows for approximate numerical composition [20, 18], and the recent characteristic function formulation of the dominating privacy loss random variables Zhu et al. [32]. Each of these formalisms have their own properties and use-cases, and none of them seem to be superior in all aspects. Regardless of their differences, they all have some shared difficulties—certain types of manipulations on them are harder to perform in the time-domain, but considerably simpler to do in the frequency-domain. For instance, Koskela et al. [20] noted that composing PLDs of two mechanisms involve convolving their probability densities, which can be numerically approximated efficiently by multiplying their Discrete Fast-Fourier Transformations (DFFT) and then inverting it back to get the convolved density using Inverse-DFFT. Such maneuvers are also frequently performed for analytical reasons while proving properties of differential privacy, often without even realizing this detour through the frequency-domain. A notable example of this is the analysis of Moments’ accountant by Abadi et al. [1], where the authors bound higher-order moments of subsampled Gaussian distributions, compose the moments through multiplication, and then derive the (\varepsilon,\delta)-DP bound on the DP-SGD mechanism. Their analysis goes the through frequency space, as the moment generating function of a random variable corresponds to the two-sided Laplace transform of its probability density function [22]. Often times when dealing with a functional notion of DP, expressing components like expectations or cumulative densities their integral form ends up being a Fourier or a Laplace transform. Realizing them as such can be tremendously useful in analysis. In this paper, we formalize these time-frequency domain dualisms enjoyed by the functional representations into a new interpretation of differential privacy. In addition to augmenting existing perspectives on DP, this interpretation provides a flexible analytical toolkit that greatly extends our cognitive reach in reasoning about DP and its underpinnings. This interpretation is based on recognizing that the privacy-profile \delta_{P|Q}(\varepsilon):=\sup_{S}P(S)-e^{\varepsilon}\cdot Q(S) and the Rényi-divergence \mathrm{R}_{q}\left(P\middle\|Q\right):=\frac{1}{q-1}\int_{\Omega}P^{q}Q^{1-q}% \mathrm{d}\theta between any two distributions P,Q on the same space \Omega can be seen as a Laplace transform111Laplace transform maps a time-domain function g(t) with t\in\mathbb{R} to a function \mathcal{L}\left\{g\right\}(s):=\int_{0}^{\infty}e^{-st}g(t)\mathrm{d}t with s\in\mathbb{C} in the complex space. Similarly, bilateral Laplace transform of g(t) is defined as \mathcal{B}\left\{g\right\}(s):=\int_{-\infty}^{\infty}e^{-st}g(t)\mathrm{d}t. of the privacy loss distribution \mathrm{PLD}(P\|Q), the distribution of privacy loss random variable Z=L_{P|Q}(\Theta) where \Theta\sim P: \displaystyle\forall\varepsilon\in\mathbb{R}\ \displaystyle:\ \delta_{P|Q}(\varepsilon)=\underset{Z\leftarrow\mathrm{PLD}(P% \|Q)}{\mathbb{E}}\left[\max\{0,1-e^{\varepsilon-Z}\}\right]=\mathcal{L}\left\{% 1-F_{Z}(t+\varepsilon)\right\}(1), (1) \displaystyle\forall q\in\mathbb{C}\ \displaystyle:\ e^{(q-1)\cdot\mathrm{R}_{q}\left(P\middle\|Q\right)}=\underset% {Z\leftarrow\mathrm{PLD}(P\|Q)}{\mathbb{E}}\left[e^{(q-1)\cdot Z}\right]=% \mathcal{B}\left\{f_{Z}(t)\right\}(1-q), (2) where F_{Z}(t)=\Pr[Z<t] is the cumulative distribution function and f_{Z}(t)=\int_{\{\theta\in\Omega:L_{P|Q}(\theta)=z\}}P\mathrm{d}\theta is the (generalized) density function of the privacy loss random variable Z. The first equality in (1) is a widely used way to represent the (\varepsilon,\delta(\varepsilon))-DP curve in literature [29, 6, 5, 20, 18, 30, 9]. Similarly, the first equality in (2) represents the well-known moment-generating function of privacy loss [23, 1, 6]. The second equalities above are part of a set of Laplace expressions presented in this paper. Together, these expressions unlock a formal approach to perform a wide-variety of manipulations and transformations on them using the fundamental properties of the Laplace functional (see Table 1). Using them, we show that the privacy-profile and Rényi divergence between any two distributions have the following equivalence. \forall q\in\mathbb{C}\ :\ e^{(q-1)\cdot\mathrm{R}_{q}\left(P\middle\|Q\right)% }=q(q-1)\cdot\mathcal{B}\left\{\delta_{P|Q}(t)\right\}(1-q), (3) which again is a Laplace transform expression. Furthermore, Zhu et al. [32]’s characteristic function of the privacy loss \phi_{P|Q}(q):=\underset{P}{\mathbb{E}}\left[e^{iq\log(P/Q)}\right] also turns out to be a Fourier transform, which is a special case of the bilateral Laplace transform: \forall q\in\mathbb{R}\ :\ \phi_{P|Q}(q)=\underset{Z\leftarrow\mathrm{PLD}(P\|% Q)}{\mathbb{E}}\left[e^{iqZ}\right]=\mathcal{B}\left\{f_{Z}\right\}(-iq). (4) These expressions can take advantage of the relationship between their time-domain and complex frequency-domain representations, as certain manipulations are more straightforward in one domain as compared to the other. Using the Laplace transform interpretations extensively, our paper presents the following findings. 1. We note that the Laplace transform expression of Rényi divergence permits the order q to be a complex number in \mathbb{C}. Based on this observation, we revisit the discussion on equivalence and interconversion between (q,\rho)-Rényi DP and (\varepsilon,\delta)-DP in literature [6, 32, 2, 9]. We show that the privacy-profile curve \delta_{P|Q}(\varepsilon) and the Rényi divergence \mathrm{R}_{q}\left(P\middle\|Q\right) as a function of q are equivalent as long as either P is absolutely continuous w.r.t. Q (denote as P\ll Q) or Q\ll P; absolute continuity in both directions is not necessary. Moreover, we establish that while \delta_{P_{1}|Q_{1}}(\varepsilon)\leq\delta_{P_{2}|Q_{2}}(\varepsilon) for all \varepsilon implies that \mathrm{R}_{q}\left(P_{1}\middle\|Q_{1}\right)\leq\mathrm{R}_{q}\left(P_{2}% \middle\|Q_{2}\right) for all q>1, the converse does not hold. This is due to the fact that the dominance relationship between privacy profiles \delta_{P_{1}|Q_{1}}(\varepsilon) and \delta_{P_{2}|Q_{2}}(\varepsilon) depends on how the Rényi divergence curves \mathrm{R}_{q}\left(P_{1}\middle\|Q_{1}\right) and \mathrm{R}_{q}\left(P_{2}\middle\|Q_{2}\right) behave along the complex line \{q\in\mathbb{C}:\mathfrak{Re}(q)=c\} at any c\in\mathbb{R} for which the two divergences exist; not along (1,\infty). 2. Among all functional notions of DP, exactly tight adaptive composition theorem is only known for Rényi DP in an explicit form222Dong et al. [11] examine tight composition under the f-DP framework by defining an abstract composition operation, denoted f_{1}\otimes f_{2}. However, they do not provide an explicit form for this operator for a general trade-off function f, offering it only for the specific case of the Gaussian trade-off function G_{\mu}.[23, 8]. And, for the PLD formalism, only non-adaptive composition theorems are known that are exactly tight333Unlike under non-adaptivity, composing two privacy loss random variables Z_{1} and Z_{2} does not amount to convolving their privacy loss distributions (PLDs) f_{Z_{1}}\circledast f_{Z_{2}} when the mechanisms are adaptive because Z_{1},Z_{2} become dependent. We note that Gopi et al. [18] seem to incorrectly assert their Theorem 5.5 to be valid under adaptivity. [18, 29, 20]. In this paper, we establish an exactly tight theorem for composing any two privacy profiles, \delta_{P_{1}|Q_{1}}(\varepsilon) and \delta_{P_{2}|Q_{2}}(\varepsilon), leveraging time-frequency dualities with their Rényi divergence curves. Our composition method also extends to adaptive scenarios, provided that the conditional distributions P_{2}^{\theta} and Q_{2}^{\theta}, given an observation \theta from the first distribution pair, are dominated by a privacy profile for all \theta, which is a standard assumption for adaptive composition guarantees. 3. We apply our composition theorem for privacy profiles to derive an optimal composition theorem for (\epsilon_{i},\delta_{i})-point guarantees of differential privacy. Our approach begins by determining the worst-case privacy profile \delta_{i}(\epsilon) that any (\epsilon_{i},\delta_{i})-DP mechanism must satisfy. We then use our composition theorem to derive the combined privacy profile \delta^{\otimes}(\epsilon). This provides the most precise composition guarantee possible when given only that a sequence of mechanisms each satisfies an (\epsilon_{i},\delta_{i})-DP point guarantee. Our bound surpasses the optimal composition result in Kairouz et al. [19, Theorem 3.3] because, whereas their result only provides a discrete set of (\epsilon,\delta) values met by the composed curve, ours forms a continuous curve. This continuity enables us to determine the tightest \epsilon value for any given \delta budget. We also show that our results align with the bounds generated by numerical accountants such as Google’s PLDAccountant [12] and Microsoft’s PRVAccountant [18]. 4. The concept of f-DP introduced by Dong et al. [11] provides a functional perspective on the indistinguishability between two distributions P and Q through hypothesis testing. The function f:[0,1]\rightarrow[0,1] represents a bound on the trade-off T(P,Q):[0,1]\rightarrow[0,1] between Type-I and Type-II errors for any test aimed at determining whether a sample \theta originates from P or Q. Unlike other functional notions of differential privacy, f-DP is unique in being not connected to the rest via a Laplace transform. Instead, Dong et al. [11] establish that the privacy profile \delta(\varepsilon) and the trade-off curve f of a mechanism exhibit a convex-conjugate relationship, also known as Fenchel duality. However, Dong et al. [11, Proposition 2.12] confirm this functional equivalence only when f is symmetric. With Poisson subsampling at probability p, the resulting amplified curve f_{p}(x)=pf(x)+(1-p)\cdot x becomes asymmetric. To ensure symmetry, the subsampling result [11, Theorem 4.2] applies a p-sampling operator C_{p}(f):\min\{f_{p},f_{p}^{-1}\}^{**} that overestimates the f_{p}-curve. We show that this symmetrization step disrupts the equivalence between f-DP and privacy profile formalisms (and thereby with other functional notions). To address this, we propose maintaining the natural asymmetry in f-DP and avoid the need for this symmetrization step by adopting a convention on the direction of skew. This completes the equivalences across all functional notions of DP. Related work: Our work builds an interpretation of differential privacy by leveraging several works that appeared before. This includes, but not limited to the works on various interpretations of privacy by Dong et al. [11], Dwork and Rothblum [14], Dwork et al. [16], Mironov [23], Bun and Steinke [8], Sommer et al. [29], Gopi et al. [18], Koskela et al. [20], Zhu et al. [32]. In particular, the work of Zhu et al. [32] shares the most similarity with ours, as they were the first to observe that many functional notions of differential privacy appear to be linked via Laplace or Fourier transforms. However, their work centers on using the characteristic function of privacy loss (in (4)) as an intermediate functional representation connecting various DP notions. In contrast, we examine the nature of these connections themselves to harness the perspective of Laplace transformations as an analytical toolkit for differential privacy. Relevant studies on composition theorems for differential privacy include Dwork et al. [15], Kairouz et al. [19], Murtagh and Vadhan [24], Bun and Steinke [8], Mironov [23], along with numerical accounting methods such as those by Gopi et al. [18], Koskela et al. [20], Doroshenko et al. [12]. Paper structure: After reviewing preliminaries on DP and Laplace transforms in Section 2, we present an equivalent description of both the \delta_{P|Q}(\varepsilon) privacy profile and the \mathrm{R}_{q}\left(P\middle\|Q\right)-Rényi divergence curve in terms of a set of Laplace transforms of the privacy loss distribution’s probability function in Section 3 and use them to connect the two notions. After discussing the implications of these connections, we provide our composition results for privacy profiles and (\varepsilon,\delta)-DP point guarantees in Section 4. Finally, in Section 5 we discuss the problem of asymmetry in functional notions and an approach to handling it without breaking equivalences."
https://arxiv.org/html/2411.08933v2,"Confidence-aware Denoised Fine-tuning of 
Off-the-shelf Models for Certified Robustness","The remarkable advances in deep learning have led to the emergence of many off-the-shelf classifiers, e.g., large pre-trained models. However, since they are typically trained on clean data, they remain vulnerable to adversarial attacks. Despite this vulnerability, their superior performance and transferability make off-the-shelf classifiers still valuable in practice, demanding further work to provide adversarial robustness for them in a post-hoc manner. A recently proposed method, denoised smoothing, leverages a denoiser model in front of the classifier to obtain provable robustness without additional training. However, the denoiser often creates hallucination, i.e., images that have lost the semantics of their originally assigned class, leading to a drop in robustness. Furthermore, its noise-and-denoise procedure introduces a significant distribution shift from the original distribution, causing the denoised smoothing framework to achieve sub-optimal robustness. In this paper, we introduce Fine-Tuning with Confidence-Aware Denoised Image Selection (FT-CADIS), a novel fine-tuning scheme to enhance the certified robustness of off-the-shelf classifiers. FT-CADIS is inspired by the observation that the confidence of off-the-shelf classifiers can effectively identify hallucinated images during denoised smoothing. Based on this, we develop a confidence-aware training objective to handle such hallucinated images and improve the stability of fine-tuning from denoised images. In this way, the classifier can be fine-tuned using only images that are beneficial for adversarial robustness. We also find that such a fine-tuning can be done by merely updating a small fraction (i.e., 1%) of parameters of the classifier. Extensive experiments demonstrate that FT-CADIS has established the state-of-the-art certified robustness among denoised smoothing methods across all \ell_{2}-adversary radius in a variety of benchmarks, such as CIFAR-10 and ImageNet.","Despite the recent advancements in modern deep neural networks in various computer vision tasks (Radford et al., 2021; Rombach et al., 2022; Kirillov et al., 2023), they still suffer from the presence of adversarial examples (Szegedy et al., 2013) i.e., a non-recognizable perturbation (for humans) of an image often fools the image classifiers to flip the output class (Goodfellow et al., 2014). Such adversarial examples can be artificially crafted with malicious intent, i.e., adversarial attacks, which pose a significant threat to the practical deployment of deep neural networks. To alleviate this issue, various approaches have been proposed to develop robust neural networks, such as adversarial training (Madry et al., 2018; Wang et al., 2019) and certified defenses (Wong & Kolter, 2018; Cohen et al., 2019; Li et al., 2023). Among these efforts, randomized smoothing (Lecuyer et al., 2019; Cohen et al., 2019) has gained much attention as a framework to build robust classifiers. This is due to its superior provable guarantee of the non-existence of adversarial examples, i.e., certified robustness (Wong & Kolter, 2018; Xiao et al., 2018), under any perturbations confined in a \ell_{2}-norm. Specifically, it builds a smoothed classifier through taking a majority vote from a base classifier, e.g., a neural network, under Gaussian perturbations of the given input image. However, it has been practically challenging to scale the model due to a critical drawback: the base classifier should be specifically trained on noise-augmented data (Lecuyer et al., 2019; Cohen et al., 2019). Recently, Lee (2021); Carlini et al. (2023) have introduced denoised smoothing which utilizes pre-trained off-the-shelf classifiers within the randomized smoothing framework. Rather than directly predicting the label of a noise-augmented image, it first feeds the perturbed image into a denoiser, e.g., a diffusion model, and then obtains the predicted label of the denoised image using off-the-shelf pre-trained classifiers that have been trained on clean images. Intriguingly, denoised smoothing with recently developed diffusion models and pre-trained classifiers, e.g., guided diffusion (Dhariwal & Nichol, 2021) and BEiT (Bao et al., 2022), shows its superior scalability with comparable certified robustness in \ell_{2}-adversary to the current state-of-the-art methods (Horváth et al., 2022b; Jeong et al., 2023). Figure 1: Overview of FT-CADIS framework. (1) Confidence-aware denoised image selection: for a given clean image, we create denoised images and find non-hallucinated images. (2) Fine-tuning with confidence-aware denoised image selection: we propose fine-tuning objectives to improve both generalizability and robustness of the smoothed classifier based on selected non-hallucinated images. On the other hand, denoised smoothing also exhibits clear limitations. Firstly, denoised images do not follow the standard pre-training distribution of the classifiers, which results in a limited robustness of the denoised smoothing framework. Secondly, fine-tuning the pre-trained classifiers with the denoised images also yields sub-optimal classifiers due to the hallucinated images (Carlini et al., 2023), i.e., the diffusion denoiser tends to generate image semantics from an incorrect class rather than the originally assigned class (see Figure 4). Consequently, denoised smoothing with such classifiers leads to a drop of the certified accuracy, especially in the large \ell_{2}-radius regime, i.e., high Gaussian variance (see Table 3). Contribution. In this paper, we aim to address the aforementioned issues of denoised smoothing by designing a fine-tuning objective for off-the-shelf classifiers that distinguishes between hallucinated images, i.e., images that have lost the original semantics after denoising, and non-hallucinated images, i.e., images that maintain the original semantics after denoising. To this end, we propose to use the “likelihood of denoised images”, i.e., confidence, of the off-the-shelf classifier with respect to the originally assigned class as a proxy for determining whether an image is hallucinated and then fine-tune the classifier with non-hallucinated images only. Consequently, we have developed a confidence-aware training objective based on the likelihood of denoised images to effectively discriminate hallucinated images (see Figure 1). Specifically, we propose a scalable and practical framework for fine-tuning off-the-shelf classifiers, coined Fine-Tuning with Confidence-Aware Denoised Image Selection (FT-CADIS), which improves certified robustness under denoised smoothing. In order to achieve this, two new losses are defined: the Confidence-aware selective cross-entropy loss and the Confidence-aware masked adversarial loss. Two losses are selectively applied only to non-hallucinated images, thereby ensuring that the overall training process avoids over-optimizing hallucinated samples, i.e., samples that are harmful for generalization, while maximizing the robustness of smoothed classifiers. Our particular loss design is motivated by Jeong et al. (2023), who were the first to investigate training objectives for randomized smoothing depending on sample-wise confidence information. We demonstrate that our novel definition of confidence in randomized smoothing, specifically through the ratio of non-hallucinated images from a denoiser, can dramatically stabilize the confidence-aware training, overcoming its previous limitation of severe accuracy degradation (e.g., see Table 3). In our experiments, we have validated the effectiveness of our proposed method on standard benchmarks for certified \ell_{2}-robustness, i.e., CIFAR-10 (Krizhevsky, 2009) and ImageNet (Russakovsky et al., 2015). Our results show that the proposed method significantly outperforms existing state-of-the-art denoised smoothing methods in certified robustness across all \ell_{2}-norm setups, while updating only 1% of the parameters of off-the-shelf classifiers on ImageNet. In particular, FT-CADIS significantly improves the certified robustness in the high Gaussian variance regime, i.e., high certified radius. For instance, FT-CADIS outperforms the best performing baseline, i.e., diffusion denoised (Carlini et al., 2023), by 29.5% \rightarrow 39.4% at \varepsilon = 2.0 for ImageNet experiments."
https://arxiv.org/html/2411.08916v1,Enhanced Secure Transmission of Medical Images through OFDM using Hyperchaotic Systems,"Orthogonal Frequency Division Multiplexing (OFDM) is a popular modulation technique for transmitting digital data over wireless radio channels, including medical images due to its high transmission capacity, low interference, bandwidth efficiency, and scalability. However, the security of medical images is a major concern, and combining OFDM with encryption techniques such as chaos-based image encryption can enhance security measures. This study proposes a secure medical image transmission system that combines OFDM, 6D hyperchaotic system, and Fibonacci Q-matrix and analyzes its impact on image transmission quality using simulation results obtained through MATLAB. The study examines the Q-PSK constellation diagram, fast Fourier transform (IFFT) signal, cyclic prefix (CP) techniques, NIST, signal noise ratio (SNR), and bit error rate (BER). The results provide insights into the effectiveness of OFDM in securely transmitting high-quality medical images.","In recent years, data security has become increasingly important for data transmission. Medical images are commonly used in various processes [1], and different methods and technologies such as data hiding [2], steganography [3], and encryption [4] have been developed to protect digital images. Among these methods, image encryption is the most straightforward because it converts meaningful images into unrecognizable noise-like images, making them unintelligible to the human eye [5]. Chaos theory is often used in cryptography due to its unique properties such as periodicity, sensitivity to initial conditions, and random-like behavior which meet the requirements of cryptography [5]. As a result, a huge number of chaos-based encryption techniques have been proposed such as the logistic-sine map proposed by Chen and Hu [6] for medical image encryption, the memristive chaotic system implemented by Chai et al. [7], and the image encryption using a hyperchaotic system proposed by Hosney et al. [4]. In addition, the evolution of technology has led to the development of OFDM as the suggested method for transmitting digital data over a wireless radio channel due to its high transmission capacity, bandwidth efficiency, reduced interference, system flexibility, and scalability [8]. OFDM splits transmissions into sub-channels, enabling high bit rates and spectrum efficiency [9]. In [10], the authors applied the OFDM system in combination with chaotic baker chart permutation to transmit encrypted images. In [8, 11], the method of encrypting direct QAM symbols is discussed in terms of security. In [12], the authors developed a method of encryption and scrambling on both sides for an OFDM system. The authors [1] have suggested an AWGN crypto-OFDM system for secure transmission. In [13], the transmission of secure images has been discussed regarding the logistic sine cosine algorithm through the OFDM communication system. Motivated by the discussions above, in this paper, we present a study focusing on a secure medical image transmission system that uses the OFDM modulation method. This system incorporates a six-dimension (6-D) hyperchaotic system to combat the effects of multipath and enhance medical image transmission security. As a result, the proposed system can use OFDM modulation for the secure transmission of medical images providing a reliable and secure method for transmitting sensitive patient data. By employing strong encryption algorithms and channel coding techniques, medical professionals can ensure that patient data remains confidential and secure during transmission. The structure of the paper is as follows: In the next section, we provide a brief explanation of OFDM modulation. In Section 3, we describe the proposed 6-D hyperchaotic system for the encryption of medical images. In Section 4, we present the results and discussion to demonstrate the effectiveness of the proposed method. Finally, we provide some conclusions in Section 5."
https://arxiv.org/html/2411.08781v1,SoK: Towards a Common Understanding of Cryptographic Agility,"Cryptographic agility is gaining attention due to its crucial role in maintaining cryptographic security in a rapidly evolving technological landscape. However, despite its increasing importance, the term cryptographic agility remains vaguely defined and there is no clear consensus on its exact meaning. This lack of clarity poses a challenge since the need for agility becomes more urgent as new cryptographic vulnerabilities and advanced computing threats emerge, emphasizing the need for a systematic approach to clarify and refine the notion on cryptographic agility.In this paper, we systematize the concept of cryptographic agility by providing three research contributions. First, we review current definitions across academic and gray literature, identifying six distinct categories to differentiate every aspect within the definitions. Second, we synthesize these insights to establish a comprehensive, canonical definition of cryptographic agility. Third, we explore the relationship between cryptographic agility and the related concepts cryptographic versatility and interoperability. In our discussion, we examine the relevance of cryptographic agility, highlight its trade-offs with complexity, assess its individual applicability, and illustrate its various contexts by offering an additional application-specific definition. Our work provides a new perspective on cryptographic agility and related concepts, based on systematical research to clarify and enhance its future use.","Cryptography is an integral part of our digital world. It ensures information security across a variety of domains, from encryption protocols used in applications like web browsers and authentication mechanisms of cloud storages [1] to the foundation for hype technologies like blockchain [2]. Almost any guarantee of trust in the IT world, be it secure data exchange, privacy protection, or the integrity of digital transactions relies heavily upon cryptography [3]. After solving the major challenge of spreading its use, the next challenge was adapting cryptography to match attackers’ advancing capabilities. During that time, the notion of cryptographic agility arose, often abbreviated as crypto-agility, and mostly depicting the capability to transition from one algorithm to a direct replacement or to adapt the length of cryptographic keys (see, e.g. [4]). At times, cryptographic advances are clear improvements; for example, the transition from DSA to ECDSA [5, 6] introduced smaller public key sizes [7]. However, this does not imply that the transition was without challenges [8, 9]. Practical issues also arose, such as ensuring web servers could interact with diverse clients [10] or comply with various national regulations like the crypto-related specifications of the US FIPS [11] vs. the Russian GOST [12] standards. Protocols that negotiate algorithms during execution and configurable cryptography helped to address these issues, though implementing them was often resource-intensive for developers \citeref[ ][]rfc6421. Still, these concepts found implementation in software and standards, suggesting the crypto-agility problem was addressed, though challenges persist [8, 13]. However, the landscape of cryptography is about to change with increasing pace. The current advances in the field of quantum computing [14] have the potential to weaken or even break essential cryptographic algorithms by solving the underlying mathematical problems significantly faster than classical computers [15, 16]. The answer to this is mostly the migration to post-quantum cryptography (PQC) [17, 18]. This transition has become a role model of applied cryptography, especially as a key driver \citeref[ ][]Moustafa18CAMustHave, Alnahawi23StateOfCA of the underlying question on how to update cryptographic schemes and – given the significantly higher complexity of some PQC approaches – also update entire protocols and technology in general. Past experience has shown that even small transitions between similar schemes can be challenging [8, 9] due to factors like key size limits, speed reductions, or backward compatibility needs. The PQC shift intensifies these issues [19]. Beyond the PQC migration, attack vectors such as side-channel attacks [20] and advancements in cryptanalysis [21] continually challenge the security of established cryptographic systems. Having more resistant cryptographic mechanisms at the ready protects infrastructures, comparable to the in-depth-security paradigm [22] just as well as, e.g., having a backup solution available before you urgently need it. In this ever-evolving environment, the ability to adapt to new threats by quickly transitioning from one cryptographic method to another is more critical than ever. Adopting new technology is complex, even for experienced organizations like the Internet Engineering Task Force (IETF), which, despite best practices, often faces challenges in updating mechanisms [23, 24, 25]. With several working groups addressing the PQC transition, it became evident how important a mutual understanding of the terminology is, as details are complex and the individual interpretation may vary drastically [26]. Confronted with the huge challenge of the PQC transition, Ott et al. raised the question, “Where is the research on cryptographic transition and agility?” \citeref[ ][]Ott19ResearchChallenges,Ott23ResearchOnCryptographicTransition. In our opinion, one starting point to answer this question is that cryptographic agility remains an ill-defined concept. Our structured literature review shows that there are numerous definitions of crypto-agility. Some authors interpret it e.g., as an inherent property of systems, while others define it as an engineering practice or even as a broader design paradigm as discussed in Section 3.2. This diversity of perspectives illustrates the different interpretations within the literature. Each perspective reflects only a portion of the extensive conceptual landscape associated with cryptographic agility. This lack of clarity leads to inconsistencies in how the term is understood and applied, potentially causing misunderstandings in both academic research and practical implementation. In the wake of the post-quantum migration, more and more people work on the interpretation of cryptographic agility, seeking a robust definition \citeref[ ][]Ott19ResearchChallenges. To address this issue, it is crucial to develop a clear and consistent understanding of the term which we do in this paper: 1. Our first main contribution is to illustrate the variety of existing definitions in the literature and derive their fundamental characteristics (cf. Section 3.2). 2. Our second contribution is a canonical definition of crypto-agility based on the analyzed literature that subsumes the existing definitions (cf. Section 3.3). 3. Finally, we discuss the relationship between cryptographic agility and other relevant terminology in the field, establishing new foundational definitions for cryptographic versatility and interoperability. (cf. Section 3.4). We follow a systematic and stringent approach to systematize the knowledge in the field (cf. Section 2). Overall, we aim to establish a new, unified terminology based on the most recent developments to facilitate research and application in this emerging area. In doing so, we hope to provide a fresh and fundamental working base for continuing research and development in the context of cryptographic agility."
https://arxiv.org/html/2411.08640v1,"Towards Secure Intelligent O-RAN Architecture: Vulnerabilities,
Threats and Promising Technical Solutions using LLMs","The evolution of wireless communication systems will be fundamentally impacted by an open radio access network (O-RAN), a new concept defining an intelligent architecture with enhanced flexibility, openness, and the ability to slice services more efficiently. For all its promises, and like any technological advancement, O-RAN is not without risks that need to be carefully assessed and properly addressed to accelerate its wide adoption in future mobile networks. In this paper, we present an in-depth security analysis of the O-RAN architecture, discussing the potential threats that may arise in the different O-RAN architecture layers and their impact on the Confidentiality, Integrity, and Availability (CIA) triad. We also promote the potential of zero trust, Moving Target Defense (MTD), blockchain, and large language models (LLM) technologies in fortifying O-RAN’s security posture. Furthermore, we numerically demonstrate the effectiveness of MTD in empowering robust deep reinforcement learning methods for dynamic network slice admission control in the O-RAN architecture. Moreover, we examine the effect of explainable AI (XAI) based on LLMs in securing the system.","Wireless systems are becoming more capable but more complex in the next generation of cellular networks. Unlike previous generations, the next generation will be flexible, agile, modular, supporting heterogeneity in services, multiple technologies, and rapid deployment [1]. Radio access networks (RAN) performance is expected to be significantly improved with O-RAN, which combines and evolves the cloud RAN (C-RAN) and virtual RAN (vRAN) to enable an open and flexible RAN. In the O-RAN architecture, the components of RANs are virtualized and decoupled, using compatible open interfaces developed for their interconnection. Moreover, the O-RAN’s architecture utilizes artificial intelligence and machine learning (AI/ML) techniques to develop intelligent RAN layers, allowing to empower intelligent, data-driven closed-loop control for the RAN [2]. These features bring many benefits to the system, including reduced capital expenditures (CAPEX) and operating expenses (OPEX), increased agility and flexibility, and enhanced visibility and security. For all its promises, and like any technological advancement, O-RAN is not without risks that need to be assessed and properly addressed to accelerate its wide adoption in future mobile networks. Indeed, recent studies have shown that the O-RAN architecture is opening the door to a new breed of security challenges brought by the new components and open interfaces defined, the use of open-source software, the disaggregation between hardware and software, and the reliance on cloud-native and AI technologies, among others [3]. Thus, a review of the security aspects needs to be carried out, considering the potential risks and vulnerabilities, as well as the concrete solutions to apply. Such an investigation is essential to strengthen the security posture of O-RAN at its early stage of development. This paper explores security threats across layers of the intelligent O-RAN architecture and proposes key technologies to mitigate them, highlighting the need for proactive measures in securing next-generation networks [2]. Unlike prior studies, our research focuses on diverse vulnerabilities in O-RAN, offering an innovative solution for securing near-Real-Time RAN Intelligent Controller (near-RT RIC) and non-Real-Time RAN Intelligent Controller (non-RT RIC) that integrate AI/ML methods for system automation, safeguarding AI/ML models against various potential threats[1, 4]. Moreover, the near-RT RIC and non-RT RIC includes third party applications which can use AI/ML techniques for the resource allocation. In addition to traditional security mechanisms, we also propose the novel use of Large Language Models (LLMs) to enhance the system’s security, particularly. The LLM system can analyze data and articulate the situation in human-readable language to assist in detecting vulnerabilities within the system. The LLM model can use explainable AI (XAI) to analyze the data pattern and realize if there are any significant changes during the time and warn of the vulnerabilities. Research contributions of this paper are listed as follows: • An in-depth analysis of vulnerabilities and threats in the O-RAN architecture arising from the introduction of new technologies and common 5G RAN security issues. • The proposal of three countermeasure approaches utilizing the zero trust concept, blockchain technology, and the LLM & MTD paradigm. • Case studies and proof-of-concept demonstrations of MTD-based robust ML in O-RAN and LLM-based robust AI/ML in O-RAN, illustrating the effectiveness of MTD in enhancing the robustness of deep reinforcement learning models. The remainder of this paper is as follows: Section II provides an overview of the O-RAN architecture, focusing on its key components: RAN, cloud, and management layers, along with ML and network slicing. Section III examines vulnerabilities and threats in the O-RAN architecture, analyzing their impact on confidentiality, integrity, and availability (CIA). Section IV explores emerging technologies such as zero-trust (ZT), blockchain, moving target defense (MTD), and LLMs to enhance O-RAN security. In Section V, we propose a novel MTD-based solution demonstrating its effectiveness in securing deep reinforcement learning (DRL) against adversarial attacks in the Near-RT RIC. Additionally, we discuss the application of LLM-based explainable AI (XAI) for detecting AI/ML attacks in O-RAN. Finally, conclusions are drawn in Section VI."
https://arxiv.org/html/2411.08460v1,Trap-MID: Trapdoor-based Defense against Model Inversion Attacks,"Model Inversion (MI) attacks pose a significant threat to the privacy of Deep Neural Networks by recovering training data distribution from well-trained models. While existing defenses often rely on regularization techniques to reduce information leakage, they remain vulnerable to recent attacks. In this paper, we propose the Trapdoor-based Model Inversion Defense (Trap-MID) to mislead MI attacks. A trapdoor is integrated into the model to predict a specific label when the input is injected with the corresponding trigger. Consequently, this trapdoor information serves as the ""shortcut"" for MI attacks, leading them to extract trapdoor triggers rather than private data. We provide theoretical insights into the impacts of trapdoor’s effectiveness and naturalness on deceiving MI attacks. In addition, empirical experiments demonstrate the state-of-the-art defense performance of Trap-MID against various MI attacks without the requirements for extra data or large computational overhead. Our source code is publicly available at https://github.com/ntuaislab/Trap-MID.","Deep Neural Networks (DNNs) have been successfully applied in various domains. However, training DNNs could involve sensitive data like facial recognition and medical diagnosis, which raises privacy concerns. Model Inversion (MI) stands as one of the important privacy attacks aimed at reconstructing private data within specific classes from a well-trained model. For example, an adversary may recover the training images of specific identities from a facial recognition system. MI attacks were first introduced by Fredrikson et al. [1, 2], reconstructing private attributes from low-capacity models. After that, Zhang et al. [3] proposed Generative Model-Inversion (GMI) attacks to reconstruct private images from DNNs, utilizing Generative Adversarial Network (GAN) as a general prior. This GAN-based framework has been widely adopted by later attacks [4, 5, 6, 7, 8, 9, 10, 11]. Among them, PLG-MI [8] achieves state-of-the-art attack performance. Previous works also demonstrated the efficacy of MI attacks under black-box [9, 10, 12] or label-only [11, 13] settings. In this paper, we focus on defending against white-box attacks, which pose a more challenging scenario. Most existing defenses focus on reducing the information leakage through Differential Privacy (DP) [1, 3], dependency regularization [14, 15], or manipulating the loss landscape [16]. However, these methods remain vulnerable to recent MI attacks [16]. In contrast, recent works proposed to mislead MI attacks by prompting models to classify fake samples as the protected class with high confidence [17, 18, 19]. Although effective, these misleading-based strategies face challenges, including additional data requirements and substantial computational overhead. Furthermore, they typically protect only a single or a limited set of classes, while other defenses aim to secure all classes simultaneously. Sharing a similar idea, Shan et al. [20] introduced Trapdoor-enabled Adversarial Detection (TeD) against targeted adversarial attacks, which aims to change the model behaviors by applying adversarial perturbations to the input data. Instead of training a robust model against such perturbations, TeD shows that injecting trapdoors into the models can mislead the adversarial attacks to result in samples with similar features to poisoned data, thereby empowering the adversarial detection by measuring their similarity to the trapdoor signatures. Inspired by previous misleading-based defenses [17, 18, 19] and TeD [20], we propose Trapdoor-based Model Inversion Defense (Trap-MID), which deceives MI attacks by incorporating trapdoors as the ""shortcuts"". We discuss the key properties of trapdoor triggers necessary for misleading these attacks, and experiments show that Trap-MID outperforms existing methods in defending against MI attacks. Our contributions can be summarized as follows: 1. We propose a trapdoor-based defense, Trap-MID, to preserve privacy by misleading MI attacks. Through extensive experimentation, it presents state-of-the-art defense performance against various MI attacks. 2. To the best of our knowledge, we are the first to establish the connection between MI defenses and trapdoor injection techniques. We theoretically discuss the importance of trapdoor effectiveness and naturalness in misleading MI attacks and showcase its efficacy with empirical experiments. 3. Compared to previous trapping defenses, our trapdoor-based framework is more computationally and data-efficient, without large computational overhead or additional data."
https://arxiv.org/html/2411.08439v1,"A Fully Local Last-Generated Rule 
in a Blockchain","An effective method for suppressing intentional forks in a blockchain is the last-generated rule, which selects the most recent chain as the main chain in the event of a chain tie. This rule helps invalidate blocks that are withheld by adversaries for a certain period. However, existing last-generated rules face an issue in that their applications to the system are not fully localized. In conservative cryptocurrency systems such as Bitcoin, it is desirable for methods to be applied in a fully local manner. In this paper, we propose a locally applicable last-generated rule. Our method is straightforward and is based on a relative time reference. By conservatively setting the upper bound for the clock skews \Delta_{O_{i}} to 200 s, our proposed method reduces the proportion \gamma of honest miners following the attacker during chain ties by more than 40% compared to existing local methods.","Blockchain is the foundational technology used in decentralized currency systems, including Bitcoin [1]. Blockchain systems are generally categorized into those that utilize proof-of-work and those that employ proof-of-stake. The focus of this study was on blockchains based on proof-of-work. The security of a blockchain system is supported primarily by its incentive mechanisms. Specifically, the system is designed such that the most profitable strategy for each miner is to extend the longest chain. However, attacks that intentionally fork the chain to increase the adversary’s rewards have been identified [2] [3] [4] [5]. Such attacks undermine the system’s consistency and lead to undesirable centralization of miners. Various methods have been proposed to prevent intentional forks [2] [6] [7] [8] [9] [10] [11] [12] [13][14]. One such method, addressed in this study, is the last-generated rule. This is a tie-breaking rule that selects the most recently generated chain as the main chain when chains are in a chain tie. Here, a tie-breaking rule refers to a rule that determines the main chain when fork choice rules, such as the longest chain rule or GHOST [15], cannot provide a unique decision. By applying the last-generated rule, it becomes possible to distinguish blocks held by adversaries for a certain period from others, effectively invalidating the adversary’s blocks. Unlike other countermeasures, the last-generated rule can be applied without requiring system-wide updates, such as hard forks or soft forks, or the need for strong synchrony, thereby making its implementation easy. However, existing last-generated rules suffer from a limitation in that their application is not localized to each miner. These methods require trusted third parties [6] or sharing new messages [13]. Conservative operations are preferred in currency systems such as Bitcoin. In such systems, it is desirable that new methods be localized. In this paper, we propose a simple last-generated rule that can be applied in a fully local manner. Here, “fully local” means that no additional communication is required. Whereas existing last-generated rules rely on an absolute time standard, our method is based on a relative time standard. Specifically, our method assumes that there is a known upper bound for the clock skews between miners. On the basis of this assumption, the proposed last-generated rule rejects blocks with significant discrepancies between the local clock and the block’s generation time instead of selecting the newer block. Furthermore, we demonstrate the superiority of our method over local methods through both theoretical analysis and simulation experiments."
https://arxiv.org/html/2411.08410v1,The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense,"The vulnerability of Vision Large Language Models (VLLMs) to jailbreak attacks appears as no surprise. However, recent defense mechanisms against these attacks have reached near-saturation performance on benchmarks, often with minimal effort. This simultaneous high performance in both attack and defense presents a perplexing paradox. Resolving it is critical for advancing the development of trustworthy models. To address this research gap, we first investigate why VLLMs are prone to these attacks. We then make a key observation: existing defense mechanisms suffer from an over-prudence problem, resulting in unexpected abstention even in the presence of benign inputs. Additionally, we find that the two representative evaluation methods for jailbreak often exhibit chance agreement. This limitation makes it potentially misleading when evaluating attack strategies or defense mechanisms. Beyond these empirical observations, our another contribution in this work is to repurpose the guardrails of LLMs on the shelf, as an effective alternative detector prior to VLLM response. We believe these findings offer useful insights to rethink the foundational development of VLLM safety with respect to benchmark datasets, evaluation methods, and defense strategies.","The pervasiveness of Large Language Models (LLMs) concurrently ushers varied challenges for both researchers and practitioners [26]. Among these, protecting the trustworthiness of free-form outputs, as defined by the 3H criterion [51], has grown increasingly critical in recent years [22, 55]. Beyond important considerations of Helpfulness and Honesty, the need for Harmlessness is far more urgent given its potential social impact. Jailbreak attacks, often referred to as red-teaming [41], serve as the most common method for assessing the harmlessness of LLMs and Vision-LLMs (VLLMs) [53, 32, 7]. They are designed to circumvent the built-in restrictions or safeguards within models [23], eliciting them to produce malicious outputs, such as content related to illegal activities, hate speech, and pornography. Compared to their LLM counterparts, the vulnerability of VLLMs to jailbreak attacks has garnered considerable attention until very recently [61, 38, 30]. Some initial methods inject high-risk content into images through typography or generative techniques like stable diffusion [45]. Leveraging such methods, datasets have been curated that easily expose a high Attack Success Rate (ASR) for both proprietary models [1] and publicly open-sourced models [31, 32]. On the other hand, without many bells and whistles, recent defense strategies - primarily focused on safety-aware supervised fine-tuning [71] and system prompt protection [60] - have surprisingly shown remarkable results on these benchmark datasets. In particular, VLLMs like LLaVA [31] can be fully safeguarded against the attacks involved (ASR \rightarrow 0) [60, 71, 18]. As such, these dual findings raise an intriguing question: does it suggest that defending against jailbreak attacks is easy, given that the attacks themselves have already been shown to be relatively simple? The observation above highlights a perplexing safety paradox that undermines the credibility of recent advancements, particularly in defense mechanisms. To address this challenge, we present the first comprehensive study that underlines and elucidates the dual ease of both jailbreak attacks and defenses in VLLMs. Our first finding challenges prior assumptions that the vulnerability to jailbreak attacks stems from catastrophic forgetting or fine-tuning [71, 42]. Instead, we reveal that the true cause lies in the inclusion of image inputs, which compromises the guardrails of the base LLMs. On the other hand, we observe that existing defense mechanisms [71, 60] tend to be overly prudent. One typical manifestation is that VLLMs with post-defense, are prone to abstaining from responding even to benign queries. This issue of over-prudence significantly impairs the helpfulness of VLLMs, reducing their utility in real-world applications. In addition, we point out that the two well-studied evaluation methods often show a limited correlation in identifying jailbreaks, making it less convincing to determine the effectiveness of an attack method or a defense strategy. Beyond understanding the safety paradox, we further note that the jailbreak defense can be re-framed into a detection-then-response process. In view of this, we propose to implement an evaluator prior to the final VLLM response. We opt not to utilize an additional VLLM for detection as [18], given the limited reliability of current VLLMs in providing robust safeguards. Instead, we explore a vision-free evaluator, where we repurpose the guardrails of recent advanced LLMs to judge the harmfulness of a given textual query. Interestingly, we find that this evaluator, when paired with a VLLM for safe response generation, suffers less from the over-prudence problem, achieving a balanced interplay between robust safety alignment and model helpfulness. To the best of our knowledge, we are the first to investigate the safety paradox problem of VLLMs. Through the empirical findings presented in this work, we seek to highlight this issue and raise more attention to its significance. Beyond this, we hope to provide insights that can support future advancements in this field, such as reaching a consensus on the nature of jailbreak attacks and their associated risks, facilitating the collection of comprehensive attack data, and developing more robust evaluations."
https://arxiv.org/html/2411.08359v1,MultiKG: Multi-Source Threat Intelligence Aggregation for High-Quality Knowledge Graph Representation of Attack Techniques,"The construction of attack technique knowledge graphs aims to transform different types of attack knowledge into structured representations that can be used to represent the attack procedure more effectively. Although previous studies have proposed various methods for constructing attack knowledge graphs, these methods generally limit their knowledge sources to textualized data such as Cyber threat intelligence (CTI) reports, which are coarse-grained and unstructured, making the knowledge graphs constructed based on them incomplete and inaccurate.To address these limitations, we attempt to expand the attack knowledge sources by introducing audit logs and static code analysis on top of CTI reports to provide finer-grained knowledge for constructing attack technique knowledge graphs. Therefore, we propose a fully automated framework across threat knowledge sources called MultiKG automates the processing of data from different sources generates knowledge graphs separately, and then builds a unified attack knowledge graph representation. Through system design and the utilization of the Large Language Model (LLM), MultiKG automates the analysis, construction, and merging of attack graphs from CTI reports, dynamic logs, and static code, respectively. MultiKG then aggregates the attack graphs across sources and merges them into a fine-grained unified attack technology knowledge graph representation that encompasses multiple sources.We implemented and deployed MultiKG, then evaluated it using 1015 real attack techniques, and corresponding 9006 attack technique intelligence from real-world CTI reports. The results show that MultiKG is capable of accurately extracting attack knowledge graphs from different sources, and efficiently aggregating and summarizing technical attack knowledge across different resources to generate accurate and complete technique attack knowledge graphs. We also show through concrete case studies that our attack knowledge graph directly benefits downstream security practices tasks such as attack reconstruction and attack detection.","Cyber attacks have evolved rapidly in recent years, using increasingly advanced, sophisticated, and versatile attack tactics and techniques to construct covert attack operations, making intrusion detection increasingly challenging. A publicly-available test conducted by MITRE ATT&CK (MITRE ATT&CK, 2024) Evaluation on intrusion detection products from top security companies such as Symantec (Symantec, 2024) and Crowd Strike (CrowdStrike, 2024) shows that most products would be bypassed by Live-off-the-Land, attack variants, and other advanced attack methods to bypass. To combat the rapidly evolving threat landscape, security practitioners are actively sharing and collecting intelligence of real attack cases, including structured Indicators of Compromises (IOCs), unstructured attack analysis reports, and so on, on public platforms. However, collecting and summarizing intelligence from the heterogeneous multi-source platforms is time- and labor-expensive. Moreover, the question of how to integrate data from different sources, formats, and representations so that it can represent attack knowledge more accurately and efficiently has not yet been addressed. (a) How to collect and summarize this multi-source and heterogeneous threat knowledge to accurately represent and cover complex attack variants? and (b) How to effectively integrate this threat knowledge to represent attacks in a more efficient and unified way? are still open questions that require further research. Table 1 list existing. We list relevant threat intelligence analysis efforts in Table 1. For Extractor (Satvat et al., 2021), ThreatRap (Gao et al., 2021b), and AttacKG (Zhenyuan et al., 2022), they try to summarize information in threat intelligence to more effectively summarize attack-related knowledge and use it to reconstruct attacks. However, because it does not consider the correlation between security threat intelligence content and real attack logs, it cannot be used for attack detection, but only for a more complete summary of attack-related knowledge. Then, for TTPDrill (Husari et al., 2017) and rcATT (Legoy et al., 2020), they only focus on single entity information without considering the correlation between these nodes, and the lack of necessary structural and semantic information, which makes it easy to generate false positives and a low detection rate. In addition, for the work of Poirot (Milajerdi et al., 2019a), iACE (Liao et al., 2016), etc., they use the complete attack process in the report as a graph to detect the attack, which makes it easy to bypass by the extension of the attack chain and the replacement of the attack technology. More importantly, all the articles mentioned in the table, use a single threat intelligence information as the information source, that is, structured and unstructured text, and do not consider other information sources except text descriptions, which makes their representation of the attack very coarse-grained, and the effect of practical application is limited. This reliance on a single source of information fails to capture a broader attack surface and complex threat patterns. In real-world environments, attackers often utilize multiple means and vectors to execute attacks, and these behaviors are dispersed and recorded in different sources, such as threat reports, audit logs, and attack codes, which are equally critical and can provide more granular threat intelligence. Therefore, future research should explore ways to integrate multiple sources of information to build a more detailed and comprehensive threat intelligence mapping in order to improve threat detection and response capabilities. In summary, the threat intelligence graph extracted from existing work has the following limitations: 1) Attack knowledge is dispersed across multiple sources, and individual sources usually focus on limited/incomplete attack representations, which do not allow for a complete restoration of the attack. 2) There is a lack of fine-grained information about the underlying attack behavior, such as audit logs during the attack process and information about the attack execution code, which leads to a lack of fine-grained semantic information about the attack semantics in its representation of the attack technique, and prevents it from accurately representing the attack. The MultiKG we propose combines three sources of information: threat intelligence, static code analysis, and dynamic log analysis. MultiKG can realize the automatic structured representation of information from different sources, as well as the fusion and summary of attack knowledge from different sources. We refine the granularity to the graph structure level of single atomic attack techniques, which can accurately restore the attack graph structure, thus realizing real attack reconstruction at the level of technique. Table 1. Comparison with cyber threat intelligence gathering approaches Mapping-to -audit-logs Graph -structure Technique -aware Cross -sources Poirot (Milajerdi et al., 2019a) ✓ ✓ ✗ ✗ iACE (Liao et al., 2016) ✓ ✗ ✗ ✗ Extractor (Satvat et al., 2021) & ThreatRaptor (Gao et al., 2021b) ✗ ✓ ✗ ✗ TTPDrill (Husari et al., 2017) & rcATT(Legoy et al., 2020), etc. ✓ ✗ ✓ ✗ AttacKG (Zhenyuan et al., 2022) ✗ ✓ ✓ ✗ MultiKG ✓ ✓ ✓ ✓ The main challenge comes from: • How to collect and summarize the attack information from different sources(including CTI reports, static code, and dynamically executed audit logs) and handle the attack data separately to get the representation of the attack technique in different sources. • How to integrate the information from different sources and different granularities, fusing multiple sources to get accurate and complete coverage of the attack techniques. To overcome these challenges, we proposed MultiKG, the first cross-source threat intelligence collection and aggregation system. In summary, this paper makes the following contributions: • Multi-source threat knowledge gathering and aggregation. We proposed the first framework MultiKG in this paper for aggregation of knowledge from different threat intelligence sources with respective advantages. Specifically, we combine three sources of information: threat intelligence, static code analysis, and dynamic log analysis. Dynamic logs can provide the topology (including edge and node information) of the actual execution process of attack techniques. Static codes can help complete missing nodes and edges in dynamic graphs. Threat intelligence can help us obtain different variants of a certain attack technology as much as possible so that our final representation of a single technology is not only accurate but also universal. • We design the complete MultiKG system to realize the above details. Specifically, we propose an effective algorithm to extract attack technique graphs from huge audit logs, we obtain attack nodes from static code by abstracting syntax trees, and we utilize LLM to analyze entities, entity types, and relationships in threat reports to obtain attack knowledge graphs. We also design single-resource fusion and multi-resource merging algorithms to process the above data to achieve a complete and unified knowledge representation at the attack technique level. • We implemented MultiKG, then evaluated it with 1015 real attack techniques and corresponding 9006 attack technique intelligence from real-world CTI reports. The results show that MultiKG can accurately extract attack knowledge graphs from different sources and efficiently summarize and aggregate technical-level attack knowledge across resources to generate technical-level accurate and complete attack representations. We also show the benefits of MultiKG through specific case studies."
https://arxiv.org/html/2411.08316v2,Evaluating Synthetic Command Attacks on Smart Voice Assistants,"Recent advances in voice synthesis, coupled with the ease with which speech can be harvested for millions of people, introduce new threats to applications that are enabled by devices such as voice assistants (e.g., Amazon Alexa, Google Home etc.). We explore if unrelated and limited amount of speech from a target can be used to synthesize commands for a voice assistant like Amazon Alexa. More specifically, we investigate attacks on voice assistants with synthetic commands when they match command sources to authorized users, and applications (e.g., Alexa Skills) process commands only when their source is an authorized user with a chosen confidence level. We demonstrate that even simple concatenative speech synthesis can be used by an attacker to command voice assistants to perform sensitive operations. We also show that such attacks, when launched by exploiting compromised devices in the vicinity of voice assistants, can have relatively small host and network footprint. Our results demonstrate the need for better defenses against synthetic malicious commands that could target voice assistants.","Voice is a natural way for people to interact with devices in their vicinity. It is one of the reasons for the increasing adoption of voice assistants such as Amazon Alexa, Google Assistant and Samsung Bixby. At the same time, applications enabled by such voice assistants are increasing at a rapid pace as can be seen by the diversity and number of Amazon Skills. The proliferation of voice assistants and applications supported by them leads to new security threats. In fact, researchers have explored how voice assistants can be targeted by malicious commands that can be issued when the attacker is in close physical proximity or across the network [diao2014your, jang2014a11y, Carlini:2016vl, schonherr2018adversarial, Carlini:2018wj, Abdullah:2018ho, Yuan:2018um, kumar2018skill, qin2019imperceptible]. Others have explored thousands of applications supported by voice assistants and the sensitive actions performed by them [shezan2020read]. We explore if attackers can leverage the ease with which speech can be harvested to launch attacks against voice assistant enabled applications. As with email addresses and phone numbers, limited and unrelated speech can be easily harvested for a large number of people. By unrelated speech, we mean speech that does not include words that must be uttered for specific voice assistant commands. For many users, unrelated speech may be available from podcasts, YouTube videos, lectures, talks, online posts, or can even be collected by making robocalls. A possible defense against malicious commands is to use the command voice itself to determine if the command is coming from an authorized user. To enable this, authorized users must set up their profiles to allow command voice to be matched against the voices of users having profiles. However, because of a variety of reasons, including usability and environmental constraints (e.g., noisy background, distance between speaker and voice assistant), a command source’s match with an authorized user voice is not required to be strict. In the context of malicious voice commands, to the best of our knowledge, the efficacy of such defenses in current voice assistants has not been explored. Although widely available services can now be used for speech synthesis [ElevenLabs2023, Coqui2023, wang2023neural], attackers will prefer to avoid them for cost or detection reasons when targeting a large number of victims. Since attack commands that target voice assistants may not require natural-sounding human quality speech, we explore the feasibility of low cost techniques for speech synthesis which allow such synthesis to be done even at compromised user devices. We have two specific goals for how commands are synthesized. First, the synthesized command needs to be intelligible to the voice assistant so it is recognized by it. Second, the command must preserve similarity with an authorized user’s voice so it can work even when the voice assistant enabled application matches commands to users with a certain level of confidence. If these two goals can be met for a significant fraction of targeted users and their voice assistants, it will demonstrate the feasibility of low cost and large scale attacks on voice-enabled applications. We present an empirical security analysis for commands of various lengths when applications check that the command voice comes from an authorized user. To conduct experiments at scale to compare both intelligibility and similarity of synthetically generated commands, we develop an experimental testbed with a popular voice assistant (e.g., Amazon Alexa). We use an efficient and lightweight concatenative speech synthesis scheme to generate attack commands. In particular, we use a unit-selection method that extracts diphones from available speech and concatenates necessary diphones to synthesize a command [lenzo2000diphone, OSHAUGHNESSY198855]. Our automated testbed allowed us to conduct over one thousand experiments for many commands and user profiles. The results of these experiments help us demonstrate the following: 1. We show that when available target speech has all diphones needed for the synthesis of a command, the Alexa voice assistant correctly recognizes 93.8% of the commands generated with a basic unit-selection concatenative synthesis method. Our experiments used both short and long commands in the voice of a diverse group of users (e.g., accents, gender etc.). 2. Similar to command recognition, we found that unit-selection concatenative synthesis also preserves voice similarity as assessed by the speaker match confidence level. In our experiments, the highest confidence level in speaker voice similarity was returned for 90% of the users who have profiles that vary in accent and gender. Thus, our results show security vulnerabilities of voice assistants to synthetic commands even when applications match command source with authorized users. 3. We demonstrate that 50% of the commands can successfully activate a voice assistant when synthesized from only 30 seconds of unrelated speech of a target. This is true even when applications processing these commands require a high level of confidence in the similarity of the command source voice with the voice of an authorized user. We show that the success rate increases as more speech becomes available, reaching 80% with 4 minutes of target speech. Our results demonstrate the ease with which voice assistant enabled applications can be targeted by harvesting speech and efficiently synthesizing attack commands. To the best of our knowledge, we are the first to show that voice profile matching, as used currently, provides little protection against such malicious commands. The paper is structured as follows. Section II discusses related work and the threat model is presented in Section III. We discuss our approach in Section IV and the system developed for carrying out experiments is described in Section V. The results of our experiments are discussed in detail in Section VI. The paper is concluded with discussions and conclusions in Sections VII and VIII."
https://arxiv.org/html/2411.08182v1,"SCORE: Syntactic Code Representations for Static
Script Malware Detection","As businesses increasingly adopt cloud technologies, they also need to be aware of new security challenges, such as server-side script attacks, to ensure the integrity of their systems and data. These scripts can steal data, compromise credentials, and disrupt operations. Unlike executables with standardized formats (e.g., ELF, PE), scripts are plaintext files with diverse syntax, making them harder to detect using traditional methods. As a result, more sophisticated approaches are needed to protect cloud infrastructures from these evolving threats. In this paper, we propose novel feature extraction and deep learning (DL)-based approaches for static script malware detection, targeting server-side threats. We extract features from plain-text code using two techniques: syntactic code highlighting (SCH) and abstract syntax tree (AST) construction. SCH leverages complex regexes to parse syntactic elements of code, such as keywords, variable names, etc. ASTs generate a hierarchical representation of a program’s syntactic structure. We then propose a sequential and a graph-based model that exploits these feature representations to detect script malware. We evaluate our approach on more than 400K server-side scripts in Bash, Python and Perl. We use a balanced dataset of 90K scripts for training, validation, and testing, with the remaining from 400K reserved for further analysis. Experiments show that our method achieves a true positive rate (TPR) up to 81% higher than leading signature-based antivirus solutions, while maintaining a low false positive rate (FPR) of 0.17%. Moreover, our approach outperforms various neural network-based detectors, demonstrating its effectiveness in learning code maliciousness for accurate detection of script malware.","Script-based malware has emerged as a potent threat vector, frequently leveraged in attacks against Linux systems due to the versatility, portability, and ease of use of modern scripting languages. As this threat becomes more prevalent, cloud environments and Linux systems have also become prime targets, especially for script malware written in server-side languages like Python, Perl, and shell scripts. These malicious scripts can serve as standalone threats, such as denial-of-service bots, ransomware, or backdoors, or function as components in multi-stage attacks by facilitating payload delivery and execution. With capabilities comparable to traditional executable malware, including data exfiltration and system resource abuse, these script-based malware poses a significant challenge for detection mechanisms that rely on static signatures or traditional machine learning (ML) techniques. The threat posed by script malware targeting cloud infrastructure is particularly concerning. These malicious scripts can directly access and manipulate underlying cloud resources, providing attackers with powerful capabilities. For instance, a recent Python-based credential harvester and hacking tool called Legion and AlienFox have been observed targeting AWS console credentials, SNS, S3, and SES services [1, 2]. Attackers can leverage the hijacked cloud infrastructure for activities like mass spamming, phishing campaigns, and privilege escalation. More importantly, the prevalence of script malware attacks has surged in recent years, with reports indicating a 100% increase since 2017 and such attacks accounting for 40% of all cyberattacks as of 2020 [3]. By early 2024, the frequency had nearly doubled again over the prior two years [4]. Given this rapid growth and powerful capabilities of scripts [5, 6, 7] mostly used in cloud environments, developing effective detection methods specifically targeting script malware is a critical security need. Malware analysis broadly spans static and dynamic analysis methods. Dynamic analysis executes the malicious file in a highly-controlled “sandbox” environment, and observes the malicious behavior directly [8]. Static analysis, on the other hand, analyzes the structure, properties, and code of a malicious file without executing it, and attempts to infer that same malicious behavior from these related data. As a result, static analysis is often cheaper, safer, and faster than dynamic analysis [9]. Typically, threat intelligence in static analysis tools—such as classical antivirus scanning products—vends as signatures, or rules, coalesced into a pattern-matching database that specifies malicious content that, if observed in a certain file, designates it ‘malicious’. However, such signature-based methods are often ineffective against more advanced malware that use simple code transformations to evade detection [9, 10]. ML-based representation learning of malware, in contrast, presents the opportunity to learn the malicious behavior patterns to capture what static signatures cannot [11, 12, 13, 14, 15, 16]. However, most light-weight ML models focus on learning code behavior from unstructured byte-strings [17, 18], while code language models are too slow and costly due to their billions of parameters. There is a need for reasonably sized ML models that can learn more global, generalizable, invariant, robust, and/or functional patterns of malicious structure in code compared to simple byte-string patterns. In contrast to single-format executables, analyzing scripts presents unique challenges. Scripts are often domain-specific, requiring specialized knowledge to extract features and model its behavior accurately for every specific script language. As the feature extraction becomes more sophisticated, complexity increases. On the other hand, byte-strings alone might not contain as much structured information as well-designed features. In this paper, we propose feature extraction methods that directly target these structures of scripts and detection methods that aim to understand the context of these scripts. In this paper, we address these research questions: • RQ1: How can code from server-side programming languages be effectively represented as features for scalable malware detection using deep learning (DL)? Which feature representation method provides the most useful information to detect malicious behavior? • RQ2: Can DL-based models learn the complex structure of scripts, and which types of DL models most accurately identify malicious code behavior? • RQ3: Does the best-performing DL-based model surpass conventional rule-based malware detectors in accuracy and threat coverage when applied to scripts? To address these questions, firstly, we propose script malware detection methods for server-side languages by leveraging popular code parsing libraries to extract features from the plain-code. We present two approaches for representing code as features: SCORE-H which is based on syntactic code highlighting and SCORE-T which is based on abstract syntax trees (ASTs). While SCORE-H parses the keywords in a sequentially hierarchical level, SCORE-T parses a program’s syntactic structure into a hierarchical tree representation. Both approaches pair their syntactic structure features with raw byte-strings of the scripts for more information-rich representations. Secondly, we propose malware detection models: a sequential model (SM) and a graph representation learning (GRL)-based model. SM contains multiple layers of convolutional neural networks (CNNs) to extract hierarchical embeddings from SCORE-H features, followed by a recurrent neural network (RNN), e.g., bi-directional long short-term memory networks (bi-LSTM). Moreover, we propose a variation of SM with simpler CNN embeddings for serialized SCORE-T features. Finally, our GRL-based model leverages graph embeddings obtained by graph similarity learning and contains a ML-based detector, such as XGBoost [19], for detecting static malicious behavior. System overview of our proposed approaches is shown in Figure 1, where each end-to-end model represents a malware detector, e.g., SM with SCORE-H features, SM with SCORE-T features and GRL model with SCORE-T features. We compare the proposed approaches against commercial antiviruses (AVs), a byte-level feature-based malware detector [18], a sequential and a graph-based neural network malware detector that utilize ASTs [12], and finally an ML detector that utilizes embeddings from CodeBERT, a multi-lingual foundation model pre-trained on Natural Language (NL) - Programming Language (PL) pairs [14]. Our SM shows significant performance for serialized features due to the sequentially written form of scripts. Serialization of the tree structure is highly significant and determined by the traversal method, such as breadth first traversal (BFT) and depth first traversal (DFT). When we have tree/graph structured features and access to threat labels during training, GRL-based model improves the detection performance of the SM for DFT. On the other hand, SM with BFT serialization shows the best performance of all the methods considered in this paper. Overall, all of our proposed approaches address the limitations of AVs, byte-level and AST-based approaches as well as token-based approaches in the literature by leveraging advanced code parsing capabilities and deep learning (DL) techniques to detect script malware attacks. Our contributions can be summarized as follows: 1. We introduce novel feature extraction techniques tailored for server-side script languages, including a syntactic highlighting-based extractor that represents code functionality as a sequence and an AST-based extractor that captures deeper understanding of code through hierarchical syntactic representation. These extractors serve as programming language tokenizers and are integrated with a sequential neural network, enabling an understanding of code rather than relying solely on pattern matching techniques. We further incorporate this hierarchical structure into embeddings by GRL. To the best of our knowledge, these approaches are novel in the realm of malware detection. 2. We have curated a comprehensive collection of malicious and benign scripts. Our extensive evaluations, coupled with comparisons to existing methodologies, demonstrate that our approaches: • Outperform a commercial AV, an open source AV, ML-based byte-level malware detectors, AST-based sequential and graph neural network detectors, and a pre-trained CodeBERT-based malware detector. • Provide coverage for more than 95% of high-priority threats, including cryptominers, ransomware, and credential stealers."
https://arxiv.org/html/2411.08069v1,Intelligent Green Efficiency for Intrusion Detection,"Artificial Intelligence (AI) has emerged in popularity recently, recording great progress in various industries. However, the environmental impact of AI is a growing concern, in terms of the energy consumption and carbon footprint of Machine Learning (ML) and Deep Learning (DL) models, making essential investigate Green AI, an attempt to reduce the climate impact of AI systems. This paper presents an assessment of different programming languages and Feature Selection (FS) methods to improve computation performance of AI focusing on Network Intrusion Detection (NID) and cyber-attack classification tasks. Experiments were conducted using five ML models - Random Forest, XGBoost, LightGBM, Multi-Layer Perceptron, and Long Short-Term Memory - implemented in four programming languages - Python, Java, R, and Rust - along with three FS methods - Information Gain, Recursive Feature Elimination, and Chi-Square. The obtained results demonstrated that FS plays an important role enhancing the computational efficiency of AI models without compromising detection accuracy, highlighting languages like Python and R, that benefit from a rich AI libraries environment. These conclusions can be useful to design efficient and sustainable AI systems that still provide a good generalization and a reliable detection.","The rapid growth of Artificial Intelligence (AI) is creating remarkable advancements in many fields, but its impact on the environment is becoming an increasingly important issue [37]. A 2020 paper by Patterson et al. [24] estimated that training a large AI model, such as OpenAI’s GPT-3, could consume about 1,287 MWh of energy, leading to CO2 emissions equivalent to those produced by 125 average homes in the U.S. in one year. According to an estimate from the World Economic Forum (WEF), the information and communications technology (ICT) sector, which includes AI, by 2020 was already accountable for between 1.4% to 5.9% of global greenhouse gas emissions [34]. According to another estimate from the WEF, computing is expected to account for up to 8% of global power demand by 2030 [5]. Considering these concerns and the cruciality of intrusion detection caused by the increasing threat of cyber-attacks [22], this research seeks to optimize security while minimizing environmental impact [9]. Green AI [43] highlights the importance of optimizing AI systems to minimize their energy consumption and environmental impact without compromising their performance [31]. The efficient usage of computational resources can definitely lead to achieving this goal [41], which gets affected by various factors like the choice of a programming language [25], the usage of technologies like Feature Selection (FS) [19], the hardware efficiency, the ML algorithm efficiency and renewable energy integration. FS in ML is the process of identifying and selecting the most relevant features from a dataset for model training [19]. It can be implemented using Filter, Wrapper, and Embedded methods [21]. Filter methods evaluate features based on their statistical properties, making them efficient for high-dimensional data. Wrapper methods assess model performance with different feature sets, considering feature-model interactions, but can be computationally intensive and prone to overfitting. Embedded methods integrate FS into the model-building process, balancing accuracy and efficiency with lower computational complexity than Wrapper methods. At the same time, the impact of FS on computational efficiency varies depending on the programming language choice. Despite being often considered, programming languages also play an essential role in impacting energy usage and computation speed [25]. This occurs because some languages have better optimized themselves to the hardware they are running on. Usually,compiled languages such as C++ take less time to execute than interpreted languages such as Python [17]. Other characteristics, such as memory handling, may also affect the performance and efficiency of the algorithms. This paper addresses the Green AI use to improve computational efficiency in Intrusion Detection Systems (IDS), seeking to optimize security while minimizing environmental impact. It assesses the choice of programming languages and FS and their relation to computational efficiency in the Green AI context and IDS. Several experiments were performed to compare quality metrics, namely accuracy, precision, recall, and F1, and footprint metrics, namely training and prediction time, in different programming languages and FS techniques. It aims to provide useful information to researchers and practitioners to help create environmentally sustainable AI systems. This paper includes five ML models (Random Forest (RF), XGBoost (XGB), LightGBM (LGBM), Multi-Layer Perceptron (MLP), and Long Short-Term Memory (LSTM)), four programming languages (Python, Java, R, Rust), and three FS methods (Chi-Square, Information Gain (IG), Recursive Feature Elimination (RFE)). This paper is organized into multiple sections, including details, that might help researchers to replicate this baseline, and make comparisons with their own results. Section 2 presents related works. Section 3 details data preprocessing techniques, FS, methodology followed, and how model fine-tuning processes were carried out. Section 4 goes into detail regarding the results obtained on all datasets. Finally, Section 5 discusses the main conclusions that have been drawn and proposes some future research directions."
https://arxiv.org/html/2411.08862v1,: Jailbreaking LLMs using RL fine-tuned LLMs,"We introduce LLM Stinger, a novel approach that leverages Large Language Models (LLMs) to automatically generate adversarial suffixes for jailbreak attacks. Unlike traditional methods, which require complex prompt engineering or white-box access, LLM Stinger uses a reinforcement learning (RL) loop to fine-tune an attacker LLM, generating new suffixes based on existing attacks for harmful questions from the HarmBench benchmark. Our method significantly outperforms existing red-teaming approaches (we compared against 15 of the latest methods), achieving a +57.2% improvement in Attack Success Rate (ASR) on LLaMA2-7B-chat and a +50.3% ASR increase on Claude 2, both models known for their extensive safety measures. Additionally, we achieved a 94.97% ASR on GPT-3.5 and 99.4% on Gemma-2B-it, demonstrating the robustness and adaptability of LLM Stinger across open and closed-source models.","Attack method / Victim LLM LLM Stinger (ours) GCG GCG-M GCG-T PEZ GBDA UAT AP SFS ZS PAIR TAP TAP-T Auto DAN PAP top5 Human DR Llama2-7B-chat 89.3 32.1 19.5 15.9 0.0 0.0 3.1 19.5 3.1 0.4 6.9 5.0 3.8 0.0 0.8 0.1 0.0 Vicuna-7B 93.08 89.9 83.9 83.1 17.6 16.9 18.2 76.1 52.8 26.5 66.0 67.9 78.0 89.3 15.6 46.7 20.1 Claude 2 52.2 - - 1.5 - - - - - 0.6 1.9 1.3 0.0 - 0.1 0.0 0.0 Claude 2.1 26.4 - - 1.4 - - - - - 0.6 2.5 1.9 0.0 - 0.1 0.1 0.0 GPT 3.5 Turbo 0613 88.67 - - 44.3 - - - - - 20.3 52.8 54.7 78.6 - 10.6 25.9 16.4 GPT 3.5 Turbo 1106 94.97 - - 56.4 - - - - - 33.6 42.1 45.9 60.4 - 11.9 3.0 36.5 GPT 4 Turbo 1106 80.50 - - 21.4 - - - - - 9.3 41.5 43.4 81.8 - 11.9 1.4 6.9 Table 1: Attack Success Rate on HarmBench (standard behaviours test split) for open-source and closed-source victim LLMs. Bold cells highlight the best-performing attack method for each victim LLM, while underlined cells indicate the second best. The dashed (-) cells indicate that the attack method is incompatible with the victim LLM, as it is a closed-source (black-box) LLM. Jailbreaking Large Language Models (LLMs) involve crafting inputs that lead safety-trained models to violate developer-imposed safety measures, producing unintended or harmful responses. One effective method for this is through suffix attacks, where specific strings are appended to the input to trigger undesired behavior. Suffix-based attacks have shown success against both white-box and black-box LLMs, offering a simpler, more efficient, and easily automated alternative without the need for complex prompt engineering and human creativity to craft situations and role-playing templates (Zou et al. 2023). Although most of the existing suffix attacks have been patched because of safety training, we observed that modifications of those suffixes can still lead to successful jailbreak attempts. However, manually crafting these modifications or using a white-box gradient-based attacker to find new suffixes is laborious and time-consuming, limiting the scalability of such efforts. In this work, we introduce LLM Stinger, a tool that uses an LLM to automatically generate highly effective adversarial suffixes that can jailbreak safety-trained LLMs. By fine-tuning an attacker LLM in a reinforcement learning (RL) loop, LLM Stinger generates new attack suffixes for a set of harmful questions from the HarmBench benchmark (Mazeika et al. 2024). This automated approach efficiently discovers new suffixes that bypass existing defenses, streamlining the process of crafting jailbreak attacks. The RL loop refines the attacker LLM with the help of reward signals that guide it toward more effective attacks. We compared LLM Stinger against 15 SOTA attack methods, and it outperformed all of them on attack success rate. For example, we were able to increase the ASR (+50.3%) on safety-trained closed-source models such as Claude. Figure 1: Architecture Diagram of LLM Stinger"
https://arxiv.org/html/2411.08133v2,Impactful Bit-Flip Search on Full-precision Models,"Neural networks have shown remarkable performance in various tasks, yet they remain susceptible to subtle changes in their input or model parameters. One particularly impactful vulnerability arises through the Bit-Flip Attack (BFA), where flipping a small number of critical bits in a model’s parameters can severely degrade its performance. A common technique for inducing bit flips in DRAM is the Row-Hammer attack, which exploits frequent uncached memory accesses to alter data. Identifying susceptible bits can be achieved through exhaustive search or progressive layer-by-layer analysis, especially in quantized networks. In this work, we introduce Impactful Bit-Flip Search (IBS), a novel method for efficiently pinpointing and flipping critical bits in full-precision networks. Additionally, we propose a Weight-Stealth technique that strategically modifies the model’s parameters in a way that maintains the float values within the original distribution, thereby bypassing simple range checks often used in tamper detection.","Neural networks have achieved human level performance across various domains (Oord et al., 2016; Silver et al., 2017; He et al., 2015) and spurred a growing interest in deploying neural algorithms in real-world. As a result, ensuring the security and robustness of deep neural networks has become a critical concern that must be addressed. Adversarial attacks (Goodfellow et al., 2014) present a significant security issue for neural networks, potentially injecting system malfunctions through input noise of constrained magnitude that is imperceptible to humans, but catastrophic for neural networks. In recent years, extensive research has been conducted on both attacking and defending against adversarial examples at the input level of neural networks and also at the network parameters (Hong et al., 2019). Bit flip attacks (Rakin et al., 2019) on neural networks are a type of fault injection attack where specific bits in the memory storing the neural network’s parameters (such as weights and biases) are deliberately altered. These attacks exploit the vulnerability of neural networks to small changes in their parameters, which can lead to significant performance degradation or completely erroneous outputs. Rowhammer attacks (Kim et al., 2014) represent a prevalent form of bit-wise software-induced fault attack. This vulnerability enables an attacker to induce single-bit corruption at the DRAM level, making it an ideal candidate for our analysis. The versatility of Rowhammer stems from its minimal requirements: an attacker only needs access to DRAM content, a ubiquitous feature in modern systems. By executing specific memory access patterns, the attacker can exert extreme stress on adjacent memory locations, leading to faults in other stored data, potentially flipping a bit. In this work we present Impactful Bit-Flip Search, a novel method for locating and flipping minimal number of bits in full precision neural models with maximum damage to the model. [TODO to be completed at the end of research]"
https://arxiv.org/html/2411.08088v1,"Safety case template for frontier AI: 
A cyber inability argument","Frontier artificial intelligence (AI) systems pose increasing risks to society, making it essential for developers to provide assurances about their safety. One approach to offering such assurances is through a safety case: a structured, evidence-based argument aimed at demonstrating why the risk associated with a safety-critical system is acceptable. In this article, we propose a safety case template for offensive cyber capabilities. We illustrate how developers could argue that a model does not have capabilities posing unacceptable cyber risks by breaking down the main claim into progressively specific sub-claims, each supported by evidence. In our template, we identify a number of risk models, derive proxy tasks from the risk models, define evaluation settings for the proxy tasks, and connect those with evaluation results. Elements of current frontier safety techniques—such as risk models, proxy tasks, and capability evaluations—use implicit arguments for overall system safety. This safety case template integrates these elements using the Claims Arguments Evidence (CAE) framework in order to make safety arguments coherent and explicit. While uncertainties around the specifics remain, this template serves as a proof of concept, aiming to foster discussion on AI safety cases and advance AI assurance.","Frontier artificial intelligence (AI) systems offer many benefits, but they are also being used to cause harm. For example, AI-generated synthetic media are increasingly used to fabricate false narratives for political manipulation or to produce non-consensual deepfake pornography [66, 70]. AI systems enable cybercriminals to personalise and automate phishing campaigns [30] and could allow authoritarian governments to enhance their surveillance capabilities [1]. Additionally, the growing integration of large language models (LLMs) across different sectors and functions risks perpetuating various social biases embedded in the underlying datasets [26]. It is probable that more capable AI systems will entail heightened risks [15, 23, 41, 58, 71]. Concerns have been raised that future systems could aid malicious actors in the development of biological weapons or lower the level of expertise required for executing increasingly sophisticated cyberattacks [24, 29, 68]. More speculatively, AI systems might become challenging to evaluate or control [15, 23, 41, 58, 71]. For these reasons, it is increasingly important for AI developers to demonstrate that their systems are sufficiently safe to deploy.111Our safety case template focuses on deployment decisions, though assurances around pre-deployment decisions (e.g. whether to train a model) are also important. One assurance strategy gaining traction for frontier AI is safety cases [7, 11, 17, 34, 73]. A safety case provides a structured and substantiated argument for why the risk associated with a safety-critical system is acceptable [38]. This method has been used in other sectors, including nuclear energy, offshore development, aviation, railway, software, and autonomous vehicles [10, 20, 25, 51, 72, 74]. Several scholars have recently discussed the possibility of applying safety cases to frontier AI [11, 6, 64, 5]. Frontier AI developers, including Anthropic [3] and Google DeepMind [27], have also expressed interest in moving in this direction. Additionally, safety cases are consistent with the Frontier AI Safety Commitments announced at the AI Seoul Summit 2024, which emphasise the need for comprehensive safety assessments to ensure responsible AI development and deployment [22]. That said, there is no readily available safety case methodology for frontier AI. While practices in other industries may provide useful guidance, the particularities of AI systems and associated risks require a tailored approach. Given the state of AI safety research, producing a holistic and scalable safety case—a comprehensive argument that addresses all potential risks across the system's lifecycle—does not seem feasible today for a frontier AI model. Still, we believe it is useful to develop safety case templates [7]. Such templates detail the argument structure and evidence for various parts of an AI safety case and may serve as building blocks for a full safety case in the future [34]. This article seeks to contribute to AI safety case methodology by sketching a safety case for cyber capabilities. This is a structured argument explaining why a given AI system does not cause a significant increase in cyber risk due to limited capabilities. We focus on cyber because the near-term risk is relatively established and because we have a relatively developed understanding of risk models [53, 54, 77, 79]. We focus on inability (‘the system is not capable of X, even absent any safeguards’) because it offers the simplest and best understood argument for why a system will not behave in a certain way. For current systems, developers largely rely on implicit or explicit inability arguments to assure safety. Other arguments for why a model will not behave in a certain way include control (‘the system is not capable of X, given existing safeguards’) and trustworthiness (‘even if the system is capable of X, it will consistently behave in a desirable way, thus avoiding X’) [17]. Finally, this safety case is structured to inform deployment decisions. We expect that safety cases for more capable systems will also be useful for different decisions, such as whether to start or continue training runs. This template serves as a proof of concept, illustrating that such a safety case could be viable in principle, while acknowledging we have significant uncertainties around the specifics. It builds on current frontier AI safety techniques, in particular model evaluations, which are already integral to safety efforts of major frontier developers [2, 57, 60]. This safety case template should be seen as an attempt to make explicit the implicit argument for safety based on these model evaluations. It does not guarantee safety; some of the claims in our template could fail to hold true in reality, invalidating the conclusion. Still, we expect that even these imperfect safety cases serve to increase the level of rigour in reasoning about development or deployment decisions. This can improve the confidence of developers—and, by extension, governments and the public—in claims about the safety of frontier AI systems. Being explicit about safety reasoning can also unveil specific points of disagreement about safety approaches. In the same vein, we hope this work lays the foundation for structured discussions on how to write AI safety cases, thereby advancing the frontier of AI assurance. The article proceeds as follows. Section 2 reviews related work on AI safety cases. Section 3 provides an overview of the structure and components of our safety case template. Section 4 details a safety case template for offensive cyber capabilities. Section 5 concludes with a summary of the article’s main contributions and suggestions for further research."
https://arxiv.org/html/2411.07795v1,InvisMark: Invisible and Robust Watermarking for AI-generated Image Provenance,"The proliferation of AI-generated images has intensified the need for robust content authentication methods. We present InvisMark, a novel watermarking technique designed for high-resolution AI-generated images. Our approach leverages advanced neural network architectures and training strategies to embed imperceptible yet highly robust watermarks. InvisMark achieves state-of-the-art performance in imperceptibility (PSNR\sim51, SSIM \sim 0.998) while maintaining over 97% bit accuracy across various image manipulations. Notably, we demonstrate the successful encoding of 256-bit watermarks, significantly expanding payload capacity while preserving image quality. This enables the embedding of UUIDs with error correction codes, achieving near-perfect decoding success rates even under challenging image distortions. We also address potential vulnerabilities against advanced attacks and propose mitigation strategies. By combining high imperceptibility, extended payload capacity, and resilience to manipulations, InvisMark provides a robust foundation for ensuring media provenance in an era of increasingly sophisticated AI-generated content. Source code of this paper is available at: https://github.com/microsoft/InvisMark.","The rapid advancement of generative AI (GenAI) technologies has revolutionized the creation of digital images, enabling the production of hyper-realistic deepfakes with unprecedented ease. While this technological leap offers exciting possibilities for creative expression, it simultaneously poses significant challenges to information integrity and public trust. The potential for these AI-generated images to be used in manipulating elections, damaging reputations, and undermining societal foundations underscores the urgent need for robust solutions to verify the origin and authenticity of digital content [18]. In response to these challenges, the Coalition for Content Provenance and Authenticity (C2PA) has emerged as a collaborative effort aimed at combating misinformation within the digital content ecosystem [31, 1]. The C2PA suggests adding signed provenance information directly into its metadata. However, this approach is vulnerable to metadata stripping by malicious actors or during content sharing on social media platforms [7, 8, 38]. In this case, the stripped provenance can potentially be recovered through soft bindings, such as fingerprinting or watermarking, from trusted repository. Fingerprinting techniques leverage near-duplicate search in trusted databases for recovering content provenance [7, 8, 38]. However, these matches often lack precision, necessitating human intervention for verification. Image watermarking offers an alternative solution by inserting an imperceptible identifier within the content itself [14]. This allows for exact matching and retrieval of associated provenance from databases. Traditional watermarking techniques have focused on embedding imperceptible patterns within images, either directly in pixel values or in transformed frequency domains. Pixel-based methods, such as least significant bit (LSB) embedding, are simple to implement but highly susceptible to removal [34]. Frequency-domain techniques, utilizing transforms like Discrete Wavelet Transform (DWT) or Discrete Cosine Transform (DCT), offer improved robustness to certain transformations and have seen industrial adoption, such as the commerical implementation of Stable Diffusion [4, 30]. Despite the improvements, frequency domain methods still suffer from vulnerability to relatively minor alterations to the image, limiting their robustness in real-world scenarios [21, 20, 29]. Figure 1: Overview of our method for watermark encoding and decoding. Watermark first passes through a preprocessing layer and then concatenates with the resized cover image. A MUNIT-based encoder generates watermark residuals, which are upscaled and added to the original image, producing the watermarked image. To ensure robustness, we select the top-k noises that yield the poorest watermark recovery from a pre-defined set of noises, these losses are incorporated into watermark training. The advent of GenAI has spurred the development of innovative watermarking algorithms that integrate seamlessly into the image generation process. These approaches include watermarking training images with pre-trained encoders and decoders, followed by fine-tuning generative models on these watermarked images [35]. Alternatively, some methods focus on fine-tuning only the decoder of a Latent Diffusion Model (LDM) while leaving the diffusion component unchanged [26]. However, these approaches are computationally intensive and often model-specific. Furthermore, their primary targeted applications have been in AI-generated image detection or user identification [32, 17], rather than content provenance tracking. In contrast, post-generation watermarking offers greater flexibility, as it can be applied to any image regardless of its origin. Methods like HiDDen [40] and RivaGAN [36] utilize encoder-decoder architecture to embed hidden messages within images. Subsequent works have proposed various improvements to enhance image quality and robustness [12, 15, 27, 33]. For instance, StegaStamp [33] introduces significant image perturbations between encoder and decoder, enabling the encoded image to withstand real-world distortions. TrustMark [10] proposed a scaling-based approach for watermarking images of any resolution. Beyond encoder-decoder architectures, SSL [17] suggests embedding watermarks within the self-supervised latent space by shifting image features into a designated area. RoSteALS [11] encodes messages in the latent space using a frozen VQVAE [16], but imperceptibility is constrained by VQVAE reconstruction quality. Despite these advancements, current watermarking algorithms face limitations in their effectiveness as a soft-binding solution for AI-generated images. Previous algorithms, often trained and evaluated on low-resolution images, encounter difficulties maintaining their performance when scaled to the higher resolutions, which is now standard in modern image generation models like Stable Diffusion and DALL\cdotE 3 [30]. Furthermore, the inherent tradeoff between capacity, imperceptibility, and robustness restricts watermark payload capacity, typically to under 100 bits. This limited capacity elevates the risk of ID collisions in the presence of bit errors, compromising the reliability of watermark extraction. Additionally, post-generation watermarking techniques can introduce perceptible artifacts, negatively impacting image quality and hindering adoption, especially in creative fields where visual fidelity is paramount. To address these challenges, we introduce InvisMark, a novel approach rooted in the insight that high-resolution images inherently possess the capacity to embed a multitude of imperceptible signals. By leveraging carefully crafted neural network architectures and training strategies, we can effectively harness this potential. Our contributions can be summarized as follows: Jpeg Compress. (min q) Brightness (bri.) Contrast (con.) Saturation (sat.) Gaussian blur (k,\sigma) Gaussian Noise (std.) Posterize (bits) 50 075/1.25 0.75/1.25 0.75/1.25 5, 0.1-1.5 0.04 4 ColorJiggle (bri., con., sat., hue) RGB shift (shift limit) Flip (prob.) Rotation (deg.) RandomErasing (scale, ratio) Perspective (scale) RandomResizedCrop (scale, ratio) 0.1, 0.1, 0.1, 0.02 0.05 1.0 0-10.0 0.02-0.1, 0.5-1.5 0.1 0.75-1.0, 3/4 - 4/3 Table 1: Noise settings used in watermark training and evaluation. For JPEG compression, q is the compression factor. For Gaussian blur, k is the kernel size and \sigma is the range for the standard deviation of the kernel. Additionally, for Brightness, Contrast and Saturation, only the lower and upper bound values are used to simulate noise, rather than random values within the range. All noise implementations utilize the Kornia library, except for JPEG compression, which employs the torchvision library. 1. Novel architecture: We apply resolution scaling during training and employ robust optimization techniques to enhance decoder resilience against common image transformations with minimal impact on encoded image quality. 2. State-of-the-art performance: InvisMark outperforms existing methods in both imperceptibility and robustness across AI-generated and non-AI-generated image datasets. 3. Larger Payload: We demonstrate the ability to embed 256 bits of watermarks while maintaining exceptional imperceptibility and robustness, expanding the practical applications of our method in real-world scenarios."
https://arxiv.org/html/2411.07702v1,"A Call to Reconsider 
Certification Authority Authorization (CAA)","Certification Authority Authentication (CAA) is a safeguard against illegitimate certificate issuance. We show how shortcomings in CAA concepts and operational aspects undermine its effectiveness in preventing certificate misissuance. Our discussion reveals pitfalls and highlights best practices when designing security protocols based on DNS.Keywords: Web PKI, DNS, CAA, Protocol Security","Web security 1 relies on X.509 certificates. Fundamental to the underlying security model is the correct issuance of certificates. This trust is challenged by unrestricted certification. Certification Authorities (CA) are generally allowed to certify any arbitrary resource, i.e., to bind a public key to a domain name by creating a certificate. If certification occurs without the consent of a name owner, the misissued certificate can be used to impersonate resources of that name owner. Causes for misissuance are manyfold. Rogue or compromised CAs 2, spoofed DNS records or compromised name servers 3, and malicious traffic rerouting 4, 5 have been part of attack vectors in the past. There are two principal directions to harden the security of the certificate issuance process. (i) Prevention by enabling a CA to verify whether the issuing request is valid. (ii) Mitigation by enabling the name owner or any third party on behalf to identify and revoke an incorrectly issued certificate. Both directions have been covered in standardization and deployment. Certificate transparency (CT) 6 makes certificates public and DNS-Based Authentication of Named Entities for TLS (DANE TLSA) 7 binds certificates to services available below a specific domain name. Prevention is specified in CA authorization (CAA) 8, which allows a name owner to note in the DNS which CA is allowed to issue a certificate. In this paper, we argue that both prevention and mitigation are important, but that prevention should be the first-class citizen because it addresses a root cause (§ 2). Unfortunately, CAA, which is not only an IETF standard but also the solution the CA/Browser Forum agreed on to be mandatory, is flawed. We revisit design decisions and deployment behaviors (§ 5) to refine on-going standards and inform our community to guide fundamentally different approaches (§ 6), hoping that these insights prevent common pitfalls in the future. Our data-driven approach (§ 4) is based on more than 4.6M unique certificates from CT logs, which we test whether they conform to CAA policies. Our major findings read: 1. Implicit semantics overshadow expressiveness. 2. Underspecified syntax allows misinterpretation. 3. Loose policy scoping raises security risks. 4. Misaligned procedures defeat reliability and trust."
https://arxiv.org/html/2411.07597v1,"A Survey on Adversarial Machine Learning for Code Data: Realistic Threats, Countermeasures, and Interpretations","Code Language Models (CLMs) have achieved tremendous progress in source code understanding and generation, leading to a significant increase in research interests focused on applying CLMs to real-world software engineering tasks in recent years. However, in realistic scenarios, CLMs are exposed to potential malicious adversaries, bringing risks to the confidentiality, integrity, and availability of CLM systems. Despite these risks, a comprehensive analysis of the security vulnerabilities of CLMs in the extremely adversarial environment has been lacking. To close this research gap, we categorize existing attack techniques into three types based on the CIA triad: poisoning attacks (integrity & availability infringement), evasion attacks (integrity infringement), and privacy attacks (confidentiality infringement). We have collected so far the most comprehensive (79) papers related to adversarial machine learning for CLM from the research fields of artificial intelligence, computer security, and software engineering. Our analysis covers each type of risk, examining threat model categorization, attack techniques, and countermeasures, while also introducing novel perspectives on eXplainable AI (XAI) and exploring the interconnections between different risks. Finally, we identify current challenges and future research opportunities. This study aims to provide a comprehensive roadmap for both researchers and practitioners and pave the way towards more reliable CLMs for practical applications.","Code Language Model (CLM)111This paper denotes the auto-regressive large language models trained on both natural language and programming language corpus as “large CLMs”. refers to the statistical language model taking programming source code as input or output. Ranging from encoder-decoder architecture like CodeBERT [1], GraphCodeBERT [2], and CodeT5 [3, 4], to decoder-only architecture like GPT-C (applied in IntelliCode Compose) [5], CodeX (applied in GitHub Copilot) [6], and GPT-4 [7], CLMs have gained widespread popularity in automatizing soft engineering tasks in recent years [8]. According to the latest research from industry [9], the usage of CLM tools on software engineering tasks can reduce the coding time consumption by 55% and increase the task completion rate by 8%. Among the practitioners using CLM tools, 60% \sim 70% of them feel more satisfied, efficient, and productive with their jobs. As pointed out by another research from academia [10], the CLMs have the potential to further reduce the resource needs and error rates in soft engineering tasks by 10x in the next few years. In real-world applications, CLMs are exposed to various security threats throughout their model lifecycle, enabling the underlying adversaries to infringe on the Confidentiality, Integrity, and Availability (CIA) properties of the CLM system. For instance, in July 2023, researchers from Mithril Security developed a poisoning attack called PoisonGPT [11] to distribute a poisoned CLM on the open-source platform HuggingFace, showing that the supply chain of the CLM can be compromised; in February 2023, Djenna et al. [12] reported that deep learning-based malware detection methods are susceptible to advanced evasion attacks such that cyber attackers can bypass the malware detector [12]; in June 2023, ChatGPT plugins by OpenAI are discovered to have risks of leaking the user’s source code and Personally Identifiable Information (PII) [13]. and in August 2019, the GPT-2 (1.5 B parameter) by OpenAI was easily replicated with only 50 K dollars before GPT-2 was officially fully released [14]. Thus, understanding the security risks is vital for the reliable deployment of CLMs in realistic applications. However, existing surveys mainly study CLMs from the perspective of foundation models, software engineering, and software security [15, 16, 17, 18, 19, 20, 21, 22], with few systematizing the CIA properties and threat models of CLMs. In security research, threat models are assumptions on the adversaries’ knowledge and capabilities and thus are pivotal in analyzing the risks in real applications. To close this gap, we surveyed the adversarial machine learning research paper on CLMs from the perspective of the CIA security properties [23], covering the topics of poisoning attacks (integrity/availability infringement), evasion attacks (integrity infringement), and privacy attacks (confidentiality infringement). We have collected the most comprehensive literature to date (79 papers) from the research fields of artificial intelligence, computer security, and software engineering. For each type of risk, we systematically categorize the attack threat models and detail the attack techniques associated with each, along with their respective countermeasures. We further adopt an eXplainable Artificial Intelligence (XAI) perspective to interpret the risks and analyze the interconnections between each risk, which are rarely adopted in existing CLM surveys. Finally, we identify five research gaps in the adversarial machine learning of CLMs for future work. The organization of this survey is summarized in Figure 1. We begin with our motivation and study organization in Section II, and then conduct a detailed survey on poisoning attacks, evasion attacks, and privacy attacks in Section III, Section IV, and Section V, respectively. We further explore the connections between these risks and present insights from an XAI perspective in Section IX and Section VII. Finally, we provide recommendations for future directions in Section VIII. Figure 1: Organization of this survey. In sum, our contributions are as follows: • Comprehensive Paper Collection on Adversarial Machine Learning of CLMs. We collect and analyze the most extensive collection of research to date on adversarial machine learning of Code Language Models (79 papers). This comprehensive survey addresses the existing gap by covering the complete CIA security model, providing a thorough understanding of the real-world security risks associated with CLMs. • Comprehensive Taxonomy of Threat Models. We provide an in-depth taxonomy of CLM risks, including poisoning attacks, evasion attacks, and privacy attacks. We categorize each risk according to threat models, attack techniques, and countermeasures, highlighting the real-world applicability of each attack technique and assessing the cost-effectiveness of various countermeasures. • Novel insights for CLM Security Risks Analysis. By incorporating XAI and the risk connections perspectives, we distill practical insights and future directions for researchers in the field of CLMs. These findings pave the way for building more trustworthy CLMs for real-world applications."
https://arxiv.org/html/2411.07535v1,Double-Signed Fragmented DNSSEC for Countering Quantum Threat,"DNSSEC, a DNS security extension, is essential to accurately translating domain names to IP addresses. Digital signatures provide the foundation for this reliable translation, however, the evolution of Quantum Computers has made traditional digital signatures vulnerable. In light of this, NIST has recently selected potential post-quantum digital signatures that can operate on conventional computers and resist attacks made with Quantum Computers. Since these post-quantum digital signatures are still in their early stages of development, replacing pre-quantum digital signature schemes in DNSSEC with post-quantum candidates is risky until the post-quantum candidates have undergone a thorough security analysis. Given this, herein, we investigate the viability of employing Double-Signatures in DNSSEC, combining a post-quantum digital signature and a classic one. The rationale is that double-signatures will offer protection against quantum threats on conventional signature schemes as well as unknown non-quantum attacks on post-quantum signature schemes, hence even if one fails the other provides security guarantees. However, the inclusion of two signatures in the DNSSEC response message doesn’t bode well with the maximum allowed size of DNSSEC responses (i.e., 1232B, a limitation enforced by MTU of physical links). To counter this issue, we leverage a way to do application-layer fragmentation of DNSSEC responses with two signatures. We implement our solution on top of OQS-BIND and through experiments show that the addition of two signatures in DNSSEC and application-layer fragmentation of all relevant resource records and their reassembly does not have any substantial impact on the efficiency of the resolution process and thus is suitable for the interim period at least until the quantum computers are fully realized.","The Domain Name System (DNS) is responsible for translating human-readable domain names into machine-understandable IP addresses. As can be imagined, any vulnerability in this system can potentially make such translation precarious, leading users to malicious servers instead of their intended destinations. The Security Extension of DNS - i.e., Domain Name System Security Extensions (DNSSEC) helps establish that the messages received by a client are from an authorized DNS server and have not been altered in transit. DNSSEC primarily leverages digital signatures for establishing the aforementioned properties of authenticity and integrity. However, the underlying security assumptions of conventional digital signature algorithms - i.e., the computational difficulty of integer factorization and discrete logarithms will not hold when an attacker has access to a Cryptographically Relevant Quantum Computer (CRQC) (Agency, 2021; Shor, 1994). To mitigate this problem, the National Institute of Standards and Technology (NIST) has recently shortlisted three post-quantum digital signatures - i.e., FALCON (Prest et al., 2020), CRYSTALS-DILITHIUM (Lyubashevsky et al., 2020), and SPHINCS+ (Hulsing et al., 2020). An important envisioned property of these post-quantum digital signature algorithms is that they can run on classical computers but also withstand the attacks conducted using CRQCs, thereby offering sufficient protection against the impending threat of CRQCs. For DNSSEC to continue offering the needed authenticity and integrity in the post-quantum era, it must be updated with post-quantum digital signatures. However, for successful transitioning to the post-quantum era, the Interim Period - i.e., from now until CRQCs are fully realized or until the security of post-quantum candidates is fully established, presents a particular challenge. Precisely, in preparation for the post-quantum era, simply switching pre-quantum cryptography (such as conventional digital signatures in DNSSEC) with post-quantum cryptography (such as the post-quantum candidates shortlisted by NIST) in the interim period may lower overall security (ENISA, 2022). The reason is that the shortlisted post-quantum candidates (both key encapsulation mechanisms and digital signatures) have yet not gone through a thorough cryptanalysis against both classical and quantum attacks. The shortlisted post-quantum candidates may be compromised even using conventional classical computers in the near future. Therefore, there is an inherent risk in simply switching from pre-quantum cryptography to post-quantum cryptography alone in that overall security may be lowered not only against CRQCs but also against today’s classical computers (see further details in Section 3). Aligned with the aforementioned discussion and the recommendations from the European Union Agency For Cybersecurity (ENISA), we in this paper investigate the plausibility of Double-Signed DNSSEC by combining the pre-quantum and post-quantum digital signatures for the interim period. However, the size of signatures and public keys of post-quantum candidates shortlisted by NIST is very large (i.e., shortlisted post-quantum signatures are 11\times to 122\times and public keys are 14\times to 20\times of their pre-quantum counterparts). These sizes further increase when double signatures are incorporated into the DNSSEC resolution process to ensure authenticity and integrity. This increased size of signatures and public keys significantly impacts the DNSSEC resolution process (Kampanakis and Lepoint, 2023). Precisely, since UDP is the preferred transport protocol for DNSSEC because of it being lightweight, any response message that exceeds 1232B triggers IP-level fragmentation which is unreliable. The maximum size of DNSSEC response messages - i.e., 1232B is obtained as: (IPv6 MTU=1280)-(IPv6 Header=40)-(UPD Header=8). With double signatures, the sizes of DNSSEC response messages that carry different resource records for facilitating reliable address resolutions are much larger than the threshold size of 1232B. Therefore, these large DNSSEC messages necessitate IP fragmentation which is unreliable over UDP (i.e., some fragments may never reach the resolver) (Van Den Broek et al., 2014). Due to this issue, the standard DNSSEC offers a fallback mechanism via TCP for sending large response messages without resorting to fragmentation. Precisely, when the response size exceeds the threshold (i.e., the resolver’s advertised EDNS(0) buffer size), name servers send back a truncated response (i.e., by setting the TC flag), indicating the resolver to retry via TCP (and thus negatively contributing to the address resolution time). However, many name servers lack support for TCP in addition to high-resolution time with TCP due to it being not as lightweight as UDP (Van Den Broek et al., 2014; van Rijswijk-Deij et al., 2016; Mao et al., 2022). To circumvent the aforementioned issues, a potential alternative is to perform the fragmentation at the application layer (i.e., within the DNS protocol itself). While there exist ways to tackle the application layer fragmentation (see (Goertzen and Stebila, 2022; Rawat and Jhanwar, 2024)), none of them can accommodate double signatures and public keys. Therefore, to accomplish this, first, we constructed a Docker-based testbed on an Amazon EC2 instance using commercial grade DNS software - i.e., BIND9 (OQS-BIND, see full details on our testbed in Section 5). Second, we explored generating two signatures (pre-quantum and post-quantum) on the Name Server using the default BIND9 tools. Third, we devised a way to fragment the DNSSEC responses with two signatures and public keys on the name servers and accurately reassemble them on the resolver. We also modified the source code of the BIND9 resolver to enable the verification of both signatures before marking the response as authenticated. Finally, using our testbed, we conducted an empirical analysis to show that our implementation can handle two signatures in DNSSEC simultaneously and quantify the impact of double signatures on the DNSSEC resolution process. Our experiments show that double signatures have a negligible impact on the average resolution time of DNSSEC compared to only post-quantum digital signatures. Therefore, this analysis confirms the efficacy of double signatures in DNSSEC for the interim period until CRQCs are fully realized. Our main contributions are summarized as follows: • We developed a fully functional Docker-based DNSSEC testbed over an Amazon EC2 instance using the widely used BIND9 software that can handle both pre-quantum and post-quantum signatures and public keys simultaneously over UDP in a single DNSSEC response message. • We thoroughly investigated the source code for BIND9 and extensively modified the resolver component to enable the verifications of both pre-quantum and post-quantum digital signatures at the same time. • Through empirical analysis we show that the addition of double signatures in the DNSSEC resolution process did not affect the performance. Hence, it is recommended for the interim period until CRQCs are fully realized. The rest of the paper is organized as follows. Section 2 presents a brief overview of DNS, while Section 3 discusses the overall quantum threat and the motivation for using double signatures in DNSSEC. Section 4 presents the details of the maximum message size issue with double signatures and how to handle them in the resolution process. Section 5 details our test setup and experimental methodology. Results are discussed in Section 6 and related works and concluding remarks are contained in Sections 7 and 8, respectively."
https://arxiv.org/html/2411.07528v1,Logs are All You Need in Security,"Large and Small Language Models (LMs) are typically pretrained using extensive volumes of text, which are sourced from publicly accessible platforms such as Wikipedia, Book Corpus, or through web scraping. These models, due to their exposure to a wide range of language data, exhibit impressive generalization capabilities and can perform a multitude of tasks simultaneously. However, they often fall short when it comes to domain-specific tasks due to their broad training data. This paper introduces SecEncoder, a specialized small language model that is pretrained using security logs. SecEncoder is designed to address the domain-specific limitations of general LMs by focusing on the unique language and patterns found in security logs. Experimental results indicate that SecEncoder outperforms other LMs, such as BERT-large, DeBERTa-v3-large and OpenAI’s Embedding (text-embedding-ada-002) models, which are pretrained mainly on natural language, across various tasks. Furthermore, although SecEncoder is primarily pretrained on log data, it outperforms models pretrained on natural language for a range of tasks beyond log analysis, such as incident prioritization and threat intelligence document retrieval. This suggests that domain-specific pretraining with logs can significantly enhance the performance of LMs in security. These findings pave the way for future research into security-specific LMs and their potential applications.","Transformers [65] are a breakthrough AI architecture that have facilitated the development of various Language Models (LMs) [27, 3]. These models can harness huge amounts of data to perform diverse tasks across language and other modalities such as audio, image and video. Some examples of LMs are BERT, RoBERTa, GPT, Gemini and PaLM [48, 19, 23, 17, 24, 25, 4, 5]. LMs can differ in their size, data sources, learning objectives, and are trained on large collections of text, such as Wikipedia, books, news articles, code, social media posts or web scraping. Certain language models, such as encoder-only models, are designed to encode the semantic and syntactic information of natural language into high-dimensional vectors, enabling them to perform downstream tasks like search, classification, summarization, translation, and question answering. Decoder-only models, on the other hand, are capable of generating natural language text by sampling from their probability distributions, producing coherent and fluent outputs. Some models, such as GPT-o and Gemini 1.5, extend these capabilities to multimodal content generation, creating images, audio, and video by utilizing a shared latent space across different modalities. Language models (LMs) have achieved state-of-the-art results across a wide range of benchmarks in natural language processing, coding, mathematics, and reasoning. They have also demonstrated impressive abilities to generate realistic and creative content, including stories, poems, songs, and jokes. These advancements in LMs have unlocked new opportunities and introduced challenges for AI research and applications, spanning areas such as natural language understanding, natural language generation, multimodal fusion, and knowledge extraction. However, LMs are not ideally suited to address domain-specific challenges, such as specialized vocabulary, terminology, knowledge, and logic, due to their design for broad and diverse applications. Previous studies highlight these limitations across several domains. In Biomedicine [31, 49], LMs struggle to capture complex relationships and semantics of biomedical entities and concepts. In Finance [66], LMs underperform compared to domain-specific counterparts. In Medicine [40], LMs lack alignment with clinical utility. In Security [38, 21], LMs fall short in domain-specific security knowledge. Similarly, in Software [33], LMs face challenges in interpreting and making sense of operational logs. Despite these limitations, the potential of Artificial Intelligence (AI), particularly generative AI, continues to attract significant interest from the security community [11, 2, 6]. These generative models hold potential as valuable tools for security professionals, serving as copilots to navigate the complexities of security tasks such as identifying phishing emails, crafting detections, or analyzing and summarizing incidents. However, the field of security presents unique challenges that can only be partially addressed by generative models. A notable challenge is the need for security professionals to handle a variety of data types beyond natural language texts, including logs and telemetries. These data are often heterogeneous, noisy, and voluminous, necessitating efficient and scalable processing and analysis methods. In this paper, we present SecEncoder, a small language model that is trained with security logs. SecEncoder is an encoder-only model, pretrained on a large corpus of security logs, which capture various events and activities related to security incidents and operations. SecEncoder aims to demonstrate the feasibility and utility of training a domain-specific language model on security logs at scale, and to provide a versatile and powerful model that can be applied to various security use cases. We evaluate SecEncoder on both intrinsic and extrinsic tasks, such as log analysis, anomaly detection, log search and incident classification. Our main contributions are: • We pretrain a security-specific small language model from scratch on a large and diverse corpus of security logs, which capture various events and activities related to security incidents and operations. We aim to train a versatile and powerful model that can generalize to various type of security use cases. • We evaluate SecEncoder using both intrinsic and extrinsic measures, using both internal and publicly available benchmarks. We also compare SecEncoder to the other LMs, such as BERT, DeBERTa and OpenAI’s embedding model (text-embeddings-ada-002), and show that SecEncoder outperforms the best results on most of the tasks, and also exhibits some unique and novel capabilities. • We present four real-world use cases for SecEncoder. Notably, some of these use cases such as incident classification and threat intelligence document retrieval demonstrate that, despite SecEncoder being primarily trained on logs, it can effectively generalize to other data modalities without specific training on them. This finding suggests that logs could serve as valuable data sources for pretraining language models across domains beyond security. • We discuss the limitations and future directions for SecEncoder, focusing on areas such as data quality and diversity, as well as improvements in robustness and inference speed. The remainder of this paper is structured as follows: Section 2 discusses related work, while Section 3 introduces the overall architecture and design. Section 4 details the various experiments conducted for testing and evaluation. Section 5 explains multiple real world use cases of SecEncoder and the corresponding results. Section 6 delves into limitations of SecEncoder and discusses future work, and finally, Section 7 provides the conclusion."
https://arxiv.org/html/2411.07498v1,Semantic Sleuth: Identifying Ponzi Contracts via Large Language Models,"Smart contracts, self-executing agreements directly encoded in code, are fundamental to blockchain technology, especially in decentralized finance (DeFi) and Web3. However, the rise of Ponzi schemes in smart contracts poses significant risks, leading to substantial financial losses and eroding trust in blockchain systems. Existing detection methods, such as PonziGuard, depend on large amounts of labeled data and struggle to identify unseen Ponzi schemes, limiting their reliability and generalizability. In contrast, we introduce PonziSleuth, the first LLM-driven approach for detecting Ponzi smart contracts, which requires no labeled training data. PonziSleuth utilizes advanced language understanding capabilities of LLMs to analyze smart contract source code through a novel two-step zero-shot chain-of-thought prompting technique. Our extensive evaluation on benchmark datasets and real-world contracts demonstrates that PonziSleuth delivers comparable, and often superior, performance without the extensive data requirements, achieving a balanced detection accuracy of 96.06% with GPT-3.5-turbo, 93.91% with LLAMA3, and 94.27% with Mistral. In real-world detection, PonziSleuth successfully identified 15 new Ponzi schemes from 4,597 contracts verified by Etherscan in March 2024, with a false negative rate of 0% and a false positive rate of 0.29%. These results highlight PonziSleuth’s capability to detect diverse and novel Ponzi schemes, marking a significant advancement in leveraging LLMs for enhancing blockchain security and mitigating financial scams.","Smart contracts, self-executing contracts with the terms of the agreement directly written into code, are foundational to decentralized finance and Web3 (sun2023panda, ; liang2024vulseye, ; liang2024towards, ; wu2022TokenScout, ). However, their widespread adoption has also facilitated the emergence of Ponzi schemes, which are fraudulent investment operations where returns to earlier investors are paid using the capital from newer investors (chen2024ponzifinder, ; liang2024ponziguard, ). These schemes exploit the transparency and automation of smart contracts by promising high returns with minimal risk, thereby attracting new investors. The funds from these new investors are used to pay earlier investors, creating a false impression of a profitable venture. As the influx of new investors diminishes, the scheme inevitably collapses, resulting in significant financial losses for most participants. Ponzi schemes erode trust in DeFi and Web3 ecosystems, cause severe economic damage, and hinder broader adoption (lou2020ponzi, ). Several methods have been developed to detect Ponzi contracts on Ethereum, which can be categorized into three main types. The first type uses bytecode or opcodes from smart contracts to train classifiers or perform static analysis (fan2021spsd, ; chen2018detecting, ). These methods analyze the low-level instructions executed by the Ethereum virtual machine to identify patterns that suggest Ponzi schemes. The second type examines the transaction behavior of smart contracts, looking at the sequence and characteristics of transactions to spot suspicious financial activities (yu2021ponzi, ; cai2023ponzi, ). The third type combines both opcode features and account features to train detection models, using a wider range of data to improve accuracy (lou2020ponzi, ; liang2024ponziguard, ). Additionally, there are rule-based approaches that rely on expert knowledge of Ponzi schemes, creating predefined rules to identify fraudulent contracts based on known patterns (chen2021sadponzi, ; sun2020early, ). Motivation. Despite significant advancements, existing detection methods face critical limitations. They depend heavily on large volumes of labeled training data, which is often challenging and costly to obtain (chen2021sadponzi, ; liang2024ponziguard, ; lu2024sourcep, ). Additionally, these methods rely on static information, which does not adequately capture the dynamic and evolving nature of Ponzi schemes, leading to poor reliability and limited interpretability. Furthermore, they struggle to detect new and previously unseen Ponzi contracts due to their inability to understand the behavioral semantics embedded in the contract code (sun2020early, ). In contrast, large language models (LLMs) have shown significant promise in understanding complex code semantics and human language (chiang2023can, ; chen2021evaluating, ; xu2023lmpa, ; 0x2a53f4, ; lin2023pushing, ; fang2024automated, ; lin2024splitlora, ). LLMs, including GPT-3, LLAMA, and their variants, are advanced neural networks trained on vast amounts of text data, enabling them to comprehend context, identify patterns, and generate accurate analyses (lin2024fedsn, ; lin2024efficient, ; fang2024ic3m, ; sun2024earpass, ; wu2024wafbooster, ; wu2019icauth, ; lin2024adaptsfl, ; wu2020caiauth, ; wu2020liveness, ; wu2022echohand, ; wu2021toward, ; sun2024maglive, ). This capability makes them particularly suited for interpreting and detecting diverse Ponzi schemes, even those not encountered during training. Therefore, our research question is: Can large language models be designed to efficiently and robustly identify Ponzi contracts, including previously unseen ones, using minimal labeled data? PonziSleuth. In this paper, we propose PonziSleuth, the first LLM-driven approach for detecting Ponzi contracts. PonziSleuth leverages LLMs to analyze source codes of contracts, enabling efficient and robust identification of Ponzi schemes with minimal labeled data. PonziSleuth combines zero-shot chain-of-thought (Zero-shot-CoT) prompting, static taint analysis, and automated code slicing, allowing PonziSleuth to grasp the complex behavioral semantics of Ponzi contracts that traditional methods miss. PonziSleuth operates in three main phases. First, in the preparation phase, the source code of a smart contract is compiled to generate an abstract syntax tree (AST), which is then parsed into an intermediate representation (IR) and a structured contract object. Second, in the contract analysis and slicing phase, static taint analysis traces the flow of funds within the contract, creating a taint propagation graph that visualizes data flow. Relevant code slices are extracted to reduce the input size for the LLM. Finally, in the LLM detection phase, the contract slices and taint propagation graph are used to generate high-quality prompts for the LLM, which then analyzes the contract to identify potential Ponzi schemes. By uncovering the semantic behavior of Ponzi contracts, PonziSleuth overcomes the limitations of traditional static analysis and rule-based detection methods. It addresses the challenge of relying on large amounts of training data and enhances the ability to recognize previously unseen Ponzi contracts. Our evaluation shows that PonziSleuth performs robustly across different LLMs, achieving a BAC of 96.06% with GPT-3.5-turbo, 93.91% with LLAMA3, and 94.27% with Mistral, demonstrating its capability in detecting both known and previously unseen Ponzi contracts. Unlike PonziGuard, which relies heavily on labeled data and may be prone to overfitting, PonziSleuth offers a more generalized approach, effectively identifying 15 new Ponzi schemes from 4,597 contracts verified by Etherscan between March 14-24, 2024, with a false negative rate (FNR) of 0% and a false positive rate (FPR) of 0.29%. We responsibly disclosed the detected Ponzi contracts to the Web3 security community. Additionally, PonziSleuth demonstrates impressive efficiency, processing each contract in an average of 5.52 seconds at a cost of $0.0027 per contract using 2601.3 tokens per detection. PonziSleuth has been deployed for real-time monitoring of Ponzi contracts, providing a powerful and innovative tool for detecting fraud in smart contracts without relying on extensive labeled data. In summary, we make the following contributions. • To our knowledge, PonziSleuth is the first LLM-driven method for detecting Ponzi smart contracts, requiring none labeled training data. • We introduce a novel two-stage Zero-shot-CoT method in PonziSleuth, enhancing LLMs with static taint analysis and automated code slicing to efficiently detect Ponzi schemes, understanding complex fraud patterns, tracing tainted data, and focusing on relevant code segments without extensive retraining. • We perform comprehensive evaluations of PonziSleuth across various settings, including extensive comparisons with existing methods, real-world detection, overhead evaluation. Results indicate that PonziSleuth is effective in detecting unseen Ponzi contracts without relying on labeled data, offering a significant advantage over existing approaches. The data and codebase of PonziSleuth can be obtained at the repository: https://github.com/tasteking/PonziSleuth-ASE24."
https://arxiv.org/html/2411.07479v1,Developers Are Victims Too : A Comprehensive Analysis of The VS Code Extension Ecosystem,"With the wave of high-profile supply chain attacks targeting development and client organizations, supply chain security has recently become a focal point. As a result, there is an elevated discussion on securing the development environment and increasing the transparency of the third-party code that runs in software products to minimize any negative impact from third-party code in a software product. However, the literature on secure software development lacks insight into how the third-party development tools used by every developer affect the security posture of the developer, the development organization, and, eventually, the end product. To that end, we have analyzed 52,880 third-party VS Code extensions to understand their threat to the developer, the code, and the development organizations. We found that 5.6% of the analyzed extensions have suspicious behavior, jeopardizing the integrity of the development environment and potentially leaking sensitive information on the developer’s product. We also found that the VS Code hosting the third-party extensions lacks practical security controls and lets untrusted third-party code run unchecked and with questionable capabilities. We offer recommendations on possible avenues for fixing some of the issues uncovered during the analysis.","Accepting the Turing Award in 1984, Thompson projected that reliance on third-party software components involves a significant amount of trust [1]. Despite this warning and subsequent research and incidents stemming from supply chain issues [2, 3, 4, 5, 6, 7], developers’ and organizations’ reliance on third-party components continues to increase. At the same time, these components create security risks through undocumented functionality and settings, bugs, or malicious code. Supply chain attacks have emerged as a pervasive and ubiquitous threat vector in recent years [8, 9, 10]. The main reason why these types of attacks can be catastrophic is due to the nature of their coverage: numerous systems and software products may be vulnerable, often without users’, administrators’, or organizations’ knowledge, as most software products do not enumerate their constituent third-party components. Mounting an attack on several hundred organizations would otherwise be daunting; however, if the attacker can compromise one of the tools or systems used by all those organizations, then the economics suddenly shift heavily to the attacker. This has become quite apparent in the wake of several high-profile attacks [2, 6, 11]. The sheer scale of these attacks is unprecedented. However, the security community has been aware of these risks for almost 40 years. Standards, like the Software Bill of Materials (SBoM), increase transparency and awareness of the third-party code that may be executed in an organizational environment. A few of the current literature on supply chain security includes securing CI/CD [12, 13] or securing update channels [14] focusing on securing the product even before it ships out of the development organization. Security literature has looked into how developers write secure code [15] or how they look for security advice [16], which is essential in securing the whole ecosystem. However, the recent high-profile attacks on a development organization emphasize the importance of securing the roots of the supply chain: the developer [17]. While SBoM helps to articulate the composition of the software code consisting of third-party software libraries, there has been an oversight on one aspect of the development activity that involves third-party code: developer tools. Developers use tools for a wide range of activities that have a direct impact on the final software code. There is already documented evidence [18, 19] of malicious developer tools that could jeopardize the integrity of the development environment, which will lead to catastrophic events such as recent attacks [17]. However, security and privacy literature lacks systematic knowledge of how third-party developer tools behave in the wild and their impact on the security posture of the developer, the organization, and the end product. To that end, we analyzed 52,880 third-party VS Code extensions using static and dynamic analysis methods. We used static analysis to examine the code and narrow the extension list with suspicious code segments. We then executed those selected extensions using an instrumented VS Code environment, logging all their execution aspects. We also used VirusTotal [20], Retire.js [21] to scan for malicious content and known vulnerabilities. With this holistic analysis approach, we found that 5.6% 111https://github.com/vulnerability-reporter/DAV2-ACAnTVSCEE of our collected extension set has suspicious behavior that could jeopardize the integrity of the development environment and/or leak sensitive information such as code and personally identifiable information. We contribute the following: • Systematically show that the third-party developer tools pose a serious security threat to the developer, the host computer, and the organization. • VS Code, one of the most popular developer tools, has a lax security architecture that lets third-party extensions run unchecked, resulting in serious security lapses. • To the best of our knowledge, the paper presents the first holistic (52, 880) analysis of developer tools and their security and privacy implications."
https://arxiv.org/html/2411.07468v2,Privacy-Preserving Verifiable Neural Network Inference Service,"Machine learning has revolutionized data analysis and pattern recognition, but its resource-intensive training has limited accessibility. Machine Learning as a Service (MLaaS) simplifies this by enabling users to delegate their data samples to an MLaaS provider and obtain the inference result using a pre-trained model. Despite its convenience, leveraging MLaaS poses significant privacy and reliability concerns to the client. Specifically, sensitive information from the client inquiry data can be leaked to an adversarial MLaaS provider. Meanwhile, the lack of a verifiability guarantee can potentially result in biased inference results or even unfair payment issues. While existing trustworthy machine learning techniques, such as those relying on verifiable computation or secure computation, offer solutions to privacy and reliability concerns, they fall short of simultaneously protecting the privacy of client data and providing provable inference verifiability.In this paper, we propose \mathsf{vPIN}, a privacy-preserving and verifiable CNN inference scheme that preserves privacy for client data samples while ensuring verifiability for the inference. \mathsf{vPIN} makes use of partial homomorphic encryption and commit-and-prove succinct non-interactive argument of knowledge techniques to achieve desirable security properties. In \mathsf{vPIN}, we develop various optimization techniques to minimize the proving circuit for homomorphic inference evaluation thereby, improving the efficiency and performance of our technique. We fully implemented and evaluated our \mathsf{vPIN} scheme on standard datasets (e.g., MNIST, CIFAR-10). Our experimental results show that \mathsf{vPIN} achieves high efficiency in terms of proving time, verification time, and proof size, while providing client data privacy guarantees and provable verifiability.","Machine learning (ML) has revolutionized the way computers interact with data, allowing them to acquire knowledge from data on their own, without the limitations of explicit programming. Deep learning, especially Convolutional Neural Networks (CNNs), has revolutionized the field of visual data analysis, enabling applications such as image classification and object recognition. However, addressing the extensive data demands of model training, computational resource constraints, and implementation expertise has led to the rise of Machine Learning as a Service (MLaaS). MLaaS is a cloud-based platform that provides access to powerful ML-assisted services (e.g., analysis, inference, prediction). Typically, MLaaS providers such as Microsoft Azure [1], Google Cloud AI Platform [2], Amazon AWS [3], Face++ [4], and Clarifai [5] operate on a pay-as-you-go model, where clients pay based on their resource usage for inference. Despite its benefits, delegating ML tasks to a server raises some privacy concerns. An adversarial server can misuse the private information of the client data. Additionally, the lack of a mechanism to verify ML operations performed by the server introduces integrity and trustworthiness issues. This becomes essential since an adversarial server may process the client request arbitrarily without relying on a dependable ML model. Moreover, there is a risk of unfair payment, where the server may charge the client more than its actual resource consumption. Consequently, the outcome of the delegated ML processing tasks can be untrustworthy. To address the aforementioned privacy and trustworthiness issues in ML, several research directions have been suggested. One line of research focuses on verifiable Machine Learning (verifiable ML), which aims to ensure the verifiability of ML computations and address concerns regarding unfair payment by requiring the server to provide mathematical proof of correct computations. Some studies, such as Safetynets [6] and VeriML [7], investigate Verifiable Computation techniques (e.g., [8, 9]) that enable the client to effectively verify the correctness of computations performed by an MLaaS server. Moreover, some verifiable ML research incorporates zero knowledge property into proofs (e.g., [10, 11, 12, 13, 14, 15, 16]) to improve server model privacy. While these studies effectively address computation integrity and overcharging concerns, the main limitation of existing verifiable ML schemes is that they do not offer privacy to the client data, where the client has to send their sample (in plaintext) to the server for inference service. The other research line focuses on Privacy-Preserving ML (PPML), in which secure computation techniques such as Homomorphic Encryption (HE) [17] and/or Multi-party Computation (MPC) [18] have been used, to protect the privacy of both client data and server model parameters during the ML evaluation. However, these techniques may not be suitable for MLaaS applications. Specifically, HE-based PPML approaches [19, 20, 21, 22, 23] do not offer computation verifiability. Consequently, the server can employ low-quality model parameters to process computation on encrypted client data, thereby making results unreliable. Meanwhile, MPC-based approaches [24, 25, 26, 27] require distributed systems with multiple non-colluding computationally resourceful entities, which may significantly increase the deployment and operational costs. While existing research attempts to address privacy and verifiability concerns in ML inference schemes, a critical gap persists in ensuring client data privacy within verifiable ML schemes. Therefore, we raise the following question: Can we design a new privacy-preserving and verifiable ML inference scheme that not only preserves the privacy of the client data but also guarantees computational and provable verifiability with efficient performance? Our Contributions. In this paper, we introduce \mathsf{vPIN}, a new privacy-preserving and verifiable ML inference scheme that allows the client to use remote ML inference services with data privacy and inference result integrity guarantees. \mathsf{vPIN} focuses on the standard CNN inference framework with multiple processing layers such as convolution, activation, pooling, and fully connected layers. \mathsf{vPIN} relies on two core building blocks including partial homomorphic encryption and commit-and-prove Succinct Non-interactive Argument of Knowledge (SNARK) to enable client privacy and server computation integrity, respectively. We provide detailed gadgets to represent arithmetic constraints for ciphertext operations (LABEL:sec:gadgets). These gadgets are not only critical for proving CNN inference evaluation on the encrypted data but also can be found useful in other applications. \mathsf{vPIN} addresses the challenges of proving complicated ML functions on the encrypted domains by incorporating new techniques to reduce the circuit complexity including elliptic curve embedding and probabilistic matrix multiplication check. As a result, \mathsf{vPIN} achieves markedly higher efficiency compared to the baseline approaches that hardcode the whole CNN inference computation on the encrypted data into the proving circuit. We provide a formal security definition for privacy-preserving and verifiable ML inference with client data privacy and verifiability, and prove that \mathsf{vPIN} satisfies all the security properties. We fully implemented and evaluated our \mathsf{vPIN} scheme, assessing its performance on real dataset and comparing it with the baseline setting in terms of proving time, verification time, and proof size. The source code for our implementation can be found at https://github.com/vt-asaplab/vPIN Remark. In this work, we only focus on user privacy and verifiability in place of server privacy. It is a challenging task to enable privacy for both the user and server simultaneously. Adding the zero-knowledge property to SNARK can only protect the server’s model privacy in the proof; however, the model can still be leaked from the plain inference outputs (e.g., via model extraction/stealing attacks [28, 29, 30, 31, 32]). Therefore, we defer the investigation that can fully protect the server model privacy as our future work. Applications Scenarios. Our proposed scheme can be useful in some applications in medical diagnosis and financial forecasting [33, 34, 35, 36]. For instance, in medical diagnosis, healthcare providers might use MLaaS inference to analyze private client medical data, such as lung images, for infection detection. However, incorrect model predictions could have a serious impact, especially if the cloud provider fails to use the actual model. Additionally, outsourcing client medical data could potentially pose some complications. To address them, \mathsf{vPIN} allows healthcare providers to encrypt the image before sharing them with the MLaaS provider. Moreover, using our scheme, the healthcare provider can verify that the correct model is used to process the image and ensure charge accuracy based on resource usage, thus preventing overbilling. In financial forecasting, businesses may use MLaaS inference to analyze datasets and make decisions about investments, market trends, and risk management. Like medical diagnosis, these datasets contain sensitive business information that cannot be shared with an MLaaS provider. Furthermore, ensuring the correct model parameters are used for prediction in financial forecasting is crucial, as it can have real-world implications (e.g., financial losses, missed opportunities). Therefore, \mathsf{vPIN} can address these concerns by processing financial data in encrypted form and providing inference verifiability to ensure the correct model is employed."
https://arxiv.org/html/2411.07433v1,SDN-Based Smart Cyber Switching (SCS) for Cyber Restoration of a Digital Substation,"In recent years, critical infrastructure and power grids have increasingly been targets of cyber-attacks, causing widespread and extended blackouts. Digital substations are particularly vulnerable to such cyber incursions, jeopardizing grid stability. This paper addresses these risks by proposing a cybersecurity framework that leverages software-defined networking (SDN) to bolster the resilience of substations based on the IEC-61850 standard. The research introduces a strategy involving smart cyber switching (SCS) for mitigation and concurrent intelligent electronic device (CIED) for restoration, ensuring ongoing operational integrity and cybersecurity within a substation. The SCS framework improves the physical network’s behavior (i.e., leveraging commercial SDN capabilities) by incorporating an adaptive port controller (APC) module for dynamic port management and an intrusion detection system (IDS) to detect and counteract malicious IEC-61850-based sampled value (SV) and generic object-oriented system event (GOOSE) messages within the substation’s communication network. The framework’s effectiveness is validated through comprehensive simulations and a hardware-in-the-loop (HIL) testbed, demonstrating its ability to sustain substation operations during cyber-attacks and significantly improve the overall resilience of the power grid.","The power system is a complex network comprising generation, transmission, and distribution stages, where substations play a pivotal role in altering voltage levels. Traditionally, substations have relied on hardwired communication lines to manually operate devices such as switches and circuit breakers (CBs), resulting in cumbersome and maintenance-heavy configurations. In addition, legacy communication protocols (e.g., DNP3 and Modbus) exacerbate complexities due to inconsistent data mapping across different vendors’ products [1]. The International Electrotechnical Commission (IEC) introduced the IEC 61850 standard to mitigate these challenges, offering substantial benefits such as multi-vendor interoperability, reduced configuration efforts, cost-effective installation, and high-speed, Ethernet-based communication for time critical signals. While these advancements have transformed substations into more efficient and intelligent systems, they have also introduced new cybersecurity vulnerabilities. The digitization and increased use of ICT in substations have exposed these critical infrastructures to risks of cyber-attacks, potentially leading to system failures and significant operational disruptions. Recent cyber-attacks on critical infrastructure, such as the coordinated attack on Ukraine’s power grid that disabled 30 substations and left approximately 230,000 residents without electricity for six hours, underscore the urgent need for improved cybersecurity measures. Despite numerous efforts to develop cybersecurity standards and defense mechanisms for power grids, significant limitations remain, including: • Limited Comprehensive Cybersecurity Solutions: Many approaches address isolated aspects of cybersecurity, lacking a holistic approach to mitigate all identified vulnerabilities. • Challenges in Localization and Isolation of Attacks: Current methods often emphasize intrusion detection without effective mechanisms to localize and isolate attacks within substations. The previous work [2] by the same authors focused on the localization of malicious hosts. • Inadequate Real-Time Response Capabilities: Existing solutions generally lack the dynamic reconfiguration abilities to respond effectively to real-time cyber threats. Software-defined networking (SDN) is an emerging paradigm known for its dynamic, controllable, and flexible nature, suitable for high-bandwidth and dynamic applications. It separates the control and forwarding planes, thus enhancing network scalability and flexibility. However, this separation also increases the attack surface, necessitating robust security measures. Centralized SDN management is susceptible to various cyber-attacks, such as fault injection and distributed denial-of-service (DDoS) attacks. In addition, some current commercial SDN switches lack the functionality for remote port reconfiguration. This limitation hampers the ability to isolate compromised network segments effectively. Other commercial SDN switches are prohibitively expensive, hindering their widespread adoption in substation cybersecurity applications. As a result, malicious packets can continue to spread during cyber-attacks, potentially disrupting substation operations. Redundant protective intelligent electronic devices (PIEDs) are vital for continuous operation in digital substations, as attackers can compromise PIED functions by injecting malicious sampled values (SV) or generic object-oriented system events (GOOSE) packets. While previous research has focused on creating backup IEDs [3], these efforts have not fully addressed the cybersecurity. To address these challenges, this paper introduces an innovative smart cyber switching (SCS) framework that incorporates concurrent intelligent electronic device (CIED) integration, representing the first implementation of these methods in tandem. This approach leverages the strengths of both SCS and CIED technologies, providing an advanced, integrated solution for real-time, resilient cyber-physical interactions within digital substations. The SCS framework comprises: • Adaptive Port Controller (APC): A module that dynamically manages OpenFlow table rules and policies to reconfigure the network in real-time. • Network-Based Intrusion Detection System (IDS): An advanced IDS [4] designed to detect severe cyber-attacks, triggering the SCS to isolate compromised devices and invoking CIED to take over essential protection functions. The primary contributions of this paper include: 1. Development and implementation of the SCS framework utilizing APC for real-time, dynamic network reconfiguration to isolate cyber threats. 2. Introduction of CIED that replicates the protection functions of compromised physical IEDs, ensuring continuous protection and control. 3. Demonstrations of the framework’s effectiveness in mitigating SV and GOOSE cyber-attacks, ensuring resilient substation operations. This integrated approach offers a comprehensive solution to the cybersecurity challenges faced by modern substations, enhancing resilience and reliability. The remainder of the paper is structured as follows: Section II provides the information of the existing hardware-in-the-loop (HIL) testbed. Section III elaborates on the proposed SCS framework and its components. Section IV focuses on the application of the proactive substation security algorithm (PSSA) for attack mitigation and isolation using SCS. Section V presents simulation and validation results and describes the transition of protection functions to CIED. Section VI concludes with recommendations for future work."
https://arxiv.org/html/2411.07308v1,X-DFS: Explainable Artificial Intelligence Guided Design-for-Security Solution Space Exploration,"Design and manufacturing of integrated circuits predominantly use a globally distributed semiconductor supply chain involving diverse entities. The modern semiconductor supply chain has been designed to boost production efficiency, but is filled with major security concerns such as malicious modifications (hardware Trojans), reverse engineering (RE), and cloning. While being deployed, digital systems are also subject to a plethora of threats such as power, timing, and electromagnetic (EM) side channel attacks. Many Design-for-Security (DFS) solutions have been proposed to deal with these vulnerabilities, and such solutions (DFS) relays on strategic modifications (e.g., logic locking, side channel resilient masking, and dummy logic insertion) of the digital designs for ensuring a higher level of security. However, most of these DFS strategies lack robust formalism, are often not human-understandable, and require an extensive amount of human expert effort during their development/use. All of these factors make it difficult to keep up with the ever growing number of microelectronic vulnerabilities. In this work, we propose X-DFS, an explainable Artificial Intelligence (AI) guided DFS solution-space exploration approach that can dramatically cut down the mitigation strategy development/use time while enriching our understanding of the vulnerability by providing human-understandable decision rationale. We implement X-DFS and comprehensively evaluate it for reverse engineering threats (SAIL, SWEEP, and OMLA) and formalize a generalized mechanism for applying X-DFS to defend against other threats such as hardware Trojans, fault attacks, and side channel attacks for seamless future extensions.","A horizontal and distributed supply chain is at the heart of the booming semiconductor industry (see Fig. 1). Creation of digital hardware such as integrated circuits (IC) typically involves the development of sub-designs (intellectual properties - IP) by small entities, the integration of 3rd party sub-designs with the in-house components at the main design house, IC layout creation, fabrication at a foundry, testing at a testing facility, and assembly by the original electronic manufacturer (OEM). This model reduces the time to market for digital systems, allows for specialization, and enables small businesses. All these steps are typically carried out by different entities in different geographical locations. Such a flow can lead to a series of problems, such as: (1) design or sub-design theft, (2) hardware cloning, and (3) malicious hardware modification/tampering. Microelectronic ICs and devices in the field also face diverse threats, such as power side channel attacks and reverse engineering. A variety of Design-for-Security (DFS) strategies such as IC metering [1], [2], watermarking [3], camouflaging [4], [5], split manufacturing [6], [7], logic locking [8, 9], gate parameter optimization [10], variable delay module insertion [11], and dummy logic insertion [12] have been developed to guarantee trust in the supply chain [13] and boost post-deployment IC/device security. However, security solutions often become obsolete with the emergence of novel attacks, while developing appropriate countermeasures requires extensive research effort, time, and expert resources. To expedite the solution search process (against novel vulnerabilities) and to create a human-understandable knowledge base of the said vulnerability, we propose an automated framework, X-DFS (EXplainable - Design For Security). X-DFS uses a heuristics-based search process to determine a large set of DFS candidate instances that might contribute towards the defense against a given vulnerability. These DFS candidates are then used to train an explainable AI model that is capable of: (1) Emitting DFS rules that can be used to efficiently mitigate the vulnerability; (2) Automatically apply these rules towards securing the design. Hence X-DFS not only secures the design, it can also help researchers obtain a deeper understanding of the vulnerability. The proposed framework is highly generalized in nature and can be applied to a wide range of vulnerabilities (with minor tweaks), while most state-of-the-art DFS techniques are typically hand-crafted for mitigating a specific vulnerability or a small set of vulnerabilities. X-DFS can modify a design towards mitigating a given vulnerability and at the same time can generate human-understandable design transformation rules. Such capabilities do not exist in current state-of-the-art DFS techniques. X-DFS is highly flexible (parameterized) and extremely efficient in terms of computation cost, while most state-of-the-art DFS techniques are static in nature (not highly configurable) and fail to work for larger designs (inefficient). Figure 1: Semiconductor supply chain security threats. Figure 2: Impacts of digital IC/design threats. Figure 3: Logic locking: (a) original netlist, (b) obfuscated netlist, and (c) synthesized netlist (structurally changed). We implement X-DFS as a highly parameterized robust framework and use it to perform a comprehensive effectiveness analysis for large-scale designs. We evaluate the X-DFS framework by testing it in the logic locking domain where X-DFS is used to automatically search for mitigation strategies (human-understandable) against three powerful logic locking attacks (SAIL [14], OMLA [15]), SWEEP [16]. X-DFS was able to learn how to defend against these attack models and at the same time extracted human-understandable rules that can be used by other logic locking frameworks (such as LeGO [17]) or human experts to carry out the locking process. In particular, we make the following research contributions: 1. Formalize a general framework and the core mechanisms for automatic exploration of the design-for-security search space for countering novel attack vector(s). 2. Design a set of algorithms (for reverse engineering attacks) that leverages this knowledge regarding an attack vector(s) to build an X-DFS model that can modify a given target design to be resilient against the attacks. 3. Define a methodology to extract and understand the defense rules (human understandable) that are learned by the X-DFS models. 4. Implement the proposed algorithms as a highly parameterized and scalable tool. 5. Qualitatively and quantitatively verify the efficacy of the X-DFS framework/tool against different reverse engineering threats (SAIL, SWEEP, and OMLA)."
https://arxiv.org/html/2411.07972v1,A Zero-Knowledge PCP Theorem,"We show that for every polynomial q^{*} there exist polynomial-size, constant-query, non-adaptive PCPs for \mathsf{NP} which are perfect zero knowledge against (adaptive) adversaries making at most q^{*} queries to the proof. In addition, we construct exponential-size constant-query PCPs for \mathsf{NEXP} with perfect zero knowledge against any polynomial-time adversary. This improves upon both a recent construction of perfect zero-knowledge PCPs for \#\mathsf{P} (STOC 2024) and the seminal work of Kilian, Petrank and Tardos (STOC 1997).","The PCP theorem [AroraS98, ALMSS92] states that for any language in \mathsf{NP}, there exists a polynomial-size proof that can be checked probabilistically by reading only a constant number of bits from the proof; or, succinctly, \mathsf{NP}\subseteq\mathsf{PCP}[\log n,1]\;, where \mathsf{PCP}[r,q] is the class of all languages that admit a PCP verifier that uses O(r) random bits and reads O(q) bits of the proof (note that logarithmic randomness complexity implies polynomial proof length). While PCPs originated from the study of zero-knowledge proofs [GoldwasserMR89], there seems to be an intrinsic tension between the two notions: PCPs achieve locality by encoding NP witnesses in a manner that spreads global information throughout the proof, whereas zero-knowledge proofs aim to hide all information except for the validity of the statement. Moreover, PCPs are fundamentally non-interactive objects, whereas interaction is often crucial for zero knowledge. Indeed, one must take care in even defining zero knowledge PCPs (ZK-PCPs), as it is impossible to achieve non-trivial zero knowledge against a malicious verifier that reads the entire proof. In their seminal work on ZK-PCPs, Kilian, Petrank and Tardos [KilianPT97] identify two regimes of interest: (a) Polynomial-size PCPs that are zero-knowledge against a verifier that makes at most a fixed polynomial number of queries q^{*} to the PCP. In this regime, they construct ZK-PCPs for NP with polylogarithmic query complexity. We refer to q^{*} as the “query bound”. (b) Exponential-size PCPs that are zero-knowledge against any polynomial-time verifier. In this regime, they construct ZK-PCPs for NEXP with polynomial query complexity. However, these constructions fall short of a “zero-knowledge PCP theorem” in a fundamental way: the query complexity is polylogarithmic, as opposed to O(1), which is a characteristic property of the PCP theorem. A major obstacle to achieving O(1) query complexity via the [KilianPT97] approach is that the technique used to obtain zero knowledge leads to an inherently adaptive honest verifier (i.e., which makes multiple rounds of queries to the proof). Known query-reduction methods apply only to non-adaptive PCPs. Aside from its theoretical interest, non-adaptivity is crucial for some applications [IshaiWY16]. Finally, these constructions only achieve statistical zero knowledge (SZK-PCP) and not perfect zero knowledge (PZK-PCP). Building on techniques developed in [BenSassonCFGRS17, ChiesaFGS18, ChenCGOS23], a recent work [GurOS2024] constructed exponential-size PZK-PCPs with polynomially many non-adaptive queries for \#\mathsf{P}. Our first result shows that such PZK-PCPs exist for \mathsf{NEXP}, with constant query complexity. Theorem 1. There exist PCPs for \mathsf{NEXP} of exponential length with a non-adaptive verifier that reads O(1) bits of the proof, which are perfect zero-knowledge against any efficient adversary. Our second result “scales down” the above to obtain polynomial-size non-adaptive PZK-PCPs for \mathsf{NP} with constant query complexity.111For our definition of the class \mathsf{PZK}\text{-}\mathsf{PCP}, see Definition 2.12. Theorem 2 (“Zero-knowledge PCP theorem”). For any q^{*}\leq 2^{\mathrm{poly}(n)}, there exist PCPs for \mathsf{NP} of length \mathrm{poly}(q^{*},n) with a non-adaptive verifier that reads O(1) bits of the proof, which are perfect zero-knowledge against any adversary reading at most q^{*} bits of the proof; i.e., \mathsf{NP}\subseteq\mathsf{PZK}\text{-}\mathsf{PCP}[\log n,1]\;. 1.1 Techniques PZK-PCPs for nondeterministic computation. This paper builds on the prior work of [GurOS2024], which proves a weaker version of 1 that only captures (decision) \#\mathsf{P}, by constructing non-adaptive PZK-PCPs for the sumcheck problem. Readers familiar with the PCP literature may wonder why this construction does not lead immediately to a ZK-PCP for NEXP; indeed, the first construction of a PCP for NEXP is essentially a reduction to sumcheck [BabaiFLS91]. However, as we discuss below, even with a PZK-PCP for the sumcheck problem, the BFLS construction is not zero knowledge. We start by briefly reviewing the BFLS construction, which is a PCP for the \mathsf{NEXP}-complete problem \mathsf{Oracle}\textsf{-}\mathsf{3SAT}, defined as follows. Definition 1 (Oracle 3-SAT). Let B\colon\{0,1\}^{r+3s+3}\to\{0,1\} be a 3-CNF. We say that B is implicitly satisfiable if there exists A\in\{0,1\}^{s}\to\{0,1\} such that for all z\in\{0,1\}^{r},b_{1},b_{2},b_{3}\in\{0,1\}^{s}, B(z,b_{1},b_{2},b_{3},A(b_{1}),A(b_{2}),A(b_{3}))=1. Let \mathsf{Oracle}\textsf{-}\mathsf{3SAT} be the language of implicitly satisfiable 3-CNFs. The BFLS PCP consists of two parts. First, the multilinear extension of the witness A; i.e., a multilinear polynomial \hat{A} over some finite field \mathbb{F} such that \hat{A}(x)=A(x) for all x\in\{0,1\}^{s}. Second, a sumcheck PCP222Strictly speaking, a PCP of proximity; we will ignore this distinction for this overview. for the following claim333To mitigate some technical issues with soundness, the actual construction uses a slightly different summand polynomial.: \sum_{\begin{subarray}{c}z\in\{0,1\}^{r}\\ b_{1},b_{2},b_{3}\in\{0,1\}^{s}\end{subarray}}\hat{B}(z,b_{1},b_{2},b_{3},\hat% {A}(b_{1}),\hat{A}(b_{2}),\hat{A}(b_{3}))=2^{r+3s}~{}, (1) where \hat{B} is an arithmetisation of the circuit B. A natural first step is to use the zero-knowledge sumcheck PCP of [GurOS2024] in place of the standard sumcheck PCP, which hides hard-to-compute information about partial sums. However, this does not yet yield a ZK-PCP, because the first part of the construction contains an encoding of the witness A, which violates the zero-knowledge condition. To hide A, we would like to use the sumcheck commitment scheme, introduced by [ChiesaFGS22] (see also [ChiesaFS17]) for their construction of an interactive PCP (IPCP) for \mathsf{NEXP}. A sumcheck commitment to the polynomial \hat{A}(\vec{X}) is a random polynomial C(\vec{X},\vec{Y}) of individual degree d^{\prime}\geq 2 in each Y-variable such that \sum_{c\in\{0,1\}^{k}}C(\alpha,c)=\hat{A}(\alpha) for all \alpha\in\mathbb{F}^{s}. It was shown in [ChiesaFGS22] that this commitment perfectly hides \hat{A} against all adversaries that make fewer than 2^{k} queries to C. The construction of [BabaiFLS91] uses the sumcheck protocol to reduce checking (1) to three random queries to \hat{A}. Building on this idea, the ZK-IPCP construction of [ChiesaFGS22] uses the interactive sumcheck protocol to open the sumcheck commitment to \hat{A} at those points. Finally, to ensure that those three evaluations do not leak information about the witness, \hat{A} is chosen to be a random multiquartic—rather than the unique multilinear—extension of A. However, the strategy above strongly relies on the interactivity of the ZK-IPCP. Indeed, observe that if we were to “unroll” this interaction into a PCP, we would simply write down all of \hat{A}! We must therefore establish (1) without opening the sumcheck commitment. That is, we would like to directly check: \sum_{\begin{subarray}{c}z\in\{0,1\}^{r}\\ b_{1},b_{2},b_{3}\in\{0,1\}^{s}\end{subarray}}\hat{B}\left(z,b_{1},b_{2},b_{3}% ,\sum_{c\in\{0,1\}^{k}}C(b_{1},c),\sum_{c\in\{0,1\}^{k}}C(b_{2},c),\sum_{c\in% \{0,1\}^{k}}C(b_{3},c)\right)=2^{r+3s}~{}, To do this, we first “pull out” the three inner summations. There are various ways this can be achieved; we follow a linearisation approach. First observe that (assuming \hat{A} takes boolean values on \{0,1\}^{s}), the LHS of (1) is equal to \sum_{a_{1},a_{2},a_{3}\in\{0,1\}}\sum_{\begin{subarray}{c}z\in\{0,1\}^{r}\\ b_{1},b_{2},b_{3}\in\{0,1\}^{s}\end{subarray}}\hat{B}(z,b_{1},b_{2},b_{3},a_{1% },a_{2},a_{3})\cdot\prod_{i=1}^{3}(\hat{A}(b_{i})-(1-a_{i}))~{}, since the product expression “zeroes out” any term of the sum for which some a_{i}\neq\hat{A}(b_{i}). Next, observe that for any b_{1},b_{2},b_{3},a_{1},a_{2},a_{3}, provided that \mathbb{F} has characteristic different from 2, \prod_{i=1}^{3}(\hat{A}(b_{i})-(1-a_{i}))=\prod_{i=1}^{3}\sum_{c\in\{0,1\}^{k}% }\left(C(b_{i},c)-\frac{1-a_{i}}{2^{k}}\right)=\sum_{c_{1},c_{2},c_{3}\in\{0,1% \}^{k}}\prod_{i=1}^{3}\left(C(b_{i},c_{i})-\frac{1-a_{i}}{2^{k}}\right)~{}. Taken together, we see that checking (1) is equivalent to checking \sum_{c_{1},c_{2},c_{3}\in\{0,1\}^{k}}\sum_{a_{1},a_{2},a_{3}\in\{0,1\}}\sum_{% \begin{subarray}{c}z\in\{0,1\}^{r}\\ b_{1},b_{2},b_{3}\in\{0,1\}^{s}\end{subarray}}\hat{B}(z,b_{1},b_{2},b_{3},a_{1% },a_{2},a_{3})\prod_{i=1}^{3}\left(C(b_{i},c_{i})-\frac{1-a_{i}}{2^{k}}\right)% =2^{r+3s}~{}, which can be proven in zero knowledge using the [GurOS2024] ZK-PCPP for sumcheck. In particular, the PCPP simulator requires only polynomially many evaluations of the summand to simulate polynomially many queries to the proof. By setting k=\omega(\log n) we can simulate those evaluations by lazily simulating C as a uniformly random polynomial (via an algorithm of [BenSassonCFGRS17]), and then evaluating the summand directly. The query complexity of the verifier is \mathrm{poly}(n). Our result for \mathsf{NP} is obtained in a similar way, scaling the parameters appropriately. In particular, for a given adversary query bound q^{*}, we can set k=O(\log q^{*}). The query complexity of the verifier is then \mathrm{poly}(\log n,\log q^{*}). We also point out that the honest prover in our construction for \mathsf{NP} is efficient, given a valid witness as input. Proof composition and zero knowledge. In order to obtain constant query ZK-PCPs for \mathsf{NP}, we would like to apply the proof composition paradigm [BenSassonGHSV06] to our ZK-PCPs. This involves composing a robust outer PCP with an inner PCP of proximity to obtain a PCP which inherits the randomness complexity of the former and query complexity of the latter. To do this, we first need to strengthen our ZK-PCPs to satisfy robust soundness (i.e., the local view of the verifier must be far from an accepting view, with high probability). Then we show that proof composition preserves the zero knowledge of the outer PCP. To the best of our knowledge, this is the first composition theorem for ZK-PCPs. Our first step is to obtain robust ZK-PCPs for \mathsf{NP} with polylogarithmic query complexity. While the PZK-PCP of [GurOS2024] (upon which our PZK-PCP builds) is an algebraic construction, it does not have constant robust soundness as written. We present a modification of the [GurOS2024] PCP for sumcheck, following the “query bundling” approach of [BenSassonGHSV06], that has constant robust soundness. The key challenge is to show that this modification preserves zero knowledge. In fact, we will show that a much more general class of “local” transformations preserve zero knowledge. This class includes not only query bundling, but also the subsequent steps of alphabet reduction and proof composition. To capture this class formally, we define a new notion, locally computable proofs. Definition 2 (Locally computable algorithms (informal; see Definition 3.1)). Let \mathcal{A} and \mathcal{A}_{0} be randomized algorithms. We say that \mathcal{A} is \ell-locally computable from \mathcal{A}_{0} if there exists an efficient, deterministic oracle algorithm f making at most \ell queries to its oracle such that, for every input x, the following two distributions are identically distributed: \mathcal{A}(x),\quad(f^{\pi_{0}}~{}|~{}\pi_{0}\leftarrow\mathcal{A}_{0}(x)). We show that if the PCP prover algorithm \mathcal{P} is \ell-locally computable from a zero-knowledge PCP prover \mathcal{P}_{0}, then provided \ell is asymptotically smaller than the query bound on the ZK-PCP, \mathcal{P} inherits the zero knowledge guarantee of \mathcal{P}_{0}. Intuitively, this is true because if a proof \pi is locally computable from a proof \pi_{0}, and \pi_{0} is zero knowledge, then we can apply f to the simulator for \pi_{0} to obtain a simulator for \pi. This notion is surprisingly versatile, and allows us to prove zero knowledge in an array of distinct settings: • Robustification of [GurOS2024]. We show that our modified [GurOS2024] construction is locally computable from the original construction, and thus inherits zero knowledge. • Alphabet reduction. Recall that alphabet reduction allows us to transform a robust PCP over a large alphabet into a boolean PCP, while maintaining robustness. It is performed by encoding each symbol of the PCP with a good error correcting code. More formally, if \pi is a distribution over robust PCP proofs over the alphabet \{0,1\}^{a} for some a\in\mathbb{N}, and \mathsf{ECC}\colon\{0,1\}^{a}\to\{0,1\}^{b} is a systematic binary error-correcting code of constant relative distance and rate, then we can obtain a robust boolean PCP by defining a new proof \tau(\alpha)\ {:=}\ \mathsf{ECC}(\pi(\alpha)) for every proof index \alpha, and writing \tau over the alphabet \{0,1\}. Then \tau is 1-locally computable from \pi by the following function: f^{\pi}(\alpha,i)=\mathsf{ECC}(\pi(\alpha))_{i}, where i\in[b]. We note that prior work on alphabet reduction for ZKPCPs [HazayVW22] required a much more complex construction because their ZKPCP achieves only a quadratic gap between the honest verifier’s query complexity and the query bound. • Proof composition. Recall that composition of an outer PCP system (\mathcal{P}_{\text{out}},\mathcal{V}_{\text{out}}) for \mathcal{L} with an inner PCP of proximity (\mathcal{P}_{\text{in}},\mathcal{V}_{\text{in}}) for circuit evaluation proceeds as follows. Let \pi_{\text{out}}\leftarrow\mathcal{P}_{\text{out}}. For every choice r\in\{0,1\}^{r_{\text{out}}} of \mathcal{V}_{\text{out}}’s randomness, the composed prover computes \mathcal{V}_{\text{out}}’s query set Q(r) and an “inner proof” \pi_{r}\ {:=}\ \mathcal{P}_{\text{in}}(\mathcal{V}_{\text{out}},\pi_{\text{out% }}|_{Q(r)}), which attests to the fact that if \mathcal{V}_{\text{out}} performed its verification of \pi_{\text{out}} using randomness r, then it would have accepted. Each inner proof is a function of at most \ell_{\text{out}} many locations of \pi_{\text{out}}, where \ell_{\text{out}} is the query complexity of \mathcal{V}_{\text{out}}. The composed proof is given by (\pi_{\text{out}},(\pi_{r})_{r\in\{0,1\}^{r_{\text{out}}}}). Hence the composed proof is \ell_{\text{out}}-locally computable from \pi_{\text{out}}, by the following function: f^{\pi_{\text{out}}}(\mathcal{O},i)=\begin{cases}\pi_{\text{out}}(i)~{}&\text{% if}~{}\mathcal{O}=\pi_{\text{out}}\\ \mathcal{P}_{\text{in}}(\mathcal{V}_{\text{out}},\pi_{\text{out}}|_{Q(r)})_{i}% ~{}&\text{if}~{}\mathcal{O}=\pi_{r},~{}\text{for some}~{}r\in\{0,1\}^{r_{\text% {out}}}\end{cases}. Note that the composed PCP is locally computable from the outer PCP, so only the outer PCP needs to be zero knowledge to ensure the composed PCP is zero knowledge. Therefore we can employ the existing (non-ZK) PCP of proximity for circuit evaluation of [BenSassonGHSV06] as the inner PCPP. 1.2 Open problems This work shows that for any polynomial q^{*}, any language in NP has a polynomial-sized proof that can be probabilistically checked by probing only O(1) bits, but where any set of q^{*} bits carries no information about the witness. This can be viewed a zero-knowledge PCP theorem that matches parameters of the original PCP theorem [AroraS98, ALMSS92]. Since then, stronger versions of the PCP theorem have been shown. It is tempting to ask whether zero-knowledge PCPs can match the strongest constructions of standard PCPs. In particular, one of the most immediate open questions is whether it is possible to obtain ZK-PCPs with nearly-linear length. Optimising the proof length of PCPs received much attention for decades after the first proof of the PCP theorem, where the current state of the art achieves quasilinear proof length [BS08, Din07]. We ask whether the same can be obtained for zero-knowledge PCPs. Open Problem 1 (Short ZK-PCPs). Do there exist O(1)-query ZK-PCPs for \mathsf{NP} with proof length \tilde{O}(n)? We remark that our algebraic zero-knowledge techniques are based on Reed-Muller arithmetisation and the sumcheck protocol, whereas (non-ZK) constructions of quasilinear length PCPs are based on Reed-Solomon arithmetisation and combinatorial gap amplification, hence new ideas are necessary for such strengthening of our theorem. Even more ambitiously, one could ask whether it is possible to transform any construction of a (non-ZK) PCP to a ZK-PCP while preserving its parameters. Open Problem 2 (PCP to ZK-PCP transformation). Is there a black-box transformation that imbues a PCP construction with zero knowledge? We note that prior to [GurOS2024], all works on ZK-PCPs (see below) followed this approach, but those transformations either introduce adaptivity or only achieve weak ZK guarantees, and none preserve query complexity. Similiar transformations are known, e.g., for multi-prover interactive proofs [BenOrGKW88] and their quantum analogues [GriloSY19, MastelS24] whereas in other models, such as in zero-knowledge streaming interactive proofs [cormode2023streaming], we have a zero knowledge sumcheck protocol but no generic transformation is known. We remark that our techniques make whitebox use of the structure of the [BabaiFLS91] PCP, and rely strongly on the [GurOS2024] ZK-PCP for sumcheck; hence, obtaining a generic transformation would require new ideas. The quantum PCP conjecture. Finally, we highlight a connection between zero-knowledge PCPs and one of the most imporant open problems in quantum complexity theory: the quantum PCP (QPCP) conjecture. Most classical constructions of PCPs rely on an encoding of the \mathsf{NP} witness via a locally-testable and (relaxed) locally-decodable code [BenSassonGHSV06, gur2020relaxed]. Even though there is growing evidence that good quantum LTCs may exist [aharonov2015quantum, leverrier2022towards, anshu2023nlts, dinur2024expansion], quantum codes cannot be locally decodable due to the no-cloning theorem. This is one of the main barriers towards applying algebraic and coding-theoretic techniques to QPCPs. Quantum codes are fundamentally tied to zero knowledge. It is a well-known fact that the erasure of a subset of qubits of a codeword is correctable if and only if the reduced density matrix on that subset is independent of the encoded state. In other words, roughly speaking, a quantum code has good distance if and only if it satisfies a quantum analogue of the PZK property for PCPs. Thus PZK-PCPs are perhaps the closest classical analogue of QPCPs, and studying them may help to shed light on the QPCP conjecture. 1.3 Related work The first zero-knowledge PCPs appeared in the work of Kilian, Petrank and Tardos [KilianPT97]. Later works [IshaiMS12, IshaiSVW13, IshaiW14, IshaiMSX15] simplified this construction, and extended it to PCPs of proximity and the closely related notion of zero-knowledge locally testable codes (LTCs). These constructions rely on an adaptive honest verifier, and hence it is unclear how to use proof composition to improve their parameters. [IshaiMS12, IshaiMSX15] showed that PCPs which are zero knowledge against any efficient adversary and where the proof oracle is described by a polynomial-sized circuit exist only for languages in \mathsf{SZK}. Another line of work, motivated by cryptographic applications, focuses on obtaining SZK-PCPs for NP with a non-adaptive honest verifier from leakage resilience. These results come with caveats, achieving either a weaker notion of zero knowledge known as witness indistinguishability [IshaiWY16], or simulation against adversaries making only quadratically many more queries than the honest verifier [HazayVW22]. See [Weiss22] for a survey of this line of work. In related models that allow for interaction, zero-knowledge proofs are easier to construct. We know that \mathsf{PZK\text{-}MIP}=\mathsf{MIP} (=\mathsf{NEXP}) [BenOrGKW88], where \mathsf{MIP} is the class of languages with a multi-prover interactive proofs. The quantum analogue of this result, \mathsf{PZK\text{-}MIP}^{*}=\mathsf{MIP}^{*} (=\mathsf{RE}), is also known to hold [ChiesaFGS22, GriloSY19, MastelS24]. The constructions in this work draw inspiration from a similar result for interactive PCPs (IPCPs) [KalaiR08], an interactive generalisation of PCPs (and special case of IOPs): \mathsf{PZK\text{-}IPCP}=\mathsf{IPCP}=\mathsf{NEXP} [BenSassonCFGRS17, ChiesaFS17]."
https://arxiv.org/html/2411.07806v1,Federated Low-Rank Adaptation with Differential Privacy over Wireless Networks,"Fine-tuning large pre-trained foundation models (FMs) on distributed edge devices presents considerable computational and privacy challenges. Federated fine-tuning (FedFT) mitigates some privacy issues by facilitating collaborative model training without the need to share raw data. To lessen the computational burden on resource-limited devices, combining low-rank adaptation (LoRA) with federated learning enables parameter-efficient fine-tuning. Additionally, the split FedFT architecture partitions an FM between edge devices and a central server, reducing the necessity for complete model deployment on individual devices. However, the risk of privacy eavesdropping attacks in FedFT remains a concern, particularly in sensitive areas such as healthcare and finance. In this paper, we propose a split FedFT framework with differential privacy (DP) over wireless networks, where the inherent wireless channel noise in the uplink transmission is utilized to achieve DP guarantees without adding an extra artificial noise. We shall investigate the impact of the wireless noise on convergence performance of the proposed framework. We will also show that by updating only one of the low-rank matrices in the split FedFT with DP, the proposed method can mitigate the noise amplification effect. Simulation results will demonstrate that the proposed framework achieves higher accuracy under strict privacy budgets compared to baseline methods.","The rapid advancement of artificial intelligence (AI) has enabled the development of powerful pre-trained foundation models (FMs), such as large language models (LLMs) and large vision models (LVMs), which demonstrate remarkable capabilities across diverse domains [1, 2, 3]. Fine-tuning these models for specific tasks often requires access to domain-specific data distributed across numerous edge devices in real-world applications. Federated Learning (FL) has emerged as a promising paradigm for decentralized model training, enabling multiple edge devices to collaboratively learn a shared model without exchanging raw data [4, 5, 6]. In the context of fine-tuning FMs, federated fine-tuning (FedFT) over wireless networks allows devices to adapt pre-trained models to their local data, leveraging collective knowledge [5]. However, fine-tuning full FMs on resource-constrained edge devices is often impractical due to the substantial computational and memory demands. To reduce these demands, parameter-efficient fine-tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), reparameterize weight updates using low-rank matrices to reduce the number of trainable parameters [7]. However, even with LoRA, deploying full models on edge devices may still exceed the capacity of edge devices. A split FedFT architecture, proposed in [8], addresses this issue by distributing components of the FM across edge devices and a central server. In this architecture, embedding and task-specific modules are placed on devices, while the computationally intensive encoder is deployed on the server. However, privacy concerns due to the potential risks associated with an untrusted server have not been considered. Specifically, gradient inversion attacks can exploit shared gradients transmitted during training to reconstruct sensitive data [9], posing significant privacy threats to user information. Differential Privacy (DP) has been proposed as a solution to protect against such adversarial attacks by adding artificial noise to shared gradients [10, 11]. In this paper, we incorporate DP into the split FedFT framework to enhance data protection. We leverage inherent channel noise in wireless networks as a natural DP mechanism, reducing the need for artificial noise at the client side [12]. We futher establish the relationship between privacy loss and wireless fading channels and introduce a privacy-aware power control policy to ensure robust privacy protection. Integrating DP into the split FedFT, however, presents additional challenges. The cascaded architecture of the FM can amplify noise as gradients propagate through multiple layers, especially within low-rank matrix updates, destabilizing model training and degrading performance [13]. To address this issue, we propose a modified LoRA-based FedFT architecture that reduces noise amplification without compromising privacy protection. By updating only one low-rank matrix and fixing another matrix as a scaled orthonormal matrix, we reduce noise amplification by eliminating higher-order noise terms during backpropagation and stabilize the covariance matrix of noise in weight updates. Experimental results will demonstrate the superior performance in model accuracy under strict privacy budgets compared to baseline methods. The proposed approach is promising for the practical deployment of large-scale AI models on resource-constrained edge devices with robust privacy guarantees."
https://arxiv.org/html/2411.07518v1,LLM App Squatting and Cloning,"Impersonation tactics, such as app squatting and app cloning, have posed longstanding challenges in mobile app stores, where malicious actors exploit the names and reputations of popular apps to deceive users. With the rapid growth of Large Language Model (LLM) stores like GPT Store and FlowGPT, these issues have similarly surfaced, threatening the integrity of the LLM app ecosystem. In this study, we present the first large-scale analysis of LLM app squatting and cloning using our custom-built tool, LLMappCrazy. LLMappCrazy covers 14 squatting generation techniques and integrates Levenshtein distance and BERT-based semantic analysis to detect cloning by analyzing app functional similarities. Using this tool, we generated variations of the top 1000 app names and found over 5,000 squatting apps in the dataset. Additionally, we observed 3,509 squatting apps and 9,575 cloning cases across six major platforms. After sampling, we find that 18.7% of the squatting apps and 4.9% of the cloning apps exhibited malicious behavior, including phishing, malware distribution, fake content dissemination, and aggressive ad injection.","Mobile app squatting [19], where attackers publish apps with identifiers (e.g., app or package names) that mimic popular apps, such as through typosquatting (e.g., changing “Facebook” to “Fecebook”), is a growing threat in the mobile ecosystem. Hu et al.[19] identified over 10,553 squatting apps targeting the top 500 apps on Google Play, with more than 51% classified as malicious and some reaching millions of downloads. These counterfeit apps pose serious risks, including data theft and malware infections. Despite mitigation efforts by platforms, the sheer number of apps and sophisticated squatting tactics make detection and prevention difficult. Inspired by the extensive research on mobile app squatting, we have turned our attention to similar threats within emerging Large Language Model (LLM) app stores [45]. With the rise of LLMs, such as ChatGPT [27], Gemini [14], and Claude [10], there has been a proliferation of applications that leverage these models in diverse domains, including chatbots, content generation tools, and virtual assistants [6, 9, 11, 13, 28, 29]. LLM-powered applications have gained immense popularity due to their ability to perform complex tasks, leading to the creation of entire app ecosystems around them. However, as these LLM app stores continue to expand rapidly, we observe that they are becoming fertile ground for LLM app squatting attacks similar to those in traditional mobile app markets, as shown in Figure 1. In this context, squatting primarily occurs at the app identifier level, where attackers create apps with names that closely mimic legitimate ones to deceive users. For example, squatting could manifest as subtle name changes or the addition of enticing words, such as “Canva Pro”, tricking users into believing they are using an official or enhanced version of a popular app. Moreover, LLM app stores have significantly lowered the barrier to entry for developers. This democratization of development allows individuals from various backgrounds, even those with limited programming experience, to create and publish apps. While this inclusivity fosters innovation, it also makes it easier for attackers to clone the entire LLM app not only the app’s name but also its functionality and behavior. We refer to this more insidious form of attack as LLM app cloning, where the cloned app mirrors the legitimate one in nearly every aspect, making it even harder for users to discern the difference. To comprehensively investigate squatting and cloning in LLM app stores, we focus on six prominent LLM app stores (i.e., GPT Store [28], FlowGPT [13], Poe [29], Coze [11], Cici [9], and Character.AI [6]) that have gained significant traction due to the widespread adoption of LLM-powered applications. In our study, we develop a tool, LLMappCrazy, designed to automatically detect squatting and cloning instances within these ecosystems. Using LLMappCrazy, we systematically examine app identifier variations and functional cloning across GPT Store, identifying potential 5,834 name squatting apps and 6094 name cloning apps. And we also detect other features of apps in six LLM app stores. Our results reveal the scope of the problem: we found 3,509 squatting apps, and 9,575 cloned apps, confirming that this phenomenon is not isolated to mobile app markets but is rapidly spreading into the LLM domain. The findings indicate that 18.7% of the squatting apps and 4.9% of cloning apps exhibited malicious behavior, and some of them had amassed significant user downloads, further exacerbating the security risks faced by users of LLM-based applications. Contributions. We make the following main contributions: 1. To the best of our knowledge, this is the first detailed investigation into squatting and cloning attacks within LLM app stores. 2. We develop LLMappCrazy, a tool that detects squatting and cloning apps using 14 squatting-generation techniques and advanced semantic analysis. 3. Using LLMappCrazy, we find 5,834 name squatting apps and 6094 name cloning apps; We conduct a large-scale empirical study across six LLM app stores, identifying 3,509 squatting apps, and 9,575 cloning apps. 4. We find that 18.7% of the identified squatting apps and 4.9% of cloning apps exhibit malicious behavior, including phishing, malware, and ad injection. And we identify 227 apps that exhibit a high degree of similarity in various features to other apps in GPT Store. 5. We study the impact of LLM app squatting and cloning, discovering that these apps have reached up to 2.7 million conversations, posing significant risks to platform trust."
https://arxiv.org/html/2411.07473v1,Nearly-Linear Time Seeded Extractors with Short Seeds,"Seeded extractors are fundamental objects in pseudorandomness and cryptography, and a deep line of work has designed polynomial-time seeded extractors with nearly-optimal parameters. However, existing constructions of seeded extractors with short seed length and large output length run in time \Omega(n\log(1/\varepsilon)) and often slower, where n is the input source length and \varepsilon is the error of the extractor. Since cryptographic applications of extractors require \varepsilon to be small, the resulting runtime makes these extractors unusable in practice.Motivated by this, we explore constructions of strong seeded extractors with short seeds computable in nearly-linear time O(n\log^{c}n), for any error \varepsilon. We show that an appropriate combination of modern condensers and classical approaches for constructing seeded extractors for high min-entropy sources yields strong extractors for n-bit sources with any min-entropy k and any target error \varepsilon with seed length d=O(\log(n/\varepsilon)) and output length m=(1-\eta)k for an arbitrarily small constant \eta>0, running in nearly-linear time, after a reasonable one-time preprocessing step (finding a primitive element of \mathds{F}_{q} with q=\operatorname{poly}(n/\varepsilon) a power of 2) that is only required when k<2^{C\log^{*}\!n}\cdot\log^{2}(n/\varepsilon), for a constant C>0 and \log^{*}\! the iterated logarithm, and which can be implemented in time \operatorname{polylog}(n/\varepsilon) under mild conditions on q. As a second contribution, we give an instantiation of Trevisan’s extractor that can be evaluated in truly linear time in the RAM model, as long as the number of output bits is at most \frac{n}{\log(1/\varepsilon)\operatorname{polylog}(n)}. Previous fast implementations of Trevisan’s extractor ran in \widetilde{O}(n) time in this setting. In particular, these extractors directly yield privacy amplification protocols with the same time complexity and output length, and communication complexity equal to their seed length.","Seeded randomness extractors are central objects in the theory of pseudorandomness. A strong (k,\varepsilon)-seeded extractor is a deterministic function \mathsf{Ext}\colon\mathopen{}\mathclose{{}\left\{{0,1}}\right\}^{n}\times% \mathopen{}\mathclose{{}\left\{{0,1}}\right\}^{d}\to\mathopen{}\mathclose{{}% \left\{{0,1}}\right\}^{m} that receives as input an n-bit source of randomness X with k bits of min-entropy111A random variable X has k bits of min-entropy if \Pr[X=x]\leq 2^{-k} for all x. Min-entropy has been the most common measure for the quality of a weak source of randomness since the work of Chor and Goldreich [CG88]. and a d-bit independent and uniformly random seed Y, and outputs an m-bit string \mathsf{Ext}(X,Y) that is \varepsilon-close in statistical distance to the uniform distribution over \mathopen{}\mathclose{{}\left\{{0,1}}\right\}^{m}, where \varepsilon is an error term, even when the seed Y is revealed. Besides their most direct application to the generation of nearly-perfect randomness from imperfect physical sources of randomness (and their inaugural applications to derandomizing space-bounded computation [NZ96] and privacy amplification [BBCM95]), seeded extractors have also found many other surprising applications throughout computer science, particularly in cryptography. For most applications, it is important to minimize the seed length of the extractor. A standard application of the probabilistic method shows the existence of strong (k,\varepsilon)-seeded extractors with seed length d=\log(n-k)+2\log(1/\varepsilon)+O(1) and output length m=k-2\log(1/\varepsilon)-O(1), and we also know that these parameters are optimal up to the O(1) terms [RT00]. This motivated a deep line of research devising explicit constructions of seeded extractors with seed length as small as possible spanning more than a decade (e.g., [NZ96, SZ99, NT99, Tre01, TZS06, SU05]) and culminating in extractors with essentially optimal seed length [LRVW03, GUV09]. In particular, the beautiful work of Guruswami, Umans, and Vadhan [GUV09] gives explicit strong extractors with order-optimal seed length d=O(\log(n/\varepsilon)) and output length m=(1-\eta)k for any constant \delta>0, and follow-up work [DKSS13, TU12] further improved the entropy loss k+d-m. The extractors constructed in these works are explicit, in the sense that there is an algorithm that given x and y computes the corresponding output \mathsf{Ext}(x,y) in time polynomial in the input length. A closer look shows that the short-seed constructions presented in the literature all run in time \Omega(n\log(1/\varepsilon)), and often significantly slower. In cryptographic applications of extractors we want the error guarantee \varepsilon to be small, which means that implementations running in time \Omega(n\log(1/\varepsilon)) are often impractical. If we insist on nearly-linear runtime for arbitrary error \varepsilon, we can use strong seeded extractors based on universal hash functions that can be implemented in O(n\log n) time (e.g., see [HT16]), have essentially optimal output length, but have the severe drawback of requiring a very large seed length d=\Omega(m). These limitations have been noted in a series of works studying concrete implementations of seeded extractors, with practical applications in quantum cryptography in mind [MPS12, FWE+23, FYEC24]. For example, Foreman, Yeung, Edgington, and Curchod [FYEC24] implement a version of Trevisan’s extractor [Tre01, RRV02] with its standard instantiation of Reed–Solomon codes concatenated with the Hadmadard code, and emphasize its excessive running time as a major reason towards non-adoption.222The reason why these works focus on Trevisan’s extractor is that this is the best seeded extractor (in terms of asymptotic seed length) that is known to be secure against quantum adversaries [DPVR12]. Instead, they have to rely on extractors based on universal hash functions, which, as mentioned above, are fast but require very large seeds. This state of affairs motivates the following question, which is the main focus of this work: Can we construct strong (k,\varepsilon)-seeded extractors with seed length d=O(\log(n/\varepsilon)) and output length m=(1-\eta)k computable in nearly-linear time, for arbitrary error \varepsilon? Progress on this problem would immediately lead to faster implementations of many cryptographic protocols that use seeded extractors. 1.1 Our Contributions We make progress on the construction of nearly-linear time extractors. Seeded extractors with order-optimal seed length and large output length. We construct nearly-linear time strong seeded extractors with order-optimal seed length and large output length for any k and \varepsilon, with the caveat that they require a one-time preprocessing step whenever k=O(\log^{2}(n/\varepsilon)). This preprocessing step corresponds to finding primitive elements of finite fields \mathds{F}_{q} with q=\operatorname{poly}(n/\varepsilon), which, as we discuss below, is reasonable in practical applications. More precisely, we have the following result. Theorem 1. For any constant \eta>0 there exists a constant C>0 such that the following holds. For any positive integers n and k\leq n and any \varepsilon>0 satisfying k\geq C\log(n/\varepsilon) there exists a strong (k,\varepsilon)-seeded extractor \mathsf{Ext}\colon\{0,1\}^{n}\times\{0,1\}^{d}\to\{0,1\}^{m} with seed length d\leq C\log(n/\varepsilon) and output length m\geq(1-\eta)k. Furthermore, • if k\geq 2^{C\log^{*}\!n}\cdot\log^{2}(n/\varepsilon), then \mathsf{Ext} is computable in time \widetilde{O}(n), where \widetilde{O}(\cdot) hides polylogarithmic factors in its argument and \log^{*}\! denotes the iterated logarithm; • if k<2^{C\log^{*}\!n}\cdot\log^{2}(n/\varepsilon), then \mathsf{Ext} is computable in time \widetilde{O}(n) after a preprocessing step, corresponding to finding a primitive element of \mathds{F}_{q} with q=\operatorname{poly}(n/\varepsilon) a power of 2.333In full rigor, the preprocessing step corresponds to finding primitive elements of O(\log\log n) fields \mathds{F}_{q} with orders q\leq\operatorname{poly}(n/\varepsilon), each a power of 2. This O(\log\log n) term has negligible influence on the complexity of this preprocessing step. Note that we can find such a primitive element in time \operatorname{polylog}(n/\varepsilon) if q\leq\operatorname{poly}(n/\varepsilon) is a power of 2 and we know the factorization of q-1, but we do not know how to do that in time \widetilde{O}(\log q). More precisely, given the factorization of q-1 we can test whether a given \alpha\in\mathds{F}_{q} is primitive in time \operatorname{polylog}(q) by checking whether \alpha^{\frac{q-1}{p}}\neq 1 for all prime factors p of q-1. We can exploit this in various ways. If we are fine with using randomness in the one-time preprocessing stage, then we can sample an element of \mathds{F}_{q} uniformly at random, test whether it is primitive, and repeat if not. If we insist on a deterministic algorithm, then we can combine the testing procedure with algorithms of Shoup [Sho90] or Shparlinski [Shp92] which identify in time \operatorname{polylog}(q) a subset of size \operatorname{polylog}(q) in \mathds{F}_{q} that is guaranteed to contain a primitive element. For an alternative faster randomized algorithm, see [DD06]. 1 follows from combining modern condensers with short seeds (namely, the lossless condenser of Kalev and Ta-Shma [KT22] and the lossy Reed-Solomon-based condenser of Guruswami, Umans, and Vadhan [GUV09]) with a careful combination and instantiation of classical recursive approaches developed by Srinivasan and Zuckerman [SZ99] and in [GUV09]. It readily implies, among other things, an \widetilde{O}(n)-time privacy amplification protocol where only O(\log(n/\varepsilon)) bits need to be communicated over the one-way authenticated public channel and almost all the min-entropy can be extracted (after a reasonable one-time preprocessing step if the min-entropy bound k is very small). A new non-recursive construction. As a conceptual contribution which may be of independent interest, we present a new “non-recursive” construction of extractors with seed length O(\log(n/\varepsilon)) and output length (1-\eta)k that is computable in nearly-linear time when k>\operatorname{polylog}(1/\varepsilon) and avoids the complicated recursive procedures from [SZ99, GUV09]. We believe this to be a conceptually better approach towards constructing seeded extractors, and we discuss it in more detail in the technical overview. Faster instantiations of Trevisan’s extractor. One of the most widely-used explicit seeded extractors is Trevisan’s extractor [Tre01, RRV02]. While by now we have extractors with better parameters, one of its main advantages is that it is one of the few examples of extractors, and in a sense the best one, which are known to be quantum proof.444An extractor is quantum proof if its output is close to uniform even in the presence of a quantum adversary that has some (bounded) correlation with X. A bit more formally, \mathsf{Ext} is quantum-proof if for all classical-quantum state \rho_{XE} (where E is a quantum state correlated with X) with H_{\infty}(X|E)\geq k, and a uniform seed Y, it holds that \rho_{\mathsf{Ext}(X,Y)YE}\approx_{\varepsilon}\rho_{U_{m}}\otimes\rho_{Y}% \otimes\rho_{E}. See [DPVR12] for more details. Trevisan’s extractor uses two basic primitives: combinatorial designs (when more than one output bit is desired), and binary list-decodable codes. A standard instantiation of such suitable codes goes by concatenating a Reed-Solomon code with a Hadamard code, and this is also what is considered in [FWE+23, FYEC24]. As they also observe, this gives a nearly-linear time construction when the output length m=1. In fact, by leveraging fast multipoint evaluation, one can also get a nearly-linear time construction for any output length m\leq\frac{n}{\log(1/\varepsilon)}, although this was not noted in previous works.555For a rigorous statement on fast multipoint evaluation, see Lemma 2.1. Our main contribution in this direction is an alternative instantiation of Trevisan’s extractor that can be computed in truly linear time on a RAM in the logarithmic cost model, for any output length m\leq\frac{n}{\log(1/\varepsilon)\cdot\operatorname{polylog}(n)}. Theorem 2. There exists an instantiation of Trevisan’s extractor, set to extract m bits with any error \varepsilon>0, that is computable in: 1. Time O(n)+m\log(1/\varepsilon)\cdot\operatorname{polylog}(n) after a preprocessing step running in time \widetilde{O}(m\log(n/\varepsilon)), on a RAM in the logarithmic cost model. In particular, there exists a universal constant c, such that whenever m\leq\frac{n}{\log(1/\varepsilon)\cdot\log^{c}(n)}, the instantiation runs in time O(n), without the need for a preprocessing step. 2. Time \widetilde{O}(n+m\log(1/\varepsilon)) in the Turing model. We note that one interesting instantiation of the above theorem is when Trevisan’s extractor is set to output k^{\Omega(1)} bits for k=n^{\Omega(1)}. In this setting, Trevisan’s extractor requires a seed of length O\mathopen{}\mathclose{{}\left(\frac{\log^{2}(n/\varepsilon)}{\log(1/% \varepsilon)}}\right), and, as long as \varepsilon is not too tiny, we get truly-linear runtime. 1.2 Other Related Work Besides the long line of work focusing on improved constructions of explicit seeded extractors and mentioned in the introduction above, other works have studied randomness extraction in a variety of restricted computational models. These include extractors computable by streaming algorithms [BRST02], local algorithms [Lu02, Vad04, BG13, CL18], AC0 circuits [GVW15, CL18, CW24], AC0 circuits with a layer of parity gates [HIV22], NC1 circuits [CW24], and low-degree polynomials [ACG+22, AGMR24, GGH+24]. Moreover, implementations in various restricted computational models of other fundamental pseudorandomness primitives such as k-wise and \varepsilon-biased generators, that often play a key role in constructions of various types of extractors, have also been independently studied (see [HV06, Hea08, CRSW13, MRRR14] for a very partial list). As mentioned briefly above, some works have also focused on constructing seeded extractors computable in time O(n\log n) motivated by applications in privacy amplification for quantum key distribution. Such constructions are based on hash functions, and are thus far restricted to \Omega(m) seed length. The work of Hayashi and Tsurumaru [HT16] presents an extensive discussion of such efforts. We also mention that nearly-linear time extractors with very short seed, in the regime k=n^{\Omega(1)} and \varepsilon=n^{-o(1)}, were given in [DMOZ22], with applications in derandomization. 1.3 Technical Overview In a nutshell, we obtain 1 by following two standard high-level steps: 1. We apply a randomness condenser with small seed length O(\log(n/\varepsilon)) to the original n-bit weak source X to obtain an output X^{\prime} that is \varepsilon-close to a high min-entropy source. 2. We apply a seeded extractor tailored to high min-entropy sources with small seed length O(\log(n/\varepsilon)) to X^{\prime} to obtain a long output that is \varepsilon-close to uniform. To realize this approach, we need to implement each of these steps in nearly-linear time \widetilde{O}(n) (possibly after a reasonable one-time preprocessing step). We briefly discuss how we achieve this, and some pitfalls we encounter along the way. Observations about nearly-linear time condensers. In order to implement Item 1, we need to use fast condensers with short seeds. Luckily for us, some existing state-of-the-art constructions of condensers already satisfy this property, although, to the best of our knowledge, this has not been observed before. We argue this carefully in Section 3.3. For example, the “lossy Reed-Solomon condenser” from [GUV09] interprets the source as a polynomial f\in\mathds{F}_{q}[x] of degree d\leq n/\log q and the seed y as an element of \mathds{F}_{q}, and outputs \mathsf{RSCond}(f,y)=(f(y),f(\zeta y),\dots,f(\zeta^{m^{\prime}}y)), for an appropriate m^{\prime} and field size q, with \zeta a primitive element of \mathds{F}_{q}. Evaluating \mathsf{RSCond}(f,y) corresponds to evaluating the same polynomial f on multiple points in \mathds{F}_{q}. This is an instance of the classical problem of multipoint evaluation in computational algebra, for which we know fast and practical algorithms (e.g., see [vzGG13, Chapter 10] or Lemma 2.1) running in time \widetilde{O}((d+m^{\prime})\log q)=\widetilde{O}(n), since d\leq n/\log q and if m^{\prime}\leq n/\log q. A downside of this condenser is that it requires knowing a primitive element \zeta of \mathds{F}_{q} with q=\operatorname{poly}(n/\varepsilon). As discussed above, if we know the factorization of q-1 and q is a power of 2, then we can find such a primitive element in time \operatorname{polylog}(q). Beyond that, having access to such primitive elements, which only need to be computed once independently of the source and seed, is reasonable in practice. Therefore, we may leave this as a one-time preprocessing step. The lossless “KT condenser” from [KT22] has a similar flavor. It interprets the source as a polynomial f\in\mathds{F}_{q}[x] and the seed y as an evaluation point, and outputs \mathsf{KTCond}(f,y)=(f(y),f^{\prime}(y),\dots,f^{(m^{\prime})}(y)), for some appropriate m^{\prime}. The problem of evaluating several derivatives of the same polynomial f on the same point y (sometimes referred to as Hermite evaluation) is closely related to the multipoint evaluation problem above, and can also be solved in time \widetilde{O}(n).666Interestingly, recent works used other useful computational properties of the KT condenser. Cheng and Wu [CW24] crucially use the fact that the KT condenser can be computed in NC1. Doron and Tell [DT23] use the fact that the KT condenser is logspace computable for applications in space-bounded derandomization. Evaluating the KT condenser does not require preprocessing. On the other hand, it only works when the min-entropy k\geq C\log^{2}(n/\varepsilon) for a large constant C>0, where n is the source length and \varepsilon the target error of the condenser. The “ideal” approach to seeded extraction from high min-entropy sources. We have seen that there are fast condensers with short seeds. It remains to realize Item 2. Because of the initial condensing step, we may essentially assume that our n-bit weak source X has min-entropy k\geq(1-\delta)n, for an arbitrarily small constant \delta>0. In this case, we would like to realize in time \widetilde{O}(n) and with overall seed length O(\log(n/\varepsilon)) what we see as the most natural approach to seeded extraction from high min-entropy sources: 1. Use a fresh short seed to transform X into a block source Z=Z_{1}\circ Z_{2}\circ\cdots\circ Z_{t} with geometrically decreasing blocks, where \circ denotes string concatenation. A block source has the property that each block Z_{i} has good min-entropy even conditioned on the values of blocks Z_{1},\dots,Z_{i-1}. 2. Perform block source extraction on Z using another fresh short seed. Due to its special structure, we can extract a long random string from Z using only the (small) seed length associated with extracting randomness from the smallest block Z_{t}, which has length O(\log(n/\varepsilon)). The classical approach to Item 2 where we iteratively apply extractors based on universal hash functions with increasing output lengths to the blocks of Z from right to left is easily seen to run in time \widetilde{O}(n) and requires a seed of length O(\log(n/\varepsilon)) if, e.g., we use the practical extractors of [TSSR11, HT16]. Therefore, we only need to worry about realizing Item 1. A standard approach to Item 1 would be to use an averaging sampler to iteratively sample subsequences of X as the successive blocks of the block source Z, following a classical strategy of Nisan and Zuckerman [NZ96] (improved by [RSW06, Vad04]). We do know averaging samplers running in time \widetilde{O}(n) (such as those based on random walks on a carefully chosen expander graph). However, this approach requires a fresh seed of length \Theta(\log(n/\varepsilon)) per block of Z. Since Z will have roughly \log n blocks, this leads to an overall seed of length \Theta(\log^{2}n+\log(1/\varepsilon)), which is too much for us. Instead, we provide a new analysis of a sampler based on bounded independence, that runs in time \widetilde{O}(n) and only requires a seed of length O(\log(n/\varepsilon)) to create the entire desired block source. We give the construction, which may be of independent interest, in Section 3.2. The caveat of this “block source creator” is that it only works as desired when the target error \varepsilon\geq 2^{-k^{c}} for some small constant c>0. Combining these realizations of Items 1 and 2 yields the desired \widetilde{O}(n)-time extractor with order-optimal seed length O(\log(n/\varepsilon)) and output length (1-\eta)n for arbitrary constant \eta>0, provided that \varepsilon\geq 2^{-k^{c}}. See Theorem 5.1 for the formal statement. Getting around the limitation of the ideal approach. We saw above that combining the ideal approach to seeded extraction from high min-entropy sources with the new analysis of the bounded independence sampler yields a conceptually simple construction with the desired properties when the error is not too small (or alternatively, whenever the entropy guarantee is large enough). However, we would like to have \widetilde{O}(n)-time seeded extraction with O(\log(n/\varepsilon)) seed length and large output length for all ranges of parameters. To get around this limitation of our first construction, it is natural to turn to other classical approaches for constructing nearly-optimal extractors for high min-entropy sources, such as those of Srinivasan and Zuckerman [SZ99] or Guruswami, Umans, and Vadhan [GUV09]. These approaches consist of intricate recursive procedures combining a variety of combinatorial objects, and require a careful analysis.777In our view, these approaches are much less conceptually appealing than the “ideal” approach above. We believe that obtaining conceptually simpler constructions of fast nearly-optimal extractors that work for all errors is a worthwhile research direction, even if one does not improve on the best existing parameters. However, we could not find such an approach that works as is, even when instantiated with \widetilde{O}(n)-time condensers and \widetilde{O}(n)-time hash-based extractors. In particular: • The GUV approach [GUV09] gives explicit seeded extractors with large output length and order-optimal seed length for any min-entropy requirement k and error \varepsilon. However, its overall runtime is significantly larger than \widetilde{O}(n) whenever \varepsilon is not extremely small (for example, \varepsilon=2^{-k^{\alpha}} for some \alpha\in(0,1/2) is not small enough). • The SZ approach [SZ99] can be made to run in time \widetilde{O}(n) and have large output length when instantiated with fast condensers, samplers, and hash-based extractors, but it is constrained to error \varepsilon\geq 2^{-ck/\log^{*}\!n}, where \log^{*} is the iterated logarithm. Fortunately, the pros and cons of the GUV and SZ approaches complement each other. Therefore, we can obtain our desired result by applying appropriately instantiated versions of the GUV and SZ approaches depending on the regime of \varepsilon we are targeting. 1.4 Future Work We list here some directions for future work: • Remove the preprocessing step that our constructions behind 1 require when k<C\log^{2}(n/\varepsilon). • On the practical side, develop concrete implementations of seeded extractors with near-optimal seed length and large output length. In particular, we think that our non-recursive construction in Section 5.1 holds promise in this direction. 1.5 Acknowledgements Part of this research was done while the authors were visiting the Simons Institute for the Theory of Computing, supported by DOE grant # DE-SC0024124. D. Doron’s research was also supported by Instituto de Telecomunicações (ref. UIDB/50008/2020) with the financial support of FCT - Fundação para a Ciência e a Tecnologia and by NSF-BSF grant #2022644. J. Ribeiro’s research was also supported by Instituto de Telecomunicações (ref. UIDB/50008/2020) and NOVA LINCS (ref. UIDB/04516/2020) with the financial support of FCT - Fundação para a Ciência e a Tecnologia."
https://arxiv.org/html/2411.07314v1,Anomaly Detection in OKTA Logs using AutoEncoders,"Okta logs are used today to detect cybersecurity events using various rule-based models with restricted look back periods. These functions have limitations, such as a limited retrospective analysis, a predefined rule set, and susceptibility to generating false positives. To address this, we adopt unsupervised techniques, specifically employing autoencoders. To properly use an autoencoder, we need to transform and simplify the complexity of the log data we receive from our users. This transformed and filtered data is then fed into the autoencoder, and the output is evaluated. Keywords: autoencoder, deep learning, cybersecurity, anomaly detection, okta","Okta’s Behavior Detection function, which is a current offering in Okta’s commercial SSO product, operates on a rules-based engine that detects potential cybersecurity events by analyzing user behavior patterns within the Okta system. While this tool can be effective at identifying certain types of anomalies, it also has several limitations that can impact its overall efficacy. One limitation to this built-in tool is that it is based on a predefined set of data and rules that may not capture all potential threat scenarios. Consequently, the tool could overlook threats that fall outside these predefined data and rules or that occur in a way that fail to trigger alerts. Additionally, as new types of threats emerge, the rules may need to be updated to effectively detect them, potentially leading to delayed detection until the rules are updated Another limitation is the restricted look-back window. The tool is designed to analyze user behavior within a limited time frame, typically an average of 20 authentication attempts. This means that threats that occur outside of this window will not be detected. Even more importantly, establishing a realistic baseline that reflects a user’s actual behavior is limited to this time frame alone. The tool may also produce false positives. False positives can lead to unnecessary alerts and can create additional work for security teams. With a restricted look back window, typical behavior may be truncated and identified as an anomaly. By expanding this window to look back through the user’s entire history, this typical behavior would be captured. The Okta’s Behavior Detection tool can be a useful capability of a larger cybersecurity strategy, but it is important to be aware of its limitations and to use it in conjunction with other security measures to provide comprehensive threat detection and response capabilities. On the other hand, when dealing with large datasets, heuristic models face scalability challenges. Identifying and filtering anomalous events can reduce data volume considerably, leading to a less complex heuristic model that’s easier to manage. Such a streamlined process improves scalability. However, moving towards an efficient and scalable anomalous data detector might lead us to supervised methods, which necessitate labeled data. Relying on heuristics for labeling could reintroduce the same scalability and maintenance concerns, limiting the historical data available for training. Our goal is to detect user behavior anomalies using the event hour, day of the week, event outcome, location and application fields of the Okta System Log dataset and do so using Autoencoders. An autoencoder can be defined as a neural network that is trained to try to emulate its input in its output. This provides a useful mechanism to detect behavior that differs significantly from a specific users’ typical login behavior. We define anomalies broadly, but offer several examples to guide our approach: • User x Logs into a meaningfully different location from normal • User x’s behavior (for example, applications accessed) has changed meaningfully from their typical behavior, based on some established baselines. • User x is engaging in behavior that is specifically strange (lots of MFA requests, login failures, lockouts, etc) within a smaller window of time. It is important to note that we use the term ”meaningful” intentionally. To accurately detect changes in user behavior, we must first establish a baseline understanding of their typical patterns within the Okta data. This allows us to identify meaningful changes and ensure our autoencoder model accurately represents the users’ behavior."
https://arxiv.org/html/2411.07224v1,TempCharBERT: Keystroke Dynamics for Continuous Access Control Based on Pre-trained Language Models,"With the widespread of digital environments, reliable authentication and continuous access control has become crucial. It can minimize cyber attacks and prevent frauds, specially those associated with identity theft. A particular interest lies on keystroke dynamics (KD), which refers to the task of recognizing individuals’ identity based on their unique typing style. In this work, we propose the use of pre-trained language models (PLMs) to recognize such patterns. Although PLMs have shown high performance on multiple NLP benchmarks, the use of these models on specific tasks requires customization. BERT and RoBERTa, for instance, rely on subword tokenization, and they cannot be directly applied to KD, which requires temporal-character information to recognize users. Recent character-aware PLMs are able to process both subwords and character-level information and can be an alternative solution. Notwithstanding, they are still not suitable to be directly fine-tuned for KD as they are not optimized to account for user’s temporal typing information (e.g., hold time and flight time). To overcome this limitation, we propose TempCharBERT, an architecture that incorporates temporal-character information in the embedding layer of CharBERT. This allows modeling keystroke dynamics for the purpose of user identification and authentication. Our results show a significant improvement with this customization. We also showed the feasibility of training TempCharBERT on a federated learning settings in order to foster data privacy.","Figure 1: Keystroke metrics based on pressing and releasing timestamps, including latency interval, dwell (or hold) time and flight time. In this work, we propose TempCharBERT, an architecture based on pre-trained language models (PLMs) and customized for KD. Although PLMs have been providing impressive results on multiple NLP benchmarks, the use of such models for specific tasks is not always straightforward and often requires some level of customization. One particular concern, for instance, is the tokenization granularity, which can be language sensitive, dependent of resource availability and can ultimately impact the performance of downstream tasks [1]. BERT and RoBERTa-based models, for example, use wordpiece tokenization and focus on processing subword inputs, rather than word or character-level tokens. This is not suitable for specific tasks, such as KD, which relies significantly on processing temporal information from each key used while typing. Thus, these models provide low performance if directly applied to keystroke dynamics. Recent character-aware PLMs, such as CharBERT [2] and CharacterBERT [3], offer an alternative as they can process both subwords and character-level information. These models, however, are still not suitable to be directly fine-tuned for keystroke dynamics as they are not optimized to account for user’s temporal typing information (e.g., dwell time and flight time111The concepts of dwell time and flight time are explained in section 2.). To address this limitation, we propose a modification on the CharBERT architecture. This change aims at incorporating keystroke metrics into the embedding layer of CharBERT. We show that these temporal-character information are enough to enhance the representation of user typing pattern, leading to significant improvement in terms of accuracy for user identification as well as in terms of equal error rate (EER) for user authentication. We also investigated the feasibility of training the proposed TempCharBERT in the Federated Learning settings to foster user data privacy. We found a small decay in the performance when compared to the centralized training settings. Thus, our contribution is summarized as follow: • We propose TempCharBERT as a new variant of CharBERT for keystroke dynamics. The originality of TempCharBERT stems from the temporal typing dynamic, which is crucial for capturing typing style. • We evaluate the proposed model on two important tasks: user identification and user authentication. Our results suggest a significant improvement in terms of accuracy and equal error rate compared to the pure CharBERT and other baseline approaches. • We show that the representation based on our customized embedding layer carries out meaningful user discriminative information. • We also show that the representation attained can be successfully used on other architectures, such as the Long-Short term Memory (LSTM). • To foster privacy, we show the feasibility of training TempCharBERT on the Federated Learning (FL) settings. The remainder of this paper is organized as follows. Section 2 presents background material and related works. Section 3 describes the proposed method, while Section 4 presents the experimental setup. Section 5 presents the experimental results. Section 6 discuss the paper limitation and Section 7 concludes the paper with final considerations."
https://arxiv.org/html/2411.07128v1,ZT-RIC: A Zero Trust RIC Framework for ensuring data Privacy and Confidentiality in Open RAN,"The advancement of 5G and NextG networks through Open Radio Access Network (O-RAN) architecture marks a transformative shift towards more virtualized, modular, and disaggregated configurations. A critical component within this O-RAN architecture is the RAN Intelligent Controller (RIC), which facilitates the management and control of the RAN through sophisticated machine learning-driven software microservices known as xApps. These xApps rely on accessing a diverse range of sensitive data from RAN and User Equipment (UE), stored in the near Real-Time RIC (Near-RT RIC) database. The inherent nature of this shared, multi-vendor, and open environment significantly raises the risk of unauthorized sensitive RAN/UE data exposure. In response to these privacy concerns, this paper proposes a privacy-preserving zero-trust RIC (dubbed as, ZT-RIC) framework that preserves RAN/UE data privacy within the RIC platform (i.e., shared RIC database, xApp, and E2 interface). The underlying idea is to employ a computationally efficient cryptographic technique called Inner Product Functional Encryption (IPFE) to encrypt the RAN/UE data at the base station, thus, preventing data leaks over the E2 interface and shared RIC database. Furthermore, ZT-RIC customizes the xApp’s inference model by leveraging the inner product operations on encrypted data supported by IPFE to enable xApp to make accurate inferences without data exposure. For evaluation purposes, we leverage a state-of-the-art InterClass xApp, which utilizes RAN key performance metrics (KPMs) to identify jamming signals within the wireless network. Prototyping on an LTE/5G O-RAN testbed demonstrates that ZT-RIC not only ensures data privacy/confidentiality but also guarantees a desired model accuracy, evidenced by a 97.9% accuracy in detecting jamming signals as well as meeting stringent sub-second timing requirement with a round-trip time (RTT) of 0.527 seconds.","The Open Radio Access Network (O-RAN) architecture heralds a transformative shift in cellular communications, featuring an open, programmable, interoperable, and virtualized RAN architecture. This novel architecture supports network flexibility and scalability besides playing a pivotal role in national security by reducing reliance on foreign vendors and driving economic growth through innovation [1]. At the heart of the O-RAN paradigm is the concept of intelligent and data-driven closed-loop control through the RAN Intelligent Controller (RIC) component, specifically the Near-Real-Time RIC (Near-RT RIC) which supports telemetry and closed-loop control across multiple RAN sites from different vendors through the use of third-party software microservices known as xApps. These xApps leverage a diverse array of machine learning (ML) techniques that operate on the stored RAN and User Equipment (UE) Key Performance Metrics (KPMs) data such as Received Signal Strength Indicator (RSSI) and Signal to Interference and Noise Ratio (SINR) in the shared RIC database within the Near-RT RIC to make RAN control decisions such as interference mitigation, scheduling, spectrum sharing and traffic steering as demonstrated in works by authors in [2, 3, 4, 5]. While the O-RAN framework offers flexibility, scalability, and cost-effectiveness for cellular networks, data privacy and confidentiality concerns are raised, particularly regarding the data used by ML-driven xApps for RAN control within the Near-RT RIC. The O-RAN Alliance’s Security Working Group (WG11) [6] has conducted a comprehensive security analysis, identifying various threat models across O-RAN components and interfaces. This analysis underscores specific threats and attack vectors affecting ML-based xApps in the Near-RT RIC. Currently, researchers evaluate the vulnerabilities in O-RAN, though concrete solutions remain sparse. A recent study developed a framework to detect protocol attacks and identified vulnerabilities in sensitive RAN and UE data, such as IMEI and IMSI, which can be exploited for attacks like IMSI extraction, posing significant privacy risks [7]. Additionally, concerns have been raised about potential ML data poisoning attacks that could manipulate stored KPMs in the RIC database to impair network performance [5, 8]. Authors in [9] propose a zero-trust security system for the O-RAN environment, featuring an access control module for packet tagging and verification of xApps, alongside a policy management module for control. Current security approaches, such as Role-Based Access Control (RBAC), traditional encryption, and IPSec enhancements recommended by WG11 [6] are considered inadequate for ensuring robust data security and implementing the zero-trust paradigm within O-RAN. Issues with these mechanisms include documented vulnerabilities such as credential leaks [10], besides the computational challenges posed by traditional encryption methods conflicting with O-RAN’s stringent latency requirements [9]. Furthermore, existing methodologies mainly focused on protocol attack [7] and ML attacks [5] and fail to address data privacy leaks/attacks within O-RAN architecture. In conclusion, there is a pressing need for effective data privacy-preserving solutions for the O-RAN that go beyond access control, policy management, and authentication approach, without compromising the real-time operational demands of O-RAN architecture. This paper makes the following key contributions: \bullet We propose ZT-RIC, a zero-trust RIC framework for ensuring data privacy/confidentiality within O-RAN architecture. ZT-RIC adapts a computationally efficient cryptographic technique, called Inner Product Functional Encryption (IPFE), which encrypts the RAN/UE data before storing it in the RIC database. Next, we quantize the ML-based xApp model to support the IPFE procedure such that the model makes inferences without decrypting the data, thus ensuring data privacy and confidentiality, unlike the conventional cryptographic methods that need data decryption. \bullet To evaluate the ZT-RIC framework, we utilize an example ML-based InterClass xApp as designed in [5], which aims to detect the presence of a jammer in a wireless environment using RAN-related key performance metrics. Then, leveraging an over-the-air LTE O-RAN testbed, we show that ZT-RIC framework ensures data privacy while achieving quantized accuracy of up to 97.9% for the InterClass xApp model. Moreover, for this level of accuracy, the encryption and model evaluation times for the InterClass xApp model summed up to 0.474s and achieved a round trip time (RTT) of 0.527s. We observe that the performances of ZT-RIC are at par with that of the baseline O-RAN framework (no data privacy protection) which validates that our proposed ZT-RIC framework addresses privacy issues without negatively impacting network performances and latency requirements. The rest of the paper is organized as follows. Section 2 covers O-RAN background, threat model, and design objectives. Section 3 presents the ZT-RIC framework while Section 4 details the O-RAN testbed and ZT-RIC prototype. Section 5 discusses experimental results, and lastly, Section 6 concludes the paper."
https://arxiv.org/html/2411.07114v1,TinyML Security: Exploring Vulnerabilities in Resource-Constrained Machine Learning Systems,"Tiny Machine Learning (TinyML) systems, which enable machine learning inference on highly resource-constrained devices, are transforming edge computing but encounter unique security challenges. These devices, restricted by RAM and CPU capabilities two to three orders of magnitude smaller than conventional systems, make traditional software and hardware security solutions impractical. The physical accessibility of these devices exacerbates their susceptibility to side-channel attacks and information leakage. Additionally, TinyML models pose security risks, with weights potentially encoding sensitive data and query interfaces that can be exploited. This paper offers the first thorough survey of TinyML security threats. We present a device taxonomy that differentiates between IoT, EdgeML, and TinyML, highlighting vulnerabilities unique to TinyML. We list various attack vectors, assess their threat levels using the Common Vulnerability Scoring System, and evaluate both existing and possible defenses. Our analysis identifies where traditional security measures are adequate and where solutions tailored to TinyML are essential. Our results underscore the pressing need for specialized security solutions in TinyML to ensure robust and secure edge computing applications. We aim to inform the research community and inspire innovative approaches to protecting this rapidly evolving and critical field.","The computing landscape has undergone a significant transformation in recent years, driven by the proliferation of connected devices and the growing demand for real-time data processing. Although cloud computing has long been the backbone of data analysis and processing to extract intelligence, the growing need for immediate decision-making and reduced latency has precipitated a shift to edge computing [1]. The transition to edge computing brings the computation closer to the data source, enabling faster response times and more efficient use of network resources. Within this evolving ecosystem, the Internet of Things (IoT) has emerged as a key driver, with tens of billions of interconnected devices that improve both everyday activities and industrial operations [1]. As these devices become more prevalent, the demand for localized and efficient computing solutions continues to grow. TinyML has emerged as a critical field at the forefront of this shift, bridging the gap between IoT devices and edge computing capabilities. Figure 1 illustrates TinyML’s unique position within the broader context of Edge Computing. By combining the resource-constrained nature of IoT devices with the computational capabilities of EdgeML, TinyML enables machine learning applications on extremely low-power devices. This convergence of technologies opens up new possibilities for intelligent and autonomous operations at the edge of networks. This innovation is set to revolutionize multiple industries, including healthcare, manufacturing, and environmental monitoring by offering localized and efficient data processing and decision-making capabilities [2, 1]. Figure 1: Venn diagram illustrating the interrelationships between IoT, Edge Computing, EdgeML, and TinyML. It illustrates how TinyML overlaps with both IoT and EdgeML within the broader scope of Edge Computing. As TinyML technology swiftly progresses and integrates into a growing number of applications, a key concern frequently neglected is security. TinyML devices operate with stringent resource limitations, possessing memory and computational power often two to three orders of magnitude below that of traditional IoT or edge devices. This significant disparity in resources introduces unprecedented challenges to implementing security measures. Whereas a standard edge device might have megabytes of RAM and processors running at gigahertz speeds, TinyML devices typically function with only kilobytes of memory and processors operating at megahertz speeds. These harsh constraints render it impractical, if not impossible, to apply conventional security methods directly. Additionally, the broad deployment of these resource-limited devices in diverse and often physically accessible locations subjects them to various potential threats. Given that TinyML systems are increasingly relied upon for sensitive tasks and data, addressing these specific security challenges is imperative for their effective and responsible use. The distinctive features of TinyML devices present a unique set of security concerns that require careful consideration. Traditional security approaches, crafted for environments with abundant resources, often clash with the stringent limitations of TinyML systems. This mismatch leads to a significant vulnerability in safeguarding these devices from various threats. First, the minimal computational power and memory of TinyML devices make it difficult to deploy strong cryptographic protocols or sophisticated authentication methods. Second, their physical presence in varied deployment settings makes them susceptible to tampering and side-channel attacks, which are especially challenging to address given the devices’ constrained resources. Third, ML models, which are frequently proprietary and include sensitive information, become attractive targets for theft or manipulation. Additionally, the network-connected nature of many TinyML applications exposes them to remote attacks, while devices lack the capacity for extensive network security measures. This issue is exacerbated by the need to balance security with the functionality of these devices, ensuring that security measures do not significantly hinder their primary operations. In this paper, we provide a thorough and organized evaluation of the security landscape within TinyML, to close the current knowledge gap by presenting a comprehensive understanding of specific security issues and countermeasures pertinent to TinyML systems. The importance of our work is emphasized by the significant gap between the rapid progress in TinyML technologies and the relatively slow pace of research on their security. Figure 2 strikingly highlights this research disparity. The figure shows the trends of TinyML research publications from 2015 to 2023. During this period, around 347 publications have been dedicated to TinyML models, hardware, and software, but only 9 have tackled the issue of TinyML security. Alarmingly, many of the security-focused papers are more concerned with utilizing TinyML to improve security than addressing the security of TinyML devices themselves. This lack of security research, despite the increasing implementation of TinyML in various fields, underscores the need for an extensive review and analysis. To conduct our vulnerability analysis, we developed a taxonomy of edge devices, distinguishing between traditional IoT devices, EdgeML devices, and TinyML devices, thereby elucidating the distinct security challenges faced by each type. In addition, we form a detailed threat model that identifies and categorizes attack vectors and unique target artifacts for TinyML devices, laying the groundwork for future security evaluations. We assess the severity and potential impact of different attacks on TinyML systems using the Common Vulnerability Scoring System (CVSS) [3], which provides a qualitative measure for risk assessment. A crucial part of our review involves evaluating the feasibility and efficiency of conventional hardware, software, and model security techniques within the tightly restricted setting of TinyML devices. We also gather findings from prior work and offer a comprehensive perspective on both the theoretical and applied aspects of TinyML security. Finally, we demonstrate the new challenges and pinpoint promising future research avenues in safeguarding TinyML systems, taking into account the blend of machine learning and resource-limited TinyML devices. Hardware Vulnerabilities: TinyML devices face significant security challenges due to their physical accessibility and resource constraints. Side-channel attacks threaten to compromise machine learning models, while leaky interfaces, though easily exploited, can be mitigated with built-in microcontroller safeguards. Fault injection attacks pose a complex threat, capable of compromising both device and model integrity. The resource limitations of TinyML devices restrict the implementation of robust security measures, such as encrypted environments or comprehensive fault injection defenses. This landscape necessitates the development of innovative, lightweight security solutions that can effectively protect TinyML systems without overwhelming their limited computational and energy resources. Future research must focus on creating efficient countermeasures that balance security needs with the practical hardware constraints of current TinyML deployments. Figure 2: TinyML publication trends from 2015 to 2024. Alarmingly, few papers focus on TinyML security despite its pervasiveness. Many security-focused papers are more concerned with utilizing TinyML to enhance security rather than addressing the security of TinyML devices themselves. Software Vulnerabilities: TinyML devices encounter distinct software security issues due to their limited compute, memory and network resources. Ensuring communication security, especially against eavesdropping and man-in-the-middle attacks, is crucial but challenging, as robust protocols like TLS cannot be implemented on these constrained devices. The trade-off between on-device ML inference and secure networking protocols remains largely unexamined, pointing to an important research gap. Additionally, over-the-air (OTA) updates for TinyML models pose a significant concern, as safeguarding their confidentiality, integrity, and authenticity is vital for protecting intellectual property and device functionality. Although solutions such as RIOT-ML provide some security controls, they might inadvertently increase the attack surface. Future research should concentrate on creating lightweight, standalone security mechanisms for secure communications and OTA updates, designed specifically for the resource constraints of TinyML devices. Understanding the trade-offs between ML inference, networking protocols, and security measures is essential for optimizing TinyML deployments. Model-specific Vulnerabilities: TinyML systems face a range of sophisticated attacks targeting their machine learning models. Adversarial examples pose a substantial threat, especially in critical applications like healthcare and autonomous vehicles. While quantized models show similar robustness to their full-precision counterparts, the efficiency of pre-processing defenses on resource-constrained devices remains an open question. Model extraction attacks are particularly concerning, as they can facilitate more effective adversarial and model inversion attacks. Encrypting inference outputs offers a first line of defense, but the vulnerability of TinyML models compared to larger systems requires further investigation. Backdoor attacks have demonstrated effectiveness on lightweight networks, showing that they need thorough screening before deployment. The transferability of conditioned backdoor attacks across compression techniques and the feasibility of on-device defenses are areas that need exploration. Model inversion attacks pose significant privacy risks, especially for networks trained on sensitive data. While maintaining parameter confidentiality and controlling access to inference results can help, a theoretical framework for assessing model inversion attack robustness is notably absent. Future research should focus on developing resource-efficient defenses, understanding the unique vulnerabilities of TinyML models, and creating theoretical frameworks for assessing and enhancing model security in these constrained environments. In summary, we address the critical gap in TinyML security research by providing a comprehensive analysis of the unique security challenges faced by these resource-constrained devices. Our examination covers hardware vulnerabilities, software security issues, and model-specific threats, offering insights into the complexities of protecting TinyML systems. By developing a taxonomy of edge devices, formulating a threat model, and assessing the feasibility of existing security techniques, we laid the groundwork for future advancements in TinyML security. The stark contrast between the rapid proliferation of TinyML technologies and the limited focus on their security underscores the urgency of our work. As TinyML continues to revolutionize various industries with its localized and efficient data processing capabilities, addressing these security concerns becomes paramount. This paper not only highlights the current state of TinyML security, but also identifies promising avenues for future research, paving the way for the development of innovative lightweight security solutions that can effectively protect TinyML systems without compromising their low resource efficiency and functionality."
https://arxiv.org/html/2411.07036v1,ProP: Efficient Backdoor Detection via Propagation Perturbation for Overparametrized Models,"Backdoor attacks pose significant challenges to the security of machine learning models, particularly for overparameterized models like deep neural networks. In this paper, we propose ProP (Propagation Perturbation), a novel and scalable backdoor detection method that leverages statistical output distributions to identify backdoored models and their target classes without relying on exhausive optimization strategies. ProP introduces a new metric, the benign score, to quantify output distributions and effectively distinguish between benign and backdoored models. Unlike existing approaches, ProP operates with minimal assumptions, requiring no prior knowledge of triggers or malicious samples, making it highly applicable to real-world scenarios. Extensive experimental validation across multiple popular backdoor attacks demonstrates that ProP achieves high detection accuracy and computational efficiency, outperforming existing methods. These results highlight ProP’s potential as a robust and practical solution for backdoor detection.","As deep neural networks (DNNs) continue to demonstrate their capabilities across various domains, the demand to ensure safety, security, and privacy has increased accordingly. In fact, DNNs have been shown vulnerable to a variety of security and privacy attacks, raising concerns about their deployment in sensitive applications. These vulnerabilities span across adversarial attacks, data poisoning, backdoor attacks, and privacy attacks [1, 2, 3, 4, 5, 6, 7, 8, 9], highlighting the need to address these issues both in research and real-world practice. One prominent security concern is the backdoor attack. In a backdoor attack, an adversary embeds a specific pattern, known as a trigger, into a subset of the training data, associating it with a designated target class. Once the model is trained, any input containing the trigger is misclassified as the target class, while the model performs normally on other inputs. This covert manipulation can lead to severe consequences, especially in sensitive applications like autonomous driving, healthcare, and finance. Existing defense mechanisms against backdoor attacks can be broadly categorized into mitigation and detection strategies. Mitigation approaches, such as activation clustering [10], scale-up [11], and input preprocessing [12], aim to spot or bypass poisoned samples within datasets. Other methods, like retraining [12] and fine-pruning [13], focus on reconstructing the malicious models. In this paper, we focus on backdoor detection, which is particularly relevant for users who train models on third-party platforms, where they may lack full control over the training process and thus require a means of ensuring model integrity. Despite some advancements, backdoor detection remains a relatively underexplored area, broadly categorized into two approaches: blind search and optimization-based strategies. Blind search methods, such as One-Pixel Signature [13], rely on brute-force techniques that test each pixel individually to identify the most impactful change. However, this process is exhaustive and inefficient. In contrast, optimization-based methods, like Neural Cleanse [14], reverse-engineer potential backdoor triggers by solving optimization problems for each class.A recent approach, BAN [15], aims to reduce complexity by optimizing neuron-specific perturbations. However, it remains computationally inefficient due to the required optimization process. To address these inefficiencies, we propose ProP (Propagation Perturbation), a novel method for efficiently detecting backdoors in over-parameterized models. Our key assumption is that backdoored models have a significantly larger output space volume for the target class. By introducing large noise into the activation function during forward propagation and analyzing the resulting output distributions, we observe that each model exhibits a unique, fixed distribution, independent of the input. Notably, backdoored models displayed an almost 100% probability of classifying inputs as the target class, clearly distinguishing them from benign models. Our main contributions are as follows: 1) Novel backdoor detection approach: We introduce ProP, a lightweight and scalable method for backdoor detection that requires no optimization or search-based strategies. ProP leverages statistical output distributions to detect compromised models and identify the target class (see Fig. 1 for details). 2) Novel metric: We propose a novel metric, the benign score \beta_{s} to quantify the output distribution of models, effectively distinguishing between benign and backdoored models. 3) Real-World Applicability: ProP operates with minimal assumptions, requiring no prior knowledge of either benign or malicious samples with triggers, making it highly adaptable to real-world scenarios. Experimental validation demonstrates that ProP consistently achieves high detection accuracy across various backdoor attacks, outperforming existing approaches in both efficiency and effectiveness."
https://arxiv.org/html/2411.06493v2,LProtector: an LLM-driven Vulnerability Detection System,"The security issues of large-scale software systems and frameworks have become increasingly severe with the development of technology. As complexity of software grows, vulnerabilities are becoming more challenging to detect. Although traditional machine learning methods have been applied in cybersecurity for a long time, there has been no significant breakthrough until now. With the recent rise of large language models (LLMs), a turning point seems to have arrived. The powerful code comprehension and generation capabilities of LLMs make fully automated vulnerability detection systems a possibility. This paper presents LProtector, an automated vulnerability detection system for C/C++ codebases based on GPT-4o and Retrieval-Augmented Generation (RAG). LProtector performs binary classification to identify vulnerabilities in target codebases. To evaluate its effectiveness, we conducted experiments on the Big-Vul dataset. Results show that LProtector outperforms two state-of-the-art baselines in terms of F1 score, demonstrating the potential of integrating LLMs with vulnerability detection.","AI has significantly progressed in various defect detection areas over a long period of time. An example is when Wang et al. [1] utilized AI for identifying medical problems, while Wu and collaborators [2], [3] employed it for detecting Electromigration problems. Defects in software, also referred to as software vulnerabilities, pose a significant challenge in the cybersecurity field. Xu et al. [4] demonstrate the various ways in which harm and software loss can arise from these vulnerabilities. Several restrictions are associated with conventional detection methods. Yao et al. [5] and Li et al. [6] discovered that Automated Program Repair (APR) depends on predetermined patterns or strategies to create patches. Nevertheless, the patches do not meet the expected quality standards. Code generated by APR may successfully meet certain test cases, but it could still struggle to address the underlying issue, leading to ineffective outcomes in different situations. Klees et al., Kai et al., and Han et al. [7], [8], [9] propose that while fuzzing tests are good at uncovering memory management errors, they are not as efficient at identifying intricate problems like race conditions and privilege escalation. Understanding the goals and procedures of a program is crucial in identifying logical errors, as depending only on fuzzing is insufficient for detecting and correcting them. Drawbacks are also linked to Static Analysis Tools (SAT). Pereia [10] highlighted that SAT tools generate many incorrect results because they do not take into account dynamic factors such as variable value changes in real-time. They frequently sound the alarm for issues that are not real. An instance could be discovering a possible buffer overflow in a code path that is not used. Additionally, Liu et al. [11] pointed out the difficulties SAT faces when handling intricate dynamic behaviors, such as dynamic memory allocation or conditional branches, leading to constraints in identifying dynamic vulnerabilities such as race conditions.These limitations highlight the need for more advanced methods. Similar innovations are needed for road damage detection. Han-Cheng Dan et al. [12]successfully improved detection accuracy and efficiency using the enhanced YOLOv7 algorithm, demonstrating the advantages of improved model strategies. In contrast, LLMs have powerful code generation and understanding capabilities [13], [14], [15], along with a rich knowledge base and strong generalization ability [16], [17], [18]. For instance, Tan et al. demonstrated that neural networks could effectively convert textual descriptions into 3D models using encoder-decoder architectures, showcasing the potential of LLMs to handle multi-modal tasks across various domains [19]. Zhang et al. finds the black-box LLMs like GPT-4o can label textual datasets with a quality that surpasses that of skilled human annotators, which offers a new perspective for automated software defect detection, as LLMs can achieve more efficient training and labeling when handling large volumes of unlabeled data [20]. Thus, we selected the GPT-4o, which is currently one of the most capable models, as the AI agent for LProtector. This ensures good robustness even in systems with strong interference [21]. We used the Big-Vul dataset [22] to evaluate how well LProtector works by measuring its performance against VulDeePecker [23] and Reveal [24]. To enhance LProtector’s cybersecurity knowledge, we used RAG methods to pick 500 random instances of CWE/CVE from the Big-Vul dataset and stored them in a vector database."
https://arxiv.org/html/2411.06486v1,DDIM-Driven Coverless Steganography Scheme with Real Key,"Typical steganography embeds secret information into images by exploiting their redundancy. Since the visual imperceptibility of secret information is a key factor in scheme evaluation, conventional methods aim to balance this requirement with embedding capacity. Consequently, integrating emerging image generation models and secret transmission has been extensively explored to achieve a higher embedding capacity. Previous works mostly focus on generating stego-images with Generative Adversarial Networks (GANs) and usually rely on pseudo-keys, namely conditions or parameters involved in the generation process, which are related to secret images. However, studies on diffusion-based coverless steganography remain insufficient. In this work, we leverage the Denoising Diffusion Implicit Model (DDIM) to generate high-quality stego-images without introducing pseudo-keys, instead employing real keys to enhance security. Furthermore, our method offers low-image-correlation real-key protection by incorporating chaotic encryption. Another core innovation is that our method requires only one-time negotiation for multiple communications, unlike prior methods that necessitate negotiation for each interaction.","As secure communication becomes increasingly critical in the digital era, steganography—techniques for concealing sensitive information within various media—has gained substantial interest. Typical steganography embeds secret data within a carrier by modifying its statistical or perceptual attributes, yet this dependence on modification can restrict the flexibility and security of applications. In response, coverless steganography has emerged, eliminating carrier modification by generating or selecting covers based on data properties alone, thereby enhancing security and adaptability. Recently, advanced generative models have opened new avenues in coverless steganography, providing more dynamic implementations. Generative Adversarial Networks (GANs)[2] are models consisting of a generator that creates synthetic data and a discriminator that evaluates whether the data is real or fake. The generator learns to produce increasingly realistic data through this process, making GANs effective for generating images and other complex data types. The earliest attempt at generation-based coverless steganography was made by Duan and Song in 2018, who introduced using GANs to create independent images from secret data. This approach marked a significant shift, generating stego-images without altering existing media. Building on this foundation, Hu et al. applied Deep Convolutional GANs (DCGANs), incorporating image quality assessments and testing resilience against stego-analysis attacks. These advancements provided a stronger security foundation but highlighted the need for greater robustness in stego-image quality. Subsequent approaches continued refining GAN-based methods. For instance, Chen et al. developed a hybrid approach with StarGAN, Peng et al. incorporated gradient descent with GANs, Zhou et al. applied the Glow model with bijective mappings between latent and image space. Despite these advancements, challenges remain in achieving optimal security, image quality, and robustness, particularly against distortions and compression artifacts. Due to limitations in GAN-based models, this paper introduces diffusion models as an alternative for coverless steganography. Diffusion models, such as Denoising Diffusion Probabilistic Models (DDPMs)[10], transform random noise into structured data through iterative refinement, generating high-quality images in a stable, progressive manner. Building on DDPMs, Denoising Diffusion Implicit Models (DDIMs)[12] further enhance efficiency by enabling a more deterministic sampling process, making them particularly suitable for steganography by ensuring consistent information extraction from the generated images. Recent works have extended the application of diffusion models to steganography, focusing on embedding different types of information into images. One scheme uses diffusion models to embed text within images[6], showcasing the model’s ability to integrate textual data with visual content. CRoSS[1] applies diffusion models to embed the secret image into a generated image. While these works demonstrate diffusion’s adaptability, both rely on pseudo-keys as mentioned above to encrypt and extract the embedded content. Besides, these methods require frequent key negotiations for communication sessions, limiting their practical applicability in secure communication. In this paper, we propose a new coverless steganography method that creates high-quality stego-images by utilizing the DDIM. Our diffusion-based method delivers similar or better visual fidelity than previous GAN-based methods[5], which frequently suffer from poor image quality. Additionally, our method overcomes the problem of pseudo-key dependency present in existing diffusion-based systems by combining reversible steganography and chaotic encryption. Rather than using pseudo-keys, we employ a steganographic real key with a wide key space, improving security and confidentiality in secure communication. Our contributions can be summarized as follows: • Diffusion-Based Steganography: We propose a practical coverless steganography method utilizing DDIM, which generates high-quality stego-images with enhanced visual fidelity compared to GAN-based methods. • Removal of Pseudo-Keys: Unlike previous methods that rely on pseudo-keys associated with the secret images, our method leverages real keys with a large key space, significantly boosting security. • Chaotic Encryption for Key Protection: Our method offers real-key protection with low correlation to the secret images, enhancing security and confidentiality by incorporating chaotic encryption. • Efficient Key Negotiation: Our method requires only one-time key negotiation for multiple communications, improving efficiency over prior methods that necessitate negotiation with each interaction."
https://arxiv.org/html/2411.06458v1,Poster: Protection against Source Inference Attacks in Federated Learning using Unary Encoding and Shuffling,"Federated Learning (FL) enables clients to train a joint model without disclosing their local data. Instead, they share their local model updates with a central server that moderates the process and creates a joint model. However, FL is susceptible to a series of privacy attacks. Recently, the source inference attack (SIA) has been proposed where an honest-but-curious central server tries to identify exactly which client owns a specific data record.In this work, we propose a defense against SIAs by using a trusted shuffler, without compromising the accuracy of the joint model. We employ a combination of unary encoding with shuffling, which can effectively blend all clients’ model updates, preventing the central server from inferring information about each client’s model update separately. In order to address the increased communication cost of unary encoding we employ quantization. Our preliminary experiments show promising results; the proposed mechanism notably decreases the accuracy of SIAs without compromising the accuracy of the joint model.","In FL (McMahan and Moore, 2017), each client independently trains a model using their own data and then sends the model update to a central server. The server aggregates these model updates to create a new joint model, which is then distributed back to the clients. The process continues iteratively for multiple rounds, usually until the model converges. However, in a naive FL architecture, the central server can directly observe the clients’ reported model updates. This may lead to various privacy attacks. For example, a colluded server could launch a membership inference attack (MIA) (et al., 2017) in order to find whether a specific data point was included in any client’s training dataset. In this paper, we focus on source inference attacks (SIAs) (Hu et al., 2021), which aim to identify exactly which client owns a data point, in a setting where the central server is honest-but-curious. If successful, a SIA can lead to a severe violation of privacy; for instance, consider a scenario where several hospitals jointly build a medical model using patients’ data to treat a disease. If an adversary identifies the hospital that owns a particular patient’s record, and that hospital mostly treats COVID-19 patients, the attacker might infer that the patient suffers from COVID-19. To the best of our knowledge, no effective defense to prevent SIAs has been proposed in the literature. A typical approach in privacy-preserving FL is to use local differential privacy (LDP) (Miao et al., 2022), where clients perturb their reported model updates by adding noise. However, this approach is not very suitable against a SIA, as it has been shown that the amount of noise necessary to prevent this kind of attacks would significantly deteriorate the accuracy of the joint model (Hu et al., 2021). Contribution In this work, our goal is to design a defense against SIAs that maintains high model accuracy. To this aim, we propose Unary-Quant; a mechanism involving a trusted shuffler which blends the clients’ model updates before releasing them to the central server. The characteristic of this mechanism is that it does not require the addition of noise. Instead, it uses a unary encoding which, combined with shuffling, significantly reduces the amount of information available to the central server. To counter the high communication cost of unary encoding, Unary-Quant uses gradient quantization. We experimentally evaluate the model accuracy of Unary-Quant on the MNIST dataset. The results show that almost no accuracy is lost, i.e. the model accuracy is close to that of standard FL. Furthermore, we conduct experiments on SIAs. The results indicate that our proposed defense can significantly decrease the effectiveness of a SIA, in the sense that the accuracy of a source inference is reduced to nearly the level of a random guess."
https://arxiv.org/html/2411.06362v1,Will Central Bank Digital Currencies (CBDC) and Blockchain Cryptocurrencies Coexist in the Post Quantum Era?,"This paper explores the coexistence possibilities of Central Bank Digital Currencies (CBDCs) and blockchain-based cryptocurrencies within a post-quantum computing landscape. It examines the implications of emerging quantum algorithms and cryptographic techniques such as Multi-Party Computation (MPC) and Oblivious Transfer (OT). While exploring how CBDCs and cryptocurrencies might integrate defenses like post-quantum cryptography, it highlights the substantial hurdles in transitioning legacy systems and fostering widespread adoption of new standards. The paper includes comprehensive evaluations of CBDCs in a quantum context. It also features comparisons to alternative cryptocurrency models. Additionally, the paper provides insightful analyses of pertinent quantum methodologies. Examinations of interfaces between these methods and blockchain architectures are also included. The paper carries out considered appraisals of quantum threats and their relevance for cryptocurrency schemes. Furthermore, it features discussions of the influence of anticipated advances in quantum computing on algorithms and their applications. The paper renders the judicious conclusion that long-term coexistence is viable provided challenges are constructively addressed through ongoing collaborative efforts to validate solutions and guide evolving policies.","When considering CDBC and blockchain-based cryptocurrencies, the famous lines from the poem ’The Road Not Taken’ by Robert Frost come to mind: ’Two roads diverged in a yellow wood, And sorry I could not travel both and be one traveler, long I stood’ [1]. However, unlike the poem, in this paper, we will discuss the possibility of choosing both roads in the post-quantum era. Metaphorically, we can liken the practical aspects of this issue to those in quantum physics, known as ”quantum superposition”, where elementary particles exist in two parallel states simultaneously until their location is observed at a specific point [2]. The quantum world supports both digital currencies approaches, and it is up to the decision-maker to determine what to choose. In this era, a notable trend is our significant progress towards the post-quantum era. This phase denotes the time following the advancement of quantum computing to a level where it can potentially compromise prevalent public-key cryptosystems like Rivest–Shamir–Adleman (RSA) and Elliptic Curve Cryptography (ECC) [3]. In parallel, we see a rise in popularity of CDBC [4] side by side to rise in usage of cryptocurrency [5]. Within this context, the importance of protocols such as Oblivious Transfer/Protocol (OT) and Multi-Party Computation (MPC) remains paramount in ensuring secure and private transactions [6]. OT is a cryptographic protocol where a sender transmits one of potentially multiple pieces of information to a receiver without knowing which specific piece the receiver obtains, enabling the receiver to access information from the sender without disclosing the chosen item’s identity [6]. The sender merely acknowledges the transfer but remains unaware of the item’s index. OT finds utility in scenarios such as privacy-preserving data mining and secure two-party computation. Notable examples encompass 1-out-of-2 OT, where the sender presents two data pieces for the receiver to select from discretely, and 1-out-of-N OT, where the sender offers N data pieces for selection without revealing the specific item index [7]. OT serves as a foundational element in secure two-party and multi-party computations, facilitating collaborative function computations over private inputs while maintaining confidentiality. MPC extends privacy protection to scenarios involving multiple entities collaborating to compute functions over their respective confidential inputs [8]. For instance, banks can collectively calculate credit scores without disclosing individual customer data. In MPC, input data is partitioned and shared among distinct, non-colluding parties to prevent any single entity from accessing another’s confidential information. Subsequently, algorithms operate on these distributed data shares to produce outputs in a privacy-preserving manner. MPC methodologies commonly rely on secret sharing, garbled circuits, and cryptographic tools like Homomorphic Encryption (HE)111A cryptographic method enabling calculations on encrypted data without the need for decryption [9]. and Zero-Knowledge Proofs (ZKPs) 222A protocol where one party can prove the truth of a given statement to another party without revealing any additional information beyond the statement’s truth [10]., with OT serving as a fundamental component in many MPC protocols. The application spectrum of MPC spans privacy-centric technologies, blockchain ecosystems, and distributed analytics involving sensitive data, offering a secure framework for collaborative computation while safeguarding individual data privacy. 1.1 Reasons for Rise of Central Bank Digital Currencies (CDBC) Based on a recent survey, at the conclusion of 2021, 90% of the 81 respondent central banks in Europe were actively exploring the potential for a CBDC [11]. In the latter half of 2023, the traction supporting CBDCs has persisted. Recent findings from CBDC tracker reveal that 130 nations are currently delving into CBDCs, encompassing 98 percent of global GDP [12]. The surge in the development of CBDCs can be attributed to several key factors [13]. Firstly, there is an escalating demand for digital payments as online commerce expands and societies transition towards cashless transactions, necessitating government-backed digital currencies to accommodate modern payment methods, a need that CBDCs fulfill. Additionally, the emergence of decentralized cryptocurrencies like Bitcoin has posed a challenge to traditional currency monopolies, prompting central banks to explore hybrid digital currencies through CBDCs to maintain control while adapting to the changing landscape. Moreover, CBDCs hold promise in promoting financial inclusion by offering a universally accessible digital currency option, thereby expanding banking services to the unbanked and underbanked populations [14]. Furthermore, the utilization of digital currencies supported by distributed ledgers could revolutionize payment systems by enabling real-time, cost-effective domestic and cross-border transactions compared to conventional financial infrastructure. CBDCs also present an opportunity for central banks to introduce additional policy tools, allowing for tailored money issuance or transaction incentives to inject economic stimulus directly into the economy during periods of downturn. Many central banks view CBDCs as a means of modernizing their payment systems and embracing blockchain-based technologies that drive future financial innovations. Lastly, the development of CBDCs serves as a preemptive strategy for central banks to proactively address potential risks such as the growing dominance of non-sovereign cryptocurrencies or the disruptive influence of emerging technologies like quantum computing on existing systems. In essence, the rise of CBDCs is driven by the increasing demand for digital payments, competition from cryptocurrencies, financial inclusion objectives, central banks’ pursuit of innovative monetary policy tools, and the push for technological advancement in payment systems. 1.2 Reasons for Rise in Usage of Crypto-assets Preserving privacy in CBDCs poses a challenge. The core priorities of transparency, anonymity, traceability and compliance posed by a CBDC model introduce tensions with privacy that are challenging to resolve through technology and policy [15]. It will require novel solutions. This is one of the reasons that support the usage of cryptocurrencies. The rise in crypto-assets 333A crypto-asset is a digital representation of value or entitlement that can be electronically transferred or stored through distributed ledger technology or comparable mechanisms [16]. as well as cryptocurrencies’ popularity can be attributed to several key reasons [5]. Firstly, it serves as an alternative store of value to fiat currencies, appealing to individuals seeking a hedge against inflation and currency devaluation due to its capped supply, making it an attractive long-term investment. Moreover, cryptocurrency’s price volatility has drawn in speculators aiming to capitalize on its increasing valuation over time, attracting a broader investor base. Its utility for international payments offers a quick and cost-effective way to transfer value across borders independently of traditional banking systems, enhancing its use for remittances and global commerce. Additionally, cryptocurrency provides a level of financial anonymity unparalleled in conventional banking systems, enhancing its attractiveness to those valuing privacy and pseudonymity in transactions [17]. Ideologically, cryptocurrency aligns with the desire for decentralization and independence from governmental or corporate influence, appealing to individuals seeking a truly decentralized currency. The growing acceptance of cryptocurrencies by major payment processors, merchants, exchanges, and the proliferation of cryptocurrency-supporting ATMs have widened its accessibility and usability, expanding its user base. Functioning outside traditional financial systems, cryptocurrency offers an alternative financial network that is immune to censorship, appealing to those interested in utilizing censorship-resistant applications and money. The increasing interest in blockchain technology by businesses has fueled general awareness and excitement around cryptocurrencies like Bitcoin, further boosting its popularity. Ultimately, cryptocurrency’s scarcity, global accessibility, privacy features, and decentralized nature drive both speculative investment and practical adoption, establishing it as a versatile asset with a range of valuable use cases. 1.3 Development of Quantum Technology As of now, we are not yet in the post-quantum era, a period that will dawn once quantum computers attain the capability to break mainstream public-key cryptography such as RSA and ECC [18]. While the largest quantum computers currently boast around 100 qubits, the task of breaking AES-256 or factoring a 2048-bit RSA key demands millions of qubits, a milestone that remains distant [19]. Quantum computing experts project that realizing quantum computers capable of breaching current cryptographic standards is still a decade or two away at the earliest, given the ongoing progress in the field. Presently, we find ourselves in the ’noisy intermediate-scale quantum’ era, characterized by quantum computers with limited qubits and high error rates. These machines are primarily utilized for small proof-of-concept demonstrations rather than general computational tasks [20]. In addition, we find several recent trials for attacking to military grade encryption using quantum computer that seems ambitious and advance the post quantum era [21, 22]. Going back to 1983, Stephen Wiesner’s paper introduced groundbreaking ideas well ahead of their time. In it, he proposed using delicate quantum states as unforgeable banknotes, drawing on emerging studies of quantum physics [23, 24]. Specifically, Wiesner envisioned generating quantum money consisting of non-orthogonal quantum states, such as polarization states of single photons. These states could not be perfectly distinguished or cloned due to the fundamental no-cloning theorem of quantum mechanics. As such, any attempt to counterfeit a quantum banknote by copying or measuring it would necessarily introduce errors. The genuine banknote could then be verified by carefully measuring in designated bases. This exploiting of intrinsic quantum properties for authentication marked the earliest concept of quantum currency. Wiesner’s work was also seminal in that it helped lay the theoretical foundation for later advances in quantum information science and cryptography [23]. His money proposal directly inspired the seminal ideas behind quantum key distribution, wherein non-orthogonal states allow securely encoding private keys [25]. While initially a theoretical thought experiment, Wiesner’s work kicked off serious research efforts to devise practical quantum money schemes. It demonstrated how quantum physics could enable fundamentally new approaches to issues like counterfeiting prevention - applications that are only now becoming technologically feasible decades later with improvements in quantum control and devices. Overall, Wiesner’s paper pioneered the study of using quantum effects for tasks like encryption, key distribution and monetary systems. It served as a prescient early thought experiment that remains highly influential on modern developments in quantum technologies and their information-theoretic applications. Blockchain technology and cryptocurrencies, starting with Bitcoin in 2009, have revolutionized digital finance and payments [26]. By leveraging distributed ledger systems secured through cryptography, blockchains enable decentralized networks for value exchange without reliance on centralized intermediaries like banks. This ushered in new possibilities for trustless, permissionless value transfer across borders via digital assets. It appealed strongly to a vision of open, democratic money for the internet age. Crypto also spurred innovation through technologies like smart contracts and decentralized applications [27]. Within just over a decade, blockchain networks and cryptocurrencies grew exponentially in users, applications and total value. Bitcoin in particular saw rising adoption as an alternative currency and speculative asset [28]. This rapid rise disrupted traditional finance and forced rethinking of money and its underlying technologies. However, most blockchains rely on asymmetric key cryptography like RSA and ECC to digitally sign transactions and protect wallet addresses/keys [29]. While sufficiently secure today, these systems will be vulnerable once quantum computers achieve substantial processing power. Specifically, algorithms like Shor’s could break the prime number factorization and discrete logarithm problems that underpin RSA and ECC [30]. This threatens the core security of blockchains by enabling forgery of signatures and theft of funds from public keys. Urgent upgrades are needed to ward off these quantum hacks. The looming quantum computing threats thus challenge the long-term viability of current blockchain systems and stored crypto value, unless they pivot to quantum-resistant cryptographic schemes still being developed. This context introduces risk and uncertainty to the technology. Blockchain technology and cryptocurrencies have introduced revolutionary changes to digital financial system in recent years [27]. The ability to conduct secure peer-to-peer value transfers without centralized intermediaries has significant implications. Blockchains provide an open, decentralized platform for developing new applications of digital assets, smart contracts, and other FinTech innovations. Cryptocurrencies like Bitcoin have also seen rising adoption as alternative payment networks and speculative assets. However, most current blockchain networks rely on encryption algorithms that are susceptible to being broken by quantum computers. In particular, public-key cryptography schemes like RSA and ECC form the backbone of how transactions, wallets, and nodes are secured on blockchains. These asymmetrical encryption algorithms work by factoring large prime numbers, but Peter Shor brilliantly devised an algorithm in 1994 that can solve this problem exponentially faster on a sufficiently powerful quantum computer [31]. Shor’s algorithm threatens to render the encryption underpinning blockchains totally insecure [30]. A quantum computer able to run Shor’s algorithm would be able to derive the private keys needed to spend cryptocurrencies from public keys, impersonate nodes on a blockchain network, and rewrite transaction histories. This applies to the signature schemes, address generation, and other core cryptographic primitives used across blockchains today from Bitcoin to Ethereum. While quantum computers with this capability may still be 10-20 years away, the long-term implications are serious. Blockchain networks, DeFi applications, and the value and integrity of held cryptocurrencies could all be jeopardized without modifications to use quantum-resistant cryptography [32]. This highlights the urgent need for the blockchain industry to start transitioning to post-quantum secure algorithms and implementations. In response to these quantum computing threats and the rise of private cryptocurrencies, central banks have begun exploring the development of their own state-backed digital currencies. Known as CBDCs, these are digital forms of fiat currency issued and governed by a country’s monetary authority [33]. Most CBDC prototypes and pilot programs utilize a centralized database or distributed ledger architecture rather than a public blockchain [34]. This model gives central banks full control over key functions like money issuance, transaction management, interest rates, and compliance. Proponents argue CBDCs can provide many of the benefits of digital cash while preserving national monetary sovereignty. Some key benefits central banks hope to achieve with CBDCs include increasing payment efficiency, reducing transaction costs, expanding financial inclusion, and facilitating fiscal stimulus measures [35]. It also gives them a secure alternative to private cryptocurrencies at a time when quantum computers may undermine current blockchain networks. Countries like China are already testing CBDC projects aimed at replacing cash usage [36]. Meanwhile, the blockchain industry is proactively developing solutions to secure digital assets against quantum attacks. This involves transitioning blockchain networks, addresses, and cryptography to post-quantum secure algorithms theorized to withstand even fault-tolerant quantum computers [37]. Standards are being proposed for quantum-resistant signature schemes, zero-knowledge proofs, and other primitives. Overall, both central banks and the blockchain industry see the need to evolve for the quantum era. CBDCs aim to provide quantum-secure centralized digital fiat money. And continuous work on post-quantum cryptography seeks to future-proof decentralized assets on blockchains and allow them to coexist alongside CBDCs. As the capabilities of quantum computers continue advancing, it raises the crucial question of how digital currencies might evolve in response. Will there be space for both centralized CBDCs controlled by central banks as well as decentralized cryptocurrencies powered by blockchain technology to coexist? Or will one dominate the other as quantum computing disruptions play out? There are reasonable arguments on both sides. On one hand, central banks may prefer the sovereignty and oversight afforded by CBDCs, pushing for them to become the dominant digital legal tender. Powerful governments could potentially restrict or ban private cryptocurrencies in such a scenario. However, blockchain networks are also proactively working to bolster security and make cryptocurrencies quantum-resistant. Demand for decentralized, censorship-resistant assets like Bitcoin remains strong among many users as well. This suggests cryptocurrencies may find ongoing utility and market demand even in a post-quantum world. Some possibilities for coexistence could emerge if central banks integrate certain blockchain or distributed ledger features into CBDCs to gain efficiencies, while maintaining centralized issuance controls. Hybrid public-private models may also see central bank-backed digital tokens running on permissioned blockchain platforms. Overall, how dominant each model becomes will likely depend on evolving technical standards as well as policy priorities around financial sovereignty, inclusion, and innovation. New collaborative frameworks between government monetary authorities and the cryptocurrency industry may also need to be explored to tap their respective advantages for users."
https://arxiv.org/html/2411.06350v1,AMAZE: ccelerated iMC Hardware rchitecture for ero-Knowledge Applications on the dge,"Collision-resistant, cryptographic hash (CRH) functions have long been an integral part of providing security and privacy in modern systems. Certain constructions of zero-knowledge proof (ZKP) protocols aim to utilize CRH functions to perform cryptographic hashing. Standard CRH functions, such as SHA2, are inefficient when employed in the ZKP domain, thus calling for ZK-friendly hashes, which are CRH functions built with ZKP efficiency in mind. The most mature ZK-friendly hash, MiMC, presents a block cipher and hash function with a simple algebraic structure that is well-suited, due to its achieved security and low complexity, for ZKP applications. Although ZK-friendly hashes have improved the performance of ZKP generation in software, the underlying computation of ZKPs, including CRH functions, must be optimized on hardware to enable practical applications. The challenge we address in this work is determining how to efficiently incorporate ZK-friendly hash functions, such as MiMC, into hardware accelerators, thus enabling more practical applications. In this work, we introduce AMAZE, a highly hardware-optimized open-source framework for computing the MiMC block cipher and hash function. Our solution has been primarily directed at resource-constrained edge devices; consequently, we provide several implementations of MiMC with varying power, resource, and latency profiles. Our extensive evaluations show that the AMAZE-powered implementation of MiMC outperforms standard CPU implementations by more than 13\times. In all settings, AMAZE enables efficient ZK-friendly hashing on resource-constrained devices. Finally, we highlight AMAZE’s underlying open-source arithmetic backend as part of our end-to-end design, thus allowing developers to utilize the AMAZE framework for custom ZKP applications.","As data privacy and security have been more of a concern in the past decade, the concept of privacy-preserving computation, which enables computation to be performed on encrypted data, has been introduced as a paradigm shift in computing. Specifically, zero-knowledge proofs (ZKPs), a privacy-preserving cryptographic primitive, allow users to prove certain attributes about their private data without revealing anything about the data. Although ZKPs in their current state of implementation on software have proven to be effective in many applications, such as authentication (37; 35), healthcare (43; 26), and emerging learning paradigms (27; 36; 46), designing ZKP applications requires careful software/algorithm co-design to ensure that computation achieves practical runtimes and resource utilization on modern systems. As computational overhead is the main challenge when building practical zero-knowledge systems, there has been a recent emergence of research and development on ZKP hardware accelerators (38; 49). While these accelerators have proven to be effective, they often target FPGA or ASIC devices with high computational power or a large amount of resources. ZKPs have been shown to be a very valuable primitive in the IoT (24; 20) and other edge computing workflows (47), which often perform computation on resource-constrained devices. These use cases further motivate the importance of ZKP hardware acceleration while introducing a novel challenge: catering custom ZKP hardware to resource-constrained edge devices. Before an end-to-end accelerator can be built to maximize efficiency with limited resources, certain underlying modules must be built and highly optimized for area and latency. In this work, we focus on two core computational building blocks for the seminal ZKP constructions - collision-resistant, cryptographic hash functions and Galois/finite field arithmetic. Collision-resistant, cryptographic hash (CRH) functions have served as a powerful tool to enable secure computation and storage in modern systems. Certain constructions of zero-knowledge proof (ZKP) protocols, such as zk-STARKs (15) and zk-SNARKs (41), utilize CRH functions to perform cryptographic hashing for various applications. Traditional NIST-approved CRH functions, such as SHA-2 and SHA-3, have been proven to be secure through extensive studies and applications. There have been comprehensive efforts towards the thorough design of hardware and software to ensure their efficiency (44). However, efficiency in the ZKP domain is dependent on several factors that are not accounted for in our current systems, such as algebraic structure and multiplicative complexity. These standard CRH functions have proven to be inefficient when translated to the ZKP domain, thus calling for ZK-friendly hashes, which are CRH functions built with ZKP efficiency in mind (31). In this work, we consider MiMC, the first and most mature ZK-friendly hash (9), consisting of a block cipher and hash function with a simplified algebraic structure that was originally designed for zk-SNARKs, but has found use in zk-STARK applications as well (17). While the algebraic structure of MiMC is relatively simple, the underlying arithmetic structure is a large Galois prime field, which typically requires computation to be done on 254-bit integers. We do note that the size of the prime field is dependent on the ZKP construction, but typically nothing less than 128 bits would be used for security purposes. Nonetheless, efficient prime field arithmetic is a very challenging task to do on resource-constrained devices, such as select FPGAs, as the hardware modules for performing fast arithmetic typically only support 16 to 27-bit arithmetic. To address this, we present AMAZE, an accelerated hardware architecture that enables underlying fast and resource-efficient Galois field arithmetic for the MiMC hash function on resource-constrained edge devices. This work provides an accessible solution for developers and businesses to incorporate the MiMC hash function on low-end FPGAs for custom zero-knowledge applications. In short, our contributions are as follows: • We propose AMAZE, a highly-optimized hardware architecture framework for computing the MiMC block cipher and hash function, a core operation in zero-knowledge proofs, on FPGA. AMAZE is designed to support resource-constrained edge devices, while still outperforming CPU. • Our open-source implementation111https://github.com/ACES-STAM/AMAZE is parameterizable to balance power, resource utilization, and latency based on the available resources, without sacrificing the security of the MiMC hash function. This is done through our novel design of the well-established Russian Peasant and Barrett modular multiplication schemes. We provide an open-source implementation of optimized Galois field arithmetic library that is compatible with the BN254 elliptic curve, a commonly used elliptic curve in zk-SNARKs. • Our extensive evaluations show that our novel, fully pipelined implementation, which uses Barrett Reduction for modular multiplication, achieves more than 13\times speedup (for one block cipher invocation or one hash round) when compared to state-of-the-art MiMC software running on a server-grade CPU. This performance achieves relatively low power consumption on a low-end FPGA with limited resources, highlighting the feasibility of AMAZE on resource-constrained edge devices for zero-knowledge proof applications."
https://arxiv.org/html/2411.06239v1,Web Scale Graph Mining for Cyber Threat Intelligence,"Defending against today’s increasingly sophisticated and large-scale cyberattacks demands accurate, real-time threat intelligence. Traditional approaches struggle to scale, integrate diverse telemetry, and adapt to a constantly evolving security landscape. We introduce Threat Intelligence Tracking via Adaptive Networks (TITAN), an industry-scale graph mining framework that generates cyber threat intelligence at unprecedented speed and scale. TITAN introduces a suite of innovations specifically designed to address the complexities of the modern security landscape, including: (1) a dynamic threat intelligence graph that maps the intricate relationships between millions of entities, incidents, and organizations; (2) real-time update mechanisms that automatically decay and prune outdated intel; (3) integration of security domain knowledge to bootstrap initial reputation scores; and (4) reputation propagation algorithms that uncover hidden threat actor infrastructure. Integrated into Microsoft Unified Security Operations Platform (USOP), which is deployed across hundreds of thousands of organizations worldwide, TITAN’s threat intelligence powers key detection and disruption capabilities. With an impressive average macro-F1 score of 0.89 and a precision-recall AUC of 0.94, TITAN identifies millions of high-risk entities each week, enabling a 6x increase in non-file threat intelligence. Since its deployment, TITAN has increased the product’s incident disruption rate by a remarkable 21\%, while reducing the time to disrupt by a factor of 1.9x, and maintaining 99\% precision, as confirmed by customer feedback and thorough manual evaluation by security experts—ultimately saving customers from costly security breaches.","In today’s cybersecurity landscape, threat actors continuously evolve their techniques to infiltrate networks by leveraging a vast array of interconnected infrastructure. This has created an urgent demand for high-quality, real-time threat intelligence (TI). However, traditional TI approaches often struggle to scale, relying on manual investigation, signature matching, static analysis, and behavioral monitoring (Networks, 2024; Security, 2024; VirusTotal, 2024). These methods are further hindered by their siloed nature, lacking broader context across the entire enterprise security landscape, resulting in a fragmented view of threat actor infrastructure (Chau et al., 2011; Tamersoy et al., 2014). Unified security operation platforms platforms, such as Microsoft USOP, are uniquely positioned to break down these silos by acting as the centralized security hub. These platforms aim to enhance efficiency and effectiveness by correlating alerts across first and third party security products, such as endpoint, email, and identity, into cohesive security incidents (Einav, 2023; Freitas and Gharib, 2024). With TITAN, we advance Microsoft USOP threat intelligence capabilities by introducing a real-time, dynamic TI graph that captures the complex relationships between millions of entities, incidents, and organizations, providing a unified view of threat activity. By infusing this graph with security domain knowledge and leveraging a guilt-by-association framework (Koutra et al., 2011), we propagate reputation scores to unknown entities, enabling early detection and disruption (i.e., pre-damage mitigation) of threat actor infrastructure. Figure 1. Overview of the TITAN architecture: an industry-scale graph mining framework that generates real-time TI by propagating reputation scores across millions of interconnected entities, incidents, and organizations. Built on a time evolving 5-partite graph, the system operates through four key components: (1) dynamic graph construction and updates, (2) integration of known TI and security domain knowledge to bootstrap reputation scores for unknown entities; (3) reputation propagation to iteratively update risk scores; and (4) model calibration to probabilistically align scores for use by security analysts. Threat intelligence at scale. Generating scalable and accurate threat intelligence presents multiple unique and exciting challenges: (1) Evolving threat environment. Adversaries continually evolve their tactics and infrastructure, creating a rapidly shifting threat landscape. Generating up-to-date intelligence while identifying and pruning stale data is a substantial challenge. (2) Complex security landscape. The vast array of commercial security products, each with thousands of custom and built-in detection rules, creates an intricate and fragmented enterprise environment. Integrating diverse security telemetry into a unified TI framework requires careful application of domain knowledge to ensure accurate and meaningful insights. (3) Scalable and robust architecture. Modern security systems generate enormous volumes of alerts across interconnected domains such as network, cloud, endpoint, and email. Scaling to analyze millions of entities and terabytes of data in real time demands a robust, low-latency, and efficient architecture. The emergence of USOP as a relatively new industry underscores the timeliness of these challenges and positions scalable threat intelligence as a pivotal frontier in cybersecurity. Innovative solutions that drive real-time TI generation will be essential to safeguarding organizations against continuously evolving threats. 1.1. Contributions We introduce TITAN (Figure 1), a novel framework designed to address the challenges of generating high-fidelity threat intelligence at scale and in real time. Our framework makes significant contributions in the following areas: • TITAN architecture. TITAN transforms the cybersecurity industry’s approach to threat intelligence by introducing advanced methods for real-time, large-scale TI generation. Key innovations include: (1) dynamic k-partite graph that captures complex relationships between entities, incidents, and organizations; (2)the integration of security domain knowledge to bootstrap initial reputation scores; (3) reputation propagation algorithms to uncover hidden threat actor infrastructure; and (4) model calibration to probabilistic align reputation scores. We also disclose key architectural design elements and operational processes, setting a precedent as the first USOP cybersecurity company to openly discuss advanced TI capabilities in such comprehensive detail. • Extensive Evaluation. We conduct a comprehensive evaluation of TITAN’s performance across three key pillars: internal assessments, collaborations with security experts, and customer feedback. In internal testing on hundreds of thousands of held out entities, TITAN achieves an impressive average cross-region macro-F1 score of 0.89 and a precision-recall AUC of 0.94. • Impact to Microsoft Customers and Beyond. TITAN is integrated into Microsoft USOP, a market leader (Mellen et al., 2024), deployed across hundreds of thousands of organizations worldwide. Each week, TITAN identifies millions of high-risk entities, enabling a 6x increase in non-file threat intelligence. This research has transformed the product’s approach to detection and disruption, increasing the overall incident disruption rate by 21\% while reducing the time to disrupt by a factor of 1.9x—saving customers from costly breaches. Collaboration with Microsoft security research experts and feedback from customers further validates the effectiveness of our TI, demonstrating 99\% precision in attack disruption scenarios. Term Definition Alert Potential security threat that was detected Detector A security rule or ML model that generates alerts Entity File, IP, etc. evidence associated with an alert Correlation A link between two alerts based on a shared entity Incident Related alerts that are correlated together Organization Company containing a USOP product Disrupted Early threat mitigation (e.g., disable user) Reputation Likelihood of an entity being malicious or benign USOP Unified Security Operations Platforms (USOP) are used to protect organizations across the entire 1st and 3rd party enterprise landscape Table 1. Terminology and definitions."
https://arxiv.org/html/2411.06221v1,Smart-LLaMA: Two-Stage Post-Training of Large Language Models for Smart Contract Vulnerability Detection and Explanation,"With the rapid development of blockchain technology, smart contract security has become a critical challenge. However, existing smart contract vulnerability detection methods face three main issues: (1) Insufficient quality and comprehensiveness of datasets, due to the lack of detailed explanations and precise vulnerability locations in current datasets. (2) Limited adaptability of large language models (LLMs) to the smart contract domain, because most LLMs are typically pre-trained on vast amounts of general text data but very little smart contract-specific data. (3) Lack of high-quality explanations for detected vulnerabilities, as most existing methods focus solely on detection without providing clear explanations for their results. These limitations significantly hinder detection performance and make it harder for developers to understand and fix vulnerabilities quickly, potentially leading to severe financial losses. To address these problems, we propose Smart-LLaMA, an advanced detection method based on the LLaMA language model. First, we construct a comprehensive dataset covering four vulnerability types with labels, detailed explanations, and precise vulnerability locations. Second, we introduce Smart Contract-Specific Continual Pre-Training, using raw smart contract data to enable the LLM to learn smart contract syntax and semantics, thereby enhancing their adaptability to the smart contract domain. Furthermore, we propose Explanation-Guided Fine-Tuning, a novel approach that fine-tunes the LLM using paired vulnerable code and explanations, enabling it to both detect vulnerabilities and provide reasoned explanations for its results. To evaluate the quality of generated explanations, we employ both LLM evaluation and human evaluation, focusing on three key aspects: Correctness, Completeness, and Conciseness. Experimental results show that Smart-LLaMA outperforms state-of-the-art baselines, with average improvements of 6.49% in F1 score and 3.78% in accuracy, while providing reliable explanations. We have made all models, datasets, and code available.","The advent of blockchain technology has seen rapid adoption across various sectors, driven by its decentralized architecture [1]. This innovative technology enables the creation of secure, distributed digital ledgers for recording transactions [2]. Utilizing advanced cryptographic methods, blockchain ensures the integrity and verification of each transaction, establishing itself as a highly reliable technological framework [3, 4]. Within this ecosystem, smart contracts function as self-executing programs on the blockchain, automating the management of digital assets such as cryptocurrencies. These contracts activate when specific conditions are met and, once deployed, become permanent fixtures on the blockchain [5]. However, the immutable nature and inherent complexity of smart contracts present significant security challenges [5]. The well-documented DAO incident [6, 7] serves as a cautionary tale, illustrating the potential severity of such vulnerabilities. This security breach resulted in the unauthorized diversion of Ethereum valued at $60 million, causing widespread disruption within the blockchain community [8, 9]. This event underscores the critical importance of enhancing smart contract security to prevent similar devastating outcomes in the future. Researchers have developed various techniques to identify vulnerabilities in smart contracts, each addressing different aspects of the challenge but also facing limitations. Symbolic execution tools like Oyente [10], Mythril [11], Osiris [12], and Manticore [13], as well as static analysis tools such as Slither [14] and SmartCheck [15], rely on predefined patterns to detect vulnerabilities. However, these methods often struggle with complex scenarios and lack generalizability. We conducted a detailed survey of existing smart contract vulnerability datasets as shown in I, evaluating multiple datasets including A [16], B [17], C [18], and D [19], and found significant limitations. These datasets typically provide only basic vulnerability labels, lacking detailed explanations and precise location information. They cover a limited range of vulnerability types, usually only 1 to 3, failing to represent the diverse potential security risks in smart contracts. This simplified labeling approach severely constrains models’ ability to comprehensively understand and detect complex vulnerability patterns. These limitations directly affect the learning effectiveness of detection models, potentially leading to questionable accuracy and reliability in detection results. Some more advanced methods have attempted to address these limitations. Clear [20] employs a Contrastive Learning (CL) model to capture complex inter-contract relationships, while Zhuang et al. [17] and Luo et al. [21] introduce graph neural network-based approaches to represent smart contracts. However, the complexity of these graph structures makes them difficult to reproduce and less effective in representing programs accurately. Peculiar [16] and PSCVFinder [18] take a different approach by fine-tuning pre-trained models for vulnerability detection. While innovative, these methods still struggle to provide clear explanations for their detections, which is crucial for practical usage. Given these limitations, researchers have begun to explore the potential of using general-purpose Large Language Models (LLMs) to address smart contract vulnerability detection issues. General-purpose LLMs show promise in adapting to new patterns [22, 23]. However, they often struggle with smart contract-specific concepts and security implications. As illustrated in Figure 1, when presented with a smart contract, a general-purpose LLM like LLaMA-3.1-8B-Instruct incorrectly identifies a non-existent reentrancy vulnerability. It misinterprets the implications of external calls in the ’gotake()’ function, failing to recognize that reentrancy vulnerabilities typically arise when contract state or balance changes occur after external calls, which is not the case in this contract. To address these challenges, we propose our Smart-LLaMA, built upon the LLaMA-3.1-8B model. To overcome the limitations of existing datasets, we construct a comprehensive smart contract vulnerability dataset with detailed explanations and precise location information, covering four vulnerability types. This dataset is constructed through a three-step process: automated generation, LLM-based evaluation, and human expert verification and refinement. Specifically, we utilize the largest parameter versions of state-of-the-art LLMs (Qwen2 and Mistral-Large) to generate detection results, explanations, and specific vulnerability locations through carefully designed prompts. Llama-3.1-70B-Instruct serves as a judge model, evaluating these explanations on correctness, completeness, and conciseness. It scores each aspect from 1 to 10 to select the highest-quality explanations. Finally, human experts review the selected high-scoring explanations, verify their accuracy and make necessary improvements. This approach addresses the issue of insufficient dataset quality and comprehensiveness in existing resources. Furthermore, we introduce Smart Contract-Specific Continual Pre-Training to enhance the model’s understanding of smart contract-specific syntax structures and vocabulary, thereby improving the adaptability of LLaMA-3.1-8B to the smart contract domain. This process involves exposing the model to a large corpus of original smart contract code, allowing it to learn the nuances and intricacies of smart contract development. Additionally, we propose Explanation-Guided Fine-Tuning, a novel approach utilizing our constructed smart contract vulnerability explanations to fine-tune the large language model. This process enables the model to comprehend the entire vulnerability detection process. By training on datasets pairing vulnerable code with detailed explanations, Smart-LLaMA learns to both identify vulnerabilities and articulate the reasoning behind its detections. To evaluate the quality of explanations generated by our Smart-LLaMA, we utilize both LLM evaluation and human evaluation. Our evaluation is based on three key dimensions: Correctness, Completeness, and Conciseness, each scored on a 4-point Likert scale [24]. For LLM evaluation, we utilize Llama-3.1-70B-Instruct, carefully designing prompts to guide the model in assessing explanations based on these criteria. For human evaluation, we invite four experienced smart contract security experts. Each expert dedicate 8 hours to the assessment process, resulting in a total of 32 hours of in-depth analysis. The experts use the same 4-point Likert scale [24]. To ensure consistency, we arrange for 20% overlapping evaluation samples. We then tabulate the number of explanations receiving each score (1-4) for each dimension, providing a clear distribution of the quality assessments for both the baseline (LLaMA-3.1-8B-Instruct) and our Smart-LLaMA approach. We evaluated our Smart-LLaMA framework on a challenging dataset [19] encompassing four major vulnerability types: reentrancy, timestamp dependency, integer overflow/underflow, and delegatecall. The results demonstrated that Smart-LLaMA significantly outperformed state-of-the-art methods across all vulnerability types. Notably, Smart-LLaMA achieved F1 scores 7.35%, 1.24%, 7.82%, and 9.55% higher for reentrancy, timestamp dependency, integer overflow/underflow, and delegatecall vulnerabilities compared to the previous best performers. In terms of accuracy, Smart-LLaMA surpassed the previous SOTA methods by 4.14%, 0.62%, 4.83%, and 5.53% for these four vulnerability types. In addition to detection performance, we evaluated the quality of vulnerability explanations generated by Smart-LLaMA. Both LLM evaluation and human evaluation demonstrated that Smart-LLaMA produced more accurate, comprehensive, and concise explanations compared to LLaMA-3.1-8B-Instruct. For instance, in the human evaluation, Smart-LLaMA achieved the highest score (4 out of 4) for correctness, completeness, and conciseness in 69.5%, 57.1%, and 65.6% of cases, respectively, significantly outperforming the baseline method. The main contributions of this paper are as follows: • We propose Smart-LLaMA, a novel method combining smart contract-specific pre-training and explanation-guided fine-tuning for smart contract vulnerability detection, achieving state-of-the-art performance on four main vulnerability types. • We construct a high-quality smart contract vulnerability dataset that not only provides label, but also includes detailed vulnerability explanations, overcoming the limitations of existing datasets. • To the best of our knowledge, we are the first to explore explanation quality in smart contract vulnerability detection. We validate Smart-LLaMA’s effectiveness in generating high-quality explanations through both LLM evaluation and human evaluation. We have made all source code and datasets utilized in this research available to the public at https://zenodo.org/records/13860344"
https://arxiv.org/html/2411.06172v1,IDU-Detector: A Synergistic Framework for Robust Masquerader Attack Detection,"In the current digital age, users store their personal information in corporate databases to access services, making data security and sensitive information protection central to enterprise security management. Given the extensive attack surface, system assets continuously face cyber security challenges such as weak authentication, exploitation of system vulnerabilities, and malicious software. Through specific vulnerabilities, attackers may gain unauthorized system access, masquerading as legitimate users, and remaining hidden. Successful attacks can lead to the leakage of user privacy, disruption of business operations, significant financial losses, and damage to corporate reputation. The increasing complexity of attack vectors is blurring the boundaries between insider and external threats. To address this issue, this paper introduces the IDU-Detector, an innovative threat detection framework that strategically integrates Intrusion Detection Systems (IDS) with User and Entity Behavior Analytics (UEBA). This integration aims to monitor unauthorized access and malicious attacks within systems, bridging functional gaps between existing systems, ensuring continuous monitoring and real-time response of the network environment, and enhancing their collective effectiveness in identifying security threats. Additionally, the existing insider threat datasets exhibit significant deficiencies in both depth and comprehensiveness, lacking sufficient coverage of diverse attack vectors. This limitation hinders the ability of insider threat detection technologies to effectively address the growing complexity and expanding scope of sophisticated attack surfaces. To address these gaps, we propose new, more enriched and diverse datasets that includes a wider range of attack scenarios, thereby enhancing the adaptability and effectiveness of detection technologies in complex threat environments. We tested our framework on different datasets, the IDU-Detector achieved average accuracy rates of 98.96% and 99.12%. These results demonstrate the method’s effectiveness in detecting masquerader attacks and other malicious activities, significantly improving security protection and incident response speed, and providing a higher level of security assurance for asset safety.","In the modern era, characterized by rapid technological advancement and accelerated digital transformation, enterprises and organizations increasingly depend on complex information systems to manage their critical business operations and sensitive data. Within this context, cybersecurity emerges as a crucial challenge for protecting corporate assets and ensuring the continuity of business operations [r1_1rawal2022identifying, r1_2jo2019mauth]. This challenge extends to various domains, including emerging technologies like federated learning systems and cloud robotics, where data privacy and security are paramount [liu2019federated, liu2019lifelong, liu2021peer] Internet-borne security threats, prevalent in today’s connected world, can be categorized into external and insider threats. Notably, insider threats pose the most harmful risk to corporate and organizational assets. These threats originate from malicious users who possess legitimate access rights. Typically, such individuals are well-acquainted with the organizational information system architecture and sensitive data access [r1_3alotibi2019feasibility, zhang2022authros], enabling them to circumvent traditional security measures relatively easily. They exploit the trust and permissions granted to legitimate users to perform operations that could cause significant, often irreversible, damage to the organization’s infrastructure and data integrity. As a result, insider threat represent a primary challenge to asset security. Conversely, external threats are usually initiated by unauthorized attackers who attempt to gain legitimate access to systems, a process known as intrusion. The successful execution of such attacks often allows attackers to acquire access rights, subsequently elevating their status to that of insiders. This malicious privilege escalation is particularly perilous because once external attackers gain access, they can operate within the network with minimal suspicion, accessing sensitive information and critical systems undetected. TABLE I: Cyber Attack Vectors and Their Implications on System Security Attack Vector Definition Impact/Purpose Pathway to Obtain Legitimate Access Classification Probe Attackers use scanning or probing techniques to gather information about the target system or network, such as vulnerabilities, configurations, open ports, and available services. Preparation for subsequent attacks (e.g., privilege escalation, data theft, DoS attacks). Probing itself does not necessarily compromise the system, but it serves as the initial step for more severe attacks, posing a potential threat to system security. Probing does not directly obtain access rights but provides pathways for subsequent attacks. Potential Intruder U2R Attackers exploit system vulnerabilities or privilege escalation techniques to elevate privileges from a regular user (low privilege) to root or administrator (high privilege). By escalating privileges, attackers gain full control over the system, allowing them to perform high-privilege operations such as modifying configurations, installing software, or accessing sensitive data. Directly exploits vulnerabilities to achieve privilege escalation, transitioning from low to high privilege. Intruder R2L Attackers remotely gain access to local system permissions through network attacks, typically by stealing credentials or exploiting vulnerabilities. Obtains local user access from a remote position, enabling further malicious activities within the system. Gains local access through remote vulnerabilities or credential theft. Intruder Attackers employ a variety of methods to gain legitimate access to user accounts, primarily through system intrusions or by exploiting system vulnerabilities. These intrusions, particularly those utilizing User to Root (U2R) and Remote to Local (R2L) attack vectors, aim to acquire legitimate user permissions. U2R attacks typically commence with the attacker exploiting vulnerabilities in ordinary user accounts on the target system to gain root or administrative privileges. These attacks leverage flaws in the operating system, applications, or scripts that grant elevated privileges, thus allowing unauthorized access to administrative controls. In contrast, R2L attacks involve attackers sending packets from a remote location to exploit vulnerabilities in a system where they do not have an account, achieving local user-level access. This access enables attackers to reach confidential data. Following network-based intrusions, attackers often remain covert within the system, masquerading as legitimate users [r1_6yuan2021deep]. These masqueraders typically exhibit two distinct behavioral patterns: one involves rapidly damaging the system upon gaining access, while the other entails creating backdoors to stay hidden within the system, posing as legitimate users and waiting for an opportune moment to act maliciously [yadav2015technical, r1_5aldweesh2020deep, sommer2010outside]. To further elaborate on the related intrusion methods and the threats they pose, We have summarized the complex strategies employed by intruders as they transition from external to insider domains, as detailed in Table I. As the attack surface continues to expand, encompassing the advanced intrusion methods outlined in Table I, current technologies are increasingly inadequate to address the evolving complexities and breadth of these sophisticated attack vectors. The underlying reasons can be traced back to the datasets. Existing insider threat datasets primarily focus on the privileges of legitimate users[bin2022insider]. However, they lacked the more complex and evolving threats activity data posed by external attackers who gain legitimate user access through advanced exploitation techniques. This oversight creates a significant gap, as the boundary between external and insider threats becomes increasingly blurred due to the rising sophistication of attacks that originate externally but manifest as insider threats once legitimate access is compromised. Specifically, current insider threat datasets often fail to represent attack vectors involving Probe, U2R, and R2L, as highlighted in Table I (Notably, in Table I, the classification labels ’Potential Intruder’ and ’Intruder’ are used to represent ’users who may transition from being external threats to insider threats’ and ’users who have already transitioned from being external threats to insider threats,’ respectively). The pathways that allow attackers to transition from external to insider positions—such as remote access exploitation or privilege escalation—are critical for understanding the full impact of these threats. By neglecting to include these transition points, existing datasets do not provide the activity data of how attackers leverage external vulnerabilities to achieve internal control, ultimately leaving substantial gaps in detection and prevention strategies. The lack of comprehensive data on how external threats evolve into insider threats severely limits the practical application of these datasets in enterprise security. In real-world environments, the ability to anticipate and recognize the fluid nature of threats that traverse external and internal boundaries is crucial. Without addressing these gaps, current insider threat datasets fall short of enabling security solutions that truly reflect the complex and interconnected nature of modern cyber threats. To ensure the security of sensitive data and the continuity of business operations, institutions often implement Identity and Access Management (IAM) [r1_7pal2019limitations] and Privileged Access Management (PAM) [r1_8tep2015taxonomy] systems. These systems provide unified management of legitimate personnel access, ensuring the secure access to necessary resources while preventing unauthorized entries. A key component of IAM is UEBA [r1_9diop2021high], which assists in detecting anomalous behaviors among legitimate users to prevent damage to system assets. However, current UEBA technologies often struggle to differentiate between masqueraded and legitimate user behaviors, making masquerade attacks particularly challenging to detect. The evolution of these threats necessitates a detection strategy that not only identifies intrusions but also tracks the transition of these threats from external to internal. An effective detection and response system is crucial not only for quickly recognizing these changes, but also for mitigating risks before substantial damage occurs. This underscores the urgent need for continuous monitoring and advanced analysis of threats. Some practical applications of machine learning techniques [liu2017singular, liu2020experiments] and recent developments in distributed and collaborative systems have introduced new challenges and opportunities for cybersecurity, particularly in areas such as cloud robotics and federated learning [liu2022elasticros, liu2023roboec2, yan2021fedcm, zhang2022authros, liu2024edgeloc, zheng2022applications]. Such systems are essential for maintaining security and are also critical for enhancing the resilience of organizational IT ecosystems against increasingly dynamic and covert cyber threats. Numerous threat detection methodologies are currently employed, yet the effectiveness of these existing solutions is undermined by several critical issues: 1. Existing methodologies face significant challenges in accurately distinguishing between normal user behaviors and those that are intentionally masqueraded [r17_2khanna2021using, meng2018deep, sharma2020user, yuan2021deep]. This difficulty arises from the complexity involved in analyzing diverse behavioral patterns, including contextual, sequential, and temporal behavior patterns. 2. The current research landscape has largely neglected the potential insider threats posed by Privilege Escalation Attacks and Remote Exploit Attacks [chen2014study]. Existing methodologies for insider threat detection predominantly focus on monitoring authorized user entities, thereby overlooking the significant risks associated with unauthorized entities that may execute external attacks to usurp legitimate access rights [verizon2008data, r17_1yuan2021deep]. 3. Existing methods are difficult to detect rare types of attacks. In real network environments, there are often characteristics of diversified attack methods and attack characteristics that are not easy to identify, therefore these methods tend to have a high false alarm rate and cannot be put into practical production applications [r11li2021sustainable, r7_2aburomman2016novel]. 4. Amid the growing complexity of attacks that blur the lines between insider and external threats, current insider threat datasets exhibit significant limitations, particularly in terms of their breadth and depth [bin2022insider, li2020deepfed]. Specifically, they overlook the critical pathways through which intruders and potential intruders transition from external probing and escalation to becoming full-fledged insider threats, as detailed in Table I. While existing insider threat datasets provide foundational data support for threat detection, they fall short in terms of depth and comprehensiveness. Advanced Persistent Threats often involve attackers gaining access and remaining dormant for extended periods, awaiting the most opportune moment to act. This behavior is often underrepresented in existing datasets, which tend to focus more on immediate attack activities rather than long-term threats. This inadequacy hampers the ability of insider threat detection technologies to keep pace with the evolving complexity and expanding scope of these sophisticated attack vectors [yadav2015technical, sommer2010outside]. To address these issues, we proposed the IDU-detector to enhance the detection capabilities for insider threat. We propose the DenseAttDNN Classifier, which includes dense connection and attention mechanism, termed DenseAttDNN. Within the classifier, dense connection promotes feature reuse, while the attention mechanism enables the model to focus on more crucial feature information. Therefore, this framework improves the model’s detection capability, particularly for rare class attacks. In summary, our contributions can be listed as follows: 1. We have proposed a novel framework aimed at detecting both insider and external security threats, tracking their transition from external to internal sources, and possessing specific capabilities for identifying intruder and potential intruder. The effectiveness of our approach has been validated using the datasets we proposed in the cybersecurity domain. 2. Our work effectively detects privilege escalation attacks and remote exploit attacks, overcoming the limitations of traditional insider threat detection techniques that are constrained to monitoring legitimate users. Furthermore, it addresses the challenges posed by unauthorized attackers exploiting system vulnerabilities to escalate privileges. By doing so, our approach provides a more comprehensive and precise defense for system security, filling gaps in existing technologies and significantly enhancing the detection and mitigation of complex threats. 3. We evaluate our model on the synergistic datasets. The experimental results show that our method has excellent classification performance, a faster response and a better performance compared with the existing methods. Also, it successfully balances accuracy, time, and computational cost and addresses the gap where the traditional approaches only seek to optimize or make the best use of resources without considering the characteristics of the real-world detection requirements. 4. To enhance the capabilities of insider threat detection technologies in addressing the evolving complexity and expanding scope of sophisticated attack vectors, we provided datasets that encompass a broader range of threat scenarios. The integration of such datasets will not only more accurately capture the dynamic evolution of current threat landscapes but also aid organizations in building more robust defensive frameworks. Therefore, we propose KDD-UEBA, NSL-UEBA, CIC-UEBA, and KDD-UNBLogs as benchmark datasets for insider threat detection models. These datasets are designed to effectively support deep learning models in accurately identifying insiders, potential insider threats, and intrusion activities. For further details on these datasets. More details are shown in the Table XIV (Table XIV in Appendix B)."
https://arxiv.org/html/2411.06137v1,A Sharded Blockchain-Based Secure Federated Learning Framework for LEO Satellite Networks,"Low Earth Orbit (LEO) satellite networks are increasingly essential for space-based artificial intelligence (AI) applications. However, as commercial use expands, LEO satellite networks face heightened cyberattack risks, especially through satellite-to-satellite communication links, which are more vulnerable than ground-based connections. As the number of operational satellites continues to grow, addressing these security challenges becomes increasingly critical. Traditional approaches, which focus on sending models to ground stations for validation, often overlook the limited communication windows available to LEO satellites, leaving critical security risks unaddressed. To tackle these challenges, we propose a sharded blockchain-based federated learning framework for LEO networks, called SBFL-LEO. This framework improves the reliability of inter-satellite communications using blockchain technology and assigns specific roles to each satellite. Miner satellites leverage cosine similarity (CS) and Density-Based Spatial Clustering of Applications with Noise (DBSCAN) to identify malicious models and monitor each other to detect inaccurate aggregated models. Security analysis and experimental results demonstrate that our approach outperforms baseline methods in both model accuracy and energy efficiency, significantly enhancing system robustness against attacks.","The advancement of satellite technology has driven the development of large Low Earth Orbit (LEO) satellite networks, with hundreds to thousands of satellites being launched. This trend has accelerated the commercialization of satellite-based Internet of Things (IoT) services and led to continuous upgrades in satellite technology [1]. As a result, modern satellites are now equipped with advanced cameras, processors, and antennas, enabling them to collect and process vast amounts of Earth imagery and sensor data through artificial intelligence (AI)-based solutions [2]. The traditional approach in LEO satellite networks relies on transmitting data to a central server. However, as data volumes grow, the centralized model training approach is becoming impractical due to high bandwidth costs, transmission delays, and the heightened vulnerability of satellite links compared to ground links [3]. Implementing blockchain [4] and federated learning (FL) [5] offers an effective solution to this problem. In FL, each satellite aggregates locally calculated parameters and transmits model updates instead of raw data to jointly train a global model. Blockchain, as a decentralized, immutable, and traceable technology, eliminates the necessity of a central server in FL. Through its decentralized ledger, blockchain enables FL to transparently track updates and client operations across the entire network. For large-scale satellite networks, blockchain sharding technology is applied to enhance entire system performance [6, 7]. Applying FL to LEO satellite networks still faces several challenges. One challenge is that FL assumes all distributed nodes are trustworthy, which is difficult to guarantee [8]. Another challenge arises from the short, intermittent communication windows between satellites and ground data centers, making data transmission to the ground both time-consuming and often unnecessary[9]. Several studies have been conducted to address the aforementioned challenges. For example, Wang et al. proposed a method utilizing cosine similarity (CS) to filter malicious models by measuring the differences in model features[10]. However, as the FL model converges, the CS between the local model in the current round and the global model from the previous round increases significantly. This makes it essential to establish a CS threshold for accurate model classification. Chen et al. divided the model into two categories by extracting model features and selected the model with the highest accuracy as the global model[8]. This method can mistakenly classify some benign models as malicious in the absence of attacks, causing a reduction in accuracy. Zhu et al. transmitted the models learned from satellites to the ground for model verification and aggregation [11]. Although their methods effectively resist poisoning attacks, they did not consider the issue of short communication windows between satellites and the ground. With the increasing number of satellites, there is an urgent need for efficient and decentralized solutions that can operate directly within satellite networks [12]. To address the aforementioned issues, we propose SBFL-LEO, a fully decentralized FL framework that integrates blockchain technology with sharding, enabling secure training on satellite networks. SBFL-LEO integrates CS and Density-Based Spatial Clustering of Applications with Noise (DBSCAN) to mitigate poisoning attacks. Specifically, model features are extracted by calculating the CS between each local model and the previous global model. DBSCAN then groups the models based on their CS values, automatically determining the number of clusters based on density thresholds and a minimum data point requirement. By enforcing a minimum cluster size, each cluster is limited to a maximum of two groups, enhancing resilience against poisoning attacks. The aggregated model with the highest accuracy is selected as the cluster model, effectively resisting poisoning attacks. In addition, satellites are categorized into three distinct roles, allowing the model trained by a learning satellite to be validated by a miner satellite rather than being transmitted to the ground. The main contributions of this paper are summarized as follows: • We propose SBFL-LEO, a blockchain-based federated learning framework to address the challenges of limited communication windows between satellites and ground stations while ensuring secure satellite communications. By assigning specific roles to satellites, SBFL-LEO avoids the need to transmit models to the ground for verification. • Within this framework, we introduce CS to extract model characteristics rather than explicitly rejecting models. DBSCAN is then applied to dynamically cluster models, preserving all benign models unaffected by compromise in the current round and reducing the accuracy loss caused by model rejection. • We evaluate SBFL-LEO using a real dataset and perform a thorough security analysis. Experimental results demonstrate that SBFL-LEO outperforms baseline methods in both learning accuracy and energy efficiency."
https://arxiv.org/html/2411.05982v1,Unmasking the Shadows: Pinpoint the Implementations of Anti-Dynamic Analysis Techniques in Malware Using LLM,"Sandboxes and other dynamic analysis processes are prevalent in malware detection systems nowadays to enhance the capability of detecting 0-day malware. Therefore, techniques of anti-dynamic analysis (TADA) are prevalent in modern malware samples, and sandboxes can suffer from false negatives and analysis failures when analyzing the samples with TADAs. In such cases, human reverse engineers will get involved in conducting dynamic analysis manually (i.e., debugging, patching), which in turn also gets obstructed by TADAs. In this work, we propose a Large Language Model (LLM) based workflow that can pinpoint the location of the TADA implementation in the code, to help reverse engineers place breakpoints used in debugging. Our evaluation shows that we successfully identified the locations of 87.80% known TADA implementations adopted from public repositories. In addition, we successfully pinpoint the locations of TADAs in 4 well-known malware samples that are documented in online malware analysis blogs.","Malware detection in real-world is achieved through a complex human-in-the-loop system with reverse engineers playing an essential role. In terms of quantity, the majority of the malware samples are detected and blocked by file signatures, including inline firewall blocking and anti-virus software. This is because most samples in the wild are 1-day or n-day samples, so their file signatures are already documented, either automatically or manually. Obviously, any signature-based methods are subject to 0-day malware attacks, and therefore different methods need to be adopted to detect and block 0-day malware. Although it is possible to detect 0-day malware through advanced static analysis, in practice this strategy can suffer from non-scalability and false negatives. For example, some samples are only at the initial stage of infection and will do no harm unless subsequent payloads are downloaded, which can only be confirmed to be malicious by concretely executing the samples. Therefore, many industry-level malware detection systems rely on automated dynamic analysis, or sandboxes [1, 2], to detect 0-day malware using behavior signatures and rules. Unfortunately, most malware authors are fully aware of the existence of sandboxes and dynamic analysis efforts, and therefore many of them nowadays will implement Techniques of Anti-Dynamic-Analysis (TADA) in their malware to evade dynamic analysis-based detection. Figure 1: Practical Malware Detection System As we will elaborate in Section II-A, malware authors usually have two goals with TADAs implemented: evading the dynamic analysis-based detection and impeding reverse engineering effort, respectively. These two goals are directly motivated by the process of detecting malware samples in the real world, which is illustrated in Figure 1. The top half of the figure is fully automated, and the bottom half involves human labor. When a sample arrives at the system, the first step is usually to check the existing signature and hash databases to see if the sample is already known to be malicious. If the signature or hash has a match, then the system can immediately conclude that the sample is malicious. Despite the disadvantage of the file signature-based detection (i.e. unable to handle 0-day), it is still worthwhile to have it in the detection system to block 1-day and n-day malware, because it is extremely fast and affordable. To keep the databases updated, whenever a malware verdict is generated in the later part of the system (meaning that the sample has not been seen), it will be used to update the databases. Subsequently, in cases where there is no match for the given sample, it will then be transferred to a sandbox, where the sample will be dynamically analyzed. The sample will be executed and monitored without human intervention. The sandbox analysis usually has 3 outcomes: malware verdict, benign verdict, and analysis failure. Since detection is achieved through conservative behavior heuristics and rules, the malware verdict is usually true positive. In contrast, a benign verdict can be false negative, due to either TADAs or missing OS components that cause the sample not to behave maliciously in the sandbox environment. These false negatives are usually found later, and the system will receive a verdict flip request. As for analysis failures, they are the cases where the sample failed to launch at all or crashed during the execution. The most common reason for false negative and analysis failures is the presence of TADA, as sandboxes are usually virtual machines (VM) that conduct automatic dynamic analysis. Consequently, at the point of failures or verdict flip requests, human intervention is necessary, and the sample is then sent to a reverse engineer for the final verdict. Back to the discussion of the goals of TADAs, the ultimate goal of the malware authors is to delay the detection of their malware as much as possible, so that more victims could be infected. Based on Figure 1, it is obvious that the best situation for malware authors’ interest is when a malware sample bypasses the signature database, bypasses sandboxes, and eventually reaches the hands of the reverse engineer, being analyzed for an extended period of time. Consequently, the first goal of TADAs is to bypass the sandbox detection, which is usually achieved by detecting the artifacts (e.g. analysis programs, common VM hardware names) in the sandbox. To fight back, there are numerous previous works studying how to prevent malware from bypassing the sandboxes in the first place [3, 2, 1, 4, 5, 6], most of which can be summarized in one single word: transparency. The idea is simple: as long as the sandbox is transparent (i.e., the sandbox is not discernible with the real machine), the malware will trigger malicious behaviors. As we will elaborate in Section III-A, despite the theoretical feasibility of complete transparency, in the real world pursuing transparency is essentially a trade-off between cost and transparency enhancement. Therefore, it is very common that a malware sample eventually will get to the hands of a reverse engineer, which corresponds to the second goal of TADAs: impeding the reverse engineering efforts to delay the analysis. Thus, from the reverse engineers’ perspective, the faster they can find the TADA implementation (in the code), the better so that they can bypass or patch the TADAs during their dynamic analysis. Although there are existing works studying detecting malware with TADA [6, 7, 8], most of them focus on the detection of anti-analysis and malicious behaviors rather than the problem of identifying the locations of TADAs. As shown in Figure 1, the scope of this research is not to detect the malware with TADAs (i.e., evasive malware) in general; rather we focus on complementing the current reverse engineering efforts on evasive malware, reducing the time and labor efforts of reverse engineers. Specifically, this research aims to identify the locations of the TADA in the code for breakpoints used in manual dynamic analysis (i.e. debugging) during the reverse engineering efforts. However, pinpointing the locations of TADA implementations can be challenging. First, there are a variety of TADAs: checking hardware (e.g., CPU cores, memory size, PCIe devices, power capabilities, fans, etc.), checking running processes, checking filesystems, checking user traces, etc. Second, not only can malware adopt different techniques, but also can they use different implementations. Taking the number of CPU core as an example, it can be checked via excessive number of implementations (e.g., CPUID, WMI Query, different APIs, etc.), all up to malware authors’ creativity. This diversity of the TADA implementations essentially makes the detection very prone to false negatives. In this paper, we present a workflow that leverages advanced static analysis and a Large Language Model (LLM) to pinpoint the locations of TADA in the code, given a malware binary executable. The workflow is based on two key insights. First, an LLM can be leveraged to address a primary limitation of current rule-based static analysis for TADAs. That is, the rule-based approaches are, in principle, not scalable when new TADAs and TADA implementations keep on emerging. Second, the semantic gap between disassembled code and natural languages results in unsatisfactory responses when the LLM is directly asked to examine disassembly code, which could be bridged by advanced static analysis. Our major contributions are as follows: • Propose a LLM-based workflow that can pinpoint the locations of the TADA in a binary executable automatically. • Construct the useful features that can enable the LLM to do the detection. • Evaluate our method using real-world malware samples of popular families. Specifically, we successfully identify the locations of 87.80% of known TADA implementations obtained from public repositories. In addition, we successfully pinpoint the locations of TADAs in 4 well-known malware samples that are documented in online malware analysis blogs."
https://arxiv.org/html/2411.05947v1,Ideal Pseudorandom Codes,"Pseudorandom codes are error-correcting codes with the property that no efficient adversary can distinguish encodings from uniformly random strings. They were recently introduced by Christ and Gunn [CRYPTO 2024] for the purpose of watermarking the outputs of randomized algorithms, such as generative AI models. Several constructions of pseudorandom codes have since been proposed, but none of them are robust to error channels that depend on previously seen codewords. This stronger kind of robustness is referred to as adaptive robustness, and it is important for meaningful applications to watermarking.In this work, we show the following.Adaptive robustness: We show that the pseudorandom codes of Christ and Gunn are adaptively robust, resolving a conjecture posed by Cohen, Hoover, and Schoenbach [S&P 2025]. Our proof involves several new ingredients, combining ideas from both cryptography and coding theory and taking hints from the analysis of Boolean functions.Ideal security: We define an ideal pseudorandom code as one which is indistinguishable from the ideal functionality, capturing both the pseudorandomness and robustness properties in one simple definition. We show that any adaptively robust pseudorandom code for single-bit messages can be bootstrapped to build an ideal pseudorandom code with linear information rate, under no additional assumptions.CCA security: In the setting where the encoding key is made public, we define a CCA-secure pseudorandom code in analogy with CCA-secure encryption. We show that any adaptively robust public-key pseudorandom code for single-bit messages can be used to build a CCA-secure pseudorandom code with linear information rate, in the random oracle model.Together with the result of Christ and Gunn, it follows that there exist ideal pseudorandom codes assuming the 2^{O(\sqrt{n})}-hardness of LPN. This extends to CCA security in the random oracle model. These results immediately imply stronger robustness guarantees for generative AI watermarking schemes, such as the practical quality-preserving image watermarks of Gunn, Zhao, and Song (2024).","Pseudorandom codes. Suppose that Alice wishes to send a message to Bob over some channel. The fields of cryptography and coding theory each address a different concern that Alice might have about the channel: • if the channel is untrusted in the sense that there may be an eavesdropper, then we use encryption; • if the channel is unreliable in the sense that information may be corrupted in transit, then we use error-correcting codes. If the channel is both untrusted and unreliable, there is typically no issue with combining these techniques. That is, if Alice wishes only to hide the content of her message, then she can simply error correct an encryption of her message. However, this is insufficient if Alice doesn’t want the channel to even know that communication has occurred.111This kind of covert communication is the classic problem of steganography, but classic stateless steganographic techniques fail on unreliable channels. Concretely, imagine that Alice wishes for it to appear as if her transmissions are uniformly random. What Alice needs is then robust, pseudorandom encryption — transmissions should appear random, and decryption should function even if they are corrupted. Such an object is called a pseudorandom code (PRC). More formally, a (secret-key) PRC is a keyed error correction scheme consisting of algorithms for encoding and decoding. To an adversary without knowledge of the key, an encoding oracle is computationally indistinguishable from an oracle that outputs a freshly random string on each query. With the secret key, one can decode codewords even after they are subjected to an error channel.222We are interested in error channels that introduce a constant rate of errors, and reserve the term “pseudorandom code” for schemes that handle such channels. This is the more practically relevant as well as theoretically interesting setting; it is easy to handle any sub-constant error rate using just one-way functions [GG24]. To build a PRC, the basic results of cryptography and coding theory fall short. The central difficulty is that Alice’s transmission must, on the one hand, appear completely structure-less to the channel, and on the other, appear highly redundant to Bob so that he can reliably recover the message even if the transmission is corrupted. Therefore pseudorandom codes force us to push the boundaries of both cryptography and coding theory, a tension that is reflected in the fact that all known constructions of PRCs have quasipolynomial-time distinguishing attacks [CG24, GM24, GG24]. Watermarking. PRCs are not only natural cryptographic and coding-theoretic objects, but also powerful tools for constructing watermarks for generative AI. See Section 1.2 for an explanation of how this works; for now let us just say that at a high level, this is because one can typically view generative AI models as approximate reparameterizations of “content” (e.g. images or text) into a space where the content has a natural distribution (e.g. normal or uniform). With this observation, [CG24, GM24] use PRCs to construct watermarks for language models that are undetectable in that the watermarked model is computationally indistinguishable from the original model (thereby ensuring that the watermark does not degrade generation quality), and robust in that the watermark tolerates a constant rate of errors. Prior text watermarks were either only undetectable but not robust [CGZ24], or robust but significantly altered the model’s output distribution [ZALW24, KTHL24]. Similarly, the only known quality-preserving and robust watermarks for image generation models uses a PRC. This approach was demonstrated in practice by [GZS24]. These schemes essentially embed a PRC codeword in the space where content has a natural distribution; pseudorandomness of the PRC implies undetectability of the watermark, and robustness of the PRC implies robustness of the watermark. As we explain in Section 1.2, PRCs are in fact equivalent to undetectable and robust watermarks for language models. Oblivious vs adaptive robustness. While existing PRCs (and their corresponding watermarks) tolerate a high rate of errors, these errors must be made obliviously, i.e., by a memoryless channel. This falls short of the worst-case error model commonly considered in coding theory. Furthermore, errors introduced by realistic adversaries are non-oblivious: A realistic adversary can see multiple watermarked responses, and sometimes even query a detection oracle. It turns out that PRCs (and watermarks) that are highly robust to oblivious attacks may be easily removable by such adversaries, as demonstrated by an example in [CHS24].333Let \operatorname{\mathcal{W}} be a watermarked model. Consider another watermarked model \operatorname{\mathcal{W}}^{\prime} that, given a prompt \pi, outputs \operatorname{\mathcal{W}}(\pi) if \pi is unwatermarked and outputs an unwatermarked response if \pi is watermarked. Then \operatorname{\mathcal{W}}^{\prime} has the same oblivious robustness as \operatorname{\mathcal{W}}, because an oblivious adversary cannot find a watermarked \pi; however, it is not at all robust to an adversary that makes just two queries to \operatorname{\mathcal{W}}^{\prime}: The adversary can simply obtain a watermarked response x from \operatorname{\mathcal{W}}^{\prime}, then query \operatorname{\mathcal{W}}^{\prime} on x to obtain an unwatermarked response. The adversary doesn’t even need to make any edits! In fact, this is not just a theoretical possibility. Several practical attacks leverage access to encoding and decoding oracles to remove or forge watermarks [JSV24, PHZS24]. Therefore it is of both practical and theoretical importance to have PRCs that are robust to non-oblivious adversaries. We say that a PRC is adaptively robust if it can handle adversaries who are given an encoding oracle (or key). We introduce ideal security and CCA security to handle cases where the adversary additionally queries a detection oracle. Adaptive robustness was previously studied in [CHS24], but they showed only that the watermark of [CGZ24] was adaptively robust to a very low (specifically, inverse security parameter) rate of errors. [CHS24] conjectured but did not prove that the PRC of [CG24] is adaptively robust. This work. We prove that a slight modification of the PRC of [CG24] is adaptively robust to a constant rate of substitutions, resolving the conjecture of [CHS24]. That is, for any \delta<1/4 (or \delta<1/2 if we wish only to detect but not decode messages), we show that no adversary can produce an error of relative weight at most \delta that causes the decoder to fail — even if the adversary is provided with the encoding key. We call this property adaptive \delta-robustness. Our proof involves an interesting combination of techniques from cryptography and coding theory. We then ask whether it is possible to achieve robustness to an adversary with access to both an encoding oracle and a decoding oracle. Rather than contribute to a growing list of separately defined properties, we take a principled approach and consider the ideal functionality of a PRC. This functionality dictates how the code should behave to any adversary, regardless of the adversary’s goal (e.g., distinguishing codewords from random, or mauling codewords). We say that a PRC satisfies ideal security (or is an ideal PRC) if the real PRC encoding and decoding oracles are indistinguishable from this ideal functionality. This definition combines soundness, pseudorandomness, and adaptive robustness into one simple definition. We show a generic and simple transformation from any adaptively robust PRC capable of encoding a single-bit message to an ideal PRC with linear information rate. Our transformation requires only a pseudorandom function, the existence of which is implied by that of a PRC. This transformation, applied to the adaptively robust PRCs discussed above, yields a construction of an ideal PRC. Finally, we turn to the public-key setting, where both pseudorandomness and robustness are defined with respect to adversaries that have access not only to an encoding oracle, but an encoding key. While ideal security is the strongest possible definition in the secret-key setting, there is no obvious “ideal” security notion in the public-key setting. Instead, we define a strengthening of adaptive robustness that we call CCA security in analogy with CCA secure encryption. This definition generalizes standard CCA security for pseudorandom encryption. In the random oracle model, we show a generic transformation that builds a CCA-secure PRC from any adaptively robust public-key PRC. We summarize our four definitions of PRC robustness in Table 1. No decoder access Decoder access Secret-key Secret-key adaptive robustness Ideal security Public-key Public-key adaptive robustness CCA security Table 1: The four definitions of robustness considered in this work. “Secret-key” means the adversary is given oracle access to the encoder and “public-key” means the adversary is given the encoding key. “No decoder access” and “decoder access” distinguish whether the adversary is additionally given oracle access to the decoder. We satisfy the “no decoder access” column in Theorems 1 and 2, ideal security in Theorem 3, and CCA security in Theorem 4. 1.1 Results In this subsection we highlight our main results, which can be summarized follows: (a) We prove unconditionally that certain PRCs based on LDPC codes are (maximally) adaptively robust in both the secret- and public-key settings; (b) In the secret-key setting, we present a generic transformation converting any adaptively robust PRC into an ideal PRC; (c) In the public-key setting, we present a generic transformation converting any adaptively robust PRC into a CCA-secure PRC in the random oracle model. In presenting our results, we use the term “\delta-robust” to indicate that the attacker can adaptively corrupt up to a \delta fraction of the codeword. By “zero-bit” PRC we mean a PRC that is only capable of encrypting a fixed message; by “single-bit” we mean that the PRC can encrypt 0 or 1. Our first two results show essentially optimal robustness of the zero-bit LDPC-based PRC from [CG24], and our single-bit version of it. Any zero-bit PRC is at most 1/2-adaptively robust, since such errors can completely randomize the codeword. Any single-bit PRC is at most 1/4-adaptively robust, since an adversary can always construct a string that is 1/4-close to two codewords encoding different messages (see the remark at the end of Section 2.3). Theorem 1 (Adaptively robust zero-bit PRC). For any \varepsilon>0, the public-key zero-bit pseudorandom code from [CG24] is adaptively (1/2-\varepsilon)-robust for appropriate choice of parameters. Theorem 2 (Adaptively robust single-bit PRC). For any \varepsilon>0, our public-key single-bit pseudorandom code is adaptively (1/4-\varepsilon)-robust for appropriate choice of parameters. Our secret-key transformation is lightweight in that it makes no additional assumptions beyond the security of the underlying PRC,444It makes use of a pseudorandom function, whose existence is implied by that of a PRC. and it preserves the quantitative level of robustness. Theorem 3 (Ideal PRC). Suppose that there exists a secret-key single-bit pseudorandom code that is adaptively \delta-robust. Then there exists a \delta-robust ideal pseudorandom code with linear information rate. The result is somewhat worse for the public-key case, incurring a constant-factor loss in robustness and requiring the random oracle model. Theorem 4 (CCA-secure PRC). Suppose that there exists a public-key single-bit pseudorandom code that is adaptively \delta-robust. Then in the random oracle model, there exists an \Omega(\delta)-robust CCA-secure pseudorandom code with linear information rate. By applying Theorems 3 and 4 to the PRCs from [CG24], which are pseudorandom assuming the certain-subexponential hardness of LPN and adaptively \delta-robust for \delta<1/4 by Theorem 2, we have the following corollary. Corollary 5. Assuming the 2^{O(\sqrt{\secpar})}-hardness of learning parity with noise (LPN), there exist separate linear-rate PRCs satisfying • \delta-robust ideal security for any \delta<1/4, and • \delta-robust CCA security for some \delta=\Omega(1) in the random oracle model. While the above robustness is optimal for the ideal PRC result, we do not achieve the optimal robustness for the CCA-secure PRC result. 1.2 Relationship to watermarking Christ and Gunn [CG24] presented a general template for using pseudorandom codes to watermark sufficiently high-entropy outputs of randomized algorithms. We recall that template here for reference. Consider a randomized algorithm that, given an input x and a random seed r, generates an output y. Suppose that there exists a randomness recovery algorithm that, given y, outputs an approximation of the random seed r used to generate it. Randomness recovery algorithms indeed exist for generative models used in practice, and their approximation accuracy increases with the amount of entropy in the output distribution. The basic observation is that replacing r with an output of \mathsf{PRC}.\mathsf{Encode}(1) immediately yields a watermark. Pseudorandomness ensures that this replacement does not perceptibly alter the quality of the model. Error correction enables detection, with the content being considered watermarked if \mathsf{PRC}.\mathsf{Decode}({\sf RandomnessRecovery}(y))=1. This watermarking approach works for any randomized algorithm with a randomness recovery algorithm whose approximation error is within the PRC’s tolerance. Remark. It is not a coincidence that randomness recovery algorithms exist in almost every generative AI framework. In some cases, randomness recovery can be viewed as an essential part of training. It is also useful to have randomness recovery in many independent generative AI applications, because it can allow for content manipulation in a more natural “latent space” where features correspond to higher-order features of the content. It is known that this framework yields an undetectable and robust watermark for large language models [CG24, GM24]. This framework has also been demonstrated experimentally by [GZS24], which builds a robust undetectable watermarking scheme for generative image models (in particular, latent diffusion models) using essentially the same PRC as we consider here. Not only are pseudorandom codes useful for constructing robust undetectable watermarks, but they are also necessary in a formal sense. Consider a model that can be prompted to output a uniformly random binary string (e.g., one can ask a language model to output a random sequence of “apple” and “orange”). Suppose we can undetectably watermark this model, with robustness to a given error channel. This implies that on each query, the model outputs a fresh pseudorandom string such that the detector outputs {\sf True} even if this string is subjected to this error channel. Soundness (i.e., the low false positive rate) of the watermark ensures that unrelated strings decode to {\sf False}. Thus, the model with this fixed prompt is exactly an encoder for a zero-bit PRC, where the PRC decoder is the watermark detector. PRC robustness and watermark robustness. Typical randomness recovery algorithms for generative AI are crucially imperfect, even on unedited generations. If one wishes to have a robust watermark, then randomness recovery will be still more inaccurate. Therefore, it is essential for applications to watermarking that we use a PRC with strong robustness. The watermark will be detectable as long as the composition of an adversary’s modifications to the output, and any intrinsic error from randomness recovery, falls within the class of error channels tolerated by the PRC. For example, a PRC with robustness to a constant rate of errors chosen by an adversary that observes only the given output translates to a watermark with robustness to an adversary making a single query to the generation algorithm. In practice, however, an adversary attempting to remove the watermark can make arbitrarily many generation queries, translating to a PRC error channel that observes many codewords. Therefore we need our PRC to be adaptively robust if we wish to handle such watermark removal attacks. Similarly, a watermark that is robust to an adversary with access to a watermark detection oracle necessitates a PRC that is robust to an adversary with a decoding oracle. This robustness is satisfied by the ideal PRCs that we define here. Similarly, an adversary given a watermark detection oracle and the watermark generation key is handled by our CCA-secure PRCs. On robustness to substitutions. In this work, we only consider PRCs with robustness to substitutions. This is sufficient for some watermarking applications, like those where the PRC is embedded in a semantic representation of the content, and general edits in the content itself translate to substitutions in the semantic space. This is the case, for instance, in the image watermark of [GZS24] because the models they consider map between images and a latent space consisting of fixed-length vectors. They embed a PRC codeword in this latent representation of the generated image, and their decoder maps the given image back to its latent representation before decoding the PRC. Since these latent vectors are of fixed size, modifications to the image translate to changes to components, but not insertions or deletions. For other applications — especially watermarking language models — it can be useful to have a PRC with robustness to more general forms of edits. Random deletions were considered in [CG24], and oblivious deletions and insertions were considered in [GM24] (over a polynomial-sized alphabet). However, we note that both of these methods work by reduction to the binary substitution channel. Therefore improved results about substitution channels may translate to improved results for editing channels, although we do not investigate this here. On the alphabet size. One can think of the watermark generation process as sampling each “component” of the output y to be correlated with the corresponding bit of the random seed r. The amount of entropy in a given component y_{i} determines how much signal from the corresponding seed bit r_{i} it contains. In order to plant one symbol of a PRC codeword in each component with significant signal, we require \Omega(\absolutevalue{\text{PRC alphabet}}) entropy per component. For language models, these “components” might be the tokens of the response. Typical language models have on the order of one bit of entropy per word, necessitating a PRC with a constant-sized (ideally, binary) alphabet.555While it is possible to increase the entropy per component by taking each component to be a sequence of k words, this harms robustness as changing one in every k words now changes every symbol of the underlying codeword. Other watermarking applications will also suffer from the use of a larger alphabet. Therefore, in this work we only consider binary-alphabet PRCs. Related work. We have referred to most of the relevant existing works, particularly those about pseudorandom codes, throughout this introduction. For a more detailed discussion on related work on watermarking, see Appendix A."
https://arxiv.org/html/2411.05901v1,ViT Enhanced Privacy-Preserving Secure Medical Data Sharing and Classification,"Privacy-preserving and secure data sharing are critical for medical image analysis while maintaining accuracy and minimizing computational overhead are also crucial. Applying existing deep neural networks (DNNs) to encrypted medical data is not always easy and often compromises performance and security. To address these limitations, this research introduces a secure framework consisting of a learnable encryption method based on block-pixel operation to encrypt the data and subsequently integrate it with the Vision Transformer (ViT). The proposed framework ensures data privacy and security by creating unique scrambling patterns per key, providing robust performance against leading bit attacks and minimum difference attacks.","Advancements in ML and DL models, supported by cloud platforms like Google Cloud and Microsoft Azure, offer high computational power but also introduce privacy risks in shared environments [1, 2, 3]. In response, the research proposed ViT enhances the learnable encryption method and obfuscates image details while preserving classification-relevant features, ensuring HIPAA compliance. When tested on MRI and histopathological datasets, the method showed strong defense against attacks such as leading bit-attack, and minimum difference attacks, keeping medical data safe in the cloud."
https://arxiv.org/html/2411.05890v1,A Comparative Analysis of Machine Learning Models for DDoS Detection in IoT Networks,"This paper presents the detection of DDoS attacks in IoT networks using machine learning models. Their rapid growth has made them highly susceptible to various forms of cyberattacks, many of whose security procedures are implemented in an irregular manner. It evaluates the efficacy of different machine learning models, such as XGBoost, K-Nearest Neighbours, Stochastic Gradient Descent, and Naïve Bayes, in detecting DDoS attacks from normal network traffic. Each model has been explained on several performance metrics, such as accuracy, precision, recall, and F1-score to understand the suitability of each model in real-time detection and response against DDoS threats.This comparative analysis will, therefore, enumerate the unique strengths and weaknesses of each model with respect to the IoT environments that are dynamic and hence moving in nature. The effectiveness of these models is analyzed, showing how machine learning can greatly enhance IoT security frameworks, offering adaptive, efficient, and reliable DDoS detection capabilities. These findings have shown the potential of machine learning in addressing the pressing need for robust IoT security solutions that can mitigate modern cyber threats and assure network integrity.","I-A Background While IoT mobile networks and IoT wired networks share some security vulnerabilities, the main distinction is how the devices connect, move, and interact across the network. Mobile networks are more vulnerable to external threats, mobility-related dangers, and issues with remote access [Fig.1]. Wired networks, on the other hand, give greater physical protection and control, but they necessitate strict internal access controls and segmentation to eliminate dangers posed by internal threats or unauthorised physical access [Fig.2]. To ensure effective security in both types of networks, strong encryption, authentication, access control, and regular monitoring are required. Figure 1: Comprehensive IoT and Mobile Network Security Framework In the context of the Internet of Things (IoT), applications to fields like smart homes, health care, and industrial automation are enhancing efficiency and convenience fabulously. But this process also extends the attack surface for possible cyber threats as IoT devices generally work with poor security protocols, hence becoming potential targets for cybercriminals [11, 8]. Of the myriad of threats, Distributed Denial of Service (DDoS) attacks prove especially devastating in attempts to flood network resources, leading to a range of consequences, from light service degradation to complete operational unavailability [12]. Figure 2: IoT-Based DDoS Attack Architecture This is further accentuated by the exponential rise in IoT deployments, which are usually characterized by heterogeneous devices and inconsistent security standards. These differences become vulnerabilities that attackers can exploit to launch DDoS attacks with increased frequency and on a larger scale, which become very difficult for conventional network defenses to mitigate [10, 7, 9]. There is an increased demand for complex detection systems, such as those based on machine learning, that provide fast and effective security solutions capable of detecting and thwarting the ever-evolving DDoS threats in real time [5]."
https://arxiv.org/html/2411.07231v1,Watermark Anything with Localized Messages,"Image watermarking methods are not tailored to handle small watermarked areas. This restricts applications in real-world scenarios where parts of the image may come from different sources or have been edited. We introduce a deep-learning model for localized image watermarking, dubbed the Watermark Anything Model (WAM). The WAM embedder imperceptibly modifies the input image, while the extractor segments the received image into watermarked and non-watermarked areas and recovers one or several hidden messages from the areas found to be watermarked. The models are jointly trained at low resolution and without perceptual constraints, then post-trained for imperceptibility and multiple watermarks. Experiments show that WAM is competitive with state-of-the art methods in terms of imperceptibility and robustness, especially against inpainting and splicing, even on high-resolution images. Moreover, it offers new capabilities: WAM can locate watermarked areas in spliced images and extract distinct 32-bit messages with less than 1 bit error from multiple small regions – no larger than 10% of the image surface – even for small 256\times 256 images.","Invisible image watermarking embeds information into image pixels in a way that is imperceptible to the human eye and yet robust. It was initially developed for intellectual property and copy protection, such as by Hollywood studios for DVDs. However, the applications of watermarking are evolving, particularly in light of the recent development of generative AI models (Kušen and Strembeck, 2018). Regulatory acts such as the White House executive order (USA, 2023), the Californian bill, the EU AI Act (Parliament and Council, 2024), and Chinese AI governance rules (of the People’s Republic of China, 2023) require AI-generated content to be easily identifiable. They all cite watermarking as either compulsory or a recommended measure to detect and label AI-generated images. Image splicing is one of the most common manipulations, whether applied for benign or malicious purposes (Christlein et al., 2012; Tralic et al., 2013). Splicing involves adding text or memes on a large portion of the image or extracting parts of images and overlaying them on others (Douze et al., 2021). It can bypass the state-of-the-art watermarking techniques, which take one global decision per image under scrutiny. Indeed, in traditional watermarking, the watermark signal fades away and is no longer detected as the surface of the watermarked area decreases. Besides, these techniques poorly answer the paradoxical question of deciding whether an image should be considered watermarked if only a small part carries the watermark. A positive decision triggered by a small area might be unfair to artists who use AI models for inpainting or outpainting. On the other hand, not being robust enough to splicing opens the door to easy removal. Figure 1: Overview. (a) The embedder creates an imperceptible image modification. (b) Traditional transformations (cropping, JPEG compression, etc.) and/or advanced manipulations (mixing watermarked and non-watermarked images, inpainting, etc.) may be applied to the image. (c) The extraction creates a segmentation map of watermarked parts and retrieves one or several messages. To address these issues, this paper redefines watermarking as a segmentation task, giving birth to the Watermark Anything Models (WAM). Our motivation is to disentangle the strength of the watermark signal from its pixel surface, in contrast to traditional watermarking. More precisely, the WAM extractor detects if the watermark is present and extracts a binary string for every pixel rather than predicting a message for the whole image. These outputs are post-processed according to the final task. For global detection, the image is deemed watermarked if the proportion of watermarked pixels exceeds a user-defined threshold. For global decoding, a majority vote recovers the hidden message. A new application, out of the reach of traditional robust watermarking, is the localization of watermarked areas and the extraction of multiple hidden messages. For that purpose, we choose to apply the DBSCAN clustering algorithm over the pixel-level binary strings because it does not require any prior on the number of watermarks (or centroids). This is detailed in Sec. 3. These new functionalities require a training with new objectives, that is split into two phases. The first phase pre-trains the embedder and extractor models for low-resolution images. It essentially targets the robustness criterion. The embedder encodes a n_{\text{bits}}-bit message into a watermark signal that is added to the original image. The augmenter randomly masks the watermark in parts of the image and augments the result with common processing techniques (e.g., cropping, resizing, compression). The extractor then outputs a (1+n_{\text{bits}})-dimensional vector per pixel to predict the parts of the image that are watermarked and decode the corresponding messages. Detection and decoding losses are used as training objectives. The second training phase targets the following new objectives: (1) minimize the watermark’s visibility in alignment with the human visual system, (2) allow for multiple messages within the same image. This two-stage training is less prone to instability, compared to previous use of adversarial networks and divergent objectives (Zhu et al., 2018). It also trains the extractor on both watermarked and non-watermarked images, for the first time in the literature. This increases the performance and the robustness of the detection. We first compare WAM with state-of-the-art methods for regular tasks of watermark detection and decoding on low and high-resolution images. Our results show that WAM achieves competitive performance in terms of imperceptibility and robustness. To further highlight the advantages of WAM, we then evaluate its performance on tasks that are not considered in the literature. Namely, we evaluate the localization accuracy between the predicted watermarked areas and the original mask and assess the ability to detect and decode multiple watermarks in a single image. For instance, when hiding five 32-bit messages, each in a 10% area of the image, detection of watermarked areas achieves more than 85% mIoU, even after images are horizontally flipped and the contrast adjusted, and bit accuracy (for a total of 160 bits) achieves more than 95% under the same augmentation (Sec. 5.5). In summary, our contributions are: • the definition of watermarking as a segmentation task; • a two-stage training able to strike a good trade-off between invisibility and robustness even for multiple watermarks and high-resolution images; • WAM, an embedder/extractor model competitive with state-of-the-art methods; • the highlight of new capabilities, localization of watermarks and extraction of multiple messages as depicted in Fig. 1, together with specially designed evaluations."
https://arxiv.org/html/2411.07089v1,Towards Characterizing Cyber Networks with Large Language Models,"Threat hunting analyzes large, noisy, high-dimensional data to find sparse adversarial behavior. We believe adversarial activities, however they are disguised, are extremely difficult to completely obscure in high dimensional space. In this paper, we employ these latent features of cyber data to find anomalies via a prototype tool called Cyber Log Embeddings Model (CLEM). CLEM was trained on Zeek network traffic logs from both a real-world production network and an from Internet of Things (IoT) cybersecurity testbed. The model is deliberately overtrained on a sliding window of data to characterize each window closely. We use the Adjusted Rand Index (ARI) to comparing the k-means clustering of CLEM output to expert labeling of the embeddings. Our approach demonstrates that there is promise in using natural language modeling to understand cyber data.","Threat hunting is an open-ended cybersecurity exploration to detect abnormal behaviors that automated tools cannot easily find. Threat hunters continuously collect and analyze data from their corporate networks looking for indicators of compromise. The complexity and changing attack surface of industrial control systems creates an increasing need for tools to help hunters effectively detect threats within diverse, complex, and statistically noisy cyber data. This project addresses this gap by developing CLEM, which is capable of ingesting cybersecurity logs from cyberphysical systems and characterizing entities in them by creating vector identities and behavioral relationships. The key innovation is the use of large language models (LLMs) to model the linguistic structures found in cybersecurity logs allowing us to cluster machines according to their behaviors over time. The rest of this paper is divided as follows: Section II provides motivation for our work and methodologies. In Section III we provide a series of related papers spanning different models, approaches, and techniques. We introduce our main contribution, CLEM, in Section IV and our results in Section V. Finally, we provide future research ideas in Section VI and conclude our work in Section VII."
https://arxiv.org/html/2411.07023v1,The Inherent Adversarial Robustness of Analog In-Memory Computing,"A key challenge for Deep Neural Network (DNN) algorithms is their vulnerability to adversarial attacks. Inherently non-deterministic compute substrates, such as those based on Analog In-Memory Computing (AIMC), have been speculated to provide significant adversarial robustness when performing DNN inference. In this paper, we experimentally validate this conjecture for the first time on an AIMC chip based on Phase Change Memory (PCM) devices. We demonstrate higher adversarial robustness against different types of adversarial attacks when implementing an image classification network. Additional robustness is also observed when performing hardware-in-the-loop attacks, for which the attacker is assumed to have full access to the hardware. A careful study of the various noise sources indicate that a combination of stochastic noise sources (both recurrent and non-recurrent) are responsible for the adversarial robustness and that their type and magnitude disproportionately effects this property. Finally, it is demonstrated, via simulations, that when a much larger transformer network is used to implement a Natural Language Processing (NLP) task, additional robustness is still observed.","Results Experimental validation of adversarial robustness To experimentally study the adversarial robustness, we employed a PCM-based AIMC chip with tiles comprising 256\times 256 synaptic unit-cells [29]. Each unit-cell contains four PCM devices (see Fig. 1b). The weights obtained via HWA training are stored in terms of the analog conductance values of the PCM devices, where two devices are used to store the positive and negative weight components, respectively. The conductance variations associated with these conductance values are thought to be the primary reason for potential adversarial robustness. As shown in Fig. 1a, the intrinsic stochasticity associated with the conductance variations is likely to make the design of an adversarial attack rather difficult. The conductance variations themselves fall into two different categories. There is a non-recurrent category that results from the inaccuracies associated with programming a certain analog conductance value. This is typically referred to as programming noise [29]. Besides this non-recurrent noise component, there is a recurrent variation in the conductance values arising from 1/f noise [30] and Random Telegraph Noise (RTN) characteristic of PCM devices. In subsequent sections, we will investigate the role of the recurrent and non-recurrent noise sources with respect to adversarial robustness. For this study, we consider three different types of adversarial attacks and five different target platforms. The different types of attacks are (i) Projected Gradient Descent (PGD) [31], (ii) Square [32], and (iii) OnePixel [33]. The attacks range from targeting small (localized) to larger (for some attacks, entire) input regions. Specifically, the PGD attack was chosen, as it is equivalent to the Fast Gradient Sign Method (FGSM) attack when the starting point is not randomly chosen and the L_{\infty} norm is used [34]. Hence, it is superior to FGSM when more than one iteration is used to generate adversarial inputs. The Square and OnePixel attacks do not rely on local gradient information, and thus, are not affected by gradient masking, which is a defensive strategy aimed at diminishing the efficacy of gradient-based attacks by obfuscating the model’s loss function, rendering it less informative or harder to optimize[35]. The OnePixel attack only permutes a small region (i.e., a single pixel) of the input, rather than a larger region, as typically targeted by other attacks. The five different attack target platforms considered are: (i) the original floating-point model (Original Network (FP32)), (ii) the floating-point HWA retrained model (HWA Retrained (FP32)), (iii) a digital hardware accelerator with 4-bit fixed-point weight precision and 8-bit activation precision (Digital), (iv) a PCM-based AIMC chip modelaaaThe PCM-based AIMC chip model is described in the Methods section and a match between the model and experimental data is provided in Supplementary Note 1. (AIMC Chip Model), and (iv) a PCM-based AIMC chip (AIMC Chip). For the first target, an existing pre-trained model is used. For the second target, the pre-trained model is re-trained using HWA training. For the third, fourth, and fifth targets, the parameters of the re-trained model are mapped to the corresponding hardware and deployedbbbAll auxiliary operations are performed in floating-point precision.. To determine the effectiveness of adversarial attacks, different evaluation metrics can be used. Typically, the clean (baseline) accuracy is directly compared to the accuracy when adversarial inputs are inferred. In this paper, we adopt the Adversarial Success Rate (ASR) metric [36], which considers only samples that are classified correctly by the network. More specifically, the ASR is determined as follows. First, model predictions are determined for both clean and adversarial inputs. Using target labels of the clean inputs, the incorrectly classified clean inputs are identified. These target labels and the predictions of adversarial inputs (adversarial predictions), which originated from the incorrectly classified clean inputs, are then masked and discarded. Traditional ML evaluation metrics can then be computed using the masked target labels and adversarial predictions. We concretely describe the determination of the ASR in terms of accuracy using Algorithm 1 (see Methods). By adopting the ASR metric, we can evaluate and compare the performance of adversarial attacks which are generated and/or evaluated using different target platforms. To demonstrate this ability, in Extended Data Table 2, we compute both the test set accuracy and ASR during training for a floating-point Resnet-based CNN trained for CIFAR-10 image classificationcccIn Supplementary Note 3, we perform simulations for a more challenging image classification task. using two different types of attacksdddThese attacks are considered in all fore-coming experiments, and are described in greater detail below., PGD and OnePixel, with different parameters. For the ASR to be truely agnostic to the target platform, it should be independent of the network accuracy. It is observed that the ASR is sufficiently independent of the test set accuracy. It is only notably perturbed for extreme scenarios (i.e., when the test set accuracy is <25%). For each type of attack, we vary two parameters. The first parameter relates to the number of attack iterations, whereas the second parameter relates to the attack magnitude, i.e., the maximum amount of distortion the attack can introduce to the input. When evaluating the ASR as a function of different attack parameters, especially for those relating to magnitude, caution must be taken so that the ground truth label is not changed by extreme permutations. If the severity of an attack or the number of attack iterations is too high, then the ground truth labels of the generated adversarial inputs can differ from the original inputs, which is problematic as the underlying semantics of the original input have now changed. Consequently, for all experiments, the maximum value for each parameter is determined by manually inspecting adversarial examples and determining the points at which the ground truth label is changed (see Methods). To comprehensively compare the adversarial robustness for all target platforms, while considering the number of attack iterations and magnitude, a contour or 3D plot, per target platform, is required. These are difficult to compare relative to each other. Hence, instead, we generate an ASR for each target platform, which is indicative of the average behavior of the objective as a function of both parameters. This is generated by projecting a straight line from the bottom-left to top-right corners of each contour plot. Along this line, the ASR is extracted. Each point in this line is associated with two values: one governing the number of attack iterations and another governing the attack magnitude. A dual x-axis is used to associate these. For each evaluation platform, aside from the AIMC chip, the strongest attack scenario is considered, i.e., attacks are both generated and evaluated using the same platform. To both generate and evaluate attacks on the AIMC chip, hardware-in-the-loop attacks would have to be performed – such attacks are non-trivial to perform on stochastic hardware, and hence, instead, are the sole focus of the Hardware-in-the-loop adversarial attacks section. Consequently, for the AIMC chip, adversarial inputs are generated using the AIMC chip model. For the Software (FP32) evaluation platform, attacks are generated using the same platform with FP32 and HWA Retrained (FP32) weights, respectively. In Fig. 2a-c, the ASR quantity is related to both aforementioned parameters for the AIMC chip evaluation platform. As can be observed in Fig. 2d-f, the envelope for the floating-point HWA retrained model is smaller than the original floating-point model, meaning it is more robust. This result is consistent with prior work [14, 13, 16, 37]. The digital hardware accelerator has an even smaller envelope, meaning it is more robust than the floating-point HWA retrained model. It is noted that like the operation of the digital hardware accelerator, the evaluation of the floating-point HWA retrained model is deterministic. Contrastingly, the digital hardware platform is subject to quantization noise and accumulation error [38]. While this error is introduced during normal system operation, as digital hardware is typically deterministic (as is assumed here), it can be predicted in advance, i.e., when adversarial inputs are generatedeeeIn fact, it has been demonstrated that adversarial attacks targeting these deterministic systems can be more effective [39].. Critically, (i) there is a good agreement between the modelled ASR for the AIMC chip and the ASR rate of the hardware models on the AIMC chip, and (ii) the hardware experiments on the AIMC chip result in the smallest envelopes, and hence, the highest level of robustness to all investigated adversarial attacks. Source of adversarial robustness In this section, we present a detailed study of the various noise sources that contribute to the higher adversarial robustness observed experimentally on the AIMC chip (see Methods). Specifically, we investigate the role of four different noise properties – the recurrence (or lack thereof), location, type, and magnitude. For this study, we rely on the hardware model of the AIMC chip. As described earlier, non-recurrent noise sources introduce noise once during operation, i.e., when weights are programmed, whereas recurrent noise sources introduce noise multiple times during operation, i.e., when a MVM operation is performed. In the AIMC chip model, the recurrent noise sources are sampled multiple times (specifically per mini-batch), whereas non-recurrent noise sources are only sampled once. All noise sources are assumed to be Gaussian and centered around zero. The non-recurrent component of stochastic weight noise primarily arises from programming noise. The recurrent components of the stochastic weight noise primarily arises from both device read noise (1/f and RTN) and output noise, which is introduced at the crossbar output, before currents are read out using Analog-to-Digital Converters. Output noise includes 1/f noise from amplifiers and other peripheral circuits, as well as other circuit non-linearities (IR drop, sneak paths, etc.) of small magnitude that can be approximated as a random perturbation on the crossbar output. Hence, in the model, non-recurrent weight noise is modelled, and at the output, these combined effects are modelled using additive recurrent noise of a fixed magnitude which is assumed to be independent of the total column conductance. For each configuration, other deterministic noise sources, e.g., input and output quantization, are also modelled. Other non-deterministic noise sources are assumed to contribute negligiblyfffAs evidenced by the strong match between the AIMC chip and the AIMC chip model across all hardware experiments., and hence, are not modelled. In order to assess how three properties of stochastic noise sourcesgggIn Supplementary Note 4, we investigate how the inherent adversarial robustness of AIMC-based hardware is effected by temporal drift., namely, recurrence, location, and type affect adversarial robustness, we modify the behavior of the AIMC chip model to focus exclusively on a single stochastic noise source. Separately, we investigate both recurrent and non-recurrent noise sources at two locations: the weights, whose effect is determined as a function of the input, and at the output, whose effect is input-independent. A total of four stochastic noise sources are considered: (i) recurrent output noise, (ii) recurrent weight noise, (iii) non-recurrent weight noise, and (iv) non-recurrent output noise. The effect of the noise magnitude is determined by, for each noise source, modifying the noise magnitude such that the resulting test set accuracy is lower than the floating-point test set accuracy by a desired percentage (i.e., drop). We consider drops of 5% and 10%. In Fig. 3a, for a large number, n=1,000, of repetitions, the test set accuracy is reported for both non-recurrent and recurrent output and weight noisehhhIn Supplementary Notes 2 and 5, we investigate adversarial robustness to varying degrees of stochasticity and combinations of output and weight noise.. It is observed that, for non-recurrent and recurrent noise sources normalized to produce the same test set accuracy, the noise magnitude is approximately equal, i.e., the recurrence property does not effect the resulting average test set accuracy. For non-recurrent noise sources the variation of the resulting test set accuracy is larger. For noise sources normalized to produce a larger test set accuracy drop (10%), the noise magnitude is larger, compared to the smaller drop (5%). Intuitively, as the noise magnitude is increased, the test set accuracy decreases and the network becomes more robust to adversarial attacks. To investigate the effect of the recurrence, location, and type properties on adversarial robustness, further experiments are performed in Fig. 3b. Comparisons are made using one of the selected attacks, PGD. For the noise magnitudes associated with the smaller test set accuracy drop (5%), the ASR envelope is determined for n=10 repetitions, for the four aforementioned noise sources. Two key observations can be made: (i) output noise exhibits greater adversarial robustness compared to weight noise, and (ii) in addition to not effecting the average test set accuracy, the recurrence property does not effect the ASR. We postulate that weight noise leads to less robustness when normalized to produce the same error, as the effect of weight noise is input dependent, whereas output noise is not. In Fig. 3c-e, we further compare the ASR envelope for the PGD, Square, and OnePixel attacks. It is observed that, on average, the model with only output noise exhibits the highest adversarial robustness. The model with only weight noise is the least robust. The AIMC chip model and AIMC chip, with test set accuracy values of 84.85% and 84.31%, respectively, i.e., drops of 3.57% and 4.11%, exhibit greater adversarial robustness than the modified AIMC chip model with only weight noise, but less adversarial robustness than the modified AIMC chip model with only output noise. From this analysis, it can be concluded that, the type and magnitude properties of stochastic noise sources have the greatest influence on adversarial robustness. The location and recurrence properties have negligible influence. Hardware-in-the-loop adversarial attacks Next, we determine the efficacy of hardware-in-the-loop attacks, i.e., where it is assumed that the attacker has full access to the AIMC chip. We compare the efficacy of one white- and one black-box attack (PGD and OnePixel). White box attacks are especially difficult to perform for stochastic hardware, as the construction of representative hardware models (with minimal mismatch) is non-trivial, and in many cases, even unfeasible. While automated ML-based in-the-loop modelling [40] approaches can be utilized, they require a significant amount of data, which is instance specific. Hence, they are not considered in this paper. For hardware-in-the-loop attacks, when a white-box attack is deployed, to perform backwards propagation, for each layer, weights and cached inputs are required [41]. Additionally, the output(s) of the network is(are) required. As ideal, i.e., floating-point precision, weights cannot be programmed, there is some deviation between the target and programmed conductances, so the target weights (if known) cannot simply be used by the attacker. Additionally, read noise introduces random fluctuations when MVM workloads are executed. Representative weights can be inferred by solving the following optimization problem, as described in [42] \mathbf{\hat{G}}=\operatorname*{argmin}_{\mathbf{\hat{G}}}\sum_{b=1}^{B}\|% \mathbf{\hat{G}}\mathbf{x}_{b}-\tilde{\mathbf{y}}_{b}\|_{2}. (1) Inputs to each layer can be cached during normal operation by probing input traces to Digital-to-Analog Converters. To perform backwards propagation, this information can be used to construct a proxy network graph (see Methods), for which gradients can be computed in floating-point precision using the chain-rule. For sake of practicality, an ideal backwards pass is assumed, i.e., straight-through-estimators are used for regions which are non-differentiable, and all values are assumed to be in floating-point precision. It is noted that, as the next candidate adversarial input to the network is usually dependent on the the result of backwards propagation of the previous candidate input, this process cannot normally be pipelined. Consequently, depending on the operation speed of the chip, attacks generated using AIMC chips can be susceptible to low-frequency noise and temporal variations, such as conductance drift [43]. To mitigate these effects, as reprogramming all devices after each adversarial example is presented is not desirable, \mathbf{\hat{G}} can be re-inferred. It is noted that, for black box attacks, these effects cannot be effectively mitigated - except in the scenario where the attacker is aware of exactly when the chip was programmed. In Fig. 4, we compare the efficacy of hardware-in-the-loop attacks generated and evaluated using the AIMC chip to three different attack scenarios, where: (i) adversarial inputs are generated and evaluated using the digital hardware accelerator, (ii) adversarial inputs are generated and evaluated using the AIMC chip model, and (iii) adversarial inputs are generated using the AIMC chip model and evaluated using the AIMC chip. When evaluated using the AIMC chip, adversarial attacks generated using hardware-in-the-loop attacks on the AIMC chip are more effective than on the AIMC chip model. Hardware-in-the-loop attacks generated and evaluated on the AIMC chip model are marginally less effective. Critically, for all scenarios involving either the AIMC chip or AIMC chip model, additional adversarial robustness is observed compared to hardware-in-the-loop attacks both generated and evaluated on the digital hardware platform. We emphasize, that to take the required circuit measurements to perform hardware-in-the-loop attacks, even for black-box attacks, where the output logits are needed, significant knowledge about the underlying hardware is required. In some instances, direct traces may be unavailable, meaning that a combination of other signals must be used as a proxy, reducing attack efficacy. In others, developing a realistic hardware model is too time consuming and not practically feasible. Similarly to when a representative hardware model is attacked, unlike for deterministic systems, e.g., digital hardware accelerators, the generated adversarial inputs are specific to that particular instance of the hardware, and hence not useful for large-scale attacks targeting many instances at the same time. Applicability to transformer-based models and natural language processing tasks Finally, to determine whether additional adversarial robustness is still observed for much larger transformer models and different input modalities, we simulate a pre-trained floating-point RoBERTa model fine-tuned for one GLUE task, MNLI. The RoBERTa model has approximately 125M parameters and the MNLI task comprises 393K training and 20K test samples (pairs of sentences). When fine-tuning the model for the down-stream MNLI task, HWA training was performed (see Methods). This model exceeds the weight capacity of the IBM HERMES Project Chip, so instead of performing hardware experiments, simulations were conducted using the AIMC chip model. A number of different hardware attacks for NLP tasks with text-based input exist. These usually target either specific characters, words, or tokens [44]. One major challenge when generating adversarial attacks for NLP tasks is semantic equivalence. For images, small permutations are usually not perceivable. However, for text, small permutations are much more notable (even at a character level), and are more likely to alter the semantics of the input. We consider the Gradient-based Distributional Attack (GBDA) attack [45], which instead of constructing a single adversarial example, as done by most types of attacks, searches for an adversarial distribution and considers semantic similarity using the BERTScore metric [46]. This enforces some degree of semantic equivalence. In Extended Data Table Supplementary Tables, we list a number of adversarial text-based inputs for different \lambda_{sim} and n_{iters} parameter values, in addition to their respective BERTScore values. Similarly to as done in Section Experimental validation of adversarial robustness, we consider varying two attack parameters, which relate to the number of attack iterations and the magnitude, n_{iter} and \lambda_{sim}, respectivelyiiiThe \lambda_{sim} parameter is inversely proportional to the attack magnitude.. In Fig. 5, the ASR is reported for the first four target platforms (listed in Section Experimental validation of adversarial robustness). Additional robustness is once again demonstrated for the AIMC chip model for a range of attack parameter values and BERTScore values. This is significant, as it indicates that additional adversarial robustness is still observed for (i) different input modalities, and (ii) much larger and more parameterized networks."
https://arxiv.org/html/2411.06835v1,HarmLevelBench: Evaluating Harm-Level Compliance and the Impact of Quantization on Model Alignment,"Warning: This report contains sensitive content and potentially harmful information. With the introduction of the transformers architecture, LLMs have revolutionized the NLP field with ever more powerful models. Nevertheless, their development came up with several challenges. The exponential growth in computational power and reasoning capabilities of language models has heightened concerns about their security. As models become more powerful, ensuring their safety has become a crucial focus in research. This paper aims to address gaps in the current literature on jailbreaking techniques and the evaluation of LLM vulnerabilities. Our contributions include the creation of a novel dataset designed to assess the harmfulness of model outputs across multiple harm levels, as well as a focus on fine-grained harm-level analysis. Using this framework, we provide a comprehensive benchmark of state-of-the-art jailbreaking attacks, specifically targeting the Vicuna 13B v1.5 model. Additionally, we examine how quantization techniques, such as AWQ and GPTQ, influence the alignment and robustness of models, revealing trade-offs between enhanced robustness with regards to transfer attacks and potential increases in vulnerability on direct ones. This study aims to demonstrate the influence of harmful input queries on the complexity of jailbreaking techniques, as well as to deepen our understanding of LLM vulnerabilities and improve methods for assessing model robustness when confronted with harmful content, particularly in the context of compression strategies.","While numerous LLMs have been developed in recent years [1] [2], aligning these models with human preferences remains a complex and ongoing challenge. LLM alignment refers to the process of guiding models to avoid generating harmful or undesired outputs, ensuring their safe and ethical use. Recent work, such as Ouyang et al. [4] and Munos et al. [5], has demonstrated that specific fine-tuning strategies can significantly reduce the risk of harmful content generation. However, as models become more advanced, their vulnerabilities also become more pronounced. LLM vulnerabilities refer to the weaknesses that can be exploited to make models generate unsafe, harmful, or unintended content [10]. These vulnerabilities may arise from the vast and often uncensored datasets used during training, or from the model’s inherent capacity to generalize and respond to a wide range of queries [3]. This makes them susceptible to adversarial manipulation, where malicious users can craft inputs to elicit harmful outputs. This has led to the rise of jailbreaking methods, which are designed to probe and exploit these vulnerabilities to better understand the limitations of models. Several state-of-the-art jailbreaking techniques continue to bypass alignment measures, successfully eliciting harmful responses or sensitive information from models [6] [11]. In the context of adversarial attacks, assessing model compliance remains a difficult problem. Even with robust alignment strategies, ensuring that models consistently adhere to ethical and safety guidelines across a wide range of queries is challenging. In addition, the adoption of compression techniques, such as quantization, has introduced new challenges in the alignment of LLMs. While quantization improves computational efficiency, Kumar et al. [12] proved that it can also influence model behavior, particularly in adversarial contexts, where models compressed through these methods may exhibit different susceptibilities to jailbreaking techniques. Understanding how compression affects model robustness and alignment remains an underexplored area, with trade-offs between model size and safety emerging as a critical concern. This paper aims to address key gaps in the current jailbreaking literature by proposing a new framework for LLM vulnerability assessment. First, we introduce HarmLevelBench, a novel dataset comprising queries across 7 harmful topics, each further categorized into 8 distinct levels of severity, enabling a fine-grained analysis of model responses. Second, we conduct a comprehensive performance comparison of 7 state-of-the-art jailbreaking techniques applied to this dataset, offering insights into their effectiveness across various harm levels. Finally, we examine how quantization techniques, such as AWQ and GPTQ, influence the alignment and robustness of models, revealing trade-offs between resilience to transferred attacks and vulnerability to direct ones."
https://arxiv.org/html/2411.06613v1,"Are Neuromorphic Architectures Inherently Privacy-preserving?
An Exploratory Study","While machine learning (ML) models are becoming mainstream, including in critical application domains, concerns have been raised about the increasing risk of sensitive data leakage. Various privacy attacks, such as membership inference attacks (MIAs), have been developed to extract data from trained ML models, posing significant risks to data confidentiality. While the mainstream work in the ML community considers traditional Artificial Neural Networks (ANNs) as the default neural model, neuromorphic architectures, such as Spiking Neural Networks (SNNs) have recently emerged as an attractive alternative mainly due to their significantly low power consumption. These architectures process information through discrete events, i.e., spikes, to mimic the functioning of biological neurons in the brain. While the privacy issues have been extensively investigated in the context of traditional ANNs, they remain largely unexplored in neuromorphic architectures, and little work has been dedicated to investigate their privacy-preserving properties. In this paper, we investigate the question whether SNNs have inherent privacy-preserving advantage. Specifically, we investigate SNNs’ privacy properties through the lens of MIAs across diverse datasets, comparatively with ANNs . We explore the impact of different learning algorithms (surrogate gradient and evolutionary learning), programming frameworks (snnTorch, TENNLab, and LAVA), and various parameters on the resilience of SNNs against MIA. Our experiments reveal that SNNs demonstrate consistently superior privacy preservation compared to ANNs, with evolutionary algorithms further enhancing their resilience. For example, on the CIFAR-10 dataset, SNNs achieve an AUC as low as 0.59 compared to 0.82 for ANNs, and on CIFAR-100, SNNs maintain a low AUC of 0.58, whereas ANNs reach 0.88. Furthermore, we investigate the privacy-utility trade off through Differentially Private Stochastic Gradient Descent (DPSGD) observing that SNNs incur a notably lower accuracy drop than ANNs under equivalent privacy constraints.","As ML systems become more sophisticated and widespread, individuals are increasingly relying on these systems, entrusting them with personal and professional data. Consequently, the risk of sensitive information exposure is growing significantly in multiple sectors (Bertino, 2016) including healthcare (Abouelmehdi et al., 2017), finance (Tripathi and Mukhopadhyay, 2020), national security (Thuraisingham, 2002), education (Florea and Florea, 2020) and consumer services (Lee et al., 2015). It is particularly alarming in fields such as healthcare, where the confidentiality of patient data is extremely sensitive as a breach could result in severe personal and financial implications, affecting patient care and institutional credibility (Luo et al., 2018; JM et al., 2018; U.S. Department of Health & Human Services, 2024). In finance, the integrity of financial transactions and records is fundamental for maintaining market stability and preventing fraud (Yu and He, 2021), while in national security , safeguarding classified information is essential to protect national interests and prevent threats to public safety (Kim et al., 2010). This has led to the development of various privacy attacks targeting ML models to extract sensitive information, including Model Inversion Attacks (Fredrikson et al., 2015), Attribute Inference Attacks (Gong and Liu, 2018), Model Stealing Attacks (Juuti et al., 2019), and Membership Inference Attacks (MIAs) (Shejwalkar et al., 2021). Among these, MIAs are particularly prominent as a significant threat to data privacy (Salomon, 2012), wherein an adversary seeks to ascertain if a specific data point was part of the dataset used to train the model. This intrusion risks exposing sensitive information about individuals in the training dataset, potentially compromising personal data confidentiality (Shokri et al., 2017). While most research on privacy attacks has concentrated on ANNs, there is limited exploration within neuromorphic computing, particularly with Spiking Neural Networks (SNNs). Designed to emulate the dynamic behavior of biological neurons (Ghosh-Dastidar and Adeli, 2009), SNNs process information through discrete, temporally encoded spikes (Roy et al., 2019), enabling them to handle time-sensitive data efficiently (Hong et al., 2017). Their suitability for edge computing (Shi et al., 2016) and resource-constrained environments further enhances their value, as SNNs effectively process real-world spatiotemporal patterns (Wang et al., 2018). This capability positions SNNs as a promising alternative to traditional neural networks for applications requiring dynamic, real-time data processing. This work addresses the privacy concerns associated with SNNs through a structured investigation of three core areas: (i) the resilience of ANN and SNN models to Membership Inference Attacks (MIAs), (ii) the factors influencing the privacy-preserving properties of SNNs, and (iii) the privacy-utility trade-off in ANN and SNN models using the DPSGD algorithm. We consider that the potential resilience of SNNs against MIAs is based on two key aspects. Firstly, the non-differentiable and discontinuous nature of SNNs may weaken the correlation between the model and individual data points, making it more challenging for an attacker to identify the membership of a particular data point in the training set (Meng et al., 2022). Secondly, the unique encoding mechanisms employed by SNNs introduce an additional layer of stochasticity (Olin-Ammentorp et al., 2021) and variability to the data representation. This added complexity can make it more difficult for an attacker to deduce the unique characteristics of individual data points, thereby making them more indistinguishable. Investigating the resilience of SNNs against MIAs, our experimental results consistently demonstrated that SNNs exhibit higher resilience to MIAs across the datasets including MNIST, F-MNIST, Iris, Breast Cancer, CIFAR-10, CIFAR-100, and ImageNet. This is evidenced by the lower Area Under the Curve (AUC) values for the Receiver Operating Characteristic (ROC) curves in SNNs compared to their ANN counterparts. Furthermore, our exploration domain encompassed various learning algorithms (surrogate gradient-based and evolutionary learning), programming frameworks (snnTorch, TENNLab, and LAVA), and a wide range of parameters within them, providing a comprehensive analysis of the factors influencing the inherent privacy-preserving properties of SNNs. This in-depth exploration revealed that evolutionary learning algorithms have shown to boost this resilience more effectively compared to the gradient-based methods. In order to enhance data privacy and explore the compromises between privacy and utility, we study the implementation of the DPSGD algorithm as a privacy defense mechanism (Xu et al., 2021). This introduces controlled noise into the training process, making it harder for attackers to infer the presence of specific data points. However, improved privacy often comes at the cost of reduced model performance, known as the privacy-utility trade-off (Song et al., 2013). Through the experiments, we observe that SNNs exhibit a notably lower performance drop compared to ANNs for the same level of privacy guarantee. This finding further reinforces our hypothesis regarding the inherent privacy-preserving properties of SNNs. This paper offers the following notable findings in the field of data privacy, particularly in the context of SNNs: • SNNs exhibit higher resilience against MIAs compared to ANNs, with lower AUC scores on CIFAR-10 (SNN: 0.59 vs. ANN: 0.82) and CIFAR-100 (SNN: 0.58 vs. ANN: 0.88), highlighting their potential as a more secure alternative in privacy-sensitive applications. • Evolutionary learning algorithms outperform gradient-based methods in MIA resilience, maintaining a consistent AUC of 0.50 across all parameters for Iris and Breast Cancer datasets, compared to 0.57 and 0.55 AUC scores for gradient-based algorithms, respectively. • Privacy-utility tradeoff analysis revealing that SNNs incur a lower accuracy drop compared to ANNs when applying DPSGD: For F-MNIST, with privacy guarantees ranging from 0.22 to 2.00, the average accuracy drop is 12.87% for SNNs, significantly lower than the 19.55% drop observed in ANNs. It should be emphasized that while this investigation highlights SNNs’ enhanced privacy characteristics, these findings are specifically contextualized within privacy-preservation applications. The architectural properties of SNNs that facilitate efficient hardware implementation and reduced computational overhead make them particularly appealing for resource-constrained environments. However, these advantages should not be interpreted as a general superiority of SNNs over ANNs across all applications. Rather, this work is motivated by the intuition that SNNs’ unique information processing mechanisms may offer specific advantages in privacy preservation, warranting systematic investigation in this particular domain."
https://arxiv.org/html/2411.06606v1,Gen-AI for User Safety: A Survey,"Machine Learning and data mining techniques (i.e. supervised and unsupervised techniques) are used across domains to detect user safety violations. Examples include classifiers used to detect whether an email is spam or a web-page is requesting bank login information. However, existing ML/DM classifiers are limited in their ability to understand natural languages w.r.t the context and nuances. The aforementioned challenges are overcome with the arrival of Gen-AI techniques, along with their inherent ability w.r.t translation between languages, fine-tuning between various tasks and domains.In this manuscript, we provide a comprehensive overview of the various work done while using Gen-AI techniques w.r.t user safety. In particular, we first provide the various domains (e.g. phishing, malware, content moderation, counterfeit, physical safety) across which Gen-AI techniques have been applied. Next, we provide how Gen-AI techniques can be used in conjunction with various data modalities i.e. text, images, videos, audio, executable binaries to detect violations of user-safety. Further, also provide an overview of how Gen-AI techniques can be used in an adversarial setting. We believe that this work represents the first summarization of Gen-AI techniques for user-safety.","Machine Learning and data mining techniques are used across domains to detect violation of user safety. Examples include classifiers used to detect whether an email is spam or a web-page is requesting bank login information. However, existing ML/DM classifiers are limited in their ability to understand natural languages w.r.t the context and nuances. The aforementioned challenges are overcome with the arrival of Gen-AI techniques, along with there inherent ability w.r.t translation between languages, fine-tuning between various tasks and domains. In this manuscript, we provide an overview of the various domains across which user safety can be violated. In particular, we provide more information on how Generative Artificial Intelligence (Gen-AI) techniques can be used towards reduction of egregious abuse (e.g. phishing, malware, anomaly detection, counterfeit, fraud prevention), misinformation and disinformation (e.g. fake news , deepfake detection), increase in content moderation, awareness about mental health (e.g. cyber-bullying prevention, crisis support) and towards robust physical safety (e.g. accessibility, autonomous systems). Further, we discuss how Gen-AI techniques can be used across various data modalities. In particular, we present how Gen-AI techniques can be used to detect user-safety violations in text and rather outperform all previous techniques w.r.t NLP tasks such as entity recognition, question answering and sentiment analysis. Further, Gen-AI techniques with their inherent ability to parse and understand images provides an easy mechanism to detect image manipulation, deepfake detection. The advantages of Gen-AI techniques go beyond text and images to other data modalities such as videos, audio and executable binaries. We also discuss how Gen-AI techniques can be used in an adversarial setting. In particular, we present how these techniques can be used to attack at scale (e.g. mass spam). Further, the attacks become more intelligent as these techniques can target humans more effectively and engage with humans with a similar cognitive capacity. Gen-AI techniques along with reinforcement learning techniques can be used to create more sophisticated attacks while using feedback from the last failure. Further, Gen-AI techniques can also make these attacks look very personalized (e.g. deep-fakes) and second order effects (e.g. Gen-AI imitating human sounding text). The organization of the paper is as follows. In section II, we provide a comprehensive overview of the various user safety domains, where Gen-AI techniques can be applied. In section-3, we discuss the various data modalities across which Gen-AI techniques can be utilized to protect user safety violation. In Section-4, we present, how Gen-AI techniques can also be used in an adversarial setting. In Section-5, we present our opinion on what does the future look like for Gen-AI techniques. Finally, in section-6, we conclude this manuscript."
https://arxiv.org/html/2411.06586v1,Combining Entangled and Non-Entangled based Quantum Key Distribution Protocol with GHZ state,"This paper presents a novel hybrid Quantum Key Distribution (QKD) protocol that combines entanglement-based and non-entanglement based approaches to optimize security and the number of generated keys. We introduce a dynamic system that integrates a three-particle GHZ-state method with the two-state B92 protocol, using a quantum superposition state to probabilistically switch between them. The GHZ-state component leverages strong three-particle entanglement correlations for enhanced security, while the B92 component offers simplicity and potentially higher key generation rates. Implemented and simulated using Qiskit, our approach demonstrates higher number of generated keys compared to standalone protocols while maintaining robust security. We present a comprehensive analysis of the protocol’s security properties and performance characteristics. The results show that this combined method effectively balances the trade-offs inherent in QKD systems, offering a flexible framework adaptable to varying channel conditions and security requirements. This research contributes to ongoing efforts to make QKD more practical and efficient, potentially advancing the development of large-scale, secure quantum networks.","Quantum Key Distribution (QKD) [1] has emerged as a promising technology for secure communication in the quantum era. By leveraging the principles of quantum mechanics, QKD offers the potential for information-theoretic security, a feat unattainable with classical cryptographic methods. As we stand on the brink of a new technological paradigm, the importance of robust and efficient QKD protocols cannot be overstated. Traditional QKD protocols can be broadly categorized into two types: those based on entanglement, such as the E91 [2] protocol, and those that do not require entanglement, like the BB84 [3] or B92 [4] protocols. Each approach has its strengths and limitations. Entanglement-based protocols offer stronger security guarantees through quantum correlations but can be more challenging to implement and may have lower key generation rates. Non-entanglement based protocols, on the other hand, can be simpler to implement and potentially faster, but may be more vulnerable to certain types of attacks. In this paper, we propose a novel approach that combines the strengths of both entangled and non-entangled QKD protocols. Specifically, we introduce a hybrid protocol that integrates a GHZ-state [5] based method (Entanglement-based protocol) with the B92 protocol (Non-entanglement-based protocol). The GHZ-state component leverages the strong correlations of three-particle entanglement, providing enhanced security, while the B92 component offers simplicity and potentially higher number of generated keys under certain conditions. The primary goal of this combined approach is to achieve a higher overall key generation rate while maintaining the high level of security afforded by entanglement-based methods. By dynamically switching between the two protocols based on a quantum superposition state, we aim to optimize the trade-off between security and efficiency. Our work builds upon the foundational research in quantum key distribution, including the seminal work of Bennett on B92, and the three-particle entanglement studies of Greenberger, Horne, and Zeilinger. We extend these ideas by proposing a unified framework that can adapt to varying channel conditions and security requirements. In the following sections, we will provide a detailed description of our combined protocol, analyze its security properties, and present simulation results using the Qiskit [6] quantum computing framework. We will demonstrate how this hybrid approach can potentially outperform single-protocol implementations in terms of key generation rate while maintaining robust security guarantees. Through this research, we aim to contribute to the ongoing efforts to make QKD more practical and efficient, bringing us closer to the realization of large-scale, secure quantum networks."
https://arxiv.org/html/2411.06441v1,Detecting AutoEncoder is Enough to Catch LDM Generated Images,"In recent years, diffusion models have become one of the main methods for generating images. However, detecting images generated by these models remains a challenging task. This paper proposes a novel method for detecting images generated by Latent Diffusion Models (LDM) by identifying artifacts introduced by their autoencoders. By training a detector to distinguish between real images and those reconstructed by the LDM autoencoder, the method enables detection of generated images without directly training on them. The novelty of this research lies in the fact that, unlike similar approaches, this method does not require training on synthesized data, significantly reducing computational costs and enhancing generalization ability. Experimental results show high detection accuracy with minimal false positives, making this approach a promising tool for combating fake images.","In the past few years, there has been significant progress in image generation. New diffusion models such as DDPM [1] and DDIM [2] have outperformed the previous state-of-the-art approach, GANs, in image generation [3] and introduced a new paradigm for generative modeling. Although these diffusion-based models achieve better quality for generated images, they have a significant drawback – they require extensive computational resources for training and generation. Additionally, they cannot be easily scaled to generate high-resolution images because they operate in a space with numerous dimensions – the pixel space. In [4] the authors proposed a new method called LDM (Latent Diffusion Model), which operates in a much smaller latent space instead of the pixel space. By conditioning the generation process on text, it became possible to generate images based on text prompts. LDM trained and generated new samples much faster, and it became a standard for text-to-image generation. Using large datasets of images and text, such as LAION-5B [5], companies began to introduce image generation capabilities into their products. Examples include MidJourney, DALLE 3, and Adobe FireFly, which allow non-technical users to create images using text queries. Some companies provide their models and code for training as open-source, such as Stable Diffusion. This enables enthusiasts to customize these models and disable built-in watermarks. The rise of user-friendly graphical interfaces for image generation, such as ComfyUI[6] and stable-diffusion-webui[7], further lowers the entry barrier for creating images. These tools allow users to generate images autonomously without relying on third parties. In 2022, OpenAI reported that users were creating more than 2 million images daily using DALL-E [8], and in 2023, Adobe reported that Adobe FireFly generated over 1 billion images in 3 months after its launch [9]. Figure 1 shows the trends of search queries related to image generation. Figure 1: Popularity graph of image generation-related search queries on Google To prevent the spread of misinformation through generated images and to properly label them, reliable detectors are needed, with a focus on minimizing false positives. This study presents a simple yet effective approach for detecting images generated by latent diffusion models. We hypothesize that artifacts introduced by the encoder and decoder of the autoencoder, which projects images from the RGB pixel space to a lower-dimensional latent space and back, are sufficient for detecting images generated by latent diffusion models. This also simplifies the detection of manipulations such as inpainting (partial image editing), as they require projecting images into the latent space. Unlike other methods, our solution leverages artifacts introduced by the autoencoder encoder and decoder to effectively detect LDM-generated images without the need for training on the generated images themselves. The proposed method shows high generalization, resilience to distortions such as JPEG compression, and significantly reduces computational costs, making it an efficient tool for detecting images created by various latent diffusion models. This constitutes the scientific novelty and practical significance of the proposed solution. The paper is organized as follows: Section II provides an overview of the current state of research and practical results in detecting diffusion model-generated images. Section III describes the proposed solution, a method for detecting latent diffusion models without training on generated images. Section IV presents experimental results demonstrating the effectiveness of the proposed approach. Section V offers a brief analysis of the strengths and weaknesses of the solution. Finally, the conclusion summarizes the findings and suggests future research directions."
https://arxiv.org/html/2411.06291v1,TinyML NLP Approach for Semantic Wireless Sentiment Classification,"Natural Language Processing (NLP) operations, such as semantic sentiment analysis and text synthesis, may often impair users privacy and demand significant on device computational resources. Centralized learning (CL) on the edge offer alternative energy efficient approach, yet requires the collection of raw information, which affects the users privacy. While Federated learning (FL) preserves privacy, it requires high computational energy on board tiny user devices. We introduce split learning (SL) as an energy efficient alternative, privacy preserving tiny machine learning (TinyML) scheme and compare it to FL and CL in the presence Rayleigh fading and additive noise. Our results show that SL reduces processing power and CO2 emissions while maintaining high accuracy, whereas FL offers a balanced compromise between efficiency and privacy. Hence, this study provides insights into deploying energy-efficient, privacy-preserving NLP models on edge devices.","Artificial Intelligence (AI) has gained widespread adoption, with applications ranging from text and image classification to text and image generation. A key area of growth is NLP, which powers virtual assistants and human language classification systems. Large language models (LLMs), such as OpenAI’s GPT series [1], Gemini [2], and BERT, built on Transformer architectures [3], have transformed NLP by enabling machines to perform complex language tasks with high accuracy. However, these models require significant computational resources for both training and inference, posing challenges for deployment on resource-constrained devices. LLMs demand substantial storage and processing power. The high dimensionality of language data and large vocabularies further increase the computational burden [4]. Data privacy is another significant concern, as training robust models often requires diverse datasets, which can raise issues when handling sensitive information, such as in healthcare or government sectors. Additionally, deploying models on edge devices introduces communication challenges. Wireless data transmission over channels like WiFi is susceptible to noise, fading, limited bandwidth and unstable connections, which ,ay affect communication efficiency. TinyML mixed models have evolved to address the above concerns. For instance, model compression techniques, such as pruning [5], quantization [6], and knowledge distillation [7], help reduce model sizes and computational requirements. TinyBERT [8] and MobileBERT [9], for example, utilize knowledge distillation to create smaller models, while quantization techniques, like in LLaMA [10], enable efficient execution on low-resource devices. FL offers a decentralized model training approach, allowing multiple users to collaboratively train a global model while keeping their data local. Although, this method addresses data privacy concerns [11], it might be power hungry for tiny device batteries. To this end, SL [12] divides the model training process between users and a server. Users transmit activations from the initial model layers, reducing computational load and enhancing data privacy. In this paper, we propose two frameworks to address computational challenges and privacy concerns in text emotion classification: one based on FL and the other on SL. First, we focus on semantic extraction by utilizing quantization to significantly reduce model size, enabling deployment on resource-limited devices. Second, we emphasize privacy preservation by ensuring that data remains on the device while transmitting only the necessary information for model training. The proposed methods prove roubustness in challenging channel conditions while reducing the overall energy consumption and CO2 emissions, contributing to TinyML initiatives. The remainder of this paper is structured as follows: Section II covers the system design and experimental setup, including both frameworks of FL and SL techniques. Section III presents the experimental results, focusing on accuracy, energy consumption, and the impact of noise and fading. Finally, Section IV provides conclusions and discusses potential directions for future work. Figure 1: Federated learning system design"
https://arxiv.org/html/2411.06263v1,"Federated Split Learning for Human Activity
Recognition with Differential Privacy","This paper proposes a novel intelligent human activity recognition (HAR) framework based on a new design of Federated Split Learning (FSL) with Differential Privacy (DP) over edge networks. Our FSL-DP framework leverages both accelerometer and gyroscope data, achieving significant improvements in HAR accuracy. The evaluation includes a detailed comparison between traditional Federated Learning (FL) and our FSL framework, showing that the FSL framework outperforms FL models in both accuracy and loss metrics. Additionally, we examine the privacy-performance trade-off under different data settings in the DP mechanism, highlighting the balance between privacy guarantees and model accuracy. The results also indicate that our FSL framework achieves faster communication times per training round compared to traditional FL, further emphasizing its efficiency and effectiveness. This work provides valuable insight and a novel framework which was tested on a real-life dataset.","In recent years, the widespread adoption of mobile devices has led to a growing interest in human activity recognition (HAR) using wearable sensors [1], [2]. This area has emerged as a novel research focus within artificial intelligence and pattern recognition [3]. This field intersects artificial intelligence and pattern recognition, with applications ranging from sports activity detection and smart homes to health support and beyond. By leveraging sensor data from devices like smartphones and wearables, HAR enables real-time monitoring and analysis of human activities. This capability is pivotal in enhancing personalized healthcare, improving athletic performance analysis, and developing intelligent environments that adapt to users’ needs seamlessly. Modern HAR systems, particularly those powered by deep learning techniques, offer several advantages over traditional methods [4]. They can automatically learn relevant features from raw sensor data, eliminating the need for manual feature engineering and thus improving accuracy and robustness. Moreover, deep learning models can handle complex, nonlinear relationships in the data, leading to better generalization across different users and activity types. Furthermore, HAR contributes to the advancement of human-computer interaction by enabling natural interfaces that respond to users’ physical actions and gestures. This technology has the potential to revolutionize how users interact with devices and how devices understand and respond to human behavior in real-world settings. However, the widespread adoption of HAR is constrained by challenges such as privacy concerns associated with personal data collected from users’ devices. Addressing these challenges requires innovative approaches in data anonymization, secure data storage, and compliance with regulations like General Data Protection Regulation (GDPR). Federated Learning (FL) concepts have risen as potential solutions to privacy concerns. FL is a collaborative framework that takes advantage of user devices’ enhanced computational power. In FL multiple users collaboratively train a machine learning model without sharing their personal data with the server, or other users. A global model is downloaded from the server by all user devices, then the users individually train the model using their local data, then finally the server averages the parameters of all users to form a new global model. This process is then repeated until the desired performance has been reached and shared with the client devices. Some models require many parameters which signify a potential limitation of FL. New concepts involving Split Learning (SL) have been developed as potential solutions to the limitations of FL [5],[6]. In SL, a model is initiated and then split into two models, the client-side model and the server-side model. Clients download their side of the model, which involves the input layer and preceding layers until the pre-defined cut layer. The Server-side model contains the rest of the model starting from the cut layer, to the output layer. Training begins at the client-side model with the client’s raw data until the cut layer is reached, the intermediate activations at this layer are then sent to the server to continue training. The server then trains its model up to the output layer, computes the loss, and starts back-propagation which will be completed on the client’s side. Federated Split Learning (FSL) has been developed to take advantage of both FL’s collaborative framework and SL’s splitting structure. The training process in FSL is similar to that in SL until the completion of the backward pass. The weights of the client models are then aggregated to produce a new global client model, and the server-side weights are updated based on the computed gradients. This method efficiently trains a deep neural network model while preserving user data privacy. I-A Related Works Several state-of-the-art techniques have introduced feature extraction and selection methods for HAR using traditional machine learning classifiers. With the emergence and advancement of high computational resources, deep learning techniques have become widely used in various HAR systems. These techniques efficiently retrieve features and perform classification, significantly enhancing the performance of HAR systems. Recently, FL has been studied to further improve the performance of HAR. Specifically, the authors in [7] evaluated FL for training a HAR classifier and compared it to centralized learning using two models—a deep neural network and softmax regression—trained on synthetic and real-world datasets. In [8], an FL system was proposed for HAR, with a perceptive extraction network for feature extraction to improve recognition accuracy. The study in [9] proposed a prototype-guided FL framework for HAR that addresses data issues in real-world environments by efficiently decoupling representation and classifier in heterogeneous FL settings. The work in [10] proposed an FL system for HAR that dynamically learns personalized models by capturing user relationships and iteratively merging model layers based on user similarity. As reported in [11], the authors designed a 2-dimensional FL framework to address data insufficiency and security in cyber-physical-social systems. Recently, SL and SFL has been recently studied in various IoT domains. The authors in [12] comprehensively surveyed the emerging applications of FL in IoT networks, exploring FL’s potential across various IoT services: data sharing, offloading, caching, attack detection, localization, mobile crowdsensing, and privacy as well as security enhancements. In [13], the authors proposed an efficient SL framework for resource-constrained edge computing systems, combining the benefits of FL and SL. The authors in [14] introduced a modified SL system with an autoencoder and adaptive threshold mechanism, reducing communication and computation overhead in an IoT system with minimal performance loss. The work in [15] proposed adaptive resource-aware SL for efficient IoT model training, accelerating processes on resource-constrained devices and minimizing straggler effects with device-targeted split points, while adapting to varying network throughput and computing resources. A dynamic FSL framework was developed by the researchers in [16] to address data and resource heterogeneity in IoT, enhancing efficiency through resource-aware split computing of deep neural networks and dynamic clustering of training participants. Despite these research efforts, the application of FSL has not been investigated for HAR in the literature. Exploring FSL in HAR is crucial as it paves the way for scalable and efficient deployment of personalized activity recognition systems in IoT and wearable technology, catering to individualized user needs while respecting data privacy and security. I-B Our Key Contributions In this paper, we propose a novel collaborative privacy-enhanced HAR framework through the development of an FSL algorithm with differential privacy (DP) [17] in edge computing. The contributions of this paper are summarized as follows. 1. We present an FSL algorithm for collaborative privacy-enhanced HAR in edge computing. In this framework, edge devices (EDs) are involved to partially perform forward propagation and backpropagation on the client-side models. The remaining workload of the model training will be executed at the edge server through the server-side model, which allows for mitigating the computation burden on resource-constrained EDs. 2. Moreover, to enhance privacy protection for activations, a DP mechanism is integrated at EDs to add a mask to hidden the shared information against potential data threats. 3. We conduct simulations on real-life HAR datasets to verify the feasibility of our FSL method. Implementation results demonstrate that our approach can significantly enhance the training performance with much lower training latency, compared with traditional training methods. We also validates the impacts of DP in HAR performance under different training settings. I-C Paper Structure The rest of the paper is structured as follows. In Section II, we present our system model, detailing the architecture and components of our proposed system. Section III presents simulations and performance evaluations for the proposed FSL approach under different network settings. Finally, Section IV concludes the paper."
https://arxiv.org/html/2411.05987v1,Multiuser Commitment over Noisy Channels,"We consider multi-user commitment models that capture the problem of enabling multiple bidders to simultaneously submit auctions to verifiers while ensuring that i) verifiers do not obtain information on the auctions until bidders reveal them at a later stage; and, ii) bidders cannot change their auction once committed. Specifically, we assume that bidders and verifiers have access to a noiseless channel as well as a noisy multiple-access channel or broadcast channel, where inputs are controlled by the bidders and outputs are observed by verifiers. In the case of multiple bidders and a single verifier connected by a non-redundant multiple-access channel, we characterize the commitment capacity region when bidders are not colluding. When the bidders are colluding, we derive an achievable region and a tight converse for the sum rate. In both cases our proposed achievable commitment schemes are constructive. In the case of a single bidder and multiple verifiers connected by a non-redundant broadcast channel, in which verifiers could drop out of the network after auctions are committed, we also characterize the commitment capacity region. Our results demonstrate how commitment schemes can benefit from multi-user protocols, and develop resilience when some verifiers may become unavailable.","Commitment without the need for a trusted third party can be traced back to Blum’s coin-flipping problem [2]. More generally, a two-party commitment problem involves a bidder, Alice, and a verifier, Bob, and operates in two phases. In the first phase, called the commit phase, Alice sends Bob information to commit to a message M, representing a bid in an auction that must remain concealed from Bob. In the second phase, called the reveal phase, Alice reveals a message M^{\prime} to Bob, who must determine whether M^{\prime} is the message that Alice committed to in the commit phase. The protocol must be binding in the sense that, in the reveal phase, Alice cannot make Bob believe that she committed to a message M^{\prime}\neq M. It is well-known that information-theoretic concealment guarantees cannot be achieved over noiseless communication channels, e.g., [3]. However, when a noisy channel is available as a resource, both concealment and binding requirements can be obtained under information-theoretic guarantees, i.e., when Alice and Bob are not assumed to be computationally limited, for some class of noisy channels called non-redundant [4]. Most of the literature on information-theoretic commitments focuses on two-party scenarios that involve a single bidder and a single verifier, e.g., [5, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], characterizing commitment capacity under increasingly complex channel models. We study here instead two specific multi-user commitment settings: one in which a verifier interacts with L bidders, each committing to individual messages, and another in which a bidder commits to B verifiers, some of whom may drop out after the commit phase. The motivation for our first setting is to explore whether a multi-bidder protocol can outperform single-bidder protocols and time-sharing when multiple bidders wish to commit to individual messages. Our motivation to consider multiple verifiers in our second setting, is to ensure positive commitment rates even if a verifier drops out of the network after the commit phase, which would be impossible with a single verifier. In our first setting, L bidders and the verifier have access to a noiseless public communication channel and a noisy discrete memoryless multiple-access channel with L inputs. Each input of the multiple-access channel is controlled by a distinct bidder and the verifier observes the output of the channel. Similar to a two-party setting, the protocol consists of a commit phase and a reveal phase. Here, the concealment requirement is that the verifier must not learn, in an information-theoretic sense, information about any message of any bidder after the commit phase. The protocol must also be information-theoretically binding in the sense that, during the reveal phase, a bidder cannot make the verifier believe that it committed to another message than the one committed to in the commit phase. For this setting, we consider both the cases of colluding and non-colluding bidders. The non-colluding bidders case corresponds to a scenario in which the bidders do not trust each other and do not want to exchange information with one another. For instance, this would be the case when the bidders commit to messages sent to an auctioneer. Under a non-redundancy condition on the multiple-access channel, we derive the capacity region for the non-colluding bidders case, and an achievable region and the sum-rate capacity for the colluding bidders case. In both cases, our achievability scheme is constructive and relies on distributing hashing with two-universal hash functions [16] for the concealment guarantees. The bindingness of our achievability scheme hinges on the non-redundancy property of the multiple access channel akin to the two-party commitment in [4]. The characterization of the sum-rate capacity relies on the polymatroidal properties of our achievability region. Additionally, we demonstrate through a numerical example that, for some channels, using a multi-bidder protocol can outperform using single-bidder protocols, e.g., [4], and time-sharing. In our second setting, a single bidder can interact with B verifiers through a noiseless public communication channel and a noisy discrete memoryless broadcast channels with B outputs. The input of the channel is controlled by the bidder and each output is observed by a verifier. Similar to our first setting, the protocol consists of a commit phase and a reveal phase, and must guarantee information-theoretic bindingness and concealment. Our results demonstrate that introducing multiple verifiers can mitigate situations in which verifiers could drop out of the network after the commit phase, as long as one verifier is available during the reveal phase to validate the committed message. For this setting, we derive the commitment capacity under a non-redundancy property of the channel. As a byproduct of independent interest, we show a simple sufficient condition, in terms of injectivity of the transition probability matrix of the channel, to guarantee channel non-redundancy. Additionally, we prove that, for channels whose input alphabet is at most three, our sufficient condition is also necessary, and thus equivalent to the non-redundancy characterization in [4]. The remainder of the paper is organized as follows. After a review of notation in Section II, we develop an auxiliary result in Section III, offering a sufficient condition to guarantee non-redundancy and an alternative characterization of non-redundant channels with input alphabet of cardinality at most three. We formally introduce our multiuser commitment models in Section IV along with the associated results. We delegate the proofs to Section V and Section VI to streamline the presentation. Finally, we provide concluding remark in Section VII."
https://arxiv.org/html/2411.05857v1,Financial Fraud Detection using Jump-Attentive Graph Neural Networks,"As the availability of financial services online continues to grow, the incidence of fraud has surged correspondingly. Fraudsters continually seek new and innovative ways to circumvent the detection algorithms in place. Traditionally, fraud detection relied on rule-based methods, where rules were manually created based on transaction data features. However, these techniques soon became ineffective due to their reliance on manual rule creation and their inability to detect complex data patterns. Today, a significant portion of the financial services sector employs various machine learning algorithms, such as XGBoost, Random Forest, and neural networks, to model transaction data. While these techniques have proven more efficient than rule-based methods, they still fail to capture interactions between different transactions and their interrelationships. Recently, graph-based techniques have been adopted for financial fraud detection, leveraging graph topology to aggregate neighborhood information of transaction data using Graph Neural Networks (GNNs). Despite showing improvements over previous methods, these techniques still struggle to keep pace with the evolving camouflaging tactics of fraudsters and suffer from information loss due to over-smoothing. In this paper, we propose a novel algorithm that employs an efficient neighborhood sampling method, effective for camouflage detection and preserving crucial feature information from non-similar nodes. Additionally, we introduce a novel GNN architecture that utilizes attention mechanisms and preserves holistic neighborhood information to prevent information loss. We test our algorithm on financial data to show that our method outperforms other state-of-the-art graph algorithms.","The rise of online payment services has significantly benefited users and financial institutions but also led to a surge in financial fraud. In 2023, e-commerce fraud alone amounted to $38 billion and is projected to exceed $362 billion between 2023 and 2028 [1]. Detecting various types of financial fraud, such as identity theft and credit card fraud, is crucial to mitigate these losses. Traditional fraud detection methods relied on rule-based systems [2], where manual rules flagged suspicious transactions. Initially effective, these methods became inadequate as fraudsters adapted and patterns evolved. Machine learning models like logistic regression, XGBoost, and neural networks [3], [15], [5], [14] improved fraud detection by statistically analyzing transaction data but failed to capture interactions between transactions and struggled with evolving fraud patterns. Graph representation learning revolutionized fraud detection by representing transaction data as graphs and using embeddings to train supervised models. Graph Neural Networks (GNNs) [31], [20], [32] advanced the field by leveraging graph topology and node properties. Despite their success, GNNs face challenges like over-smoothing and inability to adapt to rapidly changing fraud patterns. Graph Attention Networks (GATs) [22] improved upon this by using attention mechanisms to focus on relevant parts of the graph. However, they still struggle with over-smoothing, causing important features to be lost, especially in complex networks with family-based clusters where legitimate and fraudulent behaviors intertwine. This may lead to missed detections of camouflaging fraudsters who embed themselves within legitimate clusters. In this paper, we propose a novel approach to address these challenges. Our method introduces a new sampling strategy and a GNN architecture called Jump-Attentive Graph Neural Network (JA-GNN). This architecture captures crucial neighborhood information and preserves key features from non-similar nodes, enhancing the system’s accuracy and responsiveness to new and evolving fraud tactics. The key contributions of this paper are: • We introduce a novel sampling strategy that efficiently gathers neighborhood information and identifies camouflaged fraudsters, while also including non-similar nodes to prevent information loss. • We present JA-GNN, a novel GNN architecture that preserves crucial feature information from distant nodes in the final embedding. This is essential in financial fraud detection for distinguishing between legitimate and fraudulent transactions. JA-GNN incorporates residual connections from earlier layers to the output, creating a combined embedding that optimally represents the customer. • Our results show that our method outperforms state-of-the-art graph algorithms in detecting financial fraud."
https://arxiv.org/html/2411.05818v2,Open LLMs are Necessary for Current Private Adaptations and Outperform their Closed Alternatives,"While open Large Language Models (LLMs) have made significant progress, they still fall short of matching the performance of their closed, proprietary counterparts, making the latter attractive even for the use on highly private data. Recently, various new methods have been proposed to adapt closed LLMs to private data without leaking private information to third parties and/or the LLM provider. In this work, we analyze the privacy protection and performance of the four most recent methods for private adaptation of closed LLMs. By examining their threat models and thoroughly comparing their performance under different privacy levels according to differential privacy (DP), various LLM architectures, and multiple datasets for classification and generation tasks, we find that: (1) all the methods leak query data, i.e., the (potentially sensitive) user data that is queried at inference time, to the LLM provider, (2) three out of four methods also leak large fractions of private training data to the LLM provider while the method that protects private data requires a local open LLM, (3) all the methods exhibit lower performance compared to three private gradient-based adaptation methods for local open LLMs, and (4) the private adaptation methods for closed LLMs incur higher monetary training and query costs than running the alternative methods on local open LLMs. This yields the conclusion that, to achieve truly privacy-preserving LLM adaptations that yield high performance and more privacy at lower costs, taking into account current methods and models, one should use open LLMs.","Recently, there has been the trend of releasing open Large Language Models (LLMs), such as LLama [21, 60], Vicuna [11], or Mistral [27] as an alternative to their proprietary closed counterparts, such as GPT from OpenAI [2], Claude from Anthropic [4], or Gemini from Google [57]. Despite the significant progress in improving open LLMs, they are still outperformed in multiple tasks by closed LLMs [12], making the latter attractive even for learning tasks from highly private data. Since it was shown that private data can leak from the adaptations of LLMs [16, 17], in the last few months alone, an array of new methods for privacy-preserving adaptation of closed LLMs has been proposed by the machine learning community at multiple conferences (NeurIPS’23 [16] and ICLR’24 [25, 56, 63]). Given the lack of access to the closed LLMs parameters—which renders parameter-tuning based adaptations infeasible—they all rely on the generation of privacy-preserving discrete prompts. We detail their operational setup in Figure 1 (left). Method \Circled[/csteps/fill color=black,/csteps/inner color=white]A \Circled[/csteps/fill color=black,/csteps/inner color=white]B \Circled[/csteps/fill color=black,/csteps/inner color=white]C Open LLM DP-ICL [63] ✕ ✕ ✓ Not Needed PromptPATE [16] ✕ ✕ ✓ Not Needed DP-FewShotGen(1) [56] ✕ ✕ ✓ Not Needed DP-FewShotGen(2) [56] ✓ ✕ ✓ Needed DP-OPT [25] (✓) ✕ ✓ Needed \hdashlinePrivate Local Adaptation ✓ ✓ ✓ Needed Figure 1: Setup for Privacy Protection with Open vs Closed LLMs. The three parties involved are (1) an LLM provider who hosts the proprietary LLM, (2) a data curator, such as a company that curated private data, for example, of their customers’ previous transactions, and (3) a querying party, i.e., a customer of the company who wants to perform a new private transaction. There are three steps where privacy leaks: \Circled[/csteps/fill color=black,/csteps/inner color=white]A During the creation of the discrete prompt, the data curator’s private data leaks to the LLM provider. \Circled[/csteps/fill color=black,/csteps/inner color=white]B The private query of the querying party leaks to the LLM provider. \Circled[/csteps/fill color=black,/csteps/inner color=white]C Private information from the data curator leaks to the querying party through the returned answers of the prompted LLM [17]. Prior methods for closed LLMs [16, 56, 63] only provide protection against \Circled[/csteps/fill color=black,/csteps/inner color=white]C . None of them protects against \Circled[/csteps/fill color=black,/csteps/inner color=white]B . To prevent leakage through \Circled[/csteps/fill color=black,/csteps/inner color=white]A , they require access to a (powerful) local open LLM. As an alternative (dashed purple lines), the data curator could privately adapt the open LLM locally and let the querying party interact with this LLM, protecting against \Circled[/csteps/fill color=black,/csteps/inner color=white]A , \Circled[/csteps/fill color=black,/csteps/inner color=white]B , \Circled[/csteps/fill color=black,/csteps/inner color=white]C . In this work, we ask the simple yet impactful question of whether these efforts actually lead into the right direction towards the goal of achieving truly privacy-preserving LLM adaptations. Therefore, we thoroughly analyze the proposed methods both conceptually and empirically and compare them to alternatives that rely on privately adapting open local LLMs. In particular, we study each approach’s threat space, assumptions, and methodological limitations and perform extensive experiments using ten state-of-the-art open and closed LLMs of various sizes, including Vicuna, Llama 3, Open LLaMa, BERT, RoBERTa, the Pythia suite of models, Claude, two versions of GPT3 (Babbage and Davinci), and GPT4 Turbo —applied to multiple datasets both for classification and generation tasks. Our analyses cover the axes of privacy protection, performance in terms of privacy-utility trade-offs, and monetary costs for training and queries. Our results provide the following insights: (1) All current methods for adapting closed LLMs leak private query data (intended for the data owner) at inference time to the LLM provider. (2) Three out of the four methods studied also leak large fractions of the private training data to the LLM provider. The approaches that do not, require an additional locally deployed open LLM for prompt engineering. (3) All methods for closed LLMs yield lower final downstream performance than privacy-preserving local adaptations on open LLMs—even when the local methods rely on significantly smaller LLMs than their closed counterparts. (4) The training and query costs of the private adaptations of closed LLMs (API access costs imposed by the LLM provider) are significantly higher than the costs for private open LLM adaptations (estimated as the costs of training and querying on cloud-based hardware). We provide a condensed summary of our results in Figure 1 (right Table above), and Table 1. Overall, our results indicate that, from the perspective of effective privacy-preservation, current adaptations of open LLMs are strictly preferable over their closed LLM alternatives, since they are more private, more performant, and less expensive. Going beyond the concrete existing methods studied [16, 25, 56, 63], we then analyze the reasons behind the underwhelming results of privacy-preserving closed LLM adaptations and discuss potential directions for improvements. Table 1: Comparison of privacy protection, performance, and cost between private adaptations for closed vs open LLMs. We select the top-performing adaptations. For closed LLMs, we use DP-ICL [63] and leverage PrivateLoRA [64] on open LLMs for both tasks. We consider sentiment classification on SST2 and the dialog summarization on SAMSum. The training data is denoted by \mathcal{D}_{T} and the test queries by Q. Reveals represents which data are exposed to the LLM provider. The methods were trained with DP guarantees: \varepsilon=8 and \delta=1/N, where N is the number of examples in \mathcal{D}_{T}. We report the Performance (higher is better) on test data (where Acc denotes the classification accuracy). The cost (in $) is computed separately for training (Train) and for answering 10k test queries (Query). Note, the (estimated) number of parameters for closed LLMs is 1.76T for GPT4 Turbo and 175B for GPT3 Davinci, while Llama3 has only 8B and BART-Large is significantly smaller with 355M parameters. The adaptation of the open LLMs is more expensive on SST2 than on SAMSum due to the larger training data size for SST2 and a larger model. DP-ICL’s query cost is high due to the usage of an ensemble of 100 prompts to answer each query. In summary, open local LLM adaptations are more private, more performant, and less expensive. Adaptation LLM Type Model Task Reveals Performance\uparrow Train($) Query($) DP-ICL [63] Closed GPT4 Turbo SST2 \mathcal{D}_{T}+Q Acc=95.9_{\pm 0.1}\% 0 138.00 PrivateLoRA [64] Open Llama3-8B(instruct) SST2 None Acc=96.0_{\pm 0.1}\% 27.60 0.78 DP-ICL [63] Closed GPT3 Davinci SAMSum \mathcal{D}_{T}+Q RougeL=31.8_{\pm 0.3} 0 665.91 PrivateLoRA [64] Open BART-Large SAMSum None RougeL=39.1_{\pm 0.2} 3.63 0.80 On the way, to further strengthen private adaptations for open LLMs, we demonstrate how to locally apply privacy-preserving prompt-based methods to train generation tasks with high-performance—claimed impossible by prior work [35]. In particular, we show for the first time that private prompt tuning for text generation tasks PromptDPSGDGen can achieve comparable performance to private (full) fine-tuning and private low-rank adaptations (LoRA). Additionally, we demonstrate that ensemble-based few-shot prompts PromptPATEGen can privately generate high-quality text at a low privacy cost. In summary, we make the following contributions: 1. We perform a thorough conceptual and experimental study on existing privacy-preserving closed and open LLM adaptations, analyzing their threat space, assumptions, and achieved results. 2. Our extensive experiments on various open and closed LLMs and on multiple classification and generation tasks show that the local (gradient-based) adaptations outperform their current closed (discrete prompt-based) counterparts in terms of privacy, performance, and cost efficiency. 3. We propose differentially private prompts for text generation tasks that, for the first time, reach performance comparable to private LoRA or private fine-tuning."
