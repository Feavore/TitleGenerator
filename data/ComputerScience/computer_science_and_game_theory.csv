URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.10434v1,Fair Division via the Cake-Cutting Share,"In this paper, we consider the classic fair division problem of allocating m divisible items to n agents with linear valuations over the items. We define novel notions of fair shares from the perspective of individual agents via the cake-cutting process. These shares generalize the notion of proportionality by taking into account the valuations of other agents via constraints capturing envy. We study what fraction (approximation) of these shares are achievable in the worst case, and present tight and non-trivial approximation bounds as a function of n and m. In particular, we show a tight approximation bound of \Theta(\sqrt{n}) for various notions of such shares. We show this bound via a novel application of dual fitting, which may be of independent interest. We also present a bound of O(m^{2/3}) for a strict notion of share, with an almost matching lower bound. We further develop weaker notions of shares whose approximation bounds interpolate smoothly between proportionality and the shares described above. We finally present empirical results showing that our definitions lead to more reasonable shares than the standard fair share notion of proportionality.","Fair division of divisible resources among participants is a classic problem, dating back to Steinhaus (1948) from the 1940s. In this problem, there are m divisible resources, and n agents with linear utility functions over these resources. The goal is to allocate the resources to the agents in a fair fashion. Several notions of fairness have been proposed in the literature, the very first of which is proportionality: Consider the equitable split, where each resource is split equally among all agents. Then in any fair division, each agent should get at least as much utility as in an equitable split. In a sense, proportionality is the “bare minimum” notion of fairness, and typical fair allocations guarantee other properties in addition to proportionality. A different view of proportionality is via “cake cutting” Steinhaus (1948), made explicit by Budish (2011): Imagine a participant i does not know the utility functions of other participants, and needs to split the resources into n bundles. After agent i has split the resources, the other participants go first and choose one bundle each, with participant i getting the last pick. Suppose participant i wishes to maximize its utility in the worst case over the utility functions of the other agents. Then its optimal strategy is indeed to construct each bundle as having 1/n share of each resource. The max-min value of this game therefore defines a fair share for participant i, and coincides with its utility in a proportional allocation. A desirable property of the proportional share is that it is not only achievable by itself, but also achievable in combination with other fairness properties such as envy-freeness (see the “Nash welfare” allocation Nash (1950); Varian (1976)). On the other hand, the value of the fair share can be very small. As an example, there are n agents and m=n resources, where agent i has non-zero utility only for resource i. The proportional allocation only assigns 1/n fraction of resource i to agent i, while any “reasonable” allocation should allocate all of resource i to agent i. Therefore, the proportional share is a factor n smaller than what one would imagine would be a reasonable fair share. 1.1 Cake-cutting Share The above problem arises because a participant is computing the max-min shares without knowledge of other participants’ utility functions. In this paper, we seek to define such max-min shares that take into account the utility functions of other participants. We do so via a slight modification of the game described above. As before, participant i splits the resources into n bundles, and as before, the other agents go first in choosing their bundles. However, agent i now knows the utility functions of the other agents, and creates the bundles so that every other agent j likes agent i’s bundle at most as much as any of the remaining n-1 bundles. This ensures that regardless of how the other agents go first and choose bundles, agent i’s bundle is not chosen by them. As before, participant i seeks the partition that maximizes the utility they gets from its own bundle, and this utility represents its cake-cutting share. Note that in the above example where the agents have value for disjoint resources, agent i can create n bundles, where it puts the i^{th} resource in a special bundle by itself, and splits the remaining resources equally among the remaining n-1 bundles. Any other agent strictly prefers any of the latter bundles to the special bundle, so the cake-cutting share of agent i is equal to its utility for resource i. On this instance, the cake-cutting share obtains a factor n larger utility for each agent than the proportional share. It is relatively easy to show that the cake-cutting share of agent i is equivalent to the following natural process: Agent i finds a bundle A_{i} that achieves the largest utility for itself, subject to the condition that any other agent weakly prefers the proportional allocation to bundle A_{i} (see Lemma 7). Since one such bundle is the proportional allocation, the cake-cutting share is at least the proportional share for all agents. In that sense, the cake-cutting share is a relaxation of the proportional share. 1.2 Envy-free Polyhedra and Envy-free Share We can generalize the game-theoretic view of fair shares further and define a class of fair shares as follows. As before, participant i splits the resources into n bundles. However, in addition to splitting the items, agent i also assigns the bundles to the agents (including i itself). The assignment can be characterized by a partition \mathcal{A}=(A_{1},A_{2},\ldots,A_{n}), where A_{i} represents the bundle assigned to agent i. For each resource, the total amount of the resource is at most its supply. We require i to split the items in an envy-free way, so that \mathcal{A} lies in the “envy-free polyhedron” (denoted by \mathcal{E}_{j}) of any other agent j\neq i. Different definitions of shares follow from different settings of the envy-free polyhedra. In the cake-cutting share described before, any other agent j values any other bundle no less than agent i’s bundle. This corresponds to the envy-free polyhedron \mathcal{E}_{j}^{(\mathrm{CCS})}=\{\mathcal{A}\mid u_{j}(A_{i})\leq\min_{j^{% \prime}\in[n]}u_{j}(A_{j^{\prime}})\}, where u_{j}(A) is the linear utility of agent j for bundle A. Note that stricter limitations on the envy-free polyhedrons will generally lead to smaller share values. We also introduce a weaker instantiation of the envy-free polyhedron: \mathcal{E}_{j}^{(\mathrm{EFS})}=\{\mathcal{A}\mid u_{j}(A_{i})\leq u_{j}(A_{j% })\}. This polyhedron corresponds to the allocations where agent j has no envy specifically towards agent i. The maximum utility that i can generate for itself from any valid assignment \mathcal{A} defines its envy-free share (\mathrm{EFS}). Since \mathcal{E}_{j}^{(\mathrm{CCS})}\subseteq\mathcal{E}_{j}^{(\mathrm{EFS})} (i.e. the polyhedra for the envy-free share are a superset of those for the cake-cutting share), the envy-free share is at least the cake-cutting share. In addition, the envy-free share addresses an issue with the cake-cutting share. Consider the example of agents having values for disjoint resources from above. In this example, agent i’s cake-cutting share is its value for the entire resource, and hence a factor n larger than the proportional share. However, if we create a copy of agent i, then i’s bundle must give this copy at most the proportional utility (see Lemma 7), which means the cake-cutting share of i drops by a factor of n. The envy-free share fixes this aspect – agent i can create a bundle where it assigns half the resource to its copy, so that its envy-free share only drops by a factor of 2 when the copy is introduced. 1.3 Intermediate Notions of Fair Share Two intermediate notions of fair shares arise naturally from the above definitions. The first notion interpolates between \mathrm{CCS} and \mathrm{EFS}. Here, we constrain the partition \mathcal{A} to be an envy-free allocation across agents. In other words, all the agents will accept agent i’s partition \mathcal{A} if and only if they do not envy each other (including agent i). Formally, the envy-free polyhedron of agent j is \mathcal{E}_{j}^{(\mathrm{EF})}=\{\mathcal{A}\mid u_{j}(A_{j})\geq u_{j}(A_{j^% {\prime}}),\forall\,j^{\prime}\neq j\}. We show in Lemma 9 that each agent’s share for the above polyhedron is at least the cake-cutting share and at most the envy-free share, and that all these shares are at least the proportional share. A second and more interesting intermediate notion interpolates between proportionality and \mathrm{EFS}. In \mathrm{EFS}^{\Delta}, when computing its \mathrm{EFS} share, each agent i knows the utility functions of all other agents except a uniformly randomly chosen set S_{\Delta} of \lfloor\frac{n-1}{\Delta}\rfloor agents. For this set S_{\Delta} of agents, it assumes the utility function is adversarial, so agent i assigns the same bundle to these agents as it does to itself to ensure that even in the worst case over the choice of utility function, these agents do not envy agent i. An agent’s \mathrm{EFS}^{\Delta} share is the expected value of its \mathrm{EFS} share under this process, where the expectation is over the random choice of S_{\Delta}. When \Delta\geq n, this reduces to \mathrm{EFS}, and when \Delta=1, this reduces to proportionality. This notion therefore smoothly interpolates between proportionality at one extreme and \mathrm{EFS} on another. 1.4 Approximating Fair Shares Our goal in this paper is to study the cake-cutting and envy-free shares as novel fair share concepts. The key question we seek to answer is whether for either the cake-cutting or the envy-free shares, there is a fair division where the fair share value is simultaneously achievable for all agents, and if not, what factor of the agents’ fair share is simultaneously achievable via a fair division. We study this via the notion of approximation – in the worst case over all problem instances, what factor of this share is achievable simultaneously for all agents? In other words, we say that an \alpha approximation is achievable for \alpha\geq 1 if there is a partition of the resources into n bundles (one for each agent) such that each agent i’s utility from its bundle is at least a 1/\alpha factor of its fair share. We define \alpha(n,m) as the smallest achievable \alpha for all instances with m resources and n agents. Note that for any reasonable notion of fair share, an agent’s share value is at most its utility for the entire set of items. Thus we must have \alpha(n,m)\leq n since the proportional allocation guarantees each agent a 1/n fraction of its total utility for the set of items. Therefore, an approximation factor \alpha(n,m) is non-trivial only if \alpha(n,m)=o(n). Note that the proportional share itself satisfies \alpha(n,m)=1, and is hence non-trivial. However, as discussed above, this share may be too small in many instances. We ask: Does the cake-cutting share or the envy-free share also admit a non-trivial approximation factor? As an aside, we note that for any problem instance, the optimal approximation to the cake-cutting (resp. envy-free) share for this instance is computable via linear programming. (See Section 2.) Our goal is different – we wish to find a uniform bound on \alpha(n,m) as a function of m and n. 1.5 Our Results Our main result is to show that the above defined shares are interesting and non-trivial. We show this by demonstrating non-trivial bounds on \alpha(n,m), meaning that all agents can indeed simultaneously achieve a non-trivial (that is \omega(1/n)) fraction of their cake-cutting (resp. envy-free) share. We first show the following theorem, where we use \alpha(n,\cdot) to denote the worst case approximation over all instances with n agents, and an arbitrary number of items: Theorem 1 (Proved in Section 3). For the envy-free share (and hence the cake-cutting share), \alpha(n,\cdot)=O(\sqrt{n}). In other words, for any instance with n agents and m items, it is possible to find an allocation of the resources to the agents where each agent obtains utility \Omega\left(\frac{1}{\sqrt{n}}\right) times its cake-cutting (resp. envy-free) share. We next show there are instances where this bound is tight, so that it is not possible for all agents to achieve a better fraction of their cake-cutting share. Theorem 2 (Proved in Section 4). For the cake-cutting share (and hence the envy-free share), \alpha(n,\cdot)=\Omega(\sqrt{n}). We note that the above results imply the same bounds for any intermediate notion of fair shares where the constraints in the envy polyhedron are stricter than those for the envy-free share and weaker than those for the cake-cutting share. We next show a similar non-trivial (that is o(m)) approximation for the cake cutting share as a function of m, the number of resources, again using the notation \alpha(\cdot,m) to denote the worst case approximation for instances with m items and arbitrary number of agents. Theorem 3 (Proved in Section 5). For the cake-cutting share, \alpha(\cdot,m)=O(m^{2/3}). Further, there exists an instance of the fair division problem where \alpha(\cdot,m)=\Omega(\sqrt{m}). We finally extend the proof of Theorem 1 to the \mathrm{EFS}^{\Delta} share described above, where agent i does not know the utility function of a randomly chosen set S_{\Delta} of agents of size \lfloor\frac{n-1}{\Delta}\rfloor, for whom it replicates its own allocation. We show the following theorem where the approximation ratio increases smoothly in the parameter \Delta. Theorem 4 (Proved in Section 6). For the \mathrm{EFS}^{\Delta} share, \alpha(n,\cdot)=\Theta(\sqrt{\Delta}). The upper bound of O(\sqrt{\Delta}) holds for any set S_{\Delta} that i chooses, while in the lower bound instance, the \Omega(\sqrt{\Delta}) bound holds for all possible sets S_{\Delta}. Note that in agent i’s partition, agent i values its own bundle at most as much as any bundle assigned to the agents in S_{\Delta}. Thus, any agent’s \mathrm{EFS}^{\Delta}_{i} share is at most a 1/(1+\lvert S_{\Delta}\rvert) factor of its utility for the entire set of items. This implies that any agent’s \mathrm{EFS}^{\Delta}_{i} share is at most a \Delta factor larger than its proportional share, which gives an easy upper bound of \Delta for \alpha(n,\cdot). The above theorem improves this to O(\sqrt{\Delta}). Techniques. The main technical highlight is the proof of the upper bounds on approximation. To show that a O(\sqrt{n}) approximation is always achievable, we use the idea of dual fitting, where we write the worst case approximation over all problem instances as a linear program with exponentially many variables. We then show a feasible solution to the dual of this program with value O(\sqrt{n}). This linear program is non-trivial, and the techniques used for writing it may find other applications to fair division and beyond. Indeed, we find the lower bound instances by solving this linear program for small values of n. The proof of the O(m^{2/3}) upper bound on approximation is via a greedy set cover type algorithm, showcasing an interesting connection between the cake-cutting share and set cover. Empirical Results. Given that various versions of the envy polyhedron yield comparable worst-case approximation bounds, a natural question is what notion would be reasonable in practice. Towards this end, we compute the approximations on simulated and real problem instances in Section 7 and show that the cake-cutting share – the strictest envy polyhedron – comes closest to having an approximation factor of one, meaning these shares are realizable via an allocation. We further show that the approximation factor of \mathrm{EFS}^{\Delta} smoothly decreases with \Delta.These results complement our theoretical bounds, and serve as another justification for developing these novel definitions of shares. 1.6 Related Work Proportionality and Fair Allocation. The concept of proportionality in fair division was first introduced by Steinhaus (1948). This foundational work addressed the problem of dividing a heterogeneous resource (the “cake”) among several participants in such a way that each participant perceives their share as fair, where one notion of fairness presented was proportionality. A proportional allocation for divisible items is easy to achieve by evenly splitting every item; however its generalization to groups of agents has been studied for shared resources Fain et al. (2016). Fair Shares for Indivisible Goods. We refer readers to Mishra et al. (2023); Amanatidis et al. (2022) for detailed surveys of recent results on proportionality and other fair allocations of indivisible items. A natural approach for indivisible items is to approximate proportionality, and several works Conitzer et al. (2017); Aziz et al. (2020); Baklanov et al. (2021a, b) have considered such approximate notions. The work of Babaioff and Feige (2022) defines the general notion of a share that is computed for each agent. The most popular notion of a share is the maximin share (MMS) introduced by Budish (2011), which calculates each agent’s share using the cutting and choosing game described previously. Although allocations that achieve exact MMS may not exist for indivisible goods Procaccia and Wang (2014), constant approximations to this share can be achieved Akrami and Garg (2024); Babaioff et al. (2022); Hosseini et al. (2022); Akrami et al. (2023). Recently, Babichenko et al. (2024) propose the quantile share, where an agent wants its share to be better than a uniformly random allocation with constant probability. All the above notions compute agent i’s share solely as a function of the number of agents n, and agent i’s personal utility function v_{i}. As such, for divisible goods, they all reduce to proportionality. As far as we are aware, our work is the first to define and study fair shares that are cognizant of the utility functions of other agents, therefore moving beyond proportionality even for the divisible goods setting. Envy-freeness and Cake Cutting Envy-freeness – where no agent envies the allocation of another agent – is more demanding than proportionality and was first formally introduced by Gamow and Stern (1958). Envy-free allocations have been widely studied for both divisible and indivisible goods; see Varian (1974, 1976); Caragiannis et al. (2019); Lipton et al. (2004); Aziz and Mackenzie (2016) and the citations therein. Our proposed shares are related in spirit to envy-free allocations, but differ in the details, since envy in our context is with regard to the agent computing its share. We note that to the best of our knowledge, our work is the first to define fair shares (or guarantees of utility) based on envy-freeness."
https://arxiv.org/html/2411.10399v1,Game Theoretic Liquidity Provisioning in Concentrated Liquidity Market Makers,"Automated marker makers (AMMs) are a class of decentralized exchanges that enable the automated trading of digital assets. They accept deposits of digital tokens from liquidity providers (LPs); tokens can be used by traders to execute trades, which generate fees for the investing LPs. The distinguishing feature of AMMs is that trade prices are determined algorithmically, unlike classical limit order books. Concentrated liquidity market makers (CLMMs) are a major class of AMMs that offer liquidity providers flexibility to decide not only how much liquidity to provide, but in what ranges of prices they want the liquidity to be used. This flexibility can complicate strategic planning, since fee rewards are shared among LPs. We formulate and analyze a game theoretic model to study the incentives of LPs in CLMMs. Our main results show that while our original formulation admits multiple Nash equilibria and has complexity quadratic in the number of price ticks in the contract, it can be reduced to a game with a unique Nash equilibrium whose complexity is only linear. We further show that the Nash equilibrium of this simplified game follows a waterfilling strategy, in which low-budget LPs use up their full budget, but rich LPs do not. Finally, by fitting our game model to real-world CLMMs, we observe that in liquidity pools with risky assets, LPs adopt investment strategies far from the Nash equilibrium. Under price uncertainty, they generally invest in fewer and wider price ranges than our analysis suggests, with lower-frequency liquidity updates. We show that across several pools, by updating their strategy to more closely match the Nash equilibrium of our game, LPs can improve their median daily returns by $116, which corresponds to an increase of 0.009% in median daily return on investment.","Automated market makers (AMMs) are decentralized exchanges (DEXes) that allow users to exchange cryptocurrency via a smart contract that algorithmically manages liquidity and exchange rates (sok2023, ). As a dominant class of DEXes (dex-rank, ), AMMs play an outstanding role more generally as decentralized applications (dapp-rank, ). Today, AMMs drive billions of dollars in daily trading volume on several blockchains (ripple-amm, ; coingecko, ). The core functionality of AMMs is facilitated by liquidity pools, i.e., blockchain smart contracts that store and manage cryptocurrency tokens for trading. Most liquidity pools store two types of tokens, which we denote X and Y; we focus on the two-token class of AMMs in this work. A typical trade proceeds in three steps: (a) A trader proposes to pay \Delta x amount of the X token and asks for a quote. (b) The pool tells the trader they will obtain \Delta y amount of the Y token (assuming there is sufficient liquidity in the pool). (c) The trader decides whether to execute the token swap, in which case they also must pay a trading fee (in units of X) that is proportional to the amount of added tokens. The trading fee is used by the AMM to incentivize liquidity providers (LPs), who initially deposit liquidity tokens into the pool to support trades. An AMM can broadly be characterized by three intertwined policies, which are implemented algorithmically: (1) the exchange rate of \Delta x for \Delta y (and vice versa) based on the state of the pool, (2) the liquidity investment policy (i.e., what constitutes a valid deposit/withdrawal), and (3) the trading fee reward mechanism, i.e., how trading fees are allocated to LPs. In this paper, we consider two canonical types of LPs, which we call Legacy automated market makers (Legacy AMMs) and concentrated liquidity market makers (CLMMs). Legacy AMMs. Legacy AMMs determine the exchange rate via a constant product market maker (CPMM) (sok2023, ). CPMMs ensure that the product of pool reserves always remains constant. LPs that invest in legacy AMMs must deposit both X and Y tokens, and fee rewards are split among investing LPs proportionally to their initial investment (details in §2.2). Although legacy AMMs are extremely widely used (uv2, ; sushi, ; balancerv2, ; curve, ; cake, ; orca, ; raydium, ), they are known to suffer from the price slippage problem: the price of a large trade is significantly worse than that of a small trade; this is due to the hyperbolic shape of the price curve (adams2024dontletmevslip, ). Concentrated Liquidity Market Makers (CLMMs). To mitigate slippage, a number of AMMs (uv3, ; sushi, ; balancerv3, ; orca, ) adopted a scheme called concentrated liquidity, first introduced in Uniswap v3 in 2021 (uv3, ). In a CLMM, an LP decides not only the amount of liquidity they want to add, but also the price range in which the liquidity is active. As a result, the LP only earns fees from trades when the external token price lies in the LP’s range of investment. Investing in a narrower price range grants the LP a higher share of the fees from transactions within that range. In CLMMs, LPs suffer less price slippage in price ranges with high liquidity. However, CLMMs force LPs to be more strategic in their investment choices: they must choose which price range(s) to invest in, as well as the amount of investment. The consequences of these choices are complex — even if an LP could accurately predict future price ranges, they would still need to compete against other LPs over their share of the trading fee. To date, it remains unclear how strategic LPs should invest funds in CLMMs. Prior literature has studied incentives in AMMs, but no prior work has simultaneously modeled and analyzed the following three properties of CLMMs: (1) LPs can only invest up to a fixed budget, (2) LPs in CLMMs can invest different amounts in different price ranges, and (3) LPs compete against each other for fees, and thus must take into account other LPs’ strategies and their budgets (Frtisch23, ). Most existing works focus on the study of a single LP’s strategy (Fan2022, ) or on the case where LPs are identical (concave-pro-rata-game, ; Fan2022, ; Heimbach2023, ; Bayraktar24, ; he2024liquiditypooldesignautomated, ; fan2024strategicliquidityprovisionuniswap, ), and in both cases the budget is assumed to be unlimited (concave-pro-rata-game, ). For legacy AMMs, (concave-pro-rata-game, ) proposes a framework using symmetric games to show the uniqueness of the symmetric Nash equilibrium. However, the game in this work does not incorporate budget constraints, which is an important practical consideration. It also does not capture LP strategies involving complex combinations of liquidity positions in CLMMs when LPs have different capacity investments. Our goal in this work is to study the incentives of LPs in CLMMs under a game-theoretic model. In particular, we want to understand the following questions: Do there exist equilibrium investment strategies for CLMMs, and if so, what are their characteristics in relation to LPs’s investment capacity? How do the strategies of real-world LPs compare to those at Nash equilibrium? To answer these questions, we make the following contributions: (a) Game theoretic model: We model strategic liquidity provisioning as a game played by rational and selfish LPs. Each LP is constrained by their own budget, rewarded by trading fees, and penalized by impermanent loss (the opportunity cost of not choosing to hold the tokens in hand). Analysis of this game is complex due to its large strategy space, as LPs can invest in a set of price ranges that is quadratic in the number of feasible price range endpoints. However, we prove that this complex game is equivalent to a much simpler game in which each LP’s investment can be broken into a much smaller (linear) set of atomic price ranges (Thm. 3.5). (b) Nash equilibrium analysis: We prove the unique existence of a Nash equilibrium in our simplified game (Thm 3.3). We show that the Nash equilibrium exhibits a waterfilling pattern (Thm. 3.6), which reveals a division of LPs by their budget — poor LPs exhaust their budgets, while rich LPs spend equally. We characterize properties of the Nash equilibrium as a function of LP budgets. (c) Real-World Data Analysis: On the Ethereum blockchain, we compare our theoretical results to the actions of real LPs on two types of liquidity pools – stable pools and risky pools. We compare LPs’ real-world liquidity provisions against their (simulated) best-response actions and Nash equilibrium actions under our game formulation. In stable pools, we show that real LPs deploy strategies similar to the Nash equilibrium of our game, and return on investment at equilibrium is much lower than that in risky pools. In risky pools, we show that most real LPs prefer to invest in few (<5) price ranges, each with wide price coverage. While this behavior is far from our predicted Nash equilibrium strategies, we can partially predict LP behavior by solving a modified version of our game with stale information, suggesting that many LPs have not yet taken full advantage of the data available on the blockchain. We show that LPs’ median daily return on investment (ROI) can be improved by 0.009% by updating their strategy to more closely match the Nash equilibrium of our constructed game. In dollars, this corresponds to an increase in median daily utility of $116 and average daily utility of $222."
https://arxiv.org/html/2411.09867v1,To Optimize Human-in-the-loop Learning in Repeated Routing Games,"Today navigation applications (e.g., Waze and Google Maps) enable human users to learn and share the latest traffic observations, yet such information sharing simply aids selfish users to predict and choose the shortest paths to jam each other. Prior routing game studies focus on myopic users in oversimplified one-shot scenarios to regulate selfish routing via information hiding or pricing mechanisms. For practical human-in-the-loop learning (HILL) in repeated routing games, we face non-myopic users of differential past observations and need new mechanisms (preferably non-monetary) to persuade users to adhere to the optimal path recommendations. We model the repeated routing game in a typical parallel transportation network, which generally contains one deterministic path and N stochastic paths. We first prove that no matter under the information sharing mechanism in use or the latest routing literature’s hiding mechanism, the resultant price of anarchy (PoA) for measuring the efficiency loss from social optimum can approach infinity, telling arbitrarily poor exploration-exploitation tradeoff over time. Then we propose a novel user-differential probabilistic recommendation (UPR) mechanism to differentiate and randomize path recommendations for users with differential learning histories. We prove that our UPR mechanism ensures interim individual rationality for all users and significantly reduces \text{PoA}=\infty to close-to-optimal \text{PoA}=1+\frac{1}{4N+3}, which cannot be further reduced by any other non-monetary mechanism. In addition to theoretical analysis, we conduct extensive experiments using real-world datasets to generalize our routing graphs and validate the close-to-optimal performance of UPR mechanism.","With ever-increasing smart devices owned by human users in transportation networks, mobile crowdsourcing applications (e.g., Waze and Google Maps) have emerged as vital platforms for users to learn and share their observed time-varying traffic information for guiding their daily or repeated routing (​​[1, 2, 3]). These applications simply publicize all useful traffic information, which encourages selfish users to consistently choose the current shortest routes in each round to minimize their own travel costs (​​[4, 5]). As a consequence, users miss critical exploration of non-shortest paths that may turn better to route and reduce travel costs in the future. In systems characterized by dynamic and diverse information dynamics, Multi-Armed Bandit (MAB) problems emerge as a framework for investigating optimal exploration-exploitation trade-offs among stochastic arms or paths (e.g., [6, 7, 8, 9, 10, 11]). For instance, [9] utilizes MAB techniques to forecast congestion levels in rapidly changing transportation environments. Similarly, [10] employs MAB exploration-exploitation strategies to manage the recruitment of previously reliable users alongside new, unproven users in completing crowdsourcing tasks. More recently, research efforts have extended traditional MAB paradigms to distributed settings involving multiple non-myopic agents aiming to minimize overall social costs (e.g., [12, 13, 14, 15]). For instance, [12] explores the use of distributed agents to learn heterogeneous local models associated with each arm, contributing to the social planner’s decision-making process. Additionally, [13] investigates how a fixed flow of agents collaborates in a distributed MAB scenario over repeated interactions, where each agent autonomously makes optimal decisions based on privately acquired arm information. Subsequently, [14] and [15] allow agents to learn and share the arm information with their neighbors for making locally optimal decisions. Overall, these new distributed MAB solutions require all agents to be fully cooperative to adhere to the optimal policy set by the social planner, without any possible selfish deviation to favor individuals’ rewards. To facilitate human-in-the-loop learning (HILL) in repeated routing games, it is crucial to incentivize selfish users to follow the social planner’s optimal recommendations, enabling both information learning on diverse paths and the long-term reduction of traffic congestion. Prior routing game studies mainly focus on an oversimplified one-shot routing scenario without any information variation over time. They develop information hiding (e.g., [16, 17, 18]) and side-payment or pricing mechanisms (e.g., [19, 20, 21]) to regulate myopic users’ selfish routing choices. However, these mechanisms strongly assume that the social planner possesses all the traffic information to design incentives. Moreover, they only consider persuading myopic users to follow, whereas myopic users do not have past traffic observations as side information to reverse-engineer and deviate. In contrast, our HILL problem deals with a dynamic system with time-varying traffic information, necessitating the social planner to not only reduce immediate congestion caused by users but also incentivize their ongoing information learning on diverse paths. As compared to pricing or side-payment, informational mechanisms (e.g., information hiding) are non-monetary and easy to implement (​​[22, 23, 24]). In this paper, we aim to design the best informational mechanism to regulate HILL in repeated routing games to persuade non-myopic users with differential side information in routing choices. As such, we need to address the following two technical challenges. • The first question is how to evaluate existing routing policies’ performances for non-myopic users as compared to the social optimum in the dynamic system with time-varying traffic information. Prior routing game literature (e.g. [17, 18]) focuses on myopic users in oversimplified one-shot scenarios, assuming that the social planner knows all traffic conditions to decide routing policies. In practice, the social planner lacks access to live traffic conditions but relies on non-myopic users to explore different paths over time. Although non-myopic, users remain selfish, and their interests are not aligned with the social planner’s goal of balancing long-term exploitation and exploration to minimize the overall social cost. Moreover, the theoretical analysis of the existing routing policies is lacking, particularly in understanding their performance gaps from the social optimum. • The second challenge is how to persuade non-myopic users to follow the optimal path recommendations despite their selfishness and differential learning histories to reverse-engineer. In repeated routing games, each user only cares about its own travel cost. It may learn from its own traffic observations in the past to infer the current system’s status and deviate from the optimal recommendation. Existing studies of incentivized exploration (​​[25, 26, 27, 28]) and Bayesian persuasion (​​[29, 30, 32, 33]) largely assume that all users are myopic to participate and do not have private side information to play strategically against the system’s recommendations. It is worth noting that there is a recent study related to this paper (​​[34]), which introduces an informational mechanism aimed at regulating myopic users’ distributed information learning in congestion games. However, for practical human-in-the-loop learning in repeated routing games, we face non-myopic users, who focus on their own long-term rewards rather than one-shot immediate rewards. In this case, the mechanism in [34] cannot work, as non-myopic users can reverse-engineer the system status to play against the mechanism. Unlike this work, our paper addresses the two challenges above by proposing new mechanism designs and analysis. We summarize our key novelty and main contributions as follows. • To regulate human-in-the-loop learning (HILL) for repeated routing games: To the best of our knowledge, this work is the first to regulate non-myopic users with differential side-information in repeated routing games. We model the repeated routing game in a typical parallel transportation network, which generally contains one deterministic path and N stochastic paths. Here each stochastic path has time-varying unknown traffic conditions to be learned by repeatedly arriving non-atomic users. Furthermore, users’ past routing choices create differential side information for selfish routing, making it challenging to design any informational mechanism. • Theoretical comparison among information sharing, hiding mechanisms, and social optimum via PoA analysis: We analyze the information sharing and hiding mechanisms as well as social optimum to derive their corresponding routing recommendations in closed form, respectively. Then we prove that no matter under today’s information sharing mechanism (in Waze and Google Maps) or the latest information hiding mechanism in the routing game literature, the resultant price of anarchy (PoA) for measuring the efficiency loss from optimum about total users’ travel costs can be arbitrarily large. • Best informational mechanism to remedy huge efficiency loss: We propose a novel user-differential probabilistic recommendation (UPR) mechanism to randomly differentiate path recommendations for users with differential learning histories. We prove that our UPR mechanism ensures interim individual rationality for all users and significantly reduces \text{PoA}=\infty to close-to-optimal \text{PoA}=1+\frac{1}{4N+3}, which cannot be further reduced by any other informational mechanism. In addition to theoretical analysis, we conduct extensive experiments using real-world datasets to generalize our routing graphs and validate the close-to-optimal average performance of our UPR mechanism. The rest of the paper is organized as follows. In Section II, we propose the repeated routing game model, the public human-in-the-loop learning (HILL) model under information sharing mechanism, and the differential HILL model under information hiding mechanism. Then in Section III, we first formulate the optimization problems for the sharing and hiding mechanisms, as well as the socially optimal policy, and then analyze the solutions for these three policies. In Section IV, we analytically compare the two mechanisms with the social optimum via PoA analysis. Based on the analysis, we propose our UPR mechanism in Section V. We verify the average performance of our UPR mechanism in Section VI. Finally, Section VII concludes the paper. To facilitate readability, we have summarized all the key notations in Table I."
https://arxiv.org/html/2411.10294v1,Static network structure cannot stabilize cooperation among Large Language Model agents,"Large language models (LLMs) are increasingly used to model human social behavior, with recent research exploring their ability to simulate social dynamics. Here, we test whether LLMs mirror human behavior in social dilemmas, where individual and collective interests conflict. Humans generally cooperate more than expected in laboratory settings, showing less cooperation in well-mixed populations but more in fixed networks. In contrast, LLMs tend to exhibit greater cooperation in well-mixed settings. This raises a key question: Are LLMs about to emulate human behavior in cooperative dilemmas on networks? In this study, we examine networked interactions where agents repeatedly engage in the Prisoner’s Dilemma within both well-mixed and structured network configurations, aiming to identify parallels in cooperative behavior between LLMs and humans. Our findings indicate critical distinctions: while humans tend to cooperate more within structured networks, LLMs display increased cooperation mainly in well-mixed environments, with limited adjustment to networked contexts. Notably, LLM cooperation also varies across model types, illustrating the complexities of replicating human-like social adaptability in artificial agents. These results highlight a crucial gap: LLMs struggle to emulate the nuanced, adaptive social strategies humans deploy in fixed networks. Unlike human participants, LLMs do not alter their cooperative behavior in response to network structures or evolving social contexts, missing the reciprocity norms that humans adaptively employ. This limitation points to a fundamental need in future LLM design—to integrate a deeper comprehension of social norms, enabling more authentic modeling of human-like cooperation and adaptability in networked environments.","Over the last few years, the advances in artificial intelligence have ignited hopes of new methods for the behavioral and social sciences[1, 2]. In particular, chatbots powered by so-called large language models (LLM)[3] are believed to be able to emulate humans in conversations well enough to eventually replace them in experiments[4]. Even though experiments with human participants are ultimately needed to advance the behavioral and social sciences, there are expectations AI agents could aid in experimental design and provide experimental agents with programmable behavior (so-called “digital twins”) [1, 2]. LLMs are trained on extensive datasets of human-generated text and semantic knowledge from various societies [1, 4]. These models operate as conditional probability distributions, where altering the context or narrative can steer them toward more desirable outcomes by influencing the likelihood of specific responses while reducing others [4]. Consequently, LLMs are highly skilled at following instructions and embodying assigned personalities [5]. When given personalities, LLMs can display traits that resemble human nature, almost as if they possess their mind [6, 7]. This training process can also improve LLMs’ understanding and reasoning regarding cooperation, defection, and balancing individual and collective interests [8]. Recent behavioral experiments have shown that LLM agents can effectively substitute human participants in certain contexts, particularly within Western, Educated, Industrialized, Rich, and Democratic (WEIRD) societies [4]. However, this substitution does not extend to other cultural contexts [9]. Beyond cultural differences, within the same cultures, individuals’ social and strategic preferences depend on the network they are part of [10, 11, 12, 13]. Individuals can infer underlying network structures and adapt their social behavior, even without a complete overview of the network. Through interactions with neighbors and social learning, individuals discern the network they are part of, and this structure significantly influences their social behavior. A fascinating question arises: can large language models discern these network structures and adjust their behavior similarly to humans? This inquiry becomes especially intriguing when we consider their potential to help solve societal challenges, like promoting cooperation in social dilemmas. For this vision to come to life, it is crucial that AI agents consistently demonstrate behavior akin to that of humans. We have some understanding of LLMs’ capabilities in repeated prisoners’ dilemma games in well-mixed populations [5, 14, 8, 15, 16, 17], but we lack insight into how they perform in network settings. There, individuals consider not only strategies but also their interactions with specific network members [18, 19]. Previous research has shown that individuals can establish cooperation when they have the opportunity to adjust their social ties based on their experiences with neighbors [19]. Interestingly, it has been found that in a prisoner’s dilemma game, individuals can achieve stable cooperation within specific static network structures when the benefit-to-cost ratio exceeds the number of connections. However, the same parameter settings do not foster cooperation in a well-mixed population. The findings suggest that when individuals are aware of their neighborhood and the consequences of their cooperative actions, they can adapt their strategies based on the success of their neighbors, leading to stable cooperation even in networks with a few defectors [13]. In this study, we investigate whether LLMs can reliably adjust their social behavior in response to the network structure they operate within, as humans do. We study the capabilities of LLM agents, as of 2024, in the context of networked social dilemmas [18, 20, 5]. These stylized situations are designed to explore how humans prioritize between short-sighted egoistic and long-term prosocial choices. Such dilemmas have a wide range of social science applications, from behavioral economics, which explores questions like how to best incentivize people to follow policies, to anthropology and evolution, which search for the unique behavioral mechanisms behind human cooperation among known lifeforms."
https://arxiv.org/html/2411.10284v1,Optimal Capacity Modiﬁcation for Strongly Stable Matchings,"We consider the Hospital/Residents (HR) problem in the presence of ties in preference lists. Among the three notions of stability, viz. weak, strong, and super stability, we focus on the notion of strong stability. Strong stability has many desirable properties both theoretically and practically; however, its existence is not guaranteed.In this paper, our objective is to optimally increase the quotas of hospitals to ensure that a strongly stable matching exists in the modified instance. First, we show that if ties are allowed in residents’ preference lists, it may not be possible to augment the hospital quotas to obtain an instance that admits a strongly stable matching. When residents’ preference lists are strict, we explore two natural optimization criteria: (i) minimizing the maximum capacity increase for any hospital (MinMax), and (ii) minimizing the total capacity increase across all hospitals (MinSum). We show that the MinMax problem is NP-hard in general. When hospital preference lists can have ties of length at most \ell+1, we give a polynomial-time algorithm that increases each hospital’s quota by at most \ell, ensuring the resulting instance admits a strongly stable matching.We show that the MinSum problem admits a polynomial-time algorithm. However, when each hospital incurs a cost for each capacity increase, the problem becomes NP-hard, even if the costs are 0 or 1. This also implies that the problem cannot be approximated to any multiplicative factor. We also consider a related problem under the MinSum objective. Given an HR instance and a forced pair (r^{*},h^{*}), the goal is to decide if it is possible to increase hospital quotas (if necessary) to obtain a strongly stable matching that matches the pair (r^{*},h^{*}). We show a polynomial-time algorithm for this problem.","The Hospital/Residents (HR) problem [19, 31] is a many-to-one generalization of the classical stable marriage problem [16]. As the name suggests, the HR problem models the assignment of junior doctors (residents) to hospitals where agents in both sets are allowed to rank acceptable agents from the other set in a preference order. The problem is extensively investigated since it has applications in a number of centralized matching schemes in many countries, including the National Resident Matching Program in the USA (NRMP), the Canadian Resident Matching Service (CaRMS), and the Scottish Foundation Allocation Scheme (SFAS), to name a few. In addition, the HR problem models several real-world applications like assigning children to schools [1] and students to undergraduate programs [5] where agents need to be matched to programs, and both sets express preferences over each other. We consider a generalization of the HR problem where ties are allowed in the preference lists. That is, agents can be indifferent between multiple agents from the other set. This problem is known as the Hospital/Residents problem with ties (HRT). Ties in preference lists play an important role in real-world matching applications. For instance, hospitals with a large number of applicants often find it difficult to generate strict preference lists. Many of these hospitals, within the framework of a centralized matching scheme, have expressed the desire to include ties in their preference lists [21]. In case of college admissions, it is natural for colleges to have all the students with equal scores in a single tie in their preference lists. The classical notion of stability defined for strict preferences has been generalized in the literature for the case of ties, in three different ways – weak stability, strong stability and super stability (see Definition 1.1 and the footnote therein). As indicated by the names, super stability is the strongest notion and weak stability is the weakest among the three. It is well-known that every instance of the HRT problem admits a weakly stable matching (and it can be obtained by breaking ties arbitrarily and computing a stable matching in the resulting instance); however, strong or super stable matchings are not guaranteed to exist [20]. The strongest notion of stability is super-stability. However, as highlighted in [22], insisting on super-stability in practical scenarios can be overly restrictive and is less likely to be attainable. Moreover, in applications like college admissions, it is natural to require students to express strict preferences over colleges, although colleges need to put students with equal scores in a tie111We refer to this as HR-HT in this paper. In such scenarios, super and strong stability coincide. On the other hand, weak stability is too weak, and as justified in [29], it is susceptible to compromise through persuasion or bribery (also see [22, 27] for further details). Moreover, from a social perspective, weak stability may not be an acceptable notion despite its guaranteed existence. For instance, according to the equal treatment policy used in Chile and Hungary, it is not acceptable that a student is rejected from a college preferred by her, even though other students with the same score are admitted (see [12] and the references therein). Thus, strong stability is not only appealing but also essential. Given that strong stability is desirable but not guaranteed to exist, a natural option for applications is to adjust the quotas (of, say, colleges and hospitals) so that a strongly stable matching exists after the adjustment. We address this problem in this paper. We use the hospital-residents terminology, as is customary in many-to-one stable matchings. Thus we seek to increase or augment the hospital quotas to obtain a modified instance which admits a strongly stable matching. We explore two natural optimization criteria: (i) minimize the total increase (sum) in quotas across all hospitals (MinSum), and (ii) minimize the maximum increase in quota for any hospital (MinMax). Our work falls in the broad theme of capacity planning / modification which has received significant attention [11, 18, 8, 6, 2, 7] motivated by practical applications where quotas are not rigid. To the best of our knowledge, our work is the first one to explore capacity augmentation for the notion of strong stability. 1.1 Preliminaries and notation The input to our problem is a bipartite graph G=(\mathcal{R}\cup\mathcal{H},E), where the vertex set \mathcal{R} represents the set of residents, \mathcal{H} represents the set of hospitals and the edge set E represents mutually acceptable resident-hospital pairs. We define n=|\mathcal{R}|+|\mathcal{H}| and m=|E|. Every hospital h\in\mathcal{H} has an associated quota q(h) denoting the maximum number of residents that can be assigned to h in any assignment. Each vertex v\in\mathcal{R}\cup\mathcal{H} ranks its neighbors as per its preference ordering, referred to as the preference list of v, denoted as Pref(v). We say that a vertex strictly prefers a neighbor with a smaller rank over another neighbor with a larger rank. If a vertex is allowed to be indifferent between some of its neighbors and is allowed to assign the same rank to such neighbors, it is referred to as a tie. The length of a tie is the number of neighbors having equal rank. If ties are not allowed (or equivalently, all ties have length 1), the preference lists are said to be strict. We use u_{1}\succ_{v}u_{2} to denote that v strictly prefers u_{1} over u_{2} and u_{1}\succeq_{v}u_{2} to denote that v either strictly prefers u_{1} over u_{2} or is indifferent between them. A matching M in G is a subset of E such that |M(r)|\leq 1 and |M(h)|\leq q(h) for each resident r\in\mathcal{R} and hospital h\in\mathcal{H} where M(v) denotes the set of matched partners of v in M. For a resident r, if |M(r)|=0, then r is unmatched in M. In this case, we denote the matched partner of r by M(r)=\bot. A hospital h\in\mathcal{H} is said to be fully subscribed in M with respect to its quota q(h), if |M(h)|=q(h), under-subscribed in M if |M(h)|<q(h). We abuse the term matching and say that h is over-subscribed in M if |M(h)|>q(h). If left unspecified, the quota under consideration for these terms is the original quota q(h). If h is under-subscribed, then we implicitly match the remaining q(h)-|M(h)| many positions of h to as many copies of \bot. A vertex prefers any of its neighbors over \bot. Definition 1.1 (Strong stability:) For a matching M, an edge (r,h)\in E\setminus M is a strong blocking pair w.r.t. M, if either (i) or (ii) holds: (i) h\succ_{r}M(r) and there exists r^{\prime}\in M(h) such that r\succeq_{h}r^{\prime} (ii) h\succeq_{r}M(r) and there exists r^{\prime}\in M(h) such that r\succ_{h}r^{\prime}. A matching M is strongly stable matching if there does not exist any strong blocking pair w.r.t. M. 222A pair (r,h) is a super blocking pair if both prefer each other strictly or equally to their matched partners. Also, (r,h) form a weak blocking pair if they prefer each other strictly more than their matched partners. Throughout the paper, we refer to a strong blocking pair as a blocking pair. We give a simple example to illustrate that a strongly stable matching is not guaranteed to exist. Consider an instance with one resident r and two hospitals h_{1},h_{2}, where r has h_{1} and h_{2} tied at rank-1, whereas h_{1},h_{2} have unit quota each and both of them rank r as a rank-1 vertex. No matching in this instance is strongly stable, since the matching M_{1}=\{(r,h_{1})\} is blocked by (r,h_{2}) and M_{2}=\{(r,h_{2})\} is blocked by (r,h_{1}). r : (h_{1},h_{2}) [1]\ h_{1} : r [1]\ h_{2} : r Moreover, the same example illustrates that increasing hospital quotas (alone) may not help in obtaining an instance which admits a strongly stable matching. This happens because there are ties in residents’ preference lists whereas quota augmentation is possible for hospitals only. What if resident preference lists are strict and ties appear only in hospitals’ preferences? We call such instances as HR-HT (Hospital/Residents problem with ties on hospitals’ side only). There exist simple instances of HR-HT which do not admit a strongly stable matching, however, for any such instance, we can construct an augmented instance G^{\prime} by setting the quota of each hospital h equal to its degree in G. It is easy to observe that the matching M^{\prime} that assigns each resident to its rank-1 hospital, is a strongly stable matching in G^{\prime}. Thus, unlike the general HRT case, an HR-HT instance can always be augmented so that the instance admits a strongly stable matching. Our objective in this paper is to optimally increase hospitals’ quotas to ensure that a strongly stable matching exists in the modified instance. We are ready to formally define our problems. 1.2 Our problems and contributions For all our problems, unless stated explicitly, we assume that the given HR-HT instance G=(\mathcal{R}\cup\mathcal{H},E) does not admit a strongly stable matching. Deciding whether an HRT instance admits a strongly stable matching can be done in polynomial time using the algorithm by Irving et al. [22] (see Appendix 0.A). Throughout this paper the augmented instance G^{\prime} is the same as G except that q^{\prime}(h)\geq q(h) for each h. Our first objective is to minimize the total increase in quotas across all hospitals. Our first problem under this objective is MinSum-SS. MinSum-SS: Given an HR-HT instance G=(\mathcal{R}\cup\mathcal{H},E), construct an augmented instance G^{\prime} such that G^{\prime} admits a strongly stable matching and the sum of the increase in quotas over all hospitals (that is, \sum_{h\in\mathcal{H}}(q^{\prime}(h)-q(h))) is minimized. Theorem 1.2 MinSum-SS problem is solvable in polynomial time. Given the polynomial-time solution for the MinSum-SS problem, we consider the optimal total quota augmentation (if possible) for matching a pair (r^{*},h^{*}) in G. We denote this problem as MinSum-SS-FP (forced pair) and define it formally below. MinSum-SS-FP: Given an HR-HT instance G=(\mathcal{R}\cup\mathcal{H},E), which possibly admits a strongly stable matching, and an edge (r^{*},h^{*})\in E, construct an augmented instance G^{\prime}, if possible, such that G^{\prime} admits a strongly stable matching that matches (r^{*},h^{*}) and the sum of the increase in quotas over all hospitals (that is, \sum_{h\in\mathcal{H}}(q^{\prime}(h)-q(h))) is minimized. Theorem 1.3 The MinSum-SS-FP problem is solvable in polynomial time. Next, we consider a generalization of the MinSum-SS problem, where increasing the quota of a hospital incurs a cost. This problem is denoted by MinSum-Cost. MinSum-Cost: Given an HR-HT instance G=(\mathcal{R}\cup\mathcal{H},E) with costs associated with hospitals, construct an augmented instance G^{\prime} such that G^{\prime} admits a strongly stable matching, and the total cost of the increase in quotas of all hospitals is minimized. In contrast to the polynomial-time solvability for MinSum-SS, we obtain a hardness result for the MinSum-Cost problem. Theorem 1.4 The MinSum-Cost problem is NP-hard and is inapproximable to within any multiplicative factor. We now turn our attention to the alternative objective: minimizing the maximum increase in quota for any hospital and define MinMax-SS problem. MinMax-SS: Given an HR-HT instance G=(\mathcal{R}\cup\mathcal{H},E), construct an augmented instance G^{\prime} such that G^{\prime} admits a strongly stable matching, and the maximum increase in the quota for any hospital is minimized, that is, \max_{h\in\mathcal{H}}\{q^{\prime}(h)-q(h)\} is minimized. Our result for MinMax-SS is shown in Theorem 1.5 Theorem 1.5 MinMax-SS is NP-hard even when resident preferences are single-peaked, and hospital preferences are derived from a master list. Moreover, the same minimization objective with the goal of constructing an instance that admits a resident-perfect strongly stable matching (one that matches all residents) is also NP-hard. 1.3 Related Work Capacity Modification: Chen and Csáji [11] studied a problem similar to ours for the case of strict preference lists. The goal was to augment the instance by increasing hospital quotas such that the resulting instance admits a perfect stable matching. They showed that with the MinMax objective, the problem admits a polynomial-time algorithm. In contrast, somewhat surprisingly, for strongly stable matching, we get an NP-hardness result ( Theorem 1.5) for MinMax. They also consider the MinSum objective, and show NP-hardness for getting an augmented instance that admits a stable and perfect matching under the MinSum objective. Note that this also implies NP-hardness for constructing an augmented instance in the HR-HT setting for achieving a strongly stable and perfect matching. However, without the perfectness requirement, our result in Theorem 1.2 gives a polynomial-time algorithm. Capacity modification to achieve specific objectives has attracted significant interest in recent years. Bobbio et al. [8] explored the complexity of determining the optimal variation (augmentation or reduction) of hospital quotas to achieve the best outcomes for residents, subject to stability and capacity variation constraints, and showed NP-hardness results. In a follow-up work, Bobbio et al.[6] developed a mixed integer linear program to address this issue, and in [7], they provided a comprehensive set of tools for obtaining near-optimal solutions. Gokhale et al. [18] considered the problem of modifying hospitals’ quotas to achieve two objectives – (i) to obtain a stable matching so as to match a given pair, and, (ii) to stabilize a given matching, either by only augmenting or only reducing hospital quotas. Afacan et al.[2] examined capacity design in the HR setting, to achieve a stable matching that is not Pareto-dominated by any other stable matching. Kavitha and Nasre [24] and Kavitha et al. [25] addressed the capacity augmentation problem for popular matchings in the one-sided preference list setting (where every hospital is indifferent between its neighbours). It is known that a popular matching is not guaranteed to exist in this setting. Therefore, their objective was to optimally increase hospital quotas to create an instance that admits a popular matching. Although we focus on a different setting (two-sided preference lists) and a different optimality notion – strong stability, it is interesting to note that our results closely resemble those obtained by Kavitha and Nasre [24] and Kavitha et al. [25]. Strong Stability: The notion of strong stability was first studied in the one-to-one setting for balanced, complete bipartite graphs (i.e. q(h)=1 for all h\in\mathcal{H}) by Irving [20], where they gave an O(n^{4}) algorithm to compute a strongly stable matching if it exists. Since then, the strongly stable matching problem has received a significant attention in the literature. Manlove [28] extended Irving’s results [20] to the general one-to-one setting (i.e. incomplete bipartite graphs) and also showed that all strongly stable matchings have the same size and match the same set of vertices. Irving et al. [22] further extended these results to the HRT setting and gave O(m^{2}) algorithm for the strongly stable matching problem, which was later improved to O(mn) by Kavitha et al. [23]. Manlove [29] studied the structure of the set of strongly stable matchings and showed that, similar to the classical stable matchings, the set of strongly stable matchings forms a distributive lattice. Kunysz et al. [27] showed that there exists a partial order with O(m) elements representing all strongly stable matchings and also provided an O(mn) algorithm to construct such a representation. In the presence of edge weights, Kunysz [26] showed that when edge weights are small, the maximum weight strongly stable matching problem can be solved in O(mn) time, and in O(mn\ \log(Wn)) if the maximum weight of an edge is W. Strong stability w.r.t. restricted edges viz. forced, forbidden and free edges has been studied by Cseh and Heeger [12] and by Boehmer and Heeger [9]. Organization of the paper In Sections 2, 3 and 4, the objective of our problems is to minimize the total increase in quotas. In Section 5, we study the MinMax-SS problem where the objective is to minimize maximum increase in quotas."
https://arxiv.org/html/2411.09712v1,Space-Air-Ground Integrated MEC-Assisted Industrial Cyber-Physical Systems: An Online Decentralized Optimization Approach,"Cloud computing and edge/fog computing are playing a pivotal role in driving the transformation of industrial cyber-physical systems (ICPS) towards greater intelligence and automation by providing high-quality computation offloading services to Internet of Things devices (IoTDs). Recently, space-air-ground integrated multi-access edge computing (SAGIMEC) is emerging as a promising architecture combining edge computing and cloud computing, which has the potential to be integrated with ICPS to accelerate the realization of the above vision. In this work, we first present an SAGIMEC-assisted ICPS architecture that incorporates edge computing and cloud computing through seamless connectivity supported by satellite networks to achieve determinism in connectivity, networked computing, and intelligent networked control. Then, we formulate a joint satellite selection, computation offloading, communication resource allocation, computation resource allocation, and UAV trajectory control optimization problem (\text{JSC}^{4}\text{OP}) to maximize the quality of service (QoS) of IoTDs. This problem considers both the dynamics and uncertainties of the system environment, as well as the limited resources and energy of UAVs. Given the complexity of \text{JSC}^{4}\text{OP}, we propose an online decentralized optimization approach (ODOA) to solve the problem. Specifically, \text{JSC}^{4}\text{OP} is first transformed into a real-time decision-making optimization problem (RDOP) by leveraging Lyapunov optimization. Then, to solve the RDOP, we introduce an online learning-based latency prediction method to predict the uncertain system environment and a game theoretic decision-making method to make real-time decisions. Finally, theoretical analysis confirms the effectiveness of the ODOA, while the simulation results demonstrate that the proposed ODOA outperforms other alternative approaches in terms of overall system performance.","With the rapid advancement of communication technologies and Internet of Things (IoT), industrial cyber-physical systems (ICPS) are increasingly becoming a crucial pillar in driving the transition of industry 4.0 towards intelligence and automation [1]. Specifically, the vision of ICPS is to integrate the physical processes of IoT devices (IoTDs) with the computational and communication capabilities of networks, thereby stimulating a wide range of intelligent applications that significantly enhance industrial production efficiency. However, a key challenge lies in the fact that enabling these intelligent applications typically requires handling a large volume of latency-sensitive and computation-heavy tasks, which conflicts with the limited computational resources and energy consumption of IoTDs. The integration of cloud computing and edge computing into ICPS has garnered significant attention as a viable solution, providing computing offloading services with high quality of service (QoS) for IoTDs. For example, Cao et al. [2] provided a comprehensive review of edge and edge-cloud computing-assisted ICPS architectures, in which the cloud computing and edge computing are combined into ICPS by deploying ground infrastructure. Li et al. [3] considered an industrial cyber-physical IoT system that utilizes the cloud data centers to achieve centralized control and processing. Hao et al. [4] proposed a softwarized-based ICPS architecture, where multiple terrestrial edge clouds are deployed to provide data analysis and processing for the ICPS. However, the aforementioned research on cloud or edge computing-assisted ICPS architectures heavily relies on the deployment of ground infrastructure, which may lead to the high deployment costs, limited coverage, and constrained application scenarios. For instance, in remote or disaster-stricken areas where the ground infrastructure is either nonexistent or unavailable, these architectures may struggle to operate effectively. To overcome the abovementioned challenges, space-air-ground integrated multi-access edge computing (SAGIMEC) is emerging as a promising architecture to provide edge and cloud computing services. Specifically, SAGIMEC is usually a three-tier computing architecture that integrates heterogeneous network components, including a terrestrial base station network, an aerial UAV network, and a space low earth orbit (LEO) satellite network [5]. First, thanks to the wide coverage of LEO satellites and flexible mobility of UAVs, SAGIMEC greatly expands the application scenarios and coverage of edge computing. Furthermore, the low transmission latency and seamless connectivity of LEO satellites enable SAGIMEC to effectively combine cloud-edge computing resources to improve the resource utilization. Therefore, SAGIMEC showcases substantial promise in propelling the evolution of ICPS. Several studies have investigated the SAGIMEC network. For example, to minimize the total system latency of the SAGIMEC network, Cheng et al. [6] jointly optimized the resource allocation and computation offloading. Fan et al. [7] formulated an optimization problem of joint resource allocation and computation offloading to reduce the system cost, which consists of the system energy consumption and system latency. To achieve higher system energy efficiency, Hu et al. [8] formulated the optimization problem of UAV trajectory control and resource allocation. However, the aforementioned studies above usually assume that the satellite link status can be accurately measured, which may be impractical because of the high-speed mobility, long-distance transmission, and time-varying network topology of satellite networks. Moreover, these studies usually formulate the optimization problems from the system perspective to consider indicators such as energy consumption and latency, which may neglect the QoS for IoTDs. Fully exploring the benefits of combining SAGIMEC with ICPS faces several fundamental challenges. i) Computation Offloading. The heterogeneity of the networks in SAGIMEC leads to an uneven distribution of resources. Moreover, different IoTDs usually have diverse computing requirements for resources. As a result, the heterogeneous resource distribution and IoTD requirements lead to the complexity of computation offloading decisions. ii) Satellite Selection. The dynamic topology of satellite networks leads to time-varying and uncertain satellite link conditions. Therefore, when multiple satellites are accessible, it is challenging to select appropriate satellites as relay nodes for the efficient use of cloud computing services based on satellite networks. iii) Resource Management. The tasks of IoTDs are often computation-heavy and latency-sensitive, imposing strict requirements on computing and communication resources. However, UAV networks usually have limited computing and spectrum resources. Therefore, the strict computing requirements make resource allocation difficult in the resource-constrained UAV network. iv) Trajectory Control. While the mobility of UAVs enhances the elasticity and flexibility of MEC, it also introduces important challenges related to UAV trajectory control. Furthermore, the limited battery capacity of UAVs leads to finite service time, which requires balancing both the service time of UAVs and the QoS of IoTDs. The abovementioned challenges necessitate efficient optimization of computation offloading, satellite selection, resource allocation, and UAV trajectory control. However, focusing on just one aspect of these components is insufficient to fully explore the advantages of SAGIMEC-assisted ICPS. First, these optimization variables are mutually coupled. For example, optimizing computation offloading requires the simultaneous consideration of satellite selection, resource allocation, and UAV location. Second, these optimization variables collectively determine the QoS of IoTDs. Therefore, these coupled optimization variables should be jointly optimized to fully exploit the performance of SAGIMEC-assisted ICPS, as it can effectively capture the intricate and coupling interactions and trade-offs among various optimization components. Consequently, we propose a novel online decentralized optimization approach (ODOA) that enables the joint optimization of computation offloading, satellite selection, communication resource allocation, computation resource allocation, and UAV trajectory control, to effectively improve the performance of SAGIMEC-assisted ICPS. Furthermore, compared to other approaches such as deep reinforcement learning (DRL), the proposed ODOA is more suitable for the considered system by leveraging the strengths of Lyapunov optimization, online learning, and game theory. Specifically, Lyapunov optimization is good for real-time decision-making without requiring direct knowledge of system dynamics and provides interpretability. Moreover, game theory can decentralized decision-making and guarantee existence of a solution, making the ODOA more scalable. Our main contributions are outlined as follows: • System Architecture. We propose an SAGIMEC-assisted ICPS architecture, where a UAV and a cloud computing center are seamlessly connected via a satellite network to facilitate high-quality computing offload services. Moreover, within this architecture, we consider the time-varying computing requirements of IoTDs, the energy and resource constraints of the UAV, as well as the dynamics and uncertainties of the satellite links to more accurately capture the real-world physical characteristics of SAGIMEC-assisted ICPS. • Problem Formulation. We formulate a joint satellite selection, computation offloading, communication and computation resource allocation, and UAV trajectory control optimization problem (\text{JSC}^{4}\text{OP}) to maximize the QoS of IoTDs. Moreover, we show that the formulated \text{JSC}^{4}\text{OP} is difficult to solve directly because it depends on future information and contains uncertain network parameters. In addition, we demonstrate that the \text{JSC}^{4}\text{OP} is non-convex and NP-hard. • Approach Design. Since the \text{JSC}^{4}\text{OP} is difficult to be directly solved, we propose the ODOA. Specifically, we first transform the \text{JSC}^{4}\text{OP} into a real-time decision-making optimization problem (RDOP) that only depends on current information by using the Lyapunov optimization. Then, for the RDOP, we propose an online learning-based latency prediction method to predict uncertain network parameters and a game theoretic decision-making method to make real-time decisions. • Performance Evaluation. The effectiveness and performance of the designed ODOA are confirmed through theoretical analysis and simulation experiments. In particular, the theoretical analysis proves that the ODOA not only satisfies the UAV energy consumption constraint, but also exhibits polynomial complexity. Additionally, the simulation results demonstrate that the ODOA outperforms other alternative approaches in terms of the overall system performance. The subsequent sections of this work are structured as follows. We introduce the relevant models and problem formulation in Section II. We detail the proposed ODOA and theoretical analysis in Section III. In Section IV, we demonstrate and discuss simulation results. Lastly, we present the conclusions in Section V. Figure 1: The proposed SAGIMEC-assisted ICPS architecture."
https://arxiv.org/html/2411.09517v1,Randomized Truthful Auctions with Learning Agents,"We study a setting where agents use no-regret learning algorithms to participate in repeated auctions. Kolumbus and Nisan (2022a) showed, rather surprisingly, that when bidders participate in second-price auctions using no-regret bidding algorithms, no matter how large the number of interactions T is, the runner-up bidder may not converge to bidding truthfully. Our first result shows that this holds for general deterministic truthful auctions. We also show that the ratio of the learning rates of the bidders can qualitatively affect the convergence of the bidders. Next, we consider the problem of revenue maximization in this environment. In the setting with fully rational bidders, Myerson (1981) showed that revenue can be maximized by using a second-price auction with reserves. We show that, in stark contrast, in our setting with learning bidders, randomized auctions can have strictly better revenue guarantees than second-price auctions with reserves, when T is large enough. Finally, we study revenue maximization in the non-asymptotic regime. We define a notion of auctioneer regret comparing the revenue generated to the revenue of a second price auction with truthful bids. When the auctioneer has to use the same auction throughout the interaction, we show an (almost) tight regret bound of \smash{\widetilde{\Theta}(T^{3/4})}. If the auctioneer can change auctions during the interaction, but in a way that is oblivious to the bids, we show an (almost) tight bound of \smash{\widetilde{\Theta}(\sqrt{T})}.","In auction design, truthfulness is a highly sought-after property. It allows bidders to simply reveal their true valuations, simplifying the bidding process. In the standard single item setting with fully rational profit-maximizing bidders, Myerson’s seminal paper Myerson (1981) shows that an auctioneer can achieve optimal revenue by using a truthful and deterministic auction mechanism – a Second Price Auction (SPA) with a reserve price. In many applications nowadays, buyers no longer bid directly in the auction but, instead, use learning algorithms to bid on their behalf. For example, in online advertising, platforms offer automated bidding tools that manage ad campaigns on behalf of advertisers. Such bidders learn to bid over many rounds and are not fully rational. In a surprising result, Kolumbus and Nisan (2022a) show that some appealing properties of second-price auctions break down in the presence of such learning bidders. In particular, when (profit-maximizing) bidders use no-regret learning algorithms, the second-price auction does not achieve as much revenue as with fully rational bidders. Indeed, bidders do not learn to bid their value, and consequently, the runner-up bidder’s bid is less than their value with positive probability, which diminishes the second price auction’s revenue. Moreover, Kolumbus and Nisan (2022b) show that for a setting where rational agents are using learning algorithms to bid, then it is no longer optimal to truthfully submit their value as the input to the learning algorithm. This raises a crucial question: are there truthful auctions that promote convergence to the true valuations within a learning environment, and can they also guarantee strong revenue performance? In this paper we provide an affirmative answer to this question. In doing so, we also showcase the value of randomized mechanisms — often overlooked in settings with profit-maximizing bidders — for environments where bidders are learning agents. While randomization introduces inherent inefficiencies due to allocations to low-valuation bidders, this very behavior facilitates learning among low-valuation bidders. A revenue-maximizing auctioneer must now carefully balance the randomization within a truthful mechanism to incentivize learning without incurring excessive revenue loss due to mis-allocation. We build our theory based on the model presented by Kolumbus and Nisan (2022a). We consider single-item repeated interactions over T periods. There are two profit-maximizing bidders participating in the auctions, with valuations that are drawn independently from the same distribution, and fully persistent over time. This assumption is motivated by online ad auctions, where multiple auctions are taking place every second, and the valuations of the advertisers remain stable for certain time scales, e.g., a day or a week. Thus, there is typically a very large sequence of auctions where the valuations of the participating agents are persistent. Bidders use mean-based no-regret learning algorithms (Braverman et al., 2018) and receive full feedback on which they base their updates. (Many of our results extend immediately to multiple bidders. We discuss other extensions, such as the partial feedback settings, in Appendix G.) The auctioneer focuses on truthful auctions, and their objective is to maximize the total revenue they achieve over the T rounds of interaction. Our results are the following: 1.1 Our Results and Techniques Limitations of Deterministic Auctions. Our first set of results (in Section 3) characterize the convergence of learners who are using Multiplicative Weights Update (MWU) in repeated deterministic auctions. In particular, we show the following sharp phase transition: • If the learning rate of the winning type is at least as fast as the learning rate of the runner-up type, then the runner-up type will not converge to bidding truthfully, even as T\rightarrow\infty; in fact, it will be bidding strictly below its true value, in expectation. • On the other hand, we show that in many auctions, such as SPA, if the learning rate of the runner-up type is strictly faster than that of the winning type, then the runner-up type will indeed converge to truthful bidding. These generalize the results of Kolumbus and Nisan (2022a) who showed that in SPA, when bidders are using MWU with the same learning rate, then the low type will not converge to bidding truthfully. The main challenges to proving this set of results arise from our study of general deterministic auctions, which have less structure than second-price auctions. Indeed, small differences in the learning rates can affect the landscape qualitatively, as is manifested from our results. Moreover, while the auctions are deterministic, the learning algorithms are randomized and highly correlated. Hence our approach is to break down the interaction into several epochs and establish some qualitative properties which hold, with high probability, at the end of each epoch. This requires a careful accounting of the cumulative utility of each bid of both bidders within every epoch; in particular, if our estimation is off by even some \omega(1) term, then it will not be sufficient to establish our result. Strictly-IC Auctions and the Power of Randomized Mechanisms. The results in Section 3 show that since the low valuation bidder tends to underbid, an auctioneer using SPA with reserve makes strictly less revenue than that predicted by the model with rational agents. Motivated by this, we consider a special class of randomized auctions called strictly-IC auctions. These are randomized truthful auctions where for each bidder, it is strictly better to bid their true valuation compared to any other bid. We show that any strictly-IC auction is asymptotically truthful: that is, the limit point of the bidder’s bid converges to their true value. Furthermore, we provide a black-box transformation from any truthful auction A (deterministic or not) to a randomized auction A^{\prime} that has the following two properties: (i) the bidders converge towards truthful bidding, and (ii) the difference between the allocation and payment rules of the original auction A and its strictly-IC counterpart A^{\prime} are negligible for any bid profile. Hence, such an auction A^{\prime} behaves similarly to A, but, crucially, it conveys information to the low bidder to help it converge to truthful bidding. As a corollary of this result, we get that SPA with reserve is not revenue-maximizing in this setting, and that randomization can get strictly more revenue than SPA with reserve. This is in stark contrast with the seminal result of Myerson (1981) which shows that SPA with reserve is optimal for rational bidders. At a more conceptual level, our results for randomized mechanisms can be viewed as showing that having enough randomness is key to the low bidder converging to truthful bidding: this randomness can come from the process itself, e.g., if bidder values are independently drawn in each round, as in Feng et al. (2021). But if not, and if the ranking of the bidders does not change much due to the lack of inherent randomness, our results show that injecting external randomness into the auction induces the desired learning behavior and hence improves the revenue. Having persistent valuations is just one case of the ranking of the bidders remaining stable over time: studying this case allows us to showcase our main ideas, but a central message of our work is that the presence or absence of stability in the rankings of the bidders is the main factor that dictates convergence to truthful bidding. A Non-Asymptotic Analysis. Our next set of results in Section 5 address the non-asymptotic regime. Here we consider the prior-free setting, meaning that the valuations of the bidders could be drawn from potentially different distributions that are unknown to the auctioneer. In order to evaluate its revenue performance when bidders are learning agents, we introduce the notion of auctioneer regret for an auction, which measures the difference between the revenue achieved over T rounds of implementing a given auction with learning bidders and the revenue achieved by implementing the optimal auction with rational bidders (i.e., SPA with a reserve price). Proposition 5.2 shows that if the auctioneer is constrained to use the same auction rule for all T rounds, then no truthful auction — deterministic or randomized — can achieve an auctioneer-regret better than \widetilde{O}(T^{3/4}) in the setting of adversarial valuations. However, if the auctioneer can change the auction rule just once within the T rounds, with the change happening at a time independent of the bid history, then the auctioneer’s regret drops to \widetilde{O}(\sqrt{T}), as we show in Section 5 Moreover, we show in Proposition 5.4 that this bound of \widetilde{O}(\sqrt{T}) is optimal even if the auctioneer can design the auction schedule. As a byproduct of our result, we show that the first-stage randomized auction used by the mechanism leads to the fastest convergence to truthful bidding from no-regret learning agents. To show that an auctioneer facing learning bidders using MWU must suffer an \Omega(T^{3/4}) revenue loss compared to the setting when it is facing rational agents, we break down the revenue loss into two non-overlapping epochs: one where the learning bidders have not converged to truthful bidding, and the other where the bidders are truthful. Now an auctioneer using the same auction throughout the interaction faces a trade-off: they can speed up the learning process to reduce the revenue loss from the first epoch, but this loses revenue in the second epoch due to the fact that the auction now differs significantly from SPA. Our result optimizes this trade-off to show that an \Omega(T^{3/4}) revenue loss is unavoidable. This naturally suggests decomposing the interaction into two epochs: in the first one, the auctioneer uses a truthful auction to facilitate the convergence to truthful bidding, and in the second one it uses SPA. We then design an auction that guarantees the fastest convergence to truthful bidding for mean-based learners in the prior-free setting, and we show that an improved revenue loss of at most \widetilde{O}(\sqrt{T}) can be achieved with this approach. (Importantly, to maintain truthfulness, the decisions of the auctioneers are fixed before the beginning of the interaction and are not affected by the bids.) This regret of \widetilde{O}(\sqrt{T}) seems surprising, because in traditional no-regret learning settings the optimal regret is achieved when the exploration and exploitation phase are intermixed. 1.2 Related Work The most closely related works to our setting are Feng et al. (2021); Deng et al. (2022); Kolumbus and Nisan (2022a); Banchio and Skrzypacz (2022); Rawat (2023). All these works study the long-term behavior of bidding algorithms that participate in repeated auctions, focusing on first-price and second-price auctions, but they give qualitatively different results. This is because they make different assumptions across two important axes: the type of learning algorithms that the bidders use and whether their valuation is persistent across the interaction or it is freshly drawn in each round. Feng et al. (2021) studied the convergence of no-regret learning algorithms that bid repeatedly in second-price and first-price auctions, where all agents have i.i.d. valuation that are redrawn in every round from a discrete distribution that has non-negligible mass on each point. They show that in this setting the bidders exhibit the same-long term behavior in both second-price and first-price auctions that classical theory predicts, i.e., the bids in second-price auctions are truthful and the bids in first-price auctions form Bayes-Nash equilibria. Kolumbus and Nisan (2022a) studied the same setting with the crucial difference that agents’ valuations are persistent across the execution and they are not resampled from some distribution at every iteration. Interestingly, they showed that in the case of two bidders with in second-price auctions, the agent that has the highest valuation will end up bidding between the low valuation and its valuation, whereas the agent with the low type will end up bidding strictly below its valuation. Intuitively, in their setting the high type bidder quickly learns to bid above the valuation of the low type bidder and always win the auction, and thus the low type does not get enough signal to push its bid distribution up to its valuation. On the other hand, when the valuations are redrawn as in Feng et al. (2021), the competition that the agents face varies. In the long run, this gives enough information to the algorithms to realize that bidding truthfully is the optimal strategy. In the case of first-price auctions where the agents have persistent valuations, both Kolumbus and Nisan (2022a); Deng et al. (2022) provide convergence guarantees of no-regret learning algorithms. The type of “meta-games” we touch upon in our work, where we want to understand the incentives of the agents who are submitting their valuations to bidding algorithms that participate in the auctions on the behalf of these agents, were originally studied by Kolumbus and Nisan (2022a) and, subsequently, for more general classes of games by Kolumbus and Nisan (2022b). The pioneering work of Hart and Mas-Colell (2000) showed that when players deploy no-regret algorithms to participate in games they converge to coarse-correlated equilibria. Recently, there has been a growing interest in the study of no-regret learning in repeated auctions. The empirical study of Nekipelov et al. (2015) showed that the bidding behavior of advertisers on Bing is consistent with the use of no-regret learning algorithms that bid on their behalf. Subsequently, Braverman et al. (2018) showed, among other things, that when a seller faces a no-regret buyer in repeated auctions and can use non-truthful, it can extract the whole welfare as its revenue. A very recent work (Cai et al., 2023) extended some of the previous results to the setting with multiple agents. For a detailed comparison between our work and Cai et al. (2023), we refer to Appendix B. Banchio and Skrzypacz (2022); Rawat (2023) diverge from the previous works and consider agents that use Q-learning algorithms instead of no-regret learning algorithms. Their experimental findings show that in first-price auctions, such algorithmic bidders exhibit collusive phenomena, whereas they converge to truthful bidding in second-price auctions. One of the main reasons for these phenomena is the asynchronous update used by the Q-learning algorithm. The collusive behavior of such algorithms has also been exhibited in other settings (Calvano et al., 2020; Asker et al., 2021, 2022b; den Boer et al., 2022; Epivent and Lambin, 2022; Asker et al., 2022a). Notably, Bertrand et al. (2023) formally proved that Q-learners do collude when deployed in repeated prisoner’s dilemma games. In a related line of work, Zhang et al. (2023) study the problem of steering no-regret learning agents to a particular equilibrium. They show that the auctioneer can use payments to incentivize the algorithms to converge to a particular equilibrium that the designer wants them to. An interpretation of our results is that randomization is a way to achieve some kind of equilibrium steering in repeated auctions. Diverging slightly from the setting we consider, some recent papers have illustrated different advantages of using randomized auctions over deterministic ones. Mehta (2022); Liaw et al. (2023) showed that there are randomized auctions which induce equilibria with better welfare guarantees for value-maximizing autobidding agents compared to deterministic ones. In the setting of revenue maximization in the presence of heterogeneous rational buyers, Guruganesh et al. (2022) showed that randomization helps when designing prior-free auctions with strong revenue guarantees, when the valuations of the buyers are drawn independently from, potentially, non-identical distributions."
https://arxiv.org/html/2411.09355v1,"Prices, Bids, Values: Everything, Everywhere, All at Once","We study the design of iterative combinatorial auctions (ICAs). The main challenge in this domain is that the bundle space grows exponentially in the number of items. To address this, several papers have recently proposed machine learning (ML)-based preference elicitation algorithms that aim to elicit only the most important information from bidders to maximize efficiency. The SOTA ML-based algorithms elicit bidders’ preferences via value queries (i.e., “What is your value for the bundle \{A,B\}?”). However, the most popular iterative combinatorial auction in practice elicits information via more practical demand queries (i.e., “At prices p, what is your most preferred bundle of items?”). In this paper, we examine the advantages of value and demand queries from both an auction design and an ML perspective. We propose a novel ML algorithm that provably integrates the full information from both query types. As suggested by our theoretical analysis, our experimental results verify that combining demand and value queries results in significantly better learning performance. Building on these insights, we present MLHCA, the most efficient ICA ever designed. MLHCA substantially outperforms the previous SOTA in realistic auction settings, delivering large efficiency gains. Compared to the previous SOTA, MLHCA reduces efficiency loss by up to a factor of 10, and in the most challenging and realistic domain, MLHCA outperforms the previous SOTA using 30% fewer queries. Thus, MLHCA achieves efficiency improvements that translate to welfare gains of hundreds of millions of USD, while also reducing the cognitive load on the bidders, establishing a new benchmark both for practicability and for economic impact.","Combinatorial auctions (CAs) are used to allocate multiple items among several bidders who may view those items as complements or substitutes. In a CA, bidders are can submit bids for whole bundles/packages of items. CAs have enjoyed widespread adoption in practice, with their applications ranging from allocating spectrum licences (Cramton, 2013) to TV ad slots (Goetzendorff et al., 2015) and airport landing/take-off slots (Rassenti et al., 1982). The key challenge in CAs is that the bundle space grows exponentially in the number of items, making it impossible for bidders to report their full value function in all but the smallest domains. Moreover, Nisan & Segal (2006) showed that for arbitrary value functions, CAs require an exponential number of bids in order to guarantee full efficiency. Thus, practical CA mechanisms cannot provide efficiency guarantees in real world settings with more than a modest number of items. Instead, the focus has shifted towards iterative combinatorial auctions (ICAs), where bidders interact with the auctioneer over a series of rounds, providing only a limited (i.e., practically feasible) amount of information, with the aim to maximize the efficiency of the final allocation. The most established ICA following this interaction paradigm is the combinatorial clock auction (CCA) (Ausubel et al., 2006). The CCA has been extensively used for allocating spectrum licenses, generating over USD 20 billion in revenue between 2012 and 2014 alone (Ausubel & Baranov, 2017). Speed of convergence is a critical consideration for any ICA since each round entails costly computations and business modelling for the bidders (Kwasnica et al., 2005; Milgrom & Segal, 2017; Bichler et al., 2017). Large spectrum auctions following the CCA format can take more than 100 bidding rounds. In order to decrease the number of rounds, many CAs in practice use aggressive price update rules (e.g., increasing prices by up to 10% each round), which can harm efficiency (Ausubel & Baranov, 2017). Thus, it remains a challenging problem to design a practical ICA that is efficient and converges in a small number of rounds. Specifically, given the value of resources allocated in such real-world ICAs, increasing their efficiency by even one percentage point already translates into welfare gains of hundreds of millions of dollars. 1.1 ML-Powered Iterative Combinatorial Auctions To address this challenge, researchers have proposed various ways of using machine learning (ML) to improve the efficiency of ICAs. The seminal works by Blum et al. (2004) and Lahaie & Parkes (2004) were the first to frame preference elicitation in CAs as a learning problem. In more recent years, Brero et al. (2018; 2021), Weissteiner & Seuken (2020); Weissteiner et al. (2022b; a; 2023) proposed ML-powered ICAs. At the heart of those approaches lies an ML-powered preference elicitation algorithm that uses an ML model to learn each bidder’s value function to generate an informative value query (i.e., “What is your value for the bundle \{A,B\}?”), which in turn refines that bidder’s ML model.111From an optimization task perspective this setting can be viewed as a combinatorial Bayesian optimization problem. While those value-query based ML-powered ICAs lead to significant efficiency gains redefining the state-of-the-art (SOTA) efficiency results in many realistic auction domains, those approaches suffer from one common practical limitation: they fundamentally rely throughout the whole ICA on value queries (VQs). Prior research in auction design has identified demand queries (DQs) as the best way to run an auction (Cramton, 2013). Their advantages compared to value queries include elimination of tacit collusion and bid signaling, as well as simplified bidder decision-making that keeps the bidders focused on what is most relevant: the relationship between prices and aggregate demand. Additionally, value queries are cognitively complex, and thus typically should be only used sparsely in real-world ICAs. For these reasons, DQs are the most prominent interaction paradigm for auctions in practice. Following this rationale, Soumalias et al. (2024b) addressed the common limitation of prior work by designing the first practical ML-powered ICA that elicits information from bidders via DQs instead of VQs and only makes use of VQs in supplementary rounds, when bidders have already obtained a clearer picture on which bundles they can realistically hope to clinch and how much they should approximately value such bundles. While this DQ-based ICA represented a significant leap towards making ML-powered ICAs practical and at the same time outperformed the baseline CCA that is typically used in real-world applications, it still suffered from the following two important deficiencies: First, it could not reach the SOTA efficiency of the impractical VQ-based ML-powered ICAs. Second, to improve efficiency, just like the CCA, it required the use of a supplementary round, in which the bidders must decide on which additional value bids to submit to the mechanism, a cognitive complicated task for the bidders. The present paper closes these two last gaps in the realm of ICAs by designing a hybrid ML-powered ICA that combines DQ-based rounds with a sophisticated yet practical VQ-based supplementary round. Importantly, this hybrid ML-powered ICA clearly outperforms the previous SOTA ICA while still being practical in real-world applications. 1.2 Our Contributions In this paper, we introduce the Machine Learning-powered Hybrid Combinatorial Auction (MLHCA), a practical ICA that achieves unprecedented efficiency. Our contributions are as follows: 1. In Section 3, we provide a theoretical foundation and illustrative examples that demonstrate the advantages and limitations of DQs and VQs as input mechanisms for auctions and learning algorithms. 2. In Section 4, we introduce a learning algorithm capable of leveraging both types of queries. We provide strong experimental evidence of the learning benefits of combining both query types, as well as the advantages of starting an auction with DQs instead of VQs. 3. In Section 5 we combine our auction and ML insights to develop MLHCA, the first ICA to incorporate both sophisticated DQ and VQ generation algorithm. Simulations in realistic domains show that MLHCA significantly outperforms the previous SOTA, achieving higher efficiency with 40% fewer queries (Section 6), setting a new benchmark for both efficiency and practicality. 1.3 Further Related work In the field of automated mechanism design, Dütting et al. (2015; 2019), Golowich et al. (2018) and Narasimhan et al. (2016) used ML to learn new mechanisms from data, while Cole & Roughgarden (2014); Morgenstern & Roughgarden (2015) and Balcan et al. (2023) bounded the sample complexity of learning approximately optimal mechanisms. In contrast to this prior work, our design incorporates an ML algorithm into the mechanism itself, i.e., the ML algorithm is part of the mechanism. Lahaie & Lubin (2019) suggest an adaptive price update rule that increases price expressivity as the rounds progress in order to improve efficiency and speed of convergence. Unlike that work, we aim to improve preference elicitation in the main rounds while still using linear prices. Preference elicitation is a key market design challenge outside of CAs too. Soumalias et al. (2024a) introduce an ML-powered mechanism for course allocation that improves preference elicitation by asking students comparison queries. Despite the prominence of DQs in real-world applications, the only prior work apart from Soumalias et al. (2024b) on ML-based DQs that we are aware of is that of Brero & Lahaie (2018) and Brero et al. (2019), who proposed integrating ML in a price-based ICA to generate the next price vector in order to achieve faster convergence. However, this prior work does not exploit any notion of similarity between bundles that contain overlapping items, only incorporates a fraction of the information revealed by the agents’ bidding (i.e., for the bundle an agent bids on, her value for that bundle must be larger than its price), and is computationally intractable already in medium-sized auction domains. See Appendix D for further related work. 1.4 Practical Considerations and Incentives MLHCA integrates both ML-powered DQ and VQ rounds. In DQ-based auctions like the CCA or ML-CCA, ensuring truthful bidding depends heavily on well-chosen activity rules and payment rules. In Section A.3, we provide a detailed discussion of the most common activity rules used in the CCA to align incentives, and detail how MLHCA can also leverage these rules to achieve the same goal. The VQ rounds in MLHCA extend the MLCA framework (Brero et al., 2021) by incorporating information from earlier DQ rounds into bidders’ ML models. Brero et al. (2021) argued that MLCA offers strong practical incentives, and under two additional assumptions, truthful bidding is an ex-post Nash equilibrium. In Section A.4 we provide a detailed discussion of these arguments, and detail why they also apply to MLHCA’s VQ rounds. By effectively combining activity rules in the DQ rounds and leveraging the established incentive structure of MLCA in the VQ rounds, MLHCA achieves a robust incentive alignment across all its stages."
https://arxiv.org/html/2411.09646v1,"Reducing Stochastic Games 
to Semidefinite Programming","We present a polynomial-time reduction from max-average constraints to the feasibility problem for semidefinite programs. This shows that Condon’s simple stochastic games, stochastic mean payoff games, and in particular mean payoff games and parity games can all be reduced to semidefinite programming.","There are two important clusters of computational problems that are not known to be in the complexity class P: the first is related to numeric computation, and contains the sums-of-square-roots problem [GGJ76, EHS24], the Euclidean shortest path problem, PosSLP [ABKPM09], and the feasibility problem for semidefinite programs [NN94, Ram97]. The second cluster is related to tropical geometry, and contains for instance the model checking problem for the propositional \mu-calculus [EJ91, EJS93], parity games [Mar75, Mos91], mean payoff games [EM79], and/or scheduling [MSS04], stochastic mean payoff games [AM09], and simple stochastic games [Con92]. So far, no polynomial-time reduction from a problem of one of the clusters to a problem from the other cluster was known. We show that all of the mentioned problems from the second cluster can be reduced in polynomial time to the feasibility problem for semidefinite programs from the first cluster. Semidefinite programming is a generalisation of linear programming with many important algorithmic applications in discrete and continuous optimisation, both from a theoretical and a practical perspective [GLS94, BTN01, Las09]. However, already the feasibility problem for semidefinite programs is not known to be in P. By Ramana’s duality [Ram97], the problem is either in the intersection of NP and coNP, or outside of the union of NP and coNP. The semidefinite feasibility problem falls into the existential theory of the reals, which is known to be in PSPACE [Can88]. However, it has been observed by Khachiyan that the smallest feasible solution to an SDP might be of doubly exponential size (see, e.g., [PK97]), which is an obstacle for the polynomial-time algorithmic methods known for linear programming. In fact, it is possible to reduce the PosSLP problem to semidefinite programming [TV08]. PosSLP has been introduced in [ABKPM09], motivated by the ‘generic task of numerical computation’. For example, the sums-of-square-roots problem, which is a numeric computational problem not even known to be in NP and the barrier for polynomial-time tractability for numerous problems in computational geometry, such as the Euclidean shortest path problem, has a polynomial-time reduction to PosSLP [ABKPM09]. Mean payoff games are turn-based two-player games on graphs [EM79, ZP96]; it is a famous open problem in theoretical computer science whether there is a polynomial-time algorithm to decide for a given graph which of the players has a winning strategy. Finding the winning region in mean payoff games is polynomial-time equivalent to various other computational problems in theoretical computer science, for instance scheduling with and-or-constraints [MSS04], the max-atoms problem [BNRC08], as well as solvability of max-plus systems and tropical linear feasibility [ABG07, AGG12]. The latter can be seen as the tropical analog of testing feasibility of linear inequalities (which for classical geometry is known to be in P, e.g. via the ellipsoid method [Kha79, GLS94]). Furthermore, there is a simple reduction from parity games to mean payoff games by using the priorities of the parity game with a suitably large basis [Pur95]. Parity games are polynomial-time equivalent to the model-checking problem of the propositional \mu-calculus [EJS93], which has been called ‘the most important logics in model checking’ [BW18]. For parity games, a quasipolynomial algorithm has been found recently [CJK+22]. Subsequent work indicates that the various quasipolynomial methods that have been obtained lately for parity games [LPSW22, JMT22, LB20] do not extend to mean payoff games [CFGO22]. Mean payoff games can be generalised to stochastic mean payoff games (sometimes called 2\frac{1}{2}-player games), where the graph might contain stochastic nodes. If the strategy of one of the two players in such a game is fixed, the game turns into a Markov decision process, for which polynomial-time algorithms based on linear programming are known [Put05]. Stochastic mean payoff games are equivalent under polynomial-time Turing reductions to Condon’s simple stochastic games [AM09], which are known to be in the intersection of NP and coNP [Con92] (we may even assume that the simple stochastic games are stopping, see the discussion in Section 2.2.) There are many other variants of games that all have the same complexity, see [AM09]. Stochastic mean payoff games can also be reduced to max-plus-average constraints, which are still in NP \cap coNP [BM18]. Figure 1: Computational problems that are not known to be in P and not known to be NP-hard, and their relationship. Arcs indicate polynomial-time (many-one) reductions, the dotted arc indicates a polynomial-time Turing reduction. We introduce a fragment of max-plus-average constraints, which we call max-average constraints. There is a polynomial-time reduction from stopping simple stochastic games to max-average constraints (Section 3). Max-average constraints have the advantage that they can be further reduced to the feasibility problem for semidefinite programs. This implies that all the mentioned computational problems from the second cluster can be reduced to semidefinite programming. See Figure 1 for an overview of the mentioned computational problems and their relationship. The reduction uses a similar idea as the reduction of Schewe [Sch09] to reduce parity games and mean payoff games to linear programming, which is, however, not a proper polynomial-time reduction. The idea is to replace constraints of the form x_{0}=\max(x_{1},\dots,x_{k}) by x_{0}=\log_{b}(b^{x_{1}}+\dots+b^{x_{k}}) for a sufficiently large b. Following [AGS18], we extend this idea to max-average constraints and obtain a proper polynomial-time reduction to non-Archimedean SDPs. From there, we use bounds from quantifier elimination results to translate non-Archimedean SDPs to real SDPs. However, we still have the obstacle that these SDPs involve coefficients of doubly exponential size, which means that they have exponential representation size and do not lead to a polynomial-time reduction. We overcome this difficulty by using the duality of semidefinite programming to find small expressions that define coefficients of doubly exponential size, which combined with the above ideas leads to a polynomial-time reduction from max-average constraints to (real) SDPs. Our results imply that if there is a polynomial-time algorithm for SDPs, then all the mentioned problems in the second cluster can be solved in polynomial time as well. Conversely, our reduction can be used to translate interesting families of instances for (stochastic) mean payoff games or parity games into SDP instances, which might yield interesting families of instances for (algorithmic approaches to) semidefinite programming. For linear programming, this idea turned out to be very fruitful: certain instances of parity games have been used to prove exponential lower bounds for several famous pivoting rules for the simplex method in linear programming [FHZ11, Fri11, FHZ14, AF17, DFH23]. However, these results were not based on a general polynomial-time reduction from parity games to linear programming (which is not known!). We also hope that algorithmic ideas for solving games might generalise to (certain classes of) SDPs. Our results also show that two large research communities (convex real algebraic geometry, and more generally convex optimisation on the one hand, and verification and automata theory on the other hand) were working on closely related computational problems and might profit from joining forces in the future. 1.1 Related work There are multiple works that reduce games to continuous optimization problems. As mentioned above, one-player mean payoff games (Markov decision processes) can be reduced to linear programming [Put05], which implies that they can be solved in polynomial time. For the case of two-player deterministic mean payoff games, Schewe [Sch09] proposed a reduction to linear programs with coefficients of exponential representation size. Such programs can also be interpreted as linear programs over non-Archimedean ordered fields [ABGJ15, ABGJ14, ABGJ21]. The idea of Schewe was generalized to stochastic games in [BEGM17], where the authors prove that stochastic mean payoff games can be encoded by convex programs with constraints of exponential encoding size. Such programs can be solved by the ellipsoid method in pseudo-polynomial time provided that the number of stochastic nodes is fixed. The non-Archimedean approach was generalized from deterministic games to stochastic games in the work [AGS18], which proves that stochastic mean payoff games can be reduced to semidefinite programs over non-Archimedean fields. The paper [AGS18] serves as a basis for our results. These reductions express mean payoff games as convex optimization problems, but these problems are “non-standard” in the sense that they either use constraints of exponential encoding size or are expressed over non-Archimedean fields. There is yet another line of work deriving polynomial-time reductions from mean payoff games to classical optimization problems which are not convex. In particular, [Con93] expressed simple stochastic games as non-convex quadratic programs. This reduction is further studied in [KRSW22]. Another reduction to (generalized) complementarity problems with P-matrices was proposed in [GR05, SV06] and extended in subsequent works [JS08, FJS10, HIJ13]. Such linear complementarity problems form a subclass of quadratic programs and belong to the complexity class UniqueEOPL [FGMS20]. However, they are in general non-convex and can therefore not be expressed by semidefinite programs."
https://arxiv.org/html/2411.09168v1,Theory of Mind Enhances Collective Intelligence,"Collective Intelligence plays a central role in a large variety of fields, from economics and evolutionary theory to neural networks and eusocial insects, and it is also core to much of the work on emergence and self-organisation in complex systems theory. However, in human collective intelligence there is still much more to be understood in the relationship between specific psychological processes at the individual level and the emergence of self-organised structures at the social level. Previously psychological factors have played a relatively minor role in the study of collective intelligence as the principles are often quite general and applicable to humans just as readily as insects or other agents without sophisticated psychologies. In this article we emphasise, with examples from other complex adaptive systems, the broad applicability of collective intelligence principles while the mechanisms and time-scales differ significantly between examples. We contend that flexible collective intelligence in human social settings is improved by our use of a specific cognitive tool: our Theory of Mind. We identify several key characteristics of psychologically mediated collective intelligence and show that the development of a Theory of Mind is a crucial factor distinguishing social collective intelligence from general collective intelligence. We then place these capabilities in the context of the next steps in artificial intelligence embedded in a future that includes an effective human-AI hybrid social ecology.","All intelligence is collective intelligence. [70] Collectives are capable of achieving things that individuals alone cannot. Notwithstanding the simplicity or complexity of the individuals, their aggregate behaviour can often be understood as a complex processing of information that individuals store, modify, and transfer between each other producing ‘useful’ collective behaviour at the scale of the whole collective. In most instances of Collective Intelligence (CI), where the agents might be ants in an ant colony, bees in a beehive, or neurons in a neural network, the individual is not aware of the drivers of their behaviour or the behaviour of other agents. For example, a single neuron is neither aware of its own internal processes nor that of a neuron it is connected to, nor is it aware of the end goal to which its activity contributes. Despite both this lack of awareness and the lack of a centralised controller, evolutionary and learning processes have produced an intricate, precise, and highly adaptive system that is capable of functional behaviour that would be impossible for any single neuron to achieve. In other instances of CI, such as teams of humans, or businesses interacting in economic markets, the agents themselves may be highly complex and exhibit varying degrees of purposefulness and awareness. Within this context, we draw attention to the role of psychological factors in improving the CI of human social collectives and quantifying the intelligence of social collectives, both natural and artificial. In order to understand how collectives process information, we first consider the variety of ways in which agents interact. The topology of the network describing agent-to-agent interactions is well known to be important for the proper functioning of social groups [83, 79]. In particular it has been shown that mammalian social groups exhibit patterns of fractal-like topologies [40, 51] that are a result of a cognitive ability to form discrete social connections between conspicifics [49]. These links are often both spatially and temporally transient; people meet for a while, go their separate ways, and come back together later. Despite this transience, individual connections are often the basis of long term social relationships between specific individuals as in pair-bonding and friendships. Consequently an important distinction can be made regarding connections between agents in complex adaptive systems: they can be more fluid-like or more solid-like [101]. For example the links between neurons in the brain are relatively fixed in nature when compared to the brief communicative interactions between ants, either instantaneous interactions between individual ants or via transient pheromone trails that coordinate the behaviour of large numbers of ants. Solé and colleagues [101, 88] identify a distinction between solid brains, in which interactions between agents fixed in place are highly persistent in time (e.g. neural networks, spin glasses) and liquid brains, in which interactions between highly mobile agents are much more short-lived (e.g. ants, immune cells). As Solé et al. note regarding liquid brains [101]: “Here there are no neural-like elements and yet in many ways these systems solve complex problems, exhibit learning and memory, and make decisions in response to environmental conditions.” All biological agents are composed of sub-units such as organs, cells, and molecular networks [67, 69, 71]. Cells in particular are the simplest living organisms with individual intelligence, or competencies [67, 33], within their native contexts. Here, we briefly focus on the archetypal single-cell intelligence, the neural cells. It is well understood that the central nervous system is a highly developed, adaptive, complex system that exhibits emergent computational characteristics [52], both in biological and artificial neural networks. Naturally the artificial models are simplifications but the extent to which they are simplifications is not so well understood. In a 2021 study, Beniaguev et al. [9] concluded that between five and eight layers of an artificial deep neural network are required to approximate the input–output mapping of a (single) cortical neuron and that the dendritic branches can be understood as spatiotemporal pattern detectors. This demonstrates that a single neural cell can be modelled as an artificial agent with highly complex computational capabilities situated within an adaptive, complex network of other highly complex agents, all signalling to one another. These results can be compared with earlier studies in which neurons were modelled as a Bayesian agent that is trying to infer the state of a hidden variable [25]. In each of these interpretations, a single cell can be seen as an agent with computational competencies situated within the context of a network that is slowly and adaptively changing around it. We can also compare the competencies of neural cells in networks to the individual competencies of ants in an ant colony. In a recent study [56] it was shown that social structures of some ant colonies are conserved between species that are separated by more than 100 million years of evolution. In the five species studied by Kay et al. [56], they found two social clusters and similarities in the division of labour that are preserved between the species. In a different study, Richardson et al. [91] showed that individuals within an ant colony play an important leadership role and that the behaviour of these individuals significantly improved the collective performance of the ants. Ants are also capable of changing their social structure in the event of pathogenic infestation of their colony. In a 2021 article, Stockmaier and colleagues [102] review the research on social distancing and other social restructuring that occurs with conspecifics in order to reduce the impact of pathogens by changing their social cues, signals, and other behaviours for the collective benefit of the colony. These two very different systems, neural networks and ant colonies, are examples of complex collective intelligences where the individuals (neurons, ants) are complex in their own right, but they signal each other in order to restructure their relationships so as to adapt their collective competencies to external signals. The neural networks are prototypical solid brains and ant colonies are prototypical liquid brains. Human social interactions can also be viewed as a form of liquid intelligence. Migliano et al. [79] discuss the ‘fluidity’ of social relations in early human societies: “Quantification and mapping of hunter–gatherers’ social networks has revealed details of a fluid and multilevel sociality, where friendship links connect unrelated mobile households into camps of temporary composition”. They describe the key characteristics of early human society, such as egalitarianism, division of labour, cooperative living with unrelated individuals, multi-locality, fluid social structures, and high mobility between campsites, which might be thought of as a liquid brain composed of social interactions that both cluster and disperse in order to store, modify, and transfer information via social networks. The notion that human social interaction might be a form of computation is not new: Mirowski, Axtell and colleagues [82, 60, 81] have suggested that economic markets are a form of computation by which prices can be derived, and Harré recently hypothesised [45] that this could be measured using information theory as had been done earlier for financial markets [47, 42]. As Axtell et al. [60] wrote: “There is a close connection between agent computing in the positive social sciences and distributed computation in computer science, in which individual processors have heterogeneous information that they compute with and then communicate to other processors.” The emergence of computation in multi-agent systems is a well-studied area of complex adaptive systems [64, 84]. For example neuroscience has used information theory to describe the storage, transfer, and modification of bits of information in biological neural processes [117]. More broadly, Integrated Information Theory (IIT) [107, 77] has been put forward as a measure of the emergence of ‘consciousness’ in generic (non-biological, non-neural) systems. In this case, some forms of IIT explicitly use information theory [8, 78] to measure the amount of non-trivial computation a system is carrying out. More generally, there is a move towards understanding complex adaptive systems in computational terms [89, 74] by empirically measuring the inter-agent flow of information [12]. In this article we use information theory to quantify how much computation in a CI is ‘emergent’ and how much is simply independent information processing by single agents. In general, we wish to capture the notion of the whole (computational process) being greater than the sum of the (independent) parts. We translate this to the simple notion that to the extent to which this inequality holds: Whole - \sum(Parts)>0 is the extent to which we will say a system exhibits non-trivial CI, noting that there are multiple possible implementations of this approach [54]. The Parts is how much computation a single agent is carrying out from one time step to the next such that the sum is the total of all agents’ independent computations. The Whole is the totality of computation in the system, it includes all single agent computations, pairwise computations, and higher order interactions between agents. Our measure will not be unique in any of its specifics, but it serves to quantify the CI of a system for comparative analysis. This approach also has much in common with that of Moore et al. [84] in which information theory is used to measure the collective intelligence in biological systems. Not only is there diversity in the types of systems that can show positive measures of CI, but the ways in which agents manipulate a system’s computations is diverse as well. Take for example Watson and Levin’s discussion of a scientist manipulating the intercellular signalling in order to change their collective outcome [110]: This framework [of collective cellular intelligence] makes a strong prediction: if intercellular signalling (not genes) is the cognitive medium of a morphogenetic individual, it should be possible to exploit the tools of behavioural and neuro-science and learn to read, interpret and re-write its information content in a way that allows predictive control over its behaviour (in this case, growth and form) without genetic changes. A counter question is: How can single agents, such as human leaders, have predictive control over a social group? Just as a scientist external to a cell collective can manipulate inter-cellular signalling to control the outcomes of the cell collective, a leader internal to a human collective can manipulate inter-personal behaviours to control the outcomes of the human collective. In both cases, an agent with a goal-directed psychology is acting on inter-agent relationships, i.e. inter-cellular or inter-personal, to control outcomes at the next level higher, i.e. organism-scale or societal-scale. In this work we will ask an analogous question of human agents: What is there in human psychology that allows us to learn to read, interpret, and re-write our interpersonal information content in a way that allows predictive control over our collective behaviour? We will not be able to explore all of the possible interpretations of this question here, but we posit that our Theory of Mind (ToM) is a suite of cognitive skills that allows individuals to have goal directed control over collective outcomes. Originally ToM was used to describe our ability to infer the unobserved mental states of other people [34] such as desires and beliefs, an ability humans are particularly good at and other animals much less so [86, 59]. But recently it has been shown that ToM is predictive of group performance as well [121, 31], empirically demonstrating the role of ToM in going beyond representations of the internal states of others to using that knowledge in a social setting to improve the collective outcomes for the group. In order to model ToM in a tractable fashion, we will focus on the narrower game theory of mind [123], and the Beliefs, Preferences, and Constraints (BPC) interpretation of game-theoretic decisions put forward by Gintis [35]. In this approach, what agents understand of other agents’ hidden states are the BPC that structure their observable behaviours. We will consider this question in the framework of agent interactions that extend agent utilities in a simple but novel way. We quantify our results using information theory to show the impact that a correctly deployed ToM has to direct agents’ behaviours in order to increase our CI. The models are simple but they illustrate the central notion that understanding the “beliefs, preferences, and constraints” [36, 37] of others can be used to improve the CI of a complex social system. In Section 2, we describe the liquid–solid dichotomy of interacting agents, review extant models of ToM, and provide perspective on the interplay between social network structures and ToM. In Section 3, we provide illustrative examples supporting different aspects of our argument, introducing our measure of computation and applying it to a simple empirical example. In Section 4 we review the psychology of social fluidity and the variety of social outcomes that this fluidity makes possible. We also use a simple multi-agent system to describe how a ToM can be used to improve the computational processes, i.e. the CI, of interacting agents. Finally, in Section 5, we discuss the broader implications of this approach."
https://arxiv.org/html/2411.08784v1,"Towards Fair and Efficient Public Transportation:
A Bus Stop Model","We consider a stylized formal model of public transportation, where a set of agents need to travel along a given road, and there is a bus that runs the length of this road. Each agent has a left terminal and a right terminal between which they wish to travel; they can walk all the way, or walk to/from the nearest stop and use the bus for the rest of their journey. The bus can make a fixed number of stops, and the planner needs to select locations for these stops. We study notions of efficiency and fairness for this setting. First, we give a polynomial-time algorithm for computing a solution that minimizes the total travel time; our approach can capture further extensions of the base model, such as more general cost functions or existing infrastructure. Second, we develop a polynomial-time algorithm that outputs solutions with provable fairness guarantees (such as a variant of the justified representation axiom or 2-approximate core) as long as the agents’ costs only depend on the distance they need to walk. Our simulations indicate that our algorithm almost always outputs fair solutions, even for parameter regimes that do not admit theoretical guarantees.","The use of private vehicles is one of the most significant contributors to pollution. For instance, it is responsible for 43% of the greenhouse gas emissions in the European Union (European Parliament, 2019). Therefore, providing well-functioning public transport has repeatedly been identified as a key factor in fighting climate change (Waterson et al., 2003; Chapman, 2007; Kwan and Hashim, 2016). The need to model and solve problems related to public transport has been under scrutiny from an operations research perspective; see, e.g., the extensive survey by Desaulniers and Hickman (2007). In the optimization literature, the implementation of public transport infrastructure is commonly seen as a two-stage process consisting of a planning phase and an operational phase. The planning phase is concerned with the design of the transportation network as well as with determining optimal operation frequencies (Lampkin and Saalmans, 1967; Silman et al., 1974). In the operational phase, the cost of operating public transport should be minimized, e.g., by optimally assigning vehicles to routes or drivers to buses (Daduna and Pinto Paixão, 1995; Wren and Rousseau, 1995). In both phases, the primary metric used to evaluate the solution quality is the social welfare, i.e., the total/average travel time. While optimizing the social welfare is a natural and intuitively appealing goal, we believe that it is equally important to approach the design of transportation networks from a fairness perspective. That is, the proposed route networks, frequencies and types of vehicles should benefit not just the majority of the population, but also smaller and less powerful groups, providing usable connections between all neighborhoods and serving the needs of all residents. We propose to tackle this challenge using the conceptual apparatus of group fairness, building on the ideas of justified representation in multi-winner voting (Aziz et al., 2017) and core stability in cooperative game theory (Gillies, 1959). The intuition that we aim to capture is that sufficiently large groups of agents with similar preferences deserve to be represented in the selected solution, or, more ambitiously, that each group should be allocated resources in proportion to its size. While we believe that this perspective should be taken into account at all stages of transportation planning, we showcase our approach by applying it to a specific and relatively simple task: choosing the locations of the stops for a fixed bus/train route. Specifically, we consider the setting where the trajectory of the vehicle has been exogenously determined, either by topography (e.g., a mountain road or a river) or by existing infrastructure (e.g., train tracks), the number of stops has been fixed in advance due to bounds on the overall travel time, but the designer still has the freedom to decide where to place the stops. Then, to use the public transport option, the user would have to travel to a nearby stop by using private transport (such as walking, cycling, or using an e-scooter), ride the bus towards their destination and then use private transport again for the last-mile travel. Alternatively, they can opt to use private transport for the entire trip; however, we assume that private transport has higher per-mile cost (measured as physical effort, travel time, or monetary cost) than public transport. Crucially, the agents’ decision whether to use the public transport at all is influenced by the location of the stops, so the planner’s choices made at this stage may have a dramatic effect on the demand for public transport: positioning the stops without taking into account the agents’ travel needs may render the system unusable and push the residents towards private transportation solutions. For readability, when describing the model, we talk about a bus and the agents walking to/from bus stops; however, we emphasise that our model is applicable to inter-urban transportation, such as train routes and long-distance buses (in which case the agents’ last-mile transportation solutions may involve cycling or riding a scooter rather than walking). 1.1 Our Contribution We put forward a stylized model where there is a bus route that travels the length of a given road, and there are n agents who may ride this bus. Each agent wants to travel between two terminal points located along this road; they can walk all the way, or take the bus (in which case they still need to walk to/from suitable stops). The planner has a budget to build a limited number of bus stops and is given a set of possible stop locations; they then decide which stops should be built. A solution, i.e., a set of bus stop locations, is evaluated according to two criteria. First, we measure it in terms of efficiency, defined by the total time the agents spend on traveling between their terminals. Second, we investigate to what extent a solution offers proportional representation to agent groups. We assume that each of the n agents is entitled to the 1/n fraction of the available budget. We then want to achieve outcomes that are group-fair, in the sense that there is no set of agents S such that all agents in S can withdraw their shares of the budget and then pool them to build a pair (resp., a set) of stops such that all agents in S prefer the outcome where only these stops are built to the current outcome; we say that solutions with this property provide justified representation (resp., lie in the core). Our first contribution is a dynamic program that can efficiently compute cost-minimal solutions. This approach is very flexible in that it still works when we add further features to the model, such as travel costs dependent on non-homogeneous road conditions or existing infrastructure. Moreover, while computing the minimum total cost becomes \NP-complete when bus stops have variable costs, our dynamic program still runs in pseudo-polynomial time with respect to the budget. In the second part of the paper, we focus on finding solutions that provide justified representation (JR) or are (approximately) in the core. Unfortunately, efficiency and justified representation turn out to be incompatible. However, we present a polynomial-time algorithm that operates by selecting bus stops at distances proportional to the density of terminal points, and show that this algorithm finds JR solutions whenever the cost of taking the bus is zero. In fact, this algorithm offers a 2-approximation to the stronger fairness concept of the core, and exhibits excellent empirical performance (on synthetic data). In contrast, there are instances for which no solution can provide strong JR. 1.2 Related Work Fairness considerations have a long-standing history in collective decision-making (see, e.g., Rawls, 1971; Sen, 2009). In the context of transportation, fairness is often concerned with justice in terms of equity. It is then measured in terms of, e.g., availability to monetarily disadvantaged population (Pucher, 1982), distribution of the impact on health caused by pollution (Forkenbrock and Schweitzer, 1999), or general access to key infrastructure (Pereira et al., 2017). Fairness in transportation has been studied in the operations research literature, but the existing work is limited to the operational phase of transportation. For instance, Jozefowiez et al. (2009) aim at fairly levelling road occupation to avoid congestion, while Matl et al. (2018) are concerned with balancing the workload among a fleet of vehicles that have to jointly cover a given set of trips. In contrast, our approach, i.e., modeling fairness in terms of proportionality, is rooted in the (computational) social choice literature (see e.g., Conitzer et al., 2017; Jiang et al., 2020; Peters and Skowron, 2020). Our model can be viewed as a special case of multi-winner voting, and our notion of justified representation is an adaptation of a similar concept in multi-winner approval voting (Aziz et al., 2017). It is also similar to the notion of proportional fairness in fair clustering (Chen et al., 2019; Micha and Shah, 2020; Aziz et al., 2024a). In this stream of literature, Li et al. (2021) study approximate core stability, where the approximation is with respect to the size of the deviating coalition (which is similar in spirit to our approach) or with respect to the gain by the deviating agents. Kalayci et al. (2024) consider a similar approximation in the context of multi-winner voting, and Chaudhury et al. (2022) explore similar ideas in the context of federated learning. We note that placing stops on the line is similar in spirit to facility location (Chan et al., 2021); however, our focus in this work is on fairness, whereas much of the facility location literature takes a mechanism design perspective (see, however, Zhou et al. (2022); Elkind et al. (2022)). Most related to our paper are models which investigate the same cost function (Fukui et al., 2020; Chan and Wang, 2023). In particular, the model by Chan and Wang (2023) is a special cost of our model where \alpha=0 (i.e., taking the bus has no cost), all agents have the same destination, and only two bus stops are built. However, our work differs in two key aspects: we allow for more than two stops to be build and study fairness aspects (rather than strategic manipulation). The facility location literature also considers agents that are interested in more than one location (Serafino and Ventre, 2014; Anastasiadis and Deligkas, 2018), but these works use different cost functions. A recent preprint by He et al. (2024) also considers fairness in the design of transportation networks, but differs from our work in two aspects. First, the authors model fairness via a welfarist approach, i.e., they consider a family of welfare measures that interpolate between egalitarian and utilitarian welfare. Second, in their model the input is captured by an undirected graph, and the planner’s task is to build a subset of edges of this graph. Thus, while the two papers share similar high-level motivation, their technical contributions do not overlap."
https://arxiv.org/html/2411.08367v1,Surprisingly Popular Voting for Concentric Rank-Order Models,"An important problem on social information sites is the recovery of ground truth from individual reports when the experts are in the minority. The wisdom of the crowd, i.e. the collective opinion of a group of individuals fails in such a scenario. However, the surprisingly popular (SP) algorithm [prelec2017solution] can recover the ground truth even when the experts are in the minority, by asking the individuals to report additional prediction reports–their beliefs about the reports of others. Several recent works have extended the surprisingly popular algorithm to an equivalent voting rule (SP-voting) to recover the ground truth ranking over a set of m alternatives. However, we are yet to fully understand when SP-voting can recover the ground truth ranking, and if so, how many samples (votes and predictions) it needs. We answer this question by proposing two rank-order models and analyzing the sample complexity of SP-voting under these models. In particular, we propose concentric mixtures of Mallows and Plackett-Luce models with G(\geq 2) groups. Our models generalize previously proposed concentric mixtures of Mallows models with 2 groups, and we highlight the importance of G>2 groups by identifying three distinct groups (expert, intermediate, and non-expert) from existing datasets. Next, we provide conditions on the parameters of the underlying models so that SP-voting can recover ground-truth rankings with high probability, and also derive sample complexities under the same. We complement the theoretical results by evaluating SP-voting on simulated and real datasets.","The recovery of ground truth from individual reports is one of the most vital aspects of social information sharing and online discourse. The wisdom of the crowds phenomenon refers to the observation that the collective value of a group of noisy individual opinions can be used to recover the ground truth [galton1949vox]. Such a collective value cancels out the biases of individual opinions when the number of participants is large and is often deployed to recover the ground truth on online polling and Q&A platforms (e.g. Reddit). However, when the experts are in the minority, approaches that rely on the collective opinion of a group of individuals fail to recover the ground truth. The Surprisingly Popular (SP) algorithm [prelec2017solution] is a promising technique capable of recovering the ground truth even when experts are in the minority. In addition to asking individuals’ opinion (aka vote), it asks them to predict how they believe the majority’s answer is (aka prediction). The SP algorithm then picks the outcome which is surprisingly popular i.e. whose actual frequency in the votes is greater than its average predicted frequency. It provably recovers the ground truth as the number of individuals grows, even with a minority of experts. This approach has been extended to voting rules, called SP-voting, in order to recover the ground truth rankings over a set of m alternatives. The naive application of SP-algorithm to voting requires that individuals submit their prediction as a distribution over m! possible permutation of alternatives, which implies that the amount of information elicited from each voter is exponential in m. Surprisingly, SP-voting has been shown to effectively recover the ground truth in practice even when predictions are limited to a set of size m, providing a substantial improvement over classical voting rules by focusing on eliciting the most likely top alternative or ranking [hosseini2021surprisingly]. Furthermore, SP-voting has been extended to partial ranks where the voters provide reports (votes and predictions) over subsets of size k with k\ll m [hosseini2024surprising]. While SP-voting has been shown to be effective in full or partial rankings, we are yet to fully understand when SP-voting can recover the ground truth ranking, and if so, how many samples (votes and predictions) it needs. To the best of our knowledge, this question is unexplored even for the basic SP algorithm. The main difficulty of analyzing such algorithms is that they are non-parametric i.e. they don’t make any assumptions about the underlying distribution of votes and predictions, and it’s not immediately clear what type of parametric models would be a good fit for real-world datasets and are also amenable to analysis under the surprisingly popular framework. For the setting of partial rankings, \citeauthor*hosseini2024surprising [hosseini2024surprising] performed a preliminary analysis of SP-voting under a mixture of Mallows model with two groups. However, we observe that the real datasets need more than two groups and more general rank-order models. Thus, we ask the following questions: What general rank-order models can explain ranking datasets (both votes and predictions) with a ground truth ranking? Furthermore, can we analyze SP-voting under such rank-order models, and determine its sample complexity, and conditions for identifying the ground truth ranking? 1.1 Our Contributions We propose various rank-order models with a ground truth ranking, and analyse the SP-voting rule under these models. In particular, our contributions are the following. • We propose two rank-order models, the Concentric Mixture of Mallows and the Concentric Mixture of Plackett-Luce, and generalize them to accommodate populations of G\geq 2 groups. • We derive the conditions required for the identification of ground truth ranking under the SP-voting and the proposed concentric rank-order models. The derived conditions highlight a tension between the fraction of different groups and the ""expertise"" (i.e. noise levels) of different groups. • To evaluate practical viability, we fit these models to real-world datasets for populations with G=2 and G=3 groups. When G=3, besides the expert and non-expert groups, we identify an intermediate group of voters of large fraction that explains the observed datasets better than prior approaches with two groups. • Furthermore, we generate synthetic data based on these models and provide empirical results on the sample complexity of SP-Voting, comparing it against the Copeland rule. Finally, experiments on real-world datasets show that SP-voting performs significantly better than the Copeland voting rule even when the dataset size is small. 1.2 Related Work The challenge of ground truth recovery using the wisdom of the crowd has been extensively explored in social choice theory \parencitegalton1949vox, de2014essai, surowiecki2005wisdom. Several vote aggregation rules \parencitede2014essai, borda1781m, copeland1951reasonable, young1977extending have been proposed based on this concept to aggregate voters’ preferences and recover the underlying ground truth. However, this approach falters when the majority of participants are misinformed \parencitesimmons2011intuitive, biased \parencitechen2004eliminating, or when expert opinions are underrepresented within the population \parenciteprelec2017solution. To address this limitation, \citeauthor*prelec2017solution [prelec2017solution] introduced the Surprisingly Popular (SP) algorithm, which requires voters to provide two types of information: their individual vote and their prediction of the consensus vote. This framework has since been used to incentivize truthful behaviour in agents [schoenebeck2021wisdom, schoenebeck2023two], mitigate biases in academic peer review [lu2024calibrating], elicit expert knowledge [kong2018eliciting], forecast geopolitical events [debmalya2020effectiveness], and aggregate information [chen2023wisdom]. However, \citeauthor*prelec2017solution [prelec2017solution]’s SP algorithm becomes impractical when the objective is to recover true ordinal ranking, since it necessitates information across all m! possible vote configurations. The surprisingly popular algorithm was extended to recover full rankings while reducing its complexity to \binom{m}{2} votes, making it more practical for smaller values of m \parencitehosseini2021surprisingly. Further extending this line of work, SP-Voting has been generalized to handle any number of alternatives, while also introducing mechanisms for partial preference elicitation to improve the efficiency of ground truth recovery \parencitehosseini2024surprising. However, it is still unclear under what conditions SP-Voting is effective for a large number of alternatives when eliciting rankings. Specifically, the structure of the voting population and whether their voting behavior can be mathematically modeled need to be studied in detail. The modeling of ranked data can be approached from two perspectives: modeling the population of voters and modeling the ranking process itself \parencitemarden1996analyzing. To date, the SP-Voting framework has been examined primarily by classifying voters into two distinct groups. Our work extends this analysis by generalizing it to account for any number of groups, denoted as G. In terms of modeling the ranking process, several probabilistic models have been developed to represent voter preference generation. These include Order Statistic models, such as the Thurstonian model \parencitethurstone2017law; Pairwise Comparison models, like the Bradley-Terry model \parencitebradley1952rank; Multistage models, such as the Plackett-Luce model \parenciteluce1959possible, plackett1954reduction; and Distance-based models, like the Mallows’ model \parencitemallows1957non, among others. \citeauthor*marden1996analyzing [marden1996analyzing] provides a more comprehensive review of these models. The SP-Voting framework was recently studied under the assumption that voters’ preferences are drawn from an underlying probability distribution known as the Concentric Mixture of Mallows model, a variant of Mallows’ model \parencitehosseini2024surprising. In this work, we extend the SP-Voting framework by investigating two different vote distribution assumptions: the distance-based Mallows’ model and the multistage Plackett-Luce model. Specifically, we build on prior work by extending the Mallows’ model to account for G groups, allowing for a more general analysis of voter populations. Additionally, we propose a novel Concentric Plackett-Luce Mixture model, a variant of the multistage Plackett-Luce model, which similarly incorporates G groups."
https://arxiv.org/html/2411.08450v1,"DecentPeeR: A Self-Incentivised & Inclusive 
Decentralized Peer Review System","Peer review, as a widely used practice to ensure the quality and integrity of publications, lacks a well-defined and common mechanism to self-incentivize virtuous behavior across all the conferences and journals. This is because information about reviewer efforts and author feedback typically remains local to a single venue, while the same group of authors and reviewers participate in the publication process across many venues. Previous attempts to incentivize the reviewing process assume that the quality of reviews and papers authored correlate for the same person, or they assume that the reviewers can receive physical rewards for their work. In this paper, we aim to keep track of reviewing and authoring efforts by users (who review and author) across different venues while ensuring self-incentivization.To this end, we introduce DecentPeeR, a system that captures the interactions of users who use a peer review system as decentralized reputation scores. We show that our system incentivizes reviewers to behave according to the rules, i.e., it has a unique Nash equilibrium in which virtuous behavior is rewarded. Furthermore, we detail how our design ensures inclusivity, i.e., giving everyone a fair chance to publish, especially when facing dishonest users. We also report on empirical results that show the incentive mechanism works: dishonest individual and group behavior are penalized, but it is possible to recover from a poor score over time.","Peer review systems are widely used and have an extensive impact in today’s academia. Use cases of peer-review ranges from the scientific publication process [1, 2] to open source software development [3, 4]. With the popularity of peer review systems, various methods have been proposed to make the peer review procedure more inclusive. With inclusivity, authors have a chance to publish their work solely based on its quality. Ensuring inclusivity is challenging in the academic peer review process: submissions on different research topics may not be comparable; reviewers may have personal opinions depending on the topic of the submission; due to large amounts of published papers, evaluations from only few reviewers can be used decide on the quality of a submission. Previous efforts to ensure inclusivity range from enforcing prior-announcement of conflict-of-interest [5, 6], double-blindness [7, 8], and actions from the editor to promote quality, integrity, and fairness [9]. Most traditional solutions are focused on how to make a single conference more inclusive. However, authors and reviewers likely take part at multiple venues over their careers. Thus, a cross-venue measure would be more viable today, given the advancements in decentralized technologies. In this work, we keep track of the actions of users over time using a reputation system to ensure inclusivity. We build a decentralized system where users tend to follow the rules of the system based on their best interests. While incentivizing users who behave rationally, the system should not punish academic work of good quality and thus violate inclusivity. To this end, we develop a self-incentivized system based on a game theoretical approach, showing that achieving the unique Nash equilibrium is only possible by adhering to the rules of the system. We detail our system, DecentPeeR, from the perspective of an academic who wants to contribute to or organize a conference. In doing so, we also benefit from the decentralized storage mechanisms provided by blockchain technology [10, 11] that is used to keep a history of peer review systems’ data across different venues. We detail our system, DecentPeeR, from the perspective of an academic who wants to contribute to or organize a conference. In doing so, we also benefit from the decentralized mechanisms provided by blockchain technology (e.g., to keep a history of peer review systems’ data across different venues). I-A Design Goals Our aim is to design our peer-review system that satisfies the following main goals: Self-incentivization. Typically, peer review systems assume that participants are well-behaved [12], that reviewers can be assigned a reliability score [13], or that at most a small percentage of reviewers is biased [14, 15]. However, these rules might be neglected, resulting to the raise of adversarial reviews [16]. As a remedy, we aim to ensure that it is in the reviewer’s own best interest to respect rules of the system, i.e., through a provable self-incentivization. Inclusivity. A reputation system should be flexible and adaptive and in particular also support new users of the system. Our goal is to design a system where everyone would have a chance to contribute good quality work, mostly independent of their scores, and where users with bad scores have a chance to recover from a failure. Cross-venue evaluation. Academic users do not encounter peer-review systems only once: they participate in multiple venues (conferences, journals, workshops, etc.) throughout several years. Hence we aim to design a system that benefits from this fact and also aggregates data over time. I-B Contribution In this paper, we design a self-incentivized and inclusive peer review system, DecentPeeR, that works across venues and tracks. Honest reviewing behavior is rewarded with a positive influence on future borderline-scored submissions by the authors. In addition, the reviewing score provides committee chairs with a criterion to select the committee. We present a peer review game in this paper and show that it has a unique Nash equilibrium where the users play honestly. We then analyze the desired properties of the peer review game, such as inclusivity and cross-venue evaluation. Our method incorporates three mechanisms that lead to a high level of inclusivity: • Our system only considers the reputation score in borderline cases: if the high quality of a paper is already agreed upon, we consider that paper as accepted. • Our system uses a randomness mechanism to form a program committee and a reviewing team: the weighted randomness ensures that selected users can be trusted while giving chance to every user to participate. • A reputation score function has been implemented with the goal of ensuring fast recovery for users who have limited misbehavior. We show that the cross-venue aspect allows us to quickly detect adversarial behavior by reviewers. Once an adversarial reviewer however behaves correctly again, the corresponding score of the reviewer recovers. We conclude by showing that a majority attack, where adversarial reviewers collaborate in order to evaluate the paper dishonestly, is unlikely under the uniformly chosen reviewers in the review assignment. Our paper is organized as follows: we follow by relating our paper to previous works. We then detail DecentPeeR design in §II and analyze it in §III. Finally, we go over a few case studies in §IV and conclude our work in §V. I-C Related Work Peer review is a broad research topic that has been investigated from many aspects over the past years. For example, empirical studies have been conducted on peer review for classroom use [17, 18], for conference reviews [19, 20], and for funding applications [21]. In the following, we discuss different perspectives on peer review systems, coming from theoretical research as well as practical systems. Blockchain-based peer review. Our peer review system can be implemented on a blockchain-based system that supports smart contracts, like Ethereum. The decentralized nature of blockchain-based protocols has previously motivated many researchers to utilize its advantages for new designs of peer review systems. Our work is also a building block towards a Decentralized Science (DeSci) future [22]. Initial proposals for an alternative blockchain-based peer review systems [23, 24] focus on providing an alternative coin instead of Bitcoin. These attempts to alter a financial system for peer review, however, are inherently flawed: wealthy participants can game the system to their benefit. Another set of blockchain-based peer review systems focused on providing a decentralized platform to store information exchanged during a peer review process [25, 10], mostly leveraging the advantages provided by IPFS. Although it is possible to use the proposed peer review systems across venues, the systems lack the essential requirements that ensure fair treatment of all users when being used across venues. More recent systems [26, 27] aimed to tackle the challenge of a collaborative system. In doing so, they showed what are possible ways to provide self-invitation based on a game-theoretic perspective. In a nutshell, they showed that the allowed rules of the game are in the best interest of all users. However, they did not show what happens when a failure happens and how the system could recover. Furthermore, they did not consider the fact that not only a single venue exists, and a system should not be restarted whenever a new request for a venue appears. We summarize the main properties of the presented distributed peer review systems in Table I. System Cross-venue Self-incentivising Inclusive PubChain [24] ✓ ✗ ✗ Blockchain and Kudos [23] ✓ ✗ ✗ IPFS-based [25] ✓ ✗ ✗ Data Marketplaces [26] ✗ ✓ ✗ Collaborative Research [27] ✗ ✓ ✗ DecentPeeR (our work) ✓ ✓ ✓ TABLE I: Comparing DecentPeeR with previous decentralized peer review systems. Social choice perspective on peer review. In social choice studies, peer review has been investigated as an assessment method for grading homework and exams[28], programming classes[29], and conferences[30]. Some of these works focus on finding the right aggregation rules, by investigating cardinal voting rules instead of ordinal grading [31, 32] or by showing specific properties of peer review protocols, such as strategyproofness [30]. Other papers focus on determining truthful reviewers by assuming that the grading is performed with assistance [33]. There are also studies on reputation-based peer grading. Leaning on PageRank, Walsh proposed a PeerRank system [34]. In this system, each reviewer gets a reputation score, which is computed by the grades given by the reviewer weighted by the reputation score of the reviewer. Note that a reputation score calculation is connected to the grade of the submitted work, which is not necessarily true in conference reviewing. Reputation systems. Reputation systems are one of the key tools to establish trust in an untrustworthy environment [35]. For example, they have an established place in designing distributed systems, especially in connecting peers in a peer-to-peer network [36]. They have also been considered in other applications such as e-commerce [37] and transportation [38]. Most of the previous work that considers a reputation system on blockchain [39, 40] uses the reputation score as an alternative mining protocol, which is orthogonal to how we used it in our system. Similarity detection mechanisms. The digital age has made it easier to reuse the efforts of others, and hence, from early on, it was critical to create similarity detection mechanisms [41]. Also, for the peer review process similarity detection is inevitable, as it enables editors to verify the integrity of a research work. With the rise of generative AI tools, advanced similarity detection mechanisms received even more attention, especially because it became possible to rehash ideas of others into undetectable results (even by trained eyes) [42]. To detect such behavior, context-oblivious methods have been proposed, such as fingerprinting [43], text matching [44], or compression [45]. The latter one can run fast but might have low accuracy due to false negatives. In general, context-aware similarity detection approaches allow us to reduce false negatives, but they require much more computational power. As an alternative, latent semantic analysis [46] or transformer-based tools [47] have been considered in the literature. In this work, we do not focus on a particular similarity detection mechanism, but rather assume that such methods exist and can be deployed by the system designer."
https://arxiv.org/html/2411.07488v1,Selling an Item through Persuasion,"A monopolistic seller aims to sell an indivisible item to multiple potential buyers. Each buyer’s valuation depends on their private type and the item’s quality. The seller can observe the quality but it is unknown to buyers. This quality information is valuable to buyers, so it is beneficial for the seller to strategically design experiments that reveal information about the quality before deciding to sell the item to whom and at what price.We study the problem of designing a revenue-maximizing mechanism that allows the seller to disclose information and sell the item. First, we recast the revelation principle to our setting, showing that the seller can focus on one-round mechanisms without loss of generality. We then formulate the mechanism design problem as an optimization problem and derive the optimal solution in closed form. The optimal mechanism includes a set of experiments and payment functions. After eliciting buyers’ types, the optimal mechanism asks a buyer to buy and sets a price accordingly. The optimal information structure involves partitioning the quality space. Additionally, we show that our results can be extended to a broader class of distributions and valuation functions.","1 Introduction Optimal auction design stands as a critical issue in the field of economics and game theory, garnering sustained attention over the years (Myerson, , 1981; Armstrong, , 2000; Shi, , 2012; Papadimitriou and Pierrakos, , 2011; Deng et al., , 2014). The most renowned contribution in this area is the seminal work by Myerson, (1981). Most subsequent work has adhered to the standard assumption that buyers hold private information, and the seller aims to design a mechanism with the property of incentive compatibility to elicit buyers’ true information. However, in reality, the seller often possesses private information about the item for sale. For instance, the seller knows the true quality of the item, while buyers can only ascertain that after purchasing and using the item for some time. This scenario motivates us to study the mechanism design problem when both parties have private information. In such a context, if the seller aims to maximize revenue, the mechanism’s outcome (e.g., the winner and the payment) may need to be based on the seller’s private information. Consequently, the outcome itself might partially disclose the seller’s private information. As rational players, buyers will update their valuation based on this information if their valuation depends on the seller’s private information (such as the item’s quality). This leads to a potential situation where buyers might have an incentive to disobey the allocation. To prevent such adverse effects, the seller must carefully handle how the private information is disclosed when deciding the mechanism outcome. Such information asymmetry is ubiquitous in the literature and has attracted significant research attention (Akerlof, , 1978; Bergemann et al., , 2015; Schottmüller, , 2023). There is a line of works investigating how an information owner can strategically disclose information to benefit themselves, also known as ‘‘information design’’ or ‘‘Bayesian persuasion’’ (Rayo and Segal, , 2010; Kamenica and Gentzkow, , 2011; Alonso and Câmara, , 2016; Bergemann and Morris, , 2016). Additionally, some researchers explore how a party can leverage their information advantage to make profits, referred to as ‘‘selling information’’ (Babaioff et al., , 2012; Hörner and Skrzypacz, , 2016; Bergemann et al., 2022a, ). In most existing works, the information designer or seller is typically a third party rather than the item seller. To the best of our knowledge, there is limited understanding of how to design optimal mechanisms for a seller who possesses both private information and the item for sale. To fill this gap, we study the optimal mechanism design problem for an item seller who possesses an information advantage over buyers regarding the item’s quality. Inspired by the information design literature, we adopt the ‘‘Bayesian persuasion’’ framework (Kamenica and Gentzkow, , 2011) to model the way of revealing information. 1.1 Our Results In this paper, we begin by defining a general mechanism space that contains all possible mechanisms the seller can use. We then demonstrate that it is without loss of generality to focus on a much smaller subspace, where each mechanism involves only a single round of communication between the seller and the buyers. Based on this result, we formulate the optimal mechanism design problem as an optimization problem. Furthermore, we fully characterize the optimal mechanism and derive a closed-form solution to this problem. In the optimal mechanism, the seller asks the buyer with the highest ‘‘virtual value’’ (Myerson, , 1981) to buy the item, or refrain from selling if the highest virtual value does not exceed the seller’s threshold. While we allow for randomized information disclosure, the seller consistently sends deterministic messages to the buyers. The information structure features a partition of the quality space, i.e., the seller informs the buyer whether the true quality falls within a specific subset. Although our mechanism shares similarities with the Myerson auction, it is derived by considering how information disclosure impacts buyers’ valuations. We show that the Myerson auction can be seen as a special case of ours, where the quality remains constant. Finally, we show that our results can be easily extended to a more general setting. 1.2 Related work Our research is grounded in the literature on information design. We adopt the ‘‘Bayesian persuasion’’ framework, proposed by the seminal work (Kamenica and Gentzkow, , 2011), to model how the seller designs information. Most subsequent works assume that only one party has private information. The most relevant work to ours in this line of work is the persuasion model with a privately informed receiver (Kolotilin et al., , 2017). However, there is a substantial distinction between persuasion and selling an item through persuasion. In our context, the seller has only one item for sale, and not every buyer can buy it if they want to. Thus the results are not comparable. Guo and Shmaya, (2019) also studied a similar setting, showing that the optimal information structure has a nested interval structure. In contrast, we show that the information interval in our optimal mechanism can also exhibit similar structures. Our work aligns with the research on mechanism design with information revelation. Eső and Szentes, (2007) study a setting very similar to ours, except that they assume that the seller cannot observe the quality and has no reserve price. Despite these similarities, their results are quite different from ours. They show that revealing full information is optimal, whereas our information structure features a partition of the quality space. Wei and Green, (2022) study a single buyer setting and derive a closed form solution under the MHR condition. In contrast, we consider a multi-buyer setting and show that our results can be extended to a more general class of distributions. Additionally, there is a series of works focusing on information disclosure under specific auction formats, such as second-price auction (Bergemann et al., 2022b, ) and posted price auction (Castiglioni et al., , 2022). Unlike these studies, we need to jointly design both the information structure and the selling mechanism. Another related yet significantly different problem is the selling information (Liu et al., , 2021; Bergemann et al., , 2018; Chen et al., , 2020). In their setting, the seller sells information before the buyer takes action and can therefore charge an upfront fee. In contrast, we consider the setting where the seller reveals information and charges the payment for the item itself, resulting in significantly different analyses. Additionally, unlike the selling of information, an indivisible item can only be sold once."
https://arxiv.org/html/2411.07354v1,Strategyproof Learning with Advice,"An important challenge in robust machine learning is when training data is provided by strategic sources who may intentionally report erroneous data for their own benefit. A line of work at the intersection of machine learning and mechanism design aims to deter strategic agents from reporting erroneous training data by designing learning algorithms that are strategyproof. Strategyproofness is a strong and desirable property, but it comes at a cost in the approximation ratio of even simple risk minimization problems.In this paper, we study strategyproof regression and classification problems in a model with advice. This model is part of a recent line on mechanism design with advice where the goal is to achieve both an improved approximation ratio when the advice is correct (consistency) and a bounded approximation ratio when the advice is incorrect (robustness). We provide the first non-trivial consistency-robustness tradeoffs for strategyproof regression and classification, which hold for simple yet interesting classes of functions. For classes of constant functions, we give a deterministic and strategyproof mechanism that is, for any \gamma\in(0,2], 1+\gamma consistent and 1+4/\gamma robust and provide a lower bound that shows that this tradeoff is optimal. We extend this mechanism and its guarantees to homogeneous linear regression over \mathbb{R}. In the binary classification problem of selecting from three or more labelings, we present strong impossibility results for both deterministic and randomized mechanism. Finally, we provide deterministic and randomized mechanisms for selecting from two labelings.","In a growing number of machine learning applications, training data is provided by agents who have preferences over the output of the machine learning system. A commonly used example is retail distribution, where regression is employed to optimize inventory shipping. For large retailers such as Zara, part of the training data is provided by store managers who report their predicted demand for items [10, 11]. Since managers have financial incentives to increase sales in their own stores, they may strategically manipulate the predicted demand that is reported to the central warehouses. In particular, there is evidence that this manipulation has occurred when there is a limited supply of a high-selling item [11]. The potential errors in the training data due to such strategic manipulation are, of course, an issue for the robustness of machine learning models. In order to avoid such manipulations, a line of work at the intersection of machine learning and mechanism design has developed learning algorithms that are robust to strategic data sources [15, 30, 12, 14, 25, 26, 27, 29, 21, 18, 16, 2]. These algorithms, also called mechanisms, deter strategic data manipulations by satisfying a strategyproofness property that guarantees that these agents cannot benefit from reporting erroneous data, regardless of the reports of the other agents. Strategyproofness is a strong and desirable property, but even for simple risk minimization problems, it comes at a cost in the best achievable approximation ratio. The existing work on strategyproof regression and strategyproof classification assumes that the learning mechanism has no information about the agents’ data besides what these agents report. This assumption is often pessimistic. In the retail distribution example, it is reasonable to assume that central warehouses have access to, in addition to the future demands reported by store managers, rough estimates of what these future demands should be. A recent line of work on mechanism design with advice has shown that such side information can be leveraged to overcome worst-case bounds in mechanism design. Strategyproof mechanisms that achieve an improved approximation ratio when the advice is accurate (consistency) and an acceptable approximation ratio when the advice is inaccurate (robustness) have been designed for problems such as strategic facility location [1, 34, 4], auction design [23, 9, 5], strategic scheduling [3], strategic assignment [13], and metric distortion [8]. In this paper, we aim to leverage side advice to overcome pessimistic worst-case impossibility results in strategyproof learning. The model. In strategic learning, each agent i\in\{1,\ldots,n\} reports a set S_{i}=\{(x_{i,j},y_{i,j})\}_{j} of labeled data points to the learner. The points x_{i,j} are public information, but the labels y_{i,j} are private information that agent i can potentially misreport. The goal of the mechanism is to learn a function f from a function class \mathcal{F} that minimizes the global risk R(f,S)=\frac{1}{|S|}\sum_{i=1}^{n}\sum_{j=1}^{|S_{i}|}\ell(f(x_{i,j}),y_{i,j})"
https://arxiv.org/html/2411.07267v1,A Survey on Data Markets,"Data is the new oil of the 21st century. The growing trend of trading data for greater welfare has led to the emergence of data markets. A data market is any mechanism whereby the exchange of data products including datasets and data derivatives takes place as a result of data buyers and data sellers being in contact with one another, either directly or through mediating agents. It serves as a coordinating mechanism by which several functions, including the pricing and the distribution of data as the most important ones, interact to make the value of data fully exploited and enhanced. In this article, we present a comprehensive survey of this important and emerging direction from the aspects of data search, data productization, data transaction, data pricing, revenue allocation as well as privacy, security, and trust issues. We also investigate the government policies and industry status of data markets across different countries and different domains. Finally, we identify the unresolved challenges and discuss possible future directions for the development of data markets.","Data is considered an invaluable resource in the digital economy. The last decades have witnessed the explosive growth of data. As raw material for acquiring knowledge and developing products, data generates value in an indirect way. After remodeling the commercial perspective of data, data is directly monetized like other material commodities nowadays. Individuals and organizations extensively trade datasets and derived data products. In this new vision, data is no longer the enabler of products, but also the product itself. Governments around the world are seizing this new opportunity. For example, the Chinese government unveiled a guideline to improve the market-based allocation of data factors, which is the first to list data as a production factor following land, labor, capital, and entrepreneurship (China, 2020). The United States created the Federal Data Strategy Action Plan aimed at leveraging data as a strategic asset (States, 2019). Driven by the tides of data monetization, data markets have emerged. Data markets, a nascent interdiscipline of computer science and economics, are growing rapidly and evolving in myriad research directions. The history of data markets can be traced back to 1986. A seminal work by Admati and Pfleiderer (1986) studies a market where traders purchase information from a monopolistic seller. The information they trade is the data endowed or produced by individual agents. To the best of our knowledge, the term “data market” is put forward by Keenan (2008) in 2008 for the first time in the literature. They propose to exchange spatial data collected by geographic information systems in the market. In 2011, Balazinska et al. (2011) present a vision of a more general data market where commodities are derivative data products. They outline key challenges in a relational cloud data market for the database research community. Since then, data markets have experienced rapid development. Koutris et al. (2012) design the first query-based data market; Deep and Koutris (2017) propose a scalable and flexible pricing framework for relational queries; Agarwal et al. (2019) design the first two-sided marketplace for trading training data directly; Chen et al. (2019a) introduce the first model-based data market; and more recently Liu et al. (2021b) propose the first end-to-end (model-based) data market involving the interactions among sellers, brokers, and buyers. With the growing demand for data transactions, many data marketplaces have sprung up, such as AWS Data Exchange (AWS, [n. d.]), Dawex (DAWEX, [n. d.]), BDEX (BDE, [n. d.]), Factual (Fac, [n. d.]), and Snowflake (Sno, [n. d.]). Data marketplaces are online transaction locations or exchanges that facilitate the buying and selling of data products. They are authorized to host data products and conduct data transactions for the benefit of stakeholders. We propose a definition of the data market in this survey as follows. A data market is any mechanism whereby the exchange of data products including datasets and data derivatives (such as query results and trained models) takes place as a result of data buyers and data sellers being in contact with one another, either directly or through mediating agents. The data market serves as a coordinating mechanism by which several functions, including the pricing and the distribution of data as the most important ones, interact to make the value of data fully exploited and enhanced. In data markets, the life chain of data covers the process of data search, productization, monetization in pricing and transaction, and finally destruction. Trading data products naturally raises privacy, security, and trust concerns, and faces regulatory barriers to achieving compliance and traceability. Whether in academia or industry, there are rich explorations on designing data markets, where different data markets vary from each other in terms of data products, underlying functions, and market mechanisms. In this article, we present a comprehensive survey of this important and emerging direction from the aspects of data search, data productization, data transaction, data pricing, revenue allocation as well as privacy, security, and trust issues. We also investigate the government policies and industry status of data markets across different countries and different domains. Finally, we identify the unresolved challenges and discuss possible future directions for the development of data markets. 1.1. Related Surveys The existing surveys on data markets can be generally categorized based on the scope: (1) surveys on academic research (Thomas and Leiponen, 2016; Driessen et al., 2022; Liang et al., 2018; Abbas et al., 2021), (2) surveys on industry status (Schomm et al., 2013; Li et al., 2018a; Kennedy et al., [n. d.]; Azcoitia and Laoutaris, 2022), and (3) surveys on data pricing (Muschalle et al., 2012; Fricker and Maksimov, 2017; Zhang and Beltrán, 2020; Pei, 2022; Cong et al., 2022; Zhang et al., 2023a; Miao et al., 2023; Chi et al., 2023). Surveys on academic research. Efforts (Thomas and Leiponen, 2016; Abbas et al., 2021; Driessen et al., 2022; Liang et al., 2018) have been made to survey academic research for data markets within the whole lifecycle. Thomas and Leiponen (2016) provide managers with a literature review on the commercialization of big data. From a managerial and commercial perspective, they introduce six business models in the data ecosystem, led by data suppliers, data managers, data custodians, application developers, service providers, and data aggregators. Based on this taxonomy, they discuss the characteristics of the data ecosystem and conclude with the challenges faced by managers and corresponding guidelines in trading big data including pricing and privacy concerns which we reinforce in this paper. Abbas et al. (2021) examine 133 academic articles using a Service-Technology-Organization-Finance (STOF) model. They find that the existing literature on data marketplaces is primarily dominated by technology studies. Driessen et al. (2022) present a statistical analysis of works related to data markets up until 2021, discuss practical application areas for data markets, categorize the problems of designing data markets, and find corresponding solutions in the literature. Liang et al. (2018) use 4V (Volume, Velocity, Variety, and Value) to define big data and survey the lifecycle of trading big data, including data pricing, data trading, and data protection, for each of which they review corresponding issues and models. The above works (Thomas and Leiponen, 2016; Abbas et al., 2021; Driessen et al., 2022; Liang et al., 2018) mainly focus on techniques for trading data, while our survey covers state-of-the-art literature for trading general data products, including raw data and its derivatives such as queries, statistical inferences, and machine learning models. Moreover, our survey comprehensively covers key issues in main procedures in data markets from data search to data destruction. Surveys on industry status. There are four works (Schomm et al., 2013; Li et al., 2018a; Kennedy et al., [n. d.]; Azcoitia and Laoutaris, 2022) conducting industry surveys on data marketplaces. Schomm et al. (2013) present an initial survey of data marketplaces and data vendors by investigating 46 data suppliers from twelve dimensions (type, time frame, domain, data origin, pricing model, data access, data output, language, target audience, trustworthiness, size of vendor, and maturity) up until Summer 2012. Li et al. (2018a) introduce policies of China for developing data markets and discuss concerns and research opportunities including preprocessing, pricing, security, privacy, and verifiability. Azcoitia and Laoutaris (2022) investigate 180 entities which trade data on the Internet, summarize different business models, and discuss open challenges. Kennedy et al. ([n. d.]) introduce different types of data marketplaces and describe data transaction lifecycle from the perspective of buyers and sellers. They also interview buyers and sellers to understand the current status and challenges of online data marketplaces in 2022. The above works (Schomm et al., 2013; Li et al., 2018a; Kennedy et al., [n. d.]; Azcoitia and Laoutaris, 2022) provides an understanding of data marketplaces through practical investigations of entities and marketplaces. In contrast, our survey not only examines mainstream data marketplaces worldwide but also provides a list of government policies. Surveys on data pricing. There have been several surveys (Muschalle et al., 2012; Fricker and Maksimov, 2017; Zhang and Beltrán, 2020; Pei, 2022; Cong et al., 2022; Zhang et al., 2023a; Miao et al., 2023) specializing in data pricing, a subtopic that receives the most attention in data markets. Muschalle et al. (2012) investigate seven established vendors for their potential market situations, pricing approaches, and trends. Fricker and Maksimov (2017) report a literature survey of 18 papers regarding several research questions, including the maturity and targets of pricing models, types of data products, and pricing mechanisms. Zhang and Beltrán (2020) review novel data pricing studies and categorize data pricing methods based on data granularity and privacy. Pei (2022) starts with the economics of data pricing and reviews pricing models based on a set of fundamental principles. He also discusses the differences between digital products and data products, and the corresponding pricing methods. Very recently, Cong et al. (2022) survey data pricing methods in machine learning pipelines, including pricing raw data sets, pricing data labels, and pricing in collaborative machine learning models. Zhang et al. (2023a) categorize and review pricing methods for queries from the aspects of market structure, privacy notion, query type, and pricing method. Miao et al. (2023) classify data pricing techniques into three strategies and analyze thirteen pricing models. Chi et al. (2023) outline the fundamental concepts of data pricing, categorize data pricing strategies into query-based and privacy-based approaches, and offer an overview of data pricing from a data science standpoint. In addition to covering other data market functions, our survey includes a comprehensive analysis of data pricing that examines both revenue allocation for allocating compensations to data sellers and data product pricing for pricing data products to data buyers and their interactions. Furthermore, it systematically reviews emerging game-theoretic approaches to data pricing for the first time. In summary, while existing surveys approach data markets from either academic or industry perspective, our survey provides a comprehensive and general review of data markets covering both academic research and industry status including government policies across representative countries and domains. We also discuss the differences between data and other production factors and the corresponding impact on the design of data markets. While existing surveys investigate a few significant challenges in data markets, we study the interaction between key entities, summarize important desiderata for designing a well-functioning data market, and review techniques regarding data search, productization approaches, pricing mechanisms, data transactions, privacy concerns, etc, based on a formal framework as in Figure 2. Contributions. We present a comprehensive survey of data markets in both academia and industry. The purpose of this survey is to delve into subtopics of data markets in terms of computer science while covering mechanisms, regulations, and challenges in economics, law, and governance. The main contributions of this survey are summarized as follows. • Identify the unique properties of data and discuss the difference between data markets and other markets for the four production factors (land, labor, capital, and entrepreneurship). • Introduce the framework of data markets, formalize the abilities and restrictions of key roles, and illustrate the main procedures in the operations of data markets. • Present important desiderata for well-functioning data markets. • Summarize various methods of data search for various purposes, including crowdsourced dataset collection, dataset discovery in databases, data discovery in machine learning, and general dataset search. • Introduce various approaches to data productization based on versioning and data market categories. • Outline advertising strategies for data sellers and data purchase methods for data buyers in data transactions. • Review different approaches for revenue allocation and data product pricing, along with game-theoretic pricing methods. • Describe possible attacks on privacy preservation, fairness, profitability, and traceability from dishonest entities and corresponding solutions. • Present guidelines and regulations and investigate actual data marketplaces in representative countries and domains. • Discuss various open challenges and emerging directions for future research. Figure 1. The structure of the survey. 1.2. Structure of The Survey Figure 1 shows the structure of the survey. Section 2 first presents a diagram that shows different problems affecting data markets and their relationship. Section 3 summarizes important desiderata for building well-functioning data markets. Section 4 reviews methods of data search including crowdsourced dataset collection, dataset discovery in databases, dataset discovery in machine learning, and general dataset search. Section 5 describes techniques for data productization. Section 6 overviews strategies of both data buyers and data sellers in data transactions. Section 7 reviews studies investigating data product pricing and game-theoretic pricing. Section 8 reviews studies investigating revenue allocation. Section 9 deals with issues related to dishonest participants in untrusted data markets. Section 10 provides government policies and industry status in representative countries and regions. Section 11 draws a conclusion and discusses open challenges and opportunities for future work."
https://arxiv.org/html/2411.08026v1,Incentive Design with Spillovers,"A principal uses payments conditioned on stochastic outcomes of a team project to elicit costly effort from the team members. We develop a multi-agent generalization of a classic first-order approach to contract optimization by leveraging methods from network games. The main results characterize the optimal allocation of incentive pay across agents and outcomes. Incentive optimality requires equalizing, across agents, a product of (i) individual productivity (ii) organizational centrality and (iii) responsiveness to monetary incentives.","A popular method of motivating members of a team is giving them performance incentives that depend on jointly achieved outcomes. Examples of such incentives include startup executives receiving firm stock and a marketing team receiving bonuses for achieving a sales target. How should such incentive schemes be designed and how should they take into account the team’s production function? We examine these questions in a simple non-parametric model of a team working on a joint project. Each member chooses how much costly effort to exert. These actions jointly determine a real-valued team performance—for example, the quality of a product—according to a nice, increasing function of the efforts, which may entail interactions such as complementarities among agents’ efforts. Any performance level determines a probability distribution over observable project outcomes. For example, the outcome may be the revenue from a project, which is stochastically increasing in non-contractible project quality. The uncertainty reflects factors outside the team’s control, such as competing product releases. Although it is not possible to write contracts contingent on individual actions or the team performance, the principal can commit to a contract specifying nonnegative payments to each agent contingent on each possible project outcome. The principal’s goal is to design this contract in a way that maximizes profit: revenue minus compensation. The setting builds on the classic Holmström (1979) model, in which a single agent produces work of a non-contractible quality resulting in an observable outcome.111In particular, the obstacles to perfect contracting are the same: moral hazard and limited liability. In our setting, the analogue of quality is a jointly achieved performance. How incentive spillovers across agents matter for contract design—a central issue for modern firms—is not well understood, despite the immense amount learned about contract design since Holmström’s work. In this paper, we make progress on this problem by leveraging some ideas from network theory. To illustrate the basic importance of incentive spillovers, imagine that the principal slightly adjusts the contract of a particular agent, Bob, in a way that motivates him to work harder. In team production, changing one team member’s action can change other agents’ private returns to effort, holding fixed their own contracts. Those whose efforts are complements to Bob’s are now motivated to work harder, while those whose actions are substitutes have incentives to free-ride on his higher effort. This interconnectedness plays a pivotal role in the optimal allocation of incentive pay. In view of what we have just said, optimal contracts cannot be based only on the isolated contributions of agents’ efforts to team performance. Contract design must also take into account agents’ organizational positions—how their effort shapes other agents’ responses to their own contracts. Our contribution is a characterization of optimal contracts in the presence of incentive spillovers. The characterization is stated in terms of three kinds of quantities that can be associated to each agent at any contract. The first quantity, an agent’s marginal productivity, is the partial derivative of team performance in an individual’s action, holding others’ actions fixed. The second quantity is called an agent’s centrality: a measure of connectedness222Operationalized as a weighted walk count, i.e., a Katz–Bonacich centrality. in a network reflecting incentive spillovers, with connectedness to more productive agents weighted more. The relevance of these quantities for contract optimality should be intuitive in view of what we have said. The third quantity is an agent’s marginal utility of money: it accounts for the fact that an agent who has a low valuation of an additional dollar is, all else equal, a less responsive and less appealing recipient of incentive pay. Our main result is that, when the binding incentive constraints are local, optimal contracts satisfy a balance condition: the product of the three quantities described above is equal across all agents receiving any incentive pay. It turns out that the balance condition is necessary if the principal does not want to shift compensation across agents in any state—something that must hold at any optimum. The condition is interpretable, identifying the quantities that must be measured to assess the optimality of a contract. If the optimality condition is not satisfied, our results yield guidance for modifying a suboptimal incentive scheme to a better one. Indeed, a key underlying technical result computes the marginal benefit to the principal of increasing incentive pay to an agent in any contract (optimal or not). This marginal benefit turns out to be proportional to the product of the three terms. It is worth emphasizing that our general model makes no parametric assumptions and thus allows quite flexible production functions for the team. To take just one example, the team’s production function could be an arbitrary polynomial, with monomials of arbitrary degree reflecting outputs generated through the joint efforts of arbitrary subsets of the team—e.g., complementing one another in threes, fours, and so on. Nevertheless, the team’s production function matters only through its Hessian at the optimal contract—effectively a measure of biliateral spillovers (complementarities/substitutabilities) between agents. This allows us to leverage some methods from well-understood network games whose payoffs are quadratic polynomials to analyze the spillover effects of locally perturbing incentive pay under arbitrary contracts. We use this key reduction to characterize the first-order conditions that determine the principal’s optimal allocation of incentives both across agents and across outcomes. The first half of the paper presents the general result on the balance condition and explains the reasoning behind it. This result entails a variety of implications under various conditions, and the second half of the paper is devoted to exploring these. We start with an intuitive one. Agents with a higher product of productivity and centrality must have a lower marginal utility of money. If all agents have the same utility functions of money and these functions are concave, then those with the higher “productivity times centrality” index must be paid more in every outcome—a simple but novel rule of thumb. The three factors in the balance characterization depend on the contract, so a natural next step is to give explicit characterizations of optimal contracts in terms of primitives only. We do so when team performance is given by some standard production functions. Two such cases are Cobb–Douglas or constant elasticity of substitution production functions; these functional forms have not yielded tractable analyses in the literature on network games and spillovers, but it turns out that they admit nice solutions for our contract-optimization problems. We show that when team members’ efforts are more substitutable, the optimal contract is more unequal, focusing incentives more on more productive agents; when the efforts are more complementary, the contract spreads payments out across agents. Optimal contracts can respond to the environment in counterintuitive ways. We demonstrate this in a third parametric application—a simple environment where the team’s performance is given by a quadratic production function, yielding a canonical quadratic network game among the agents (Ballester, Calvó Armengol, and Zenou, 2006) for any fixed contract. In this special case of the model, agents have linear utilities of money and the output is equal to the sum of individual efforts plus a quadratic polynomial that we can identify with an exogenous network of bilateral productive complementarities. We show that the optimal contract equalizes all agents’ spillovers on their neighbors in a suitable sense, which entails muting the incentives of more technologically central agents, all else equal. The contracts that achieve this can be computed explicitly and turn out to be quite different from those that are optimal in related models—e.g., Claveria-Mayol, Milán, and Oviedo-Dávila (2024)—where incentives are allocated in proportion to Katz–Bonacich centralities in an exogenous network. Our balance condition thus turns out to have some implications challenging standard intuitions in this setting. The canonical quadratic game setting also facilitates some comments of the extensive margin of our problem—the “team design” question of which agents should be given incentive pay at all. For a given network of technological complementarities, we find that the principal can have strong reasons to give steep incentives to tightly knit teams with strong internal complementarities and exclude many others from incentive pay entirely. Finally, we study a setting where contracts are constrained to take specific forms, such as equity pay, which gives agents shares of output that are fixed across outcomes. Such constraints frequently arise in practice, where principals do not seem to tailor incentives precisely at each different state (e.g., level of revenue) but instead use standard contracts such as equity or options. Even in this more restrictive type of contracting environment, a version of our balance characterization holds, with corresponding rankings of incentive pay across agents. This illustrates that our analysis of incentive spillovers and the optimization of across-agent allocation is flexible about how optimization is handled across states. In particular, it does not rely on full contract flexibility and can accommodate some realistic constraints. Related literature In the contract theory literature, the Holmström (1979) model—studying incentives for a single agent under moral hazard and imperfect observability—is a special case of our multi-agent setup. We use the first-order approach (see Rogerson (1985) and Jewitt (1988)). To our knowledge, there is not much work on how first-order conditions for contract optimality depend on spillovers.333Itoh (1991) allows for a form of spillovers in a two-agent model. Indeed, the extensive literature on moral hazard beginning with Holmström (1982) mainly focuses on different questions. In that strand, a key question is how a principal can use observed outcomes to separately detect agents’ deviations from a desired action profile, often a nearly first-best one (see, e.g., Mookherjee, 1984; Legros and Matsushima, 1991; Legros and Matthews, 1993). Several features of our model prevent such schemes from achieving first-best.444In particular, the contractible outcome (which has only finitely many possible values) is stochastically determined by a one-dimensional team performance, and there is limited liability. In this type of situation, when observability and fine-grained auditing are fundamentally constrained, we examine how optimal contracts depend on spillovers in the production function. Some of the closest work on optimal incentives in the presence of spillovers is found in the literature on networks. This includes, in addition to work already mentioned, papers such as Candogan, Bimpikis, and Ozdaglar (2012), Bloch (2016), Belhaj and Deroïan (2018), Galeotti, Golub, and Goyal (2020), Gaitonde, Kleinberg, and Tardos (2020), and Shi (2022). Our main contribution to this literature is a study of a natural non-parametric formulation, both in terms of the production function and the form of incentives. We show that network game techniques permit some general characterizations of optimal contracts without the parametric assumptions common in the network games literature. When we specialize to a canonical parametric environment in Section 5, we also contrast the more specific implications of our analysis with existing parametric networks models—of which the closest is the contemporaneous work by Claveria-Mayol, Milán, and Oviedo-Dávila (2024) on optimal linear incentive contracts in quadratic network games. Both our analysis and results end up being quite different. The problem of designing multi-agent contracts has also recently attracted attention in a new algorithmic contract theory literature—e.g., Dütting, Ezra, Feldman, and Kesselheim (2023), Ezra, Feldman, and Schlesinger (2024), and Duetting, Ezra, Feldman, and Kesselheim (2025). A starting point of this work is that many standard approaches to finding optimal team contracts may make heavy demands on the analyst’s knowledge of the entire production function and ability to perform computations on it. This literature studies environments with finitely many actions where combinatorial problems create obstacles to tractable optimization and examines whether contracts can be devised that achieve some fraction of optimal performance. Our approach is very different methodologically in that actions and team performance are continuous, but the results give a perspective—complementary to the algorithmic contract theory work—on parsimonious ways to assess and improve on contract performance. Recent work by Zuo (2024), which discusses the structure of optimization problems in a model closely related to some of our special cases, shows that there are interesting computational questions in our more continuous type of setting as well."
https://arxiv.org/html/2411.07679v1,Safe Exploitative Play with Untrusted Type Beliefs,"The combination of the Bayesian game and learning has a rich history, with the idea of controlling a single agent in a system composed of multiple agents with unknown behaviors given a set of types, each specifying a possible behavior for the other agents. The idea is to plan an agent’s own actions with respect to those types which it believes are most likely to maximize the payoff. However, the type beliefs are often learned from past actions and likely to be incorrect. With this perspective in mind, we consider an agent in a game with type predictions of other components, and investigate the impact of incorrect beliefs to the agent’s payoff. In particular, we formally define a tradeoff between risk and opportunity by comparing the payoff obtained against the optimal payoff, which is represented by a gap caused by trusting or distrusting the learned beliefs. Our main results characterize the tradeoff by establishing upper and lower bounds on the Pareto front for both normal-form and stochastic Bayesian games, with numerical results provided.","“The Chinese symbol for crisis is composed of two elements: one signifies danger and the other opportunity.” — Lewis Mumford, 1944 The famous interpretation of the Chinese word for ‘crisis’ (known as{CJK*}UTF8bsmi 危機), although based on mistaken etymology, captures the dual nature of danger and opportunity. It provides an interesting metaphor for the inherent complexities within real-world multi-agent systems. These systems, where risk and opportunity are often inextricably linked, play a pivotal role across diverse domains, ranging from human-AI collaboration Yan et al. (2024) and cyber-physical systems Wang et al. (2016), to highly competitive environments like real-time strategy games Vinyals et al. (2019) and poker Brown and Sandholm (2019). In conventional applied and theoretical frameworks, it is typically assumed that all agents either cooperate or adhere to pre-defined policies Lowe et al. (2017); Albrecht and Stone (2018); Rashid et al. (2020); Anagnostides et al. (2024). However, real-world scenarios often defy these simplifications, presenting agents that display a spectrum of behaviors ranging from cooperative, to heterogeneous, irrational, or even adversarial Li et al. (2023a). This deviation from expected behavior patterns complicates the dynamics of multi-agent systems, as agents cannot reliably predict the actions of their counterparts. The uncertainty regarding whether to trust or distrust predictions naturally leads to a critical tradeoff between the coexisted risk and opportunity, reflecting the dual aspects highlighted in Mumford’s remark. As critical examples, Bayesian games Harsanyi (1967, 1968) provide an approach for modeling differing types of agents in strategic environments. In these games, players form beliefs about others’ types and update beliefs in response to observed actions and choose their actions accordingly. For example, in competitive settings like poker or even in simple games like matching pennies, deviating from the game theoretic optimal (GTO) strategy Friedman (1971); Zadeh (1977) to exploit weaker opponents can be beneficial, but this approach also relies on potentially flawed type beliefs Albrecht and Stone (2018), making it risky to take advantage of such side-information. Similarly, in real-world problems like security games, having a prior distribution of attackers’ behavioral types in a Bayesian game setting leads to advantages. However, exploiting incorrect types can be risky compared to just using minimax strategies. Despite the ubiquity of incorrect type beliefs in practical scenarios, limited attention has been paid to explore such a tradeoff, with exceptions in designing heuristically safe and exploitative strategies in specific contexts, such as with Byzantine adversaries Li et al. (2023a) and sequential games Milec et al. (2021). Inaccuracies in beliefs about others’ types may arise from factors such as using out-of-distribution data to generate priors, changes in opponents’ behaviors, or mismatches between hypothesized types and the optimal type space, etc. As noted in Albrecht et al. (2016), it is evident that prior beliefs significantly affect the long-term performance of type-based learning algorithms like the Harsanyi-Bellman Ad Hoc Coordination (HBA). Although algorithms like HBA demonstrate asymptotic convergence to correct predictions or type distributions through various methods of estimating posterior beliefs, and methods exist for detecting inaccuracies in type beliefs using empirical behavioral hypothesis testing Albrecht et al. (2016), the theoretical capabilities of general algorithms remain uncertain. In general, relying on learned beliefs presents a fundamental tradeoff between the potential payoffs from exploitative play and the risk of incorrect type beliefs. Given the potential inaccuracies in these type beliefs, relying on them to exploit opponents could lead to high-risk strategies. Conversely, not exploiting these beliefs might result in overly cautious play. Therefore, it is natural to investigate the impact of incorrect beliefs on the agent’s payoff, in terms of a tradeoff between trusting or distrusting the beliefs of types provided by type-based learning algorithms (e.g., Bayesian learning Jordan (1991), best response dynamics Bichler et al. (2023), and policy iteration with neural networks Bichler et al. (2021), etc.) in multi-agent systems. To summarize the focus of this paper, we aim to address the following critical question: What is the fundamental tradeoff between trusting/distrusting type beliefs in games? Contributions. Motivated by the above question, we analyze the following payoff gap that arises from erroneous type beliefs: \displaystyle\Delta(\varepsilon;\pi) \displaystyle\coloneq\max_{d\left(\theta,\theta^{\star}\right)\leq\varepsilon}% \left(\max_{\phi}\mathsf{Payoff}(\phi,\theta^{\star})-\mathsf{Payoff}(\pi(% \theta),\theta^{\star})\right), (1) where \theta and \theta^{\star} are predicted and true type beliefs respectively; d(\cdot,\cdot) measures the distance between \theta and \theta^{\star} bounded from above by \varepsilon and will be formally specified with concrete model contexts in Section 3 and Section 4, together with the payoff, denoted by \mathsf{Payoff}(\cdot,\cdot) as a function of the true type \theta^{\star} and the used strategy \pi. The agent uses a strategy \pi that depends on the type belief \theta. Overall, the payoff difference (1) above quantifies the worst-case gap between the optimal payoff (obtained using an optimal strategy that maximizes \mathsf{Payoff}(\cdot,\theta^{\star})), and the payoff corresponding to \pi. In particular, when the agent’s strategy \pi trusts the belief \theta, it takes the opportunity to close the gap in (1) when the belief error \varepsilon is small. However, when \varepsilon increases, such a strategy incurs a high risk since it trusts the incorrect \theta. Evaluating the payoff gap in (1) naturally yields a tradeoff between opportunity and risk, as illustrated on the right of Figure 1. To be more precise, we measure the tradeoff between two important quantities: (Missed) Opportunity: \Delta(0;\pi), corresponding to the case when the type beliefs are correct; Risk: \max_{\varepsilon>0}\Delta(\varepsilon;\pi) measuring the payoff difference incurred by worst-case incorrect beliefs. In summary, the (missed) opportunity measures the discrepancy of a strategy \pi from the optimal strategy, which is aligned with the ground truth belief \theta^{\star} of other players in terms of the obtained payoff. Additionally, the risk quantifies how inaccurate beliefs impact the difference in terms of payoffs in the worst case. The goal of this paper is to investigate safe and exploitative strategies in Bayesian games that achieve near-optimal opportunity and risk. Figure 1: Left: A stochastic Bayesian game where an agent interacts with an environment and opponents, with a belief of their types \theta\in\Theta. Right: The tradeoff between trusting and distrusting type beliefs, with trust leading to higher risk and opportunity and distrust resulting in lower risk and opportunity, implying an opportunity-risk tradeoff with varying strategy \pi. Our main results are two-fold. Firstly, in normal-form Bayesian games, we characterize a tradeoff between the opportunity and risk. We consider a strategy as a convex combination of a safe strategy and the best response given type beliefs. Upper bounds on opportunity and risk are provided in Theorem 3.1. Conversely, lower bounds that hold for any mixed strategy are shown in Theorem 3.2. Notably, when the game is fair, and the hypothesis set \Theta is sufficiently large, these bounds tightly converge. Secondly, we explore a dynamic setting in stochastic Bayesian games, where an agent, provided with type beliefs about other players, engages in interactions over time, as illustrated in Figure 1. Unlike the normal-form approach, we utilize a value-based strategy that establishes upper bounds on opportunity and risk, as outlined in Theorem 4.1. Additionally, Theorem 4.2 provides lower bounds on opportunity and risk that differ from the upper bounds by multiplicative constants, yielding a characterization of the opportunity-risk tradeoff. Finally, a case study of a security game, simulating a defender protecting an elephant population from illegal poachers, is provided in Section 5."
https://arxiv.org/html/2411.07362v1,Factorised Active Inference for Strategic Multi-Agent Interactions,"Understanding how individual agents make strategic decisions within collectives is important for advancing fields as diverse as economics, neuroscience, and multi-agent systems. Two complementary approaches can be integrated to this end. The Active Inference framework (AIF) describes how agents employ a generative model to adapt their beliefs about and behaviour within their environment. Game theory formalises strategic interactions between agents with potentially competing objectives. To bridge the gap between the two, we propose a factorisation of the generative model whereby each agent maintains explicit, individual-level beliefs about the internal states of other agents, and uses them for strategic planning in a joint context. We apply our model to iterated general-sum games with 2 and 3 players, and study the ensemble effects of game transitions, where the agents’ preferences (game payoffs) change over time. This non-stationarity, beyond that caused by reciprocal adaptation, reflects a more naturalistic environment in which agents need to adapt to changing social contexts. Finally, we present a dynamical analysis of key AIF quantities: the variational free energy (VFE) and the expected free energy (EFE) from numerical simulation data. The ensemble-level EFE allows us to characterise the basins of attraction of games with multiple Nash Equilibria under different conditions, and we find that it is not necessarily minimised at the aggregate level. By integrating AIF and game theory, we can gain deeper insights into how intelligent collectives emerge, learn, and optimise their actions in dynamic environments, both cooperative and non-cooperative.","Collective intelligence, the emergent ability of groups to solve problems more effectively than individuals, is a phenomenon observed across biological, social, and artificial systems. Understanding the mechanisms that drive this collective behaviour is essential for advancing fields as diverse as economics, neuroscience, and multi-agent systems. Game theory models incentivised social interactions with potentially competing objectives, where a utility function maps behaviour to the real numbers. A Nash equilibrium represents the point where agents, independently maximising their utility, have no incentive to change their strategy. Bridging the gap between idealised game-theoretical models and the often messy realities of agents interacting in complex environments presents a persistent challenge. Traditional game theory often falters when agents deviate from perfect rationality Camerer (2011). This challenge becomes particularly salient in the face of strategic uncertainty, where agents grapple with uncertainty about the actions and intentions of others, and equilibrium selection, where multiple potential equilibria exist without clear mechanisms for convergence. Shoham et al. Shoham et al. (2007) drew attention to a key issue with equilibrium selection: It seems to us that sometimes there is a rush to investigate the convergence properties, motivated by the wish to anchor the central notion of game theory in some process, at the expense of motivating that process rigorously. The \acAIF, a process theory rooted in neuroscience, can offer a compelling perspective on these challenges. AIF provides a principled way to describe how agents adapt their behaviour based on probabilistic beliefs about their environment. In recent years, AIF models for single-agent tasks have developed rapidly and reached maturity Friston et al. (2024a). However, the application of AIF to multi-agent settings remains in its infancy Friston et al. (2024b). We begin by reviewing recent work on the intersection of AIF and Bayesian agents, game theory, and multi-agent systems (§2). Integrating the two ends of the spectrum, we propose a factorisation of the generative model whereby an agent maintains explicit, individual-level beliefs about the internal states of other agents and uses them for strategic planning in a joint context (§3). We apply our model to iterated general-sum games with 2 and 3 players and study the ensemble effects of game transitions, where the agents’ preferences (game payoffs) and their associated equilibria change over time (§4). We present a dynamical analysis of two key active inference quantities: the \acVFE (§4.1) and the \acEFE (§4.2) from numerical simulation data. The ensemble-level EFE allows us to characterise the basins of attraction of games with multiple Nash Equilibria (such as the Stag Hunt) under different conditions, and we find that it is not necessarily minimised at the aggregate level."
https://arxiv.org/html/2411.07335v1,"Multimodal Fusion Balancing Through
Game-Theoretic Regularization","Multimodal learning can complete the picture of information extraction by uncovering key dependencies between data sources. However, current systems fail to fully leverage multiple modalities for optimal performance. This has been attributed to modality competition, where modalities strive for training resources, leaving some underoptimized. We show that current balancing methods struggle to train multimodal models that surpass even simple baselines, such as ensembles. This raises the question: how can we ensure that all modalities in multimodal training are sufficiently trained, and that learning from new modalities consistently improves performance? This paper proposes the Multimodal Competition Regularizer (MCR), a new loss component inspired by mutual information (MI) decomposition designed to prevent the adverse effects of competition in multimodal training. Our key contributions are: 1) Introducing game-theoretic principles in multimodal learning, where each modality acts as a player competing to maximize its influence on the final outcome, enabling automatic balancing of the MI terms. 2) Refining lower and upper bounds for each MI term to enhance the extraction of task-relevant unique and shared information across modalities. 3) Suggesting latent space permutations for conditional MI estimation, significantly improving computational efficiency. MCR outperforms all previously suggested training strategies and is the first to consistently improve multimodal learning beyond the ensemble baseline, clearly demonstrating that combining modalities leads to significant performance gains on both synthetic and large real-world datasets.","Exploiting multimodal data has made significant progress, with advances in generalizable representations and larger datasets enabling solutions to previously unattainable tasks [27; 29; 37; 43; 42; 44; 50; 53; 62]. However, studies indicate that multimodal data is often utilized suboptimally, underperforming compared to ensemble unimodal models or even the best single modality [55; 60]. The expectation that adding a modality should improve performance, assuming independent errors and above-chance predictive power [17], is frequently contradicted in practice. Huang et al. [20] attribute this issue to modality competition, where one modality quickly minimizes training error, misdirecting and suppressing the learning of others. Factors like noise levels, relationship complexity with the target, feature dimensionality, and data quality can cause one modality to fit faster than another. This imply that adding task-relevant information doesn’t guarantee better performance, primarily due to complications during training. To address these issues, it’s crucial to monitor each modality’s contribution during training and apply corrective measures. Several balancing strategies have been proposed to tackle this issue [5; 6; 9; 10; 21; 26; 28; 40; 41; 54; 55; 57; 60]. A central aspect of these methods is estimating each modality’s contribution to the output. Most assume distributional independence between modalities on predicting the target, measuring contribution via unimodal performance [60; 26; 40; 6; 54]. Some methods bypass this assumption by estimating influence based on prediction differences between original and perturbed inputs [28; 21; 10]. Perturbations can take various forms, such as zeroing values [28], adding Gaussian noise [10], or using task-specific augmentations [21; 31]. These methods aim to amplify a modality’s influence by increasing the impact of perturbations on the output. However, this also makes the network more sensitive to these changes (e.g., noise), risks becoming overly reliant on the perturbations, and struggles to scale when multiple perturbations are required. Additionally, increasing the contribution of one modality can be achieved by overshadowing others, leading to an imbalance that undermines overall performance and makes the objective counterproductive. Given these challenges, how can we design an efficient regularization method that addresses multimodal competition, ensuring balanced and effective learning across all modalities? Figure 1: (Left) Illustration of the conditional mutual information (\operatorname{CMI}) terms, \operatorname{CMI}_{1}:I(X_{1};Y\mid X_{2}) and \operatorname{CMI}_{2}:I(X_{2};Y\mid X_{1}), representing the unique contributions (U_{1} and U_{2}) of each modality to the target. The shared task-relevant information (S) between the modalities is defined as I(X_{1};X_{2})-I(X_{1};X_{2}\mid Y). (Right) Accuracy as a function of the ratio between the unique information (U_{1}) from modality X_{1} and the shared information (S) between the modalities. Synthetic data are generated as X_{1}=N_{1}+Y,X_{2}=N_{1}+Y where N_{1},N_{2} are independent noise for each modality. We consider S the percentage of the datapoints that both modalities have information about the label Y, U_{1} and U_{2} when only one has with the other modality equating to noise for those datapoints. In the experiment we keep U_{2} constant while changing U_{1} and S. As U_{1} increases and S decreases, accuracy deteriorates, reflecting intensified multimodal competition. Among the various methods, including Singleloss, Multiloss, Ensemble, unimodally pretrained and finetuned encoders (Uni-Pre Fine), OGM [40], AGM [28], and MLB [26]our regularization method MCR demonstrates a slower decline in accuracy. For further detals prease refer to Section 4.1 In this paper, we introduce the Multimodal Competition Regularizer (\operatorname{MCR}), a loss function designed to promote the exploration of task-relevant information across all available modalities. By decomposing the joint mutual information (\operatorname{MI}), we separately model shared and unique task-relevant information within the modalities. To efficiently capture the unique information from each modality, we employ a computationally inexpensive permutation-based approach. Our method maximizes the lower bounds of each MI term to encourage the network to learn both shared and unique information, while minimizing upper bounds on terms to suppress task-irrelevant information. We frame the problem in a game-theoretic setting, exploring strategies that involve both collaboration and competition among modalities to address the conflicting objectives that arise when increasing all modalities’ contributions simultaneously. This approach allows their contributions to adapt dynamically, achieving balance during training. We extensively evaluate \operatorname{MCR} on synthetic datasets and several established real-world multimodal benchmarks, including action recognition on AVE [49] and UCF [45], emotion recognition on CREMA-D [4], human sentiment on CMU-MOSI [61], human emotions on CMU-MOSEI [63] and egocentric action recognition on Something-Something [15]. Our results demonstrate that \operatorname{MCR} is the first balancing method to significantly improve supervised multimodal training over the ensemble baseline across a variety of datasets and models. Our key contributions are summarized as follows: 1. An analysis of multimodal competition, defining the error increase caused in multimodal training, while demonstrating in our results that most previous methods do no outperform simple baselines, such as unimodal ensembles. 2. A novel multimodal training strategy, \operatorname{MCR}, designed to regularize multimodal competition, which includes: • Defining lower and upper bounds of the \operatorname{MI} terms, encouraging the exploration of information across all modalities. • Introducing a game-theoretic perspective where modalities form the players that compete for training resources, assisting the regularization through balancing the corresponding MI terms. • Suggesting latent-space perturbations as an efficient way to estimate the lower bound of the \operatorname{CMI} reducing the computational cost of multiple forward passes."
https://arxiv.org/html/2411.07099v1,Bounded Rationality Equilibrium Learning in Mean Field Games,"Mean field games (MFGs) tractably model behavior in large agent populations. The literature on learning MFG equilibria typically focuses on finding Nash equilibria (NE), which assume perfectly rational agents and are hence implausible in many realistic situations. To overcome these limitations, we incorporate bounded rationality into MFGs by leveraging the well-known concept of quantal response equilibria (QRE). Two novel types of MFG QRE enable the modeling of large agent populations where individuals only noisily estimate the true objective. We also introduce a second source of bounded rationality to MFGs by restricting agents’ planning horizon. The resulting novel receding horizon (RH) MFGs are combined with QRE and existing approaches to model different aspects of bounded rationality in MFGs. We formally define MFG QRE and RH MFGs and compare them to existing equilibrium concepts such as entropy-regularized NE. Subsequently, we design generalized fixed point iteration and fictitious play algorithms to learn QRE and RH equilibria. After a theoretical analysis, we give different examples to evaluate the capabilities of our learning algorithms and outline practical differences between the equilibrium concepts.","Learning equilibria in multi-agent games is of great practical interest but hard to scale to many agents (Daskalakis et al., 2009; Deng et al., 2023). Mean field games (MFGs) allow scaling to arbitrarily many exchangeable agents at fixed complexity. MFGs are of recent interest as a tractable method to learn approximate equilibria of rational, selfish agents (Guo et al., 2019; Cui & Koeppl, 2021; Xie et al., 2021; Laurière et al., 2022; Anahtarci et al., 2023). Thus, MFGs are applied in various settings ranging from finance to engineering (Djehiche et al., 2017; Achdou et al., 2020; Carmona, 2020). A common solution concept in the realm of multi-agent learning, and consequently in the context of MFGs, is the Nash equilibrium (NE). In a NE, each player’s strategy is considered optimal, given the strategies of the other agents, resulting in an equilibrium where no agent has an incentive to change their strategies. The optimality notion inherent in NE assumes full rationality of the individual agents. However, in many real-world situations individuals may not behave perfectly rational due to limited information processing capabilities, psychological factors, social considerations or other factors. Deviations from perfect rationality are described by the fundamental concept of bounded rationality (Simon, 1955, 1979; Kahneman & Tversky, 1982; Selten, 1990; Gigerenzer & Selten, 2002; Kahneman, 2013). Bounded rationality implies that for many real-world scenarios NE are insufficient due to their rigorous perfect rationality assumption. Instead of NE, we require a more realistic equilibrium concept accounting for partially irrational agents. A popular game-theoretic approach to modeling bounded rationality of agents are quantal response equilibria (QRE) (McKelvey & Palfrey, 1995, 1998) which are used, e.g. in economics (Breitmoser et al., 2010), robust RL (Reddi et al., 2024) and for efficient NE approximation (Gemp et al., 2024). Intuitively, in a QRE agents perceive rewards perturbed by noise and act optimally with respect to these perturbed rewards. In our work, we extend QRE to the domain of MFGs to model the behavior of a large number of agents who deviate from perfect rationality. Meanwhile on the control-theoretic side, a common approximately optimal control method is model predictive control (MPC) (Kouvaritakis & Cannon, 2016), also known as receding horizon control. To further enhance modeling of bounded rationality in MFGs, we incorporate a receding horizon method, where agents make decisions based on a limited future time horizon, reflecting more realistic decision-making processes. In contrast to MPC-based variants of MFGs such as (Inoue et al., 2021), we analyze the resulting novel receding horizon equilibria and instead focus on learning such equilibria, in a discrete-time setting. Beyond realism, introducing bounded rationality yields possible tractability advantages. NE computation for MFGs can be hard, motivating the search for alternative equilibrium notions. In this work, we show that under certain assumptions, QRE can be computed using a fixed point iteration (FPI). Moreover, QRE solutions can be seen as NE approximations with arbitrarily accurate design (Eibelshäuser & Poensgen, 2019). Recently, different equilibria have been introduced as NE approximations in MFGs (Cui & Koeppl, 2021). We compare QRE with these equilibria both theoretically and empirically and provide a new algorithm to compute QRE which extends to these equilibria. Meanwhile, for receding horizon equilibria we find novel general algorithms that work both in practice and in theory. Our main contributions are: • We formulate QRE for MFGs to incorporate bounded rationality for a more realistic MFG framework; • We integrate a receding horizon method tailored to the limited lookahead capacity of realistic agents; • We give theoretical and empirical results to put MFG QRE in context to existing equilibrium concepts; • We generalize the known fictitious play (FP) and FPI algorithms for NE to learn QRE and other equilibria; • We provide empirical examples to demonstrate the capabilities of our learning algorithms."
https://arxiv.org/html/2411.06924v1,Maximizing Nash Social Welfare in 2-Value Instances: A Simpler Proof for the Half-Integer Case,"A set of m indivisible goods is to be allocated to a set of n agents. Each agent i has an additive valuation function v_{i} over goods. The value of a good g for agent i is either 1 or s, where s is a fixed rational number greater than one, and the value of a bundle of goods is the sum of the values of the goods in the bundle. An allocation X is a partition of the goods into bundles X_{1}, …, X_{n}, one for each agent. The Nash Social Welfare (\mathit{NSW}) of an allocation X is defined asThe \mathit{NSW}-allocation maximizes the Nash Social Welfare. In [ACH+22] it was shown that the \mathit{NSW}-allocation can be computed in polynomial time, if s is an integer or a half-integer, and that the problem is NP-complete otherwise. The proof for the half-integer case is quite involved. In this note we give a simpler and shorter proof.","A set of m indivisible goods is to be allocated to a set of n agents. Each agent i has an additive valuation function v_{i} over goods. The value of a good g for agent i is either 1 or s, i.e., v_{i}(g)\in\{1,s\}; here s is a fixed rational number greater than one. The value of a bundle X_{i} of goods for i is the sum of the values of the goods in the bundle, i.e., v_{i}(X_{i})=\sum_{g\in X_{i}}v_{i}(g). We call a good g heavy if v_{i}(g)=s for some i, and light otherwise, i.e., v_{i}(g)=1 for all i. An allocation X is a partition of the goods into bundles X_{1}, …, X_{n}, one for each agent, i.e., \cup_{i}X_{i}=[m] and X_{i}\cap X_{j}=\emptyset for i\not=j. The Nash Social Welfare (NSW) of an allocation X is defined as \mathit{NSW}(X)=\left(\prod_{i}v_{i}(X_{i})\right)^{\nicefrac{{1}}{{n}}}. The \mathit{NSW}-allocation maximizes the Nash Social Welfare. In [ACH+22] it was shown that the \mathit{NSW}-allocation can be computed in polynomial time, if s is an integer or a half-integer, and that the problem is NP-complete otherwise. The proof for the half-integer case is quite involved. In this note we give a simpler and shorter proof. In [ACH+22], it was argued that for the case of s being integral, the allocation of the heavy and the light goods can be considered separately, and that this is not the case when s is half-integral. The following example is given. Consider two agents with identical valuations and s=\nicefrac{{3}}{{2}}. There are two heavy goods and either two or three light goods. If there are two light goods, each agent should receive a heavy and a light good, and if there are three light goods, one agent should receive the two heavy goods and the other should receive the three light goods. So, the number of available light goods influences the allocation of the heavy goods. The authors conclude that it is necessary to consider heavy and light goods together throughout the algorithm. We give an algorithm that consists of two phases and considers the light goods only in the second phase. This is the first simplification. The proof in [ACH+22] is algorithmic, i.e., the paper introduces a collection of improvement rules and shows that their repeated application produces a \mathit{NSW}-allocation. In contrast, we characterize the \mathit{NSW}-allocation and discuss the algorithmics after the characterization. This is the second simplification. Whenever an improvement rule is applied in [ACH+22], a detailed accounting of how the heavy and light goods are reassigned is given. In many situations we are able to replace the detailed accounting by a global argument. This is the third simplication. It is convenient to represent an instance of our problem as a bipartite graph. The two sides of the graph are the agents and the goods respectively, each agent is connected to each good, and the edge from agent i to good g is labelled as either heavy or light. An allocation A is an assignment of the goods to the agents. In this paper, we restrict attention to allocations that assign every heavy good to an agent that considers it heavy; the general case is treated in Section 4.4 of [ACH+22] and we have no simplication to offer for this part. The restricted case is handled in Sections 4.1, 4.2, and 4.3 of [ACH+22] that span a total of 25 pages. The current presentation has only half the length. Related Work: In general, determining the optimal \mathit{NSW} is NP-complete. Good approximation algorithms were obtained by [CG15, CDG+17, AGSS17, BKV18a]. The current best factor is e^{\nicefrac{{1}}{{e}}}\approx 1,445. For binary valuations, i.e., v_{i}(g)\in\{0,1\} for all i and g, the \mathit{NSW}-allocation can be determined in polynomial time [BKV18b]. For two-valued valuations, i.e., v_{i}(g)\in\{1,s\} for all i and g, and s a rational number greater than one, the \mathit{NSW}-allocation can be computed in polynomial time if s is an integer or a half-integer. Otherwise, the problem is NP-complete [ACH+22]."
https://arxiv.org/html/2411.06141v1,Online Bayesian Persuasion Without a Clue,"We study online Bayesian persuasion problems in which an informed sender repeatedly faces a receiver with the goal of influencing their behavior through the provision of payoff-relevant information. Previous works assume that the sender has knowledge about either the prior distribution over states of nature or receiver’s utilities, or both. We relax such unrealistic assumptions by considering settings in which the sender does not know anything about the prior and the receiver. We design an algorithm that achieves sublinear—in the number of rounds—regret with respect to an optimal signaling scheme, and we also provide a collection of lower bounds showing that the guarantees of such an algorithm are tight. Our algorithm works by searching a suitable space of signaling schemes in order to learn receiver’s best responses. To do this, we leverage a non-standard representation of signaling schemes that allows to cleverly overcome the challenge of not knowing anything about the prior over states of nature and receiver’s utilities. Finally, our results also allow to derive lower/upper bounds on the sample complexity of learning signaling schemes in a related Bayesian persuasion PAC-learning problem.","Bayesian persuasion has been introduced by Kamenica and Gentzkow (2011) to model how strategically disclosing information to decision makers influences their behavior. Over the last years, it has received a terrific attention in several fields of science, since it is particularly useful for understanding strategic interactions involving individuals with different levels of information, which are ubiquitous in the real world. As a consequence, Bayesian persuasion has been applied in several settings, such as online advertising (Emek et al., 2014; Badanidiyuru et al., 2018; Bacchiocchi et al., 2022; Agrawal et al., 2023), voting (Alonso and Câmara, 2016; Castiglioni et al., 2020a; Castiglioni and Gatti, 2021), traffic routing (Vasserman et al., 2015; Bhaskar et al., 2016; Castiglioni et al., 2021a), recommendation systems (Cohen and Mansour, 2019; Mansour et al., 2022), security (Rabinovich et al., 2015; Xu et al., 2016), e-commerce (Bro Miltersen and Sheffet, 2012; Castiglioni et al., 2022) medical research (Kolotilin, 2015), and financial regulation (Goldstein and Leitner, 2018). In its simplest form, Bayesian persuasion involves a sender observing some information about the world, called state of nature, and a receiver who has to take an action. Agents’ utilities are misaligned, but they both depend on the state of nature and receiver’s action. Thus, sender’s goal is to devise a mechanism to (partially) disclose information to the receiver, so as to induce them to take a favorable action. This is accomplished by committing upfront to a signaling scheme, encoding a randomized policy that defines how to send informative signals to the receiver based on the observed state. Classical Bayesian persuasion models (see, e.g., (Dughmi and Xu, 2016, 2017; Xu, 2020)) rely on rather stringent assumptions that considerably limit their applicability in practice. Specifically, they assume that the sender perfectly knows the surrounding environment, including receiver’s utilities and the probability distribution from which the state of nature is drawn, called prior. This has motivated a recent shift of attention towards Bayesian persuasion models that incorporate concepts and ideas from online learning, with the goal of relaxing some of such limiting assumptions. However, existing works only partially fulfill this goal, as they still assume some knowledge of either the prior (see, e.g., (Castiglioni et al., 2020b, 2021b, 2023; Babichenko et al., 2022; Bernasconi et al., 2023)) or receiver’s utilities (see, e.g., (Zu et al., 2021; Bernasconi et al., 2022; Wu et al., 2022)). 1.1 Original contributions We address—for the first time to the best of our knowledge—Bayesian persuasion settings where the sender has no clue about the surrounding environment. In particular, we study the online learning problem faced by a sender who repeatedly interacts with a receiver over multiple rounds, without knowing anything about both the prior distribution over states of nature and receiver’s utilities. At each round, the sender commits to a signaling scheme, and, then, they observe a state realization and send a signal to the receiver based on that. After each round, the sender gets partial feedback, namely, they only observe the best-response action played by the receiver in that round. In such a setting, the goal of the sender is to minimize their regret, which measures how much utility they lose with respect to committing to an optimal (i.e., utility-maximizing) signaling scheme in every round. We provide a learning algorithm that achieves regret of the order of \widetilde{\mathcal{O}}(\sqrt{T}), where T is the number of rounds. We also provide lower bounds showing that the regret guarantees attained by our algorithm are tight in T and in the parameters characterizing the Bayesian persuasion instance, i.e., the number of states of nature d and that of receiver’s actions n. Our algorithm implements a sophisticated explore-then-commit scheme, with exploration being performed in a suitable space of signaling schemes so as to learn receiver’s best responses exactly. This is crucial to attain tight regret guarantees, and it is made possible by employing a non-standard representation of signaling schemes, which allows to cleverly overcome the challenging lack of knowledge about both the prior and receiver’s utilities. Our results also allow us to derive lower/upper bounds on the sample complexity of learning signaling schemes in a related Bayesian persuasion PAC-learning problem, where the goal is to find, with high probability, an approximately-optimal signaling scheme in the minimum possible number of rounds. 1.2 Related works Castiglioni et al. (2020b) were the first to introduce online learning problems in Bayesian persuasion scenarios, with the goal of relaxing sender’s knowledge about receiver’s utilities (see also follow-up works (Castiglioni et al., 2021b, 2023; Bernasconi et al., 2023)). In their setting, sender’s uncertainty is modeled by means of an adversary selecting a receiver’s type at each round, with types encoding information about receiver’s utilities. However, in such a setting, the sender still needs knowledge about the finite set of possible receiver’s types and their associated utilities, as well as about the prior. A parallel research line has focused on relaxing sender’s knowledge about the prior. Zu et al. (2021) study online learning in a repeated version of Bayesian persuasion. Differently from this paper, they consider the sender’s learning problem of issuing persuasive action recommendations (corresponding to signals in their case), where persuasiveness is about correctly incentivizing the receiver to actually follow such recommendations. They provide an algorithm that attains sublinear regret while being persuasive at every round with high probability, despite having no knowledge of the prior. Wu et al. (2022); Gan et al. (2023); Bacchiocchi et al. (2024c) achieve similar results for Bayesian persuasion in episodic Markov decision processes, while Bernasconi et al. (2022) in non-Markovian environments. All these works crucially differ from ours, since they strongly rely on the assumption that receiver’s utilities are known to the sender, which is needed in order to meet persuasiveness requirements. As a result, the techniques employed in such works are fundamentally different from ours as well. Finally, learning receiver’s best responses exactly (a fundamental component of our algorithm) is related to learning in Stackelberg games (Letchford et al., 2009; Peng et al., 2019; Bacchiocchi et al., 2024a). For more details on these works and other related works, we refer the reader to Appendix A."
https://arxiv.org/html/2411.07166v1,The Shapley index for music streaming platforms,"We study an index to measure the popularity of artists in music streaming platforms. This index, which can be used to allocate the amount raised via paid subscriptions among participating artists, is based on the Shapley value, a centerpiece in cooperative game theory. We characterize this Shapley index combining several axioms formalizing principles with normative appeal. This permits to place the index in the literature, as an alternative to the well-known (and widely used in the industry) pro-rata and user-centric indices.","Platform businesses have gained enormous attention in recent years, which has been reflected into the literature on economics research (e.g., Cabral et al., 2019; Belleflamme and Peitz, 2021; Calveras and Ganuza, 2021; Jullien et al., 2021). Among other things, platforms have transformed the ways in which cultural content is produced and consumed (e.g., Aguiar et al., 2024).111Nieborg and Poell (2018) have coined the term platformization of cultural production. This is particularly the case with music. In the old times, consumers typically learned about music from radio stations (or word of mouth) and eventually moved on to buy from record stores. This started to change when digital music emerged and spread universally. After some initial years in which file-sharing platforms, such as Napster, were under scrutiny by the music industry, Apple managed to persuade record companies to sell individual tracks for 99 cents. Gradually, the industry found new profitable paths, eventually embracing streaming, a massive success nowadays. To wit, according to Statista, in the second quarter of 2024, Spotify (the largest music streaming platform) reached an all-time high with 626 million active users worldwide. This marked an increase of 75 million users in just one year. Platforms cash such a massive success of streaming in various ways. But it is estimated that almost 90% of the total revenue that platforms raise comes from premium consumers. That is, consumers that gain access to all the music on the platform after paying a monthly subscription.222Nevertheless, more than half of platform users do not pay any money (instead listening to ads, while using the platform). The hybrid approach, offering both an ad-supported free version and an ad-free subscription version is not exclusive of music platforms. Netflix, Hulu, YouTube, or Pandora, to name a few, have also adopted it to deal with the trade-off between viewership and subscription profits. An interesting question, which we shall not study here, is how those multiple versions should be designed and priced (e.g., Goli et al., 2024). It is estimated that streaming platforms redistribute among artists around 65-70% of the revenue they raise, which thus becomes a major aspect in the management of streaming platforms.333To be more precise, streaming platforms pay “right holders”, who may be the artists themselves if they are independent, or the record labels if the artist is signed to one. The problem of sharing the revenue raised from paid subscriptions to streaming platforms among artists is a new form of revenue sharing problems under bundled pricing (e.g., Adams and Yellen, 1976; Ginsburgh and Zang, 2003; Bergantiños and Moreno-Ternero, 2015). As such, it offers new insights with respect to the classical literature on industrial organization (e.g., Belleflamme and Peitz, 2015). In the early years, platforms used the pro-rata method, in which artists were rewarded in proportion of their total streams. Gradually, they have been moving to a user-centric method, in which, instead, the amount paid by each user is shared among the artists this user streamed, in proportion of the user’s overall streams. The two methods have been recently scrutinized in the scientific literature (e.g., Alaei et al., 2022; Bergantiños and Moreno-Ternero, 2024). In this paper, we study a third method, which is obtained following the tradition of analyzing problems involving agents’ cooperation with a game-theoretical approach.444Classical instances are bankruptcy problems from the Talmud (e.g., Aumann and Maschler, 1985), cost alocation problems (e.g., Tijs and Driessen, 1986), river sharing (e.g., Ambec and Sprumont, 2002), allocating benefits of horizontal cooperation (e.g., Lozano et al., 2013), or the value captured in hierarchical chains (e.g., Henkel and Hoffmann, 2018). More precisely, as in Bergantiños and Moreno-Ternero (2024), we associate to each streaming problem (to be understood as the problem of allocating the overall amount raised from paid subscriptions among artists streamed in the platform) a cooperative (TU) game in which the worth of each coalition of artists is determined by the amount users streaming only those artists pay. We then consider the well-known Shapley value (e.g., Shapley, 1953) of the resulting game as an allocation rule for streaming problems.555Schlicher et al. (2024) associate another cooperative game to a streaming problem. Both Bergantiños and Moreno-Ternero (2024) and Schlicher et al. (2024) are concerned with the core of the resulting games, rather than the Shapley value of such games. Gonçalves-Dosantos et al. (2024a) point out that each of their indicators for streaming (not necessarily music) platforms coincide with the Shapley value of different cooperative games that can suitably be associated to the class of problems they analyze. It turns out that the Shapley value of such a TU game can be easily computed (which, in other settings, is not always the case). In words, it says that the subscription of each user is equally (and fully) allocated among the artists this user streamed. The ensuing allocation rule, which will be the object of our study, is what we dub the Shapley index for streaming problems. The Shapley index is closer to the user-centric index than to the pro-rata index mentioned above, as it also imposes that the amount each user pays is distributed only among artists streamed by such a user. Now, the Shapley index states that it is equally distributed among them, whereas the user-centric says that it is proportionally distributed among them. In that sense, both indices represent the two long-standing (and widely supported) principles of distributive justice: egalitarianism and proportionality (e.g., Young, 1994; Moulin, 2004; Thomson, 2019). Beyond some preliminary game-theoretical results, which yield interesting features for the Shapley index, we concentrate on its normative foundations. More precisely, following the tradition initiated by Nash (1950) and Arrow (1951), we take an axiomatic approach to streaming problems.666Bergantiños and Moreno-Ternero (2024) and Gonçalves-Dosantos et al. (2024a, 2024b) have also applied the axiomatic approach to streaming problems. Other recent instances of this approach, dealing with various problems, are Asheim et al., (2020), Flores-Szwagrzak and Treibich (2020) and Csóka and Herings (2021). To do so, we formalize several principles with normative appeal, referring to operational or ethical aspects of streaming problems, as axioms of indices. We show that several combinations of these axioms characterize the Shapley index. This permits a more thorough comparison between this index and the other two main indices that existed so far. The rest of the paper is organized as follows. In Section 2, we present the preliminaries of the model to analyze streaming problems. In Section 3, we present the Shapley index and some game-theoretical aspects of it. In Section 4, we present our axiomatic analysis. In Section 5, and based on the results from the axiomatic analysis, we properly place the Shapley index in the literature. Finally, Section 6 concludes. Some extra material (mostly referring to the tightness of our characterization results) is gathered in the appendix."
https://arxiv.org/html/2411.06788v1,Designing Local Distributed Mechanisms,"In this work we introduce a new notion: local mechanisms. These are truthful mechanisms that have an implementation as fast distributed algorithms and non-trivial approximation guarantees. We show how monotone distributed optimisation algorithms can be turned into truthful mechanisms using Myerson’s Lemma. We demonstrate mechanisms for four fundamental graph problems: maximum-weight independent set, minimum-weight vertex cover, minimum-weight dominating set, and a variant of weighted colouring.We show how these mechanisms can be implemented in the distributed setting. The key observation is that computing the so-called critical prices of a monotone algorithm can be done with the same time complexity as the original algorithm in the LOCAL model of distributed computing. Our work establishes a new connection between algorithmic mechanism design and distributed graph algorithms. We pose several open questions, such as can critical prices be computed with small messages. It also points to the importance of designing monotone distributed optimisation algorithms.Our work extends previous work in Distributed Algorithmic Mechanism Design (DAMD) in a new direction. Instead of studying global problems like routing or leader election, we study local resource allocation problems. Our algorithms are simple and thus potentially practical. Local algorithms are particularly interesting for highly dynamic large-scale systems, and there are many potential future application domains, e.g. demand-side load management in electric grids or resource allocation in IoT computing.","In mechanism design the goal is to implement a social choice function as a game. Participating agents hold private information that is critical to choosing a good assignment. The task of the mechanism designer is to come up with an algorithm and incentives such that the agents should always reveal their private information to the mechanism. A classic example is selling a single item: how should we assign the item and what should it cost? One of the most common auctions is a sealed-bid first-price auction: each participant sends a bid in secret and the largest bidder gets the item, paying its bid. The problem with this mechanism is that it is game-theoretically unstable: the optimal behaviour for the winner is to bid just above the (unknown) second-highest bid. Trying to reason about the valuations of the other agents makes participating in this auction difficult. The second-price auction fixes this issue: the winner pays the second bid, removing the need to guess what the second bid was [Vic61]. This auction is incentive-compatible: bidding the true valuation is a dominant strategy. Mechanism design has many important practical applications in addition to auctions. The deferred acceptance algorithm can be used to assign students to schools [GS62, otRSAoS12] and the top trading cycles algorithm to create stable chains of organ donations [RSÜ04]. Google uses large-scale automated auctions to sell the advertisements on its searches [EOS07]. Mechanism design typically assumes a single entity that runs the underlying algorithm and with which all participating agents must interact. In this work we study how this bottleneck can be removed: we want to design mechanisms that can be implemented as fast distributed algorithms. Mechanism design for distributed systems has been studied previously (distributed algorithmic mechanism design (DAMD)) [FSS07]. To our knowledge, all previous work in this area has been on fundamentally global problems, such as routing and leader election. In contrast, we want to study fundamentally local graph problems, where the goal is to solve locally constrained graph problems, such as scheduling conflicting transmissions or ensuring spatial coverage. We believe understanding such mechanisms will be an important building block in future automated systems: as examples, mechanisms have been proposed for resource allocation in IoT networks [SMH17] and simple mechanisms are already used to change consumer behaviour in electric grids, trying to match electricity demand and generation [Jor19]. We introduce the notion of local distributed mechanisms. These are mechanisms for resource allocation in networks that can be implemented as fast distributed algorithms. As an example, consider the problem of computing an independent set of large weight in the mechanism design setting: each agent has a private parameter, its weight, that tells how much utility it gains from being part of the independent set. The goal is to, essentially, arrange a distributed auction to decide which agents should join the independent set. We study distributed algorithms in the standard LOCAL and CONGEST models [Pel00]. A distributed algorithm is local, if its output for each node in the network only depends on the inputs around that node. In contrast, a problem is global, if it requires \Omega(n) time (allowing information to be gathered from the whole network). A distributed auction can be designed based on the result known as Myerson’s lemma [Mye81]. This result states that in single-parameter settings such as the independent set problem, an incentive-compatible mechanism exists exactly when the allocation rule is monotone: increasing an agent’s bid will never cause it to no longer be selected. Crucially the result does not depend in any way on the algorithm that implements the allocation rule: we can use a monotone distributed optimisation algorithm. Myerson’s lemma also gives the formula for the payments. In the context of the independent set mechanism, these are the so-called critical prices: each selected agent has to pay the smallest bid that would have gotten it selected. This is a generalisation of the second-price auction [Vic61]: the algorithm essentially bids optimally for a first-price auction. Local distributed mechanisms are well-suited to dynamic settings, as the input changing in one part of the network only requires re-computation in the local neighbourhood of the change. The mechanism design setting typically requires a centralised authority to run the mechanism – A local mechanism can be run in distributed manner without a single point of failure. Local mechanisms may also be of interest from the centralised perspective, as such mechanisms are by definition decomposable. This allows for a natural parallelisation to multiple entities responsible for running the mechanism. We emphasise that in this work, the assumption is that the communication network itself is not controlled by the strategic agents. Instead, we assume a trusted distributed network that will faithfully execute the distributed implementation of the mechanism based on the values reported by the agents. 1.1 Our contribution We present incentive-compatible approximate mechanisms for several classic graph problems. These mechanisms are implemented as distributed algorithms in the standard LOCAL and CONGEST models of message passing algorithms [Pel00]: These are the first examples of local mechanisms, and our work opens a new direction in the study of distributed graph algorithms. In Section 3 we discuss the generic template for a distributed mechanism given by Myerson’s Lemma (Theorem 1). The key insight is that there is a large class of optimisation problems such that any monotone optimisation algorithm can be automatically transformed into an incentive-compatible mechanism (Theorem 2). An optimisation algorithm is monotone if winning agents cannot lose by bidding more (in maximisation problems). To demonstrate specific mechanisms, we present four mechanisms for classic graph problems. Here n denotes the number of nodes, \Delta the maximum degree, and W the number of possible private values (see third challenge, below). 1. Maximum-weight independent set (MWIS): we present a mechanism that computes a \Delta-approximation and runs in O(\Delta W+\log^{*}n) rounds in the CONGEST model. It is based on the simple greedy algorithm for MWIS [STY03] (Theorem 3). This is a mechanism for which we can compute the critical prices in the CONGEST model. 2. Minimum-weight vertex cover (MWVC): we present a mechanism that computes a 2-approximation and runs in O(\Delta+\log^{*}n) rounds in LOCAL model (Theorem 4). It is based on the local ratio algorithm [BYE81] which has no dependency on W in the running time. 3. Minimum-weight dominating set (MWDS): we present a mechanism that computes a (1+\ln(\Delta+1))-approximation and runs in O(\Delta^{3}W+\log^{*}n) rounds in the LOCAL model (Theorem 5). It is based on the greedy set cover algorithm [Joh74, Chv79]. 4. Slot assignment (a type of weighted graph colouring): we present a mechanism that computes an O(\Delta)-approximation and runs in O(\Delta W+\log^{*}n) rounds (Theorem 6). It uses a simple greedy algorithm. This mechanism showcases the application of Myerson’s Lemma to problems more general than selecting a subset of nodes. All mechanisms have optimal dependency on n, as breaking symmetry requires \Omega(\log^{*}n) rounds [Lin92]. We make the following standard assumption: Mechanisms compute solutions in the sense that bidding the true valuation is a weakly dominant strategy and therefore we assume that the agents bid truthfully. There are four main challenges to designing distributed local mechanisms: First, we must use monotone optimisation algorithms, which does restrict the design space. We mostly use greedy algorithms, but in Section 5 we consider the local ratio algorithm [BYE81] for weighted vertex cover. Second, in order to have a mechanism we must also be able to compute payment function. As we show in Section 3, this can be done with the same complexity as the original algorithm if we don’t put any limits on the size of the messages. However, this can be highly non-trivial when messages have bounded size, such as in the CONGEST model [Pel00]. Computing the critical prices given by Myerson’s Lemma with small messages is the key primitive for implementing mechanisms in the CONGEST model. Third, since we are using monotone algorithms, these are often greedy algorithms. Greedy algorithms typically rely on some sequential ordering in which the agents are considered. If we assume no restrictions on the weights, such weights can form arbitrarily long chains of dependency. To get around this issue, we show in Section 8 how the values can be discretised without losing the incentive-compatibility of the mechanism. Finally, fourth, a mechanism requires tie-breaking. In the centralised setting this is essentially trivial, but it becomes an issue in a distributed setting. We use a natural approach of colouring the input to break symmetry. This ensures that the tie-breaking does not cause new long chains of dependency. We discuss the necessity of colouring as tie-breaking in Section 2.3.2. 1.2 Related work There exist two lines of research that are related to the two aspects of our work. Distributed Algorithmic Mechanism Design (DAMD) was introduced for studying mechanism design for distributed systems [FPS01, FPSS02]. The goal is to have distributed implementations of mechanisms run by the strategic agents themselves. The problems studied in this line of research have been fundamentally global in nature (i.e. computation requires gathering information from the whole network to one point). These include problems such as routing [FPSS02, Rou07], multicast cost sharing [AFK+04, FPS01], scheduling [CG11] or leader election [AGLFS14]. For a survey on DAMD, we refer to a book chapter by Feigenbaum, Schapira, and Schenker [FSS07]. Second, in algorithmic mechanism design there has been some work on mechanisms for local graph problems. Studied problems include weighted vertex cover [Cal15, EGG07] and set cover [DMV03, LSW+10]. These works have been, however, from the centralised perspective. The Vickrey-Clarke-Groves (VCG) mechanism is a generic framework for solving any optimisation problem truthfully [Vic61, Cla71, Gro73]. This mechanism does not have a fast distributed implementation, as it requires the computation of an optimal solution. This requires linear time in the LOCAL model for any reasonable optimisation problem. Distributed graph algorithms for the tasks we study here have received a lot of attention, but most works do not consider any game-theoretic elements. Collet, Fraigniaud, and Penna [CFP18] study the equilibria of simple randomised algorithms for basic symmetry breaking tasks. Hirvonen, Schmid, Schmid, and Chatterjee [HSCS24] present games where best-response dynamics simulate distributed algorithms. The incentive-compatible deferred acceptance algorithm [GS62] for computing a stable matching has a natural implementation as a distributed proposal algorithm. While the operations are local, the algorithm can take \Omega(n^{2}) rounds to converge [OR15] and requires \Omega(n) rounds in the LOCAL model [FKPS10]. To circumvent this, Floréen, Kaski, Polishchuk, and Suomela [FKPS10] and Ostrovsky and Rosenbaum [OR15] study distributed algorithms for computing so-called almost stable matchings. These algorithms are no longer incentive-compatible. Hirvonen and Ranjbaran showed that the variant where one side has common preferences admits a local distributed algorithm [HR24]. 1.3 Open questions Our work demonstrates a connection between algorithmic mechanism design and distributed graph algorithms. This opens many questions regarding the implementation of mechanisms as distributed algorithms. (Q1) Mechanisms with small messages. Our generic distributed algorithm for implementing a mechanism based on a monotone optimisation algorithm requires that each node simulates the algorithm with different input to compute the critical prices. This is easy in the LOCAL model, but requires large messages. In Section 4 we show that a simple combinatorial property of the algorithm allows computing the critical prices in CONGEST. We ask if there exists a general method for computing the critical prices efficiently with small messages? (Q2) Monotonicity in distributed optimisation. Myerson’s lemma automatically gives a distributed mechanism based on any monotone distributed local optimisation algorithm. This means that it is important to understand which distributed algorithms are monotone and which are not. We propose that this is an important property that is of interest in any new proposed distributed optimisation algorithms. Similarly it is interesting to understand which algorithmic design techniques contradict this property. For example, algorithms working on some ”good” subset of the input most likely are not monotone, but, as we have shown in Section 8, grouping nodes by their weight can be effective and retain truthfulness. (Q3) Mechanisms with large degrees. We have studied distributed mechanisms with the assumption that the underlying graph has small maximum degree \Delta. This is because we use colouring for tie-breaking, forcing a linear in \Delta term into the running time of each of our algorithms. This appears to be difficult to avoid, as we discuss in Section 2.3.2. We ask whether there exist distributed mechanisms with non-trivial optimisation guarantees and sublinear-in-\Delta running times? (Q4) Strategy-proof implementations. In the distributed setting it would be preferable if the mechanism itself could be run in a strategy-proof way by the strategic agents themselves. One major challenge for this is that information cannot be propagated without further assumptions. For example, the model may assume that the communication network is complete, removing the issue of an agent withholding information by refusing to pass it along (correctly) [AGLFS14]. It is important to understand what would be the minimal assumptions that would allow strategy-proof execution of distributed graph algorithms. (Q5) Impossibility results for mechanism design. Since distributed mechanisms operate in a narrower design space, it is natural to ask if we can prove improved lower bounds for monotone distributed optimisation. In particular, it would be interesting to understand if the running time dependencies on \Delta and W are necessary. 1.4 Outline We continue in Section 2 by giving the necessary background information in mechanism design and distributed computing. In Section 3 we present the template for designing distributed local mechanisms for optimisation problems. In Sections 4–7 we present four mechanisms for different optimisation problems. In Section 8 we show how real-valued weights can be turned into discrete weights without losing the incentive-compatibility of a mechanism."
https://arxiv.org/html/2411.06545v1,A dynamic auction for multilateral collaboration,We study the problem of multilateral collaboration among agents with transferable utilities. Any group of agents can sign a contract consisting of a primitive contract and monetary transfers among the signatories. We propose a dynamic auction that finds a stable outcome when primitive contracts are gross complements for all participants.,"Equilibrium and stability are important solution concepts in various markets. In the problem of allocating objects among agents, economists often design auctions to produce equilibria or stable outcomes, which are typically both efficient and immune to deviations. When a competitive equilibrium exists in a decentralized market, Scarf (1960) demonstrated that the market does not necessarily converge to an equilibrium.footnote 1footnote 1footnote 1In his examples, Scarf (1960) assumed the rate of change of the price of each commodity is proportional to the excess market demand for that commodity. Starting from any set of prices other than equilibrium, the prices in the examples do not converge to an equilibrium. Consequently, designing an auction to find an equilibrium has the advantage of producing a desirable outcome in a centralized manner, avoiding the potential convergence issues that can arise in decentralized markets. This paper designs an auction for multilateral collaboration, that is, allocating contracts rather than objects among agents. Our work closely relates to and builds upon the study of Rostek and Yoder (2020) (henceforth, RY) on multilateral cooperation with transferable utilities. In their framework, any group of agents can collaborate by signing contracts. Each contract comprises a primitive contract and monetary transfers among the signing agents. RY established that a stable outcome exists when primitive contracts are gross complements for all agents. They defined gross complementarity as the monotonicity of agents’ demand correspondences in the strong set order in an environment with externalities. In the absence of externalities, their definition is equivalent to supermodularity of agents’ valuations and, as shown by Yokote (2023), is the polar opposite of the gross substitutes condition of Kelso and Crawford (1982).footnote 2footnote 2footnote 2See Definition 3 and Lemma 3. Gross complementarity captures the key feature of complementarities in real-life multilateral cooperations. RY also investigated a parallel setting with nontransferable utilities and complementary contracts. Notably, a stable outcome in this setting can be found by a novel one-sided Deferred Acceptance algorithm; however, there is no counterpart of this algorithm in the setting with transferable utilities. In this paper, we devise a dynamic auction for identifying stable outcomes in multilateral cooperation with transferable utilities and gross-complement primitive contracts. There are various multi-item auction formats to choose from.footnote 3footnote 3footnote 3See, for example, Milgrom (2004) and Krishna (2010) for other forms of multi-item auctions. Dynamic price-adjustment procedures offer a key advantage: they do not necessitate information on agents’ valuations, requiring only that agents report their demands at given prices. We assume agents’ valuations take integer values. This assumption aligns with real-world practices, where agents’ values are measured in currency denominations with an indivisible smallest unit (such as pennies or cents). However, unlike RY, we do not consider externalities in agents’ preferences, as an agent in any round of an auction does not know what contracts other agents wish to sign at current prices. Stable outcomes are efficient in the absence of externalities but may not be efficient when externalities are present. No incentive-compatible mechanism exists that can find a stable outcome in the market we study.footnote 4footnote 4footnote 4See Section 4.1. We assume agents report truthfully in our auction. Previous dynamic auctions have been primarily monotone, with prices either ascending or descending. However, in our problem, monetary transfers in a contract sum to zero. Hence, when disagreements arise over a contract among the participants, it is natural for the auctioneer to increase some participants’ payments while decreasing some other participants’ payments within the same contract. In Section 1.2, we briefly explain how we adjust prices in the auction to resolve disagreements in agents’ collaborations. 1.1 Related literature This paper studies multilateral matching in which agents have transferable utilities and any set of agents can collaborate by signing contracts. Hatfield and Kominers (2015) examined a multilateral matching market with continuous contracts. Multilateral matching with nontransferable utilities have been studied by Rostek and Yoder (2020), Bando and Hirai (2021), and Huang (2023). However, the literature has primarily focused on two-sided matching, which matches agents on one side to agents on the other side (e.g., Koopmans and Beckmann, 1957, Shapley and Shubik, 1971, and Kojima et al., 2020, 2024). This problem is closely related to exchange economies with indivisible goods (e.g., Bikhchandani and Mamer, 1997, Danilov et al., 2001, and Baldwin and Klemperer, 2019) and trading networks (e.g., Hatfield et al., 2013, 2021 and Candogan et al., 2021). Walrasian-tâtonnement processes are useful for allocating indivisible items.footnote 5footnote 5footnote 5Walrasian tâtonnement is a series of price adjustments from initial prices in which the price of each item increases or decreases according to whether the excess demand is positive or negative. Demange et al. (1986) designed an ascending-price auction for allocating multiple commodities among agents with unit demands.footnote 6footnote 6footnote 6However, the existence of equilibria does not rely on the assumption of transferable utilities when agents have unit demands; see, e.g., Gale (1984), Quinzii (1984), and Kaneko and Yamamoto (1986). Ausubel (2004) proposed an ascending-price auction in the setting with homogeneous goods and decreasing marginal utilities. Both auctions are efficient and strategy-proof since they produce Vickrey-Clarke-Groves (VCG) outcomes. Based on the existence of a competitive equilibrium under gross substitutability (Kelso and Crawford, 1982), Gul and Stacchetti (2000) proposed an ascending-price auction that finds an equilibrium for allocating gross-substitute commodities; Ausubel (2006) proposed a strategy-proof ascending-price auction that yields VCG outcomes for this problem. Sun and Yang (2006, 2009) generalized gross substitutability in an economy with two groups of commodities by allowing cross-group complements, and provided a double-track auction for their economy. Ascending-price auctions have been successfully used for the sale of radio spectrum licenses in many countries; see, for example, Milgrom (2000, 2004), Klemperer (2004), and McAfee et al. (2010). Recent studies provided positive results on the existence of equilibria in economies without the assumption of transferable utilities; see Fleiner et al. (2019), Baldwin et al. (2023), Rostek and Yoder (2023), and Nguyen and Vohra (2024). When utilities are transferable, the single improvement property (Gul and Stacchetti, 1999) of gross substitutability is crucial for auction design. When utilities are imperfectly transferable, Schlegel (2022) showed that the law of aggregate demand is needed to recover the single improvement property. 1.2 Mitigating disagreements In each round of our auction, each agent reports all her demand sets at the current prices, and the auctioneer mitigates disagreements among agents in a manner similar to the Walrasian-tâtonnement process. At given prices that specify the payments in all primitive contracts, a primitive contract is called partially agreeable if it is included in a demand set of some participants but not included in any demand set of some other participants. A primitive contract is called strongly demanded by some agent at given prices if it is included in every demand set of this agent. If the current prices admit a competitive equilibrium, the outcome should involve all primitive contracts that are strongly demanded by some agent but should not involve any partially agreeable primitive contract. Hence, if there is a partially agreeable primitive contract w that is strongly demanded by Ana but not included in any demand set of Bob, there is no competitive equilibrium at the current prices, and the auctioneer should resolve the disagreement between Ana and Bob on the primitive contract w. This is an elementary type of disagreement in the market, and it is natural for the auctioneer to mitigate the disagreement by increasing Ana’s payment in the primitive contract w by one unit and decreasing Bob’s payment in w by one unit. If Ana’s valuation takes integer values, at the new prices the primitive contract w is still in some demand set of Ana,footnote 7footnote 7footnote 7Since Ana’s valuation takes integer values, at the original prices each of Ana’s optimal choices yields a utility at least one unit higher than the utility of her any suboptimal choice. Hence, at the new prices Ana’s original optimal choices yield a utility no less than the utility of her any other choices. and Bob may demand some set containing w as the cost of this set decreases by one unit. This price adjustment reduces the value of a Lyapunov function, which was used in the market with divisible goods (see, e.g., Arrow and Hahn, 1971 and Varian, 1981) and introduced by Ausubel (2006) to the problem with indivisibilities. The minimum of the Lyapunov function corresponds to a competitive equilibrium.footnote 8footnote 8footnote 8See Section 2.2. We show that, under gross complementarity, the general type of disagreements that the auctioneer should resolve arise from a structure called complements chain that connects a partially agreeable primitive contract and a strongly demanded one.footnote 9footnote 9footnote 9See Proposition 1. For instance, suppose at given prices there is an Ana-w-Bob-w^{\prime}-Carol complements chain, where the primitive contract w is not in any demand set of Ana, and Carol strongly demands w^{\prime}. The primitive contract w is a complement to w^{\prime} for Bob at the current prices in the sense that all his demand sets including w^{\prime} also include w. An equilibrium of the current prices must include the primitive contract w^{\prime} as Carol strongly demands it and must exclude w as Ana does not demand it. Then we know that the current prices do not admit an equilibrium since Bob signs w^{\prime} only when he also signs w. Hence, the auctioneer should resolve the disagreement from this complements chain. In our auction, the auctioneer mitigate the disagreement by decreasing Ana’s payment in w by one unit, increasing Bob’s payment in w by one unit, decreasing Bob’s payment in w^{\prime} by one unit, and increasing Carol’s payment in w^{\prime} by one unit. This price adjustment reduces the value of the Lyapunov function by one unit. In each round of our auction, the auctioneer identifies at least one complements chain at the current prices and adjusts the prices of primitive contracts in the complements chain. Since the value of the Lyapunov function is reduced by at least one unit in each round of the auction, the auction converges to a competitive equilibrium in a finite number of steps. The remainder of this paper is organized as follows. Section 2 introduces the model of multilateral cooperation with transferable utilities. Section 3 analyzes agents’ demands under gross complementarity and provides a procedure for verifying whether the current prices admit an equilibrium. We propose the dynamic auction in Section 4. Omitted proofs are provided in the Appendix."
https://arxiv.org/html/2411.06312v1,Multidimensional Screening with Rich Consumer Data,"A multi-product monopolist faces a buyer who is privately informed about his valuations for the goods. As is well-known, optimal mechanisms are in general complicated, while simple mechanisms—such as pure bundling or separate sales—can be far from optimal and do not admit clear-cut comparisons. We show that this changes if the monopolist observes sufficiently rich data about the buyer’s valuations: Now, pure bundling always outperforms separate sales; moreover, there is a sense in which pure bundling performs essentially as well as the optimal mechanism. To formalize this, we characterize how fast the corresponding revenues converge to the first-best revenue as the monopolist’s data grows rich: Pure bundling achieves the same convergence rate to the first-best as optimal mechanisms; in contrast, the convergence rate under separate sales is suboptimal.","A classic problem in economic theory is how a multi-product monopolist should sell its goods to a buyer whose valuations for the goods are unknown to the monopolist. While of clear economic relevance from both a positive and normative perspective, this problem—and related multi-dimensional screening problems—is well-known to be quite intractable. Even in simple subcases (e.g., two goods for which the buyer has independently distributed, additive values), the seller’s optimal mechanism can be difficult to characterize and look complicated: For instance, it may require the buyer to choose between a continuum of differently priced lotteries over product bundles (e.g., Daskalakis, Deckelbaum, and Tzamos, 2017).111Finding the optimal mechanism can also be computationally intractable (e.g., Daskalakis, Deckelbaum, and Tzamos, 2014), and it can exhibit counterintuitive features, e.g., nonmonotonic revenues with respect to first-order stochastic dominance increases in buyer values (Hart and Reny, 2015). At the same time, the multi-product selling mechanisms used in practice are often quite simple: For example, many firms only present buyers with some limited number of deterministic product bundles, or even engage in pure bundling, i.e., offer only the grand bundle of all products at a take-it-or-leave-it price. In this paper, we provide a novel perspective on the use of such simple multi-product selling mechanisms. We consider a revenue-maximizing monopolistic seller who is endowed with a finite set G of indivisible goods. There is one potential buyer, whose valuations for the goods are summarized by a type vector \theta\in\mathbb{R}_{++}^{|G|} (drawn from an arbitrary prior distribution) and whose payoffs are additive across goods. (Section 6.1 extends the analysis to nonadditive buyer utilities, negative valuations, and seller production costs). The realization of \theta is only known to the buyer. However, as in the literature on price discrimination, we assume that the seller observes some information about the buyer’s type that she can use in designing a selling mechanism. For example, the seller might observe various buyer characteristics (or noisy signals thereof) that are correlated with \theta within the population from which the buyer is drawn. The question we ask is: Are there simple mechanisms that perform well at exploiting such information, and, if so, which ones? If the seller’s information is arbitrary, the comparison between different simple mechanisms is not clear-cut: For example, it is well-known that, depending on the seller’s posterior, pure bundling may yield higher or lower revenue than separate sales (i.e., setting a separate price for each individual good). Moreover, both these simple mechanisms are in general far from optimal. However, the key insight of our paper is that there is a sharp answer if the seller’s information is sufficiently precise: In this case, we show that the seller is always better off using pure bundling than separate sales; what is more, we formalize a sense in which pure bundling performs essentially as well as the optimal mechanism. The assumption of precise seller information may, for instance, reflect the proliferation of consumer data to which retailers have access in digital marketplaces.222Retailers increasingly use sophisticated tools to draw inferences about consumers’ preferences from data such as their browsing and search history, geolocation, and operating system. Retailers have also started to personalize selling mechanisms based on such data, not only via outright price discrimination but also more subtle channels such as personalized discounts, add-on offerings, or product displays (e.g., Hannak, Soeller, Lazer, Mislove, and Wilson, 2014; Gonzaga, 2018). To tractably formalize such information, our baseline model assumes that the seller observes n independent signals from some distribution P_{\theta} that depends on \theta. (Section 6.1 discusses more general settings). Thus, the seller’s information is more precise the richer the amount of data n, and we are interested in the case where n is large. Crucially, to evaluate and compare the performance of different classes of mechanisms, we do not explicitly derive the seller’s revenues at any particular n. Instead, we take a convergence rate approach: We analyze how fast the gap between the seller’s expected revenue and the first-best (i.e., known type) revenue vanishes as n grows large. This provides a parsimonious way of comparing revenues across different classes of mechanisms under rich consumer data: Mechanisms with a faster convergence rate to the first-best yield higher expected revenues than mechanisms with a slower convergence rate at all large enough n. As a one-dimensional statistic, convergence rates also succinctly characterize how revenues at large n depend on features of the environment, such as the seller’s signal distribution P_{\theta}. Most importantly, while optimal mechanisms by definition approximate the first-best revenue at the fastest rate, this does not rule out that some simple (but suboptimal) mechanisms may achieve this same optimal convergence rate. If this is the case, this suggests that a seller who observes rich consumer data loses very little from using such simple mechanisms rather than the optimal mechanism. In particular, our main result (Theorem 3.1) shows that, regardless of whether the seller optimizes over general selling mechanisms or is restricted to pure bundling, her expected revenue converges to the first-best equally fast as n grows large: In both cases, the revenue gap relative to the first-best vanishes as fast as \lambda^{G}\sqrt{\frac{\ln n}{n}}. Here, the coefficient \lambda^{G} captures how fast the seller’s posterior standard deviation of the value \sum_{g\in G}\theta_{g} of the grand bundle decays; this coefficient depends on the signal distribution P_{\theta} only through a standard statistical informativeness measure, Fisher information. In contrast, if the seller optimizes over separate sales mechanisms, Theorem 3.1 shows that her expected revenue converges to the first-best more slowly: Now, the revenue gap relative to the first-best vanishes as fast as \sum_{g\in G}\lambda^{g}\sqrt{\frac{\ln n}{n}}, where the coefficients \lambda^{g} capture how fast the seller’s posterior standard deviation of the values \theta_{g} of each individual good decays. The coefficient \sum_{g\in G}\lambda^{g} is greater than the optimal coefficient \lambda^{G}: By subadditivity of standard deviations, the sum of the standard deviations of all \theta_{g} always exceeds the standard deviation of the sum \sum_{g\in G}\theta_{g}. To interpret, note that in the limit as n\to\infty, both pure bundling and separate sales approximate the first-best revenue; however, at any finite n, both mechanisms in general yield strictly lower revenues than the optimal mechanism, which may screen the buyer via (possibly elaborate) menus of bundles or lotteries over bundles. This leaves open the possibility that, even at large n, the revenue gap relative to the first-best under these simple mechanisms may be many times bigger than under the more complicated optimal mechanism. The key implication of Theorem 3.1 is that under pure bundling this is not the case: Since pure bundling and the optimal mechanism approximate the first-best revenue equally fast, using the optimal mechanism rather than pure bundling reduces the revenue gap to the first-best by only a negligible amount at all large enough n. For example, as we discuss, the revenue gain from using the optimal mechanism rather than pure bundling is smaller than the gain from having access to even an arbitrarily small fraction of additional signals. In contrast, under separate sales, Theorem 3.1 implies that the revenue gap to the first-best at all large enough n is \sum_{g\in G}\lambda^{g}/\lambda^{G} times as big as under the optimal mechanism and pure bundling, where depending on parameters this ratio can be arbitrarily large. Thus, even though at small n, the seller’s revenue can be higher under separate sales or pure bundling, pure bundling always outperforms separate sales when n is large enough. A potential concern in assessing the economic relevance of Theorem 3.1 is whether it requires such unrealistically rich data n that the seller can almost perfectly discriminate between different buyer types. This concern can be mitigated by analyzing numerical examples. For instance, in a Gaussian environment, Section 3.2 finds that pure bundling starts to approximate the optimal mechanism and to significantly outperform separate sales at n where the corresponding revenues are still only a moderate fraction of the first-best. The proof of Theorem 3.1 first exploits the Bernstein-von Mises theorem to reduce the analysis to a setting in which the seller’s posterior is deterministic and Gaussian. The analysis is then based on three main insights that we illustrate in Section 4: First, focusing on the single-good case, we shed light on how rich data affects the tradeoff between two forms of revenue loss that the seller incurs away from the first-best benchmark: Losses on the intensive margin (due to having to shade the price relative to the known type case) and on the extensive margin (due to the probability that the buyer is unwilling to pay the seller’s chosen price). We show that, as the seller’s posterior grows more and more precise, these two margins become strikingly imbalanced: The seller optimally prices the good in such a way that her revenue losses are driven almost entirely by the intensive margin; in contrast, extensive-margin losses become negligible at large n. This dominance of the intensive margin lies at the heart of why, in the multi-good setting, the convergence to the first-best under pure bundling cannot be improved upon by general deterministic mechanisms. Under the latter, the seller can engage in mixed bundling, i.e., screen the buyer by offering a menu of product bundles. Relative to pure bundling, this benefits the seller by extracting revenue from buyers who are unwilling to purchase the grand bundle. However, since this benefit only affects the extensive margin, it has a negligible impact on the revenue gap relative to the first-best at large n.333Relative to pure bundling, mixed bundling also in principle allows the seller to reduce the magnitude of intensive-margin losses by appropriately raising the price of the grand bundle. However, we show that this increases extensive-margin losses by an order of magnitude that more than outweighs the intensive-margin gains. Second, we show that intensive-margin losses under single-good monopoly vanish as fast as the (scaled) standard deviation of the seller’s posterior about the buyer’s type. In the multi-good case, this implies that comparing the revenue gap to the first-best at large n under pure bundling vs. separate sales boils down to comparing the seller’s posterior standard deviation about the value \sum_{g\in G}\theta_{g} of the grand bundle vs. the sum of her posterior standard deviations about each \theta_{g}. As noted, the former is smaller than the latter, as standard deviation is subadditive. At a high level, this relates to the classic intuition (e.g., Adams and Yellen, 1976; Armstrong, 1999) that bundling reduces the seller’s uncertainty relative to separate sales, as overestimating the valuations of some goods but underestimating those of others can cancel out when estimating the value of the grand bundle. However, for this intuition to be valid in our setting, it is crucial to show that the relevant measure of the seller’s uncertainty is standard deviation, as other measures of uncertainty (e.g., variance) are not subadditive. Finally, we show that pure bundling achieves the same convergence rate as the optimal mechanism, which may additionally involve randomization. To get around the aforementioned challenge that optimal mechanisms are difficult to characterize, we introduce a more tractable relaxed problem that yields an upper bound on the seller’s optimal revenue. We then prove that even this upper bound converges to the first-best no faster than the pure bundling revenue. The relaxed problem partitions the type space into line segments and imposes incentive compatibility only within each segment. As Section 5 explains, optimal mechanisms in this problem are tractable to characterize and take an “almost deterministic” form. Moreover, we show that by carefully choosing the line segments, we can ensure that the pure bundling revenue in the relaxed problem is the same as in the original problem. Based on this, similar arguments to the comparison of pure vs. mixed bundling yield that pure bundling achieves the optimal convergence rate. 1.1 Related Literature Our paper relates to the large classical literature on multi-dimensional screening (Adams and Yellen, 1976; McAfee, McMillan, and Whinston, 1989; Armstrong, 1996; Rochet and Choné, 1998; Manelli and Vincent, 2006, 2007, among many others). Several of these papers develop general methodologies to characterize optimal mechanisms. However, as discussed in the Introduction, optimal mechanisms are complicated unless specific conditions are imposed on type distributions. Some papers derive conditions under which pure bundling is optimal (e.g., Pavlov, 2011; Daskalakis, Deckelbaum, and Tzamos, 2017; Haghpanah and Hartline, 2021; Ghili, 2023).444Other recent work (e.g., Yang, 2023; Bergemann, Bonatti, Haupt, and Smolin, 2021) provides conditions under which more general menus of bundles (e.g., nested bundling) are optimal. Our setting allows for general type distributions, and pure bundling is in general suboptimal at any n. Another recent strand of this literature characterizes optimal mechanisms in settings where a seller maximizes her worst-case expected revenue across a set of possible type distributions (e.g., Carroll, 2017; Deb and Roesler, 2023; Che and Zhong, 2024). Depending on the structure of this set, optimal mechanisms can take simple forms such as separate sales or pure bundling. In contrast to the above papers, we depart from the criterion of exact optimality. Instead, we provide a rationale for pure bundling based on the idea that this simple mechanism allows sellers who observe rich data about buyers’ types to approximate the first-best revenue at the optimal rate. This contrasts with a notion of approximate optimality that is often studied in computer science: worst-case guarantees, i.e., lower bounds on the performance ratio of simple vs. optimal mechanisms that are uniform with respect to type distributions (or other features of the environment; see the survey by Roughgarden and Talgam-Cohen, 2019).555Our question is also different from work in computer science that studies how fast single- or multi-good sellers who observe samples from an unknown distribution of consumer types can approximate the second-best (i.e., known distribution) revenue (e.g., Cole and Roughgarden, 2014; Gonczarowski and Weinberg, 2021). Neither pure bundling, separate sales, nor more generally the class of all deterministic mechanisms admit a non-zero worst-case guarantee (Hart and Nisan, 2019).666However, positive worst-case guarantees can be derived under more restrictive assumptions on the environment (Hart and Nisan, 2017; Babaioff, Immorlica, Lucier, and Weinberg, 2020). While we fix a set of goods G, Armstrong (1999) and Bakos and Brynjolfsson (1999) study the many-good limit. Under independent and additive valuations, they show that pure bundling approximates the first-best as |G|\to\infty, because the value of the grand bundle becomes deterministic by a law of large numbers; in contrast, separate sales does not, because the value of individual goods is random.777Fang and Norman (2006) provide joint non-asymptotic conditions on the number of goods and type distribution under which pure bundling outperforms separate sales and vice versa. In our setting, bundling also reduces uncertainty relative to separate sales, but in a different sense: Regardless of the type distribution, both bundling and separate sales approximate the first-best as the amount of consumer data grows large, because this data allows the seller to learn the valuations for all goods; however, we show that the rate of convergence under bundling is always faster than under separate sales. More importantly, we prove that the convergence rate under bundling is the same as under the optimal mechanism. As noted, a key step of our proof is to upper-bound the optimal revenue via a relaxed problem that partitions buyer types into line segments. Exploiting the linear structure of this problem, Proposition 5.1 shows that optimal mechanisms involve at most one random allocation (that is consumed only by types with binding IR). This almost deterministic structure makes optimal mechanisms tractable to analyze and is notable, as even under one-dimensional types, optimal mechanisms with multi-dimensional allocations in general rely on more extensive randomization. As we discuss, this result may be useful in economic settings beyond multi-good monopoly. Haghpanah and Hartline (2021) impose a stochastic monotonicity assumption on type distributions and use this to construct a different relaxed problem: There, types are decomposed into (not necessarily linear) one-dimensional paths and the optimal mechanism is pure bundling.888For related decomposition approaches, see also Armstrong (1996) and Wilson (1993). More broadly, we contribute to a resurgent literature on price discrimination, which studies the implications of a seller’s ability to condition selling mechanisms on additional information about buyers (e.g., Bergemann, Brooks, and Morris, 2015; Haghpanah and Siegel, 2023). Motivated by the availability of rich consumer data in digital marketplaces, some papers focus on sellers with very precise information (e.g., Rhodes and Zhou, 2024). In the context of multi-good monopoly, we highlight that sufficiently precise information can serve as a rationale for particular classes of simple selling mechanisms, such as pure bundling. An important step in our analysis is the insight that, under single-good monopoly, such information leads to a stark imbalance between intensive and extensive-margin considerations. Finally, convergence rates to a perfect information benchmark have been used as a performance measure in other contexts: In single-agent decision problems, Moscarini and Smith (2002) study how fast an agent who observes many i.i.d. signals about the state and follows an optimal strategy can approximate the perfect information payoff; they use this to define a ranking over signal structures. Subsequent work conducts related exercises in other learning settings.999This includes multi-agent learning and misspecified learning (e.g., Harel, Mossel, Strack, and Tamuz, 2021; Frick, Iijima, and Ishii, 2023, 2024b). Aside from our different economic environment and research question, a technical departure from these papers is that our setting features continuous actions and types. This necessitates different mathematical techniques from the large-deviation theory tools that apply in the finite state and action settings of these papers. This technical difference also applies relative to our previous work, Frick, Iijima, and Ishii (2024a), which uses convergence rates to evaluate the performance of simple but suboptimal contracts in a moral hazard problem where a principal observes rich monitoring data about an agent’s action. The “hidden action” nature of that problem makes the analysis quite different from the current “hidden type” setting in other respects as well.101010More broadly, convergence rates to full efficiency have been used to study the performance of simple mechanisms or strategies in the context of trade in large markets (e.g., Rustichini, Satterthwaite, and Williams, 1994) and repeated games with patient players (Sugaya and Wolitzky, 2024)."
https://arxiv.org/html/2411.06069v1,Model Selection for Average Reward RL with Application to Utility Maximization in Repeated Games,"In standard RL, a learner attempts to learn an optimal policy for a Markov Decision Process whose structure (e.g. state space) is known. In online model selection, a learner attempts to learn an optimal policy for an MDP knowing only that it belongs to one of M>1 model classes of varying complexity. Recent results have shown that this can be feasibly accomplished in episodic online RL. In this work, we propose MRBEAR, an online model selection algorithm for the average reward RL setting which is based on the idea of regret balancing and elimination. The regret of the algorithm is in \tilde{O}(MC_{m^{*}}^{2}\mathsf{B}_{m^{*}}(T,\delta)) where C_{m^{*}} represents the complexity of the simplest well-specified model class and \mathsf{B}_{m^{*}}(T,\delta) is its corresponding regret bound. This result shows that in average reward RL, like the episodic online RL, the additional cost of model selection scales only linearly in M, the number of model classes. We apply MRBEAR to the interaction between a learner and an opponent in a two-player simultaneous general-sum repeated game, where the opponent follows a fixed unknown limited memory strategy. The learner’s goal is to maximize its utility without knowing the opponent’s utility function. The interaction is over T rounds with no episode or discounting which leads us to measure the learner’s performance by average reward regret. In this application, our algorithm enjoys an opponent-complexity-dependent regret in \tilde{O}(M(\mathsf{sp}(h^{*})B^{m^{*}}A^{m^{*}+1})^{\frac{3}{2}}\sqrt{T}), where m^{*}\leq M is the unknown memory limit of the opponent, \mathsf{sp}(h^{*}) is the unknown span of optimal bias induced by the opponent, and A and B are the number of actions for the learner and opponent respectively. We also show that the exponential dependency on m^{*} is inevitable by proving a lower bound on the learner’s regret.","Most work in average reward-based reinforcement learning assumes that the underlying model class—i.e., state space and actions—is known, even if the probabilities that define rewards and the transitions between states must be learned. However, in practice the definitions of the state space must be specified. This leads to a tradeoff between the richness of the model and the hardness of learning within it. A model class that encodes every possible observation in its states will have a very large set of states, making it very challenging to learn an optimal policy; conversely, a model class that encodes too little information into its states may not be rich enough to accurately describe the behavior of the domain. One way to resolve this tradeoff is by selecting the model class during learning based on interactions with the environment. In the episodic regret setting, it has been shown that online model selection using M different model classes (at least one of which is assumed to be well-specified) can attain a regret bound that is only a factor of M worse than learning using the simplest well-specified model class from the collection. Since the most general model class can have regret bounds that are superlinearly (often exponentially) greater than those of the optimal model class, this results in an asymptotic improvement over the naive approach of learning in a single complex model class that is guaranteed to be well-specified. This asymptotic improvement is attained by using a meta-algorithm which selects one model class per time step, and runs one step of a learning algorithm on the selected model class. Over time, an optimal policy is learned for a well-specified model class. The meta-algorithm typically uses a criterion either from a bandit algorithm or a technique known as regret balancing (Abbasi-Yadkori et al.,, 2020; Pacchiano et al.,, 2020) to achieve its guarantee. Main Question. The main question that we ask is whether we can achieve a similar regret bound for online model selection in the average reward setting; namely, one which is only a linear factor of M worse than the regret bound of the optimal model class. We answer this question affirmatively, by designing an algorithm based on regret balancing called MRBEAR. The average reward setting presents challenges that are not present in the episodic setting. Any planning effects from a policy must be fully realized by the end of an epoch; so running a base learning algorithm in a selected model class for a single epoch is sufficient to generate an accurate sample of a policy’s performance. In contrast, a single iteration (i.e., a single action in the MDP) is unlikely to give an accurate sample of a policy’s performance; but since the average reward setting has no notion of episodes, the algorithm must decide on a number of iterations to commit to a model class after each selection. This results in a stricter notion of regret (3). Application. We apply our model selection result to the repeated game setting against an opponent with limited memory, with the goal of maximizing the learner’s cumulative utility. This goal, together with having no discounting or reset during the T interactions, leads us to use average reward regret to evaluate the learner’s performance. The opponent can condition their choice in each game on the past 0\leq m^{*}<M plays, where the upper limit M is known to us but the true limit m^{*} is not. A naive approach would be to learn a policy in the induced Markov Decision Process (MDP) class where each state contains the last M choices; however, this would result in an MDP with exponentially more states than the true induced MDP, which has one state for each possible sequence of m^{*} past choices. Model selection instead allows us to learn the opponent’s memory limit simultaneously with learning an optimal policy, by treating the state space induced by each potential memory limit as a separate model class. Average regret with respect to the optimal policy against the opponent is a more natural performance measure in this setting than episodic regret or PAC guarantees, since all of the rewards during the course of learning are important. When both players are assumed to have unbounded memory and common knowledge of rationality (i.e., both are utility-maximizers, and know that the other is a utility-maximizer, and know that the other knows, etc.), an optimal strategy for each agent can be found using backward induction(Shoham and Leyton-Brown,, 2008). However, in this work we relax both assumptions. Common knowledge of rationality in practice requires both agents to know the others’ utility; in practice, this will rarely be true. We further assume that the opponent’s policy depends only on a fixed number of past actions taken in the game, but that this memory limit is not known to the learner. Thus, our problem can be understood as finding the best response, in the repeated game, to an opponent’s fixed, finite-memory strategy. Note that we are interested in finding the optimal policy for the full repeated game, which in general might differ from a policy that best-responds to the opponent’s action in each stage game individually. If we knew the opponent’s memory bound, then any procedure that learned a policy with a sublinear regret bound would eventually converge to a best response to the opponent’s policy in the full repeated game. Since this memory bound is unknown, we learn it simultaneously with learning a policy by solving a model selection problem using MRBEAR. The rest of the paper is structured as follows. After surveying related work and defining our setting, we describe the base learning algorithm that we use in section 2.2. In section 3 we describe our proposed model selection algorithm. Section 4 proves a regret bound for our algorithm that depends on the unknown complexity of the MDP. Section 5 specializes our results to the repeated-game setting, giving bounds that depend on the unknown memory and complexity of the opponent’s strategy, including a minimax lower bound on the performance of any algorithm. We wrap up with a discussion of our conclusions in section 6. 1.1 Related Work Average Reward. Auer et al., 2008a achieve a regret bound of O(DS\sqrt{AT}) with their seminal algorithm UCRL2. Further work improves the dependency on the diameter D by considering different techniques (Fruit et al.,, 2020; Bourel et al.,, 2020; Wei et al.,, 2020; Ortner,, 2020; Talebi and Maillard,, 2018). Diameter based algorithms usually assume communicating MDPs. However, Fruit et al., 2018a propose a UCRL-based algorithm for weakly communicating MDPs. When rewards are bounded, it is not hard to show that \mathsf{sp}(h^{*})\leq D. There even exist weakly communicating MDPs with infinite diameter, but with finite \mathsf{sp}(h^{*}). These show that results that capture \mathsf{sp}(h^{*}) tend to be stronger (Fruit et al., 2018b, ; Zhang and Ji,, 2019; Boone and Zhang,, 2024). The leading algorithm is the very recently introduced PMEVI, which efficiently achieves the optimal regret bound of \tilde{O}(\sqrt{\mathsf{sp}(h^{*})}SAT) without prior knowledge of \mathsf{sp}(h^{*}) (Boone and Zhang,, 2024). Model Selection. There is a rich literature on model selection for statistical learning (Vapnik,, 2006; Lugosi and Nobel,, 1999; Massart,, 2007; Bartlett et al.,, 2002) and online learning (Freund and Schapire,, 1997; Foster et al.,, 2017). Abbasi-Yadkori et al., (2020); Pacchiano et al., (2020) use online misspecification tests, like checking whether the empirical regret satisfies known bounds, to determine which of the model classes are well specified. Some work assumes nested structure on the model classes (Ghosh et al.,, 2021; Foster et al.,, 2019), but some others do not (Agarwal et al.,, 2017). The cost of model selection under different assumptions can be either multiplicative (Pacchiano et al.,, 2020) or additive (Ghosh et al.,, 2021) to the regret of the best well-specified model class (Marinov and Zimmert,, 2021; Krishnamurthy et al.,, 2024). All of the above papers analyze episodic regret; to the best of our knowledge, there is no work in model selection for the average reward setting. Our results are a first step in extending the model selection approach to the average reward setting. Learning in Markov Games. Another related direction to this work is learning in stochastic games (Wei et al.,, 2017; Zhong et al.,, 2021; Xie et al.,, 2020). The setting in this paper differs from the previous works either in the notion of regret, assuming different levels of memory for the opponent, or the measure of complexity. An attractive recent line of research studies regret minimization when the opponent runs online learning algorithms (Brown et al.,, 2024; Deng et al.,, 2019; Braverman et al.,, 2018). In our work, we bound average reward regret, a more demanding benchmark which better captures utility maximization in repeated play than external regret. A recent paper by Assos et al., (2024) shows utility maximization against an opponent deploying multiplicative weights updates (MWU) algorithms is tractable in zero-sum repeated games, while there is no Fully Polynomial Time Approximation Scheme (FPTAS) for utility maximization against a best-responding opponent in general sum games. We circumvent this impossibility result by assuming that the opponent has limited memory. Another very recent work is (Nguyen-Tang and Arora,, 2024) in which a bounded memory for the opponent is assumed and the performance of the learner is measured by policy regret (Arora et al.,, 2018). However Nguyen-Tang and Arora, (2024) model the interaction as a Stackelberg game, in episodic Markov Games. We study average reward in simultaneous repeated games which are arguably more complicated settings. Zhong et al., (2021) use reinforcement learning to solve the Stackelberg equilibrium for myopic followers. Wei et al., (2017) study stochastic games under average reward using the diameter as the complexity measure. Our own work proves bounds based on \mathsf{sp}(h^{*}) which are tighter than those based on diameter. A more detailed literature review is in appendix A."
https://arxiv.org/html/2411.05990v2,"Game-theoretic LLM:
Agent Workflow for Negotiation Games","This paper investigates the rationality of large language models (LLMs) in strategic decision-making contexts, specifically within the framework of game theory. We evaluate several state-of-the-art LLMs across a spectrum of complete-information and incomplete-information games. Our findings reveal that LLMs frequently deviate from rational strategies, particularly as the complexity of the game increases with larger payoff matrices or deeper sequential trees.To address these limitations, we design multiple game-theoretic workflows that guide the reasoning and decision-making processes of LLMs. These workflows aim to enhance the models’ ability to compute Nash Equilibria and make rational choices, even under conditions of uncertainty and incomplete information. Experimental results demonstrate that the adoption of these workflows significantly improves the rationality and robustness of LLMs in game-theoretic tasks. Specifically, with the workflow, LLMs exhibit marked improvements in identifying optimal strategies, achieving near-optimal allocations in negotiation scenarios, and reducing susceptibility to exploitation during negotiations. Furthermore, we explore the meta-strategic considerations of whether it is rational for agents to adopt such workflows, recognizing that the decision to use or forgo the workflow constitutes a game-theoretic issue in itself.Our research contributes to a deeper understanding of LLMs’ decision-making capabilities in strategic contexts and provides insights into enhancing their rationality through structured workflows. The findings have implications for the development of more robust and strategically sound AI agents capable of navigating complex interactive environments. Code and data supporting this study are available at https://github.com/Wenyueh/game_theory.","Large Language Models (LLMs), such as GPT-4 and Claude, have achieved remarkable progress in natural language understanding and generation zhang2024supervised ; ding2024hybrid ; fang2024large , driving advancements in fields ranging from conversational AI dam2024complete ; dong2023towards to content creation liang2024monitoring ; shao2024assisting and agentic task delegation guo2024embodied ; agashe2023evaluating ; xi2023rise . LLMs are increasingly integrated into applications that influence everyday activities, such as planning, acting, and decision-making. Therefore, the ability of LLMs to navigate complex situations has significant implications for their deployment in applications requiring strategic interaction, such as automated negotiations, economic modeling, and collaborative problem-solving bianchi2024well ; horton2023large ; li2024econagent ; chen2024comm ; li2023metaagents . Despite the wide exploration and utilization, LLM’s capacity for rational behavior, particularly in strategic settings represented by game theory, remains an open question leng2023llm ; stade2024large ; wu2024shall ; de2023emergent ; lan2023llm . In this context, rationality implies an agent’s ability to make decisions that maximize expected utility based on available information, an essential component of intelligent and adaptive decision-making. In the realm of game theory, rational agents are expected to act strategically, considering not only their own preferences but also the potential actions and preferences of others. This is especially critical in incomplete-information games, where uncertainty about other players’ information necessitates sophisticated reasoning and belief updating. This paper investigates the capacity of LLMs to behave rationally in game-theoretic scenarios and explores methodologies to enhance their rational decision-making capabilities. We begin by assessing the performance of several state-of-the-art LLMs, including Claude-3.5 Sonnet, Claude-3 Opus, GPT-4o and o1 zhong2024evaluation , in both complete-information and incomplete-information games such as the Prisoner’s Dilemma, Battle of the Sexes, the Escalation Game, and Deal-or-No-Deal lewis2017deal , presented in Figure 1. Our analysis reveals LLMs often deviate from rational strategies, particularly as the complexity of the game increases with larger payoff matrices or deeper sequential trees (Section 4). They also exhibit a lack of robustness to noise and uncertainty, leading to suboptimal outcomes (Section 6). To address these limitations, we introduce a novel approach by proposing game-theory-inspired workflows specifically designed to guide the reasoning and decision-making processes of LLMs. This is the first attempt to systematically integrate classic game-theoretic strategies into LLM-based agent workflow, aiming to enhance their rational behavior and decision-making capabilities in strategic settings. These workflows incorporate principles such as Dominant Strategy Search, which involves identifying strategies that yield the highest payoff regardless of the opponent’s actions; Backward Induction, a method of solving extensive-form games by analyzing them from the end states backward to the initial decision nodes to determine optimal strategies; and Bayesian belief updating, which allows agents to refine their beliefs about other players’ valuations based on observed actions and signals during the game. Cringed on these well-defined and well-studied game-theoretic methods, we design algorithms to guide the behavior and thinking process of LLM-based agents. Additionally, we integrate fairness considerations like envy freeness and pareto optimality, which promote equitable and efficient outcomes in negotiations by ensuring that no agent prefers another agent’s allocation to their own and that no improvements can be made without making at least one agent worse off. Contribution Summary • Comprehensive Evaluation of LLMs in Strategic Games and Identification of Rationality Limitations in LLMs (Section 4 and 6): Through empirical analysis, we uncover that LLMs often fail to behave rationally in strategic settings, exhibiting a lack of robustness to noise and randomness. • Design of Game-Theory-Inspired Workflows (Section 4.4 and 5.2): We develop novel workflows inspired by game-theoretic concepts to guide the reasoning and decision-making processes of LLMs, incorporating analysis and algorithms from classic game theory. • Emerging Research Direction (Section 5.5.3 and 5.6): Through the application of workflows, we identify a promising new research direction in meta-strategy, specifically focusing on the decision of whether to adopt a workflow and, potentially, which workflow to employ in varying scenarios. Figure 1: Game-theoretic Landscape Investigated in this Paper."
