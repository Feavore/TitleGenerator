URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.10393v1,Guaranteed Bounds on Posterior Distributions of Discrete Probabilistic Programs with Loops,"We study the problem of bounding the posterior distribution of discrete probabilistic programs with unbounded support, loops, and conditioning. Loops pose the main difficulty in this setting: even if exact Bayesian inference is possible, the state of the art requires user-provided loop invariant templates. By contrast, we aim to find guaranteed bounds, which sandwich the true distribution. They are fully automated, applicable to more programs and provide more provable guarantees than approximate sampling-based inference. Since lower bounds can be obtained by unrolling loops, the main challenge is upper bounds, and we attack it in two ways. The first is called residual mass semantics, which is a flat bound based on the residual probability mass of a loop. The approach is simple, efficient, and has provable guarantees.The main novelty of our work is the second approach, called geometric bound semantics. It operates on a novel family of distributions, called eventually geometric distributions (EGDs), and can bound the distribution of loops with a new form of loop invariants called contraction invariants. The invariant synthesis problem reduces to a system of polynomial inequality constraints, which is a decidable problem with automated solvers. If a solution exists, it yields an exponentially decreasing bound on the whole distribution, and can therefore bound moments and tail asymptotics as well, not just probabilities as in the first approach.Both semantics enjoy desirable theoretical properties. In particular, we prove soundness and convergence, i.e. the bounds converge to the exact posterior as loops are unrolled further. We also investigate sufficient and necessary conditions for the existence of geometric bounds. On the practical side, we describe Diabolo, a fully-automated implementation of both semantics, and evaluate them on a variety of benchmarks from the literature, demonstrating their general applicability and the utility of the resulting bounds.","Probabilistic programming Probabilistic programming is a discipline that studies programming languages with probabilistic constructs (Barthe et al., 2020). The term is overloaded however. At the intersection with randomized algorithms and program analysis, it usually means a programming language with a construct for probabilistic branching or sampling from probability distributions. As such, it is simply a language to express programs with random numbers and researchers study program analysis techniques for termination probabilities, safety properties, cost analysis, and others. At the intersection with statistics and machine learning, probabilistic programming is used to express (Bayesian) statistical models (van de Meent et al., 2018). Bayesian inference is a very successful framework for reasoning and learning under uncertainty: it updates prior beliefs about the world with observed data to obtain posterior beliefs using Bayes’ rule. As such, the programming languages for Bayesian models provide a construct for conditioning on data in addition to sampling from distributions. Since Bayesian inference is a difficult problem, a lot of research focuses on inference algorithms, in particular their correctness and efficiency. This paper contributes to both areas by developing methods to bound the distributions arising from probabilistic programs, especially those with loops. \displaystyle Throws:=0;Die:=0; \displaystyle{\color[rgb]{.5,0,.5}\mathsf{while}}\,{Die\neq 6}\,\allowbreak\{ \displaystyle\quad Die\sim{\color[rgb]{0,.5,.5}\mathsf{Uniform}}\{1\mathchar 4% 4\relax\nolinebreak[3]\dots\mathchar 44\relax\nolinebreak[3]6\}; \displaystyle\quad{\color[rgb]{.5,0,.5}\mathsf{observe}}\,Die\in\{2\mathchar 4% 4\relax\nolinebreak[3]4\mathchar 44\relax\nolinebreak[3]6\}; \displaystyle\quad Throws\mathbin{{+}{=}}1\} Figure 1. A probabilistic program with a loop and conditioning. Example 1.1. To illustrate the concept, consider the following puzzle due to Elchanan Mossel. You throw a fair six-sided die repeatedly until you get a 6. You observe only even numbers during the throws. What is the expected number of throws (including the 6) conditioned on this event? This is a surprisingly tricky problem and most people get it wrong on the first try111In a survey on Gil Kalai’s blog, only 27% of participants chose the correct answer (https://gilkalai.wordpress.com/2017/09/07/tyi-30-expected-number-of-dice-throws/)., based on the incorrect assumption that it is equivalent to throwing a die with only the three faces 2, 4, and 6. Probability theory and statistics abound with such counterintuitive results (e.g. the Monty-Hall problem), and probabilistic programming offers a precise way to disambiguate their description and make them amenable to automatic analysis and inference tools. Mossel’s problem can be expressed as the probabilistic program in Fig. 1. The program has a loop that samples a die until it shows 6, and conditions on the number being even. In each iteration, the counter Throws is incremented. 1.1. Challenges Bayesian inference In Bayesian inference, Bayes’ rule is used to update prior distributions p(\theta) of model variables \theta with observed data x to obtain posterior distributions: p(\theta\mid x)=\frac{p(x\mid\theta)p(\theta)}{p(x)}. In practice, such Bayesian statistical models are too complex for manual calculations and inferring their posterior distribution is a key challenge in Bayesian statistics. There are two approaches: exact and approximate inference. Exact inference aims to find an exact representation of the posterior distribution. Such methods impose heavy restrictions on the supported probabilistic programs and do not usually scale well. Practitioners therefore mostly use approximate methods that do not aim to compute this distribution exactly, but rather to produce unbiased or consistent samples from it. If the probabilistic program does not contain conditioning, samples can simply be obtained by running the program. But with observations, program runs that violate the observations must be rejected. Since the likelihood of the observations is typically low, simple rejection sampling is inefficient, and thus practical samplers use more sophisticated techniques, such as Markov chain Monte Carlo. While more scalable, these approaches typically do not provide strong guarantees on the approximation error after a finite amount of time (Gelman et al., 2013, Section 11.5). Loops Loops are essential to the expressiveness of programming languages but notoriously hard to analyze. This applies even more strongly to the probabilistic setting, where deciding properties like termination is harder than in the deterministic setting (Kaminski and Katoen, 2015). Even if a program does not use conditioning, loops can still make sampling difficult. For example, a program may terminate almost surely, but its expected running time may be infinite. This prevents sampling-based approaches since they need to run the program. Furthermore, many inference algorithms are not designed to handle unbounded loops and may return erroneous results for such programs (Beutner et al., 2022). On the formal methods side, various approaches for probabilistic loop analysis have been proposed, employing techniques such as martingales, moments, and generating functions (see Section 7). If all program variables have finite support, the program can be translated to a probabilistic transition system and techniques from probabilistic model checking can be used. None of these analysis techniques can be applied to Example 1.1 however: methods from cost analysis do not support conditioning and probabilistic model checking requires finite support (but Throws is supported on \mathbb{N}). The approach by Klinkenberg et al. (2024) via generating functions is theoretically applicable, but requires the user to provide a loop invariant template, i.e. a loop invariant where certain parameters may be missing. Unfortunately, such an invariant cannot always be specified in their language (Klinkenberg et al., 2024, Example 25). Even in cases where this is possible, we argue that figuring out its shape is actually the hard part: it already requires a good understanding of the probabilistic program and its distribution, so it is not a satisfactory solution. 1.2. Guaranteed bounds To deal with the above challenges, we investigate guaranteed bounds on the program distribution. “Guaranteed” here refers to a method that computes deterministic (non-stochastic) results about the mathematical denotation of a program (Beutner et al., 2022). Such bounds are applicable more often than exact inference, e.g. in the presence of loops/recursion, and provide more assurance than approximate methods, which have at best stochastic guarantees. Why are such bounds useful? Partial correctness properties In quantitative program analysis, one can verify safety properties by bounding the probability of reaching an unsafe state. Bounding reachability probabilities is also a common problem in probabilistic model checking and quantitative program verification, yet it has not seen much attention in the context of probabilistic programming with conditioning, aside from the work by Beutner et al. (2022) and Wang et al. (2024). Neither of those can bound moments of infinite-support distributions, whereas our work finds tight bounds on the expected value of Throws in Fig. 1 (see Section 6.4). Figure 2. Histogram of samples from two inference algorithms (importance sampling and Pyro’s HMC), and the guaranteed bounds from Beutner et al. (2022). The bounds show that Pyro’s HMC produces wrong results. (Source: Beutner et al. (2022)) Checking approximate inference In the context of Bayesian inference, the bounds can be useful to check and debug approximate inference algorithms and their implementations. If the approximate results clearly contradict the bounds, the inference algorithm is likely incorrect, or some of its assumptions are violated, or it has not converged. Beutner et al. (2022) provide an example of this: the inference tool Pyro yields wrong results for a probabilistic program with loops, but their bounds can detect this issue (Fig. 2).222 The cause turned out to be an undocumented assumption in the inference algorithm. Pyro’s implementation seems to assume that the dimension (number of samples in a program run) of the problem is constant, which is violated when sampling inside probabilistic loops. Another problem with approximate inference is the tail behavior of the posterior distribution, which is often crucial for the quality of the approximation (Liang et al., 2023). Previous work on guaranteed bounds (Beutner et al., 2022; Wang et al., 2024) does not address this aspect, but our work can bound the tail behavior as well. Table 1. Comparison of our two approaches with the most relevant related work on probabilistic programs with loops. (Cond.: supports (Bayesian) conditioning; Inf.: branching on variables with infinite support is allowed; Cont.: continuous distributions allowed; Auto.: fully automated; Prob.: computes/bounds probabilities; Mom.: computes/bounds moments; Tails: computes/bounds tail asymptotics of this shape. Partial support is denoted by “\sim”.) Type Cond.? Inf.? Cont.? Auto.? Prob.? Mom.? Tails? Moosbrugger et al. (2022) exact ✗ ✗ ✓ ✓ \sim ✓ O(n^{-k}) Beutner et al. (2022) bounds ✓ ✓ ✓ ✓ ✓ ✗ ✗ Wang et al. (2024) bounds ✓ ✓ ✓ ✓ ✓ ✗ ✗ Klinkenberg et al. (2024) exact ✓ ✓ ✗ ✗ ✓ ✓ ✗ Resid. mass sem. (Section 3) bounds ✓ ✓ ✗ ✓ ✓ ✗ ✗ Geom. bounds (Section 4) bounds ✓ ✓ ✗ ✓ ✓ ✓ O(c^{n}) Problem Statement Given a probabilistic program with posterior distribution \mu on \mathbb{N}, our goal is to bound: (1) probability masses: given n\in\mathbb{N}, find l\mathchar 44\relax\nolinebreak[3]u\in[0\mathchar 44\relax\nolinebreak[3]1] such that l\leq\mathbb{P}_{X\sim\mu}[X=n]\leq u; (2) moments: given k\in\mathbb{N}, find l\mathchar 44\relax\nolinebreak[3]u\in\mathbb{\mathbb{R}}_{\geq 0} such that l\leq\mathbb{E}_{X\sim\mu}[X^{k}]\leq u; (3) tail asymptotics: find c\in[0\mathchar 44\relax\nolinebreak[3]1) such that \mathbb{P}_{X\sim\mu}[X=n]=O(c^{n}). 1.3. Contributions In this paper, we develop two new methods to compute guaranteed bounds on the distribution of discrete probabilistic programs with loops and conditioning. Lower bounds can simply be found by unrolling each loop a finite number of times. The main challenge is upper bounds and we attack it in two ways: the first is simple, always applicable, and efficient, but coarse; the second is more sophisticated and expensive, but yields much more informative bounds if applicable. A summary of the most relevant related work is presented in Table 1 and a detailed account in Section 7. The first semantics, called residual mass semantics (Section 3), is based on the simple idea of bounding the remaining probability mass after the loop unrollings, which has not previously been described, to our knowledge. We make the following contributions: • We introduce the residual mass as a simple but effective idea to bound posterior probabilities. • We prove soundness and convergence of the bounds to the true distribution (as loops are unrolled further and further). • We implement the semantics in a tool called Diabolo and demonstrate empirically that the implementation is more efficient than previous systems (Section 6.3). The second semantics, called geometric bound semantics (Section 4), is the main novelty of this paper. The idea is to bound the distribution of loops in a more fine-grained manner with geometric tails, rather than a flat bound as in the first semantics. • We present the concept of a contraction invariant for a loop, which yields upper bounds on the distribution (Section 4.1). • We introduce a family of distributions called eventually geometric distributions (EGDs) that are used as an abstract domain to overapproximate the distribution of a loop (Section 4.2). • We present the geometric bound semantics (Section 4.3) which reduces the synthesis problem of such an invariant to solving a system of polynomial inequalities. If successful, it immediately yields bounds on probability masses and, contrary to the first semantics, also on moments and tail probabilities of the program distribution. • We prove soundness of the semantics and convergence of the bounds, as loops are unrolled further and further (Section 4.5). • We identify necessary conditions and sufficient conditions for its applicability (Section 4.5). • We fully automate it in our tool Diabolo (Section 5): contrary to previous work (Klinkenberg et al., 2024), it does not rely on the user to provide a loop invariant (template). • We demonstrate its applicability on a large proportion of benchmarks from the literature and compare it to previous approaches and the residual mass semantics (Section 6). Full proofs and additional details can be found in Appendices A, B, C and D and Zaiser (2024b). 1.4. Limitations Our work deals with discrete probabilistic programs with hard conditioning. This means that programs cannot sample or observe from continuous distributions. Variables in our programming language take values in \mathbb{N}; negative numbers are not supported (see Section 8.1 for possible extensions). While our language is Turing-complete, some arithmetic operations like multiplication as well as some common infinite-support distributions (e.g. Poisson) are not directly supported (see Section 2.2 for details on our language’s expressivity). The initial values of the program variables are fixed: our methods cannot reason parametrically about these inputs. The residual mass semantics can yield bounds on the distribution of any such probabilistic program, but convergence with increasing unrolling is only guaranteed if the program terminates almost surely. If the program distribution has infinite support, we cannot bound the moments or tails: the bound does not contain enough information for this. The geometric bound semantics yields EGD bounds, which allow bounding moments and tails. On the other hand, such bounds do not exist for all programs. Our experiments show that this is not a big concern for many probabilistic programs with loops in practice: EGD bounds exist for a majority of examples we found in the literature. Another limitation of EGD bounds is that they cannot represent correlation of the tails of two variables, which may lead to imprecise tail bounds or failing to find bounds at all. Finally, solving the system of polynomial inequalities arising from the semantics, while decidable, can be hard in practice and does not scale to very large programs. It should be noted that scalability is a general issue in probabilistic program analysis owing to the hardness of the problem (Dagum and Luby, 1993) and not specific to our work. 1.5. Notation and conventions We use the Iverson brackets [\varphi] to mean 1 if \varphi is satisfied and 0 otherwise. We write variables representing vectors in bold ({\bm{\alpha}}), tensors (multidimensional arrays) in uppercase and bold (\mathbf{T}), and random or program variables in uppercase (X). We write \mathbf{0} and \mathbf{1} for the constant zero and one functions. We write \mathbf{0}_{n} and \mathbf{1}_{n} for the zero and one vectors in \mathbb{R}^{n}. To update the k-th component of a vector {\bm{\alpha}}, we write {\bm{\alpha}}[k\mapsto v]. Vectors {\bm{\alpha}}\in\mathbb{R}^{n} are indexed as \alpha_{1}\mathchar 44\relax\nolinebreak[3]\dots\mathchar 44\relax\nolinebreak% [3]\alpha_{n}. We abbreviate [d]:=\{0\mathchar 44\relax\nolinebreak[3]\dots\mathchar 44\relax\nolinebreak[3% ]d-1\}. Tensors \mathbf{T}\in\mathbb{R}^{[d_{1}]\times\dots\times[d_{n}]} are indexed as \mathbf{T}_{i_{1}\mathchar 44\relax\nolinebreak[3]\dots\mathchar 44\relax% \nolinebreak[3]i_{n}} where i_{k} ranges from 0 to d_{k}-1. We write \mathbf{0}_{[d_{1}]\times\dots\times[d_{n}]} or simply \mathbf{0} for the zero tensor in \mathbb{R}^{[d_{1}]\times\dots\times[d_{n}]}. We write |\mathbf{T}|=(d_{1}\dots\mathchar 44\relax\nolinebreak[3]d_{n}) for the dimensions of \mathbf{T}\in\mathbb{R}^{[d_{1}]\times\dots\times[d_{n}]}, and in particular |\mathbf{T}|_{i}=d_{i}. To index \mathbf{T} along dimension k, we write \mathbf{T}_{k:j}\in\mathbb{R}^{[d_{1}]\times\cdots\times[d_{k-1}]\times[d_{k+1% }]\times\cdots\times[d_{n}]}, which is defined by (\mathbf{T}_{k:j})_{i_{1}\mathchar 44\relax\nolinebreak[3]\dots\mathchar 44% \relax\nolinebreak[3]i_{k-1}\mathchar 44\relax\nolinebreak[3]i_{k+1}\mathchar 4% 4\relax\nolinebreak[3]\dots\mathchar 44\relax\nolinebreak[3]i_{n}}=\mathbf{T}_% {i_{1}\mathchar 44\relax\nolinebreak[3]\dots\mathchar 44\relax\nolinebreak[3]i% _{k-1}\mathchar 44\relax\nolinebreak[3]j\mathchar 44\relax\nolinebreak[3]i_{k+% 1}\mathchar 44\relax\nolinebreak[3]\dots\mathchar 44\relax\nolinebreak[3]i_{n}}. We often write tensor indices as {\bm{i}}:=(i_{1}\mathchar 44\relax\nolinebreak[3]\dots\mathchar 44\relax% \nolinebreak[3]i_{n}) for brevity. We also abbreviate {\bm{\alpha}}^{{\bm{i}}}:=\prod_{k=1}^{n}\alpha_{i}^{i_{k}}. Other binary operations (+, -, \min, \max, etc.) work elementwise on vectors and tensors, e.g. ({\bm{\alpha}}+{\bm{\beta}})_{j}:=\alpha_{j}+\beta_{j} and {\bm{\alpha}}\leq{\bm{\beta}} if and only if \alpha_{j}\leq\beta_{j} for all j."
https://arxiv.org/html/2411.08833v1,Advanced OOP and new syntax patterns for Javascript,"We present OBJS, a new transpiler project featuring the implementation of typified variables and functions call management in Javascript, as well as several new operators and syntax patterns that could make coding more agile and versatile. The goal is to empower this language. According to this point of view, this transpiler aims at implementing Object Oriented Programming paradigms into Javascript. The author opines that this would be likely the best evolution of this language in ways that should be proper to the original syntax, that is, by adopting native C{}^{\textnormal{\tiny++}}standards, so that there would be no promiscuity between old and new patterns, benefiting those who come from similar languages.","Javascript first appeared in the mid 1990s and has since become one of the most widely used programming languages for front–end services of Web applications, at both small and large scale projects. The increasing popularity of Javascript has propelled it to the forefront of front-end services at any scale. In line with the growing ambitions shown by recent systemic applications, the original Javascript requires some enhancements in order to meet a broader range of demands. The development of Typescript demonstrated that there are strong motivations and consistent margins for the implementation of new patterns that are further presented and pertain to areas that the original Javascript has not yet covered. We presented some new ideas for the promotion of new coding paradigms, inherent to advanced Object Oriented Programming (OOP) or the ‘code event response’, based on our vision of code as a timeline. Furthermore, the ability of ‘speaking in Javascript’ to develop custom dialects in order to lower the entry level to code writing, especially for beginners. These new features have been implemented into the transpiler OBJS.222The acronym of ‘OBject JavaScript’."
https://arxiv.org/html/2411.08388v1,Multi-Lingual Development & Programming Languages Interoperability: An Empirical Study,"As part of a research on a novel in-process multiprogramming- language interoperability system, this study investigates the interoperability and usage of multiple programming languages within a large dataset of GitHub projects and Stack Overflow Q&A. It addresses existing multi-lingual development practices and interactions between programming languages, focusing on in-process multi-programming language interoperability. The research examines a dataset of 414,486 GitHub repositories, 22,156,001 Stack Overflow questions from 2008-2021 and 173 interoperability tools. The paper’s contributions include a comprehensive dataset, large-scale analysis, and insights into the prevalence, dominant languages, interoperability tools, and related issues in multi-language programming. The paper presents the research results, shows that C is a central pillar in programming language interoperability, and outlines simple interoperability guidelines. These findings and guidelines contribute to our multi-programming language interoperability system research, also laying the groundwork for other systems and tools by suggesting key features for future interoperability tools.","multi-lingual development and interoperability of programming languages have existed for many years and are a known practices (Yang et al., 2024) (Mayer et al., 2017) (Abidi et al., 2019) (Li et al., 2024) (Li et al., 2021) (Tomassetti and Torchiano, 2014). As part of a research on a novel in-process multi-programming-language interoperability system, we want to survey the existing usage and interactions between programming languages. While these topics have been studied before in multiple articles (e.g. (Yang et al., 2024) (Li et al., 2024) (Li et al., 2022a) (Grichi et al., 2021)), our research focuses on programming languages (instead of any software language) and in-process interoperability. Also, this research is done on a large dataset of GitHub (Inc, 2021a) projects and StackOverflow (Inc, 2021c) Q&A questions. Both GitHub and StackOverflow are known sources and have been used for empirical studies in the past (e.g. (Ray et al., 2017), (Grichi et al., 2021), (Yang et al., 2024)). The discussion in this paper addresses the related work. The first contribution is a dataset of 414,486 open source projects (also called repositories interchangeably) metadata from GitHub, and 22,156,001 Q&A questions from StackOverflow website. The projects and questions dating from 2008 to the end of 2021, as the data acquisition was done at the beginning of 2022. The datasets are available using links shown in section 3. The second contribution is the analysis we have performed on a large dataset and the answer to the research questions detailed below. As part of the research, we want to check if the problem is really wide spread on a large-scale dataset to validate the relevant finding of previous studies conducted by others on smaller scale datasets. Although we focus on findings for an interoperability system research, we extract statistics and discuss the results of previous work to compare to the large dataset findings. As the dataset is huge, in some metrics we cannot perform fine-tune analysis as done in previous work. Therefore, we are taking different approaches to mitigate the scale of the dataset. This study answers the following research questions on a large-scale dataset: RQ_{1}: How common is multi-lingual and multi-PL development? RQ_{2}: What are the dominant languages and programming languages? RQ_{3}: Which programming languages are mostly used together? and which binding mechanisms? RQ_{4}: What are the common interoperability tools? RQ_{5}: How many issues and discussions relate to multi-PL? By answering the research questions, we can validate the results done by previous work and understand better how developers use multi-PL development, which PLs and tools they use, and how many issues and discussions are there on this topic. These results allow us to better define the required features for future interoperability tools, in order to provide simpler and more intuitive multi-PL programming. As noted above, in order to answer our research questions, we have conducted an empirical study on GitHub(Inc, 2021a) source repository and Stack Overflow (Inc, 2021c) Q&A website. GitHub and Stack Overflow were chosen as they are prevalent in their fields (Similarweb, [n. d.])(SimilarWeb, [n. d.]). We have analyzed 414,486 GitHub projects (also called repositories interchangeably), 22,156,001 Stack Overflow questions, dating from 2008 to 2021, and 173 interoperability tools to understand the current usage of multi-PL. Also, we analyze the GitHub projects’ issue boards and discussion groups to detect issues relating to multi-PL. Our focus is on programming languages and techniques of interoperability, as opposed to other languages used in software development, not strictly for writing the program instructions and logic. Hence, we discuss and classify a list of programming languages while taking into account existing classifications (Wikipedia contributors, 2023)(git, 2023) and we catalog different interoperability techniques of programming languages based on our findings. As mentioned earlier, although similar studies have been conducted in the past, our novelty lies in • a significantly larger dataset over a longer period • focusing the study on multi-PL programming, which is different from ”multi-lingual” • classification of PL compared to other existing classifications (git, 2023)(Wikipedia contributors, 2023) used in previous work (e.g. (Tomassetti and Torchiano, 2014), (Bissyandé et al., 2013) (Ray et al., 2017)) • classifying binding types of interoperability tools on a large scale dataset using a different approach from previous work (e.g. (Li et al., 2024) (Li et al., 2022b)) • showing that C is a central pillar in programming language interoperability • defining a guideline for interoperability tools named simple interoperability Section 2 discusses previous and related work. Section 3 details the datasets and how we have collected them. Section 4 details how we classify which languages are programming languages. Section 5 explains the terminology used throughout the paper. Section 6 presents the finding and answers to the research questions. Section 7 shows threats to validity. Section 8 discusses the implications of the research questions’ answers and presents some insights. Section 9 concludes the study, its results and the discussion of the paper."
https://arxiv.org/html/2411.07718v2,: AST Differencing for Solidity Smart Contracts,"Smart contracts, primarily written in Solidity, are integral to blockchain software applications, yet precise analysis and maintenance are hindered by the limitations of existing differencing tools. We introduce SoliDiffy, a novel Abstract Syntax Tree (AST) differencing tool specifically designed for Solidity. SoliDiffy enables fine-grained analysis by generating accurate and concise edit scripts of smart contracts , making it ideal for downstream tasks such as vulnerability detection, automated code repair, and code reviews. Our comprehensive evaluation on a large dataset of real-world Solidity contracts demonstrates that SoliDiffy delivers shorter and more precise edit scripts compared to state-of-the-art tools, while performing consistently in complex contract modifications. SoliDiffy is made publicly available at https://github.com/mojtaba-eshghie/SoliDiffy.","Smart contracts are self-executing programs that implement real-world contracts by encoding contract terms directly into code [1, 2]. These programs are deployed on blockchain platforms like Ethereum [3], allowing for automated and trustless transactions. Solidity, a statically-typed programming language, has become the most popular choice for developing these smart contracts [4]. Developers working with Solidity can greatly benefit from a fine-grained source code differencing in several scenarios. For instance, when updating a smart contract to patch a security vulnerability, Abstract Syntax Tree (AST)-based differencing allows developers to pinpoint specific changes in the code’s structure rather than sifting through line-by-line text changes, which might miss subtle yet crucial modifications. This precision is vital when reviewing updates for correctness and security implications before deployment on an immutable blockchain. Other examples are in automated debugging [5] and program repair [6, 7, 8, 9], where tools need to detect bugs and vulnerabilities [10, 11, 12, 13, 14] and suggest changes at a syntactic level [15] ; fine-grained differencing enables these tools to generate precise edit scripts that align with the semantic intentions of the code, helping with automated and context-aware fixes. Furthermore, in code clone detection [16, 17, 18], where identifying syntactically similar but not identical code blocks is necessary, AST differencing can accurately capture variations that line-based tools overlook, thus enhancing the detection of potential code reuse or duplication issues. Traditional text and structural differencing tools [19, 20, 21, 22, 23, 24, 25] are insufficient for smart contracts due to their inability to capture the semantic and structural details of smart contracts or their lack of support for Solidity. AST differencing for Solidity smart contracts provides a more granular approach at detection of changes at the syntactic level. To summarize, we problem we address in this paper is the fundamental limitations of existing differencing tools for Solidity developers. In this paper, we introduce SoliDiffy, a novel AST differencing tool tailored for Solidity smart contracts. SoliDiffy contains key AST transformations specifically designed for Solidity smart contracts. SoliDiffy accurately processes and compares Solidity smart contracts, providing precise edit scripts that can be used for tasks such as vulnerability detection. To evaluate the effectiveness of SoliDiffy, we conduct a comprehensive comparison on 354\,187 pairs of smart contracts. This dataset is founded on real-world Solidity smart contracts, including a subset of modified controlled syntactic changes (simple to complex transformation) and another subset mined from commit history of a popular smart contract repository. We assess the performance of SoliDiffy in terms of edit script accuracy. The results clearly demonstrate that SoliDiffy outperforms the only existing tool for Solidity differencing, Difftastic [21]. SoliDiffy maintains consistent effectiveness regardless of edit distance and code complexity, highlighting its suitability for advanced software engineering tasks in the blockchain domain. To summarize, our contributions are: • We introduce SoliDiffy, a novel AST differencing approach specifically designed for Solidity smart contracts, addressing the limitations of existing differencing tools in accurately capturing the syntactic and semantic changes within smart contracts. • We design Solidity-specific AST transformation and pruning rules that enhance the precision and conciseness of edit scripts compared to a raw concrete syntax tree. • We conduct a comprehensive evaluation of SoliDiffy on a dataset of 354\,187 pairs of smart contracts to diff, demonstrating that it outperforms the state-of-the-art tool Difftastic in terms of edit script length and accuracy. SoliDiffy maintains consistent effectiveness regardless of edit distance and code complexity. • We make SoliDiffy publicly available as an open-source tool at https://github.com/mojtaba-eshghie/SoliDiffy, facilitating further research and applications in smart contract analysis. This paper is structured as follows: Section II introduces the necessary background concepts, including ASTs and code differencing techniques. In Section III, we describe the architecture of SoliDiffy. Section IV outlines the experimental protocol including research questions that guide our investigation. Section V presents the results of our evaluation, comparing SoliDiffy with Difftastic and exploring the impact of different types and severity of syntactic changes on the performance of these tools. Section VII reviews existing work related to AST differencing and identifies gaps that SoliDiffy addresses. Finally, Section VI elaborates the lessons learnt and threats to validity of our work, and Section VIII concludes the paper. Figure 1: The design of the SoliDiffy smart contract differencing tool."
https://arxiv.org/html/2411.06383v2,Program Analysis via Multiple Context Free Language Reachability,"Context-free language (CFL) reachability is a standard approach in static analyses, where the analysis question (e.g., is there a dataflow from x to y?) is phrased as a language reachability problem on a graph G wrt a CFL \mathcal{L}. However, CFLs lack the expressiveness needed for high analysis precision. On the other hand, common formalisms for context-sensitive languages are too expressive, in the sense that the corresponding reachability problem becomes undecidable. Are there useful context-sensitive language-reachability models for static analysis?In this paper, we introduce Multiple Context-Free Language (MCFL) reachability as an expressive yet tractable model for static program analysis. MCFLs form an infinite hierarchy of mildly context sensitive languages parameterized by a dimension d and a rank r. Larger d and r yield progressively more expressive MCFLs, offering tunable analysis precision. We showcase the utility of MCFL reachability by developing a family of MCFLs that approximate interleaved Dyck reachability, a common but undecidable static analysis problem.Given the increased expressiveness of MCFLs, one natural question pertains to their algorithmic complexity, i.e., how fast can MCFL reachability be computed? We show that the problem takes O(n^{2d+1}) time on a graph of n nodes when r=1, and O(n^{d(r+1)}) time when r>1. Moreover, we show that when r=1, even the simpler membership problem has a lower bound of n^{2d} based on the Strong Exponential Time Hypothesis, while reachability for d=1 has a lower bound of n^{3} based on the combinatorial Boolean Matrix Multiplication Hypothesis. Thus, for r=1, our algorithm is optimal within a factor n for all levels of the hierarchy based on the dimension d (and fully optimal for d=1).We implement our MCFL reachability algorithm and evaluate it by underapproximating interleaved Dyck reachability for a standard taint analysis for Android. When combined with existing overapproximate methods, MCFL reachability discovers all tainted information on 8 out of 11 benchmarks, while it has remarkable coverage (confirming 94.3\% of the reachable pairs reported by the overapproximation) on the remaining 3. To our knowledge, this is the first report of high and provable coverage for this challenging benchmark set.","Static analysis via language reachability. Static analyses are a standard approach to determining program correctness, as well as other useful properties of programs. They normally operate by establishing an approximate model for the program, effectively reducing questions about program behavior to algorithmic questions about the model. One popular type of modeling in this direction is language reachability, where the program abstraction is via an edge-labeled graph G (Reps, 1997; Reps et al., 1995). Language reachability is a generalization of standard graph reachability, parameterized by a language \mathcal{L}. Intuitively, given two nodes u, v, the problem asks to determine whether v is reachable from u via a path whose sequence of labels produce a string that belongs to \mathcal{L}. Such a path captures program executions that relate the objects represented by v and u (e.g., control-flow between two program locations or data-flow between two variables). The role of \mathcal{L} is normally to increase the analysis precision by filtering out paths that represent spurious program executions, owing to the approximate nature of G. Context-free language reachability. Language-reachability based static analyses are most frequently phrased with respect to context free languages (CFL), known as CFL reachability, which have various uses. For example, they are used to increase the precision of interprocedural analyses (aka whole-program analyses), where modeled executions cross function boundaries, and the analysis has to be calling-context sensitive111It might sound paradoxical that a context-free language makes the analysis context-sensitive, but this is just a naming coincidence. “Context sensitivity” refers to calling contexts, and the CFL simulates the call stack.. A CFL \mathcal{L} captures that a path following an invocation from a caller function \mathtt{foo}() to a callee \mathtt{tie}() must return to the call site of \mathtt{foo}() when \mathtt{tie}() returns (as opposed to some other function \mathsf{bar}() that also calls \mathtt{tie}()). This approach is followed in a wide range of interprocedural static analyses, including data-flow and shape analysis (Reps et al., 1995), type-based flow analysis (Rehof and Fähndrich, 2001) and taint analysis (Huang et al., 2015), to name a few. In practice, widely-used tools, such as Wala (Wal, 2003) and Soot (Bodden, 2012), equip CFL-reachability techniques to perform the analysis. Another common use of CFLs is to track dataflow information between composite objects in a field-sensitive manner. Here, a CFL \mathcal{L} captures a dataflow between variables x and y if, for example x flows into z.f (i.e., field f of composite object z), and z.f itself flows into y (as opposed to z.g flowing into y). This approach is standard in a plethora of pointer, alias and data dependence analyses (Lhoták and Hendren, 2006; Reps, 1997; Chatterjee et al., 2018; Sridharan et al., 2005; Sridharan and Bodík, 2006; Lu and Xue, 2019). The need for context-sensitive models. Although CFLs offer increased analysis precision over simpler, regular abstractions, there is often a need for more precise, context sensitive models. For example, almost all analyses are designed with both context and field sensitivity, as this leads to obvious precision improvements (Milanova, 2020; Lhoták and Hendren, 2006; Sridharan and Bodík, 2006; Späth et al., 2019). The underlying language modeling both sensitivities is the free interleaving of the two corresponding CFLs, which is not a CFL, and is commonly phrased as interleaved Dyck reachability. Unfortunately, interleaved Dyck reachability is well-known to be undecidable (Reps, 2000). Since it is desirable to maintain at least some precision of each type, there have been various approximations that generally fall into two categories: (i) apply some kind of k-limiting, which approximates one of the CFLs by a regular (or even finite) language (the most common approach), or (ii) solve for each type of sensitivity independently (Späth et al., 2019), possibly in a refinement loop (Ding and Zhang, 2023; Conrado and Pavlogiannis, 2024). Observe that both cases essentially fall back to context-free models, forgoing the desirable precision of context sensitivity for which the analysis was designed in the first place. This leads to a natural question: Are there natural, efficient (polynomial-time), and practically precise, context-sensitive approximations for interleaved Dyck reachability? Multiple Context Free Languages. One of the most natural generalizations of CFLs towards mild context sensitivity is that of Multiple Context Free Languages (MCFLs) (Seki et al., 1991). These languages are generated by the corresponding Multiple Context Free Grammars (MCFGs), which form a hierarchy of expressiveness parameterized by a dimension d and rank r (concisely denoted as d\text{-}\operatorname{MCFG}(r)). Intuitively, an MCFG in d dimensions performs simultaneous context-free parsing on d substrings of a word, and can thus capture bounded context-sensitivity between these substrings. The rank r limits the number of non-terminals that can appear in a single production rule. MCFGs have received considerable attention, as they are regarded as a realistic formalism for natural languages (Clark, 2014), while several popular classes of formal languages fall into specific levels in this hierarchy, e.g., CFLs are MCFLs of dimension 1, and Tree Adjoining Languages (TALs) (Joshi, 1987) and Head Languages (Pollard, 1984) fall in dimension 2 (Seki et al., 1991). Despite the context sensitivity, for each r\geq 2, d\text{-}\operatorname{MCFL}(r) forms a full abstract family of languages (AFL – closed under homomorphism, inverse homomorphism, intersection with regular sets, union, and Kleene closure) (Rambow and Satta, 1999), with decidable membership and emptiness (Vijay-Shanker et al., 1987). As such, they form an elegant class of mildly context sensitive languages that are amenable to algorithmic treatment. In a static analysis setting, language reachability with MCFLs has the potential to yield higher modeling power. Moreover, this power is utilized in a controllable way, owing to the higher expressivity along the MCFL hierarchy. However, neither (i) the modeling power of MCFL reachability (what can MCFLs express in a program analysis setting?) nor (ii) the algorithmic question (how fast can we solve MCFL reachability?) have been studied. This paper addresses these questions, by (1) designing a family of MCFLs for approximating the common static analysis problem of interleaved Dyck reachability, with remarkable coverage in practice, and (2) developing a generic algorithm for MCFL reachability (for any dimension d and rank r), as well as proving fine-grained complexity lower bounds for the problem. The following motivating example illustrates the problem setting and our approach. 1.1. Motivating Example In a standard dataflow analysis setting, the task is to identify pairs of variables x, y such that the value of x may affect the value of y. This is achieved by following def-use chains in the program P. The program model is a dataflow graph G, where nodes represent variables, and an edge x\to y results from an instruction of the form x=f(y) (for some uninterpreted function f). To address the common issue of high false positives, the analysis must be both context-sensitive and field-sensitive. For example, consider the program P in Fig. 1, and the dataflow graph G in Fig. 2(a). {mdframed} [backgroundcolor=black!7!white,rightline=false,leftline=false,linewidth=0.25mm,] ⬇ 1pair tie(int x,int y){ 2 pair p; 3 p.first = x; 4 p.second = y; 5 return p; 6} ⬇ 1void foo() { 2 int a = 2; 3 int b = 3; 4 pair q = tie(a,b); 5 int c = q.first; 6 return; 7} ⬇ 6void bar() { 7 int d = 5; 8 pair r = tie(d,7); 9 int e = r.second; 10 return; 11} Figure 1. A program P containing three functions tie(), foo(), bar() and composite objects of type pair. adbxyp\text{tie}_{\text{ret}}qrce{\color[rgb]{0.2549019607843137,0.2117647058823529,0.9823529411764706}% \definecolor[named]{pgfstrokecolor}{rgb}{% 0.2549019607843137,0.2117647058823529,0.9823529411764706}(_{10}}{\color[rgb]{0.2549019607843137,0.2117647058823529,0.9823529411764706}% \definecolor[named]{pgfstrokecolor}{rgb}{% 0.2549019607843137,0.2117647058823529,0.9823529411764706}(_{16}}{\color[rgb]{0.2549019607843137,0.2117647058823529,0.9823529411764706}% \definecolor[named]{pgfstrokecolor}{rgb}{% 0.2549019607843137,0.2117647058823529,0.9823529411764706}(_{10}}{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{pgfstrokecolor}{rgb}{% 0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.24}[_{% 1}}{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{pgfstrokecolor}{rgb}{% 0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.24}[_{% 2}}{\color[rgb]{0.2549019607843137,0.2117647058823529,0.9823529411764706}% \definecolor[named]{pgfstrokecolor}{rgb}{% 0.2549019607843137,0.2117647058823529,0.9823529411764706})_{10}}{\color[rgb]{0.2549019607843137,0.2117647058823529,0.9823529411764706}% \definecolor[named]{pgfstrokecolor}{rgb}{% 0.2549019607843137,0.2117647058823529,0.9823529411764706})_{16}}{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{pgfstrokecolor}{rgb}{% 0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.24}]_{% 1}}{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{pgfstrokecolor}{rgb}{% 0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.24}]_{% 2}} (a) A graph G modeling context-sensitive and field-sensitive data flow in program P from Fig. 1. efghijk{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{pgfstrokecolor}{rgb}{% 0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.24}[_{% 1}}{\color[rgb]{0.2549019607843137,0.2117647058823529,0.9823529411764706}% \definecolor[named]{pgfstrokecolor}{rgb}{% 0.2549019607843137,0.2117647058823529,0.9823529411764706}(_{100}}{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{pgfstrokecolor}{rgb}{% 0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.24}]_{% 1}}{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{pgfstrokecolor}{rgb}{% 0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.24}[_{% 1}}{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{pgfstrokecolor}{rgb}{% 0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.24}]_{% 1}}{\color[rgb]{0.2549019607843137,0.2117647058823529,0.9823529411764706}% \definecolor[named]{pgfstrokecolor}{rgb}{% 0.2549019607843137,0.2117647058823529,0.9823529411764706})_{100}}{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{pgfstrokecolor}{rgb}{% 0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.24}]_{% 1}} (b) A subgraph of the uranai benchmark in a taint analysis for Android. Figure 2. Two graphs modeling context and field sensitivity through edge labels. Context sensitivity. Let us momentarily ignore edge labels in G. We have a path b\rightsquigarrow e, signifying a dataflow from b to e. This, however, does not correspond to a valid program execution: the path goes through the call of function tie() from foo() (where b is declared), but when tie() returns, the execution continues on foo, rather than bar() where e is declared. Call-context sensitivity is achieved by modeling call sites using parenthesis labels, and only considering reachability as witnessed by paths that produce a properly balanced parenthesis string. Formally we require that the label of the path forms a string that belongs to the Dyck language over parentheses (which is a CFL). Now, the path b\rightsquigarrow e is invalid, since {\color[rgb]{0.2549019607843137,0.2117647058823529,0.9823529411764706}% \definecolor[named]{pgfstrokecolor}{rgb}{% 0.2549019607843137,0.2117647058823529,0.9823529411764706}(_{10}} (along the edge b\xrightarrow{{\color[rgb]{% 0.2549019607843137,0.2117647058823529,0.9823529411764706}\definecolor[named]{% pgfstrokecolor}{rgb}{0.2549019607843137,0.2117647058823529,0.9823529411764706}% (_{10}}}y) does not match {\color[rgb]{0.2549019607843137,0.2117647058823529,0.9823529411764706}% \definecolor[named]{pgfstrokecolor}{rgb}{% 0.2549019607843137,0.2117647058823529,0.9823529411764706})_{16}} (along the edge \text{tie}_{\text{ret}}\xrightarrow{{\color[rgb]{% 0.2549019607843137,0.2117647058823529,0.9823529411764706}\definecolor[named]{% pgfstrokecolor}{rgb}{0.2549019607843137,0.2117647058823529,0.9823529411764706}% )_{16}}}r), thus the analysis avoids reporting this false positive. Field sensitivity. With parentheses modeling call-context sensitivity, consider the path d\rightsquigarrow e. The parenthesis string along this path is {\color[rgb]{0.2549019607843137,0.2117647058823529,0.9823529411764706}% \definecolor[named]{pgfstrokecolor}{rgb}{% 0.2549019607843137,0.2117647058823529,0.9823529411764706}(_{16}}{\color[rgb]{% 0.2549019607843137,0.2117647058823529,0.9823529411764706}\definecolor[named]{% pgfstrokecolor}{rgb}{0.2549019607843137,0.2117647058823529,0.9823529411764706}% )_{16}}, which is balanced, representing the fact that call contexts are respected. However, in P there is no dataflow from d to e, this time due to unmatched fields: x is assigned to \mathit{p.first}, and although there is a dataflow from p to r, e gets assigned \mathit{r.second}. Field-sensitivity is achieved by modeling object fields using (square) bracket labels, and only considering reachability as witnessed by paths that produce a properly balanced bracket string. Formally we require that the label of the path forms a string that belongs to the Dyck language over brackets. Now, the path d\rightsquigarrow e is invalid, since {\color[rgb]{0.24,0.24,0.24}\definecolor[named]{pgfstrokecolor}{rgb}{% 0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.24}[_{% 1}} (along the edge x\xrightarrow{{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{pgfstrokecolor}{% rgb}{0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.2% 4}[_{1}}}p) does not match {\color[rgb]{0.24,0.24,0.24}\definecolor[named]{pgfstrokecolor}{rgb}{% 0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.24}]_{% 2}} (along the edge r\xrightarrow{{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{pgfstrokecolor}{% rgb}{0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.2% 4}]_{2}}}e), thus the analysis avoids reporting this false positive. Context and field sensitivity, simultaneously. To capture both context and field sensitivity, the analysis must decide reachability via paths that are well-balanced wrt both parentheses and brackets. However, these two types of symbols can be interleaved in an arbitrary way. For example, out of all 6 possible source-sink pairs \{a,d,b\}\times\{c,e\}, the only real dataflow is from a to c, witnessed by a path producing the string {\color[rgb]{0.2549019607843137,0.2117647058823529,0.9823529411764706}% \definecolor[named]{pgfstrokecolor}{rgb}{% 0.2549019607843137,0.2117647058823529,0.9823529411764706}(_{10}}{\color[rgb]{% 0.24,0.24,0.24}\definecolor[named]{pgfstrokecolor}{rgb}{0.24,0.24,0.24}% \pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.24}[_{1}}{\color[rgb]% {0.2549019607843137,0.2117647058823529,0.9823529411764706}\definecolor[named]{% pgfstrokecolor}{rgb}{0.2549019607843137,0.2117647058823529,0.9823529411764706}% )_{10}}{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{pgfstrokecolor}{rgb}{% 0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.24}]_{% 1}}. As the corresponding reachability problem is undecidable (Reps, 2000), existing techniques focus on overapproximating interleaved Dyck reachability, mostly by some context-free model. This implies that these analysis results may still contain false positives in terms of reachability in the dataflow graph. Illustration on a real benchmark. To further illustrate the challenge, consider the dataflow graph in Fig. 2(b), which is a subgraph of a common taint analysis for Android (Huang et al., 2015). From an overapproximation standpoint, consider the potential reachability from e to j. Notice that there are valid context-sensitive paths and valid field-sensitive paths e\rightsquigarrow j; these are, respectively e\xrightarrow{{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{pgfstrokecolor}{% rgb}{0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.2% 4}[_{1}}}g\xrightarrow{{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{% pgfstrokecolor}{rgb}{0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}% \pgfsys@color@gray@fill{0.24}]_{1}}}i\xrightarrow{{\color[rgb]{% 0.2549019607843137,0.2117647058823529,0.9823529411764706}\definecolor[named]{% pgfstrokecolor}{rgb}{0.2549019607843137,0.2117647058823529,0.9823529411764706}% )_{100}}}j\qquad\text{and}\qquad e\xrightarrow{{\color[rgb]{0.24,0.24,0.24}% \definecolor[named]{pgfstrokecolor}{rgb}{0.24,0.24,0.24}% \pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.24}[_{1}}}g% \xrightarrow{{\color[rgb]{% 0.2549019607843137,0.2117647058823529,0.9823529411764706}\definecolor[named]{% pgfstrokecolor}{rgb}{0.2549019607843137,0.2117647058823529,0.9823529411764706}% (_{100}}}h\xrightarrow{{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{% pgfstrokecolor}{rgb}{0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}% \pgfsys@color@gray@fill{0.24}]_{1}}}f\xrightarrow{{\color[rgb]{0.24,0.24,0.24}% \definecolor[named]{pgfstrokecolor}{rgb}{0.24,0.24,0.24}% \pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.24}[_{1}}}e% \xrightarrow{{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{pgfstrokecolor}{% rgb}{0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.2% 4}[_{1}}}g\xrightarrow{{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{% pgfstrokecolor}{rgb}{0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}% \pgfsys@color@gray@fill{0.24}]_{1}}}i\xrightarrow{{\color[rgb]{% 0.2549019607843137,0.2117647058823529,0.9823529411764706}\definecolor[named]{% pgfstrokecolor}{rgb}{0.2549019607843137,0.2117647058823529,0.9823529411764706}% )_{100}}}j Because of the presence of both paths, an overapproximation algorithm may fail to conclude that e does not reach j through a path that is simultaneously context and field-sensitive. In fact, even newer overapproximation methods such as (Ding and Zhang, 2023) indeed report that e reaches j, thereby producing a false positive. From an underapproximation standpoint, consider the reachability from e to k, witnessed by the path e\xrightarrow{{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{pgfstrokecolor}{% rgb}{0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.2% 4}[_{1}}}g\xrightarrow{{\color[rgb]{% 0.2549019607843137,0.2117647058823529,0.9823529411764706}\definecolor[named]{% pgfstrokecolor}{rgb}{0.2549019607843137,0.2117647058823529,0.9823529411764706}% (_{100}}}h\xrightarrow{{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{% pgfstrokecolor}{rgb}{0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}% \pgfsys@color@gray@fill{0.24}]_{1}}}f\xrightarrow{{\color[rgb]{0.24,0.24,0.24}% \definecolor[named]{pgfstrokecolor}{rgb}{0.24,0.24,0.24}% \pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.24}[_{1}}}e% \xrightarrow{{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{pgfstrokecolor}{% rgb}{0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.2% 4}[_{1}}}g\xrightarrow{{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{% pgfstrokecolor}{rgb}{0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}% \pgfsys@color@gray@fill{0.24}]_{1}}}i\xrightarrow{{\color[rgb]{% 0.2549019607843137,0.2117647058823529,0.9823529411764706}\definecolor[named]{% pgfstrokecolor}{rgb}{0.2549019607843137,0.2117647058823529,0.9823529411764706}% )_{100}}}j\xrightarrow{{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{% pgfstrokecolor}{rgb}{0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}% \pgfsys@color@gray@fill{0.24}]_{1}}}k Observe that the path is non-simple, as we have to traverse the cycle once to obtain a valid string. Moreover, the string interleaves parentheses with brackets, which means that it cannot be captured in a single Dyck language involving both parentheses and brackets. In this work we demonstrate that MCFLs are an effective and tractable context-sensitive language formalism for underapproximating interleaved Dyck reachability that yields good approximations for real-world benchmarks. 1.2. Summary of Results To benefit readability, we summarize here the main results of the paper, referring to the following sections for details. We relegate all proofs to the Appendix. 1. MCFL reachability as a program model. We introduce MCFL reachability as an expressive, yet tractable, context-sensitive formalism for static analyses. Parameterized by the dimension d and rank r, d\text{-}\operatorname{MCFL}(r) yields an infinite hierarchy of progressively more expressive models that become Turing-complete in the limit. We illustrate the usefulness of MCFL reachability by using it to under-approximate the (generally, undecidable) problem of interleaved Dyck reachability, which is the standard formulation of a plethora of static analyses. In particular, for each d\geq 1, we obtain a d\text{-}\operatorname{MCFL}(2) that achieves increased precision as d increases (i.e., it discovers more reachable pairs of nodes), and becomes complete (i.e., it discovers all reachable pairs) in the limit of d\to\infty. Our MCFL formulation is, to our knowledge, the first non-trivial method that approximates the reachability set from below, thus having no false positives. Although underapproximations are less common in static analyses, they have many uses, such as excluding false positives (Psalm, 2024; Hicken, 2023), reporting concrete witnesses, acting as a tool for bug-finding (Le et al., 2022; Bessey et al., 2010) and performing “must” analyses (Godefroid et al., 2010; Xu et al., 2009; Smaragdakis and Balatsouras, 2015). Our underapproximation, when paired with existing overapproximate methods, allows limiting the set of potentially false negatives dramatically, and even find a fully-precise answer (as often is the case in our experiments). 2. MCFL reachability algorithm. We develop a generic algorithm for solving d\text{-}\operatorname{MCFL}(r) reachability, for any value of d and r. Our algorithm generalizes the existing algorithms for CFL reachability (Yannakakis, 1990) and TAL reachability (Tang et al., 2017). In particular, we establish the following theorem. {restatable} theoremthmupperbound All-pairs d\text{-}\operatorname{MCFL}(r)-reachability given a grammar \mathcal{G} on a graph G of n nodes can be solved in (1) O(\operatorname{poly}(|\mathcal{G}|)\cdot\delta\cdot n^{2d}) time, if r=1, where \delta is the maximum degree of G, and (2) O(\operatorname{poly}(|\mathcal{G}|)\cdot n^{d(r+1)}) time, if r>1. As CFLs and TALs are 1\text{-}\operatorname{MCFL}(2) and 2\text{-}\operatorname{MCFL}(2), respectively, Section 1.2 recovers the known bounds of O(n^{3}) and O(n^{6}) for the corresponding reachability problems. We also remark that the simpler problem of d\text{-}\operatorname{MCFL}(r) membership is solved in time O(n^{d(r+1)}) time on strings of length n (Seki et al., 1991). Section 1.2 states that reachability is no harder than membership, as long as the current bounds hold, for bounded-degree graphs (\delta=O(1)) or when r>1. 3. MCFL membership and reachability lower bounds. Observe that the bounds in Section 1.2 grow exponentially on the dimension d and rank r of the language. The next natural question is whether this dependency is tight, or it can be improved further. Given the role of MCFL reachability as an abstraction mechanism, this question is also practically relevant. For example, consider a scenario where a 3-dimensional MCFL is used in a static analysis setting, but the analysis is too heavy for the task at hand. The designer faces a dilemma: “should we attempt to improve the analysis algorithm, or should we find a simpler model, e.g., based on a 2-dimensional MCFL?”. A proven lower bound resolves this dilemma in favor of receding to 2 dimensions222Of course, one should also look for heuristics that offer practical speedups. We touch on this in Section 8.. We prove two such lower bounds based on arguments from fine-grained complexity theory. First, we study the dependency of the exponent on the dimension d. For this, we fix r=1 and arbitrary d, for which the membership problem, as well as the reachability problem on bounded-degree graphs, takes O(n^{2d}) time. We establish a lower-bound of n^{2d} based on the Strong Exponential Time Hypothesis (SETH). {restatable} theoremthmovhard For any integer d and any fixed \epsilon>0, the d\text{-}\operatorname{MCFL}(1) membership problem on strings of length n has no algorithm in time O(n^{2d-\epsilon}), under SETH. Section 1.2 is based on a fine-grained reduction from Orthogonal Vectors. The k-Orthogonal Vectors (OV) problem asks, given a set of m\cdot k Boolean vectors, to identify k vectors that are orthogonal. The corresponding hypothesis k-OVH states that this problem cannot be solved in O(m^{k-\epsilon}) time, for any fixed \epsilon>0 (it is also known that SETH implies k-OVH (Williams, 2005)). Section 1.2 is obtained by proving that a d\text{-}\operatorname{MCFL}(1) can express the orthogonality of 2d vectors. This implies that the dependency 2d in the exponent of Section 1.2 cannot be improved, while for r=1 our reachability algorithm is optimal on sparse graphs. Second, note that, on dense graphs (i.e., when \delta=\Theta(n)), the bound in Section 1.2 Item 1 is a factor n worse than the lower bound of Section 1.2. Are further improvements possible in this case? To address this question, we focus on the case of d=1, for which this upper bound becomes O(n^{3}). We show that the problem has no subcubic combinatorial algorithm based on the combinatorial Boolean Matrix Multiplication Hypothesis (BMMH). {restatable} theoremthmtrianglehard For any fixed \epsilon>0, the single-pair 1\text{-}\operatorname{MCFL}(1)-reachability problem on graphs of n nodes has no algorithm in time O(n^{3-\epsilon}) under BMMH. Hence, the \delta factor increase in the complexity cannot be improved in general, while Section 1.2 is tight for d=1 and r=1, among combinatorial algorithms. 4. Implementation and experimental evaluation. We implement our algorithm for MCFL reachability and run it with our family of d\text{-}\operatorname{MCFL}(2)s on standard benchmarks of interleaved Dyck reachability that capture taint analysis for Android (Huang et al., 2015). To get an indication of coverage, we compare our underapproximations with recent overapproximations. Remarkably, our underapproximation matches the overapproximation on most benchmarks, meaning that we have fully sound and complete results. For the remaining benchmarks, our underapproximation is able to confirm (94.3\%) of the taint information reported by the overapproximation. To our knowledge, this is the first report of such high, provable coverage for this challenging benchmark set."
https://arxiv.org/html/2411.06086v1,On Decidable and Undecidable Extensions of Simply Typed Lambda Calculus,"The decidability of the reachability problem for finitary PCF has been used as a theoretical basis for fully automated verification tools for functional programs. The reachability problem, however, often becomes undecidable for a slight extension of finitary PCF with side effects, such as exceptions, algebraic effects, and references, which hindered the extension of the above verification tools for supporting functional programs with side effects. In this paper, we first give simple proofs of the undecidability of four extensions of finitary PCF, which would help us understand and analyze the source of undecidability. We then focus on an extension with references, and give a decidable fragment using a type system. To our knowledge, this is the first non-trivial decidable fragment that features higher-order recursive functions containing reference cells.","Higher-order model checking, or the model checking of higher-order recursion schemes, has been proven to be decidable in 2006 [32], and practical higher-order model checkers (which run fast for many inputs despite the extremely high worst-case complexity) have been developed around 2010’s [15, 17, 6, 18, 7, 37]. Since then, higher-order model checking has been applied to fully automated verification of higher-order functional programs [16, 20, 33, 19, 38]. Among others, Kobayashi et al. [19, 38] used the decidability of the reachability problem for finitary PCF (i.e., the simply-typed \lambda-calculus with recursion and finite base types), which is a direct consequence of the decidability of higher-order model checking,111We are not sure whether it has been known before Ong’s result [32], but at least, the above-mentioned development of practical higher-order model checkers enabled the use of the result in practice. and combined it with predicate abstraction (which approximates a value in an infinite data domain by a tuple of Booleans that represent whether certain predicates are satisfied [9, 2, 19]) and counterexample-guided abstraction refinement, to obtain a fully automated model checker MoCHi for a subset of OCaml, just like software model checkers for first-order programs have been developed based on the decidability of finite-state or pushdown model checking [3, 1, 4]. Although MoCHi222https://github.com/hopv/MoCHi [19, 38] now supports a fairly large subset of OCaml, including exceptions, references, algebraic data types, and modules, it often fails to verify correct programs using certain features. For example, the recent version of MoCHi fails to verify even the following, trivially safe program: let x = ref 1 in assert(!x = 1). That is because MoCHi approximates a given program by replacing any read from reference cells with non-deterministic values; the above program is thus replaced by assert(Random.int(max_int) = 1) which may obviously lead to an assertion failure. Whilst there would be some ad hoc ways to partially address the problem (for example, in the case of the above program, we can combine dataflow analysis to infer the range of values stored in a reference cell), a fundamental problem is that higher-order model checking is undecidable for finitary PCF extended with references [12, 31]. If we apply predicate abstraction to a functional program with references, we would obtain a program of finitary PCF extended with references, but no reasonable model checker is available to decide the safety of the resulting program. Similar problems exist for exceptions carrying functions333In contrast, exceptions carrying only base-type values do not cause a problem; they can be transformed away by a CPS transformation [38]. and algebraic effects (which have been recently incorporated in OCaml 5), as extensions of finitary PCF with those features are also undecidable [22, 21]. In view of the above situation, the general goals of our research project are: (i) to understand and analyze the source of undecidability of various extensions of finitary PCF; (ii) to find decidable fragments of those extensions; (iii) to use them for improving automated higher-order program verification tools like MoCHi; and also (iv) possibly to redesign functional languages to help programmers write “well-behaved” programs that can be mapped (by predicate abstraction, etc.) to the decidable fragments. As a first step towards the goals, this paper makes the following contributions. • We provide simple proofs for the undecidability of (the reachability problem of) four simple extensions of finitary PCF: (a) finitary PCF with exceptions, (b) one with algebraic effects, (c) one with name creation and equality check, and (d) one with Boolean references. The undecidability of those extensions is either known ([22] for (a); [21] for (b); and [12] (Theorem 3) and [31] (Lemma 39) for (d) ) or probably folklore (for (c)), but the simplicity and uniformity (via encoding of Minsky machines) of our proofs may help us better understand and analyze the source of undecidability, and find decidable fragments. • We give a decidable fragment of finitary PCF with Boolean references. This is obtained by controlling the usage of (function) closures containing Boolean references by using a kind of linear types. Our technique has been partially inspired by recent work on the use of ownerships for automated verification of first-order programs [23, 39], but to our knowledge, ours is the first to handle higher-order recursive functions. We allow closures to capture references and even other closures containing references, like let x = ref true let rec f() = ... x := not(!x) ... let rec g() = ... f() ... Here, a reference cell x is captured by the function f (so that the value of x changes each time f is called), and f is further captured by g (so that the call of g may also change the value of x). The decidability is obtained by a translation from the fragment into finitary PCF. The improvement of existing verification tools (such as MoCHi) and redesign of a functional language (stated above as the general research goals (iii) and (iv)) are out of the scope of the present paper, but we hint on how our result may be used to achieve such goals, and report preliminary experimental results to show that would indeed be the case. The rest of this paper is structured as follows. Section 2 reviews Boolean PCF and Minsky machines. Section 3 shows the undecidability of the reachability problem for closed terms of the four extensions of Boolean PCF. Section 4 introduces a fragment of Boolean PCF with references, and shows that the reachability problem is decidable for that fragment. Section 4.5 discusses applications of the result to automated verification of higher-order functional programs with references and reports preliminary experiments. Section 5 discusses related work, and Section 6 concludes the paper."
